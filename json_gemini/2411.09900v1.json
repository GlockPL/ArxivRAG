{"title": "Statistical Analysis of Policy Space Compression Problem", "authors": ["Majid Molaei", "Marcello Restelli", "Alberto Maria Metelli", "Matteo Papini"], "abstract": "Policy search methods are crucial in reinforcement\nlearning, offering a framework to address continuous\nstate-action and partially observable problems. How-\never, the complexity of exploring vast policy spaces\ncan lead to significant inefficiencies. Reducing the\npolicy space through policy compression emerges as\na powerful, reward-free approach to accelerate the\nlearning process. This technique condenses the policy\nspace into a smaller, representative set while main-\ntaining most of the original effectiveness. Our re-\nsearch focuses on determining the necessary sample\nsize to learn this compressed set accurately. We em-\nploy R\u00e9nyi divergence to measure the similarity be-\ntween true and estimated policy distributions, estab-\nlishing error bounds for good approximations. To\nsimplify the analysis, we employ the l\u2081 norm, de-\ntermining sample size requirements for both model-\nbased and model-free settings. Finally, we corre-\nlate the error bounds from the l\u2081 norm with those\nfrom R\u00e9nyi divergence, distinguishing between poli-\ncies near the vertices and those in the middle of\nthe policy space, to determine the lower and upper\nbounds for the required sample sizes.", "sections": [{"title": "Introduction", "content": "Over recent decades, reinforcement learning [6]\nhas become a powerful tool for tackling sequential\ndecision-making under uncertainty, with notable suc-\ncesses in areas such as game-solving, robotic manipu-\nlation, and text generation. However, achieving these\nbreakthroughs often requires reinforcement learning\nalgorithms to be trained on a massive number of sam-\nples collected from the environment, limiting their ap-\nplicability to a broader range of real-world problems.\nA significant contributor to this sample inefficiency\nis the vast size of the so-called policy space the set\nof decision strategies from which the optimal one is\nlearned. The more decision strategies available, the\ngreater the potential for achieving desirable perfor-\nmance, but this also makes the learning process less\nefficient.\nA key approach in reinforcement learning (RL) for\nsolving complex decision-making problems is the use\nof policy search methods [1]. These methods involve\ndefining a parametric policy space, where the pol-\nicy is represented as a function parameterized by a\nset of variables. By optimizing these parameters,\npolicy search aims to discover a policy that maxi-\nmizes the expected reward in a given environment.\nThis approach allows for flexibility in representing\na wide range of policies, making it particularly ef-\nfective for handling high-dimensional and continuous\naction spaces, as well as for tasks that require so-\nphisticated strategies. Notable methods in this area\ninclude policy gradient techniques like REINFORCE\n[3] and trust region policy optimization (TRPO) [2],\nwhich have been widely used in both discrete and\ncontinuous action spaces.\nRecently, online learning methods have been pro-\nposed that perform policy search over a set of finite\nstochastic policies, providing theoretical guarantees\nabout regret minimization [5, 4]. However, the effec-\ntiveness of these approaches depends heavily on how\nwell the set of candidate policies covers the broader\npolicy space. A poor representation of the policy\nspace can lead to suboptimal performance, as the pol-\nicy search may overlook better strategies. Thus, con-\nstructing a well-distributed and diverse set of policies\nis crucial for the success of these methods."}, {"title": "Preliminaries", "content": "In this section, we introduce essential concepts and\nmathematical formulations necessary for understand-\ning the subsequent sections of this paper. These in-\nclude controlled Markov processes, policy optimiza-\ntion, importance sampling, and R\u00e9nyi divergence."}, {"title": "Controlled Markov Processes", "content": "A Controlled Markov Process (CMP) [7] is a mathe-\nmatical model used to describe a system that transi-\ntions between states based on specific actions. For-\nmally, it is defined as a tuple:\n\u039c := (S, \u0391, \u03a1, \u03bc, \u03b3).\nwhere:\n\u2022 S: The set of all possible states of the system.\n\u2022 A: The set of all possible actions that an agent\ncan take.\n\u2022P:S\u00d7A \u2192 A(S): The transition model, where\nP(s's, a) represents the probability of transition-\ning to state s' from state s when action a is\ntaken.\n\u2022 \u03bc: \u0394(S): The initial state distribution, indicat-\ning the probability distribution over states at the\nbeginning of the process.\n\u2022 \u03b3\u03b5 (0,1]: The discount factor, which models\nthe preference for immediate rewards over future\nrewards.\nAn agent interacts with the CMP through a policy\n\u03c0\u03bf : S \u2192 \u0394(A), where \u03c0\u03bf(\u03b1|s) represents the prob-\nability of taking action a in state s. The policy is\noften parameterized by 0, which can be adjusted to\noptimize performance."}, {"title": "Policy Optimization", "content": "Policy Optimization (PO) is a central concept in re-\ninforcement learning, focusing on finding the policy\n\u03c0\u03bf that maximizes the expected cumulative reward.\nThe performance of a policy is defined by the objec-\ntive function:\n$J(0) := E_{\\substack{t=0}} \\left[\\gamma^{t} R(s_{t}, a_{t})\\right] = \\frac{1}{1-\\gamma} E_{(s,a)~d_\\theta} [R(s, a)].$\nwhere R(s, a) is the reward received after taking ac-\ntion a in state s. This expectation is taken over the\nstate-action distribution induced by the policy \u03c0\u03b8.\nTo estimate J(0), we can use a Monte Carlo method\nwith a batch of N samples {sn, an}_1 generated by\npolicy \u03c0\u03c1:\n$\\widehat{J}(\\theta) = \\frac{1}{(1-\\gamma)N} \\sum_{n=1}^{N}R(S_n, a_n).$"}, {"title": "Importance Sampling and R\u00e9nyi Divergence", "content": "Importance Sampling (IS) [9] [10] is a technique used\nto estimate properties of a particular distribution\nwhile only having samples generated from a different\ndistribution. This is particularly useful in policy op-\ntimization, where we might want to evaluate a target\npolicy \u03c0\u03b8 using samples from a behavior policy \u03c0\u03b8.\nThe importance weight, which re-weights the sam-\nples from the behavior policy to represent the target\npolicy, is given by:\n$w_{\\theta^{\\prime}/\\theta}(s, a) := \\frac{d_{\\theta^{\\prime}}(s, a)}{d_\\theta(s, a)}$\nUsing these weights, a Monte Carlo estimate of the\nperformance of the target policy \u03c0\u03bf\u03b9 can be expressed\nas:\n$\\widehat{J}_{IS}(\\theta^{\\prime}/\\theta) = \\frac{1}{(1-\\gamma)N} \\sum_{n=1}^{N}w_{\\theta^{\\prime}/\\theta} (S_n, a_n)R(S_n, a_n).$\nThe variance of the importance weights is related to\nthe exponentiated 2-R\u00e9nyi divergence $D_2$, a measure\nof similarity between two distributions. The variance\ncan be expressed as:\n$Var_{(s,a)~d_\\theta} [w_{\\theta^{\\prime}/\\theta}(s, a)] = D_2 (d_{\\theta^{\\prime}}||d_\\theta) - 1.$\nwhere the exponentiated 2-R\u00e9nyi divergence is de-\nfined as:\n$D_2(d_{\\theta^{\\prime}}||d_\\theta) := \\sum_{s\\in S} \\sum_{a\\in A} d_{\\theta}(s,a) \\left(\\frac{d_{\\theta^{\\prime}}(s, a)}{d_\\theta(s, a)}\\right)^2$\nThe variance of the IS estimator is bounded by:\n$Var_{(s,a)~d_\\theta} [\\widehat{J}_{IS}(\\theta^{\\prime}/\\theta)] \\leq (\\frac{R_{max}}{1-\\gamma})^2 \\frac{D_2(d_{\\theta^{\\prime}}||d_\\theta)}{N}$"}, {"title": "Concentration Bounds for Empir-ical Estimates", "content": "Before moving forward with the analysis, it is worth\nreporting two useful results on the Hoeffding-type\nconcentration of the empirical estimates [14] [13] [11].\nLemma 2.1. Let {X}1 be i.i.d. random values\nover [a] such that P(X\u2081 = m) = pm, and let spm ="}, {"title": "Problem Formulation", "content": "Policy space compression aims to identify a set of\nK representative policies whose corresponding state-\naction distributions are close enough (according to\nsome divergence D(\u00b7||\u00b7)) to the state-action distribu-\ntion de of any policy \u03c0e in the original policy space.\nIn particular, we want to guarantee that for every\npolicy \u03c0\u03bf there exists at least one policy in the com-\npressed set such that the divergence D(.||.) between\ntheir state-action distributions is less than a thresh-\nold \u03c3:\n$\\max_\\theta \\min_{\\kappa\\in[K]} D(d_\\theta||d_{\\theta_\\kappa}) \\leq \\sigma.$\nIn the following, we will consider two divergences:\n\u2022 Total variation\n$D_{TV} (d_\\theta||d_{\\theta_\\kappa}) = \\sum_{s\\in S} \\sum_{a\\in A} |d_\\theta(s, a) \u2013 d_{\\theta_\\kappa}(s, a)|$\n\u2022 Exponentiated R\u00e9yni divergence of order 2\n$D_2 (d_\\theta||d_{\\theta_\\kappa}) = \\sum_{s\\in S} \\sum_{a\\in A} \\frac{d_\\theta^2(s, a)}{d_{\\theta_\\kappa} (s,a)}$\nIn this paper, we assume access to an optimization\noracle, allowing us to separate the statistical analysis\nfrom the computational aspects of the problem. At\nthis stage, our primary focus is to analyze the policy\nspace compression problem from a statistical stand-\npoint.\nDerived from the optimization problem mentioned\nabove, the statistical study of the compression prob-\nlem can be defined as follows: Determining the num-\nber of samples needed to ensure, with a predefined\nprobability 8, that each estimated vector of the state-\naction pair distributions induced by the k representa-\ntive policies with parameters 01, ..., \u03b8k (i.e., do\u2081, ...,\ndor), is within o distance of the true vector (i.e., de1,\ndok). All these vectors have |SA| components.\nSo, the problem can be formalized using the following\ninequality expression:\n$P (D(d_{\\theta_\\kappa}||d_{\\theta_\\kappa}) > \\sigma) \\leq \\delta, \\forall \\kappa \\in [K],$\nwhere \u03b4\u2208 [0,1] is a fixed confidence, and ois the\nthreshold to the estimation error measured according\nto the selected divergence."}, {"title": "Estimation Error Range", "content": "Let's first study the role of the estimation error\nthreshold o in the statistical study of the policy space\ncompression problem. First, we will revise the geom-\netry of the problem. Then, we will discuss how to set\nthe error threshold properly."}, {"title": "Geometry of the Problem", "content": "The policy space is a bounded convex polytope with\n|SA| zero-dimensional vertices (the i-th vertex will\nbe denoted by (s,a)i), ($|S|*|A|\\choose{1}$) one-dimensional edges,\n($|S|*|A|\\choose{2}$) two-dimensional faces, and so on, up to the\noriginal |SA|-dimensional polytope, which is essen-\ntially a shape similar to the convex hull of all the\nvertices.\nThe space of all state-action distributions is an |SA|-\ndimensional polytope, where the vertices represent\nthe situation in which a single state is visited with\nprobability one and a single action is taken in that\nstate with probability one.\nHaving established the basic geometry, it is worth ex-\namining the values of the parameter o, i.e., the esti-\nmation error threshold, from a geometric standpoint."}, {"title": "Upper Bound to the Error Thresh-old", "content": "It can be shown that \u03c32 \u2265 |SA| or $\u03c3_{TV} \\geq \\frac{|SA|-1}{\\sqrt{|SA|}}$\nis not meaningful, as demonstrated by the following\nproposition:\nProposition 1. If the threshold 02 \u2265 |SA| or $\u03c3_{TV} >\\frac{|SA|-1}{\\sqrt{|SA|}}$, then even without a single sample, we can\nbe assured that the estimated state-action pair distri-\nbution is within o distance of its exact value. This\nis because, in this case, with one representative pol-\nicy the policy that induces a uniform distribution\nover all state-action pairs-all the points inside the\npolytope space are within o distance of that represen-\ntative policy."}, {"title": "Analysis using Total Varia-tion", "content": "In this section, we provide lower bounds on the num-\nber of samples needed to ensure, in high probability\n(1-8), that the estimation error, measured using the\ntotal variation, is less than \u03c3\u03c4\u03bd:\n$P (D_{TV} (d_{\\theta_\\kappa}||d_{\\theta_\\kappa}) > \\sigma_{TV}) \\leq \\delta, \\forall \\kappa \\in [K].$\nIn particular, leveraging Lemmas 2.1 and 2.2, we\nwill provide results for scenarios where the transition\nmodel P is either known or unknown."}, {"title": "Number of Samples with KnownModel", "content": "In the case where the transition model P is known, to\ndetermine the number of samples required to ensure\nthat the estimated state-action pair distribution of a\nspecific policy is within a \u03c3\u03c4\u03bd distance of its exact\nvalue, we can apply Lemma 2.2 directly:\n$2exp \\{-\\frac{\\gamma_0^2}{(2-\\gamma_0)}N\\sigma_{TV}^2 \\}= \\delta$\n$\\Rightarrow N = \\frac{8(2-\\gamma_0)}{\\gamma_0^2\\sigma_{TV}^2}ln(\\frac{2}{8}).$\nWe should note that these samples can be collected\nby the agent interacting with the environment under\nour policy.\nFor K policies, to ensure that the total variation dis-\ntance between the exact and estimated values is at\nmost \u03c3\u03c4\u03bd, the number of samples N needed is:\n$N = \\frac{2K(2 - \\gamma_0)}{\\gamma_0^2\\sigma_{TV}^2}ln(\\frac{2}{8})$"}, {"title": "Number of Samples with Un-known Model", "content": "In case the transition model P is unknown, consider\nLemma 2.1 and also the following lemma:\nLemma 5.1. The Total Variation divergence be-\ntween the state-action distributions induced by over"}, {"title": "Analysis using R\u00e9nyi Diver-gence", "content": "In this section, we provide the sample complexity re-\nsults in the case where the R\u00e9nyi divergence is used\nto measure the distance between the state-action dis-\ntributions induced by the policies.\n$P (D_2(d_{\\theta_\\kappa}||d_{\\theta_\\kappa}) > \\sigma_2) \\leq \\delta, \\forall \\kappa \\in [K].$\nTo exploit the results provided in the previous sec-\ntion, we need to establish a connection between the\nthreshold considered for the Total Variation diver-\ngence (orv) and the one for the R\u00e9nyi divergence\n(\u03c32)."}, {"title": "The Relationship between \u03c32 and\u03c3\u03c4\u03bd", "content": "In the following, we will derive upper and lower\nbounds to the threshold in Total Variation given the\nthreshold in the 2-R\u00e9nyi divergence. First of all, we\nneed to make some considerations about the thresh-\nold to be considered in the case of the R\u00e9nyi diver-\ngence:\n\u2022 For a specific value of 02, the acceptable error\nrange for our K policies depends on the loca-\ntion of their state-action pair distribution vector\nwithin the polytope space. We can show that\nas we move towards the vertices of the polytope,\nthe acceptable error range becomes tighter.\n\u2022 For policies where the state-action pair distribu-\ntion vector is near a vertex, the tightest accept-\nable error range is achieved when the probabil-\nity value for the state-action pair associated with\nthat vertex is fixed."}, {"title": "Upper Bound to the Total Variationgiven the 2-R\u00e9nyi divergence", "content": "Here, we want to establish how large the Total Vari-\nation divergence between two distributions can be\ngiven their exponentiated 2-R\u00e9nyi divergence. To do\nthis, we consider as a representative distribution der\nthe one with uniform probabilities over all the state-\naction pairs. In fact, to maximize the Total Varia-\ntion divergence between two distributions subject to\na constraint on the 2-R\u00e9nyi divergence, one of the\ndistributions needs to be the uniform one. To find\na policy at the 62 distance from our representative\npolicy Ok, we can use the following lemma:"}, {"title": "Conclusion", "content": "This paper addresses a critical challenge in reinforce-\nment learning-sample inefficiency-by investigating\nthe statistical foundations of policy space compres-\nsion. By reducing the size of the policy space while\npreserving most of its decision-making capabilities,\npolicy compression accelerates learning in environ-\nments with vast state-action spaces. Our research\noffers a detailed analysis of the sample complexity\nrequired to accurately learn this compressed policy\nset.\nWe achieve this by leveraging R\u00e9nyi divergence to\nmeasure the discrepancy between the true and esti-\nmated policy distributions, ultimately deriving error\nbounds that ensure near-optimal policy performance\nwith high confidence. Additionally, we simplify our\nanalysis through the use of the l\u2081 norm, determining\nsample size requirements for both model-based and\nmodel-free settings. The correlation between the er-\nror bounds from the l\u2081 norm and those from R\u00e9nyi\ndivergence enables us to establish clear upper and\nlower bounds on the necessary sample sizes, distin-\nguishing between policies near the vertices and those\nin the middle of the policy space.\nUltimately, our findings provide valuable insights into\nthe sample efficiency of policy compression, offering\na framework that balances learning speed and deci-\nsion accuracy. This work lays the groundwork for\nfuture research in enhancing reinforcement learning\nalgorithms by addressing the fundamental issue of\nsample complexity through the lens of policy space\ncompression."}]}