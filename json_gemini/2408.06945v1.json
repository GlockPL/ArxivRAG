{"title": "Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation", "authors": ["Yanjie Dong", "Haijun Zhang", "Gang Wang", "Shisheng Cui", "Xiping Hu"], "abstract": "Abstract-By using an parametric value function to replace the Monte-Carlo rollouts for value estimation, the actor-critic (AC) algorithms can reduce the variance of stochastic policy gradient so that to improve the convergence rate. While existing works mainly focus on analyzing convergence rate of AC algorithms under Markovian noise, the impacts of momentum on AC algorithms remain largely unexplored. In this work, we first propose a heavy-ball momentum based advantage actor-critic (HB-A2C) algorithm by integrating the heavy-ball momentum into the critic recursion that is parameterized by a linear function. When the sample trajectory follows a Markov decision process, we quantitatively certify the acceleration capability of the proposed HB-A2C algorithm. Our theoretical results demonstrate that the proposed HB-A2C finds an e-approximate stationary point with O(\u20ac\u00af\u00b2) iterations for reinforcement learning tasks with Markovian noise. Moreover, we also reveal the dependence of learning rates on the length of the sample trajectory. By carefully selecting the momentum factor of the critic recursion, the proposed HB-A2C can balance the errors introduced by the initialization and the stoschastic approximation.", "sections": [{"title": "I. INTRODUCTION", "content": "In model-free reinforcement learning (MFRL) algorithms, an agent optimizes a long-term cumulative reward (a.k.a., value function) by interacting with an unknown stochastic environment that can be articulated as a Markov decision process (MDP). When combined with the function approximators (e.g., linear approximators and neural networks), the MFRL algorithms have achieved human-level control and extraordinary empirical success in many domains, e.g., video games [1], robotic control [2], [3], autonomous vehicles [4], [5], and linear quadratic control tasks [6], [7].\nThe current MFRL algorithms can be classified into three categories, i.e., policy-based MFRL [8]\u2013[14], value-based MFRL [7], [15]\u2013[18], and actor-critic MFRL algorithms [19], [20]. The policy-based MFRL algorithms aim at optimizing the behavior policy based on the policy gradient theorem [21]. When using a parametric policy, the policy-based MFRL can directly optimize the policy parameters via the stochastic gradient descent (SGD) [8]\u2013[11]. However, the policy-based MFRL algorithms require access to the gradient of the value function with respect to a given policy. In practical scenarios with the unknown transition kernels for the MDP, the policy gradients should be estimated from the Monte-Carlo roll-outs. Consequently, the policy-based MFRL algorithms often encounter significant variance in policy gradients and high sampling costs due to the stochastic approximation. Besides, the policy-based MFRL algorithms demand for sufficiently small learning rates to guarantee the stable convergence under the function approximators. Therefore, the policy-based MFRL algorithms can suffer from a slow convergence. While appropricte geometry engineering can improve the convergence [12]-[14], it is still in high demand for reducing the gradient variance of the policy-based MFRL algorithms.\nThe value-based MFRL algorithms recursively update the the long-term cumulative rewards for different state-action pairs based on the Bellman equation and determine the policy based on the action-value function, e.g., Q-learning [18]. Moreover, SARSA can speed the learning process of Q-learning by by using the policy improvement operators [16]. By estimating the value of successor states via the bootstrapping operation, the value-based MFRL algorithms can efficiently converge to a satisfying behavior policy based on the fixed-point recursions. Besides, the value-based MFRL can also be used to evaluate a behavior policy so that to track the future rewards of all states, e.g., temporal-difference (TD) learning algorithms [15]. When parameterizing the value function via myopical function approximators, the value- based MFRL algorithms become unstable or diverge for the environments with continuous state and/or action spaces. Therefore, an extensive hyperparameter tunning can be re- quired to obtain stable behavior policy when using value- based MFRL algorithms. To handle the sample inefficiency and the divergence of the aforementioned MFRL algorithms, recent researches aim at reducing the variance of policy gradient by integrating the policy evalution into the policy improvement so that to propose the actor-critic (AC) algo- rithms [19]\u2013[22]. More specficially, the AC algorithms are designed to use a critic recursion to estimate the value of a current policy and then apply an actor recursion to improve the behavior policy based on feedback from the critic [23].\nThe current AC algorithms can be categorized into double-loop AC algorithms and single-loop AC algorithms. In the context of double-loop setting, the critic is consecutively updated for several rounds to obtain an accurate value estima-"}, {"title": "II. PRELIMINARIES", "content": "We consider an MDP that is described by a quintuple (S, A, P, r, \u03b3), where A is the continuous action space, S is the continuous state space, IP is the unknown transition kernel that maps each state-action pair (s, a) \u2208 S \u00d7 A to a distri- bution IP(s, a) over state space S, r : S \u00d7 A \u2192 [-Rr, Rr] specifies the bounded reward for state-action pair (s, a), and y is the discount factor.\nA policy maps state s to a distribution \u03c0(\u00b7|s) over the action sapce A. To evaluate the expected discounted reward starting from a state so under the policy \u03c0, the value function is defined as\n$$V(s) = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^k r(s_k, a_k) \\middle| s_0 = s \\right]$$\nwhere each action ak follows the policy \u03c0(sk), and the successor state Sk+1 ~ P(\u00b7|sk, a\u03ba).\nGiven a policy \u03c0, the value function (1) satisfies the Bellman equation as [15], [21], [32]\n$$V(s) = \\mathbb{E}[r(s, a) + \\gamma V(s')]$$\nwhere the expectation is taken over the action a ~ \u03c0(\u00b7|s) and the successor state s' ~ IP(\u00b7|s, a).\nThe objective is to estimate the optimal policy \u03c0* so that to maximize the expected discounted reward J(\u03c0) as\n$$\u03c0^* \\in \\arg \\max_\\pi J(\\pi) := (1 - \\gamma)\\mathbb{E}[V(s)].$$\nWhen considering the continuous state and action spaces, it becomes computational burdensome to obtain the optimal policy \u03c0* or even intractable due to the notorious issue of curse of dimensionality (CoD). One popular way to handle the CoD issue is to approximate each policy \u03c0\u03ba and the value function V(s) by a neural network and a linear-function approximator, respectively. In this work, the policy \u03c0(as) and the value function V(s) are respectively parameterized by the actor parameter v \u2208 Rdv and the critic parameter w\u2208 Rdw. More specficially, the parametric policy is denoted by \u03c0(as) = \u03c0\u03c5(as), and the parametric value function is denoted by V(s) \u2248 Vw(s) = $\u2020(s)w with ||w|| \u2264 Rw and the feature embedding $(s) satisfying ||$(s)|| < 1, s \u2208 S. Note that the optimal value V*(s) = Vw*(s) when the radius Rw is sufficient large [15].\nBased on the parametric policy \u03c0\u03c5, parametric value func- tion Vw, and the Bellman equation (2), we can recast the ob- jective in (3) as a bilevel optimization that optimize the actor"}, {"title": "III. HEAVY-BALL BASED ACTOR-CRITIC FOR RL TASKS", "content": "We consider a fully data-driven technique that maintains a running estimate of the value function (cf. the inner prob- lem in (4)) while performing policy updates based on the estimated state values (cf. the outer problem in (4)). A multi- step bootstrapping is employed to estimate the target value Vtar(s). One of the merits of multi-step bootstrapping is the ability to balance bias and variance during the estimation of the target value. Furthermore, as we will justify later, the multi-step bootstrapping allows for a larger learning rate when solving the inner problem of (4) using recursive updates, thereby reducing the number of recursions required for the critic parameter. Consequently, we consider the MDP to operate on two timescales, where each coarse-grain slot (i.e., frame) consists of T fine-grain slots (i.e., T steps). For notational brevity, we recast the reward r(sk,t, ak,t), the feature embedding $(sk,t), the policy \u03c0\u03c5\u03ba, and the the optimal critic w as rk,t := r(sk,t, ak,t), $k,t := $(Sk,t), \u03c0\u03c5\u03ba := \u03c0\u03ba, and w: w, respectively.\nInner optimization. For the inner optimization, the critic parameter w can be updated via the stochastic semi-gradient\n$$g(w_k; O_k) = \\mathbb{E}[V_\\omega(s_{k,0}) - V_{w_k}(s_{k,0})]\\nabla V_{w_k}(s_{k,0})$$\nwhere the parametric value Vwk (Sk,0) = $wk; the tar- get value V (Sk,0) is estimated by a T-step bootstrap- ping as Vu (sk,0) = \u03a3\u03c4ork,t + \u03b3\u2122\u03c6\u03ba\u03c4\u03c9\u03ba; \u039f\u03ba =\n[Ok,t] is the T-step trajectory; and the observation Ok,t = (Sk,t, Ak,t, Sk,t+1) follows the distribution Pk,t \u2297 \u03c0\u03ba & P with Pk,t := P(Sk,t \u2208 \u00b7 Sk,0; Uk) as the t-step transition kernel.\nThe compact form stochastic semi-gradient is denoted by\n$$g(w_k; O_k) = \\Phi_k w_k - b_k$$\nwhere \u03a6k = \u03a6k,0[\u03a6k,o\u2212\u03b3\u03a6k,T]\u2020 and bk = \u03a6k,0 \u03a3\u03c4=0 \u03b3\u2122rk,t The stochastic semi-gradient (6) equals to the sum of full semi-gradient and gradient bias as\n$$g(w_k; O_k) = \\mathbb{E}[g(w_k; \\bar{O}_k)] + \\zeta(v_k, w_k; O_k)$$\nwhere the gradient bias is (vk, Wk; Ok) := \u03a6kwk \u2013 bk \u2013 E[g(wk; Ok)], and the full semi-gradient E[g(wk;\u014ck)] = E[\u03a6kwk \u2013 bk] with \u03a6k = \u03a6k,0[\u03a6\u03ba,0 \u2013 \u03b3\u03a6k,T]\u2020 and bk ="}, {"title": "IV. RESULTS", "content": ""}]}