{"title": "Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation", "authors": ["Yanjie Dong", "Haijun Zhang", "Gang Wang", "Shisheng Cui", "Xiping Hu"], "abstract": "Abstract-By using an parametric value function to replace the Monte-Carlo rollouts for value estimation, the actor-critic (AC) algorithms can reduce the variance of stochastic policy gradient so that to improve the convergence rate. While existing works mainly focus on analyzing convergence rate of AC algorithms under Markovian noise, the impacts of momentum on AC algorithms remain largely unexplored. In this work, we first propose a heavy-ball momentum based advantage actor-critic (HB-A2C) algorithm by integrating the heavy-ball momentum into the critic recursion that is parameterized by a linear function. When the sample trajectory follows a Markov decision process, we quantitatively certify the acceleration capability of the proposed HB-A2C algorithm. Our theoretical results demonstrate that the proposed HB-A2C finds an \\(\\epsilon\\)-approximate stationary point with O(\\({\\epsilon}^{-2}\\)) iterations for reinforcement learning tasks with Markovian noise. Moreover, we also reveal the dependence of learning rates on the length of the sample trajectory. By carefully selecting the momentum factor of the critic recursion, the proposed HB-A2C can balance the errors introduced by the initialization and the stoschastic approximation.\n\nIndex Terms-Acceleration, actor-critic algorithms, heavy-ball momentum.", "sections": [{"title": "I. INTRODUCTION", "content": "In model-free reinforcement learning (MFRL) algorithms, an agent optimizes a long-term cumulative reward (a.k.a., value function) by interacting with an unknown stochastic environment that can be articulated as a Markov decision process (MDP). When combined with the function approximators (e.g., linear approximators and neural networks), the MFRL algorithms have achieved human-level control and extraordinary empirical success in many domains, e.g., video games [1], robotic control [2], [3], autonomous vehicles [4], [5], and linear quadratic control tasks [6], [7].\n\nThe current MFRL algorithms can be classified into three categories, i.e., policy-based MFRL [8]\u2013[14], value-based MFRL [7], [15]\u2013[18], and actor-critic MFRL algorithms [19], [20]. The policy-based MFRL algorithms aim at optimizing the behavior policy based on the policy gradient theorem [21]. When using a parametric policy, the policy-based MFRL can directly optimize the policy parameters via the stochastic gradient descent (SGD) [8]\u2013[11]. However, the policy-based MFRL algorithms require access to the gradient of the value function with respect to a given policy. In practical scenarios with the unknown transition kernels for the MDP, the policy gradients should be estimated from the Monte-Carlo roll-outs. Consequently, the policy-based MFRL algorithms often encounter significant variance in policy gradients and high sampling costs due to the stochastic approximation. Besides, the policy-based MFRL algorithms demand for sufficiently small learning rates to guarantee the stable convergence under the function approximators. Therefore, the policy-based MFRL algorithms can suffer from a slow convergence. While appropricte geometry engineering can improve the convergence [12]\u2013[14], it is still in high demand for reducing the gradient variance of the policy-based MFRL algorithms.\n\nThe value-based MFRL algorithms recursively update the the long-term cumulative rewards for different state-action pairs based on the Bellman equation and determine the policy based on the action-value function, e.g., Q-learning [18]. Moreover, SARSA can speed the learning process of Q-learning by by using the policy improvement operators [16]. By estimating the value of successor states via the bootstrapping operation, the value-based MFRL algorithms can efficiently converge to a satisfying behavior policy based on the fixed-point recursions. Besides, the value-based MFRL can also be used to evaluate a behavior policy so that to track the future rewards of all states, e.g., temporal-difference (TD) learning algorithms [15]. When parameterizing the value function via myopical function approximators, the value-based MFRL algorithms become unstable or diverge for the environments with continuous state and/or action spaces. Therefore, an extensive hyperparameter tunning can be required to obtain stable behavior policy when using value-based MFRL algorithms. To handle the sample inefficiency and the divergence of the aforementioned MFRL algorithms, recent researches aim at reducing the variance of policy gradient by integrating the policy evalution into the policy improvement so that to propose the actor-critic (AC) algorithms [19]\u2013[22]. More specficially, the AC algorithms are designed to use a critic recursion to estimate the value of a current policy and then apply an actor recursion to improve the behavior policy based on feedback from the critic [23]. The current AC algorithms can be categorized into double-loop AC algorithms and single-loop AC algorithms. In the context of double-loop setting, the critic is consecutively updated for several rounds to obtain an accurate value estima-"}, {"title": "II. PRELIMINARIES", "content": "We consider an MDP that is described by a quintuple (S, A, P, r, \\(\\gamma\\)), where A is the continuous action space, S is the continuous state space, IP is the unknown transition kernel that maps each state-action pair (s, a) \\(\\in\\) S \\(\\times\\) A to a distribution IP(s, a) over state space S, r : S \\(\\times\\) A \\(\\rightarrow\\) [-Rr, Rr] specifies the bounded reward for state-action pair (s, a), and \\(\\gamma\\) is the discount factor.\n\nA policy maps state s to a distribution \\(\\pi\\)(\\cdot|s) over the action sapce A. To evaluate the expected discounted reward starting from a state s0 under the policy \\(\\pi\\), the value function is defined as\n\n\\(V(s) = E[\\sum_{k=0}^{\\infty} \\gamma^{k} r(s_{k}, a_{k}) | s_{0}=s]\\)   (1)\n\nwhere each action ak follows the policy \\(\\pi\\)(sk), and the successor state sk+1 ~ P(\\({\\cdot}|s_{k}, a_{k}\\)).\n\nGiven a policy \\(\\pi\\), the value function (1) satisfies the Bellman equation as [15], [21], [32]\n\n\\(V(s) = E[r(s, a) + \\gamma TV(s')]\\)  (2)\n\nwhere the expectation is taken over the action a ~ \\(\\pi\\)(\\cdot|s) and the successor state s' ~ IP(\\({\\cdot}|s, a\\)).\n\nThe objective is to estimate the optimal policy \\(\\pi\\)* so that to maximize the expected discounted reward J(\\({\\pi}\\)) as\n\n\\(\\pi\\)* \\(\\in\\) arg \\(max_{\\pi} J(\\pi) := (1 - \\gamma)E[V(s)]\\). (3)"}, {"title": "A. Problem description", "content": "We consider an MDP that is described by a quintuple (S, A, P, r, \\(\\gamma\\)), where A is the continuous action space, S is the continuous state space, IP is the unknown transition kernel that maps each state-action pair (s, a) \\(\\in\\) S \\(\\times\\) A to a distribution IP(s, a) over state space S, r : S \\(\\times\\) A \\(\\rightarrow\\) [-Rr, Rr] specifies the bounded reward for state-action pair (s, a), and \\(\\gamma\\) is the discount factor.\n\nA policy maps state s to a distribution \\(\\pi\\)(\\cdot|s) over the action sapce A. To evaluate the expected discounted reward starting from a state s0 under the policy \\(\\pi\\), the value function is defined as\n\n\\(V(s) = E[\\sum_{k=0}^{\\infty} \\gamma^{k} r(s_{k}, a_{k}) | s_{0}=s]\\)   (1)\n\nwhere each action ak follows the policy \\(\\pi\\)(sk), and the successor state sk+1 ~ P(\\({\\cdot}|s_{k}, a_{k}\\)).\n\nGiven a policy \\(\\pi\\), the value function (1) satisfies the Bellman equation as [15], [21], [32]\n\n\\(V(s) = E[r(s, a) + \\gamma TV(s')]\\)  (2)\n\nwhere the expectation is taken over the action a ~ \\(\\pi\\)(\\cdot|s) and the successor state s' ~ IP(\\({\\cdot}|s, a\\)).\n\nThe objective is to estimate the optimal policy \\(\\pi\\)* so that to maximize the expected discounted reward J(\\({\\pi}\\)) as\n\n\\(\\pi\\)* \\(\\in\\) arg \\(max_{\\pi} J(\\pi) := (1 - \\gamma)E[V(s)]\\). (3)"}, {"title": "B. Function approximation", "content": "When considering the continuous state and action spaces, it becomes computational burdensome to obtain the optimal policy \\(\\pi\\)* or even intractable due to the notorious issue of curse of dimensionality (CoD). One popular way to handle the CoD issue is to approximate each policy \\(\\pi\\)k and the value function V(s) by a neural network and a linear-function approximator, respectively. In this work, the policy \\(\\pi\\)(a|s) and the value function V(s) are respectively parameterized by the actor parameter v \\(\\in\\) Rdv and the critic parameter w \\(\\in\\) Rdw. More specficially, the parametric policy is denoted by \\(\\pi\\)(a|s) = \\(\\pi\\)v(a|s), and the parametric value function is denoted by V(s) \\(\\approx\\) Vw(s) = \\({\\phi}\\)(s)tw with ||w|| \\(\\leq\\) Rw and the feature embedding \\({\\phi}\\)(s) satisfying ||\\({\\phi}\\)(s)|| < 1, s \\(\\in\\) S. Note that the optimal value V*(s) = Vw*(s) when the radius Rw is sufficient large [15].\n\nBased on the parametric policy \\(\\pi\\)v, parametric value function Vw, and the Bellman equation (2), we can recast the objective in (3) as a bilevel optimization that optimize the actor"}, {"title": "III. HEAVY-BALL BASED ACTOR-CRITIC FOR RL TASKS", "content": "We consider a fully data-driven technique that maintains a running estimate of the value function (cf. the inner problem in (4)) while performing policy updates based on the estimated state values (cf. the outer problem in (4)). A multi-step bootstrapping is employed to estimate the target value Vtar(s). One of the merits of multi-step bootstrapping is the ability to balance bias and variance during the estimation of the target value. Furthermore, as we will justify later, the multi-step bootstrapping allows for a larger learning rate when solving the inner problem of (4) using recursive updates, thereby reducing the number of recursions required for the critic parameter. Consequently, we consider the MDP to operate on two timescales, where each coarse-grain slot (i.e., frame) consists of T fine-grain slots (i.e., T steps). For notational brevity, we recast the reward r(sk,t, ak,t), the feature embedding \\({\\phi}\\)(sk,t), the policy \\(\\pi\\)vk, and the the optimal critic w as rk,t := r(sk,t, ak,t), \\({\\phi}\\)k,t := \\({\\phi}\\)(Sk,t), \\(\\pi\\)vk := \\(\\pi\\)k, and w := w, respectively.\n\nInner optimization. For the inner optimization, the critic parameter w can be updated via the stochastic semi-gradient\n\n\\(g(w_{k}; O_{k}) = [\\nabla_{W}(S_{k,0}) - V_{w_{k}}(S_{k,0})]\\nabla V_{w_{k}}(S_{k,0})\\)  (5)\n\nwhere the parametric value Vwk (Sk,0) = \\({\\phi}\\)twk; the target value Vtar (Sk,0) is estimated by a T-step bootstrapping as Vtar (sk,0) = \\(\\sum_{t=0}^{T-1}\\) \\(\\gamma\\)tr\\({k,t}\\) + \\(\\gamma\\)t\\({\\phi}\\)kwk; Ok := [Ok,t] is the T-step trajectory; and the observation Ok,t = (Sk,t, Ak,t, Sk,t+1) follows the distribution Pk,t \\(\\otimes\\) \\(\\pi\\)k \\(\\otimes\\) P with Pk,t := P(Sk,t \\(\\in\\) \\(\\cdot\\)|Sk,0; vk) as the t-step transition kernel.\n\nThe compact form stochastic semi-gradient is denoted by\n\n\\(g(w_{k}; O_{k}) = {\\Phi}_{k}w_{k} - b_{k}\\)  (6)\n\nwhere \\({\\Phi}\\)k = \\({\\phi}\\)k,0[\\({\\phi}\\)k,0 - \\(\\gamma\\)\\({\\phi}\\)k,T]t and bk = \\({\\phi}\\)k,0 \\(\\sum_{t=0}^{T-1}\\) \\(\\gamma\\)trk,t.\n\nThe stochastic semi-gradient (6) equals to the sum of full semi-gradient and gradient bias as\n\n\\(g(w_{k}; O_{k}) = E[g(w_{k}; \\overline{O_{k}})] + \\zeta (v_{k}, w_{k}; O_{k})\\)  (7)\n\nwhere the gradient bias is \\(\\zeta\\)(vk, wk; Ok) := \\({\\Phi}\\)kwk \u2013 bk - E[g(wk; \\overline{O_{k}})], and the full semi-gradient E[g(wk; \\overline{O_{k}})] = E[\\({\\Phi}\\)kwk \u2013 bk] with \\({\\Phi}\\)k = \\(\\sum_{t=0}^{T-1}\\) \\({\\phi}\\)k,0 = \\(\\sum_{t=0}^{T-1}\\)\\({\\phi}\\)k,t. Denoting the \\(\\pi\\)k-induced stationary distribution by \\(\\mu\\)k, the T-step sample trajectory Ok is obtained from the stationary distribution \\(\\mu\\)k \\(\\otimes\\) \\(\\pi\\)k \\(\\otimes\\) P."}, {"title": "A. Algorithm development", "content": "We consider a fully data-driven technique that maintains a running estimate of the value function (cf. the inner problem in (4)) while performing policy updates based on the estimated state values (cf. the outer problem in (4)). A multi-step bootstrapping is employed to estimate the target value Vtar(s). One of the merits of multi-step bootstrapping is the ability to balance bias and variance during the estimation of the target value. Furthermore, as we will justify later, the multi-step bootstrapping allows for a larger learning rate when solving the inner problem of (4) using recursive updates, thereby reducing the number of recursions required for the critic parameter. Consequently, we consider the MDP to operate on two timescales, where each coarse-grain slot (i.e., frame) consists of T fine-grain slots (i.e., T steps). For notational brevity, we recast the reward r(sk,t, ak,t), the feature embedding \\({\\phi}\\)(sk,t), the policy \\(\\pi\\)vk, and the the optimal critic w as rk,t := r(sk,t, ak,t), \\({\\phi}\\)k,t := \\({\\phi}\\)(Sk,t), \\(\\pi\\)vk := \\(\\pi\\)k, and w := w, respectively.\n\nInner optimization. For the inner optimization, the critic parameter w can be updated via the stochastic semi-gradient\n\n\\(g(w_{k}; O_{k}) = [\\nabla_{W}(S_{k,0}) - V_{w_{k}}(S_{k,0})]\\nabla V_{w_{k}}(S_{k,0})\\)  (5)\n\nwhere the parametric value Vwk (Sk,0) = \\({\\phi}\\)twk; the target value Vtar (Sk,0) is estimated by a T-step bootstrapping as Vtar (sk,0) = \\(\\sum_{t=0}^{T-1}\\) \\(\\gamma\\)tr\\({k,t}\\) + \\(\\gamma\\)t\\({\\phi}\\)kwk; Ok := [Ok,t] is the T-step trajectory; and the observation Ok,t = (Sk,t, Ak,t, Sk,t+1) follows the distribution Pk,t \\(\\otimes\\) \\(\\pi\\)k \\(\\otimes\\) P with Pk,t := P(Sk,t \\(\\in\\) \\(\\cdot\\)|Sk,0; vk) as the t-step transition kernel.\n\nThe compact form stochastic semi-gradient is denoted by\n\n\\(g(w_{k}; O_{k}) = {\\Phi}_{k}w_{k} - b_{k}\\)  (6)\n\nwhere \\({\\Phi}\\)k = \\({\\phi}\\)k,0[\\({\\phi}\\)k,0 - \\(\\gamma\\)\\({\\phi}\\)k,T]t and bk = \\({\\phi}\\)k,0 \\(\\sum_{t=0}^{T-1}\\) \\(\\gamma\\)trk,t.\n\nThe stochastic semi-gradient (6) equals to the sum of full semi-gradient and gradient bias as\n\n\\(g(w_{k}; O_{k}) = E[g(w_{k}; \\overline{O_{k}})] + \\zeta (v_{k}, w_{k}; O_{k})\\)  (7)\n\nwhere the gradient bias is \\(\\zeta\\)(vk, wk; Ok) := \\({\\Phi}\\)kwk \u2013 bk - E[g(wk; \\overline{O_{k}})], and the full semi-gradient E[g(wk; \\overline{O_{k}})] = E[\\({\\Phi}\\)kwk \u2013 bk] with \\({\\Phi}\\)k = \\(\\sum_{t=0}^{T-1}\\) \\({\\phi}\\)k,0 = \\(\\sum_{t=0}^{T-1}\\)\\({\\phi}\\)k,t. Denoting the \\(\\pi\\)k-induced stationary distribution by \\(\\mu\\)k, the T-step sample trajectory Ok is obtained from the stationary distribution \\(\\mu\\)k \\(\\otimes\\) \\(\\pi\\)k \\(\\otimes\\) P."}, {"title": "B. Convergence analysis", "content": "Hereinafter, our goal is to analyze the convergence rate of the proposed HB-A2C algorithm for a realistic setting where the transitions are sampled along a trajectory of the MDP. To proceed, we need the ensuing assumptions on behavior to facilitate our analysis."}, {"title": "Algorithm 1 HB-A2C Algorithm", "content": "1: Initialization: critic hyper-parameters: stepsize \\(\\beta\\), parameter w0, momentum factor \\(\\eta\\)1, and momentum parameter \\(\\eta\\)\u22121 = 0; actor hyper-parameters: stepsize \\(\\alpha\\) and parameter v0\n2: for k = 0, 1, ..., K \u2212 1 do\n3: Rolling out T-step observations Ok = [ok,t]=0 via the behavior \\(\\pi\\)k\n4: Update the critic parameter based on (6) as\n\n\\(\\eta_{k} = (1 - \\eta_{1})\\eta_{k-1} + \\eta_{1}g(w_{k}; O_{k})\\) (12a)\n\n\\(w_{k+1} = proj R [w_{k} - \\beta\\eta_{k}]\\)   (12b)\n\nwhere the operator \\(\\Pi\\)R[] denotes the projection onto the region ||w|| < Rw\n5: Calculate the stochastic policy gradient based on (11) as\n\n\\(H (v_{k}, w_{k}; O_{k}) = (1 - \\gamma) \\sum_{t=0}^{T-1}\\gamma^{t}h(v_{k}, w_{k}; O_{k,t})\\)   (13)\n6: Update the actor parameter based on (13) as\n\n\\(v_{k+1} = v_{k} + \\alpha H (v_{k}, w_{k}; O_{k})\\)  (14)\n7: end for\n\nAssumption 1: For each state-action pair (s, a) \\(\\in\\) (S, A), the behavior policy satisfies\n\n|| \\(log \\pi_{v}(a|s)\\) || \\(\\leq\\) R\\(\\pi\\) (15a)\n\n||\\(log \\pi_{v}(a|s) - \\nabla log \\pi_{v'}(a|s)\\)|| \\(\\leq\\) Lt, ||v \u2013 v'|| (15b)\n\n|\\(\\pi\\)v(a|s) \u2013 \\(\\pi\\)(a|s)| \\(\\leq\\) L\\(\\pi\\)||v \u2013 v' || (15c)\n\nwhere v, v' \\(\\in\\) Rdv.\n\nAssumption 1 is standard for the analysis of policy gradient based methods, see e.g., [10], [16], [20], [24], [28]\u2013[31]. The Lipschitz continuity assumption holds for canonical parametric policies, such as, the Gaussian policy [33] and Boltzman policy [34]. Assumption 1 guarantees that the expected discounted reward J(v) has L-Lipschitz continuous gradient as\n\n\\((VJ(v), v' \u2013 v) \\leq\\) J(v') \u2013 J(v) + \\frac{1}{2}L||v' \u2013 v||2 (16)\n\nwhere v \\(\\in\\) Rdv and v' \\(\\in\\) Rdv. The detailed derivations are relegated to Lemma 6 in Appendix.\n\nAssumption 2: For each behavior \\(\\pi\\)k, the induced Markov chain is ergodic and has a stationary distribution \\(\\mu\\)k with \\(\\mu\\)k(s) > 0, s \\(\\in\\) S. Moreover, there exist contants c0 and \\(\\rho\\) \\(\\in\\) (0,1) such that\n\n||Pk,t - \\mu\\)k ||TV < C0\\(\\rho\\)t   (17)\n\nwhere the total variation distance for the two probability measures Pk,t and \\(\\mu\\)k is defined as ||Pk,t - \\(\\mu\\)k||tv = \\(\\int\\), |Pk,t(s) \u2013 \\(\\mu\\)k(s)| ds.\n\nThe first part of Assumption 2 (i.e., ergodicity) ensures that all states are visited an infinite number of times and the existence of a mixing time for the MDP. The second part (i.e., the mixing time of the policy in (17)) guarantees that the optimal policy can be obtained from a single sample trajectory of the MDP. It is worth remarking that Assumption 2 is a standard requirement for theoretical analysis of the RL algorithms; see e.g., [15], [26], [28], [31], [35]."}, {"title": "III. HEAVY-BALL BASED ACTOR-CRITIC FOR RL TASKS", "content": "We consider a fully data-driven technique that maintains a running estimate of the value function (cf. the inner problem in (4)) while performing policy updates based on the estimated state values (cf. the outer problem in (4)). A multi-step bootstrapping is employed to estimate the target value Vtar(s). One of the merits of multi-step bootstrapping is the ability to balance bias and variance during the estimation of the target value. Furthermore, as we will justify later, the multi-step bootstrapping allows for a larger learning rate when solving the inner problem of (4) using recursive updates, thereby reducing the number of recursions required for the critic parameter. Consequently, we consider the MDP to operate on two timescales, where each coarse-grain slot (i.e., frame) consists of T fine-grain slots (i.e., T steps). For notational brevity, we recast the reward r(sk,t, ak,t), the feature embedding \\({\\phi}\\)(sk,t), the policy \\(\\pi\\)vk, and the the optimal critic w as rk,t := r(sk,t, ak,t), \\({\\phi}\\)k,t := \\({\\phi}\\)(Sk,t), \\(\\pi\\)vk := \\(\\pi\\)k, and w := w, respectively.\n\nInner optimization. For the inner optimization, the critic parameter w can be updated via the stochastic semi-gradient\n\n\\(g(w_{k}; O_{k}) = [\\nabla_{W}(S_{k,0}) - V_{w_{k}}(S_{k,0})]\\nabla V_{w_{k}}(S_{k,0})\\)  (5)\n\nwhere the parametric value Vwk (Sk,0) = \\({\\phi}\\)twk; the target value Vtar (Sk,0) is estimated by a T-step bootstrapping as Vtar (sk,0) = \\(\\sum_{t=0}^{T-1}\\) \\(\\gamma\\)tr\\({k,t}\\) + \\(\\gamma\\)t\\({\\phi}\\)kwk; Ok := [Ok,t] is the T-step trajectory; and the observation Ok,t = (Sk,t, Ak,t, Sk,t+1) follows the distribution Pk,t \\(\\otimes\\) \\(\\pi\\)k \\(\\otimes\\) P with Pk,t := P(Sk,t \\(\\in\\) \\(\\cdot\\)|Sk,0; vk) as the t-step transition kernel.\n\nThe compact form stochastic semi-gradient is denoted by\n\n\\(g(w_{k}; O_{k}) = {\\Phi}_{k}w_{k} - b_{k}\\)  (6)\n\nwhere \\({\\Phi}\\)k = \\({\\phi}\\)k,0[\\({\\phi}\\)k,0 - \\(\\gamma\\)\\({\\phi}\\)k,T]t and bk = \\({\\phi}\\)k,0 \\(\\sum_{t=0}^{T-1}\\) \\(\\gamma\\)trk,t.\n\nThe stochastic semi-gradient (6) equals to the sum of full semi-gradient and gradient bias as\n\n\\(g(w_{k}; O_{k}) = E[g(w_{k}; \\overline{O_{k}})] + \\zeta (v_{k}, w_{k}; O_{k})\\)  (7)\n\nwhere the gradient bias is \\(\\zeta\\)(vk, wk; Ok) := \\({\\Phi}\\)kwk \u2013 bk - E[g(wk; \\overline{O_{k}})], and the full semi-gradient E[g(wk; \\overline{O_{k}})] = E[\\({\\Phi}\\)kwk \u2013 bk] with \\({\\Phi}\\)k = \\(\\sum_{t=0}^{T-1}\\) \\({\\phi}\\)k,0 = \\(\\sum_{t=0}^{T-1}\\)\\({\\phi}\\)k,t. Denoting the \\(\\pi\\)k-induced stationary distribution by \\(\\mu\\)k, the T-step sample trajectory Ok is obtained from the stationary distribution \\(\\mu\\)k \\(\\otimes\\) \\(\\pi\\)k \\(\\otimes\\) P."}, {"title": "B. Convergence analysis", "content": "Hereinafter, our goal is to analyze the convergence rate of the proposed HB-A2C algorithm for a realistic setting where the transitions are sampled along a trajectory of the MDP. To proceed, we need the ensuing assumptions on behavior to facilitate our analysis."}, {"title": "Before characterizing the convergence properties of the proposed HB-A2C algorithm, we start by establishing several lemmas.", "content": "Lemma 1: The (stochastic) semi-gradient of critic and (stochastic) policy gradient of actor are bounded as\n\n||g(wk; Ok)|| \\(\\leq\\) Rg, ||E[g(wk; \\overline{O_{k}})]|| \\(\\leq\\) Rg   (18a)\n\n||H(vk, wk; Ok)|| \\(\\leq\\) Rh, ||\\(\\nabla\\)J(vk)|| \\(\\leq\\) Rh  (18b)\n\nwhere Rg = (1+\\(\\gamma\\)T)Rw+c1(\\(\\gamma\\))Rr and Rh = R\\(\\pi\\)[Rr+(1+\\(\\gamma\\))Rw] with c1(\\(\\gamma\\)) = \\((1-\\gamma^{T})/(1-\\gamma)\\).\n\nProof: See Appendix A.\n\nLemma 1 provides bounds for both the (stochastic) semi-gradient of the critic and the (stochastic) policy gradient of the actor that are useful for controlling (i.e., upper-bounding) the drifts of the critic and actor parameters as follows.\n\nThe recursion in (12a) can be recast as \\(\\eta\\)k = \\(\\eta\\)1 \\(\\sum_{t=0}^{k}\\)(1-\\({\\eta}_{1}\\))k-t+ g(wt; Ot) when \\(\\eta\\)\u22121 = 0. Therefore, the upper bound of ||\\(\\eta\\)k|| is derived as\n\n||\\(\\eta\\)k|| = || \\(\\eta\\)1 \\(\\sum_{t=0}^{k}\\)(1-\\({\\eta}_{1}\\))k-t+ g(wt; Ot) || \\(\\leq\\) Rg.  (19)\n\nBased on the recursion in (12b) and (19), we obtain the one-frame drift of critic as\n\n||wk+1 - wk|| \\(\\leq\\) ||\\(\\beta\\eta\\)k|| \\(\\leq\\) Rg\\(\\beta\\)  (20)\n\nwhere the first inequality follows from the non-expansive property of the projection operation.\n\nBased on (14) and Lemma 1, we obtain the one-frame drift of actor as\n\n||vk+1 - vk|| \\(\\leq\\) Rh\\(\\alpha\\).   (21)\n\nBased on Lemma 1 and (20), we can investigate the properties of the gradient bias term \\(\\zeta\\)(vk, wk; Ok).\n\nLemma 2: Suppose Assumptions 1 and 2 hold. When length of each trajectory satisfies T > \\(log c_{0}^{-1}\\)\\(\\beta\\) / log \\(\\rho\\), the gradient bias \\(\\zeta\\)(vk, wk; Ok) satisfies\n\n||\\(\\zeta\\)(vk, wk; Ok) \u2013 \\(\\zeta\\)(vk, wk\u22121; Ok)|| \\(\\leq\\) 8Rg\\(\\beta\\)  (22)\n\nand\n\n||E[\\(\\zeta\\)(vk, wk\u22121; Ok)|Fk\u22121]|| (23)\n\nwhere C2 > 0, and Fk-1 denotes the filtration that contains all randomness prior to frame k 1.\n\nProof: See Appendix B.\n\nLemma 2 implies that: 1) the one-frame drift of gradient bias \\(\\zeta\\)(vk, \\(omega\\)k; Ok) with respect to wk can be confined by the critic stepsize \\(\\beta\\); and 2) the gradient bias does not rapidly increase with ||vk - vk\u22121 ||, which serves as one of the keys in developing our subsequent convergence of the critic sequence of the HB-A2C algorithm.\n\nBased on (6), we observe that the optimal critic w per frame k is a function of vk. Therefore, we are motivated to investigate the drift of optimal critic with respect to the actor parameters."}, {"title": "Lemma 3: When Assumption 1 is satisfied, the optimal critic parameter w per frame satisfies", "content": "||w - w* || \\(\\leq\\) L* ||v \u2013 v' ||  (24)\n\n||w|| \\(\\leq\\) G*   (25)\n\nwhere L* > 0", "Rdv.\n\nProof": "See Appendix C.\n\nLemma 3 shows that the drift of the optimal critic is controlled by the drift of the actor. Before analyzing the convergence behavior of the actor recursion (14)", "4": "When Assumption 1 is satisfied", "Rdw\n\nProof": "See Appendix D.\n\nBased on Lemma 4", "1": "Suppose Assumptions 1 and 2 hold", "J(v_{0})": "frac{1"}, {"wk.\n\nProof": "The finite-time convergence analysis of actor starts from that the expected discounted reward J(v) under state s has the L-Lipschitz continuous gradient. Together with the recursion in (14) and the inequality (21), we have\n\n\\(\\frac{1}{2}\\alpha E[\\Omega k"}]}