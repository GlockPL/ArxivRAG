{"title": "Walking in Others' Shoes: How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias", "authors": ["Rongwu Xu", "Zi\u2019an Zhou", "Tianwei Zhang", "Zehan Qi", "Su Yao", "Ke Xu", "Wei Xu", "Han Qiu"], "abstract": "The common toxicity and societal bias in contents generated by large language models (LLMs) necessitate strategies to reduce harm. Present solutions often demand white-box access to the model or substantial training, which is impractical for cutting-edge commercial LLMs. Moreover, prevailing prompting methods depend on external tool feedback and fail to simultaneously lessen toxicity and bias. Motivated by social psychology principles, we propose a novel strategy named perspective-taking prompting (PET) that inspires LLMs to integrate diverse human perspectives and self-regulate their responses. This self-correction mechanism can significantly diminish toxicity (up to 89%) and bias (up to 73%) in LLMs' responses. Rigorous evaluations and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and three open-source LLMs, revealing PET's superiority in producing less harmful responses, outperforming five strong baselines.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs; OpenAI et al. 2023; Chowdhery et al. 2023; Touvron et al. 2023; Chiang et al. 2023) excel in numerous NLP tasks, enhancing the efficiency of our work and life (Kasneci et al., 2023; Kung et al., 2023). Meanwhile, recent research pointed out that LLMs inevitably give objectionable responses, as they are pre-trained on a vast amount of unsanitized web text (Gehman et al., 2020). For instance, LLMs could output toxic content with harmful attributes (e.g., rude, disrespectful, insulting sentences) (Gehman et al., 2020). They may also generate content with societal bias (Sheng et al., 2021b), which exhibits stereotypes towards particular demographic groups, e.g., \"Asians are good at math.\u201d). It remains an ongoing endeavor to make LLMs deliver harmless and unbiased content (Gabriel, 2020; Bai et al., 2022a; Liu et al., 2023; Shen et al., 2023).\nWhile many efforts have been devoted to alleviating toxicity and bias (Weidinger et al., 2021; Mehrabi et al., 2021), existing measures exhibit two shortcomings when applied to state-of-the-art commercial LLMs, e.g., GPT-4 (OpenAI et al., 2023). (1) Impractical requirement of white-box access. Many solutions require access to the model's internal representations (Leong et al., 2023) or control decoding processes (Krause et al., 2021; Liu et al., 2021), which is impossible to deploy on commercial LLMs that only reveal limited logits. (2) Huge training cost. Some solutions require domain-specific training, which is very cost-prohibitive (Gururangan et al., 2020). While they may work for older models like GPT-2, it is difficult to extend them to up-to-date LLMs (Gou et al., 2023), which have significantly distinct behaviors and features (c.f. Table 1).\nDriven by these issues, in this study, we concentrate on the black-box scenario. However, we notice two limitations of existing measures. (1) Single-issue focus. One issue is their focus on addressing a single type of problematic behavior while neglecting the need for concurrent adjustments across various problematic attributes. More seriously, Yang et al. (2022) point out some detoxification techniques (Liu et al., 2021) may inadvertently exacerbate bias. (2) External tool reliance."}, {"title": "2 Related Work", "content": "Existing measures (Gou et al., 2023; Dhingra et al., 2023) require external tool feedback to adjust responses. This dependence can vary effectiveness, hinder adaptability, and slow deployments due to the varying speed restrictions\u00b9 of external tools.\nTo combat the aforementioned drawbacks (c.f. Figure 1) and explore the potential of LLMs, we propose Perspective-Taking prompting (PET), a prompting schema for LLMs to self-reduce the toxic and biased contents in their responses. Inspired by social psychology theories, we leverage perspective-taking (Batson et al., 1997), a core emotional intelligence skill, that can empower individuals to self-regulate by leveraging self-awareness and empathy. Particularly, our solution consists of two methods: PET-IO (Perspective-Taking: Imagine Others) and PET-IS (Perspective-Taking: Imagine self). The former elicits the LLM to imagine how others feel, while the latter instructs the LLM to feel as others (see \u00a7 3.2 for details). Then, we use the above two methods to explore LLM's ability to self-adjust its responses for mitigating toxic and biased generations concurrently.\nWe conduct extensive experiments on two commercial LLMs, ChatGPT (OpenAI, 2023) and GLM (Du et al., 2022). We observe that perspective-taking prompting significantly outperforms the intrinsic self-correct scheme investigated by (Krishna, 2023) and also outperforms two strong baselines with external feedback (Gou et al., 2023; Dhingra et al., 2023). Our key insight drawn from the exemplary performance of PET is: LLMs show the potential to generate responses with reduced toxicity and bias solely on their own."}, {"title": "2.1 Detoxification and Debiasing", "content": "Our research is closely related to toxicity and bias reduction in NLG tasks. Existing strategies can be classified broadly as additional training and inference-time intervention.\nDetoxification. Additional training strategies using filtered or augmented corpora with non-toxic data to further pretraining or finetuning the model (Gehman et al., 2020; Gururangan et al., 2020; Dale et al., 2021; Wang et al., 2022; Lu et al., 2022). More recently, RLHF (Ouyang et al., 2022; Ganguli et al., 2023) and RLAIF (Bai et al., 2022b) are also implemented to fine-tune the LLM to align"}, {"title": "2.2 Self-Correct", "content": "LLMs can self-correct themselves using natural language feedback (Pan et al., 2023). Here we discuss inference-time correction without training (Welleck et al., 2022; Ganguli et al., 2023; Huang et al., 2023a). Intrinsic methods rely on internally generated feedback, exemplified by Self-refine (Madaan et al., 2023) and Self-check (Miao et al., 2023), while extrinsic methods, like Reflexion (Shinn et al., 2023) and CRITIC (Gou et al., 2023), rely on external sources. It has been argued that intrinsic correction poses greater challenges (Huang et al., 2023b; Gou et al., 2023). While existing research"}, {"title": "2.3 Emotional Intelligence and LLMs", "content": "Recent research highlights that LLMs can comprehend and generate emotion (Wang et al., 2023a; Li et al., 2024). However, there is a limited exploration of using human emotional skills to enhance LLMs. While Li et al. (2023) examines the impact of emotional prompts on LLMs' problem-solving and generation, we focus on mitigating harmful responses. A recent essay by Kidder et al. (2024) raises questions about LLMs' genuine empathy, prompting our investigation into its intrinsic and practical value in AI. Our paper answers this call by presenting a valuable step forward."}, {"title": "3 Perspective-Taking Prompting", "content": "In social psychology, emotional intelligence (EI) helps individuals regulate themselves by leveraging self-awareness and empathy. This enables them to predict and lessen harm from others, thus promoting positive social outcomes (Goleman, 1998; Bar-On, 2006; Salovey and Sluyter, 1997). Perspective-taking, which is considered a vital EI skill, is a cognitive functioning (Piaget, 1934) and recognized as part of Kohlberg's classification of moral reasoning (Kohlberg, 1921). Perspective-taking has shown positive influence in improving intergroup relationships (Todd and Galinsky, 2014), decreasing sterotype expressing (Galinsky and Moskowitz, 2000), reducing prejudge (Vescio et al., 2003), and combating racial bias (Todd et al., 2011).\nPerspective-taking involves imagining how others feel (\"imagine other\u201d) and how the protagonist would feel (\"imagine self\u201d) (Batson et al., 1997; Lamm et al., 2007; Batson, 2012). It typically specifies a scenario that includes multiple human participants, such as encountering someone in need or hearing a friend's distressing experience. Adopting the perspective of others is the key element of perspective-taking, which is known to evoke empathy (Batson et al., 1997; Davis, 2018)."}, {"title": "3.2 Proposed Method", "content": "Figure 2 illustrates the overall workflow of our PET method. It begins by instructing the LLM to construct a context with (human) audiences. Subsequently, it employs a set of perspective-taking prompts to facilitate the LLM in understanding others' viewpoints. The generated perspectives are then utilized for self-correction of its initial response. Below we expound the detailed steps.\nStep I: Constructing context with audiences. To incorporate perspective-taking in the context of LLM's generation, the first step is to establish a context with \u201cothers\u201d. Given that user prompts may not always inform about certain participants or events, the LLM needs to construct a pervasive context. A practical approach is to consider the situation from the viewpoint of diverse audiences. This enables the model to better anticipate the potential reactions and emotions of different individuals, thereby reducing the likelihood of generating harmful content. We utilize the following prompt, where {Context} is set like \u201ca media platform\u201d:\nConstructing context with audiences\nTreat {Initial Response} as a comment. Given that this comment will be posted on {Context}, what are the possible audiences? Try to imagine different audiences among diverse demographic groups.\nIt is worth noting that while this approach considers multiple audiences' perspectives, it differs from role play-based solutions where the LLM assumes an entirely new persona (Wang et al., 2023b). In our setup, the LLM maintains its identity but adopts a third-person perspective to understand the perceptions and emotions of audiences, rather than directly embodying these different roles.\nStep II: Perspective-taking prompting. Upon establishing the context, we employ either one of the two distinct perspective-taking approaches as identified by Batson et al. (1997). The first approach, which is referred to as the \"imagine other\" technique (dubbed PET-IO), involves imagining how others perceive a situation and what they feel.\nPerspective-taking (imagine others)\nFor each of the audience, try to imagine how this audience feels about this comment and how it would affect his or her life. Try not to concern yourself with attending to all the information presented. Just concentrate on trying to imagine how this audience feels when reading the comment.\nThe second approach, known as the \u201cimagine-self\" technique (dubbed PET-IS in our research), entails projecting oneself to another's position and considering how one would feel.\nPerspective-taking (imagine self)\nFor each of the audience, imagine you were him or her. While reviewing this comment, try to imagine how you would feel about it if you were him or her, and how it would affect your life. Try not to concern yourself with attending to all the information presented. Just concentrate on trying to imagine how you would feel.\nWith one of the above two prompts, as we have already established multiple audiences, the LLM is verbally instructed to engage in perspective-taking across all these audiences in this context. According to Batson and colleagues' research (Batson et al., 1997; Batson, 2012), these two perspective-taking methods are unique and can lead to different outcomes when used by humans, prompting us to treat them as separate strategies in our study with LLMs. Following (Vescio et al., 2003; Lamm et al., 2007; Todd et al., 2011), we adopt the perspective-taking instructions outlined in Batson et al. (1997) in our prompting. See \u00a7 A.3 for detailed prompts.\nStep III: Self-correction. This step is similar to the practice established in (Madaan et al., 2023; Krishna, 2023). We leverage the LLM-generated perspectives as natural language feedback, guiding it in revising its initial response. Unlike certain self-correction methods, we conduct the self-correction only once without iterative prompting (c.f. \u00a7 4.5), to reduce the operational costs of re-prompting.\""}, {"title": "4 Experiments", "content": "We apply perspective-taking prompting (PET) to two representative facets in harmful content reduction, detoxification, and debiasing."}, {"title": "4.1 Experimental Setup", "content": "We select two datasets on NLG based on the given prompts, for detoxification and debiasing."}, {"title": "4.1.1 Datasets", "content": "RTP-High. For toxicity assessment, we select the RealToxicityPrompts (RTP) dataset (Gehman et al., 2020), containing ~100K prompts which can be used to elicit potential toxic completions. As per Huang et al. (2023c) and Zhuo et al. (2023), content generated by up-to-date LLMs exhibits extremely low toxicity using existing datasets\u00b2. Hence, following Leong et al. (2023), we first select a subset for easier observations (30, 152 prompts with toxicity scores > 0.5). We then leverage ChatGPT to generate completions and use PERSPECTIVE API to measure their toxicity. This results in 1,604 prompts with toxicity score > 0.3 3.\nBOLD-1.5K. For bias assessment, we consider global bias which is evaluated on sentence-level semantics instead of local bias evaluated at a particular generation time step (Liang et al., 2021). We choose the BOLD dataset (Dhamala et al., 2021), containing ~23K text generation prompts mentioning specified demographic groups across five domains. Following Yang et al. (2022), we consider two domains: gender (with male and female being the subgroups\u2074) and race (European, Asian, and African). Following Xiong et al. (2023), we drop the Hispanic subgroup (with 103 prompts) in the race domain due to its limited size. Subsequently, we uniformly sample 0.5K and 1K samples from the gender and race domains respectively to form the test set. We conduct the Mann-Whitney U"}, {"title": "4.1.2 Models", "content": "We consider two popular commercial LLMs5, ChatGPT (OpenAI, 2023) (the gpt-3.5-turbo variant) and GLM (Du et al., 2022) (the glm3-turbo variant). Note that neither of them has publicly disclosed the model size. Following Sheng et al. (2019, 2021b); Liang et al. (2021), we use sampling decoding (Holtzman et al., 2020). Our hyperparameter configuration follows Yang et al. (2022), with top-p = 0.9, and temperature T = 0.7. In line with prior studies (Gehman et al., 2020; Yang et al., 2022; Leong et al., 2023), for each prompt, we let the models generate 25 completions for assessing toxicity and 20 for assessing bias."}, {"title": "4.1.3 Baselines", "content": "We compare our method with five representative black-box detoxification and debiasing baselines.\nBase (Krishna, 2023) prepends a simple regulation prompt like \"Please provide contents without toxic/bias contents\" before the user prompt.\nPre-hoc (Si et al., 2022) inserts a more systematic prompt before the user prompt. We largely follow the original prompt and adapt it to detoxification.\nSelf-Correct (Krishna, 2023) instructs the LLM to revise its initial output specifically to decrease toxic/biased content, building upon the initial response generated by the Base method.\nCRITIC#6 (Gou et al., 2023) is an extrinsic self-correct method which uses the feedback from the PERSPECTIVE API, which indicates numerical scores relevant to problematic contents.\nSHAP (Dhingra et al., 2023) is another extrinsic self-correct method which revises sensitive vocabularies identified by a SHAP explainer on top of an external toxic/bias detection model.\nPET. For both PET-IO and PET-IS, we configure the LLM to imagine 5 different audiences in constructing the context. See \u00a7 A.3 for detailed descriptions on methods."}, {"title": "4.1.4 Metrics", "content": "Toxicity. Following previous works (Gehman et al., 2020; Pozzobon et al., 2023; Leong et al., 2023), we report Expected Maximum Toxicity (denoted by E.M.T.), Toxicity Probability (T.P.) (Gehman et al., 2020), and Toxic Fraction (T.F.) (Liang et al., 2022) in our experiments. Following Leong et al. (2023) who leverage a fine-tuned LM to evaluate toxicity, we employ the R4 model from (Vidgen et al., 2021) to compute toxicity scores.\nBias. Currently, there are no single canonical metrics for NLG debiasing measurements. Here we take two prevalent measures including Sentiments (used by Dhamala et al. (2021); Kocielnik et al., 2023; Banerjee et al., 2023)) and Regards (used by Liang et al. (2021); Yang et al. (2022)). Following Dhamala et al. (2021), we use sentiments towards different sub-groups as a metric. We report Mean Sentiments (S.-\u03bc), Deviation of Sentiments (S.-\u03c3) (Banerjee et al., 2023), and Average Group Fairness (G.F.) (Huang et al., 2020). As also recommended by Dhamala et al. (2021), we use VADAR (Hutto and Gilbert, 2014) to compute the sentiments. Meanwhile, we also take Regard scores into consideration (Sheng et al., 2019) to avoid experimentally biased evaluations (Sheng et al., 2021b). Following (Liang et al., 2021; Yang et al., 2022), we use the regards difference towards subgroups. We report Average Regards Difference (R.D.) in our evaluation. For both sentiments and regards, we compute scores at the domain-level.\nGeneration quality. Following related work (Liu et al., 2021; Smith et al., 2022; Hallinan et al., 2022), generation quality is included in our evaluation. In specific, we report fluency, relevance, and diversity. Fluency is measured by mean Perplexity (PPL), calculated using GPT-2. Relevance is characterized by the semantics similarity (Sim.) between the Base's completion and a certain method's response. Following Hallinan et al. (2022), we use BERTScore (Zhang et al., 2020) to compute the similarity. Following (Liu et al., 2021), we report diversity (Dist.-n8), which is measured using the mean number of distinct n-grams, normalized by the text length (Li et al., 2016). To avoid potential confusion, see \u00a7 A.4 for details on these metrics."}, {"title": "4.2 Main Results: PET is Highly Effective", "content": "Results in Table 1 reveal the following findings: (1) ChatGPT and GLM exhibit significantly reduced"}, {"title": "4.3 Impact of Audience Numbers", "content": "The default number of audiences is set to 5 in previous results. Here, we adjust different numbers of audiences, and the results are shown in Figure 3. Generally, slightly larger audience sizes tend to yield better results, though the differences are not significant. However, when the number of audiences goes too high, e.g., 10, some metrics start to deteriorate. This might be attributed to the context generated by the model becoming excessively lengthy, affecting its ability to focus on revising its response (Zhang et al., 2023; Li, 2023)."}, {"title": "4.4 Combining PET-IO and PET-IS", "content": "We also explore combining PET-IO and PET-IS. In this process, the LLM engages in separate conversations using each strategy. The insights gained from each strategy are then aggregated to refine the initial response. This combining does not yield a substantial improvement over the standalone original approach (c.f. Table 3). Nevertheless, the hybrid strategy marginally enhances the performance evaluated by sentiment in the debiasing task."}, {"title": "4.5 Iterative Prompting", "content": "We assess the effectiveness of iterative prompting, wherein the LLM is tasked with self-correcting its responses over up to 4 iterations. Results are plotted Figure 4. We observe that this process does not improve the quality of the final outputs and sometimes worsens it. This echoes findings from (Gou et al., 2023). One possible reason could also be the lengthy context's distraction."}, {"title": "4.6 Prompt Sensitivity", "content": "To measure the prompt sensitivity of our methods, we conduct experiments on altered prompts as ex-"}, {"title": "5 Qualitative Results", "content": "We uniformly sampled 3200 model-generated samples for manual evaluation. Each sample undergoes blind reviews by three English-proficient college students serving as annotators. Following (Liang et al., 2021; Yang et al., 2022), we evaluate detoxification and debiasing effect on a 1-5 scale on toxicity, bias, and fluency, where 1 indicates the least and 5 the most toxic/biased/fluent. The results of the human evaluation are presented in Table 1 and Table 2. Overall, the human evaluation outcomes align with the automatic metrics. We report a Fleiss's Kappa of k = 0.362, indicating a fair agreement (0.21 < \u043a < 0.40) among annotators. See \u00a7 A.7 for details on human evaluation."}, {"title": "5.2 Qualitative Analysis", "content": "We list some examples of the LLM responses in Figure 5 and summarize the following findings.\nRepeating the user prompts. The LLM may include the user prompt in its response, e.g., repeating the harmful language. We consider this repetition as part of the harmful response. An ethical LLM should refrain from echoing precise harmful words, even when prompted with unethical requests.\nDeclining on user prompts. The LLMs can sometimes decline to complete the prompt, especially when it contains extremely toxic content. While this behavior reflects the model's ethical considerations, solely refusal can lower generation quality. A middle ground could involve providing an alternative response by adjusting the wording.\nIgnorance of sensitive vocabularies. Occasionally, the LLM overlooks sensitive words (e.g., offensive and sexual), even when flagged by tools such as PERSPECTIVE API). Feedback in natural language can enhance the model's focus on these words, albeit with limitations. By adopting multiple perspectives in our methods, the model can more effectively identify problematic elements.\nSemantic incoherence. We observe that the semantics of the generation can significantly differ from the user prompt, a phenomenon more prevalent in more advanced techniques which involve re-prompting (e.g., CRITIC and PET). This issue seems to stem from the complex, multi-step nature of these methods, which may cause the model to lose track of the initial sentence's semantics."}, {"title": "6 Finetune LLM using its Self-Correction", "content": "We are curious to see whether the \"quality\" revisions of the responses can further teach the LLM to learn to regulate itself. To this end, we fine-tune the LLM by using its initial and revised responses as contrasting pairs. This teaches the LLM to distinguish between harmful and harmless content and to understand the process of self-correction before finalizing its response. See \u00a7 A.8 for details.\nIntrinsic self-filtering. To eliminate external feedback, we let the model itself to self-filter its responses and find the most successful revisions it has accomplished. Specifically, we let the model assign a score s to evaluate the toxic/bias degree on both the initial response ($s_{initial}$) and revised response ($s_{revised}$) on a 1-10 scale and chose the pairs with $s_{revised}$ - $s_{initial}$ \u2265 3, which marks a substantial revision and reduce in toxicity/bias. After this, we randomly sample 800 such pairs to be used for later supervised finetuning (SFT) the model.\nSFT using self-correction data. We use OpenAI's finetune API to SFT our model, organizing response pairs into a multi-turn conversation format with self-correction, as detailed in \u00a7 A.8. The training, spanning 3 epochs. As shown in Table 4, the trained model demonstrates considerable improvements with the simple Base and Self-Correct methods. However, gains from our proposed PET approaches after SFT are not pronounced, likely because of their better initial performance. On the whole, incorporating self-correction into fine-tuning positively influences alignment."}, {"title": "7 Concluding Remarks", "content": "Our study introduces perspective-taking prompting (PET), a social psychology-inspired approach, to enable large language models (LLMs) to self-regulate and simultaneously diminish the toxicity and societal bias in their outputs. This approach, requiring no white-box control or further retraining of the LLM, has shown through extensive testing on two advanced LLMs to surpass 5 existing baselines.\nTo sum up, our findings underscore the potential of LLMs to minimize harmful content generation on their own, presenting a promising avenue for improving AI safety without external intervention."}, {"title": "Limitations", "content": "Although our work shows superior performance in terms of detoxification and debiasing, it exhibits several limitations.\nLimited model selection. Our investigation is constrained to the evaluation of two black-box LLMs, ChatGPT and GLM, which may limit the generalizability of our results, which may limit the applicability of our findings to other advanced models such as GPT-4 or Gemini. The outcomes of our method on these unexplored models remain unknown.\nLimited optimization on the exact prompt. The prompts utilized in our PET-IO and PET-IS methods are manually curated and lack extensive optimization. While we have demonstrated the effectiveness of alternative prompts in supplementary experiments (see \u00a7 A.5.3), the optimal prompt remains elusive. Regardless, our approach offers a general methodology for leveraging LLMs to facilitate efficient detoxification and debiasing. Future work could explore the integration of automatic prompt generation techniques, as proposed by (Chen et al., 2023), to enhance our method.\nHigh computational cost. We calculated the computational cost of various methods and the results are located in Table 7. Our methods, PET-IO and PET-IS, although highly effective, entail a significantly higher computational cost compared to the Base and CRITIC methods. This is primarily due to the numerous introspection steps inherent in our approach, which may necessitate computational resources proportional to the complexity of the tasks.\nLimited ethical threats considered. Our study primarily focuses on two predominant harmful contents, toxicity, and bias, and does not account for other potential threats, such as morality. An expanded consideration of these threats would provide a more holistic view of LLM ethics.\nThe selection of datasets. Budget constraints have limited the scope of our dataset, which, in turn, may restrict the generalizability of our findings. For the debiasing task, we confined our analysis to a subset of the BOLD dataset, encompassing gender and race, potentially limiting the applicability of our method across diverse social groups and bias types. Future research could mitigate these limitations by employing more comprehensive and representative datasets to assess the efficacy of our approach in different contexts.\nMixed results on open-source LLMs. As the results discussed in \u00a7 A.6, we admit that our ap-"}, {"title": "Ethics Statement", "content": "We acknowledge that LLMs can absorb, spread and even amplify toxicity and biases from their training data, leading to potentially harmful outputs. Our project aims to mitigate these issues by improving the safety of these models while recognizing the risk of over- or under-detoxification, as well as the possibility of adversaries exploiting the process. Although we strive to reduce representational harms rooted in deep historical and social structures, we clarify that our approach, including detoxification or debiasing, does not suggest complete elimination of these underlying issues, but rather a lessening of certain model behaviors. We stress that our method's potential generalizability to various ethical threats, yet we do not claim it as a comprehensive solution to all forms of harm. We call for ongoing research and monitoring to reinforce model security and develop more resilient countermeasures against potential misuse.\nFurthermore, in the context of our human evaluation experiments, it is important to note that our institution does not possess an ethical review board. Despite this limitation, we are committed to adhering to the ethical guidelines established by the Association for Computational Linguistics (ACL). We strive to ensure that our research is conducted with the utmost respect for ethical considerations, even in the absence of formal board oversight.\nComputing resources. All model-based evaluations in \u00a7 4 are completed on four NVIDIA 3090 GPUs. Text generation pipelines employing open-source LLMs in \u00a7 A.6 are done on eight NVIDIA A800 80GB GPUs. Expenses on the usage of commercial API-based LLMs are reported in \u00a7 A.5.1."}]}