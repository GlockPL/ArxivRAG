{"title": "Contrast Similarity-Aware Dual-Pathway Mamba for Multivariate Time Series Node Classification", "authors": ["Mingsen Du", "Meng Chen", "Yongjian Li", "Xiuxin Zhang", "Jiahui Gao", "Cun Ji", "Shoushui Wei"], "abstract": "Multivariate time series (MTS) data is generated through multiple sensors across various domains such as engineering application, health monitoring, and the internet of things, characterized by its temporal changes and high dimensional characteristics. Over the past few years, many studies have explored the long-range dependencies and similarities in MTS. However, long-range dependencies are difficult to model due to their temporal changes and high dimensionality makes it difficult to obtain similarities effectively and efficiently. Thus, to address these issues, we propose contrast similarity-aware dual-pathway Mamba for MTS node classification (CS-DPMamba). Firstly, to obtain the dynamic similarity of each sample, we initially use temporal contrast learning module to acquire MTS representations. And then we construct a similarity matrix between MTS representations using Fast Dynamic Time Warping (FastDTW). Secondly, we apply the DPMamba to consider the bidirectional nature of MTS, allowing us to better capture long-range and short-range dependencies within the data. Finally, we utilize the Kolmogorov-Arnold Network enhanced Graph Isomorphism Network to complete the information interaction in the matrix and MTS node classification task. By comprehensively considering the long-range dependencies and dynamic similarity features, we achieved precise MTS node classification.", "sections": [{"title": "1. Introduction", "content": "In recent years, significant advancements have been made in the field of time series analysis, driven by the growing availability of complex high-dimensional data from various sources such as sensor networks [1], financial markets [2], and biomedical applications [3]. Accurately capturing and analyzing these time series data is crucial for applications such as activity recognition [4], anomaly detection [5], and predictive modeling [6].\nAmong them, MTS classification is a significant research topic with applications in various fields, such as human activity recognition [4], leading to numerous studies in recent years [7, 8]. The focus of attention has been on the similarities and long-range dependencies inherent in MTS data [9]. However, modeling MTS data presents challenges due to temporal changes that complicate the capture of long-range dependencies, as well as high dimensional characteristics that hinder the identification of similarities between samples.\nTo address the above two challenges, various methods have been proposed. For similarities extraction challenge between time series, DTW and its variants focus on measuring the dynamic similarities between time series through nonlinear alignment. While effective in capturing temporal alignment, these methods often exhibit limitations when handling high-dimensional data, particularly in terms of expensive time complexity, which is $O(mn\u00b2)$(Here, m is the dimension of the MTS and n is the length). This means that when processing long sequences and high-dimensional data, the computational overhead of DTW can become prohibitively large, leading to inefficiency. Consequently, numerous methods, such as indexing (Time Series Indexing) [10], sparsification [11], lower bounding [12], and constraint path [13] have been proposed to reduce complexity and improve computational efficiency. Another difficulty is how to align the temporal dynamics to obtain more accurate similarity. When using DTW to calculate high-dimensional MTS, the dimensions are usually added together to calculate the average, thus losing various key features of each dimension.\nFor long-range dependencies extraction challenge, deep learning methods have recently made significant progress recently. Long Short-Term Memory networks (LSTM) [14] effectively capture long-term dependencies in time series. However, LSTM models may face challenges such as high computational complexity and overfitting when dealing with long time series. Another important development is the Transformer model [15, 16, 17, 18], whose self-attention mechanism makes it possible to capture global dependencies. While Transformer suffer from quadratic complexity, leading to low computational efficiency and high costs. Recently, Selective State Space Models (SSM) like Mamba [19, 20] becomes popular, which can handle long-range dependencies in sequences while maintaining near-linear complexity, have garnered widespread attention. Numerous works have emerged in the fields of time series [20] [21], images [22], graph data [23], and natural language [24]. SSM effectively capture the dynamic properties of time series using state transition equations to describe system evolution over time. Their ability to utilize hidden states allows SSMs to simultaneously capture long-range dependencies across multiple time points, making them ideal for modeling delays and memory effects in time-dependent data.\nAfter getting similarities and long-range dependencies, to combine both is another key challenge. In recent years, Graph Neural Networks (GNN) [25, 26] have found extensive application in the field of time series analysis, particularly in handling complex dependencies within graph-structured data involving multiple time series dimensions. In time series data, GNN can combine temporal and structural features to achieve more accurate modeling and prediction. Examples include GIN [27], GCN [28], GAT [29], Spatial-Temporal GNN [30], and GraphSAGE [31]. However, many of these methods model either static [32] or dynamic dependencies [33] between dimensions. While current time series GNN are often used to obtain feature representations of time series.\nTherefore, to combine similarities and long-range dependencies, each sample in the data set can be regarded as a node in the graph. The edge relationship between nodes can be modeled using similarities, and the node features can be modeled using long-range dependencies. Recently, several researches [34, 35] combine similarities based edge relationship and node features to complete node classification.\nIn response to the above three challenges, we propose CS-DPMamba for MTS node classification. Firstly, to obtain the similarity of each sample in the dataset, we initially used TemCL to acquire time series representations and then constructed a similarity matrix using FastDTW. Then, we apply DPMamba model, which considers the bidirectional nature of time series, allowing us to better capture both long-range and short-range dependencies in the time series. Finally, we utilize the KAN-GIN network for MTS node classification. By comprehensively considering the long-range dependencies and dynamic similarity features, we achieve more accurate MTS node classification.\nThe main contributions of this study include:\n1. We propose contrast similarity-aware dual-pathway Mamba for MTS node classification and conduct experiments on the UEA dataset, demonstrating the superiority.\n2. We use temporal contrastive learning to acquire time series representations and then constructed a similarity matrix between representations using FastDTW, capturing the dynamic similarities of the time series.\n3. We use dual-pathway Mamba to extract high-level features from bidirectional time series while effectively managing complex data, capturing both long-range and short-range dependencies."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Mamba-Based Methods", "content": "Mamba was proposed [19] to address weaknesses in discrete modes by setting SSM parameters as input functions, allowing the model to selectively propagate or forget information based on the current token. Mamba integrates these selective SSMs to construct a simplified end-to-end neural network architecture that does not use attention or Multi-Layer Perceptron (MLP) block. The architecture exhibits linear scalability and can handle real-world data sequences up to millions in length. Recently, many Mamba-based methods have been proposed. TSCMamba [21] introduces a novel multi-view approach that fuses frequency-domain and time-domain features using spectral features from continuous wavelet transforms. It utilizes State Space Models (SSMs) for efficient and scalable sequence modeling, incorporating a unique tango scanning scheme to enhance sequence relationship modeling. C-Mamba [36] employs a newly developed SSM to capture cross-channel dependencies while maintaining linear complexity and preserving a global receptive field. It integrates channel mixing to enhance the training set by combining two channels, alongside a channel attention-enhanced Mamba encoder that leverages SSM to model cross-time dependencies and inter-channel correlations. FMamba [37] first extracts temporal features of input variables through an embedding layer, then calculates dependencies between input variables with a fast attention module. Finally, Mamba selectively processes the input features, and a MLP block further extracts temporal dependencies of the variables."}, {"title": "2.2. Graph-Based Methods", "content": "GNN [38] can effectively capture complex relationships between different dimensions in time series by representing these dimensions and their interactions through graph structures.\nRecently, many GNN-based methods have been proposed. Many models overlook seasonal effects and the evolving characteristics of shapelets. Time2Graph++ [39] addresses this by introducing a time-level attention mechanism to extract time-aware shapelets. Subsequently, time series data is transformed into shapelet evolution graphs to capture evolving patterns, further enhanced by graph attention to improve shapelet evolution. Existing methods (e.g., Transformer) struggle to effectively utilize spatial correlations between variables. To address this, Graphformer [40] efficiently learns complex temporal patterns and dependencies across multiple variables. It automatically infers an implicit sparse graph structure through a graph self-attention mechanism. Current methods mainly focus on temporal consistency, neglecting the importance of spatial consistency. TS-GAC [41] aims to enhance the spatial consistency of MTS data. Specifically, it introduces graph augmentation techniques for nodes and edges to maintain the stability of sensors and their associations. Subsequently, robust sensor and global features are extracted through node- and graph-level contrast. Graph networks can only capture spatiotemporal dependencies between node pairs and cannot handle higher-order correlations in time series. DHSL [42] uses the k-nearest neighbor method to generate a dynamic hypergraph structure from time series data and optimizes the structure through a hyper graph structure learning to capture higher-order correlations. Finally, the dynamically learned hypergraph structure is applied to a spatiotemporal hyper GNN. Recently, several research [34] treats time series as nodes in a graph, with node similarity calculated using DTW and incorporated into the GNN. Due to the high complexity of DTW, further research [35] uses the lower bound of DTW to calculate similarity, reducing the time complexity. However, the lower bound provides only an estimate, not an exact distance. This means that in some cases, the final similarity measure may not be accurate enough, especially in applications requiring high precision."}, {"title": "3. Methods", "content": ""}, {"title": "3.1. CS-DPMamba Overview", "content": "The overview involves the following steps, as shown in Fig. 1. The batch training with negative sampling is shown in Algorithm 1.\n\u2022 Compute similarity matrix through ContrastFastDTW (in Section 3.2, Step 1 in Algorithm 1): Use the TemCL module to get time series representations and FastDTW to compute the similarity matrix between representations based on the sampled time-series.\n\u2022 Extract long-range dependencies through DPMamba (in Section 3.3, Step 2 in Algorithm 1): Apply the DPMamba method to extract long-range dependencies from the MTS data. This step captures the relationships between different time-series data points.\n\u2022 Perform node classification (in Section 3.4, Step 3 in Algorithm 1): Use the KAN-GIN to perform node classification based on long-range dependencies features and similarity matrix.\n\u2022 Model update (Step 4 in Algorithm 1): Finally, update the model parameters using the processed batch to improve the model's accuracy over multiple iterations."}, {"title": "3.2. ContrastFastDTW based Similarity Matrix Constructing", "content": "First, to correctly align temporal dynamics and obtain more accurate similarity, we use TemCL module to obtain MTS representations (as shown in Fig. 2). And then, to obtain similarity more efficiently, we construct a similarity matrix between MTS representations using FastDTW, capturing the dynamic similarities of time series.\nThe similarity matrix is computed according to the following steps, detailed as shown in Algorithm 2 and Algorithm 3.\n\u2022 Obtain MTS Representations (step 1 in Algorithm 2): Use temporal contrastive learning to obtain representations of the time series data. This step captures the essential features of the time series.\n\u2022 Clustering (step 2 in Algorithm 2): Apply KNN clustering algorithm to get clusters based on the number of classes.\n\u2022 Compute similarities within clusters (Algorithm 3 and step 3 in Algorithm 2): Calculate the distance between the representations of the MTS using FastDTWx. If they are not in the same cluster, they are considered dissimilar and a value of -1 is assigned."}, {"title": "3.2.1. TemCL", "content": "We design temporal contrast learning module (TemCL) by leveraging self-supervised learning techniques, enhances the model's robustness to variations and noise in the time series, improves generalization performance by learning meaningful representations that can be used across different tasks. The TemCL model architecture is shown in Fig. 2. We extract temporal features by stacking multiple layers of convolution, ReLU, and max pooling modules, and construct negative samples by adding noise and cutting time series. TemCL module has the following advantages. First, the convolutional layer can effectively extract key features in the time series through the local receptive field mechanism. This feature extraction method can capture short-term and long-term pattern changes. Second, the pooling layer reduces the dimension of the data by downsampling, thereby reducing the computational complexity. Finally, the convolutional pooling structure is robust to noise and interference. Therefore, TemCL makes the similarity measurement more accurate by optimizing the feature representation."}, {"title": "3.2.2. Pairwise Distance Matrix for MTS", "content": "DTW is an algorithm used to measure the similarity between two time series signals. It allows for nonlinear time alignment, enabling the optimal matching of sequences even when they differ in speed or have temporal discrepancies. The DTW calculation follows the following steps:\na) DTW Formulation\nGiven two time series $X = (x_1, x_2,...,x_n)$ and $Y = (y_1, y_2, ..., y_m)$, where $x_i$ and $y_j$ are elements of the series, DTW aims to find a nonlinear alignment that minimizes the alignment cost.\nb) Distance Matrix Calculation\nFirst, compute the distance matrix D as Eq. (4), where each element D(i, j) represents the distance between the i-th element of X and the j-th element of Y. In Eq. (4), dist(xi, yj) is typically the Euclidean distance or another distance metric.\n$D(i, j) = dist(x_i, y_j)$ (4)\nc) Recursive Computation\nThe distance DTW(X, Y) is then recursively calculated using dynamic programming as Eq. (5). Define the cumulative distance matrix C, where C(i, j) represents the minimum cumulative distance from the start point (1, 1) to (i, j).\n$C(i, j) = dist(x_i, y_j) + min\\begin{cases} C(i - 1, j), & \\text{(vertical move)} \\\\ C(i, j \u2212 1), & \\text{(horizontal move)} \\\\ C(i - 1, j \u2212 1) & \\text{(diagonal move)} \\end{cases}$ (5)\nThe initial condition is Eq. (6). The final DTW distance is C(n, m), representing the minimum cumulative distance from the start of series X to the end of series Y.\n$C(1,1) = dist(X_1,Y_1)$ (6)\nd) FastDTW\nFastDTW [13] is an efficient algorithm for computing time series similarity, reducing the traditional $O(N^2)$ complexity of DTW to O(N) through a multiresolution approximation."}, {"title": "3.2.3. FastDTW based Matrix", "content": "Let $X = {X_1, X_2, ..., X_N }$ be a dataset of N MTS, where each $x_i \u2208 R^{T\u00d7N}$ represents a time series with T time steps and N features. The FastDTW distance between $x_i$ and $x_j$ is denoted as FastDTW($x_i$, $x_j$). We define the pairwise distance matrix $D \u2208 R^{NXN}$ as Eq. 7.\n$D_{ij} = FastDTW(x_i, x_j)$, for i, j = 1, 2, . . ., N (7)\nThe matrix D is a symmetric matrix where each entry $D_{ij}$ represents the FastDTW distance between time series $x_i$ and $x_j$ as Eq. 8.\n$D = \\begin{pmatrix} 0 & FastDTW (x_1, x_2) & ... & FastDTW (x_1, x_N) \\\\ FastDTW(x_2, X_1) & 0 & ... & FastDTW (x_2, x_N) \\\\ : & : & : & :\\\\ FastDTW(X_N, X_1) & FastDTW(X_N, X_2) & ... & 0\\end{pmatrix}$ (8)\nIn Eq. 8, $D_{ij}$ is the FastDTW distance between time series $x_i$ and $x_j$. If i = j, $D_{ii} = 0$, since the distance between a time series and itself is zero. D: A symmetric matrix where $D_{ij} = D_{ji}$, representing all pairwise FastDTW distances among the time series in the dataset."}, {"title": "3.2.4. Final Similarity Matrix", "content": "After getting the FastDTW matrix D, to further describe the similarity, we perform the following processing. First, we introduce a scaling hyperparameter \u03b1 \u2208 [0, \u221e) to control the importance of top neighbors. Specifically, let $D_{ij}$ denote the (i, j) entry of D. The adjacency matrix A is obtained by the following formula as Eq. 9.\n$A_{ij} = \\frac{1}{e^{\u03b1D_{ij}}}$ , \u2200i, j (9)\nwhere $A_{ij}$ represents the (i,j) entry of A. A larger \u03b1 will give more importance to the top neighbors.\nNext, to filter out irrelevant neighbors, we sample the top K neighbors for each node. Specifically, for each row $a_i$ in A, we keep only the K entries with the largest weights and set the others to zero, resulting in a sparse matrix. When handling similarity, we define a similarity function as Eq. 10.\n$similarity_{ij} =\\begin{cases} 1 & if A_{ij} = 0 \\\\ \\frac{1}{1+A_{ij}} & if A_{ij} > 0 \\\\ 0 & if A_{ij} < 0\\end{cases}$ (10)\nWhen $A_{ij}$ = 0, the similarity is 1, indicating maximum similarity. When $A_{ij}$ > 0, the value is calculated using $\\frac{1}{1+A_{ij}}$, which means that when $A_{ij}$ is close to 0, similarity approaches 1, while larger values of $A_{ij}$ will lead to similarity approaching 0. When $A_{ij}$ < 0, the similarity is directly set to 0, indicating complete dissimilarity.\nFinally, we normalize the adjacency matrix using the following formula as Eq. 11.\n$\u00c2_{ij} = \\frac{A_{ij}}{\u03a3_j A_{ij}}$ , \u2200i, j (11)\nIn the context of a GNN for node classification, Each MTS $x_i$ is represented as a node in a graph. The matrix D defines the edge weights between nodes, where a smaller DTW distance indicates a stronger similarity and thus a stronger edge connection between nodes i and j. The GNN can use this matrix as input to perform node classification based on the structural properties of the graph formed by these time series."}, {"title": "3.3. Dual-Pathway Processing Using Mamba Model", "content": "We apply the dual-pathway Mamba (DPMamba, as shown in Algorithm. 4 and Fig. 3) to consider the bidirectional nature of MTS, allowing us to better capture long-range and short-range dependencies.\nDPMamba captures temporal dependencies in both forward and backward directions, providing a more comprehensive modeling of time series data. The combined output from both directions can improve the accuracy of predictions, especially in cases where future data points influence past states."}, {"title": "3.3.1. State Evolution in Forward Direction", "content": "Given a MTS x(t) as input, the continuous-time state evolution is described as Eq. (12).\n$h'(t) = Ah(t) + Bx(t), y(t) = Ch(t)$ (12)\nwhere h(t) represents the hidden state at time t, x(t) is the input vector at time t, A is the state transition matrix, B maps the input x(t) to state updates, C maps the hidden state h(t) to the output y(t).\nAfter discretization, the formula for the forward direction as Eq. (13).\n$h_t = Ah_{t-1} + Bx_t, y_t = Ch_t$ (13)"}, {"title": "3.3.2. State Evolution in Reverse Direction", "content": "For reverse processing, we consider the time-reversed sequence x(t), where x(t) = x(T - t). The state evolution for the reverse direction as as Eq. (14). This reverse state evolution captures dependencies from future to past, complementing the forward processing.\n$h_t^R = Ah_{t+1}^R + Bx_t^R, y_t^R = Ch_t^R$ (14)"}, {"title": "3.3.3. Combined Bidirectional Output", "content": "The final output is a combination of the forward and reverse outputs as Eq. (15).\n$y^{final} = \u03b1y_t + \u03b2y_t^R$ (15)\nwhere \u03b1 and \u03b2 are coefficients that balance the influence of forward and reverse directions, $y^{final}$ is the final output at time t after considering both directions."}, {"title": "3.4. KAN-GIN Layer", "content": "We utilize the KAN to enhance the expressive power of the GIN for node classification. Although MLP are flexible in handling feature data, they fall short in capturing the complex dynamic characteristics of time series. In contrast, the KAN enhances expressive power, allowing for better integration of temporal dependencies and graph structure features in time series, thereby improving the accuracy of node classification.\nAfter obtaining the time series similarity matrix and the node features from the Mamba model, we proceed with node classification using the KAN-GIN model. The KAN-GIN layer is defined as Eq. (16).\n$h^{(l)}(v) = KAN^{(l)}\\Bigg((1 + \u0454) \u00b7 h^{(l-1)}(v) + \u03a3_{u\u2208N(v)} h^{(l-1)}(u) \\Bigg)$ (16)\nwhere $h^{(l)}(v)$ represents the hidden state of node v at layer l, $h^{(l-1)}(v)$ is the hidden state of node v at the previous layer l \u22121, N(v) denotes the set of neighbors of node v, \u0454 is a learnable parameter that controls the contribution of the node's own features, $KAN^{(l)}$ is the kernel adaptive network function applied at layer l."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": ""}, {"title": "4.1.1. Dataset", "content": "We selected 10 datasets from the UEA Archive [43] for our MTS classification experiments. These datasets represent the common intersection used by various comparative methods in existing study, ensuring the comparability and validity of our research. The main features and statistics of each dataset are summarized in Table. 1, covering key metrics such as the number of train and test, time series length, dimensionality, and number of classes. We also provide supervised classification for supervised node classification, with the training and test sets as shown in the training and testing columns of Table 1. For the semi-supervised version, we randomly sample labeled instances from each class in the training data with labeled data accounted for 5%, 10%."}, {"title": "4.1.2. Comparison Methods", "content": "We compared 14 implementations of the following MTS classifiers, covering distance-based classifiers, state-of-the-art pattern-based models, deep learning models, and graph neural network models: ED, DTWI, DTWD (including normalized and unnormalized versions) [43]: These are commonly used distance-based models. WEASEL+MUSE [44]: An efficient time series pattern analysis toolkit. HIVE-COTE [45]: A heterogeneous ensemble classification method for time series. MLSTM-FCN [46]: A deep learning MTS classification framework that combines LSTM layers and convolutional layers. TapNet [47]: A framework that integrates traditional methods with deep learning. MTPool [48]: Utilizes variational pooling and adaptive adjacency matrices to compute similarity using Euclidean distance. MF-Net [25]: Integrates local, global, and spatial features through graph convolution. Smate [49]: a novel semi-supervised model for learning interpretable Spatio-Temporal representations. USRL [50]: an unsupervised method for learning universal embeddings of time series, utilizing causal dilated convolutions and triplet loss with time-based negative sampling. ShapeNet [51]: a model that embeds shapelet candidates of varying lengths into a unified space using cluster-wise triplet loss. TodyNet [52]: Captures latent spatio-temporal dependencies without predefined structures, utilizing dynamic graphs and a temporal graph pooling layer. MICOS [53]: A mixed supervised contrastive learning framework that employs mixed supervised contrastive Loss to effectively leverage labels and capture complex spatio-temporal features. SVP-T [54]: Incorporates shape-level inputs to capture both temporal and variable dependencies, utilizing a variable-position encoding layer and a VP-based self-attention. DKN [55]: Combining convolutional network and transformer, while employing densely dual self-distillation for enhanced representation learning."}, {"title": "4.1.3. Experimental Environment", "content": "All models were trained in a Python 3.8 environment using PyTorch 1.10.0 with Cuda 11.3, with training lasting for 1000 epochs. The training environment was configured with the Ubuntu 20.04 operating system, equipped with an NVIDIA GeForce RTX 2080 GPU (20GB VRAM) and an Intel(R) Xeon(R) Platinum 8352V CPU (48GB RAM)."}, {"title": "4.1.4. Experimental Parameters", "content": "The initial learning rate was initially set to 10-3, and we used Negative Log Likelihood Loss as the loss function to optimize the model parameters. Additionally, the Adam optimizer with ReduceLROnPlateau strategy was employed for parameter updates.\nThe radius for FastDTW was set to 1. For the contrastive learning part, we conduct training for 500 epochs to obtain MTS representations. In the DPMamba model, we choose a single-layer structure. The KAN-GIN model was also set to both single-layer and multi-layer configurations. This design makes the model more concise and easier to train and debug, while maintaining efficiency in feature extraction. The single-layer setup helps to avoid overfitting, especially in cases with limited data."}, {"title": "4.2. Experimental Analysis", "content": "This section includes both supervised (Ours) and semi-supervised (Ours 5%, Ours 10%) experiments. Table. 2 and Table. 3 present the average accuracy results of CS-DPMamba and other MTS classifiers on 10 UEA datasets. The best performances are highlighted in bold, with AVG representing the accuracy of the method across all datasets and Wins indicating the number of datasets where the best accuracy was achieved.\nAs shown in Table. 2 and Table. 3, CS-DPMamba outperformed other 14 state-of-the-art MTS classification methods on 4 datasets, while maintaining competitiveness across datasets with varying numbers of variables and lengths. The ranking of critical difference (CD) diagram reflects the average performance of each method across all datasets, with our method ranking second and best average accuracy (0.712), demonstrating the superiority of our approach, as shown in Fig. 4. According to the CD diagram, we can see that our semi-supervised model (Ours 5%, Ours 10%) still surpasses many methods and achieves certain results.\nOur method achieved the best results on the AF, FM, and SWJ datasets, which exhibit high complexity and diversity, making them suitable for feature extraction through dynamic similarity and long-range dependencies. MT-Pool primarily utilizes Euclidean distance to calculate similarity, while our method captures dynamic similarity using FastDTW, allowing it to better handle the nonlinear features of time series. ShapeNet focuses on embedding shape candidates into a unified space, whereas our method demonstrates greater adaptability in similarity computation and bidirectional modeling, enabling it to handle more diverse inputs. Dyformer addresses the limitations of transformers through hierarchical pooling and adaptive learning, but our method combines graph neural networks to manage more complex structures and dynamic characteristics, achieving higher accuracy in classification tasks. TodyNet focuses on latent spatiotemporal dependencies, but our model captures richer feature relationships through similarity matrix construction and enhanced graph neural networks, improving classification results."}, {"title": "4.3. Ablation Study", "content": ""}, {"title": "4.3.1. Representations Ablation Experiment", "content": "We employed two methods to validate the effectiveness of the time series representations through knn: ContrastFastDTW, which combines time series representations obtained from TemCL module with FastDTW, and standard time series with FastDTW. As show in Table. 4 and Fig. 5, ContrastFastDTW achieves higher accuracy than using the original data on all datasets. The experimental results demonstrate that TemCL can effectively generate high-quality time series representations, thereby enhancing classification performance. This indicates that ContrastFastDTW is better able to capture features and patterns of representations through the strategy of contrastive learning."}, {"title": "4.3.2. Components Ablation Experiment", "content": "We conducted an ablation study to verify the impact of key components in CS-DPMamba on the results, specifically comparing the following models: Only DPMamba, Only KAN-GIN, Only ContrastDTW, and the complete CS-DPMamba model. As shown in Table. 5 and Fig. 6, the experimental results indicate that the complete model achieves the best performance, highlighting the importance of the collaborative effect of each component on the overall performance."}, {"title": "4.3.3. Scale Ablation Experiment", "content": "In this section, we use different proportions (5%, 10%, 100%) of labeled data sets to conduct node classification experiments. The experimental results are shown in Table 6. We can find that as the label ratio increases, so does the accuracy."}, {"title": "5. Conclusion", "content": "For the difficulty to model similarities and long-range dependencies of MTS. We propose contrast similarity-aware DPMamba for MTS node classification. First, to obtain the similarity in the dataset, we initially used TemCL to acquire time series representations and then constructed a similarity matrix between MTS samples using FastDTW, capturing the dynamic similarity characteristics of the time series. Second, we applied the DPMamba to consider the bidirectional nature of MTS, allowing us to better capture long-range and short-range dependencies within the data. Finally, we utilized the KAN to enhance the expressive power of the GIN for MTS node classification. By comprehensively considering the long-range dependencies and dynamic similarity features of time series, we achieved precise node classification. In the future, we will explore more efficient modeling methods for calculating the similarity between samples."}]}