{"title": "Benchmarking Vision Language Models for Cultural Understanding", "authors": ["Shravan Nayak", "Kanishk Jain", "Rabiul Awal", "Siva Reddy", "Sjoerd van Steenkiste", "Lisa Anne Hendricks", "Karolina Sta\u0144czak", "Aishwarya Agrawal"], "abstract": "Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding \u2013 recognizing objects, attributes, and actions \u2013 rather than cultural comprehension. This study introduces CULTURALVQA, a visual question-answering benchmark aimed at assessing VLM's geo-diverse cultural understanding. We curate a collection of 2,378 image - question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CULTURALVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly lower performance for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CULTURALVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.", "sections": [{"title": "1 Introduction", "content": "Recent multimodal vision-language models (VLMs) (Radford et al., 2021; Liu et al., 2023; Peng et al., 2023; Chen et al., 2024; Lu et al., 2024) have shown impressive performance in tasks such as image-to-text generation, visual question answering, and image captioning, inter alia. These tasks predominantly focus on general scene understanding capabilities such as recognizing attributes, objects, and actions in scenes containing objects in their common context (Lin et al., 2014). However, given the advancing capabilities of VLMs, we believe the time is now ripe to hold VLMs to higher standards. We believe that to support increasingly global digital interactions, VLMs must also be capable of understanding the cultural values (Liu et al., 2021) such as beliefs, rituals, and traditions, for a variety of cultures in the world.\nIn order to adequately assess whether the current state-of-the-art VLMs \u2013 including proprietary models such as GPT-4V (OpenAI, 2023) and GEMINI (Gemini Team et al., 2023) \u2013 encode cultural knowl-"}, {"title": "2 Related work", "content": "Cultural understanding is closely related to geo-diverse understanding. Existing geo-diverse datasets, for instance, the Dollar Street dataset (Gaviria Rojas et al., 2022) includes 38,479 images of everyday household items from homes around the world, while the GLDv2 dataset (Weyand et al., 2020) contains 5 million images and 200k distinct instance labels of natural and human-made landmarks, but both only test recognition capabilities as opposed to cultural understanding. The GD-VCR dataset (Yin et al., 2021) probes cultural understanding, but its reliance on cinematic scenes limits the diversity of real-world cultural contexts it can have. Another related line of work focuses on multilingual understanding. For instance, Bugliarello et al. (2022) bring together five datasets across a number of tasks in 20 languages. However, their focus lies in multilingual understanding (as opposed to cultural understanding). Another multilingual dataset, MaRVL (Liu et al., 2021), tests visually grounded reasoning across multiple languages and cultures. However, MaRVL does not explore the cultural common sense of rituals and traditions. Additionally, the XM3600 dataset (Thapliyal et al., 2022), includes image captions from 36 regions and languages, thus providing a broad geographical coverage but nonetheless contains mostly Western content and lacks depth in the included cultural concepts (Pouget et al., 2024). Closest to our work, the MaXM benchmark (Changpinyo et al., 2023), building on the XM3600 dataset, and the concur-"}, {"title": "3 CULTURALVQA: Dataset Creation", "content": "Cultural Taxonomy Culture is a multifaceted concept that describes the way of life of a collective group of people, distinguishing them from other groups with different cultures (Hofstede et al., 2010; Hershcovich et al., 2022). In this paper, we use the concept of a country as a proxy for a cultural group (Adilazuarda et al., 2024). Our work assumes common ground within a cultural group by probing culturally relevant concepts that are collectively understood, as well as shared cultural com-"}, {"title": "4 Dataset Analysis", "content": "This section provides a detailed analysis of our dataset's composition and characteristics. In particular, we offer an analysis of images, questions, answers, and cultural concepts included in the CULTURALVQA dataset.\nOur dataset comprises of 2,328 unique images. The concepts depicted in the images are sourced from 11 countries, selected through a strategic process to ensure extensive cultural representation.\nWe collected 2,378 questions in total. The questions have an average length of 10.98 words. Most frequent question types include 'What'(51.3%), 'Which'(11.2%), 'In' (5.6%), 'Why' (3.4%),\u2018Where'(3.1%) 'Identify'(3.0%), and 'How' (2.7%) questions. For ex-"}, {"title": "5 Benchmarking VLMs on CULTURALVQA", "content": "Evaluation Metric Evaluating open-ended VQA is challenging. Traditionally, string matching has been used but it is known to underestimate model performance. Based on findings from Ma\u00f1as et al. (2024), which demonstrate the effectiveness of reference-based LLM evaluation for open-ended VQA tasks, we adopt LAVE, their proposed metric, as our evaluation metric with GPT-4 as the LLM. We validated the effectiveness of LAVE for our use case by computing correlation with human judgments. LAVE judgment agrees with human judgment 79% of the times for GPT-4, 73% of the times for GEMINI, and 76% of the times for INTERN-VL.\nWe benchmark several state-of-the-art VLMs on the proposed CULTURALVQA dataset, ranging from closed-source models like GPT-4 (GPT-40) and GEMINI PRO (GEMINI-PRO-VISION 1.0) to a wide variety of open-source models, ranging from 7 to 25 billion parameter count: BLIP2 (Li et al., 2023), INSTRUCTBLIP (Dai et al., 2024), LLAVA1.5 (Liu et al., 2023), LLAVA_NEXT (Liu et al., 2024), IDEFICS2 (Lauren\u00e7on et al., 2024), and INTERN-VL 1.5 (Chen et al., 2024).\nTo investigate this, we employ the following baselines. LLM-only: This baseline uses an LLM to answer questions based on solely the question input. It helps gauge the extent to which the questions in our dataset can be addressed without any visual context, solely relying on the language-only cultural information encoded in the parameters of the LLM. LLM + Country: It introduces country-specific context into the LLM prompts to deter-"}, {"title": "6 Conclusions", "content": "In this paper, we highlight the significance of evaluating VLMs not just on general scene understanding but also on their ability to comprehend diverse cultural contexts. We introduce CULTURALVQA, a novel cultural VQA benchmark for assessing VLMs on their cultural understanding. By curating a diverse collection of images from 11 countries across 5 continents and collecting 2,378 hand-crafted questions and 7,206 answers about cultural concepts presented in these images, written by annotators, we ensured a broad representation of cultural concepts pertinent to diverse cultural groups.\nBenchmarking state-of-the-art models on CULTURALVQA reveals notable disparities in their performance across regions. Specifically, models demonstrate substantially higher accuracy in answering questions related to North American cultures compared to African-Islamic ones. Further, we find a stark performance disparity between closed- and open-source models, with a 29.78% gap between the highest-performing proprietary model and its open-source counterpart in the country for which the models perform the worst. The benchmarked VLMs also showed varying levels of proficiency across cultural facets, performing well on questions about clothing, rituals, and traditions, but less effectively on those concerning food and drink. Our results underscore the current limitations of VLMs in achieving uniform cultural comprehension and pinpoint specific areas that require improvement."}, {"title": "7 Limitations", "content": "Our study faces limitations due to our data collection methods, the scope of the CULTURALVQA dataset, and our focus on the English language. We approximated cultural groups using geographical regions for annotator recruitment, potentially oversimplifying cultural identities and conflating culture with nationality due to practical constraints like annotator availability. Our use of English-only data may also miss key cultural nuances available only in native languages. Although our dataset aims for cultural diversity, it does not capture the full spectrum of global cultural diversity. Future work will expand the dataset to represent diverse cultures and regions more broadly and develop multilingual datasets for greater inclusivity.\nCollecting culturally rich content from diverse annotators proved challenging, particularly because the images and concepts were limited to those available on English-language websites. This restriction likely omits important cultural details. Allowing annotators to skip inadequate images did not fully overcome the drawbacks of limited image quality, impacting the depth of the questions created."}, {"title": "8 Ethical Considerations", "content": "Our CULTURALVQA benchmark involves culturally specific questions and answers, developed by professional annotators from the relevant countries. We sought wide cultural representation by engaging with three different communities, compensating annotators at $10-15 per hour for both included and excluded contributions after pilot testing. This reflects our best effort to maintain fairness and inclusivity in our data collection process.\nDespite these efforts, we recognize our approach's limitation in equating cultural groups with national borders, potentially overlooking the complex realities of minority and diaspora communities. We urge future research to explore finer distinctions within cultural groups to enhance representation. Although we have rigorously tried to remove biases, some subjective content may persist; however, a substantial portion of the dataset has been verified as unbiased. We acknowledge these constraints but are hopeful that our work will advance the understanding of cultural nuances in VLMs."}, {"title": "9 Acknowledgements", "content": "We would like to extend our gratitude to David Ife-oluwa Adelani for connecting us with Masakhane, F\u0131rat \u00d6ncel for assisting with annotators in Turkey, Saba Ahamadi for securing annotators from Iran, and Qian Yang for sourcing annotators from China. We also appreciate the valuable feedback provided by Ibrahim Alabdulmohsin on the early draft. The technical support from the Mila IDT team in managing the computational infrastructure is greatly appreciated. Additionally, Aishwarya Agrawal re-ceived support from the Canada CIFAR AI Chair"}]}