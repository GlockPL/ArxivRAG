{"title": "Benchmarking Vision Language Models for Cultural Understanding", "authors": ["Shravan Nayak", "Kanishk Jain", "Rabiul Awal", "Siva Reddy", "Sjoerd van Steenkiste", "Lisa Anne Hendricks", "Karolina Sta\u0144czak", "Aishwarya Agrawal"], "abstract": "Foundation models and vision-language pre-training have notably advanced Vision Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their performance has been typically assessed on general scene understanding recognizing objects, attributes, and actions rather than cultural comprehension. This study introduces CULTURALVQA, a visual question-answering benchmark aimed at assessing VLM's geo-diverse cultural understanding. We curate a collection of 2,378 image - question pairs with 1-5 answers per question representing cultures from 11 countries across 5 continents. The questions probe understanding of various facets of culture such as clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on CULTURALVQA, including GPT-4V and Gemini, reveals disparity in their level of cultural understanding across regions, with strong cultural understanding capabilities for North America while significantly lower performance for Africa. We observe disparity in their performance across cultural facets too, with clothing, rituals, and traditions seeing higher performances than food and drink. These disparities help us identify areas where VLMs lack cultural understanding and demonstrate the potential of CULTURALVQA as a comprehensive evaluation set for gauging VLM progress in understanding diverse cultures.", "sections": [{"title": "1 Introduction", "content": "Recent multimodal vision-language models (VLMs) (Radford et al., 2021; Liu et al., 2023; Peng et al., 2023; Chen et al., 2024; Lu et al., 2024) have shown impressive performance in tasks such as image-to-text generation, visual question answering, and image captioning, inter alia. These tasks predominantly focus on general scene understanding capabilities such as recognizing attributes, objects, and actions in scenes containing objects in their common context (Lin et al., 2014). However, given the advancing capabilities of VLMs, we believe the time is now ripe to hold VLMs to higher standards. We believe that to support increasingly global digital interactions, VLMs must also be capable of understanding the cultural values (Liu et al., 2021) such as beliefs, rituals, and traditions, for a variety of cultures in the world.\nIn order to adequately assess whether the current state-of-the-art VLMs \u2013 including proprietary models such as GPT-4V (OpenAI, 2023) and GEMINI (Gemini Team et al., 2023) \u2013 encode cultural knowl-"}, {"title": "2 Related work", "content": "Cultural understanding is closely related to geo-diverse understanding. Existing geo-diverse datasets, for instance, the Dollar Street dataset (Gaviria Rojas et al., 2022) includes 38,479 images of everyday household items from homes around the world, while the GLDv2 dataset (Weyand et al., 2020) contains 5 million images and 200k distinct instance labels of natural and human-made landmarks, but both only test recognition capabilities as opposed to cultural understanding. The GD-VCR dataset (Yin et al., 2021) probes cultural understanding, but its reliance on cinematic scenes limits the diversity of real-world cultural contexts it can have. Another related line of work focuses on multilingual understanding. For instance, Bugliarello et al. (2022) bring together five datasets across a number of tasks in 20 languages. However, their focus lies in multilingual understanding (as opposed to cultural understanding). Another multilingual dataset, MaRVL (Liu et al., 2021), tests visually grounded reasoning across multiple languages and cultures. However, MaRVL does not explore the cultural common sense of rituals and traditions. Additionally, the XM3600 dataset (Thapliyal et al., 2022), includes image captions from 36 regions and languages, thus providing a broad geographical coverage but nonetheless contains mostly Western content and lacks depth in the included cultural concepts (Pouget et al., 2024). Closest to our work, the MaXM benchmark (Changpinyo et al., 2023), building on the XM3600 dataset, and the concur-"}, {"title": "3 CULTURALVQA: Dataset Creation", "content": "Cultural Taxonomy Culture is a multifaceted concept that describes the way of life of a collective group of people, distinguishing them from other groups with different cultures (Hofstede et al., 2010; Hershcovich et al., 2022). In this paper, we use the concept of a country as a proxy for a cultural group (Adilazuarda et al., 2024).  Our work assumes common ground within a cultural group by probing culturally relevant concepts that are collectively understood, as well as shared cultural com-"}, {"title": "4 Dataset Analysis", "content": "This section provides a detailed analysis of our dataset's composition and characteristics. In particular, we offer an analysis of images, questions, answers, and cultural concepts included in the CULTURALVQA dataset.\nQuestions We collected 2,378 questions in total. In Fig. 3, we present the number of unique questions per country. The questions have an average length of 10.98 words (see Fig. 3 for country-wise breakdown). Most frequent question types include 'What'(51.3%), 'Which'(11.2%), 'In' (5.6%), 'Why' (3.4%), \u2018Where'(3.1%) 'Identify'(3.0%), and 'How' (2.7%) questions. For ex-"}, {"title": "5 Benchmarking VLMs on CULTURALVQA", "content": "Evaluation Metric Evaluating open-ended VQA is challenging. Traditionally, string matching has been used but it is known to underestimate model performance. Based on findings from Ma\u00f1as et al. (2024), which demonstrate the effectiveness of reference-based LLM evaluation for open-ended VQA tasks, we adopt LAVE, their proposed metric, as our evaluation metric with GPT-4 as the LLM (see App. F for the LLM prompt used). We vali-dated the effectiveness of LAVE for our use case by computing correlation with human judgments. LAVE judgment agrees with human judgment 79% of the times for GPT-4, 73% of the times for GEMINI, and 76% of the times for INTERN-VL.\nVLMs used for benchmarking We benchmark several state-of-the-art VLMs on the proposed CULTURALVQA dataset, ranging from closed-source models like GPT-4 (GPT-40) and GEMINI PRO (GEMINI-PRO-VISION 1.0) to a wide variety of open-source models, ranging from 7 to 25 billion parameter count: BLIP2 (Li et al., 2023), INSTRUCTBLIP (Dai et al., 2024), LLAVA1.5 (Liu et al., 2023), LLAVA_NEXT (Liu et al., 2024), IDEFICS2 (Lauren\u00e7on et al., 2024), and INTERN-VL 1.5 (Chen et al., 2024). See App. G for a detailed discussion of the selected models.\nWe evaluate the baselines using GPT-4 as the underlying LLM. The LAVE accuracies for these baselines, as well as for the GPT-4 VLM (which also incorporates an image as input in addition to the question), are presented in Fig. 5. We see that although the country information and the coarse visual entities help improve the performance on top of the LLM-only baseline, the performance of the strongest baseline (LLM + Lens) is still far from that of the VLM. This verifies that the questions in our dataset require sufficient visual understanding to answer them accurately."}, {"title": "6 Conclusions", "content": "In this paper, we highlight the significance of evaluating VLMs not just on general scene understanding but also on their ability to comprehend diverse cultural contexts. We introduce CULTURALVQA, a novel cultural VQA benchmark for assessing VLMs on their cultural understanding. By curating a diverse collection of images from 11 countries across 5 continents and collecting 2,378 hand-crafted questions and 7,206 answers about cultural concepts presented in these images, written by annotators, we ensured a broad representation of cultural concepts pertinent to diverse cultural groups. Benchmarking state-of-the-art models on CULTURALVQA reveals notable disparities in their performance across regions. Specifically, models demonstrate substantially higher accuracy in answering questions related to North American cultures compared to African-Islamic ones. Further, we find a stark performance disparity between closed- and open-source models, with a 29.78% gap between the highest-performing proprietary model and its open-source counterpart in the country for which the models perform the worst. The benchmarked VLMs also showed varying levels of proficiency across cultural facets, performing well on questions about clothing, rituals, and traditions, but less effectively on those concerning food and drink. Our results underscore the current limitations of VLMs in achieving uniform cultural comprehension and pinpoint specific areas that require improvement."}, {"title": "7 Limitations", "content": "Our study faces limitations due to our data collection methods, the scope of the CULTURALVQA dataset, and our focus on the English language. We approximated cultural groups using geographical regions for annotator recruitment, potentially oversimplifying cultural identities and conflating culture with nationality due to practical constraints like annotator availability. Our use of English-only data may also miss key cultural nuances available only in native languages. Although our dataset aims for cultural diversity, it does not capture the full spectrum of global cultural diversity. Future work will expand the dataset to represent diverse cultures and regions more broadly and develop multilingual datasets for greater inclusivity.\nChallenges in collecting culturally informative data Collecting culturally rich content from diverse annotators proved challenging, particularly because the images and concepts were limited to those available on English-language websites. This restriction likely omits important cultural details. Allowing annotators to skip inadequate images did not fully overcome the drawbacks of limited image quality, impacting the depth of the questions created."}, {"title": "8 Ethical Considerations", "content": "Our CULTURALVQA benchmark involves culturally specific questions and answers, developed by professional annotators from the relevant countries. We sought wide cultural representation by engaging with three different communities, compensating annotators at $10-15 per hour for both included and excluded contributions after pilot testing. This reflects our best effort to maintain fairness and inclusivity in our data collection process.\nDespite these efforts, we recognize our approach's limitation in equating cultural groups with national borders, potentially overlooking the complex realities of minority and diaspora communities. We urge future research to explore finer distinctions within cultural groups to enhance representation. Although we have rigorously tried to remove biases, some subjective content may persist; however, a substantial portion of the dataset has been verified as unbiased (see App. E). We acknowledge these constraints but are hopeful that our work will advance the understanding of cultural nuances in VLMs."}, {"title": "9 Acknowledgements", "content": "We would like to extend our gratitude to David Ife-oluwa Adelani for connecting us with Masakhane, F\u0131rat \u00d6ncel for assisting with annotators in Turkey, Saba Ahamadi for securing annotators from Iran, and Qian Yang for sourcing annotators from China. We also appreciate the valuable feedback provided by Ibrahim Alabdulmohsin on the early draft. The technical support from the Mila IDT team in managing the computational infrastructure is greatly appreciated. Additionally, Aishwarya Agrawal received support from the Canada CIFAR AI Chair"}, {"title": "A Data Statement", "content": "We provide a data statement (Bender and Friedman, 2018) to document the generation and provenance of CULTURALVQA.\nCuration Rationale CULTURALVQA benchmark is designed to evaluate VLMs' cultural understanding capacities across various cultures. The images are sourced from the CANDLE dataset (Nguyen et al., 2023), which offers a comprehensive collection of Cultural Commonsense Knowledge (CCSK) from the C4 corpus (Raffel et al., 2020), consisting of 1.1 million entries each linked to relevant CCSK data via URLs to webpages. Annotators writing questions and answers for this project are recruited through the MTurk platform, an African NLP organization, and an international academic AI research institute.\nLanguage Variety All texts included in the dataset are in English, primarily authored by non-native speakers, and may thus contain ungrammatical structures both in questions and answers.\nAnnotator Demographics All annotators come from the following 11 countries: China, Turkey, Iran, Ethiopia, Nigeria, Rwanda, Germany, USA, Canada, Brazil, and India. Other demographics such as age and gender are unknown. All annotators were compensated at an hourly rate of 10-15$ per hour depending on a task and the number of completed HITs.\nB Image Filtering\nGiven the potential noise inherent in an image dataset derived from web scraping, we implement a series of heuristic filters to refine our selection. First, we apply aspect ratio filtering, retaining only images with an aspect ratio between 0.5 and 2, effectively removing many banner-like advertisements. Next, we discard any images smaller than 100 pixels due to their inadequate detail for analysis. We also exclude images containing specific"}, {"title": "F System Prompt for the Evaluation Metric", "content": "System prompt used for the LAVE evaluation metric\nYou are an expert cultural anthropologist tasked with evaluating the correctness of candidate answers for cultural visual question answering. Given a question, a set of reference answers by an expert, and a candidate answer by a model, please rate the candidate answer's correctness. Use a scale of 1-2, where 1 indicates an incorrect, irrelevant, or imprecise answer, and 2 indicates a correct and precise answer. Specify the rating in the format \u2018rating=X', where X is either 1 or 2. Also, provide the rationale for your rating."}, {"title": "GVLMs Used for Benchmarking", "content": "We benchmark the following state-of-the-art open-source VLMs on our proposed CULTURALVQA dataset: BLIP2 (Li et al., 2023), INSTRUCTBLIP (Dai et al., 2024), LLAVA1.5 (Liu et al., 2023), LLAVA_NEXT (Liu et al., 2024), IDEFICS2 (Lauren\u00e7on et al., 2024), and INTERN-VL 1.5 (Chen et al., 2024). These models were selected based on their release year and parameter size (7 to 25 billion) to test how these aspects affect cultural understanding. INSTRUCTBLIP, fine-tuned with instruction tuning, is compared to BLIP2 to see if instruction tuning enhances cultural understanding. IDEFICS2, with 8 billion parameters, is evaluated for its performance on open datasets, surpassing larger models. INTERN-VL 1.5, with 25 billion parameters, bridges the gap between open-source and proprietary models, showing strong multimodal benchmark performance, even outperforming proprietary models on some benchmarks. Finally, we also evaluate closed-source models \u2013 GPT-4 (GPT-40) and GEMINI (Gemini-Pro-Vision 1.0) - using their API endpoints."}, {"title": "H Prompt for VLM Inference", "content": "Prompt used to test VLM inference\nYou will be given an image depicting a cultural concept and a question about the image. Answer the question with a precise, culturally specific response (e.g., 'sushi' instead of 'food', 'Diwali' instead of 'festival') of 1-3 words."}, {"title": "I Prompt for Few-Shot Inference using GPT-4", "content": "Prompt used for few-shot inference\nYou will be given an image depicting a cultural concept and a question about the image. Answer the question with a precise, culturally specific response (e.g., 'sushi' instead of 'food', 'Diwali' instead of 'festival') of 1-3 words. Here are some examples of the described task.\n{image}\n{question}\n{answer}"}, {"title": "J Inference Using Closed-Source Models", "content": "In this section, we provide the sample code used for accessing GEMINI and GPT-4.\nK Human Judgment of Model Predictions\nWe evaluate model responses for questions from India, with each answer rated by 5 humans on a scale of 1 to 5: 1 (completely correct), 2 (correct but not culturally specific), 3 (correct but not precise), 4 (correct but neither culturally specific nor precise), and 5 (completely incorrect). The detailed instructions given to the annotators can be found in Fig. 13."}]}