{"title": "Gradient Map-Assisted Head and Neck Tumor Segmentation: A Pre-RT to Mid-RT Approach in MRI-Guided Radiotherapy", "authors": ["Jintao Ren", "Kim Hochreuter", "Mathis Ersted Rasmussen", "Jesper Folsted Kallehauge", "Stine Sofia Korreman"], "abstract": "Radiation therapy (RT) is a vital part of treatment for head and neck cancer, where accurate segmentation of gross tumor volume (GTV) is essential for effective treatment planning. This study investigates the use of pre-RT tumor regions and local gradient maps to enhance mid-RT tumor segmentation for head and neck cancer in MRI-guided adaptive radiotherapy. By leveraging pre-RT images and their segmentations as prior knowledge, we address the challenge of tumor localization in mid-RT segmentation. A gradient map of the tumor region from the pre-RT image is computed and applied to mid-RT images to improve tumor boundary delineation. Our approach demonstrated improved segmentation accuracy for both primary GTV (GTVp) and nodal GTV (GTVn), though performance was limited by data constraints. The final DSCagg scores from the challenge's test set evaluation were 0.534 for GTVp, 0.867 for GTVn, and a mean score of 0.70. This method shows potential for enhancing segmentation and treatment planning in adaptive radiotherapy. Team: DCPT-Stine's group.", "sections": [{"title": "1 Introduction", "content": "Radiation therapy (RT) is a key treatment modality for head and neck cancer (HNC), but tumor delineation for RT planning remains a major challenge. Traditional RT planning relies heavily on manual delineation of tumor volumes by clinicians, a process that is both time-consuming and prone to high inter-observer variability (IOV), particularly in HNC where complex anatomical structures and critical organs at risk (OARs) lie in close proximity to the tumor [28,17,23]. MRI-guided RT has emerged as a promising approach offering superior soft tissue contrast and the advantage of avoiding additional ionizing radiation during imaging. Further, MRI-guided adaptive RT holds significant potential to improve clinical outcomes by maximizing tumor control while minimizing side effects [3,15,14]. In addition, the use of multimodality images is also recommended [9].\nWith the advancements in imaging techniques and artificial intelligence (AI), recent research has increasingly focused on automating the segmentation of HNC tumors and OARs using deep learning methods [13,19]. These AI-driven approaches aim to overcome the limitations of manual segmentation by providing faster and potentially more consistent delineations [5,6]. However, due to the lack of gold-standard imaging, these applications face a common challenge of uncertainty. Specifically, for HNC, tumors often have ambiguous borders, particularly in the primary tumor volume (GTVp), where the lack of clear distinction between healthy and malignant tissues further complicates tumor delineation for both human annotators and AI models. Strong IOV is particularly problematic for GTVp, where inconsistent manual annotations can significantly impact the training of deep learning models [4].\nTo further improve tumor segmentation performance for head and neck cancers, previous studies have explored methods that incorporate prior segmentations or prompts (e.g. bounding boxes, scribbles and clicks) to refine subsequent segmentation tasks. For example, Outeiral et al. applied bounding box cropping methods, which led to an increase in DSC for MRI images [18]. Ren et al. found that adding a bounding box as an additional channel improved nnUNet performance, raising GTVp/lymph node (GTVn) DSC from 0.68/0.63 to 0.88/0.89 on multimodal data (CT, PET, MRI) [21]. Wang et al. used a RetinaNet model to narrow segmentation fields on CT and PET scans, improving precision [33]. Wei et al. [34] demonstrated that incorporating minimal training steps after human interactions raised GTV accuracy from a DSC of 0.65 to 0.82. Interactions such as single or multiple clicks within the GTV have also demonstrated significant improvements in segmentation accuracy [20,25].\nBuilding upon these advancements, this study addresses the challenges of MRI-guided adaptive RT for HNC, focusing on the second task of the HNTS-MRG 2024 challenge [29]. This task involves segmenting GTVp and GTVn using pre-RT and mid-RT T2-weighted (T2w) MRI images, with mid-RT images taken after RT treatment. The main challenge is accurately identifying all malignant tumor regions and determining the correct contours, complicated by similar soft tissue contrasts and the lack of definitive ground truth [24]. Tumor shrinkage or disappearance in mid-RT images further complicates boundary delineation.\nTo overcome these challenges, pre-RT images and their corresponding delineations can serve as valuable prior knowledge for assisting in the segmentation of tumors on mid-RT images. However, the optimal way to leverage this pre-RT information for mid-RT segmentation remains an open question [32]."}, {"title": "2 Material and Methods", "content": "The dataset used in this study was provided by the organizer of the HNTS-MRG 2024 challenge task2 which were 150 HNC patients, predominantly oropharyngeal cancer (OPC). Imaging provided for each patient was T2w fat saturated anatomical sequences of the head and neck region taken at MD Anderson Cancer Center [29]. Images include pre-RT (1-3 weeks before the start of RT) and mid-RT (2-4 weeks intra-RT) scans. Multiple physician expert observers (n = 3 to 4) have independently segmented GTVp and GTVn structures for all cases (pre-RT and mid-RT) based on MRI images provided in the challenge. The ground truth was obtained via the Simultaneous Truth And Performance Level Estimation algorithm (STAPLE). Pre-RT images and delineations were deformably registered (DR) to the mid-RT images (DR pre-RT)."}, {"title": "2.2 Incorporating pre-RT Tumor Location with Gradient Maps for mid-RT", "content": "We consider pre-RT images and their delineations as valuable prior knowledge for segmenting tumors on mid-RT images, especially for identifying tumor locations. To leverage this information, we performed connected component analysis on the GTV segmentation masks from DR pre-RT images to identify individual tumor instances (GTVp and GTVn). For each instance, 3D coordinates of the tumor boundaries were extracted, and a bounding box was created around the tumor, with random perturbations of 2-6 voxels in the x, y, and z directions to account for spatial variations. Next, we computed the gradient magnitude (to construct a gradient map) from mid-RT T2w MRI images within these bounding boxes to capture intensity changes around tumor boundaries. The preprocessing involved normalizing the T2w MRI based on its full intensity range and extracting the regions defined by the pre-RT mask. A gradient map was then generated using a Gaussian filter (sigma = 1) to highlight intensity changes. Gradient values were normalized to the range 0-1, with any values greater than 1 clipped to maintain consistency across all patients.\nAlthough pre-RT and mid-RT images are temporally related, their higher-level features, such as tumor morphology and intensity patterns, can differ significantly, likely due to treatment effects or natural variability. To account for this, we treated the pre-RT and mid-RT images of the same patient as independent data points, effectively doubling the dataset size from 150 to 300 samples. Gradient maps were computed for both pre-RT and mid-RT images using the similar process. This approach allowed the model to learn from a broader range of spatial and intensity variations by expanding the training set.\nAt test time, the bounding boxes of tumors from DR pre-RT were used to construct the gradient maps of mid-RT T2w MR images. Therefore, both mid-RT T2w and the gradient maps were treated as two-channel inputs for the network. The overall process is outlined in detail in the flowchart, as illustrated in Figure 1."}, {"title": "2.3 Deep learning configurations", "content": "We utilized the nnUNet framework [7] to implement and train the model, employing the nnUNetResEncM planner to design the network architecture[8]. The model was based on a Residual Encoder U-Net with six stages. Each stage contained a specified number of convolutional blocks: Stage 1 had 1 block, Stage 2 had 3 blocks, Stage 3 had 4 blocks, and Stages 4 through 6 each had 6 blocks. The Conv3D layers employed a kernel size of 1x3x3 for the first layer, followed by 3x3x3 kernels for the subsequent layers. Strides were set to 1x1x1 in the first stage and either 1x2x2 or 2x2x2 in the later stages to reduce dimensionality. InstanceNorm3D was used for normalization, and LeakyReLU served as the activation function.\nThe training process used a batch size of 8 with a patch size of 48x192x192 voxels. Both the input T2w images and gradient maps were normalized using Z-Score normalization for intensity standardization. Cubic interpolation (order 3) was applied for data resampling, while first-order interpolation was used for segmentation masks. The median image size was 123x512x511 voxels, with voxel spacing set at 1.2x0.5x0.5 mm. Training was performed using the SGD optimizer with a PolyLR scheduler with exponent=0.9, starting with a learning rate of 0.01. The loss function was a combination of cross-entropy and dice loss.\nThe 150 patients were randomly divided into 5 folds, with each fold containing 240 images/delineations (comprising pre-RT and mid-RT images for 120 patients) for training, while 30 mid-RT images/delineations for validation. The maximum number of epochs was set to 1000 for each model, and the final models from the last epoch were saved for prediction. An ensemble of all models trained across the five folds was used to generate predictions on the test set for the final challenge submission. In accordance with guidelines for reproducibility and verification [6], all source code and trained weights for gradient map generation and deep learning have been made publicly available on GitHub\u2020."}, {"title": "2.4 Evaluation Metrics", "content": "The HNTS-MRG challenge utilizes the mean aggregated Dice Similarity Coefficient (DSCagg) as the primary evaluation metric for ranking [1]. In addition, we also used the conventional Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95) and mean surface distance (MSD) as supplementary metrics to assess the segmentation performance for both GTVp and GTVn."}, {"title": "2.5 System Environment", "content": "The experiments were performed on a system featuring dual AMD Ryzen Thread-ripper 3990X processors with 64 cores (128 threads) and 256GB of RAM. Training was conducted using an NVIDIA RTX A6000 GPU with 48GB of VRAM. The software setup consisted of Python 3.12.4, PyTorch 2.4.0, CUDA 12.6 and nnU-Net 2.5.1, while distance metrics were computed using MedPy 0.5.2."}, {"title": "3 Results", "content": ""}, {"title": "3.1 5-Fold cross-validation results", "content": "We evaluated our approach using 5-fold cross-validation on the training data (n=150). For each fold, average DSCagg, HD95, and MSD scores were calculated for both GTVp and GTVn, as shown in Table 1 and Table 2. The results indicated variability in GTVp segmentation accuracy, with DSCagg ranging from 0.469 to 0.697, while GTVn exhibited more consistent performance, with DSCagg ranging from 0.786 to 0.871. The HD95 scores ranged from 9.3 to 15.6 mm for GTVp and 4.2 to 7.4 mm for GTVn, while MSD scores varied between 2.5 to 5.4 mm for GTVp and 1.0 to 1.8 mm for GTVn."}, {"title": "3.2 Comparison between with and without gradient map", "content": "We further compared the performance of using T2w images with (w/) and without (w/o) gradient maps on fold-0 of the validation set (n=30). For GTVp, the mean DSC improved from 0.355 to 0.538 (p < 0.005), and for GTVn, it increased from 0.688 to 0.825 (p < 0.001), based on Wilcoxon signed-rank tests. In Figure 2, the violin plots illustrate this comparison, showing a clear shift toward higher DSC values for both GTVp and GTVn when the gradient map is applied. The distributions highlight the overall improvement in segmentation accuracy when incorporating gradient maps.\nFigure 3 presents two patient cases. In patient a, the use of the gradient map successfully segmented a previously missed GTVn on the T2w-only image, improving the DSC from 0.0 to 0.83. For patient b, both GTVp and GTVn segmentations improved, with DSC scores increasing from 0.14 to 0.80 and 0.73 to 0.87, respectively. However, part of the GTVp was missed as it extended beyond the bounding box range defined by the gradient map."}, {"title": "3.3 Correlation between tumor volume change and DSC scores", "content": "As shown in Figure 4, cross-validation results (n=150) revealed no strong correlation between tumor volume change and DSC scores for either GTVp or GTVn. Spearman's correlation coefficients were used to measure these relationships. We observed numerous instances where significant tumor shrinkage resulted in false predictions, particularly for GTVp, with DSC scores of 0.0. Moderate negative correlations were identified in specific volume ranges; for GTVp, a mid-RT volume bin between 0.8 and 3.0 cubic centimeters (cc) showed a Spearman's correlation coefficient of -0.41. Similarly, for GTVn, a volume bin ranging from 2.567 to 8.5 cc had a coefficient of -0.34."}, {"title": "3.4 Final test score", "content": "After completing training and validation, we submitted our trained model's Docker container to the challenge platform. Predictions were generated using an ensemble of all five models trained across the 5-folds. The final DSCagg scores on the test set, as evaluated by the challenge, were 0.534 for GTVp, 0.867 for GTVn, with a mean score of 0.70."}, {"title": "4 Discussion", "content": "In this study, we integrated pre-RT tumor location and gradient maps to enhance tumor segmentation in mid-RT images for head and neck cancer. Results from the 5-fold cross-validation show that leveraging pre-RT information improved the delineation of both GTVp and GTVn. The average performance across folds, measured by DSCagg, HD95, and MSD, demonstrated consistent and reliable segmentation. The comparison of T2w without gradient map versus T2w with gradient map revealed that gradient maps provided more precise boundary localization. These findings highlight the potential of combining spatial and localized gradient intensity information to enhance segmentation accuracy in MRI-guided adaptive radiotherapy.\nDeep learning-based AI approaches have garnered significant interest in enhancing RT patient treatment, with notable progress made in HNC tumor auto-segmentation [5]. Much of this advancement has been driven by MICCAI public data challenges, such as the HECKTOR challenges [1,2]. Numerous studies have participated and contributed diverse approaches to the challenge. Notably, Xie and Peng [35] achieved a DSC of 0.778 using a 3D SE UNet integrated with the nnUNet pipeline. Similarly, Naser et al. [16] employed a Resnet U-Net, achieving a mean Dice score of 0.69 and further validating the model on MRI, where the mean DSC reached 0.73 \u00b1 0.12 for T2w+ T1w imaging [30].\nDespite these advancements, there remains a scarcity of studies or publicly available datasets that leverage the temporal dependency between pre-RT and mid-RT images for head and neck tumor segmentation, especially using MRI data. The HNTS-MRG 2024 Challenge helps address this gap. A primary difficulty in this challenge is accurately identifying all malignant regions in the mid-RT MRI, where using pre-RT ground truth delineations as prior information can help overcome this difficulty. As demonstrated in our study, even a rough propagation of pre-RT information, such as incorporating a bounding box, led to significant improvements in segmentation accuracy. Specifically, the mean DSC for GTVp improved from 0.355 to 0.538 (p < 0.005), and for GTVn, it increased from 0.688 to 0.825 (p < 0.001). These results highlight the potential of utilizing pre-RT data; however, accurately tracking tumor evolution and precisely propagating contours from pre-RT to mid-RT images remains a challenging task, suggesting that more refined methods are needed to fully leverage this temporal information.\nIn addition to leveraging pre-RT information, our findings indicate that other factors may influence segmentation accuracy. Specifically, in cases where the tumor was absent after treatment (mid-RT GTVp volume = 0), the model failed to predict this absence, resulting in multiple cases of DSC=0.0. This limitation may be exacerbated by the presence of gradient maps in the original tumor region, contributing to false-positive predictions. Furthermore, our results revealed moderate negative correlations in certain volume bins, indicating that as tumors shrink, particularly in the mid-RT moderate volume range GTVp (0.8-3 cc) and GTVn (2.567-8.5 cc), segmentation accuracy tends to decline. This decrease is likely due to the greater complexity in detecting and delineating smaller tumors during the mid-RT phase, where reduced volume leads to greater boundary ambiguity.\nDeep learning-based tumor segmentation is inherently data-intensive, and the limited availability of annotated datasets presents a significant challenge. This issue is particularly pronounced in head and neck cancer for mid-RT, where tumor volumes diminish significantly during treatment. Although the dataset consists of 150 training cases, this may be insufficient for advanced techniques like deformation or image propagation networks, which require large datasets to optimize both image alignment and segmentation objectives [11]. Furthermore, variability in patient data distributions complicates training, reducing the model's ability to generalize across diverse cases.\nOur approach aimed to mitigate these challenges by simplifying the problem through a data manipulation approach. Instead of propagating the full image evolution from pre-RT to mid-RT, which would require more complex modeling and a larger dataset, we focused on providing rough tumor localization through the use of bounding boxes derived from pre-RT images. By narrowing the region of interest, we reduced the complexity of the segmentation task. Moreover, we calculated the gradient of the mid-RT image within the bounding box to simplify the training process, allowing more detailed boundary information to be incorporated alongside the location data fed into the network.\nA limitation of our approach is the use of a bounding box with an arbitrary expansion of 2-6 voxels around the deformable registered pre-RT image, which may not always align well with the mid-RT image. This misalignment becomes more critical if there are significant registration errors or substantial anatomical changes in the patient during the course of treatment. The arbitrary choice of expanding the bounding box by 2-6 voxels in all three dimensions carries inherent risks. If the expansion is too small, there is a risk of missing part of the actual tumor in the mid-RT image, as shown on Figure 3(b). On the other hand, if the expansion is too large, it may reduce the precision of tumor localization, as the box would include more irrelevant tissue, thereby diluting the intended localization benefit.\nIn routine clinical practice, human involvement is often required to refine such critical tumor definition tasks. Clinicians may define tumor locations interactively, using tools such as scribbles, clicks, or bounding boxes, to provide more accurate localization cues. Studies have demonstrated that integrating such human-in-the-loop methods can significantly enhance detection and segmentation accuracy [25,21,34]. This approach is particularly useful when automated registration struggles with large anatomical changes or complex tumor morphologies, making precise localization difficult without manual input.\nOur results demonstrate a significant difference in segmentation accuracy between GTVp and GTVn, with DSCagg scores of 0.534 for GTVp and 0.867 for GTVn. The validation performance for GTVp ranged widely (0.47 to 0.69), whereas GTVn showed more consistency (0.79 to 0.87). This disparity is partly because GTVp often diminishes after treatment, making it barely visible on mid-RT images but still indicated by the gradient map. Additionally, the inherent ambiguity in defining GTVp tumor contours contributes to this difference. The challenge organizers used a STAPLE consensus from multiple annotators (3-4) as the ground truth, but due to low contrast at GTVp boundaries, there may exists considerable IOV, resulting in less agreement and higher uncertainty in GTVp segmentations [26].\nTo address this issue, the ground truth delineations should strictly adhere to established guidelines [10]. However, GTVp is often overestimated, leading to a high false positive rate compared to histology, highlighting flaws in delineations and the need for improved imaging techniques [27]. From a deep learning developer's view, an alternative approach could involve developing a probabilistic model based on multiple annotators' input for GTVp, rather than using a consensus method like STAPLE or averaging. Such probabilistic models can significantly improve uncertainty modeling and provide better confident calibration scores, leading to more reliable segmentation maps [22,12,31]."}, {"title": "5 Conclusion", "content": "In conclusion, our novel approach effectively leverages pre-RT information to enhance mid-RT segmentation accuracy by incorporating gradient maps derived from pre-RT tumor delineations. Our results show that utilizing pre-RT delineations improves the model's ability to delineate both GTVp and GTVn for mid-RT, with significant gains in segmentation accuracy. However, the improvements for GTVp are less pronounced, likely due to reduced tumor volume, inherent ambiguity, and variability in tumor contours. We suggest that incorporating a semi-automatic, human-in-the-loop approach could help mitigate false predictions for GTVp, particularly when tumor boundaries are unclear in imaging. Future work could focus on integrating probabilistic models based on multiple annotators to better address this uncertainty. Overall, our approach demonstrates potential for enhancing segmentation accuracy in MRI-guided adaptive radiotherapy."}]}