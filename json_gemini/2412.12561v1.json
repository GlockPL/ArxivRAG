{"title": "Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring Multi-Object Tracking", "authors": ["Wenjun Huang", "Yang Ni", "Hanning Chen", "Yirui He", "Ian Bryant", "Yezi Liu", "Mohsen Imani"], "abstract": "Referring multi-object tracking (RMOT) is an emerging cross-modal task that aims to localize an arbitrary number of targets based on a language expression and continuously track them in a video. This intricate task involves reasoning on multi-modal data and precise target localization with temporal association. However, prior studies overlook the imbalanced data distribution between newborn targets and existing targets due to the nature of the task. In addition, they only indirectly fuse multi-modal features, struggling to deliver clear guidance on newborn target detection. To solve the above issues, we conduct a collaborative matching strategy to alleviate the impact of the imbalance, boosting the ability to detect newborn targets while maintaining tracking performance. In the encoder, we integrate and enhance the cross-modal and multi-scale fusion, overcoming the bottlenecks in previous work, where limited multi-modal information is shared and interacted between feature maps. In the decoder, we also develop a referring-infused adaptation that provides explicit referring guidance through the query tokens. The experiments showcase the superior performance of our model (+3.42%) compared to prior works, demonstrating the effectiveness of our designs.", "sections": [{"title": "1. Introduction", "content": "Over the past decades, multi-object tracking (MOT) has played an important role in computer vision and has been widely used in different scenarios, such as public security [40], traffic surveillance [18], autonomous driving [25], broadcasting [23], and wildlife research [26], where understanding and monitoring the movement of multiple entities over time is critical. MOT can be defined as the process of following the trajectories of a set of objects through different frames while keeping their identities discriminated. Compared with single object tracking (SOT) [2,3,14], MOT not only detects and associates more objects but also manages unique identities for each object and handles frequent occlusions.\nTraditional MOT methods often lack the nuanced understanding required to follow specific targets when guided by natural language descriptions, a challenge that becomes particularly evident when users wish to focus on objects of interest described semantically. Meanwhile, referring understanding [11, 16, 37, 45]that integrates natural language processing into scene perception has raised great attention, with the advancement of vision-language models (VLMs). It aims to localize targets of interest in images or videos under the instruction of human language. In this paper, we focus on an emerging task named referring multi-object tracking (RMOT), which enhances conventional MOT and takes into account language understanding. RMOT improves MOT's ability to meet human intentions, significantly broadens the applicability, and boosts functional efficiency. Instead of tracking all visible objects in the scene, RMOT aims at tracking only the referent targets. For example, if we input \u201cA person riding a bike\u201d as the text prompt, the tracker should only track the ones meeting the description while ignoring other objects such as \"cars\" and \"a person on foot\", which are also tracked in MOT. Fig. 1 illustrates the typical tracking pipelines for SOT, MOT, and our proposed pipeline for RMOT.\nNevertheless, current transformer-based models are faced with several challenges that lead to a sub-optimal performance in RMOT [37,50]. First of all, based on the transformer architecture, they train a joint decoder for newborn target detection and existing target tracking. However, the imbalanced distribution of newborn targets and existing targets in the dataset impairs the training of newborn target detection. While the query tokens for existing targets, referred to as \"track queries\u201d are activated and trained during the whole lifespan of the targets, the queries for newborn targets, referred to as \"detection queries\", are only activated once when the targets first appear in the video. This insufficient training in newborn target detection leads to poor performance when dealing with uncommon targets.\nIn terms of language guidance, current designs fuse the text embedding with image features right after the vision backbone, providing a mixed-modal feature map to later stages. However, the fused feature is not a direct input to the most critical decoder and contains no explicit semantic information, leading to relatively weak and indirect language guidance that cannot be effectively reasoned in the decoder. Recent work, such as Segment Anything (SAM) [17, 31], adopts a different paradigm to fuse images and prompts. Specifically, SAM concatenates the prompt embedding and query tokens as the input to the decoder, providing strong and direct guidance. However, models like SAM cannot be naively applied in RMOT. Despite its strong zero-shot segmentation performance, SAM requires explicit point or box prompts to focus on a specific instance and does not support arbitrary language guidance or newborn target detection, not fitting the need of the RMOT task.\nObserving the aforementioned challenges, we propose a new state-of-the-art tracking algorithm with referring support. On one hand, we propose a strategy that relaxes the matching criteria and thereby increases the activation frequency of the detection queries. During training, the existing targets are not solely matched with track queries but also can be matched with detection queries. On the other hand, we propose a query adaptor that directly fuses the text prompt with the queries, providing strong guidance and enhancing the model's reasoning capability. Prior to the decoder, we also develop a unified encoder that generates a well-rounded fusion of both modalities and effectively incorporates interaction among multi-scale feature maps and text inputs. The key contributions of this work are outlined as follows:\n\u2022 We introduce an effective training strategy to boost the model's detection performance by jointly training the detection queries and track queries, which alleviates the impact caused by the imbalanced distribution of targets.\n\u2022 Prior works leverage a simple but limited architecture to enable referring in MOT tasks, leading to weak multi-modal fusion and difficulties in understanding nuanced user intention. In contrast, our proposed algorithm provides stronger and more direct guidance to ensure more accurate tracking.\n\u2022 In the decoder, we redesign its architecture to better integrate the text prompt into the decoder queries; outside of the decoder, we develop a new cross-modal encoder that boosts the information exchange between the multi-modal and multi-scale features.\n\u2022 Extensive experiments confirm the effectiveness of our proposed components, leading to a +3.42% improvement."}, {"title": "2. Method", "content": "In this section, we elaborate on each component of our proposed system. Taking the video stream and a language query as inputs, our goal is to output the track boxes of the corresponding query. The detection part of our model mainly follows Deformable DETR [51]. On top of it, we integrate three components to enhance the model capacity:\n(1). Collaborative Query Matching (CQM). The imbalance of newborn targets and existing targets impairs the overall model performance. We use CQM to facilitate newborn object detection while maintaining the performance of existing target tracking. (2). Referring-Infused Query Adaptation (RIQA). In addition to the indirect fusion of the language description and image in the encoder, we inject a direct information change between the reference and the queries in the decoder, which explicitly guides the queries to detect desired targets. (3). Cross-Modal Encoder (CME). The encoder of previous work suffers from the limited perceptive field of the image features. We develop a new CME to boost the multi-modal fusion by facilitating the exchange of information between the image features. The overview of our method is illustrated in Fig. 2."}, {"title": "2.1. Transformer-based RMOT", "content": "Our model consists of four key components: feature extractor, encoder, decoder, and temporal reasoning module. The feature extractor first produces visual and linguistic features for the raw video and text. Formally, given a N-frame video, an image backbone extracts the frame-wise pyramid feature maps $I_n \\in \\mathbb{R}^{C_i \\times H_i \\times W_i}$, where n represents the frame index, and $C_i$, $H_i$, $W_i$ represents the channel depth, height, width of the $i^{th}$ level feature map, respectively. At the same time, a linguistic model embeds the text description T into a set of word embeddings $S_w \\in \\mathbb{R}^{L \\times D}$, where D is the embedding dimension, and L is the number of embedded tokens.\nThen, an encoder fuses the features of two modalities per frame and gets a stack of vision-language fused embeddings $E_n = \\{ E_n^1, \\dots, E_n^h \\}$. For each level of feature maps:\n$E_n^i = \\text{Attn}(Q = PE^i(I_n), K = PE^S(S_w), V = S_w)$  (1)\n, where $PE^i(\\cdot)$, $PE^S(\\cdot)$ integrate the positional embeddings into image features and text features, respectively. \"Attn\" refers to a few attention blocks. A multi-scale deformable attention [51] is then adopted to further refine the fused embeddings.\n$E_n^i = \\text{MSDeformAttn}(E_n^i, p_q, E_n^i)$  (2)\n, where \"MSDeformAttn\" follows the notation in [51], and $p_q$ denotes the reference points for deformable attention. Next, a decoder is used to make predictions for object detection and tracking by updating a set of learnable queries $Q_n = \\{Q_n^{\\text{Detect}}, Q_n^{\\text{Track}} \\}$. The learnable queries are categorized into two types: detection queries $Q_n^{\\text{Detect}}$, and track queries $Q_n^{\\text{Track}}$. $Q_n^{\\text{Detect}}$ detects potential newborn targets in the current frame, and $Q_n^{\\text{Track}}$ represents the tracked targets from the previous frames that aim to locate the same target in the current frame.\n$Q'_n = \\text{Decoder}(E_n, Q_n)$ (3)\n$B_n^{\\text{Detect}}, B_n^{\\text{Track}} = RH(Q'_n)$ (4)\n, where $Q'_n = \\{Q_n^{\\text{Detect'}}, Q_n^{\\text{Track'}} \\}$ are the updated queries, and $B_n^{\\text{Detect}}$, $B_n^{\\text{Track}}$ are the predicted bounding boxes of newborn targets and tracked targets at the current frame, calculating from $Q'_n$ via a referent head (RH). The RH consists of three branches: class, box, and referring. The class branch uses a linear projection to output a binary probability, indicating whether the resulting embedding represents a real object. The box branch is a 3-layer feed-forward network (FFN) with ReLU activations, except for the final layer, and predicts the bounding box location for all visible objects. The referring branch is another linear projection that outputs referent scores as binary values, reflecting the likelihood that the object matches the given expression.\nAfter decoding the queries from single-frame features, the temporal reasoning module integrates the information from past frames, in order to refine the boxes and queries. It can be formulated as follows:\n$\\begin{aligned} Q_{\\text{temp}} &= \\text{Attn}(Q = PE^T(Q_n), \\\\ K &= PE^T(Q_{n-K:n-1}^{\\text{temp}}), \\\\ V &= Q_{n-K:n-1}^{\\text{temp}}) \\end{aligned}$ (5)\n$Q_{\\text{temp}}' = \\text{Attn}(Q = PE^Q(Q_{\\text{temp}}), K = PE^Q(Q_{\\text{temp}}), V = Q_{\\text{temp}})$ (6)\n, where $Q_{n-K:n-1}^{\\text{temp}}$ are the refined temporal queries of previous K frames; $PE^T$, $PE^Q$ represent temporal positional encoding, and query positional embedding, respectively. $Q_{\\text{temp}}'$ is the temporal refined queries of the current frame.\nGiven the temporal refined queries, we calculate the offsets to refine the boxes and get the final predictions $B_n^{\\text{Detect'}}, B_n^{\\text{Track!}}$.\n$\\Delta B^{\\text{Detect}}, \\Delta B^{\\text{Track}} = FFN(Q_{\\text{temp}}')$ (7)\n$B^{\\text{Detect'}}, B^{\\text{Track!}} = \\Delta B^{\\text{Detect}}+B^{\\text{Detect}}, \\Delta B^{\\text{Track}}+B^{\\text{Track}}$ (8)"}, {"title": "2.2. Collaborative Query Matching", "content": "Traditional transformer-based MOT algorithms adopt one-to-one bipartite matching for detection queries in all decoder layers. However, the algorithms underestimate the imbalanced activations between detection queries and track queries caused by the natural difference in the number of newborn targets and existing targets in the dataset. That is, any object newly showed up will become existing targets in later frames, making newborn targets relatively sparse in the dataset. Once the target is detected, it is assigned to a track query in the following frames, therefore, each target only activates the detection query once but activates the track query multiple times in the subsequent frames. The insufficient training of detection queries significantly impairs the model performance. To resolve the interference in newborn detection caused by track queries, we propose collaborative query learning.\nIn current auxiliary training, as the example depicted in Fig. 3, the intermediate outputs of each decoder layer are also treated as final outputs and contribute to the loss. In each layer, track queries (\u2460 and \u2461) are trained to localize the same pre-matched targets (2 and 3), and detection queries do a one-to-one bipartite matching with those newborn targets (1). Therefore, in this example, only one detection query (1) is effectively activated and trained.\nInstead of urging a one-to-one matching, CQM allows the detection queries to match the existing targets in the intermediate layers, as shown in Fig. 3. Except for the final output, the existing targets can be discovered by another detection query besides the assigned track query. In the example, and are not only matched with \u2460 and 2, respectively, but also are matched with detection queries 2 and 3. Therefore, in addition to 1 is trained by matching with 1, 2 and 3 are also activited. Compared with traditional auxiliary training, CQM significantly boosts the training frequency of detection queries and, as a consequence, improves the model performance."}, {"title": "2.3. Referring-Infused Query Adaptation", "content": "Recent works [37,50] focus on fusing the text prompt with image features in the early stages of the system, which lacks direct guidance for object detection in the later decoder stage. To tackle this, we encode the user semantic intention directly into the queries to provide explicit guidance. The organization of our queries, which follows previous work [22, 29, 36, 51], consists of two parts: position part, and content part. The position part presents the spatial prior, and the content part represents the semantic prior of the query. In our study, each query has a dimension of $\\mathbb{R}^{1 \\times 2D}$. The first half of each query is the position part and the second half of each is the content part. Inspired by the decoder architectures of Deformable-DETR [51] and SAM [17], we propose two different types of RIQAs, i.e., pre-decoder adaptation and in-decoder adaptation, that inject sentence embedding into the content part of each query. Both types first generate a sentence embedding $S_s \\in \\mathbb{R}^{1 \\times D}$ of the text prompt T via a frozen sentence encoder and a trainable FFN.\n$S_s = FFN(\\text{SentenceEncoder}(T))$ (9)\nFor infusing referring text, we especially choose sentence embedding over individual word embeddings to provide more general, flexible and meanwhile less restricted guidance in query."}, {"title": "2.3.1 Pre-Decoder Adaptation", "content": "The overview of pre-decoder adaptation is depicted in Fig. 4 (a). It first fuses linguistic intention with the queries, then feeds the referring-infused queries into the decoder same as in previous work. Formally, element-wises, $S_s$ is added to the content part of each query from the last frame $Q_{n-1}[\\text{content}]$.\n$Q_{n-1}[\\text{content}] = Q_{n-1}[\\text{content}] \\oplus S_s$ (10)\n, where $\\oplus$ represents element-wise addition. The referring-infused queries for the current frame $Q_n$ are obtained through a self-attention layer outside the decoder.\n$Q_n = \\text{Attn}(Q = PE^T(Q_{n-1}), K = PE^T(Q_{n-1}), V = Q_{n-1})$ (11)"}, {"title": "2.3.2 In-Decoder Adaptation", "content": "The in-decoder adaptation uses a different way to fuse linguistic intention with queries, as depicted in Fig. 4 (b). The sentence embedding $S_s$ is concatenated with a trainable position part $q_p \\in \\mathbb{R}^{1 \\times D}$ to form a referring query $S'_s \\in \\mathbb{R}^{1 \\times 2D}$. Then we concatenate this extra query with the original detection and track queries to form a new set of queries $Q_{\\text{nadapt}} = \\{S'_s, Q_n\\}$ for in-decoder adaptation. Taking the concatenated queries $Q_{\\text{nadapt}}^{i-1}$, each decoder layer j computes the outputs as follows. We first fuse the information across the queries via self-attention.\n$\\begin{aligned} Q_{\\text{nadapt}}^{i} &= \\text{Attn}(Q = PE^Q(Q_{\\text{nadapt}}^{i-1}), \\\\ K &= PE^Q(Q_{\\text{nadapt}}^{i-1}), \\\\ V &= Q_{\\text{nadapt}}^{i-1}) \\end{aligned}$ (12)\nIt can be decoupled into two parts $Q_{\\text{nadapt}}^{i} = \\{S'_s, Q_n^{i'} \\}$. Then we only do deformable attention between the language fused embeddings $E_n$ and the non-linguistic queries, i.e., $Q_n^{i'}$.\n$Q_n^{i' deform} = \\text{MSDeformAttn}(Q_n^{i'}, p_q, E_n)$ (13)\nThe output queries of the decoder layer j are obtained by catenating $S'_s$ with $Q_n^{i' deform}$ and forwarding to an FFN with a residual connection.\n$Q_{\\text{nadapt}}' = FFN(\\text{Concate}(S'_s, Q_n^{i' deform}) + Q_{\\text{nadapt}})$ (14)"}, {"title": "2.4. Cross-Modal Encoder", "content": "As introduced in Sec. 2.1, the conventional encoder is constructed by a multi-modal fuser (Eq. (1)) followed by multiple deformable encoding layers (Eq. (2)). However, during the fusion of modalities, it disregards the information exchange between the patches on each feature map, as visualized in Fig. 5 (a). In Eq. (1), the word embeddings $S_w$ perform cross-attention on the extracted feature maps pyramid. Although information passes between the feature maps on different scales (as the blue arrow in Fig. 5), each patch on the same scale independently performs cross-attention with the word embeddings, lacking information exchange between others. Due to the lack of information exchange, each patch only represents the information of a limited perceptive field, leading to an unsatisfied fusion when the object appears across the patches. As shown in Fig. 5 (b), every patch only captures a part of the object, therefore, effortlessly performs attention with the word embeddings.\nObserving the limitation, we propose to first perform deformable attention on each feature map, exchanging information between patches, and then the model performs cross attention with word embedding to fuse both modalities. Formally, our proposed CME is defined as follows:\n$I_n = \\text{MSDeformAttn}(I_n, p_q, I_n)$ (15)\n$E_n^i = \\text{Attn}(Q = PE^i(I_n), K = PE^S(S_w), V = S_w)$ (16)\n, and $E_n = \\{ E_n^1, \\dots, E_n^h \\}$."}, {"title": "3. Experiments", "content": "Datasets. We evaluate the proposed method on two datasets: Refer-KITTI [37] and Refer-KITTI-V2 [50].\nEvaluation Metrics. To ensure fair comparison with prior baseline [37], we also employ Higher Order Tracking Accuracy (HOTA) [24] as the primary evaluation metric. HOTA measures the alignment between the predicted and ground-truth trajectories. It provides a comprehensive and balanced assessment by jointly considering the performance of detection and association. It is defined as the geometric mean of detection accuracy (DetA) and association accuracy (AssA), i.e., HOTA = $\\sqrt{\\text{DetA} \\cdot \\text{AssA}}$. Additionally, we adopt the following sub-metrics: detection recall/precision (DetRe/DetPr), association recall/precision (AssRe/AssPr), and localization accuracy score (LocA).\nModel Details. We leverage ResNet-50 [15] as the backbone to extract image embeddings and all-mpnet-base-v2 [34] as the text encoder to extract both word embeddings and the sentence embedding. As with deformable DETR [51], we adopt the last four feature maps of the backbone as the input to the CME. The parameters associated with the CME are initialized with random values, and the parameters of the text encoder are frozen during training. The remaining parameters are initialized with official pre-trained weights from [51] on the COCO dataset [20]. Our optimization employs AdamW with a base learning rate of $10^{-4}$, except for the visual backbone with a learning rate of $10^{-5}$. We decrease the learning rate by a factor of 10 from the 40th epoch. The window length K for temporal reasoning is set to 5. We conduct end-to-end training on 6 NVIDIA RTX A6000 GPUs, with a batch size of 6. During inference, the model operates without the need for post-processing, such as non-maximum suppression [7]. We employ detection thresholds $B_{\\text{obj}} = 0.7$ and a referring threshold $B_{\\text{ref}} = 0.3$ to localize visible objects and filter referent targets."}, {"title": "3.2. Qualitative Results", "content": "We visualize some examples in Fig. 6. In each referring example, the upper panels visualize the predicted tracked referent targets by our model, and the lower panels show all detected visible objects by our model. As depicted, our method can identify and track the referent targets accurately, even in various challenging situations, such as multiple objects, change of object status, and varying number of instances. Our model can precisely understand and recognize the meaning of object category, color, and position intentions in the text prompts. Consider the above panels of each example. In Fig. 6 (a), our model successfully identifies the concepts \u201cvehicles with light color\", and \"opposite direction\". In Fig. 6 (b), the model understands the meaning of \"red\" and \"ahead of us\", tracking the red car with ID 551, while filtering out the red car with ID 555 on the left. Similarly, the model only tracks the people on the left while filtering out all people on the right. On the other hand, our model maintains outstanding object detection performance, as illustrated in the bottom row of each example."}, {"title": "3.3. Quantitative Results", "content": "We examine the proposed method and several competitors in Tab. 1. For the \"detect-and-track\" methods, i.e., FairMOT [48], ByteTrack [47], we integrated the encoder into the detection module, followed by independent trackers to associate each referent target, for a fair comparison. iKUN [13] adopts the same paradigm exploiting a foundation model CLIP [30] to adaptively extract visual features. For the one-stage Transformer-based methods, we compare our model with TransRMOT [37] and TempRMOT [50]. On both datasets, our method achieves a superior performance (HOTA of 55.63% on Refer-KITTI, and 37.22% on Refer-KITTI-V2, respectively). Specifically, we surpass the previous best model, TempRMOT [50], by a significant margin of 2.18% on HOTA, and beats all other competitors in seven of eight matrics. This outstanding performance in both detection and association illustrates the effectiveness of our proposed CEM, RIQA, coupled with CQM during training."}, {"title": "3.4. Ablation Study", "content": "To investigate the effect of core components in our model, we conduct extensive ablation studies on Refer-KITTI-V2. Table 2 illustrates the results of all combinations of our proposed components. Every combination exhibits a positive impact on the overall performance. Specifically, using CME effectively fuses the information from different modalities, remarkably improving the association (+2.84% on AssA). This may be because the information exchange between the patches benefits the feature extraction of the objects spanning multiple patches. CQM, on the other hand, improves detection and association simultaneously, thanks to more activations for the detection queries during training. RIQA provides explicit intention guidance to the queries, boosting the reasoning ability of the model and, therefore, leading to a significant improvement in association (+3.32% on AssA, and +8.02% on AssPr).\nWe also investigate the effect of two types of RIQA, in Tab. 3. For pre-decoder adaptation, we examine the effect of infusing sentence embedding with detection queries, track queries, and both. The results indicate that RIQA improves performance compared to decoding by vanilla queries. Surprisingly, we do not observe significant improvement in in-decoder adaptation compared with the baseline. We assume this is because the self-attention between the sentence embedding and the queries interferes with the semantic information of the queries, which impacts the performance.\nLastly, we investigate the effect of referring threshold $B_{\\text{ref}}$ in Tab. 4. In inference, depending on this threshold, the model predicts whether the detected object fits the referring text. Overall, $B_{\\text{ref}}$ has a significant impact on the balance between precision and recall. A lower threshold favors a better balance and, therefore, maximizes HOTA. When the threshold goes up, the model prioritizes precision at the cost of recall, reducing HOTA. In the experiment, our model achieves the best performance when $B_{\\text{ref}} = 0.3$."}, {"title": "4. Related Work", "content": "The core challenge of referring understanding is to model the semantic alignment of cross-modal sources. Early methods [9, 21, 27] mainly fuse the sources in two stages: 1). Adopting an off-the-shelf object detector to propose massive object proposals. 2). Leveraging a semantic alignment model to learn the similarity between proposals and language expression and find best-fitted objects. Nevertheless, the performance of these methods heavily relies on the quality of the object detector. Later approaches [4, 19] fuse multiple modalities on early features employing a cross-modal attention mechanism instead of proposals. Additionally, some works provide better semantic alignment interpretability via graph modeling [41], progressive reasoning [42], or multi-temporal-range learning [12]."}, {"title": "4.2. Multi-Object Tracking", "content": "Prior works [1, 5, 10] adopt a two-stage detect-and-track paradigm thanks to the advanced development of image-level object detection [8, 32, 33]. They first detect objects in each frame, and then associate the detections across frames, thereby tracking individual objects over time. Recent works [28, 46, 49] propose one-stage trackers, mostly based on trainable transformer [35] encoder-decoder architecture. They angle MOT as a set prediction problem, by representing objects implicitly in the decoder queries, which are embeddings used by the decoder to output bounding box coordinates and class predictions. Each query refers to a low-dimensional vector that contains object information. Utilizing these semantically rich vectors to model temporal relationships is efficient and effective To further improve the performance, some efforts investigate the use of temporal memory [6], domain adaptation [43], off-the-shelf detector guidance [49], and label reassignment [39, 44]."}, {"title": "4.3. Referring Tracking", "content": "Referring single-object tracking has been studied for several years. Most recent SOTA solutions mainly follow the joint tracking paradigm. MTTR [4] applies a DETR-like [8] multi-modal module to decode instance-level features into a set of multimodal sequences. ReferFormer [38] inputs a set of object queries conditioned on language descriptions into Transformer to estimate the referred object. As for RMOT, Along with Refer-KITTI, the first baseline model TransRMOT [37] is introduced. It is built upon the end-to-end multi-object tracking method MOTR [46] to accept the cross-modal input. The latter approach, iKUN [13], follows a two-stage paradigm. It first explicitly extracts object proposals and then selects the objects matched with the language expression. On the other hand, it introduces a neural version of Kalman filter to dynamically adjust process noise and observation noise based on the current motion status. On top of TransRMOT, TempRMOT [50] integrates historical information into the model, refining the predictions with the help of the outputs from previous frames."}, {"title": "5. Discussion", "content": "In this paper, we present a novel end-to-end framework RMOT. We introduce a new matching strategy during training, which effectively alleviates the imbalanced activations between detection queries and track queries caused by the difference in the number of newborn targets and existing targets in the dataset. We also propose a query adaptation component that explicitly fuses the linguistic intention with the decoder queries, and enhances the reasoning. Besides, we redesign the encoder that considers the information exchange between the image patches at the same feature scale, improving the multi-modal fusion efficiency. Our model is evaluated on the widely used datasets and achieves the SOTA performance, demonstrating the effectiveness of our proposed components."}]}