{"title": "Absorb & Escape: Overcoming Single Model Limitations in Generating Genomic Sequences", "authors": ["Zehui Li", "Yuhao Ni", "Guoxuan Xia", "William Beardall", "Akashaditya Das", "Guy-Bart Stan", "Yiren Zhao"], "abstract": "Recent advances in immunology and synthetic biology have accelerated the development of deep generative methods for DNA sequence design. Two dominant approaches in this field are AutoRegressive (AR) models and Diffusion Models (DMs). However, genomic sequences are functionally heterogeneous, consisting of multiple connected regions (e.g., Promoter Regions, Exons, and Introns) where elements within each region come from the same probability distribution, but the overall sequence is non-homogeneous. This heterogeneous nature presents challenges for a single model to accurately generate genomic sequences. In this paper, we analyze the properties of AR models and DMs in heterogeneous genomic sequence generation, pointing out crucial limitations in both methods: (i) AR models capture the underlying distribution of data by factorizing and learning the transition probability but fail to capture the global property of DNA sequences. (ii) DMs learn to recover the global distribution but tend to produce errors at the base pair level. To overcome the limitations of both approaches, we propose a post-training sampling method, termed Absorb & Escape (A&E) to perform compositional generation from AR models and DMs. This approach starts with samples generated by DMs and refines the sample quality using an AR model through the alternation of the Absorb and Escape steps. To assess the quality of generated sequences, we conduct extensive experiments on 15 species for conditional and unconditional DNA generation. The experiment results from motif distribution, diversity checks, and genome integration tests unequivocally show that A&E outperforms state-of-the-art AR models and DMs in genomic sequence generation. A&E does not suffer from the slowness of traditional MCMC to sample from composed distributions with Energy-Based Models whilst it obtains higher quality samples than single models. Our research sheds light on the limitations of current single-model approaches in DNA generation and provides a simple but effective solution for heterogeneous sequence generation. Code is available at the Github Repo\u00b9.", "sections": [{"title": "1 Introduction", "content": "DNA sequences, as the blueprint of life, encode proteins and RNAs and directly interact with these molecules to regulate biological activities within cells. The success of deep generative models in image [28], text [27], and protein design [35] has drawn the attention of deep learning researchers to the problem of DNA design, i.e. applying these models to genomic sequence generation [4, 34, 38]. However, one rarely explored issue is how well existing methods can handle the unique property of DNA sequences: heterogeneity. DNA sequences are highly heterogeneous, consisting of multiple connected functional regions (e.g. Promoter Regions, Exons, and Introns) in sequential order. While elements within each functional region might be homogeneous (coming from the same distribution), the overall sequence is non-homogeneous. This heterogeneity, along with the discrete nature of genomic sequences, poses challenges to popular deep generative methods."}, {"title": "Limitations of Existing Single-Model Approaches in Generating Genomic Sequences", "content": "AutoRegressive Models (AR) [8, 17, 24] are one of the most dominant approaches for discrete sequence generation. To model the data distribution of a sequence x of length T, the probability of x is factorized as:\n $$p^{AR}(x) = \\prod_{i=1}^{T}P_{\\theta}(x_{i}|x_{1}, x_{2}, ..., x_{i-1}).$$\nAn issue arises when modeling heterogeneous data, where the value of @ may vary significantly from one segment to another. Additionally, AR models assume a dependency between the current element and previous elements; this assumption may not hold true for heterogeneous sequences, potentially hindering the learning process (see Section 3 for details).\nOn the other hand, Diffusion Models (DMs), initially proposed by [30], have been dominant in image generation. In the probabilistic denoising view [16], DMs gradually add noise to the input data xo, and a reverse diffusion (generative) process is trained to gradually remove the noise from the perturbed data xt. DMs directly model the data distribution without AutoRegressive factorization, thereby avoiding the issues associated with AR models. However, it has been shown that DMs are less competent than AR models for discrete data generation [21, 36]. When it comes to modeling heterogeneous genomic sequences, it remains unclear how the performance of DMs compares to AR models within each homogeneous segment."}, {"title": "Model Composition As a Solution", "content": "Balancing the ability of generative algorithms to capture both local and global properties of the data distribution is central to the problem. An obvious solution could be to combine these two types of models and perform generation using the composed models. However, this typically requires converting these two models into an Energy-Based Model and then sampling using Markov Chain Monte Carlo (MCMC), which can be inherently slow due to the sampling nature of the algorithm [10], and the potential long inference time of individual models. With the goal of accurate and efficient DNA generation, we aim to investigate two key questions in this work: (i) How well does a single AR model or DM perform in DNA generation, given the heterogeneous nature of genomic sequences? (ii) Is there an efficient algorithm to combine the benefits of AR models and DMs, outperforming a single model? In answering these two questions, our contribution is three-fold:\n(a) We study the properties of AR models and DMs in heterogeneous sequence generation through theoretical and empirical analysis (Section 3)."}, {"title": "3 Single Model Limitations in Heterogeneous Sequence Generation", "content": "How powerful are AR models and DMs in modelling heterogeneous sequences? We first provide a theoretical analysis, and then perform experiments on synthetic sequences to validate our assumption.\nAutoRegressive (AR) Models Suppose a heterogeneous sequence x consist of two homogeneous segments of length k, then x = {{X1,X2,\u2026,Xk},{Xk+1,Xk+2,\u2026\u2026,X2k}}. AR models factorize p(x) into conditional probability in eq. (4); consider the case where the true factorisation of p(x) follows eq. (5).\n $$p^{AR}(x) = P_{\\theta}(x_{1})P_{\\theta}(x_{2}|x_{1})\\cdot\\cdot\\cdot P_{\\theta}(x_{k}|x_{1:k-1}) \\cdot P_{\\theta}(x_{k+1}|x_{1:k})P_{\\theta}(x_{k+2}|x_{1:k+1}) \\cdot\\cdot\\cdot P_{\\theta}(x_{2k}/x_{1:2k-1})$$\n $$p_{data}(x) = p_{1}(x_{1})p_{1}(x_{2}|x_{1}) \\cdot\\cdot\\cdot p_{1}(x_{k}|x_{1:k-1})\\cdot p_{2}(x_{k+1})p_{2}(x_{k+2}|x_{k+1})\\cdot\\cdot\\cdot p_{2}(x_{2k}|x_{k+1:2k-1})$$\nAR factorisation allows the accurate modelling of the first homogeneous segment; however, it may struggle to disassociate the elements of the second segment from the first segment. More precisely, sufficient data is needed for AR model to learn that po(xk+1), Po(Xk+2),\u00b7\u00b7\u00b7,P\u04e9(X2k) should be independent to the elements {xX1,X2,\u00b7\u00b7\u00b7,Xk} in the first segments. Secondly, when the context length of the AR model is shorter than the sequence length 2k, it could struggle to capture the difference between p\u2081 and p2 with a single set of parameters \u03b8.\nDiffusion Models (DMs) On the other hand, DMs estimate the overall probability distribution p(x) without factorization. The elements of x are usually generated in parallel. Thus, they do not suffer from the conditional dependence assumption. However, the removal of the conditional dependence assumption may also decrease the accuracy of generation within each homogeneous segment compared to AR models, as DMs do not explicitly consider previous elements."}, {"title": "3.1 A Toy Example", "content": "To evaluate the performance of Autoregressive (AR) models and Diffusion Models (DMs) in generating heterogeneous sequences, we consider a toy example with 50,000 heterogeneous sequences X = {x(n)}50000 Each sequence contains 16 segments, as illustrated in Figure 2(a), and each segment comprises 16 elements, resulting in a total sequence length of 256 (x \u2208 N256). A simple Hidden Markov Model (HMM) is used to generate each segment, as shown in Figure 2(b), with deterministic transition and emission probabilities that ensure homogeneity within each segment. The emitted tokens differ from one segment to another, mimicking the properties of real DNA sequences. Whilst it is possible to use more complex distributions for each segment, doing so could complicate the evaluation of the generated sequences."}, {"title": "4 Method", "content": ""}, {"title": "4.1 The Absorb & Escape Framework", "content": "Given a pretrained AutoRegressive model $p_{\u03b8}^{AR}(x)$ and a Diffusion Model $p_{\u03b8}^{DM}(x)$, we aim to generate a higher quality example x from the composed distribution $p_{\u03b8,\u03b2}(x) = p_{\u03b8}^{AR}(x) p_{\u03b8}^{DM}(x)$. However, directly computing $p_{\u03b8,\u03b2}(x)$ is generally intractable, as both the autoregressive factorizations from $p^{AR}$ and score functions from $p^{DM}$ are not directly composable [9, 10]. We propose the Absorb & Escape (A&E) framework, as shown in Algorithm 1, to efficiently sample from $p_{\u03b8,\u03b2}(x)$.\nAbsorb-Step Inspired by Gibbs sampling [12], which iteratively refines each dimension of a single sample and moves to higher density areas, our algorithm starts with a sequence $x^{0}$ ~ $p_{\u03b8}^{DM}(x)$, generated by the diffusion model and then refines the samples through the Absorb step and Escape step. By exploiting the heterogeneous nature of the sequence, we assume that $x^{0}$ can be factorized into multiple segments {$s_{1}, s_{2}, ..., s_{n}$}. For each segment $s_{k}$, we set i and j as the start and end indices, respectively. During the Absorb step, we sample a subset of segments S \u2286 {$s_{1}, s_{2}, ..., s_{n}$} and refine each segment $s_{k}$ by sampling $x_{i:j}$ ~ $p(x_{i:j}|x_{0:i-1}, x_{j+1:L}) \u2248 p^{AR}(x_{i:j}|x_{0:i-1})$, using the autoregressive model to approximate the conditional probability.\nEscape-Step After refining the segment in the Absorb step, we proceed with the Escape step where we update the refined segment $x_{i}$ to $\\check{x_{ij}}$. This iterative process continues for each selected segment $s_{k}$, with t incrementing after each update. By leveraging the ability of the diffusion model to capture the overall data distribution and the autoregressive model to refine homogeneous sequences within each segment, our algorithm efficiently improves the quality of the generated samples. The final output $x^{t}$ is hereby closer to the true data distribution $p(x)$ compared to the initial sample $x^{0}$. A proof for the convergence in Proposition 1 is provided in Appendix C.\nProposition 1. The Absorb & Escape (A&E) algorithm converges to the target distribution $p_{\u03b8,\u03b2}(x) = p_{\u03b8}^{AR}(x) p_{\u03b8}^{DM}(x)$, under the assumptions that both models are properly trained, the segments of x are homogeneous, the subset of segments is chosen randomly, and the conditional distribution $p(x_{i:j}|x_{0:i-1}, x_{j+1:L})$ is accurately approximated by $p^{AR}(x_{i:j}|x_{0:i-1})$."}, {"title": "4.2 Practical Implementation: Fast A&E", "content": "While A&E offers a method to sample from a compositional distribution, two practical issues remain unresolved. Firstly, the algorithm may take a considerable amount of time to converge. Secondly, a crucial step in Line 3 of Algorithm 1 involves splitting x into homogeneous segments {S1, S2,..., Sn} and then sampling a subset of these segments. Segmentation is straightforward when the boundaries of functional regions of the DNA sequence are known, such as protein-coding regions, exons, or introns, where each region naturally forms a homogeneous segment. However, this information is often unavailable in practice.\nTo address these challenges, we propose a practical implementation termed Fast A&E. For generating a sequence x \u2208 N, it requires at most L forward passes through the AR model. As shown in Algorithm 2 and Figure 1b, Fast A&E adopts a heuristic-based approach to select segments for refinement. It scans the sequence from left to right, identifying low-quality segments through a thresholding mechanism. Tokens with predicted probabilities smaller than the Tabsorb threshold trigger the absorb action, while the autoregressive process terminates once the probability of a token generated by the AR model $p_{\u03b8}^{DM}(x')$ is smaller than that of the diffusion model $p_{\u03b8}^{AR}(x;)$. In this manner, Fast A&E corrects errors made by the diffusion model with a maximum running time of O(TDM + TAR), where TDM and TAR are the times required for generating a single sequence from the diffusion model and autoregressive model, respectively."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Transcription Profile (TP) conditioned Promoter Design", "content": "We first evaluate Fast A&E in the task of TP-conditioned promoter design, following the same evaluation procedures and metrics as used by DDSM [4] and Dirichlet Flow Matching (DFM) [32]."}, {"title": "5.2 Multi-species Promoter Generation", "content": ""}, {"title": "5.2.1 Experimental Setup", "content": "Dataset Construction Prior efforts in DNA generation have been constrained by small, single-species datasets [19, 34]. To better evaluate the capability of various generative algorithms in DNA generation, we construct a dataset with 15 species from the Eukaryotic Promoter Database (EPDnew) [23]. Table 3 compares our EPD dataset with those used in previous studies, including DDSM[4], ExpGAN [38], and EnhancerDesign [33]. The key advantage of EPD is its diversity in both species types and DNA sequence types. Additionally, although the number of sequences in EPD is on a similar scale to that of DDSM, EPD offers greater uniqueness: each sequence corresponds to a unique promoter-gene combination, a guarantee not provided by the other datasets.\nBaseline Model We evaluate the state- of-the-art diffusion models for DNA sequence generation: DDSM [4], DNADiffusion [26], DDPM [2, 3], and a AR model Hyena [20, 24]. In ad- dition, we implement a VAE with a CNN-based encoder-decoder architec- ture. Adding UNet as the denoising network to VAE results in another baseline latent diffusion model termed DiscDiff. For a fair evaluation, we maximally scale up the denoising net- works of each diffusion model to fit into an 40GB NVIDIA A100. Additionally, we adapt four pretrained Hyena models from Hugging- Face for comprehensive fine-tuning. The additional details of the network architectures are shown in Appendix D.\nModel Training All the models are implemented in Pytorch and trained on a NVIDIA A100-PCIE- 40GB with a maximum wall time of 48 GPU hours per model; most of the models converged within the given time. Adam optimizer [7] is used together with the CosineAnnealingLR [22] scheduler. The learning rate of each model are detailed in Appendix D. For the evaluation of various diffusion models in unconditional generation (see Section 5.2.2), we sample 50,000 sequences from each model. For"}, {"title": "5.2.2 Evaluating Diffusion Models on Mammalian Model Organisms", "content": "One prerequisite of Fast A&E (Algorithm 2) is that the diffusion model $p_{\u03b8}^{DM}$ should be properly trained and provide accurate approximations of underlying data distribution. We first evaluate existing Diffusion Models on a subset of EPD datasets. This subset includes sequences from four mammalians H. Sapiens (human), Rattus Norvegicus (rat), Macaca mulatta, and Mus musculus (mouse), which collectively represent 50% of the total dataset. Training on this subset allows for a more precise assessment of the generative algorithm's accuracy in a unconditional generation setting."}, {"title": "Metrics", "content": "1. Motif Distribution Correlation (Corm) and Mean Square Error (MSEM): Corm is the Pearson correlation between the motif distributions of generated and natural DNA sequences for motifs like TATA-box, GC-box, Initiator, and CCAAT-box. MSEM is the average squared differences between these motif distributions.\n2. S-FID (Sei Fr\u00e9chet Inception Distance): Measures the distance between distributions of generated and natural DNA sequences in latent space similar to the FID metric [15] for images, replacing the encoder with the pre-trained genomic neural network, Sei [37]."}, {"title": "5.3 Multi-species DNA Sequences Generation", "content": "We compare our model Fast A&E with Hyena [24] and the best-performing diffusion model from Section 5.2.2, DiscDiff, on the task of generating species-specific DNA sequences.\nMotif-centric Evaluation We consider four types of motifs closely related to promoter activities {TATA-box, GC-box, Initiator, CCAAT-box}. We calculate 4 types of motif distributions for 15 species across 3 models, resulting in 180 frequency distributions."}, {"title": "Sequence Diversity", "content": "To assess the diversity of the generated sequences, we applied BLASTN [18] to check (1) the similarity between the training set (Natural DNA) and generated sequences from three models, and (2) the similarity within the generated sequences. BLASTN takes a query DNA sequence and compares it with a database of sequences, returning all the aligned sequences in the database that are similar to the query. For each alignment, an alignment score, the alignment length (AlignLen), and statistical significance (eValue) are provided to indicate the quality of the alignment, where a larger alignment score, a smaller statistical significance (eValue), and a longer alignment sequence (AlignLen) indicate a higher similarity be- tween the query sequence and the database sequences. Ideally, when using generated sequences to query the training dataset, a good generative sequence should align better than a random sequence, but not replicate the sequences in the training set."}, {"title": "Genome Integration with Promoter Sequences", "content": "As shown in Figure 5, to evaluate the functional properties of sequences generated by Hyena, DiscDiff, and A&E, we inserted the generated promoter sequences of length 128 bp upstream (5') of three commonly studied genes in oncology: TP53, EGFR, and AKT1 [6, 25, 29], which are closely related to tumor activities. Our goal was to determine which model generates promoter sequences that produce gene expression levels closest to those of natural promoters when reinserted into the human genome.\nWe use Enformer [5] to predict transcription profiles. Enformer takes a DNA sequence of 200k bps as input and outputs a matrix P\u2208 ]R896\u00d7638, representing a multi-cell type transcription profile. We sampled 300 promoter sequences from each source: Natural DNA promoters, Hyena, DiscDiff, and A&E. For each set, we calculated the average transcription profile across the sequences. The Sum of Squared Errors (SSE) between these average transcription profiles of the generated sequences and those of natural promoters are shown in Table 6. The results indicate that A&E produces the smallest SSE, suggesting it best captures the properties of natural DNA. This finding highlights the potential of generative algorithms to create promoter sequences that effectively regulate gene expression, with applications in bioproduct manufacturing and gene therapy."}, {"title": "5.3.1 Sensitivity Analysis of Tabsorb", "content": "We perform a sensitivity analysis of A&E algorithm over hyperparameter Tabsorb. As shown in Figure 6, with a small Tabsorb, the sequences generated by A&E are dominated by the diffusion model. As Tabsorb increases, the AR helps to correct the errors made by the DM. Finally, when Tabsorb is larger than 0.7, the correlation flattens and fluctuates. In conclusion, A&E is robust under different values of Tabsorb, and it is best to use the validation dataset to choose the optimal value. However, a wide range of Tabsorb can still be used with improved performance."}, {"title": "6 Conclusion", "content": "This paper demonstrates that (i) both the AutoRegressive (AR) model and Diffusion Models (DMs) fail to accurately model DNA sequences due to the heterogeneous nature of DNA sequences when used separately, and (ii) this limitation can be overcome by introducing A&E, a novel sampling algorithm that combines AR models and DMs. Additionally, we developed a fast implementation of the proposed algorithm, Fast A&E, which enables efficient generation of realistic DNA sequences without the repetitive function evaluations required by conventional sampling algorithms. Experimental results across 15 species show that Fast A&E consistently outperforms single models in generating DNA sequences with functional and structural similarities to natural DNA, as evidenced by metrics such as Motif Distribution, Sequence Diversity, and Genome Integration. Regarding the future work, the generated DNA sequences still require validation through wet-lab experiments before they can be directly used in clinical settings."}]}