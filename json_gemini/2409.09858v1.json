{"title": "A Survey of Out-of-distribution Generalization for Graph Machine Learning from a Causal View", "authors": ["Jing Ma"], "abstract": "Graph machine learning (GML) has been successfully applied across a wide range of tasks. Nonetheless, GML faces significant challenges in generalizing over out-of-distribution (OOD) data, which raises concerns about its wider applicability. Recent advancements have underscored the crucial role of causality-driven approaches in overcoming these generalization challenges. Distinct from traditional GML methods that primarily rely on statistical dependencies, causality-focused strategies delve into the underlying causal mechanisms of data generation and model prediction, thus significantly improving the generalization of GML across different environments. This paper offers a thorough review of recent progress in causality-involved GML generalization. We elucidate the fundamental concepts of employing causality to enhance graph model generalization and categorize the various approaches, providing detailed descriptions of their methodologies and the connections among them. Furthermore, we explore the incorporation of causality in other related important areas of trustworthy GML, such as explanation, fairness, and robustness. Concluding with a discussion on potential future research directions, this review seeks to articulate the continuing development and future potential of causality in enhancing the trustworthiness of graph machine learning.", "sections": [{"title": "1 Introduction", "content": "In recent years, graph machine learning (GML), such as graph neural network (GNN) [1,2], has garnered tremendous attention across various research communities, springing up in many high-stakes scenarios such as economic analysis [3], scientific discovery [4], crime prediction [5], and pandemic screening [6]. Despite its burgeoning success in various tasks, GML still faces critical challenges, particularly in generalization on out-of-distribution (OOD) data, which casts doubts on its trustworthiness for broader applications. Compared with other data types, generalization on graphs faces unique challenges due to the complex nature of graph structure and the intrinsic dependencies within it, along with the multiple types of distribution shifts in node attributes and graph structure. Therefore, directly employing OOD generalization approaches in other data types often fails on graphs. The unique challenges of GML generalization have spurred a wave of research aimed at enhancing the generalization capabilities of GML through diverse approaches [7].\nAmong existing studies in GML generalization, causality-involved GML methods [8] have made eye-catching progress. Unlike conventional GML approaches that primarily leverage statistical dependencies for downstream tasks, recent studies [9, 10] suggest that causality often plays a more pivotal role in understanding the underlying data generation mechanisms. For instance, traditional GML techniques tend to overfit and rely excessively on spurious correlations [11], which, while effective in some scenarios, often fail to capture the causal mechanisms that fundamentally govern the relationships within the data, thereby undermining the models' ability to generalize across new data domains. Causality [12], with its focus on causal relationships rather than merely statistical dependencies, thereby can naturally discern true causal mechanisms and eliminate spurious correlations. This motivation has led to a growing enthusiasm for integrating causality into GML frameworks.\nIn this paper, we systematically review recent advancements in causality-involved GML generalization, covering their objectives, technologies, and effectiveness from different angles. More specifically, we first introduce the principal concepts and intuition that employ causality to enhance the generalization of graph models. We then categorize existing methods into distinct branches, detailing their techniques and exploring their interconnections. Additionally, we broaden our discussion to include the application of causality in other"}, {"title": "Differences from existing surveys", "content": "critical aspects of trustworthy graph machine learning. Finally, we highlight unresolved issues, articulate ongoing challenges, and outline prospective directions for future research in this rapidly evolving field.\nDifferences from existing surveys. There have been a couple of surveys in related areas, including graph OOD generalization [7], causality-inspired graph neural networks [8, 13], causality learning in graphs [9], causal machine learning [10], and trustworthy GML [14\u201316]. The surveys closest to us are [7] and [8], but our work distinguishes itself from existing ones in the following aspects: (1) Main Focus. Our survey mainly centers on OOD generalization within GML from a causal perspective, a topic that other surveys have not primarily addressed. (2) Organization. We have structured the review of existing works differently, offering a unique layout that enhances understanding and integration of the mentioned materials. (3) Timeline. We provide coverage of the most recent advancements and discussions in this field. To be best of our knowledge, there has not been a comprehensive survey focusing on this unique topic.\nThe general organization of this survey is presented below:\n\u2022 Overview and preliminaries. In Section 2, we introduce the background knowledge and give an overview of this area. First, we will present the key concepts in causality learning, the common scenarios and tasks of GML, and the generalization issues. We also introduce the motivations and challenges of incorporating causality into graph machine learning.\n\u2022 Method review. In Section 3, we categorize existing approaches of causality-involved GML generalization into different groups and introduce their techniques and connections. We will start with the basic intuition of these studies, and then review the related frameworks and technologies. These studies mainly cover invariant learning, causal model-based methods, and stable learning.\n\u2022 Connection to other trustworthy domains. In Section 4, we extend our discussion to the use of causality in other related areas of trustworthy GML, such as explanation, fairness, and robustness. We explore the intrinsic connections between these domains to offer readers a more comprehensive and expandable sight for the whole picture of the related research areas.\n\u2022 Future work. In Section 5, we summarize current efforts in causality-involved GML generalization. Furthermore, we look forward and outline promising future research directions."}, {"title": "2 Preliminaries", "content": "2 Preliminaries\n2.1 Causality and Causal Inference\nIn this section, we provide important background knowledge on causal inference. We start with the key tasks and concepts in causal inference, introducing the notations, definitions, and frameworks. Generally, causal inference aims to investigate the causality between different data variables.\nDefinition 1 (Structural causal model (SCM)). Structural causal model [12] is a widely adopted framework to model causal relationships. An SCM can be defined with a triplet of sets $(\\mathcal{U},\\mathcal{V}, \\mathcal{F})$, here, $\\mathcal{U}$ is a set of exogenous variables, $\\mathcal{V}$ is a set of endogenous variables, and $\\mathcal{F} = \\{f_1(\\cdot), f_2(\\cdot), ..., f_{|\\mathcal{V}|}(\\cdot)\\}$ is a set of functions (known as structural equations) that describe the causal relationships between variables. For each $V \\in \\mathcal{V}$, there is a structural equation $V = f_V(pa_V, U_V)$, where $pa_V \\subseteq \\mathcal{V} \\backslash V$, $U_V \\subseteq \\mathcal{U}$ are variables that directly cause $V$.\nEach SCM is associated with a causal graph, usually, it is a directed acyclic graph (DAG) with variables represented as nodes and causal relationships represented as directed edges. The conditional independence relationships between variables are straightforwardly reflected by the causal graph. Different from dependencies, causal relations only spread through directed paths whose edges are in the same direction. On causal graphs, there are three types of basic junctions: chain (e.g., $X \\rightarrow Z \\rightarrow Y$, here $X$ has causal effects on $Y$ through a mediator $Z$), fork (e.g., $X \\leftarrow Z \\rightarrow Y$, here $Z$ serves as a confounder which brings non-causal dependency"}, {"title": "2.1 Causality and Causal Inference", "content": "between $X$ and $Y$), and collider (e.g., $X \\rightarrow Z \\leftarrow Y$, here $Z$ is a collider bringing non-causal dependency between $X$ and $Y$ when conditioning on $Z$). One of the key challenges in causal inference is to identify the causal relationships or effects out of all the statistical dependencies. The gold standard approach of causal inference - randomized controlled trials (RCTs) are often infeasible or unethical to practice in the real world. Here, we introduce a couple of other commonly used approaches and their related concepts. A foundational concept in SCM is the do-operator $do(\\cdot)$, which stands for an intervention. Based on this, we have the following definition of causal effect in an average case:\nDefinition 2 (Average treatment effect). The average causal effect of a certain treatment (a.k.a. cause) $T$ (for simplicity, we usually assume it is a binary variable) on an outcome $Y$ can be formalized as follows:\n$ATE = E[Y|do(T = 1)] \u2013 E[Y|do(T = 0)]$\nFor a pair of treatment $T$ and outcome $Y$, there often exist backdoor paths that possibly bring non-causal dependencies for them. A backdoor path neither is a directed path nor contains any collider. Typical ways for unbiased causal effect estimation include backdoor adjustment, frontdoor adjustment, and instrumental variable (IV)-based approaches.\nDefinition 3 (Backdoor adjustment). Under certain basic causal assumptions \u00b9, for a pair of treatment $T$ and outcome $Y$, if there are variables $Z$ that (1) block all the backdoor paths between $T$ and $Y$, (2) do not contain any descendants of $T$, we have\n$P(Y\\vert do(t)) = \\sum_Z P(Z)P(Y\\vert t, Z)$\nIn certain cases (e.g., when hidden confounders exist), it is difficult to find an observed variable set $Z$ for backdoor adjustment. Alternatively, we can search for other variable sets which either meet the frontdoor criterion, or serve as instrumental variables.\nDefinition 4 (Frontdoor criteron). A set of variables $M$ satisfies the frontdoor criterion relative to $T$ and $Y$ if: (1) $M$ completely mediates the effect of $T$ on $Y$; (2) There is no unblocked backdoor path from $T$ to $M$; (3) All backdoor paths from $M$ to $Y$ are blocked by $T$.\nDefinition 5 (Instrumental variable (IV)). Given treatment $T$ and outcome $Y$, a variable $I$ can serve as an instrumental variable if it satisfies the following conditions: (1) (Relevance) $I$ is relevant to the treatment $T$; (2) (Exclusion restriction) The causal effect from $I$ to $Y$ is mediated by $T$; (3) (Instrumental unconfoundedness) There is no unblocked backdoor path from $I$ to $Y$.\nFor variables satisfying frontdoor criterion, we can use frontdoor adjustment [12] to estimate causal effect. Similarly, for instrumental variables (IVs), causal effects can be determined using appropriate IV-related methodologies [17]."}, {"title": "2.2 Causality and Graph Machine Learning", "content": "2.2 Causality and Graph Machine Learning\nA graph can be denoted by $G = (\\mathcal{X}, A)$, where $\\mathcal{X} \\in \\mathbb{R}^{n \\times d}$ denotes the node features and $A \\in \\{0, 1\\}^{n \\times n}$ is an adjacency matrix representing the graph edges. Here, $n$ is the number of nodes, and $d$ is the feature dimension. GML involves a variety of tasks related to graphs with the prediction target $Y$ in different granularities, such as local-level prediction tasks like node classification and link prediction, as well as graph-level tasks like graph classification. In GML, models are specifically designed to handle the unique challenges posed by graph data, e.g., capturing the dependencies and relationships between nodes. Prominent models in this domain include many branches extending traditional neural network architectures to operate on graph structures, such as the representative graph convolutional network (GCN) [46], graph attention network (GAT) [47], graph"}, {"title": "3 Methodologies", "content": "3 Methodologies\nIn this section, we will introduce the principles and methodologies of leveraging causality for GML generalization. We categorize the mainstream of works into invariant learning, causal modeling, and stable learning. Fig. 1 shows an overview of the categorization for these methods. A more detailed comparison of these methods is in Table 1.\n3.1 Invariant Learning\nInvariant learning targets on capturing the relations that are invariant across different domains. It is motivated by the fact that spurious correlations often vary under distribution shifts. The general principle of invariant learning is improving generalization by extracting the invariant factors to make predictions, while the spurious correlations are filtered out. The formal assumptions of invariant learning slightly vary in different works, but generally, there should exist invariant factors $\\Phi(G)$ from input $G$, that $P_e(Y|\\Phi(G))$ remains stable in different domain"}, {"title": "3.1 Invariant Learning", "content": "e. Although invariant learning is not explicitly driven by causal inference, much literature has discussed their intrinsic connections [50,51]. In general, in a causal view, the direct causes for the label $Y$ should have invariant relationships in different domains. The idea of invariant learning is first proposed for tabular data, including representative methods such as invariant risk minimization (IRM) [11] and EIIL [52]. However, on graphs, directly applying these methods often results in unsatisfying performance. Invariant learning on graphs often extracts a subgraph as a rationale that generalizes across domains. Accordingly, many graph-specific invariant learning methods have been proposed in recent years, and take one of the mainstreams in OOD generalization for GML.\nNode-level invariance learning. Explore-to-Extrapolate Risk Minimization (EERM) [18] uses multiple adversarial context generators to simulate (virtual) environments even under a single (real) environment, and a GNN model is trained by minimizing the mean and variance of risks from these simulated environments. Different from the single environment setting in EERM, Li et al. [19] argue that nodes are often from multiple latent environments in the real world, and propose an approach INL that can infer node environments with a contrastive modularity-based graph clustering method and learn invariant node representations. A recent unique work Flexible invariant Learning framework for Out-Of-Distribution generalization on graphs (FLOOD) [20] combines invariant learning and bootstrapped learning. It first constructs multiple training environments based on data augmentation, then adopts a bootstrapped learning module. In this way, their encoder is more flexible than traditional invariant encoders as it can be refined on the test set for better generalization.\nGraph-level invariance learning. Graph Invariant Learning (GIL) [23] is a GNN-based model that identifies the invariant subgraph for graph classification tasks. It is the first work of invariant graph representation learning under mixed latent environments without the supervision of environment labels. Discovering Invariant Rationales (DIR) [24] is an algorithm that infers invariant causal parts by conducting causal interventions, but it needs a complicated iterative process to break and assemble subgraphs during training. Another more straightforward method Graph Stochastic Attention (GSAT) [25] is based on the information bottleneck principle. It learns invariant subgraphs by learning stochasticity-reduced attention. Li et al. [26] propose Rationale-aware Graph Contrastive Learning (RGCL), which combines invariant rationale discovery with graph contrastive learning to improve generalization and interoperability. CIGA [27], with a supported causal graph shown in Fig. 2(e), proposes an information-theoretic objective to extract invariant subgraphs with a theoretical guarantee to handle distribution shift under different SCMs. In a follow-up work, Chen et al. [28] analyze the failure cases of existing methods such as CIGA, and introduce minimal assumptions for feasible invariant graph learning. They propose a framework GALA with provable invariant subgraph identifiability for OOD generalization. DisC [29], inspired by a causal graph shown in Fig. 2(d), disentangles the given graph into a causal substructure and a bias substructure. The disentanglement is conducted with a parameterized edge mask generator. Then two GNNs are trained to encode the causal and bias substructures respectively into their representations trained with causal/bias-aware loss functions. To further decorrelate causal and bias variables, DisC also generates unbiased counterfactual training samples. For all of the invariant learning biased methods, a common conclusion is that the effectiveness of invariant learning is greatly dependent on the variety of environments. Realizing this, a co-mixup strategy IGM [30] is proposed, which jointly adopts environment mixup and invariant mixup to generate sufficiently diverse environments. Many existing graph OOD generalization methods either do not assume the existence of environment labels, or do not fully exploit them. Differently, LECI [31] is proposed to utilize pre-collected environment information for graph-specific OOD generalization. It discovers causal invariant subgraphs by leveraging the two causal independence properties regarding label and environment: $E \\perp G_c$ and $Y \\perp G_s$ and designs an adversarial learning strategy to jointly optimize these two causal independence properties. Here, $E$ is the environment, $G_c$ and $G_s$ are the causal subgraph and spurious subgraph, respectively.\nDomain-specific invariant learning. SILD [21] is the first work to study distribution shifts on dynamic graphs in the spectral domain, it captures invariant and variant spectral patterns with disentangled spectrum masks for both node classification and link prediction tasks. Dynamic graph Attention network (DIDA) [22] handles complex spatio-temporal distribution shifts in dynamic graphs by using a spatio-temporal attention network to identify variant and invariant spatio-temporal patterns. In molecular representation learning (MRL),"}, {"title": "3.2 Causal Modeling", "content": "3.2 Causal Modeling\nInspired by studies exploring causal relationships within graphs [8,9], a notable line of research has emerged that explicitly constructs SCMs on graphs to enhance GML OOD generalization. This research typically relies on predefined assumptions about the underlying SCM, which are illustrated through clear causal graphs that illustrate the causal relationships among the graph, labels, causal features, and spurious features. An overview of these commonly used causal graphs is presented in Fig. 2. These methods often employ traditional causal inference principles to reduce spurious correlations and improve generalization. Based on the key techniques, we categorize these methods into several branches: backdoor adjustment, frontdoor adjustment, instrumental variable-based methods, and those related specifically to graph modeling.\nBackdoor adjustment. Causal Attention Learning (CAL) [34] is based on the causal assumption that there exist shortcut features that serve as confounders between the causal features and graph prediction. Under this assumption, CAL employs attention modules to estimate the causal features and shortcut features of the input graph, and conduct backdoor adjustment to mitigate the spurious correlations led by the backdoor path"}, {"title": "Backdoor adjustment", "content": "$C\\leftarrow G\\rightarrow S\\rightarrow Z\\rightarrow Y$, shown in Fig. 2(a). A follow-up work CAL+ [35] generally inherits the idea of CAL, but it further enhances the method with a memory bank to improve the diversity of shortcut feature samplings and a prototype module to enhance the consistency of intra-class causal features. For node-level distribution shift, CaNet [36] uses a straightforward causal graph shown in Fig. 2(b), relying on backdoor adjustment and variational inference. It counteracts the confounding bias by collaboratively training an environment estimator and a GNN predictor.\nFrontdoor adjustment and instrumental variable. Wu et al. [37] also attribute the OOD distribution shift to the confounder effect. As shown in Fig. 2(c), they propose a method Deconfounded Subgraph Evaluation (DSE) which introduces a surrogate $G$ between the explanatory subgraph $G_s$ and model prediction $Y$ to mitigate confounding bias via frontdoor adjustment. The surrogates are generated by a generative model based on a conditional variational graph auto-encoder. Another approach RCGRL [38] eliminates confounding bias by generating instrumental variables (IV) under unconditional moment restrictions. On graphs, the conditions of instrumental variables are often hard to satisfy, instead, RCGRL proposes an active IV generation approach that transfers the conditional moment restrictions into unconditional ones with theoretical support.\nGraph models inspired causal models. $E$-invariant GR [39] constructs a fine-granularity causal graph (shown in Fig. 2(f)) including graphon, label, training/text environment, node attributes and edges of training/test graph, and number of nodes. With this causal graph, it aims to learn environment-invariant graph representations that are generalizable to shifts in graph size and node attributes. The invariant representations are learned based on the stability of subgraph densities in graphon random graph models [53]. This unique work incorporates Stochastic Block Models (SBMs) [54,55] and graphon random graph models [53,56] inside its causal model. A study in a similar setting [40] extends to link prediction problem by proposing a new family of structural pairwise embeddings gMPNN, with its causal graph shown in Fig. 2(g).\nCounterfactual reasoning. In the context of causal inference, the term \"counterfactual\" stands for a hypothetical scenario that deviates from actual events. For instance, one might ask, \"Would the label have changed if a specific subgraph had been altered?\". In this line of research, CFLP [41] employs counterfactual reasoning for OOD link prediction. It focuses on the causal relationships between the graph structure and the existence of links, i.e., \u201cwould the link still exist if the graph structure became different?\u201d. This method is achieved by training GNN-based link predictors to predict both actual (factual) links and counterfactual links."}, {"title": "3.3 Stable Learning", "content": "3.3 Stable Learning\nStable learning [43,57], with its primary goal of learning a model that can perform uniformly well in any environment, originates from the sampling reweighting or covariate balancing strategies in causal effect estima- tion [58]. More specifically, many traditional causal inference methods estimate causal effects by using sampling reweighting to assign sample weights that balance the distribution of covariates across different treatment groups. In stable learning, each input feature is treated as a treatment, while other variables are considered covariates. It focuses on learning weights to decorelate the features, thereby balancing the covariate distribution with respect to each feature treated as a treatment. In this way, the correlation between each feature (treatment) and the prediction label (outcome) results from a direct causal effect. Then a predictive model based on correlation can achieve better generalization across varied data environments.\nIn this area of studies, OOD-GNN [42] adopts a nonlinear graph representation decorrelation method that leverages random Fourier features. It reduces the statistical dependencies between relevant and irrelevant graph representations by iteratively optimizing the sample weights and the graph encoder. This approach promotes model generalization against multiple shift types, including graph size, node attribute, and graph structure. Different from OOD-GNN which learns a single embedding for each graph, StableGNN [43] extracts high-level representations for subgraphs with graph pooling layers from the input graph. Based on these high-level representations, StableGNN designs a causal variable distinguishing regularizer to de-bias the training distribution through sample weight learning. Similarly, Debiased GNN (DGNN) [44] devises a differentiated decorrelation regularizer for debiasing at the node level. Later on, researchers argue that methods like StableGNN and OOD-GNN may suffer from the overly aggressive objective that eliminates the dependencies between all the"}, {"title": "3.4 Discussion", "content": "variables across the graph representations, leading to an excessively small sample size. With this motivation, they propose L2R-GNN [45] which first clusters the variables in graph representations based on the correlation stability, and then only learns weights to eliminate correlations between variables across different clusters, instead of removing correlations between any pair of variables.\n3.4 Discussion\nDespite the categorizations outlined above, many methods demonstrate deep interconnections and underlying equivalencies. For example, several invariant learning techniques, such as CIGA [27] and Disc [29], are explicitly underpinned by structural causal models, as shown in Fig. 2, positioning them at the intersection of multiple categories. Additionally, some other techniques are also closely related to the approaches discussed, including disentangled representation learning [29], representation decorrelation [42, 43], causal interventions [36], counterfactual reasoning [41], and data augmentation [29,59]. These connections highlight the complex, often overlapping landscape of methodologies within this field."}, {"title": "4 Connection to Other Domains in Trustworthy Graph Machine Learning", "content": "4 Connection to Other Domains in Trustworthy Graph Machine Learning\nEven though this survey mainly focuses on causality-based generalization for GML, it is worth mentioning that many principles and technologies in this area closely connect to other domains of trustworthy GML. Here, we give a conceptual overview of these intrinsic connections.\n4.1 Explanation\nAlthough explanation has many different definitions, in graphs, there are generally two main categories for GML explanation, including those that aim to identify the rationale that contributes most to prediction (factual explanation) with the question \"what contributes most to the prediction?\", and counterfactuals that can achieve a certain desired outcome (counterfactual explanation) with the question \"what is the slightest change I can make on the input graph to achieve a desired prediction?\".\nFactual explanation. With its ultimate goal, factual explanation naturally relates to generalization w.r.t. the common focus on the rationale that causes the prediction target. This concept is intuitively equivalent to the invariant variables in invariant learning, or causal variables in causal models that are stable across environments. Therefore, many explanation methods also naturally have a dual objective in generalization, including invariant learning methods such as DIR [24], GSAT [25], GREA [59]; causal modeling methods like CAL/CAL+ [34,35] and DSE [37], and Granger causality [60] based methods such as GEM [61] and CI-GNN [62].\nCounterfactual explanation. Counterfactual explanation [63] studies the problem of making perturba- tions on the input graph to change the model prediction. Even though the original concept of counterfactual explanation does not involve causal inference, recent studies [64, 65] have started to discuss the benefit of explicitly incorporating causality into counterfactual explanation. In this context, it is worth noting that making perturbations itself would result in certain outcomes, which relates to counterfactual reasoning in a causal context. Therefore, many recent works utilize causal methods for graph counterfactual explanation such as CLEAR [65], which involves a deep graph generative model to promote causality in counterfactual explanation. Another work CF2 [66] combines both factual reasoning and counterfactual reasoning to obtain the necessary explanation for graph models."}, {"title": "4.1 Explanation", "content": "4.2 Robustness\nAdversarial robustness refers to the ability of a model to perform correctly and maintain its integrity under adversarial attacks. These attacks are typically slight perturbations on the original input data, also known as adversarial examples. These adversarial examples are usually imperceptible by human eyes but are crafted to mislead the model into making mistakes. Adversarial robustness and OOD generalization are interconnected areas as the adversarial samples can be considered from an adversarial distribution outside of the training data. Explorations in this area from a causal view are rare, but still offer valuable insights. For example, IDEA [67]"}, {"title": "4.3 Fairness", "content": "4.3 Fairness\nFairness in machine learning aims to eliminate the bias against any demographic groups or individuals. It is widely regarding certain sensitive features (e.g., age, gender). There have been lots of notions of fairness defined from different perspectives. In recent years, causality-based fairness has attracted more and more attention since it can track the root and path of bias in its generation process, providing explanatory and controllable approaches for mitigating potential discrimination. Counterfactual fairness [68] is one of the most well-known notions in this line, which measures fairness by comparing the model prediction under one's original sensitive feature and a counterfactual case. An example question is: would a male applicant have the same chance to get a job offer if he had been a female?\nA fair model must effectively handle various sensitive feature values, which are often regarded as different domains. In this sense, a fair model should also demonstrate strong generalization capabilities across these different domains. From a causal view, it requires identifying the causal path from sensitive features to other variables used in prediction, and eliminating (a part of) these paths to achieve fairness. This relates to the mitigation of environment effects on model prediction in generalization. On graphs, fairness is a more complicated task due to the bias (causally) propagated through graph links. Studies for counterfactual fairness, such as GEAR [65] and RFCGNN+ [69] incorporate causal reasoning in graphs."}, {"title": "5 Future Work", "content": "5 Future Work\nIn this paper, we have conducted a comprehensive review of causality-involved approaches for OOD gener- alization of graph machine learning. We begin by highlighting the motivations and challenges inherent to this area, providing a structured categorization of existing methodologies based on their technical approaches. Furthermore, we discuss the commonalities and differences of methods in this field, and also introduce the connections between them and related studies in other areas. Through this analysis, we have extracted valuable insights and established a robust foundation for ongoing exploration in related fields.\nLooking ahead, there are several promising avenues for further studies:\n\u2022 Application in high-stakes domains for graph trustworthiness: Causal models in important domains such as science, finance, law, and health often incorporate domain knowledge and require a high degree of trustworthiness. These fields demand rigorous frameworks when modeling the causal relationships as well as the graph structure due to their reliance on precise and professional knowledge. Future research could focus on customizing domain-specific causality-enhanced graph models to maintain the trustworthiness and application of GML in high-stakes domains.\n\u2022 Uncertainty quantification of causality-involved GML: While existing GML methods have demon- strated impressive performance, it is crucial to include uncertainty quantification, especially under dis- tribution shifts on graphs. This aspect might intersect with emerging research on conformal prediction in causal inference [70] and graph learning [71]. Addressing uncertainty quantification will enhance the reliability and applicability of GML in varying environments.\n\u2022 Graph-related AGI incorporating causal knowledge: Another exciting direction is the integration of causal knowledge into graph foundation models (GFMs). GFMs have the capability of addressing a broader range of tasks beyond traditional GML limits. While large models exhibit superior generalization, they still face challenges such as bias from training domains. Incorporating human-provided or data-driven causal insights into these foundation models has the potential to effectively improve their trustworthiness"}, {"title": "5 Future Work", "content": "and mimic complex human reasoning processes, which represents a significant step toward achieving artificial general intelligence (AGI). But currently, this task is much more challenging than traditional causality-involved GML due to the large scale of GFMs."}]}