{"title": "Context Aware Lemmatization and Morphological Tagging Method in Turkish", "authors": ["\u00c7a\u011fr\u0131 Sayallar"], "abstract": "The smallest part of a word that defines the word is called a word root. Word roots are used to increase success in many applications since they simplify the word. In this study, the lemmatization model, which is a word root finding method, and the morphological tagging model, which predicts the grammatical knowledge of the word, are presented.\nThe presented model was developed for Turkish, and both models make predictions by taking the meaning of the word into account. In the literature, there is no lemmatization study that is sensitive to word meaning in Turkish. For this reason, the present study shares the model and the results obtained from the model on Turkish lemmatization for the first time in the literature. In the present study, in the lemmatization and morphological tagging models, bidirectional LSTM is used for the spelling of words, and the Turkish BERT model is used for the meaning of words.\nThe models are trained using the IMST and PUD datasets from Universal Dependencies. The results from the training of the models were compared with the results from the SIGMORPHON 2019 competition. The results of the comparisons revealed that our models were superior.", "sections": [{"title": "1 Introduction", "content": "A root is the lowest, most significant component of a word. Due to the agglutinative nature of the Turkish language, the same root word can appear in many words. For this reason, in applications such as sentence analysis, machine translation, and small datasets, the root of the word is utilized instead of the actual word. According to the meaning of the word, the two word root-finding methods are categorized as stemming and lemmatization. The stemming method is an algorithm that reduces the word to smaller parts than predetermined prefixes. The reason why it is called the \"smaller part\" is that stemming algorithms cannot always obtain the root of the word. In other words, it guesses a completely meaningless word fragment. In the stemming method, the content of the sentence is not considered when the root of the word is found. Words are evaluated independently of the sentence and word roots are sought. For this reason, it cannot accurately predict ambiguous words that have the same spelling but whose root words will change according to their meaning in the sentence. For example, while the root of the word \"oku\" in Turkish can be the verb \"oku(-mak)\", it can also be derived from the noun \"ok\". Without the context of the statement, stemming can only speculate against such confusing words. The lemmatization method has been produced as a solution to the problem of the stemming method, which is unaware of the sentence content and the meaning of the word. The proposed lemmatization method finds the root word while taking into account the meaning of each word in the phrase. In the lemmatization approach, a word's root can vary from sentence to sentence, but in the stemming method, words written in the same way always produce the same root.\nThe purpose of this study is to develop a word-meaning-sensitive lemmatization and morphological tagging method for Turkish individuals. Depending on whether it relies on the results of the morphological tagging model, the developed lemmatization model consists of two alternative models. Words are morphologically classified into categories such as noun, verb, adjective, pronoun, number, punctuation mark, and gender. A word can have more than one morphological tag at the same time. The morphological tag of a word can change according to the meaning of the word, as in the lemmatization method. For example, the word \"oku\" can be labeled a noun or a verb depending on its meaning in the sentence. Here, correct guessing of the morphological tag of the word can also help the lemmatization process, as seen from the example given. For this reason, studies use the output of one of these two tasks as the input of the other, but when the mentioned studies are examined, sending the output obtained from the morphological labeling model to the lemmatization model does not always increase success [1-3]. In our study, two distinct lemmatization models were used, and the results were given in accordance with the input of the morphological tag for the lemmatization model, since it is difficult to find datasets with the morphological tag of the word. In lemmatization and morphological tagging models, the meaning of the word must be known. In both models, the word vector obtained from the Turkish Bidirectional Encoder Representations from Transformers (BERT) [4] model was used to describe the meaning of the word. Bidirectional long-short-term memory (LSTM) [5] is"}, {"title": "2 Literature Review", "content": "Many algorithms have been proposed for stemming methods that break words into smaller parts. The most well-known of these is the Snowball [8] stemming method, which is based on the Porter [9] algorithm. Snowball divides the word into smaller parts by removing the suffixes at the end. Although the Snowball algorithm often yields better results than other stemming algorithms, it still cannot predict the root word correctly. In Turkish, for example, the root of the word \"bakan\" can be either the verb \"bak(-mak)\" or the noun \"bakan.\" When the mentioned word is sent to the Turkish Snowball algorithm, its root is estimated as \"baka\", and this word is a meaningless word that does not exist in Turkish. In addition to the stemming algorithms, studies have been carried out only for the Turkish language. In one of these studies [10], a finite-state machine (FSM) was developed to find the root of Turkish words. The developed method involves removing the suffixes of the word to identify the root of the word. They used 130 different predetermined suffixes from the Turkish language when deciding how to affix the noun. In this study, root estimation is performed according to the spelling of the word, not the meaning of the word, so their work is similar to that of the stemming algorithms. In the study [11], a deep learning model was used for root prediction. By running"}, {"title": "3 Method", "content": "Lemmatization and morphological tagging are comparable techniques. Both the word's spelling and its meaning must be accurately reflected for the prediction to be accurate. Since the approaches for solving morphological tagging and lemmatization problems are similar, similar models are used for both problems in this study. The only difference between the morphological tagging and lemmatization models is that the lemmatization model takes morphological tags as input. Similar models are used for both problems in this work, since the approaches for solving the morphological tagging and lemmatization problems are similar. The only difference between morphological tagging and lemmatization is that the lemmatization model can use morphological tags as input. The model, which is independent of the lemmatization and morphological tagging model, is called a separate model. A diagram of the sequenced and separate models is shown in Figure 1. The morphological tagging model and the lemmatization model are trained independently in the separate model. For the sequenced model, the morphological labeling model was trained first, and the model with the highest success rate was obtained. The lemmatization model is then trained using the morphological tag predictions as input that"}, {"title": "3.1 Encoder", "content": "The encoder part of the model is responsible for generating the spelling vector and the meaning vector of the word. For word meaning, a single vector is obtained by applying a mask to the outputs obtained from the BERT model, as mentioned before. Character embedding and bidirectional LSTMs are used for the word spelling vector. Word characters are represented by numbers. The one-dimensional vector representing this word is passed through the embedding"}, {"title": "3.2 Decoder", "content": "The information from the encoder layer is used to produce outputs in the decoder layer step by step. These outputs are morphological tags in the morphological tagging model and letters in the word root in the lemmatization"}, {"title": "4 Experiments and Results", "content": "This section first describes the data set and then describes the training parameters used to train the suggested model. The training results are then evaluated by comparing them with the outcomes of the SIGMORPHON competition."}, {"title": "4.1 Dataset", "content": "The Universal Dependencies Turkish IMST and PUD datasets are used with our model. The version of the datasets shared by the SIGMORPHON competition is used for a fair comparison of model results. The data set includes each word in the sentence, the root of the word, and its morphological tags. Since the data set is in Conllu format, the Conllu library [19] is used for processing. Because there are different numbers of morphological tag classes in the IMST and PUD datasets, different predetermined morphological tag lists were established for both datasets when tagging the data. The word morphological tags are arranged in order before they are included in the training data because the morphological tags are mixed in both datasets and their order is irrelevant. More success is achieved with sequential labeling of morphological tags than with mixed tagging."}, {"title": "4.2 Training Parameters", "content": "There are two phases in model training. The BERT model is frozen during the first step of training. The BERT model is also trained with a slower learning rate in the second stage. To greatly reduce training time, the output of the BERT model is created prior to training and sent as input to the model in the first stage of training. The BERT model is also trained during the second stage.\nIn the first stage of training, the model is trained for 128 epochs. The weight that gives the best success is recorded. The value 64 is used as the batch size. The Adam function [20] was used for the optimization function, and the 1E-3 value was used for the learning rate. Categorical cross-entropy was used as the loss function, whereas Softmax was used for classification. The weight with the highest degree of success from the first phase of training is used to continue training in the second phase. At this point, the BERT model is trained with a batch size of 32 and a learning rate of 1e-5. Training is stopped early if the second stage has been trained for 48 epochs and has successfully completed the required number of epochs in a row under a particular loss value. Once more, the best result is recorded."}, {"title": "4.3 Tests", "content": "The model output contrasts with the results of the SIGMORPHON 2019 competition. The competition owners made available the datasets that were used in the contest. The data sets were obtained from the Universal Dependencies database; however, the contest owners modified the data sets. For this reason, the Universal Dependencies Turkish IMST and PUD datasets shared in the competition were used to compare the results. These data sets are compared across the categories determined in the SIGMORPHON competition. The code by which the success of our model is measured is the benchmark code shared by the SIGMORPHON competition. The achievements of our model and the best achievements in the IMST dataset in the SIGMORPHON competition are shared in Table 1, and the achievements in the PUD dataset are shared in Table 2."}, {"title": "5 Conclusion", "content": "This work offers Turkish-specific morphological tagging and lemmatization models that are mindful of word meaning. Two models, the separate model and the sequenced model, depend on whether the output of the morphological tagging model is used as input or not and are presented in the lemmatization topic. The analysis of the data revealed that the morphological tags increased the lemmatization in the IMST data set but had the opposite effect in the PUD data set. The work is significant because it presents the results of the first lemmatization model specifically focused on the Turkish language. In two data sets and almost all comparison measures, the results show that this study surpasses the findings of the SIGMORPHON competition, which are the results of a multilingual study with the highest success rate shared in Turkish. The studies in the SIGMORPHON competition focused on multiple languages, whereas our study considered only one language."}, {"title": "Declarations", "content": ""}, {"title": "Data Availability", "content": "The datasets used in this study is available at this repository."}, {"title": "Conflict of Interest", "content": "We have no conflicts of interest to disclose."}, {"title": "Funding", "content": "The authors received no financial support for research, authorship, and / or publication of this article."}, {"title": "Ethical approval", "content": "This article does not contain studies with human participants or animals performed by any of the authors."}]}