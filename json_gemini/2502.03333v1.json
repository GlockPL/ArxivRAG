{"title": "RadVLM: A Multitask Conversational Vision-Language Model for Radiology", "authors": ["Nicolas Deperrois", "Hidetoshi Matsuo", "Samuel Ruip\u00e9rez-Campillo", "Moritz Vandenhirtz", "Sonia Laguna", "Alain Ryser", "Koji Fujimoto", "Mizuho Nishio", "Thomas M. Sutter", "Julia E. Vogt", "Jonas Kluckert", "Thomas Frauenfelder", "Christian Bl\u00fcthgen", "Farhad Nooralahzadeh", "Michael Krauthammer"], "abstract": "The widespread use of chest X-rays (CXRs), coupled with a shortage of radiologists, has driven growing interest in automated CXR analysis and AI-assisted reporting. While existing vision-language models (VLMs) show promise in specific tasks such as report generation or abnormality detection, they often lack support for interactive diagnostic capabilities. In this work we present RadVLM, a compact, multitask conversational foundation model designed for CXR interpretation. To this end, we curate a large-scale instruction dataset comprising over 1 million image-instruction pairs containing both single-turn tasks - such as report generation, abnormality classification, and visual grounding and multi-turn, multi-task conversational interactions. After fine-tuning RadVLM on this instruction dataset, we evaluate it across different tasks along with re-implemented baseline VLMs. Our results show that RadVLM achieves state-of-the-art performance in conversational capabilities and visual grounding while remaining competitive in other radiology tasks. Ablation studies further highlight the benefit of joint training across multiple tasks, particularly for scenarios with limited annotated data. Together, these findings highlight the potential of RadVLM as a clinically relevant AI assistant, providing structured CXR interpretation and conversational capabilities to support more effective and accessible diagnostic workflows\u00b9.", "sections": [{"title": "1 Introduction", "content": "X-rays have played a fundamental role in medicine since their discovery in 1895 (R\u00f6ntgen, 1895), and continue to be the most frequently used medical imaging modality worldwide due to their convenience and cost-effectiveness (Akhter et al., 2023). Chest X-ray (CXR) remains the most commonly performed radiological exam globally, particularly important for diagnosing and monitoring thoracic conditions such as pneumonia, heart failure, and lung cancer (\u00c7all\u0131 et al., 2021). Problematically, the growing volume of CXRs and other imaging studies in recent years have lead to a reduction in the time available for radiologists to thoroughly evaluate each case (Peng et al., 2022). As a result, in many countries, the responsibility of interpreting CXRs is often transferred to non-radiology physicians, who typically possess less specialized training and experience. This shift increases the risk of diagnostic errors or misinterpretations (Shammari et al., 2021; Peng et al., 2022).\nThe shortage of trained personnel for CXR interpretation has led to the exploration of automated agents to assist physicians in diagnostic tasks. In recent years, various deep learning models have shown promise in clinical applications, such as the detection of conditions like COVID-19 pneumonia (Nishio et al., 2020) or pulmonary nodules (Homayounieh et al., 2021). Another extensively studied task is the automated generation of free text reports from CXR images using transformer-based architectures (Nooralahzadeh et al., 2021; Yang et al., 2023; Hyland et al., 2023; Chaves et al., 2024). These models can provide preliminary drafts summarizing key observations from the CXR, offering a potential enhancement to the diagnostic workflow. There is a need to expand the scope of these tools beyond report generation, towards the capability to answer questions about the CXR technique, findings in a region of interest, location of specific abnormalities, and definitions of medical terms. In addition, physicians should be allowed to formulate their queries flexibly and in any order, potentially within a multi-turn, conversational interaction with the assistant (Tu et al., 2024).\nRecently, significant advancements in the field of multimodal artificial intelligence (AI) have enabled the development of models such as GPT-4 Vision (GPT4-V, OpenAI, 2024) and Claude (Anthropic, 2024), which have the ability to describe and converse about images with increasing reliability. The training principle behind these models relies on an initial stage to align pretrained vision and language modules, followed by a visual instruction tuning phase where the model learns to respond to image-related queries and commands (Li et al., 2024). Such advancements have inspired the adaptation of multimodal AI assistants for medical applications (Singhal et al., 2023; Li et al., 2023a; Saab et al., 2024).\nDespite these advancements, there remains a need for specialized multimodal conversational assistants tailored specifically for CXR interpretation. In this direction, models such as CheXagent (Chen et al., 2024), RaDialog (Pellegrini et al., 2023), or MAIRA-2 (Bannur et al., 2024) were developed, extending beyond report generation to tasks such as observation grounding and visual question answering, covering a larger part of the clinical workflow. However, their capacity to handle diverse and complex user queries, or to respond accurately to multiple prompts within an arbitrary conversational framework, remains limited. Adding these capabilities is critical for comprehensively supporting clinicians' daily work.\nIn this study, we build upon state-of-the-art visual instruction-tuning techniques inspired by general-domain applications (Liu et al., 2023; Wang et al., 2024) to construct a compact, multitask conversational foundation model specialized in CXR interpretation. To achieve this aim, we create comprehensive CXR datasets, each featuring diverse modalities including free-text reports, abnormality labels, and visual coordinates, and organize them into a unified instruction dataset. This dataset is comprised of single-turn image-instruction pairs for different tasks and image-conversation pairs designed for more flexible and multi-turn interactions. We then fine-tune a vision-language architecture (Li et al., 2024) on this instruction dataset, naming the resulting model RadVLM, and develop an evaluation pipeline to assess its performance across multiple tasks, systematically comparing it to state-of-the-art generalist and CXR-specific foundation models. Our results show that, despite its relatively compact size, RadVLM achieves competitive performance on individual tasks relevant to clinical practice, providing conversational capabilities within a simple and flexible interface, providing a reliable and user-friendly tool for physicians. Additionally, we compare RadVLM with several existing vision-language models and show that RadVLM matches or outperforms these across both individual and conversational tasks. In summary, the contributions of this work are as follows:\n\u2022 We develop a unique instruction dataset that extends beyond report generation to encompass diverse CXR-based tasks, including multi-turn conversational interactions tailored for clinical workflows.\n\u2022 We design and train RadVLM, a multitask conversational foundation model designed to assist physicians in CXR analysis that solely relies on visual information avoiding the need for providing additional metadata.\n\u2022 We employ an evaluation pipeline, re-implementing existing models for comparison and ensuring reproducibility of results.\n\u2022 We evaluate RadVLM systematically across multiple tasks, demonstrating competitive performance against state-of-the-art vision-language models, both generalist, and medical-specific. In particular, we evaluate conversational abilities in clinical contexts and demonstrate that RadVLM significantly outperforms existing general and clinical VLMs in this aspect."}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Instruction tuning and vision-language models", "content": "The advent of autoregressive large language models (LLMs) based on the transformer architecture (Vaswani, 2017) and pre-trained on vast text corpora (Radford et al., 2019; Brown, 2020) has provided the possibility to perform a wide range of language-based downstream tasks. However, the widespread success and accessibility of LLMs, such as ChatGPT, are largely attributed to the instruction-tuning process (Wei et al., 2021; Ouyang et al., 2022). This process commonly involves fine-tuning a pre-trained model on a labeled dataset of diverse instruction-following tasks, ensuring the model can generalize to diverse user instructions in a zero-shot setting.\nInstruction-following datasets generally consist of instruction-output pairs and/or multi-turn dialogues (Zheng et al., 2023a) mimicking real-life interaction between users and AI assistants. While early instruction datasets were manually crafted (Wei et al., 2021), a more scalable approach leverages larger LLMs to generate synthetic instruction data (Wang et al., 2022; Peng et al., 2023; Liu et al., 2023), reducing annotation costs."}, {"title": "2.2 Vision-language models in radiology", "content": "The success of VLMs in the general domain has spurred the development of medical-based VLMs, particularly in domains where image-based interpretation is critical. Proprietary models such as Med-PaLM(Singhal et al., 2023) and Med-Gemini (Saab et al., 2024) have shown remarkable performance across a range of multimodal medical tasks, including medical visual question answering (VQA), report generation, summarization. In parallel, open source models such as LLaVA-Med (Li et al., 2023a) have been developed following similar training strategies as LLaVA (Liu et al., 2023), leveraging biomedical datasets from PubMed (NIH, nd) to design instruction prompts and muti-turn conversations.\nAmong medical applications, CXR interpretation remains a key area of interest. Early AI-driven models primarily focus on report generation (Nooralahzadeh et al., 2021; Alfarghaly et al., 2021; Tanida et al., 2023; Chaves et al., 2024), supported by the development of clinically relevant evaluation metrics (Jain et al., 2021; Yu et al., 2023a). More recently, research has expanded toward multimodal, multitask CXR assistants capable of integrating multiple functionalities beyond report generation, such as classification, grounding or image generation. Notable examples include CheXagent (Chen et al., 2024) or RoentGen (Bluethgen et al., 2024), though these models lack conversational capabilities.\nOther approaches, such as Wolf (Kang et al., 2024), RaDialog (Pellegrini et al., 2023), and M4CXR (Chen et al., 2024), incorporate conversational features but are constrained by predefined response templates, limiting their adaptability in real-world interactions. In this work, we introduce a model that integrates multiple CXR interpretation tasks while enabling flexible, multi-turn dialogue, bridging the gap between task-specific AI models and interactive clinical assistants."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Instruction dataset", "content": "A key step in the development of RadVLM is the construction of an instruction dataset. For this purpose, we first aggregate and process multiple publicly available datasets containing CXR images paired with various attributes, including free-text reports, categorical labels, and bounding boxes. From these sources, we generate a dataset of over 1 million instruction instances, each consisting of a frontal CXR image and a corresponding user-assistant interaction derived from the available attributes. These interactions can be in the form of a single Q&A designed for predefined tasks (single instructions) or of a multi-turn exchange (conversations). The composition of this instruction dataset is detailed below and summarized in Table 1."}, {"title": "3.1.1 Free-text report generation", "content": "In alignment with existing CXR models, we aim to generate clinically coherent radiology reports from CXR images. To achieve this, we collect public datasets containing CXRs paired with anonymized free-text reports. Radiology reports often compare to and refer to prior X-ray examinations when discussing current radiological findings. These earlier images should be provided as part of the prompt when conducting report generation based on raw radiology reports (Kim et al., 2023; Bannur et al., 2024). As we here focus on the analysis of a single CXR image, we use GPT40-mini to remove mentions of prior studies, in line with recent work on report generation (Chen et al., 2024; Chaves et al., 2024).\nWe leverage two public datasets for the report generation task:\nMIMIC-CXR (Johnson et al., 2019), which contains 377,110 CXR images paired with free-text radiology reports describing findings and patient history. After filtering, we retain 232,344 image-text pairs in the training set and 3,282 in the test set.\nCheXpert-Plus (Chambon et al., 2024), which features 223,228 image-report pairs from 187,711 studies. Applying the same filtering process as for MIMIC-CXR results in a dataset of 178,368 image-text pairs."}, {"title": "3.1.2 Abnormality classification", "content": "Another essential task an AI-assistant should be capable of is to identify the presence of abnormalities on a CXR. While simpler than generating detailed, unstructured observations, this functionality serves as a quick and helpful overview for physicians, highlighting key observations before they dive into a more detailed analysis. For this task, we collect CXR datasets paired with abnormality labels. These labels were extracted from the original textual reports via the CheXbert automatic labeling tool (Smit et al., 2020), which identifies whether each of 14 possible abnormalities is present, absent, or uncertain. In our setup, we only consider frontal images and-in line with previous work (Yang et al., 2024b; Chaves et al., 2024)-consider \u201cuncertain\" abnormalities as \"absent\".\nFor this task, we use 191,027 image-labels pairs from CheXpert (Irvin et al., 2019) and 237,912 pairs from MIMIC-CXR (Table 1). The instructions are designed such that the user asks for the abnormalities present on the CXR and the assistant answers by providing the list of abnormalities (Figure 1b)."}, {"title": "3.1.3 Visual grounding", "content": "Detecting the location of specific anatomical regions or pathologies on a CXR is an important task for AI assistants. In addition to providing a textual description of the image, they should be able to spot where specific observations are located. This is usually done by predicting bounding box coordinates of top-left and bottom-right corners [x1, Y1, x2, y2]. While classical object detectors (Ren et al., 2016; Redmon, 2016) tackle this task by leveraging specialized architectures and learning rules, we embark on it with the other text-based tasks via next-token prediction (Equation 1), formatting coordinates into text, enclosed between square brackets. As input images are pre-processed to a unique size by the vision encoder, we normalize these coordinates to the original dimension to obtain floating values between 0 and 1, similarly as in You et al. (2023); Park et al. (2024); Zhang et al. (2024a).\n$$p(\\mathcal{X}_{\\alpha} | x_v, x_q) = \\prod_{i=1}^{L} p(x_i | x_v, x_{q,<i}, x_{\\alpha,<i}),$$\nwhere $x_v$ denotes the visual tokens, and $x_{q,<i}$ and $x_{\\alpha,<i}$ represent the question and assistant tokens preceding token $x_i$, respectively. Here, $L$ is the number of tokens in the target assistant sequence. This formulation also applies for the multi-turn conversations, where $x_{q,<i}$ and $x_{\\alpha,<i}$ include the chat history from previous rounds. Following recent trends in visual instruction tuning (Li et al., 2024; Lauren\u00e7on et al., 2024), the whole architecture is trained, using a learning rate of 2e-6 in the vision encoder weights and le-5 in the 2-layer MLP and language model weights. RadVLM is trained over 1 epoch of the instruction dataset using full-fine-tuning. We use 128 GH GPUs, each with 96GB of memory (Fusco et al., 2024) for approximately 12 hours.\nFor the ablation studies, we follow the same fine-tuning procedure but limit the process to a subset of the instruction dataset, focusing on isolated types of tasks."}, {"title": "3.1.4 Conversations", "content": "Fine-tuning a VLM on single instructions, as presented above, is useful to acquire maximal precision in specific tasks but would not be sufficient to build a robust, flexible, and conversational radiology assistant. First, in a real-life setting, we cannot assume that physicians prompt the model with a limited set of instructions. Various types of questions could be posed, such as asking about the characteristics of a specific organ (lungs, heart), the orientation of the X-ray, or the definition of certain medical terms. More importantly, interactions can be decomposed over several Q&A rounds, with sometimes a question referring to previous answers (e.g., asking about the location of a specific observation from the previous answer). The model should thus be tuned to sequentially connect visual concepts (textual observations, presence or absence of abnormalities, fine-grained information) throughout a single conversation thread.\nTo develop this capability in RadVLM, we constructed an instruction-tuning dataset mimicking a real-life multi-turn interaction between user and assistant, named \"conversation dataset\". Here, questions can be asked in different order, and the assistant reacts to the content of previous answers. Inspired by the vision-language models LLaVA (Liu et al., 2023) and LLaVA-Med (Li et al., 2023a), we prompt a larger text-only LLM (GPT-"}, {"title": "3.2 Model finetuning", "content": "We leverage an existing vision-language backbone, LLaVA-OneVision-7B (Li et al., 2024). Its architecture is based on the SigLIP vision encoder (Zhai et al., 2023) connected to the language model qwen-2 (Yang et al., 2024a) via a 2-layer multi-layer perceptron (MLP). It was originally pretrained and instruction-tuned on image-text datasets in the general domain. We also follow the Higher AnyRes strategy (Chai et al., 2022) by encoding multiple patches of the input image in different resolutions (in addition to the full image) and feeding the concatenated output representation from the vision encoder to the language model. Chai et al. (2022) showed that the scaling in image resolution is successful in the general domain.\nWe fine-tune the LLaVA-OneVision-7B architecture on our instruction dataset by optimizing an auto-regressive loss on the target assistant tokens, $x_\\alpha$. Specifically, for each token $x_i$ in the assistant's output sequence, we model\n$$ \\mathcal{L} = \\sum_{i=1}^{L}  p(x_i | x_v, x_{q,<i}, x_{\\alpha,<i}), $$\nwhere $x_v$ denotes the visual tokens, and $x_{q,<i}$ and $x_{\\alpha,<i}$ represent the question and assistant tokens preceding token $x_i$, respectively. Here, $L$ is the number of tokens in the target assistant sequence. This formulation also applies for the multi-turn conversations, where $x_{q,<i}$ and $x_{\\alpha,<i}$ include the chat history from previous rounds. Following recent trends in visual instruction tuning (Li et al., 2024; Lauren\u00e7on et al., 2024), the whole architecture is trained, using a learning rate of 2e-6 in the vision encoder weights and le-5 in the 2-layer MLP and language model weights. RadVLM is trained over 1 epoch of the instruction dataset using full-fine-tuning. We use 128 GH GPUs, each with 96GB of memory (Fusco et al., 2024) for approximately 12 hours.\nFor the ablation studies, we follow the same fine-tuning procedure but limit the process to a subset of the instruction dataset, focusing on isolated types of tasks."}, {"title": "4 Experiments & Results", "content": "In this section, we describe our evaluation pipeline, the existing baseline models we use for comparison, report results and highlight the capabilities of our RadVLM system."}, {"title": "4.1 Evaluation pipeline", "content": "In order to assess the quantitative performance of RadVLM, we design an evaluation pipeline based on the individual tasks from our instruction dataset. This pipeline leverages existing metrics for report generation, abnormality classification and visual grounding and creates novel evaluation tasks to assess the model's performance in conversational abilities."}, {"title": "4.1.1 Report generation", "content": "To evaluate generated reports, we employ both lexical and radiology-specific metrics. Lexical metrics particularly quantify word overlap between generated and ground truth reports, among which we report BertScore, a metric for text generation based on computing token similarity using contextual embeddings (Zhang et al., 2020), and Rouge-L (Lin, 2004), which quantifies the length of the longest common subsequence between predicted and reference reports.\nIn contrast with lexical metrics, radiology-specific metrics ignore irrelevant variations in phrasing and focus on the clinically relevant semantics of the generated text, such as the presence or absence of an abnormality. In particular, we provide results for the RadGraph F1 and GREEN metrics.\nRadGraph F1 (Delbrouck et al., 2022; Yu et al., 2023a): computing this metric requires to map each generated and ground truth reports to a structured graph, named RadGraph (Jain et al., 2021), containing radiology-specific entities (anatomy or observations) and the relations between them (\"suggestive of\", \"located at\"). The RadGraph F1 score (Delbrouck et al., 2022) computes the overlap in inferred structured graphs (extracted entities and relations) from both generated and ground truth reports. In our study, we use the recently released RadGraph-XL model to extract graphical representations (Delbrouck et al., 2024) and report the partial reward (described in Delbrouck et al., 2022).\nGREEN (Generative Radiology Report Evaluation and Error Notation, Ostmeier et al., 2024): a recently developed report generation metric that leverages the LLM-as-Judge mechanism of language models (Zheng et al., 2023b) to identify clinically significant errors in generated radiology reports (Yu et al., 2023b; Calamida et al., 2023, 2024), and that highly aligns with expert preferences as compared to other LLM-based evaluations (e.g., GPT-4)."}, {"title": "4.1.2 Abnormality classification", "content": "We use the test split of the CheXpert dataset and prompt the model by asking to list the abnormalities present on the CXR. The mentioned abnormalities in the model's answer are extracted via key-word matching and compared to the ground truth list. We calculate the F1 score for the 14 abnormalities and report the macro-averaged F1-score over all categories."}, {"title": "4.1.3 Visual grounding", "content": "We assess the model's performance in visual grounding by prompting the model to detect specific features (anatomy, abnormality, phrase) and extract the bounding box coordinates generated within the model's answer. Although we included the abnormality detection task in the instruction set, we do not evaluate it here due to the lack of comparable models for this task.\nFor the three grounding tasks, we use mean Average Precision (mAP) at an Intersection Over Union (IoU) threshold of 0.5. At this setting, we evaluate both precision and recall for predicted boxes that overlap with ground truth by at least 50%."}, {"title": "4.1.4 Multi-turn evaluation within conversational interactions", "content": "Evaluating conversational aspects of a model is essential to assess the utility and performance of an AI assistant in a multi-turn setting. We designed an LLM-based evaluation method carefully crafted for conversations, following the \"LLM-as-judge\" scheme (Zheng et al., 2023b) previously adopted by LLaVA-Med (Li et al., 2023a). We created a test set of conversations following our generation process (see Section 3.1.4). For this test set, we used GPT-40 instead of GPT-40-mini (which was used for the training set) to ensure a higher quality of generated conversations, ensuring that the computed performance metrics accurately reflect the VLM's capabilities rather than limitations in the test set. As as result, we obtained 157 conversations containing grounding questions (derived from MS-CXR test set), and 523 conversations without (Table 1)."}, {"title": "4.2 Baseline models", "content": ""}, {"title": "4.3 Results", "content": ""}, {"title": "4.3.1 Report generation", "content": "Firstly, we evaluate RadVLM on a core radiology task: generating a textual report from a frontal CXR. As our evaluation set, we use a filtered version of the MIMIC-CXR test set, excluding statements about findings from prior examinations. We report in Table 3 both lexical (BertScore, Rouge-L) and clinical (RadGraph F1, GREEN) metrics, which capture different performance aspects of the report generation task.\nAs mentioned in Section 4.2, five additional VLMs - including three specific to CXR (RaDialog, CheXagent, MAIRA-2) - are also evaluated, with minor adaptations to each model's recommended prompt template. Many of these models were originally evaluated under different conditions (sometimes benefiting from extra inputs like prior images/reports or patient details or omitting certain metrics such as GREEN). To ensure that our setup remains consistent across all of them, we apply the same evaluation pipeline (test set and metrics).\nFor non-CXR specific VLMs, we observe a poor performance in both lexical and clinical metrics, with an unexpected improved performance for the generalist model (LLaVA-OneVision) over the medical one (LLAVA-Med), presumably profiting from a better architecture and training process (Li et al., 2024). CXR-specific models perform significantly better than generalists, with a notable advantage of CheXagent in terms of clinical metrics. While MAIRA-2 was shown to perform optimally in the report generation task (Bannur et al., 2024), it seems to perform worse when a single image is provided. Overall, RadVLM achieves competitive results, attaining the highest performance in lexical metrics and the second-best in clinical metrics. This validates our approach, even though our training methodology was not specifically designed for the report generation task."}, {"title": "4.3.2 Abnormality classification", "content": "In this section, we assess each model's ability to predict which abnormalities are visible on the CXR. We use the manually curated CheXpert test set, prompt RadVLM to list any observed abnormalities, and compare them against the ground truth. As for the report generation task, we adapt the prompt for each compared"}, {"title": "4.3.3 Visual grounding", "content": "In this section, we evaluate RadVLM's visual grounding capabilities, which could help clinicians localize specific regions or pathologies on a CXR. This is particularly useful once a pathology has already been identified either by a radiologist's input or through our previously described AI tasks since it allows one to pinpoint exactly where the abnormality appears on the image."}, {"title": "4.3.4 Conversational abilities", "content": "Our model can already handle multiple advanced tasks for CXR interpretation. However, we have only tested its ability to follow instructions in a single-turn, question-answer format so far. An essential aspect of a VLM's performance is also its capacity to manage follow-up questions in a multi-turn exchange with the user. Simply generating a full report or marking image features in one shot does not necessarily imply that the model can handle a varied sequence of precise questions - covering observations, locations, clarifications of medical terms, and more.\nRadVLM's instruction dataset also includes image-conversation pairs (Figure 2) generated with GPT-40-mini, by providing it with a radiology report, any available grounded phrases, and additional details. As mentioned in Section 4.1.4, we created two held-out evaluation sets using GPT-40: one without grounded questions and one focused on grounded questions derived from the MS-CXR test set (Table 1). We run the evaluation on both test sets and report the average score across all samples. We also evaluate the other baseline models that possess conversational abilities, thoroughly respecting their prompting template within a multi-turn setting. The average GPT-40 scores for RadVLM and other models are gathered in Table 5.\nOur results show that RadVLM, trained on a broad range of image-conversation pairs, achieves an average score of 6.95/10, substantially higher than the other conversational models, whose responses often prove incorrect or vague (Figure 6). This advantage may stem from RadVLM's ability to handle varied question types and sequences in multi-turn exchanges, a skill reinforced by the sparse nature of its training conversations. The gap becomes even more pronounced in grounded scenarios, where RadVLM maintains strong performance (6.55/10) while others drop even more. Notably, this suggests that even the limited number of grounded conversations included (Table 1) was sufficient to equip RadVLM with robust grounding capabilities in a multi-turn setting."}, {"title": "4.4 Ablation studies", "content": "In this section, we investigate a fundamental question in LLM fine-tuning: do models perform individual tasks more effectively when they are trained separately, or does joint training on multiple tasks provide a greater overall benefit? To address this question, we perform ablation studies where we train separate models - each dedicated to a single task (e.g., report generation, classification) \u2013 using the same training settings and hyperparameters as RadVLM, then evaluate their performance on that respective task.\nOur results reveal that RadVLM generally outperforms models trained on individual tasks. First, this difference is pronounced in grounding performance, particularly for tasks with smaller training sets (e.g., phrase grounding; see Table 1). We attribute this to the complementary nature of the various grounding tasks, which collectively improve the model's ability to localize visual elements (including overlapping categories, such as \"abnormalities\" also mentioned in \u201cphrases\") and prevent overfitting.\nA similar trend emerges in the conversation scores, where RadVLM outperforms a model trained solely on the conversation dataset. While this ablated model still performs significantly better than its baseline backbone surpassing LLaVA-OneVision's score in Table 2 - RadVLM leverages knowledge from the other tasks, such as report generation, that contains a higher amount of data points. This presumably enables it to generalize more effectively to new questions from the conversation test set.\nReport generation is the only task that turns out equivalent when trained in isolation. One likely reason is the high amount of training data for this task, and the richness of information textual reports contain over other tasks, which might suffice for the model to generalize over new instances."}, {"title": "5 Conclusions", "content": "There is a growing need for medical AI tools that are both accurate and user-friendly, and can interact with clinicians in a natural, multi-turn conversation. In this study, inspired by advances in both general and CXR-specific vision-language models, we explored the potential of constructing an assistant that not only aims for strong performance in individual tasks but also for handling them in a multi-turn, interactive setting. To achieve this, we built an instruction dataset that integrates both stand-alone tasks and multi-task dialogues, then fine-tuned a VLM backbone on it. Our findings suggest that this method performs well and offers promising insights for designing future radiology assistants.\nOne key contribution of this study is our consistent re-implementation and evaluation of existing models (alongside RadVLM) under a unified experimental framework. In an era where AI research is proliferating rapidly, and new models are introduced almost daily, it can be challenging to compare results that rely on different setups. By systematically re-implementing models and replicating experiments within the same context, we enhance both reproducibility and reliability. This standardized approach aligns with initiatives such as the ReXrank leaderboard for CXR report generation (Zhang et al., 2024b), which similarly evaluates state-of-the-art models using a consistent protocol. Ultimately, these efforts provide a clearer understanding of each model's true capabilities and help researchers identify the most promising directions for future work.\nAnother key takeaway is the flexibility of VLMs in handling diverse data types, ranging from free-text reports and classification labels to fine-grained information, all within a multi-turn, \"real-life\" scenario. Traditionally, each task might have been tackled by distinct, specialized architectures - e.g., separate object detectors (Sun and Cong, 2022) or classifiers (He et al., 2016) \u2013 but our findings support the recent trend, employed in both general and CXR-specific visual modeling (Lin et al., 2024; Wang et al., 2024; Sharma et al., 2024; Chen et al., 2024), where many data formats can now be tokenized and processed through a common next-token generation paradigm. Our ablation studies further show that while each task differs in purpose, they collectively enhance the model's overall understanding of a CXR image, an effect likely amplified by training them together. Even though multi-agent approaches emerge in the medical setting (Schmidgall et al., 2024), these results support the value of joint training for developing comprehensive, single-agent solutions.\nThe simplicity of RadVLM's training strategy also sheds light on potential directions for future CXR-based modeling. Here, we built upon LLaVA-OneVision (Li et al., 2024), a VLM already pretrained for instruction following, and fine-tuned it on our curated dataset for just a single epoch. Although other studies have employed more intricate pipelines, such as pre-training the vision encoder, adapter and LLM components successively (Chaves et al., 2024; Chen et al., 2024; Bannur et al., 2024), Given the competitive performance of RadVLM with other models, we hypothesize that the quality of the instruction dataset, coupled with the backbone architecture (as shown by Lauren\u00e7on et al., 2024) are the most critical for the overall performance of such a visual foundation model. That said, for specific tasks like report generation, specialized solutions that incorporate additional patient details, prior and lateral images, or extended training, could offer greater gains. This likely explains why our model does not surpass the state of the art for that particular task. From this, we envision a future where AI assistants are bifurcated into (i) non-interactive, report-focused tools designed for experienced radiologists, and (ii) more adaptable, conversational agents that clinicians or radiology students can easily query in a multi-turn context.\nAs radiology datasets grow in both scale and quality, and as vision-language modeling advances at a rapid"}, {"title": "A Appendix", "content": "I will provide you with some visual information about a chest X-ray (information that you\ncannot see) and ask you to create a 5-10 round Q&A conversation between a user and an AI\nassistant. The assistant"}]}