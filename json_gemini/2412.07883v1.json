{"title": "On Faster Marginalization with Squared Circuits via Orthonormalization", "authors": ["Lorenzo Loconte", "Antonio Vergari"], "abstract": "Squared tensor networks (TNs) and their generalization as parameterized computational graphs - squared circuits \u2013 have been recently used as expressive distribution estimators in high dimensions. However, the squaring operation introduces additional complexity when marginalizing variables or computing the partition function, which hinders their usage in machine learning applications. Canonical forms of popular TNs are parameterized via unitary matrices as to simplify the computation of particular marginals, but cannot be mapped to general circuits since these might not correspond to a known TN. Inspired by TN canonical forms, we show how to parameterize squared circuits to ensure they encode already normalized distributions. We then use this parameterization to devise an algorithm to compute any marginal of squared circuits that is more efficient than a previously known one. We conclude by formally showing the proposed parameterization comes with no expressiveness loss for many circuit classes.", "sections": [{"title": "1 Introduction", "content": "Tensor networks (TNs) are low-rank tensor factorizations often used to compactly represent high-dimensional probability distributions, both in quantum physics (Or\u00fas 2013; Biamonte and Bergholm 2017) and in ML (Stoudenmire and Schwab 2016; Glasser, Pancotti, and Cirac 2018; Cheng et al. 2019; Glasser et al. 2019; Novikov, Panov, and Oseledets 2021). A TN factorizing a complex function $\\psi$ over a set of variables $X = \\{X_i\\}_{i=1}^n$ having domain dom(X) can be used to represent a probability distribution via modulus squaring, i.e., $p(X) = Z^{-1}|\\psi(X)|^2 = Z^{-1}\\psi(X)\\psi(X)^*$, where $( )^*$ denotes the complex conjugation, and $Z = \\int_{\\text{dom}(X)} |\\psi(x)|^2dx$ is the partition function.\nRecently, Loconte et al. (2023, 2024b) showed that TNs can be generalized into deep computational graphs called circuits (Choi, Vergari, and Van den Broeck 2020). This is done by casting tensor contraction operations into layers of sums and products whose feed-forward evaluation corresponds to a complete contraction to evaluate $\\psi$. The language of circuits offers the opportunity to flexibly build novel TN structures by just stacking layers of sums and products as \"Lego blocks\" (Loconte et al. 2024b); include different basis input functions, and offering a seamless integration with deep learning (Shao et al. 2022; Gala et al. 2024a,b). Furthermore, casting TNs as circuits provides conditions as to compose them and compute many probabilistic reasoning tasks in closed-form, such as expectations and information-theoretic measures (Vergari et al. 2021), which is crucial, e.g., for reliable neuro-symbolic AI (Ahmed et al. 2022; Zhang et al. 2023). This is done with probabilistic circuits (PCs), circuits encoding probability distributions, that are classically restricted to have positive parameters only \u2013 also called monotonic PCs (Shpilka and Yehudayoff 2010).\nOne can increase the expressiveness of PCs by equipping them with complex parameters, squaring them (Loconte et al. 2024a), similarly to TNs, or even more by mixing squared PCs (Loconte, Mengel, and Vergari 2024). Differently from classical monotonic PCs, which are not squared, the squared PCs require additional computation to be normalized, i.e., to compute $Z$, which is quadratic in the circuit size. This computational overhead carries over to computing marginals and hinders their application in a number of tasks such as lossless compression (Liu, Mandt, and Van den Broeck 2022) and sampling (Loconte et al. 2024a), where performing fast marginalization is crucial.\nIn this paper, we show that the solution to this inefficiency of squared PCs comes from the literature of TNs, where canonical forms are adopted in order to simplify the computation of probabilities (Schollwoeck 2010). For instance, instead of computing $Z$ explicitly in the case of a matrix-product state (MPS) TN (P\u00e9rez-Garc\u00eda et al. 2007), a canonical form ensures $|\\psi(X)|^2$ is an already-normalized probability distribution, i.e., $Z = 1$. In practice, canonical forms are obtained by parameterizing a TN using unitary matrices, i.e., matrices $A \\in \\mathbb{C}^{n\\times n}$ satisfying $A^{\\dagger}A = AA^{\\dagger} = I_n$, where $I_n$ denotes the identity matrix of size $n$. For rectangular matrices, semi-unitary matrices $A \\in \\mathbb{C}^{m\\times n}$ are considered, i.e., matrices satisfying $AA^{\\dagger} = I_n$ if $m > n$ or $AA^{\\dagger} = I_m$ if $m < n$. Under this view, computing $Z$ simplifies into operations over identity matrices. However, computing different marginals over different TN structures requires a different canonical form and these cannot be immediately translated to squared PCs, because the latter allows us to build factorizations that might not correspond to any known TN. This begs the question: how can we parameterize squared circuits as to be already normalized and allow us to accelerate the computation of any marginal? We answer it in the following.\nContributions. We derive sufficient conditions via orthonormal parameterizations to ensure that squared PCs are already normalized (Section 3). These conditions are based on semi-unitary matrices, similarly to canonical forms in TNs, but defined within the language of tensorized circuits instead. Then, by leveraging squared orthonormal PCs, we present a general algorithm to compute any marginal that can be much more efficient than the previously known algorithm for squared PCs which required a quadratic increase in complexity instead (Section 4). Our algorithm, which exploits the concept of variable dependencies of the circuit layers, can be used to speed up the computation or arbitrary marginals for TNs as well. Finally, we show how the proposed parameterization can be enforced efficiently in squared PCs, thus theoretically guaranteeing no loss of expressiveness for certain circuit families (Section 5)."}, {"title": "2 Circuits and Squared Circuits", "content": "We start by defining circuits in a tensorized formalism (Vergari, Di Mauro, and Esposito 2019; Loconte et al. 2024b).\nDefinition 1 (Tensorized circuit). A tensorized circuit $c$ is a parameterized computational graph encoding a function $c(X)$ and comprising of three kinds of layers: input, product and sum. A layer $l$ is a vector-valued function defined over variables $sc(l)$, called scope, and every non-input layer receives the outputs of other layers as input, denoted as $in(l)$. The scope of each non-input layer is the union of the scope of its inputs. The three kinds of layers are defined as follows:\n\u2022 Each input layer $l$ has scope $X \\in \\mathcal{X}$ and computes a collection of $K$ input functions $\\{f_i: \\text{dom}(X) \\rightarrow \\mathbb{C}\\}_{i=1}^K$, i.e., $l$ outputs a $K$-dimensional vector.\n\u2022 Each product layer $l$ computes either an element-wise (or Hadamard) or Kronecker product of its $N$ inputs, i.e., $\\bigodot_{i=1}^N l_i(sc(l_i))$ or $\\bigotimes_{i=1}^N l_i(sc(l_i))$, respectively.\n\u2022 A sum layer $l$ with $l_1$ as input, is parameterized by $W\\in \\mathbb{C}^{K_1 \\times K_2}$ and computes the matrix-vector product $l(sc(l)) = W l_1(sc(l_1))$.\nEach non-input layer is a vector-valued function, but each entry it computes is a scalar-valued function computed over certain entries of its input vectors. We denote as $size(l)$ the number of scalar inputs to each scalar function encoded in $l$. That is, an Hadamard layer consists of $K$ scalar functions each computing the product of $N$ other scalars, thus $size(l) = NK$. A Kronecker layer consists of $K^N$ scalar functions each computing the product of $N$ other scalars, i.e., $size(l) = N K^N$. Finally, a sum layer consists of $K_1$ scalar functions each receiving input from $K_2$ other scalars and computing a linear combination, i.e., $size(l) = K_1 K_2$. The size of a layer is also its computational complexity.\nA tensorized PC is a tensorized circuit $c$ computing non-negative values, i.e., for any $x$ we have $c(x) \\geq 0$. Thus, a PC $c$ encodes a probability distribution $p(x) = Z^{-1}c(x)$. A PC $c$ supports tractable marginalization of any variable subset (Choi, Vergari, and Van den Broeck 2020) if (i) the functions encoded by the input layers can be integrated efficiently and (ii) it is smooth and decomposable."}, {"title": "3 Squared Orthonormal Circuits", "content": "Our representation of squared PCs is based on the definition of orthonormal circuits we introduce below.\nDefinition 3 (Orthonormal circuits). A tensorized circuit $c$ over variables $\\mathcal{X}$ is said orthonormal if (1) each input layer $l$ over $X \\in \\mathcal{X}$ encodes a vector of $K$ orthonormal functions, i.e., $l(X) = [f_1(X), ..., f_K(X)]^\\top$ such that $\\forall i, j \\in [K]^2$: $\\int_{\\text{dom}(x)} f_i(x)f_j(x)^* dx = \\delta_{ij}$, where $\\delta_{ij}$ denotes the Kronecker delta; and (2) each sum layer is parameterized by a (semi-)unitary matrix $W \\in \\mathbb{C}^{K_1 \\times K_2}$, $K_1 < K_2$, i.e., $WW^\\dagger = I_{K_1}$, or the rows of $W$ are orthonormal.\nIf we take a tensorized circuit $c$ that is orthonormal, then the squared PC obtained from $c$ is guaranteed to encode a normalized probability distribution, as formalized below.\nProposition 1. Let $c$ be a structured-decomposable tensorized circuit over variables $\\mathcal{X}$. If $c$ is orthonormal, then its squaring encodes a normalized distribution, i.e., $Z = 1$.\nAppendix B.1 shows our proof. The idea is that integrating products of input layers encoding orthonormal functions yields identity matrices in $c^2$. Then, the (semi-)unitarity of parameter matrices in sum layers is used to show the output of each layer in $c^2$ is (the flattening of) an identity matrix, thus eventually yielding $Z = 1$.\nThe computation of the partition function $Z$ represents a special case of marginalization, where all variables are marginalized out. In general, computing any marginal probabilities in squared PCs requires worst-case time $O(LS^2)$ (Loconte et al. 2024a). In the next section, we show how to exploit the properties of orthonormal circuits (Definition 3) to also provide an algorithm that computes any marginal probability with a better complexity."}, {"title": "4 A Tighter Marginalization Complexity", "content": "The idea of our algorithm is that, when computing marginal probabilities using $c^2$, we do not need to evaluate the layers whose scope depends on only the variables being integrated out. This is because they would always result in identity matrices, as noticed in our proof for Proposition 1.\nIn addition, we observe that we do not need to square the whole tensorized circuit $c$, but only a fraction of the layers depending on both the marginalized variables and the ones being kept. By doing so, a part of the complexity will depend on $S$ rather than $S^2$. We formalize our result below.\nTheorem 1. Let $c$ be a structured-decomposable orthonormal circuit over variables $\\mathcal{X}$. Let $Z \\subseteq \\mathcal{X}$, $Y = \\mathcal{X} \\setminus Z$. Computing the marginal likelihood $p(y) = \\int_{\\text{dom}(Z)} |c(y, z)|^2 dz$ requires time $O(|\\phi_y|S + |\\phi_{y,z}|S^2)$, where $\\phi_y$ (resp. $\\phi_{y,z}$) denotes the set of layers in $c$ whose scope depends on only variables in $Y$ (resp. on variables both in $Y$ and in $Z$).\nWe prove it in Appendix B.2 and show our algorithm in Algorithm B.1. Note that the complexity shown in Theorem 1 is independent on the number of layers whose scope depend on $Z$ only, i.e., $\\phi_z$. Depending on the circuit structure and the variables $Z$, Algorithm B.1 can be much more efficient than $O(LS^2)$. For example, the tree structure of a circuit defined over pixel variables can be built by recursively splitting an image into patches with horizontal and vertical cuts (Loconte et al. 2024b). If $Z$ consists of only the pixel variables of the left-hand side of an image (i.e., we are computing the marginal of the right-hand side $Y$), then $|\\phi_{y,z}| << L$ since only a few layers near the root will depend on both $Y$ and $Z$. We illustrate an example in Fig. 1."}, {"title": "5 Are Orthonormal Circuits less Expressive?", "content": "Orthonormal tensorized circuits restrict their input layers to encode orthonormal functions, and require parameter matrices to be (semi-)unitary (Definition 3), thus arising the question whether these conditions make them less expressive when compared to non-orthonormal ones. Below, we start by analyzing which input layer functions we can encode in terms of orthonormal functions.\nAre orthonormal functions restrictive? Depending on whether a variable is discrete or continuous, we have different ways to model it with orthonormal functions. For a discrete variable $X$ with finite domain $\\text{dom}(X) = [u]$, any function $f(X)$ can be expressed as $f(x) = \\sum_{k=1}^{u} f(k) \\delta_{xk}$, i.e., $f$ can be written in terms of $u$ basis functions $\\{\\delta_{xk}\\}_{k=1}^{u}$ that are orthonormal. That is, we have that $\\sum_{x \\in \\text{dom}(X)} \\delta_{xk} \\delta_{xk'} = \\delta_{kk'}$ for $k, k' \\in [u]$. Therefore, any input layer over a finitely-discrete variable $X$ can be exactly encoded with a sum layer having a layer encoding the orthonormal basis $\\{\\delta_{xk}\\}_{k=1}^{u}$ as input.\nIn the case of a continuous variable $X$, many classes of functions can be expressed in terms of orthonormal basis functions. For instance, periodic functions can be represented by Fourier series of sines and cosines basis that form an orthonormal set of functions (Jackson 1941). Under certain continuity conditions, functions can be approximated arbitrarily well by finite Fourier partial sums (Jackson 1982). Moreover, depending on the support of $X$, many classes of functions can be described in terms of families of orthonormal polynomials, such as Legendre, Laguerre and Hermite polynomials (Abramowitz, Stegun, and Miller 1965). Notably, Hermite functions generalize Gaussians and are a basis of square-integrable functions over all $\\mathbb{R}$ (Roman 1984).\nAre (semi-)unitary matrices restrictive? Next, we investigate whether the requirement of sum layer parameter matrices to be (semi-)unitary may reduce the expressiveness of squared PCs. In the following, we answer to this question negatively, as we can enforce this condition in polynomial time w.r.t the number of layers and the layer size.\nTheorem 2. Let $c$ be a tensorized circuit over variables $\\mathcal{X}$. Assume that each input layer in $c$ encodes a set of orthonormal functions. Then, there exists an algorithm returning an orthonormal circuit $c'$ in polynomial time such that $c'$ is equivalent to $c$ up to a multiplicative constant, i.e., $c'(X) = Z^{-\\frac{1}{2}}c(X)$ where $Z = \\int_{\\text{dom}(x)} |c(x)|^2dx$.\nAppendix B.3 presents our proof, and Algorithm 1 shows our algorithm to \u201corthonormalize\u201d a tensorized circuit. The idea of Algorithm 1 is that we can recursively make sub-circuits orthonormal by (i) factorizing the sum layer parameter matrices via QR decompositions, and (ii) by \u201cpushing\u201d the non-unitary part of such a decomposition towards the output, until the reciprocal square root of the partition function of $c^2$ is retrieved at the top level of the recursion. Fig. B.1 illustrates the algorithm. Therefore, Theorem 2 guarantees that squared orthonormal PCs are as expressive as general squared PCs with orthonormal input functions.\nFinally, we note that Algorithm 1 is the dual of a result about monotonic PCs shown by Peharz et al. (2015): they show an algorithm that updates the positive weights of a smooth and decomposable PC such that the distribution it encodes is already normalized. Here, we show an algorithm that updates the (possibly) complex weights of a structured-decomposable circuit such that its squaring encodes an already-normalized probability distribution."}, {"title": "6 Related Work and Conclusion", "content": "In this paper, we presented a parameterization of squared PCs inspired by canonical forms in TNs, based on orthonormal functions and (semi-)unitary matrices, as to speed-up the computation of marginal probabilities. As squared orthonormal PCs support faster marginalization, they are amenable for future works on applications where computing marginals is key, e.g., lossless compression (Yang, Mandt, and Theis 2022; Liu, Mandt, and Van den Broeck 2022).\nOrthonormal basis functions have been used to parameterize shallow squared mixtures encoding already-normalized distributions, both in signal processing (Pinheiro and Vidakovic 1997) and in score-based variational inference (Cai et al. 2024). Our squared orthonormal PCs generalize them, as they represent deeper squared mixtures.\nWe plan to investigate different methods as to learn squared orthonormal PCs from data for distribution estimation, and compare how do they perform w.r.t. squared PCs with unconstrained parameters. For instance, there are many ways of parameterizing unitary matrices, with different advantages regarding efficiency, numerical stability, and optimization (Arjovsky, Shah, and Bengio 2015; Huang et al. 2017; Bansal, Chen, and Wang 2018; Casado and Mart\u00ednez-Rubio 2019). Moreover, Hauru, Damme, and Haegeman (2020); Luchnikov et al. (2021) proposed optimizing the parameters of MPS TNs and quantum gates by performing gradient descent over the Stiefel manifold (Absil, Mahony, and Sepulchre 2007). Furthermore, recent works parameterize more expressive monotonic PCs using neural networks (Shao et al. 2022; Gala et al. 2024a,b), thus motivating parameterizing squared orthonormal PCs similarly."}]}