{"title": "Image Embedding Sampling Method for Diverse Captioning", "authors": ["Sania Waheed", "Na Min An"], "abstract": "Image Captioning for state-of-the-art VLMs has significantly improved over time; however, this comes at the cost of increased computational complexity, making them less accessible for resource-constrained applications such as mobile devices and assistive technologies. Alternatively, smaller VLMs prioritize high-level scene descriptions, overlooking finer details that contribute to a richer understanding of an image. In this paper, we introduce a training-free framework that enhances caption diversity and informativeness by explicitly attending to distinct image regions using a comparably small VLM, BLIP, as the backbone. Our approach leverages structured segmentation to produce hierarchical representations that capture both global and localized semantics. Without requiring additional model training, we demonstrate that our method allows smaller VLMs to achieve performance comparable to larger models in terms of image-caption alignment, semantic integrity, and diversity. We evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets, achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset respectively, while maintaining strong image-caption relevancy and semantic integrity with the human-annotated captions.", "sections": [{"title": "1 Introduction", "content": "Visual-Language Models (VLMs) have seen rapid advancements in image captioning, benefiting from increasingly sophisticated architectures and larger training datasets (Alayrac et al., 2022; Li et al., 2022b; Radford et al., 2021a; Wang et al., 2022a). State-of-the-art large-scale models generate highly detailed and diverse captions, yet their extensive computational requirements can be prohibitive in resource-constrained settings. Conversely, smaller VLMs, while more efficient, often prioritize dominant visual elements and overlook fine-grained details, resulting in captions that lack the depth and specificity seen in human-generated captions (Aneja et al., 2019a; Bianco et al., 2023; Chen et al., 2023; Yuksekgonul et al., 2022).\nInspired by previous work (Ji et al., 2021; Shao et al., 2023; Shukor et al., 2022) that demonstrates the advantages of hierarchical approaches in image understanding, our method leverages structured segmentation to capture both global and regional aspects of an image. We sample segmentation-driven embeddings to explicitly attend to distinct image regions while preserving contextual relationships, generating captions at multiple levels of granularity. This approach enables smaller VLMs to achieve performance comparable to larger models in terms of caption diversity and image-caption alignment.\nWe validate our approach, namely, HBoP Hierarchical Bags of Phrases, by evaluating generated captions for MSCOCO, Flickr30k, and Nocaps datasets on conventional diversity metrics such as mBLEU-4, n-gram diversity (Aneja et al., 2019b), and newly presented pairwise cosine distance (PCD). Our findings show that structured caption generation effectively improves diversity while maintaining relevancy with images and human-generated captions (compare BLIP (Li et al., 2022a), HBoP, and gold captions in Figure 1)."}, {"title": "2 Related Works", "content": "Vision-language models have demonstrated remarkable capabilities in various multi-modal tasks, with caption generation being a primary benchmark for evaluating their performance. Models such as CLIP (Radford et al., 2021a), Flamingo (Alayrac et al., 2022), and BLIP-2 (Li et al., 2023) leverage contrastive learning and large-scale pre-training to improve vision-language alignment. Although these approaches improve caption fluency and coherence, they often generate high-level scene descriptions without capturing fine-grained details, limiting their utility for applications requiring detailed image understanding. Traditional captioning models treat images holistically, often overlooking fine-grained hierarchical details (Xu et al., 2021), unless trained with an explicit training objective of improving diversity, such as ModeCap (Chen et al., 2022) which explores various modes in the training corpus and Seq-CVAE (Aneja et al., 2019b) that uses latent variables to generate every word within a sentence.\nInspired by hierarchical representation techniques (Ji et al., 2021; Shao et al., 2023; Shukor et al., 2022), our approach samples the latent image embeddings corresponding to structured segmentation to generate captions that reflect multiple levels of contextual details. PnP-VQA (Tiong et al., 2022) proposes a similar methodology but focuses primarily on visual question answering. While PnP-VQA leverages image segmentation to generate captions, its sampling approach results in less meaningful captions due to its reliance on high-activation regions extracted using Grad-CAMS (Selvaraju et al., 2017)."}, {"title": "3 Methodology", "content": "In this section, we introduce our proposed framework, HBoP (depicted in Fig 2), a modular architecture that uses pre-trained segmentation and captioning models. We show that HBoP ensures multiple levels of captions (i.e., global, regional, fine-grained) by inducing a hierarchical structure for image understanding."}, {"title": "3.1 Image Segmentation Module (ISM)", "content": "The first component of HBOP, ISM, selects patch embeddings (Ex) corresponding to image regions (X = (X1, X2, ..., Xn)) from the original image embeddings extracted using a Vision Transformer (ViT) (Dosovitskiy et al., 2020) encoder. These regions are selected based on the segmentation masks produced by the state-of-the-art Segment Anything model (SAM) (Kirillov et al., 2023). If we select a set of p segmentation masks for the image, the resulting masks for the selected image regions would be: Mx = { MX1, MX2, ..., MXp } = SAM(X), X \u2208 \\mathbb{R}^{H\u00d7W\u00d7C}, where H, W, and C represent the height, width, and channels of X."}, {"title": "3.2 Hierarchical Composition Module (HCM)", "content": "The second component, HCM, is a key component that can control the level of captions. Specifically, we present three types of captions that can be derived using HCM."}, {"title": "Global/Fine-grained level captions", "content": "The global segmentation masks (MG) are selected by choosing the top-k (5 in our case) largest segmentation masks from Mx after applying non-maximum suppression (NMS) (Hosang et al., 2017):\nMG = {Mg1, Mg2, \u2026, Mng },\nMg\u2081 = NMS(Top-k(Mx)), i = 1, ..., ng\nNMS removes multiple segmentation masks with overlapping, similar contexts using the Intersection over Union (IoU) and predicted confidence from SAM. The remaining masks, after applying NMS, can also be used to generate fine-grained captions (discussed in Appendix D.3):\nMF = {Mf1, Mf2,..., Mnf},\nMf\u2081 = NMS(Mx) \\ MG, i = 1, ..., n f"}, {"title": "Regional level captions", "content": "To create regional-level segmentation masks, MR, we use K-means clustering to partition all the segmentation masks (Mx) and apply NMS to each cluster individually:\nMR = {Mr1, Mr2, ..., MK},\nMr\u2081 = NMS(K-means(Mx)), i = 1, ..., K\nThe hierarchical segmentation masks (Mg, Mr and Mf) are used to extract relevant patch embeddings, Eg, Er and Ef using Ex from the first stage. We extract (0) the corresponding embeddings by concatenating the extracted patch embeddings of different levels. Thus, the final selected image embeddings can be categorized as:\nEG = { Eg1, Eg2, ..., Egng }, Egi = Ex \u00a9 Mgi\nER = { Er1, Er2, ..., EK }, Er\u2081 = Ex \u00a9 Mr;\nEF = { Ef1, Ef2, ..., Enf }, Ef\u2081 = Ex \u00a9 Mfi"}, {"title": "3.3 Image Captioning Module (ICM)", "content": "To generate captions for different levels of image embeddings, we use BLIP fine-tuned on image captioning (Li et al., 2022a) with the stochastic sampling method, following the same procedure as (Tiong et al., 2022). The caption generation process is repeated for ng, nr, and nf patch embeddings corresponding to the number of selected hierarchical masks. Since the patch embedding size may vary due to the different mask sizes, we use zero padding before using the captioning module. Our final HBOP captions would be:\nHBOPG = { $91, ..., $ng }, Sg\u2081 = BLIP(Eg\u2081)\nHBOPR = { $r1, ..., SK }, Sr\u2081 = BLIP(Er\u2081)\nHB0PF = { $f1, ..., Snf }, 8f\u2081 = BLIP(Ef\u2081)"}, {"title": "4 Results", "content": "HBOP achieves the best diversity scores while maintaining relevance among smaller VLMs. We compare the diversity and relevance of captions generated by different models, as shown in Table 1. While larger VLMs with LLM-based text encoders generally achieve stronger overall performance, HBoP remains competitive, achieving the closest diversity scores to the gold captions among smaller VLMs in terms of PCD (see Appendix C for details), mBLEU-4 and Div-2 (Aneja et al., 2019b).\nSpecifically, compared to captions generated by BLIP (Li et al., 2022a) and BLIP-2 (Li et al., 2023), HBOP achieves a PCD score closest to that of the gold captions, along with the lowest mBLEU-4 and highest Div-2 scores among BLIP (both with and without nucleus sampling (NS) (Li et al., 2022a)), Seq-CVAE (Aneja et al., 2019b), and ModeCAP\u00b9(Chen et al., 2022). Notably, in some cases, HBOP also surpasses larger VLMs with LLM-based encoders, such as BLIP-2 (Li et al., 2023), Honeybee-7B (Cha et al., 2023), and LLaVA-1.5 (Liu et al., 2023a), in diversity metrics, as indicated by lower mBLEU-4 and higher Div-2 scores (notice red mBLEU-4 and blue Div-2, marking stronger diversity performance for HBoP).\nAt the same time, HBoP maintains strong similarity between generated captions and reference texts, as measured by SBERT (Reimers and Gurevych, 2019), and preserves high image-text alignment, as indicated by CLIP-Score (Hessel et al., 2021). Notably, HBoP achieves SBERT and CLIP-Score values that are comparable to BLIP, BLIP-NS, and LLaVA, while outperforming HoneyBee in both metrics. Although BLIP-2 scores the highest, HBoP remains competitive, demonstrating a strong balance between relevance and diversity.\nHBOP generates semantically meaningful captions. We evaluate the semantic integrity of HBOP captions against those generated by other models, using LLama-2-13b (Touvron et al., 2023) and GPT-4 (Fu et al., 2023). As shown in Table 2, HBOP achieves semantic integrity scores close to the gold captions. Notably, HBoP outperforms models like PnP-VQA in this metric. We attribute this surpass to our method, which samples more meaningful image embeddings using the proposed HCM component."}, {"title": "5 Conclusion", "content": "We propose HBoP, a hierarchical caption generation framework that leverages a modular architecture combining lightweight pre-trained VLMs and segmentation models to generate semantically meaningful yet diverse captions. Our experimental results demonstrate HBoP's ability to produce meaningful image embeddings for captioning, achieving performance comparable to larger VLMs and human-generated captions. HBoP sets a solid baseline for future work aiming to extract more relevant knowledge by controlling the intermediate image embeddings."}, {"title": "6 Limitations", "content": "The HBoP architecture uses bounding box information to extract image embeddings, where the bounding boxes may contain additional information beyond the segmented object. The exploration of caption generation solely based on irregular-shaped segmentation masks is deferred as future work."}, {"title": "7 Ethical Statement", "content": "Captions generated with HBoP might inadvertently contain harmful content. However, the final caption outputs mainly depend on the image content and pretrained image captioning model. Therefore, unless the images themselves are harmful or the pretrained model produces unsafe captions, HBOP captions are expected to pose minimal risk."}, {"title": "A Appendix", "content": ""}, {"title": "B Additional Related Works", "content": ""}, {"title": "B.1 Vision-Language Models (VLMs)", "content": "A growth of interest in VLMs has continued due to the wide availability of multi-modal data on the web. Foundation VLMs can be applied to a range of tasks in a zero-shot manner. Notably, CLIP (Radford et al., 2021b) jointly pre-trains an image encoder and a text encoder by maximizing and minimizing the cosine similarity of correct and incorrect image-text pair embeddings respectively with image-text contrastive (ITC) loss. In contrast, BLIP (Li et al., 2022a) uses both ITC and image-text matching (ITM) loss for enhanced image-text data representation. Additionally, the BLIP (Li et al., 2022a) captioner uses language modeling (LM) loss for autoregressive image caption generation along with a filter, capfilt to improve the quality of image-text pairs for training.\nFlamingo (Alayrac et al., 2022) shows remarkable zero-shot ability in image captioning, visual question-answering (VQA), and image-text retrieval (ITR) tasks by leveraging the few-shot learning ability of pre-trained vision-only and language-only models. It simply interleaves input visual data with task-specific text examples, producing free-form texts for unseen visual data. Another general-purpose model, BEIT3 (Wang et al., 2022b) with Multiway Transformer structure, uses different types of modality experts to perform fusion and modality-specific training. A masked modeling objective on images only and image-text pairs is performed for computer vision tasks (e.g., image classification, semantic segmentation, object detection) and vision-language tasks (e.g., VQA), respectively. Whereas the VQA task uses a fused encoder for image-text pairs, the ITR task encodes images and texts independently with ITC loss. Lastly, sequence-to-sequence learning is applied to generate texts from images for the image captioning task. Inspired by these previous works, we propose a meta-VLM model that utilizes a pre-trained BLIP (Li et al., 2022a) image captioning module to generate enhanced textual representations, which can later serve as useful data for various downstream tasks."}, {"title": "B.2 Hierarchical Representation", "content": "Identifying and extracting regions of interest within images is crucial for a hierarchical representation. The most intuitive way to achieve this would typically involve the use of object detectors (Yao et al., 2019; Cornia et al., 2020; Zhang et al., 2021). However, the heavy computational demands of the object detectors inevitably lead to inefficiency during the inference stage (Yao et al., 2019; Cornia et al., 2020; Zhang et al., 2021). In response, recent works sought to replace these cumbersome detectors by adopting visual concepts in the form of object tags (Fang et al., 2022; Shukor et al., 2022) as an alternative. However, this detector-free approach is contingent upon the availability of object-specific data within the dataset. Employing pre-trained models is a more efficient way to identify areas of interest within images. GradCAM (Selvaraju et al., 2017) highlights essential regions that the pre-trained models used to predict any target concept using its gradients with respect to feature map activations of the final convolutional layer. DINOv2 (Oquab et al., 2023) capitalizes on existing self-supervised pre-trained models to generate robust, all-purpose visual features, supporting a wide array of tasks ranging from image-level classification to pixel-level segmentation. However, the image regions/features delineated by GradCAM/DINOv2 tend to show saliency for specific tasks and are unable to capture the full spectrum of visual representations. Conversely, SAM (Kirillov et al., 2023) intricately segments every semantically significant component of an image into high-quality segmentation masks generated by prompting with various inputs such as point, box, mask, or free-form text, unrestricted with types of tasks. In our framework, we integrate SAM (Kirillov et al., 2023) to create semantically meaningful segmentation masks for an entire image automatically.\nSeveral prior studies have incorporated the principles of hierarchy or multi-scale representation into their model architectures, aiming to enhance the alignment between images and texts (Ji et al., 2021; Shao et al., 2023; Shukor et al., 2022). SHAN (Ji et al., 2021) deconstructs the image-text matching process into two distinct facets: fragment-level and context-level alignments enabling matches across three different scopes: local-to-local, global-to-local, and global-to-global. HiVLP (Shao et al., 2023) leverages both low-"}, {"title": "B.3 Caption Evaluation", "content": "Common image captioning evaluation metrics, including BLEU (Papineni et al., 2002a), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015) scores are primarily n-gram approaches that assess the quality of generated captions by considering their overlap with human-generated captions. Most SOTA VLMs frequently exhibit promising scores across these conventional evaluation metrics. However, these metrics are limited in their capabilities to measure the diversity of the generated captions. This limitation leads to a bias in these models towards generating an \"average\" and \"safe\" caption reflecting the most basic information in the image, rendering them less informative than human-generated captions. To address this gap, we incorporate several diversity metrics, including mBLEU-4, Div-2 (Aneja et al., 2019b), and the proposed pairwise cosine distance (PCD), along with semantic integrity and relevance scores to ensure that the captions generated by our framework are not only diverse but also meaningful and directly relevant to the given image and human-annotated captions."}, {"title": "C Experiments", "content": ""}, {"title": "C.1 Implementation Details", "content": "The ISM (Section 3.1) employs the fully automated SAM with no prompting (Kirillov et al., 2023), along with the image encoder initialized from ViT (ViT-L/16) pre-trained on ImageNet (Dosovitskiy et al., 2021), following the same settings as BLIP (Li et al., 2022a). Note that we use BLIP (Li et al., 2022a) for captioning instead of BLIP-2 (Li et al., 2023) since BLIP-2 uses intermediate representations trained on pairs of entire images and texts for caption generation using an LLM, which is not directly applicable to HBoP that uses pairs of image patches and texts. The HCM (Section 3.2) creates the global level by selecting the top (k =) 5 masks with the largest areas and designating the remaining masks as fine-grained. To create the regional level, K-means clustering, with (K =) 5 clusters per image, is applied to the bounding boxes of the segmentation masks. NMS with a threshold of 0.1 is applied at all three levels. Lastly, the ICM (Section 3.3) follows the methodology outlined in Tiong et al., 2022.\nAlthough HBoP presents a three-tier hierarchical structure, it is crucial to note that we adjust the different hierarchy levels depending on a given dataset. A dataset with information-rich complex images would require using all three hierarchy levels. However, a dataset with relatively simpler images, such as the MSCOCO dataset (Lin et al., 2014), would benefit from a two-tier hierarchy with just the global and regional captions. We use the first two levels during evaluations unless specified otherwise.\nAll the model captions in Tables 1 and 3 are regenerated, except for Seq-CVAE (Aneja et al., 2019b), where the results are taken directly from the original paper. While HBoP benefits from bounding box information, it is important to note that other baseline methods (e.g., ModeCap) have the additional advantage of explicit learning objectives to improve diversity. The exact prompts we use for Honeybee (Cha et al., 2023) (top) and LLaVA-1.5/1.6 (Liu et al., 2023a) are in Table 4."}, {"title": "C.2 Evaluation", "content": "We evaluate the model captions using three distinct metrics: 1) diversity across captions per image, 2) relevancy with images, and 3) semantic coherence and meaningfulness. The datasets we use for evaluation are: the Karpathy test split (Karpathy and Fei-Fei, 2015) of MSCOCO (5k images) (Lin et al., 2014), Flickr30K zero-shot (1k test images) (Young et al., 2014), and NoCaps validation (4.5k images) (Agrawal et al., 2019)."}, {"title": "C.2.1 Diversity", "content": "We measure the diversity in the generated captions using the cosine similarity between the sentence embeddings of all the corresponding captions per image. The comparison baselines are random captions,"}, {"title": "C.2.2 Relevancy", "content": "While confirming that each dataset contains captions with high semantic integrity is crucial, the captions must also be relevant to the corresponding images. We employ CLIP-Score (Hessel et al., 2021) that calculates the correlation between visual and textual CLIP embeddings (Radford et al., 2021b) using pre-trained ViT (openai/ clip-vit-base-patch32) without relying on human-generated references. Similar to the comparison baseline datasets for semantic integrity evaluation, we compare HBoP with PnP-VQA (Tiong et al., 2022), BLIP (Li et al., 2022a), BLIP-2 (Li et al., 2023), gold captions, along with random captions. We generate random captions by selecting five random captions for each image from a pool of HBoP captions corresponding to different images. In other words, although the random caption itself should make sense, they depict mismatched images. We randomly select one out of a total of five captions per image for each dataset and compute the correlation between CLIPScores of generated captions and gold captions.\nAdditionally, we measure the semantic similarity between ground-truth (or gold) captions and captions generated with models using transformer-based SBERT (Reimers and Gurevych, 2019). Note that this metric is robust to synonyms or paraphrasing, unlike n-gram metrics (Papineni et al., 2002b; Lin, 2004)."}, {"title": "C.2.3 Semantic Integrity", "content": "We measure the semantic integrity of the generated captions (Table 2) to ensure meaningfulness and cohesiveness using LLM evaluation as it has empirically shown high correlation with human judgment (Chiang and yi Lee, 2023; Liu et al., 2023b; Fu et al., 2023). We prompt Llama-2-13B (Llama-2-13b-chat-hf) (Touvron et al., 2023) to access the semantic integrity of HBoP captions along with gold and other baselines (PnP-VQA (Tiong et al., 2022), BLIP (Li et al., 2022a), BLIP-2 (Li et al., 2023)) captions. Specifically, we randomly select two captions out of a total of five captions per image for each dataset and evaluate the semantic integrity by averaging the coherency and meaningfulness scores for each caption using the prompt shown in Table 5. We use the prompt \"This is a picture of\" to generate captions for all models in our experiments. This deliberate choice ensures a fair comparison of the general caption"}, {"title": "D Additional Results", "content": ""}, {"title": "D.1 Relevancy", "content": "In Figure 3, HBOP captions (y-axis values in the last column) show comparable relevance scores with gold captions (x-axis values in the last column) with the slope of a linear regression lines being close to 0.5. Although the slopes of these regression lines (MSCOCO (Lin et al., 2014): 0.42, Flickr30k (Young et al., 2014): 0.39, Nocaps (Agrawal et al., 2019): 0.34) are less than those of BLIP (Li et al., 2022a) (0.49, 0.44, and 0.45) and BLIP-2 (Li et al., 2023) (0.51, 0.45, 0.43), we observe a trend of having relevance scores in the range of 20 to 40 for both x and y axes values. On the other hand, relevance scores for random and PnP-VQA (Tiong et al., 2022) captions have a spurious and less-correlated relation with those of gold captions."}, {"title": "D.2 GradCAM Results", "content": "In addition to the evaluation results of the generated captions (samples in Figure 4), we illustrate how the generated captions correlate with specific image regions through GradCAMs (Selvaraju et al., 2017). The visual representation identifies the image regions on which the generated captions are based. Specifically, we aggregate the gradients from all cross-attention layers of the pre-trained ITM model in PnP-VQA (Tiong et al., 2022). Whereas PnP-VQA (Tiong et al., 2022) feeds the question for the textual input, we input BLIP (Li et al., 2022a) and gold captions, along with HBoP captions. As shown in Figures 1 and 5, the highlighted regions in the image for HBoP captions closely resemble the same pattern as those observed using human-generated captions. On the contrary, BLIP exhibits a more constrained range, predominantly concentrating on specific image regions."}, {"title": "D.3 Fine-grained Captions", "content": "Although not evaluated in the perspectives of three main evaluation metrics, we can also create what we refer to as fine-grained captions that can serve as image tags using our proposed methodology. These serve as supplementary information, enhancing the depth of understanding of the image. They are more vital when dealing with complex images containing various small or intricate objects, which conventional caption generation processes may often overlook. By introducing the additional layer of granularity, our approach ensures a more detailed and inclusive interpretation of the image."}]}