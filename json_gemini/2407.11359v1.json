{"title": "Feature Inference Attack on Shapley Values", "authors": ["Xinjian Luo", "Yangfan Jiang", "Xiaokui Xiao"], "abstract": "As a solution concept in cooperative game theory, Shapley value\nis highly recognized in model interpretability studies and widely\nadopted by the leading Machine Learning as a Service (MLaaS)\nproviders, such as Google, Microsoft, and IBM. However, as the\nShapley value-based model interpretability methods have been thor-\noughly studied, few researchers consider the privacy risks incurred\nby Shapley values, despite that interpretability and privacy are two\nfoundations of machine learning (ML) models.\nIn this paper, we investigate the privacy risks of Shapley value-\nbased model interpretability methods using feature inference at-\ntacks: reconstructing the private model inputs based on their Shapley\nvalue explanations. Specifically, we present two adversaries. The\nfirst adversary can reconstruct the private inputs by training an\nattack model based on an auxiliary dataset and black-box access\nto the model interpretability services. The second adversary, even\nwithout any background knowledge, can successfully reconstruct\nmost of the private features by exploiting the local linear correla-\ntions between the model inputs and outputs. We perform the pro-\nposed attacks on the leading MLaaS platforms, i.e., Google Cloud,\nMicrosoft Azure, and IBM aix360. The experimental results demon-\nstrate the vulnerability of the state-of-the-art Shapley value-based\nmodel interpretability methods used in the leading MLaaS platforms\nand highlight the significance and necessity of designing privacy-\npreserving model interpretability methods in future studies. To\nour best knowledge, this is also the first work that investigates the\nprivacy risks of Shapley values.", "sections": [{"title": "1 INTRODUCTION", "content": "Nowadays, machine learning (ML) models are being increasingly\ndeployed into high-stakes domains for decision making, such as\nfinance, criminal justice and employment [6, 13, 18, 45]. Although\ndemonstrating impressive prediction performances, most deployed\nML models behave like black boxes to the developers and practition-\ners because of their complicated structures, e.g., neural networks,\nensemble models and kernel methods [13, 31]. To foster trust in\nML models, researchers design numerous \"explainable\" or \"inter-\npretable\" methods to help humans understand the predictions of\nML systems. In this paper, we focus on Shapley value-based local\ninterpretability methods, i.e., the class of methods that use Shapley\nvalue to explain individual model predictions.\nShapley value [50] is first proposed to distribute the surplus\namong a coalition of players in cooperative game theory. Since\nit theoretically satisfies a collection of desirable properties, i.e.,\nsymmetry, linearity, efficiency and null player [50], Shapley value is\nwidely recognized and adopted as the model interpretability method\nin both academia [13, 14, 17, 18, 31, 34, 56] and the leading MLaaS\nplatforms, such as Google Cloud, Microsoft Azure and IBM aix360 [2,\n8, 15]. But with the increasing deployment of ML models in the\nhigh-stakes domains, few studies related to model interpretability\nconsider the privacy issues, despite that privacy and interpretability\nare two fundamental properties demanded by regulations [26, 48].\nMotivation. Interpretability is a significant aspect of ML models.\nSpecifically, \"the right to explanation\" has been included in the Euro-\npean Union General Data Protection Regulation (GDPR) [26, 30, 48].\nModel interpretability is especially critical in the high-stakes do-\nmains. For example, in a data-driven loan platform supported by\na financial institution, a customer who submits a loan applica-\ntion will receive an approval decision recommended by an ML\nmodel [6, 44]. If the loan application is denied, the customer has\nthe right to demand an explanation report behind the denial from\nthe ML platform, as shown in Fig. 1. It is worth noting that unlike\nthe private model inputs which are classified into highly sensitive\ninformation in real-world applications, the explanation reports\u00b9\ncan be available to other personas except the customers and loan\nofficers. We give some examples of explanation releases as follows.\nFor companies, the explanations can help evaluate model bias and\ncorrectness, e.g., identifying dataset shift where training data is\ndifferent from test data [44], and understanding where the model\nis more or less confident for further improving its performance [6].\nIn these cases, the company needs to transmit model explanations\nto ML service providers (which could be the third party) for analy-\nsis [6]. For customers, the explanations could be shared on social\nmedia and obtained by the adversary [69]. Because the privacy risks\nin Shapley values are not well studied, both the companies and cus-\ntomers may underestimate the sensitivity of model explanations\nand transmit them in plaintext. Therefore, we are inspired to ask:\nare there any privacy risks in the model explanations generated via\nShapley value? Although some studies exploit membership infer-\nence [51] and model extraction [3, 37] on model explanations, all\nof them focus on the gradient-based heuristic methods instead of\nShapley values, and the information inferred by these attacks has\nlittle relation to the ground-truth model inputs. In this paper, we\ndemonstrate the vulnerability of Shapley values by feature infer-\nence attack, i.e., reconstructing the private features of model inputs\nbased on their Shapley value explanations.\nChallenges. There are three main challenges when performing fea-\nture inference on Shapley values. First is how to develop an in-depth\nanalysis on exactly how much privacy could be leaked by Shapley\nvalues. Since the privacy analyses in previous works [3, 37, 51]\nmainly target on the gradient-based explanations which are totally\ndifferent from Shapley values, we have to analyze the informa-\ntion flow from private inputs to their Shapley values from scratch.\nSecond is how to design generalized attacks applicable to both\nthe original Shapley values and its variants [31, 34]. For compu-\ntational efficiency, the sampling methods, e.g., Shapley sampling\nvalues [34] and SHAP [31], are preferred in real-world applications.\nConsidering that the explanations produced by sampling methods\ncould randomly and greatly deviate from the ground-truth Shapley\nvalues [31], the accuracies of feature inference attacks could be\nsignificantly impacted by these unstable explanations, because fea-\nture inference focuses on recovering private information with the\nfinest granularity, i.e., the feature values of the model inputs. Third\nis how to accurately infer private features via a limited number\nof queries to the MLaaS platforms. Because different from previ-\nous adversaries [36, 51] who have unlimited access to the target\nmodels, the adversary in the real-world setting needs to access the\nML service in a pay-per-query pattern [64]. A brute force method\nthat sends unlimited queries to the explanation platform may help\naccurately estimate the private features, but can be easily detected\nby the service provider and bring prohibitive financial and time\ncosts to the adversaries. An attack algorithm that accurately recon-\nstructs private features with a bounded error from a small number\nof queries is necessary in real-world applications.\nContributions. In this paper, to design attacks that can be used on\nShapley sampling values with unknown sampling errors, we first\nanalyze the connections between model inputs and the associated\nexplanations in an information-theoretical perspective, and show\nthat the information of private features is contained in its Shapley\nvalues as long as the variance of sampling errors is smaller than\nthat of the Shapley values. Then, we reveal the vulnerability of\nShapley value-based model interpretability methods by two feature\ninference attacks. These attacks can be applied to both the original\nShapley values and Shapley sampling values, and a limited number\nof queries (e.g., 100) to the ML services can enable the adversary to\naccurately reconstruct the important private features correspond-\ning to the target explanation reports. Specifically, we present two\nadversaries. The first adversary follows a similar setting to current\nstudies [24, 36, 47, 51] in which the adversary owns an auxiliary\ndataset of model inputs. Based on the auxiliary dataset and black-\nbox access to the explanation service, the adversary can train an\nattack model by empirically minimizing the sampling errors. The\nsecond adversary is presented with a relaxed assumption of the first\none, who has only black-box access to the ML platform and no back-\nground knowledge about the private features. Feature inference is\nharder to achieve for this adversary because the search space of pri-\nvate features is infinite and the errors of the reconstructed features,\nif any, are hard to be estimated without any auxiliary knowledge.\nNevertheless, we analyze the correlations between model inputs\nand outputs as well as the Shapley values, and find that the local\nlinearity of model decision boundaries for some important features\ncan be passed to Shapley values. Accordingly, we propose an at-\ntack algorithm by first approximating the black-box model with\na generalized additive model and then estimating the private fea-\ntures via linear interpolations based on a set of randomly generated\nmodel inputs. The estimation error of the second adversary can\nbe further bounded by Chebyshev's inequality and Hoeffding's\ninequality. In the experiments performed on three leading model\ninterpretability platforms (Google Cloud [15], Microsoft Azure [8]"}, {"title": "2 PRELIMINARY", "content": "2.1 Machine Learning\nWe focus on the supervised machine learning tasks. Let D =\n{(x,y) \u2208 X \u00d7 Y}\u2081 denote a training dataset with m samples,\nwhere X \u2282 Rn denotes the input space with n features, and Y \u2282 Rc\ndenotes the output space with c classes. Machine learning aims to\ntrain a model f with parameters \u03b8 such that the following loss\nfunction is minimized:\nmin = l(f(x\u00b2; \u03b8), y\u00b2) + \u03a9(\u03b8), (1)\n\u04e9\u0442\nt=1\nwhere 2(0) denotes the regularization term for preventing the\nmodel from overfitting.\nNeural Networks. Neural network (NN) models consist of one\ninput layer, one output layer and multiple hidden layers, where\nnon-linear transformations, such as ReLU and Sigmoid, are used as\nthe activation functions after the hidden layers. Although demon-\nstrating impressive performance over the last few years, NN is less\ntrustable compared to the traditional linear models because the\ninformation flow in NN is hard to be inspected [4].\nEnsemble Models. Ensemble methods can improve the prediction\nperformance by first training multiple models and then combining\ntheir outputs as the final prediction [46]. Two of the most popular\nensemble models are Random Forest (RF) [11] and Gradient Boosted\nDecision Trees (GBDT) [22]. Although both of them are composed\nof multiple decision trees, the training phases of these two models\nare mostly different. Each tree in RF is independently trained based\non a subset of randomly selected samples and features, whereas\nthe trees in GBDT are connected and one successive tree needs to\npredict the residuals of the preceding trees. Because the predictions\nof the ensemble models are aggregated from the outputs of multiple\ntrees (e.g., voting in RF and addition in GBDT), it is still a difficult\ntask to interpret these ensemble ouputs [46].\nKernel Methods. Kernel methods are mainly used to recognize\nthe nonlinear patterns in the training datasets [41]. The main idea\nis to first project the input features into a high-dimensional space\nvia non-linear transformations and then train linear models in that\nspace. One of the most famous kernel models is the support-vector\nmachine (SVM) with kernel trick [10]. Note that kernel SVMs learn\ndata patterns of the transformed features in the high-dimensional\nspace instead of the original features, which makes its predictions\nhard to be interpreted [42].\n2.2 Shapley Value\nShapley value [50] is a widely recognized method for interpreting\nmodel outputs in machine learning [17, 18, 31]. In this paper, we\nfocus on local interpretability: for an input sample x = {xi}=1 with\nn features and a specific class in the model outputs, Shapley value\ncomputes a vector s = {si}=1 in which each element si denotes\nthe influence of the corresponding feature xi on the target class of\nmodel outputs. Now we introduce how to compute the importance\nvalue si for feature xi.\nLet N = {1,..., n} be the index set of all features, S \u2286 N be a\ncollection of features, and x\u00b0 = {x}=1 be a reference sample. A\nsample x[s] is composed as follows: Vj \u2208 N, (x[s])j = xj if j \u2208 S\nand (x[s])j = x if j \u2208 N\\S. Now given the feature indexes N and\na trained model f, the Shapley value of feature i is defined by:\nSi = 1 f(x[su{i}})-f(x[s]) (2)\nn\nSCN\\{i}\nNote that mi (S, x) = f(x[Su{i}])-f(x[s]) represents the marginal\ncontribution of feature i to the feature set S. For example, suppose\nx = [3928], x = [6034] and N = {1, 2, 3, 4}, we\ncompute the marginal contribution of feature i = 1 to the feature\nset S = {2,3} by f(x[{1,2,3}]) \u2212 f(x[{2,3}]) = f ([6038]) -\nf ([3 0 3 8]). Mathematically, we can interpret s\u012f as the ex-\npectation of marginal contributions of feature i to all possible sets\ncomposed by the other features S \u2286 N\\{i} [14, 23].\nShapley sampling value. The O(2n) complexity of Eq. 2 is com-\nputationally prohibitive for most ML applications, thus most stud-\nies [18, 31] and MLaaS platforms [15] use a sampling method [34]\nto compute the approximate Shapley values: let rm; be the range\nof the marginal contributions of feature i, and {P} Pk }k=1 be v per-\nmutations of N. For each permutation PN, S denotes the set of\nln()rmi\nfeatures that precede i. By setting v \u2265 , we can compute\n2\u20ac2\nthe approximate Shapley values \u015di by Eq. 2 such that\nPr(\u015di Si \u2265 \u20ac) \u2264 \u03b4. (3)\nHere \u20ac > 0 denotes the estimation error."}, {"title": "3 PROBLEM STATEMENT", "content": "System Model. In this paper, we consider a system model in which\na commercial institution trains a black-box model f based on a sensi-\ntive dataset Xtrain and deploy it on an ML-as-a-service (MLaaS) [57]\nplatform, such as Google Cloud [15] and Microsoft Azure [8]. Users\ncan access the model based on a pay-per-query pattern [64]. Specif-\nically, a user can send a private sample x = {xi}=1 to the service\nprovider and obtain a prediction vector \u0177 = {\u0177i}=1 with an expla-\nnation vector s = {si}=1 w.r.t. a specific class, as shown in Fig. 1.\nNote that the service provider can also return c explanation vectors,\neach of which corresponds to a class, but returning one vector s w.r.t.\na predefined class is the default setting in related studies [51] and\nMLaaS platforms, e.g., Google Cloud [15]. Thus, for practicability\nand without loss of generality, we consider one explanation vector\nw.r.t. a specific class in this paper. We summarize the frequently\nused notations in Tab. 1 for reference.\nAttack Model. The previous studies in ML privacy [36, 40, 47, 51]\nassume that the adversary can collect an auxiliary dataset Xaux\nwhich follows the same underlying distribution of the target sample\nx. For example, a bank can synthesize a real-world dataset after\nobtaining the feature names on the explanation report from its\ncompetitors. In this paper, we study two adversaries by relaxing\nthe assumptions of previous studies. The first adversary follows the\nsetting in [36, 40, 47, 51], i.e., the attacker aims to infer the private\nsample x based on the associated explanation vector s, an auxiliary\ndataset Xaux and black-box access to the prediction model f on\nthe MLaaS platforms: x = A1 (s, Xaux, f), where x denotes the\nreconstructed values of x. For the second adversary, we use a more\nrestricted but practical setting that the attacker has only black-box\naccess to the ML services, and no background knowledge about the\ntarget sample x is available: x = A2(s, f)."}, {"title": "4 FEATURE INFERENCE ATTACKS BASED ON\nSHAPLEY VALUES", "content": "In this section, we present the proposed feature inference attacks\nunder different settings. Based on an auxiliary dataset, the first\nadversary can first learn a regression model with empirical risk\nminimization, then map the explanation vector s into the private\nfeature space and accordingly obtain the estimation of x. With\nonly black-box access to the MLaaS platform, the second adversary\ncan accurately reconstruct the important features of x by exploiting\nthe linearity of local model decision boundaries."}, {"title": "4.1 Adversary 1: Feature Inference Attack with\nan Auxiliary Dataset", "content": "4.1.1 Motivation. Given an input sample x = {xi}\u2081, a reference\nsample x\u00ba and the access to a black-box model f, we can simplify\nthe computation of Shapley value in Eq. 2 as follows:\nSi = h(xi; X1,\u00b7\u00b7\u00b7, Xi\u22121, Xi+1, \u00b7 \u00b7 \u00b7, Xn, x\u00ba) + \u20ac, (4)\nwhere h = go f is the composition of the black-box model f and a\nlinear transformation (i.e., the mathematical expectation) g on f. e\ndenotes the random noise caused by the sampling phase (Eq. 3).\nNow we characterize the dependence between si and x\u012f via a\npopular information metric called Mutual Information [13, 14]:\nI(xi; Si) = [ dsi [ dxiP(xi, si) [log log P(xi, si) (5)\nP(xi)P(si)\nTo reach a perfect security level of Eq. 4, we need to make I(xi; Si) =\n0 such that the adversary can not obtain any useful information of\nxi by observing si [68].\nFor simplicity, we denote the variance and mean of a variable\nx by \u03c3\u03b5 and \u00b5x, respectively. We assume the noise e caused by\nrandom sampling and the Shapley value si computed by Eq. 4 follow\nGaussian distributions with mean 0 and some variances\u00b2 because\nof the central limit theorem [5, 16, 34], thus\nP(si) = 1 exp - (6)\n\u221a2\u03c0\u03c3 Si\n20 Si\nNote that because e is Gaussian noise and e = si - h(xi), then\ngiven xi, si also follows a Gaussian distribution sixi \u039d(psixi x\u2081),\nwhere psixi = h(xi) and \u03c3s\u2081\\x\u2081 = \u03c3\u03b5. Specifically,\n(si - h(xi))2\nP(sixi) = 1 exp - (7)\n\u221a2\u03c0\u03c3\n202\nThen we can rewrite Eq. 5 as follows:\nI(xi; Si) = [ dsi [ dxiP(xi, Si) log P(sixi) (8)\nP(si)\n= -E[ In - (si - h(xi))2 (9)\n2\u03c0\u03c3\n202\n=- E[ In - (10)\n2\u03c0\u03c3\n=- log 22 - (11)\nwhere Eq. 10 follows because for the second term of Eq. 9, \u20ac =\nsi - h(xi) and E(\u03b5\u00b2) = \u03c3\u00b2 + \u03bc\u03b5 = \u03c32, making the numerator cancel\nwith the denominator, and so does for the third term."}, {"title": "4.1.2 The Attack with Auxiliary Datasets", "content": "Previous studies, such as\nmembership inference [47, 51] and property inference [24, 36], as-\nsume that the adversary owns an auxiliary dataset Xaux following\nthe same underlying distribution of the target samples. This assump-\ntion is reasonable because the competing companies own datasets\nthat have the same features but different customer bases [29]. The\nusers of the MLaaS platforms can also collect a dataset Xaux by\ncollusion [38]. In this section, we follow this assumption and show\nthat the adversary can reconstruct the target samples by training a\nregression model on Xaux.\nBased on the analysis in Section 4.1.1, we know that the expla-\nnation vector s is related to the private sample x. Although the\nintensity of the noise e is unknown, the adversary can still learn a\nmodel to reconstruct x by minimizing the noise component. Specifi-\ncally, the adversary can send prediction queries for all xaux \u2208 Xaux\nto the MLaaS platform and obtain the corresponding explanation\nset Saux. Now we rewrite Eq. 4 as\ns = $(x; x\u00ba). (12)\n: Saux \u2192 Xaux that\nRn represents a real-\nThe task becomes learning a hypothesis\nserves as the attack model. Here, Xaux\nworld dataset with finite cardinality. Additionally, both x and s are\nreal-valued vectors, and we have |Xaux| = |Saux. The key obser-\nvation is that our experiments exhibit a one-to-one correspondence\nbetween Xaux and Saux. This correspondence enables us to learn a\nhypothesis : Saux \u2192 Xaux to model the relations between input\nsamples and Shapley values. We can then use / to produce the\ntarget x from an unknown s. Note that collisions may occur during\nthe generation of Saux from Xaux; that is, multiple input samples\nmight result in the same Shapley vector. However, such collisions\nare infrequent and have not been observed in our experiments to\ndate. When collisions do occur, we can address them by removing\nthe affected (x, s) pairs before learning the hypothesis \u03c8.\nIt is also important to note that learning a hypothesis : Saux \u2192\nXaux does not imply that the function : Xaux \u2192 Saux is in-\nvertible. The invertibility of f depends on the properties of the\nunderlying models, which is not guaranteed for most model types."}, {"title": "4.2 Adversary 2: Feature Inference Attack with\nData Independence", "content": "4.2.1 Motivation. Suppose the adversary has no auxiliary datasets\nXaux, it will be impossible for the adversary to learn an attack\nmodel as discussed in Section 4.1 because the distributions of the\ntarget private features are unknown. A naive way to restore x from\ns is that the adversary generates a random dataset Xrand, sends the\nsamples in this dataset to the MLaaS platform separately and obtains\nthe corresponding explanations Srand. Provided that the size of the\nrandom dataset is large enough, there will be some srand \u2208 Srand\nsuch that ||srand - $||2 \u2264 where \u00a7 is a small threshold. The\nadversary can use the xrand associated with srand as the estimation\nof x. But there are two problems in this method. First is that to\nimprove the estimating accuracy, the adversary needs to randomly\ndraw a large number of samples, e.g., Xrand > 10000, from the\npossible feature space and send them to the MLaaS platform, which\ncan be costly on the pay-per-query service and increase the risk of\nbeing detected and blocked by the service provider. Second is that\nalthough ||srand - s||2 \u2264 holds, the corresponding xrand can be\na false positive case in which ||xrand - x||2 is large.\nIn the experiments, we observe that the level of correlations be-\ntween a feature xi and the model output \u0177 is closely consistent with\nthe correlations between xi and its explanation si. Fig. 3 shows an\nexample. First, we train a neural network on a real-world Diabetes\ndataset [55] and deploy it to Google Cloud [15]. Then, we send\nqueries of x = {x}}=1 to the deployed model and obtain the model\noutputs \u0177 and explanations s = {si}=1. We compute the Pearson\ncorrelation coefficients\u00b3 between the first seven features and \u0177 as"}, {"title": "4.3 Attack Generalization", "content": "It is worth noting that our attacks are designed based on the inher-\nent limitations of Shapley values. Specifically, we can simplify the\ncomputation of Shapley values into three steps: first use an affine\ntransformation to generate a feature replacement dataset from x\nand x\u00ba, then feed the dataset into the target model, and finally use\nanother affine transformation on the model outputs to obtain Shap-\nley values. Because the mapping from private features to Shapley\nvalues is fixed, we can then propose the inverse mapping attack\n(attack 1) and the GAM transformation-based attack (attack 2) to\nreconstruct the inputs from their explanations.\nOn the contrary, other popular explanation methods, e.g., LIME [44]\nand DeepLIFT [53], focus on approximating the linear decision\nboundary around the target features via a set of heuristic rules,\nwhere the mappings between features and explanations among\ndifferent samples are unstable and various [4], leading to little con-\nvergence of attack 1 and invalidating the GAM transformation and\nerror bounds of attack 2. For justification, we compared the perfor-\nmance of attack 1 on Shapley values, LIME, and DeepLIFT tested\non the Google Cloud platform [15], where the testing model and\ndataset are NN and Adult [19]. Note that attack 2 is not applicable\nto other explanation methods because the GAM transformation is\nbased on the computational formula of Shapley values. For attack 1,\nthe averaging l\u2081 error of feature reconstruction tested on Shapley\nvalues is 0.0768, which is far better than 0.1878 on LIME, 0.1527\non DeepLIFT, and 0.1933 on the empirical random guess baseline\n(see Section 5.1). The conclusion is that the proposed attacks are\ntargeted on Shapley values, and specific optimizations are needed\nto apply them to other heuristic explanation methods. We leave it\nas our feature work."}, {"title": "5 EXPERIMENTS", "content": "5.1 Experimental Setting\nThe proposed algorithms are implemented in Python with PyTorch 4,\nand all attack experiments are performed on a server equipped with\nIntel (R) Xeon (R) Gold 6240 CPU @ 2.60GHz\u00d772 and 376GB RAM,\nrunning Ubuntu 18.04 LTS.\nExplanation Platforms. To validate the effectiveness of the pro-\nposed attacks on real-world applications, we conduct experiments\non three leading ML platforms: Google Cloud AI platform [15],\nMicrosoft Azure Machine Learning [8] and IBM Research Trusted\nAI [2]. For the experiments on Google and Microsoft platforms, we\nfirst train models and deploy them in the cloud, then send predic-\ntion and explanation queries to the deployed models to obtain the\ncorresponding explanation vectors. The experiments on the IBM\nplatform are performed locally because a python package called\nAI Explainability 360 [2] is available. In addition, to test the at-\ntack performance w.r.t. different sampling errors, we implement\na Vanilla Shapley sampling method [56], because the explanatory\nAPI provided by Google only supports up to 50 permutations for\nsampling computation (e.g., v \u2264 50 in Section 2.2) while the APIs\nprovided by Microsoft and IBM are based on SHAP [31] which fails\nto provide a theoretical error bound for Shapley approximation."}, {"title": "5.2 Attack Performance w.r.t. Different\nNumbers of Queries", "content": "In both attacks, the adversary sends queries to the MLaaS platforms\nin a pay-per-query pattern. If the number of queries needed is large,\nthe cost for the adversary could be unacceptable. In this section, we\nevaluate the performance of attack 1 and 2 under different number\nof queries. The number of permutations used to compute Shapley\nsampling values is set to 50. We first vary the number of queries,\ni.e., the size of the auxiliary dataset |Xaux or the random dataset\nXrand, in {100, 200, 400, 800, 1600}, then test the performance of\nattack 1 and 2, respectively. Fig. 5(a)-5(d) show the l\u2081 losses of attack\n1. Fig. 5(e)-5(h) and Tab. 3 show the l\u2081 losses and success rates of\nattack 2. We have two observations from the results.\nFirst, with the increase of queries, the estimation errors of adver-\nsary 1 and 2 slightly decrease (Fig. 5(a)-5(h)), and the success rates\nof adversary 2 generally increase (Tab. 3). The reason is straight-\nforward: in adversary 1, the attack model y can obtain a better\ngeneralization performance when the training dataset Xaux be-\ncomes larger; for adversary 2, a finer sampling of random points\ncan help produce smaller value ranges of the candidate estimations\non the private features, thus reducing the estimation error bound"}, {"title": "5.3 Attack Performance w.r.t. Different MLaaS\nPlatforms", "content": "In this section, we evaluate the proposed attacks on different MLaaS\nexplanation platforms. Because the methods for computing Shap-\nley sampling values among different platforms are different, we\nimplement a Vanilla Shapley sampling method based on [56] for\ncomparison. The number of permutations sampled for computing\nShapley values is set to 50 in both the Vanilla method and Google\nCloud platform. Note that this parameter is not supported in the Mi-\ncrosoft Azure and IBM aix360 platforms because their explanation\nmethods are based on SHAP [31], which computes Shapley values\nvia a heuristic regression method instead of sampling permutations.\nThe numbers of queries in both of the adversaries are set to 1000.\nThe Shapley sampling error \u00a7 in adversary 2 (Algorithm 2) is empir-\nically set to among all platforms for comparison. Fig. 6(a)-6(d) and\n6(e)-6(h) show the performance of adversary 1 and 2, respectively.\nTab. 4 shows the success rates of adversary 2 corresponding to\nFig. 6(e)-6(h).\nFrom Fig. 6(a)-6(d) we observe that the attacks performed on\nMicrosoft and IBM platforms achieve similar performance, which\nis expectable because these two platforms compute Shapley values\nvia the same regression method. We also observe slight differences\nbetween the attacks performed on Vanilla and Google platforms\nalthough both of their implementations are based on the same Shap-\nley sampling method [56]. Note that for efficiency, the maximum\nnumber of sampling permutations in Google Cloud is fixed to 50,\nwhich greatly reduces the computation costs from O(2\") to O(50),\nmeanwhile producing relatively large sampling error. As discussed\nin Section 4.1, a large error can override the information of private\nfeatures in the Shapley values, leading to unstable reconstructions\n(see Eq. 11). Nevertheless, the proposed attacks can still reconstruct\nthe private features with high accuracy.\nFrom Tab. 4, we observe that the success rates of adversary 2\nperformed on IBM and Microsoft platforms are lower than the\nsuccess rates on the Vanilla method and Google Cloud. Considering\nthat SHAP [31] computes Shapley values heuristically without\nproviding a theoretical approximation bound, the real sampling\""}, {"title": "5.4 Attack Performance w.r.t. Different Feature\nImportance", "content": "As discussed in Section 4.2, the important features can be more\naccurately reconstructed than the less important features. In this\nsection, instead of averaging the reconstruction loss over all fea-\ntures, we give the attack loss per feature and dissect the connections\nbetween attack accuracies and feature importance.\nThree synthesis datasets with 12 features are used for the experi-\nments (see Tab. 2 for the details). To generate the features of different\nimportance, we first randomly draw five point clusters around five\nvertices of a three-dimension cube and label each cluster with a\nunique class. The three features of these random three-dimensional\npoints are called the key features. After that, we generate n, redun-\ndant features by randomly and linearly combining the key features.\nThe rest 12-3-n, features are generated from random noises. The\nkey features and redundant features are called important features.\nWe vary the percentages of important features in {25%, 50%, 75%}\nand accordingly generate three synthesis datasets. In addition, two\nreal-world datasets, Credit and Diabetes, are also used for justifica-\ntion. The importance of feature xi is defined as the Mean Absolute\nCorrelation Coefficients (MACC) between xi and model outputs \u0177:\nMACC(x, y) = \u03a3 abs(p(xi, \u0177j)), (21)\nj=1\nwhere \u0177 = {\u0177j}=1 denotes the model outputs and p is the Pearson\ncorrelation coefficient [27, 62]. Note that MACC measures the\naveraging linear correlation between xi and \u0177. A larger MACC\nindicates that the change of xi can produce larger variations in \u0177."}, {"title": "5.5 Attack Performance w.r.t. Different Shapley\nSampling Errors", "content": "As discussed in Section 4.1 and 4.2"}, {"title": "Feature Inference Attack on Shapley Values", "authors": ["Xinjian Luo", "Yangfan Jiang", "Xiaokui Xiao"], "abstract": "As a solution concept in cooperative game theory, Shapley value\nis highly recognized in model interpretability studies and widely\nadopted by the leading Machine Learning as a Service (MLaaS)\nproviders, such as Google, Microsoft, and IBM. However, as the\nShapley value-based model interpretability methods have been thor-\noughly studied, few researchers consider the privacy risks incurred\nby Shapley values, despite that interpretability and privacy are two\nfoundations of machine learning (ML) models.\nIn this paper, we investigate the privacy risks of Shapley value-\nbased model interpretability methods using feature inference at-\ntacks: reconstructing the private model inputs based on their Shapley\nvalue explanations. Specifically, we present two adversaries. The\nfirst adversary can reconstruct the private inputs by training an\nattack model based on an auxiliary dataset and black-box access\nto the model interpretability services. The second adversary, even\nwithout any background knowledge, can successfully reconstruct\nmost of the private features by exploiting the local linear correla-\ntions between the model inputs and outputs. We perform the pro-\nposed attacks on the leading MLaaS platforms, i.e., Google Cloud,\nMicrosoft Azure, and IBM aix360. The experimental results demon-\nstrate the vulnerability of the state-of-the-art Shapley value-based\nmodel interpretability methods used in the leading MLaaS platforms\nand highlight the significance and necessity of designing privacy-\npreserving model interpretability methods in future studies. To\nour best knowledge, this is also the first work that investigates the\nprivacy risks of Shapley values.", "sections": [{"title": "1 INTRODUCTION", "content": "Nowadays, machine learning (ML) models are being increasingly\ndeployed into high-stakes domains for decision making, such as\nfinance, criminal justice and employment [6, 13, 18, 45]. Although\ndemonstrating impressive prediction performances, most deployed\nML models behave like black boxes to the developers and practition-\ners because of their complicated structures, e.g., neural networks,\nensemble models and kernel methods [13, 31]. To foster trust in\nML models, researchers design numerous \"explainable\" or \"inter-\npretable\" methods to help humans understand the predictions of\nML systems. In this paper, we focus on Shapley value-based local\ninterpretability methods, i.e., the class of methods that use Shapley\nvalue to explain individual model predictions.\nShapley value [50] is first proposed to distribute the surplus\namong a coalition of players in cooperative game theory. Since\nit theoretically satisfies a collection of desirable properties, i.e.,\nsymmetry, linearity, efficiency and null player [50], Shapley value is\nwidely recognized and adopted as the model interpretability method\nin both academia [13, 14, 17, 18, 31, 34, 56] and the leading MLaaS\nplatforms, such as Google Cloud, Microsoft Azure and IBM aix360 [2,\n8, 15]. But with the increasing deployment of ML models in the\nhigh-stakes domains, few studies related to model interpretability\nconsider the privacy issues, despite that privacy and interpretability\nare two fundamental properties demanded by regulations [26, 48].\nMotivation. Interpretability is a significant aspect of ML models.\nSpecifically, \"the right to explanation\" has been included in the Euro-\npean Union General Data Protection Regulation (GDPR) [26, 30, 48].\nModel interpretability is especially critical in the high-stakes do-\nmains. For example, in a data-driven loan platform supported by\na financial institution, a customer who submits a loan applica-\ntion will receive an approval decision recommended by an ML\nmodel [6, 44]. If the loan application is denied, the customer has\nthe right to demand an explanation report behind the denial from\nthe ML platform, as shown in Fig. 1. It is worth noting that unlike\nthe private model inputs which are classified into highly sensitive\ninformation in real-world applications, the explanation reports\u00b9\ncan be available to other personas except the customers and loan\nofficers. We give some examples of explanation releases as follows.\nFor companies, the explanations can help evaluate model bias and\ncorrectness, e.g., identifying dataset shift where training data is\ndifferent from test data [44], and understanding where the model\nis more or less confident for further improving its performance [6].\nIn these cases, the company needs to transmit model explanations\nto ML service providers (which could be the third party) for analy-\nsis [6]. For customers, the explanations could be shared on social\nmedia and obtained by the adversary [69]. Because the privacy risks\nin Shapley values are not well studied, both the companies and cus-\ntomers may underestimate the sensitivity of model explanations\nand transmit them in plaintext. Therefore, we are inspired to ask:\nare there any privacy risks in the model explanations generated via\nShapley value? Although some studies exploit membership infer-\nence [51] and model extraction [3, 37] on model explanations, all\nof them focus on the gradient-based heuristic methods instead of\nShapley values, and the information inferred by these attacks has\nlittle relation to the ground-truth model inputs. In this paper, we\ndemonstrate the vulnerability of Shapley values by feature infer-\nence attack, i.e., reconstructing the private features of model inputs\nbased on their Shapley value explanations.\nChallenges. There are three main challenges when performing fea-\nture inference on Shapley values. First is how to develop an in-depth\nanalysis on exactly how much privacy could be leaked by Shapley\nvalues. Since the privacy analyses in previous works [3, 37, 51]\nmainly target on the gradient-based explanations which are totally\ndifferent from Shapley values, we have to analyze the informa-\ntion flow from private inputs to their Shapley values from scratch.\nSecond is how to design generalized attacks applicable to both\nthe original Shapley values and its variants [31, 34]. For compu-\ntational efficiency, the sampling methods, e.g., Shapley sampling\nvalues [34] and SHAP [31], are preferred in real-world applications.\nConsidering that the explanations produced by sampling methods\ncould randomly and greatly deviate from the ground-truth Shapley\nvalues [31], the accuracies of feature inference attacks could be\nsignificantly impacted by these unstable explanations, because fea-\nture inference focuses on recovering private information with the\nfinest granularity, i.e., the feature values of the model inputs. Third\nis how to accurately infer private features via a limited number\nof queries to the MLaaS platforms. Because different from previ-\nous adversaries [36, 51] who have unlimited access to the target\nmodels, the adversary in the real-world setting needs to access the\nML service in a pay-per-query pattern [64]. A brute force method\nthat sends unlimited queries to the explanation platform may help\naccurately estimate the private features, but can be easily detected\nby the service provider and bring prohibitive financial and time\ncosts to the adversaries. An attack algorithm that accurately recon-\nstructs private features with a bounded error from a small number\nof queries is necessary in real-world applications.\nContributions. In this paper, to design attacks that can be used on\nShapley sampling values with unknown sampling errors, we first\nanalyze the connections between model inputs and the associated\nexplanations in an information-theoretical perspective, and show\nthat the information of private features is contained in its Shapley\nvalues as long as the variance of sampling errors is smaller than\nthat of the Shapley values. Then, we reveal the vulnerability of\nShapley value-based model interpretability methods by two feature\ninference attacks. These attacks can be applied to both the original\nShapley values and Shapley sampling values, and a limited number\nof queries (e.g., 100) to the ML services can enable the adversary to\naccurately reconstruct the important private features correspond-\ning to the target explanation reports. Specifically, we present two\nadversaries. The first adversary follows a similar setting to current\nstudies [24, 36, 47, 51] in which the adversary owns an auxiliary\ndataset of model inputs. Based on the auxiliary dataset and black-\nbox access to the explanation service, the adversary can train an\nattack model by empirically minimizing the sampling errors. The\nsecond adversary is presented with a relaxed assumption of the first\none, who has only black-box access to the ML platform and no back-\nground knowledge about the private features. Feature inference is\nharder to achieve for this adversary because the search space of pri-\nvate features is infinite and the errors of the reconstructed features,\nif any, are hard to be estimated without any auxiliary knowledge.\nNevertheless, we analyze the correlations between model inputs\nand outputs as well as the Shapley values, and find that the local\nlinearity of model decision boundaries for some important features\ncan be passed to Shapley values. Accordingly, we propose an at-\ntack algorithm by first approximating the black-box model with\na generalized additive model and then estimating the private fea-\ntures via linear interpolations based on a set of randomly generated\nmodel inputs. The estimation error of the second adversary can\nbe further bounded by Chebyshev's inequality and Hoeffding's\ninequality. In the experiments performed on three leading model\ninterpretability platforms (Google Cloud [15], Microsoft Azure [8]"}, {"title": "2 PRELIMINARY", "content": "2.1 Machine Learning\nWe focus on the supervised machine learning tasks. Let D =\n{(x,y) \u2208 X \u00d7 Y}\u2081 denote a training dataset with m samples,\nwhere X \u2282 Rn denotes the input space with n features, and Y \u2282 Rc\ndenotes the output space with c classes. Machine learning aims to\ntrain a model f with parameters \u03b8 such that the following loss\nfunction is minimized:\nmin = l(f(x\u00b2; \u03b8), y\u00b2) + \u03a9(\u03b8), (1)\n\u04e9\u0442\nt=1\nwhere 2(0) denotes the regularization term for preventing the\nmodel from overfitting.\nNeural Networks. Neural network (NN) models consist of one\ninput layer, one output layer and multiple hidden layers, where\nnon-linear transformations, such as ReLU and Sigmoid, are used as\nthe activation functions after the hidden layers. Although demon-\nstrating impressive performance over the last few years, NN is less\ntrustable compared to the traditional linear models because the\ninformation flow in NN is hard to be inspected [4].\nEnsemble Models. Ensemble methods can improve the prediction\nperformance by first training multiple models and then combining\ntheir outputs as the final prediction [46]. Two of the most popular\nensemble models are Random Forest (RF) [11] and Gradient Boosted\nDecision Trees (GBDT) [22]. Although both of them are composed\nof multiple decision trees, the training phases of these two models\nare mostly different. Each tree in RF is independently trained based\non a subset of randomly selected samples and features, whereas\nthe trees in GBDT are connected and one successive tree needs to\npredict the residuals of the preceding trees. Because the predictions\nof the ensemble models are aggregated from the outputs of multiple\ntrees (e.g., voting in RF and addition in GBDT), it is still a difficult\ntask to interpret these ensemble ouputs [46].\nKernel Methods. Kernel methods are mainly used to recognize\nthe nonlinear patterns in the training datasets [41]. The main idea\nis to first project the input features into a high-dimensional space\nvia non-linear transformations and then train linear models in that\nspace. One of the most famous kernel models is the support-vector\nmachine (SVM) with kernel trick [10]. Note that kernel SVMs learn\ndata patterns of the transformed features in the high-dimensional\nspace instead of the original features, which makes its predictions\nhard to be interpreted [42].\n2.2 Shapley Value\nShapley value [50] is a widely recognized method for interpreting\nmodel outputs in machine learning [17, 18, 31]. In this paper, we\nfocus on local interpretability: for an input sample x = {xi}=1 with\nn features and a specific class in the model outputs, Shapley value\ncomputes a vector s = {si}=1 in which each element si denotes\nthe influence of the corresponding feature xi on the target class of\nmodel outputs. Now we introduce how to compute the importance\nvalue si for feature xi.\nLet N = {1,..., n} be the index set of all features, S \u2286 N be a\ncollection of features, and x\u00b0 = {x}=1 be a reference sample. A\nsample x[s] is composed as follows: Vj \u2208 N, (x[s])j = xj if j \u2208 S\nand (x[s])j = x if j \u2208 N\\S. Now given the feature indexes N and\na trained model f, the Shapley value of feature i is defined by:\nSi = 1 f(x[su{i}})-f(x[s]) (2)\nn\nSCN\\{i}\nNote that mi (S, x) = f(x[Su{i}}])-f(x[s]) represents the marginal\ncontribution of feature i to the feature set S. For example, suppose\nx = [3928], x = [6034] and N = {1, 2, 3, 4}, we\ncompute the marginal contribution of feature i = 1 to the feature\nset S = {2,3} by f(x[{1,2,3}]) \u2212 f(x[{2,3}]) = f ([6038]) -\nf ([3 0 3 8]). Mathematically, we can interpret s\u012f as the ex-\npectation of marginal contributions of feature i to all possible sets\ncomposed by the other features S \u2286 N\\{i} [14, 23].\nShapley sampling value. The O(2n) complexity of Eq. 2 is com-\nputationally prohibitive for most ML applications, thus most stud-\nies [18, 31] and MLaaS platforms [15] use a sampling method [34]\nto compute the approximate Shapley values: let rm; be the range\nof the marginal contributions of feature i, and {P} Pk }k=1 be v per-\nmutations of N. For each permutation PN, S denotes the set of\nln()rmi\nfeatures that precede i. By setting v \u2265 , we can compute\n2\u20ac2\nthe approximate Shapley values \u015di by Eq. 2 such that\nPr(\u015di Si \u2265 \u20ac) \u2264 \u03b4. (3)\nHere \u20ac > 0 denotes the estimation error."}, {"title": "3 PROBLEM STATEMENT", "content": "System Model. In this paper, we consider a system model in which\na commercial institution trains a black-box model f based on a sensi-\ntive dataset Xtrain and deploy it on an ML-as-a-service (MLaaS) [57]\nplatform, such as Google Cloud [15] and Microsoft Azure [8]. Users\ncan access the model based on a pay-per-query pattern [64]. Specif-\nically, a user can send a private sample x = {xi}=1 to the service\nprovider and obtain a prediction vector \u0177 = {\u0177i}=1 with an expla-\nnation vector s = {si}=1 w.r.t. a specific class, as shown in Fig. 1.\nNote that the service provider can also return c explanation vectors,\neach of which corresponds to a class, but returning one vector s w.r.t.\na predefined class is the default setting in related studies [51] and\nMLaaS platforms, e.g., Google Cloud [15]. Thus, for practicability\nand without loss of generality, we consider one explanation vector\nw.r.t. a specific class in this paper. We summarize the frequently\nused notations in Tab. 1 for reference.\nAttack Model. The previous studies in ML privacy [36, 40, 47, 51]\nassume that the adversary can collect an auxiliary dataset Xaux\nwhich follows the same underlying distribution of the target sample\nx. For example, a bank can synthesize a real-world dataset after\nobtaining the feature names on the explanation report from its\ncompetitors. In this paper, we study two adversaries by relaxing\nthe assumptions of previous studies. The first adversary follows the\nsetting in [36, 40, 47, 51], i.e., the attacker aims to infer the private\nsample x based on the associated explanation vector s, an auxiliary\ndataset Xaux and black-box access to the prediction model f on\nthe MLaaS platforms: x = A1 (s, Xaux, f), where x denotes the\nreconstructed values of x. For the second adversary, we use a more\nrestricted but practical setting that the attacker has only black-box\naccess to the ML services, and no background knowledge about the\ntarget sample x is available: x = A2(s, f)."}, {"title": "4 FEATURE INFERENCE ATTACKS BASED ON\nSHAPLEY VALUES", "content": "In this section, we present the proposed feature inference attacks\nunder different settings. Based on an auxiliary dataset, the first\nadversary can first learn a regression model with empirical risk\nminimization, then map the explanation vector s into the private\nfeature space and accordingly obtain the estimation of x. With\nonly black-box access to the MLaaS platform, the second adversary\ncan accurately reconstruct the important features of x by exploiting\nthe linearity of local model decision boundaries."}, {"title": "4.1 Adversary 1: Feature Inference Attack with\nan Auxiliary Dataset", "content": "4.1.1 Motivation. Given an input sample x = {xi}\u2081, a reference\nsample x\u00ba and the access to a black-box model f, we can simplify\nthe computation of Shapley value in Eq. 2 as follows:\nSi = h(xi; X1,\u00b7\u00b7\u00b7, Xi\u22121, Xi+1, \u00b7 \u00b7 \u00b7, Xn, x\u00ba) + \u20ac, (4)\nwhere h = go f is the composition of the black-box model f and a\nlinear transformation (i.e., the mathematical expectation) g on f. e\ndenotes the random noise caused by the sampling phase (Eq. 3).\nNow we characterize the dependence between si and x\u012f via a\npopular information metric called Mutual Information [13, 14]:\nI(xi; Si) = [ dsi [ dxiP(xi, si) [log log P(xi, si) (5)\nP(xi)P(si)\nTo reach a perfect security level of Eq. 4, we need to make I(xi; Si) =\n0 such that the adversary can not obtain any useful information of\nxi by observing si [68].\nFor simplicity, we denote the variance and mean of a variable\nx by \u03c3\u03b5 and \u00b5x, respectively. We assume the noise e caused by\nrandom sampling and the Shapley value si computed by Eq. 4 follow\nGaussian distributions with mean 0 and some variances\u00b2 because\nof the central limit theorem [5, 16, 34], thus\nP(si) = 1 exp - (6)\n\u221a2\u03c0\u03c3 Si\n20 Si\nNote that because e is Gaussian noise and e = si - h(xi), then\ngiven xi, si also follows a Gaussian distribution sixi \u039d(psixi x\u2081),\nwhere psixi = h(xi) and \u03c3s\u2081\\x\u2081 = \u03c3\u03b5. Specifically,\n(si - h(xi))2\nP(sixi) = 1 exp - (7)\n\u221a2\u03c0\u03c3\n202\nThen we can rewrite Eq. 5 as follows:\nI(xi; Si) = [ dsi [ dxiP(xi, Si) log P(sixi) (8)\nP(si)\n= -E[ In - (si - h(xi))2 (9)\n2\u03c0\u03c3\n202\n=- E[ In - (10)\n2\u03c0\u03c3\n=- log 22 - (11)\nwhere Eq. 10 follows because for the second term of Eq. 9, \u20ac =\nsi - h(xi) and E(\u03b5\u00b2) = \u03c3\u00b2 + \u03bc\u03b5 = \u03c32, making the numerator cancel\nwith the denominator, and so does for the third term."}, {"title": "4.1.2 The Attack with Auxiliary Datasets", "content": "Previous studies, such as\nmembership inference [47, 51] and property inference [24, 36], as-\nsume that the adversary owns an auxiliary dataset Xaux following\nthe same underlying distribution of the target samples. This assump-\ntion is reasonable because the competing companies own datasets\nthat have the same features but different customer bases [29]. The\nusers of the MLaaS platforms can also collect a dataset Xaux by\ncollusion [38]. In this section, we follow this assumption and show\nthat the adversary can reconstruct the target samples by training a\nregression model on Xaux.\nBased on the analysis in Section 4.1.1, we know that the expla-\nnation vector s is related to the private sample x. Although the\nintensity of the noise e is unknown, the adversary can still learn a\nmodel to reconstruct x by minimizing the noise component. Specifi-\ncally, the adversary can send prediction queries for all xaux \u2208 Xaux\nto the MLaaS platform and obtain the corresponding explanation\nset Saux. Now we rewrite Eq. 4 as\ns = $(x; x\u00ba). (12)\n: Saux \u2192 Xaux that\nRn represents a real-\nThe task becomes learning a hypothesis\nserves as the attack model. Here, Xaux\nworld dataset with finite cardinality. Additionally, both x and s are\nreal-valued vectors, and we have |Xaux| = |Saux. The key obser-\nvation is that our experiments exhibit a one-to-one correspondence\nbetween Xaux and Saux. This correspondence enables us to learn a\nhypothesis : Saux \u2192 Xaux to model the relations between input\nsamples and Shapley values. We can then use / to produce the\ntarget x from an unknown s. Note that collisions may occur during\nthe generation of Saux from Xaux; that is, multiple input samples\nmight result in the same Shapley vector. However, such collisions\nare infrequent and have not been observed in our experiments to\ndate. When collisions do occur, we can address them by removing\nthe affected (x, s) pairs before learning the hypothesis \u03c8.\nIt is also important to note that learning a hypothesis : Saux \u2192\nXaux does not imply that the function : Xaux \u2192 Saux is in-\nvertible. The invertibility of f depends on the properties of the\nunderlying models, which is not guaranteed for most model types."}, {"title": "4.2 Adversary 2: Feature Inference Attack with\nData Independence", "content": "4.2.1 Motivation. Suppose the adversary has no auxiliary datasets\nXaux, it will be impossible for the adversary to learn an attack\nmodel as discussed in Section 4.1 because the distributions of the\ntarget private features are unknown. A naive way to restore x from\ns is that the adversary generates a random dataset Xrand, sends the\nsamples in this dataset to the MLaaS platform separately and obtains\nthe corresponding explanations Srand. Provided that the size of the\nrandom dataset is large enough, there will be some srand \u2208 Srand\nsuch that ||srand - $||2 \u2264 where \u00a7 is a small threshold. The\nadversary can use the xrand associated with srand as the estimation\nof x. But there are two problems in this method. First is that to\nimprove the estimating accuracy, the adversary needs to randomly\ndraw a large number of samples, e.g., Xrand > 10000, from the\npossible feature space and send them to the MLaaS platform, which\ncan be costly on the pay-per-query service and increase the risk of\nbeing detected and blocked by the service provider. Second is that\nalthough ||srand - s||2 \u2264 holds, the corresponding xrand can be\na false positive case in which ||xrand - x||2 is large.\nIn the experiments, we observe that the level of correlations be-\ntween a feature xi and the model output \u0177 is closely consistent with\nthe correlations between xi and its explanation si. Fig. 3 shows an\nexample. First, we train a neural network on a real-world Diabetes\ndataset [55] and deploy it to Google Cloud [15]. Then, we send\nqueries of x = {x}}=1 to the deployed model and obtain the model\noutputs \u0177 and explanations s = {si}=1. We compute the Pearson\ncorrelation coefficients\u00b3 between the first seven features and \u0177 as"}, {"title": "4.3 Attack Generalization", "content": "It is worth noting that our attacks are designed based on the inher-\nent limitations of Shapley values. Specifically, we can simplify the\ncomputation of Shapley values into three steps: first use an affine\ntransformation to generate a feature replacement dataset from x\nand x\u00ba, then feed the dataset into the target model, and finally use\nanother affine transformation on the model outputs to obtain Shap-\nley values. Because the mapping from private features to Shapley\nvalues is fixed, we can then propose the inverse mapping attack\n(attack 1) and the GAM transformation-based attack (attack 2) to\nreconstruct the inputs from their explanations.\nOn the contrary, other popular explanation methods, e.g., LIME [44]\nand DeepLIFT [53], focus on approximating the linear decision\nboundary around the target features via a set of heuristic rules,\nwhere the mappings between features and explanations among\ndifferent samples are unstable and various [4], leading to little con-\nvergence of attack 1 and invalidating the GAM transformation and\nerror bounds of attack 2. For justification, we compared the perfor-\nmance of attack 1 on Shapley values, LIME, and DeepLIFT tested\non the Google Cloud platform [15], where the testing model and\ndataset are NN and Adult [19]. Note that attack 2 is not applicable\nto other explanation methods because the GAM transformation is\nbased on the computational formula of Shapley values. For attack 1,\nthe averaging l\u2081 error of feature reconstruction tested on Shapley\nvalues is 0.0768, which is far better than 0.1878 on LIME, 0.1527\non DeepLIFT, and 0.1933 on the empirical random guess baseline\n(see Section 5.1). The conclusion is that the proposed attacks are\ntargeted on Shapley values, and specific optimizations are needed\nto apply them to other heuristic explanation methods. We leave it\nas our feature work."}, {"title": "5 EXPERIMENTS", "content": "5.1 Experimental Setting\nThe proposed algorithms are implemented in Python with PyTorch 4,\nand all attack experiments are performed on a server equipped with\nIntel (R) Xeon (R) Gold 6240 CPU @ 2.60GHz\u00d772 and 376GB RAM,\nrunning Ubuntu 18.04 LTS.\nExplanation Platforms. To validate the effectiveness of the pro-\nposed attacks on real-world applications, we conduct experiments\non three leading ML platforms: Google Cloud AI platform [15],\nMicrosoft Azure Machine Learning [8] and IBM Research Trusted\nAI [2]. For the experiments on Google and Microsoft platforms, we\nfirst train models and deploy them in the cloud, then send predic-\ntion and explanation queries to the deployed models to obtain the\ncorresponding explanation vectors. The experiments on the IBM\nplatform are performed locally because a python package called\nAI Explainability 360 [2] is available. In addition, to test the at-\ntack performance w.r.t. different sampling errors, we implement\na Vanilla Shapley sampling method [56], because the explanatory\nAPI provided by Google only supports up to 50 permutations for\nsampling computation (e.g., v \u2264 50 in Section 2.2) while the APIs\nprovided by Microsoft and IBM are based on SHAP [31] which fails\nto provide a theoretical error bound for Shapley approximation."}, {"title": "5.2 Attack Performance w.r.t. Different\nNumbers of Queries", "content": "In both attacks, the adversary sends queries to the MLaaS platforms\nin a pay-per-query pattern. If the number of queries needed is large,\nthe cost for the adversary could be unacceptable. In this section, we\nevaluate the performance of attack 1 and 2 under different number\nof queries. The number of permutations used to compute Shapley\nsampling values is set to 50. We first vary the number of queries,\ni.e., the size of the auxiliary dataset |Xaux or the random dataset\nXrand, in {100, 200, 400, 800, 1600}, then test the performance of\nattack 1 and 2, respectively. Fig. 5(a)-5(d) show the l\u2081 losses of attack\n1. Fig. 5(e)-5(h) and Tab. 3 show the l\u2081 losses and success rates of\nattack 2. We have two observations from the results.\nFirst, with the increase of queries, the estimation errors of adver-\nsary 1 and 2 slightly decrease (Fig. 5(a)-5(h)), and the success rates\nof adversary 2 generally increase (Tab. 3). The reason is straight-\nforward: in adversary 1, the attack model y can obtain a better\ngeneralization performance when the training dataset Xaux be-\ncomes larger; for adversary 2, a finer sampling of random points\ncan help produce smaller value ranges of the candidate estimations\non the private features, thus reducing the estimation error bound"}, {"title": "5.3 Attack Performance w.r.t. Different MLaaS\nPlatforms", "content": "In this section, we evaluate the proposed attacks on different MLaaS\nexplanation platforms. Because the methods for computing Shap-\nley sampling values among different platforms are different, we\nimplement a Vanilla Shapley sampling method based on [56] for\ncomparison. The number of permutations sampled for computing\nShapley values is set to 50 in both the Vanilla method and Google\nCloud platform. Note that this parameter is not supported in the Mi-\ncrosoft Azure and IBM aix360 platforms because their explanation\nmethods are based on SHAP [31], which computes Shapley values\nvia a heuristic regression method instead of sampling permutations.\nThe numbers of queries in both of the adversaries are set to 1000.\nThe Shapley sampling error \u00a7 in adversary 2 (Algorithm 2) is empir-\nically set to among all platforms for comparison. Fig. 6(a)-6(d) and\n6(e)-6(h) show the performance of adversary 1 and 2, respectively.\nTab. 4 shows the success rates of adversary 2 corresponding to\nFig. 6(e)-6(h).\nFrom Fig. 6(a)-6(d) we observe that the attacks performed on\nMicrosoft and IBM platforms achieve similar performance, which\nis expectable because these two platforms compute Shapley values\nvia the same regression method. We also observe slight differences\nbetween the attacks performed on Vanilla and Google platforms\nalthough both of their implementations are based on the same Shap-\nley sampling method [56]. Note that for efficiency, the maximum\nnumber of sampling permutations in Google Cloud is fixed to 50,\nwhich greatly reduces the computation costs from O(2\") to O(50),\nmeanwhile producing relatively large sampling error. As discussed\nin Section 4.1, a large error can override the information of private\nfeatures in the Shapley values, leading to unstable reconstructions\n(see Eq. 11). Nevertheless, the proposed attacks can still reconstruct\nthe private features with high accuracy.\nFrom Tab. 4, we observe that the success rates of adversary 2\nperformed on IBM and Microsoft platforms are lower than the\nsuccess rates on the Vanilla method and Google Cloud. Considering\nthat SHAP [31] computes Shapley values heuristically without\nproviding a theoretical approximation bound, the real sampling\""}, {"title": "5.4 Attack Performance w.r.t. Different Feature\nImportance", "content": "As discussed in Section 4.2, the important features can be more\naccurately reconstructed than the less important features. In this\nsection, instead of averaging the reconstruction loss over all fea-\ntures, we give the attack loss per feature and dissect the connections\nbetween attack accuracies and feature importance.\nThree synthesis datasets with 12 features are used for the experi-\nments (see Tab. 2 for the details). To generate the features of different\nimportance, we first randomly draw five point clusters around five\nvertices of a three-dimension cube and label each cluster with a\nunique class. The three features of these random three-dimensional\npoints are called the key features. After that, we generate n, redun-\ndant features by randomly and linearly combining the key features.\nThe rest 12-3-n, features are generated from random noises. The\nkey features and redundant features are called important features.\nWe vary the percentages of important features in {25%, 50%, 75%}\nand accordingly generate three synthesis datasets. In addition, two\nreal-world datasets, Credit and Diabetes, are also used for justifica-\ntion. The importance of feature xi is defined as the Mean Absolute\nCorrelation Coefficients (MACC) between xi and model outputs \u0177:\nMACC(x, y) = \u03a3 abs(p(xi, \u0177j)), (21)\nj=1\nwhere \u0177 = {\u0177j}=1 denotes the model outputs and p is the Pearson\ncorrelation coefficient [27, 62]. Note that MACC measures the\naveraging linear correlation between xi and \u0177. A larger MACC\nindicates that the change of xi can produce larger variations in \u0177."}, {"title": "5.5 Attack Performance w.r.t. Different Shapley\nSampling Errors", "content": "As discussed in Section 4.1 and 4.2, the proposed attacks can be\napplied to the explanation methods with unknown sampling errors.\nTo evaluate the robustness of the proposed algorithms, we first vary\nthe Shapley sampling errors es in {r/5,r/10, r/20, 0} and accord-\ningly generate Shapley values with different errors via the Vanilla\nmethod, then test the attack performance on these explanations.\nThe 8 in Eq. 3 is set to 0.1. The test model is GBDT. We empirically\nset & in Algorithm 2 to 5 in all experiments, which corresponds to\nthe real-world scenario that the adversary has no information about\nthe sampling error. The numbers of queries in both attacks are set\nto 1000. The attack losses of adversary 1 and 2 on Credit are shown\nin Fig. 8(a), with the success rates (0.4685, 0.4689, 0.4758, 0.4824}\nof adversary 2. The losses on Diabetes are shown in Fig. 8(b), with\nthe success rates (0.4730, 0.4921, 0.4955, 0.5057} of adversary 2.\nWe make two observations from the results. First, the recon-\nstruction accuracies of the proposed attacks slightly improve as\nthe sampling errors decrease. This is reasonable because with the\ndecrease of random noises, the information of the private features\ncontained in Shapley values will increase, which is beneficial to the\nreconstruction algorithms. But the improvement is slight, demon-\nstrating the robustness and efficiency of the proposed attacks even\nunder large sampling errors. For justification, only 37 random per-\nmutations of features are needed to achieve the sampling error of\nr/5, whereas 2"}]}]}