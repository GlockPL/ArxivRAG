{"title": "Feature Inference Attack on Shapley Values", "authors": ["Xinjian Luo", "Yangfan Jiang", "Xiaokui Xiao"], "abstract": "As a solution concept in cooperative game theory, Shapley value is highly recognized in model interpretability studies and widely adopted by the leading Machine Learning as a Service (MLaaS) providers, such as Google, Microsoft, and IBM. However, as the Shapley value-based model interpretability methods have been thoroughly studied, few researchers consider the privacy risks incurred by Shapley values, despite that interpretability and privacy are two foundations of machine learning (ML) models.\nIn this paper, we investigate the privacy risks of Shapley value-based model interpretability methods using feature inference attacks: reconstructing the private model inputs based on their Shapley value explanations. Specifically, we present two adversaries. The first adversary can reconstruct the private inputs by training an attack model based on an auxiliary dataset and black-box access to the model interpretability services. The second adversary, even without any background knowledge, can successfully reconstruct most of the private features by exploiting the local linear correlations between the model inputs and outputs. We perform the proposed attacks on the leading MLaaS platforms, i.e., Google Cloud, Microsoft Azure, and IBM aix360. The experimental results demonstrate the vulnerability of the state-of-the-art Shapley value-based model interpretability methods used in the leading MLaaS platforms and highlight the significance and necessity of designing privacy-preserving model interpretability methods in future studies. To our best knowledge, this is also the first work that investigates the privacy risks of Shapley values.", "sections": [{"title": "1 INTRODUCTION", "content": "Nowadays, machine learning (ML) models are being increasingly deployed into high-stakes domains for decision making, such as finance, criminal justice and employment [6, 13, 18, 45]. Although demonstrating impressive prediction performances, most deployed ML models behave like black boxes to the developers and practitioners because of their complicated structures, e.g., neural networks, ensemble models and kernel methods [13, 31]. To foster trust in ML models, researchers design numerous \"explainable\" or \"interpretable\" methods to help humans understand the predictions of ML systems. In this paper, we focus on Shapley value-based local interpretability methods, i.e., the class of methods that use Shapley value to explain individual model predictions.\nShapley value [50] is first proposed to distribute the surplus among a coalition of players in cooperative game theory. Since it theoretically satisfies a collection of desirable properties, i.e., symmetry, linearity, efficiency and null player [50], Shapley value is widely recognized and adopted as the model interpretability method in both academia [13, 14, 17, 18, 31, 34, 56] and the leading MLaaS platforms, such as Google Cloud, Microsoft Azure and IBM aix360 [2, 8, 15]. But with the increasing deployment of ML models in the high-stakes domains, few studies related to model interpretability consider the privacy issues, despite that privacy and interpretability are two fundamental properties demanded by regulations [26, 48].\nMotivation. Interpretability is a significant aspect of ML models. Specifically, \"the right to explanation\" has been included in the European Union General Data Protection Regulation (GDPR) [26, 30, 48]. Model interpretability is especially critical in the high-stakes domains. For example, in a data-driven loan platform supported by a financial institution, a customer who submits a loan application will receive an approval decision recommended by an ML model [6, 44]. If the loan application is denied, the customer has the right to demand an explanation report behind the denial from the ML platform, as shown in Fig. 1. It is worth noting that unlike the private model inputs which are classified into highly sensitive information in real-world applications, the explanation reports\u00b9 can be available to other personas except the customers and loan officers. We give some examples of explanation releases as follows. For companies, the explanations can help evaluate model bias and correctness, e.g., identifying dataset shift where training data is different from test data [44], and understanding where the model is more or less confident for further improving its performance [6]. In these cases, the company needs to transmit model explanations\nNote that whether or not to include the private features in the explanation reports depends on the practitioners and target users. For example, QII [18] and the demo on Google AI Platform [15] only illustrate Shapley values in explanation reports. Meanwhile, the demos on IBM x360 [2] and SHAP library [31] include both the explanations and private features. In this paper, we focus on the first case."}, {"title": "2 PRELIMINARY", "content": "2.1 Machine Learning\nWe focus on the supervised machine learning tasks. Let $D = \\{(x,y) \\in X \\times Y\\}^m_{t=1}$ denote a training dataset with m samples, where $X \\subseteq \\mathbb{R}^n$ denotes the input space with n features, and $Y \\subseteq \\mathbb{R}^c$ denotes the output space with c classes. Machine learning aims to train a model f with parameters $\\theta$ such that the following loss function is minimized:\n$\\min_{\\theta} = \\frac{1}{m} \\sum_{t=1}^{m} l(f(x^t; \\theta), y^t) + \\Omega(\\theta),$ (1)\nwhere $\\Omega(\\theta)$ denotes the regularization term for preventing the model from overfitting.\nNeural Networks. Neural network (NN) models consist of one input layer, one output layer and multiple hidden layers, where non-linear transformations, such as ReLU and Sigmoid, are used as the activation functions after the hidden layers. Although demonstrating impressive performance over the last few years, NN is less trustable compared to the traditional linear models because the information flow in NN is hard to be inspected [4].\nEnsemble Models. Ensemble methods can improve the prediction performance by first training multiple models and then combining their outputs as the final prediction [46]. Two of the most popular ensemble models are Random Forest (RF) [11] and Gradient Boosted Decision Trees (GBDT) [22]. Although both of them are composed of multiple decision trees, the training phases of these two models are mostly different. Each tree in RF is independently trained based on a subset of randomly selected samples and features, whereas"}, {"title": "2.2 Shapley Value", "content": "Shapley value [50] is a widely recognized method for interpreting model outputs in machine learning [17, 18, 31]. In this paper, we focus on local interpretability: for an input sample $x = \\{x_i\\}^n_{i=1}$ with n features and a specific class in the model outputs, Shapley value computes a vector $s = \\{s_i\\}^n_{i=1}$ in which each element $s_i$ denotes the influence of the corresponding feature $x_i$ on the target class of model outputs. Now we introduce how to compute the importance value $s_i$ for feature i.\nLet $N = \\{1,\\dots, n\\}$ be the index set of all features, $S \\subseteq N$ be a collection of features, and $x^o = \\{x^o_i\\}^n_{i=1}$ be a reference sample. A sample $x[s]$ is composed as follows: $ \\forall j \\in N, (x[s])_j = x_j$ if $j \\in S$ and $(x[s])_j = x^o_j$ if $j \\in N\\backslash S$. Now given the feature indexes N and a trained model f, the Shapley value of feature i is defined by:\n$s_i = \\frac{1}{n}\\sum_{S \\subseteq N\\{i\\}}  \\frac{1}{\\binom{n-1}{|S|}} \\left[ f(x[s\\cup \\{i\\}]) - f(x[s])\\right]$ (2)\nNote that $mi(S, x) = f(x[S\\cup\\{i\\}]) - f(x[s])$ represents the marginal contribution of feature i to the feature set S. For example, suppose $x = [3 9 2 8], x^o = [6 0 3 4]$ and $N = \\{1, 2, 3, 4\\}$, we compute the marginal contribution of feature $i = 1$ to the feature set $S = \\{2,3\\}$ by $f(x[\\{1,2,3\\}]) - f(x[\\{2,3\\}]) = f ([6038]) - f ([3 0 3 8])$. Mathematically, we can interpret $s_i$ as the expectation of marginal contributions of feature i to all possible sets composed by the other features $S \\subseteq N\\backslash\\{i\\}$ [14, 23].\nShapley sampling value. The $O(2^n)$ complexity of Eq. 2 is computationally prohibitive for most ML applications, thus most studies [18, 31] and MLaaS platforms [15] use a sampling method [34] to compute the approximate Shapley values: let $\\rho_{mi}$ be the range of the marginal contributions of feature i, and $\\{P_N^k \\}^v_{k=1}$ be v permutations of N. For each permutation $P_N$, $S_i^{P_N}$ denotes the set of features that precede i. By setting $v \\ge \\frac{\\ln(\\frac{n}{\\delta})\\rho_{mi}^2}{2\\epsilon^2}$, we can compute the approximate Shapley values $\\hat{s_i}$ by Eq. 2 such that\n$\\Pr(\\vert\\hat{s_i} - s_i\\vert \\ge \\epsilon) \\le \\delta.$ (3)\nHere $\\epsilon > 0$ denotes the estimation error."}, {"title": "3 PROBLEM STATEMENT", "content": "System Model. In this paper, we consider a system model in which a commercial institution trains a black-box model f based on a sensitive dataset Xtrain and deploy it on an ML-as-a-service (MLaaS) [57] platform, such as Google Cloud [15] and Microsoft Azure [8]. Users can access the model based on a pay-per-query pattern [64]. Specifically, a user can send a private sample $x = \\{x_i\\}^n_{i=1}$ to the service provider and obtain a prediction vector $\\hat{y} = \\{\\hat{y_i}\\}^c_{i=1}$ with an explanation vector $s = \\{s_i\\}^n_{i=1}$ w.r.t. a specific class, as shown in Fig. 1. Note that the service provider can also return c explanation vectors, each of which corresponds to a class, but returning one vector s w.r.t. a predefined class is the default setting in related studies [51] and MLaaS platforms, e.g., Google Cloud [15]. Thus, for practicability and without loss of generality, we consider one explanation vector w.r.t. a specific class in this paper. We summarize the frequently used notations in Tab. 1 for reference.\nAttack Model. The previous studies in ML privacy [36, 40, 47, 51] assume that the adversary can collect an auxiliary dataset Xaux which follows the same underlying distribution of the target sample x. For example, a bank can synthesize a real-world dataset after obtaining the feature names on the explanation report from its competitors. In this paper, we study two adversaries by relaxing the assumptions of previous studies. The first adversary follows the setting in [36, 40, 47, 51], i.e., the attacker aims to infer the private sample x based on the associated explanation vector s, an auxiliary dataset Xaux and black-box access to the prediction model f on the MLaaS platforms: $x = A_1(s, Xaux, f)$, where $x$ denotes the reconstructed values of x. For the second adversary, we use a more restricted but practical setting that the attacker has only black-box access to the ML services, and no background knowledge about the target sample x is available: $x = A_2(s, f)$."}, {"title": "4 FEATURE INFERENCE ATTACKS BASED ON SHAPLEY VALUES", "content": "In this section, we present the proposed feature inference attacks under different settings. Based on an auxiliary dataset, the first adversary can first learn a regression model with empirical risk minimization, then map the explanation vector s into the private feature space and accordingly obtain the estimation of x. With only black-box access to the MLaaS platform, the second adversary can accurately reconstruct the important features of x by exploiting the linearity of local model decision boundaries."}, {"title": "4.1 Adversary 1: Feature Inference Attack with an Auxiliary Dataset", "content": "4.1.1 Motivation. Given an input sample $x = \\{x_i\\}^n_{i=1}$, a reference sample $x^o$ and the access to a black-box model f, we can simplify the computation of Shapley value in Eq. 2 as follows:\n$s_i = h(x_i; x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n, x^o) + \\epsilon,$, (4)\nwhere $h = g \\circ f$ is the composition of the black-box model f and a linear transformation (i.e., the mathematical expectation) g on f. $\\epsilon$ denotes the random noise caused by the sampling phase (Eq. 3).\nNow we characterize the dependence between $s_i$ and $x_i$ via a popular information metric called Mutual Information [13, 14]:\n$I(x_i; s_i) = \\int ds_i \\int dx_i P(x_i, s_i) \\left[ \\log \\frac{P(x_i, s_i)}{P(x_i)P(s_i)} \\right]$ (5)\nTo reach a perfect security level of Eq. 4, we need to make $I(x_i; s_i) = 0$ such that the adversary can not obtain any useful information of $x_i$ by observing $s_i$ [68].\nFor simplicity, we denote the variance and mean of a variable x by $\\sigma_x^2$ and $\\mu_x$, respectively. We assume the noise $\\epsilon$ caused by random sampling and the Shapley value $s_i$ computed by Eq. 4 follow Gaussian distributions with mean 0 and some variances\u00b2 because of the central limit theorem [5, 16, 34], thus\n$P(s_i) = \\frac{1}{\\sqrt{2\\pi \\sigma_{s_i}^2}} \\exp{-\\frac{s_i^2}{2\\sigma_{s_i}^2}}.$ (6)\nNote that because $\\epsilon$ is Gaussian noise and $\\epsilon = s_i - h(x_i)$, then given $x_i$, $s_i$ also follows a Gaussian distribution $s_i | x_i \\sim N(\\mu_{s_i | x_i}, \\sigma_{s_i | x_i}^2)$, where $\\mu_{s_i | x_i} = h(x_i)$ and $\\sigma_{s_i | x_i}^2 = \\sigma_\\epsilon^2$. Specifically,\n$P(s_i | x_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma_{\\epsilon}} \\exp{-\\frac{(s_i - h(x_i))^2}{2\\sigma_\\epsilon^2}}.$ (7)\nThen we can rewrite Eq. 5 as follows:\n$I(x_i; s_i) = \\int ds_i \\int dx_i P(x_i, s_i) \\log \\frac{P(s_i | x_i)}{P(s_i)}$ (8)\n$= -E\\left[ \\ln \\frac{1}{\\sqrt{2\\pi}\\sigma_{\\epsilon}} \\exp{-\\frac{(s_i - h(x_i))^2}{2\\sigma_\\epsilon^2}} \\right] + E\\left[ \\ln \\frac{1}{\\sqrt{2\\pi}\\sigma_{s_i}} \\exp{-\\frac{s_i^2}{2\\sigma_{s_i}^2}} \\right]$ (9)\n$= -\\frac{1}{2}\\left[ \\ln{(2\\pi)} + \\ln{\\sigma_{\\epsilon}^2} + 1 \\right] + \\frac{1}{2}\\left[ \\ln{(2\\pi)} + \\ln{\\sigma_{s_i}^2} + 1 \\right]$ (10)\n$= -\\frac{1}{2} \\log \\frac{\\sigma_{\\epsilon}^2}{\\sigma_{s_i}^2}.$ (11)\nwhere Eq. 10 follows because for the second term of Eq. 9, $\\epsilon = s_i - h(x_i)$ and $E(\\epsilon^2) = \\sigma_\\epsilon^2 + \\mu_\\epsilon^2 = \\sigma_\\epsilon^2$, making the numerator cancel with the denominator, and so does for the third term."}, {"title": "4.1.2 The Attack with Auxiliary Datasets.", "content": "Previous studies, such as membership inference [47, 51] and property inference [24, 36], assume that the adversary owns an auxiliary dataset $X_{aux}$ following the same underlying distribution of the target samples. This assumption is reasonable because the competing companies own datasets that have the same features but different customer bases [29]. The users of the MLaaS platforms can also collect a dataset $X_{aux}$ by collusion [38]. In this section, we follow this assumption and show that the adversary can reconstruct the target samples by training a regression model on $X_{aux}$.\nBased on the analysis in Section 4.1.1, we know that the explanation vector s is related to the private sample x. Although the intensity of the noise $\\epsilon$ is unknown, the adversary can still learn a model to reconstruct x by minimizing the noise component. Specifically, the adversary can send prediction queries for all $x^{aux} \\in X_{aux}$ to the MLaaS platform and obtain the corresponding explanation set $S_{aux}$. Now we rewrite Eq. 4 as\n$s = \\phi(x; x^o).$ (12)\nThe task becomes learning a hypothesis $\\psi: S_{aux} \\rightarrow X_{aux}$ that serves as the attack model. Here, $X_{aux} \\subseteq \\mathbb{R}^n$ represents a real-world dataset with finite cardinality. Additionally, both x and s are real-valued vectors, and we have $|X_{aux}| = |S_{aux}|$. The key observation is that our experiments exhibit a one-to-one correspondence between $X_{aux}$ and $S_{aux}$. This correspondence enables us to learn a hypothesis $\\psi: S_{aux} \\rightarrow X_{aux}$ to model the relations between input samples and Shapley values. We can then use $\\psi$ to produce the target x from an unknown s. Note that collisions may occur during the generation of $S_{aux}$ from $X_{aux}$; that is, multiple input samples might result in the same Shapley vector. However, such collisions are infrequent and have not been observed in our experiments to date. When collisions do occur, we can address them by removing the affected (x, s) pairs before learning the hypothesis $\\psi$.\nIt is also important to note that learning a hypothesis $\\psi: S_{aux} \\rightarrow X_{aux}$ does not imply that the function $\\phi: X_{aux} \\rightarrow S_{aux}$ is invertible. The invertibility of f depends on the properties of the underlying models, which is not guaranteed for most model types."}, {"title": "4.2 Adversary 2: Feature Inference Attack with Data Independence", "content": "4.2.1 Motivation. Suppose the adversary has no auxiliary datasets $X_{aux}$, it will be impossible for the adversary to learn an attack model as discussed in Section 4.1 because the distributions of the target private features are unknown. A naive way to restore x from s is that the adversary generates a random dataset $X_{rand}$, sends the samples in this dataset to the MLaaS platform separately and obtains the corresponding explanations $S_{rand}$. Provided that the size of the random dataset is large enough, there will be some $s_{rand} \\in S_{rand}$ such that $||s_{rand} - s||_2 \\le \\xi$ where $\\xi$ is a small threshold. The adversary can use the $x_{rand}$ associated with $s_{rand}$ as the estimation of x. But there are two problems in this method. First is that to improve the estimating accuracy, the adversary needs to randomly draw a large number of samples, e.g., $|X_{rand}| > 10000$, from the possible feature space and send them to the MLaaS platform, which can be costly on the pay-per-query service and increase the risk of being detected and blocked by the service provider. Second is that although $||s_{rand} - s||_2 \\le \\xi$ holds, the corresponding $x_{rand}$ can be a false positive case in which $||x_{rand} - x||_2$ is large.\nIn the experiments, we observe that the level of correlations between a feature $x_i$ and the model output $\\hat{y}$ is closely consistent with the correlations between $x_i$ and its explanation $s_i$. Fig. 3 shows an example. First, we train a neural network on a real-world Diabetes dataset [55] and deploy it to Google Cloud [15]. Then, we send queries of $x = \\{x_i\\}^n_{i=1}$ to the deployed model and obtain the model outputs $\\hat{y}$ and explanations $s = \\{s_i\\}^n_{i=1}$. We compute the Pearson correlation coefficients\u00b3 between the first seven features and $\\hat{y}$ as"}, {"title": "4.2.2 The Attack with A Random Dataset", "content": "In this attack, because the adversary has no information on the data distribution and feature interactions, different features in the random dataset $X_{rand}$ have to be drawn independently, i.e., the features are independent of each other in $X_{rand}$, which means that the features of x need to be restored separately from s. To achieve this task, we introduce a generalized additive model (GAM) [35] to approximate the black-box model f:\n$f(x) = f_1(x_1) + f_2(x_2) + \\cdot\\cdot\\cdot + f_n(x_n) + \\epsilon_f,$ (14)\nwhere $\\epsilon_f$ denotes the error caused by GAM approximation, and $f_i$ denotes an unknown smooth univariate function. Note that GAM can theoretically minimize the Kullback-Leibler distance to the original function [35] and has been used for analyzing the data flow in neural networks [1, 66] and random forests [27]. Replace Eq. 14 into Eq. 2, we have\n$s_i = f_i(x_i) - f_i(x^o_i) + \\epsilon_s,$ (15)\nwhere $f_i(x^o_i)$ is a constant, and $\\epsilon_s$ denotes the sampling error. If $s_i$ is linearly correlated with $x_i$ (i.e., $f_i$ is monotonic around $x_i$), we can first find a set of $\\{(x^{rand}_j)\\}_{j=1}^k$ from the random dataset $X_{rand}$ with $|(s^{rand}_j) - s_i| \\le \\xi$ then estimate $x_i$ by $x_i = \\frac{1}{k}\\sum_{j=1}^{k}(x^{rand}_j)$.\nBounding the estimation error. Now we theoretically analyze the estimation error of the interpolation-based attack method. Since the features are reconstructed independently, it suffices to focus on only one feature in the target Shapley values s, say $s_i$. Let $x_i$ be the private feature associated with $s_i$, $X$ be a random variable whose Shapley value $s_x$ is close enough to $s_i$, i.e., $|s_x - s_i| \\le \\xi$, where $\\xi$ is"}, {"title": "4.3 Attack Generalization", "content": "It is worth noting that our attacks are designed based on the inherent limitations of Shapley values. Specifically, we can simplify the computation of Shapley values into three steps: first use an affine transformation to generate a feature replacement dataset from x and $x^o$, then feed the dataset into the target model, and finally use another affine transformation on the model outputs to obtain Shapley values. Because the mapping from private features to Shapley values is fixed, we can then propose the inverse mapping attack (attack 1) and the GAM transformation-based attack (attack 2) to reconstruct the inputs from their explanations.\nOn the contrary, other popular explanation methods, e.g., LIME [44] and DeepLIFT [53], focus on approximating the linear decision boundary around the target features via a set of heuristic rules, where the mappings between features and explanations among different samples are unstable and various [4], leading to little convergence of attack 1 and invalidating the GAM transformation and error bounds of attack 2. For justification, we compared the performance of attack 1 on Shapley values, LIME, and DeepLIFT tested on the Google Cloud platform [15], where the testing model and dataset are NN and Adult [19]. Note that attack 2 is not applicable to other explanation methods because the GAM transformation is based on the computational formula of Shapley values. For attack 1, the averaging l\u2081 error of feature reconstruction tested on Shapley values is 0.0768, which is far better than 0.1878 on LIME, 0.1527 on DeepLIFT, and 0.1933 on the empirical random guess baseline (see Section 5.1). The conclusion is that the proposed attacks are targeted on Shapley values, and specific optimizations are needed to apply them to other heuristic explanation methods. We leave it as our feature work."}, {"title": "5 EXPERIMENTS", "content": "5.1 Experimental Setting\nThe proposed algorithms are implemented in Python with PyTorch 4, and all attack experiments are performed on a server equipped with Intel (R) Xeon (R) Gold 6240 CPU @ 2.60GHz\u00d772 and 376GB RAM, running Ubuntu 18.04 LTS.\nExplanation Platforms. To validate the effectiveness of the proposed attacks on real-world applications, we conduct experiments on three leading ML platforms: Google Cloud AI platform [15], Microsoft Azure Machine Learning [8] and IBM Research Trusted AI [2]. For the experiments on Google and Microsoft platforms, we first train models and deploy them in the cloud, then send prediction and explanation queries to the deployed models to obtain the corresponding explanation vectors. The experiments on the IBM platform are performed locally because a python package called AI Explainability 360 [2] is available. In addition, to test the attack performance w.r.t. different sampling errors, we implement a Vanilla Shapley sampling method [56], because the explanatory API provided by Google only supports up to 50 permutations for sampling computation (e.g., $v \\le 50$ in Section 2.2) while the APIs provided by Microsoft and IBM are based on SHAP [31] which fails to provide a theoretical error bound for Shapley approximation."}, {"title": "5.2 Attack Performance w.r.t. Different Numbers of Queries", "content": "In both attacks, the adversary sends queries to the MLaaS platforms in a pay-per-query pattern. If the number of queries needed is large, the cost for the adversary could be unacceptable. In this section, we evaluate the performance of attack 1 and 2 under different number of queries. The number of permutations used to compute Shapley sampling values is set to 50. We first vary the number of queries, i.e., the size of the auxiliary dataset $|X_{aux}|$ or the random dataset $|X_{rand}|$, in {100, 200, 400, 800, 1600}, then test the performance of attack 1 and 2, respectively. Fig. 5(a)-5(d) show the l\u2081 losses of attack 1. Fig. 5(e)-5(h) and Tab. 3 show the l\u2081 losses and success rates of attack 2. We have two observations from the results.\nFirst, with the increase of queries, the estimation errors of adversary 1 and 2 slightly decrease (Fig. 5(a)-5(h)), and the success rates of adversary 2 generally increase (Tab. 3). The reason is straightforward: in adversary 1, the attack model $\\psi$ can obtain a better generalization performance when the training dataset $X_{aux}$ becomes larger; for adversary 2, a finer sampling of random points can help produce smaller value ranges of the candidate estimations on the private features, thus reducing the estimation error bound"}, {"title": "5.3 Attack Performance w.r.t. Different MLaaS Platforms", "content": "In this section, we evaluate the proposed attacks on different MLaaS explanation platforms. Because the methods for computing Shapley sampling values among different platforms are different, we implement a Vanilla Shapley sampling method based on [56] for comparison. The number of permutations sampled for computing Shapley values is set to 50 in both the Vanilla method and Google Cloud platform. Note that this parameter is not supported in the Microsoft Azure and IBM aix360 platforms because their explanation methods are based on SHAP [31], which computes Shapley values via a heuristic regression method instead of sampling permutations. The numbers of queries in both of the adversaries are set to 1000. The Shapley sampling error $\\xi$ in adversary 2 (Algorithm 2) is empirically set to among all platforms for comparison. Fig. 6(a)-6(d) and 6(e)-6(h) show the performance of adversary 1 and 2, respectively. Tab. 4 shows the success rates of adversary 2 corresponding to Fig. 6(e)-6(h).\nFrom Fig. 6(a)-6(d) we observe that the attacks performed on Microsoft and IBM platforms achieve similar performance, which is expectable because these two platforms compute Shapley values via the same regression method. We also observe slight differences between the attacks performed on Vanilla and Google platforms although both of their implementations are based on the same Shapley sampling method [56]. Note that for efficiency, the maximum number of sampling permutations in Google Cloud is fixed to 50, which greatly reduces the computation costs from $O(2^n)$ to $O(50)$, meanwhile producing relatively large sampling error. As discussed in Section 4.1, a large error can override the information of private features in the Shapley values, leading to unstable reconstructions (see Eq. 11). Nevertheless, the proposed attacks can still reconstruct the private features with high accuracy.\nFrom Tab. 4, we observe that the success rates of adversary 2 performed on IBM and Microsoft platforms are lower than the success rates on the Vanilla method and Google Cloud. Considering that SHAP [31] computes Shapley values heuristically without providing a theoretical approximation bound, the real sampling"}, {"title": "5.4 Attack Performance w.r.t. Different Feature Importance", "content": "As discussed in Section 4.2, the important features can be more accurately reconstructed than the less important features. In this section, instead of averaging the reconstruction loss over all features, we give the attack loss per feature and dissect the connections between attack accuracies and feature importance.\nThree synthesis datasets with 12 features are used for the experiments (see Tab. 2 for the details). To generate the features of different importance, we first randomly draw five point clusters around five vertices of a three-dimension cube and label each cluster with a unique class. The three features of these random three-dimensional points are called the key features. After that, we generate $n_r$ redundant features by randomly and linearly combining the key features. The rest $12-3-n_r$ features are generated from random noises. The key features and redundant features are called important features. We vary the percentages of important features in {25%, 50%, 75%} and accordingly generate three synthesis datasets. In addition, two real-world datasets, Credit and Diabetes, are also used for justification. The importance of feature $x_i$ is defined as the Mean Absolute Correlation Coefficients (MACC) between $x_i$ and model outputs $\\hat{y}$:\n$MACC(x_i, y) = \\frac{1}{c} \\sum_{j=1}^{c} |\\rho(x_i, \\hat{y_j})|,$ (21)\nwhere $\\hat{y} = \\{\\hat{y_j}\\}_{j=1}^c$ denotes the model outputs and $\\rho$ is the Pearson correlation coefficient [27, 62]. Note that MACC measures the averaging linear correlation between $x_i$ and $\\hat{y}$. A larger MACC indicates that the change of $x_i$ can produce larger variations in $\\hat{y}$."}, {"title": "5.5 Attack Performance w.r.t. Different Shapley Sampling Errors", "content": "As discussed in Section 4.1 and 4.2, the proposed attacks can be applied to the explanation methods with unknown sampling errors. To evaluate the robustness of the proposed algorithms, we first vary the Shapley sampling errors $\\epsilon_s$ in {r/5,r/10, r/20, 0} and accordingly generate Shapley values with different errors via the Vanilla method, then test the attack performance on these explanations. The $\\delta$ in Eq. 3 is set to 0.1. The test model is GBDT. We empirically set $\\xi$ in Algorithm 2 to 5 in all experiments, which corresponds to the real-world scenario that the adversary has no information about the sampling error. The numbers of queries in both attacks are set to 1000. The attack losses of adversary 1 and 2 on Credit are shown in Fig. 8(a), with the success rates {0.4685, 0.4689, 0.4758, 0.4824} of adversary 2. The losses on Diabetes are shown in Fig. 8(b), with the success rates {0.4730, 0.4921, 0.4955, 0.5057} of adversary 2.\nWe make two observations from the results. First, the reconstruction accuracies of the proposed attacks slightly improve as the sampling errors decrease. This is reasonable because with the decrease of random noises, the information of the private features contained in Shapley values will increase, which is beneficial to the reconstruction algorithms. But the improvement is slight, demonstrating the robustness and efficiency of the proposed attacks even under large sampling errors. For justification, only 37 random permutations of features are needed to achieve the sampling error of r/5, whereas 2\u207f permutations are needed to achieve zero sampling errors. Second, the losses of adversary 1 are slightly larger than the losses of adversary 2. The reason is that adversary 2 can only reconstruct roughly 50% important features, while adversary 1 can reconstruct all features, among which the less important features contain more noise and thus reduce the overall attack accuracies of adversary"}]}