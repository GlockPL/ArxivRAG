{"title": "M-LLM Based Video Frame Selection for Efficient Video Understanding", "authors": ["Kai Hu", "Feng Gao", "Xiaohan Nie", "Peng Zhou", "Son Tran", "Tal Neiman", "Son Tran", "Lingyun Wang", "Mubarak Shah", "Raffay Hamid", "Bing Yin", "Trishul Chilimbi"], "abstract": "Recent advances in Multi-Modal Large Language Models (M-LLMs) show promising results in video reasoning. Popular Multi-Modal Large Language Model (M-LLM) frameworks usually apply naive uniform sampling to reduce the number of video frames that are fed into an M-LLM, particularly for long context videos. However, it could lose crucial context in certain periods of a video, so that the downstream M-LLM may not have sufficient visual information to answer a question. To attack this pain point, we propose a light-weight M-LLM-based frame selection method that adaptively select frames that are more relevant to users' queries. In order to train the proposed frame selector, we introduce two supervision signals (i) Spatial signal, where single frame importance score by prompting a M-LLM; (ii) Temporal signal, in which multiple frames selection by prompting Large Language Model (LLM) using the captions of all frame candidates. The selected frames are then digested by a frozen downstream video M-LLM for visual reasoning and question answering. Empirical results show that the proposed M-LLM video frame selector improves the performances various downstream video Large Language Model (video-LLM) across medium (ActivityNet, NEXT-QA) and long (EgoSchema, LongVideoBench) context video question answering benchmarks.", "sections": [{"title": "1. Introduction", "content": "The rise of Large Language Model (LLM) has revolutionized numerous domains in Artificial Intelligence (AI) [3, 29, 36, 39]. In the past year, Multi-Modal Large Language Model (M-LLM) has significantly improved the performances of Vision-Language Models (VLM) to an unprecedented level in tasks such as image captioning and Visual Question Answering (VQA) [2, 4, 12, 41]. Video QA task requires a model to understand consecutive images along the time to answer a question, raises new challenges to the current M-LLM. One of most critical challenge is the length of the video context, where a model needs to comprehend all frames in a video.\nTo balance the trade-off between the capability of understand all video frames and the available context length in a specific M-LLM, conventional practices [48, 54] rely on uniform sampling of frames, wherein frames are extracted at pre-defined intervals regardless of their relevance to the specific questions. Although this method ensures the full coverage of a video in time-axis, it also introduces insufficient visual information, as many frames, which may be irrelevant or redundant are included, on the other some important frames are ignored. This dilutes the model's ability to focus on key events in the video and sometimes increases the computational cost. Generally in video QA [21, 52], some specific frames are more likely to contain information pertinent to the question, this one-size-fits-all approach limits both the performance and practicality of M-LLMs. The need for adaptive frame selection is becoming important, especially when working with long videos or resource-limited environments.\nTo address these limitations, a most straightforward idea is to select frames instead of uniform sampling [30, 50]. By focusing on the frames that helps answer the question most, we can significantly reduce the visual context that an M-LLM needs to be processed without sacrificing the quality of video understanding. Illustrated as an example in Figure 1, frames that contains the most informative visuals can help downstream model to answer the question. To implement the above idea, we propose a light weight frame selector that employs fine-tuned version of an LLM. It makes use of M-LLM's capability of multi-modality understanding to effectively capture the relationship between video frames and the question. We introduce two design choices to make the framework lightweight. First, we demonstrate that small LLMs are capable to understand complicated user questions. Second, we compress per video frame tokens to balance the long context trade-off.\nAlthough video question-answering tasks often require a large number of tokens per frame to capture content details, we hypothesize that determining frame importance does not require excessive tokens. Instead of leveraging all visual tokens of the frames, we apply an aggressive pooling-based token reduction on each video frame, which significantly improves the computational efficiency and increases the number of frames that the LLM selector can ingest.\nWe also propose a method to train our LLM-based video frame selector. Given the fact that there are very limited resources of well-maintained video frame selection datasets for video QA, we are not able to simply apply supervised training. To overcome this limitation, we use two pseudo-labeling strategies to estimate the importance of each frame in a video. First is the spatial understanding, where we use a video QA question to prompt a well-trained M-LLM, which generates the importance score for each frame accordingly. However, the context length of an M-LLM is limited, it impossible to jointly estimate the importance scores of all frames from the temporal perspective. We then take advantage of an LLM to select top-k relevant frames in a video using their captions. Using captions rather than visual tokens, an LLM is able to understand a larger number of frames at a time. We combine these two approaches to obtain pseudo-labels indicating frame importance in given a specific question, enabling effective training of the frame selector.\nWithout fine-tuning the downstream M-LLM for question answering, our frame selector enhances the video QA performance by reducing the noise introduced by irrelevant frames and sharpens the model's focus on pertinent video segments. Our proposed approach shows significant improvements with various popular M-LLMs in video question answering tasks across short, medium (ActivityNet, NEXT-QA) and long (EgoSchema) context video QA benchmarks. To summarize, our contributions are three folds:\n\u2022 We propose a lightweight M-LLM-based adaptive video frame selector to for both efficient and stronger video QA performances of M-LLMs.\n\u2022 We propose spatial and temporal pseudo-labeling to generate importance scores for video frame select training.\n\u2022 Our proposed method is plug-and-play friendly. We demonstrates comprehensive video QA improvements across popular M-LLMs with further fine-tuning."}, {"title": "2. Related Work", "content": "Multi-Modal Large Language Model (M-LLM) As Large Language Model (LLM) continue to demonstrate impressive abilities in language comprehension and reasoning [1, 3, 36, 39], interest is growing within the computer vision community to explore their potential for handling multi-modal inputs. Flamingo [2] demonstrates the capacity to process image and text inputs for a wide range of multi-modal tasks. BLIP-2 [13] introduces a Q-Former to map learned image features into the text embedding space of LLMs, while LLaVA [20] employs simple Multi Layer Perceptron (MLP) projector to align visual and textual features. Further research has focused on best practices for M-LLM, including areas such as dynamic high-resolution [6, 21], instruction-tuning data [15, 23], and different visual encoders [38, 44]. MM1 [28] and Idefics2 [11] provide comprehensive ablation studies on the design space of M-LLM.\nVideo M-LLMs As image-based M-LLM become more mature, research naturally extends to the video modality. Video-ChatGPT [26] and Valley [25] use pooling over video features to generate compact visual tokens for downstream M-LLM. Video-LLaVA [18] aligns images and videos before projection, allowing LLM to learn from a unified visual representation. Video-Teller [19] points out the importance of modality alignment in pre-training. PLLaVA [48] studies how different pooling of visual features affects downstream video question answering performance. Regarding long video inputs, LLaMA-VID [17] represents each frame with two tokens to reduce the overload of long videos while preserving critical information. MovieChat [34] propose an effective memory management mechanism to reduce the computation complexity and memory cost, enabling very long video with over 10K frames understanding. Weng et al. [45] proposes to extract video representations as sequences of short-term local features, and integrate global semantics into each short-term segment feature.\nVideo Frame Selection Before the rise of video M-LLM, language-aware video key-frame selection and localization had already attracted great interest [7, 8, 22, 43]. [5] optimized an end-to-end pipeline that uses ground truth question-"}, {"title": "3. Method", "content": "This section introduces our video frame selector designed for efficient video-LLM QA. Section 3.1 discusses the motivation behind frame selection. Section 3.2 outlines the design details of the frame selector. Section 3.3 explains the generation of pseudo labels for training the frame selector. Section 3.4 describes the training process of the frame selector.\n3.1. Rethinking Uniform Sampling in Video LLMs\nA typical framework for video LLM An n-frame framework is widely adopted in existing research on video M-LLM [14, 48, 54]. The number of frames in the input video, denoted by T, is variable. For example, a 3-minute video at 30 frames per second (FPS) contains T = 5400 frames. The n-frame framework uniformly samples a fixed number of frames, [x1, x2,\u00b7\u00b7\u00b7, xn], from the total T frames, where each frame xi \u2208 RH\u00d7W\u00d73, with H \u00d7 W representing the frame resolution, and typically n < T. A pre-trained visual encoder fv extracts visual features from n frames. These features are subsequently projected into the LLM's space using an alignment projector ga and then flattened. Spatial pooling may also be applied to reduce the number of output tokens:\nh1 = AvgPooling(ga(fv(xi))), hi \u2208 Rm\u00d7d (1)\nwhere m is the number of tokens to represent a frame and d is the hidden dimension of the LLM. Let Q\u2208 Rlxd denote the input embedding of the input text question. The n-frame framework generates a response r as following:\nr = LLM(h1,\u2026\u2026, hn, Q). (2)\nUniform sampling is not optimal In the n-frame framework, as shown in Figure 2 (a), the input video is represented by n \u00d7 m tokens where m is the number of visual tokens per frame. For example, in LLaVA-NeXT-Video [54], where n = 32 and m = 12 \u00d7 12, this results in 4608 tokens. To reduce the computational cost of LLM inference, previous work has either chooses to reduce n with sliding"}, {"title": "3.2. Design of the Frame Selector", "content": "An ideal frame selector should be able to understand complex questions and analyze temporal relationships from the video input. To achieve this, we fine-tune an LLM to function as the frame selector. The frame selector utilizes the base LLM's strong language comprehension and reasoning abilities to identify key frames in a video.\nSimilar to existing video M-LLM, our M-LLM-based frame selection method takes as inputs n sampled video frames along with the corresponding text-based question. Instead of generating an answer to the question, the frame selector identifies the most relevant frames for answering the question. Specifically, we make use of well-trained decoder only LLM as the frame selector to output an n-dimensional vector s as follows:\ns = FrameSelector(x1,\uff65\uff65\uff65, xn, Q) \u2208 Rn (3)\nwhere the ith element in the vector s indicates the importance score of the ith input frame. To achieve this, we append a learnable score query q \u2208 R1\u00d7d to the end of all input tokens and use the concatenation of the visual tokens, text tokens and the score token as the LLM as input:\ne1,..., en, e, eq = LLM(x1,\uff65\uff65\uff65,Xn, Q, qscore) (4)\nwhere ei and eq denote the intermediate output of xi and qscore from the penultimate transformer block. Because the nature of causal attention, eq aggregates information from all visual and text tokens. We then employ an MLP to exact frame importance information from the intermediate output of the score query from the penultimate transformer block:\ns = MLP(eq), s \u2208Rn (5)\nFigure 2 (b) presents an overview of the proposed architecture for the M-LLM-based frame selector. Instead of generating tokens that represents the frame selection, we append a learnable query vector at the end of the input sequence and supervisedly learn this query token. The hidden vector of this query token, eq, is then served as the input to generate the n-dimensional importance vector s.\nSelect frames from the importance score After obtaining per-frame importance scores, we need to sample k frames for downstream video question-answering via a video M-LLM. Naively selecting the top k frames with the highest importance scores is suboptimal because neighboring frames within short time intervals often have similar scores. For example, if frame i has the highest importance score, the i + 1 or i - 1 frames usually have a closely high scores. The adjacent frame adds little additional information if frame i is already selected. To address this issue, we use a greedy algorithm combined with non-maximum suppression (NMS) to select the most informative frames. The \u201cGreedy\" approach involves sequentially selecting the top k frames, without replacement, by choosing the frame with the highest importance score from the remaining set of frames. With \u201cNMS\u201d, once a frame is selected, its neighboring frames are excluded as they contain similar information. The \u201cNMS-Greedy\" procedure is detailed in Algorithm 1. In practice, when selecting k frames from n frames, frame j is considered a neighboring frame of frame i if |i \u2013 j| < (n/4k).\nEfficiency of the frame selector As discussed in Section 3.1, dense uniform sampling is inefficient for video M-LLM. However, we still employ dense uniform sampling for the frame selector.\nAlthough dense uniform sampling is inefficient for video M-LLM, we keep using it at the beginning to maximally preserve the video information. To overcome the drawback of dense uniform sampling, we apply spatial pooling to reduce the token count per video frame before feeding the frames into the selector. In particular, the spatial pooling reduce the number of visual token to a smaller value, e.g., m = 3 \u00d73 (9 tokens), which is substantially fewer than the tokens per frame used in existing video LLMs, e.g., m = 12 \u00d7 12 (144 tokens). This design follows such an assumption: for video question answering, the model requires a substantial number of tokens per frame to capture visual details, but far fewer tokens are needed to determine whether a frame is important. A rough outline of the frame is sufficient. Our empirical results Table 6 also justify this assumption."}, {"title": "3.3. Pseudo Labels for the Frame Selector", "content": "To train the frame selector, we need supervision signals for the output importance scores s \u2208 Rn. Unfortunately, there is no existing dataset to label the frame level importance score for video QA, we propose two methods to generate pseudo labels for training the frame selector.\nSpatial Pseudo Labels Previous works use M-LLMs to evaluate whether a video frame is relevant to a question [32, 50]. A common practice is to prompt an M-LLM and ask if the video frame provides useful information to answer the question. The relevant score of the frame is the probability that the M-LLM generates the \"Yes\" token. In our experiments, we observe that this method does not always provide a reasonable estimation. Even if the model agrees the frame is relevant, the M-LLM may not generate the \"Yes\" token but other expressions depending on its text generation style. To address this issue, we apply chain-of-thoughts (CoT), asking the M-LLM to explain first then generate a Boolean evaluation (check appendix for the detailed prompt). This prompts allows the M-LLM to improve the evaluation quality by extra inference time reasoning. An ideal M-LLM should adhere to the instruction by generating either \u201cTrue\u201d or \u201cFalse\u201d. In a few cases the model may fail to follow the instruction, and we manually append the text \u201cEvaluation: True\" to the end of the generated response. Thus we can always compute the probabilities of generating \u201cTrue\u201d and \u201cFalse\", denoted as PTrue and PFalse respectively. The importance score of the input frame is determined by\ns = PTrue/(PTrue + PFalse) (6)\nIn our experimental setup, we uniformly sample n = 128 frames from the video and obtain the spatial pseudo labels for each frame independently. Let si denote the score for the ith frame, we normalize the score vector as si/maxj sj.\nTemporal Pseudo Labels A significant limitation of single-frame evaluation is the lack of temporal reasoning. For example, considering the question: \"What did the man do after picking up his hat?\", the video content following the action of picking up the hat is crucial for answering this question. However, when generating the spatial pseudo labels, it only consider one frame without taking care of the temporal context. Therefore, the spatial labels don't know what occur after the action.\nTo address this issue, we propose the temporal pseudo labeling. Since most of the publicly available M-LLMs are not able to consume a large number of image tokens, we alter to take advantages of the frame captions and use a LLM to reason over all captions. Specifically, we first obtain detail captions of all n frames by prompting a M-LLM. Second, we feed the captions of all frames together as well as the question to a strong text-only LLM. Then the LLM can temporally reason the helpfulness of all frames.\nWe find it challenging to generating floating-point scores for an extensive list of frames for an LLM. Consequently, we ask the model to produce a list of the index of most helpful frames (see appendix for the detailed prompt). Frames included in this list are assigned a score of 1, while those excluded receive a score of 0.\nWhile temporal pseudo labels can capture temporal relations in videos, it may suffer from information loss and model hallucination due to its two-stage evaluation process. Therefore, we combine two methods into the final pseudo-labels by averaging the scores obtained from spatial and temporal pseudo labels."}, {"title": "3.4. Training of the Frame Selector", "content": "We consider a two-stage instruction-tuning procedure. In stage 1, we freeze the pre-trained vision and LLM backbones and train the parameters of the alignment projector"}, {"title": "4. Experiments", "content": "We begin by outlining our experimental setup in Section 4.1. Then we demonstrate that our frame selector improves the performance of well-trained M-LLMs without changing their parameters by selecting better frames in Section 4.2. We also conduct ablation studies to demonstrate the effectiveness and efficiency of our frame selection framework and results are in Section 4.3. At last, we showcase some qualitative examples of frames selected from the video according to the question in Section 4.4.\n4.1. Experiment Setup\nTraining data We compile the training dataset from three sources: 1) 800K data from VideoChat2 [16], 2) 125K data from the TimeIT dataset [33], and 3) 178K data from the LLaVA-Video-178K dataset [56]. For the visual instruction tuning task, we use the entire training dataset. For the importance score prediction task, we use 400K video QA data where the video length exceeds 5 seconds.\nPseudo label generation we utilize Qwen2-VL-7B [41] to generate importance scores for each frame in the spatial pseudo labeling and concise captions for all frames in the temporal pseudo labeling. We use GPT-40 mini to propose the most helpful frames given the concise captions in the multi-frame evaluation.\nEvaluation benchmarks Since our framework selects frames for video question answering, we evaluated its performance on benchmarks consisting of relatively longer videos, including open-ended video QA on ActivityNet-QA [51], and multi-choice QA on NEXT-QA [47] and"}, {"title": "4.2. Comparison with SOTA Video-LLMs", "content": "We choose two strong video M-LLMS, PLLaVA [48] and LLaVA-NeXT-video [55] and two (multi-)image based M-LLM Idefics [11] and Qwen2-VL [41], to be the baselines to illustrate how our frame selector enhances the video question-answering (QA) performance of these models. For each model, we compare the performance of using uniformly sampled frames as inputs versus using frames selected by our frame selector. The number of frames seen by the video M-LLM is the same during the comparison.\nTable 1, Table 2 and Table 3 present the comparison on ActivityNet-QA [51], NEXT-QA [47] and EgoSchema [27] respectively. Following existing practice [26], performance on ActivityNet-QA is measured as \u201caccuracy/correctness\u201d metrics, and both metrics are evaluated using GPT-3.5, with higher values indicating better performance. Performances on NEXT-QA and EgoSchema are the accuracy of of multi-choice questions where each question has 5 options. We"}, {"title": "4.3. Ablation Studies", "content": "We perform ablation studies on several components of the frame selection framework. First, we analyze the performance of the following frame selection methods on the video QA task:\n\u2022 Uniform sampling: the default uniform sampling\n\u2022 Pseudo labels from CLIP similarity: define the importance score of a frame as the image-text similarity between the frame and the text question, computed using a CLIP model. Then sample frames using Algorithm 1.\n\u2022 Pseudo labels from SeViLA: define the importance score of a frame as in SeViLA [50] to sample frames.\n\u2022 Spatial pseudo labels: define the importance score using the spatial pseudo labels only.\n\u2022 Spatial & temporal pseudo: define the importance score as the average of the spatial and temporal pseudo labels."}, {"title": "4.4. Visualization of selected frames", "content": "Figure 4 presents two examples of selected frames from the video conditioned the question. Each question involves two events and thus requires temporal reasoning. Estimating the importance of a single frame is challenging without reference to prior or subsequent frames. The frame selector effectively identifies frames containing the answers to the questions."}, {"title": "5. Conclusion", "content": "In this paper, we propose a lightweight M-LLM-based frame selector to improve both performances and efficiency in video QA. This selector is question-aware and takes dense video frames and the question as input, selecting the most relevant frames for answering the question. We can then use any multi-image or video M-LLMs with the selected frames to complete the question answering. To train the frame selector, we introduce spatial and temporal pseudo-labeling due to the limited public annotations for video frame-level importance. Our experiments on two medium-length video QA benchmarks (ActivityNet QA and NExt-QA) and two long-video QA benchmarks (EgoSchema and LongVideoBench) demonstrate the effectiveness of our proposed method."}]}