{"title": "LIBMOE: A LIBRARY FOR COMPREHENSIVE BENCHMARKING MIXTURE OF EXPERTS IN LARGE LANGUAGE MODELS", "authors": ["Nam V. Nguyen", "Thong T. Doan", "Luong Tran", "Van Nguyen", "Quang Pham"], "abstract": "Mixture of Experts (MoEs) plays an important role in the development of more efficient and effective large language models (LLMs). Due to the enormous resource requirements, studying large scale MoE algorithms remain in-accessible to many researchers. This work develops LibMoE, a comprehensive and modular framework to streamline the research, training, and evaluation of MoE algorithms. Built upon three core principles: (i) modular design, (ii) efficient training; (iii) comprehensive evaluation, LibMoE brings MoE in LLMs more accessible to a wide range of researchers by standardizing the training and evaluation pipelines. Using LibMoE, we extensively benchmarked five state-of-the-art MoE algorithms over three different LLMs and 11 datasets under the zero-shot setting. The results show that despite the unique characteristics, all MoE algorithms perform roughly similar when averaged across a wide range of tasks. With the modular design and extensive evaluation, we believe LibMoE will be invaluable for researchers to make meaningful progress towards the next generation of MoE and LLMs. Project page: https://fsoft-aic.github.io/fsoft-LibMoE.github.io.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent years have witnessed a tremendous success of Mixture-of-Experts (MoE) and its sparse variant (MoE) in facilitating the training of large language models (LLMs) (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Dai et al., 2024; Jiang et al., 2024; Abdin et al., 2024) and multimodal LLMs (Du et al., 2022; Chen et al., 2024b; Xue et al., 2024; Li et al., 2024b; Han et al., 2024). By only activating a subset of parameters per input, MoE greatly improves the training efficiency, and allows researchers to train models with hundreds of billions param-eters with low computational overhead. Interestingly, MoE algorithms often achieved superior performances compared to their dense counterpart (Shazeer et al., 2017), which has gather an increased interest in the community to develop more advanced algorithms and better theories to understand their behaviour (Xue et al., 2024; Chen et al., 2024a). Thus, MoE and SMoE have becoming one of the most prominent strategy towards the next generation of intelligent machines.\nDespite the algorithmic advancements, training MoE mod-els is undeniably costly, making large-scale studies accessi-ble only to a few groups of researchers with an enormous amount of compute resources. For example, Muennighoff et al. (2024) needed 256 H100 GPUs to train OLMOE-1B-7B, and Lin et al. (2024) used 256 A100 GPUs for their experiments. On the other hand, a majority of researchers can only afford to test their ideas on small settings (Chi et al., 2022; Do et al., 2023; Pham et al., 2024; Nguyen et al., 2024a), or even synthetic datasets (Nguyen et al., 2024b;c). Such a discrepancy severely undermines the po-tential of MoE where it truly shines under the large scale training settings. Consequently, the most successful models in practice (Wei et al., 2024; Sukhbaatar et al., 2024; Xie et al., 2022; Dai et al., 2022) are still based on the original variant proposed in (Fedus et al., 2022) although there exists a plethora of advanced MoE algorithms (Zhou et al., 2022; Huang et al., 2024; Do et al., 2023; Pham et al., 2024), they introduce significant changes in the operational methods and gating mechanisms of MoE.\nThis work aims at providing a streamlined toolkit to facili-tate the research of MoE and SMoE in LLMs. To this end, we carefully develop LibMoE to support distributed training and comprehensive evaluations of various MoE algorithms. With a modular design, LibMoE provides not only a com-prehensive evaluation over a wide range of zero-shot tasks, but also a full customization of MoE algorithms, including sparsity, expert-router interactions, balancing losses, etc. Furthermore, we incorporate the state-of-the-art technique of sparse upcycling (Komatsuzaki et al., 2022) to skip the expensive costs of training from scratch, allowing MoE to be incorporated into any existing dense LLM checkpoints at an affordable compute. Notably, our full training pipeline can be completed within 55 hours using only 4 \u00d7 A100 GPUs while the MoE upcyling step can be finished within 32 hours only. Consequently, LibMoE offers an experiment setting that is affordable to a wide-range of researchers while the evaluation remains faithful and truly reflects the"}, {"title": "2 RELATED WORK", "content": "2.1 Mixture of Experts\nMoE is an important class of machine learning algorithms and has been extensively studied in the literature (Jacobs et al., 1991). Recently, its sparse variant, SMoE (Shazeer et al., 2017), has gathered much interests in the community thanks to its successful applications in scaling up LLMs (Ab-din et al., 2024; Jiang et al., 2024; Yang et al., 2024; Wei et al., 2024). Since then, extensive efforts have been devoted"}, {"title": "3 LIBMOE", "content": "3.1 Preliminary: Mixture of Experts\nThe core methodology of a Sparse Mixture-of-Experts (MoE) is using a gating network to activate different sub-sets of expert networks for different inputs. An MoE layer includes a set of N experts (E1, E2, ..., EN), where each expert is structurally identical to a standard FFN, and a gating network (G) designed to choose the top-k experts with the largest affinity scores. Building on the framework established by Shazeer et al. (2017), the mathematical rep-resentation of MoE layer can be expressed as:\n$$y = \\sum_{i=1}^{N} G_i(x)E_{i}(x),$$\n$${G(x) = TopK(\\sigma(xW_g + b) + R_e, k),}$$\n$${TopK(G(x), k) = \\begin{cases} G(x)_i, & \\text{if } G(x)_i \\text{ is the top k,} \\\\ 0, & \\text{otherwise.} \\end{cases}}$$\nWhere x is the input, N denotes the total number of experts, and K denotes the number of experts activated per sample. The gate model G(.) is a Noisy Top-K Network (Shazeer et al., 2017) with parameters $W_g$ and noise $R_e$. $G(x)_i$ denotes the weight value for the i-th expert, and $\\sigma(\u00b7)$ denotes a similarity measure, commonly implemented as a softmax function. This sparsity character ensures the conditional computation in the MoE layer that each token be assigned and computed only k experts.\n3.2 Training and Evaluation Pipelines\nA key challenge in training MoE models is obtaining a massive dataset and a large amount of compute. In this work, we propose to incorporate MoE training into any existing dense LLM checkpoints via the Sparse Upcycling technique (Komatsuzaki et al., 2022), which duplicates the original model to create experts and continue training them on a downstream dataset as a normal MoE. Consequently, we can bypass the expensive pre-training step and evaluate MoE algorithms with the most advanced public LLMs.\nTraining pipeline For training, we adopt the vision-language pre-training task, which is one of the more chal-lenging problem and only requires a rather small amount of data to start (around 1e9 tokens). To this end, we fol-low the CUMO framework (Li et al., 2024b) to upcycle the LLaVA model (Liu et al., 2023a), which consists of three modules: a pre-trained visual encoder, a pre-trained LLM, and a randomly initialized MLP connector. Training fol-lows a two-stage process. First, the dense training stage initializes the MLP connector and train it to connect the pre-trained visual encoder to the pre-trained LLM. Second, the MoE training stage upcyles the model to become MoE and also trains all components to obtain the visual instruction following capabilities, Figure 2 shows the model training process in detail. Importantly, we follow Li et al. (2024b) to only upcyle the MLP connector and the visual encoder since upcycling a dense LLM is found to be worse than just using an MoE LLM. Moreover, we highlight that the dense"}, {"title": "3.3 Design Principles", "content": "We design LibMoE to be modular and highly extensible. At a high level, LibMoE comprises three major modules. First, the MoE module implements the key MoE logics such as the router design, the balancing losses, or the expert-router inter-actions. Second, the training module optimization processes for all three training stages, which supports loading state-of-the-art LLMs, handling custom datasets, hyper-parameter configurations, the sparse upcyclying algorithms, and the main training loop. Lastly, the evaluation module is our custom implementation of LMMS-Eval to support almost 100 benchmarks and various metrics proposed in this work. Figure 1 depicts the design of LibMoE, showing its key modules and their interactions.\nOverall, LibMoE offers a modular, highly extendable im-plementation to study MoE. LibMoE empowers users to tailor and experiment with different MoE configurations, supporting rapid prototyping and fair evaluation of novel methodologies without the necessity for extensive reconfig-uration of the underlying framework. More importantly, by incorporating MoE to state-of-the-art LLMs, we ensure that future algorithms based on LibMoE will have high utility in real-world, large scale scenarios."}, {"title": "4 EXPERIMENTS", "content": "Stage\nPre-Training\nPre-FineTuning\nVIT (332K)\nVIT (665K)\n4.1 Experiment Settings\nTraining task and datasets We consider the vision-language pretraining task (Lu et al., 2019) and follow CUMO (Li et al., 2024b) to upcycle the LLava model (Liu et al., 2023a), allowing us to evaluate various MoE algo-"}, {"title": "4.2 Main Results", "content": "4.2.1 Performance Comparison\nTable 2 reports the performances of the MoE algorithms con-sidered, implemented by our LibMoE. We observe that there is not a clear winner consistently across all benchmarks. Al-though Hyper Router and Perturbed Cosine Router may perform slightly better in some settings, their improvements over the second best method is quite marginal. We argue that such a gap can happen because of the randomness in initialization, data shuffling, and will be diminished when increasing the amount of training data. Overall, using Lib-MoE to compare state-of-the-art MoE algorithms under a fair and large scale setting, our result agrees with the com-mon practice that the original SMoE strategy is still the most attractive choice thanks to its simplicity and scalability.\nGeneralization throughout training Next, we investi-gate the algorithms performances throughout training to get a more fine-grained understanding of their behaviors. To this end, we report the performance of the intermediate check-points (after each 10% of training data) in Figure 3. Due to space constrains, we only plot three representative bench-marks and the averaged of all 10 benchmarks in Figure 3 and provide the plot for all benchmarks in Figure 9, Ap-pendix A. Interestingly, we observe that the last checkpoint is often not the one with the best performance. This result shows that although early stopping is not extensively stud-ied in the literature, it can be beneficial for vision-language pre-training and could be a promising research direction.\n4.2.2 Expert Selection Analysis\nBeyond the empirical performance, we conduct extensive analyses to understand the considered algorithms' behaviors. Particularly, we are interested in how the experts are selected throughout training according the different algorithms. Our findings reveal that the five MoE algorithms demonstrate distinct characteristics in adapting to these conditions, of-fering a novel perspective on their dynamic behavior. In the following, we will investigate the expert selection patterns according to various scenarios.\n(a) Analyzing the Impact of Training Data Proportions on Expert Selection in MoE Algorithms\nIn this experiment, we examine how varying training data"}, {"title": "(b) Expert Responsiveness for Capacity Specialization on Subtasks", "content": "We investigate the behaviors of the expert selection mecha-nism by exploring how often each expert is selected in the MME benchmark.\nTo this end, we analyze the frequency of the expert selection across different subtasks to gain insights into the specializa-tion behavior of each expert. Given an MoE algorithm with N experts and L layers, the selection frequency of each ex-pert i at a given layer l is denoted as $freq_{i}^{(l)}$ (i = 1, 2, . . ., N and l = 1, 2, . . ., L). Note that this selection frequency is counted across all samples in the benchmark, in this case we choose to be MME. Then, the entropy $H^{(l)}$ at each layer l is calculated by integrating the probability of selecting expert i into Shannon's entropy formula as follows:\n$$H^{(l)} = - \\sum_{i=1}^{N} \\frac{freq_{i}^{(l)}}{\\sum_{i=1}^{N}freq_{i}^{(l)}} log_{2} \\frac{freq_{i}^{(l)}}{\\sum_{i=1}^{N}freq_{i}^{(l)}},$$\nwhere:\n\u2022 $freq_{i}^{(l)}$: The number of times expert i is selected at layer l.\n\u2022 N: The total number of experts in the MoE algorithms.\n\u2022 $\\sum_{i=1}^{N} freq_{i}^{(l)}$: The total number of expert selections at layer l.\n\u2022 $H^{(l)}$: The entropy value at layer l, measuring the un-certainty or diversity in expert selections.\nFor a single metric to evaluate the diversity in expert uti-lization throughout the entire model, we propose $H_{total}$ - the total entropy which averages of the entropy values across the L layers as:\n$$H_{total} = \\frac{1}{L} \\sum_{l=1}^{L}H^{(l)}.$$\nWith $H_{total}$, we can measure the frequency of expert selec-tions across all layers $H_{total}$. Importantly, high $H_{total}$ Indi-cates a balanced expert utilization across all layers, where the model tends to distribute selections evenly among all ex-perts. In contrast, low $H_{total}$ Suggests a concentrated usage of a few experts in most layers, indicating specialization or a preference for certain experts."}, {"title": "(c) Does Overconfidence in Gated Expert Routing Impact MoE Performance?", "content": "In Figure 6, we assess the confidence levels of various MoE algorithms by examining the entropy in expert probabil-ity distributions across samples. This entropy is averaged per task (OCR Task, Coarse-Grained Tasks, and Reasoning Tasks) to reveal trends in confidence within each MoE con-figuration. The results display distinct behavioral patterns among routers, especially in their probability distributions across experts.\nThe SMoE Router and Hyper Router demonstrate lower en-tropy for perception-oriented tasks, such as the OCR Task, implying strong confidence in expert selection for these data types. For more cognitively challenging tasks, such as Rea-soning, both routers show a significant increase in entropy, indicating a broader distribution across multiple experts. This variability in entropy between perception and reason-ing tasks suggests that the SMoE Router and Hyper Router exhibit higher confidence in simpler, perceptual tasks, pos-sibly due to the straightforward nature of the input. Notably, the Hyper Router maintains the lowest entropy overall, a result of its phased training strategy that begins with top-1 expert selection and later transitions to top-2. This phased approach appears to bolster the Hyper Router's confidence in expert assignments.\nIn contrast, the Cosine Router and Perturbed Cosine Router exhibit a different trend, displaying their lowest entropy in the Reasoning Task, with entropy values that are consis-tently higher than those of the SMOE Router and Hyper Router. This pattern suggests a more uniform allocation of probabilities across experts, thus avoiding strong biases toward any single expert. The minimal variation in entropy values across tasks for these routers (within le-5) indicates a consistently balanced distribution of probabilities among experts, with negligible differences across tasks. This bal-anced behavior in the Cosine Router and Perturbed Cosine Router likely mitigates the risk of overcommitting to spe-cific experts, thereby maintaining flexibility and adaptability across diverse tasks.\nInterestingly, despite the higher confidence (indicated by low entropy) exhibited by the SMOE Router and Hyper Router, their overall performance can sometimes be worse than that of the Cosine and Perturbed Routers, particu-larly when trained with more data (Table 2, LLAVA-665K dataset). This discrepancy may be attributed to an over-confidence effect, whereby the SMOE Router and Hyper Router concentrate probabilities heavily on a primary ex-pert, resulting in the underutilization of auxiliary experts. Such an overreliance on a single expert diminishes the ad-vantages of using multiple experts, failing to harness the potential diversity and complementary strengths available within an ensemble. Our experiments illustrate that exces-sive confidence in expert selection can inadvertently reduce the effectiveness of using many experts, highlighting the importance of balanced probability distributions to fully leverage the potential of MoE architectures."}, {"title": "(d) Transitioning from Generalization to Specialization: Layer-Wise Expert Selection", "content": "A key question we investigate is the behavior of expert selec-tion across different layers. To this end, we follow Xue et al. (2024) to report the frequency of expert selection across"}, {"title": "4.2.3 Impact of the architectural choice on expert selection", "content": "Lastly, we examine the impact of the architecture choices to the expert selection pattern."}, {"title": "4.3 Summary of Key Results", "content": "Using LibMoE, we have extensively evaluate five state-of-the-art MoE algortithm. The empirical results show that no algorithms perform consistently the best when averaging across 11 zero-shot benchmarks. However, a closer look at their training curves reveals that there exists an intermedi-ate checkpoint that can achieve much better performances than the commonly used last checkpoint. However, we choose to not use the best checkpoint as the algorithm's performance because of the lack of a reliable early-stopping mechanism and the expensive cost of evaluating all inter-mediate checkpoints across many benchmarks. Then, we investigate the expert selection behaviors and show that dif-ferent algorithms often have distinct patterns when faced with tasks of different characteristics such as perception or reasoning tasks. Furthermore, we show that over-confident in expert selection might not always be optimal. Lastly, we highlight the impact of the architecture choice where Siglip is a better backbone for SMoE than CLIP. Our empirical results not only agree with the common practice for training large scale sMoE models but also reveal promising research direction for future studies. Lastly, we emphasize that all ex-periments conducted are readily supported by our LibMoE and we will also provide all the dense training checkpoints to facilitate the future research."}, {"title": "5 CONCLUSION", "content": "In this work, we present LiBMoE, a specialized toolkit de-veloped to advance research in Mixture-of-Experts (MoE) algorithms. LibMoE offers a streamlined approach to train-ing and evaluation MoE algorithms with LLMs, while being efficient and accessible to many researchers. LibMoE offers a suite of algorithms, standardized training settings, and a wide-range of benchmarks, all of which can be easily cus-tomized and extendable by the community. We benchmark five state-of-the-art MoE algorithms extensively and pro-vide a comprehensive study of their unique characteristics, many of which are not discussed in the literature. We firmly believe that LibMoE will serve as an indispensable tool for researchers to study MoE."}]}