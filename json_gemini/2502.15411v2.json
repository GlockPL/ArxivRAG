{"title": "HIFI-KPI:\nA Dataset for Hierarchical KPI Extraction from Earnings Filings", "authors": ["Rasmus Aavang", "Giovanni Rizzi", "Rasmus B\u00f8ggild", "Alexandre Iolov", "Mike Zhang", "Johannes Bjerva"], "abstract": "The U.S. Securities and Exchange Commission (SEC) requires that public companies file financial reports tagging numbers with the machine readable inline eXtensible Business Reporting Language (iXBRL) standard. However, the highly complex and highly granular taxonomy defined by iXBRL limits label transferability across domains. In this paper, we introduce the Hierarchical Financial Key Performance Indicator (HIFI-KPI) dataset, designed to facilitate numerical KPI extraction at specified levels of granularity from unstructured financial text. Our approach organizes a 218,126-label hierarchy using a taxonomy-based grouping method, investigating which taxonomy layer provides the most meaningful structure. HIFI-KPI comprises ~1.8M paragraphs and ~5M entities, each linked to a label in the iXBRL-specific calculation and presentation taxonomies. We provide baselines using encoder-based approaches and structured extraction using Large Language Models (LLMs). To simplify LLM inference and evaluation, we additionally release HIFI-KPI Lite, a manually curated subset with four expert-mapped labels. We publicly release all artifacts.", "sections": [{"title": "Introduction", "content": "Key accounting metrics explain over 70% of a public company's share price (Sadka, 2007). Hence, the ability to accurately assess a company's financial health, can lead to investors earning staggering returns over short time periods (Ke and Ramalingegowda, 2005). While several Natural Language Processing (NLP) datasets have been created for the financial domain (Chen et al., 2022b; J\u00f8rgensen et al., 2023) and from SEC filings (Loukas et al., 2021, 2022; Sharma et al., 2023; Lai et al., 2024), the potential of parsing the information-rich iXBRL for financial downstream tasks and applications,\n however, remains unexplored. Our parsing of the iXBRL format enables context by preserving the relationship between tags and their associated time periods, numerical values and currencies. This enables more precise context for financial data extraction. Mandated by the SEC, XBRL and later iXBRL enable financial reporting, iXBRL adopts the HTML format. Additionally, our approach enables prediction by different levels of granularity by introducing a recursive approach to ascending the presentation and calculation taxonomies.\nContributions. We contribute the following: 1 HIFI-KPI: A large iXBRL-based dataset of ~1.8M paragraphs and ~5M entities, plus a smaller HIFI-KPI Lite with four expert-defined label clusters; \u2461 A taxonomy-based granularity selection method that streamlines the calculation and presentation taxonomies; \u2462 Baselines for text classification, sequence labeling, and LLM-based structured extraction over HIFI-KPI; \u2463 Merged and company-specific versions of the taxonomies, alongside insights for future research on conceptual representations in finance."}, {"title": "Related Work", "content": "Financial NLP Datasets. Recent financial NLP datasets cover sentiment analysis (Gupta et al., 2020), named entity recognition (Alvarado et al., 2015; Shah et al., 2022), and numerical reasoning (Chen et al., 2022a). While early approaches used rule-based methods (Cong et al., 2007; Sheikh and Conlon, 2012; Hutto and Gilbert, 2014), modern efforts leverage large corpora e.g. from SEC fillings (Loukas et al., 2021). Unlike FiNER-139 (Loukas et al., 2022), where 80.42% of entries do not contain any tags at all, our dataset maximizes informative content with 2.77 tags/sentence.\nLanguage Models in Finance. Transformer-based models (Vaswani, 2017) led to specialized models like FinBERT (Yang et al., 2020). Recent LLMs including BloombergGPT (Wu et al., 2023) as well as more general models like NuExtract (Cripwell et al., 2024), Qwen2.5 (Qwen et al., 2025), and Deepseek (DeepSeek-AI et al., 2024) excel at structured extraction. Our HIFI-\u039a\u03a1I directly ties labels to the XBRL taxonomy, enabling downstream tasks beyond text labeling."}, {"title": "SEC Filings (10-Q & 10-K)", "content": "U.S. public companies must file quarterly (10-Q) and annual (10-K) reports (U.S. SEC, 2000, 2024a), which contain standardized financial statements."}, {"title": "XBRL Taxonomy", "content": "iXBRL standardizes financial reporting (Caltuna, 2020), using the .cal (calculation) and .pre (presentation) files for arithmetic and organizational structures (XBRL International, 2025b,a; XBRL US, 2015). XBRL requires annotators to choose the most specific tag, creating high granularity, limiting standardization across companies (Landry, 2022). The presentation relationships in the XBRL US GAAP Taxonomy structure elements hierarchically to aid user navigation (XBRL US, 2015). In the XBRL framework, taggers are expected to tag numerical values to the most specific applicable label within the taxonomy (XBRL US, 2015). XBRL tags are long e.g., us-gaap:CumulativeEffectOf-NewAccountingPrincipleInPeriodOfAdoption, which, while detailed, poses challenges for analysis. Thus, our dataset exhibits a high degree of specificity, this granularity complicates cross-company generalization, as the open nature of XBRL permits diverse taxonomy implementations (Landry, 2022). As unstructured data in finance grows (Lewis and Young, 2019), there is a strong interest in developing NLP benchmarks and methods for tasks like financial sentiment analysis, risk assessment, and finance-related question answering. By coupling iXBRL-derived tags with text data, HIFI-KPI provides a rich resource for these applications."}, {"title": "HIFI-KPI: Taxonomy-based Granularity Selection", "content": "HIFI-KPI supports multiple downstream tasks such as text classification, sequence labeling, structured information extraction, multi-label classification, and financial question answering.\nDataset Creation. Table 1 summarizes key statistics for both the full dataset and a smaller \"Lite\" subset for LLM inference. We collected all 10-K and 10-Q filings published between 2017-01-01 and 2024-06-01, yielding 41,211 quarterly (10-Q) and 14,188 annual (10-K) reports. While iXBRL became mandatory on June 15, 2020 (SEC, 2018), our start date also captures early voluntary adopters. We parsed each iXBRL document using beautifulsoup (Richardson, 2007) and regular expressions to extract text snippets along with every embedded XBRL tag, its date range, and numeric value. Any misparsed snippets were discarded, and we further filtered text for minimal formatting quality (e.g., removing leading punctuation or non-capitalized starts). From these steps, we obtained ~1.8M paragraphs (~1.07M from 10-Qs; ~0.70M from 10-Ks) and ~5M tagged entities. An example dataset entry appears in Listing 1 (Appendix E), illustrating the iXBRL-derived fields (Label, Currency, Value, and date range).\nHierarchy. Each filing has an associated XBRL attachment describing parent-child relationships in two key files: .cal (calculation) and .pre (presentation). We used a combination of edgartools (Gunning, 2024) and a custom scraper to download these attachments, followed by Arelle (Arelle Development Team, 2025) to parse them into JSON."}, {"title": "Recursively Ascending the Hierarchy", "content": "We then aggregated the per-document hierarchies to build two \"master\u201d taxonomies ($P_{master}$):\n$P_{master}(t) = \\arg \\max_{P \\in P(t)} count(p,t)$,\nwhere $t$ is a tag, $P(t)$ its possible parents, and $count(p, t)$ the frequency of each parent-child relation. We release both presentation and calculation taxonomies, as well as each document's taxonomy, to facilitate work on conceptual representation.\nRecursively Ascending the Hierarchy. XBRL tags (e.g., us-gaap:LineOfCreditFacility-CurrentBorrowingCapacity) are highly specific, complicating cross-company comparisons. Starting with over 200K ultra-fine-grained labels we iteratively ascend the taxonomy from leaf to parent to reduce specificity but heighten representation. At each step, a leaf node inherits its parent label, merging many uncommon tags into broader buckets (e.g., us-gaap:LineOfCreditFacility-CurrentBorrowingCapacity becomes us-gaap:DebtInstrumentLineItems). This bottom-up approach preserves conceptual similarity while enabling more robust classification and clustering. In Algorithm 1 (Appendix A), we show a pseudo-algorithm of the method we employ.\nHIFI-KPI Lite. To evaluate how well our methods generalize to the broader financial domain and facilitate further research on a more focused subset, we introduce HIFI-KPI Lite. This dataset was developed in collaboration with an industry expert, who assisted in mapping selected XBRL terms to their corresponding general finance concepts. By bridging the gap between highly specific XBRL terminology and more commonly used financial language, HIFI-KPI Lite provides a valuable resource for assessing the applicability of our approach at clustering and classification beyond the specific XBRL tags. We then process the larger dataset, retaining only snippets where more than 50% of the entities align with the expert-curated mapping, ensuring relevance and consistency."}, {"title": "Experiments", "content": "We demonstrate the usefulness of our granularity selection method and establish three baselines using different methods, namely text classification, sequence labeling, and structured data extraction with LLMs. We report the aggregated macro F1 over the cumulative support. Cumulative support defined as the total count of included ground truth tags. This is to showcase how good the model is at encapsulating a given amount of the dataset, at a given granularity.\nText Classification. We define a simple classification task: Predict the first entity's label from the paragraph. To enable rapid experimentation with different levels of granularity we use the all-MiniLM-L6-v2 model from sentence-transformers (Reimers and Gurevych, 2019) to embed each paragraph and then fine-tune a single classification head for classification with Adam (Kingma and Ba, 2017) (lr = 1e-5) for 20 epochs on HIFI-KPI.\nSequence Labeling. Next, we consider a token-level prediction task: identify and classify each entity within a paragraph. We use bert-base-uncased (Devlin et al., 2019) with a standard token classification head, using the Adam optimizer (Kingma and Ba, 2017), learning rate of lr = 1e-5, and max. 50 epochs on the 1,000 most frequent tags of HIFI-KPI with early stopping. Tags outside this set are mapped to a OOS (out-of-scope) label.\nLLM Based Structured Data Extraction. Finally, we use HiFi-KPI Lite to evaluate structured extraction, including tags, dates, currency, and numeric values. We compare three LLMs: NuExtract, Qwen-2.5-14B, and Deepseek-v3. We provide identical system prompts (see Figure 4; Appendix H) and a small number of examples (i.e., 1-shot). NuExtract requires a specific JSON template format, so we tailored prompts accordingly."}, {"title": "Results", "content": "Text Classification. Figure 2 shows that more coarse-grained labels of the . pre taxonomy generally boosts macro-F1, as the bottom-up approach reduces label sparsity. For .cal, performance saturates quickly, possibly because its hierarchy is less deep. Beyond n = 3\u20134, improvements decrease. The model struggles on infrequent tags, showing opportunities to develop stronger methods.\nSequence Labeling. Sequence labeling shows significantly higher performance than text classification. We observe the effectiveness of the . pre taxonomy, as the most common special OOS (out-of-scope) label has significantly lower support than others, ensuring accurate broader-category classification. The most common ungrouped tags achieves the highest macro F1, followed by . cal, and then .pre, before all representations converge to the same macro F1 for long-tail labels.\nLLM-based Extraction. Table 2 shows large performance variations. Qwen and Deepseek often extract dates, currency and numeric values correctly but struggle more with labels. NuExtract generally struggles across the board. Overall, performance demonstrates the difficulty of extracting full, well-structured records from financial text. Details about metric calculations in Appendix F."}, {"title": "Discussion", "content": "HIFI-KPI Dataset. The results suggest that the presentation layer provides greater flexibility in its ability to present finer and more coarse-grained labels, while also being an easier target to predict. The results also suggest that higher support correlates with higher performance. Lastly performance of the text classification task is significantly lower than for sequence labeling, indicating that fine-tuning the BERT model for sequence labeling improves extractions, over training only the classification head for text classification.\nHIFI-KPI Lite. Results on HIFI-KPI Lite clearly shows that bigger models are better at the structured JSON generation task, highlighting the need for reasoning and understanding of the domain. NuExtract, though designed for JSON extraction, often failed in our setup, likely due to its templated approach that seems to prioritize exact text extraction over following our label set. However, the NuExtract model always uses the date format it has been fine-tuned on and not specified by the system prompt or dataset. Finally, our fine-tuned smaller embedding models, especially sequence labeling, clearly outperform the Few-shot LLM-based models in label extraction, emphasizing the value of domain specific data and tuning."}, {"title": "Conclusion", "content": "Our HIFI-KPI dataset takes a significant step towards structuring the iXBRL label set, making possible different degrees of granularity in predictions. Our HIFI-KPI dataset demonstrates the potential of parsing iXBRL to provide contextual details for labels, including temporal, currency, and numeric value relations. Experiments on the expert-labeled HIFI-KPI Lite subset suggest that few-shot prompted generative LLMs can extract this context and generate structured JSON, though fine-tuned encoder-based models achieve better performance. Our results highlight a promising approach to structuring the taxonomy, with HIFI-KPI Lite highlighting the potential for even better aggregation algorithms."}, {"title": "Limitations", "content": "A limitation of our experiments is that the annotation quality may vary throughout our dataset, evidenced by the fact that the SEC regularly publishes Data Quality Reminders. For instance, the SEC has noted that some filers use different labels for the same element on income statements across periods (U.S. SEC, 2023) or don't report the most fundamental key figure earnings per share correctly (U.S. SEC, 2024b). Lastly there is a bias in the dataset created as the text snippets in the dataset consists only of the spans in these documents, and we only include snippets that match our simple parser methodology."}, {"title": "Ethical Considerations", "content": "Our work adheres to the ACL Code of Ethics. We do not anticipate any ethical concerns arising from the development and release of our dataset."}]}