{"title": "Debate Helps Weak-to-Strong Generalization", "authors": ["Hao Lang", "Fei Huang", "Yongbin Li"], "abstract": "Common methods for aligning already-capable models with\ndesired behavior rely on the ability of humans to provide su-\npervision. However, future superhuman models will surpass\nthe capability of humans. Therefore, humans will only be able\nto weakly supervise superhuman models. This expected defi-\nciency of human evaluation would weaken the safety of fu-\nture AI systems. Scalable oversight and weak-to-strong gen-\neralization are two complementary approaches to tackle this\nissue. In this paper, we attempt to combine the strengths of\nthese two approaches to further improve alignment. Specif-\nically, we investigate ways of improving human supervision\nwith a strong pretrained model and then supervise the strong\nmodel with enhanced weak human supervision. To make it-\nerative empirical progress, we consider an analogy: can we\nuse a strong model to improve weak model supervision and\nthen use it to supervise the strong model? We empirically\ntest it by finetuning a small weak model on ground truth la-\nbels with the additional help from a large strong model, and\nthen finetuning the strong model on labels generated by the\nweak model. We find that debate can assist a weak model\nin extracting trustworthy information from an untrustworthy\nstrong model, which provides leverage as context on samples\nwhen training a weak model. We also show that an ensem-\nble of weak models helps exploit long arguments generated\nby strong model debaters and obtain a more robust supervi-\nsion estimate. Extensive experiments on the OpenAI weak-to-\nstrong NLP benchmarks show that the combination approach\nleads to better alignment, which indicates that debate has the\npotential to help weak-to-strong generalization.", "sections": [{"title": "Introduction", "content": "Current AI alignment techniques heavily rely on the avail-\nability of human labelled data, such as human demonstra-\ntions for supervised finetuning (SFT) (Wei et al. 2021;\nChung et al. 2024) and human preferences for reinforcement\nlearning from human feedback (RLHF) (Christiano et al.\n2017; Ouyang et al. 2022; Bai et al. 2022). These techniques\ncan be leveraged to build the most capable Al systems cur-\nrently deployed (OpenAI 2023; Anthropic 2023).\nHowever, as models grow increasingly more capable, they\nwill surpass the ability of humans (CAIS 2023). In that case,\neven human experts can not reliably verify the quality or cor-\nrectness of model outputs, and the role of human evaluation\nwill evolve into non-experts overseeing experts (Amodei\net al. 2016; Bowman et al. 2022; Burns et al. 2023; Khan\net al. 2024). The expected deficiency of human evaluation\nwill limit the effectiveness of most existing alignment ap-\nproaches (Casper et al. 2023; McAleese et al. 2024). More-\nover, these predicted inaccurate training signals could lead to\nreward overoptimization and reward tampering during pol-\nicy training that seriously weakens its safety (Gao, Schul-\nman, and Hilton 2023; Denison et al. 2024).\nThere are two complementary approaches to tackle the\nabove issue: scalable oversight (SO) and weak-to-strong\ngeneralization (W2SG) (Leike 2023). SO approaches aim\nto improve the ability of humans to supervise more capa-\nble models, such that accurately labelled data can be used\nfor alignment (Bowman et al. 2022). Instead of improving\nhuman supervision, W2SG approaches finetune a strong pre-\ntrained model to generalize accurately from weak human su-\npervision (Burns et al. 2023).\nWe note that most prior SO and W2SG techniques are\nstudied separately. In contrast, we attempt to combine the\nstrength of SO and W2SG to further improve AI alignment.\nWe investigate ways of improving human supervision with a\nstrong pretrained model and then supervise the strong model\nwith enhanced weak human supervision. To make iterative\nempirical progress, we consider an analogy (Burns et al.\n2023; Kenton et al. 2024): can we use a strong model to im-\nprove weak model supervision and then use it to supervise\nthe strong model?\nIn this paper, we empirically test it by finetuning a small\nweak model on ground truth labels with the additional help\nof knowledge from a large strong model, and then finetun-\ning the strong model on labels generated by the weak model.\nWe assume a strong model pretrained on internet-scale data\ncan provide contextual information on samples when train-\ning a weak model (Brown et al. 2020). This gives us hope\nthat a weak-strong model team could create a better weak\nsupervisor to elicit the capabilities of the strong model.\nA major challenge in building a weak-strong model team\ninvolves finding ways of extracting trustworthy information\nfrom untrustworthy models (Bowman et al. 2022). More\nspecifically, strong pretrained models have huge capabili-\nties but are not well aligned with human values and inten-"}, {"title": "Related Work", "content": "AI alignment. The goal of AI alignment is to steer already-\ncapable models to behave in line with human values and in-\ntentions (Leike et al. 2018; Ji et al. 2023). Current alignment\nmethods finetune pretrained LLMs using imitation learning\non human demonstrations (Bain and Sammut 1995; Atkeson\nand Schaal 1997; Wei et al. 2021; Chung et al. 2024), rein-\nforcement learning from human feedback (RLHF) (Chris-\ntiano et al. 2017; Stiennon et al. 2020; Ouyang et al. 2022;\nBai et al. 2022), or direct alignment algorithms like direct\npreference optimization (DPO) (Rafailov et al. 2024b,a).\nBoth imitation learning and preference learning rely on\nhigh-quality human supervision, a demand that becomes in-\ncreasingly challenging as models become more capable than\nhumans (Amodei et al. 2016).\nScalable oversight. Scalable oversight techniques seek to\nimprove the ability of humans to supervise more capable\nmodels (Bowman et al. 2022). This is typically pursued\nthrough taking advantage of special problem structure, such\nas the assumption that evaluation is easier than genera-\ntion (Karp 1975; Goodfellow et al. 2014) or decomposabil-\nity (Christiano, Shlegeris, and Amodei 2018). There have\nbeen many promising scalable oversight proposals in theory,\nincluding Recursive Reward Modeling (Leike et al. 2018),\nDebate (Irving, Christiano, and Amodei 2018), Market-\nMaking (Hubinger 2020), Self-Critique (Saunders et al.\n2022), and many more (Lightman et al. 2023; McAleese\net al. 2024; Sun et al. 2024). Recent empirical studies in this\ndirection demonstrate that human-machine teams can im-\nprove evaluation accuracy on question answering tasks over\nthe human-only baseline (Bowman et al. 2022).\nDebate was originally proposed for AI safety (Irving,\nChristiano, and Amodei 2018). From then on, a body of\nwork has explored the usability of debate for scalable over-\nsight, with human or LLM debaters (Parrish et al. 2022b,a;\nMichael et al. 2023; Khan et al. 2024; Kenton et al. 2024).\nThese studies are all conducted to improve inference-time\njudge accuracy, while in our work debate is leveraged to\ntrain a better weak supervisor. We could in turn use the\nweak supervisor to align strong models. LLM-based debate\nhas also been investigated in several other applications, like\ntranslation (Liang et al. 2023), text assessment (Chan et al.\n2023), reasoning and content generation (Du et al. 2023).\nWeak-to-strong generalization. In contrast to improv-\ning human supervision, weak-to-strong generalization tech-\nniques finetune a strong pretrained model to generalize well\nfrom weak human supervision (Burns et al. 2023). The hope\nfor these techniques is that strong pretrained models should\nalready have good representations of the alignment-relevant\ntasks. Therefore, we simply need a weak supervisor to elicit\nwhat the strong model already knows. Recently, a theoretical\nframework is introduced to understand weak-to-strong gen-\neralization with misfit error (Charikar, Pabbaraju, and Shi-\nragur 2024). Prior work has mainly explored how to super-\nvise a strong model with a fixed weak supervisor, while in"}, {"title": "Preliminaries", "content": "We review the weak-to-strong generalization pipeline\nin (Burns et al. 2023), which has also been adopted in sub-\nsequent work (Liu and Alahi 2024; Charikar, Pabbaraju, and\nShiragur 2024). It usually consists of three phases:\n1. Create the weak supervisor. We create the weak su-\npervisor by finetuning a small pretrained model on ground\ntruth labels. We call the performance of the weak supervisor\nthe weak performance."}, {"title": "Methods", "content": "In this study, we build the strong student model following\nthree steps: 1. Generate arguments from the debate between\ntwo instances of a large pretrained model; 2. Train an ensem-\nble of weak models using these debate arguments; 3. Train"}, {"title": "Argument Generation through Debate", "content": "We assume large pretrained models embed broad-coverage\nknowledge that can help a variety of tasks (Brown et al.\n2020). Our goal is to extract trustworthy information from\na capable but untrustworthy strong model via debate (Bow-\nman et al. 2022). So we could in turn use the trustworthy\ninformation to help train a better weak model.\nWe first describe the debate protocol we investigated to\nelicit truth from strong models, following (Michael et al.\n2023; Khan et al. 2024; Kenton et al. 2024). Given a ques-\ntion and its two answer choices (one correct, one incorrect),\ntwo instances of a large pretrained model (debaters) are ran-\ndomly assigned to argue for these two opposing answers.\nDebate is turn-based textual exchanges between the two de-\nbaters, which take turns to review arguments from previous\nturns and generate their arguments for the next turn. After a\npre-determined number of turns, the debate is ended and the\ntranscript of arguments from the debate is kept. During the\ndebate, each debater presents the most compelling evidences\nfor its assigned answer and arguments to explain why its op-\nponent's claims are false."}, {"title": "Weak Model Ensemble Training", "content": "For each input sample of weak models, we append it with the\nkept debate transcript. We train a weak model by finetuning\na small pretrained model on these augmented samples with\nground truth labels. We note that the debate transcripts gen-\nerated in a multi-turn debate are long, which may be diffi-\ncult for a weak model to fully process. Therefore, we train\nan ensemble of weak models {W1, ..., Wk} to help improve\nrobustness (Lakshminarayanan, Pritzel, and Blundell 2017).\nWe explore two types of ensembles: debate ensembles,\nwhere the debate transcript used by each member is gener-\nated with a different random seed, and finetune ensembles,\nwhere all members share the same debate transcript, but use\na different seed when finetuned on the augmented samples.\nDebate ensembles are much more expensive to train, but are\nmore diverse and thus likely to lead to a more robust predic-\ntion. Unless stated otherwise, we train an ensemble consist-\ning of four individual weak models."}, {"title": "Training Strong Models using Ensembles", "content": "We finally train a strong student model by finetuning a large\npretrained model on weak labels constructed by the weak\nmodel ensemble. We simply take the mean of the predictions\nfrom different weak models within the ensemble as the weak\nlabel for each training sample (Ganaie et al. 2022)."}, {"title": "Experiments", "content": "We adopt the evaluation protocol of prior work (Burns\net al. 2023), and conduct experiments in NLP tasks on\nfour classification datasets: SciQ (Welbl, Liu, and Gard-\nner 2017), BoolQ (Clark et al. 2019), CosmosQA (Huang\net al. 2019), and AnthropicHH (Bai et al. 2022). We convert\neach dataset to a binary classification problem. For multiple-\nchoice datasets, given a data point with a question Q and\nk candidate answers A, we construct k new data points of\nthe form (Q, A\u2081), where the label is 1 for the correct an-\nswers and 0 for all the incorrect answers. We also keep the\nsame number of correct and incorrect answers per question\nto maintain class balance."}, {"title": "Experimental Setups and Metrics", "content": "We randomly sample at most 20k data points from each task\nand split them in half. We train a weak model on the first\nhalf of the data points and use its prediction on the other half\nas the weak labels. The weak labels are soft labels (Hinton,"}, {"title": "Main Results", "content": "In Table 2, we report the results of each approach on the bi-\nnary classification tasks converted from SciQ, BoolQ, Cos-\nmosQA, and AnthropicHH datasets. Here, our approach\nuses debate ensembles. In each task, we observe that PGRS\nof strong student models finetuned on weak labels are all\npositive. This indicates that student models consistently out-\nperform their weak supervisors across all weak-to-strong\ngeneration approaches and tasks that we studied. Simulta-\nneously, this promising weak-to-strong generalization also\nsuggests that our experimental settings can help make itera-\ntive empirical progress in tackling the weak supervision is-\nsue for aligning future superhuman models.\nAt the same time, we find that our approach signifi-\ncantly outperforms each strong student baseline, including\nthe naive baseline finetuned on weak labels or more sophis-\nticated baselines equipped with a confidence loss term on all\nfour tasks. Compared with the promising baseline Finetune\nw/ aux. loss, our approach brings up from a PGR of 41.2% to\n76.5% in SciQ, 56.4% to 69.2% in BoolQ, 17.4% to 56.5%\nin CosmosQA, and 35.0% to 70.0% in AnthropicHH. Our\napproach also obtains the best test accuracy among all com-\npared strong students. The performance gain demonstrates\nthe advantage of extracting trustworthy information from the\nstrong model via debate, which helps create a better weak\nsupervisor to elicit the capabilities of the strong model.\nIn addition, we also see that adding a confidence loss to\nthe standard cross entropy objective (Finetune w/ aux. loss\nand Finetune w/ pro. loss) generally gives a modest boost\nin generalization performance. In our experimental settings,\nthe gaps in compute between weak and strong models are\nnot significantly large, which may limit their performances."}, {"title": "Ablation Studies", "content": "Finally, we provide comprehensive ablation studies to under-\nstand the efficacy of debate for weak-to-strong generation.\nAblation on different scalable oversight approaches.\nWe demonstrate the effectiveness of debate as a mechanism"}, {"title": "Limitations and Conclusion", "content": "Limitations. In this work, we attempt to combine the\nstrength of two complementary approaches, i.e., scalable\noversight and weak-to-strong generalization, to tackle the\nissue of weak supervision for aligning future superhuman\nmodels. For this purpose, we explore a simple combination\nmethod, i.e., extracting trustworthy information via debate\nfrom strong models and using it to create a better weak su-\npervisor to elicit the capabilities of strong models. Although\nour proposed method is found to be effective in all our ex-\nperiments and ablation studies, there are many more ways\nto combine scalable oversight and weak-to-strong general-\nization, such as Task decomposition + W2SG (Leike 2023).\nMore empirical work is needed in this area.\nIn our setup, the difference between strong and weak\nmodels is only in the size of pretrained models. In the future,\nstronger models may also differ in reasoning and planning\nabilities. Furthermore, the gaps in compute between weak\nand strong models are not significantly large in this work (7B\nvs. 14B). It would be interesting to verify our conclusions\non more large and advanced models, such as Qwen/Qwen2-\n72B (Yang et al. 2024). Finally, our approach is expensive\nas it requires both two instances of debaters and a multi-turn\ndebate procedure.\nConclusion. In this paper, we present an approach to im-\nprove the performance of weak-to-strong generalization via\ndebate. We believe the perspective of having scalable over-\nsight and weak-to-strong generalization methods working in\ncombination to tackle the weak supervision issue will prove\nto be a fruitful area of research in superhuman alignment."}]}