{"title": "HART: EFFICIENT VISUAL GENERATION WITH HYBRID AUTOREGRESSIVE TRANSFORMER", "authors": ["Haotian Tang", "Yecheng Wu", "Shang Yang", "Enze Xie", "Junsong Chen", "Junyu Chen", "Zhuoyang Zhang", "Han Cai", "Yao Lu", "Song Han"], "abstract": "We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024\u00d71024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7\u00d7 higher throughput and 6.9-13.4\u00d7 lower MACs. Our code is open sourced at https://github.com/mit-han-lab/hart.", "sections": [{"title": "INTRODUCTION", "content": "The rapid advancement of large language models (LLMs) is pushing artificial intelligence into a new era. At the core of LLMs are autoregressive (AR) models, which have gained popularity due to their generality and versatility. These models typically predict the next token in a sequence based on the previous tokens as input. While originating from natural language processing, autoregressive models have also recently been adopted for visual generation tasks. These approaches utilize a visual tokenizer to convert images from pixel space into discrete visual tokens through vector quantization (VQ) (Van Den Oord et al., 2017). These visual tokens are then processed in the same manner as language tokens. Benefiting from techniques proven successful in the LLM field, autoregressive visual generation methods have demonstrated their effectiveness in diverse tasks, including text-to-image generation, text-to-video generation, and image editing (Van Den Oord et al., 2017; Esser et al., 2021; Chang et al., 2022; Yu et al., 2022b; Kondratyuk et al., 2023; Tian et al., 2024).\nAutoregressive image generation models have also demonstrated significant potential for building unified visual language models (Gemini Team, Google, 2023; OpenAI, 2024), such as Emu3 (Emu3 Team, BAAI, 2024), VILA-U (Wu et al., 2024), and Show-o (Xie et al., 2024).\nConcurrently, another major trend in visual generation from Ho et al. (2020); Rombach et al. (2022); Chen et al. (2024a); BlackForest Labs (2024) has centered on diffusion models. These models employ a progressive denoising process, beginning with random Gaussian noise. Diffusion models achieve better generation quality compared with autoregressive models, but they can be computationally expensive to deploy: even with an efficient DPM-Solver sampler from Lu et al. (2022), it still takes DiT-XL/2 (Peebles & Xie, 2023) 20 denoising steps to generate an image, which translates to 86.2T MACs at 1024\u00d71024 resolution. In contrast, generating a comparable image using a similarly sized AR model capable of predicting multiple tokens in parallel (Tian et al., 2024) requires only 10.1T MACs at the same resolution, which is 8.5\u00d7 less computationally intensive.\nThis paper addresses the following question: Can we develop an autoregressive model that matches the visual generation quality of diffusion models while still being significantly faster?\nCurrently, visual generation AR models lag behind diffusion models in two key aspects:\n1. Discrete tokenizers in AR models exhibit significantly poorer reconstruction capabilities compared to the continuous tokenizers used by diffusion models. Consequently, AR models have a lower generation upper bound and struggle to accurately model fine image details.\n2. Diffusion models excel in high-resolution image synthesis, but no existing AR model can directly and efficiently generate 1024\u00d71024 images.\nTo address these challenges, we introduce HART (Hybrid Autoregressive Transformer) for efficient high-resolution visual synthesis. HART bridges the reconstruction performance gap between discrete tokenizers in AR models and continuous tokenizers in diffusion models through hybrid tokenization. The hybrid tokenizer decomposes the continuous latent output of the autoencoder into two components: one as the sum of discrete latents derived from a VAR tokenizer (Tian et al., 2024), and the other as the continuous residual, representing the information that cannot be captured by discrete tokens. The discrete tokens captures the big picture, while continuous residual tokens focus on fine details (Figure 3). These two latents are then modeled by our hybrid transformer: the discrete latents are handled by a scalable-resolution VAR transformer, while the continuous latents are predicted by a lightweight residual diffusion module with 5% parameter and 10% runtime overhead.\nHART achieves significant improvements in both image tokenization and generation over its discrete-only baseline. Compared with VAR, it reduces the reconstruction FID from 2.11 to 0.30 at 1024x1024 resolution on MJHQ-30K (Li et al., 2024a), enabling HART to lower the 1024px generation FID on the same dataset from 7.85 to 5.38 (a 31% relative improvement). Furthermore, we demonstrate that HART achieves up to a 7.8% improvement in FID over VAR for class-conditioned generation on ImageNet (Deng et al., 2009). HART also outperforms MAR on this task with 13\u00d7 higher throughput.\nNotably, HART closely matches the quality of state-of-the-art diffusion models in multiple text-to-image generation metrics. Simultaneously, HART achieves 3.1-5.9\u00d7 faster inference latency, 4.5-7.7\u00d7 higher throughput, and requires 6.9-13.4\u00d7 less computation compared with diffusion models."}, {"title": "RELATED WORK", "content": "Visual generation has become a key focus in machine learning research. Initial work by Kingma & Welling (2013) introduced variational autoencoders (VAEs) for image synthesis. Subsequently, Goodfellow et al. (2014) proposed generative adversarial networks (GANs), which were further improved by Brock et al. (2018); Karras et al. (2019); Kang et al. (2023).\nDiffusion models from Ho et al. (2020); Nichol & Dhariwal (2021); Dhariwal & Nichol (2021); Ramesh et al. (2022); Betker et al. (2023) have emerged as the state-of-the-art approach for generating high-quality images after VAE and GAN. The latent diffusion model from Rombach et al. (2022); Podell et al. (2023) applies U-Net to denoise the Gaussian latent input, and is succeeded by DiT from Peebles & Xie (2023) and U-ViT from Bao et al. (2023) which replaces the U-Net with transformers. Chen et al. (2023; 2024b;a) scale up DiTs to text-to-image (T2I) generation. Concurrently, Kolors Team (2024); Ma et al. (2024a); Li et al. (2024a) further scaled up T2I diffusion models to billions of parameters. Recent research from Esser et al. (2024); Auraflow Team (2024); BlackForest Labs (2024) also explored rectified flow for fast sampling.\nAutoregressive models pioneered by Chen et al. (2020) generate images as pixel sequences, rather than denoising an entire latent feature map simultaneously. Early research VQVAE and VQGAN from Van Den Oord et al. (2017); Esser et al. (2021) quantize image patches into discrete tokens and employ a decoder-only transformer to predict these image tokens, analogous to language modeling. VQGAN was subsequently enhanced in several aspects: Yu et al. (2022a) improved its autoencoder modeling, Chang et al. (2022); Yu et al. (2023a); Li et al. (2023) increased its sampling speed with MaskGIT, while Mentzer et al. (2023), Yu et al. (2023b), and Yu et al. (2024); Li et al. (2024c) enhanced its tokenization efficiency. Lee et al. (2022) introduced residual quantization to reduce tokenization error. Building on this, Tian et al. (2024) developed VAR, which innovatively transformed next-token prediction in RQVAE to next-scale prediction, significantly improving sampling speed. There were also efforts that scaled up autoregressive models to text-conditioned visual generation: Ramesh et al. (2021); Ding et al. (2021; 2022); Liu et al. (2024b); Sun et al. (2024); Crowson et al. (2022); Gafni et al. (2022); Emu3 Team, BAAI (2024); Liu et al. (2024a) were T2I generation methods based on VQGAN. Among these work, concurrent work from Liu et al. (2024a) achieves high quality 1024px image generation through AR modeling. We will demonstrate that HART excels in efficiency compared with their approach. Chang et al. (2023); Villegas et al. (2022); Kondratyuk et al. (2023); Xie et al. (2024); Chen et al. (2024c); Bai et al. (2024) extended MaskGIT. STAR, VAR-CLIP (Ma et al., 2024b; Zhang et al., 2024) were extensions of VAR. ControlVAR (Li et al., 2024d), ControlAR (Li et al., 2024e) and CAR Yao et al. (2024) also study controllable AR image generation. LANTERN (Jang et al., 2024) accelerates AR models through speculative decoding.\nHybrid models represent a new class of visual generative models that synergize discrete and continuous image modeling approaches. GIVT from Tschannen et al. (2023) predicted continuous visual tokens with autoregressive models while VQ-Diffusion from Gu et al. (2022) extended diffusion to discrete latents. MAR from Li et al. (2024b) and DisCo-Diff from Xu et al. (2024) conditioned a diffusion model with autoregressive prior. This idea was also concurrently explored in visual language models by Ge et al. (2024); Jin et al. (2023). Transfusion (Zhou et al., 2024) fuses DiT and LLM into a single model, and is natively capable of multi-modal generation. Concurrent work DART by Gu et al. (2024) also supports native multi-modal generation by flattening the denoising trajectory in diffusion models into multiple autoregressive sampling steps."}, {"title": "HART: HYBRID AUTOREGRESSIVE TRANSFORMER", "content": "We introduce Hybrid Autoregressive Transformer (HART) for image generation. Unlike all existing generative models that operate exclusively on either discrete or continuous latent spaces, HART models both discrete and continuous tokens with a unified transformer. The key factors enabling this are a hybrid tokenizer and residual diffusion."}, {"title": "HYBRID VISUAL TOKENIZATION", "content": "Conventional autoregressive visual generation encodes images into discrete tokens using trained tokenizers. These tokens map to entries in a vector-quantized (VQ) codebook and can reconstruct the original images from the VQ tokens. This approach transforms text-to-image generation into a sequence-to-sequence problem, where a decoder-only transformer, or LLM, predicts image tokens from text input. The tokenizer's reconstruction quality sets the upper limit for image generation. Constrained by their finite vocabulary codebooks, discrete tokenizers often struggle to faithfully reconstruct images with intricate, high-frequency details such as human faces, as in Figure 4.\nHybrid tokenization. We introduce our hybrid tokenizer in Figure 5. The primary goal of hybrid tokenization is to enable the decoding of continuous features during generation, thereby overcoming the poor generation upper bound imposed by finite VQ codebooks. We begin with a CNN-based visual encoder that transforms the input image into continuous visual tokens in the latent space. These tokens are then quantized into discrete tokens across multiple scales, following VAR (Tian et al., 2024). The multi-scale vector quantization process results in a difference between the accumulated discrete features and the original continuous visual features, which can not be accurately represented using VQ codebook elements. We term this difference residual tokens, which are subsequently modeled by residual diffusion, as detailed in Section 3.2.\nAlternating training. To train our hybrid tokenizer, we begin by initializing the visual encoder, quantizer (i.e., codebook), and visual decoder from a pretrained discrete VAR tokenizer. We then freeze the visual encoder and quantizer, allowing only the visual decoder to be trained. During each training step, we randomly choose with equal probability (50%) whether to provide the decoder with discrete or continuous visual tokens for reconstructing the input image. Specifically, when the"}, {"title": "HYBRID AUTOREGRESSIVE MODELING WITH RESIDUAL DIFFUSION", "content": "Hybrid tokenization offers superior rFID and a better generation upper bound compared to discrete tokenization. We introduce HART (Figure 6) to efficiently translate this improved upper bound into real enhancements in generation quality. HART models the continuous image tokens as the sum of two components: (1) discrete tokens, modeled by a scalable-resolution autoregressive transformer, and (2) residual tokens, fitted by an efficient residual diffusion process.\nScalable-resolution autoregressive transformer. Our discrete token modeling extends VAR to text-to-image generation and improves scalability at higher resolutions. HART concatenates text tokens with visual tokens during training, in contrast to VAR which use a single class token. The text tokens are visible to all visual tokens, as in Figure 6 (right). Our approach is 25% more parameter-efficient than STAR (Ma et al., 2024b)'s cross-attention method (Chen et al., 2023).\nHART directly generates 1024px images using a single model, offering superior efficiency compared to existing AR-based solutions that rely on super-resolution (Yu et al., 2022b) or one-token-per-step prediction (Liu et al., 2024a). To mitigate the O(n4) training cost for high-resolution AR transformers, we finetune from pretrained low-resolution checkpoints. We convert all absolute position embeddings (PEs) in VAR to interpolation-compatible relative embeddings, including step (indicating the resolution each token belongs to) and token index embeddings. We utilize sinusoidal PE for step embeddings, which naturally accommodates varying sampling steps in 256/512px (10 steps) and 1024px (14 steps) generation. For token index embeddings, we implement a hybrid approach: 1D rotary embeddings for text tokens and 2D rotary embeddings (Sun et al., 2024; Ma et al., 2024a; Wang et al., 2024) for visual tokens. The position indices of visual tokens directly continue from those of text tokens. We found these relative embeddings significantly accelerates HART convergence at higher resolutions.\nResidual diffusion. We employ diffusion to model residual tokens, given their continuous nature. Similar to MAR (Li et al., 2024b), we believe that a full DiT is unnecessary for learning this residual. Instead, a lightweight (37M parameters) residual diffusion MLP would be sufficient. This MLP is"}, {"title": "EFFICIENCY ENHANCEMENTS", "content": "While our scalable-resolution AR transformer and residual diffusion designs are crucial for high-quality, high-resolution image generation, they inevitably introduce inference and training overhead. We address these efficiency challenges in this section.\nTraining. The residual diffusion module introduces both computational and memory overhead during training. To mitigate this, we discovered that discarding 80% of the tokens (on average) in the final step and applying supervision only to the subsampled tokens during training does not result in performance degradation. This approach accelerates training by 1.4\u00d7 at 512px and 1.9\u00d7 at 1024px, while also reducing training memory usage by 1.1\u00d7. In the appendix, we explain the effectiveness of this method by demonstrating that the attention pattern in our autoregressive transformer is mostly local. Consequently, although token subsampling during training may compromise global interactions between tokens, it has small impact on attention calculation.\nInference. For inference, we observed that relative position embeddings introduced multiple memory-bound GPU kernel calls, in contrast to the single call required for absolute position embeddings in VAR (Tian et al., 2024). To optimize performance, we fused these computations into two kernels: one for sinusoidal calculation and another for rotary embedding. This optimization resulted in a 7% improvement in end-to-end execution time. Additionally, fusing all operations in RMSNorm into a single GPU kernel also improved total runtime by 10%."}, {"title": "EXPERIMENTS", "content": "In this section, we evaluate HART's performance in tokenization and generation. For generation, we present both text-to-image and class-conditioned image generation results."}, {"title": "SETUP", "content": "Models. For class-conditioned image generation models, we follow VAR (Tian et al., 2024) to construct HART models with varying parameter sizes in the AR transformer: 600M, 1B, and 2B. The diffusion MLP contains an additional 37M parameters. We replace VAR's attention and FFN blocks with Llama-style (Touvron et al., 2023) building blocks. For text-conditioned image generation, we start with the 1B model and remove all AdaLN (Peebles & Xie, 2023) layers, resulting in a 30% reduction in parameters. We employ Qwen2-1.5B (Yang et al., 2024) as our text encoder and follow LI-DiT (Ma et al., 2024a) to reformat user prompts."}, {"title": "MAIN RESULTS", "content": "Hybrid tokenization. We evaluate the HART hybrid tokenizer on ImageNet and MJHQ-30K, two datasets not observed during training. As shown in Table 1, our hybrid tokenization offers significant advantages over discrete tokenization, reducing the 1024px rFID from 2.11 to 0.30. This matches the performance level of the SDXL tokenizer, indicating that the generation upper bound of HART is comparable to that of diffusion models. The discrete rFID of our hybrid tokenizer also ensures that the discrete tokens still capture the majority of image structure, so that the residual tokens remain easily learnable.\nText-to-image generation. We present quantitative text-to-image generation results in Table 2. On MJHQ-30K, our method achieved superior FID compared to all diffusion models. HART also demonstrates better image-text alignment than the 3.6\u00d7 larger SD-XL (Podell et al., 2023), as indi-"}, {"title": "ABLATION STUDIES AND ANALYSIS", "content": "Residual diffusion: effectiveness. Table 5 demonstrates the effectiveness of learning residual tokens in HART. For ImageNet 256\u00d7256 generation, residual diffusion yields a 10-14% improvement in FID and up to a 6.4% increase in inception score compared to the baseline model, HART-VAR."}, {"title": "CONCLUSION", "content": "We introduce HART (Hybrid Autoregressive Transformer), an early autoregressive model capable of directly generating 1024\u00d71024 images from text prompts without super-resolution. HART achieves quality comparable to diffusion models while being 3.1-5.9\u00d7 faster and offering 4.5-7.7\u00d7 higher throughput. Our key insight lies in the decomposition of continuous image latents through hybrid tokenization, producing discrete tokens that capture the overall structure and residual tokens that refine image details. We model the discrete tokens using a scalable-resolution AR transformer,"}, {"title": "APPENDIX", "content": "We visualize the attention patterns of a pretrained VAR (Tian et al., 2024) in Figure 9. Our empirical analysis reveals that for each VAR stage (i.e., sampling step), the attention score is predominantly concentrated on three key areas: the current stage, the preceding stage, and the initial three stages.\nWithin the current stage, where the attention score is highest, we further examine the spatial attention map, as depicted in the rightmost three subfigures of Figure 9. Interestingly, despite the VAR attention mechanism allowing all tokens within the last stage to interact, the attention map exhibits a surprisingly localized pattern: each token primarily attends to its immediate neighbors, similar to convolution operations.\nThis observation has important implications. Even when we significantly reduce the number of tokens in the last stage during training (by up to 80%), the fundamental attention pattern remains intact due to the limited global interaction between tokens. This explains why the partial supervision approach during training (discussed in Section 3.3) does not compromise generation quality.\nWe have also empirically verified that explicitly restricting attention patterns to the first 3 stages plus 2 local stages during training does not impact final results. Consequently, implementing a sparse attention kernel to further accelerate training is feasible, which we leave as a future direction."}]}