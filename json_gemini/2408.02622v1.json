{"title": "Language Model Can Listen While Speaking", "authors": ["Ziyang Ma", "Yakun Song", "Chenpeng Du", "Jian Cong", "Zhuo Chen", "Yuping Wang", "Yuxuan Wang", "Xie Chen"], "abstract": "Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM), have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategies\u2014early fusion, middle fusion, and late fusion\u2014are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts2.", "sections": [{"title": "1 Introduction", "content": "Dialogue is the most natural way of human-computer interaction (HCI). With the rapid development of GPT-style [29] large language models (LLM) and the scaling of Transformer-style [39] architectures, textual conversational AI, such as ChatGPT [27, 1] and LLaMA [36, 37], have become a significant part of daily life. However, these models are limited to text input and output and cannot interact directly with humans in arbitrary scenarios.\nIncorporating spoken and auditory interfaces into conversational AI enhances HCI convenience. Leveraging techniques from text LLMs, the speech language model (SLM) processes speech similarly to text. This paradigm involves encoding the speech signal into discrete tokens or continuous embeddings, modeling them with a language model, and decoding the speech tokens or embeddings back to the speech signal. Some studies [19, 17, 26] utilizes this paradigm for speech continuation, generating expressive speech and natural multi-round dialogue. Other research employs this paradigm"}, {"title": "2 Related Work", "content": "Figure 1 illustrates the distinctions between simplex, half duplex, and full duplex speech language models from a telecommunication perspective. An SLM with full duplex modeling (FDM) capability can be referred to as an interactive speech language model (iSLM)."}, {"title": "2.1 Simplex and Half Duplex Speech Language Model", "content": "Simplex SLMs, depicted in Figure 1(A) and 1(B), are limited to a single channel, either for listening or speaking. With the assistance of LLM, simplex SLMs exhibit strong understanding capabilities."}, {"title": "2.2 Full Duplex Speech Language Model", "content": "Full duplex SLMs, as shown in Figure 1(D), have the capability to listen and speak simultaneously, allowing for turn-taking whenever a human interrupts the machine. Recent efforts [49, 41] have attempted to build full duplex capabilities on text-centric LLMs with cascade ASR and TTS modules. Cutting-edge products like GPT-40 3 and Moshi 4 exhibit full duplex capability in their spoken dialogue systems. Despite these advancements, there are no publicly available open-source models or detailed analyses of full duplex SLMs. This gap highlights the need for further research and development to fully understand and optimize full duplex capability in speech language models."}, {"title": "3 Full Duplex Modeling (FDM)", "content": "A simplex or half duplex spoken dialogue system can be modeled by finding the parameters 0 that maximize the log-likelihood function, formulated as:\n$\\\\max \\sum log P_0(R|C)$"}, {"title": "4 Proposed LSLM", "content": "The core difference between LSLM and previous speech language models lies in its capability to simultaneously speak and listen. We first introduce the speaking capability of LSLM, followed by its listening capability, and finally, we discuss various fusion methods that integrate these capabilities, endowing LSLM with full duplex ability."}, {"title": "4.1 Speaking Ability", "content": "To simulate the speaking ability of the LSLM, we utilize an autoregressive token-based TTS model. Unlike VALL-E-styled models that combine autoregressive (AR) and non-autoregressive (NAR) approaches with multi-layer residual vector quantization (RVQ) tokens, our model employs a single layer of discrete audio tokens. This design better meets the requirements for real-time interaction, as it eliminates the need to wait for the completion of AR token synthesis before performing NAR operations. Given target speech XR, an SSL encoder Enc is utilized to obtain a continuous embedding R, which can be written as:\n$R = Enc(X_R).$\nTo train an autoregressive TTS model based on discrete tokens, we quantize the speech embedding R, denoted by:\n$R^0 = Qnt(R),$\nwhere Qnt is the discretization operation and R\u00ba are the discrete tokens. Given the context informa- tion C, in this scenario the text content to be synthesized, the model synthesizes the corresponding speech discrete tokens autoregressively. We minimize the negative log-likelihood of the target se- quence to train the decoder-only model, conditioned on the preceding tokens and the context. The loss function is defined as:\n$L(\\theta_s) = - \\sum_{t=1}^{tEOS} log P(\\mathbf{r}_t^0 | \\mathbf{R}_{1:t-1}^0, C; \\theta_s),$"}, {"title": "4.2 Listening Ability", "content": "Given the audio input XS of the listening channel, the same SSL encoder Enc in Equation 4 is used to obtain a continuous embedding S, which can be written as:\n$S = Enc(X_S),$\nwhere XS can be a variety of sound signals, including environmental noise and human speech. Unlike training the speaking ability, which involves a discretization module, the listening channel embedding S is fed into the neural network end-to-end via a projection module Proj, which can be written as:\n$S^P = Proj(S),$\nwhere the listened audio signal is mapped to a space that can be processed by the AR model."}, {"title": "4.3 FDM Ability", "content": "LSLM has two channels: speaking and listening. At time step t, all previous information of the speaking channel R1:t\u22121 and the processed information of the listening channel S:t\u22121 are considered by the model simultaneously. Here we revise Equation 6 as follows:\n$L(\\Theta_{LS}) = \\begin{cases} \\sum_{t=1}^{T_{IRQ}} log P(\\mathbf{r}_t^0 | \\mathbf{R}_{1:t-1}^0, \\mathbf{S}_{1:t-1}^P, C; \\Theta_{LS}) & \\text{if turn-taking,} \\\\ \\sum_{t=1}^{T_{EOS}} log P(\\mathbf{r}_t^0 | \\mathbf{R}_{1:t-1}^0, \\mathbf{S}_{1:t-1}^P, C; \\Theta_{LS}) & \\text{otherwise.} \\end{cases},$\nwhere OLS are the parameters to model the proposed LSLM with listening-while-speaking ability. In addition to the EOS token, we add an interruption token IRQ to the tokenizer vocabulary to allow the model to terminate early if turn-taking occurs. For example, if a human interrupts, the model should stop speaking within a detection interval \u03bc seconds after the interruption starts. During inference, the model samples from a conditional distribution based on the already generated tokens Rit-1, the context C, and most important, real-time listened audio tokens Sp.t-1. The revised formula from Equation 8 is written as follows:\n$\\mathbf{\\hat{r}} \\sim P(\\mathbf{r} | \\mathbf{R}_{1:t-1}^0, \\mathbf{S}_{1:t-1}^P, C; \\Theta_{LS}),$\nin which, an essential requirement for the SSL encoder Enc is that it is streaming. Thus, LSLM can obtain real-time audio features during inference. This is detailed further in Section 5.1.\nTo comprehensively explore the integration of a listening channel to the proposed LSLM, we try to fuse the listening channel and the speaking channel with early, middle, and late methods, as shown in Figure 3.\nEarly Fusion integrates the listening and speaking channels at the input embeddings before autoregressive prediction.\nMiddle Fusion merges the listening and speaking channels at each Transformer block. Specifically, in addition to the hidden states of the speaking channel and positional embeddings, the listening channel is additionally added to the input of each Transformer block.\nLate Fusion combines the channels at the output logits before the softmax operation."}, {"title": "5 Setup", "content": ""}, {"title": "5.1 Model Details", "content": "The backbone of the proposed LSLM employs a decoder-only Transformer architecture consisting of 12 Transformer blocks, 12 attention heads, 768 embedding dimensions, and 3072 feed-forward layer dimensions, resulting in 106M parameters. SSL encoder vq-wav2vec [2] is employed to extract audio features and further convert speech features to discrete tokens. vq-wav2vec, a fully convolutional self-supervised pre-trained model with 20 layers of 1D convolutional neural networks with 34M parameters, is naturally suitable for streaming audio feature extraction. A simple linear layer serves as the projection module to adapt the listening channel features to the AR model. A GAN-based token-to-waveform vocoder [12] is utilized to recover discrete audio tokens to speech waveform."}, {"title": "5.2 Data Details", "content": "We evaluate the proposed LSLM under two full duplex modeling (FDM) settings: command-based FDM and voice-based FDM. Table 1 summarizes the datasets and experimental settings. For the TTS datasets, we utilize the LibriTTS dataset [47] with 585 hours of speech-text pairs for training and validation. LibriTTS-testsetB [12] is adopted for testing, which contains 500 utterances sampled from the test-clean subset of LibriTTS with 37 unseen speakers. Background noise is uniformly sourced from the Freesound portion of the MUSAN dataset [34], which includes high-frequency noise such as telephone ringing and sounds of the explosion, as well as low-frequency noise such as white noise and traffic noise. The model needs to distinguish the human voice from the noise, so as to avoid turning-taking with any random input signals and avoid trivial solutions. Different interruption data is constructed based on the FDM settings.\nCommand-based FDM. In this setting, LSLM can only be interrupted by specific keywords. Timbre of 22 boutique speakers from SEED-TTS [31] is used to synthesize the command \"Honey\" for the command-based FDM.\nVoice-based FDM. In this setting, LSLM can be interrupted by a variety of different words. The Speech Commands Dataset [47] is a set of one-second audio, each containing a single spoken English word. We split the dataset into training, validation, and test sets in an 8 : 1 : 1 ratio, resulting in 51,088, 6, 798, and 6, 835 pieces of data, respectively. In addition, we use a speaker independence setting, which guarantees that the speakers in the test set do not appear in the training set, simulating more challenging and realistic scenarios."}, {"title": "5.3 Training and Inference Details", "content": "We train the model with TTS, interruption, and noise datasets for 20 epochs. For each sample, noise is added with a 50% probability, and interruption with a 50% probability, to the listening tokens. If a sample is selected to include an interruption, we modify the sentence to output the IRQ token \u03bc = 0.5 seconds after the start of the interruption and then stop outputting the remaining speaking tokens. This ensures that the model can correctly handle different audio signal combinations in the listening channel. The optimization strategy involves using AdamW [23] with a max learning rate of 5 \u00d7 10-4 without weight decay and a batch size of 4. The learning rate scheduler involves a warm-up phase for the first 5,000 steps, followed by a cosine decay of the learning rate. Validation is performed at the end of each epoch, and the checkpoint with the lowest loss is selected for inference. The generation process employs Top-P sampling with a top-p value of 0.99 and a temperature of 1.0."}, {"title": "6 Experiments", "content": ""}, {"title": "6.1 Evaluation Metrics", "content": "TTS capability evaluation. We evaluate whether the speech generation capability is affected by the full duplex modeling in the proposed LSLM. The word error rate (WER) comparing the generated speech to the original text is considered as the TTS capability evaluation metrics using Whisper large v35 [30].\nInteractive capability evaluation. Interactivity capability evaluation aims to measure how well the proposed LSLM responds to real-time and unpredictable input from the listening channel. A successful turn-taking is defined as the model stopping speaking within the [0, 2\u03bc] interval (1 second in our setting) after the interruption begins. Based on this, we categorize the outcomes into four cases: interruption and hit (TP), interruption and miss (FN), no interruption and hit (FP), and no interruption and miss (TN). From these cases, we construct a confusion matrix and calculate the Precision, Recall, and F1 score. These metrics consider both the success rate of turn-taking (Recall) and the rate of misjudgments (Precision), providing a comprehensive evaluation of the model's interactivity capabilities."}, {"title": "6.2 Experiments results", "content": "We conduct a series of experiments to evaluate the command-based and voice-based FDM for both TTS capability and interactive capability. For TTS capability, we use a test set consisting of 500 utterances, referred to as LibriTTS-testsetB [12], without any interruptions in the listening channel. The primary metric for this evaluation is WER. For the interactive capability evaluation, we employ a set of 1000 utterances divided into two equal parts: 500 utterances with interruptions at a random time step and 500 utterances without interruptions. Interactive capability is measured using Precision, Recall, and F1 Score.\nAdditionally, we test the models under two listening channel conditions: without noise, donated as Clean, and with noise, donated as Noise. For the baseline Vanilla TTS model, since it does not involve a listening channel, the input is inherently clean. By comparing the clean scenarios, we assess whether the intrinsic TTS capability is affected. Additionally, integrating noisy external inputs provides a better simulation of real-world scenarios.\nCommand-based FDM. For command-based FDM, we test the three architectures described in Section 4.3 to fuse the listening channel and the speaking channel, which are early fusion (LSLMEF), middle fusion (LSLMMF), and late fusion (LSLMLF). The results are shown in Table 2. For TTS capability, The baseline Vanilla TTS model without a listening channel achieves a WER of 4.28%. LSLMMF outperforms LSLMEF and LSLMLF with a WER of 4.05% in clean conditions and maintains a relatively low WER of 4.51% in noisy conditions. The TTS ability of LSLMEF Sshows a notable decrease, likely due to the fusion of input embeddings, making it difficult for the model to distinguish the information of the listening and speaking channels, negatively impacting the next token prediction. For interactive capability, all three architectures perform well with an oracle clean listening channel. However, LSLMLF shows a notable drop in performance under noisy conditions, with the F1 score falling to 94.89%. Observing that the late fusion method appears to mainly affect the precision score when the listening channel is noisy, suggests that the LSLMLF model reduces the discrimination of noise and human voice, leading to misjudgments of interruptions. In summary, the middle fusion approach demonstrates superior performance in TTS capability and competitive performance in interactive capability. Therefore, LSLMMF is concluded to be the best-performing model among those tested.\nVoice-based FDM. We utilized a more diverse set of interruption commands compared to the command-based FDM and involved unseen speakers in the testing procedures. The best configuration from the command-based FDM, the LSLMMF model, was selected to evaluate the voice-based FDM capability. The results are shown in Table 3. LSLM shows a higher WER of 5.33% in clean conditions and 8.50% in noisy conditions compared to the Vanilla TTS model, demonstrating the challenges posed by the real-world turn-taking problem. Comparing the results with the command- based FDM using the LSLMMF model, we find that the voice-based setting faces greater challenges in maintaining high performance, especially under noisy conditions with Precision at 87.69%, Recall at 82.77%, and an F1 score of 85.15%. The diverse set of interruption commands and the involvement of unseen speakers add complexity, resulting in higher error rates."}, {"title": "6.3 Ablation Study", "content": "In this section, we conduct an ablation study on LSLM with middle fusion architecture to evaluate the impact of different training methods on the performance of TTS capability and interactive capability. The training methods are categorized as training from scratch (X), loading the pre-trained model and fixing the parameters (\u2714), and loading the pre-trained model and continuing training (+). The detailed results are presented in Table 4.\nThe vanilla TTS model, trained from scratch, achieves a WER of 4.28% concerning TTS capability. For the interactive capability, the vanilla TTS model does not have a listening channel, hence no metrics are available. For the LSLM model, the best performance is observed when both the TTS backbone and streaming SSL encoder are loaded and continue training (+ & +), achieving the lowest WER of 4.05% and highest Precision of 97.80%, Recall of 98.19%, and F1 Score of 98.00%. Some conclusions can also be drawn from these experiments. For example, the SSL encoder of the listening channel performs better when it can be continued training than fixed the parameters. One potential reason is that the SSL encoder has not encountered diverse noise during pre-training, creating a bottleneck for extracting audio with mixed human voice and noise when using fixed pre-trained parameters."}, {"title": "7 Conclusion", "content": "In this paper, we address the challenges of enhancing real-time interaction by introducing full duplex modeling (FDM) in interactive speech language models (iSLM). We introduce listen-while-speaking language model(LSLM), an innovative end-to-end model designed to handle real-time turn-taking. LSLM integrates a token-based decoder-only TTS model for speech generation and a streaming SSL encoder for audio input, enabling simultaneous listening and speaking. We propose three strategies for fusing duplex signals: early fusion, middle fusion, and late fusion. Among these, Middle Fusion demonstrates a superior balance between speech generation and real-time interaction capabilities. The proposed LSLM is evaluated in two settings: command-based FDM and voice-based FDM. Our experiments show that LSLM is robust to noisy environments and responsive to diverse instructions from unseen speakers, achieving effective duplex communication with minimal impact on system performance. Our work is an initial exploration into full duplex interactive speech language models, and there is still a long way to go to achieve smooth human-computer speech interaction. There is a lot to explore in the future, such as developing speech-in speech-out dialogue systems with full duplex modeling ability, incorporating speaker-following capability to identify interrupting speakers, and exploring audiovisual co-guidance for improved turn-taking."}]}