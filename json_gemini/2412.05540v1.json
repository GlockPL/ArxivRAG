{"title": "Towards 3D Acceleration for low-power\nMixture-of-Experts and Multi-Head Attention\nSpiking Transformers", "authors": ["Boxun Xu", "Junyoung Hwang", "Pruek Vanna-iampikul", "Yuxuan Yin", "Sung Kyu Lim", "Peng Li"], "abstract": "Spiking Neural Networks(SNNs) provide a brain-inspired and event-driven mechanism that is believed to be critical to unlock energy-efficient deep learning. The mixture-of-experts approach mirrors the parallel distributed processing of nervous systems, introducing conditional computation policies and expanding model capacity without scaling up the number of computational operation. Additionally, spiking mixture-of-experts self-attention mechanisms enhance representation capacity, effectively capturing diverse patterns of entities and dependencies between visual or linguistic tokens. However, there is currently a lack of hardware support for highly parallel distributed processing needed by spiking transformers, which embody a brain-inspired computation. This paper introduces the first 3D hardware architecture and design methodology for Mixture-of-Experts and Multi-Head Attention spiking transformers. By leveraging 3D integration with memory-on-logic and logic-on-logic stacking, we explore such brain-inspired accelerators with spatially stackable circuitry, demonstrating significant optimization of energy efficiency and latency compared to conventional 2D CMOS integration.", "sections": [{"title": "I. INTRODUCTION", "content": "Transformer models have significantly enhanced capabilities in language and vision tasks, gaining widespread adoption across diverse application domains [1], [2]. Compared to Convolutional Neural Networks (CNNs), the self-attention mechanism in transformers captures contextual relationships globally among all tokens in a long sequence, unifying global and local sequence details into a unified representation. Within transformer architectures, Mixture-of-Experts (MoE), inspired by conditional computing mechanisms in neuroscience, has been widely adopted [3], [4]. MoE enables individual experts to specialize in learning specific features or storing task-specific knowledge, achieving significant performance improvements [5], [6]. By leveraging a learnable gating function to efficiently route input tokens to appropriate experts, MoE decouples computational cost from model parameter size, achieving high scalability.\nAs the third generation of neural networks [7], spiking neural networks (SNNs) exhibit a closer resemblance to biological neurons than conventional non-spiking artificial neural network (ANN) counterparts [8]. SNNs leverage powerful temporal coding, enable spatiotemporal computation through binary activations, and achieve ultra-low energy consumption on dedicated neuromorphic hardware platforms [9]\u2013[11]. Recent advancements in spiking neural network-based transformer models have successfully integrated self-attention mechanisms from traditional transformers into spiking neuron architectures. These models demonstrate superior performance over conventional network architectures [12]\u2013[15], mirroring the trend observed in ANNs where vision transformers outperform ResNets.\nCurrently, there is a lack of dedicated hardware architectures designed for spiking transformers [12]\u2013[15], particularly for spiking MoE transformers, presenting unique design challenges. First, existing neuromorphic accelerators typically provide limited parallelism, constrained either to temporal dimensions [16]\u2013[18] or spatial dimensions [19]. Moreover, these designs are predominantly tailored for accelerating spiking CNNs, making them ill-suited to address the computational demands and unique characteristics of large-scale spiking MoE transformer models. Second, implementing brain-inspired algorithms on 2D silicon faces significant hardware overhead, including high memory access costs, complex routing requirements, and low computational density. These limitations prevent 2D designs from achieving the high computational efficiency of three-dimensional brain-like computing architectures because they defy the need for low power consumption in SNN. Third, a naive 3D expert-by-expert implementation doesn't fully exploit the parallelism in spiking MoE transformers, leading to repeated weight loading and ignoring the distributed parallelism of spiking experts.\nChallenges and Contributions In this work, we leverage face-to-face (F2F)-bonded 3D integration technology to design dedicated spiking Mixture-of-Experts (MoE) transformer accelerators incorporating spiking Multi-Head Attention (MHA) mechanisms. The proposed architecture employs both memory-on-logic and logic-on-logic configurations.\n\u2022 We present the first dedicated 3D accelerator architecture for spiking MoE transformers, efficiently exploring spatial and temporal parallelism weight reuse within modularized spiking experts and exploiting spiking expert parallelism for both MoE and MHA, supporting a scalable and efficient spike-based computation in MoE transformers.\n\u2022 We explore the first 3D memory-on-logic and logic-on-logic interconnection schemes for the parallel distributed spiking MoE transformers to significantly reduce energy consumption and latency, thereby delivering highly efficient spiking neural computing systems with minimal area overhead.\nCompared to the 2D CMOS integration, the 3D accelerators dedicated for spiking MoE transformers offer substantial improvements. For the spiking MoE and MHA workloads, it provides a 3%-5.1% increase in effective frequency, 39%-41% area reduction, 26.9%-29% memory access latency reduction and up to 14.4% power reduction."}, {"title": "II. BACKGROUND", "content": "A. Spiking Neural Networks\n1) Neuron Models in SNNs: The spiking neuron models, as temporal activation functions, are widely used in SNNs. Leaky Integrate-and-Fire (LIF) models and Integrate-and-Fire (IF) models are commonly adopted. The LIF model simulates and mimics a neuron's response, exhibiting the following temporally discrete behaviors over multiple timesteps:\n$V_i [k] = V_i [t_{k-1}] + \\sum_{j \\in RF} W_{ji}S_j[t_k] \u2013 V_{leak}$\n$S_i [t_k] = \\begin{cases} 1 \\text{ if } V_i[t_k] > V_{th} \\rightarrow V_i[t_k] = 0 \\\\ 0 \\text{ else } V_i[t_k] = V_i[t_k] \\end{cases}$\n2) Spiking Multi-head Attention: The Spiking Multi-Head Attention Mechanism splits the $D$ features of spiking $Q/K/V \\in R^{T\\times N\\times D}$ into $H$ slices, denoted as the number of attention heads, as $Q/K/V \\in R^{N\\times T\\times H\\times d}$, where $D = H\\times d$. Here, $T$ denotes the number of timesteps, and $N$ denotes the number of tokens. Each $Q^h \\in R^{N\\times T\\times d}$ and $K^h \\in R^{NxTxd}$ formulates $H$ spiking attention maps across $A \\in R^{H\\times T\\times N\\times N}$. Each attention map is applied to the spiking value to generate the output as $Y = AV$."}, {"title": "B. Mixture-of-Experts (MoE) Models", "content": "Mixture-of-Experts (MoE) is a machine learning architecture that has gained traction for its high scalability. MoE models, leveraging a learnable routing network $W_r \\in R^{D_{in}\\times E}$ to compute gating scores for $E$ experts, intelligently route input tokens to one or more of the most appropriate experts. These models are typically built on top of transformer-based models, where the traditional feed-forward network in each transformer layer is replaced with a combination of a gating network and multiple experts. To scale efficiently, MoE typically distributes experts across multiple GPUs [20], assigning one or more experts to each GPU while replicating non-expert parameters across all GPUs. However, the high energy consumption of GPUs conflicts with the low-power requirements of spiking neural networks (SNNs). To address this challenge, we propose a compact distributed MoE accelerator based on 3D integrated circuits, bridging the gap between scalability and energy efficiency [21]."}, {"title": "C. Neuromorphic Accelerators", "content": "Various neuromorphic accelerators have been developed to support SNN inference at multiple levels, including devices [22], circuits [23] and architectures [10], [11], [19]. While these accelerators contribute to the advancement of generic SNNs, they lack optimization for the scalable spiking MoE transformers. The exploration of 3D integrated circuits (3D ICs) for SNN hardware primarily focuses on monolithic 3D (M3D) [24] and face-to-face (F2F) bonding techniques [25], which are often based on traditional liquid state machine (LSM) architectures. Although these methodologies improve power-performance-area (PPA) metrics, they are limited by constraints in neuron count, which hinder effective dataflow optimization."}, {"title": "III. PROPOSED 3D SPIKING MIXTURE-OF-EXPERTS(MOE)\nTRANSFORMER ACCELERATORS", "content": "A. Workload Processing of the Proposed Spiking MoE Layers\nSpiking Mixture-of-Expert(MoE) layers in spiking transformers encompass five core processing steps: Spiking (tokens) Conditional Routing(SCR), \u2461 Spiking Synaptic Integration(SSI), \u2462 Spiking Membrane potential Accumulation(SMA), \u2463 Spiking Conditional Generation(SCG), and \u2464 Spiking Aligned Merging(SAM). Among these steps, \u2460 and \u2464 are inter-expert operations, which handle communication for routing and merging between different spiking expert pathways, while \u2461\u2462\u2463 are intra-expert computations, responsible for the computationally intensive processing leveraging each individual spiking expert's knowledge.\nStep processes the pre-synaptic activation $S_{in} \\in {0,1}^{N\\times T\\times D_{in}}$ to compute spiking expert scores $I \\in R^{N\\times E}$, which quantify the importance of each spiking expert for a given spiking token. The top-K spiking experts are selected and assigned to the corresponding spiking tokens by routing. In step 2, each spiking expert processes the routed pre-synaptic spikes tokens for expert e, denoted as $S_{in}^{(e)} \\in {0,1}^{N_e\\times T\\times D_{in}}$. Each expert adapts pre-trained expert-specific weight $W^{(e)} \\in R^{D_{in} \\times D_{out}}$, to compute the (post-)synaptic integration $X^{(e)} \\in R^{N_e\\times T\\times D_{in} \\times D_{out}}$. The step sequentially accumulates the synaptic integration of each neuron $i$ for each token $n$ at timestep $t$, denoted by $X_n^{(e)}[t]$ onto the membrane potential at timestep $(t-1)$, to update membrane potential $V_i^{(e)}[t]$. Following this, each spiking expert, in step \u2463, performs conditional spike generation at each timestep as outlined in Equ.2, generating $S_{out}^{(e)} \\in {0,1}^{N_e\\times T\\times D_{out}}$. Finally, as a merging function, \u2464 aggregates the outputs from all experts ${S_{out}^{(0)}, S_{out}^{(1)}, ..., S_{out}^{(E-1)}}$ to generate the aligned $S_{out} \\in {0,1}^{N\\times T\\times D_{out}}$ as outputs.\nB. 3D Integrated Mixture-of-Expert (MoE) Spiking Transformer Accelerators\nIn our proposed 3D integrated two-tier Spiking MoE accelerator design, as shown in Fig. 2(a), we assemble multiple modularized Spiking Expert (SE) cores on a single chip and place two sharable expert-weight Global Buffers (GLBs) between the distributed SEs on the top tier. A centralized spiking activation GLB is also placed on the top tier, managing the spiking workload. Additionally, a two-tier spiking token router is placed between the four SE cores to enable the spiking expert parallelism as shown in Fig. 2(c) by handling and", "subsections": []}, {"title": "Algorithm 1 Kernel-Fused Parallel Processing for Spiking\nDistributed Mixture-of-Experts Layers.", "content": "Input: the number of spiking expert $E$, spiking activation $S_{in} \\in {0,1}^{N\\times T\\times D_{in}}$, expert routing weight $W_r \\in R^{T\\times D_{in}\\times E}$, distributed expert weight $W \\in R^{E\\times D_{in}\\times D_{out}}$.\nOutput: spiking output $S_{out} \\in {0,1}^{N\\times T\\times D_{out}}$.\nLoad $W_r$ and $S_{in}$ to Expert Routing Score Array\nCompute expert scores $I$\nRoute $S_{in}$ into ${S_{in}^{(0)}, ..., S_{in}^{(E-1)}}$ by selecting Top-K experts\nfore = 0 to $E-1$ do\nPreload $W^{(e)}$ from $W$ GLB0/1 into the LB.\nCompute $S_{out}^{(e)}$ adapting modularized spiking expert accelerators\nWriteThrough $S_{out}^{(e)}$ to Spiking Act LB\nend for\nMerge ${S_{out}^{(0)}, ..., S_{out}^{(E-1)}}$ to $S_{out}$ and WriteThrough to Act GLB\n\u2464. Each SE core is responsible for performing computations corresponding to \u2461+\u2462+\u2463 in parallel.\nAs illustrated in Alg. 1, the router adapts $W_r$ to compute expert scores and based on the scores to route spiking tokens to distributed spiking expert cores. Each spiking expert $e$ receives routed workload $S_{in}^{(e)}$ and applies expert-specific weight $W^{(e)}$ locally to generate $S_{out}^{(e)}$ in parallel, which is then merged and written back to the Activation GLB. Correspondingly, within the spiking token router, an expert routing score array is placed at bottom tier which computes the routing score for tiled tokens and the scores can be vertically extracted to the top tier. Then, the router selects the top-K spiking experts for each token, and routes the packed spiking tokens to multiple spiking experts. Within each SE core, the systolic PE array core placed at the bottom tier and a dedicated spiking generator core is placed at the top, executing kernel-fused operations of \u2461+\u2462+\u2463.\nAs illustrated in Fig. 2(a), the optimized dataflow proceeds as follows: Pre-synaptic activation tiles ($S_{in}$) and expert routing weights ($W_r$)"}, {"title": "Algorithm 2 Kernel-Fused Parallel Processing for Spiking\nMulti-head Attention Layers", "content": "Input: Spiking query $Q \\in {0,1}^{T\\times N\\times H\\times d}$, Spiking key $K \\in {0,1}^{T\\times N\\times H\\times d}$, Spiking value $V \\in {0,1}^{T\\times N\\times H\\times d}$, Kernel-fused operations of Spiking Attention core SpikingAttention.\nOutput: spiking output $S_{out} \\in {0,1}^{T\\times N\\times D}$.\nPartition Q/K/V loaded from spiking Act GLB into ${Q^{(0)}, ..., Q^{(H-1)}}$, ${K^{(0)}, ..., K^{(H-1)}}$, ${V^{(0)}, ..., V^{(H-1)}}$ \nfor h = 0 to H-1 do\nDispatch $Q^{(h)}, K^{(h)}, V^{(h)}$ to Spiking Attention Core h\n$S_{out} \\gets SpikingAttention(Q^{(h)}, K^{(h)}, V^{(h)})$ \nend for\nMerge $S_{out} \\gets Concat(S_{out}^{(0)}, ..., S_{out}^{(S^{H-1})})$\nWrite Through $S_{out}$ to Spiking Act GLB\n$S_{out}^{(e)}$ propagate vertically and are reused across output neurons.\nAs shown in Fig. 2(c), each PE is designed with a synaptic integration-stationary approach. Synaptic integrations stored within PEs are 3D-extractable via dedicated readout ports, enabling spiking generators in the top tier to access data efficiently, thereby improving computation density."}, {"title": "IV. PROPOSED 3D SPIKING MULTI-HEAD\nATTENTION(MHA) ACCELERATORS", "content": "A. Workload Processing of the MHA Layers\nThe computation of spiking MHA layers is another bottleneck and has several key operations: \u2460 the computation of spiking attention maps ($A = QK^T$), \u2461 attention-weighted synaptic integration ($X = AV$), which provides inputs to a set of LIF neurons for generating the final binary spike-based attention output, \u2462 membrane potential accumulation of these LIF neurons, and \u2463 conditional generation of the LIF neuron output spikes as the final attention output. In operation, the spiking query $Q$ and spiking key $K$, initially shaped as $R^{T\\times N\\times D_{in}}$, are divided into $R^{T\\times N\\times H\\times d}$. Here, $T$ represents the number of timesteps; $N$ denotes the number of tokens; $H$ and $d$ indicate the number of self-attention heads and the number of features per head, respectively. A spiking attention map $S \\in R^{T\\times H\\times N\\times N}$ is computed for each head at each timestep. For instance, the spiking attention map at t-th timestep for h-th self-attention head results from the binary matrix multiplication of the spiking query and key at the specific head and timestep. In 2, the attention-weighted synaptic integration is executed for each head at each timestep. The spiking attention map A, serving as the attention weights, is combined with the spiking value V, shaped in $R^{T\\times N\\times H\\times d}$ to compute attention-weighted synaptic integration, denoted by X shaped as $R^{T\\times N\\times H\\times d}$.\nB. 3D Integrated Spiking Multi-Head Attention Transformer Accelerators\nIn the MHA mechanism described above, the computations of different heads can be neatly separated and processed by different spiking attention expert cores. The 3D MHA accelerators partition along with features and dispatch the partitioned features into different spiking attention experts to compute spiking outputs in parallel, and then neatly concat"}, {"title": "V. EVALUATIONS", "content": "A. Experiment Settings\nSoftware Settings We evaluated the spiking MoE transformer models on CIFAR10 and CIFAR100 [26]. The models utilize 8-bit quantized weights and 16-bit quantized synaptic integration. Each expert is allocated the same amount of weight parameters and trained for 100 epochs. We set feature size, patch size, head size and batch size to 128, 4x4, 16 and 512, respectively. We set top-1 routing to keep an approximate number of operations with increasing the number of experts. We demonstrate that as the number of spiking experts scale up, the performance is improved significantly in Tab. I.\nHardware Settings In this work, we use the commercial 28nm PDK to implement both 2D and 3D F2F designs. The 2D design consists of 6 metal layers, while the 3D design features a double metal stack of the 2D design, with the F2F bond pitch varying from 0.5\u00b5m to 1\u00b5m. We use Synopsys Design Compiler to synthesize the RTL to a gate-level netlist and Cadence Innovus to perform physical synthesis. We utilize the pin-3D [27] flow, where the top and bottom dies are iteratively optimized across placement, CTS, routing, and sign-off stages. For memory, we utilize SRAM modules generated by a commercial memory compiler for various global buffers, local buffers, and other storage functions within our system architecture. On the top tier, for MoE accelerators, we adapt two 8K\u00d7128b SRAM units employed for the Weight GLB A and Weight GLB B, which store shareable expert weights. For both MoE and MHA accelerators, an additional 8K\u00d7128b SRAM unit is adapted to manage and store spiking activations. Within each modularized expert core, 3K\u00d7128b SRAM units are employed for Activation, Weight, and Synaptic Integration Local Buffers. Additionally, smaller 96\u00d7128b SRAM macros are allocated for the Query (Q) buffer, Key/Value (K/V) buffer, and Spiking (S) buffer on the bottom tier. Two 96\u00d7256b SRAM macros are configured to serve as extended X buffers. The memory macro placement is determined based on the architecture information in Fig. 3 (for the MHA accelerator) and Fig. 2 (for the MoE accelerator). In MoE accelerators, we use a 16 \u00d7 128 PE array with a 16 \u00d7 8 routing score computing array; For the modularized spiking attention expert core, the size of reconfigurable attention array is 16 \u00d7 16."}, {"title": "B. Performance Comparison between 2D and 3D", "content": "1) Layout Comparision between 2D and 3D: In Fig. 4 and Fig. 5, the placement and layout differences between the 2D and 3D designs of spiking MLP accelerators and spiking self-attention accelerators are presented. In Fig. 4(a)(b), the 2D design occupies 1.76 mm \u00d7 3.413 mm, while the stacked 3D spiking design occupies 1.921 mm \u00d7 1.75 mm. On the top tier, the W GLBs and Act GLB are placed between expert cores, as shown in Fig. 4(c)(d), while modularized Expert LBs and spiking generators are placed at the edges. Within each expert module, the weight and spike buffers are placed on the edge, and the spiking spatiotemporal array is positioned below the spiking generators. In Fig. 5, the 2D design of the spiking attention accelerator occupies 0.984 mm \u00d7 3.02 mm, while the stacked 3D spiking design occupies 0.984 mm \u00d7 1.779 mm. In the 3D MHA design, the Act GLB is stored on the top tier, while other memories are stacked at the edges.\n2) PPA Comparison between 2D and 3D: The performance comparison between 2D and 3D implementations of modularized spiking expert MHA and MoE accelerators, as shown in Tab. II, demonstrates improvements across multiple metrics through 3D integration. In terms of performance, the 3D structure operates at higher effective frequencies of 2.24 GHz and 1.74 GHz for MHA and MoE, respectively, compared to their 2D counterparts at 2.13 GHz and 1.69 GHz, while achieving a reduced area footprint.\nThe area results of the 3D implementation show 39% and 41% reductions in MHA and MoE designs, respectively. Specifically, the MHA design area decreases from 5.53 mm\u00b2 to 3.36 mm\u00b2, while the MoE design reduces from 2.97 mm\u00b2 to 1.75 mm\u00b2. This area reduction is achieved while maintaining similar cell counts, indicating effective vertical integration without functionality loss. Power consumption results also show improvements in the 3D structure. In the MHA implementation, total power consumption decreases from 912 mW to 896 mW; the MoE design shows a larger improvement, with total power consumption reducing from 6989 mW to 5983 mW, representing a 14.4% reduction. These improvements are observed across internal, leakage, and memory access power components."}, {"title": "C. Analysis of Memory Access", "content": "Memory access efficiency is a critical factor in determining the overall performance of neural network accelerators. Our analysis focuses on the memory access characteristics of both 2D and 3D implementations of MHA and MoE designs, as shown in Tab. II. The 3D structure demonstrates improved memory access performance compared to its 2D counterpart. In the MHA design, memory access latency decreases from 160 ps to 112 ps, representing a 30% reduction. Similarly, the MoE implementation shows a 15% reduction in memory access latency, from 202 ps to 172 ps. These reductions in latency can be attributed to the shortened interconnect distances achieved through vertical integration. The improved memory access efficiency is also reflected in power consumption. The MHA design shows a 29% reduction in memory access power, decreasing from 6.23 mW to 4.41 mW. Similarly, the MoE design achieves a 26.9% reduction, from 7.11 mW to 5.2 mW. These reductions in both latency and power consumption demonstrate how 3D integration can optimize memory access patterns in neural network accelerators."}, {"title": "D. Average Wirelength of 2D and 3D designs", "content": "The wirelength comparison between 2D and 3D implementations, as shown in Tab. IV, demonstrates notable reductions through 3D integration. For the single expert implementation, the MHA design shows a slight decrease from 0.621 m to 0.616 m, while the MoE design achieves a more significant reduction from 2.178 m to 1.959 m. The improvement becomes more pronounced in the 4-modularized expert systems, where the MHA design's wirelength reduces from 3.654 m to 3.290 m, and the MoE design shows a substantial decrease from 11.352 m to 9.816 m. The reduction in wirelength directly contributes to the improved performance metrics observed in both designs. This is particularly evident in the hierarchical memory access characteristics detailed in Tab. III.\nAt the Global Buffer (GLB) level, the MHA design shows reduced activation latency from 220 ps to 209 ps, with power consumption decreasing from 10.9 mW to 7.56 mW. The MoE design demonstrates even more substantial improvements, with activation GLB latency reducing from 148 ps to 117 ps and notable reductions in weight GLB latencies from 241/147 ps to 94/71 ps. The Local Buffer (LB) and buffer-level metrics show similar improvements, with particularly significant reductions in the activation and weight buffer latencies and power consumption. These improvements can be attributed to the optimized wirelength and more efficient signal routing achieved through 3D integration. These results demonstrate how the reduced wirelength in 3D designs contributes to enhanced system performance through improved signal propagation and reduced power consumption in memory access operations."}, {"title": "VI. CONCLUSIONS", "content": "We present the first dedicated 3D acceleration for MoE and MHA spiking transformers, leveraging spatial and temporal parallelism, modularized 3D spiking expert acclerators, and efficient interconnections. Our 3D acceleration achieves significant improvements over 2D CMOS integration, delivering scalable and energy-efficient spiking neural computation with minimal hardware overhead. This work enables a practical deployment of large-scale spiking MoE transformers."}]}