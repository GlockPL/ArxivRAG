{"title": "Dive into Time-Series Anomaly Detection: A Decade Review", "authors": ["PAUL BONIOL", "QINGHUA LIU", "MINGYI HUANG", "THEMIS PALPANAS", "JOHN PAPARRIZOS"], "abstract": "Recent advances in data collection technology, accompanied by the ever-rising volume and velocity of streaming data, underscore the vital need for time series analytics. In this regard, time-series anomaly detection has been an important activity, entailing various applications in fields such as cyber security, financial markets, law enforcement, and health care. While traditional literature on anomaly detection is centered on statistical measures, the increasing number of machine learning algorithms in recent years call for a structured, general characterization of the research methods for time-series anomaly detection. This survey groups and summarizes anomaly detection existing solutions under a process-centric taxonomy in the time series context. In addition to giving an original categorization of anomaly detection methods, we also perform a meta-analysis of the literature and outline general trends in time-series anomaly detection research.", "sections": [{"title": "1 Introduction", "content": "A wide range of cost-effective sensing, networking, storage, and processing solutions enable the collection of enormous amounts of measurements over time [109\u2013111, 122, 137, 138, 141, 143, 179, 181, 186]. Recording these measurements results in an ordered sequence of real-valued data points commonly referred to as time series. More generic terms, such as data series or data sequences, have also been used to refer to cases where the ordering of data relies on a dimension other than time (e.g., the angle in data from astronomy, the mass in data from spectrometry, or the position in data from biology) [176]. Analytical tasks over time series data are necessary virtually in every scientific discipline and their corresponding industries [14, 61, 62, 78, 161, 182, 190\u2013192, 201], including in astronomy [4, 102, 245], biology [11\u201313, 64], economics [36, 74, 148, 155, 213, 221, 240], energy sciences [6, 9, 158], engineering [112, 162, 203, 243, 248], environmental sciences [77, 84, 100, 101, 164, 207, 247], medicine [57, 199, 206], neuroscience [21, 119], and social sciences [36, 160]. The analysis of time series has become increasingly prevalent for understanding a multitude of natural or human-made processes [187, 188]. Unfortunately, inherent complexities in the data generation of these processes, combined with imperfections in the measurement systems as well as interactions with malicious actors, often result in abnormal phenomena. Such abnormal events appear subsequently in the collected data as anomalies. Considering that the volume of the produced time series will continue to rise due to the explosion of Internet-of-Things (IoT) applications [75, 105, 151], an abundance of anomalies is expected in time series collections.\nThe detection of anomalies in time series has received ample academic and industrial attention for over six decades [1, 27\u201330, 32, 39, 69, 121, 142, 159, 165, 175, 180, 185, 235, 239]. With the term anomalies we refer to data points or groups of data points that do not conform to some notion of normality or an expected behavior based on previously observed data [16, 45, 80, 91, 107]. In the literature, alternative terms such as outliers, novelties, exceptions, peculiarities, aberrations, deviants, or discords often appear to describe the occurrences of such rare, unusual, often hard-to-explain anomalous patterns [2, 40, 65]. Depending on the application, anomalies can constitute [2]: (i) noise or erroneous data, which hinders the data analysis; or (ii) actual data of interest. In the former case, the anomalies are unwanted data that are removed or corrected. In the latter case, the anomalies may identify meaningful events, such as failures or changes in behavior, which are the basis for subsequent analysis.\nRegardless of the purpose of the time series and the semantic meaning of anomalies, anomaly detection describes the process of analyzing a time series for identifying unusual patterns, which is a challenging task because many types of anomalies exist. They appear in different sizes and shapes. According to Foorthuis [68], research on general-purpose anomaly detection dates back to 1777, where Bernoulli's work seems to be the first addressing issues of accepting or rejecting extreme cases of observations [19]. Robust theory in that area was developed during the 1800s (e.g., method of least squares in 1805 [225]) [63, 76, 83, 198, 230] and 1900s [60, 85, 106, 197, 212] but it was not until the 1950s when the first works focused specifically in time series data [175]. In 1972, Fox conducted one of the first studies to examine anomalous behaviors across time and defined two types of anomalies: (i) an anomaly affecting a single observation; and (ii) an anomaly affecting an observation and subsequent observations [69]. In 1988, Tsay extended these definitions into four types for univariate time series [239] and subsequently for multivariate time series [241]. In the same time frame, the first few approaches appear for detecting anomalies in time series, with a focus on utilizing statistical tests such as the Likelihood-ratio test [48, 242]\nSince then, a large number of works have appeared in this area, which continues to expand at a rapid pace. Additionally, numerous surveys have been published to provide an overview of the current advancements in this field [22, 33, 43, 46, 47, 54, 56, 86, 99]. Unfortunately, the majority of the surveys focus on general-purpose anomaly detection methods and only a portion of them briefly review methods for time-series anomaly detection. Even though traditional anomaly detection methods may treat time series as any other high-dimensional vector and attempt to detect anomalies, our focus is on approaches that are specifically designed to consider characteristics of time series. To illustrate the importance of this point, in Figure 1, we present three examples of anomalies in time series applications where the temporal ordering and the collective consideration of points enable the detection of anomalies. For example, in 1(a), considering each point in isolation cannot reveal the underlying anomaly in data.\nTherefore, time-ordering features are important to be considered in the anomaly detection pipeline. Depending on the research community, multiple solutions have been proposed to tackle the above-mentioned challenge. For example, a group of methods proposed in the literature will propose a transformation step that converts time information into a relevant vector space and then apply traditional outlier detection techniques. In addition, other groups of methods will consider distances (or similarity measures dedicated to time series) to identify unusual time series or subsequences. Then, methods proposed in the deep learning community will benefit from specific architectures that embed time information (such as recurrent neural networks or convolutional-based approaches)."}, {"title": "2.1 On the Definition of Anomalies in Time Series", "content": "Attesting to the challenging nature of the problem at hand, we observe that there does not exist a single, universal, precise definition of anomalies or outliers. Traditionally, anomalies are observations that deviate considerably from the majority of other samples in a distribution. The anomalous points raise suspicions that a mechanism different from those of the other data generated the specific observation. Such a mechanism usually represents either an erroneous data measurement procedure or an unusual event.\nIn cases of errors in the data measurement procedure, the anomalous observations are marked as \"noise\" - unwanted data that are not attractive to the analyst and should be removed in the data cleaning process. Many pieces of literature have been dedicated to this type of problem, particularly in the sensor setting, where the main objective is to eliminate transmission error and render accurate predictions. In time-series anomaly detection, however, recent literature begins to center on detecting anomalous events, which indicate \"novelty\" \u2013 unusual but interesting phenomena that originate from an inherent variability in the domain of the data. A natural example of this type of problem is fraud detection for credit information, where the principal aim is to detect and analyze the fraud itself.\nThe detection of these two types of anomalies (anomalies and outliers can be used interchangeably) can be achieved via expert knowledge. By knowing exactly how the system works, the experts can set the parameter to fit a distribution of values that represent the healthy functioning state. Anomalies are then detected by marking points of more than three standard deviations away from the mean of data distribution estimated by the experts. To validate the detection process, we also need to perform extensive tests to test the distribution (and its parameters) against the dataset.\nNevertheless, in several real-world problems, we do not know precisely the data distribution that has generated a set of points (and all the different artifacts that played a role). Besides, the data distributions that we encounter in practice, are almost always rather complex and very hard to identify or approximate effectively. Consequently, defining and identifying anomalies using their distance from a mean value defined by experts is sometimes hardly practical.\nDespite the challenge of estimating distribution parameters by experts, recent developments in computational power have liberated us from an alternative approach to analyzing data distribution from the data itself. Using a variety of learning methods, researchers may apply computer algorithms to analyze raw data, estimate a fair distribution, and detect anomalies without expert knowledge. Even though being strongly dependent on the quality and the context of the datasets, these methods seem to show strong results in achieving relatively complex tasks. In this paper, we focus on this type of method."}, {"title": "2.2 Types of Anomalies in Time Series", "content": "There is a further complication in time-series anomaly detection. Due to the temporality of the data, anomalies can occur in the form of a single value or collectively in the form of sub-sequences. In the specific context of point, we are interested in finding points that are far from the usual distribution of values that correspond to healthy states. In the specific context of sequences, we are interested in identifying anomalous sub-sequences, which are usually not outliers but exhibit rare and, hence, anomalous patterns. In real-world applications, such a distinction between points and sub-sequences becomes crucial because even though individual points might seem normal against their neighboring points, the shape generated by the sequence of these points may be anomalous.\nFormally, we define three types of time series anomalies: point, contextual, and collective anomalies. Point anomalies refer to data points that deviate remarkably from the rest of the data. Figure 2(a) depicts a synthetic time series with a point anomaly: the value of the anomaly is outside the expected range of normal values. Contextual anomalies refer to data points within the expected range of the distribution (in contrast to point anomalies) but deviate from the expected data distribution, given a specific context (e.g., a window). Figure 2(b) illustrates a time series with a contextual anomaly: the anomaly is within the usual range of values (left distribution plot of Figure 2(b)) but outside the normal range of values for a local window (right distribution plot of Figure 2(b)). Collective anomalies refer to sequences of points that do not repeat a typical (previously observed) pattern. Figure 2(c) depicts a synthetic collective anomaly. The first two categories, namely, point and contextual anomalies, are referred to as point-based anomalies. whereas, collective anomalies are referred to as sequence-based anomalies.\nAs an additional note, there is another case of sub-sequence anomaly detection referred to as whole-sequence detection, relative to the point detection. In this case, the period of the sub-sequence is that of the entire time series, and the entire time series is evaluated for anomaly detection as a whole. This is typically the case in the sensor cleaning environment where researchers are interested in finding an abnormal sensor among all the functioning sensors."}, {"title": "2.3 Univariate versus Multivariate", "content": "Another characteristic of time-series anomaly detection algorithms comes from the dimensionality of the data. Univariate time series consists of an ordered sequence of real values on a single dimension, and the anomalies are detected based on one single feature. In this case, as illustrated in Figure 3(b.1), a subsequence can be represented as a vector. On the other hand, Multivariate time series is either a set of ordered sequences of real values (with each ordered sequence having the same length) or an ordered sequence of vectors composed of real values. In this specific case, as illustrated in Figure 3(b.2), a subsequence is a matrix in which each line corresponds to a subsequence of one single dimension. Instances of anomalies are detected according to multiple features, where values of one feature, when singled out, may look normal despite the abnormality of the sequence as a whole."}, {"title": "2.4 Unsupervised, Semi-supervised versus Supervised", "content": "This task can be divided into three distinct cases: (i) experts do not have information on what anomalies to detect; (ii) experts only have information on the expected normal behaviors; (iii) experts have precise examples of which anomalies they have to detect (and have a collection of known anomalies). This gives rise to the distinction among (i) unsupervised, (ii) semi-supervised, and (iii) supervised methods.\nUnsupervised: In case (i), one can decide to adopt a fully unsupervised method. These methods have the benefit of working without the need for a large collection of known anomalies and can detect unknown abnormal behavior automatically. Such methods can be used either to monitor the health state or to mine the historical time series of a system (to build a collection of abnormal behaviors that can then be used on a supervised framework).\nSemi-supervised: In case (ii), methods can learn to detect anomalies based on annotated examples of normal sequences provided by the experts. This is the classical case for most of the anomaly detection methods in the literature. One should note that this category is often defined as Unsupervised. However, we consider it unfair to group such methods with the category mentioned above, knowing that they require much more prior knowledge than the previous one.\nSupervised: While in case (i) and (ii) anomalies were considered unknown, in case (iii), we consider that the experts know precisely, what type of pattern(s) they want to detect, and that a collection of time series with labeled anomalies is available. In that case, we have a database of anomalies at our disposal. In a supervised setting, one may be interested in predicting the abnormal sub-sequence by its prior sub-sequences. Such sub-sequences can be called precursors of anomalies."}, {"title": "2.5 Anomaly Detection Pipelines", "content": "Upon summarizing the various different algorithms on different domains, we realized a common pipeline for time-series anomaly detection algorithms. The pipeline consists of four parts: data pre-processing, detection method, scoring, and post-processing. Figure 4 illustrates the process. The decomposition of the general anomaly detection process into small steps of a pipeline is beneficial for comparing different algorithms on various dimensions. Understanding of algorithms' function in the pre-processing step helps interpret its treatment of time series data specifically.\nThe data processing step represents how the anomaly detection method processes the time series data at the initial step. We have noticed all the anomaly detection models are somehow based on a windowed approach initially - converting the time series data into a matrix with rows of sliding window slices of the original time series. The pre-processing step consists of the additional processing procedure besides the window transformation, which varies from statistical feature extraction to fitting a machine learning model and building a neural network.\nAfter the data is processed, different detection methods are implemented on the dataset, which might be simply calculating distances among the processed sub-sequences, fitting a classification hyper-plane, or using the processed model to generate new sub-sequences and comparing them with original sub-sequences. The detection methods are usually traditional outlier detection methods for vector data.\nThen, during the scoring process, the results derived in the detection methods will be converted to an anomaly score that assesses the abnormality of individual sub-sequences by a real value (such as a probability of being an anomaly). The scores will be further used to infer the score of the individual point. The resulting score is a time series of the same length as the initial time series.\nLastly, during the post-processing step, the anomaly score time series is processed to extract the anomalous points or intervals. Usually, a threshold value will be determined, where the points with anomaly scores surpassing the threshold will be marked as the anomaly.\nThis categorization of time-series anomaly detection pipelines inspires our process-centric taxonomy of the detection methods, which will be discussed thoroughly in the next section."}, {"title": "3 Anomaly Detection Taxonomy", "content": "In this section, we describe our proposed process-centric taxonomy of the detection methods. We divide methods into three categories: (i) distance-based, (ii) density-based, and (iii) prediction-based. The first family contains methods that focus on the analysis of sub-sequences for the purpose of detecting anomalies in time series, mainly by utilizing distances to a given model. Second, instead of measuring nearest-neighbor distances, density-based methods focus on detecting globally normal or isolated behaviors. Third, prediction-based methods aim to train a model (on anomaly-free time series or with very few anomalies) to reconstruct the normal data or predict the future expected normal points."}, {"title": "3.0.1 Distance-based", "content": "As its name suggests, the distance-based method detects anomalies purely from the raw time series using distance measures. Given two sequences (or univariate time series), $A \\in R^l$ and $B \\in R^l$, of the same length, $l$, we define the distance between A and B as $d(A, B) \\in R$, such as $d(A, B) = 0$ when A and B are the same. There exist different definitions of d in the literature. The classical distance widely used is the Euclidean distance or the Z-normalized Euclidean distance (Euclidean distance with sequences of mean values equal to 0 and standard deviations equal to 1). Then, Dynamic Time Warping (DTW) is commonly used to cope with misalignment issues. Overall, the distance-based algorithms merely treat the numerical value of time series as it is, without further modifications such as removing seasonality or introducing a new structure built on the data. Within the distance-based models, there come three second-level categories: proximity-based, clustering-based, and discord-based models.\n(1) The proximity-based model measures proximity by calculating the distance of a given sub-sequence to its close neighborhood. The isolation of a sub-sequence with regards to its closest neighbors is the main criterion to consider if this sub-sequence is an anomaly or not. This notion of isolation with regard to a given neighborhood has been proposed for non-time series data. Thus, the methods contained in this category have been introduced for the general case of multi-dimensional outlier detection. In our specific case, the sub-sequence of a time series can be considered a multi-dimensional point with the number of dimensions equal to the length of the sub-sequence.\n(2) The clustering-based model infers anomalies from a cluster partition of the time series sub-sequences. In practice, the anomaly score is calculated by the non-membership of a sub-sequence of each of the clusters learned by the model. Other considerations, such as cluster distance and cluster capacity, can also be considered. The clustering issue is related to the anomaly detection problem in that points may either belong to a cluster or be deemed anomalies. In practice, the fact that a sub-sequence belongs or not to a cluster is assessed by the computation of the distance between this sub-sequence and the cluster centroid or medoid."}, {"title": "3.0.2 Density-based", "content": "The density-based does not treat the time series as simple numerical values but imbues them with more complex architecture. The density-based method processes time series data on top of a representation of the time series that aims to measure the density of the points or sub-sequence space. Such representation varies from graphs, trees, and histograms to a grammar induction rule. The density-based models have four second-level categories: distribution-based, graph-based, tree-based, and encoding-based.\n(1) The distribution-based anomaly detection approach is building a distribution from statistical features of the points or sub-sequences of the time series. By examining the distributions of features of the normal sub-sequences, it tries to recover relevant statistical models and then uses them to infer the abnormality of the data.\n(2) A graph-based method represents the time series and the corresponding sub-sequences as a graph. The nodes and edges represent the different types of sub-sequences (or representative features) and their evolution in time. For instance, the nodes can be sets of similar sub-sequences (using a predefined distance measure), and the edge weights can be the number of times a sub-sequence of a given node has been followed by a sub-sequence of another node. The detection of anomalies is then achieved using characteristics of the graph, such as the node and edge weights, but also the degree of the nodes.\n(3) A tree-based approach aims to divide the point or sub-sequence of a time series using trees. For instance, such trees can be used to split different points or sub-sequences based on their similarity. The detection of anomalies is then based on the statistics and characteristics of the tree, such as its depth.\n(4) A encoding-based anomaly detection model compresses or represents the time series into different forms of symbols. The encoding-based model suggests that a time series can be interpreted as a sequence of context-free, discrete symbols or states. For instance, anomalies can be detected by using grammar rules with the symbols extracted from the time series. It should be noted that an encoding-based model is not exclusive to itself; it may even be based on a graph-based or tree-based model."}, {"title": "3.0.3 Prediction-based", "content": "Prediction-based methods aim to detect anomalies by predicting the expected normal behaviors based on a training set of time series or sub-sequences (containing anomalies or not). For instance, some methods will be trained to predict the next value or sub-sequence based on the previous one. The prediction is then post-processed to detect abnormal points or sub-sequences. Then, the prediction error is used as an anomaly score. The underlying assumption of prediction-based methods is that normal data are easier to predict, while anomalies are unexpected, leading to higher prediction errors. Such assumptions are valid when the training set contains no or few time series with anomalies. Therefore, prediction-based methods are usually more optimal under semi-supervised settings."}, {"title": "3.1 Scoring Process", "content": "As summarized in the pipeline, anomaly detection algorithms distinguish outliers by inference on the \"anomaly scores\" of each temporal data point. The anomalies are marked by points whose scores exceed the threshold value. Due to the special features of time series data, a further general categorization can be provided based on the algorithm's strategy of scoring the anomalies. We include this generalization as a complement to our taxonomy."}, {"title": "6.1 Proximity-based Methods", "content": "Proximity-based methods use distance to close neighborhoods as the main step to detect anomalies. We detail below two method types of proximity-based methods.\n6.1.1 Kth Nearest Neighbor. One of the first distance-based and proximity-based methods introduced in the literature for anomaly detection is using K-th Nearest Neighbor (KNN) principle [91]. KNN-type methods utilize a metric among neighboring sub-sequences to infer the abnormality scores of the time series' sub-sequences. A distance measure d(,) (also called dissimilarity measure) is used to find the nearest neighbors for each subsequence. Common distance measures are Euclidean, Manhatten, or in general, Minkowski distances. The k-anomaly score $A_k: {T_{i,l}}_{i\\in I} \\rightarrow R$ for the set of time series' sub-sequences $ {T_{i,l}}_{i\\in I}$ is calculated based on each sub-sequences' kth nearest neighbors using a variable aggregation function agg : $R^k \\rightarrow R$:\n$A_k(T_{i,l}) = \\inf\\limits_{J \\subset I, |J|=k+1} agg_{j \\in J} d(T_{i,l}, T_{j,l})$\nIn the above equation, I is the fixed length of the sliding window, and k + 1 accounts for trivial matches. Following the intuition of [202] that the anomaly score for a subsequence $T_{i,l}$ is the distance to its $k^{th}$ nearest neighbor, we can use agg = $\\sum$. Alternative proposals for $A_k$ may utilize other aggregation methods, such as median, minimum, or other functions that pool the distances into scalar scores. With different aggregation functions and distance metrics choices, one can propose different KNN-type models that are appropriate for various anomaly detection problems.\nIn addition to the standard KNN technique [202], where the maximum distance to the $k^{th}$ nearest neighbor is used as anomaly score, other variants of KNNs have been suggested by researchers. For instance, KnorrSeq and KnorrSeq2 are also two variants of KNNs proposed in the litterature [177]. The first algorithm KnorrSeq is based on a tumbling window and discovers only global outliers by marking those sub-sequences for which at least p% of the other subsequence are further away than a threshold D. Their second algorithm KnorrSeq2 is an implementation of KNN that detects sub-sequences as outliers if at least p% of the k preceding or k succeeding sub-sequences are further away than a threshold D. The anomaly score is calculated using $\\sum$ as the aggregation function:\n$A_k(T_{i,l}) = \\sum\\limits_{j \\in J} \\begin{cases}\n1, & \\text{if } d(T_{i,l}, T_{j,l}) > D\\\\\n0, & \\text{otherwise}\n\\end{cases}$\nwith:\n$\\inf\\limits_{J \\subset I, |J|=2k+1, \\forall j \\in J, |j-i| \\leq k}$\nThe anomalous sub-sequences are selected using a threshold \u03c4 = pk on the anomaly scores: $A_k(T_{i,l}) <= \\tau$.\n6.1.2 Local Outlier Factor. The most commonly used proximity-based approach is the Local Outlier Factor (LOF) [35], which measures the degree of being an outlier for each instance. Unlike the previous proximity-based models, which directly compute the distance of sub-sequences, LOF depends on how the instance is isolated to the surrounding neighborhood. This method aims to solve the outlier detection task where an outlier is considered as \"an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism\" (Hawkins definition [91]). This definition is coherent with the anomaly detection task in time series where the different mechanism can be either an arrhythmia in an electrocardiogram or a failure in the components of an industrial machine.\nFirst, let's consider $T_{i,l}$ and $T_{j,l}$ two sub-sequences of the same time series. In the following paragraphs, we note these two sub-sequences, A and B, respectively. Given k-distance(A) the distance between A and its $k^{th}$ nearest neighbor ($N_k(A)$ the set of these k neighbors), LOF is based on the following reachability distance definition:\n$rd_k(A,B) = max(k-distance(B), d(A,B))$\nAs illustrated in Figure 9, the main concept behind this distance definition is to stress out the homogeneity of distances between instances within the k-neighborhood (i.e., the k-neighborhood will have the same distance between each other). Thus the local reachability can be defined as follow:\n$lrd_k(A) = \\frac{|N_k(A)|}{\\sum\\limits_{B\\in N_k(A)} r d_k(A,B)}$\nGiven an instance, A, $lrd_k(A)$ is the inverse of the average reachability of A from its neighborhood, i.e., the average distance at which A can be reached from its neighbors. Therefore, LOF is given by:"}, {"title": "6.2 Discord-based Methods", "content": "A practical modification to the KNN-type model is to use the discord, which evolves from comparing distances to the nearest neighbor to comparing distances to the kth neighbor. Such adaptations assist in edge conditions where a small number of anomalies are clustered along with limited distances, and the conventional KNN approach struggles to recognize them. The following gives the specific definitions of discord:\nDefinition 6.1 (Top-k $m^{th}$-discord). [25, 37, 70, 116, 144, 147, 219, 261] Suppose the window is of length l. Given a collection of sub-sequences ${T_{j,l}}_{j\\in I}$, let $f_m$ denote $m^{th}$-discord function measuring the distance to $m^{th}$ nearest neighborhood so that $f_m(T_{j,l})$ = $\\min_{i \\in I\\{i}} d(T_{i,l}, T_{j,l})$. A sub-sequence $T_{i,l}$ is a Top-k $m^{th}$-discord if $f_m(T_{i,l})$ is the $k^{th}$ maximum among the set ${T_{j,l}}_{j\\in I}$.\nNote that the $m^{th}$-discord is the special case of Top-k $m^{th}$-discord when k = 1, and discord is the special case of $m^{th}$-discord when m = 1."}, {"title": "6.2.1 Matrix Profile", "content": "Matrix Profile [261, 270] is a discord-based method that represents time series as a matrix of closest neighbor distances. Compared to its predecessor, Matrix Profile proposed a new metadata time series computed effectively, capable of providing various valuable details about the examined time series, such as discords. For simplicity, we can call this metadata series matrix profile, and we can define it as follows:\nDefinition 6.2 (Matrix Profile). A matrix profile MP of a time series T of length n is a time series of length n - l - 1 where the ith element of MP contains the Euclidean normalized distance of the sub-sequence of length $\u0142$ of T starting at i to its nearest neighbor.\nHowever, the latter definition does not tell us where that neighbor is located. This information is recorded in the matrix profile index:\nDefinition 6.3 (Matrix Profile Index). A matrix profile index IMP is a vector of the index where IMP [i] = j and j is the index of the nearest neighbor of i.\nTwo general definitions of Join matrix computation can be inferred. The first, called Self-Join, corresponds exactly to the matrix profile. The second, called Join, corresponds to the same operation for two different time series. Formally we have the following:\nDefinition 6.4 (Time Series Self-Join). Given a time series T, the self-join of T with sub-sequence length l, denoted by T<\u0119T, is a meta time series, where: |T><\u0119 T| = |T| \u2212 l + 1 and \\forall i.1 \u2264 i \u2264 |T><\u0119 T|, (T><\u0119 T)i,1 = dist(Ti,e, $1^{st}$ NN of Ti,e).\nDefinition 6.5 (Time Series Join). Given two time series A and B, and a sub-sequence length l, the Join between A and B denoted by (A, B), is a meta time series, where: |A>< B| = |B| l + 1 and \\forall i.1 \u2264 i \u2264 |AB|, (Ae B)i,1 = $\\min(dist(B_{i,l}, A_{1,l}), ..., dist(B_{i,l}, A_{|A|-l+1,l}))$.\nThese metadata are computed using Mueen's ultra-fast Algorithm for Similarity Search (MASS) [166] that requires just O(n * log(n)) time by exploiting the Fast Fourier Transform (FFT) to calculate the dot products between the query and all the sub-sequences of the time series. Once these metadata are generated, retrieving the Top-k discord is possible"}, {"title": "6.3 Clustering-based Methods", "content": "Approaches based on clustering strategies have been proposed for the anomaly detection task. The main idea behind these methods is to partition the sub-sequence space and then evaluate how one sub-sequence fits into the partition.\n6.3.1 K-means Method. The k-means clustering algorithm is a widely used unsupervised learning technique in data mining and machine learning. Its main objective is to partition a given dataset into k distinct clusters, where each data point belongs to the cluster with the closest mean value. The algorithm operates iteratively, starting with an initial random assignment of k centroids. For the specific case of time series, the Euclidean or the DTW distance is commonly used. K-means algorithm can also be used for anomaly detection in time series [91]. The computational steps are the following:\n\u2022 All the sub-sequences of a given length l (provided by the user) are clustered using the k-means algorithm. The Euclidean distance is used, and the number of clusters has to be provided by the user.\n\u2022 Once the partition is learned. We compute the anomaly scores of each sub-sequence based on the distance to the centroid of its assigned cluster.\n\u2022 The larger the distance, the more abnormal the time series will be considered.\nSuch a method is straightforward but has been shown to be very effective for the specific case of multivariate time series [216]. Moreover, extensions such as Hybrid-k-means [228] can be used for anomaly detection as well. Finally, the k-means method can be used on top of other pre-processing and representation steps. For instance, DeepKMeans [163] uses an Autoencoder to learn a latent representation of the time series and applies k-means on top of the latent space to identify anomalies."}, {"title": "6.3.2 DBSCAN", "content": "Another commonly used clustering-based method is the Density-Based Spatial Clustering of Appli- cation with Noise algorithm (DBSCAN) [215", "categories": "i) core points", "DBSCAN": "the radius e of neighbors of the analyzed point and the minimum number \u00b5 of points in each normal cluster. Given these parameters", "follows": "n\u2022 A e-neighborhood of $T_{i,l}$ is $B_{\\rho} (T_{i,l}, e) \\cap T$, where T is the training data ${T_{i,t}}_{i \\in I}$. And $B_{\\rho}(T_{i,l}, \u0454)$ is the ball of radius e centered at $T_{i,l}$ with respect to the norm (., .).\n\u2022 $T_{i,l}$ is a core point if the size of the e-neighborhood of $T_{i,l}$ is greater than \u03bc.\n\u2022 $T_{i,l}$ is a border point if $T_{i,l}$ contains a core point in its e-neighborhood.\n\u2022 $T_{i,l}$ is identified as an anomalous sub-sequence if $T_{i,l}$ is neither a border nor a core point.\nDBSCAN has been applied for anomaly detection on a univariate time series that contains observations with average daily temperature over 33 years [41"}]}