{"title": "Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations", "authors": ["R\u00f3bert Csord\u00e1s", "Christopher Potts", "Christopher D. Manning", "Atticus Geiger"], "abstract": "The Linear Representation Hypothesis (LRH) states that neural networks learn to encode concepts as directions in activation space, and a strong version of the LRH states that models learn only such encodings. In this paper, we present a counterexample to this strong LRH: when trained to repeat an input token sequence, gated recurrent neural networks (RNNs) learn to represent the token at each position with a particular order of magnitude, rather than a direction. These representations have layered features that are impossible to locate in distinct linear subspaces. To show this, we train interventions to predict and manipulate tokens by learning the scaling factor corresponding to each sequence position. These interventions indicate that the smallest RNNs find only this magnitude-based solution, while larger RNNS have linear representations. These findings strongly indicate that interpretability research should not be confined by the LRH.", "sections": [{"title": "1 Introduction", "content": "It has long been observed that neural networks encode concepts as linear directions in their representations (Smolensky, 1986), and much recent work has articulated and explored this insight as the Linear Representation Hypothesis (LRH; Elhage et al. 2022; Park et al. 2023; Guerner et al. 2023; Nanda et al. 2023; Olah 2024). A strong interpretation of the LRH says that such linear encodings are entirely sufficient for a mechanistic analysis of a deep learning model (Smith, 2024).\nIn this paper, we present a counterexample to the Strong LRH by showing that recurrent neural networks with Gated Recurrent Units (GRUs; Cho et al. 2014) learn to represent the token at each position using magnitude rather than direction when solving a simple repeat task (memorizing and generating a sequence of tokens). This leads to a set of"}, {"title": "2 Related Work", "content": "The Linear Representation Hypothesis Much early work on 'word vectors' was guided by the idea that linear operations on vectors could identify meaningful structure (Mikolov et al., 2013; Arora et al., 2016; Levy and Goldberg, 2014). More recently, Elhage et al. (2022) articulated the Linear Representation Hypothesis (LRH), which says that (1) features are represented as directions in vector space and (2) features are one-dimensional (see also Elhage et al. 2022; Park et al. 2023; Guerner\net al. 2023; Nanda et al. 2023). Engels et al. 2024 challenged (2) by showing some features are irreducibly multi-dimensional. Olah (2024) subsequently argued that (1) is the more significant aspect of the hypothesis, and it is the one that we focus on here. Smith (2024) adds important nuance to the LRH by distinguishing a weak version (some concepts are linearly encoded) from a strong one (all concepts are linearly encoded).\nOur concern is with the strong form; there is ample evidence that linear encoding is possible, but our example shows that other encodings are possible. In onion representations, multiple concepts can be represented in a linear subspace by storing each concept at a different order of magnitude, i.e., a \u2018layer' of the onion, and any direction will cross-cut multiple layers of the onion.\nIntervention-based Methods Recent years have seen an outpouring of new methods in which interventions are performed on linear representations, e.g., entire vectors (Vig et al., 2020; Geiger et al., 2020; Finlayson et al., 2021; Wang et al., 2023), individual dimensions of weights (Csord\u00e1s et al., 2021) and hidden vectors (Giulianelli et al., 2018; De Cao et al., 2020; Davies et al., 2023), linear subspaces (Ravfogel et al., 2020; Geiger et al., 2024b; Belrose et al., 2023), or linear features from a sparse dictionary (Marks et al., 2024; Makelov et al., 2024). These methods have provided deep insights into how neural networks operate. However, the vast and varied space of non-linear representations is woefully underexplored in a causal setting.\nRNNS Recurrent Neural Networks (RNNs) were among the first neural architectures used to process sequential data (Elman, 1990, 1991). Many variants arose to help networks successfully store and manage information across long sequences, including LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al., 2014). Bidirectional LSTMs provided the basis for one of the first large-scale pretraining efforts (ELMo; Peters et al. 2018). With the rise of Transformer-based models (Vaswani et al., 2017), RNNs fell out of favor somewhat, but the arrival of structured state-space models (Gu et al., 2021b,a; Gu and Dao, 2023; Dao and Gu, 2024) has brought RNNs back into the spotlight, since such models seek to replace the Transformer's potentially costly attention mechanisms with recurrent connections. We chose GRUs for our studies, with an eye towards better understanding structured state space models as well."}, {"title": "3 Models", "content": "In this paper, we focus on how RNNs solve the repeat task. As noted in section 2, this question has taken on renewed importance with the development of structured state-space models that depend on recurrent computations and are meant to provide efficient alternatives to transformers.\nDefine an RNN as \\(h_t = f(h_{t-1},x_t), h_0 = 0\\), where \\(f(\\cdot,\\cdot)\\) is the state update function, \\(t \\in \\{1,...,T\\}\\) is the current timestep, \\(x_t \\in \\mathbb{R}^N\\) is the current input, and \\(h_t \\in \\mathbb{R}^N\\) is the state after receiving the input \\(x_t\\). The output of the model is \\(y_t = g(h_t)\\). Vectorized inputs \\(x_t\\) are obtained with a learned embedding \\(E \\in \\mathbb{R}^{N_s \\times N}\\), using the indexing operator \\(x_t = E[i_t]\\), where \\(i_t \\in \\{1, ..., N_s\\}\\) is the index of the token at timestep \\(t\\).\nIn our experiments, we use GRU cells over the more widely-used LSTM cells because they have a single state to intervene on, as opposed to the two states of the LSTM. GRU-based RNNs defined as:\n\\[\nz_t = \\sigma (W_zx_t + U_zh_{t-1}+b_z) \\tag{1}\n\\]\n\\[\nr_t = \\sigma (W_rx_t + U_rh_{t} + b_r) \\tag{2}\n\\]\n\\[\nu_t = \\tanh (W_hx_t + U_h(r_t \\odot h_t) + b_h) \\tag{3}\n\\]\n\\[\nh_t = (1 - z_t) h_{t-1} + z_t u_t \\tag{4}\n\\]\nFor output generation, we use \\(g(h_t) = softmax(h_tW_o + b_o)\\). The learned parameters are weights \\(W_*, U_* \\in \\mathbb{R}^{N \\times N}\\), and biases \\(b_* \\in \\mathbb{R}^{N}\\).\nWe will investigate how the final hidden state \\(h_T\\) of a GRU represents an input token sequence \\(i = i_1, i_2, ..., i_{i_L}\\). The final state is a bottle-neck between the input token sequence and the output."}, {"title": "4 Repeat Task Experiments", "content": "Our over-arching research question is how different models learn to represent abstract concepts. The repeat task is an appealingly simple setting in which to explore this question. In this task, the network is presented with a sequence of random tokens \\(i = i_1, i_2, . . ., i_{i_L}\\), where each \\(i_j\\) is chosen with replacement from a set of symbols \\(N_s\\) and the length \\(L\\) is chosen at random from \\(\\{1 . . . L_{max}\\}\\). This is followed by a special token, \\(i_{L+1} = `S'\\), that indicates the start of the repeat phase. The task is\nto repeat the input sequence: \\(Y_{L+1+j} = i_j\\). The variables in this task will represent positions in the sequence and take on token values.\nAs a preliminary step, we evaluate RNN models on the repeat task. The core finding is that all of the models solve the task. This sets us up to explore our core interpretability hypotheses in sections 5\u20137."}, {"title": "4.1 Setup", "content": "For our experiments, we generate 1M random sequences of the repeat task. The maximum sequence length is \\(L_{max} = 9\\), and the number of possible symbols is \\(N_s = 30\\). For testing, we generate an additional 5K examples using the same procedure, ensuring that they are disjoint at the sequence level from those included in the train set.\nWe use the same model weights during both the input and decoding phases. During the input phase, we ignore the model's outputs. No loss is applied to these positions. We use an autoregressive decoding phase: the model receives its previous output as input in the next step. We investigate multiple hidden state sizes, from \\(N = 48\\) to \\(N = 1024\\).\nWe train using a batch size of 256, up to 40K iterations, which is sufficient for each model variants to converge. We use an AdamW optimizer with a learning rate of \\(10^{-3}\\) and a weight decay of 0.1."}, {"title": "4.2 Results", "content": "reports on model performance at solving the repeat task. It seems fair to say that all the models solve the task; only the smallest model comes in shy of a perfect score, but it is at 95%. Overall, these results provide a solid basis for asking how the models manage to do this. This is the question we take up for the remainder of the paper."}, {"title": "5 Hypothesis 1: Unigram Variables", "content": "Intuitively, to solve the repeat task, the token at each position will have a different feature in the state vector \\(h_T\\) (the boundary between the input and output phrases). In line with the LRH, we hypothesize these features will be linear subspaces."}, {"title": "5.1 Interchange Intervention Data", "content": "In causal abstraction analysis (Geiger et al., 2021), interchange interventions are used to determine the content of a representation by fixing it to the counterfactual value it would have taken on if a different input were provided. These operations require datasets of counterfactuals. To create such examples, we begin with a random sequence y of length L consisting of elements of our vocabulary. We then sample a set of positions \\(I \\subseteq \\{1, . . ., L\\}\\), where each position k has a 50% chance of being selected. To create the base b, we copy y and then replace each \\(b_k\\) with a random token, for \\(k \\in I\\). To create the source s, we copy y and then replace each \\(s_i\\) with a random token, for \\(j \\notin I\\). Here is a simple example with \\(I = \\{1,3\\}\\):\n\\[\ny = b d a c\n\\]\n\\[\nb = X d Y c\n\\]\n\\[\ns = b 4 a 1\n\\]\nOur core question is whether we can replace representations obtained from processing b with those obtained from processing s in a way that leads the model to predict y in the decoding phase."}, {"title": "5.2 Method: Interchange Interventions on Unigram Subspaces", "content": "Our goal is to localize each position k in the input token sequence to a separate linear subspaces \\(S_k\\) of \\(h_L\\). We will evaluate our success using interchange interventions. For each position in \\(k \\in I\\), we replace the subspace \\(S_k\\) in the hidden representation \\(h_T\\) for base input sequence b with the value it takes in \\(h_T\\) for source input sequence s. The resulting output sequence should exactly match y. If we succeed, we have shown that the network has linear representations for each position in a sequence.\nThere is no reason to assume that the subspaces will be axis-aligned. Thus, we use Distributed Alignment Search (DAS) and train a rotation matrix \\(R \\in \\mathbb{R}^{N \\times N}\\) to map h into a new rotated space h. However, a remaining difficulty is to determine which dimensions in the rotated space belong to which position. The size of individual subspaces\nmay differ: for example, the first input of a repeated sequence, \\(b_1\\), is always present, and the probability of successive inputs decreases due to the random length of the input sequences. Thus, the network might decide to allocate a larger subspace to the more important variables that are always present, maximizing the probability of correct decoding for popular sequence elements.\nTo solve this problem, we learn an assignment matrix \\(A \\in \\{0,1\\}^{N \\times (L+1)}\\) that assigns dimensions of the axis-aligned representation h with at most one sequence position. Allowing some dimensions to be unassigned provides the possibility for the network to store other information that is outside of these positions, such as the input length.\nWe can learn this assignment matrix by defining a soft version of it \\(\\hat{A} \\in \\mathbb{R}^{N \\times (L+1)}\\), and taking the hard gumbel-softmax (Jang et al., 2017; Maddison et al., 2017) with straight-through estimator (Hinton, 2012; Bengio et al., 2013) over its columns for each row (\\(r \\in \\{1 . . . N\\}\\)) independently:\n\\[\nA[r] = gumbel\\_softmax(\\hat{A}^{[r]}) \\tag{5}\n\\]\nFor intervening on the position \\(k \\in N\\), we replace dimensions of the rotated state h, that are 1 in \\(A[\\cdot, v]\\). Specifically, intervention \\(\\hat{h}^b\\) is defined:\n\\[\n\\hat{h}^b = Rh^b \\tag{6}\n\\]\n\\[\n\\hat{h}^s = Rh^s \\tag{7}\n\\]\n\\[\n\\hat{h}^\\prime = A[\\cdot, v] \\odot \\hat{h}^s + (1 - A[\\cdot, v]) \\odot \\hat{h}^b \\tag{8}\n\\]\n\\[\nh^b = R^{-1} \\hat{h}^\\prime \\tag{9}\n\\]\nWhen learning the rotation matrix R and assignment matrix A, we freeze the parameters of the already trained GRU network. We perform the intervention on the final state of the GRU, after encoding the input sequences, and use the original GRU to decode the output sequence \\(\\hat{y}\\) from the intervened state \\(h_T^\\prime\\). We update R and A by back-propogating with respect to the cross entropy loss between the output sequence \\(\\hat{y}\\) and the expected output sequence after intervention y."}, {"title": "5.3 Results", "content": "We use the same training set as the base model to train the intervention model, and we use the same validation set to evaluate it. The first row of shows the accuracy of the unigram intervention. It works well for \"big\" models, with \\(N \\geq 512\\). In these cases, we can confidentially conclude that the model has a separate linear subspace for each position in the sequence."}, {"title": "5.4 Discussion", "content": "The above results suggest that the model prefers to store each input element in a different subspace if there is \"enough space\" in its representations relative to the task. However, Hypothesis 1 seems to be incorrect for autoregressive decoders where \\(N< 512\\). Since these models do solve our task, we need to find an alternative explanation for how they succeed. This leads us to Hypothesis 2."}, {"title": "6 Hypothesis 2: Bigram Variables", "content": "Our second hypothesis is a minor variant of Hypothesis 1. Here, we posit that, instead of representing variables for unigrams, the model instead stores tuples of inputs \\((i_t, i_{t+1})\\) we call bigram variables."}, {"title": "6.1 Intervention Data", "content": "We create counterfactual pairs using the same method as we used for Hypothesis 1 (section 5.1). In this case, each token \\(i_t\\) affects two bigram variables (if present). Thus, the subspace replacement intervention must be performed on both of these variables. This also means that, for each \\(k \\in I\\), the tokens \\(s_{k-1}\\) and \\(s_{k+1}\\) in the source sequence input must match \\(b_{t-1}\\) and \\(b_{t+1}\\) in the base sequence, because the bigram at position t \u2212 1 depends on \\((i_{t-1}, i_t)\\) and the bigram at t depends on \\((i_t, i_{t+1})\\)."}, {"title": "6.2 Method: Interchange Interventions on Bigram Subspaces", "content": "For a sequence of length L, there are L \u2212 1 bigram variables. To try to identify these, we use the same interchange intervention method described in section 5.2. Because targeting a single position in the base input sequence requires replacing two bigram variables, we intervene on only a single token at a time. Otherwise, the randomized sequence could be too close to the original, and most of the subspaces would be replaced at once, thereby artificially simplifying the task."}, {"title": "6.3 Results", "content": "We show the effectiveness of bigram interventions in the middle row of . The intervention is successful on most sizes, but fails for the smallest models (N < 64)."}, {"title": "6.4 Discussion", "content": "We hypothesize that the models prefer to learn bigram representations because of their benefits for autoregressive input: the current input can be compared to each of the stored tuples, and the output can be generated from the second element of the tuple. This alone would be enough to repeat all sequences which have no repeated tokens. Because our models solve the task with repeat tokens, an additional mechanism must be involved. Regardless, bigrams could provide a powerful representation that is advantageous for the model.\nTwo additional remarks are in order. First, successful unigram interventions entail successful bigram interventions; a full argument is given in Appendix E.1. Second, one might worry that our negative results for smaller models trace to limitations of DAS on the small models. Appendix E.2 addresses this by showing DAS succeeding on a non-autoregressive control model (N \u2264 64) that solves the copy task. This alleviates the concern, suggesting that the small autoregressive model does not implement the bigram solution and highlighting the role of autoregression in the bigram solution.\nHowever, we still do not have an explanation for how the smallest models (N \u2264 64) manages to solve the repeat task; Hypotheses 1 and 2 are unsupported as explanations for this model. This in turn leads us to Hypothesis 3."}, {"title": "7 Hypothesis 3: Onion Representations", "content": "In an effort to better understand how the smallest GRUs solve the repeat task, we inspected the gate values \\(z_t\\) as defined in equation 1 from the GRU definition (section 3).\nvisualizes the first 64 input gates for the \\(N = 1024\\) model (Appendix figure 5 is a larger diagram with all the gates). The x-axis is the sequence (temporal dimension) and the y-axis depicts the gate for each dimension. One can see that this model uses gates to store inputs by closing position-dependent channels sharply, creating a position-dependent subspace for each input. (This gating pattern is consistent across all inputs.)"}, {"title": "7.1 Intervention Data", "content": "For the causal analysis of onion representations, we do not use interchange interventions. Instead, we learn an embedding matrix for each token that"}, {"title": "7.2 Method: Onion Interventions", "content": "To replace token \\(i_j\\) with token \\(i_j\\), we add the difference of the corresponding token embeddings scaled by a factor determined by the position j. We parameterize this as:\n\\[\nx = E[i_j] \\tag{10}\n\\]\n\\[\nx' = E[i_j'] \\tag{11}\n\\]\n\\[\ns = g \\gamma^j + \\beta j + b \\tag{12}\n\\]\n\\[\nh' = h + s (x' - x) \\tag{13}\n\\]\nwhere \\(E \\in \\mathbb{R}^{N_s \\times N}\\) is the embedding for the tokens (distinct from the the GRU input embedding, learned from scratch for the intervention), and \\(g, \\gamma, \\beta,b \\in \\mathbb{R}^{N}\\) are learned scaling parameters. Intuitively, s is the scale used for the token in position j. Its main component is the exponential term \\(\\gamma\\). In order to replace the token in the sequence, compute the difference of their embeddings, and scale them to the scale corresponding to the given position. Different channels in the state \\(h \\in \\mathbb{R}^{N}\\) might have different scales. depicts an example intervention, extending figure 1."}, {"title": "7.3 Results", "content": "The last row of shows that our onion intervention achieves significantly better accuracy on the small models compared to the alternative unigram and bigram interventions. For example, for \\(N = 64\\), the onion intervention achieves 87% accuracy compared to the 1% of the bigram intervention. As a control, if we fix \\(\\gamma = 1\\) and \\(\\beta = 1\\), we only reach 21% accuracy."}, {"title": "7.4 Discussion", "content": "Why do GRUs learn onion representations? In order to distinguish \\(N_s\\) tokens stored in \\(L_{max}\\) possible positions, the model needs to be able to distinguish between \\(N_s \\times L_{max}\\) different directions in the feature space. In our experiments this is 300 possible directions, stored in a 64-dimensional vector space. In contrast, for onion representations, they only have to distinguish between \\(N_s = 30\\) directions at different orders of magnitude.\nOnion representations require unpeeling via autoregression. We train a variety of probes to decode the final representation \\(h_T\\) after encoding the input sequence of GRUs with \\(N = 64\\), which learn onion representation. We show our results in . The linear and MLP probes predict the entire sequence at once by mapping the hidden vector \\(h_T \\in \\mathbb{R}^{N}\\) to the logits for each timestep \\(Y_{all} \\in \\mathbb{R}^{N_S \\times L_{max}}\\). The GRU Autoregressive (GRU \u2013 AR) probe is equivalent to the original model, and we use it as a check to verify that the decoding is easy to learn. The GRU \u2013 No input probe is similar, but unlike the original decoder of the model, it does not receive an autoregressive input.\nThe probe results confirm that it's not merely a free choice whether the decoder uses an autoregressive input or not: if an onion representation is learned during the training phase, it is impossible to decode it with a non-autoregressive decoder, contrary to the same-size models that are trained without an autoregressive input, shown in Table 4 in Appendix E.3. We also show the special probe we designed for onion representations in a similar spirit to the intervention described in section 7.2, which performs almost perfectly. More details can be found in Appendix E.3.\nWhat is the feature space of an onion representation? Together, the embeddings E learned for each token and the probe P that predicts the to-"}, {"title": "8 Discussion and Conclusion", "content": "The preceding experiments show that GRUs learn highly structured and systematic solutions to the repeat task. It should not be overlooked that two of these solutions (those based in unigram and bigram subspaces) are consistent with the general guiding intuitions behind the LRH and so help to illustrate the value of testing hypotheses in that space. However, our primary goal is to highlight the onion solution, as it falls outside the LRH.\nOur hope is that this spurs researchers working on mechanistic interpretability to consider a wider range of techniques. The field is rapidly converging around methods that can only find solutions consistent with the LRH, as we briefly reviewed in section 2. In this context, counterexamples to the LRH have significant empirical and theoretical value, as Olah (2024) makes clear:\nBut if representations are not mathematically linear in the sense described above [in a definition of the LRH], it's back to the drawing board a huge number of questions like \u201chow should we think about weights?\u201d are reopened.\nOur counterexample is on a small network, but our task is also very simple. Very large networks solving very complex tasks may also find solutions that fall outside of the LRH.\nThere is also a methodological lesson behind our counterexample to the LRH. Much interpretability work is guided by concerns related to AI safety. The reasoning here is that we need to deeply understand models if we are going to be able to certify them as safe and robust, and detect unsafe mechanisms and behaviors before they cause real harm. Given such goals, it is essential that we analyze these models in an unbiased and open-minded way."}, {"title": "9 Limitations", "content": "The generality of onion representations. Onion representations are well fit for memorizing a sequence in order or in reverse order, but they cannot provide a general storage mechanism with arbitrary access patterns. It is unclear if such representations are useful in models trained on more complex real-world tasks.\nUsing GRU models. Our exploration is limited to GRU models, which themselves might have less interest in the current Transformer-dominated state of the field. However, we suspect that the same representations are beneficial for other gated RNNs as well, such as LSTMs. Although we have a reason to believe that such representations can emerge in Transformers and state space models as well, we do not verify this hypothesis empirically.\nOnion representations only emerge in small models. This might indicate that onion representations are not a problem for bigger models used in practice. However, this might not be the case: LLMs, which are much bigger, operate on an enormous feature space using a relatively small residual stream. Thus, the pressure to compress representations and the potential for similar representations to emerge could be well motivated there as well.\nNumerical precision. The number of elements that can be stored in onion representations depends on the numerical precision of the data type used for the activations. We found that the network finds it easy to use these representations even with 16-bit floating point precision (bf16), potentially because multiple redundant channels of the state can be used as an ensemble. It remains unclear what the capacity of such representations is."}, {"title": "10 Acknowledgements", "content": "Christopher D. Manning is a CIFAR Fellow. This research is in part supported by a grant from Open Philanthropy."}]}