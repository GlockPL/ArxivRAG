{"title": "Preserving Generalization of Language Models in Few-shot Continual Relation Extraction", "authors": ["Quyen Tran", "Thanh Nguyen", "Anh Nguyen", "Nam Le", "Trung Le", "Linh Ngo Van", "Thien Huu Nguyen"], "abstract": "Few-shot Continual Relations Extraction (FCRE) is an emerging and dynamic area of study where models can sequentially integrate knowledge from new relations with limited labeled data while circumventing catastrophic forgetting and preserving prior knowledge from pre-trained backbones. In this work, we introduce a novel method that leverages often-discarded language model heads. By employing these components via a mutual information maximization strategy, our approach helps maintain prior knowledge from the pre-trained backbone and strategically aligns the primary classification head, thereby enhancing model performance. Furthermore, we explore the potential of Large Language Models (LLMs), renowned for their wealth of knowledge, in addressing FCRE challenges. Our comprehensive experimental results underscore the efficacy of the proposed method and offer valuable insights for future work.", "sections": [{"title": "1 Introduction", "content": "Continual Relations Extraction (CRE) is a learning scenario that requires a model to identify emerging relationships between entities or objects in texts while maintaining the accuracy of existing classifications and avoiding the problem of Catastrophic forgetting (Thrun and Mitchell, 1995; French and Chater, 2002). In many real-world situations, models must learn from a few new samples due to the limited availability of labeled training data for relations. As a result, Few-short Continual Relation Extraction (FCRE) methods have been proposed (Qin and Joty, 2022; Chen et al., 2023) to enable models to solve new tasks where each new relation has only a minimal number of corresponding samples. However, due to the lack of data, FCRE models are often biased towards the current task compared to related scenarios, which can lead to forgetting previous knowledge and losing highly general priori from the pre-trained backbone. Thus, the challenge of FCRE is not only catastrophic forgetting but also severe overfitting.\nRecent works (Wang et al., 2023; Qin and Joty, 2022; Chen et al., 2023) tackles these issues by employing memory-based approaches inspired by traditional Continual Learning methods (Rolnick et al., 2019; Buzzega et al., 2020; Lopez-Paz and Ranzato, 2017), along with various strategies to enhance the model's ability to distinguish relation representations. Nevertheless, these methods solely fine-tune pre-trained BERT-based backbones for few-shot tasks, which leads to eroding prior knowledge from the pre-trained model and hindering the final performance. Additionally, these methods often neglect the pre-trained LM head in favor of training a new classifier from scratch, even though this component contains rich and general knowledge that remains untapped. Therefore, we propose our Mutual Information Maximization (MIM) strategy that leverages pre-trained LM heads during training FCRE models for the first time. Our proposed strategy not only helps preserve the knowledge on the backbone but also assists in aligning the main classifier to improve representation learning. Extensive experimental results on benchmark datasets demonstrate the effectiveness of our novel approach in preserving the pre-trained LM's generalization capability and reducing forgetting, leading to remarkable results.\nFurthermore, pre-trained Large Language Models (LLMs) (et al, 2023a; Jiang et al., 2023) with billions of parameters are known for their excellence in autoregressive text generation tasks. They have also been extensively studied in text classification and information extraction (Zhao et al., 2021; Wei et al., 2023). However, these models often underperform compared to discriminative encoder models like BERT due to their generation-focused mechanism. To address this, recent work (Li et al., 2023) proposed replacing ineffective LLM heads with classification heads in the restricted space of the classification problem. This approach has shown promise, but the potential of LLMs in CL, specifically in FCRE, remains underexplored. Therefore, we conduct extensive experiments to answer: How the performance would LLMs yeild for FCRE? How will limited data in this scenario impact the generalization of LLMs? We also assess the effectiveness of our MIM strategy when using LLM heads, which were eliminated due to their unsuitability. The results offer valuable insights for the community.\nTo sum up, our main contributions are twofold:\n\u2022 First, we introduce a novel approach to enhance FCRE models by strategically leveraging the LM heads. Through maximizing mutual information between these components and the primary classifiers, we can better preserve prior knowledge from pre-trained backbones, as well as strengthen representation learning. The experimental results demonstrate our effectiveness.\n\u2022 We also investigate the application of pre-trained LLMs to FCRE tasks, including evaluating the effectiveness of the proposed method when using LLM heads, which were discarded in classification-based problems due to their unsuitability. Our comprehensive experimental results offer valuable insights."}, {"title": "2 Related work", "content": "Continual Learning (CL) is a learning scenario that requires models to continually acquire new knowledge from a sequence of tasks while preventing the loss of previously learned information. The main challenge in CL is catastrophic forgetting (French, 1993). To address this problem, memory-based approaches prove to be effective methods for both machine learning (Rebuffi et al., 2017; Shin et al., 2017) and NLP problems (Wang et al., 2019; Han et al., 2020). In particular, models need to save a few representative samples from the current task in a memory buffer and replay these samples when learning new tasks to review old knowledge.\nFewshot Continual Relation Extraction is a challenging scenario, which was introduced by (Qin and Joty, 2022) for Relation Extraction problems. This challenge arises due to the limited availability of data for new tasks, coupled with the high cost and time involved in obtaining high-quality data. Recent work like Wang et al. (2023); Chen et al. (2023); Ma et al. (2024) propose memory-based solutions, which suggest imposing objective functions on the embedding space and classification head. Specifically, Wang et al. (2023) employs serial objective functions based on contrastive and distillation, Qin and Joty (2022) leverage extra training data from unlabeled text, and Chen et al. (2023) proposes a consistent prototype learning strategy to help the model distinguish between different relation representations, thus enhancing representation learning efficiency."}, {"title": "3 Background", "content": "However, in these methods, eliminating the pre-trained LM head and training a new classifier still leads to overfitting and forgetting due to limited data, as it emphasizes discriminative features only. To address this problem, we propose a novel approach that leverages LM heads, which are often overlooked in pre-trained models for downstream tasks. Our method not only helps preserve prior knowledge from the backbone but also supports the training of the main classifier, thereby further reducing both catastrophic forgetting and overfitting."}, {"title": "3.1 Problem Formulation", "content": "In the setting of FCRE, a model needs to continually acquire new knowledge from a series of tasks. For each task t, also denoted as $T_t$, the model is trained on the training set $D_t = \\{(x_i, y_i)\\}_{i=1}^{K*N}$. Here, N and K represent the number of classes in the new relation set $R_t$ and the number of samples corresponding to each relation, respectively. Each sample $(x, y)$ consists of a sentence $x_i$ with a pair of entities $(e_n, e_t)$ and a relation label $y_i \\in R_t$. This type of task is also known as \"N-way-K-shot\". Once task $T_t$ is completed, $D_t$ is no longer available for future learning. Finally, the model will be evaluated on all task data so far in order to identify relations in $R_t = \\bigcup_{i=1}^t R^i$."}, {"title": "3.2 Existing Concept of FCRE Models", "content": "Current FCRE methods (Wang et al., 2023; Chen et al., 2023; Ma et al., 2024) have considered tackling two main issues: catastrophic forgetting and overfitting. This has been achieved by exploiting the power of pre-trained BERTs and various motivated techniques which can divided into 3 main groups, including (i) using objective functions (i.e., Lo) to enhance representation learning ability, (ii) implementing a prompt design, and (iii) employing a memory management strategy to store and retrieve knowledge of old tasks. In this paper, we propose a novel strategy that can flexibly integrate with and improve these methods (Figure 3).\nMoreover, to explore the potential of pre-trained LLMs when dealing with the FCRE problems, we need to apply the current SOTA methods for LLMs, which were originally designed for \"encoder-only\" models. On the other hand, the examined LLMs (LLAMA2, Mistral) are \"decoder-only\", operating in the auto-regressive mechanism (Xie, 2017; Yang et al., 2019). Due to the differences between these models, we have to modify the original designs mentioned above (see Sec. 4.2)."}, {"title": "4 Proposed Method", "content": "In this section, we first present our efficient strategy in Section 4.1 that can flexibly adapt to the existing FCRE methods and enhance model performance. After that, in Section 4.2, we explain in detail the motivation and research questions when investigating LLMs in FCRE."}, {"title": "4.1 Mutual Information Maximization (MIM)", "content": "According to recent work (Li et al., 2023; Xu et al., 2023), using pre-trained LMs (BERTs) with their classification heads often leads to poor results. This is because the models must return responses in the vocabulary's high-dimensional space (i.e., $||V||$). Therefore, in downstream tasks like Relation Extraction, LM heads of pre-trained LMs are often discarded. Instead, existing work (Wang et al., 2023; Ma et al., 2024) opt for training a classification head across tasks as a better solution. However, in FCRE, training a new classifier from scratch often encourages models to emphasize only discriminative features derived from sparse data streams and memory buffers. This biased behavior can make the model seriously overfit and rapidly lose prior knowledge from the pre-trained backbone and, thus, hinder the final performance.\nTherefore, we propose an MIM strategy that exploits the overlooked LM head to solve the drawbacks of existing FCRE methods. Intuitively, leveraging knowledge from pre-trained LM heads will support the primary classifier, aiding the model in capturing information more holistically and better preserving old knowledge of the pre-trained backbone. In particular, inspired by (Guo et al., 2022), we aim at maximizing Mutual Information (MI) between latent representations on the LM head branch and on our main classifier branch as follows:\n$MI = I[g_\\theta(x), g_{\\phi M}(x)]$\nwhere $g_\\theta$ corresponds to the class-discriminative feature representation at the classification head, $g_{\\phi M}$ denotes the representation at the LM head. According to (van den Oord et al., 2018):\n$MI \\geq log B + InfoNCE(\\{x_i\\}_{i=1}^B; h)$\nwhere we have defined\n$InfoNCE(\\{x_i\\}_{i=1}^B; h) = \\frac{1}{B} \\sum_{i=1}^B log \\frac{h(g_\\theta(x_i), g_{\\phi M}(x_i))}{\\sum_{j=1}^B h(g_\\theta(x_i), g_{\\phi M}(x_j))}$\n$h(g_\\theta(x_i), g_{\\phi M}(x_j)) = exp\\frac{g_\\theta(x_i)^T W g_{\\phi M}(x_j)}{\\tau}$\nwhere $\\tau$ is the temperature, B is mini-batch size and W is a trainable parameter. Then, the MI loss function in our implementation is:\n$L_{MI} = - \\sum_{(x_i, y_i) \\in D_{k}^{train}} InfoNCE(\\{x_i\\}_{i=1}^B; h)$\nTherefore, the objective function of the model can be summarized as:\n$L = L_o + L_{MI}$\nwhere $L_o$ is the loss function of the original method. In this work, to demonstrate the effectiveness of our method, we integrate it into three existing methods CPL (Ma et al., 2024), ConPL (Chen et al., 2023) and SCKD (Wang et al., 2023) (see Appendix A.2).\nDiscussion: Although using pre-trained LM heads directly in downstream tasks is challenging, this does not hinder us from tapping into their wealth of knowledge to enhance our model performance in FCRE.\n\u2022 First, maintaining the LM heads while fine-tuning them with a carefully controlled learning rate encourages the pre-trained backbones to retain prior knowledge and inherent behaviors. Thus, this strategy can mitigate the risk of overfitting, especially when models are trained on limited data for each task, enhancing their overall robustness and reliability.\n\u2022 Second, applying MIM on different representation layers of the data will be a powerful aid for Lo in learning representations. Specifically, the mutual information of samples with the same label will be enhanced, while the information corresponding to features of different labels will be restricted. As a result, feature vectors of the same class will become more condensed, and representations of different classes will be more separated."}, {"title": "4.2 Exploiting LLMs for FCRE", "content": "Motivations and Research questions Pre-trained LLMs (et al, 2023a; Jiang et al., 2023) are known for containing rich knowledge with billions of parameters, which have achieved impressive results in auto-regressive text generation tasks. These models have also been extensively examined in classification-based problems (Zhao et al., 2021; Wei et al., 2023). However, these models often do not outperform discriminative encoder models such as BERT because their original generation-focused mechanism, which generates answers over a large vocabulary, may not capture task-specific patterns as efficiently as label-supervised BERT models. To address this drawback, recent work (Li et al., 2023) proposed directly extracting latent representations from the final LLaMA decoder layer and mapping them into the label space through feed-forward layers. Specifically, the LLM heads, which have been found ineffective, are removed and replaced by a classification head trained from scratch using CrossEntropy loss. This approach has shown promising results. However, exploration in the area of Continual Learning, specifically Few-shot Continual Relation Extraction (FCRE), has not yet been thoroughly investigated. Therefore, in this work, we conduct extensive experiments to answer the following research questions (RQs):\n\u2022 RQ1: How the performance would LLMs yield in FCRE tasks? Will it yield significantly better results compared to conventional BERT-based models? How will the limited data in the FCRE scenario impact the generalization of this model class? It would be interesting to examine the behavior of an LLM, which contains rich prior knowledge in the context of the FCRE problem, where each task only has very little data, and the model will usually be forgotten and severely overfit.\n\u2022 RQ2: Our study also aims to assess the effectiveness of employing our MIM strategy for LLMs, particularly in addressing the challenges of forgetting prevention and overfitting reduction. Does using LLM heads according to our strategy eliminate the prejudice about the unsuitability of LLMs in classification-based problems, specifically FCRE?\nHow to adapt BERT-based FCRE methods to LLMs? Because current FCRE methods are used for BERT-based backbones, which are \"encoder-only\" language models. It is essential to modify their original design to adapt to \"decoder-only\" LLMs like LLAMA2-7B (et al, 2023a), Mistral-7B (Jiang et al., 2023), which operate in the auto-regressive mechanism (Xie, 2017; Yang et al., 2019; et al, 2023b). In particular:\n\u2022 (i) The prompted inputs will be in the form of: \"[Original sentence]. The relation between [Entity 1] and [Entity 2] is [Answer]\";\n\u2022 (ii) The embedding used for the main classifier (i.e., $g_\\theta(\\cdot)$) is now the embedding of the word \"is\" in the corresponding input, instead of \"[MASK] embedding\" in Figure 3."}, {"title": "5 Experimental Results", "content": "In this part, we first present the experiment setup in Section 5.1, followed by the results that demonstrate the effectiveness of our proposed method (Section 5.2) when using BERT-based backbones. We then discuss the investigation results of using pre-trained LLMs for FCRE tasks in Section 5.3."}, {"title": "5.1 Experiment Setup", "content": "In our experiments, we use three current state-of-the-art methods as baselines, including: SCKD (Wang et al., 2023), ConPL (Chen et al., 2023), and CPL (Ma et al., 2024). Besides, the models are evaluated using pre-trained models consisting of BERT (Devlin et al., 2018), LLAMA2-7B (et al, 2023a), and Mistral-7B (Jiang et al., 2023), on two benchmark datasets: FewRel (Han et al., 2018) and TACRED (Zhang et al., 2017). We note that we have reproduced the results of ConPL (Chen et al., 2023) under the same setting as SCKD and CPL. The reason is that the evaluation strategy in this paper is impractical for continual learning scenarios. Please refer to Appendix A for more details."}, {"title": "5.2 Evaluation", "content": "a. Using LM heads significantly improves the model's accuracy. Table 1 reports the results of baselines and our proposed method (+MI), which exploits pre-trained LM heads beside the primary classifiers. In general, our method consistently helps improve the performance of existing methods in all cases. On both datasets, our strategy improved the final accuracy by around 2% when integrated with CPL and ConPL and around 1% when combined with SCKD. Moreover, considering accuracy after learning immediate tasks, ConPL+MI, when using our proposed strategy, can exceed the original version by about 15% on TACRED.\nb. Exploiting the LM head effectively helps reduce forgetting and overfitting. Figure 1 and Table 2 show the accuracy drop after completing 8 tasks in various cases. The results indicate that our method significantly helps reduce forgetting for the baselines by approximately 1 to 3%. Moreover, Figure 2 shows generalization gaps (i.e., d = test loss \u2013 train loss) after training each task of different models. The results show that our MIM strategy helps the models minimize these gaps significantly, thereby increasing their generalization.\nc. The LM head supports representation learning. Figure 4 presents representations in the latent space of CPL model before and after exploiting our MIM strategy (CPL+MI) on data of Task 1, after learning 8 tasks. It can be seen that the test features belonging to different categories of CPL+MI are better separated and therefore achieve better results. In addition, we provide a t-SNE visualization about features of the first task in the latent space on the LM head after learning the final tasks (Figure 5), confirming the benefits when taking advantage of this component to enhance the performance of models.\nd. Ablation study\nFor further analyzing the effectiveness of our proposed method, we make an ablation study and present the experimental results in Table 3. Regarding ConPL, it becomes evident that our MIM (e.i., +MI) plays a pivotal role compared to the loss components proposed in the original paper. Specifically, the elimination of each loss component among $L_{cc}$, $L_{dc}$ and $L_{fc}$ leads to only a marginal decline in performance. However, removing MI results in a notable decrease in accuracy across tasks, except for tasks 1 and 5. In the case of the SCKD, we note a substantial impact when excluding the distillation element (i.e., $L_{dst}$). This underscores the pivotal role of this component in mitigating forgetting while our proposed MI mechanism continues to enhance the performance of the overall model.\nMoreover, we also explore a scenario in which the LM head is frozen to retain the knowledge from the pretraining phase fully. We notice inconsistent changes during the task learning process, with certain tasks demonstrating performance improvements while others exhibit declines. We hypothesize that in specific cases, the LM's pretraining-derived general knowledge can facilitate recognizing specific relations. Consequently, fine-tuning the model on domain-restricted data might compromise this capability. Conversely, for other relations,"}, {"title": "5.3 Using LLM for FCRE", "content": "RQ1: How the performance would LLMs yield in FCRE tasks? Table 4 depicts the increase in final accuracy after learning 8 FCRE tasks when the BERT-based backbone is replaced by the LLM backbone. Specifically, improvements can be as much as 3.75% in the case of LLAMA-2-7B, and 8.75% for Mistral-7B across both datasets. In addition, Table 6 shows the full results of FCRE models on both datasets. Mostly, during the training of eight tasks, the LLMs tend to provide higher accuracy than the BERT-based models. For some immediate tasks, LLAMA2-7B can achieve up to 16% higher accuracy than BERT-based models in TACRED, although their accuracy can be slightly lower in other cases. Besides, the differences in performance after training the first task and the last task (Accuracy drop - column \u0394 \u2193) in LLMs are smaller than in BERT-based models, from 2 to 5% in the case of LLAMA2-7B and as much as 8% for Mistral-CPL. These experimental results confirm the general superiority of LLM in solving FCRE compared to the class of conventional BERT-based models.\nOn the other hand, pre-trained LLMs are known to be knowledge-rich models with high generalization capabilities. However, for the first task, LLMs achieve accuracies of around 96% on FewRel and around 86% on TACRED, having no clear advantage over BERT-based models. Besides, the results in Table 6 clearly demonstrate the degradation of prior knowledge when applying pre-trained LLM in FCRE. In particular, the model's accuracy can drop by 30 - 32% for LLAMA2-7B and by 20 - 25% for Mistral-7B, after training 8 tasks.\nThanks to thorough training on large datasets, LLMs with billions of parameters contain a wealth of knowledge and have great potential in downstream tasks. However, in some cases, with the current operating mechanism of an autoregressive decoder, employing such a model with billions of parameters, as opposed to one with hundreds of millions (BERT), proves exceedingly expensive for only marginal improvements in accuracy. Even on TACRED, the final accuracy of LLAMA2-7B-CPL is lower than that of CPL+MI, indicating that our method with the BERT-based model can effectively replace the LLM in this case. These findings necessitate the development of more effective methodologies to ensure the effectiveness of LLMs within this challenging setting\nRQ2: The effectiveness of exploiting our MIM strategy for LLMs in FCRE tasks Figure 1 and Table 6 clearly show that our strategy significantly mitigates accuracy drop in LLMs, which could reach up to 6% on TACRED and 4% on FewRel, and better than on BERT-based models. Besides, Figure 2 consistently illustrates the effectiveness of our method in reducing overfitting. It can be said that with our proposed strategy, LLM heads are no longer an obstacle when applying pre-trained LLMs to classification tasks. On the contrary, using LLMs demonstrates the clearest and most significant improvement in mitigating catastrophic forgetting and reducing overfitting."}, {"title": "6 Conclusion", "content": "In this work, we introduce a novel method that utilizes pre-trained language model heads to maintain the generalization of LMs in FCRE problems. By making use of this often ignored component through a mutual information strategy, our approach also significantly improves the comprehensiveness of the representation on the main classifier. Additionally, we present comprehensive experimental results that demonstrate the impact of using LLMs for FCRE and provide valuable insights to the community."}, {"title": "Limitations", "content": "\u2022 First, our proposed method and current investigations in this paper apply only to high-level RE tasks, where all entities are assumed to be given. Therefore, to achieve more practical results, it is motivating to consider end-to-end RE problems, covering entity recognition to relation extraction between entities in the future.\n\u2022 Another potential limitation could arise from the fact that pre-trained LMs used in our work might inherit biases from their pre-training data. These biases can manifest in various forms, such as gender, racial, or cultural biases, and could be exacerbated in scenarios with limited labeled data, as in FCRE tasks. Our method endeavors to transfer the knowledge within the LMs to the classification head by leveraging Mutual Information (MI), which could inadvertently perpetuate biased representations. Such biased representations may have adverse consequences, potentially resulting in misidentifying relations associated with biased information. This raises an open question for the research community to investigate further, exploring the impact of bias on FCRE tasks when utilizing LLMs."}, {"title": "A Implementation details", "content": "For each reported result, we conduct 6 independent runs with different random seeds and report the mean. Our code is available at https://github.com/thanhnx12/CRE-via-MMI\nNote: As discussed in (Li et al., 2023), LLaMA-2-7B model gives better results compared with LLaMA-2-13B. Therefore, we opt to use LLaMA-2-7B to examine in our experiments."}, {"title": "A.1 Datasets", "content": "Our experiments utilize the following two benchmarks:\n\u2022 FewRel (Han et al., 2018) includes 100 relations with 70,000 samples. Following Qin and Joty (2022), we employ a setup with 80 relations, partitioned into 8 tasks, each comprising 10 relations (10-way). Task T\u00b9 includes 100 samples per relation, whereas the remaining tasks are characterized as few-shot tasks conducted under 5-shot settings.\n\u2022 TACRED (Zhang et al., 2017) encompasses 42 relations with 106,264 samples extracted from Newswire and Web documents. Consistent with the approach outlined by Qin and Joty (2022), we exclude instances labeled as \"no_relation\" and allocate the remaining 41 relations across 8 tasks. Task T\u00b9 comprises 6 relations, each with 100 samples, while each subsequent tasks involve 5 relations (5-way) in 5-shot setups."}, {"title": "A.2 Baselines", "content": "In this work, we showcase our approach through thorough experiments using three recent SOTA methods in FCRE as the baselines, including:\n\u2022 SCKD (Wang et al., 2023): adopts a systematic strategy for knowledge distillation, which aims to preserve old knowledge from previous tasks. Besides, this method employs contrastive learning techniques with pseudo samples to enhance the distinguishability between representations of different relations.\nIn this paper, to conduct the ablation study in Table 3, we denote $L_{dst}$ as the representative of all the losses serving the distillation and contrastive learning mentioned above and $aug$ as the augmentation technique on the memory buffer.\n\u2022 ConPL (Chen et al., 2023) proposes a method that consists of three fundamental modules: a prototype-based classification module, a memory-enhanced module, and a novel consistent learning module that enforces distribution consistency to prevent forgetting. Additionally, ConPL leverages prompt learning to improve representation learning and incorporate focal loss to alleviate confusion among closely related classes.\nThis paper conducts the ablation study in Table 3where the role of each component of ConPL's objective function is analyzed. In particular, $L_{cc}$ helps constrain the consistency between samples and corresponding prototypes of old tasks, $L_{dc}$ forces the consistency regarding the distribution of samples and prototypes, and $L_{fc}$ is a focal loss that alleviates the difficulty of choosing negative classes during inference.\n\u2022 CPL (Ma et al., 2024) CPL proposes a Contrastive Prompt Learning framework, which designs prompts to generalize across categories and uses margin-based contrastive learning to handle hard samples, thus reducing catastrophic forgetting and overfitting. Besides, the authors employ a memory augmentation strategy to generate diverse samples with ChatGPT, further mitigating overfitting in low-resource scenarios of FCRE."}, {"title": "A.3 Evaluation Protocol", "content": "Metric We use final average accuracy to evaluate methods in our experiments. The average accuracy at task Tj is calculated as follows:\n$ACC_j = \\frac{1}{j} \\sum_{i=1}^{j} ACC_{j,i}$\nwhere $ACC_{j,i}$ is the accuracy on the test set of task $T_i$ after training the model on task $T_j$.\nPrediction mechanism As mentioned in 5.1, our methods follow the evaluation strategy in the setting of SCKD and CPL. Specifically, during the testing phase, the learned model is required to evaluate all classes/ relations it has been trained on so far.\nNote that in the original code repository of ConPL (e.g., Lines 18-53 in this file), this method follows a different evaluation process. In particular, after training on task Tk, the model has been trained on a set of $R_t$ relations. However, for each relation r, ConPL defines a set of negative candidate classes $M_r$, so that predictions are made on the set $(R_t \\cap M_r)$. This means that the model does not make predictions with all the classes it has learned so far but rather with a predefined subset specific to each relation. While enhancing the performance reported for ConPL, this targeted prediction approach does not align with the practical requirements of CL. In this challenging scenario, each model has to dynamically adapt and make predictions across the expanding set of relations without relying on some fixed set of classes. Therefore, despite its efficacy in controlled evaluations, the ConPL method is impractical for real-world continual learning applications."}]}