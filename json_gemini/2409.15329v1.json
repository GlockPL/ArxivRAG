{"title": "CAUSALITY-DRIVEN REINFORCEMENT LEARNING FOR JOINT COMMUNICATION AND SENSING", "authors": ["Anik Roy", "Serene Banerjee", "Jishnu Sadasivan", "Arnab Sarkar", "Soumyajit Dey"], "abstract": "The next-generation wireless network, 6G and beyond, envisions to integrate communication and sensing to overcome interference, improve spectrum efficiency, and reduce hardware and power consumption. Massive Multiple-Input Multiple Output (mMIMO)-based Joint Communication and Sensing (JCAS) systems realize this integration for 6G applications such as autonomous driving, as it requires accurate environmental sensing and time-critical communication with neighboring vehicles. Reinforcement Learning (RL) is used for mMIMO antenna beamforming in the existing literature. However, the huge search space for actions associated with antenna beamforming causes the learning process for the RL agent to be inefficient due to high beam training overhead. The learning process does not consider the causal relationship between action space and the reward, and gives all actions equal importance. In this work, we explore a causally-aware RL agent which can intervene and discover causal relationships for mMIMO-based JCAS environments, during the training phase. We use a state dependent action dimension selection strategy to realize causal discovery for RL-based JCAS. Evaluation of the causally-aware RL framework in different JCAS scenarios shows the benefit of our proposed framework over baseline methods in terms of the beamforming gain.", "sections": [{"title": "1 INTRODUCTION", "content": "The use of a large number of antennas at the transmitter and receiver to enable Massive Multiple-Input Multiple Output (mMIMO) is a key characteristic of future generation Joint Communication and Sensing (JCAS) systems as it significantly compensates for path loss and ensures sufficient signal strength. These systems use analog beamformers where the transceivers employ networks of phase shifters [1] to enable antenna beamforming of beam codebooks. Traditionally, beam codebooks consists of a large number of single-lobe directional beams which are used for data transmission and/or sensing [2]. However, the design of beam codebooks is challenging due to the highly dynamic nature of wireless channels. Furthermore, the beam design criteria for communication and sensing functions are different. For example, a communication beam is designed to focus the transmitter power towards the receiver, while a sensing beam is designed to focus on sensing targets in the environment [3]. This requires that the beamforming technique adapts to its environment in relation to user distribution and sensing targets, which can potentially be achieved if the mMIMO system incorporates a data-driven artificial intelligence component. Traditionally, Reinforcement Learning (RL)-based techniques have been employed for JCAS-based optimized beamforming as they can account for the complex interactions between the environment and the mMIMO antenna system [4, 5].\nHowever, traditional RL techniques suffer from high beamforming training overhead to cover all possible directions using a large number of beams in the beam codebook. This high training overhead is mainly due to the difficulty in exploration caused by the large action space and state space of the beamformers in the beamforming codebook. Techniques such as immitation learning [6], [7] cannot be used for the reduction of exploration space in the context of wireless communication due to the highly dynamic nature of the state space. The dynamic nature of the problem, with its larger state and action spaces, makes it computationally expensive.\nWe address the gap in the state-of-art as follows; the agent, that learns the antenna beam pattern, needs to discover the causal relationship for the environment to select useful action space during policy training to improve learning efficiency. In this work, we propose the use of the State-Wise Action-Refined Temporal Difference Learning algorithm with IC-INVASE framework (TD3-INVASE) presented in [8] to conduct interventions on the action space during training for discovering the causal relationship between the action space and the reward in a mMIMO-based JCAS setup. This model enables the selection of relevant actions based on causal discovery, which will facilitate sample efficient training for extremely large action and state space applications like mMIMO-based JCAS. In addition to capturing the causal relationships via TD3-INVASE, the framework is also capable of capturing causal relationships from expert and domain knowledge.\nThis is the first work that incorporates causal discovery for efficient state space exploration in mMIMO-based JCAS applications. The State-Wise Action-Refined Temporal Difference Learning framework [8] intervenes the learning process and selects the relevant actions based on the causal discovery. This reduces the exploration for extremely large action space such as in case of JCAS applications. In this context, our proposed model is the first work that incorporates causal discovery in mMIMO-based JCAS.\nThe main contributions of this paper are as follows:\n1.  We propose a novel RL-based framework for beamforming in JCAS environments that learns codebook beam patterns for both communication and sensing based on different environments, user distributions, array geometry and sensing target locations.\n2.  In this context, we explore the use of causal RL based neural learning architectures like TD3-INVASE [8] that enables causal relationship learning between actions and states, leading to exploration of exremely high dimensional beamforming action space.\n3.  Though we demonstrate our approach using TD3-INVASE, the framework allows the capture of causal relationships via expert, domain knowledge, known correlations of the radio propagation environment, or other approaches as well.\n4.  For validating our approach, we consider a diverse set of mobility scenarios generated using well known benchmarking tools. Our experiments establish that causality ascription between state and action spaces in the context of JCAS-based beamforming provides for sample efficient training and higher beamforming gain in comparison to baseline RL training policies employed for communication-only beamforming [9]."}, {"title": "2 BACKGROUND", "content": "Reinforcement Learning: In general, a RL task is defined by a Markov Decision Process (MDP) [16] denoted by a tuple $(\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$, where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, $P$ is the transition probability function of the environment, $R$ is the reward function, and $\\gamma \\in [0, 1)$ is the discount factor. A policy $\\pi\\in \\Pi$ is a probability function that maps states to actions, i.e., $\\pi(S) \\rightarrow Pr(A)$, where $\\Pi$ is the set of all policies. At each time instant $t$, the agent in state $s_t \\in \\mathcal{S}$, takes an action $a_t \\in \\mathcal{A}$ given by the policy $\\pi(s_t)$, transitions to state $s_{t+1} \\sim P(.|s_t, a_t)$, and receives a reward $r_t = R(s_t, a_t)$. The objective of the RL algorithm is to learn a policy $\\pi^* \\in \\Pi$ that maximizes the agent's expected cumulative reward $\\mathbb{E}_{\\tau \\sim \\pi}[\\sum_{t=1} ^\\infty \\gamma^t r_t]$, where $\\tau$ is the trajectory obtained from $\\pi$ under the influence of the transition dynamics $P$.\nIn many environments, it becomes impossible to model the transition probability function $P$ as all the state variables may not be visible to the agent. This makes design of the MDP infeasible. Model-free learning allows the agent to make decisions based on experiences from interactions with the environment, treated as a black box. The reward function $R$ acts as a state-action quality evaluator, returning a value that reflects the quality of the state-action pair. In Deep Reinforcement Learning (DRL), neural networks are used to represent policy $\\pi(s)$ and the state-action quality evaluator $Q(s, a)$. One prominent DRL approach is the actor-critic framework [17]. In the actor-critic model, the policy $\\pi$, which maps the agent's state $s_t \\in \\mathcal{S}$ to action $a_t \\in \\mathcal{A}$, is represented by the actor network with parameter $\\upsilon$ as $\\pi_\\upsilon$. The critic network with parameter $\\phi$ approximates the estimated return for the current state-action pair $(s_t, a_t)$ as $Q(s_t, a_t) = \\mathbb{E}_{(s_t, a_t) \\sim \\pi_\\upsilon, \\tau}[\\sum_{k=1} ^\\infty \\gamma^k r_{t+k}]$. Each interaction of the agent with the environment is stored in the replay buffer $\\mathcal{B}$ as a tuple $(s_t, a_t, r_t, s_{t+1})$. For a set of tuples $(s_t, a_t, r_t, s_{t+1}) \\sim \\mathcal{B}$, the critic is optimized to minimize the Temporal Difference (TD) error [18],\n$\\mathcal{L}_{TD} = \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1})\\sim\\mathcal{B}}[(r_t + \\gamma Q_{\\phi}(s_{t+1}, \\pi_{\\upsilon}(s_{t+1}))\n- Q(s_t, a_t))^2]$\n(1)\nThe reward estimate $Q_{\\phi}(s_t, a_t)$ from the critic network is used to update the policy (actor) network by gradient descent such that the critic's output is maximized, i.e., $\\pi_{\\upsilon}(s_t) = \\arg \\max_{a_t} Q_{\\phi}(s_t, a_t)$. The gradient of the policy network is given by $\\mathbb{E}_{s=s_t, a=\\pi_{\\upsilon}(s_t)}[\\nabla_a Q(s, a) \\nabla_{\\upsilon} \\pi_{\\upsilon}(s)]$.\nINVASE: This particular learning architecture was proposed in [19] to perform instance-wise feature selection to reduce over-fitting in predictive models by discovering a subset of most relevant features at each sampling instance. A selection function is learned by minimizing a Kullback-Leibler (KL) divergence $D_{KL}$ between the conditional probability distribution for all features $p(Y|X = x)$ and minimal-selected-features $p(Y|X(F(x))=x_{F(x)})$, which is represented as,\n$\\min_F \\mathcal{L} = D_{KL}(p(Y|X = x)||p(Y|X(F(x))=x_{F(x)}))\n+ \\lambda |F(x)|_0$\n(2)\nHere, $X$ is the input feature and $Y$ is the predicted output. Function $F : X \\rightarrow {0,1}^d$ is a feature selection function, $|F(x)|_0$ denotes the number of selected features, $d$ is the dimension of input features, and $x_{F(x)} = F(x) \\odot x$ denote the element-wise product of $x$ with the selection function $F(x)$. The KL divergence for any random variables $W$ and $V$ with densities $p_W$ and $p_V$ is defined as $D_{KL}(W||V) = \\mathbb{E}[\\log \\frac{p_W}{p_V}(W)]$. An optimal selection function $F$ can minimize Equation 2. INVASE uses the actor-critic framework to optimize the $F$. The selection function $F$ is represented by a neural network $f_{\\theta}(.|x)$, parameterized by $\\theta$, and uses a stochastic actor to generate probabilities for each feature. Prediction and baseline networks (used for variance reduction) are trained to return the loss function $\\mathcal{L}$, based on which $f_{\\theta}(.|x)$ is optimized using gradient descent $\\mathbb{E}_{(x,y) \\sim p}[\\mathcal{L} \\nabla_{\\theta} \\log f_{\\theta}(.|x)]$"}, {"title": "3 PROBLEM FORMULATION", "content": "We consider a scenario depicted in Fig. 1, where a mMIMO base station with $M$ number of antennas is communicating with single antenna users (fleet of vehicle users in Fig. 1) and is sensing targets in the environment. The fleet of vehicles are equipped with lidar and radar systems that can only sense Line-of-Sight (LOS) traffic. Therefore, incoming traffic from Non-Line-of-Sight (NLOS) can be potential hazards to these vehicle users. The incoming NLOS traffic are sensing targets for the base station, so that the vehicle users are notified about the potential hazard beforehand. The base station uses analog-only beamformers with a network of $r$-bit quantized phase shifters. Let the beamforming codebook $\\mathcal{W}$ adopted by the base station consist of $N$ beamforming vectors, where each vector takes the form\n$w_i = \\frac{1}{\\sqrt{M}} [e^{j\\theta_{i1}}, e^{j\\theta_{i2}}, ..., e^{j\\theta_{iM}}]^T, \\textrm{ for } i = 1, 2, ..., N$\n(3)\nwhere each $\\theta_m$ phase shifter takes the values from a finite set of $2^r$ possible discrete values drawn uniformly from $(-\\pi, \\pi]$. Let $N_C$ be the number of communication beams and $N_S$ be the number of sensing beams in the $N$-sized beam codebook. Fig. 1 depicts dashed beam ($w_1$) to serve communicating users and solid beam ($w_2$) to sense potential hazards. Here, $w_1$ is beamforming vector responsible for communication, while $w_2$ is the vector responsible for sensing.\nLet us consider that the communicating users do uplink transmission by sending some signal $x$ to the base station with power $\\mathbb{E}[|x|^2]$. Then, the received signal at the base station after combining with the beamforming vector $w_1$ can be expresses as $y_u = w_i^H h_u x + n$, where $h_u \\in \\mathbb{C}^{M\\times 1}$ is the uplink channel vector between the user (or multiple users with similar channel) $u$ and the base station antennas, and $n \\sim \\mathcal{N}_c(0, \\sigma_n^2 I)$ is the received noise vector at the base station [20]. Here, $i \\in \\mathcal{C}$, where $\\mathcal{C}$ is the set of communication beam indices in the beam codebook and $|\\mathcal{C}| = N_C$. The beamforming gain of the communication beamformer $w_i$ for user $u$ is $g_u = |w_i^H h_u|^2, \\forall i \\in \\mathcal{C}$.\nIn the context of sensing, the base station antennas' array response is given as\n$b_r = [e^{j2\\pi d_1 sin\\theta},\\textrm{ } e^{j2\\pi d_2 sin\\theta}, ..., e^{j2\\pi d_M sin\\theta}]^T$\n(4)\nwhere $d_1, d_2, ..., d_M, m = 1, 2, ..., M$ denotes the transmit antenna positions. The angle $\\theta$ is the Angle of Arrival (AoA) of the echo signal for sensing from the target. The received sensing gain at the base station is given as $g_r = |w_i^H b_r|, \\forall i \\in \\mathcal{S}$. Here, $\\mathcal{S}$ is the set of sensing beam indices in the beamforming codebook and $|\\mathcal{S}| = N_S$.\nOur objective is to learn optimal beamforming vectors $w_{i,opt}, \\forall i \\in \\mathcal{C} \\cup \\mathcal{S}$ for mMIMO-based JCAS. We formulate a joint optimization problem where the communication beam learning problem can be formulated as,\n$\\begin{aligned}\nw_{i,opt} = \\textrm{argmax }\\frac{1}{\\mathbb{E}_{h_u}} \\sum_{h_u \\in \\mathcal{H}} |w_i^H h_u|^2,\\\\\ns.t.\\textrm{ } \\forall i \\in \\mathcal{C}, \\\\\n|w_m| = \\frac{1}{\\sqrt{M}}e^{j\\theta_m}, \\textrm{ } \\theta_m \\in \\Theta, \\forall m = 1, 2, ..., M,\\\\\n\\end{aligned}$\n(5)"}, {"title": "4 DRL BEAM CODEBOOK LEARNING FOR JCAS", "content": "In this section, we describe our multi-network DRL architecture to optimize the JCAS beam learning problem formulated in the previous section with reference to the Fig. 2. The work [9] has presented a three stage beam codebook learning framework for communication. We extend their model to JCAS by incorporating a three stage pipeline for sensing beam learning in our proposed framework. We will first discuss the stages for communication that has been proposed in the existing literature. Then we will discuss our proposed approach for sensing beam design in order to realize JCAS.\n\n4.1 COMMUNICATION BEAM DESIGN\n4.1.1 STAGE 1\nThe first stage partitions or clusters the users in the environment based on channel similarity. The clustering method relies on a sampled subset $\\mathcal{F} = {f_1, f_2, ..., f_C }$ of the communication beam vectors (Equation 3) to gather information about the user distribution in the environment in the form of received gain, where $f_s \\in \\mathbb{C}^M, \\forall s \\in {1, ..., C}$. Let $\\mathcal{K}'$ channels contribute to the received gain in the clustering process and they are denoted by $\\mathcal{H}_{cl} = {h_1, h_2, ..., h_{\\mathcal{K}'}}$, where $\\mathcal{H}_{cl} \\subseteq \\mathcal{H}_s$. The received gains are used to construct a matrix $P$ given as,\n$P = \\begin{bmatrix}\n|f_1^H h_1|^2 & ... & |f_1^H h_{\\mathcal{K}'}|^2\\\\\n: & ... & :\\\\\n|f_C^H h_1|^2 & ... & |f_C^H h_{\\mathcal{K}'}|^2\\\n\\end{bmatrix}$\n(7)\nHere, each column represents the received gains of each user cluster in the environment. Applying a clustering algorithm directly on $P$ has been empirically shown to yield overlapping clusters that are hard to separate [9]. Therefore, the authors in [9] propose to construct a feature matrix $U = [u_1, u_2, ..., u_{\\mathcal{K}'}]$ by finding the pair-wise differences of the columns in matrix $P$, which is constructed as follows,\n$u_k = \\frac{1}{C} \\sum_{c=1} ^C \\begin{bmatrix}\n|f_c^H h_k|^2 - |f_1^H h_k|^2\\\\\n:\\\\\n|f_c^H h_k|^2 - |f_C^H h_k|^2\\\\\n\\end{bmatrix}, \\forall k \\in {1, 2, ..., \\mathcal{K}'}$\n(8)\nNow, the clustering is applied on the columns of the matrix $U$ to form $N_C$ channel (user) clusters using K-means algorithm [21]. The columns $u_k$, where $k = 1, 2, ..., \\mathcal{K}'$, are $C$-dimensional vectors given as input to the distortion measure objective function $J = \\sum_{n=1}^{N_C} \\sum_{k=1}^{\\mathcal{K}'} r_{kn}||u_k - \\mu_n||^2$, where $\\mu_n, \\forall n \\in {1, ..., N_C }$ is a $C$-dimensional vector representing the centre of the $n^{th}$ cluster and $r_{kn} \\in {0,1}$ indicates whether $u_k$ is assigned to cluster $\\mu_n$. The objective function $J$ is minimized in two phases, (a) re-assignment of columns $u_k$ to clusters, and (b) re-computing the cluster centres $\\mu_k$, until there is no further change in clustering assignment. The channels in $\\mathcal{H}_{cl}$ are partitioned into $N_C$ disjoint sets as $\\mathcal{H}_{cl} = \\mathcal{H}_1 \\cup \\mathcal{H}_2 \\cup ... \\cup \\mathcal{H}_{N_C}$, where $\\mathcal{H}_k \\cap \\mathcal{H}_l = \\Phi, \\forall k \\neq l$.\n4.1.2 STAGE 2\nThe second stage assigns each of $N_C$ channel clusters to $N_C$ different DRL agents, where each DRL learns the communication beam for the assigned channel cluster. Let $\\tilde{\\mathcal{H}}_{cl} = \\tilde{\\mathcal{H}}_1 \\cup \\tilde{\\mathcal{H}}_2 \\cup ... \\cup \\tilde{\\mathcal{H}}_{N_C}$ be the new cluster assignment based on the k-means clustering algorithm discussed before. Let $\\Gamma = {\\tilde{w}_1, ..., \\tilde{w}_{N_C}}$ be the \"temporarily best\" beamforming vectors of each of the $N_C$ DRL agents assigned to corresponding channel clusters in $\\tilde{\\mathcal{H}}_{cl}$. The goal is to find the cluster-network assignment that minimizes a cost sum which is a function of the average beamforming gain calculated using the newly assigned channels $\\mathcal{H}_{cl}$ and current beamforming vector $\\Gamma$. The cost function is calculated as $Z_{nn'} = \\frac{1}{|\\mathcal{H}_{n'}|} \\sum_{h \\in \\mathcal{H}_{n'}} |\\tilde{w}_n h|^2$, where $Z_{nn'}$ is the average beamforming gain of the $n^{th}$ temporarily best beamforming vector in $\\Gamma$ with the $n'$-th channel cluster in $\\tilde{\\mathcal{H}}_{cl}$. Using the cost matrix, the clustering assignment is formulated as a linear sum assignment problem,\n$\\begin{aligned}\n\\min_X - \\sum_{n=1}^{N_C} \\sum_{n'=1}^{N_C} X_{nn'} Z_{nn'}\\\\\ns.t.\\textrm{ } X \\textrm{ is a permutation matrix}\\\\\n\\end{aligned}$\n(9)\n(10)\nThis problem is solved using Hungarian algorithm [22] to obtain $N_C$ tuples which associates each beamforming vector to the channel cluster as $(\\tilde{w}_n, \\tilde{\\mathcal{H}}_{n'}), n, n' \\in {1, 2, ..., N_C}$.\n4.1.3 STAGE 3\nAfter channel clustering and their assignment, each channel cluster is used by a DRL agent to train its own beam. The user clustering and assignment are two key stages that are performed at frequent intervals in order to enable adaptibility to dynamic environment. The beam pattern learning at each of the $N_C$ DRL agents proceed according to the procedure which we will discuss in Section 5, with an additional fine-tuning step involving perturbation of the beam vector with exploration noise, quantization of the vector, and it's evaluation on the assigned user cluster.\n4.2 SENSING BEAM DESIGN\n4.2.1 STAGE 1\nThe first stage involves identifying the sensing targets. Let there be $\\mathcal{T} = {t_1, t_2, ..., t_{N_S} }$ targets that need to be sensed in the environment and tracked by the sensing beams of the base station. We provide our model the initial location of each of the targets in $\\mathcal{T}$ as coordinates $(x_i, Y_i), \\forall i \\in {1, 2, ..., N_S}$.\n4.2.2 STAGE 2\nThe second stage calculates the AoA $\\theta_i, \\forall i \\in {1, 2, ..., N_S}$ at the antenna steering vector (Equation 4) located on the base station. The AoA for each target in $\\mathcal{T}$ is obtained as\n$\\theta_i = tan^{-1}(\\frac{Y_i - Y_{bs}}{X_i - X_{bs}}), \\forall i \\in {1, 2, ..., N_S}$\n(11)"}, {"title": "5 CAUSAL DRL FOR BEAM PATTERN DESIGN", "content": "We propose the use of the INVASE framework used in [8], to adjust the phase shifters for addressing the beam pattern design problem. The beam design problem aims to either maximize the beamforming gain of communication user or maximizing the beamforming gain for sensing targets, depending on the of the DRL agent. The RL setup consists of the following building blocks.\n*   State: We define state $s_t$ as a vector that consists of the phases of all the phase shifters at the $t^{th}$ iteration, $s_t = [\\theta_1, \\theta_2, ..., \\theta_M]^T$, where $\\theta_1, \\theta_2, ..., \\theta_M$ are the phase angles of the M BS antennas that constitute the beamforming vector $w_i$ and they lie within the range $(-\\pi, \\pi]$.\n*   Action: We define action $a_t$ as the element-wise changes to all the phases in $s_t$. The action becomes the next state, i.e., $s_{t+1} = a_t$.\n*   Reward: We define a reward mechanism for both communication and sensing such that the reward at the $t^{th}$ iteration, denoted as $r_t$, takes values from {$+1,0,-1$}. To compute the reward, we compare the current beamforming gain, denoted by $g_t$, with (a) an adaptive threshold $\\beta_t$, and (b) the previous beamforming gain $g_{t-1}$. The reward at $t^{th}$ iteration is,\n$r_t =\n\\begin{cases}\n+1, & \\textrm{if } g_t > \\beta_t\\\\\n0, & \\textrm{if } g_t \\leq \\beta_t \\textrm{ and } g_t > g_{t-1}\\\\\n-1, & \\textrm{if } g_t \\leq \\beta_t \\textrm{ and } g_t \\leq g_{t-1}\\\\\n\\end{cases}$\n(12)\nThe adaptive threshold $\\beta_t$ does not rely on any prior knowledge of the channel distribution or the AoA of the sensing target. The threshold has an initial value of zero. Whenever the beamforming gain $g_t$ surpasses the current threshold $\\beta_t$, the system updates the threshold with the current gain $g_t$, i.e., $\\beta_{t+1} = g_t, \\textrm{ if } r_t = +1$. The sensing beam threshold has a different value than the communication beam threshold. To generalize, there could be other data and domain driven approaches for site-specific reward.\nNow, we discuss the vanilla INVASE architecture [19] using temporal difference learning that is used to train our DRL agents. The architecture has been illustrated in Fig. 3. The agent interacts with the environment and stores the state, action, reward, and next state in the replay buffer as a tuple $(s, a, r, s')$. States and actions sampled from replay buffer are fed into the selector network which selects the selection probabilities of different dimensions of the action. A selection mask is generated according to the selection probability vector and multiplied with the action vector to obtain the most relevant actions. Hence, the selector network intervenes and discovers the causal relationship between the action dimensions and the state. The critic network is trained to minimize the Temporal Difference (TD) error with the states and the selected action dimensions, while the baseline network is trained to minimize the TD error with the states and the primal action space. The difference between the TD errors is used to conduct policy gradient to update the selector network. We use the Iterative Curriculum INVASE (IC-INVASE) [8] which incorporates two improvements over vanilla INVASE to make it suitable for action selection when the action dimension in RL is extremely high, for instance, the huge action dimension in mMIMO beamforming applications. The first improvement is based on curriculum learning and it is used to tackle the problem of convergence of the INVASE to sub-optimal solutions by pruning all actions when the $\\lambda$ parameter in Equation 2 is large [19]. The second improvement provides an iterative structure of the action selection such that the selection function $G$ (previously defined as $F$ in Section 2) conducts hierarchical action selection with less computational expenses.\n5.1 CURRICULUM FOR HIGH DIMENSIONAL ACTION SELECTION\nCurriculum learning [23] mimics human learning by gradually training to make decisions for more difficult tasks and its effectiveness has been demonstrated in several works [24-26]. Intuitively, it will be easier to select $P$ useful actions out of $L$ primal actions when $P$ is large, the most trivial solution being that all of the $L$ actions are selected, i.e.,"}, {"title": "5.2 ITERATIVE SELECTION OF ACTIONS", "content": "The selection function $G(x)$ is be applied for multiple times to perform coarse-to-fine action selection. The $i^{th}$ dimension of the action after selection $x^{(G(x))}$ is given as\n$x^{(G(x))}= \\begin{cases}\n1, & \\textrm{if } G_i(x) = 1\\\\\n0, & \\textrm{if } G_i(x) = 0\\\\\n\\end{cases}$\n(14)\nwhere $G_i(x)$ is the selection function output for the $i^{th}$ action dimension. Then $x^{(G(x))}$ is fed into the selector network to form the hierarchical action selection over $n$ selection processes given as\n$\\begin{aligned}\nx^{(1)} &= (G(x) \\odot x),\\\\\nx^{(2)} &= (G(x^{(1)}) \\odot x^{(1)}),\\\\\n:\\\\\nx^{(n)} &= (G(x^{(n-1)}) \\odot x^{(n-1)})\\\\\n\\end{aligned}$\n(15)\nAfter each selection operation, the most relevant $p_r$ actions are selected, for instance, if $p_r = 0.5, 50\\%, 25\\%, \\textrm{and }12.5\\%$ most important actions will be selected after the first three selection operations. IC-INVASE is a combination of these two improvements and this contributes to high learning efficiency in high-dimensional action selection tasks."}, {"title": "5.3 TEMPORAL DIFFERENCE STATE-WISE ACTION REFINEMENT", "content": "We use the algorithm named TD-SWAR proposed in [8] on top of the vanilla Twin Delayed Deep Deterministic Policy Gradient (TD3) [27] algorithm (referred to as TD3-INVASE) to minimize the loss function for the IC-INVASE architecture that we have employed for the beam pattern learning by DRL agent (see Figure 2). The pseudocode for the algorithm is presented in Algorithm 1, which depicts the steps for updating the parameters of the modules in Fig. 3."}, {"title": "6 EXPERIMENTAL SETUP", "content": "In this section, we will discuss about the channel dataset generation from the training scenario used in our experiments, followed by the training setup.\n6.1 DATASET GENERATION FOR JCAS\nTo conduct the performance evaluation of the proposed TD3-INVASE for beamforming in a JCAS setup, we consider a dynamic scenario where the base station serves the communication among static users while sensing the vehicles nearest to the communicating users. In our experiments, we demonstrate JCAS at the base station by learning beams for communicating users and beams for sensing vehicles nearest to the communicating users. Therefore, we require the channel dataset (vectors) for training communication beams, and the locations of vehicles, users and base station (coordinates) to compute the AoA for training sensing beams. The channel, vehicle and base station location datasets are sampled from the dynamic scenario, as shown in Figure 4, at 100 ms and 1000 such sampled snapshots are available as a part of the MIMO channel dataset obtained from benchmarking channel generation tool DeepMIMO [28].\n\n6.2 TRAINING MODEL\nIn all experiments performed in the subsequent section, we use the standard neural network structure of actor and critic. Baseline Deep Deterministic Policy Gradient (DDPG) [30] algorithm is used to train this neural network structure. The input to the actor network is the state, i.e. phases of the antenna phase shifters, and hence has a dimension of M. The actor has two hidden layers of 16M neurons which are followed by Rectified Linear Unit (ReLU) activations. The actor's output layer is activated using hyperbolic tangent (tanh) scaled by $\\pi$ to give the predicted action of dimension M. The critic network takes as input the concatenation of state and action with dimension 2M and has two hidden layers with 32M neurons followed by ReLU activations. The critic's output is a 1-dimensional predicted Q value of the input state-action pair. Baseline TD3 used in our experiments has two critics, each having two hidden layers of 32M neurons followed by ReLU activations. For TD3-INVASE, the two critics take as input the 2M dimensional state-selected actions pair, while the baseline networks takes as input the 2M dimensional state-primal actions pair. Both the critic and baseline networks have two hidden layers of 32M neurons followed by ReLU activations and each of them give 1-dimensional Q value output. The selector network have two hidden layers of 100 neurons and they are followed by sigmoid activation."}, {"title": "7 RESULTS", "content": "In our experiments, we have $M$ antennas on the base station, and hence state and action dimensions are $M$. We add $M_{Red}$ fictitious antennas to these actual antennas, which is analogous to the fact that we are injecting $M_{Red}$ redundant actions into the action space. These redundant action dimensions will not affect the state transitions or the reward calculation, but the learning agent needs to identify the redundant actions among the $M + M_{Red}$ actions by finding the state-action causal relationships and pruning them during training in order to perform the learning process efficiently."}, {"title": "7.1 TRAINING RESULTS", "content": "We provide quant-itative comparison between TD3-INVASE, and baselines vanilla TD3 and DDPG policies for learning communication and sensing beams by training the DRL agents on the channel and sensing datasets. The hyper-parameters used for training can be found in Table 2. In mMIMO systems, the number of antennas on the transmitter/receiver units are higher than 16 [31"}]}