{"title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents", "authors": ["Aobo Kong", "Wentao Ma", "Shiwan Zhao", "Yongbin Li", "Yuchuan Wu", "Ke Wang", "Xiaoqian Liu", "Qicheng Li", "Yong Qin", "Fei Huang"], "abstract": "Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex goal-oriented social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across a variety of agent tasks. Existing DPO-based approaches for multi-turn interactions are divided into turn-level and session-level methods. The turn-level method is overly fine-grained, focusing exclusively on individual turns, while session-level methods are too coarse-grained, often introducing training noise. To address these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which focuses on specific key segments within interactions to optimize multi-turn agent behavior while minimizing training noise. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO's potential to advance the social intelligence of LLM-based agents. We release our code and data at this url.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have significantly enhanced their capabilities in language understanding and generation, particularly within the realm of human-machine interaction. By incorporating identity-specific information, LLM-based agents can simulate human social behaviors, demonstrating basic social intelligence in tasks such as role-playing casual conversations (Wang et al., 2024a; Lu et al., 2024) and navigate simulated social environments (Park et al., 2023). However, recent studies (Zhou et al., 2024) have shown that, in more complex, goal-oriented social scenarios, such as negotiation, competition, and cooperation, LLMs still struggle to exhibit the nuanced decision-making abilities that are characteristic of human social interactions.\nIn response to these challenges, several methods have been developed to better align LLM behavior with human preferences in multi-turn interactions. These approaches offer promising strategies for improving social decision-making in LLMs. Specifically, we focus on Direct Preference Optimization (DPO)-based methods, which can be divided into two primary categories: turn-level DPO (Rafailov et al., 2023), and session-level DPO approaches, such as ETO (Song et al., 2024) and DMPO (Shi et al., 2024). Turn-level DPO, for example, identifies a single conversational turn and uses a \"positive-negative\" pair of responses from that turn to optimize the model via a preference loss function. While turn-level DPO has demonstrated some effectiveness, its focus on individual turns limits its ability to model goal completion in goal-oriented social dialogues, where success often relies on high-quality interactions spanning multiple conversational turns.\nTo more effectively align agent behavior in multi-turn interactions, session-level alignment methods such as ETO and DMPO have been proposed. These methods extend the sampling scope from individual turns to entire sessions, constructing pairs of good and bad sessions and applying an adapted DPO loss for training. However, these methods still exhibit certain limitations due to their relatively coarse alignment granularity:\n(i) Turns without errors in the negative sessions are still treated as bad outputs, introducing significant noise that can negatively affect the training process.\n(ii) Sampling from scratch provides the interlocutor with a vast action space. A higher score for a positive session may result from changes in the interlocutor's behavior, making it challenging for the model to learn the correct behavior pattern"}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 SOTOPIA Environment", "content": "Unlike previous social benchmarks that primarily test through static QA formats (Sap et al., 2019; Chen et al., 2024), SOTOPIA offers an interactive, open-ended, and realistic simulation environment, enabling a more precise assessment of agents' social intelligence. A social task in SOTOPIA involves a scenario, two role profiles, and their private social goals to be achieved through interaction. The diverse combinations of scenarios and social goals encompass a broad spectrum of social interactions, such as negotiation, collaboration, and competition. SOTOPIA defines seven dimensions for evaluating social agents. We focus primarily on the \"goal\" (0-10, int) and \"relationship\" (-5 to 5, int), as GPT-4o's ratings in these metrics closely align with human evaluations. SOTOPIA-\u03c0 (Wang et al., 2024b) is a follow-up work that leverages GPT-4 to automatically construct a set of scenarios (completely non-overlapping with SOTOPIA), which serves as the training dataset for our study. Additionally, we restructure the prompt organization format of SOTOPIA to support multi-turn alignment, and the details are provided in Appendix A."}, {"title": "2.2 Task Formulation", "content": "In a SOTOPIA task, we denote the background information available to the agent as b, which includes the scenario, role profiles, and its goal. The interaction history $h_n$ faced by the agent in the n-th round is as follows:\n$h_n \\begin{cases} b, y_0, Y_0, ..., y_{n-1}, Y_{n-1} & \\text{if speak first} \\\\ b, Y_0, Y_0, ..., y_{n-1}, Y_n, & \\text{if speak later} \\end{cases}$                                                                                                                                         (1)\nHere, $y_i \\sim \\pi_\\theta(\\cdot|h_i)$ represents the output generated by the LLM-based agent in round i according to its policy $\\pi_\\theta$ with parameter $\\theta$. On the other hand, $y_i'$ represents the output of the interlocutor, which is drawn from an unknown distribution. Based on this formulation, we present the DPO and ETO loss functions (ETO directly extends turn-level DPO to the session level without rigorous proof) in Appendix B.1 and B.2."}, {"title": "2.3 DMPO", "content": "DMPO introduces the state-action occupancy measure (SAOM) to help derive the loss of DPO in multi-turn interaction scenarios. The discounted SAOM $d^{\\pi}(s, a)$ of a policy $\\pi$ is as follows:\n$d^{\\pi}(s = s_t, a = a_t) = \\gamma^t \\cdot P(s_0) \\cdot \\prod_{k=0}^{t-1} \\pi(a_k|s_k)P(s_{k+1}|s_k, a_k).$                                                                                                                                 (2)\nSimilar to DPO, DMPO defines the following RL objective based on $d^{\\pi}$:\n$\\max_{\\pi_\\theta} E_{(s,a) \\sim d^{\\pi_\\theta}(s,a)} [r(s, a)] - \\beta D_{KL}[d^{\\pi_\\theta}(s,a)||d^{ref}(s,a)],$                                                                                                                             (3)\nwhere $\\pi_{ref}$ represents the reference policy. The reward function in DMPO takes the form:\n$r(s, a) = \\beta \\log \\frac{d^{\\pi^*}(s, a)}{d^{ref}(s,a)} + \\beta \\log Z,$                                                                                                                                            (4)\nwhere Z is the partition function and remains constant for all (s, a) pairs. Then apply the BT model as follows:\n$p(\\tau^w > \\tau^l|s_0) = \\sigma(\\sum_{t=0}^{T^w-1} r(s_t^w,a_t^w) - \\sum_{t=0}^{T^l-1} r(s_t^l,a_t^l)),$                                                                                                                         (5)\nwhere $\\tau^w$ and $\\tau^l$ represent the \"win\" and \"lose\" trajectories respectively, $T_w, T_l$ denote the number of rounds in each. However, since $T^w \\neq T^l$, the partition function Z can not be canceled directly"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Behavioral Cloning", "content": "Behavioral cloning (Pomerleau, 1991), as an effective method of imitation learning, is widely used in the construction of various LLM-based agents (Xu et al., 2024; Song et al., 2024). In this work, we utilize GPT-4-turbo as the expert to collect expert sessions on SOTOPIA-\u03c0 through self-chat and interactions with GPT-4o. Based on this data, we fine-tune open-source LLMs like Llama-3.1, establishing the initial social agent for our experiments."}, {"title": "3.2 Preference Data Construction", "content": "Building high-quality segment-level preference data pairs is the core of our approach. On SOTPIA-\u03c0, our social agent engages in self-chat and interactions with GPT-40. We set a threshold of 7 for the goal dimension, and all dialogues with a goal completion level below this threshold are considered potential negative samples. Given a negative session, the pipeline for generating positive data involves three steps, as illustrated in Figure 6.\nError Locaton Unlike scenarios with clear error definitions such as math, errors in social dialogues are a relatively ambiguous concept. In a negative session, if our agent's utterance in a specific turn meets the following criteria: (1) the turn is critical for achieving the role's goal, (2) there is still room for improvement in the goal completion or their relationship, we identify that turn as erroneous. The error location is performed by GPT-40.\nPositive Session Sampling After the error location, we sample 5 complete sessions based on the interaction history prior to that turn. Among these sessions, we select the one with the highest goal and relationship scores (with goal completion prioritized over relationship). If the goal or relationship score of the optimal session is higher than that of the negative sample, this session and the negative sample form a data pair; otherwise, the negative sample is discarded.\nSegment Selection Once we obtain session-level data pairs, we provide both the positive and negative samples to GPT-40, prompting it to select a segment from the positive sample. This segment should correspond to the part that contributes to the positive sample achieving higher goal and relationship scores. Subsequently, we extract a segment of the same length from the negative sample and pair it with the positive sample to form a segment-level data pair. This process aims to exclude turns, such as pleasantries, that are not directly related to achieving the goal.\nThe discussion of GPT-40's performance on error location and segment selection, along with the related prompts for these two steps, are provided in Appendix C.2 and C.3, respectively.."}, {"title": "3.3 SDPO Loss", "content": "Session-level ETO and DMPO can not control the length of positive and negative sessions. To address this, DMPO utilizes the length normalization to eliminate Z in Eq (5). Different from them, SDPO selects a segment from both the positive and negative sessions for optimization, allowing free control over their lengths. Specifically, after selecting a segment from the positive session using GPT-40, SDPO selects a segment of equal length from the negative session. By ensuring the two segments are of equal length, we can directly eliminate Z, resulting in the following concise SDPO loss:\n$L_{SDPO} = -E_{(h_e, h^w, h^l) \\sim D} \\log \\sigma(\\sum_{t=e}^{e+k} \\beta \\log \\frac{\\pi_\\theta(y_t^w|h_t)}{\\pi_{ref}(y_t^w|h_t)} - \\sum_{t=e}^{e+k} \\beta \\log \\frac{\\pi_\\theta(y_t^l|h_t)}{\\pi_{ref}(y_t^l|h_t)}),$                                                                                                 (7)\nwhere e denotes the round number of the erroneous turn, and k represents the total number of rounds within the selected segments."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Datasets", "content": "SOTPIA-\u03c0, used for training, includes a total of 410 scenarios: 100 scenarios for BC, with 10 role pairs per scenario, and 310 scenarios for alignment, with 8 role pairs per scenario. SOTOPIA, used for testing, includes 90 scenarios, each with 5 role pairs, resulting in a total of 450 tasks for self-chat and 900 tasks for non-self-chat."}, {"title": "4.2 Experimental Setup", "content": "Training We primarily use Llama-3.1-8B-Chat (Dubey et al., 2024) as the base LLM to build the social agent. The maximum token limit is set to 4096, and AdamW optimization is employed for all training processes. During the SFT phase, the batch size is 32, the dropout rate is 0.2, and the learning rate is 1e\u22125 with 5% warm-up ratio and a cosine decay schedule. For the training phase of SDPO, the batch size remains 32, \u03b2 in SDPO loss is 0.1, and the learning rate is 1e-6 with no warm-up but a cosine decay schedule. The statistics of SDPO training data are detailed in Appendix C.1.\nSOTOPIA During the sampling of positive data, the temperature of the target agent is set to 1.0, while the other agent's temperature is set to 0.7. For testing, we set the temperature of both interacting agents to 0.7. Though the temperature introduces randomness to the agents' outputs, we find that the evaluation results remain numerically stable. Thus, we report the results based on a single test."}, {"title": "4.3 Baselines", "content": "We compare the proposed SDPO with several strong baselines. 1) OpenAI proprietary LLMs. We provide the specific model versions in Appendix D.1. 2) SFT Behavioral Cloning fine-tunes LLMs on expert interaction data, producing a resulting model that serves as the base agent for SDPO and the following baselines. 3) DPO optimizes the agent policy based on turn-level data, specifically targeting the first differing turn in the positive and negative samples used by SDPO. 4) ETO optimizes the agent policy using session-level data. ETO utilizes the same negative sessions as SDPO while sampling five new sessions from scratch to form the data pairs. 5) DMPO leverages the same data as ETO and employs a new loss function to update the policy. 6) Preferred-SFT fine-tunes the base gent on the positive sessions in SDPO."}, {"title": "4.4 Results", "content": "We present the results of SDPO and all the baselines on SOTOPIA in Table 1. As shown, in both the goal and relationship dimensions, SDPO significantly outperforms turn-level DPO, session-level ETO, and DMPO, even surpassing proprietary LLMs like GPT-4o by a large margin, highlighting the effectiveness of segment-level alignment. By analyzing the interaction histories in SOTOPIA, we find that weaker agents often exhibit stubbornness and only express their demands repeatedly. This leads to lower goal and relationship levels, especially in self-chat scenarios. Behavioral cloning using expert data can effectively improve this situation, making the agent more communicative. The reason why Llama-8B+BC's goal rate drops in its interaction with GPT-4o is that the agent becomes persuadable. We also observe that aligned agents achieve simultaneous improvements in both goal and relationship. This indicates that alignment methods indeed enhance the social intelligence of models, rather than achieving goals through behaviors that violate social norms like threatening and deception.\nWe also repeat the above experiments using Mistral-Instruct-v0.3 (Jiang et al., 2023), with the results presented in Table 2. The detailed experimental setup for Mistral is provided in Appendix D.2. SDPO consistently outperforms all baselines, demonstrating the generalization of our method."}, {"title": "4.5 Analysis", "content": "Necessity of Multi-turn Alignment After DPO adjusts the first-turn output probabilities for positive and negative segments, will the probabilities of positive segments increase and those of negative segments decrease in subsequent turns? To explore this, we plot the probability differences between positive and negative segments for DPO and SDPO during training, as shown in Figure 2 (only SDPO can be directly compared with DPO; therefore, ETO and DMPO are not mentioned here.). The DPO-turn trajectory is nearly parallel to the DPO trajectory, indicating that DPO has almost no influence on the probability differences of subsequent turns. In contrast, the SDPO trajectory rises more steeply. These results demonstrate the necessity of explicitly modifying the probability distribution across turns within the entire segment, providing an explanation for the superiority of multi-turn alignment over DPO.\nVariation in Model Output Length We present the output length of various agents during their interactions with GPT-40 in Figure 3. Compared to the BC agent, all alignment methods increase the output length of the models. This phenomenon is commonly observed when DPO is applied to AI chatbots. However, unlike the users' potential bias toward longer responses, which might be misleading, effective social strategies in social scenarios often require more tokens for communication. Thus,"}, {"title": "4.6 Ablation Study", "content": "Segment Selection We explore different segment selection methods of SDPO, with the results presented in Table 3 (in the square brackets, the length of the negative segment is listed first, followed by the positive segment.). The segment length refers to the number of turns contained within the segment. For symmetric segment lengths, methods with fixed lengths of 3 and 5 outperform the length of 1 (DPO), demonstrating the efficacy of multi-turn alignment. The method with a segment length of 5 is less effective than that with a length of 3, indicating that longer segments are not always better. Building on this insight, we leverage GPT-40 to automatically identify key segments from each positive sample, achieving the best results. For asymmetric segment lengths, model training for segment lengths of [3,1] and [5,3] collapse and can not interact normally. Other asymmetric segments underperform compared to their symmetric counterparts, supporting the theoretical discussions in Sections 2.3 and 3.3. Furthermore, we observe that as the degree of asymmetry decreases, the model's performance improves. This improvement could be attributed to the reduced effect caused by the un-eliminated Z on the loss as asymmetry diminishes. This finding also helps explain the effectiveness of ETO, which does not impose constraints on the lengths of positive and negative sessions.\nInterlocutor for Sampling The alignment data for SDPO is collected separately using the BC agent itself and GPT-40 as interlocutors. We train models on each subset of data independently using SDPO, with the results summarized in Table"}, {"title": "5 Related Work", "content": "Social Intelligence Social intelligence can be defined as an agent's ability to understand, adapt to, and respond to the emotions, intentions, and behaviors of others in social interactions. Most research on social intelligence has centered around evaluation. For example, SOCIALIQA (Sap et al., 2019) emphasizes commonsense reasoning about social situations, while SocialIQ (Zadeh et al., 2019) extends evaluation modalities from plain text to video. Shapira et al. (2023) assess large language models (LLMs) using the Faux Pas Test, and Social-Bench (Chen et al., 2024) evaluates the sociality of role-playing agents at both individual and group levels. Additionally, some studies (Le et al., 2019; Shapira et al., 2024) examine models' social intelligence from a theory-of-mind perspective. However, with the advancement of LLM, LLM-based social agents are now capable of interacting in real social scenarios. The traditional static QA-style benchmarks are no longer sufficient to evaluate the social intelligence of the agents. SOTOPIA (Zhou et al., 2024) is currently the only dynamic and interactive social benchmark, providing simulated testing environments for contemporary social agents. We hope this work will inspire further research aimed at enhancing the social intelligence of models through methodological innovation.\nAlignment Methods of Different Granularities Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) is an effective approach for aligning LLMs' outputs with human preferences. However, it has notable drawbacks, including instability, complexity, and high resource consumption. To address these challenges, (Rafailov et al., 2023) introduce DPO, which simplifies training by using offline data with a straightforward classification loss. We classify DPO as a turn-level alignment method, and various alignment algorithms at different granularities have been developed based on DPO. Token-level DPO (TDPO) (Zeng et al., 2024) integrates forward KL divergence constraints at the token level, enhancing both alignment and diversity. Step-DPO (Lai et al., 2024) utilizes individual reasoning steps for preference optimization instead of holistic answer-level evaluation. However, in multi-turn interaction scenarios such as goal-oriented social dialogues or web navigation, single-turn alignment is insufficient. To tackle this, ETO and DMPO extend DPO to multi-turn contexts by leveraging session-level data. We take a step further by proposing SDPO, which introduces a segment-level sample pair optimization framework to achieve finer-grained alignment in multi-turn interactions."}, {"title": "6 Conclusion", "content": "In this paper, we introduce Segment-Level Direct Preference Optimization (SDPO) to improve the performance of LLM-based agents in multi-turn, goal-oriented social dialogues. Unlike existing alignment methods such as turn-level DPO and session-level approaches including ETO and DMPO, SDPO focuses on optimizing the agent policy by targeting specific key segments within a session. Our extensive evaluation on the SOTOPIA benchmark shows that SDPO significantly outperforms existing methods, highlighting the superiority of segment-level alignment. Looking ahead, we plan to apply SDPO to other agent tasks to further explore its versatility and effectiveness."}, {"title": "7 Limitations", "content": "Our proposed SDPO assumes equal lengths for positive and negative segments, achieving state-of-the-art performance under this assumption. Specifically, after selecting a segment from the positive sample, we choose a segment of the same length from the negative sample to eliminate the partition function Z. However, this approach has certain limitations. Negative segments may include irrelevant or error-free turns, or fail to capture all erroneous turns, highlighting the need for more fine-grained control when selecting segments from negative samples. Currently, we have not identified a theoretical framework that effectively supports the alignment of segments with unequal lengths. We hope our work will inspire further research and encourage diverse theoretical analyses for addressing this issue in multi-turn alignment.\nAdditionally, as SOTOPIA is currently the only available interactive social benchmark, our experiments are conducted exclusively on this dataset. In the future, we plan to incorporate additional interactive agent tasks to further validate the generalizability of SDPO."}, {"title": "A Modifications to SOTOPIA", "content": "In SOTOPIA, each interaction is structured as a single-turn format, which does not support multi-turn alignment. To address this limitation, we modify the prompt organization format, as illustrated in Figure 5. These modifications are applied before invoking LLMs' APIs, ensuring they remain invisible to SOTOPIA itself and do not impact the evaluation of GPT-40. Further details can be found in our code repository."}, {"title": "B Supplementary Theoretical Analysis", "content": ""}, {"title": "B.1 DPO", "content": "Rafailov et al. (2023) propose Direct Preference Optimization (DPO), a method that leverages pairwise preference data to train policy models. In the context of social dialogue, we denote the number of the erroneous round as e. The DPO loss function is as follows:\n$L_{DPO} = -E_{(h_e, y^w, y^l) \\sim D} \\log \\sigma(\\beta \\log \\frac{\\pi_\\theta(y^w|h_e)}{\\pi_{ref}(y^w|h_e)} - \\beta \\log \\frac{\\pi_\\theta(y^l|h_e)}{\\pi_{ref}(y^l|h_e)}),$                                                                                                               (8)\nwhere $y^w, y^l, y_e \\sim \\pi_\\theta(\\cdot|h_e)$ represent positive and negative output in the erroneous turn respectively."}, {"title": "B.2 ETO", "content": "Song et al. (2024) propose Exploration-Based Trajectory Optimization (ETO), which extends turn-level DPO to the session level without rigorous proof. The loss function is as follows:\n$L_{ETO} = -E_{(b, h^w, h^l) \\sim D} \\log \\sigma(\\sum_{t=0}^{T^w-1} \\beta \\log \\frac{\\pi_\\theta(y_t^w|h_t)}{\\pi_{ref}(y_t^w|h_t)} - \\sum_{t=0}^{T^l-1} \\beta \\log \\frac{\\pi_\\theta(y_t^l|h_t)}{\\pi_{ref}(y_t^l|h_t)}),$                                                                                   (9)\nwhere $h^w, h^l$ represent complete positive and negative interaction histories respectively, $T_w, T_l$ denote the number of rounds in each."}, {"title": "B.3 Discussion on DMPO", "content": "In Eq (5), the reward for the entire sequence should be calculated as the summation over all (s, a) pairs. Let's first discuss why it is valid to sum over time steps t. For LLMs, the state s can be viewed as the input context, while a represents the model's output. In multi-turn interactions, all (s, a) pairs within the sequence are unique. Thus, summing over time steps t is equivalent to summing over (s, a) pairs, making the process more straightforward. However, essentially, each (s, a) pair should be treated equally in the summation, without any inherent concept of time step t. Therefore, introducing a discount factor $\\gamma^t$ is not appropriate. The correct Eq (5) is shown as follows:\n$p(\\tau^w > \\tau^l|s_0) = \\sigma(\\sum_{t=0}^{T^w-1} r(s_t^w, a_t^w) - \\sum_{t=0}^{T^l-1} r(s_t^l, a_t^l)).$                                                                                         (10)\nMoreover, DMPO introduces regularization for rounds based on Eq (5) to eliminate Z. However, it does not discuss the impact of length normalization, and this transformation lacks rigorous theoretical justification."}, {"title": "C Data Construction Details", "content": ""}, {"title": "C.1 Statistics and Analysis of SDPO Data", "content": "SDPO dataset consists of 1019 pairs. The distribution of erroneous turns identified by GPT-40 is presented in Table 5. The distribution of segment lengths identified by GPT-4 is shown in Table 6. Additionally, the distribution of truncated turns is provided in Table 7.\nCombining Table 3 and 6, though segments of length 3 account for nearly 90% in the automatic segment length selection, the performance of the"}]}