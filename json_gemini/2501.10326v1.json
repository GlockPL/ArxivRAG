{"title": "Large language models for automated scholarly paper review: A survey", "authors": ["Zhenzhen Zhuang", "Jiandong Chen", "Hongfeng Xu", "Yuwen Jiang", "Jialiang Lin"], "abstract": "Large language models (LLMs) have significantly impacted human society, influencing various domains. Among them, academia is not simply a domain affected by LLMs, but it is also the pivotal force in the development of LLMs. In academic publications, this phenomenon is represented during the incorporation of LLMs into the peer review mechanism for reviewing manuscripts. We proposed the concept of automated scholarly paper review (ASPR) in our previous paper. As the incorporation grows, it now enters the coexistence phase of ASPR and peer review, which is described in that paper. LLMs hold transformative potential for the full-scale implementation of ASPR, but they also pose new issues and challenges that need to be addressed. In this survey paper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin with a survey to find out which LLMs are used to conduct ASPR. Then, we review what ASPR-related technological bottlenecks have been solved with the incorporation of LLM technology. After that, we move on to explore new methods, new datasets, new source code, and new online systems that come with LLMs for ASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and investigate the attitudes and reactions of publishers and academia to ASPR. Lastly, we discuss the challenges associated with the development of LLMs for ASPR. We hope this survey can serve as an inspirational reference for the researchers and promote the progress of ASPR for its actual implementation.", "sections": [{"title": "1. Introduction", "content": "In November 2022, ChatGPT\u00b9, a chatbot developed by OpenAI, was stunningly launched. This chatbot can answer almost all kinds of questions with high quality, such as integrating and summarizing literature, completing student compositions, composing musical scores, writing scripts for poems and lyrics, writing and debugging code, emulating various terminals, and even Linux systems. ChatGPT reached 100 million users in just about two months after its release, 2 Bill Gates believes that the importance of ChatGPT is comparable to the invention of the Internet and \u201cwill change our world.\u201d 3. As a sign, large language models (LLMs), the core technology behind ChatGPT, gradually came into the public view, and began to change the living habits and working styles of human society\nin just about two years. The research of natural language processing (NLP) and artificial intelligence (AI) thus ushered in important progress.\nLLMs (Zhao et al., 2023) are language models with a large number of parameters that have been trained through deep learning techniques. These models are capable of capturing and modeling the complexity and diversity of natural language. Therefore, they are able to process and generate natural language in a high-quality manner.\nLLMs are \"large\" because they have hundreds of millions or even hundreds of billions of parameters that are trained on large amounts of textual data, allowing the model to learn the complex patterns and structures of language. As the size of the models increases, so does their ability to understand and generate language.\nWith the development of technology, the ability of LLMs has evolved from the single module of processing text only to dealing with multiple modules of accepting different types of input. The latest version of OpenAI GPT-40 is a multimodal LLM that supports any combination of text, audio, and image inputs and generates any combination of text, audio, and image outputs. GPT-40 is particularly good at visual and audio understanding, allowing for real-time inference across audio, vision, and text. With this multimodal capability, the capabilities of LLMs have been greatly enhanced and their applications have been developed to a greater extent.\nResearch shows that, in the current peer review process, LLMs have already been used for generating the content of reivew report (Liang et al., 2024). In the era of LLMs, we have no choice but to face the emergence of LLMs influencing peer review. Our previous paper proposes the concept and pipeline of automated scholarly paper review (ASPR) and envisions its future (Lin et al., 2023a). At that time, ChatGPT was just released, and we clearly stated that the future directions of ASPR should \"embrace the changes brought by ChatGPT and other large language models (LLMs)\". As can be seen, since ChatGPT was released, numerous researchers have used LLMs to conduct ASPR or perform similar work. Also, there also exists some work that is not originally proposed for ASPR but can be used for ASPR. This phenomenon is exactly the coexistence phase of ASPR and peer review we presented. ASPR serves as a human assistant to write a better review report in peer review. At this phase, ASPR generation effects, related technologies, new resources, corresponding policies, ethics, etc. are further developed than before. These contents are closely concerned by the academic community, and are relevant to every researcher. It deserves to be collected, organized, and reviewed, therefore, we conduct this survey.\nIn this survey paper, we conduct a review of ASPR at the coexistence phase in the era of LLMs. The structure of our review is as follows:\n\u2022 In Section 2, we start by reviewing which LLMs researchers use to conduct ASPR.\n\u2022 In Section 3, we analyze how LLMs address the technological bottlenecks for ASPR.\n\u2022 In Section 4, we review what new methods are employed in LLMs for ASPR.\n\u2022 In Section 5 and 6, we summarize what new LLMs resources are available for ASPR.\n\u2022 In Section 7 and 8, we elaborate on where the performance and issues of LLMs for ASPR lie in.\n\u2022 In Section 9 and 10, we access how publishers and academia think about and react to LLMs for ASPR.\n\u2022 In Section 11, we conclude this survey by pointing out what the challenges are for LLMs in ASPR and where the future direction is."}, {"title": "2. Which LLMs are used", "content": "In the current development landscape, LLMs have shown diversified development trends, which are mainly di- vided into two camps: closed-source and open-source. Closed-source LLMs are represented by OpenAI's GPT4 (OpenAI, 2023), Google's Gemini series (Team et al., 2024), and Anthropic's Claude series (Claude, 2023). They are usually developed by powerful technology companies. With abundant computing resources and a large amount of high-quality data, these companies make the closed-source models excel in performance, stability, and generalizability. However, they lack transparency. Their training process and data are not open to the public, which restricts academic exploration and optimization (Zhao et al., 2023). Among the open source LLMS, OpenAI's early ChatGPT (OpenAI, 2022), Meta's LLAMA series (Touvron et al., 2023a), Mistral AI's Mistral (Jiang et al., 2023), and Alibaba Group's Qwen2 (Yang et al., 2024) have attracted much attention. They are mainly based on the Transformer architecture, which has the advantages of strong customizability and can be adjusted on demand. These models have a high level of community participation and can accelerate iterative optimization. Lower development and deployment costs reduce the threshold of using innovation and promoting the application of technology. However, these open-source models also have common shortcomings: insufficient understanding of specific domain knowledge, unstable performance on complex fuzzy tasks, and high requirements of hardware resources and technology for training and deployment (Zhao et al., 2023)."}, {"title": "3. New technologies in the era of LLMs for ASPR", "content": "In our ASPR paper (Lin et al., 2023a), we have reviewed the technologies and challenges of conducting ASPR before the area of LLMs and pointed out the direction to embrace LLMs. The latest development of LLMs has many advanced technologies that solved the previous challenges.\n3.1. Long text modeling\nScholarly papers, especially full-length research articles, usually have a certain length. These lengths vary from different disciplines and requirements of journals. Generally from 4 pages (conference papers, around 3k words) to unlimited. However, even 6k is out of the processing ability of the conventional method and the staring ear of pre-trained language models. As a result, special methods are adopted to deal with this situation, like extracted-then-generate method (Yuan et al., 2022) and modular guided method (Lin et al., 2023b). While, these methods still have the deficiency of processing and dealing with the whole manuscript in a global aspect.\nThis situation changed in the era of LLMs in June 2023. The gpt-3.5-turbo-16k model shows his ability to process the input of 16k at the beginning. Just after a short while, the accepted input length has increased to 32k, then 64k. This ability is now enough for most of the scholarly papers. With this ability, LLMs solve this issue of input of processing the whole scholarly paper in a time. This capability essentially lays the technical foundation for the development and further processing of the various steps of ASPR.\n3.2. Multi-module input\nA scholarly paper consists of multiple elements, and text is only its main presenting form. In most of the papers we collected and reviewed of ASPR, they are only focusing on text. However, figures are important parts of papers. An excellent figure can provide straightforward and illustrative information on the amount of text. With the advance of LLMs, they have been evoluted from single-module into multi-module (Yin et al., 2023), from processing text only to having the ability to process image, audio, and even video.\nWith the augmentation of this multimodal capability, LLMs truly have the same capabilities as human reviewers to review the whole content of the paper including text, tables, figures, and its related additional resources such as demonstration audio, demonstration video outer websites, and corresponding source code. This capability allows LLMs to have full access to every detail of a manuscript's content as input to perform more accurate evaluations.\n3.3. Multi-round conversation\nIn the traditional peer review process, reviewing a paper is a process of a multi-round pipeline. Authors, reviewers, and editors undergo a multi-round discussion to ensure and improve the quality of the manuscript in order to publish. In our previous paper, we define ASPR as a one round process since at that time, multi-round, or in other words, long text multi-round conversation in natural language process research is not mature enough.\nIn this area of LLMs, this situation has changed and evolved. Long text multi-round conversation is a main advance of LLMs. The ability of multi-round conversation contributed by sending the previous inputs (prompts) into the next conversation. This is also made possible by its advanced long text processing capabilities. With the ability, LLM is able to engage in multiple rounds of conversation to simulate the full review process, including\nauthor responses and interactions between reviewers (Tan et al., 2024). Therefore, evaluating the author responses, checking their revision, and providing further feedback is possible.\n3.4. Instance knowledge acquisition\nIn the peer review process, there is a situation that a reviewer encounters descriptions and clarifications out of his knowledge scope. This may happen if these descriptions and clarifications are based on the latest updated news or research advanced. Or it does not fall into the research interest of the reviewers. When this happens, the reviewer would search, check, and confirm what they are not sure. In the early stage of LLMs, the knowdgebase is frozen at the training time. For example, the knowledge of GPT-40 is updated to October 2023.4 It is so called knowledge cutoff date. LLM providers tend to continue to update the models. However, news appears every new day and the research is developing. Even if the LLMs can be updated every week, they cannot get up with the last situation. Moreover, training an LLM requires a large amount of resources and cost. It is not unrealistic to update the model in a short period.\nTo overcome this handel, LLM providers integrated functions with search engines, such as Kimi5 and ChatGPT6. As the model's knowledge is outdated or insufficient, it can perform real-time searching on the Internet to produce a more accurate answer. This feature allows LLMs to keep up with the latest scientific research and to evaluate the ideas and originality of papers more comprehensively."}, {"title": "4. New methods for generating review reports by LLMs", "content": "Review reports are the core output of both peer review and ASPR, in the era of LLMs, researchers mainly use three types of methods to generate review reports.\n4.1. Prompt engineering\nPrompt engineering refers to the process of designing and optimizing textual prompts that are input to LLMs. These prompts are intended to guide LLMs to produce output that meets expectations and is of high quality (Marvin et al., 2024; Sahoo et al., 2024). Researchers optimize prompts based on specific requirements, guiding LLMs to generate more accurate, comprehensive, and format-compliant review reports.\nThelwall (2024) uses the customized function of OpenAI to design a self-defined ChatGPT-4 based on the UK Research Excellence Framework (REF) 20217. He applies it to scoring for his own 51 papers and compares with his own quality judgments. Du et al. (2024) use ICLR review guidelines, randomly chosen accepted and rejected papers review written by human, and an ICLR 2024 review format template as a long prompt for LLMs generation. Liu & Shah (2024) design prompts for LLMs to identify errors in papers, verify checklist questions, and choose the \"better\" paper in the designed compared pairs. Zhou et al. (2024a) summarized the review criteria of top journals in the field of cell biology: originality, accuracy, conceptual advance, timeliness, and significance. Then, they prompt\nthe LLM to evaluate a given Pubmed ID paper with the above aspects in a three-star scoring system, and finally provide an overall evaluation. Zhou et al. (2024b) evaluate the ability of LLMs in predicting score and generation review comments. They also construct a review-revision multiple-choice questions (RR-MCQ) dataset to check the LLMs' performance in answering multiple-choice questions about paper.\n4.2. Supervised fine-tuning\nSupervised fine-tuning refers to further training based on a pre-trained model by providing labeled data to make the model perform better on a specific task or domain (Han et al., 2024; Wang et al., 2024). Researchers offer collected and constructed new datasets and use these datasets for fine-tuned LLMs for generating review reports. Tan et al. (2024) introduce ReviewMT, a dataset including thorough annotations for each turn during the peer review process. They propose a method to convert the peer review process into a multi-turn dialogue involving different LLMs acting as roles of authors, reviewers, and decision-makers. By fine-tuning LLMs in these roles, they propose metrics for assessing the text quality, score, and decision, which provides a new perspective for evaluating the performance of LLMs in peer review. Gao et al. (2024) develop a two-stage review generation system named REVIEWER2. REVIEWER2 utilizes two fine-tuned language models: the first analyzes papers and generates prompts for aspects of a review that should be interested in; the second generates specific reviews based on these prompts. In addition, they propose two new evaluation metrics for assessing the specificity and coverage of the generated reviews. Faizullah et al. (2024) construct a dataset LimGen containing 4,068 research papers and their corresponding limitations. They use this dataset to fine-tune LLMs with Non-truncated, Dense Passage Retrieval, and Chain Modeling method. The proposed method is effective in generating limitations of a given paper by considering the whole paper context.\n4.3. Multi-instances frameworks\nPrompt engineering and supervised fine-tuning are relatively direct methods for review report generation, and all the above methods use one single LLM instance. Some researchers are not satisfied with the performance of single LLM instance generation. They combine multiple LLM instances and design special strategies to let them work together for generation.\nTyser et al. (2024) develops AI-Driven Review Systems by proposing two main methods. One is to combine the fix questions and adaptive LLM-generated questions to prompt LLMs for reviewing. This approach is more customized and enables raising more relevant questions to be asked for particular papers. Another is to let LLMs act as program chair, senior area chair, area chair, reviewers, and authors. Through simulating the real conference paper submission to increase the quality of review report. Chamoun et al. (2024) propose a system called SWIFT that aims to provide specific, actionable, and coherent comments by leveraging multiple LLMs to identify weaknesses or suggest changes in scientific papers. The system consists of four components: a planner, an investigator, a reviewer, and a controller. SWIFT is innovative in that it breaks down the process of generating scientific feedback into a series of steps, and introduces a plan rescheduling methodology to optimize the generation of coherence, structure, and specificity of the plan, thus improving the effectiveness of the scientific feedback generation process. Yu et al. (2024) present SEA, a framework consisting of Standardization, Evaluation, and Analysis modules. Standardization module leverages the\nability of GPT-4 to consolidate multiple review comments into a uniformly formatted review content. Evaluation module fine-tunes the standardized data to generate high-quality structure review. Analysis module introduces a new assessment metric, the mismatch score, to evaluate the consistency between the content of the paper and the review comments. The integration of the three modules creates a systematic review process that can effectively deliver useful and specific comments. D'Arcy et al. (2024) create MARG, shorted for multi-agent review generation, a method that utilizes multiple LLM instances, called agents, for internal discussions to generate review on scientific papers. Agents are of three types: leader agents (coordinating tasks and communications), worker agents (receiving parts of a paper), and expert agents (specializing in assisting in the generation of specific types of comments). The method, through its multi-agent architecture, context management strategies, and refinement stages, can generate more specific, useful, and accurate reviews of scientific papers. Taechoyotin et al. (2024) develops MAMORX, a multi-agent multi-modal scientific review generation system. MAMORX uses a multi-agent architecture, where each agent focuses on a different aspect of the paper, such as novelty assessment, figure critic assessment, clarity assessment, etc. The system can handle not only textual information, but also multi-modal inputs such as figures, tables, and citations. It also enables to obtain the latest research advances and domain knowledge by accessing Semantic Scholar, in order to improve the quality and accuracy of review comments."}, {"title": "5. New datasets", "content": "Datasets play an integral role in the ASPR process. As mentioned above, they play a key role in the supervised fine-tuning process and are used to assess the performance of tasks. The quality of the datasets largely determines the accuracy of the review results and the quality of the review reports. We have previously provided a comprehensive review of datasets for ASPR (Lin et al., 2023a). With the advent of LLMs, many researchers have constructed their own datasets for various ASPR tasks. At the same time, there have also been a number of newly constructed datasets designed to provide a baseline for tasks based on traditional methods."}, {"title": "6. Code and online tools using LLMS", "content": "Source code is critical for researchers and application developers because it allows them to reproduce the methods and results in AI papers (Lin et al., 2022). This is especially important for ASPR. First, unlike theoretical research, ASPR research relies on experiments, which cannot be reproduced without the support of source code. Second, when building a complete ASPR process, existing source code can significantly reduce the amount of replication. Although some studies have not yet opened source code, they provide online systems that allow us to experience the latest ASPR technology. For example, Tyser et al. (2024) have developed three online review systems designed to provide feedback on academic papers, perform trend analysis, and improve the quality of reviews."}, {"title": "7. LLMs performance to conduce ASPR", "content": "LLMs have shown their power in text generation. Researchers have evaluated LLMs' performance in ASPR, and they found that LLMs have done well in the following aspects.\n7.1. Improving review efficiency\nLLMs can speed up the peer review process by aiding in screening, offering rapid response to manuscript suitability, and identifying potential issues or ethics concerns. This expedites the whole publication schedule (Biswas et al., 2023). In this coexistence phase of ASPR, it also largely reduces the workload of human reviewers. The experiment of Robertson (2023) shows that LLMs generated review comment share the same helpfulness rating of human reviewers, which is 3 out of 5. This experiment confirms that LLMs are useful and helpful in conducting ASPR in the view of authors. The development of LLMs can contribute to making the reviewing process more efficient and robust, saving researchers' precious time, and increasing robustness and satisfaction (Kuznetsov et al., 2024).\n7.2. Generating high-quality structured comment\nSummarization is the first part of a review report (Lin et al., 2023a). LLMs have the ability to generate high quality summary text. It is worth mentioning that its generated summaries are not directly copied from the reviewing manuscript, which demonstrates its strong power (Du et al., 2024). This ability is also examined by Thelwall (2024). By this setting, LLMs can produce a \"quality evaluation rationales\" in compliance with REF criteria.\n7.3. Validating given checklists\nLLMs can verify manuscripts with given checklists, to ensure the submitted manuscripts fulfill the requirements of special conferences and journals. The experiment shows that LLMs achieve 86.6% accuracy of the checklist checking test, reaching the human level. Moreover, 50% of the LLMs errors are caused by the unavailable ability of the LLMs at the experiment time, for example, checking figure information to the single model LLMs at that time.\n7.4. Checking technical errors\nLLMs are able to check the correctness of the paper and identify errors and loopholes, including mathematical and conceptual errors, which helps to improve the quality of the paper. The experiment of Liu & Shah (2024) shows that LLMs identified deliberately inserted errors in 7 of the 13 short papers. This is comparable to the performance of human experts."}, {"title": "8. Main existing issues", "content": "In the peer review process, reviewers are expected to provide objective and constructive comments. Even though LLMs take forward the advancement of ASPR in a large step, in the current stage, it still have several issues to take the role of human reviewers. In this section, we review and present the demerits that using LLMs for ASPR, and analyze its internal mechanism.\n8.1. Lack of professional knowledge and profound understanding\nReviewing a paper is a complex process, it requires thorough knowledge and profound experience in the field of a given paper. Justification in research is a process of \"truth-conducive\". Verification of results and interpretation of data are two essential parts. LLMs still cannot reach the human level in these two parts (Takagi, 2023). LLMs have technical limitations in understanding highly specialized terminology or the latest research advances. LLMs lack actual understanding of domain-specific expertise and capacity to contextualize feedback in the nuanced framework of a particular field of study (Biswas, 2024). LLMs are more likely to offer short sentences and general words, suggesting that its evaluation is only at the stage of imitatively evaluating the characteristics of the text, and that it is not yet capable of substantively evaluating the core elements of the paper, such as originality and soundness (Zhou et al., 2024a). Moreover, LLMs tend to suggest out-of-scope comments since their reviews are not so paper-specific and are superficial to avoid technical errors (Du et al., 2024).\nThe current mechanisms of LLMs output are predicting missing words based on billions or even trillions of parameters deep neural networks. These networks are trained by unsupervised machine learning methods of enormous amounts of text from many different sources. Most of them are open-domain web resources, and the academic contents only account for a tiny portion (Brown et al., 2020). This leads directly to the inability of LLMs to evaluate highly sophisticated cutting-edge topics and niche subjects in depth and detail. At the same time, humans are endowed with intuition, perception, empathy, deduction, self-awareness, and social awareness. These abilities enable human reviewers to gain an insight into conflicting theories and findings, inherent and problematic assumptions, and lasting but not outdated contributions. Unfortunately, LLMs lack these abilities (Weber, 2024). These factors result in the failure of LLMs to perform profound understanding.\n8.2. Bias review\nIt is the pursuit of peer review to give an objective and fair evaluation of the reviewed text merely on the manuscript quality. However, due to the internal generation mechanisms, the output of LLMs is subject to many factors such user prompts, model training data (Tan et al., 2024). This leads to the instability of its generation results.\nLLMs have a tendency to rate most papers higher scores (Zhou et al., 2024a; Du et al., 2024). The research from Thelwall (2024) found that LLMs never give the lowest score for the reviewing papers, and most of the time allocated the middle scores. Additionally, OpenAI acknowledged that its LLM may generate biased content (OpenAI, 2023). These biases, relating to class, race, geography, etc., which are born with LLMs might be reproduced or even amplified (Hosseini & Horbach, 2023).\nThese biases are all inherited from the training data of LLMs (Leung et al., 2023). Whether admitted or not, most of the data LLMs used for training come from people with biases that may not be aware of them. These biases include preferring positive results, being more tolerant or intolerant of authors with specific demographic profiles, or from certain institutions and countries (Hosseini & Horbach, 2023). Moreover, minority groups or options may have little or no included in training data in the LLMs. As with low-quality manuscripts and their corresponding review comments, leads to bias when LLMs deal with these rarely seen or unseen content.\n8.3. Inaccuracies and erroneous comments\nIdentifying potential problems accurately in the manuscripts and giving insightful comments properly to the authors is expected of all the reviewers, and at least, reviewers are supposed not to make incorrect comments. Many researchers have evaluated LLMs for ASPR and pointed out that LLMs may generate inaccuracy or even error comments. LLMs may \u201challucinate\u201d (Huang et al., 2024) inexplicably as their fabricate information with supporting references to generate convicting comments even when inaccurate (Spitale et al., 2023; Tyser et al., 2024), and they may judge low-quality manuscript a plausible veneer of rigor and decency (Nath et al., 2024). LLMs frequently accept the author's claims without checking them thoroughly, and view indentify the manuscript strengths as a text summarization task of the manuscript itself (Du et al., 2024). When LLMs are subjected to reviewing the same identical papers for multi times, they show instability and inconsistency. This is demonstrated by the fact that different scores, and different expressions are generated in different rounds for the same paper (Thelwall, 2024). Also, research shows that LLMs fail to evaluate the writing level (Du et al., 2024), and are incompetent in scoring manuscript accuracy (Zhou et al., 2024b).\n8.4. Privacy and data security risks\nIn the conventional peer review process, the manuscript is strictly confidentiality. Editors and reviewers have the responsibility to protect the author's unpublished manuscripts from being disclosed. While there are occasional extremes (Laine, 2017), this has basically become the academic consensus. When using the LLMs for ASPR, most of the early concerns involved confidentiality (Conroy, 2023). LLMs may unintentionally incorporate the content of manuscripts into their training data. Because LLMs may store input text, using the LLM to review unpublished manuscripts may raise concerns about the confidential issues (Hosseini & Horbach, 2023).\nThis is due to the deployment and interaction methods of LLMs. Training and deploying a LLM require large resources, the common method to use LLMs is to use it by a LLM provider through the Internet. Manuscript must be uploaded to the LLMs provider servers to conduct ASPR. In this process, all the uploaded content and the user prompts may be used to improve the LLMs themselves.8\n8.5. Weak customization capability\nBased on the tastes of the editorial board and the long-established style of publishing papers, different journals have their own review criteria. ASPR is intended to review a paper \"for a specific publication venue\", and meet their \"aims and scopes, review standards, and publication requirement\" (Lin et al., 2023a). Notably, in the current stage,"}, {"title": "9. Policy of reviewing by using AIGC tools from publishers", "content": "The development of LLMs made the coexistence stage of ASPR be possible. Many reviewers have used LLM to generate review reports for themselves (Liang et al., 2024). Main steam publishers have realized this situation and published corresponding policies."}, {"title": "10. Suggestions from academia", "content": "In contrast to the cautious attitude of the publishing community, the academic community has been relatively tolerant of the use of LLMs for ASPR. Besides sharing the same concerns about data security, the academic community in general had an open mind and offered some suggestions for usage and development.\n10.1. Data security\nAcademia considers data security the same importance as the industries. Hosseini & Horbach (2023) claims that materials with sensitive information or protected data should not be sent as input to LLMs before necessary actions to deal with data security are taken. One possible and actionable solution to handle the data security issues is to deploy privately hosted LLMs (Conroy, 2023). By using the privately hosted LLMs, all the data including these confidence ones are produced with the users' own service. They will not submit the published LLMs service provider and protect the data.\nThis solution has some issues. It requires technical experts in LLMs and powerful hardware resources to support the deployment. Additionally, the performance of the open-source LLMs, in most of the time, is not as good as the close-source ones.\n10.2. Undergoing training\nTraining in a standardized system is generally considered a necessary measure when confronted with new matters. Mehta et al. (2024) suggest that before integrating LLMs into the peer review process, researchers ought to first receive ethics training and be aware of the possible limitations and biases of the novel technology. Hosseini & Horbach (2023) share the similar opinions. They claim that as the critical part of researcher training, the content of peer review training must include the potential advantages and disadvantages of using LLMs for review. This kind of training helps researchers use LLMs in a responsible way and makes them conscious of possible weaknesses and biases of LLMs. They also recommend that, in such a fast technology development day, the training should update frequently to catch up with the LLMs' evolution.\n10.3. Declaration of usage and transparency\nUnlike publisher's attitude of forbidding the use of LLMs in the actual peer review process, academia shows a relatively tolerant attitude and accepts the use of LLMs for reviewing in certain conditions. Drori & Te'eni (2024) consider that when reviewers use LLMs in their reviewing, self-declaration or watermark produced by the machine should be explicitly made known. This measure is intended to increase transparency in the use of LLMs. Hosseini & Horbach (2023) suggested that besides revealing the use of LLMs, reviewers should also be responsible for the originality, accuracy, and tone of the presentation in their review reports. At the same time, editors are also supposed to disclose the use of LLMs in the whole process when they deal with the manuscripts. Mehta et al. (2024) recommend that reviewers and editors should share the experience and outcome of using LLMs in order to contribute to collective learning. Kuznetsov et al. (2024) claim that to improve the transparency in LLMs assisted review, explicitly clarifying which types of tools are used in a given review activity and publishing thorough information of the models used and model cards is an actionable policy.\n10.4. Suggested usage\nResearchers have gained a lot of different experiences and techniques when working with LLMs.\nLLMs are developing at a quick pace, a same LLM providers might provide a different version of LLMs at the same time. These different versions are various in their performance and cost of use. Using the last and updated version is beneficial to the integration of LLMs and ASPR (Saad et al., 2024). At the same time, there are many tasks in ASPR. Some simple tasks only require simple models. To explore an effective way to switch faster and cost fewer models for processing the simpler task, is a valuable research direction (D'Arcy et al., 2024).\nTo get more accurate performance, prompt and get the outputs from LLMs for many rounds and make the averaging scores evaluated to be more effective than a single round (Thelwall, 2024). To reduce errors, it is better to divide manuscripts into smaller and manageable parts before sending to LLMs (Mehta et al., 2024). Researchers are also encouraged to continue on keeping experimenting with LLMs and sharing their experiences and results to overcome the uncertainties with regard to the competencies, limitations, and internal workings of the LLM (Hosseini & Horbach, 2023). Last but not least, researchers are better not relying on LLMs as the only review means (Mehta et al., 2024), to prevent the case of a technical breakdown (Drori & Te'eni, 2024).\n10.5. Aligned with the values and goals of academic research\nAlthough different human reviewers might have different opinions and comments on the same manuscript, academics share common values and objectives to promote the development of the human world and create a better life in the future. Robertson (2023) suggested that additional research is required to ensure that the LLMs used in the peer review process are consistent with our values and goals for scholarly research. Takagi (2023) recommend that when using LLM for reviewing manuscripts, it should be ensured that their value judgments are aligned with the assessments of human researchers to make sure that review comments are consistent with human perspectives and values, but not the standpoint and aspects form the LLMs. Drori & Te'eni (2024) consider that LLMs should be monitored and reported on for compliance with the journal's guidelines for conduct, including following review forms and rules of editorial and professional organizations."}, {"title": "11. Conclusion and future directions", "content": "In this survey paper, we review and synthesize existing literature to provide an overview of technologies, methods, resources, policies, responses, and suggestions related to ASPR within the scope of LLMs. Initially, when LLMs first came into existence, they were met with skepticism and resistance from prestigious institutions and universities, as their utility and ethical implications were questioned. Over time, however, these models demonstrated their potential to revolutionize various tasks, ultimately gaining acceptance and becoming integral to modern AI systems. 9 A similar trajectory is now being observed with ASPR. The idea of automating this traditionally human-led process initially is facing resistance as it enters the coexistence phase with peer review. Despite this, advancements in LLMs have begun to show that ASPR can be achieved in full sense, and we are confident that in the future, ASPR will be recognized by the publishers with an open mind and become a standard tool in academic publication. Ultimately, the objective is not about replacing one with the other, but to enhance the overall process and foster greater efficiency and quality in scholarly evaluation.\nBased on the review presented, we highlight several open challenges as follows to warrant further exploration over this topic.\nCorrecting hallucinations. The problem of hallucinations (Huang et al., 2024) has existed since the inception of LLMs and is rooted in their complex generation mechanisms. For paper reviewing, the accuracy and clarity of the text are required to eliminate the illusion and ensure that the content is authentic and reliable. Currently, although some researchers have devised"}]}