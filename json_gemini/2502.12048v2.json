{"title": "A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond", "authors": ["Shreya Shukla", "Jose Torres", "Abhijit Mishra", "Jacek Gwizdka", "Shounak Roychowdhury"], "abstract": "Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs. Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEG-based generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based language models and contrastive learning methods. Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction.", "sections": [{"title": "1 Introduction & Motivation", "content": "The convergence of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) is transforming human-computer interaction by enabling direct brain-to-device communication. These advancements have enabled applications in assistive communication for individuals with disabilities, cognitive neuroscience, mental health assessment, augmented reality (AR)/virtual reality (VR), and neural art generation. Electroencephalography (EEG), a widely used non-invasive neural recording technique, enables both passive and active Brain-Computer Interfaces (BCIS) and holds potential for applications in real-time adaptive human-computer interaction (Zander et al., 2010; Wolpaw and Boulay, 2010). Recent advancements in deep learning and generative models have significantly improved the decoding of EEG signals, enabling the translation of neural activity into text, images, and speech. Specifically, Generative AI, including Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and Transformers (Vaswani et al., 2017), has significantly advanced brain decoding, facilitating visual reconstruction, language generation, and speech synthesis (Bai et al., 2023; Srivastava and Shinde, 2020; Lee et al., 2023b). GANs improve cross-subject classification and EEG data augmentation (Song et al., 2021), while Transformer-based architectures and multimodal deep learning frameworks (Liu et al., 2024a; Wang and Ji, 2022) enhance EEG-to-text translation and semantic decoding (Ali et al., 2024), pushing the boundaries of brain-signal interpretation.\nIn light of recent breakthroughs in Generative Al, this survey provides a scope review of recent advancements in EEG-based generative AI, with a focus on two primary directions. The first explores how brain signals can be used to generate or reconstruct visual stimuli, utilizing models such as GANs and Diffusion Models to decode perceptual representations. The second investigates the application of deep learning for EEG-to-text translation, where recurrent neural network and Transformers (Vaswani et al., 2017) based language models, and contrastive learning techniques play a crucial role in learning linguistic representation. The survey also examines emerging trends in speech decoding from EEG signals and multimodal integration considerations surrounding the use of generative AI for brain signal interpretation. Through this, we hope to provide a structured understanding of"}, {"title": "2 Related Work", "content": "Several surveys have explored EEG-based brain-computer interfaces (BCIs) and deep learning techniques for neural decoding. (Chen et al., 2022) provides a broad overview of EEG-based BCI applications, discussing signal processing methodologies and traditional machine learning techniques but lacks an in-depth analysis of generative models. (Gong et al., 2021) and (Weng et al., 2024) review deep learning techniques, particularly CNNs and RNNs, for EEG classification, emotion recognition, and cognitive state monitoring, yet they focus more on supervised learning approaches rather than self-supervised and generative modeling. More recently, (Murad and Rahimi, 2024) examined EEG-to-text decoding but primarily covered classification-based approaches without exploring the role of large language models (LLMs) or multimodal integration with vision-based EEG applications. Additionally, (Sun and Mou, 2023) discusses self-supervised learning (SSL) techniques for EEG but does not emphasize their applicability to generative tasks, such as EEG-to-image or EEG-to-text generation. Our survey specifically examines the intersection of EEG and generative AI, focusing on recent ap-"}, {"title": "3 EEG-to-Image Generation", "content": "This section explores regenerating images from visually evoked brain signals via EEG. It covers use cases, concerns addressed, techniques employed, and EEG feature encoding methods for image generation used by surveyed studies."}, {"title": "3.1 Use Cases and Addressed Concerns", "content": "Surveyed studies address key challenges like low signal-to-noise ratio of EEG signals (Bai et al., 2023; Lan et al., 2023; Zeng et al., 2023a), limited information and individual differences in EEG signals (Bai et al., 2023), lower performance on natural object images compared to digits and characters (Mishra et al., 2023) and small dataset sizes (Singh et al., 2023). Additionally, some efforts explore alternatives to supervised learning (Li et al., 2020; Song et al., 2023), since it demands large amount of data. Song et al. (2023) addresses concerns regarding convolution layers applied separately along temporal and spatial dimensions, which disrupts the correlation between channels and hinders the spatial properties of brain activity. Overall, these approaches aim to enhance the training, performance, and interpretation of brain data (Li et al., 2024).\nKavasidis et al. (2017), Song et al. (2023) and Mishra et al. (2023) extract class-specific EEG encodings that contain discriminative information to improve image generation quality, while (Nemrodov et al., 2018) focus on utilizing spatiotemporal EEG information to determine the neural correlates of facial identity representations and (Khaleghi et al., 2022) map EEG signals to visual saliency maps corresponding to each image. Other Com-"}, {"title": "3.2 Techniques Used Across Studies", "content": "Various computer vision generative models are employed to reconstruct images from EEG signals. These include Variational Autoencoders (Kavasidis et al., 2017; Wakita et al., 2021), Generative Adversarial Networks (GANs) (Kavasidis et al., 2017; Khaleghi et al., 2022; Mishra et al., 2023; Singh et al., 2024; Li et al., 2024), and conditional GANS (Singh et al., 2023; Ahmadieh et al., 2024). Diffusion models, including prior diffusion models that refine EEG embeddings into image priors (Shimizu and Srinivasan, 2022), as well as pre-trained diffusion models such as Stable Diffusion (Bai et al., 2023), are also commonly used. Additionally, diffusion modules based on U-net architecture have been used in (Zeng et al., 2023a; Lan et al., 2023) to further enhance EEG-to-image reconstruction.\nContrastive learning is another popular approach to align multimodal embeddings, employed in studies (Singh et al., 2023; Lan et al., 2023; Song et al., 2023; Sugimoto et al., 2024) to obtain dis-"}, {"title": "3.3 EEG Feature Encoding Techniques", "content": "In EEG-to-image reconstruction, the process typically begins with an encoder identifying the latent feature space of EEG signals, followed by a decoder that converts these features into an image. Long Short-Term Memory (LSTM)-based architectures are widely used due to their effectiveness in capturing temporal dependencies in EEG signals. Kavasidis et al. (2017) employs an LSTM network to generate a compact and class-discriminative feature vector, which is also used for object recognition. Similarly, (Singh et al., 2023) integrates LSTM with a triplet-loss-based contrastive learning approach to enhance feature discrimination. Singh et al. (2024) extends this approach by incorporating both CNN and LSTM architectures trained under EEG label supervision with triplet loss, further improving discriminative feature learning. Additionally, (Ahmadieh et al., 2024) uses LSTM to extract EEG features across two dimensions (EEG channels and signal duration) and enhances feature generation through various regression methods, including polynomial regression, neural network regression, and type-1 and type-2 fuzzy regression.\nSeveral studies also leverage convolutional architectures to capture spatial dependencies in EEG."}, {"title": "3.4 Evaluation Metrics", "content": "EEG-to-image generation often begins with object classification to ensure extracted EEG features contain useful class-discriminative information. Metrics like top-k accuracy are commonly used (Shimizu and Srinivasan, 2022; Lan et al., 2023; Song et al., 2023), along with qualitative visual analysis and quantitative evaluations. Key quantitative metrics include Inception Score (IS) (Salimans et al., 2016), used by (Kavasidis et al., 2017; Li et al., 2020; Bai et al., 2023; Singh et al., 2023) which measures the quality of images, Frechet Inception Distance (FID) for measuring realism (Bai et al., 2023; Singh et al., 2024; Ahmadieh et al., 2024), and saliency metrics such as Structural Similarity Index (SSIM) for assessing perceptual fidelity (Khaleghi et al., 2022; Shimizu and Srinivasan, 2022; Bai et al., 2023; Ahmadieh et al., 2024; Sugimoto et al., 2024). Other useful metrics are PixCorr (Pixel-wise Correlation) (Shimizu and Srinivasan, 2022), Kernel Inception Distance (KID) (Singh et al., 2024), LPIPS (Learned Perceptual Image Patch Similarity) (Bai et al., 2023) and Diversity"}, {"title": "4 EEG-to-Text Generation", "content": "This section discusses how AI learns brain signal representations from EEG data and maps them to linguistic representations, with an overview depicted in Figure 3. We survey use cases, techniques, concerns, and EEG feature encoding methods for text generation."}, {"title": "4.1 Use Cases and Addressed Concerns", "content": "The studies referenced in this section share a common use case: generating text from EEG signals. Several studies (Biswal et al., 2019; Srivastava and Shinde, 2020; Yang et al., 2023; Rathod et al., 2024) use the closed vocabulary approach, relying on a fixed set of pre-defined words for EEG-based decoding. Among these, Srivastava and Shinde (2020); Yang et al. (2023) investigate text generation using morse code representation of EEG signals, where users' active intent is captured, mapped to morse codes, and then translated to text format. Recent studies (Wang and Ji, 2022; Feng et al., 2023; Duan et al., 2023; Liu et al., 2024a; Wang et al., 2024; Amrani et al., 2024; Tao et al., 2024; Mishra et al., 2024; Ikegawa et al., 2024; Chen et al., 2025) overcome closed-vocabulary limitations by exploring open-vocabulary text generation to emulate naturalistic conversations. These studies also address the impact of subjectivity in subject-dependent EEG representation (Feng et al., 2023; Amrani et al., 2024), learn cross-modal representation (Wang et al., 2024; Tao et al., 2024), and capture long-term dependencies in text and also global contextual information from EEG data that transformers might miss (Rathod et al., 2024; Chen et al., 2025).\nA significant challenge is the reliance on eye-tracking fixation data as a marker for word-level EEG, which studies like (Duan et al., 2023; Liu"}, {"title": "4.2 Techniques Used Across Studies", "content": "A noteworthy aspect of these studies is the utilization of Large Language Models (LLMs), particularly BART. Several works (Wang and Ji, 2022; Liu et al., 2024a; Wang et al., 2024; Amrani et al., 2024; Tao et al., 2024; Chen et al., 2025) have used BART for text generation. In a study by Mishra et al. (2024), LLMs were fine-tuned on EEG embeddings, image and text data in the training stage to generate text from just EEG signals during inference.\nContrastive learning is widely used in studies like (Feng et al., 2023; Tao et al., 2024; Wang et al., 2024) to identify positive EEG-text pairs (e.g., EEG data from the same sentence across subjects) and negative pairs (e.g., EEG data from different sentences or subjects), improving the model's ability to align EEG signals with corresponding text representations. Another key technique is masked signal modeling, employed by Liu et al. (2024a), where a transformer model is pre-trained to reconstruct randomly masked EEG signals from raw data, enabling the model to learn context, relationships, and semantics within sentence-level EEG signals. An integrated approach by Tao et al. (2024) combines contrastive learning with masked signal modeling, where word-level EEG feature sequences are randomly masked and sentence-level sequences deliberately masked, guided by an intra-modality self-reconstruction objective.\nIn addition to these techniques, bi-directional Gated Recurrent Units (GRUs) are used to dynamically handle the varying lengths of word-level raw EEG signals (Amrani et al., 2024). Hierarchical GRUs further improve EEG data processing by capturing both long-range dependencies and local contextual information through the organization of hidden layers hierarchically (Chen et al., 2025). A unique approach by (Rathod et al., 2024) employs"}, {"title": "4.3 EEG Feature Encoding Techniques", "content": "For text generation tasks, EEG signals are encoded into features to capture temporal patterns and semantic information. In the study by Biswal et al. (2019), which focuses on generating medical reports, EEG signals are encoded using stacked CNNs to capture shift-invariant features and RCNNs to capture temporal patterns. These features are then used to generate key phenotypes, which hierarchical LSTMs utilize to produce detailed explanations. Srivastava and Shinde (2020) employs an ensemble model to extract EEG embeddings, using CNNs to capture spatial variations and LSTMs to model temporal sequences and long-range dependencies. Another study by Yu-Hao Chen et al. (2025), proposes two objectives: classification and sequence-to-sequence (seq2seq) text generation, employing residual blocks for feature extraction in both tasks to capture both spatial and temporal features of the EEG signals effectively.\nOther studies explore the extraction of spectral and statistical features alongside temporal or spatial patterns. Yang et al. (2023), aiming to translate active intention into text using Morse code, employed Short-Term Fourier Transform (STFT) to extract spectral features and concatenated these with statistical features (e.g., min, max etc. for each channel), in addition to using 1D CNN for spatial features and RNN for temporal features. Rathod et al. (2024), another closed-vocabulary solution, used features such as Wavelet Transform (WT), Common Spatial Patterns (CSP), and statistical features to generate EEG feature vectors for classification.\nVarious studies have used state-of-the-art transformer architecture for encoding EEG features. (Wang and Ji, 2022) uses a multi-layer transformer encoder to obtain EEG mapping from word-level EEG sequences. Feng et al. (2023) uses a transformer-based pre-encoder to convert word-level EEG features into the Seq2Seq embedding space. Another study by Tao et al. (2024) also uses an encoder to extract EEG embeddings and store them in a cross-modal codebook alongside word embeddings obtained from a transformer-based"}, {"title": "4.4 Evaluation Metrics", "content": "In the surveyed studies, generated text is evaluated against reference text using various established metrics. The commonly used text evaluation metrics are as follows: METEOR (Banerjee and Lavie, 2005), employed by (Biswal et al., 2019; Chen et al., 2025); BLEU score (Papineni et al., 2002), utilized by (Biswal et al., 2019; Wang and Ji, 2022; Feng et al., 2023); ROUGE score (Lin, 2004), adopted by (Wang and Ji, 2022; Feng et al., 2023; Duan et al., 2023; Liu et al., 2024a; Wang et al., 2024); and BERTScore (Zhang et al., 2019), used by (Amrani et al., 2024; Mishra et al., 2024). Other metrics include Word Error Rate (WER) used by (Feng et al., 2023), Translation Error Rate (TER), and BLEURT (Sellam et al., 2020), used by (Chen et al., 2025)."}, {"title": "5 EEG-to-Sound/Speech Generation", "content": "We review studies focused on EEG-based generation of sound, speech, voice or music and cover use cases, concerns, techniques, and EEG feature encoding methods for generating Sound or Speech from from EEG."}, {"title": "5.1 Use Cases and Addressed Concerns", "content": "EEG-based generation has been explored in various fields beyond image reconstruction, particularly in audio and speech-related applications. These include speech synthesis (Krishna et al., 2021; Lee et al., 2023a), music decoding and reconstruction (Ramirez-Aristizabal and Kello, 2022; Postolache et al., 2024), emotive music generation (Jiang et al., 2024), voice reconstruction (Lee et al., 2023b), talking-face generation (Park et al., 2024), and speech recovery (Mizuno et al., 2024). While some studies focus on decoding audio signals for listening tasks in speech or music perception (Krishna et al., 2021; Ramirez-Aristizabal and Kello, 2022; Park et al., 2024; Mizuno et al., 2024; Postolache et al., 2024; Jiang et al., 2024), others also investigate speaking tasks and imagined speech (Krishna et al., 2021; Lee et al., 2023b,a).\nFor more naturalistic communication, Lee et al. (2023b) converts EEG signals recorded during imagined speech into the user's own voice, aiming for personalized speech synthesis. Similarly, Park et al. (2024) synthesizes speech from EEG along with generating a talking face with lip-sync. Furthermore, these studies tackle issues such as generating fragmented or abstract outputs (Park et al., 2024), challenges of synthesizing complete speech from EEG (Mizuno et al., 2024), being restricted to simpler music with limited timbres (Postolache et al., 2024), and the absence of a standardized vocabulary for aligning EEG and audio data (Jiang et al., 2024)."}, {"title": "5.2 Techniques Used Across Studies", "content": "Convolutional Neural Network (CNN)-based deep learning models have been used in studies (Krishna et al., 2021; Ramirez-Aristizabal and Kello, 2022) to generate audio waveforms from EEG input. Krishna et al. (2021) explores speech synthesis for both speaking and listening tasks, using a deep learning architecture with temporal convolution layers, 1D layer, and a time-distributed layer to generate audio waveforms directly. Similarly, Ramirez-Aristizabal and Kello (2022) reconstructs"}, {"title": "5.3 EEG Feature Encoding Techniques", "content": "For speech, voice, and music decoding or generation from EEG, EEG signals are either transformed into intermediate representations, such as mel-spectrograms, or decoded into acoustic and articulatory features(Krishna et al., 2021), or EEG temporal features (Jiang et al., 2024) are utilized. Mel-spectrograms are especially useful, as they offer a shared representational state for both neural signals and audio, enabling more efficient translation between the two modalities.\nKrishna et al. (2021) incorporates an attention model to predict articulatory features and another attention-regression model to convert these predicted features into acoustic features. Similarly, Jiang et al. (2024) extracts EEG tokens through a multi-step process which includes DBSCAN clustering algorithm to derive EEG temporal features. These features are eventually transformed EEG positional encoding EEG features using positional encoding, which are used to form EEG tokens.\nIn studies using mel-spectrograms as intermediate representations, Ramirez-Aristizabal and Kello"}, {"title": "5.4 Evaluation Metrics", "content": "EEG-to-speech generation is evaluated using quantitative and qualitative metrics, based on its time-series structure, which also enables its representation as mel-spectrograms. Mel Cepstral Distortion (MCD) and Root Mean Square Error (RMSE) measure similarity between reconstructed and original speech signals (Krishna et al., 2021; Park et al., 2024), while Structural Similarity Index (SSI) and Peak Signal-to-Noise Ratio (PSNR) assess spectrogram quality (Ramirez-Aristizabal and Kello, 2022). Linguistic accuracy is evaluated using Word Error Rate (WER), Character Error Rate (CER), and BERTScore (Mizuno et al., 2024), and perceptual quality is quantified with Frechet Audio Distance (FAD) (Postolache et al., 2024). Additional metrics include Hits@k for search relevance (Jiang et al., 2024) and Mean Opinion Score (MOS) for subjective quality assessment (Lee et al., 2023b)."}, {"title": "6 Conclusion and Future Work", "content": "With advancements in Generative AI, EEG - once primarily used for classification tasks - is now being harnessed for generation, which marks a significant step toward brain-computer interaction (BCI) applications. Given its portability and non-invasive nature, EEG has strong potential for real-time, widespread applications, particularly in assistive communication by enabling direct thought-to-speech or thought-to-text systems that enhance accessibility and human-computer interaction. However, comparing studies in this field remains challenging due to the lack of standardized benchmarks. Even when studies utilized the same datasets, the subject-dependent nature of EEG data allowed for multiple ways of splitting and processing, either"}, {"title": "Limitations", "content": "While this survey provides a comprehensive overview of EEG-based generative AI applications, certain limitations exist due to the focused scope of this work. Firstly, this survey primarily covers EEG-based Brain-Computer Interfaces (BCIs), deliberately excluding other neuroimaging techniques such as fMRI, Magnetoencephalography (MEG), and Near-Infrared Spectroscopy (NIRS). Although these modalities play a significant role in BCI research and offer complementary advantages in terms of spatial resolution and multimodal integration, their detailed discussion is beyond the scope of this work.\nSecondly, due to space constraints, in-depth discussions on the cognitive underpinnings of EEG signals - such as their biological origins, neural interpretations, and relationships with brain activity - have been omitted. Similarly, technical details regarding EEG hardware, electrode configurations, and device specifications have been largely excluded for brevity. While these aspects are crucial for practical EEG-based applications, our focus remains on the computational and generative modeling aspects of EEG data processing.\nFinally, this survey assumes a general background in EEG signal processing, and generative modeling and expects familiarity with these foundational concepts. While we provide essential explanations, a more in-depth introduction to the fundamentals of EEG and BCI technology is outside the scope of this review."}, {"title": "Ethics Statement", "content": "EEG data is inherently sensitive, as it contains neural activity patterns that can potentially reveal cognitive states and sometimes personal information. While the majority of the works covered in this survey adhere to established ethical guidelines and standards, some studies may require additional ethical justifications. We have not conducted an exhaustive review of the ethical compliance of each cited work but emphasize the importance of ethical transparency in EEG research. We do not endorse studies that raise ethical concerns or lack proper ethical oversight. Any research involving EEG data collection and analysis should rigorously follow ethical protocols, including obtaining informed consent, ensuring data anonymity, and minimizing risks to participants.\nAdditionally, we acknowledge the use of OpenAI's ChatGPT-4 system solely for enhancing writing efficiency, generating LaTeX code, and aiding in error debugging. No content related to the survey's research findings, citations, or factual discussions was autogenerated or retrieved using Generative AI-based search mechanisms. Our work remains grounded in peer-reviewed literature and ethical academic standards."}]}