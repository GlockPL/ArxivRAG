{"title": "Advancing Depth Anything Model for Unsupervised Monocular Depth Estimation in Endoscopy", "authors": ["Bojian Li", "Bo Liu", "Jinghua Yue", "Fugen Zhou"], "abstract": "Depth estimation is a cornerstone of 3D reconstruction and plays a vital role in minimally invasive endoscopic surgeries. However, most current depth estimation networks rely on traditional convolutional neural networks, which are limited in their ability to capture global information. Foundation models offer a promising avenue for enhancing depth estimation, but those currently available are primarily trained on natural images, leading to suboptimal performance when applied to endoscopic images. In this work, we introduce a novel fine-tuning strategy for the Depth Anything Model and integrate it with an intrinsic-based unsupervised monocular depth estimation framework. Our approach includes a low-rank adaptation technique based on random vectors, which improves the model's adaptability to different scales. Additionally, we propose a residual block built on depthwise separable convolution to compensate for the transformer's limited ability to capture high-frequency details, such as edges and textures. Our experimental results on the SCARED dataset show that our method achieves state-of-the-art performance while minimizing the number of trainable parameters. Applying this method in minimally invasive endoscopic surgery could significantly enhance both the precision and safety of these procedures.", "sections": [{"title": "I. INTRODUCTION", "content": "Depth estimation is a crucial component of augmented reality navigation systems in minimally invasive endoscopic surgery, where accurate 3D depth information is essential for enhancing surgical precision and safety [1], [2]. Given the difficulties in obtaining ground truth from endoscopic images and the fact that monocular endoscopes are more flexible and practical compared to stereo endoscopes, unsupervised monocular depth estimation (UMDE) algorithms have attracted broader research interest [3], [4]. These algorithms typically transform the depth estimation task into an image synthesis problem between adjacent viewpoints, using image synthesis error to guide network training [5]. Despite their potential, conventional depth estimation algorithms face significant challenges when applied to endoscopic images due to factors such as lighting variations, tissue deformation, and sparse textures [6].\nTo address the challenges of UMDE in endoscopy, particularly those related to lighting variations, several approaches have been proposed. For instance, AF-SfMLearner [7] designs an appearance flow extraction network for nonlinear adjustment of image lighting. IID-SfMLearner [6] combines image intrinsic decomposition with depth estimation, incorporating reflectance invariance to supervise network training. These strategies have significantly improved the performance of monocular endoscopic depth estimation. However, the depth estimation networks in these methods rely on simple convolutional neural networks, which primarily capture local information and are limited in their ability to perceive global information.\nThe emergence of foundation models has sparked a new revolution in the field of computer vision [8], [9]. These models, often based on Vision Transformers (ViT), excel at effectively extracting global image features. For dense prediction tasks like semantic segmentation and depth estimation, having a global receptive field for each pixel relative to the scene allows the network to estimate local information with greater accuracy [10]. Moreover, these models are typically trained on extremely large-scale datasets, endowing them with robust zero- and few-shot capabilities across a wide range of downstream scenarios [11].\nRecently, a foundation model for depth estimation in natural images (Depth Anything Model) has been proposed [12]. However, as shown in Fig. 1, this model exhibits significant performance degradation when directly applied to endoscopic images [13]. Furthermore, due to the scarcity of annotated endoscopic data, training a foundation model for endoscopic depth estimation from scratch is impractical. Therefore, fine-tuning the Depth Anything Model for specific endoscopic scenarios has become an effective approach to addressing this issue.\nThere are numerous strategies for fine-tuning foundation models, with Low-Rank Adaptation (LoRA) being one of the most commonly used methods [14]. However, LoRA does not always perform optimally and some variations show better performance in certain tasks. To address this, we explore a novel fine-tuning strategy aimed at enhancing performance without increasing the number of parameters in LoRA. Inspired by recent studies that highlight the surprising effectiveness of models using random weights and projections [15], [16], we propose a low-rank adaptation method based on random vectors (RVLORA) that enhances model performance without adding additional trainable parameters. Additionally, transformers tend to focus more on"}, {"title": "II. RELATED WORKS", "content": "UMDE has been effectively explored on natural images. Zhou et al. [4] were the first to propose using view synthesis for UMDE. Subsequently, Godard et al. [5] further enhanced the performance by utilizing per-pixel loss minimization and automatic masking. Zheng et al. [19] improved depth estimation in nighttime scenarios by jointly learning image enhancement and depth estimation.\nDespite the advances these methods have made for natural images, most of them rely on the assumption of photometric consistency, which renders them less effective in endoscopic scenarios with varying illumination. To address this, Ozyoruk et al. [20] and Shao et al. [7] first adjusted the lighting of images before performing depth estimation. Li et al. [6] improved the results by employing intrinsic decomposition to separate illumination variations and using reflectance consistency. Besides this line of research, Yang et al. [21] proposed a lightweight model combining CNNs and transformers, but this came at the cost of depth estimation accuracy. Cui et al. [13] combined foundation model techniques for depth estimation, achieving relatively better results."}, {"title": "B. Low-Rank Adaptation", "content": "LORA [14] marked a significant breakthrough in efficiently training large language models for specific tasks by decomposing matrices into two low-rank matrices, A and B, thereby drastically reducing training resource consumption. Based on this idea, several improvements have been proposed recently. For example, LoRA+ [22] further enhanced efficiency by introducing different learning rates for matrices A and B. Zhang et al. [23] introduced AdaLoRA, which selects different ranks for different adapters. This method effectively enhances LoRA's learning capability and training stability. VeRA [15] reduced the parameter count even further by freezing matrices A and B and introducing additional vectors for training, though this led to a slight loss in accuracy. Different fine-tuning strategies exhibit varying performance across different tasks. Therefore, we need to explore a fine-tuning strategy that is suitable for depth estimation tasks."}, {"title": "III. METHOD", "content": "In this section, we first review the framework of image intrinsic-based unsupervised monocular depth estimation, which we have integrated with the Depth Anything Model. Following that, we provide a detailed explanation of our proposed fine-tuning strategy. Fig. 2 illustrates our network framework."}, {"title": "A. Image Intrinsic-Based UMDE", "content": "In classic UMDE framework [4], taking one frame as the target image $I_t$ and adjacent frames as source images $I_s$."}, {"title": "B. Advancing Depth Anything Model for UMDE", "content": "The depth estimation networks typically employed in classic UMDE methods are standard convolutional neural networks, which lack sufficient global perception capabilities for accurately capturing depth information. Fine-tuning a foundation model is a straightforward approach to enhance long-range dependencies without significantly increasing the number of trainable parameters. Although the Depth Anything Model [12] has been proposed for depth estimation, it was trained on natural images, and applying it directly to endoscopic images leads to poor performance. To address this, we propose a novel fine-tuning strategy specifically tailored for endoscopic images. Our approach introduces two key components: low-rank adaptation based on random vectors and residual blocks utilizing depthwise separable convolution."}, {"title": "1) Low-rank Adaptation Based on Random Vectors (RVLORA)", "content": "As models grow larger, full fine-tuning becomes increasingly impractical. Neural networks consist of many dense layers that perform matrix multiplications, and the weight matrices in these layers are typically full-rank. However, when adapting to specific tasks, pre-trained foundation models exhibit low \"intrinsic dimensions\" [24]. This means they can still learn effectively even when their parameters are projected into smaller, random subspaces. Inspired by this, LoRA was proposed, assuming that weight updates during adaptation also have low \"intrinsic rank\" [14]. For a pre-trained weight matrix $W_o \\in \\mathbb{R}^{m \\times n}$, its update can be represented using low-rank decomposition as $W_o + AW = W_o + BA$, and the modified forward pass is:\n$h = W_o x + AWx = W_o x + BAx$                                                                (2)\nwhere $B \\in \\mathbb{R}^{m \\times r}$ and $A \\in \\mathbb{R}^{r \\times n}$, with rank $r \\ll \\min(m, n)$. During training, $W_o$ is frozen and does not receive gradient updates, while A and B are trainable. In this way, the pre-trained model weights are frozen, and the trainable low-rank decomposition matrices are injected into each layer of the transformer architecture, significantly reducing the number of trainable parameters for downstream tasks.\nRecent studies have shown that models utilizing random weights and predictions have remarkable effectiveness, and introducing random vectors can significantly enhance the model's adaptive capability [15], [16]. Furthermore, various deep learning techniques rely on affine transformations of learned features. Freezing the randomly initialized parameters can enhance the network's ability to capture these affine transformations [25]. Additionally, in monocular depth estimation tasks, an inherent issue is scale ambiguity. Introducing random vectors for scale adjustment can enhance the model's ability to adapt to varying scales. Inspired by this, we propose a new parameter-efficient fine-tuning method: low-rank adaptation based on random vectors (RVLORA). Fig. 3 illustrates the detailed structure of RVLORA. This method builds on LoRA by adding randomly initialized"}, {"title": "2) Residual Block Based on Depthwise Separable Convolution (Res-DSC)", "content": "Transformers offer advantages like dynamic attention, global context awareness, and strong generalization capabilities. However, they often struggle to extract fine details and local features. On the other hand, convolutions excel at capturing local features and come with inherent benefits such as shift, scale, and distortion invariance, though they are limited in their ability to perceive global context. Therefore, combining CNNs with Transformers creates a complementary effect, enhancing overall model performance. Inspired by this, we designed a residual block based on depthwise separable convolution (Res-DSC), as illustrated in Fig. 4. This structure consists of three convolutional layers: the first layer is a 1\u00d71 convolution to reduce the number of channels; the second layer is a 3\u00d73 depthwise separable convolution, which has fewer parameters, faster computation, and a more streamlined model compared to traditional convolution; the final layer is a 1\u00d71 convolution to restore the number of channels. To reduce the model's parameters while ensuring the accuracy of depth estimation, we only add Res-DSC after the 3rd, 6th, 9th, and 12th transformer blocks. In Fig. 2, only the Res-DSC blocks after the 9th and 12th transformer blocks are shown, while the others are omitted for brevity."}, {"title": "3) Decoder:", "content": "The decoder part adopts the same structure as Depth Anything Model. The features from the last four layers of the encoder are fed into the decoder. Through the frozen Reassemble & Fusion module in the decoder, features at different resolutions are obtained and then aggregated. These aggregated features are then fed into the trainable Depth-Head module for the final dense prediction, resulting in depth estimation outputs at four scales."}, {"title": "C. Total Training Framework", "content": "We integrated our depth estimation network within the IID-SfMLearner framework [6]. In this framework, the intrinsic decomposition module performs intrinsic decomposition of the images, using decomposing-synthesis loss $L_{ds}$ and albedo loss $L_a$ to ensure the accuracy of the decomposition results and reflectance consistency between adjacent frames. The synthesis reconstruction module flexibly adjusts the illumination map and utilizes the decomposition results of the source image to reconstruct the target image, introducing mapping-synthesis loss $L_{ms}$ to supervise network training. Besides, we apply an edge-aware depth smooth loss $L_{es}$ [27] to enhance smoothness in non-edge regions while preserving sharp edges and details:\n$L_{es}(D_t, I_t) = |\\partial_x D_t e^{-\\partial_x I_t}| + |\\partial_y D_t e^{-\\partial_y I_t}|$                                               (4)\nwhere $D_t$ and $dI_t$ are the first derivative of the depth map $D_t$ and the target image $I_t$. The final loss function is defined as:\n$loss = \\lambda_{ds} L_{ds} + \\lambda_a L_a + \\lambda_{ms} L_{ms} + \\lambda_{es} L_{es}$                                            (5)\nwhere $\\lambda_{ds}$, $\\lambda_a$, $\\lambda_{ms}$ and $\\lambda_{es}$ are weighting factors."}, {"title": "IV. EXPERIMENT", "content": "All models are implemented using PyTorch [28] and trained end-to-end using the Adam optimizer [29] with $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$. We train for 30 epochs on a GeForce RTX 3090 GPU with a batch size of 8. The initial learning rate is set to $1 \\times 10^{-4}$ and is multiplied by a scale factor of 0.1 after 10 epochs. In our experiments, the rank for RVLORA is set to 4, the loss weights $\\lambda_{ds}$, $\\lambda_a$, $\\lambda_{ms}$, and $\\lambda_{es}$ are set to 0.2, 0.2, 1, and 0.01, respectively."}, {"title": "B. Dataset and Metrics", "content": "We conducted our experiments on the SCARED dataset, which was collected using the da Vinci Xi endoscope during the dissection of fresh pig abdomens [30]. High-quality depth ground truth was obtained using structured light encoding during the acquisition process. Following the splitting strategy from prior works [6], [7], we divided the SCARED dataset into 15,351 frames for training, 1,705 frames for validation, and 551 frames for testing.\nWe adopted commonly used evaluation metrics such as Abs Rel, Sq Rel, RMSE, RMSE log, and Threshold $\\delta$, with their calculation methods shown in Table I. Similar to previous methods [6], [7], during the evaluation, we scale the predicted depth map via the median scaling, which can be expressed as:\n$D_{scaled} = d * (median(d^*)/median(d))$                                                                                   (6)\nwhere d represents the predicted depth, and $d^*$ represents the ground truth. The upper limit for scaling the depth map is set to 150 millimeters, which effectively covers almost all pixels in the SCARED dataset."}, {"title": "C. Comparison Experiments", "content": "We compared our method with seven unsupervised depth estimation methods: SfmLearner [4], Monodepth2 [5], Endo-SfmLearner [20], AF-SfmLearner [7], IID-SfmLearner [6], Depth Anything [12], and EndoDAC [13]. Among these, EndoDAC is currently the state-of-the-art method on the SCARED dataset. The first three methods are reproduced from scratch following the code provided by the authors, and the last four methods are tested using the optimal model provided by the authors. Table II presents the results of the comparative experiments.\nSfMLearner and Monodepth2, which are designed for natural scenes, perform poorly on endoscopic images. While Endo-SfMLearner, AF-SfMLearner, and IID-SfMLearner have introduced algorithmic improvements tailored to the unique characteristics of endoscopic images, their performance gains remain limited. Depth Anything Model, the latest foundation model for depth estimation, also falls short due to the absence of medical images in its training datasets. EndoDAC, which similarly fine-tunes the Depth Anything Model, has achieved significant improvements over earlier methods. In comparison, our method outperforms all others across various metrics. Notably, our approach has the fewest trainable parameters, totaling only 1.38M, which accounts for just 1.4% of the overall parameters.\nAdditionally, we compared our method with other fine-tuning strategies, and the results are presented in Table III. It is evident that our approach provides the optimal fine-tuning for the depth estimation foundation model."}, {"title": "D. Ablation Study", "content": "To further demonstrate the effectiveness of the proposed modules, we conducted ablation experiments, as shown in Table IV. The first row in the table represents the results obtained by directly testing the Depth Anything Model without any fine-tuning. The subsequent rows show the results of progressively adding the proposed two modules. The experimental results indicate that both modules effectively improve the depth estimation performance.\nIn our method, the Res-DSC modules are evenly distributed across the 12 transformer modules of the encoder. For instance, when using three Res-DSC modules, they are placed at the 4th, 8th, and 12th layers. Therefore, to determine the optimal number of Res-DSC modules, we conducted ablation experiments. The results, shown in Table V, indicate that using 4 Res-DSC modules achieves the best performance while maintaining a relatively low parameter count."}, {"title": "E. Pose Estimation", "content": "In unsupervised monocular depth estimation, camera pose estimation and depth estimation are jointly learned tasks, where the accuracy of the pose network's estimations can also indirectly reflect the precision of the depth estimation. Therefore, we also evaluated the performance of the pose network. We assessed two video sequences using the Absolute Trajectory Error (ATE) metric, comparing our results only with the most competitive method, EndoDAC [13]. The quantitative results are presented in Table VI, where our method outperforms the compared method across the two sequences. The visual results, shown in Fig. 6, demonstrate reduced trajectory drift and a closer alignment with the ground truth for our method. This further validates the effectiveness of our depth estimation approach."}, {"title": "V. CONCLUSIONS", "content": "In this work, we apply foundation models to the task of endoscopic image depth estimation. Addressing the challenge of limited medical image data and the inability to train foundation models from scratch, we propose a low-rank adaptation module based on random vectors to fine-tune existing depth estimation foundation model. To overcome the issue of transformers' weak perception of local information such as edges and textures, we introduce a residual block based on depthwise separable convolution to enhance the network's capability to capture local features. Comparative experiments show that our method achieves optimal performance on the SCARED dataset with the minimum number of trainable parameters. Ablation studies validate the effectiveness of the proposed modules, and the favorable results from the pose estimation experiments further reflect the accuracy of our approach. Integrating this method into augmented reality navigation systems for minimally invasive endoscopic surgery could significantly enhance the safety and precision of surgical procedures."}]}