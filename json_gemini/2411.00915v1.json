{"title": "V-LoRA: An Efficient and Flexible System Boosts Vision Applications with LoRA LMM", "authors": ["Liang Mi", "Weijun Wang", "Wenming Tu", "Qingfeng He", "Rui Kong", "Xinyu Fang", "Yazhu Dong", "Yikang Zhang", "Yuanchun Li", "Meng Li", "Haipeng Dai", "Guihai Chen", "Yunxin Liu"], "abstract": "Large Multimodal Models (LMMs) have shown significant progress in various complex vision tasks with the solid linguistic and reasoning capacity inherited from large language models (LMMs). Low-rank adaptation (LoRA) offers a promising method to integrate external knowledge into LMMs, compensating for their limitations on domain-specific tasks. However, the existing LoRA model serving is excessively computationally expensive and causes extremely high latency. In this paper, we present an end-to-end solution that empowers diverse vision tasks and enriches vision applications with LoRA LMMs. Our system, V-LoRA, enables accurate and efficient vision tasks by 1) an accuracy-aware LoRA adapter generation approach that generates LoRA adapters rich in domain-specific knowledge to meet application-specific accuracy requirements, 2) an adaptive-tiling LoRA adapters batching operator that efficiently computes concurrent heterogeneous LoRA adapters, and 3) a flexible LoRA adapter orchestration mechanism that manages application requests and LoRA adapters to achieve the lowest average response latency. We prototype V-LoRA on five popular vision tasks on three LMMs. Experiment results reveal that V-LoRA improves 24-62% of the accuracy compared to the original LMMs and reduces 20-89% of the latency compared to the state-of-the-art LoRA model serving systems.", "sections": [{"title": "1 Introduction", "content": "Encouraged by the success of LLMs in NLP applications [7, 8, 10, 13, 56], Large Multimodal Models (LMMs) [11, 12, 98] have attracted great attention from both academia and industry. They enhance LLMs by perceiving and interpreting multimodal signals (e.g., visual inputs [19, 60, 73]) and well accomplish many complex multi-modal tasks that prior models cannot. For example, GPT-40 [12] achieves leading accuracy on many multimodal tasks such as visual question answering [37]. Yet when applied to practical applications requiring domain-specific knowledge, LMMs often show suboptimal performance, similar to the early LLMs that experienced hallucinations [95].\nLow-rank adaptation (LoRA) [30, 40] provides a promising way to integrate the external knowledge into LMM. It fine-tunes a small portion of model parameters, known as LoRA adapters, on domain-specific datasets to learn target knowledge, and freezes the base model to preserve its original capability (more in \u00a72). LLMs often leverage retrieval-augmented generation (RAG) [52] to meet this goal. Unlike LoRA, which modifies model parameters, RAG appends the retrieved knowledge (e.g., documents) onto requests (i.e., input data) for accurate response. However, this data-augmented method is not appropriate for time-sensitive vision applications. Its retrieval process and appended long-context requests incur >10\u00d7 response delay [44]. Conversely, LORA merges fine-tuned adapters into LMMs at runtime and efficiently generates high-quality and consistent domain-specific responses without additional overhead.\nDespite these advantages of LoRA, it introduces complex system challenges. Recent work [25, 69, 81], focusing on system optimization for linguistic applications with LoRA LLM, has made noticeable progress. Punica [25] and S-LoRA [69] propose unmerged inference to overcome the limitation of merged inference that can merge only one adapter at once. It computes multiple LoRA adapters in parallel while batching the shared base model computation across different requests, to boost system efficiency. dLORA [81] further balances the throughput and latency by merged and unmerged inference mode switch (more in \u00a72). However, these efforts fail to meet the high efficiency and diverse requirements of vision applications (more in \u00a73.2).\nIn this paper, we answer the following research question: Can we leverage LoRA LMMs to enrich vision applications while meeting their performance requirements? We argue that yes, but need to tackle the following challenges.\nFirst, many existing small models, trained on domain-specific datasets and used in current vision applications, outperform LMMs on target tasks. To keep accuracy, their knowledge must be integrated into LMM with LoRA adapters. However, simply training one LoRA adapter for each task is uneconomical, while fusing too much external knowledge (e.g., multiple small models) into a single adapter causes inevitable accuracy degradation.\nSecond, a vision application often involves diverse external knowledge. When serving multiple vision applications,"}, {"title": "2 Background", "content": "Vision applications in today exploit AI technology to process images or videos in RGB spaces [49, 67, 93]. In video analytics, for instance, multiple DNNs that are well-trained on domain-specific datasets separately take care of one target task and together serve the application well [46, 47, 89]. However, the limited capabilities of small models hinder the development of vision applications. Current applications yet stay on the simple combination of vision tasks such as image classification [90], vehicle counting [57], and target detection [31]. With the natural language interface inherited from LLM, LMM can enrich future vision applications. For example, serving by LMM, the police officer can find the right target when only given a text-described query such as \u201cA boy wearing a red sweater lost at the corner\u201d. Therefore, this paper tries to empower vision tasks with LoRA LMM and enrich future vision applications.\nLarge multimodal models (LMMs) aim to achieve stronger general intelligence via extending LLMs with multimodal inputs. Since more than 80% of human beings' perception and activities are mediated through vision [63], it is natural to start the exploration by equipping LLMs with \u201ceyes\u201d. By introducing a visual receptor, comprised of a visual encoder (e.g., ViT [65]) and a vision-language projector (e.g., Q-former [53]), LMMs power LLMs with visual capacity. Fig. 1 illustrates the inference procedure of LMMs. Given an image input and its prompt, the visual encoder splits the image into small patches (e.g., 14\u00d714 pixels block) and extracts the visual features of each. The vision-language projector then converts patches' visual features into visual tokens and feeds into LLM [17, 28, 42, 76], together with the embedded text tokens from the prompt, to generate the answer. LLM maps the input into high-level features and predicts the probability"}, {"title": "3 Motivation and Challenges", "content": "This section explores two questions: (1) What benefits can LORA LMM bring to vision applications (\u00a73.1)? (2) What challenges must be tackled when empowering vision applications with LORA LMM (\u00a73.2)?"}, {"title": "3.1 Potential Benefits from LoRA LMM", "content": "LMMs offers state-of-the-art performance on many complex vision tasks. To demonstrate it, we take zero-shot grounding and visual question answering as examples, and conducted experiments on Aircraft [9] and VQAv2 [38] datasets with Qwen-VL-7B [20] (as the LMM), YOLO [33] and OSCAR [55] (as the baseline small models). Aircraft contains 103 remote sensing images that are not pre-trained on Qwen-VL and YOLO, being the zero-shot test; VQAv2 is the most popular visual question answering dataset which includes text and visual modalities, to test the multi-modal ability. Fig. 3 shows that, with the solid linguistic and reasoning capabilities inherited from LLMs, Qwen-VL greatly outperforms small models. It delivers 48.9% higher F1-score in zero-shot grounding than YOLO. Fig. 3(a) visualizes the results on data #38, where Qwen-VL bounds more accurate boxes than YOLO. For multi-modal tasks, Qwen-VL achieves 78.8% accuracy, being 7.5% higher than OSCAR. Fig. 3(b) exemplifies a typical vehicle counting task in video analytics applications, only Qwen-VL generates the correct answer.\nWith external knowledge from LoRA adapters, LMM attains remarkable accuracy gain on domain-specific tasks. To investigate the accuracy improvement from external knowledge, we fine-tune three LoRA adapters for image classification, object detection, and video classification, respectively, on external datasets, AID [82], Aircraft [9], and UCF101 [72]. Fig. 4 shows the results. With fine-tuned LoRA adapters, Qwen-VL receives 45.2%, 24.5%, and 62.2% accuracy gains on three domain-specific task, respectively. Note that"}, {"title": "3.2 Challenges of Empowering Vision Applications with LoRA LMM", "content": "To empower diverse vision applications, the LoRA LMM system must offer accurate and efficient responses to the vision tasks involved and meet the distinct application-specified performance requirements. To this, we face three challenges."}, {"title": "C1: Limited capacity of LoRA adapter.", "content": "Integrating external knowledge from existing small models or specific datasets into LMM is essential to generate accurate results. Parameter-efficient fine-tuning LoRA adapters show promise. However, it is challenging because the LoRA adapter has only limited capacity and varies on the vision tasks. Fig. 5 demonstrates this by fusing external knowledge from different numbers of small models on diverse tasks into a single LoRA adapter (experiment setup details in \u00a76.1). Training a separate adapter for each small model consistently achieves high accuracy but results in significant adapter capacity waste, while fusing too many small models into one adapter incurs significant accuracy degradation. For example, the LoRA adapter that fuses six image classification models in Fig. 5 retains over 95% accuracy, while fusing six video classification models decreases remarkable accuracy."}, {"title": "C2: Inefficient concurrent requests batching.", "content": "One vision application often involves multiple small models [31, 43]. Hence, serving multiple vision applications with LMM very likely leads to the simultaneous invocation of multiple heterogeneous LoRA adapters. However, current LoRA model inference systems struggle to process them efficiently, particularly under high concurrency. To demonstrate this, we measure three state-of-the-art systems. Punica [25] and S-LORA [69] customize CUDA operator, respectively, to batch heterogeneous LoRA adapters computation in unmerge mode (more details in \u00a74.3.1), while dLoRA [81] calls PyTorch operator Einsum [3]. The experimental workload randomly generates 2-4 requests ranging from 128 to 1024 length of input tokens per second, and we repeat 1,000 times to measure the latency. For fairness, all experiments run on a server device equipped with NVIDIA A100 GPU [2] via PEFT [61] framework, and use Qwen-VL-7B [20] as the base model.\nFig. 6 plots the results. The bars of dLoRA, S-LORA, and Punica denote their extra latency than the merged inference, e.g., latencydLora \u2013 latencymerge. The bar of the base model denotes its time cost under the same workload. Unmerged inference yields up to 140ms additional latency when"}, {"title": "C3: Inflexible LoRA adapters orchestration.", "content": "To cope with the distinct performance requirements specified by vision applications, an orchestration that can carefully manage LoRA adapters and flexibly schedule application requests is necessary. We believe the inference mode switch like dLoRA is promising, yet it falls significantly short of efficiency. Its mode switch yields unacceptable overhead. Fig. 7 illustrates a real scheduling state and mode switch latency for two consecutive inference slots of dLoRA. In this case, dLoRA serves 8 requests, each with an input length of 256, in a first-come-first-service manner. In the first slot, dLoRA serves requests 1-3 in merge mode using the same LoRA adapter, and the heterogeneous requests 4-7 are processed in an unmerged mode in the following slot. A mode switch delay of over 53 ms makes the last request (the dotted arrow in Fig.7) have to wait 165ms until the next inference slot begins. This significant cost stems from 1) unnecessary memory copy of LoRA matrices due to dLoRA's inefficient memory management, and 2) the substantial overhead of LoRA matrices \\(AW\\) computation, by matrix multiplication A \u00d7 B, then added (merge) or subtracted (unmerge) \\(AW\\) onto or from the base model, by invoking torch.addmm, per layer. Conceivably, if the mode switch can be reduced to <10ms (as this paper achieved in \u00a74.4.1), the average response time of Fig.7 case can save 45ms, with the last request only need to wait <80ms."}, {"title": "4 V-LoRA Design", "content": "V-LORA is an end-to-end system that empowers diverse vision tasks and enriches vision applications with LoRA LMM by addressing the above challenges. We first provide an overview of V-LoRA's operation, then describe three core techniques it leverages."}, {"title": "4.1 System Overview", "content": "V-LORA includes two phases. During the offline phase, the accuracy-aware LoRA adapter generation approach takes the external knowledge, from the existing domain-specific small models or datasets, as well as the accuracy requirements specified by vision applications (as the dotted arrows plotted in Fig.8), to generate the minimum number of LoRA adapters (\u00a74.2). The generated LoRA adapters are rich in domain-specific knowledge that can output accurate responses to tasks involved in vision applications.\nDuring the online phase, the flexible LoRA adapter orchestration ingests the requests from vision applications (as the solid arrow plotted in Fig.8), organizes them into batches, chooses the inference mode, and orchestrates their corresponding LoRA adapters, to minimize the average response latency while guaranteeing each vision application's latency constraint (\u00a74.4). Each request batch is delivered to the corresponding adapters and LMM and inferred in the chosen mode. The LoRA adapter batching and inference mode switcher are implemented with ATMM, the adaptive-tilling matrix multiplication operator (\u00a74.3), achieving high efficiency."}, {"title": "4.2 Accuracy-aware LoRA Adapter Generation", "content": "To offer accurate results on domain-specific tasks, we propose the accuracy-aware LoRA adapter generation consisting of an accuracy-aware knowledge-fusion algorithm and the vision task head."}, {"title": "4.2.1 Accuracy-aware knowledge-fusion algorithm.", "content": "To make it easy to manage at runtime, we aim to integrate external knowledge into the fewest LoRA adapters without violating the accuracy requirements of any vision tasks. To this end, the training method must account for the limited capacity of the LoRA adapter and the complex accuracy variations arising from the knowledge fusion."}, {"title": "4.2.2 Vision task head.", "content": "To reduce the inference latency, we design the vision task head. It is designed as a trainable linear layer as a part of the LoRA adapter, as shown in Fig. 11, to predict task-specific results based on the output features of LMM. Vision task heads can be flexibly customized to various vision tasks during the LoRA adapter training, e.g., action recognition head in Fig. 11, provided the fusing knowledge is from the same task type. Fig. 11 compares doing action recognition with the original language modeling (LM) head and the vision task head. By replacing LM head with the vision task head, LMM saves 4 inference rounds, around 180ms time cost. The reason to do so is that the outputs of a large portion of vision tasks are a limited discrete set of candidate options, such as the number of vehicle counts [57], classes of action recognition [90], and binary query for a specific target on image or video [86].\nWe retain the LM head for vision applications that need the natural language interface. For example, when video query applications ask for \"A boy wearing a red sweater lost at the corner\" and specify the person detection, V-LoRA will invoke the corresponding LoRA adapter containing a detection head for efficient response. By using the LoRA"}, {"title": "4.3 Adaptive-tiling LoRA Adapters Batching", "content": "Serving vision applications with LMM very likely invokes heterogeneous LoRA adapters concurrently. To compute them in high performance, we propose an Adaptive-Tiling Matrix Multiplication operator, ATMM, enabling efficient unmerged inference, inference mode switching (\u00a74.4.1), and mixture inference mode (\u00a74.4.2)."}, {"title": "4.3.1 ATMM: Adaptive-tiling matrix multiplication operator.", "content": "Directly batching heterogeneous adapters computation upon standard kernels, as batched GEMM (General Matrix Multiplication) in dLoRA [81], is feasible, but yields excessive latency and hardware underutilization. This stems from the significant padding arising from the heterogeneity of application request lengths and LoRA adapter ranks. Hence, a customized kernel is necessary.\nTo motivate our design, we first analyze two existing customized kernels from S-LORA [69] and Punica [25], respectively. S-LoRA's kernel utilizes tiling technique to avoid the significant padding. Unlike batched GEMM, which pads heterogeneous input matrices to a uniform shape, it splits them into fine-grained blocks and computes the output for each block in parallel on CUDA cores. Punica\u02bcs kernel also employs the tiling technique and further enhances efficiency by leveraging the well-developed CUTLASS [15] library and higher-performance Tensor cores [16]. However, as the motivational study in \u00a73.2, both kernels fail to achieve satisfactory efficiency. The root cause is their static tiling configurations, which are inadequate to handle diverse input shapes, resulting in underutilized computational resources.\nOur key observation is that the computational efficiency varies significantly with different tiling configurations. We conduct an experiment with two input shapes and three"}, {"title": "4.4 Flexible LoRA Adapters Orchestration", "content": "To meet distinct performance requirements of vision applications, we propose an orchestrator to schedule requests, manage LoRA adapters, and switch inference modes, to enable efficient and flexible LORA LLM inference runtime. We first implement two tools with ATMM, a swift mode switcher and a mixture inference mode, to facilitate the orchestrator."}, {"title": "4.4.1 Swift inference mode switch.", "content": "As discussed in \u00a73.2, prior systems (e.g., dLoRA [81]) introduce excessive extra latency during mode switch. To reduce these overheads, one common method is pre-computing LoRA matrices (i.e.,, matrix \\(AW\\), B \u00d7 A) for the entire base model and storing them"}, {"title": "4.4.2 Mixture inference mode.", "content": "Compared to the unmerged mode, merged inference supports only one LoRA adapter at once, which results in the starvation of requests from other vision tasks. To alleviate starvation, we propose a novel inference mode, deLoRA, to enable the simultaneous execution of merged and unmerged inference. As shown in Fig.13, LoRA\u2081 handles the requests in merged mode, while other LoRA adapters, LoRAx, process their requests in unmerged mode. To maintain the consistent results of LoRAx's request, we introduce the deLoRA branch to prevent contamination from LoRA\u2081. The weight of deLoRA is the same as that of the merged LoRA. Based on the distributive property of matrix multiplication, the correctness can be verified as follows.\n\\(output_x = input_x \\times (W_{merge} - W_{deLORA_1} + W_{LORA_x})\\)\n\\(= input_x \\times (W_{base} + W_{LORA})\\),\nin which \\(W_{merge} = W_{base} + W_{LORA_1}\\), \\(W_{LORA_1} = W_{deLORA_1}\\), and \\(W\\) means the weight of the base model, deLoRA, and"}, {"title": "4.4.3 Scheduling policy.", "content": "To minimize the average response latency and meet each request's latency constraint, our orchestrator must carefully orchestrate requests, adapters, and inference modes. Our policy follows a greedy heuristic which includes two principles. (1) Executing in merged mode whenever possible, as it produces the fastest response and without extra overhead. (2) When starvation occurs, switch to mixture first, then unmerge mode, in order of the switching cost and extra computation. Alg.1 shows the pseudo-code. To alleviate starvation, it assigns each request a credit, indicating its waiting time adds the execution time in current mode and the mode switch latency (line #2), and sets a tolerance threshold e as the mixture mode condition. When the request workload meets the criteria for switching to merge mode, the algorithm switches the mode to merge (line #5-8). When the number of starving requests exceeds 0, Alg.1 processes them with mixture mode immediately (line #9-12). When it further exceeds half the maximum batch size, it switches to unmerge mode (line #13-15)."}, {"title": "5 Implementation", "content": "We implement V-LoRA upon several tools, including Pytorch (v2.0.1) [6], Triton (v2.0.1) [74], CUTLASS (v3.5.1)[15], and VLLM (v0.3.0) [51], with ~7.1K LOC. Most of the code is implemented in Python (v3.9) except the ATMM in CUDA. We use vLLM, especially the LightLLM [1] version, to build V-LORA because of its advanced features, such as PagedAttention, iteration-level scheduling, and token-based memory management. The three key techniques in V-LoRA are implemented as follows. (1) We implement the accuracy-aware LoRA adapter generation for popular LLMs, including Qwen-VL [20] and LLaVA [59] series, based on transformers [80] and PEFT [61] library. (2) We implement ATMM using CUDA C++ based on CUTLASS. Its hash table, which stores optimal input-tiling pairs, is implemented with a 128-bit unsigned integer as a key to map the input shapes. Since CUTLASS operators cannot be dynamically compiled, we created a Python interface to bind and package the code implementation of optimal tiling configurations with Pybind11 [41] and setuptools [5]. (3) We integrate ATMM into vLLM to support unmerged inference, mixture inference, and swift inference mode switch. To manage the complicated adapters and requests in unmerged and mixture mode, we transform the LoRA type of each request into a one-hot vector and build a"}, {"title": "6 Evaluation", "content": "We evaluate V-LoRA with two vision applications involving five tasks on three LMMs. The key takeaways are:\n\u2022 V-LORA decreases 20-89% end-to-end latency compared to state-of-the-art serving systems and achieves comparable accuracy with small models on specific domains. (\u00a76.2)\n\u2022 Accuracy-aware LoRA Adapter Generation brings remarkable accuracy and throughput benefits; Adaptive-tiling LORA Adapters Batching and Flexible LoRA Adapters Orchestration boost great latency and throughput. (\u00a76.3)\n\u2022 V-LORA shows strong stability to diverse workloads and LoRA adapter numbers, as well as scalability to multiple GPU resources. (\u00a76.4)"}, {"title": "6.1 Experimental Setup", "content": "Vision applications and datasets. We select two distinct types of visual applications: visual retrieval and video analytics, to evaluate the performance of V-LORA. Visual retrieval aims to analyze images and respond to queries. It involves visual question-answering, image caption, and specific-target detection tasks when needed by queries. We evaluate visual retrieval on SharedGPT-4V [26] and RefCOCO [48, 88] datasets. Video analytics ingests and analyzes each RGB frame from the video, then outputs results of fixed vision tasks, including object detection and video understanding like prior work [78, 90]. Object detection locates and identifies objects on each video frame on YODA [83] and Cityscapes [29]. Video understanding recognizes actions on consecutive video frames on UCF101 [72]."}, {"title": "6.2 End-to-End Performance", "content": "This section reports the E2E performance of V-LORA on multiple LMMs and applications.\nSystem performance. V-LoRA achieves notably lower average token latency than dLoRA, Punica, and SLORA regardless of vision applications and LMMs. The first row in Fig. 14 shows their performance for visual retrieval on three LMMs. Across three LMMs, V-LoRA reduces 72%, 50%, and 20% average token latency, compared to dLoRA, Punica, and S-LoRA, respectively. To Punica and S-LoRA, this acceleration is obvious. They only work in unmerge mode, which ignores the merge-friendly workload pattern, e.g., 60% of requests asking for the same LoRA adapter. dLoRA, though, takes the preference of both inference modes into account, its high mode switching cost and inefficient Einsum operator in unmerged inference incurs this 20% drop; conversely, V-LORA's ATMM"}, {"title": "6.3 Comprehensive Component-wise Analysis", "content": "We provide an in-depth performance analysis of individual system components. If not mentioned, all results are tested on the Qwen-VL model with 10 requests per second."}, {"title": "6.3.1 Accuracy-aware LoRA Adapter Generation helps", "content": "V-LoRA achieve great throughput improvement while keeping high accuracy. The accuracy gain has been discussed above; we only analyze the throughput gain from the vision task head. Especially for video analytics tasks, the vision task head employed by V-LoRA significantly reduces the rounds of autoregressive decodes, thereby greatly enhancing system performance. As illustrated in Fig. 16, V-LoRA achieves a 41-63% reduction in latency compared to the original language modeling head. This gain is attributed to the video analytics head's contribution to minimizing the prompt length and requiring only one inference round. In video understanding tasks, V-LoRA equipped with the video analytics head can match the accuracy of certain small models and handle 3-4 video streams in real time."}, {"title": "6.3.2 Adaptive-tiling LoRA Adapters Batching", "content": "gives the most efficient and stable matrix multiplication by ATMM among all comparisons. As shown in Fig. 17, by testing over 100 rounds after 10 warm-ups on large amounts of diverse inputs, ATMM achieves the lowest average latency across different batch sizes, speeds up 2.7\u00d7, 2.3x, and 3.4\u00d7 of S-LoRA, Punica, and dLoRA, respectively. On stability, the statistical results plotted in Fig. 18 show that ATMM delivers the most robust performance, which reduce the latency fluctuation by 3x, 2x, and 2\u00d7 compared to S-LoRA, Punica, and dLoRA. These benefits stem from the profile-based optimal tiling search at the offline phase. When the batch size exceeds 1024, for instance, ATMM adaptively adjusts to a larger tile shape, fully utilizing hardware resources, while other operators suffer from static tiling.\nAt the decode stage with small input matrix shapes, the left part in Fig. 17, ATMM maintains high efficiency by adapting smaller tile shapes. It delivers comparable latency to S-LORA, outperforming dLoRA and Punica by 4.5\u00d7 and 2.6\u00d7. dLORA suffers from the large context-switching overhead of repeated kernel calls by Einsum, while Punica results in a"}, {"title": "6.3.3 Flexible LoRA Adapters Orchestration", "content": "dynamically selects and switches inference modes with our swift switcher and deLoRA operation offering the best service among comparisons. As shown in Fig. 19, V-LoRA outperforms merge only, unmerge only, and dLoRA by 33%, 59%, and 21% of latency under different skewness, respectively. The skewness indicates the proportion of the most required LoRA adapter. Merge only processes requests invoking the same LoRA adapter, leading to underutilized resources and small batch sizes, while unmerge only introduces significant extra computation. dLoRA shows benefits only in highly skewed workloads as the poor performance of its unmerged inference operator Einsum. Our scheduling policy performs the best because it fully utilizes low-latency merge mode, and invents mixture mode to eliminate some mode switch. deLora significantly reduces the latency compared to unmerged inference as plotted in Fig. 20. Its early execution for the starved requests saves an average of 62% computation overhead when the starved requests' number is lower than 50% of max batch size. On the other hand, the swift inference mode switcher contributes a lot, too. Supported by ATMM, it yields 1.2x and 1.4\u00d7 speed up compared to dLoRA and unmerge in Fig. 21 case that infers with two LoRA adapters."}, {"title": "6.4 Stability and Scalability", "content": "V-LORA demonstrates great stability and scalability.\nImpacts of different skewness of requests. V-LoRA achieves the best average token latency compared to other systems under diverse skewness. Fig. 22 shows that V-LORA achieves a reduction in average token latency by 76-81%, 72-83%, and 63-76% compared to dLoRA, Punica, and S-LORA under four different skewness conditions. This superiority arises from the V-LoRA's timely mode switch and proper requests and adapters orchestration. With the swift switcher and mixture mode, it responds to workload changes fast.\nImpacts of different number of LoRA adapters. V-LoRA maintains the best and most stable performance when the number of LoRA adapters increases. As shown in Fig. 23, it suffers the minimal impact, which benefits from V-LoRA's"}, {"title": "7 Related Works", "content": "Large model serving systems recently leveraged system optimization techniques to improve LLM's inference efficiency. With paged thinking, vLLM [50] proposes a PagedAttention operator cooperating with its block-based KV cache to minimize GPU memory fragmentation. From advanced batching mechanisms, Orca [87] introduces iteration-level scheduling to continuously batch requests of varying lengths, and DeepSpeed-FastGen [39] further improves it with a dynamic split-fuse strategy. With a distributed architecture, FlexGen [70] employs offloading to enhance LLM serving throughput, while Mooncack [64] features a KVCache-centric architecture separating the prefill and decoding clusters. With LLM inference characteristics, SpecInfer [62] utilizes speculative decoding to reduce latency, while SARATHI [18] schedules requests by piggybacking decodes and chunked prefills.\nS-LORA [69], Punica [25], dLoRA [81] are the only three systems that also serve multiple LoRA LLMs by batching requests destined for different adapters. Their shortcomings are deeply analyzed in this paper. An earlier study, PetS [97], also considers the scenario of serving multiple parameter-efficient DNN models, but it does not consider serving autoregressive LLMs and the unique system characteristics of LoRA adapters. Compared to them, V-LoRA provides a"}, {"title": "8 Conclusion", "content": "In this paper, we first explore the utilization of LMMs as foundation models for vision applications to achieve high serving efficiency and user-friendly nature language interface. To achieve this, we propose V-LoRA, an end-to-end system that adapts LMMs for domain-specific visual tasks with LoRA adapters and efficiently manages them at runtime to enrich vision applications. Across two typical vision applications, we show that V-LORA enables the effective utilization of a single LMM to achieve superior performance and generalization in multiple visual tasks. While V-LoRA by no means is the final answer, we hope it serves as a stepping stone towards poly-basic design for future vision applications and demonstrates the potential of adapting LMM for visual tasks."}, {"title": "A Adaptive-tiling Matrix Multiplication.", "content": "As shown in Fig. 24, ATMM adaptively divides input matrices into thread blocks tiles, warp tiles, and thread tiles in order of granularity guided by the hash table, then transfers the data and computes by invoking the corresponding executable kernel."}, {"title": "B Profile-based Optimal Tiling Search Algorithm", "content": "Algorithm 2 Tiling Search(model, hardware)\nInput: model, hardware\nOutput: Optimal configuration for each input shape\nfunction TILINGSEARCH(model, hardware)\nmin, max=GETRANGE(model, hardware)\nconfiglist=GETCONFIG(harware) Limit the input size Limit the tiling step s.t. the Reduce the search step\nfor shape in range(min, max, 32) do\nfor rank in ranklist do\nfor tbtile in configlist[0] do\nfor warptile in configlist[1] do\nPROFILE(shape, tbtile, warptile) UPDATEBESTCONFIG()\n16"}, {"title": "CPrompt Template of Vision Applications", "content": "We report the prompt template for two vision applications, visual retrieval and video analytics, as shown in Fig. 25. Visual retrieval includes three tasks: referring expression task, visual question answering, and image captioning. The black text represents the prompt, and the blue text shows the response. Video analytics involves two tasks: object detection and video understanding. Object detection uses a similar"}]}