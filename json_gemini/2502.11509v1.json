{"title": "DifCluE: Generating Counterfactual Explanations with Diffusion Autoencoders and modal clustering", "authors": ["Suparshva Jain", "Amit Sangroya", "Lovekesh Vig"], "abstract": "Generating multiple counterfactual explanations for different modes within a class presents a significant challenge, as these modes are distinct yet converge under the same classification. Diffusion probabilistic models (DPMs) have demonstrated a strong ability to capture the underlying modes of data distributions. In this paper, we harness the power of a Diffusion Autoencoder to generate multiple distinct counterfactual explanations. By clustering in the latent space, we uncover the directions corresponding to the different modes within a class, enabling the generation of diverse and meaningful counterfactuals. We introduce a novel methodology, DifCluE, which consistently identifies these modes and produces more reliable counterfactual explanations. Our experimental results demonstrate that DifCluE outperforms the current state-of-the-art in generating multiple counterfactual explanations, offering a significant advancement in model interpretability.", "sections": [{"title": "1 INTRODUCTION", "content": "Real-world classification problems often deal with situations in which the feature set can have multiple modes but are classified within a single class. For example, a skin lesion can be classified as malignant due to many different value combinations of clinical features like a asymmetry, size, uneven colour, irregular borders etc (Figure 1). In order to achieve larger trust in the outcomes of deep models, we often rely on counterfactual explanations. Generating counterfactual explanations in such a case can prove to be tricky, since the class can have multiple modes. This challenge that has received limited attention. Prior work on counterfactual generation utilizes three distinct approaches; (a) generate multiple counterfactuals based on a distinctness loss [9], (b) use semantic information gathered from an encoder to generate counterfactuals [8], and (c) use gradient information from the target class for counterfactual explanations of input instances [14].\nHowever, during the generation of counterfactuals, when there are multiple modes in a class, the generation method needs to generate counterfactual exemplars for each mode with in the class. This is important to get the coverage of all valid feature perturbations that would classify an input into a target class, and helps us to get a better understanding of the target class. Additionally, being able to generate perturbations at the conceptual feature-level, and not just at the pixel-level or at the image-attribution level, would improve counterfactual exemplars and in-turn improve the level of explainability of the counterfactual examples for the end-users.\nIn our work, we explore the ability of a Diffusion Autoencoder to come up with multiple distinct counterfactual explanations by performing clustering in latent space, when we have a class which contains more than one mode. We show that clustering in the latent space can help us to uncover the various modes within a class helping us to generate multiple distinct counterfactual explanations. Diffusion probabilistic models (DPMs) have been shown to outperform GANs in terms of the quality of the images generated. In this paper, we make following key contributions:\n(1) We propose our methodology DifCluE (Diffuse, Cluster and Explain), that generates distinct counterfactual explanations. This approach outperforms the current state of the art models for various quality parameters such as modal coverage, and quality of images generated.\n(2) Our results demonstrate superior performance in terms of mode capture and counterfactual generation when compared to prior approaches (See Figure 2).\n(3) We demonstrate that DifCluE approach which is based on a diffusion autoencoder to capture the data distribution, and clustering the resulting class embeddings leads to a better discovery of intra-class modes. Our experimental results show that after mixing different classes into a single class and employing clustering we are able to closely recover the original classes, suggesting that some degree of disentanglement is being achieved in the intra class modes."}, {"title": "2 RELATED WORK", "content": "Counterfactual explanation generation using diffusion models represents a cutting-edge approach in interpretable AI. Diffusion models, known for their ability to generate high-quality samples, are now being adapted to create counterfactuals-hypothetical scenarios that explain model decisions by altering input features. By leveraging the stochastic nature of diffusion processes, these models can produce diverse and plausible counterfactuals, enhancing the transparency and robustness of AI systems in fields such as finance, healthcare, and legal decision-making.\nDiffusion probabilistic models (DPM) have achieved superlative image generation quality [13, 26-28] and Diffusion-based image editing has drawn much attention. There are two main categories of image editing and generation. Firstly, image-guided generation works to edit an image by mixing the latent variables of DPM and the input image [6, 22, 23]. However, using images to specify the attributes for editing may cause ambiguity, as pointed out by [18]. Secondly, the classifier guided works [1, 7, 19] edit images by utilizing the gradient of an extra classifier. In our work, we build upon the second category, aiming to generate multiple counterfactual explanations with the the help of clustering and gradients of the classifier.\nBengio et al. introduced disentangled representation learning, where the target is to discover underlying explanatory factors of the observed data [3]. The disentangled representation is defined such that each dimension (or set of dimensions) of the representation correspond to an independent factor. Based on this definition, some VAE-based techniques achieve disentanglement by constraining the probabilistic distributions of representations [5, 12, 17]. In [21], authors point out the identifiable problem by proving that only these constraints are insufficient for disentanglement and that extra inductive bias is required.\nA number of work has been done in generation of counterfactual explanations. Nemirovsky et al. uses a Residual Generative Adversarial Network (RGAN) to generate counterfactuals [24]. They enhance a regular GAN output with a residual that appear like perturbations used in counterfactual search, and then use a fixed target classifier to provide the counterfactuals. Support Vector Data description based counterfactual generation is described in [4]. They use data envelopes extracted via singular value decomposition to generate counterfactuals for multi-class setting. However, different modes in a class does not seem to be addressed. In [29], authors generate class-specific attribution maps based on counterfactuals to find which region of the image are important for that classification and then use a simple logistic regression classifier to make predictions. Authors claim these methods to be inherently interpretable. In [2] authors describe NeuroView-RNN as a family of new RNN architectures that explains how each hidden state per time step contributes to the decision-making process in a quantitative manner."}, {"title": "3 METHODOLOGY", "content": "The core concept of DifCluE is its ability to identify and generate counterfactual explanations for classes with multiple modes, offering a deeper understanding of complex class structures. This approach enhances explainability by providing insights into the varied conditions that define a class. To assess its effectiveness, we aim to compare DifCluE with existing state-of-the-art models and thoroughly evaluate its performance."}, {"title": "3.1 DifCluE Approach", "content": "Our DifCluE approach is designed to generate multiple distinct counterfactual explanations, starting with a Diffusion Autoencoder that serves as the foundation. We first train an Encoder-Decoder model based on a diffusion process, which effectively captures the key features of input data in its latent space. This latent space encodes the essential characteristics of each data point, providing a rich and compact representation that is crucial for the subsequent steps in our method (See Figure 3).\nOnce the latent space is constructed, we apply clustering techniques to identify different modes within each class. These modes represent distinct subgroups or patterns within the class, reflecting the inherent diversity in the data. By clustering in the latent space, we can uncover all possible variations within a class, enabling us to better understand its internal structure. This step is crucial as it sets the stage for generating meaningful counterfactual explanations by highlighting the various directions in which data points can vary while still belonging to the same class.\nAfter identifying the modes, we proceed to generate multiple counterfactual explanations by perturbing the data points along the directions identified through clustering. We use a linear classifier in conjunction with these clusters to determine the specific directions of perturbations that can lead to different outcomes. Notably, this approach does not require explicit concept labels; we only need class labels to guide the generation of counterfactuals. This makes our method both flexible and powerful, as it can be applied in situations where detailed annotations are unavailable, yet still provides valuable insights into the decision-making process of the model.\nThe Diffusion Autoencoder effectively captures the most critical features of an image within its encodings, while a Diffusion Probabilistic Model (DPM) handles the remaining stochastic variations. These encodings have proven highly effective for classification, even with a simple linear classifier. By leveraging these encodings, we can generate multiple counterfactual explanations, which provide a more nuanced understanding of the internal variations within a class, thereby enhancing the interpretability of the model's decisions.\nGiven a set of images (I) and the corresponding labels (Y) with n distinct classes and within those classes there might be more than one mode (M), i.e. there might be various sub classes within a class as shown in Figure 1. Our method seek to generate counterfactual explanation images for each of these modes within a class. Firstly, the Diffusion Autoencoder (Enc) is trained such that $\\text{Enc}(I_i) = (Z_{\\text{sem},i}, X_{T,i})$. The encoding have two parts i) $(z_{\\text{sem}})$ which captures the semantic information from the image and ii) $x_T$ which contains low level stochastic subcode.\nSince most of the semantic information is captured by $z_{\\text{sem}}$, we utilise that to find the various modes within a class. This is done with the help of clustering such that $f: Z_{\\text{sem}} = \\{C_1, C_2, ..., C_k \\}$. These cluster labels (C) can be used for generating the directions in which the $z_{\\text{sem}}$ of an image is to be perturbed in order to generate counterfactual explanations. This can be done using a linear classifier on the semantic encoding with the cluster labels $g: Z_{\\text{sem}}\\rightarrow C$. Corresponding to each of these clusters $(C_i)$ we get direction $(w_i)$. Next, perturbations can be made to an encoding of an image $(z_{\\text{semi}})$ such that $z' = Z_{\\text{semi}} + \\alpha * w_j$, where $w_j$ are the weights of linear classifier corresponding to cluster $(C_j)$ and $\\alpha$ is an adjustable parameter. Finally, counterfactual explanations corresponding to cluster $C_j$ can be generated such that $I_{,j} = \\text{Dec}(z'_{\\text{sem},i}, X_{T,i})$."}, {"title": "3.2 Comparison between DISSECT and DifCluE", "content": "Our DifCluE approach is meticulously designed to generate a diverse set of counterfactual explanations, beginning with a Diffusion Autoencoder as its core. The process starts with training an Encoder-Decoder model grounded in diffusion processes, which excels at capturing the most salient features of input data within its latent space. This latent space acts as a compressed yet information-rich representation of each data point, preserving the essential characteristics needed for the subsequent analysis and generation of counterfactuals.\nWith the latent space established, we employ sophisticated clustering techniques to discern the various modes present within each class. These modes correspond to distinct subgroups or patterns within the class, encapsulating the inherent variability and complexity of the data. Clustering in the latent space allows us to uncover all possible variations that exist within a class, offering a deeper understanding of its internal dynamics. This step is pivotal, as it sets the foundation for generating meaningful and contextually relevant counterfactual explanations by revealing the different trajectories along which data points can be altered while maintaining their class membership.\nFollowing the identification of these modes, we generate multiple counterfactual explanations by strategically perturbing data points in the directions indicated by the clustering results. To refine this process, we integrate a linear classifier with these clusters, which helps pinpoint the specific directions of perturbation that are most likely to produce alternative outcomes. A key advantage of our method is that it does not require detailed concept labels; class labels alone suffice to drive the generation of counterfactuals. This feature makes DifCluE both adaptable and robust, enabling its application across various domains where comprehensive annotations are scarce, while still providing deep insights into the decision-making processes of the model.\nA major distinction between DISSECT and DifCluE lies in their flexibility regarding the number of counterfactual explanations generated. DISSECT requires retraining the model if the number of desired counterfactuals increases, which adds to the computational burden. In contrast, DifCluE allows for easy adjustment of the number of counterfactual explanations without necessitating any retraining of the network. This makes DifCluE a more computationally efficient and scalable option compared to the DISSECT approach."}, {"title": "3.3 Evaluation of Counterfactual Explanations", "content": "In order to assess the quality of the counterfactual explanations generated by each of these models, we used various evaluation parameters to assess the following properties [9]:\n(1) Realism: The counterfactual explanations should lie on the data manifold, often referred to as realism. Frechet Inception Distance (FID) are based on how similar the two groups are in terms of statistics on computer vision features of the raw images calculated using the Inception v3 model used for image classification.[10]\n(2) Substitutability: The representation of a sample in terms of concepts should preserve relevant information. Substitutability measures an external classifier's performance on real data when it is trained using only synthetic images.\n(3) Importance: Explanations should produce the desired outcome as we make larger and larger perturbations. As we increase the parameter for the perturbation the classification probability for the target probability should also increase.\n(4) Distinctness: Another desirable quality for explanations is to represent inputs with non-overlapping concepts, often referred to as diversity. We want to access if the produced counterfactual explanations are distinct from one another or not."}, {"title": "3.4 Evaluating the alignment of Counterfactual Explanations with actual classes", "content": "Our aim here is to objectively evaluate the quality of the counterfactual explanations generated by the two different models, as well as evaluate how much the counterfactual explanations correspond to the actual class that we mixed. In order to do so, we use a separate ResNet model trained to identify the 40 attributes of the CelebA dataset. The accuracy of the model in identifying the features is as shown in Table 1."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "We first use the FFHQ dataset to train the Diffusion Autoencoder [16]. Our aim here is to train on a large dataset. Thereafter, we use CelebA dataset to evaluate DifCluE's ability to generate multiple counterfactual explanations. CelebA dataset has labels for 40 attributes, including separate labels for Celebs with Bangs, and Celebs with blondes [20]. These labels are essential to evaluate the counterfactual explanations in a quantifiable manner.\nFor our experiments, the Bang hairstyle and Blonde hair classes were deliberately assigned the same labels. The images which were labelled in this manner, were then fed to the autoencoder to get their latent representation. K-means clustering was applied on this latent representation to obtain the two clusters. Using the linear classifier, we get the direction for our perturbations and generate counterfactual explanations by perturbing the images along these directions.\nWe first assess the quality of the counterfactual explanations generated by each of these models. We used various evaluation parameters mentioned in methodology section to assess this. From Table 2, we observe that the FID scores seem to suggest that our model out performs the DISSECT model by a clear margin. Therefore, we can say that in terms of realism, the DifCluE model seems to be generating slightly better quality images.\nTable 3 seem to suggest that the distinctness between the two counterfactual explanations is closely matched. Distinctness between the two counterfactual explanations is measured by an external CNN classifier trained on classifying if the image belongs to counterfactual explanation 1 or 2. The accuracy and precision are better in case of DISSECT model however the recalls are better in case of DifCluE. However, the results are quite close to each other suggesting the two models perform similar accuracy levels.\nTable 4 compare the substitutability for the models. Substitutability measures an external classifier's performance on real data, when it is trained using only synthetic images. Here, it can be seen that the performance of DISSECT and DifCluE model is comparable.\nImportance property is compared in Table 5. The idea here is that, as we make bigger and bigger perturbations, the classification score of the generated images must also increase for that particular class along which perturbations are made. This suggests that the results are quite similar. KL score and MSE score are both better in case of DISSECT model; however R-value and the correlation coefficient are better for DifCluE model. We can observe from Tables 2,3,4,5 that for most of the evaluation parameters, DifCluE and the DISSECT model perform quite comparably.\nNext, we also want to evaluate the disentanglement of the generated counterfactual explanations. In order to do so, firstly we train an external classifier capable of classifying the images into the 40 categories labelled in the dataset. Once we have this classifier, the generated counterfactual explanations are fed this network. If we have a negative sample, we could generate two distinct counterfactual explanations and these counterfactual explanations should be aligned with one of the classes, since the positive samples in our experiments were obtained by mixing two classes.\nTherefore with the help of our current pipeline, we should be able to disentangle the two classes that we had mixed in this experiment. Hence, it is expected that the two counterfactual explanations should be aligned with the two constituent classes. For example, if we trained a Diffusion Autoencoder on data where both 'Bangs' and 'Blondes' were classified in the same class and the rest of the samples were labeled as 'negative', so the generated counter factual explanations should be images that are 'Bangs' and 'Blondes'. Hence these images should be classified by the external classifier as such. We have here recorded the percentage of the counterfactual explanations that got the desired results. The results for this example are summarized in Table6.\nWe conducted this experiment with 4 different pairs of classes being mixed (Tables: 6,7,8,9). From these set of experiments we observe that in each case, DifCluE outperformed the DISSECT model. The counterfactual images that DISSECT generated were only being classified as either of the two constituent classes, 53 per cent of the time as compared to 83 percent of the times in case of DifCluE. Therefore, it is observed that the DifCluE approach performed much better in generating disentangled and distinct counterfactual explanations."}, {"title": "5 CONCLUSION", "content": "We have introduced a methodology for generating counterfactual exemplars for each mode (or sub-class) within a given target class, focusing on perturbations at the conceptual feature level rather than the low-level pixel level. By clustering input encodings in the latent space, our method effectively captures the distinct characteristics of various modes within a class. This information is then leveraged to generate meaningful counterfactuals by determining the specific perturbations needed to produce a counterfactual for each mode. Since these perturbations are applied at the semantic level, the resulting counterfactual exemplars offer enhanced explainability, making them more intuitive and accessible to end-users. This approach not only clarifies the conceptual feature-level changes that influence classification decisions but also enhances trust in the underlying model. In future work, we plan to extend this approach to other domains, such as time-series applications, to further explore its versatility and impact."}]}