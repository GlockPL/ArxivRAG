{"title": "Knowledge Bases in Support of Large Language Models for Processing Web News", "authors": ["Yihe Zhang", "Nabin Pakka", "Nian-feng Tzeng"], "abstract": "Large Language Models (LLMs) have received considerable interest in wide applications lately. During pre-training via massive datasets, such a model implicitly memorizes the factual knowledge of trained datasets in its hidden parameters. However, knowledge held implicitly in parameters often makes its use by downstream applications ineffective due to the lack of common-sense reasoning. In this article, we introduce a general framework that permits to build knowledge bases with an aid of LLMs, tailored for processing Web news. The framework applies a rule-based News Information Extractor (NewsIE) to news items for extracting their relational tuples, referred to as knowledge bases, which are then graph-convoluted with the implicit knowledge facts of news items obtained by LLMs, for their classification. It involves two lightweight components: 1) NewsIE: for extracting the structural information of every news item, in the form of relational tuples; 2) BERTGraph: for graph-convoluting the implicit knowledge facts with relational tuples extracted by NewsIE. We have evaluated our framework under different news-related datasets for news category classification, with promising experimental results.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models have garnered widespread enthusiasm lately as conversational agents for diverse applications. Noticeably, ChatGPT [46] takes queries and creates responses, Google's Bard [32] can produce creative replies due to its coding and multi-lingual capabilities, ChatSonic [9] generates update-to-date replies with the aid of Google Search for accurate and informative content creation, among others. Meanwhile, a simple and yet powerful language representation model, known as Bidirectional Encoder Representations from Transformers (BERT) [20], was introduced. As an LLM family member, BERT comprises a stack of transformer encoders and is pre-trained via unlabeled texts. Most LLMs are fine-turnable using small amounts of domain-specific data for improving the performance of the given domain tasks. In particular, the BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks [20], such as question answering and language inference, with little changes to the models.\nConventional LLMs usually take massive amounts of unstructured text data for pre-training, before made them available for general applications. Although a pre-trained LLM can produce replies to its inputs (such as Web news items), it does not inherently capture structural and relational information of tokens existing in its every input, presenting an opportunity for improving the quality of replies. To this end, we resort to knowledge bases (KBs), which are created by pre-processing Web news items individually to extract structural and relational information of token in each item, such as the triggers, the arguments, the temporal relations, etc. This KB creation is undertaken in the rule-based manner automatically without requiring any relation-specific training data. Pre-processing news items extracts a number of relational tuples (Arg1, Pred, Arg2) per item, realized by the News Information Extractor (NewsIE) we have developed. The KBs created out of news items make it possible to produce better replies to inputs (i.e., news items) that are fed to a given LLM for processing, by complementing the LLM outputs. This results from the fact that LLM outputs contain no structural information while the KBs hold structural and relational information of individual news items.\nIn this article, we consider the framework that lets KBs support LLMs for improving Web news processing performance. The framework consists of an LLM and a graph convolutional network (GCN) [12], which is inputted with the LLM output and KBs for graph-convolutive operations to let KBs complement the LLM output. The GCN serve to convolute relational tuples with the implicit knowledge facts obtained by the LLM, which is fine-tuned by Web news items for improving the performance of news processing. The fine-tuned LLM takes raw Web news themselves directly as its input. While any LLM may be adopted to form the framework, we use BERT [20] as an example LLM in this article, realized our BERTGraph. Without making any change to BERT for use in BERTGraph, our framework is in contrast to prior designs, like KG-BERT [59] and K-BERT [29], which change the BERT input format, and Relphormer [3], which modifies the BERT encoder structure. However, any change to the input format or the encoder structure of BERT destroys its original embedding, requiring prohibitively expensive LLM model re-training, making our BERTGraph more favorable in practice and able to fully utilize implicit knowledge learned by BERT. More details of the BERTGraph framework can"}, {"title": "2 RELATED BACKGROUND AND PRIOR WORK", "content": "Background and prior work related to our proposed BertGraph design include knowledge bases, large language models, graph convolution networks, and information extraction, as provided next in sequence."}, {"title": "2.1 Knowledge Bases", "content": "Knowledge bases (KBs) are structured repositories of information and data. Such a repository contains facts and relationships among its entities. The Resource Description Format (RDF) [36] has been considered to be the standard format for knowledge bases in Natural Language Processing (NLP). Various KBs have been created out of different data sources, and they are not all of high quality. In particular, DBpedia [27] created a multilingual knowledge base, extracted from Wikipedia. It uses the DBpedia ontology, comprising 320 classes, to store the facts in the RDF format. However, the lack of a consensus on the contributors of DBpedia poses an issue about its quality [39]. WordNet [37] is an online database, which is an effective combination of traditional lexicographic information and modern computing. Parts of Speech of English are organized into sets of synonyms, with their semantic relations linking the synonym sets. Later knowledge bases, like YAGO [14], are of higher quality due to the addition of knowledge about individuals like persons, organizations, products, etc., with their semantic relationships also in existence. YAGO claims its accuracy approaching 95%, based on empirical evaluation of fact correctness. Meanwhile, DEAP-FAKED [34] and Boshko et al. [25] use an existing knowledge base, Wikidata5m, for Named Entity Disambiguation (NED). Embedding methods, such as TransE and ComplEX, are used to obtain embeddings for news classification.\nThe proposed BERTGraph framework relies on quality knowledge bases that are generated out of Web News datasets automatically, by our devised News Information Extractor (NewsIE). Generating knowledge bases at scale without manual effort, NewsIE is detailed in Section 3.2."}, {"title": "2.2 Large Language Models", "content": "With the introduction of Transformer [50] in 2017, the NLP field has seen a rising number of efficient language models. Pre-trained Large Language Models (LLMs), like ChatGPT [46], Bard [32], ChatSonic [9], BERT [20], XLNet [58], ROBERTa [30], etc., have been used extensively for NLP and other tasks. Since pre-trained models sll contain huge amounts of textual information, they can handle a variety of tasks easily and often effectively by down-streaming the models for tasks at hand, especially after model fine-tuning via task-specific datasets.\nBidirectional Encoder Representation from Transformers (BERT) [20] has been fine-tuned for NLP tasks, such as sentiment analysis [16, 57], text classification [25, 43, 49], etc. Upon training, BERT introduces masks to input sentences and predicts masked tokens. Due to its reliance on masks, BERT neglects dependency between the masked tokens positions [58]. XLNet [58] overcomes the limitation of BERT using its autoregressive formulation. BERT is trained modestly to achieve sound results [30]. ROBERTa ehnances BERT by training with bigger batches of more data, dynamicaly changing the masking pattern, training on longer word sequences, and removing the next sentence prediction. Although pre-trained models are capable for NLP tasks, they are not always efficient for knowledge-driven tasks due to the discrepancy of fine-tuning's specific domain and pre-training's wide domains [29]."}, {"title": "2.3 Graph Convolution Networks", "content": "The generalization of convolutional neural networks (CNNs) to signals defined on more general domains was first attempted in [4], treating two onstructions of deep neural networks for processing graphic data efficiently. Later, gneralizing CNNs for use in high-dimensional irregular domains, such as graphs, was demonstrated in [12] to learn local, stationary, and compositional features on graphs. Meanwhile, the Graph Convolutional Network (GCN) was proposed for semi-supervised learning on graph-structured data, based on a variant of CNNs [23]. It scales linearly with the graph edge count and learns hidden layer representations that encode both the local graph structure and node features node, to yield superior performance. Relational Graph Convolutional Networks (R-GCNs) were introduced specifically to handle the highly multi-relational data characteristic of realistic knowledge bases, able to soundly outperform their decoder-only baselines. A comprehensive review on GCNs is made by grouping existing models into two categories, based on the types of convolutions, and also by categorizing them according to the areas of their applications [61]."}, {"title": "2.4 Information Extraction", "content": "Various information extraction (IE) mechanisms have been considered. Specifically, TextRunner [60] pursues Open Information Extraction (OIE), that makes a single, data-driven pass over the entire corpus to extracts a large set of relational tuples autonomously. It utilizes a set of patterns in order to obtain propositions but does not capture the 'context' of each clause for effective extraction. A follow-up study relies on semantic features (semantic roles) for the OIE task, demonstrating that Semantic role labeling (SRL) can be used to increase the precision and recall of OIE [8]. Separately, a greedy parser, which relies on a classifier to predict the correct"}, {"title": "3 BERTGRPAH FRAMEWORK AND KNOWLEDGE BASES", "content": "This section first overviews the proposed BertGraph framework, which comprises an LLM (e.g., BERT [20]) followed by a GCN, as shown in Fig. 2. A key component in support of BERTGraph, called News information extractor (NewsIE), is then detailed. NewsIE is responsible for extracting structural information out of every Web news item individually to constitute the quality knowledge bases of news datasets. Such a knowledge base and the BERT's output are fed to the GCN for processing."}, {"title": "3.1 BERTGraph Overview", "content": "Pre-trainned LLMs show robust performance when fine-tuned with only some domain-specific examples [5] before putting them for use. Such robust performance nature stems from models' implicit knowledge. However, materializing the implicit knowledge of a given LLM (e.g., BERT) is challenging. This work proposes to address the challenge by appending a Graph Convolution Layer to BERT, constituting BERTGraph. The Graph Convolution Layer is realized by a typical GCN, which is inputted with BERT's output plus the dataset-specific knowledge base produced automatically by our NewsIE. Our BERTGraph framework aims to answer the key question of: Can structured KBs enhance the performance of LLM's downstream tasks, given that LLM's ouputs are unstructured? To this end, we provide the model and architecture of BERTGraph, in Sec 3.1.1, followed by a light-weighted solution for dealing with the incompatibility issue of the BERT output format and the GCN input format, called the Text-to-Graph Adapter, in Sec. 3.1.2."}, {"title": "3.1.1 Model and Architecture", "content": "BERTGraph takes the BERT Base [28] as its backbone. It should be noted that all other transformer-based LLM models are also suitable for BERTGraph, making it a general design framework. BERT consists of transformer encoders that focus on understanding the meanings of its inputted texts, sufficing our purpose of news category classification. The BERT Base is a pre-trained language model that leverages 12 transformer encoder layers to capture semantic-rich information from inputted texts. Its pre-training is on a large-scale unlabeled general domain corpus. Each input (with no more than 512 words) fed to BERT is first split into a list of tokens, by a tokenizer (which comes with a pre-trained BERT model), to become compatible with the BERT architecture. A token first transfers to an input embedding by adding its 1) token embedding, 2) segment embedding, and 3) positional embedding, where the token embedding captures the semantics of individual tokens, the segment embedding distinguish tokens across different inputs, and the positional embedding indicates the positions of the tokens situated in the input. An input embedding can then be forwarded to BERT for encoding, starting from its first transformer encoder layer. The stacked transformer layers of BERT learn the relationships among input tokens and output the following: 1) hidden states, which represent the contextualized representations and attentions, and 2) Attention scores, which denote attention probabilities assigned to input tokens. Notice that BERT outputs do not contain any structural information.\nWithout making any change to BERT, BERTGraph adds structure information, derived from NewsIE, directly to the BERT output, for graph-convolutive processing. Hence, the BERTGraph architecture is in contrast to prior designs, like KG-BERT [71] and K-BERT [38], which change the BERT input format, and Relphormer [3], which modifies the BERT encoder structure. The rationale behind our architecture is as follows. First, any change to the input format or the encoder structure destroys the original embedding, requiring prohibitively expensive LLM model re-training. Second, an input format chance calls for developing a new suitable data format, possibly a daunting task even for a specific domain. Conversely, BERTGraph makes full use of implicit knowledge learned by BERT, by forwarding its outputs, together with structure information (created by NewsIE; see Sec. 3.2 below), to the appended GCN for graph-convolutive processing. For the BERT output to be compatible with the GCN input, a text-to-graph adapter is devised, as stated next."}, {"title": "3.1.2 Text-to-Graph Adapter", "content": "The Text-to-Graph (T2G) adapter indispensable for BERTGraph is shown in Fig. 3. It takes the BERT's output of a given input, plus the knowledge base produced out of the input (by NewsIE), as its input. The T2G Adapter generates Customized Pooling Layers, according to the knowledge base, by first examining those distinct nodes present therein. For each examined node, a dedicated pooling layer is generated by T2G Adapter, called the Customized Pooling Layer, which averages the token embeddings of its associated nodes, where the embeddings are from the BERT outputs, as depicted in Fig. 3. If there are a total of K distinct nodes extracted by NewsIE, T2G Adapter creates the Customized Pooling Layer K times. Each pooling output is then assigned with a new label by the T2G Adapter, starting form 0 and with an increment of K. This helps to train the appended GCN in the batch mode upon BERTGraph fine-tuning, when the GCN re-indexes knowledge bases. Finally, the pooling output and the edge list (present in knowledge bases) serve as the GCN's input."}, {"title": "3.2 News Information Extractor", "content": "As a key component that supports our BertGraph framework, the News Information Extractor (NewsIE) aims to identify the informative and structural aspects of each raw Web news item individually, to arrive at knowledge bases. NewsIE operates in a customized rule-based manner, relying on a set of rules derived from traditional NLP taggings, such as Dependency Parsing (DP) [11] and Part of Speech (POS) [40]. It automatically and efficiently identifies the informative and structural aspects of news items at scale, comprising predicate-argument structure extraction, clause type identification, and element aggregation. Specifically, the subject, predicate, object, complement and adverbial informative aspects of individual news items are identified and extracted. Next, based on the extracted informative aspects, NewsIE categories the extracted informative aspects per news item into their proper clause types. Finally, the structural aspects of each news item are obtained by applying our developed rules to its categoried clause types. Realized by using Python NLP library SpaCy [18], our NewsIE is detailed below."}, {"title": "3.2.1 Predicate-argument Structure Extraction", "content": "Web news evolve constantly, with new subjects and events emerging daily. However, there are two drawbacks if deploying LLMs directly for processing Web news. First, LLMs require substantial computational resources in their training, making it impractical to retrain them even after abundant new subjects and events have emerged. Second, LLMs lack scalability when exploring previously unknown subjects or events. For instance, the BERT model assigns an 'UNKNOWN' token to any word non-existing in its dictionary (upon prefix search failures). To tackle these drawbacks, we extract factual information from the unstructured text of each news item as pre-processing, bapplying Part-of-speech tagging (POS) to get token-level grammatical category information, e.g., \u2018NOUN' and \u2018VERB'. This factual information extraction is inspired by the Open Information Extraction (OpenIE) technique. Following the rule-based OpenIE method, clausIE [7, 13], our structure extractor employs dependency paring (DP) [17] to get token-level relationship tagging. Note that Stanford DP (StanDP) [5] may be suitable for extraction as well. Unless stated otherwise, we adopt DP as the default method. After DP, a plain news sentence is transferred to a tree-like syntactic structure, where each node is a word in the sentence, and edges or arcs between pairs of nodes represent grammatical relationships. The root node"}, {"title": "3.2.2 Clause Type Identification", "content": "After extracting five kinds of chunks, i.e., subjects ('S'), predicates ('V'), objects ('Od' for direct object, 'Oi' for indirect object, 'Op' for prepositional object), complements ('C'), and adverbials ('A'), we next classify the types of clauses via a rule-based manner, where a clause comprises multiple extracted chunks. The clause type indicates how those extracted chunks are syntactically connected among one another. Before clause type identification for a given news item, we first apply Verb-Net [45] and PropBank [22] on the news item to categorize its verbs and get the argument frames of categorized verbs. We then map the extracted chunks of the news item to the argument frames given by VerbNet and PropBank. This way can quickly identify different"}, {"title": "3.2.3 Element Aggregation", "content": "After the clause types are identified, we then aggregate the chunks with different clause types, aiming to transform structured information into knowledge bases. Each clause type can generate one (or two) tuple(s). An element is denoted by a single node in the knowledge base. Since an 'SV' clause involves only two elements, a dummy node (dented as 'DUMMY'), which contains no information, is added to it, enabling its predicate node ('V') to link with its subject node (S) and the dummy node ('DUMMY'). For the clause type of 'SVO', its predicate node ('V') is directly linked with its subject node ('S') and its object node ('O'). Likewise, for the clause type of SVC, its predicate node ('V') is directly linked with its subject node ('S') and its complement node ('C'). The clause type of 'SVA' has its predicate node ('V') linked directly with its subject node (S) and its adverbial node ('V'). The 'SVOC' clause type has its predicate node ('V') linked with its subject node (S) and its object node ('O'), with its complement node ('C') appended to the object node ('O'). Similarly, an SVOA clause type has its predicate node (V') linked with its subject node (S) and its object node ('O'), with its adverbial node ('A') appended to object node ('O'). An \u2018SVOO' clause type combines its predicate node ('V') and its direct object node ('Od') to become a new node that denotes the relationship between the subject node (S) and the indirect object node ('O\u00a1'). Hence, knowledge bases are generated automatically at scale without labeling effort."}, {"title": "4 EXPERIMENTAL EVALUATION AND RESULT DISCUSSION", "content": "This section first introduces the datasets used in our evaluation study, followed by the experimental setup for experimental evaluation. Experimental results and discussion are then provided."}, {"title": "4.1 Datasets", "content": "Three datasets N24News [54], SNOPES [47], and politifact [41] are considered for the evaluation of the proposed model. N24News is a multimodal news dataset containing both text and images from New York Times with 24 categories. The dataset is comprised of 60K image-text pairs, but this paper uses only the text part of the dataset. SNOPES is a rumor dataset consisting of a number of variety of news categories. Among the categories, only 12 categories are selected for the classification task totalling 7060 instances of news. Politifact is a fact-check dataset collected from PolitiFact website. There are 21152 instances and 6 categories of expert verified facts."}, {"title": "4.2 Metric", "content": "We use F1-score (F1), accuracy (Acc), and precision (Pre) as evaluation metrics. Precision measures the accuracy of positive inference made by the model. In classification, it is the ratio of True Positive (TP) of a class to the sum of TP and False Positive (FP) of the same class. Recall is another metric which measures the ability of a model to correctly predict the positive instances. It is the ratio of TP to the sum of TP and False Negative (FN). F1-score is used to evaluate model on imbalanced data. It is the harmonic mean of Precision and Recall which provides balance between them. Accuracy measures the number of correctly inferred instances from the whole dataset."}, {"title": "4.3 Experimental Setup", "content": "The models were trained on DELL PRECISION TOWER 7910 containing 2 NVIDIA TITAN RTX TU102 GPUs with 24GB of memory each. Batch size of 16 is used with the learning rate of 1e-5 and Adam [21] is used as optimizer. The size of the training dataset varies, ranging from 5% to 80%, with 10% for validation dataset, and the remainder for test datset. There are 4 layers of GNN with hidden layer dimension of 768. For evaluation, Accuracy is the default metric for classification tasks but due to its lack for imbalanced dataset [35], F1 score is also considered."}, {"title": "4.4 Experimental Results and Discussion", "content": "We conduct a series of experiments on the three dataset, i.e., Snopes, N24News, and Politifact to evaluate the performance of our proposed BERTGraph."}, {"title": "4.4.1 Comparing to the BERT baseline", "content": "To compare BERTGraph with BERT baseline, we employ various fine-tuning configurations. Besides BERT and BERTGraph, we explore two variations of the BERTGraph model: 1) BERTGraph-s, this variant maintains the stability of BERT without conducting any fine-tuning; 2) BERTGraph-1, this variant, fine-tuning is applied solely to the last layer of BERT. We first use 10% of the data to fine-tune BERT, BERTGraph, BERTGraph-0 and BERTGraph-1, the results are shown in Table. 1. From the table, it is evident that BERTGraph outperforms the other models on Snopes and N24News dataset across all performance"}, {"title": "4.4.2 Trianing Size impacts", "content": "A comparative study of the baseline model and BERTGraph on variable training size of datasets is shown in Fig. 5. It is clear that our proposed model performs better than the baseline on both F1 score and accuracy. But the difference is reduced, or even BERT performs better when the test and train datasets are nearly equal in proportion. However, this is rarely the case in real-world scenarios. The real-world problems have very little data to learn from. Sometimes, the model has to learn in one shot. The BERTGraph learns even from less amount of data due"}, {"title": "4.4.3 Comparison between BERT based and GCN based models", "content": "All three datasets are trained in two Graph Neural Network models, GCN [24] and GraphConv [38]. They are compared only based on the accuracy. The dataset is trained for a maximum of 50 epochs with various layers for GCNs. Only the best-performing model is used for evaluation. Table 3 clearly shows that the accuracy of the GCN model depends on the dataset. GCN has 61% accuracy on Snopes while GraphConv only has 22% accuracy. The proposed model either outperforms GCN-based models or is comparable in accuracy. Besides the Politifact dataset, BERT-based models outperform GCN-based models. The pre-trained model captures general information from the sentences by embedding. The embeddings contain grammatical and structural information, contributing to their accuracy."}, {"title": "4.4.4 Discussion", "content": "In this project, we explore a crucial question that was evolving alongside the LLMs: Can structured KBs enhance the performance of LLM's downstream tasks, given that LLM's outputs are unstructured? Our experiments, as described in Subsections 4.4.1 and Subsections 4.4.2, have yielded valuable insights. We have observed that structural information plays a pivotal role during the fine-tuning process. It can avoid the model overfitting on specific tasks. The LLMs utilizing the attention mechanism, denoted as full attention, maybe overkill and excessive, leading to an overemphasis on input noise, which hurts the overall quality of the representations. Structure information constrains the LLMs to fine-tune according to specific passes, encouraging the model to ignore the input noise. This discovery holds the potential for LLMs to be more intelligent."}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce the BertGraph frameworks and evaluate its effectiveness in downstream news-related classification tasks. The BERTGraph is an extension of BERT, taking both original text and structured information extracted by NewsIE as its inputs. Our evaluation demonstrates that, the BERTGraph outperforms its BERT Base counterpart in terms of almost all performance metrics for three news datasets available to the public, achieving up to 5% of accuracy increase on the Snopes dataset. The evaluation results underscore the importance of incorporating structural information for LLMs. Furthermore, we observe that as the training dataset size grows, Bert suffers the over-fitting problem, which is not observed in BERTGraph. Consequently, our model presents a more robust fine-tuning structure overall. Additionally, we highlight the value of LLMs as rich sources of knowledge. Our lightweight BERTGraph can easily utilize the implicit knowledge of LLMs. In the future, we will evaluate BertGraph on larger datasets for a broader range of tasks, further exploring its potentials."}]}