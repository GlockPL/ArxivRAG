{"title": "Online Decision MetaMorphFormer: A Casual Transformer-Based Reinforcement Learning Framework of Universal Embodied Intelligence", "authors": ["Luo Ji", "Runji Lin"], "abstract": "Interactive artificial intelligence in the motion control field is an interesting topic, especially when universal knowledge is adaptive to multiple tasks and universal environments. Despite there being increasing efforts in the field of Reinforcement Learning (RL) with the aid of transformers, most of them might be limited by the offline training pipeline, which prohibits exploration and generalization abilities. To address this limitation, we propose the framework of Online Decision MetaMorphFormer (ODM) which aims to achieve self-awareness, environment recognition, and action planning through a unified model architecture. Motivated by cognitive and behavioral psychology, an ODM agent is able to learn from others, recognize the world, and practice itself based on its own experience. ODM can also be applied to any arbitrary agent with a multi-joint body, located in different environments, and trained with different types of tasks using large-scale pre-trained datasets. Through the use of pre-trained datasets, ODM can quickly warm up and learn the necessary knowledge to perform the desired task, while the target environment continues to reinforce the universal policy. Extensive online experiments as well as few-shot and zero-shot environmental tests are used to verify ODM's performance and generalization ability. The results of our study contribute to the study of general artificial intelligence in embodied and cognitive fields. Code, results, and video examples can be found on the website https://rlodm.github.io/odm/.", "sections": [{"title": "1 Introduction", "content": "Research of embodied intelligence focus on the learning of control policy given the agent with some morphology (joints, limbs, motion capabilities), while it has always been a topic whether the control policy should be more general or specific. As the improvement of large-scale data technology and cloud computing ability, the idea of artificial general intelligence (AGI) has received substantial interest Reed et al. [2022]. Accordingly, a natural motivation is to develop a universal control policy for different morphological agents and easy adaptive to different scenes. It is argued that such a smart agent could be able to identify its 'active self' by recognizing the egocentric, proprioceptive perception, react with exteroceptive observations and have the perception of world forward model Hoffmann and Pfeifer [2012]. However, there is seldom such machine learning framework by so far although some previous studies have similar attempts in one or several aspects.\nReinforcement Learning(RL) learns the policy interactively based on the environment feedback therefore could be viewed as a general solution for our embodied control problem. Conventional RL could solve the single-task problem in an online paradigm, but is relatively difficult to implement and slow in practice, and lack of generalization and adaptation ability. Offline RL facilitates the"}, {"title": "2 Related Work", "content": "Classic RL: Among conventional RL methods, on-policy RL such as Proximal Policy Optimization (PPO) Schulman et al. [2017] is able to learn the policy and therefore has a good adaptive ability to environment, but is slow to convergence and might have large trajectory variations. Off-policy RL such as DQN Mnih et al. [2015] improves the sampling efficiency but still requires the data buffer updated dynamically. In contrast, offline RL Fujimoto et al. [2019], Kumar et al. [2020], Kostrikov et al. [2021] can solve the problem similar to supervised learning, but might have degraded performance because of the distribution shift between offline dataset and online environment. In our work, we aim to reach state-of-the-art performance for different embodied control tasks, therefore a model architecture compatible with on-policy Rl is proposed.\nTransformer-based RL: Among these efforts, Decision Transformer (DT) Chen et al. [2021] and Multi-game Decision Transformer Lee et al. [2022] embodied the continuous state and action directly and use a GPT-like casual transformer to solve the policy offline. Their action decision is conditioned on Return-to-Go (RTG), either arbitrarily set or estimated by the model, since RTG is unknown during inference. Instead, Trajectory Transformer (TT) Janner et al. [2021] discards RTG in the sequential modeling to avoid that approximation.PromptDT Xu et al. [2022] adds the task difference consideration into the model by using demonstrated trajectory as prompts. ODT Zheng et al. [2022] first attempts to solve transformer-based RL in an online manner but mainly focus on supervised on actions instead of maximizing rewards. In our work, we propose a similar model architecture that is able to conduct both offline learning and on-policy, actor-critic learning. Online learning employs PPO as the detailed policy update tool with the objective as reward maximization.\nMorphology-based RL: There are some other studies that focus on the agent's morphology informa-tion, including GNN-based RL which models agent joints as a kinematics tree graph Huang et al. [2020], Amorpheus Kurin et al. [2021] which encodes a policy modular for each body joint, and MetaMorph Gupta et al. [2022] which intuitively use a transformer to encode the body morphology as a sequence of joint properties and train it by PPO. In our work, we have the morphology-aware encoder which is similar with MetaMorph and has the same PPO update rule. However, compared with MetaMorph, we encode the morphology on not only the state but also historical actions, and consider the historical contextual consideration."}, {"title": "3 Preliminaries and Problem Setup", "content": "3.1 Reinforcement Learning\nWe formulate a typical sequential decision-making problem, in which on each time step t, an embodied agent conceives a state $s_t \\in \\mathbb{R}^{n_s}$, performs an action $a_t \\in \\mathbb{R}^{n_a}$, and receives a scalar reward $r_t \\in \\mathbb{R}^1$. Given the current state, the agent generates an action from its policy $\\pi(a_t|s_t)$ and push the environment stage forward. This interactive process yields the following episode sequence:\n$T_{0:T} = \\{s_0, a_0, r_0, s_1, a_1, r_1,\\ldots,s_T,a_T,r_T\\}$\nin which T means the episode ends or reaching the maximum time length."}, {"title": "3.2 Proximal Policy Optimization", "content": "The classical Proximal Policy Optimization (PPO) methodology Schulman et al. [2017] inherits from the famous actor-critic framework in which a critic estimates the state value function V(s), while an actor determines the policy. The critic loss is calculated from Bellman function by bootstrapping the state value function\n$L^{Critic} = E_{s\\sim d^{\\pi}} [r_t + \\gamma(V_{\\theta_{old}}(s_{t+1}) - V_{\\theta}(s_t))^2]$\nOn the other hand, generalized advantage estimation (GAE) Schulman et al. [2016] is employed to help calculate the action advantage $A_t$ by traversing the episode backward\n$A_t = \\delta_t + (\\gamma \\lambda)\\delta_{t+1} + \\cdots + (\\gamma \\lambda)^{T-t+1}\\delta_{T-1}$\n$\\delta_t = r_t + V(s_{t+1}) - V(s_t)$\nthen the actor is then learned by maximizing the surrogate objective of $A_t$ according to the policy-gradient theorem\n$L^{Actor} = CLIP(E_t \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)} A_t - \\beta KL[\\pi_{\\theta_{old}}(\\cdot|s_t), \\pi_\\theta(\\cdot|s_t)])$\nin which the CLIP function means clipping the object by $[1 - \\epsilon, 1 + \\epsilon] A_t$, and KL denotes the famous K-L divergence. A PPO policy update is then conducted by minimizing the objective upon each iteration:\n$L^{PPO} = -\\eta_A L^{Actor} + \\eta_C L^{Critic}$"}, {"title": "3.3 Problem Setup", "content": "Here we redefine the aforementioned conventional RL notations in a more 'embodied style', although still generalized enough for any arbitrary agent with a multi-joint body. Inspired by the idea of Gupta et al. [2022], we differentiate the observation into the agent's proprioceptive observations, the agent's embodied joint-wise self-perception (e.g. angular, angular velocity of each joint), as well as the exteroceptive observation, which is the agent's global sensory (e.g. position, velocity). Given a K-joint agent, we denote the proprioceptive observation by $o^{pro} \\in \\mathbb{R}^{K \\times n}$ in which each joint is embedded with n dimension observations. The exteroceptive observation is x-dimensional which results in $o^{ext} \\in \\mathbb{R}^x$ and $s := [o^{pro}, o^{ext}]$.\nStepping forward from Gupta et al. [2022], we also define the action in the joint-dependent way; that is, assuming each joint has m degree of freedom (DoF) of movements (e.g. torque), the action is reshaped as $a \\in \\mathbb{R}^{K \\times m}$. To allow the room of different agent body shapes, we introduce binary masks which have the same shapes of $o^{pro}$ and a and zero-pad the impossible observations or actions (e.g. DoF of a humanoid's forearm should be smaller than its upper-arm due to their physical connection)."}, {"title": "3.4 Attention-based Encoding", "content": "Given a stacked time sequence vector $x \\in \\mathbb{R}^{T \\times e}$ with T as the time length and e as the embedding dimension, an time sequence encoder can be expressed as\n$Enc_T(x) = Attention(Q=x, K=x,V=x) \\in \\mathbb{R}^{T \\times e}$\naccording to the self-attention mechanism Vaswani et al. [2017] with Q, K, V denoting query, key and value. Analogously, given a stacked joint sequence vector $p \\in \\mathbb{R}^{K \\times e}$ with K as the number of joints, a morphology-aware encoder instead learns the latent representation on this joint sequence\n$Enc_M(p) = Attention(Q=p, K=p, V=p) \\in \\mathbb{R}^{K \\times e}$\nBy pre-tokenizing either $o^{pro}$ or a into p, within the latent space with dimension e, their 'pose' latent variables can be encoded by $Enc_M$. For each form of encoder, timestep or joint position info is encoded by lookup embedding then adding to encoded vector. More details can be referred to the MetaMorph paper Gupta et al. [2022]."}, {"title": "4 Methodology", "content": "We have developed a specialized transformer model architecture that includes a morphology-dependent encoder and decoder to model the trajectory sequence over time. A two-stage training paradigm is proposed that involves general pre-training on diverse offline data followed by task-specific fine-tuning training."}, {"title": "4.1 Model Architecture", "content": "Our ODM model structure contains a unified backbone and task-specific modules. Within this paper's context, the task might be related to a different agent (potentially different body types), environment, and reward mechanisms. Since the original system variables might have different dimensions, task-specific modules map them into a uniform-dimensional latent space (e in Eq. 6) and reverse operations. The backbone has a two-directional, morphology-time transformer structure, including morphology-aware encoders and a casual transformer. Architecture details are specified in Fig 2.\nTokenizer At each time t, observations and actions are first embedded into the latent space\n$o_t^{pro} = Embed_o(o_t^{pro}) \\in \\mathbb{R}^{K,e}$\n$x_t^{ext} = Embed_x(o_t^{ext}) \\in \\mathbb{R}^{e}$\n$a_t = Embed_a(a_t) \\in \\mathbb{R}^{K,e}$"}, {"title": "Morphology-aware Encoder", "content": "Corresponding pose embedding vectors are obtained by traversing the agent's kinematic tree and encoding the morphology by Eq. 6:\n$o_t^{pro} = Encm(o^{pro})$\n$s_t^s = MLP([s_t, x_t]) \\in \\mathbb{R}^{e}$\n$a_t^s = Encm(a_t)$\nCasual Transformer To capture the morphology difference, we apply the prompt technique as in Xu et al. [2022], but embedding the morphology specifications instead of imitations\nPrompt = Embed(K, n, m, x)\nThe casual transformer then translates the prompt and the input sequence into the output sequence\noutput =Ency (Prompt, input)\ninput :={$BOS, s_0^s, a_0^s, s_1^s,\\ldots,a_{t-1}^s,s_t^s$}\noutput :={$s_1^a, a_1^s,\\ldots, a_{t-1}^a,s_t^a$}\nwith a forward casual time mask. Detailed structure is inherited from GPT2, a decoder-only structure as in Chen et al. [2021], Janner et al. [2021], Zheng et al. [2022]. For practical consideration, input and output sequences are truncated by a window length Tw, with a padding time mask for episodes shorter than Tw. The timestep embedding is also considered and concatenated into the latent variable.\nTo emphasize the instant impact, we further conduct multi-head attention by querying the target variable and marking the input variable as key and value:\n$a_t^s \\leftarrow a_t^s + Attention(Q=a_t^s, K=s_t^s, V=s_t^s)$\n$s_t^s \\leftarrow s_t^s + Attention(Q=s_t^s, K=a_{t-1}^s, V=a_{t-1}^s)$\nProjector The task-specific projectors map latent outputs back to the original spaces:\n$\\hat{a_t} = Proj_a(a_t^s), \\hat{s_t} = Proj_s(s_t^s), V_t = Proj_v(s_t^s)$\nEmbedo, Embedx, Embeda, Embeds, Proja, Projs, Projy are all modeled as MLPs with LayerNorm and Relu between layers. More configuration details are on the website."}, {"title": "4.2 Training paradigm", "content": "ODM has a two-phase training paradigm including pretraining and finetuning, as in Algorithm 1.\nPretraining To mimic the learning process of the human infant, we design a curriculum-based learning mechanism in which the training dataset transverses from the easiest to the most complicated one. During each epoch, the current dataset is trained in an auto-regressive manner with two loss terms:\n$L^{pretrain}_{imitation} = MSE(a_t, \\hat{a_t}), L^{pretrain}_{prediction} = MSE(s_t, \\hat{s_t})$\n$L^{pretrain} = \\eta_i L^{pretrain}_{imitation} + \\eta_p L^{pretrain}_{prediction}$\nwhere MSE denotes the mean-square-error. $L^{pretrain}_{imitation}$ corresponds to the imitation of action from demonstrations, while $L^{pretrain}_{prediction}$ encourages the agent to reconstruct observations in the existence of casual mask 2.\nFinetuning one extra predict head is activated to predict the state value $V_t$; this head as long as the very last prediction head of $\\hat{a_t}$ are employed as outputs of actor and critic:\n$V_t \\rightarrow V(s_t), \\hat{a_t} \\rightarrow \\pi(s_t)$\nActor and critic can then be trained by PPO. Keeping some extent of $L^{pretrain}$ as auxiliary loss, this finetuning becomes a self-supervised (and model-based, in the aspect of state-action jointly learning) RL\n$L^{finetune} = \\eta_1 L^{PPO} + \\eta_2 L^{pretrain}$"}, {"title": "5 Experiments", "content": "This section highlights our experiment setup and results. Further details (such as the videos) can be found on our website."}, {"title": "5.1 Configurations", "content": "Table 2 introduces the configuration details of our experiments, including the environments, agents, baselines, and demonstration pioneers. For further details of hyper-parameters, please refer to Table 8 in the appendix."}, {"title": "5.1.1 Environments & Agents", "content": "We practice with enormous agents, environments and tasks, to validate the general knowledge studied by ODM. These scenes include:\nBody shape: including swimmer (3-joints, no foot), reacher (1-joint and one-side fixed), hopper (1 foot), halfcheetah (2-foot), walker2d (2-foot), ant (4-foot), and humanoid on the gym-mujoco platform \u00b3; walker (the agent called ragdoll has a realistic humanoid body) 4 on the unity platform Juliani et al. [2018]; and finally unimal, a mujoco-based enviroment which contain 100 different morphological agents Gupta et al. [2021]."}, {"title": "5.1.2 Baselines", "content": "We compare ODM with four baselines, each representing a different learning paradigm:\nMetamorph: a morphological encoding-based online learning method to learn a universal control policy Gupta et al. [2022].\nDT: As a state-of-the-art offline learning baseline, we implement the decision transformer with the expert action inference as in Lee et al. [2022] and deal with continuous space as in Chen et al. [2021]. We name it DT in the following sections for abbreviation.\nPPO: The classical on-policy RL algorithm Schulman et al. [2017]. Code is cloned from stable-baseline3 in which PPO is in the actor-critic style.\nRandom: The random control policy by sampling each action from uniform distribution from its bounds. This indicates the performance without any prior knowledge especially for the zero-shot case."}, {"title": "5.1.3 Demonstrating pioneers", "content": "For purpose of pretraining, we collect offline data samples of hopper, halfcheetah, walker2d and ant from D4RL Fu et al. [2020], as sources of pioneer demonstrations. For these environments, D4RL provides datasets sampled from agents of different skill levels, which corresponds to different learning pioneers in our framework, including expert, medium-expert, medium, medium-replay and random. We also train some baseline expert agents and use them to sample offline datasets on walker and unimal. This dataset contains more than 25 million data samples, with detailed statistics shown on our website. Within each curriculum, we also rotate demonstrations from the above pioneers for training, as indicated in Algorithm 1."}, {"title": "5.2 Pretraining", "content": "Model is trained with datasets of hopper, halfcheetah, walker2d, ant, walker and unimal, from the easiest to the most complex. Table 3 shows statistics of all dataset used in the pretraining phase. Pretraining is computed distributed on 4 workers, each with 8 gpu, 4000 cpu and 400M memory. For both pretraining and finetuning, the ADAM optimizer is applied with the decay weight of 0.01.\nFigure 3 shows the loss plot. One can observe that the training loss successfully converges within each curriculum course; although its absolute value occasionally jumps to different levels because of the environment (and the teacher) switching. Validation set accuracy is also improved with walker and unimal as exhibition examples in Figure 3."}, {"title": "5.3 Online Experiments", "content": "To make online learning faster, we use 32 independent agents to sample the trajectory in parallel, with 1000 as the maximum episode steps. The experiment continues more than 1500 iterations after the performance converges. Figure 4 provides a quick snapshot of online performances. Compared with ODM without pretraining, returns of ODM are higher at the very beginning, indicating knowledge from pretraining is helpful. As online learning continues, the performance degrades slightly until finally grows up again, and converges faster than the other two methods, although the entire training time (pretraining plus finetuning) is longer.\nDuring online testing, 100 independent episodes are sampled and analyzed to evaluate the agent's performance. The average episode return and episode length are recorded in Table 4. One can observe that our ODM outperforms MetaMorph (in walker) or is similar to MetaMorph (in unimal). Note there are large performance deviations in walker since this environment has substantial process noise implemented to challenge the learning. It is also worth noting that DT does not work for unimal, indicating the limitation of the pure offline method with changing agent body shapes."}, {"title": "5.4 Few-shot Experiments", "content": "We examine the transfer ability of policy by providing several few-shot experiments. Pretrained ODM is loaded in several unseen tasks, which are listed in Table 5. As a few-shot test, online training only lasts for 500 steps before testing. ODM obtains the best performance except for humanoid on flat terrain, indicating ODM has better adaptation ability than MetaMorph."}, {"title": "5.5 Zero-shot Experiments", "content": "Zero-shot experiments can be conducted by inferencing the model directly without any online finetuning. The unimal environment allows such experiment in which the flat terrain (FT) can be replaced by variable terrain (VT) or obstacles. Results are shown in Table 6. It can be observed that ODM reaches state-of-the-art performance for zero-shot tests, indicating that ODM has strong generalization ability by capturing general high-level knowledge from pretraining, even without any prior experience."}, {"title": "5.6 Ablation Studies", "content": "To verify the effectiveness of each model component, we conduct the ablation tests for ODM with only the online finetuning phase (wo pretrain) and with only pretraining (wo finetune); within the pretraining scope, we further examine ODM without the curriculum mechanism (wo curriculum) and morphology prompt (wo prompt). The DT method could be viewed as the ablation of both $L_{prediction}$ and $L_{PPO}$, so we do not list the ablation results of these two loss terms. We conduct the ablation study on unimal (all 3 tasks) as well as walker, with results shown in Table 7. Results show that ODM is still the best on all these tasks, which indicating both learning from others' imitation and self-experiences are necessary for intelligence agents."}, {"title": "5.7 Typical Visualizations", "content": "Generalist learning not only aims to improve the mathematical metrics, but also the motion reason-ability from human's viewpoint. It is difficult for traditional RL to work on this issue which only solve the mathematical optimization problem. By jointly learning other agent's imitation and bridge with the agent's self-experiences, we assume ODM could obtain more universal intelligence about body control by solving many different types of problems. Here we provide some quick visualizations about generated motions of ODM, comparing with the original versions 5.\nBy examining the agent motion's rationality and smoothness, we first visualize the motions of trained models in the walker environment. Since the walker agent has a humanoid body (the 'ragdoll'), the reader could easily evaluate the motion reasonability based on real-life experiences. Figure 5 exhibits key frames of videos at the same time points. In this experiment, we force the agent to start from exactly the same state and remove the process noise. By comparing ODM (the bottom line) with PPO (the upper line), one can see the ODM behaves more like a human while PPO keeps swaying forward and backward and side to side, with unnatural movements such as lowering shoulders and twisting waist.\nWe compare the motion agility by visualizing the unimal environment, in which the agent is encour-aged to walk toward arbitrary direction. Figure 6 compares ODM with Metamorph. Metamorph wastes most of the time shaking feet, fluid, and gliding, therefore ODM walks a longer distance than Metamorph, within the same time interval 6."}, {"title": "6 Discussion", "content": "Our work can be viewed as an early attempt of an embodied intelligence generalist accommodated for varied body shapes, tasks, and environments. One shortcoming of the current approach is that ODM still has task-specific modules (tokenizers and projectors) for varied body shapes. By using"}, {"title": "7 Conclusion", "content": "In this paper, we propose a learning framework to provide a universal body control policy for arbitrary body shapes, environments, and tasks. This work is motivated by the intelligence development process in the natural world, where the agent can learn from others, reinforce with their own experiences, and utilize the world knowledge. To achieve this, we propose a two-dimensional transformer structure that encodes both the state-action morphological information and the time-sequential decision-making process. This framework is designed to be trained in a two-phase training paradigm, where the agent first learns from offline demonstrations of experts on different skill levels and tasks, then interacts with its own environment and reinforces its own skill through on-policy reinforcement learning. By pretraining the demonstration datasets, ODM can quickly warm up and learn the necessary knowledge"}, {"title": "A Additional Methodology Details", "content": "During the RL study, the agent generate its action from its policy $\\pi(a_t|s_t)$ and push the environment stage forward. This interactive process yields the following episode sequence:\n$T_{0:T} = \\{s_0, a_0, r_0, s_1, A_1, r_1,\\ldots,s_t,a_t,r_t\\}$\nin which T means the episode ends or reaching the maximum time length. RL then solves the sequential decision making problem by finding $\\pi$ such that max $E_{\\tau\\sim\\pi}(R)$ in which the episode return defined as\n$R:= \\sum_{t=0}^{T}\\gamma r_t$\nwith $\\gamma \\in [0, 1)$ as the discounted factor.\nThe classical PPO methodology inherits from the famous actor-critic framework. The critic generates the state value estimate V(s), with its loss calculated from Bellman function by bootstrapping the state value function\n$L^{Critic} = E_{s\\sim d^{\\pi}} [(r_t + \\gamma(V_{\\theta_{old}}(s_{t+1}) - V_{\\theta}(s_t))^2]$\nOn the other hand, generalized advantage estimation (GAE) [Schulman et al., 2016] is employed to help calculate the action advantage $A_t$ by traversing the episode backward\n$A_t = \\delta_t + (\\gamma \\lambda)\\delta_{t+1} + \\cdots + (\\gamma \\lambda)^{T-t+1}\\delta_{T-1}$\n$\\delta_t = r_t + V(s_{t+1}) - V(s_t)$\nthen the actor is learned by maximizing the surrogate objective of $A_t$ according to the policy-gradient theorem\n$L^{Actor} = CLIP(E_t \\frac{\\pi_{\\theta_{old}}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)} A_t - \\beta KL[\\pi_{\\theta_{old}}(\\cdot|s_t), \\pi_\\theta(\\cdot|s_t)])$\nin which the CLIP function means clipping the object by $[1 - \\epsilon, 1 + \\epsilon] A_t$, and KL denotes the famous K-L divergence.\nA PPO policy update is then conducted by minimizing the objective upon each iteration:\n$L^{PPO} = -\\eta_A L^{Actor} + \\eta_C L^{Critic}$"}, {"title": "B Additional Implementation Details", "content": "For practical consideration, input and output sequences are truncated by a window length Tw, with padding time mask for episodes shorter than Tw. The timestep embedding is also considered and concatenated into the latent variable.\nTo emphasize the instant impact, we further conduct a multi-head attention by querying the target variable and marking the input variable as key and value:\n$a_t^s \\leftarrow a_t^s + Attention(Q=\\hat{a_t}, K=s_t^s, V=s_t^s)$\n$s_t^s \\leftarrow s_t^s + Attention(Q=\\hat{s_t}, K=a_{t-1}^s, V=a_{t-1}^s)$\nTable 8 lists most hyper-parameters in our implementation."}]}