{"title": "Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution", "authors": ["Muhammad Umair Haider", "Hammad Rizwan", "Hassan Sajjad", "Peizhong Ju", "A.B. Siddique"], "abstract": "Interpreting and controlling the internal mechanisms of large language models (LLMs) is crucial for improving their trustworthiness and utility. Recent efforts have primarily focused on identifying and manipulating neurons by establishing discrete mappings between neurons and semantic concepts. However, such mappings struggle to handle the inherent polysemanticity in LLMs, where individual neurons encode multiple, distinct concepts. This makes precise control challenging and complicates downstream interventions. Through an in-depth analysis of both encoder and decoder-based LLMs across multiple text classification datasets, we uncover that while individual neurons encode multiple concepts, their activation magnitudes vary across concepts in distinct, Gaussian-like patterns. Building on this insight, we introduce Neuron Lens, a novel range-based interpretation and manipulation framework that provides a finer view of neuron activation distributions to localize concept attribution within a neuron. Extensive empirical evaluations demonstrate that NeuronLens significantly reduces unintended interference, while maintaining precise control for manipulation of targeted concepts, outperforming existing methods.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have shown unprecedented performance in various tasks involving natural language understanding, generation, and transformation (Brown et al., 2020; Bommasani et al., 2022; Touvron et al., 2023; Raffel et al., 2019). However, the inner workings of LLMs remain largely opaque (Burkart & Huber, 2021), as their representations are distributed across billions of parameters. This lack of interpretability raises critical concerns about reliability, fairness, and trustworthiness, particularly in high-stakes domains such as healthcare, law, and education. To this end, neuron-level interpretability can address these concerns by enabling researchers to uncover how individual neurons encode semantic concepts and contribute to model outputs. With this understanding, researchers can diagnose safety risks (Wei et al., 2024; He et al., 2024), refine model outputs (Meng et al., 2023; Rizwan et al., 2024), optimize efficiency through pruning (Frankle & Carbin, 2019; Haider & Taj, 2021), and steer models toward desired objectives (Subramani et al., 2022).\nRecent research efforts have made significant progress in neuron-level interpretation by identifying salient neurons that influence model behavior (Dalvi et al., 2019a; Antverg & Belinkov, 2022; Conmy et al., 2023; Marks et al., 2024). Approaches such as maximal activation analysis identify neurons that exhibit the highest activation values for specific concepts (Foote et al., 2023; Frankle & Carbin, 2019). Probe-based methods employ auxiliary classifiers to distinguish between concepts using hidden representations (Dalvi et al., 2019a;b), while probeless approaches bypass the need for classifiers by directly analyzing neurons, improving simplicity and interpretability (Antverg & Belinkov, 2022). Other techniques include circuit discovery, which attributes concepts to groups of interacting neurons (Olah et al., 2020; Conmy et al., 2023), and causal analysis, which intervenes on internal components to identify their role in model behavior (Vig et al., 2020).\nWhile these methods have advanced our understanding, they often rely on discrete neuron-to-concept mappings, which assume that entire neurons encode single concepts. However, neurons frequently exhibit polysemanticity \u2013 the ability to encode multiple, seemingly unrelated concepts (Lecomte et al., 2024; Marshall & Kirchner, 2024). Given this heterogeneous encoding of concepts, traditional approaches often lead to unintended consequences when manipulating neurons, as changes intended for one concept may inadvertently affect others encoded by the same neuron or suboptimal interpretations of concepts (Sajjad et al., 2022).\nDespite being traditionally viewed as a challenge, could polysemanticity instead provide a unique lens for advancing interpretability and model control? If individual neurons encode multiple concepts, might their activation spectrum reveal distinct and identifiable patterns for each concept? Could these patterns enable precise interventions that adjust one concept while minimizing interference with others, overcoming the limitations of coarse, monolithic neuron-to-concept mappings?\nThis work seeks to address these questions by analyzing the activation patterns of neurons in both encoder-based and decoder-based LLMs. Through statistical and qualitative analysis, we find that neuronal activations for concepts follow Gaussian-like distributions, with distinct patterns for different concepts. Our key insight is that the true fundamental unit of interpretability lies at a level more fine-grained than the neuron itself. Within a neuron's activation spectrum, activation ranges corresponding to specific concepts emerge as the fundamental unit. This novel perspective enables a more precise approach to neuron interpretation and manipulation, addressing the limitations of traditional, discrete neuron-to-concept mappings.\nBuilding upon these insights, we introduce NeuronLens, a novel range-based framework for neuron interpretation and manipulation. Neuron Lens maps specific activation ranges within a neuron's distribution to individual concepts, rather than attributing entire neurons to single concepts. For each concept, NeuronLenscalculates a range that covers approximately 98.76% of the activation spectrum, capturing the concept-specific activations while excluding unrelated concepts. Through experiments on encoder-based and decoder-based LLMs across several text classification datasets, we show that Neuron Lens. significantly reduces unintended interference by up to 25 percentage points, while maintaining precise control for manipulation of targeted concepts, outperforming existing methods.\nIn summary, this work makes the following contributions:\n\u2022 We uncover that neuronal activations in LLMs form Gaussian-like distributions, with salient neurons exhibiting limited overlap in their activation patterns across concepts.\n\u2022 We show that activation ranges within a neuron's activation spectrum are the true fundamental unit of interpretability, offering a precise framework for neuron-level analysis.\n\u2022 We propose NeuronLens, a range-based framework for interpreting and manipulating neuronal activations, which enables fine-grained concept attribution and reduces unintended interference."}, {"title": "2. Neuron Interpretation Analysis", "content": "2.1. Preliminaries\nNeuron. We refer to the output of an activation as a neuron. In a transformer model, we consider neurons of hidden state vectors of different transformer layers. Formally, given a hidden state vector $h^l \\in \\mathbb{R}^d$ of size d produced by layer l, $h'_j$ denotes its j-th neuron, i.e., the j-th component of $h^l$.\nConcept. A concept $c \\in C$ is a high-level semantic category that groups each input instance (or components of every instance), where $C$ is the set of all concepts. For example, in a language task, a sentence can be categorized into 4 types: declarative, interrogative, imperative, and exclamatory, where each type is a concept. Words of a sentence can also have concepts like noun, verb, adjective, adverb, etc. In this study, we focus on the situation that all input samples are labeled with concepts.\nSaliency Ranking. A saliency ranking orders the importance of neurons based on some saliency metric. For a hidden state vector $h^l \\in \\mathbb{R}^d$, $s_{j,c}$ denotes the value of the saliency metric for the j-th neuron with respect to a concept c. The saliency ranking $(r_c(1), r_c(2), \\ldots, r_c(d))$ is a permutation of the indices of neurons $(1, 2, ..., d)$, where $r_c(j) < r_c(i)$ if $s_{j,c} > s_{i,c}$. The saliency metric is usually predetermined, e.g., the absolute value of each neuron.\nConcept Learning. Given a hidden state vector $h^l$ as input, the associated concept can be the output of an appended neural network (e.g., several fully connected layers). The parameters of this appended neural network can be trained using training samples labeled with concepts.\n2.2. Concept Erasure\nTo assess the performance of neuronal ranking obtained using attribution methods, concept erasure acts as a critical diagnostic intervention to determine the causal effect of identified neurons for a given concept (Dalvi et al., 2019b; Dai et al., 2022; Dalvi et al., 2019c; Morcos et al., 2018). The core idea is that by manipulating the salient neurons, it is possible to precisely eliminate a specific identified concept while causing minimal disruption to the other concepts learned by the model. This can be formalized as follows: given a concept-learning model $M$ that maps any input in-"}, {"title": "3. Polysemanticity", "content": "The polysemanticity of neuronal units, salient neurons encode information about multiple concepts, pose a challenge to neural network interpretation and manipulation. In this section, we discuss the degree of polysemanticity in salient neurons in detail.\nSuperposition. Polysemanticity (i.e., superposition of neurons) is primarily thought to arise when a model must represent significantly more features than the available capacity of its representational space or due to the training paradigms used. Limited representation space forces to encode multiple unrelated features within the same neuron to achieve high performance (Anthropic, 2023). Additionally, training paradigms such as subtokenization in tokenization schemes, which aim to reduce vocabulary size to decrease the model size and improve generalization for tokens with the low natural frequency. This results in context-dependent token decomposition, where a single token may be split differently based on its surrounding context (Sennrich et al., 2016). This contextual nature leads to a state of superposition, where multiple semantic meanings are encoded within the same neural activations (Elhage et al., 2022; Meng et al., 2022). Weight initialization can also contribute to polysemanticity, as demonstrated empirically by (Lecomte et al., 2024). Their findings reveal that neurons exhibiting activation patterns equidistant from or similarly close to multiple conceptual representations at initialization can lead to polysemanticity, even when the network's capacity (i.e., the number of neurons) exceeds the number of concepts to be represented.\nPolysemanticity in salient neurons. Given that salient neurons have a strong causal association with the concept of interest, their tendency to be mono-semantic should be high, but we find that there is a high degree of polysemanticity in salient neurons.\nWe investigate this by extracting 30% salient neurons (i.e.: Max neurons) for different datasets on the GPT-2 model. The results show that there is a considerable overlap of salient neurons between concepts (classes) as shown in Figure 2. In the case of a two-class dataset IMDB, the overlap of salient neurons, selected by max, is more than 60%. This shows a high degree of polysemanticity.\nConsequently, we extrapolate that salient neural representations may exist in a superimposed configuration, wherein a subset of the salient neurons encode information through intricate, overlapping activation patterns. The monolithic attribution paradigm potentially oversimplifies the complex, distributed nature of neuronal activation as can be seen in polysemanticity (Lecomte et al., 2024; Marshall & Kirchner, 2024) where single neuron learns multiple seemingly unrelated concepts and elucidates them at different activation values"}, {"title": "4. Neuronal Activation Patterns", "content": "In this section, we analyze the properties of neuronal activations of the salient neurons (including polysemantic) extracted via maximal activation. Our findings indicate that neuronal activations on a concept form a Gaussian-like distribution. We also find that salient neurons have a distinct Gaussian distribution of activations for different concepts with minimal overlap with other concept activations.\nQualitative Evaluation. To visually demonstrate that neuron activations for a concept c follow a Gaussian-like distribution, we extract model representations as described in Section 2.1. Using saliency ranking $r_c$ for a single concept class, we examine neurons from different ranking positions in the GPT-2 model on the AG-News dataset: two top-ranked neurons ($r_c \\leq 2$), two middle-ranked neurons ($r_c \\approx d/2$), and two bottom-ranked neurons ($r_c \\geq d-1$). In Figure 3 and 4 we use Kernel Density Estimation (KDE) to visualize these distributions. Figure 3 reveals that while the activations are Gaussian-like for different concepts, salient neurons demonstrate distinct activation patterns with limited overlap, middle-ranked neurons show a higher degree of overlap than the top ones, whereas non-salient neurons (bottom two) exhibit the highest overlap in their activation distributions. On GPT-2 model.\nAdditionally in Figure 4, we identify and visualize two distinct types of polysemantic neurons that appear in the salient sets across all classes, when 5% salient set was selected, in the dataset. The first type, exemplified by neuron 480, maintains partially separable activation patterns despite being polysemantic, suggesting some degree of class-specific behavior. In contrast, the second type, represented by neuron 675, exhibits completely overlapping activation patterns across all classes, making it hard to disentangle. To further investigate this phenomenon, Figure 5 presents a broader analysis of neurons from the polysemantic subset, identified using a 5% saliency threshold. By examining these neurons' behavior across four randomly selected classes (out of 14 total classes), we observe that most polysemantic neurons exhibit a high degree of separability, for some classes while they respond to multiple classes, they tend to operate in partially separable activation ranges, supporting the possibility of meaningful disentanglement.\nQuantitative Evaluation. To quantify the effect of Gaussian-like distribution of neurons, for $c \\in C$, we perform statistical analysis of activations. We computed the skewness, kurtosis (Joanes & Gill, 1998) and analyzed the normality of neuronal activations using Kolmogorov-Smirnov (KS) test (Massey Jr, 1951). The results for basic distributional properties across all neurons are presented in Table 2. The average skewness is close to 0 across all datasets, indicating strong symmetry (ideal normal distribution: 0), and the average kurtosis is close to 3, nearly identical to the expected value for a normal distribution (3.0), on GPT-2 model.\nTo quantitatively assess normality, while accounting for practical significance, we employ the KS test with an effect size threshold of 10%. This approach tests whether the distribution remains within a reasonable bound of normality, rather than testing for perfect normality, which is overly strict for real-world data. For each neuron, we normalize the activations to zero mean and unit variance, then compute the KS statistic against a standard normal distribution. The KS statistic represents the maximum absolute difference between the empirical and theoretical cumulative distribution functions. Using a threshold of 0.1 (allowing maximum 10% deviation from normal), we find that close to 100% of the neurons exhibit practically normal distributions. The combination of near-ideal skewness and kurtosis values, visual confirmation through KDEs, and our effect size-based KS tests provide strong evidence that the activations follow approximately normal distributions."}, {"title": "5. Activation Ranges-guided Intervention", "content": "Given that the neuronal activations are Gaussian-like with separable means, we can interpret and manipulate the neurons in a more precise manner instead of zeroing out the neurons. Specifically, we only zero out a salient neuron (by saliency ranking) when its value is within a certain range. The core part of this design is selecting a suitable range that is highly correlated to the target concept c that we want to remove. Intuitively, this range-based method conducts fine grained manipulation of neurons and thus alleviates the negative impact on other non-target concepts.\nIn this work, we adopt a straightforward choice of the aforementioned range, which is easy to calculate and implement. Specifically, we first calculate the empirical average $\\mu \\in \\mathbb{R}$ and standard deviation $\\sigma \\geq 0$ of the values of the salient neuron for all samples associated with the target concept $c \\in C$. After that, we assign the range as $[\\mu - \\tau \\chi \\sigma, \\mu + \\tau \\chi \\sigma]$, where $\\tau > 0$ is a hyperparameter to make a tradeoff between erasing the target concept c (using larger t) and smaller impact on other concepts (using smaller $\\tau$). For this work, we use $\\tau = 2.5$. We chose this value to cover the majority of the target concept activations to produce a similar deterioration of the targeted concept.\nMathematically, our range-based method can be described as follows. To erase a target concept c for a sample x (e.g., a sentence or a word), we manipulate the salient neuron $h^l_j(x)$ (j-th element of the hidden state vector $h^l(x) \\in \\mathbb{R}^d$ at layer l).\n\n$$h^l_j(x) = \\begin{cases} 0 & \\text{if } h^l_j(x) \\in \\text{CorrelatedRange}(l, j, c), \\\\ h^l_j(x) & \\text{otherwise}, \\end{cases}$$\nwhere in this work we let\n$\\text{CorrelatedRange}(l, j, c) = [\\mu - 2.5\\sigma, \\mu + 2.5\\sigma]$,\n$\\mu = \\frac{1}{|H^c|} \\sum_{h \\in H^c} h$, $\\sigma = \\sqrt{\\frac{1}{|H^c|} \\sum_{h \\in H^c} (h - \\mu)^2}$.\nNotice that $H^c$ was defined in problem setup and preparation of Section 2.3, which denotes the set of hidden state vector $h^l(x_c)$ at layer l for all training samples $x_c$ associated with concept c. Here $|\u00b7 |$ denotes the cardinality of a set."}, {"title": "6. Related Work", "content": "While we have discussed closely related approaches in Section 2, here, we briefly review additional relevant techniques. Circuit discovery identifies groups of neurons that jointly encode concepts, providing a structured view of model behavior (Marks et al., 2024; Conmy et al., 2023; Olah et al., 2020). However, extracting circuits is computationally intensive and lacks fine-grained neuron-level attribution. Gradient-based methods attribute predictions to input features by tracking gradients through the network, with integrated gradients (Sundararajan et al., 2017; Dai et al., 2022) being a widely used approach. However, they struggle with polysemanticity, as they do not disentangle overlapping concepts within neurons. Causal analysis methods intervene on internal components to assess their role in encoding concepts. Causal tracing measures the effect of corrupting activations on model performance (Vig et al., 2020; Meng et al., 2022), while causal mediation analysis quantifies information propagation through neurons (Vig et al., 2020). Although effective, these techniques require costly perturbation experiments. Beyond neuron-level analysis, representation-level methods examine hidden states and their relationship to model outputs. Logit lens (nos-"}, {"title": "7. Conclusion", "content": "In this work, we challenged traditional assumptions about neuron interpretability by reframing polysemanticity as a resource rather than a limitation in interpreting neurons. Through an in-depth analysis of encoder- and decoder-based large language models across multiple text classification datasets, we uncovered that neuronal activations for individual concepts exhibit distinct, Gaussian-like distributions. This discovery allows for a more precise understanding of how neurons encode multiple concepts, enabling us to move beyond coarse, monolithic neuron-to-concept mappings. Building upon these insights, we proposed NeuronLens, a novel range-based framework for neuron interpretation and manipulation. NeuronLens offers fine-grained control that reduces interference with unrelated concepts by attributing specific activation ranges within neurons to individual concepts. Extensive empirical evaluations demonstrated that NeuronLens outperforms existing methods in maintaining concept-specific precision while minimizing unintended side effects. The hyperparameter $\\tau$ was set to ensure fair comparison across methods. Future work could explore mathematically deriving optimal $\\tau$ values for individual neurons. Additionally, our work proved that the fundamental unit of interpretation can be a range within the activation space, which can be exploited for better interpretability."}, {"title": "8. Impact Statement", "content": "This work advances neural network interpretability by providing a fine-grained understanding of concept encoding in language models. The proposed NeuronLens framework enables precise control of model behavior, benefiting research in model safety and reliability. While this improved understanding could potentially be misused, the work's theoretical nature and focus on interpretability methods makes immediate harmful applications unlikely."}, {"title": "A. Limitations", "content": "While NeuronLens can disentangle polysemanticity to a degree using Gaussian Like Distribution, it is unable to completely disentangle concepts encoded in the polysemantic neurons, because there still is a significant overlap in the distributions of concepts in activations. Additionally, in this work, we use $\\tau$ to be a fixed value of 2.5 to make the comparison of approaches fair, but $\\tau$ selection can be optimized to be more sophisticated. We also get results primarily from the penultimate layer, and not the intermediate or earlier layers, however, we do give ablation and rationale for this choice in Appendix E"}, {"title": "B. Training Details", "content": "For BERT, DistilBERT, and Llama, we utilize pretrained models. Since BERT, and DistilBert are not inherently trained as a conversational agent, we use top-performing fine-tuned models from the Hugging Face repository. For the Llama model, few-shot prompt completion is employed to predict class labels. This involves providing a small number of training samples from the dataset to guide the model's predictions.\nFor GPT-2, we fine-tune the pretrained model across all datasets for three epochs. The input sequence is constructed by concatenating the text with a <sep> token, followed by the class label, and ending with an  token. During training, the loss is back-propagated only for the class label token, while all other tokens are assigned a skip label (-100). Additionally, all class labels are added to the model's dictionary as special single-token entries.\nDataset Preprocessing for Llama For Llama we process whole datasets in few shout settings and only curate 2000 samples per class, where the model prediction was correct."}, {"title": "C. Saliency details", "content": "Max activations. Frankle & Carbin (2019) extract high neural activations as a saliency ranking metric relying upon the rationale that maximally activating neurons are salient as these neurons play a critical role in controlling the model's output, highlighting their importance for a concept c.To identify them, the column-wise mean of absolute neuronal activations in $H^c$, $H^c$ is defined in Section2.3, is computed, given that high negative activations also carry significant signals (Voita et al., 2023). The magnitude of the means is then considered as a ranking for concept c.\nProbe analysis. Dalvi et al. (2019b) train a linear classifier on the hidden representations $H^c$ to distinguish between concepts. The learned model weights are then utilized as a saliency ranking. This process involves learning a weight matrix $W \\in \\mathbb{R}^{d\\times|c|}$, where d is the hidden dimension and |c| is the number of concept classes. The absolute weight values of each row in the weight matrix are used as a ranking for the importance of each neuron for a given concept. To prevent the emergence of redundant solutions characterized by minimal variations in the weights, the probe is trained using the elastic regularization technique.\nProbeless. Antverg & Belinkov (2022) examine individual neurons, without the need for auxiliary classifiers, using the element-wise difference between mean vectors. The element-wise difference between mean vectors is computed as $r = \\sum_{c,c'\\in c}|q(c) \u2013 q(c')|$, where $r \\in \\mathbb{R}^d$ and d is the hidden dimension. The final neuron saliency ranking is obtained by sorting r in descending order."}, {"title": "D. Full Results", "content": "Here we provide the complete results for the datasets shown in Table 3. In Table 5 we provide results on IMDB dataset on all selected models. In Table 6 we provide results on SST2 dataset on all selected models. In Table 7 we provide results on Emotions dataset on all selected models. In Table 8 we provide results on DBPedia-14 dataset on all selected models."}, {"title": "E. Layer Ablation", "content": "In Table 9 and Table 10 we provide results of applying both approaches on all layers of GPT-2 model on Emotions dataset. From the results we can see that: Early layers (1-3) show highly variable and often severe impacts: Layer 1 exhibits minimal effects ($\\Delta Acc = \u22120.113$, $\\Delta CAcc = -0.064$), while Layers 2-3 show extreme degradation (Acc ~ -0.7, $\\Delta CAcc > -0.5$). Middle layers (4-8) demonstrate inconsistent behavior with high variance in impacts. Layer 12, however, achieves an optimal balance: it maintains substantial primary task impact ($\\Delta Acc = -0.571$) while minimizing auxiliary concept interference ($\\Delta CAcc = -0.060$). This pattern holds true for both neuron masking and range masking techniques, with range masking showing slightly better preservation of auxiliary concepts ($\\Delta CAcc = -0.045$). The mid-range primary task degradation combined with minimal auxiliary impact makes Layer 12 the most suitable for targeted interventions, offering better control and specificity compared to earlier layers."}]}