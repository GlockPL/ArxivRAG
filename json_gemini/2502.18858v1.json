{"title": "Intelligence Test", "authors": ["Jingtao Zhan", "Jiahao Zhao", "Jiayu Li", "Yiqun Liu", "Bo Zhang", "Qingyao Ai", "Jiaxin Mao", "Hongning Wang", "Min Zhang", "Shaoping Ma"], "abstract": "How does intelligence emerge? We propose that intelligence is not a sudden gift or random occurrence, but rather a necessary trait for species to survive through Natural Selection. If a species passes the test of Natural Selection, it demonstrates the intelligence to survive in nature. Extending this perspective, we introduce Intelligence Test, a method to quantify the intelligence of any subject on any task. Like how species evolve by trial and error until they find a way to survive, Intelligence Test quantifies intelligence by the number of failed attempts before success. Fewer failures correspond to higher intelligence. When the expectation and variance of failure counts are both finite, it signals the achievement of an autonomous level of intelligence. Using Intelligence Test, we comprehensively evaluate existing artificial intelligence (AI) systems. Our results show that while AI systems achieve a level of autonomy in simple tasks, they are still far from autonomous in more complex tasks, such as vision, search, recommendation, and language. While scaling current AI technologies might help, this would come at an astronomical cost. Projections suggest that achieving autonomy for general tasks would require unimaginable 10^{26} parameters. To put this into perspective, loading such a massive model requires so many H100 GPUs that their total value is 4 \u00d7 10^{7} times that of Apple Inc.'s market value. Even with continuous hardware improvements suggested by Moore's Law, such a parameter scale would take 70 years to support. This staggering cost highlights the complexity of human tasks and the inadequacies of current AI technologies. To further understand this phenomenon, we conduct a theoretical analysis of Intelligence Test and its experimental results. Our findings suggest that human tasks possess a \u201ccriticality\" property. As a result, autonomy requires a deep understanding of the task's underlying mechanisms. Current AI systems, however, do not fully grasp these mechanisms and instead rely on superficial mimicry, making it difficult for them to reach an autonomous level. We believe Intelligence Test is a powerful tool. It not only guides the future development of AI but also offers profound insights into the intelligence of any species, including humans ourselves.", "sections": [{"title": "1 Introduction", "content": "We propose to consider the question\u00b2, \u201cWhat is intelligence?\u201d This should begin with the definition of the meaning of the term \"intelligence\". The definition might be framed so as to reflect so far as possible the normal use of the word, but this attitude is dangerous, If the meaning of the word \u201cintelligence\" is to be found by examining how it is commonly used, it is difficult to escape the conclusion that the answer to the question is to be sought in a statistical survey. But this is absurd. Instead of attempting such a definition, we shall replace the question with another, which is closely related to it and is expressed in relatively unambiguous words.\nWe turn to another question: \"How does intelligence emerge?\" We believe that intelligence is not an innate gift but rather a necessity shaped by Natural Selection. The diversity of life forms we see today, including humans, animals, and plants, has endured countless challenges imposed by the natural world over vast stretches of time. Take humankind as an example. We have withstood the threats of predators, survived devastating plagues, and endured two world wars. We have invented steam engines, harnessed electricity, and built the internet. None of these challenges were overcome by mere chance; none of these inventions appeared in an instant. Every step forward was the result of relentless experimentation, repeated failures, and the eventual success of those who possessed the intelligence to find a way forward. Thus, we argue that intelligence is to pass the test of Natural Selection. It drives living beings to persistently explore, seek solutions in the face of uncertainty, and ultimately prevail.\nIn this light, we can view Natural Selection as a test of intelligence in the domain of survival. This test challenges its subjects on their ability to endure and adapt within the natural world. The subjects must strive to solve the challenges, experimenting again and again until they succeed. If they cannot find a solution, they fail the test and thus do not survive. Those who pass this test can be considered intelligent in terms of survival. Even though the plants and animals that exist today may not have built a civilization like humans have, they have all successfully passed the test of Natural Selection. In contrast, the countless species that have gone extinct did not. In this sense, all living beings that continue to exist, including humans, possess intelligence in the task of survival.\nInspired by Natural Selection, we propose Intelligence Test. Similar to how species find a way to survive through trial and error in Natural Selection, Intelligence Test quantifies intelligence by counting the number of failures a subject experiences before finding the correct solution. Fewer failures correspond to higher intelligence. When applied to practical tasks, the number of failures is a discrete random variable, and smaller expectations and variances of the failure count indicate higher intelligence. Based on the convergence of the expectations and variances of failure count, the Intelligence Test divides intelligence into three levels: Limited, Capable, and Autonomous. If both the expectation and variance diverge, the subject is at the Limited Level. At this level, the subject is comparable to blindly enumerating possible solutions. If both the expectation and variance converge, the subject reaches the Autonomous Level. At this level, the subject can stably find the correct solution with only a few trials, thereby being able to autonomously operate at an affordable cost. As we can see, the results of the Intelligence Test have clear physical meaning about the subject's intelligence level.\nThe Intelligence Test can be applied to any task and any species. In this paper, we are particularly interested in artificial intelligence (AI) systems. Therefore, we conduct Intelligence Test on state-of-the-art AI systems available today. The results demonstrate that a system with better modeling of the task can reach a higher level of intelligence. Current AI technologies can reach the Autonomous Level on simple tasks like handwritten digit recognition. However, they are mostly at Limited Level on more complex tasks, including vision, search, recommendation, and language. This indicates that most AI systems are at a preliminary stage: they are unable to substantially narrow down the range of possible answers and their performance is comparable to brute-force enumeration. This indicates that directly applying these AI technologies can result in very high costs and serious errors, so they cannot operate autonomously, and human supervision is essential. These findings challenge conclusions from previous studies which suggest that AI has already reached a very high level of intelligence.\nMoreover, we empirically find that the intelligence measurement of the Intelligence Test exhibits an approximately log-linear relationship with the scale of AI systems. If we assume this relationship continues to hold, we can predict the scale required to achieve Autonomous-Level intelligence. The projection suggests that, for general language tasks, an AI system would need a parameter size of 10^{26} to reach the Autonomous Level. To put this scale into perspective, this is equivalent to 10^5 times the total number of neurons in all of humanity's brains combined. Loading a model of this size onto H100 GPUs would necessitate 5 \u00d7 10^{15} H100 cards, a"}, {"title": "2 Related Work", "content": "Defining a test for intelligence is a fundamental issue. For AI research, it allows us to understand, apply, and develop AI technology. More broadly, it enables us to gain a deep understanding of intelligence, leading to a profound insight into both humanity and the natural world.\nIn 1950, Alan Turing proposed the Imitation Game to test whether a machine can possess human intelligence. Since then, it has been highly influential in the AI field. Many researchers have developed methods to practically implement or further improve Imitation Game. In this section, we review some of the most influential approaches.\n\u2022 Imitation Game, aka Turing Test: Intelligence is the ability to imitate human responses convincingly in a text-based conversation. If a human evaluator cannot reliably distinguish between a machine and a human based on their answers, the machine is considered intelligent.\n\u2022 Total Turing Test: It is an extended version of the Turing Test that assesses a machine's ability to interact with the world in a human-like way. It goes beyond text-based conversations to include physical interaction and sensory perception.\n\u2022 Chinese Room Argument : It argues that the Imitation Game only evaluates syntactics and yet AI should also understand semantics, such as knowing the actual meaning of each word.\n\u2022 Lovelace Test : It argues that intelligence is about creativity. For example, AI should be able to originate art, music, or poetry.\n\u2022 Reverse Turing Test : Instead of asking whether a machine can act like a human, it asks whether an AI can differentiate between humans and machines.\n\u2022 Universal Intelligence : Beyond the conversation task in Imitation Game, it measures an agent's ability to achieve goals in a wide range of environments.\n\u2022 Winograd Schema Challenge : It tests whether AI can identify the antecedent of an ambiguous pronoun in a statement. It requires world knowledge and contextual understanding.\n\u2022 General intelligence : It defines intelligence as the ability to achieve a wide range of goals and handle new problems in different contexts and environments.\n\u2022 Visual Turing Test : It adds the visual understanding ability to the Imitation Game. It tests whether AI can answer complex questions about images.\n\u2022 Economical Value : It tests whether AI can be a highly autonomous system that outperforms humans at most economically valuable work."}, {"title": "3 Methodology", "content": "The origin of species, including humans, is Natural Selection. Nature has relentlessly filtered out the unintelligent, allowing only those with intelligence to survive. Inspired by this, we propose Intelligence Test, a universal framework to quantify the intelligence of any subject in any task. It builds upon the core concept of Natural Selection to test how well a subject can autonomously explore and find solutions.\nIn the following subsections, we first revisit Natural Selection and formalize it as a Survival Game. Then, we extend Survival Game as Intelligence Test to quantify intelligence at any task. Next, we interpret the results of Intelligence Test into three levels of intelligence. Finally, we discuss how Intelligence Test differs from previous studies."}, {"title": "3.1 Survival Game", "content": "The process of Natural Selection is extraordinarily complex, involving competition between species, genetic mutations, etc. Rather than delving into these intricate details, we simplify Natural Selection into a conceptual framework which we call the Survival Game.\nImagine a species with a sufficiently large population. Its individuals stand in line outside a room. A sign at the entrance warns them that once inside, they will face a critical question. One by one, the individual enters the room and gives answers. An incorrect answer makes the individual vanish, while a correct answer lets it survive. One survivor can mark the species as having passed the test.\nDespite the simplification, Survival Game captures the essence of Natural Selection. Throughout history, nature has posed countless challenges to humankind. When asked how to survive predators, the intelligent among us answered fire and tools. When faced with the threat of starvation, the intelligent among us developed agriculture. When confronted with disease, the intelligent among us advanced medicine. Civilization itself has been forged through these relentless Survival Games. If it were not always for someone to step up and pass the Survival Game, modern society as we know it would not exist.\nBased on the description of the Survival Game above, we will now translate it into mathematical terms to make it clearer.\nLet N be the population size of a species. Let X represent the number of individuals who fail before the correct answer is found. X takes values in the range of 0 \u2264 X \u2264 N, where X = 0 means the first individual answers correctly, while X = N means that all individuals fail. If at least one individual succeeds, i.e., X < N, the species passes the game.\nWe can see that the number of failures, X, is a direct measure of a species' survival intelligence. The smaller the value of X, the less effort the species needs to solve problems. Inspired by this, our proposed Intelligence Test will similarly measure intelligence."}, {"title": "3.2 Measuring Intelligence on Any Task", "content": "Survival Game directly measures the intelligence of species on the survival task. Building on this, we introduce Intelligence Test, which extends the Survival Game to any task and measures any subject's intelligence. To achieve this, Intelligence Test models failure counts as a discrete random variable and uses statistical metrics for evaluation. The modifications are two-fold:\n\u2022 Modeling Failure Count as a Discrete Random Variable: One task can involve numerous variations, and the conclusions of the Survival Game may be very different across these variations. For instance, consider testing a subject's ability to solve mathematical problems. A small change in the numbers or the context of the problem could lead to a significant shift in the subject's failure counts. Similarly, when a task is classifying images, different pictures can result in substantial fluctuations in performance. Therefore, the variability within the task can cause the conclusions to be unstable. To account for this variability, Intelligence Test models failure count as a discrete random variable, which allows us to handle the variations across task variants effectively.\n\u2022 Statistical Criteria for Evaluation: Population size N serves as a threshold value in the Survival Game. It directly affects the conclusion. The larger the value of N, the more attempts are available to the subject, and consequently, the higher the likelihood of success. Yet, for tasks other than survival, the notion of what constitutes an \"appropriate\" N can vary from one researcher to another. This variability in determining an appropriate N leads to inconsistencies in the conclusions. Therefore, Intelligence Test does not use a pre-defined threshold for measurement. It quantifies intelligence as the distribution of failure count. A lower probability of a large failure count suggests higher intelligence.\nWith these statistical improvements, we formally define Intelligence Test as follows:\nLet X be a discrete random variable representing the number of a subject's failure attempts before providing a correct solution for a task. Then, X serves as the measure of this subject's intelligence on the task. Smaller expectations and variances of X correspond to higher intelligence."}, {"title": "3.3 Classifying Intelligence into Three Levels", "content": "In this subsection, we analyze the distribution of failure counts obtained from the Intelligence Test to gain a clear understanding of the subject's level of intelligence. First, we introduce an Infinity Assumption to define the least intelligent scenario. Based on this, we then propose three levels of intelligence. Finally, we explain how to classify subjects into these three intelligence levels based on the distribution of their failure counts."}, {"title": "3.3.1 Infinity Assumption", "content": "What situation represents a subject having almost no intelligence related to the task? Imagine a scenario in an Intelligence Test where a monkey sits in front of a computer and types to see if it can produce Shakespeare's works. If it deviates from Shakespeare's works, we let it attempt again. The failure count refers to the number of attempts before success. The monkey has no understanding of human language and just types randomly. In theory, since the human vocabulary is finite and Shakespeare's works are also of limited length, the monkey could use an enumeration method, blindly trying all possible combinations of words. Even though most of these combinations are completely nonsensical to us, the monkey can eventually type out Shakespeare's works. However, this blind, exhaustive enumeration shows that the subject lacks any real intelligence. It is also disconnected from practical reality because the cost of such an exhaustive search would far exceed any reasonable resource limitations, much like how it is completely unrealistic to expect a monkey to eventually produce Shakespeare's works. Shakespeare did not create his works by randomly typing and waiting for greatness to emerge. Instead, he produced the masterpieces through intentional creativity within the limitations of human life. Therefore, when the failure count approaches the cost of exhaustive enumeration, it almost certainly indicates that the subject has no intelligence related to the task.\nWe note that the high cost of blind enumeration closely resembles the mathematical concept of infinity. In mathematics, infinity describes a scenario where a quantity is beyond the scale we can measure or endure. For example, when measuring objects on Earth, we can assume the distance from the Sun to the Earth is infinite,"}, {"title": "3.3.2 Three Intelligence Levels", "content": "The above Infinity Assumption links intelligence with infinity. It enables us to clearly define different levels of intelligence in mathematical terms. Based on this, we compare the statistical measures of failure count with infinity and define three levels of intelligence:\n\u2022 Limited Level: A subject belongs to this category if the expectation of failures is infinite: $E(X) \\to \\infty$. At this intelligence level, the subject is comparable to blindly enumerating all possible outcomes. The cost for the subject to autonomously solve the task is unacceptable in real-world scenarios. It requires external supervision to improve itself and reliably operate within the task.\n\u2022 Capable Level: A subject belongs to this category if the expectation of failures is finite, but the variance remains infinite: $E(X) < \\infty$, $Var(X) \\to \\infty$. At this intelligence level, the subject is, in principle, capable of solving the given task. However, the number of failures vary drastically across different cases. Its performance is highly unpredictable and failures can still occur frequently. As a result, autonomous operation is risky, and external supervision remains necessary to ensure reliability.\n\u2022 Autonomous Level: A subject belongs to this category when both the expectation and variance of failures are finite: $E(X) < \\infty$, $Var(X) <\\infty$. Subjects at this level can reliably find solutions for the given task. They may operate autonomously without relying on external supervision.\nIf a subject reaches the Autonomous Level, it can reliably find solutions with affordable trials and errors. If we imagine that the subject will use the correct solutions as supervision signals to improve itself, the Autonomous Level implies that the subject no longer requires external supervision to provide correct answers. Instead, it can rely solely on their attempts to find the solution. In this way, the subject can independently generate supervision data and improve itself to further reduce the failure counts. In AI, this process is similar to reinforcement learning, where the system autonomously explores solutions and uses the results to update itself. If the subject has not reached the Autonomous Level, it is almost infeasible to find solutions on its own. More precisely, subjects at the Limited Level require an infinite number of attempts, which is completely beyond reasonable limits, while subjects at the Capable Level are very unstable in finding the solution. These factors make it challenging for the system to autonomously explore solutions and instead necessitate external supervision."}, {"title": "3.3.3 Decay Rate Classification", "content": "Before presenting how to practically determine intelligence levels, let us revisit the Infinity Assumption. Although infinity does not exist in the physical world, this does not prevent us from treating certain quantities, which far exceed our capacity to measure or endure, as if they were infinite. In the case of the Intelligence Test, the total number of possible solutions is finite, but as described in the Infinity Assumption, we lack the resources or willingness to blindly enumerate all of them. Therefore, the Infinity Assumption treats the number of possible solutions as if it were infinite.\nBased on the Infinity Assumption, the distribution of the failure count can be seen as extending from 0 to infinity. Therefore, we can assess the convergence of the expectation and variance according to the distribution of the failure count. Note that the convergence of expectation and variance is determined by the tail behavior"}, {"title": "3.4 Comparison with Related Work", "content": "Since we have introduced Intelligence Test, we can pick up our discussion from Section 2. In contrast to the subjective tests in prior studies, Intelligence Test provides an objective way to evaluate intelligence:\n\u2022 Objective (Species-Agnostic) View of Intelligence: We define intelligence not by its similarity to humans, but by the ability to pass a test akin to Natural Selection. Any entity that can independently find solutions demonstrates intelligence, regardless of whether it is human, artificial, or another species. Even humans may not necessarily be at the Autonomous Level in some tasks, and the test is always applicable no matter whether AI surpasses humans.\n\u2022 Objective Choice of Tasks: We recognize that intelligence is inherently task-dependent. Unlike previous approaches that attempt to define universal intelligence, Intelligence Test does not prescribe any specific task. Instead, it allows researchers to evaluate intelligence in any task of interest, ensuring that the definition of intelligence remains grounded in the actual demands of a given task."}, {"title": "4 Evaluation with Intelligence Test", "content": "In this section, we evaluate state-of-the-art AI systems with Intelligence Test. We adopt a wide range of tasks, including vision, search, recommendation, and language.\nQuantify AI's failures: We calculate the number of failures based on the scores output by the AI system. More precisely, for a given task, existing AI systems output a score for each potential answer. For instance, an image classification model assigns a score to each class; a search engine model predicts a relevance score for each document; a recommendation system assigns a score to each product; and a language model outputs a score to each word. A higher score represents a higher possibility the AI system predicts that this is the correct answer. We rank the answers based on the model's output score from highest to lowest. This ranking list is the model's attempt sequence, and the failure count equals the position of the reference answer minus one.\nThe following presents the evaluation results of these models across various tasks. We will see that current models only reach the Autonomous Level in simple tasks and are at the Limited Level in most complex tasks. At the end of this section, we revisit existing AI techniques and show that these techniques are exactly developed in the context of Limited-Level intelligence."}, {"title": "4.1 A Beginner's Task: MNIST", "content": "MNIST is a handwritten digit recognition task. It consists of a collection of images depicting the digits 0-9, written by different people. The task is for an AI system to correctly identify the digit in each image. Many people consider MNIST to be a relatively simple task, and it is often used as an introductory challenge for beginners to experiment with various AI algorithms. As such, we also start with this task to test whether Intelligence Test can effectively distinguish different types of AI algorithms.\nWe used three AI algorithms: a linear classifier, a multilayer perceptron (MLP) classifier, and a convolutional neural network (CNN). Neither the linear classifier nor the MLP classifier takes into account the specific characteristics of the task; they both flatten the 2D image into a 1D vector and perform transformations on this vector to do the classification. The transformation for the linear classifier is linear, while the MLP classifier introduces non-linear activation functions. In contrast, CNN is equipped with a deeper understanding of images. It uses convolution to capture local features and employs multiple layers to extract abstract semantic information. Therefore, from the perspective of task modeling, CNN performs more in-depth modeling compared to both the MLP and the linear classifier. We train the three models on MNIST's"}, {"title": "4.2 Vision", "content": "In this subsection, we test whether current AI models can effectively execute complex vision tasks. We select two types of tasks for evaluation. The first is an image classification task. Given an image, the model needs to identify what animal or object is present and categorize it appropriately. For this task, we use a widely recognized dataset, ImageNet-1K. The second task is more complex: given a natural language description, the model should find the corresponding image from a large set of images. Compared to image classification, this task requires the model to understand the meaning of a long natural language description and have a deeper understanding of complex images. We use two popular datasets for this task: MS COCO and Flickr30k.\nWe evaluate state-of-the-art AI models currently available in the field. In the first image classification task, we use CLIP model and MAE models of various sizes. CLIP is widely used for visual tasks, such as text-to-image generation. The MAE models are among the best-performing on ImageNet. For the second task, we select several top-performing models from the relevant task leaderboard, namely DFN-VIT-L, ConvN-XXL, and EVA01-G. These models are not only large in parameter size but also in the size of the training data. They represent the best models in the field."}, {"title": "4.3 Search", "content": "Next, we evaluate the performance of text search models. Text search should be familiar to many people. It has widespread applications in search engines like Google, Bing, and Baidu. Given a query, the text search model ranks the candidate documents in order of relevance from highest to lowest. We regard this ranking list as its attempt sequence when applying Intelligence Test.\nWe use a diverse range of datasets. We synthesize a basic dataset so that readers can have better understanding of the task. We use Wikipedia as the raw data and construct a text search task with its titles and documents. Given a title, the search model ranks all the documents and should put the corresponding document at the top of the ranking list. The number of failure attempts is equal to how many incorrect documents are ranked higher than the correct ones. Besides this synthetic dataset, we also use many real-world search datasets. We adopt two web search datasets, MS MARCO and T2Ranking. The former is in English and the latter is in Chinese. Both were derived from real user queries on search engines. They are widely used to benchmark the effectiveness of text search models. We also use datasets from finance domain and social platforms: FiQA, CqadupStack, and Quora. FiQA requires the model to find the relevant answers to financial questions. CqadupStack and Quora are released by StackExchange and Quora social platforms, respectively. Given a query, they require models to find duplicate queries.\nWe use three distinct search models for evaluation. The first is BM25, a popular model that was proposed decades ago. It is based on exact match and term frequency weighting. We implement it with Anserini toolkit. The second is dense retrieval which represents both the query and the documents as semantic vectors and ranks them based on vector similarity. We use two open-sourced models from BGE since they are top performers on the related leaderboard. The two models vary in size, and we denote them as DR Small and DR Base. The third is cross-encoder which takes both the query and the document as input and uses attention mechanisms to model their interaction. In this way, it captures more nuanced matching signals and predicts relevance more accurately. We use two strong open-sourced models. On the English dataset, we use MiniLM cross-encoder. On the Chinese dataset, we use BGE cross-encoder.\nThe experimental results are shown in Figure 4. The first two rows show the performance of the Wikipedia synthetic dataset and the English web search dataset, respectively. The third row shows the performance on other datasets. We can see that on all datasets and for all text search models, the performance remains at the Limited Level. On the synthetic Wikipedia dataset, the current models' performance is close to the Capable Level. On other real-world datasets, the models are far from the Capable Level. Besides, from the results in the first two rows, as the models become larger and more complex, their decay rate increases and data points move closer to the Capable Level.\nLimited Level has a clear physical meaning in the text search scenario: when a user submits a query, the document that the user truly wants is likely to be ranked at the end of the list. The user needs to read, in statistical terms, infinite irrelevant documents before reaching the document they are looking for. This"}, {"title": "4.4 Recommendation", "content": "After examining the results of search engines, let's turn our attention to another widely used AI application: recommendation systems. Recommendation systems predict what a user likes based on past behavior and profile information. These systems have extensive applications in areas such as e-commerce, short videos, etc.\nWe adopt many real-world datasets from a wide range of domains. We use the Amazon Beauty dataset to represent e-commerce recommendations. It focuses on skincare product recommendations on the Amazon platform. We use MovieLens to represent movie recommendations. It is constructed based on user ratings of movies. We use Steam dataset to represent game recommendations. It recommends games to players on the Steam platform. We use Douban Book to represent book recommendations. Douban is a popular Chinese internet platform and this dataset is to recommend books to users. We use Douban Music to represent music recommendations. It is also collected from the Douban platform and is to recommend music to users. Finally, we use Gowalla dataset to represent location recommendations. Gowalla is a location-based online social network application where users share their current location. The dataset is to recommend places users might like to visit.\nWe test four widely recognized recommendation methods. The first is a popularity-based recommendation method. As the name suggests, it ranks items based on their popularity and recommends them accordingly. Although it is straightforward, it is effective and commonly used in real-world applications. The other three methods are neural-based models: GRU4Rec, SASRec and ComiRec. They differ in architecture. GRU4Rec employs recurrent neural networks to build user profiles based on the interaction history. SASRec uses attention mechanisms to"}, {"title": "4.5 Language", "content": "We have assessed AI models in vision, search, and recommendation tasks. Now, we proceed to language tasks. Some studies claim that large language models have already achieved exceptionally high-level intelligence and passed the Turing Test . With Intelligence Test, we can examine their intelligence levels and re-think this conclusion. We will use four tasks for a comprehensive evaluation, including coding, mathematics, question answering, and writing.\nExperimental Setup: We input the question to large language models and examine the models' correctness in predicting the answer. The answer written by humans is regarded as the correct one. If the answer contains more than one word, such as writing a math proof or a long passage, we concatenate the question and the first n answer words as the models' input and evaluate the performance in predicting the n + 1-th answer word."}, {"title": "4.5.1 Coding", "content": "We test models' ability to write code. Code has a clear structure, which makes it easier to predict compared to natural language. We use three widely recognized coding benchmarks. All three are designed for beginner-level programming tasks. The first is HumanEval. It provides the function signature as well as docstring and requires subjects to write the function body. The second is MBPP. It requires subjects to write functions based on a natural language description. Answers for both HumanEval and MBPP are function definitions. The third is CRUXEval. It requires subjects to understand a function and infer its output for a given input. The answer is usually a code object, such as a string or a list.\nThe experimental results are shown in Figure 6. The three rows present results on HumanEval, MBPP, and CRUXEval, respectively. We can see that models with more parameters are closer to the Capable Level. For 70B models, a few data points are already within the Capable Level region, yet a long tail of data points still falls at the Limited Level region. Therefore, although current models are relatively strong and approaching the Capable Level in coding, they are mostly at the Limited Level. It means that they cannot reliably find correct solutions for basic coding questions. Thus, human supervision is essential."}, {"title": "4.5.2 Mathematics", "content": "Next, we test models in another structured domain, namely mathematics. We use three popular datasets. The first is CMath. It is a Chinese dataset that focuses on elementary school-level math problems. It requires subjects with the ability of addition and subtraction. The second is GSM8K. It is an English dataset with similar problems to CMath. For these two datasets, the correct answer that the model needs to output is a number, usually no more than two digits. The third is MATH competition dataset. It contains complex math problems derived from math competitions. The answers to these math problems are usually a long text, such as a mathematical proof or the step-by-step process of solving the problem.\nThe experimental results are shown in Figure 7. From top to bottom, the rows correspond to CMath, GSM8K, and MATH. We can see that on the first two datasets, models are far away from the Capable Level. Thus, current models can hardly perform basic addition and subtraction. In contrast, results in the third row suggest that models are relatively strong in Math Competitions. The data points are approaching the Capable Level. In summary, the models have difficulty solving simple elementary school math problems, yet they perform much better on complex competition-level math problems. This reflects a significant difference between AI and human intelligence. Besides, we also observe that as the model size increases, there is a clear trend of moving closer to the Capable Level.\nTherefore, we should exercise caution when using large language models to solve mathematical problems. Although they might solve some complex math questions, they still make significant errors on basic math problems that are easy for humans. In general, current models are at Limited Level. This means that they require a large amount of trials before finding the correct solutions. Thus, it is always necessary to validate their outputs."}, {"title": "4.5.3 Question-Answering", "content": "Next, we examine the models' ability in the Question Answering (QA) task. We select three widely used datasets: MMLU-Pro, Natural Questions (NQ), and Trivia QA. MMLU-Pro consists of multiple-choice questions across various fields such as mathematics, chemistry, law, etc. The model needs to choose one answer from ten options. NQ is a dataset of real-world questions about factual information. TriviaQA is similar to NQ. Answers in both datasets are only several words long.\nThe experimental results are shown in Figure 8. The three rows represent MMLU-Pro, NQ, and TriviaQA, respectively. From the results, we observe that all four models are at Limited Level. In MMLU-Pro, the models' failure decay rate is less than 1. In NQ and TriviaQA, the performance is slightly better than in MMLU-Pro, but the models are still far from reaching the Capable Level. Furthermore, we can see that as the model size increases, the decay rate also increases, gradually moving toward the Capable Level. However, the marginal gains diminish: there is a significant improvement when going from 0.5B to 16B, but then the progress slows down. This suggests that the improvement is sublinear with respect to model size.\nResults reflect that question-answering systems can make serious mistakes. In some cases, the systems regard the correct answer as completely incorrect. As a result, we cannot fully trust current question-answering systems, and it is crucial to verify the accuracy of their outputs."}, {"title": "4.5.4 Writing", "content": "Now, we evaluate general writing ability. Based on many human-written articles, we examine whether current systems can also write like humans. During the evaluation, we use the first n words as the input and examine whether subjects can accurately predict the n + 1-th word. The number of failure attempts equals the number of words scored higher than the n + 1-th word written by humans. To ensure the model has sufficient context to make its prediction, we only consider cases where the input prefix is long enough, such as when n \u2265 1,000. Since those AI systems are trained on large amounts of human data to mimic humans' writing, we believe it is appropriate to adopt human's next token as a reference."}, {"title": "4.6 Revisiting Current AI Techniques", "content": "In previous subsections", "Intelligence Test Loss\" in this subsection.\nIntelligence Test Loss has a strong physical meaning and naturally reflects the performance of AI systems. Yet, most advanced AI systems are stuck at the Limited Level and Intelligence Test Loss diverges, making directly adopting this loss infeasible. In the following, we demonstrate that many current AI technologies are profoundly related to the divergence of Intelligence Test Loss, even though these technologies were not explicitly designed or used with this awareness in mind.\nHard Negative Sampling": "Hard negative sampling is a widely used optimization technique in many AI fields, including vision, search, recommendation, and language tasks. It penalizes the model's top-k most incorrect predictions (i.e., hard negatives), rather than punishing all of its wrong predictions (i.e., random negatives). Researchers explain its effectiveness with various hypotheses, such as increasing gradient magnitudes, bootstrapping the training data, simulating an easy-to-hard curriculum learning process, etc. However, from the perspective of Intelligence Test Loss, its effectiveness becomes easy to"}]}