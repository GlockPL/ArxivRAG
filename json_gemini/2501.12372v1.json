{"title": "Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL", "authors": ["Yeounoh Chung", "Gaurav T. Kakkar", "Yu Gan", "Brenton Milne", "Fatma \u00d6zcan"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.\nIn this work, we explore the performance and the latency trade-offs of the extended context window (a.k.a., long context) offered by Google's state-of-the-art LLM (gemini-1.5-pro). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Google's gemini-pro-1.5 achieve a strong performance with 67.41% on BIRD benchmark (dev) without finetuning and expensive self-consistency based techniques.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in LLMs have particularly focused on en- hancing their retrieval and reasoning abilities and expanding their context windows, thus broadening the scope of their potential ap- plications. The ability to process and retain longer sequences of information within the context window empowers LLMs to capture nuanced dependencies and intricate relationships within the input data, offering unprecedented possibilities for improved language understanding and generation. One domain that stands to signifi- cantly benefit from these advancements is Natural Language to SQL (NL2SQL). NL2SQL is a challenging task that entails translating natural language questions into structured SQL queries that can be executed on a database. The inherent ambiguity of natural lan- guage questions, coupled with the necessity for a deep understand- ing of complex database schemas and semantics, makes NL2SQL a very challenging problem [8]. Recent works [4, 10, 26, 28] cre- ated NL2SQL pipelines involving schema linking, self-consistency and self-corrections with many LLM calls. These solutions care- fully create prompts that include various context information, and use Chain-of-Though (CoT) [25, 42] reasoning and/or in-context- learning."}, {"title": "2 RELATED WORKS", "content": "Early approaches in NL2SQL focused on rule-based and semantic parsing methods [20, 37]. With the advent of deep learning, neural network-based models have become dominant in NL2SQL research [34, 41]. These models learn to map natural language questions to SQL queries using large datasets of question-query pairs. These models treat NL2SQL as a machine translation task, using encoder- decoder architectures to translate natural language questions into SQL queries. More recently, LLMs with human-like ability to under- stand and generate texts is leading the significant improvement in accuracy and efficiency in NL2SQL and similar tasks [9, 13, 14, 39].\nLLMs can also learn from a few examples provided in context at inference (few-shot in-context learning). In the NL2SQL do- main, prior research [27, 29] focused on leveraging this few-shot in-context learning (ICL) approach to guide the SQL generation. The few-shot examples for NL2SQL consist of question and SQL pairs and are typically filtered by question embedding similarity to fit within the limited context size. In other problem domains, many-shot ICL enabled by the newly expanded context window is shown to consistently outperform the few-shot approach [2].\nA new long context benchmark [17] shows that the latest long- context LLMs can match the capability of state-of-the-art retrieval systems, while under-performing on compositional reasoning tasks, like NL2SQL against specialized fine-tuned models. This benchmark study covers NL2SQL and uses un-tuned gemini-1.5-pro long context LLM. They leveraged the long context by putting all the DB tables and also fixed few-shot examples as in [9]. While their NL2SQL pipeline under-performs against the original [9] with fine-tuned model on SPIDER 1.0 benchmark[38], our long-context strategy outperforms both [9, 17], by better leveraging the long context with appropriate information. [3, 22] also studied ICL with long context for other non-NL2SQL domains. These studies show that many commercial long context LLMs struggle with processing all the information presented in the long context, where there is a"}, {"title": "3 LEVERAGING LONG CONTEXT FOR NL2SQL", "content": "In this work, we focus on the additional contextual information that we can pass to the extended context window. We hypothesize that leveraging the extended context window and long-context LLMs' strong retrieval capability can make the retrieval step in NL2SQL pipeline less critical; we can also make LLM agent calls to generate, fix & re-write, and to verify more accurate. We explore various contextual information, like SQLite documentation, user hints for clarification, extra column sample values and examples for many-shot in-context learning (ICL).\nThis section is divided into three sub-sections, generate \u2192 fix & rewrite \u2192 verify, corresponding to the three agentic workflow"}, {"title": "3.1 Generate", "content": "Focusing on the value of information rather than squeezing into the limited context window, we can provide a lot more contextual information to address challenges in schema linking and seman- tic errors stemming from ambiguous user question and complex data schema. We explore the following to assist the SQL generation.\nAll database tables for high recall schema linking.\nIn a typical NL2SQL pipeline, the relevant schema elements (table column names and values) are retrieved based on question embed- ding similarity. Accurate schema linking is a key to accurate SQL generation [8]. Prior research works focus on improving schema linking accuracy via more accurate table and column selection [4, 32]. Accurate schema retrieval can uplift the NL2SQL perfor- mance, but there is still a lot of headroom (see Appendix A.2 for more discussion). In fact, table schema retrieval can have missing tables and columns \u2013 which prevents LLMs from writing correct SQL queries.\nHere we improve schema linking via passing all database tables to guarantee that all relevant elements are provided along with lots of extra irrelevant ones. This raises a concern that the model"}, {"title": "3.2 Fix & Rewrite", "content": "We allow the model to fix its errors and rewrite the SQL output based on execution. If the generated SQL query returns an error during execution, it triggers correction modules [4, 33] and retries until a valid SQL is returned for a fixed number of times. [29] showed that LLMs can correct for minor syntax errors in SQL. While the existing self-correction mechanisms and the latest LLMs themselves are effective in fixing SQL syntax errors, they do not address more nuanced semantic errors like invalid literal references and incorrect join paths. Semantic errors may persist after syntax error correction (e.g., referencing a valid literal value for a wrong column) and they are hard to detect without the ground-truth query results. We check for potential semantic errors when the query returns empty result; we ask the model to rewrite the query given that the previous query resulted in an empty result set and with more extensive lists of sample column values. There is a risk of false positive in relying on null (an empty result) for detecting semantic errors. In a case that the ground truth should be null, this would incur additional cost of repeating the task. If the question is not ambiguous, the model should return the same correct null query even with the extra sample column values. If the question is ambiguous and the model re-generates a non-empty result, then we prefer that over the null and move to the next verification step. Seeing the full list of column values enables the model to select the right keyword and columns and to reason about more correct alternative join paths. Passing the full lists of sample column values inflate the context size significantly and requires long context and LLMs that can retrieve accurately over the long context. We evaluate the impact of this long context disambiguation strategy in Section 4.6. This technique is expensive, but it can complement imperfect table & column selection and schema linking, which often leads to sub- optimal NL2SQL performance [4]."}, {"title": "3.3 Verify", "content": "We use the untuned gemini-1.5-pro LLM to verify the correctness of the final output. The model is given the entire database table schema and question (with additional hints if provided) to judge (see Ap- pendix Fig. 10 for the prompt). We believe that fine-tuning a verifier or a picker for self-consistency [28, 35, 40] can further improve the accuracy. While we focus on leveraging the long context ICL for a typical NL2SQL pipeline with simpler techniques (self-correction, verification without fine-tuning), the aforementioned techniques are orthogonal and can be used in conjunction. We demonstrate how this verification step can impact the final performance and the generation cost in Section 5."}, {"title": "4 DETAILED ANALYSIS OF LONG CONTEXT NL2SQL TECHNIQUES", "content": "4.1 Evaluation setup\nWe use public Google Cloud (GCP) Vertex Al gemini API for all our experiments. This is important for reproducibility of our exper- iments and analysis. We use the latest publicly available gemini- 1.5-pro-002 and gemini-1.5-flash-002 checkpoints. gemini-1.5 is a long-context LLM class with up to 2-million token context for pro and 1-million token context for flash. Our test VMs and Vertex AI endpoints are located in the same GCP region (us-central1-b).\nWe report the full pipeline performances on various NL2SQL benchmark datasets (BIRD [21], SPIDER 1.0 [38], KaggleDBQA [15] and BEAVER [5]) in Section 4.2. We run our micro-benchmark experiments using BIRD dev as a representative dataset, since it contains the most number of questions with varying degrees of difficulties spanning over multiple tables with varying schema com- plexity. BIRD is also currently the most popular NL2SQL benchamrk with a leaderboard (all the latest NL2SQL research publications use this dataset to compare their performances).\nFor the performance and cost metrics, we use popular execution accuracy (Ex Acc) and the accumulated tokens per request (L), and/or normalized latency per request (T) as the absolute latency measure behind the public API endpoints can vary from time to time a single gemini-1.5-pro request latency with 8k-token is used as a reference unit (T=1.0); we report floating-point values to one decimal place, ignoring differences of less than 10%. Likewise, we refrain from reporting the monetary cost directly, which depends on the pricing model [11] that is subject to change. Instead, we report the accumulated number of tokens per request L, which is highly correlated to the total monetary cost and invariant to the pricing model change.\nThere is a strong positive correlation between L and T, as dis- cussed in Section 4.8, and we report both in the ablation study. We report the average metrics across requests with tight variance (margin of error/variability less than 2%); if the observed variance is large, we report the mean plus one standard deviation (e.g., L + \u03c3) to bound the majority of requests.\nOur \"full\" pipeline employs all the information and the tech- niques as discussed in Section 3 and Section 5; for the individual experiments we use the \"baseline\" pipeline to study specific ques- tions being addressed. The baseline pipeline includes entire DB schema, sample column values, instructions and hints, but excludes"}, {"title": "4.2 Benchmark evaluation", "content": "Table 2 shows the full pipeline evaluation results with various benchmark datasets. The popular benchmark datasets (BIRD dev, SPIDER 1.0 test, KaggleDBQA test) provide a good set of questions of mixed difficulties over multiple domains and put our perfor- mance in perspective with other state-of-the-arts. Our approach leveraging the extended context of gemini-1.5 model can yield very competitive results in all the benchmarks, without techniques like fine-tuning and self-consistency (Table 1). The newer benchmark for enterprise warehouse data (BEAVER dw) is also interesting be- cause the business-oriented questions and real enterprise tables are more complex and larger \u2013 BIRD is the most complex among the three popular benchmarks, and on the average, its questions have 0.918 JOINs over 6.82 tables/DB; BEAVER questions have 4.25 JOINs over 105.0 tables/DB. BEAVER is a new benchmark and is still in early stage. The best 1-shot Ex Acc on BEAVER (dw + 45 additional questions) from [5] is 2.0 in PL/SQL using gpt-4o with relevant table schema retrieval and 0.0 without retrieval.\nThe normalized generation latency Tg increases with both larger context size and task difficulty (question and schema complexity), as it includes the self-correction and retries (with exponential back-off and max retries per request set to 5). For instance, BEAVER takes much longer to generate on average as our long context pipeline produces context size exceeding the limits (2-million tokens for pro and 1-million tokens for flash) or results in semantically invalid (empty) outputs, in which case the model retries a fixed number of times until it finds a better answer. For this reason, flash can take longer on average than pro.\nIn general, gemini-1.5-pro is better at complex reasoning tasks like NL2SQL, whereas gemini-1.5-flash is more cost-effective (lower pricing per token). We have noticed that the latest releases of the gemini models (released for public access on Sep. 24th, 2024) show significant performance improvements, and most notably, flash now performs more comparably to pro on the select benchmarks. This makes the more cost-efficient flash a very attractive choice for production NL2SQL."}, {"title": "4.3 Schema linking with all database tables", "content": "We explore the impact of passing the entire table schema set col- lected from entire databases. Schema linking is such a critical com- ponent that we strive to achieve a high-recall in production. With a long context model, we can pass a lot more table definitions and schema more loosely than the most relevant top-k tables. As with a standard practice, we retrieve the most relevant tables based on question and full table creation statement with column values and descriptions, embedding similarity. All the benchmark datasets pro- vide specific target DB domain per question, which is different from the production setup we assumed in Figure 2 for the illustration. This narrows the search space, but even so, getting the perfectly ac- curate retrieval results is often infeasible. The goal is to ensure near perfect (higher) recall at the cost of lower precision by providing more tables, if not all, via long context.\nThe results shown in Table 3 demonstrates how the model does not get confused with a large number of mostly irrelevant table definitions in the context; thus providing more tables rather than solely relying on inaccurate table retrieval mechanism could make sense if the context size permits. The result (k = 1) also re-affirms that schema linking is critical, since without all the ground truth tables in the prompt (k = 1) the model cannot generate good quality SQL outputs. On the flip side, providing all tables from the target DB ensures perfect TBR recall and results in higher Ex Acc. This means that TBR recall is more important than precision or accuracy as the model generation quality does not degrade much with many irrelevant tables (93.12 irrelevant tables on the average). all DB tables use up to 13 tables to each request depending on the target DB; the average number of tables across different DBs is 6.82. Notice that the execution accuracy result for all dev tables (100 tables from BIRD dev) is slightly higher (within 5% margin of error we use medium temperature for LLM generation and Ex Acc vary slightly per run), which indicates that the results are comparable. However, the average context size increases significantly as with all dev tables.\nOn the one hand, it is better to include as many tables (e.g., user history, all other tables within the target DB) to guarantee high TBR recall, which should be possible with long-context LLMs. On the other hand, the latency cost of processing larger context is higher (see Fig. 5) and to some extent, adding more irrelevant tables would not help and justify the increase cost. Furthermore, some production setting require processing hundreds of very wide tables, which prevents putting all the tables in the context prohibitively"}, {"title": "4.4 Many-shot ICL with example selection", "content": "Here we evaluate the impact of many-shot in-context learning (ICL) using examples selected from BIRD train dataset. The examples are retrieved based on the question embedding similarity.\nTable 4(a) shows that the train examples did not provide much boost and instead resulted in worse performance. This means that the model's ability to learn from similar examples from BIRD train is quite limited and can be distracted from them. Also notice that the impact of distraction seems to be limited, say going from 1 similar example (11k tokens) to 100 similar examples (50k tokens). To examine the model's sensitivity to the many irrelevant train examples further, we injected 1 ground-truth example from BIRD dev in the \"middle\" of the example blob (the average context size is omitted as it should be almost identical to the previous table). While the model can select the ground truth example among many other irrelevant ones, the presence of those irrelevant train examples can confuse the model as opposed to just having that ground truth. The recall matters more than precision, but the model is still sensitive to bad examples (low precision) to a limited degree - note that the accuracy with 100 random examples and 1 ground truth is much higher than no examples at all.\nIt is also important to note that the model is robust to the position and ordering of the ground-truth example. [23] showed that LLMs tend to put more emphasis on the information in the beginning and the end, where the order of information in context matters and calls proper ranking and ordering for RAG. As in Table 4(b-c), where"}, {"title": "4.5 Synthetic example generation VS. example selection", "content": "Fig. 4 illustrates how synthetic examples improve the quality of generated SQL outputs and compare with other types of examples retrieved from different example pools. As with a standard practice, the example retrieval was done via question embedding similarity using gecko embedding model. Note that using the ground-truth SQL queries from dev dataset provides performance ceilings, but is not allowed without an oracle (i.e., passing the answer as part of the request prompt). The gap between and indicates that providing the GT as one of the many examples is better than retrieving relevant examples from the pool that contains the GT, due to the imperfect retrieval process. \u03c3(train) is the common practice where the relevant examples are selected from available datasets, such as the training dataset (train). Providing relevant train examples does as least as good as the base- line (no example) with a slight improvement when fewer than 200 shots are provided. Interestingly, synthetic examples provide significant uplift over \u03c3(train). The gap between \u03c3(synthetic) and original synthetic indicates that one can be more economical with the context size (achieving higher per- formance with a fewer examples) by filtering the relevant synthetic examples."}, {"title": "4.6 Self-correction with entire schema and more column values", "content": "Table 5 shows how LLM's self-correction can help with SQL genera- tion and its refinement. We compare the LLM-based self-correction (SC) strategy with the ones with more advanced techniques, disam- biguation and filtered schema. disambiguation is the long-context enabled technique where an extended lists of sample column values are shown to the model upon detecting empty results from syntacti- cally correct SQL; filtered schema uses the column selection results mapping relevant schema elements per user request, as prescribed in [32]. Both disambiguation and filtered schema can help LLMs correct for any invalid literal and column column references. One potential issue with disambiguation is that the extra column values can distract the model, especially in the case of false positive detec- tion (e.g., the answer should be null). We empirically show that it helps more and the technique performs slightly better or at least as good as SC. While comparable in performance, the techniques vary in terms of the cost. The accumulated average number of prompt tokens used for correction increases significantly for disambigua- tion, as with the full schema and extra sample column values. In general. accurate column selection can uplift NL2SQL performance, and the filtered schema does not increase the context size during correction.\nHowever, column selection process to prepare filtered sche\u0442\u0430 comes with retrieval cost (see Appendix A.2 for more discussion). filtered schema is a great strategy, if accurate column selection pro- cess is available. In our final long-context pipeline, we use SC + disambiguation to avoid extra retrieval/selection step and do every- thing in-context. We report the mean tokens/request plus 1-S.D, since the variance is large and to show how much tokens we use for the majority (68%) of the requests."}, {"title": "4.7 In-context learning with SQLite documentation", "content": "We test if the model can actually learn and improve its generation quality with SQLite documentation. We downloaded the entire SQLite documentation from its official homepage [1]. The original documentation comprises 16 million tokens across 792 HTML files, each covering a distinct theme. We applied two strategies to split the entire documentation into chunks so that we can augment the prompts with relevant information for ICL. The coarse-grained chucking strategy split the documentation by HTML file, while the fine-grained chucking strategy further separate each file by sections which ends up with 4125 smaller chunks in total. Similar to example retrieval, we embed the natural language questions and document chunks into vectors with the Gecko text embedding model [18] and employ nearest neighbor search to identify the most relevant document chunks to the given question. We could further compress the context via summarization, but decided not to, since it drops important details, like syntax rules and examples.\nWe show the execution accuracy and context size for document retrieval in table 6. The retrieval is challenging because the natural language question does not reveal the full structure and features of the corresponding SQL query; furthermore writing a correct SQL query requires a combination of concepts from multiple sections from the documentation. However, we believe that the less than ideal retrieval process is not the reason why the documentation adds little to no value to generation. It is actually the nature of the errors that the LLMs make, which are mostly semantic errors (e.g., executable queries returning semantically irrelevant results).\nThe Gemini-1.5-Pro model is already well-versed with the SQLite syntax and function details that are illustarted in the documenta- tion. The SQLite documentation is likely already in the pretrain mixture. Furthermore, the model can self-correct itself for syntactic errors based on the error message and without any documenta- tion or examples. A recent work [32] based on GPT-4 also points out that there is no obvious errors based on simple syntax misuse (e.g., function names), but rather more subtle formatting (e.g., does not follow required date format) and semantic errors (output SQL is executable but incorrect w.r.t. the request). Reading the SQLite documentation would not help with such semantic errors be- cause the model is already producing executable queries that are semantically incorrect. While there is no accuracy gain, putting the"}, {"title": "4.8 Long context and latency relationship", "content": "In this section, we look at the context size (tokens) and latency relationship. Fig. 5 illustrates that there is near-linear relationship (strong positive correlation R\u00b2=92.6). It is interesting to note that the latency and the variance starts increasing significantly beyond context size >32k. The larger context LLMs require inference com- putation scattered across more host and accelerators, which incurs more queueing delay. While we expect the smaller gemini-1.5-flash variant to have lower latency, due to this queueing delay and re- source allocation differences it also suffers from increased latency at the long tail. The generation latency increases significantly (>> 4 seconds) with larger context size (>32k tokens), thus we should increase the context only if there is a clear gain in generation qual- ity. The average measures between context size and latency are linearly correlated, which makes it easy to model.\nTable 7 illustrates the average latency difference for the two gemini-1.5 model variances, large and more expensive pro and small and cheaper flash. The difference in verification latency is more pro- nounced because the context size is smaller, whereas the generation with larger context pro and flash requires more prefill computation, which results in increased and similar queueing delay. Both models experience increased and similar average generation latency. For"}, {"title": "5 ABLATION STUDY", "content": "Overall, we focus on identifying useful information for improving NL2SQL performance - and measure the impacts of the contex- tual information in terms of both performance and cost. We ex- clude the rules and SQLite documentation chunks from the ablation study, since their separate contributions are negligible. The rules are fixed and included as part of the instructions. We also skip popular techniques, like Chain-of-Thought (CoT) prompting and self-consistency. CoT can help LLMs by breaking down a complex task into sub-tasks enabling multi-step reasoning to solve complex reasoning task, like NL2SQL [25, 31]; however, we observe that CoT does not improve the final output quality while increasing the context size by a few thousands tokens.\nTable 8 reports the generation performance (Ex Acc) along with accumulated context size (tokens/req) and latency (sec/req) per user request. The study is done adding one technique or extra contex- tual information at a time. The techniques and information that are more commonly used for NL2SQL are added earlier (schema, hints, column samples and self-correction, respectively) and our long-context pipeline specific components are added later (disam- biguation and synthetic examples).\nBecause the latest long-context LLM, gemini-pro-1.5, has really strong retrieval capability, we see the overall performance increase with more added information. However, when we break down the questions from BIRD dev into sub-categories by difficulties, we also see that some information are more helpful for simpler or harder requests versus the others. For instance, providing extra sample column values (or providing them all for disambiguation) is more helpful for challenging questions, where the question is more nuanced and/or ambiguous that seeing exact column values can help pick the right column and join paths. Synthetic examples show the opposite where it helps with simple and moderate questions, but hurt the challenging questions. Synthetic examples are generated to illustrate common SQL features and clauses using the schema elements from the target DB [28]; the examples tend to be simpler and have less ambiguous mappings from the question and the SQL as the examples questions are generated by the same model describing its generated SQL. The hints are also interesting in that it appears to be one of the most critical ingredients for accurate SQL generation and it benefits moderate questions the most. BIRD datasets contain hints as to clarify nuanced concepts (e.g., \"eligible free rate for K-12\") with a correct expression (\"Eligible free rate for K-12 = 'Free Meal Count (K-12)' / 'Enrollment (K-12)'\"), and moderate queries often consist of more nuanced concepts that require such clarifications.\nOne key aspect of leveraging long context is the cost. Table 8 illustrates how each long context information or technique con- tributes to the overall context size as the latency, which are highly correlated (we discuss the near-linear relationship between the con- text size and the latency in Section 4.8). For both context size L and latency T, we report the average plus 1-S.D. because the variance of the measures are quite high given the retries and self-correction mechanisms. This way, the reported measures capture the majority"}, {"title": "6 DISCUSSION AND LIMITATIONS", "content": "6.1 Error analysis\nTo gain a deeper understanding of the discrepancies between gen- erated SQL and ground truth SQL, we randomly sampled 20% of the instances where our baseline output deviated from the provided golden SQL in the BIRD dev. Figure 6 presents a breakdown of the observed error categories and their respective proportions within this sampled subset.\nThe errors are categorized hierarchically. The inner ring of the chart depicts three high-level classifications: \"Vague Question\" rep- resenting instances where the generated SQL was broadly correct but was marked incorrect due to the vagueness of the correspond- ing NL question; \"Incorrect Generated SQL,\" indicating cases where"}, {"title": "6.2 Further performance improvement via self consistency", "content": "Self-consistency is another very popular technique for NL2SQL [7, 9, 10, 28, 32, 35], where multiple candidates are generated and the most consistent answer or the best one is selected by a fine-tuned picker. As evidenced by the latest state-of-the-arts [10, 28] on the BIRD leaderboard using this technique, generating multiple candidates, with different generators (a.k.a. NL2SQL pipelines) and each generator trying multiple times, and choosing the best one has become a crucial technique for achieving high accuracy in NL2SQL. One of the state-of-the-arts, CHASE-SQL, in Table 1 uses our long context pipeline as one of the three candidate generators to yield one of the strongest results on the BIRD leaderboard. One caveat of self-consistency is that this can quickly become expensive in terms of latency and the number of LLM calls. Self-consistency is orthogonal to our work, and can further improve the NL2SQL performance if used together with our pipeline. Table 9 illustrates how repeatedly generating multiple output candidates from our long context pipeline can perform (generation temperature set to 0.5). With an oracle picker (i.e., at least one of the candidates is correct per question) we can uplift the accuracy beyond 70%. In [10, 28] the candidate sets are generated using multiple pipelines as different pipelines with different strategies result in more diverse outputs, boosting the final accuracy of self-consistency variants even further.\nSelf-consistency is orthogonal to the use of long context and outside the scope of this paper. In this work we focus on the quality of individual output candidate and study the performance impli- cations of leveraging long context (extra contextual information), which has not been explored for NL2SQL before."}, {"title": "6.3 Long context in production and limitations", "content": "Our study reveals (Table 8) that a good chunk of requests (roughly 68%) from BIRD dev can reach 400k accumulated tokens and take up to 130 T units. This would likely be prohibitive for most production services. And the cost will be even higher with techniques like self- consistency. Long context is a great means to add more appropriate information, but it has to be used with some care. To leverage long context properly in production, one may pick and choose which extra information and techniques are appropriate based on our analysis. For instance, one can skip disambiguation and online example generation to bound the latency of the majority of requests to at around <1.5 normalized seconds. The example can be generated offline and in Section 4.5 we show that example selection works well with synthetic examples. Using flash can further reduce the cost, roughly -20% in latency and -94% in dollars per request given the current pricing and the observed latency.\nOne limitation of our study, as we study the impact of long context and extra contextual information on NL2SQL, is missing benchmark datasets that model certain enterprise use cases where there are hundreds of very wide tables with thousands of columns. Also there is this aspect of less descriptive and similar table and column names that render schema linking much more challenging. Such use cases would be challenging in general for schema linking and also exacerbates the costs of long context techniques as with"}, {"title": "7 CONCLUSION", "content": "In this work, we explored the potential of leveraging the extended context window offered by Google's gemini-1.5-pro for NL2SQL. Our findings demonstrate that long-context LLMs can effectively utilize the additional context, achieving strong performances on various benchmarks (Table 2), including of 67.41% on the BIRD benchmark without fine-tuning or computationally expensive self-consistency techniques. This performance highlights the robustness of the long- context models in retrieving and reasoning over extensive con- textual information. Specifically, we explore the performance im- plications of including entire DB schema details, user provided hints, sample column values, synthetic examples for many-shot ICL, for accurate SQL generation. We also show that self-correction and verification can improve accuracy further at the cost of in- creased latency and accumulated tokens. Overall, we observe that gemini-1.5-pro exhibits a strong retrieval capability over the ex- tended context window, even with mostly irrelevant information. We also see that the long context model is more robust to or no longer \"lost in the middle\" [23]).\nOur study is a unique example of how enhanced capabilities of LLMs, in this case the extended large context size, impact the way we approach the NL2SQL problem. To the contrary to prior works where we focused on squeezing the filtered information onto limited context size, we explored the potential of providing more useful information to the model. And we showed that more infor- mation can be useful in NL2SQL, helping to overcome the semantic issues (SQL syntactic issues for the common SQL dialect are already well addressed by the base model). We also investigated the cost implication of using the long context techniques and conclude that it can be complementary and is more efficient with accurate schema and example retrievals and with a more emphasis on recall. In fact, perfect table schema retrieval would yield stronger performance narrowing down the schema linking search space during generation [5, 8, 32], but it is very difficult to achieve in practice.\nIn a world where the retrieval and ranking are less than perfect, providing more information and relying on the long context models' strong retrieval capability can be a viable alternative strategy, while being more expensive. Improving the cost efficiency of long context model serving would be an important area of research to make long context more practical."}, {"title": "A APPENDIX", "content": "A.1 Table retrieval simulation\nFor a realistic table retrieval simulation, we ran the BIRD bench against BigQuery's SQL Code Assist feature [12]. The result differs from the standard BIRD bench setup, as it mirrors the challenges of serving production users of BigQuery. Conventionally when running the BIRD bench, each query is linked to a single database containing up to 13 tables. BigQuery operates differently; users may ask questions about any table that they have access to, and BigQuery's retrieval system uses user interaction histories to re- duce the search space. For realistic results the user interactions were seeded to match the distribution of queries in BigQuery's production traffic, with a bias towards session-starting queries (less repeated queries), as these present a more difficult retrieval problem.\nFor each example in the benchmark, table retrieval operates as follows: 1) The simulated user view and query interactions narrow the search space down to 1-100 candidates; 2) The candidates are embedded and re-ranked with Gecko [18] for relevance to the user query; 3) A fixed top-k candidates are passed on to the NL2SQL stage.\nAt the time of test, the recall of the first stage was 82%. As such, 82% is the maximum attainable end-to-end recall, and represents a near perfect result from the re-ranking stage.\nA.2 Impact of good column selection"}, {"title": "4.  Prompt templates", "content": "The following are the generation, correction and verification prompt templates."}]}