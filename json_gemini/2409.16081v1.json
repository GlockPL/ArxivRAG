{"title": "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition", "authors": ["Zhili Lai", "Chunmei Qing*", "Junpeng Tan", "Wanxiang Luo", "Xiangmin Xu"], "abstract": "Utilizing functional near-infrared spectroscopy (fNIRS) signals for emotion recognition is a significant advancement in understanding human emotions. However, due to the lack of artificial intelligence data and algorithms in this field, current research faces the following challenges: 1) The portable wearable devices have higher requirements for lightweight models; 2) The objective differences of physiology and psychology among different subjects aggravate the difficulty of emotion recognition. To address these challenges, we propose a novel cross-subject fNIRS emotion recognition method, called the Online Multi-level Contrastive Representation Distillation framework (OMCRD). Specifically, OMCRD is a framework designed for mutual learning among multiple lightweight student networks. It utilizes multi-level fNIRS feature extractor for each sub-network and conducts multi-view sentimental mining using physiological signals. The proposed Inter-Subject Interaction Contrastive Representation (IS-ICR) facilitates knowledge transfer for interactions between student models, enhancing cross-subject emotion recognition performance. The optimal student network can be selected and deployed on a wearable device. Some experimental results demonstrate that OMCRD achieves state-of-the-art results in emotional perception and affective imagery tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Emotion recognition is an important research task in affective computing, which aims to represent and explain human mental states by acquiring psychological or non-psychological signals. Accurate emotion recognition can help us better sense a person's brain activity and thoughts. Given contemporary society's challenging employment and living conditions, emotion recognition technology is increasingly prevalent across diverse industries. Among them, common applications include health diagnostics, safe driving, medical services, and human-computer interaction.[1-5].\nTo this end, commonly processed data for affective computing include facial videos, body movements, speech, text scales, and physiological signals [6\u20139]. Comparing these data sources, physiological signals come from the human brain nervous system, less influenced by subjective consciousness or deception, and offer more dependable insights into human mood shifts. Functional Near-Infrared Spectroscopy (fNIRS) stands out as a potential method for neural signal capture and visualization, to create a real-time communication link between the human brain and external devices. fNIRS signals indicate brain activity by tracking alterations in oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (HbR) levels across various brain areas. Two primary features exist in the fNIRS signal: region-level features representing activation levels in different regions, and channel-level features showing the overall change in blood oxygen concentration across time.\nWhile fNIRS has been crucial in measuring physiological signals, there remain numerous challenges in advancing fNIRS data research. Current deep learning-based fNIRS research primarily concentrates on distinct tasks, like Brain-Computer Interface (BCI) [10] and mental health diagnosis [11], while exploration in fNIRS emotion recognition is still in the early stages of development. With the rapid development of portable wearable physiological devices, lightweight network models can maintain reasonable measurement accuracy while adapting to the resource constraints of wearable devices and application requirements. Finally, the fNIRS recordings related to emotions exhibit substantial inter-subject variabilities due to the physiological differences among individuals. This is a considerable challenge for cross-subject emotion recognition.\nTo address the issues mentioned above, this paper proposes a novel Online Multi-level Contrastive Representation Distillation framework for fNIRS emotion recognition, named OMCRD. We explore and experiment with fNIRS emotion recognition to improve deficiencies in this field. Specifically, the OMCRD framework employs a one-stage online multi-student network mutual Knowledge Distillation (KD) strategy with the multi-level (region-level and channel-level) fNIRS feature extractor, which exploits the mutual learning between multiple lightweight student networks during training to eliminate the dependence on complex teacher models. Furthermore, we also propose the Inter-Subject Interaction Contrastive Representation loss (IS-ICR). \"Inter-Subject\" means that fNIRS signals collected from different subjects facing the same stimulus are treated as the same class, facilitating the learning of their similarities. \"Interaction\" refers to establishing collaborative learning relationships across peer networks. Different networks can communicate with each other and optimize parameters during training. This ensures that each network learns additional contrastive representation knowledge from other peer networks. This approach facilitates the deployment of the final best-performing student model to a wearable fNIRS physiological monitoring device.\nIn summary, our primary contributions are as follows:\n(1) We propose a novel fNIRS emotion recognition method (OM-CRD), which is an online multi-level contrastive representation distillation framework with the multiple student networks for learning multi-view fNIRS features and reducing model complexity. To our knowledge, it is the first work to apply knowledge distillation in fNIRS emotion recognition.\n(2) A novel Inter-Subject Interaction Contrastive Representation loss (IS-ICR) is proposed, which is a multi-level contrastive representation learning. IS-ICR effectively enables different student models to obtain region-level and channel-level knowledge across subjects and optimizes multi-view features between student models.\n(3) Extensive experiments are conducted on a publicly available fNIRS dataset to assess the proposed method. The results demonstrated the effectiveness and robustness of the proposed framework."}, {"title": "2 RELATED WORK", "content": "In this segment, to enhance comprehension of the proposed approach, we mainly introduce several related works: physiological signals emotion recognition, knowledge distillation, and contrastive learning."}, {"title": "2.1 Physiological Signals Emotion Recognition", "content": "Recently, physiological signals emotion recognition has gradually become a research hotspot in affective computing. Unlike emotion recognition of behavioral modalities [6, 8, 12], Physiological signals are challenging to disguise and cannot be intentionally or consciously controlled. Physiological signals include electroencephalogram (EEG) [9, 13], electrodermal activity (EDA) [14], electrocardiogram (ECG) [15, 16], Galvanic Skin Response (GSR) [17]. They are frequently used in emotion recognition systems because they can mimic the real physiological changes of human emotions. Like, Xiao et al. [16] enhanced emotion recognition performance by acquiring spatial-temporal representations of various ECG regions and implementing a dynamic weight allocation layer to modify the influence of each ECG region. To ensure the recognition of emotional states in a long-time series, Susanto et al. [17] analyzed GSR signals based on a 1D convolutional neural network and a residual bidirectional gated loop unit. Meanwhile, Yu et al. [14] investigated the performance of different deep neural networks on a subject-independent EDA-based emotion classification task. Gong et al. [9] proposed a spatio-temporal two-stream fusion network based on an attention mechanism for EEG-based emotion recognition.\nNotably, as an emerging non-invasive brain imaging technology, fNIRS has gradually gained attention in emotion recognition research due to its advantages of flexibility, ease of operation, and low cost. Si et al. [18] combined the CNN branch and the statistical branch to construct a dual-branch joint network for cross-subject fNIRS emotion recognition, which is the first introduction of deep learning techniques in the field. Chen et al. [19] presented a pioneering wearable bimodal system. The system combined fNIRS and EEG technology and used the temporal convolutional network to identify implicit emotional states in real-time. fNIRS is crucial for processing emotional brain responses and shows promise in emotion recognition. However, due to minimal AI research and challenges in feature extraction, algorithm accuracy is currently very low. Implementing a multi-student network strategy for optimizing training with multi-view features could be beneficial."}, {"title": "2.2 Knowledge Distillation", "content": "The Knowledge Distillation (KD) method was initially proposed by Hinton et al. [21], focusing on distilling knowledge from a large teacher model to improve a smaller student network. KD methods facilitate the deployment of large-parameter models in practical applications. In the KD research based on physiological signals, Gu et al. [22] designed a novel frame-level teacher-student framework, achieving the best performance in subject-independent EEG emotion recognition. Wang et al. [23] attempted to distill the knowledge from ResNet34 into a smaller model ResNet8 in EEG emotion recognition tasks to achieve performance improvement and model compression. Liu et al. [24] proposed a cross-modal (GSR and EEG) knowledge distillation framework, effectively transferring heterogeneous and interactive knowledge from multimodal to unimodal GSR models, enhancing emotion recognition performance while reducing dependence on EEG signals.\nHowever, these methods employ an offline learning strategy and require pre-training of a suitable and powerful \"teacher\" model. Moreover, this two-stage training process is very time-consuming, which is challenging in real-time fNIRS emotion recognition tasks. Compared to traditional KD, online KD aims to improve performance by leveraging collaborative learning across multiple student networks. Online KD employs one-stage training and does not rely on a pre-trained teacher model. Therefore, inspired by the success of online KD in the image domain [25-28], we design a novel online distillation framework to ensure knowledge transfer and interaction optimization among multi-student networks."}, {"title": "2.3 Contrastive Learning", "content": "The idea of contrastive learning is to learn which data pairs are similar or different, thereby acquiring the general features of the dataset. It is a form of self-supervised learning algorithm. It has achieved outstanding performance in diverse fields, including Computer Vision (CV) [29], Natural Language Processing (NLP) [30], and bioinformatics [31]. Khosla et al [32] extended contrastive loss to supervised environments, effectively using label information to extract more discriminative representations. Recently, contrastive learning has also been gradually applied to physiological signals studies. For instance, Soltanieh et al [33] investigated the effectiveness of various data augmentation techniques for contrastive self-supervised learning of ECG signals. This enables the model to enhance learning of the generalized ECG representation, thereby enhancing the accuracy of arrhythmia detection. Kalanadhabhatta et al. [11] introduced a multi-task supervised contrastive learning method to extract fNIRS, GSR, and facial video embeddings for early childhood mental disorder identification. Ensuring high-quality brain signal acquisition with portable devices involves addressing the cross-subject effects and device parameters. Shen et al. [34] introduced a contrastive learning-based approach for cross-subject EEG emotion recognition to enhance generalization ability. Wang et al. [35] utilized contrastive learning for cross-subject cognitive workload prediction using fNIRS signals. Limited research exists on cross-subject emotion recognition in human-computer interaction based on fNIRS, with low algorithm recognition rates and deployment challenges. To address the problem of cross-subject fNIRS emotion recognition, we propose a cross-subject multi-level contrastive learning strategy. It enables the model to aggregate region-level or channel-level representations of different subjects belonging to the same class at training time, while optimizing the complementary properties between peer models."}, {"title": "3 METHODOLOGY", "content": "3.1 Overview of the proposed framework\n3.1.1 Notation.\nAs shown in Figure 1, each peer/student network f(.) consists of a multi-level fNIRS feature extractor (.) and a linear classifier FC(.). The fNIRS signal x is mapped to the logit vector z through the network f, i.e.\n$$z = FC(q(x)) = f(x).$$\nSpecifically, the input x is processed by the extractor q to obtain the region-level embedding $e^{rg}$ and the channel-level embedding $e^{ch}$, which are concatenated and then fed into FC. For the intermediate feature embeddings $e^{rg}$ and $e^{ch}$, we introduce two projection modules $pr^{g} (.)$ and $ch (\u00b7)$ respectively. The process is as follows:\n$$v^{rg} = \\phi_{rg} (e^{rg})$$"}, {"title": null, "content": "$$v^{ch} = \\phi_{ch} (e^{ch})$$\nwhere $pr$ is composed of a linear layer and the $l_2$-normalization to linearly transform these embeddings into contrastive embeddings $v^{rg}, v^{ch} \\in R^d$, and d is the embedding size. The embeddings $v^{rg}$ and $v^{ch}$ are used for the calculation of the proposed IS-ICR.\n3.1.2 Training and deployment.\nDuring training, M (M \u2265 2) peer networks are optimized together. Notably, to learn diverse representations, all identical networks {$f_m$}$_{m=1}^M$ are assigned different weights initially, a crucial factor for mutual learning success. In the evaluation phase, projection modules $ \\phi $ are omitted, allowing the assessment of each network independently. The optimal performing network can be selected for final deployment. As the structure of the preserved network is identical to the others, there are no extra computational expenses during testing."}, {"title": "3.2 Multi-level fNIRS Feature Extractor", "content": "The fNIRS signal has two important features: region-level features and channel-level features. Region-level features represent the spatial correlation between different fNIRS signal channels. Over a while, changes in blood oxygen concentration from different channels often exhibit different dependencies, such as the higher correlation between channels from similar functional brain regions or neighboring spatial locations. Channel-level features represent the temporal continuity of blood oxygen concentration changes within multiple fNIRS signal channels. We designed two feature extractors to capture these two types of features. One is a hybrid feature extractor based on CNN+LSTM, and the other is a feature extractor based on Transformers.\n3.2.1 CNN+LSTM extractor.\nFor the hybrid feature extractor with CNN and LSTM, we design a CNN module $ \\phi_{CNN}(\u00b7)$ to extract region-level features $e^{rg}$. The CNN mainly consists of two 1D convolutional layers (with kernel sizes of 50 and 10, strides of 10 and 2, and output channels of 32 and 16, respectively) and a fully connected layer. Simultaneously, we also design an LSTM module \u222eLSTM (\u00b7) containing two LSTM layers (with 64 hidden units) to extract channel-level features $e^{ch}$. The formula is defined as follows:\n$$e^{rg} = \\phi_{CNN} (x)$$\n$$e^{ch} = \\phi_{LSTM} (x)$$\nThe resulting representation vectors $e^{rg}$ and $e^{ch}$ serve as inputs to the linear classifier. It is worth noting that to accommodate the CNN+LSTM feature extractor, the input is $x \\in R^{2n\u00d7T}$, where n is the number of HbO (HbR) channels and T is the signal length.\n3.2.2 Transformer extractor.\nFor the Transformer-based feature extractor, with $x \u2208 R^{2\u00d7n\u00d7T}$ as input. Inspired by iTransformer [36], each fNIRS channel's complete sequence serves as a token for region-level modeling. Moreover, building on the idea in Informer [37], the information from multiple fNIRS channels at the same timestamp acts as a token for capturing channel-level feature information.\nSpecifically, the input x is first reshaped into $x^{rg} \u2208 R^{n\u00d7(2\u00b7T)}$ and $x^{ch} \u2208 R^{T\u00d7(2n)}$. Then $x^{rg}$ and $x^{ch}$ are fed into the region-level"}, {"title": null, "content": "$$\\phi_{REB} (\u00b7)$ and the channel-level encoding branch $ \\phi_{CEB} (\u00b7)$, respectively. The process is as follows:\n$$e^{rg} = \\phi_{REB} (x^{rg})$$\n$$e^{ch} = \\phi_{CEB} (x^{ch})$$\nBoth encoding branches consist of a linear projection layer, positional encoding, the Transformer encoder [38], and a Global Average Pooling (GAP) layer [39] (see figure 2 for details)."}, {"title": "3.3 Learning Objectives", "content": "Given a sample set $D = {(x_i, y_i)}_{i=1}^N$ containing N instances (sample from different subjects) from C classes, where $y_i \u2208 {1, 2, ..., C}$. D as the input to M networks.\n3.3.1 Learning from labels.\nEach network is optimized by Cross-Entropy loss (CE) between probability distributions and hard labels. The probability of class c for sample $x_i$ given by the m-th network is calculated as:\n$$P_m(x_i) = \\frac{exp(z_m^c)}{\\sum_{c=1}^C exp(z_m^c)}$$\nwhere the logit $z_m^c$ from $f_m$ is the input of the \"softmax\" layer. Therefore, the CE loss of the m-th network is computed as:\n$$L_{CE}^m = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^C I_{y_i=c}log (p_m (x_i))$$\nwhere $I_{y_i=c} \u2208 {0, 1}$ represents the indicator function, which takes 1 when $y_i = c$ holds, and 0 otherwise. Overall, CE loss of M networks is:\n$$L_{CE} = \\sum_{m=1}^M L_{CE}^m$$\n3.3.2 Distillation from soft labels.\nWe simply construct an \"online teacher\" by softening the true label with temperature T. The probability distribution of the soft label contains higher quality logit knowledge. It is as follows:\n$$p^o(y_i) = \\frac{exp(y_i^c/T)}{\\sum_{c=1}^C exp(y_i^c/T)}$$\nwhere $y_i^c \u2208 {0, 1}$ is the c-th element in the one-hot encoding $y_i$ of label $y_i$. The soft probability of class c corresponding to sample"}, {"title": null, "content": "from network $f_m$ is:\n$$p_m(x_i) = \\frac{exp(z_m^c/T)}{\\sum_{c=1}^C exp(z_m^c/T)}$$\nTo measure the alignment between the predictions of the network $f_m$ and the soft labels, while also improving generalization, we adopt the Kullback Leibler (KL) Divergence. The KL loss of $f_m$ is calculated as follows:\n$$L_{KL} = D_{KL} (p^o(y_i)||p_m(x_i)) =  \\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^C p^o(y_i) log( \\frac{p^o(y_i)}{p_m(x_i)} )$$\nThus, the total KL loss of M networks is calculated as follows:\n$$L_{KL} = \\sum_{m=1}^M L_{KL}^m$$\n3.3.3 Inter-subject interaction contrastive representation loss.\nDuring network training, contrastive learning techniques can push positive pairs from the same class as the anchor closer together in the embedding space, while pushing negative pairs from different classes further apart. To this end, we propose a novel Inter-Subject Interaction Contrastive Representation Loss (IS-ICR). This loss is designed to enhance the representation similarity between samples of the same class from different subjects (as shown in Figure 3) while capturing the general distribution of the same class of emotions. Notably, IS-ICR also makes full use of the information interaction between various peer networks to learn better representations through cross-network interaction."}, {"title": null, "content": "Given two networks $f_a$ and $f_b$ for illustration, where a, b \u2208 {1, 2, \u2026\u2026, M}, a \u2260 b. The embeddings generated from D are {$v_i^a$}$_{i=1}^N$ and {$v_i^b$}$_{i=1}^N$, respectively. From the perspective of $f_a$ to $f_b$ (as shown in Figure 3), given the anchor embedding $v_i^a$ of instance $x_i$ from $f_a$, and the embeddings in the contrastive embeddings {$v_j^b$}$_{i=1}^N$ that is of the same class as $v_i^a$ are considered as positive embeddings, otherwise as negative embeddings. Since the embeddings are pre-processed through $l_2$-normalization, the dot product measures the similarity distribution between the anchor and contrastive embeddings. The formula is defined as follows:\n$$L_{CR}^{ab} = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{y_i=yj} log ( \\frac{exp(v_i^a \\cdot v_j^b / \u03c4)}{\\sum_k exp(v_i^a \\cdot v_k^b / \u03c4) + Z_i})$$\n$$Z_i = \\sum_{yi \\neq yk} exp(v_i^a \\cdot v_k^b / \u03c4)$$\nwhere $y_j$, $y_k$ denote the labels of anchor sample $x_i$ from $f_a$ and contrastive samples $x_j$ and $x_k$ from $f_b$, respectively. $N_{y_i}$ represents the number of samples whose label is $y_i$ in D. \u03c4 is the temperature coefficient. Intuitively, each network can benefit from Eq.(15) by gaining additional comparative knowledge from other networks. Generic inter-subject fNIRS representations that capture specific emotions using network clusters can help generalize to unseen emotional stimuli or unseen subjects to predict emotional categories. When scaling to M networks, the overall loss function can be listed as follows:\n$$L_{CR} = \\sum_{1<a<b<M} (L_{CR}^{ba} + L_{CR}^{ab})$$\nConsidering region-level and channel-level representations, two losses $L_{CR}^{org}$ and $L_{CR}^{och}$ are obtained.\n3.3.4 Overall loss.\nThe overall loss function L is a weighted sum of the loss terms mentioned above, which is illustrated as follows:\n$$L = L_{CE} + T^2 L_{KL} + \u03b1L_{CR}^{org} + \u03b2L_{CR}^{och}$$"}, {"title": "5 CONCLUSION", "content": "In this study, we propose a novel method (OMCRD) for lightweight the fNIRS emotion recognition model. The framework allows multiple students to jointly optimize the network and teach cross-subject knowledge at region and channel levels interactively. Furthermore, the IS-ICR loss boosts the similarity between identical emotional representations from various individuals, notably improving the model's capability for cross-subject adaptive recognition. Our framework shows effective emotion recognition, robustness across subjects, and deployability in extensive experiments. The proposed OMCRD is a general framework for multi-channel fNIRS sequences that can be extended to large-scale models in fields like mental health diagnosis and cognitive assessment."}]}