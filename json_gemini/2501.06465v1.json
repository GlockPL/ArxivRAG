{"title": "MedCT: A Clinical Terminology Graph for Generative AI Applications in Healthcare", "authors": ["Ye Chen", "Dongdong Huang", "Haoyun Xu", "Cong Fu", "Lin Sheng", "Qingli Zhou", "Yuqiang Shen", "Kai Wang"], "abstract": "We introduce the world's first clinical terminology for the Chinese healthcare community, namely MedCT, accompanied by a clinical foundation model MedBERT and an entity linking model MedLink. The MedCT system enables standardized and programmable representation of Chinese clinical data, successively stimulating the development of new medicines, treatment pathways, and better patient outcomes for the populous Chinese community. Moreover, the MedCT knowledge graph provides a principled mechanism to minimize the hallucination problem of large language models (LLMs), therefore achieving significant levels of accuracy and safety in LLM-based clinical applications. By leveraging the LLMs' emergent capabilities of generativeness and expressiveness, we were able to rapidly built a production-quality terminology system and deployed to real-world clinical field within three months, while classical terminologies like SNOMED CT have gone through more than twenty years development. Our experiments show that the MedCT system achieves state-of-the-art (SOTA) performance in semantic matching and entity linking tasks, not only for Chinese but also for English. We also conducted a longitudinal field experiment by applying MedCT and LLMs in a representative spectrum of clinical tasks, including electronic health record (EHR) auto-generation and medical document search for diagnostic decision making. Our study shows a multitude of values of MedCT for clinical workflows and patient outcomes, especially in the new genre of clinical LLM applications. We present our approach in sufficient engineering detail, such that implementing a clinical terminology for other non-English societies should be readily reproducible. With our hope to motivate further research on LLM-based healthcare digitalisation, and at large the wellbeing of humankind, we openly release our terminology, models and algorithms, along with real-world clinical datasets for the development.", "sections": [{"title": "1 INTRODUCTION", "content": "Standard clinical terminologies, e.g., SNOMED CT, LOINC, ICD, can enable a multitude of values for global healthcare systems. For individual patients and clinicians, terminology or ontology coded electronic health records (EHR) greatly increase the consistency and interoperability of clinical data, and in turn increase the opportunities for real-time decision support for care delivering, retrospective reporting and analytics for research, precision medicine, and management. For populations, standardized clinical information boosts evidence-based healthcare, early identification of emerging health issues, and hence agile response in clinical practices [22, 33, 37].\nWhile clinical terminology, ontology, or knowledge graph has been widely perceived as pivotal for healthcare practice and research, the daunting cost of building and optimization has hindered their wide adoption or effective use. At present, albeit with conventional natural language processing (NLP) techniques, the development of these terminological systems is largely a manual, time-consuming, and error-prone process. SNOMED CT is considered to be the most comprehensive and widely adopted clinical terminology in the world [5]. It was established in 1965, has undergone more than twenty years development with over one hundred million dollar investment [36]. Yet SNOMED CT still lacks coverage for populous languages such as Chinese, Portuguese, and Arabic. Other deficiencies of SNOMED CT or the similar, are largely inherent in their classical ontological and NLP methodologies. For instance, the dilemma between the presumption of context-free meaning in terminologies and the contextualization nature of human languages [30], the complexity incurred for a robust ontological foundation inevitably sacrificed user friendliness and scalability [26].\nThe remarkable advances of large language models (LLMs) [6, 8, 51], especially the emergent capabilities of semantic understanding, generativeness and interactiveness, have inspired us to explore rapidly building high-quality and reliable terminologies for the healthcare domain. In particular, we address the problem of developing a SNOMED-like [33] terminology for the Chinese clinical domain. With a LLM-based holistic approach, we were able to develop a working version of clinical terminology within three months at a relatively low cost of computing and human labor, i.e., about 100K dollar. Our approach is seamlessly simple, consisting of bootstrapping, truth grounding, entity recognition and linking, and iterative optimization with human in the loop. We report our methodology with implementation details, motivating rapid reproduction for other unattended language and subcultural societies.\nAs LLMs have been increasingly applied and deployed to real-world healthcare and clinical settings [9, 46], hallucinations or fabricated information, remains one of the prominent challenges, entangled with other constraints including lack of explainability, security and privacy concerns [35]. These limitations are intrinsic in LLMs' probabilistic nature and unsupervised learning paradigm, in which lies their immense power of scaling. However, the safety-critical nature of the healthcare domain requires a more deterministic approach to restraining hallucination, therefore motivating us to explore the other side of the Al world. In particular, we augment LLMs with a model of truth, manifested by a standard terminology, ontology or enriched as a knowledge graph (KG). We took a holistic approach to knowledge augmentation, from raw data standardization, through introducing clinical modality into model training, to graph-augmented or guided generation in inference.\nWe further deployed the MedCT terminology to a representative spectrum of real world clinical and research applications, harvesting the remarkable capabilities of LLMs, while reining their hallucinations. Our laboratory experiments showed that MedCT terminology and associated models and algorithms achieved SOTA performances in a wide spectrum of clinical tasks, and our field study in real-world setting validated the significant values of the developed terminology, especially in LLM-based applications. To summarize, we believe that we have made the following contributions to the global healthcare system in the Al era, and the rest of the paper is logically structured likewise.\n(1) MedCT: the world's first open Chinese clinical terminology at the scale comparable to SNOMED CT.\n(2) A suite of models and algorithms for readily adoption of the above terminology, namely, MedBERT, a pretrained foundation model, and MedLink, a fine-tuned entity linking model.\n(3) A holistic approach with implementation details for rapid and cost-efficient development of clinical terminology for other unattended languages.\n(4) A wide and representative spectrum of real-world clinical applications utilizing the MedCT system, to demonstrate its value propositions and provide a reference framework of truth-augmented LLM applications in the healthcare domain.\n(5) Finding and observations from the field with regards to the status quo of applying LLMs in real-world clinical setting, e.g., large or small models, LLM or classical NLP techniques, general or domain-specialized models."}, {"title": "2 METHODOLOGY", "content": "We bootstrap our development from SNOMED CT, that is considered to be the most comprehensive and widely adopted clinical terminology, therefore inheriting decades of its achievements. We first applied LLM to contextualize and translate the SNOMED concepts into Chinese, thus forming our initial MedCT terminology. We then collaborated with a tertiary care hospital for truth-grounding the terminology, through annotating real-world EHRs with MedCT while revising the terminology for correction and localization.\nAt the core of the models and algorithms to utilize MedCT is a clinical foundation model, called MedBERT. We pretrained MedBERT from scratch using a thoughtfully curated clinical dataset, and yielded SOTA performance in semantic understanding. Next, with the MedCT annotated clinical data, we trained MedLink, fine-tuned models for clinical terminology named entity recognition (NER) and linking (NEL), or collectively entity linking (EL). After we deployed MedCT in the field, the learning process is iteratively reinforced, for both the MedCT terminology and entity linking models. Our work is inspired by the SNOMED CT entity linking challenge [23], and our method largely follows the model-based winning solution SNOBERT [27]. We managed to push the boundary further in model performance and multilingual coverage, by leveraging LLMs and well-curated real-world clinical data. We now describe each stage of our method and experimental results."}, {"title": "2.1 Bootstrapping: contextualized translation", "content": "We first acquired the May 2024 SNOMED CT International Edition release, represented as a directed graph that contains 367,584 vertices (concepts) and 1,215,543 edges (relationships). We focus on three types of hierarchies, namely, body structure, procedure, and clinical finding, by extracting 223,437 concepts (60.8%) from the whole graph.\nWe then employed machine translation to bootstrap a Chinese terminology from English SNOMED CT. Neural machine translation (NMT) [48] is the dominant approach today, e.g., Google translate, while directly prompting a generative LLMs can also produce highly competitive translations [20]. Specialized trained NMT models usually take a context-free setting, that is, texts are translated independent of contexts. The context-independent setting is not optimized for short terms such as SNOMED CT, especially for disambiguation (see some examples in Table 2). We leverage the expressiveness and interactiveness of LLMs, to contextualize the terms to be translated into their descriptions. For each concept, we first extracted its fully specified name, synonyms, hierarchy, and relationships from the SNOMED graph. We then composed a collective description from these elements. The prompt to LLM for contextualized translation is: {concept description}+\"\\n\"+In the above context, translate the term {concept synonym} into Chinese:. Intuitively and empirically, the contextual enrichment greatly boosts the LLM's capability to grasp and disambiguate the meaning of terms, as shown by some comparisons in Table 2.\nWe used the Tigerbot-3 LLM for contextualized translation, given its superior general-purpose performance and good multilingual and biomedical domain coverage, especially in Chinese [8]. We deployed a 100K context window Tigerbot-3-70B-Chat model on a server of 8x NVIDIA RTX4090-24G GPUs. The batch inference for translating 384,531 synonyms took 106 hours (or 848 GPU-hours), with an average length of input being about 500 characters and 180 tokens (mainly description and generation length is small)."}, {"title": "2.2 Truth grounding: linguistic and cultural localization", "content": "With the initial MedCT clinical terminology, we want to further validate the correctness (linguistically and grammatically) and appropriateness (culturally conform to local practices). Meanwhile, we also need an annotated dataset to train the clinical NER and NEL models. We conducted an annotation task to serve both purposes.\nIn collaboration with a tertiary care hospital in Zhejiang, China, we curated a dataset of clinical notes and EHRs, named as MedCT-clinical-notes. The dataset contains 3,109,181 EHR examples involving 596,680 patients, collected from 23 clinical departments over the first quarter of year 2024. The data was de-identified and organized into a format similar to MIMIC-IV free-text clinical notes [25], see Appendix A for some example snippets. Notably, our clinical data is significantly more comprehensive than MIMIC-IV notes, from patient's biography and social history information, to daily ward admission records, and even atrial fibrillation follow-ups if applicable. We curated this clinical data for multifold purposes, not only the entity linking task but also downstream applications. With these desiderata and the auto-regressiveness of generative LLM, we structured the clinical notes along the patient dimension, following a temporal or causal order. This way, a patient's clinical data can be analogous to a text document, one document per patient. This data structure is so designed for LLM-based clinical applications, including EHR auto generation, treatment pathway simulation for assistive diagnosis and treatment. We then randomly sampled 1,000 patients' clinical data or \"document\" for annotation.\nWe formed a group of 14 doctors with at least five years clinical experiences. These physician were instructed to perform two tasks of annotation at the same time as follows.\n(1) Annotate and link spans of text in clinical notes with specific concepts in the MedCT clinical terminology.\n(2) Comment linguistic and translation mistakes and cultural inappropriateness based on their practices.\nBoth the Chinese MedCT and original English SNOMED CT terminologies were given to the doctors for cross validation. Two sessions of in-depth and hands-on training were given to both the management and physicians, and the annotation tasks were facilitated by the open-source annotation tool doccano [34]. For corrections and additions suggested from the comments, if more than half of the annotators suggests the same and the director doctors of the relevant departments approve, the amendments will be merged into the next release of MedCT.\nWe emphasized the quality and even serendipities during the annotation process, since essentially the entity linking model to be trained is learning and encoding the domain expertises from the annotator clinicians' minds. The annotators were encouraged to discuss with their non-annotator fellow clinicians, and to consult with official literature and thesaurus, should there be any questions. For the sampled 1,000 patients' EHRs or 7,437 clinical notes (notes can be considered as different sections in a patient's document, e.g, diagnosis and treatment plan, brief hospital course, and discharge instructions). The group of annotators exercised at a carefully balanced speed of about 200 notes per day, so it took more than one month to annotate the dataset. As a result, the annotated dataset contains 61,660 entity mentions or about 8 annotations per note."}, {"title": "2.3 MedBERT: a clinical foundation model", "content": "Of the central importance for most clinical NLP tasks is a foundation model to encode broad and basic semantics in the domain. Previous work shows that for domains with copious amounts of unlabeled texts, pretraining language models from scratch yielded substantial gains over continual pretraining from general-domain models [18]. Biomedicine is one of such high-resourced domains. Specifically, we pretrain a BERT model from scratch using a biomedical dataset curated with the following design considerations, with statistics and sources outlined in Table 5.\n(1) A large corpora of biomedical literature and publications with comprehensive and timely coverage of the domain, e.g., the PubMed Central (PMC) repository [3].\n(2) Data from the field and directly relevant to downstream tasks, e.g., clinical guidelines [15, 16, 28, 39, 41] and real-world clinical notes MIMIC-IV [25].\n(3) Clinical terminologies and their contexts, e.g., SNOMED C\u0422 and MedCT terms and descriptions.\n(4) Multilingual coverage, i.e., English and Chinese.\nWe compared the prediction accuracy of the fill-mask task between our MedBERT and other SOTA biomedical and general-domain models, as the results exhibited in Table 6. First, we verified that domain-specific training has advantages, as biomedical models outperform general-domain BERT models by about twenty percentage points. Second, multilingual expansion is critical. Although the evaluation dataset only has less than 10% Chinese data, the multilingual and Chinese BERT surpass the English-only models by a large margin. Furthermore, the scale and quality of the training data tends to yield better model performance, as seen that BiomedBERT trained with PMC full text wins those with PubMed abstracts only.\nNevertheless, our model MedBERT achieves substantial gains over both the biomedical SOTA and general-domain BERT models. Most popular biomedical models were primarily trained on scientific papers in the domain, e.g., BiomedBERT used PubMed [3] and SciBERT was trained on Semantic Scholar papers [2]. Our MedBERT training corpus mixed in about 20% data from the field that is directly concerning downstream clinical tasks, i.e., clinical guidelines and protocols, real clinical notes, and terminologies. Healthcare is such a high-resourced domain that data shift may appear as domain shift, conjecturally explaining the performance gain of MedBERT over other popular models."}, {"title": "2.4 MedLink: clinical entity recognition and linking", "content": "We implemented a two-stage approach to recognizing clinical entities from free-text notes and linking the entities to the built MedCT concepts, as follows.\n(1) First stage: A NER segmentation task to detect spans of texts as clinical entity mentions.\n(2) Second stage: A NEL ranking task to predict the MedCT concepts for the recognized entities from the first stage.\nFor the first stage NER task, we fine-tuned a token classification model from the MedBERT foundation model, as described in Section 2.3. We classify each token into four classes: {finding, procedure, body, none}, using the BIO format [42], therefore a token tagging task with seven labels: {0, B-find, I-find, B-proc, I-proc, B-body, I-body}. We trained the NER model on the annotated clinical note data described in Section 2.1. The annotated data contains 1,000 patients, 7,437 clinical notes, and 61,660 entity mention annotations. Our annotated dataset is ten times larger than that of the SNOMED CT challenge [23], in both size and annotation examples. Without specially instructed, our annotated classes follows a similar distribution (find: proc: body = 0.59 0.18 0.23) as SNOMED CT challenge annotation data (find proc: body = 0.54 0.34 0.12), except for the class procedure. Further investigation with clinicians reveals that the data discrepancy largely comes from the healthcare gap between China and America, which is in turn influenced by various factors such as cultural preferences, medical standards, and insurance coverage. One reported example is the popularity difference in C-Section [17]. These discrepancies in healthcare practices underscore the value proposition from localization of clinical terminologies such as the MedCT in our endeavor.\nWe first split the data into four folds with random shuffling, used fold 1 to 3 for training and held out fold 0 for validation. The tokenized data is chunked into a max sequence length of 512, and repeated 10 times for each step. We fine-tuned for 200 epochs over the training data with a batch size of 8, a initial learning rate of 5e-5 and linear decaying schedule.\nAt the second stage NEL task, we need to link segmented entity mentions or text spans to concepts in the MedCT ontology. This is a semantic matching task, which we therefore simply formulate it as a ranking problem in the embedding space. We chose the SapBERT models for embedding [31, 32], which are fine-tuned BERT models specially for aligning biomedical synonyms by leveraging the UMLS dataset [38]. Specifically, we used SapBERT for English tasks, and its cross-lingual extension SapBERT-all-lang for multilingual and Chinese applications. For each concept from the MedCT terminology, we first embed each synonyms into a latent space of length 512, and then average over synonym embeddings as the concept representation. In inference time, we calculate the embeddings for the recognized entity mentions, and rank to predict their corresponding concepts from the MedCT ontology embedding database using cosine similarity.\nWe measure the performance of trained models with character-level concept-averaged intersection-over-union (IoU) defined as follows,\n$IoU_{concept} = \\frac{P_{char} \\cap G_{char}}{P_{char} \\cup G_{char}}$\n$IoU_{all} = \\frac{\\Sigma_{concept \\in P \\cup G} IoU_{concept}}{N_{concept \\in P \\cup G}}$\nwhere $P$ and $G$ denote predicted and ground-truth character-concept assignment, respectively. For each experiment, we keep the checkpoint with the highest IoU score as candidate model. We conducted two sets of experiments, one on English MIMIC-IV data (same as SNOMED CT challenge) and the other on Chinese clinical notes (as used for MedCT). Table 7 exhibits the experimental results. Our MedLink model achieves SOTA performance in both English and Chinese clinical NER and NEL tasks. We deem that a stronger multilingual foundation model MedBERT and copious annotated real-world clinical training data largely contribute into the gain."}, {"title": "2.5 Iterative reinforcement: human in the loop", "content": "As we deployed our MedCT system in the field to a tertiary care hospital in Zhejiang, China. We took a reinforcement learning approach to iteratively optimize the terminology, for both coverage and precision. A sample of 1,000 MedCT tagged clinical notes was reviewed by physicians on a monthly basis, mistakenly tagged examples were corrected and then fed into the NER and entity linking models for continual fine-tuning, using a rejection sampling mechanism. If a certain amount of wrongly tagged examples was attributed to the lacking in the terminology, clinicians would amend those missing concepts into the terminology."}, {"title": "3 EXPERIMENTS AND APPLICATIONS", "content": ""}, {"title": "3.1 Large or small models", "content": "Other than our approach of specialized trained BERT models, directly using generalized pretrained LLMs for NER and NEL is intriguingly appealing. MedCT is a bidirectional transformers model, while LLMs are autoregressive models such as GPT. Like most BERT models, MedCT is relatively small in model size, with 438M parameters in our release. LLMs on the other hand, typically have several hundred billion parameters, like GPT or Llama. Large models have demonstrated substantial improvements on a wide spectrum of NLP tasks, while remain highly general paradigms for training, inference and deployment. This generality is particularly appealing for cost efficient application development, which usually amount to few-shot prompt engineering or lightweight task-specific fine-tuning.\nHowever, training and inferring general-purpose LLMs is dauntingly expensive today. For example, it took 7.0M NVIDIA H100 GPU hours or about 13.7M US dollars to train Llama-3.1-70B [1]. Besides development and recurring computational cost, It remains unclear whether general autoregressive language models can accurately capture complex domain structures. Small yet task-specific designed and trained models may lack the versatility hence requires one-time development effort. Nevertheless, its potential for deep specialization, agile experimentation, and low cost for deployment and use, presents benefits for critical and infrastructural tasks.\nIn this experiment, we compared the two methodologies in the medical NER and NEL applications: small specialized models versus large generalized models (LLMs). For the LLM approach, we first prompt the OpenAI GPT-40 [40] for the NER task. After a mild prompt engineering, we use a simple few-shot prompt as showed in Figure 1.\nThe LLM applies its general world knowledge and the few-shot examples provided to perform the NER task. With the presumably state-of-the-art LLM GPT-40, the result seems impressive, as one snippet showed in Figure 2.\nIn the second stage NEL task, we apply the same approach as in MedLink (as described in Section 2.4). The LLM tagged entity mentions were linked to the MedCT concepts using embedding similarity match. In the training stage for both the small and large model approaches, we count the most frequent concept for each entity mention from the annotated training dataset. This gives a static dictionary (mention \u2192 concept) to simply lookup text spans for concepts by phrase matching. But this static dictionary still requires a specially annotated training dataset. In this experiment, we will also conduct an ablation study to analyze the value of the static dictionary lookup.\nWe ran the LLM-based NER and NEL experiments in both English and Chinese settings. For the English NEL task, we used a left-out validation dataset of 51 MIMIC-IV clinical notes (one fourth of the SNOMED challenge data). For the Chinese experiment, we also used the validation dataset of 1,860 clinical note snippets from the MedCT training data built in-house (as elaborated in Section 2.2). The experimental results are shown in Table 8.\nLet us first consider the most generalized LLM approach that does not rely on any specialized data or static dictionary lookup. Under this setting, GPT-40 only yields 0.11 IoU on English data and 0.17 IoU on Chinese data, substantially inferior to our MedLink small model approach (0.51 IoU for English and 0.30 IoU for Chinese). Although the LLM results are visually appealing as illustrated above, its numerical measurement of performance is suboptimal. By adding static dictionary lookup, the English IoU improved to 0.35 and Chinese metric barely changes to 0.18, still a considerable gap from MedLink. Noticeably, simple lookup from training set dictionary accounts for more than half of the performance with LLM approach, in English test. In Chinese data, the static lookup barely contributes to performance metrics, primarily because of less overlapping of entity mentions between training and validation sets. Moreover, LLM approach with GPT-40 incurs considerably more inference time than small model approach with MedCT, more than 10 times for English and 30 times for Chinese tests.\nWe also experimented with Llama-3.1-70B, one of prominent open-source LLMs. Somewhat unexpectedly, Llama-3.1-70B achieved comparable accuracy performance as the proprietary GPT-40 in our medical NER and NEL tasks, in both English and Chinese evaluations. Open-source alternatives are attractive since they are white-box and cost-efficient solutions. Our empirical results show that, for domain-specific tasks with some degree of complexity, the performance gap between close and open models may become negligible.\nAll evaluations were ran on mainstream compute resources. The LLM approach with GPT-40 used OpenAI API, Llama-3.1-70B was deployed on a local machine with 8\u00d7 NVIDIA A800-80G GPUs, while MedLink ran on one NVIDIA A800-80G GPU. Overall, besides the unsatisfactory accuracy performance, both the slower response time and the expensive inference hardware makes even the SOTA LLMs less appealing for broad adoption on fundamental tasks such as medical NER and NEL."}, {"title": "3.2 Retrospective retrieval of health records", "content": "The wide adoption of Electronic Health Record (EHR) greatly improves communication and availability of relevant information for both retrospective research and real-time clinical decision support. Accurate retrospective retrieval or search of EHRs is a fundamental task for evidence-based healthcare, clinical research, precision medicine, and community health management. For individuals, accurate access to rich and relevant case records enables real-time clinical decision support, sharing and analytics of appropriate information in a common way, contributing evidence for better treatment, and reducing costs for inappropriate and duplicative testing. For populations, storing and sharing health records in a common and accurate way facilitates early identification of emerging health issues, and hence agile response to evolving clinical practices, reducing costly or even deadly negligence and errors.\nHowever, a simple use of conventional information retrieval (IR) techniques, such as in popular daily search engines, provides only limited benefits to clinical research and decision support. Low recall and imprecise results requires further heavy-duty data post-processing, therefore hinders the wide usage of retrospective retrieval of health records in real-world clinical setting. The major reason is that general-purpose IR is a language-level system. In a critical domain such as healthcare, we need a meaning-based retrieval system at the clinical domain level. One example from the field is as follows. Clinicians want to find \"historical patients with type 2 diabetes complicated by diabetic nephropathy\". A modern general-purpose IR system would likely word segment \"type 2\" apart from \"diabetes\", and assign non-trivial relevance scores to examples of \"type 1 diabetes\" (similar in dense retrieval in embedding space). From the clinical perspective, however, these two diseases have differences in causes, symptoms and treatment, thus \"type 1 diabetes\" results are imprecise. A standardized clinical terminology like our MedCT becomes a systematic approach to bringing semantic understanding from linguistic level to domain level. In our experiments, all clinical notes and health records with mentions of \"type 2 diabetes\" were accurately recognized and linked to the MedCT concept: \"44054006 (find) | Diabetes mellitus type II | II \u578b\u7cd6\u5c3f\u75c5\u201d.\nIn this experiment, we wish to validate and measure the value of the clinical terminology MedCT in the application of health record retrieval. A majority of retrospective retrieval of EHRs involves finding cases with similar or related diseases, in reference past evidences in testing, diagnosis, treatment and outcome. Therefore, from the MedCT-clinical-notes dataset (as described in Section 2.2), we took a corpus of discharge summaries for this retrieval experiment. The corpus contains 13,863 examples or discharge notes, entered from all departments during the first quarter of year 2024 in a tertiary care hospital. The data was organized into relevant textual fields including bio, admission, treatment pathway, discharge summary and instruction. We interviewed a panel of 12 senior physicians to collected a set of 20 queries representative of real-world clinical practice and research. The clinical query set was chosen with non-trivial complexity such that a straightforward keyword match conceivably cannot yield satisfactory results. One example is \"post-stroke with pneumonia\", and the full query set can be found in Appendix B."}, {"title": "3.3 Health records auto-generation by LLMs", "content": "Next we evaluate MedCT on the task of health records auto-generation by LLMs. There have been emerging applications of LLMs in the healthcare domain [10", "47": ".", "9": ".", "49": ".", "14": "continually from Llama-3.1 [1", "chief complaint\" and \"physical examination\" and therefore should capture key clinical information more comprehensively and accurately.\nTo evaluate the generation results, we recruited a panel of nine hospitalists to review summary generations from the above two methods, along with the human summary by doctors, in a blind fashion, and then cast Likert scale to each testing example. Evaluating text summarization models is nontrivial, especially with automatic metrics. In general domains, such as TL;DR Reddits and CNN/DM news article summarization, previous works have used ROUGE or reward models to predict human preference [50, 53": "."}]}