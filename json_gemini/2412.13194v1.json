{"title": "Proposer-Agent-Evaluator (PAE):\nAutonomous Skill Discovery for Foundation\nModel Internet Agents", "authors": ["Yifei Zhou", "Qianlan Yang", "Kaixiang Lin", "Min Bai", "Xiong Zhou", "Yu-Xiong Wang", "Sergey Levine", "Erran Li"], "abstract": "The vision of a broadly capable and goal-directed agent, such as an Internet-browsing agent in the digital world\nand a household humanoid in the physical world, has rapidly advanced, thanks to the generalization capability of foundation\nmodels. Such a generalist agent needs to have a large and diverse skill repertoire, such as finding directions between two\ntravel locations and buying specific items from the Internet. If each skill needs to be specified manually through a fixed set\nof human-annotated instructions, the agent's skill repertoire will necessarily be limited due to the quantity and diversity of\nhuman-annotated instructions. In this work, we address this challenge by proposing Proposer-Agent-Evaluator(PAE), an\neffective learning system that enables foundation model agents to autonomously discover and practice skills in the wild. At\nthe heart of PAE is a context-aware task proposer that autonomously proposes tasks for the agent to practice with context\ninformation of the environment such as user demos or even just the name of the website itself for Internet-browsing agents.\nThen, the agent policy attempts those tasks with thoughts and actual grounded operations in the real world with resulting\ntrajectories evaluated by an autonomous VLM-based success evaluator. The success evaluation serves as the reward signal for\nthe agent to refine its policies through RL. We validate PAE on challenging vision-based web navigation, using both real-world\nand self-hosted websites from WebVoyager and WebArena. Our results show that PAE significantly improves the zero-shot\ngeneralization capability of VLM Internet agents (more than 30% relative improvement) to both unseen tasks and websites.\nOur model also achieves an absolute advantage of over 10% (from 22.6% to 33.0%) comparing to other state-of-the-art open\nsource VLM agents including Qwen2VL-72B. To the best of our knowledge, this work represents the first effective learning\nsystem to apply autonomous task proposal with RL for agents that generalizes real-world human-annotated benchmarks\nwith SOTA performances. Our open-source checkpoints and code can be found in https://yanqval.github.io/PAE/.", "sections": [{"title": "1. Introduction", "content": "The vision of a broadly capable and goal-directed agent has long captured the imagination: from\nagents that can browse the web to fulfill user instructions to robots that could perform diverse\ntasks in the home, AI systems that can independently accomplish open-world goals would not only"}, {"title": "2. Related Works", "content": "Foundation model agents.Thanks to the generalization capabilities of Large Language Models\n(LLMs) and Vision Language Models\n(VLMs) recent\nworks have successfully extended such agents to more general real-world use cases\nBesides constructing prompting\nwrappers around proprietary VLMs and fine-tuning open-source VLMs with expert\ndemonstrations a recent trend has emerged involving interactive improvement\nof LLM/VLM, in particular web/GUI agents, through autonomous evaluator feedback where evaluator LLMs/VLMs are\nprompted to evaluate the success of the agents to serve as the reward signal. This approach aims to\nelicit goal-oriented and reward-optimizing behaviors from foundation models with minimal human\nsupervision. However, these methods still depend on a static set of human-curated task templates,\nconstraining their potential and scalability. Our work introduces a framework where agents can\ndiscover and practice the skills they find useful, thereby eliminating the reliance on predefined and\nhuman-curated task templates and opening up new possibilities for scalability and adaptability in\ntraining generalist autonomous LLM/VLM agents.\nSelf-generated instructions. Self-generated instructions for improving LLMs have been shown to\nbe effective in single-turn LLM alignment and reasoning domains without interactions with an external\nenvironment. AgentGen takes a step further to fine-tune LLM agents with expert\ntrajectories in self-generated environments and tasks. However, its feasibility in the self-play agent\nsetting with RL and autonomous evaluators has not been understood. On the other hand, the closest\nworks to ours employ autonomous RL and foundation model task proposers to simplified environments\nsuch as games and robotics settings"}, {"title": "3. Proposer-Agent-Evaluator (PAE): Autonomous Skill Discovery system For Foundation Model Agents", "content": "Next, we will explain the technical contributions of this paper. In this section, we will define the\ngeneral system of PAE including a task proposer, an agent policy, and an autonomous evaluator.\nWe will begin by formalizing the learning goal of this system and then detail the roles of each key\ncomponent. In the section to follow, we will explain how we apply PAE to VLM Internet agents.\nProblem setup We begin by formalizing the problem setup of autonomous skill discovery for real-\nworld agents. The learning goal of PAE is to find a reward-maximizing policy \u03c0 parameterized by \u03b8 in\na contextual Markov Decision Process (MDP) environment defined by \\(M = \\{S,A,T,R, H, C\\}\\), where\nS, A are the state space and action space respectively, and H is the horizon within which the agent\nmust complete the task. We assume that the agent has access to the environment and can collect\nonline roll-out trajectories through accessing the dynamics model T as a function of determining\nthe next states given the current states and actions. Crucially, we assume that the ground-truth task\ndistribution C and the reward function R are hidden during training and we have to use a proxy task\ndistribution \\(\\hat{C}\\) and reward function \\(\\hat{R}\\) instead. Consider the setting of training a real-world Internet\nagent. The dynamics model T would be a simulated browser environment that the Internet agent\ncan interact with. The ground-truth task distribution C might be the distribution of tasks that would\nbe asked by the real users when the Internet agent is deployed and a possible choice for the reward\nfunction R might be whether the agent has satisfactorily completed the tasks for the real users. In\nsuch a real-world setting, although the agent can freely access resources from the Internet through\na simulated browser environment during training, assuming knowledge of the ground-truth task\ndistribution and reward function is impractical. Therefore, we employ VLM-based task proposers \\(\\hat{C}\\)\nand reward model \\(\\hat{R}\\) as proxies. The desired outcome is that improving the policy \\(\\pi_{\\theta}\\) with \\(\\hat{C}\\) and \\(\\hat{R}\\) can\nlead to an improved policy that can successfully generalize to the ground-truth task distribution\nand reward functions which are only used as evaluations.\nKey components Figure 1 shows the interplay between the key components in our framework,\nincluding a context-aware task proposer, an agent policy, and an autonomous evaluator. The role\nof the task proposer \\(\\hat{c}\\) is to serve as a proxy to improve on the ground-truth task distribution C\nduring the learning process. However, it might be unrealistic to expect the task proposer to generate\nfeasible tasks without knowledge of the environment. To provide more context of the functions and"}, {"title": "4. PAE for VLM Internet Agents", "content": "With the general framework set up, we are now ready to discuss the concrete instantiation of PAE\nin the setting of VLM Internet agents. We start by introducing the environment of vision-based web\nnavigation and then explain how we implement the key components from PAE in this setting.\n4.1. Vision-Based Web Browsing Environment\nWe consider the general vision-based web browsing environment. The goal for VLM agents in this environment is to navigate through realistic web pages to complete\nsome user tasks \\(c_{t}\\) such as \u201cInvestigate in the Hugging Face documentation how to utilize the 'Trainer'\nAPI for training a model on a custom dataset, and note the configurable parameters of the Trainer\nclass\u201d. As illustrated in Figure 2, each observation \\(s_{t}\\) from the observation space contains only the\nscreenshot of the last web page just like how humans interact with the Internet. To provide better\naction grounding, we follow the practice from prior works to\naugment the observation space with number marks on top of each interactive element such as web\nlinks and text boxes. To execute a web browsing action, the Internet agent can directly output the\nnumber of the element to interact with and the corresponding action such as clicking and typing,\nwithout the need of locating the coordinates of each web element. Therefore each web action \\(a_{t}\\)\ncontains the type of the action to perform and the number of the element to interact with. Each\nepisode finishes either when the agent chooses to finish through the \u201cAnswer\u201d action or when a\nmaximum number of 10 steps have been reached. In our experiments, we use ground-truth success\ndetectors (based on either human annotations or functional verifiers) and human annotated tasks\nfrom WebArena and WebVoyager to evaluate the performance\nof different policies. Crucially, both the ground-truth success detector and the distribution of human\ntasks are kept hidden, which challenges the generalization capability of the learnt skills to generalize"}, {"title": "4.2. Context-Aware Task Proposer", "content": "In order to generate a diverse set of feasible tasks, we frame task proposing \\(\\hat{C}\\) as a conditional\nauto-regressive generation based on the context information of the websites. Thanks to the vast\npre-training knowledge of relevant context for popular websites like Amazon.com, we find it suffice\nto use only website name as \\(z_{m}\\). However, for less common or access restricted websites such as\nself-hosted websites in WebArena, it is necessary to supply the task proposer with richer context.\nIn the cases of user demos being available, we consider an alternative to sample some additional\nscreenshots from the user demos to serve as the context information. In our experiments, we consider\nboth using proprietary models such as Claude-3-Sonnet and open-source models\nsuch as Qwen2VL-7B for the task proposers, with promptsd in Appendix C."}, {"title": "4.3. Image-Based Outcome Evaluator", "content": "To take full advantage of the asymmetric capability of SOTA VLMs as agents and as evaluators\n(experimental evidence presented in Section 5), we find it most robust for the autonomous evaluators\nto complete the easiest evaluation: evaluating the success of the final outcome based on the final three screenshots and the agents' final answers to provide only\n0/1 response in the end. Other alternatives such as code-based or step-based\nevaluations are either impractical without access to hidden state information or too\nnoisy because of the hallucination issues present even in SOTA VLMs. In our experiments, we also\nconsider both using proprietary models such as Claude-3-Sonnet and open-source\nmodels such as Qwen2VL-7B Yang et al. (2024a), with prompts presented in Appendix C."}, {"title": "4.4. Chain-of-Thought Agent Policy", "content": "Crucially, as the ultimate goal for the agent policy is to complete human requests, the agent should\nnot only learn diverse skills on the proposed tasks but also reflect on the skills learnt so that they can\nbe helpful for unseen human requests. Therefore, we incorporate an additional reasoning step to\noutputs the agent's chain-of-thought before the actual web operation. This reasoning step is optimized\nwith the RL algorithm just like the actual web operation. Because of the 0/1 reward structure and\ninfrastructure complexity of thousands of distributed fully-functioning web browsers, we employ\nthe most simple online policy optimization algorithm Filtered Behavior Cloning (Filtered BC) that\nsimply imitates all thoughts and actions in successful trajectories with the negative log-liklihood loss.\nThis simple method has been widely adopted in prior literature of RL+LLM such as We find that this simple policy optimization objective can already lead\nto a superior generalization capability of the learnt agent. In our experiments, our agent policy is\ninitialized from LLaVa-1.6-Mistral-7B and LLaVa-1.6-Yi-34B."}, {"title": "5. Experiments", "content": "The goal of our experiments is to understand the effectiveness of PAE to complete real-world visual web\ntasks. Specifically, we design experiments to answer the following questions: (1) Can our autonomous\nskill discovery framework successfully discover skills useful for zero-shot transfer to tasks from an\nevaluation task distribution unseen to the task proposer? (2) How does the models trained with\nPAE compare with other open-source VLM agents? (3) How does the effectiveness of PAE scale with\nthe size and performance of the base model? (4) Whether the effectiveness of PAE is limited by the\nperformance of the task proposer and evaluator model? (5) How does the use of different contexts\n(e.g. website names and user demos) affect the performance?\n5.1. Environments\nWebVoyager contains a set of 643 tasks spanning 15 websites in the real world\nsuch as ESPN and Arxiv. As tasks in Google Flights and Booking domain are no longer feasible due to"}, {"title": "5.2. Baseline Comparisons", "content": "We validate the effectiveness of PAE by comparing it with (1) proprietary VLMs, (2) state-of-the\nart open-source VLMs, and (3) an alterative supervised fine-tuning (SFT) approach. We consider\nClaude 3 Sonnet and Claude 3.5 Sonnet for proprietary VLMs, and Qwen2VL-\n7B, Qwen2VL-72B, InternVL-2.5-XComposer-7B, and\nLLaVa-Next-7B/34B for SOTA open-source VLMs. All models are prompted similar\nto He et al. (2024) using set-of-marks augmented screenshot observations and including chain-of-\nthought in the action outputs. The prompts are included in Appendix C. As SOTA open-source models\nstruggle to achieve non-trivial performance in the challenging web navigation benchmarks except the\nlargest Qwen2VL-72B, we include another baseline LLaVa-SFT that fine-tunes LLaVa with Claude 3\nSonnet (Anthropic, 2024) agent trajectories on self-generated tasks on 85 real-world websites not"}, {"title": "5.3. Main results", "content": "We present our main baseline comparisons of PAE with other baselines in Table 1, 2, and 3. Overall,\ncomparing to the SFT checkpoint using demonstration data, LLaVa-7B PAE can achieve an average of\n7.4% and 10.8% absolute improvement in terms of success rates on WebVoyager and WebArena Easy\nrespectively. A similar improvement of 10.4% on WebVoyager is observed for LLaVa-34B PAE as well,\nindicating a favorable scaling performance of PAE. As a result, our resulting model LLaVa-34B PAE\nachieves an absolute success rate of 10.4% on WebVoyaer over the prior state-of-the-art open-source\nVLM agents. Similarly, LLaVa-7B PAE also establishes a new state-of-the-art performance on WebArena\nEasy, surpassing the prior best performing model Qwen2VL-72B with 10\u00d7 more parameters. More\nimportantly, our analysis shows that PAE can enable Internet agents to learn general web browsing\ncapabilities that zero-shot transfer to unseen websites.\nHow does existing open-source and proprietary models perform in vision-based web navigation?\nFirst, we note the difficulty and significance of real-world vision-based web navigation, even for\nstate-of-the-art medium-size open-source VLM agents such as Qwen2VL-7B and InternVL2.5-8B\nwith set-of-marks augmented observations and chain-of-thought prompting. In particular, on the\nWebVoyager benchmark, among open-source VLM agents, only the largest Qwen2VL-72B can achieve\na non-trivial average success rate of 22.6% on WebVoyager, while all other open-source agents\ncompletely fail on this benchmark with average success rate under 2%. On the other hand, closed-\nsource proprietary models start to show promise in becoming a generalist Internet agent with Claude\n3.5 Sonnet achieving an average success rate at 50.5% and 50.1% on WebVoyager and WebArena\nEasy. Comparing LLaVa-7B SFT and LLaVa-7B, we find that supervised fine-tuning on demonstration\ndata can significantly improve the general web browsing capabilities of open-source VLM agents.\nEven if the SFT demonstration data is collected on out-of-distribution online websites, the general"}, {"title": "6. Discussions", "content": "The effect of additional reasoning step. We also perform an additional ablation on the effect of the\nPAE design choice of asking the VLMs to output their thoughts first prior to the actual web operations.\nWe consider an additional baseline of directly outputting the web operations without thoughts, and\ncarry out the similar SFT and Filtered BC experiments using the same setup described in Section 5.2.\nAs reported in Figure 3, although PAE without reasoning can also achieve improvements in the\nproposed set, the lack of additional reasoning step results in a significant inferior performance in its\ngeneralization to the unseen human-written evaluation set.\nThe effect of choice of evaluators. Finally, we present ablation results on the effect of different\ndesign choices of evaluators in Figure 3. We compare the outcome-based evaluator included in PAE\nwith other choices of evaluators in the related literature such as step-based evaluators and function-based evaluators. In our implemantation\nof step-based evaluator, we ask Claude 3 Sonnet to evaluate whether each step is correct or not (i.e.\nwhether it gets the agent closer to the goal) and behavior clone all the steps considered correct by\nthe step-based evaluator. In our implementation of function-based evaluator, we provide 3 examples\nof verification functions as used by WebArena and ask Claude 3 Sonnet to also\ncome up with verification functions to functionally verify the final task success rate (e.g. checking\nif the final website url is the same as the ground-truth url). As shown in Figure 3, both step-based\nevaluator and function-based evaluator perform worse than the outcome-based evaluator, where the\nuse of step-based evaluator even leads to a worse performance compared to the SFT checkpoint to\nstart with. We found that the step-based evaluator hallucinated more often and tended to be too\n\"generous\" in terms of considering the success of each step, potentially because the task of evaluating"}, {"title": "7. Conclusions and Future Work", "content": "In this paper, we introduced an effective learning system, PAE, for autonomous skill discovery with\nfoundation model agents, addressing the limitations of using a static set of human-annotated instruc-\ntions for fine-tuning agents. Instead of manually specifying what the agents should learn, our system\nenables the agents to explore, practice, and refine new skills autonomously through open-ended inter-\nactions with various environments. The framework's key components\u2014task proposer, action policy,\nand autonomous evaluator\u2014work together to generate, attempt, and evaluate tasks without any\nhuman intervention, leading to more than 10% improvement over prior state-of-the-art performance\nacross benchmarks like WebVoyager among open-source VLM agents (22.6% to 33%). This work\npaves the way for more capable open-source foundation model agents, with future research focused\non extending this approach to other domains and integrating it with better approaches to make use\nof the context information.\nReproducibility Statement\nTo facilitate reproducibility of our work, we plan to open-source the model checkpoint and code. To\nprovide more details about our practical algorithm, we have included the algorithm pseudo-code in\nAlgorithm 1. We have also included all the prompts that we have used for the task proposer, the agent\npolicy, and the autonomous evaluator in Appendix C. More details for gathering and processing the\nSFT dataset have been included in Appendix E. An discussion of the hyperparameter tuning of our\nmethod has been included in Appendix H."}, {"title": "Appendices", "content": "A. Algorithm\nIn Algorithm 1, we include a formal definitions of our practical algorithm of PAE as presented in\nSection 3.\nAlgorithm 1 Proposer-Agent-Evaluator: Practical Algorithm\nRequire: Context information zm, task proposer \\(\\hat{C}\\), autonomous evaluator \\(\\hat{R}\\).\n1: Initialize policy \\(\\pi_{\\theta}\\) from a pre-trained checkpoint.\n2: Initialize replay buffer \\(D \\leftarrow \\{\\}\\).\n3: ## Propose tasks based on the context information.\n4: Obtain proposal task distribution \\(\\hat{C}(z_{M})\\).\n5: for each global iteration do\n6: for each trajectory to be collected do\n7: Sample a task from the task proposer \\(c \\sim \\hat{C}(Z_{M})\\).\n8: Reset the environment to obtain the initial observation \\(s_{0}\\)\n9: for each environment step t do\n10: Sample \\(a_{t} \\sim \\pi(\\cdot|s_{t}, c), s_{t+1} \\sim T(\\cdot|s_{t}, a_{t}, c)\\).\n11: if done then\n12: ## Autonomously evaluate the outcome of the agent rollout.\n13: \\(r_{t} \\leftarrow \\hat{R}(s_{t}, a_{t}, c)\\).\n14: else\n15: \\(r_{t} \\leftarrow 0\\).\n16: end if\n17: \\(D \\leftarrow D \\cup \\{(s_{t}, a_{t}, r_{t}, s_{t+1}, c)\\} \\).\n18: end for\n19: end for\n20: ## Update the agent policy with any RL algorithm.\n21: \\(\\pi \\leftarrow RL\\_update(\\pi, D)\\)\n22: end for\nB. Details on Human Annotations and User Demos\nDetails on User Demos. User demos from experiments in Figure 6 are collected by the authors\nwithout appealing to actual users. For each website, the authors attempt 10 tasks that we think are\nrepresentative of the use of the particular website, and 10 most distinctive web pages are identified\nin the process of attempting those 10 tasks.\nDetails on Human Annotations. Five annotators of PhD students participate in the user study and\nthe entire process takes around 40 annotator hours with the help of a designated user interface\nprogrammed in Gradio. To clarify the precise definition of the different error categories used in\nSection 6, we provide the following instruction to give more comprehensive explanations with example\ntrajectories:\n(1) Low-level skill missing errors refer to cases where the agent has a reasonable plan to solve the\nproblem but fails to execute precise actions on the website, such as not knowing which button to click\nto reach the desired page. We classify trajectories where the agent seems to follow a reasonable plan"}]}