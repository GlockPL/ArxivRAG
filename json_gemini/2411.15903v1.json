{"title": "Bimanual Grasp Synthesis for Dexterous Robot Hands", "authors": ["Yanming Shao", "Chenxi Xiao"], "abstract": "Humans naturally perform bimanual skills to handle large and heavy objects. To enhance robots' object manipulation capabilities, generating effective bimanual grasp poses is essential. Nevertheless, bimanual grasp synthesis for dexterous hand manipulators remains underexplored. To bridge this gap, we propose the BimanGrasp algorithm for synthesizing bimanual grasps on 3D objects. The BimanGrasp algorithm generates grasp poses by optimizing an energy function that considers grasp stability and feasibility. Furthermore, the synthesized grasps are verified using the Isaac Gym physics simulation engine. These verified grasp poses form the BimanGrasp-Dataset, the first large-scale synthesized bimanual dexterous hand grasp pose dataset to our knowledge. The dataset comprises over 150k verified grasps on 900 objects, facilitating the synthesis of bimanual grasps through a data-driven approach. Last, we propose BimanGrasp-DDPM, a diffusion model trained on the BimanGrasp-Dataset. This model achieved a grasp synthesis success rate of 69.87% and significant acceleration in computational speed compared to BimanGrasp algorithm.", "sections": [{"title": "I. INTRODUCTION", "content": "HUMANS can seamlessly coordinate both hands to perform complex tasks in daily life. This is mainly due to several advantages of bimanual manipulation. For instance, bimanual manipulation enables diverse object interaction skills effortlessly, such as tying ropes, knitting clothes, and performing kitchen chores. Compared to using only a single hand, bimanual manipulation reduces human fatigue and improves body balance by distributing payloads more evenly [1]. This capability is particularly beneficial when handling large and heavy objects, as it enhances both productivity and safety.\nIn the field of robotics, the growing market for humanoid robots has driven the development and use of multi-fingered dexterous hands for object manipulation. Recent works [2]\u2013[5] have mainly focused on unimanual dexterous grasping based on object geometry. These techniques have simplified the use of dexterous hands with high degrees of freedom (DoF). However, they typically focused on objects of a size and mass suitable for a single hand. This narrow focus overlooks many larger and heavier objects (e.g., heavy bottles, home appliances, and furniture) that exceed the volume of unimanual grasps. Moreover, these works involved only one dexterous hand and did not fully exploit the potential of humanoid robots, which naturally possessed two hands.\nOn the other hand, there have been prior works focusing on bimanual manipulation. These works focus mainly on learning various object interaction skills. State-of-the-art works in this area have utilized Reinforcement Learning (RL) and imitation learning to acquire skills such as opening bottle lids and closing doors [6]\u2013[9]. Although these studies involve bimanual grasping skills, they target very specific objects with unique functional affordances, requiring an ad-hoc training process for each object in simulation. Therefore, a research gap remains unbridged when designing general grasping skills for arbitrary objects. To our knowledge, there are no existing tools or grasp pose datasets for bimanual dexterous hand grasping. Although some studies have developed frameworks for bimanual object grasping [10], they are limited to parallel-jaw grippers. This highlights a research gap in the use of bimanual dexterous hands.\nFrom a technical perspective, synthesizing bimanual grasp poses for dexterous hands presents greater challenges compared to the conventional grasp search problem. One of the main reasons is the significantly larger action space, which greatly increases the computational cost. For instance, the DoF of a Shadow Hand manipulator exceed 20 [11]. When extended to bimanual grasping, the DoF doubles, leading to a substantial increase in computational cost. In practice, reducing computational cost has been the main focus of most studies on dexterous grasp synthesis [4], [12], where the ability to generate grasps in quasi-real time when encountering new objects is highly desirable.\nIn this paper, we aim to develop a bimanual grasp synthesis pipeline optimized for both grasp quality and generation speed. To achieve this, we first propose BimanGrasp algorithm, a grasp synthesis method that searches for bimanual grasp poses in a high-dimensional configuration space using stochastic optimization. Secondly, by implementing BimanGrasp algorithm with GPU-based optimization, we have synthesized the BimanGrasp-Dataset, which comprises over 150k grasps. Each grasp has been verified through simulations in the Isaac Gym environment. [13]. The validation results demonstrate that the bimanual grasp strategy can handle large and heavy objects, which were previously unattainable with unimanual grasping techniques. Lastly, by utilizing the proposed dataset, we have significantly accelerated bimanual grasp synthesis by transforming it into a data-driven paradigm. We introduce BimanGrasp-DDPM, the first diffusion model capable of efficiently generating diverse bimanual grasp poses.\nTo summarize, the contributions of this paper are as follows:\n\u2022 BimanGrasp algorithm: an offline bimanual grasp synthesizer based on stochastic optimization.\n\u2022 BimanGrasp-Dataset: A dataset of bimanual robot grasping poses, validated through physics simulation.\n\u2022 BimanGrasp-DDPM: a quasi-real time bimanual grasp generator based on DDPM.\n\u2022 Quantitative studies comparing the performance of bimanual grasping with unimanual grasping."}, {"title": "II. RELATED WORKS", "content": "Robots are cyber-physical systems that possess the ability to interact with various objects in the physical environments. Hence, object manipulation has long been a key research area. Alone this line of research, previous works have focused on developing new skills, improving efficiency and safety. Due to the complex nature of environments, object manipulation encompasses diverse forms. One category is prehensile ma- nipulation, where robots aim to grasp objects. The other cate- gory is non-prehensile manipulation, which primarily includes non-grasping skills [14]. Typical applications include planar pushing [15], throwing and catching [16], solving a Rubik's Cube [17], and playing instruments [18].\nIn this paper, we focus on developing bimanual object grasping skills, which belong to the prehensile manipulation category. To enable bimanual grasping, it requires obtaining cooperative grasp poses that enclose objects inside. This pro- cess is known as bimanual grasp synthesis. Previous research has extensively studied the synthesis of grasp poses, but mainly for grippers [19]. These studies have reportedly achieved both high grasp success rates and real-time computational efficiency [20], [21]. However, due to the limited capability of low-DoF grippers, there is a growing need for synthesizing grasps for high-DoF manipulators, such as dexterous hands.\nSynthesizing grasps for dexterous hands is more challenging than for grippers. Early researches used a grasp synthesis pipeline similar to the conventional approach for grippers. This method involves sampling grasp poses and analyzing the likelihood that each grasp pose could satisfy the force-closure condition [12], [19]. The advantage of these approaches is the ability to synthesize grasp poses for almost arbitrary object shapes. However, they are computationally expensive for manipulators with high DoF due to a significantly larger action space.\nTo accelerate grasp generation, recent works have adopted data-driven approaches to generate grasp poses directly [22]. Common data-driven generative models include variational au- toencoders [23], [23]\u2013[25], normalizing flows [22], and diffu- sion models [26]\u2013[29]. This line of research has demonstrated improved speed and quality in the synthesis of unimanual grasps [22], [27]. However, it has not yet been applied to bimanual manipulation."}, {"title": "B. Bimanual Manipulation", "content": "Bimanual manipulation refers to the coordinated use of both hands to manipulate objects. This type of manipulation is essential for a wide range of activities, from daily tasks to complex professional operations. In everyday life, actions such as tying shoelaces, opening jars, and typing on a keyboard depend on the synchronized use of both hands. In professional fields, bimanual manipulation is crucial in areas like surgery and component assembly, where precision and coordination between both hands are paramount [30].\nBimanual manipulation is also a critical skill for robots. Historically, humanoid robots capable of bimanual object manipulation emerged in the 2000s [31], [32]. Since then, biman- ual skills such as moving kitchen cookware [33], dishwashing [34], and object pick-and-place [35] have been developed. However, these early strategies were based on two parallel grippers rather than dexterous hands.\nRecent advancements in robotics have increasingly focused on learning dexterous bimanual manipulation with multi-fingered hands, often using human demonstrations. This progress has enabled robots to perform coordinated actions with greater precision than unimanual approaches [7], [9], [36]. Despite these advancements, current researches on dex- terous bimanual manipulation target very specific objects and lack large-scale datasets or diverse hand pose types [37], [38]. To bridge the gap, we propose techniques and a dataset that aim to help humanoid robots develop more general bimanual grasping skills."}, {"title": "III. METHOD", "content": "The problem of bimanual grasp synthesis is formulated as Eq. (1). Given an object mesh denoted as O, our goal is to obtain grasp poses by maximizing the grasp quality score S that is empirically defined. The metric G used for calculating the score also considers rigid body poses \\(T_i \\in SE(3)\\) and joint configurations \\(\\theta_i \\in \\mathbb{R}^{22}\\), where \\(l \\in \\{1, 2\\}\\) denotes the left and right manipulators, respectively. This optimization paradigm is employed for synthesizing bimanual grasps, corresponding to the BimanGrasp algorithm proposed in Sec. III-B.\n\\[\\max_{T_1,\\theta_1,T_2,\\theta_2} S = G(O, T_1, \\theta_1, T_2, \\theta_2).\\]\nAlthough this approach has been commonly adopted for grasp synthesis problems involving manipulators with lower DoF (e.g., [12], [39], to mention a few), one issue lies in computational efficiency. Given the high DoF of two dexterous hands, obtaining feasible bimanual grasps in quasi-real time is difficult. This limitation hinders the applications of such methods, especially for scenarios where timely responses are crucial.\nCompared to the optimization-based paradigm discussed above, synthesizing bimanual grasps through deep learning models could be more efficient. To verify this idea, we propose BimanGrasp-DDPM model (Sec. III-D). Nevertheless, training such a generative model requires a dataset of bimanual grasp poses, which does not currently exist. To bridge this gap, we introduce the BimanGrasp-Dataset, which is synthesized offline using the optimization-based BimanGrasp algorithm. The overall system architecture for achieving this is shown in Fig. 2."}, {"title": "B. Synthesis Bimanual Grasp via Stocastic Optimization", "content": "This section describes the BimanGrasp algorithm, which is capable of synthesizing grasp poses conditioned on object meshes. The algorithm consists of two steps: 1) initialize grasp poses around the target object, and 2) iteratively optimize an energy function, during which the poses are adjusted based on grasp stability and penetration. The detailed procedures for achieving this are as follows:\nStep 1: Initialize Bimanual Hand Poses. Humans naturally grasp objects by facing them and approaching from two opposite sides. In our algorithm, we empirically initialize each hand's pose symmetrically around the object's center to increase the similarity to human grasps and reduce the chance of penetration. Specifically, we adopt the initialization procedures from [4]. First, two hands are placed on an inflated convex hull enveloping the object, with the palms facing the object. Then, we progressively decrease the hull's size, reducing the gap between the hands and the object until they make contact. Note that it's important to introduce variations in the initial hand poses and joint angles. This randomization technique enlarges the search space, allowing for more diverse grasps and helping avoid sub-optimal local minima.\nStep 2: Improve Grasp Quality. The grasp quality is improved by optimizing an energy function, defined as the weighted sum of all terms in Table. I. In the table, we denote \\(d(p, q)\\) for \\(p, q \\in \\mathbb{R}^3\\) as the Euclidean distance between two points, and \\(d(p, O) = \\min_{q \\in O}(p, q)\\) as the distance between p and the object mesh. We denote \\(H_l\\) as the mesh of each hand with \\(l \\in \\{1, 2\\}\\), and \\(P(H_l)\\) as the anchor points selected from the hand mesh to compute penetration.\nThe empirical quality metric considers both the grasp stability and feasibility. One main goal is to keep hands close to object's surface. To achieve this, we construct the term \\(E_{dis}\\), which quantifies the distance between the two hands and the object. By minimizing this term, the fingers and palms land close to the object surface. Here \\(x_a\\) with \\(a \\in \\{1, 2, ..., n\\}\\) denotes a point cloud with \\(n = 4000\\) points sampled from both hands' surfaces.\nThe term \\(E_{fc}\\) represents the force closure, which serves as the main heuristic for grasp stability. In \\(E_{fc}\\), \\(c\\) is the contact normal vector at the contact points \\(x_j = (x_j, y_j, z_j)\\), where \\(j \\in \\{1, 2, ..., 8\\}\\) is the index of contact points. Note that unlike unimanual grasping, our approach leverages 8 contact points for grasp collaboration (4 from each hand). This formulates the grasp matrix G:\n\\[G = \\begin{bmatrix} R_1 & R_2 & ... & R_4 & R_5 & ... & R_8 \\end{bmatrix}\\]\nwhere\n\\[R_j = \\begin{bmatrix} 0 & -z_j & y_j \\\\ z_j & 0 & -x_j \\\\ -y_j & x_j & 0 \\end{bmatrix}, I = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\nIn addition, \\(E_{vew}\\) is defined to prevent the Gram matrix \\(GG^T\\) from being ill-conditioned. This ensures that the grasp can effectively resist small external wrench disturbances from any direction. Notably, optimizing \\(E_{fc}\\) and \\(E_{vew}\\) establishes the differential force closure condition proposed by [12].\nWe also prevent penetration failures by accounting for the following penetration patterns: (1) between two hands and the object; (2) between the left and right hands; and (3) within each hand and itself. The energy terms that prevent these three types of penetrations are \\(E_{objpen}\\), \\(E_{bimpen}\\), and \\(E_{selfpen}\\), respectively. Each of these energy terms is calculated with the distance between some anchor points from \\(P(H_1)\\) and \\(P(H_2)\\), unless it is lower than a fixed small threshold \\(\\epsilon\\).\nIn addition, an energy term \\(E_{joint}\\) is introduced to handle joint limit violations. For each joint angle \\(\\theta_i\\) with \\(i \\in \\{1, 2, ..., 44\\}\\), we denote \\(\\theta_{min}^i\\) as its lower limit and \\(\\theta_{max}^i\\) as its upper limit. If it is outside [\\(\\theta_{min}^i\\), \\(\\theta_{max}^i\\)], the violation is penalized using the out-of-range value, in form of \\(\\max(0 - \\theta_{max}^i, 0) + \\max(\\theta_{min}^i - \\theta, 0)\\).\nWe jointly optimize the weighted sum of all the aforementioned energy terms. Given the non-convexity of the energy function, we employ the Metropolis-adjusted Langevin algo- rithm (MALA) optimizer, which introduces stochasticity to circumvent local optima [12]. The hand configurations during optimization process are showcased in Fig. 4."}, {"title": "C. Dataset Generation", "content": "To prepare data for training generative models, we syn- thesized grasp poses for a dataset of objects from Google's Scanned Objects (GSO) Dataset [40]. The manipulator used is a pair of Shadow Hands. Each Shadow Hand has 22 actuated joints (denoted as \\(\\theta_i\\) each), and a 6 dimensional rigid body pose. For both manipulators, our action space has (22+6) \u00d7 2 = 56 dimensions in total.\nWe followed the grasp synthesis procedures outlined in Sec. III-B to generate an initial set of grasp poses. Then, we used the Isaac Gym environment [13] to label whether a grasp can successfully lift and hold objects, as shown in Fig. 5. The friction coefficient for both the objects and hands are fixed at 3 (same as [4], [26]). To control the motor output, we employed a PD controller in Isaac Gym, with stiffness \\(K_p = 1000.0\\) and damping \\(K_d = 10.0\\).\nA grasp configuration is labeled as successful if the object remained in the hand for 2.0 seconds of simulation time (i.e., 120 steps at 60 Hz) over 6 evaluation trials under a gravity of 9.8 \\(m \\cdot s^{-2}\\). During each evaluation, the object and hands were randomly rotated together to verify the grasp under varying gravity force directions.\nWe also checked for the three types of penetration described in Sec. III-B. If the total penetrations exceeded 1.5 mm, the grasp configuration fails the evaluation. All successful grasps, along with the objects they were conditioned on, were saved into our BimanGrasp-Dataset."}, {"title": "D. Grasp Generation Through Data Driven approach", "content": "We then developed a generative model based on the dataset of successfully grasped poses. The proposed generative model employed the Denoising Diffusion Probabilistic Models (DDPM) architecture [41], which is widely used for various generative tasks. Conditioned on a feature vector \\(O \\in \\mathbb{R}^{1024}\\) (extracted by PointNet [42] from a point cloud sampled from O), the DDPM model aims to transform standard Gaussian noise \\(h \\sim \\mathcal{N}(0; I)\\) into a grasp pose \\(h^0\\). This was achieved through an iterative denoising process, as described in Eq. (4):\n\\[p_\\theta(h^0|O) = p(h^T) \\prod_{t=1}^T p_\\theta(h^{t-1}|h^t, O)\\]\nFor each stage of the denoising process, the implementation was based on the reparameterization trick [43], as given in Eq. (5). We leveraged the U-Net model to predict the mean \\(\\mu_\\theta\\) and the standard deviation \\(\\Sigma_\\theta\\) of the noise added at each diffusion step.\n\\[p(h^{t-1}|h^t, O) = \\mathcal{N}(h^t; \\mu_\\theta(h^t, t, O), \\Sigma_\\theta(h^t, t, O))\\]\nThen, the learning objective is given in Eq. (6):\n\\[L_\\theta(h^0|O) = \\mathbb{E}_{t, \\epsilon, h^0} [||\\epsilon - \\epsilon_\\theta(\\sqrt{\\alpha_t}h^0 + \\sqrt{1 - \\alpha_t} \\epsilon, t, O)||^2].\\]\nwhere \\(\\epsilon\\) is the noise to estimate; \\(\\epsilon_\\theta\\) is the noise predicted by the model; t represents the denoising steps; \\(\\alpha_t\\), defined as \\(\\prod_{k=1}^t \\bar{\\alpha}_k\\), represents the noise intensity.\nNevertheless, grasp poses directly obtained from the DDPM model could be infeasible. This is mainly because pene- trations are not explicitly considered during the generative process. While preventing DDPM from generating penetrated grasps requires nontrivial customization, we leverage a post- processing technique. That is, after obtaining the grasp poses from DDPM, we improve the poses by optimizing on energy terms defined in Table I. We use much fewer (100) steps, compared to 10000 steps in BimanGrasp Algorithm, to keep the computational efficiency."}, {"title": "IV. EXPERIMENTS", "content": "To validate our proposed approaches, we conducted experi- ments to: 1) evaluate the performance of the BimanGrasp algo- rithm quanlitatively and quantitatively (Sec. IV-A); 2) evaluate the performance of the DDPM trained on the BimanGrasp- Dataset (Sec. IV-B). Last but not least, 3) we provide discus- sions on experiments, ablation studies, and broader insights (Sec. IV-C)."}, {"title": "A. Evaluation on Bimanual Grasp Synthesis", "content": "Visualization of Grasps. First, we evaluate our proposed BimanGrasp algorithm on everyday objects from the GSO [40] dataset. These objects include various household items, such as containers and kitchen utensils, as shown in Fig. 6. The generated grasps for these daily-life objects are diverse. For instance, some grasps secure the body of a cylindrical container, while others pinch the edges of a box. These results successfully demonstrate the reliable generation of grasp poses for objects with diverse shapes.\nOne question is whether the generated grasps are human- like [4], [25]. To validate this, we adopted the evaluation approach used in [25]. We employed GPT-4 Vision to score each bimanual grasp on a scale of 1 to 3 points (evaluating 1,000 grasps, with 3 views per grasp). The average score obtained was 2.67.\nGrasp Success Rate. Next, we quantitatively evaluate the quality of the grasp poses synthesized by BimanGrasp. The evaluation was conducted on the synthesized 450k bimanual grasps (900 objects, with 500 poses per object). A grasp pose is considered successful if it meets two criteria: 1) no penetration occurs, and 2) object does not slip away during the physics verification process, as outlined in Sec. III-C. During physics verification, all 900 objects were used in their original sizes provided in [40], with a density of \\(\\rho = 2500 kg \\cdot m^{-3}\\) and object friction coefficient of 3 (following setting [4], [26]).\nOur experimental results visualize the relationship between the object's diameter (d) versus grasp success rate, as illustrated in Fig. 7. We compared our bimanual grasping strategy with two unimanual grasp baselines from a current state-of- the-art approach [4]. After benchmarking all grasp poses, we visualized the success rate distribution in Fig. 7. Objects were grouped into seven categories based on their diameters. The results show that our bimanual grasping strategy consistently achieves a higher success rate than unimanual strategies across all object sizes. Furthermore, the performance advantage of the bimanual strategy increases with object diameter. Unimanual grasps almost entirely fail for objects larger than \\(d = 0.5\\) m, while bimanual grasping remains effective for objects up to \\(d = 0.7\\) m. This highlights the superior effectiveness of bimanual grasps, especially for larger objects.\nIn addition, we evaluate the grasp robustness by varying the friction coefficient. To do this, we fix the object density at 2500 \\(kg \\cdot m^{-3}\\) and allow the object friction coefficient to range from 0.5 to 3.0 (following [44]). The overall simulation success rate is shown in Table III. When the friction coeffi- cient was significantly reduced to 0.5, the grasp success rate decreased to 45.40% from 54.03%, indicating that around 84% of all grasps are still valid. This demonstrates the robustness of our generated grasps."}, {"title": "B. Evaluation on the Bimanual DDPM", "content": "Using the synthesized dataset, we trained a DDPM model following procedures described in Sec. III-D. The training was performed on the successful grasps from BimanGrasp- Dataset, (randomly selected 900 \u00d7 75% = 675 objects). We then evaluated the grasping success rate on the remaining 25% of objects (225 unseen objects) using Isaac Gym with 500 grasps per object. The performance of the synthesized grasps for unseen objects was evaluated under a uniform diameter of d = 0.2 m and varying object densities \\(\\rho = 5000\\), 2500, and 500 \\(kg \\cdot m^{-3}\\). The average success rate was 42.39% for \\(\\rho = 5000 kg \\cdot m^{-3}\\), 54.06% for \\(\\rho = 2500 kg \\cdot m^{-3}\\), and 69.87% for \\(\\rho = 2500 kg \\cdot m^{-3}\\). These results are comparable to the success rates of the analytically synthesized bimanual grasp poses from BimanGrasp algorithm.\nNext, we aimed to assess whether the performance of Bimanual-DDPM is comparable to other methods. Since no existing methods currently address bimanual grasp synthesis, we manually crafted two baselines. (1) We customized a Conditional Variational Autoencoder (CVAE) [23] and re- trained the network using a bimanual grasping protocol based on the BimanGrasp dataset. (2) We used the approach from [26] to generate grasps for each hand separately (referred to as Uni2Bim (dm)), without utilizing our dataset priors that account for hand collaboration. When evaluated on objects with a density of 2500 \\(kg m^{-3}\\), CVAE achieved a success rate of 11.85%, while Uni2Bim (dm) reached 36.52%. Both baselines performed significantly worse than BimanGrasp- DDPM, although CVAE has advantages in its running time (only 18 ms per grasp).\nLast, we showcase the generalizability of our model to objects from other datasets. For this, we selected 60 objects from the DDG, YCB, and ContactDB datasets (which differ from the GSO Dataset used for training). We randomly scaled object diameters within the range [0.2 m,0.4 m]. The object density was fixed at 1000 \\(kg \\cdot m^{-3}\\). We achieved an average success rate of 63.23%. Together with grasp poses generated for GSO objects, the visualizations are shown in Fig. 8."}, {"title": "C. Discussions", "content": "Limitations. While we have demonstrated the effective- ness of our BimanGrasp-DDPM model, the DDPM's grasp synthesis process does not explicitly consider penetration. Consequently, the generative model can still produce infeasible grasp poses. This issue is currently mitigated through a post- processing step. As previously described in Sec. III-D. We anticipate that this limitation can be addressed by leveraging more recent diffusion models that account for physical con- straints, such as the approaches described in [45].\nIn addition, the algorithms may generate grasp poses that are not human-like. Given that our dataset provides a large number of diverse grasps, we believe a possible solution is to automatically select a subset of the dataset using a visual language model scorer and then retrain the DDPM model under a human-like grasp data distribution.\nDiversity. The diversity of grasps was evaluated using an entropy metric \\(H_{mean}\\) adapted from [4], [26]. The mean entropy \\(H_{mean}\\) of grasps generated by BimanGrasp algorithm and BimanGrasp-DDPM is 4.39 and 3.72, with standard deviations \\(H_{std}\\) of 0.47 and 0.49, respectively. Together with the visualization results in Sec. IV-B, this proves that DDPM can generate multi-modal and diverse bimanual grasps.\nFailure Cases. As part of an ablation study, we highlight failure cases from the BimanGrasp algorithm synthesized grasps in Fig. 9. All these failure patterns could happen both in BimanGrasp algorithm and BimanGrasp-DDPM. Our obser- vations indicate that penetration remains the primary cause of grasp failure. The optimization procedure occasionally fails to retract the hand from objects due to local optima. Additionally, in some instances, the hands may become detached from the objects' surface, causing the object to slip away before finger attached to the object surface.\nComputational Cost. The synthesis of the BimanGrasp Dataset was achieved on a server with four Nvidia A40 GPUs. It requires 170 GB of GPU memory and takes 117 minutes to generate 4,500 grasps per batch. For DDPM, the inference stage can be accomplished only on a commercial GPU. Using an RTX 4090 GPU, we can parallelize the inference of 64 grasps, with the inference time being 8.19 seconds."}, {"title": "V. CONCLUSION", "content": "Humans naturally utilize bimanual actions for various object manipulation tasks. However, algorithms for synthesizing bi- manual dexterous hands grasping poses were barely studied so far. To bridge this gap, we proposed the BimanGrasp algorithm, which successfully synthesizes bimanual grasps for diverse objects by leveraging stochastic optimization algo- rithms guided by energy-based heuristics. These grasps are then refined through physical validation in the Isaac Gym envi- ronment. Through this process, we obtained the BimanGrasp- Dataset, which contains over 150k verified pairs of grasps for 900 objects. This dataset enables the training of data-driven models capable of accelerating the grasp synthesis process. To prove this, we developed a BimanGrasp-DDPM model that excels in offering efficient grasp poses and with grasp success rate comparable to that of the BimanGrasp algorithm.\nIn the future, we plan to further improve the grasp success rate. We also plan to conduct experimental validations of the proposed algorithms using a real-world bimanual humanoid robot. We believe that the proposed technique can provide impacts to humanoid robots by enhancing their ability to han- dle various daily life objects in homes and other unstructured environments."}]}