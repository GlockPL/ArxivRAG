{"title": "FORLAPS: An Innovative Data-Driven Reinforcement Learning Approach for Prescriptive Process Monitoring", "authors": ["Mostafa Abbasi", "Maziyar Khadivi", "Maryam Ahanga", "Patricia Lasserre", "Yves Lucet", "Homayoun Najjaran"], "abstract": "The application of AI and machine learning has garnered considerable attention and is evolving rapidly in academia and practice. However, considering the effort required for implementing AI and ML in business process management\u2014particularly regarding data quality and the skills of process analysts\u2014the potential of these technologies has not yet been fully explored. Moreover, beyond data quality issues, continuous changes in business processes introduce significant uncertainty, leading to inefficiencies and the infeasibility of using existing optimization methods. We present a novel 5-step framework called Fine-Tuned Offline Reinforcement Learning Augmented Process Sequence Optimization (FORLAPS), which aims to identify optimal execution paths in business processes using reinforcement learning. We implemented this approach on real-life event logs from our case study an energy regulator in Canada and other real-life event logs, demonstrating the feasibility of the proposed method. Additionally, to compare FORLAPS with the existing models (Permutation Feature Importance and multi-task LSTM-Based model), we experimented to evaluate its effectiveness in terms of resource savings and process time span reduction. The experimental results on real-life event log validate that FORLAPS achieves 31% savings in resource time spent and a 23% reduction in process time span. Using this innovative data augmentation technique, we propose a fine-tuned reinforcement learning approach that aims to automatically fine-tune the model by selectively increasing the average estimated Q-value in the sampled batches. The results show that we obtained a 44% performance improvement compared to the pre-trained model. This study introduces an innovative evaluation model, benchmarking its performance against earlier works using nine publicly available datasets. Robustness is ensured through experiments utilizing the Damerau-Levenshtein distance as the primary metric. In addition, we discussed the suit-", "sections": [{"title": "1. Introduction", "content": "Machine learning (ML) has been significantly impacting business processes in recent years, addressing a variety of objectives (e.g., remaining time prediction, next activity prediction, drift detection, etc.) [25]. Each objective aims to improve one or more of four key goals: quality, cost, time, and flexibility. The use of process-aware information systems and rich historical data has led to an increasing trend in implementing ML/AI methods for process improvement projects [25]. Businesses constantly strive to optimize costs and time, particularly in their operational processes. Hence, scheduling, sequencing, and queuing activities are pivotal in achieving these goals. The advent of object-centric process mining has further emphasized the importance of these efforts by accurately capturing the complexity of real-world business processes, where multiple activities often occur simultaneously.[1, 11]. While there is a substantial body of research on scheduling problems, such as job-shop and flow-shop scheduling, these methods often fall short in practical business settings. Variations in occurrence frequency, shorter durations, and increased uncertainty distinguish business process scheduling from traditional problems. Thus, the unique challenges in business process scheduling demand innovative approaches that account for extensive data and the specific nature of process activities.\nThe importance of activity sequencing and the best next activity becomes particularly critical when managing multiple revisions for a specific case. In addition to the inherent loops in business processes, the object-centric nature of processes can affect the effectiveness of cross-departmental efforts [18]. For instance, a disruption in any activity can lead to disruptions in other activities, ultimately impacting the overall process outcome. An example of this problem is an energy regulator in Canada, which operates as a public commission. They need to review multiple activities in various combinations depending on each case, with many activities potentially running in parallel. However, the disapproval of any activity within a process case can result in the disapproval of other activities (whether in progress or already approved), thereby affecting the overall process efficiency. Consequently, the client may need to resubmit the case, and the review process may be re-initiated. In this context, the sequencing and location of executing activities significantly influence the process performance. Therefore, an approach to finding the optimal pathway and minimizing the inefficiency caused by disapproved activities is crucial."}, {"title": "2. Related work", "content": "Activity and task sequence optimization and scheduling have been extensively developed over the decades, employing various exact methods, heuristic, and meta-heuristic optimization techniques [9, 10]. However, these approaches often fail to capture the inherent uncertainties within process models. While some research has explored the use of optimization methods, such as ant colony optimization, in business processes\u2014covering areas like process navigation, and simulation\u2014there remain significant challenges [17]. These include the difficulty of implementing real-life simulators for discrete event simulation, which limits the broader application of these methods in practice.\nIn addition, predictive approaches in process mining aim at tasks like automated process model discovery and next activity prediction. However, these methods, while capable of adapting to patterns in datasets, do not directly address process inefficiencies or offer improvement recommendations. In contrast, process improvement techniques such as root-cause analysis and simulation [14] focus on removing inefficiencies by eliminating unnecessary tasks and optimizing resource allocation [2]. To address these limitations, Prescriptive Process Monitoring leverages AI/ML to enable intelligent and proactive interventions. [24] introduced a prescriptive business process monitoring (PrBPM) model that optimizes \"next best actions\" based on key performance indicators (KPIs), marking one of the first approaches in this area. Unlike predictive monitoring, which predicts activities without directly optimizing for KPIs, this model uses an LSTM model for prediction and business process simulation to recommend KPI-aligned actions, ensuring process conformance and improved KPI outcomes which is throughput time (in-time value). Similarly, Agarwal et al. [3] present a KPI-based goal-oriented framework for the next activity recommendation by using a combined GAN-LSTM model and RL. Their framework predicts the next best activity and estimates the goal's likelihood, facilitating online interaction by providing real-time goal estimates at each stage. They evaluated their model on four"}, {"title": "3. Methodology", "content": "The dataset utilized is sourced from our industry partner's databases and has been extracted and transformed into an event log format. This event log specifically pertains to the approval process within the oil and gas industry, encompassing all cases submitted since 2016. It includes 70,164 events associated with approximately 14,000 applications. Within the dataset, there are 12 distinct activities (also known as Reviews), which have been anonymized to maintain confidentiality. Some cases in the dataset underwent up to 10 revisions, highlighting the importance of determining the optimal location of activities to significantly reduce inefficiency in their processes. Table 2 shows the examples of the training event log traces from the case study. As there is a limited number of possible scenarios, using a model-free approach to es-"}, {"title": "3.2. Data Driven (Offline) Reinforcement Learning", "content": "Reinforcement learning requires interaction between the agent and the environment. Consequently, the Markov Decision Process (MDP) provides a framework to describe and formulate the environment.\nDefinition: In a MDP formulation, a tuple $M = (S, A, T, d_0, r, \\gamma)$ is defined, where the system comprises distinct states (S) and actions (A). each state represents a specific configuration of activities. To guide the process toward optimal completion. The transition function"}, {"title": "3.3. Process Data Augmentation", "content": "To enhance the performance of the offline reinforcement learning algorithm, we aim to use an online interaction mechanism informed by offline observations. Hence, after updating the Q-values from the initial offline RL baseline, the model is further refined using offline-to-online fine-tuning. A key challenge is determining the number of steps required for adequate online interactions to effectively train online RL algorithms. To address this issue, we conducted experiments with 100K and 200K steps separately and compared the convergence of Q-values between isolated models and a combined model. To enhance the dataset (event logs) for training, we introduce variability by adding random noise to the timestamp fields, altering them by up to 10% of the time differences, which affects the order of activities. Additionally, we focus on valuable data by randomly removing 5% of the groups where all activities are completed and by selectively removing certain activities within the groups, while excluding groups with two or fewer activities and critical activities. This data augmentation ensures that the new dataset differs significantly from the original one since it is necessary for the Q-values for the out-of-distribution (OOD) actions to not be severely overestimated [28]. The goal of"}, {"title": "4. Results and discussion", "content": "We evaluate our algorithm's performance by continuously updating the Q-values on static datasets. The results of these experiments, illustrated in Figure 2a, display the progression of Q-values over episodes, with the shaded region representing the standard deviation. It can be observed that the average Q-value increases as training progresses for the buffers, indicating that the model has learned a more effective policy. This improvement in the policy results in higher estimated Q-values over time, demonstrating that the value estimation converges as training continues. We conducted a comprehensive hyperparameter search to analyze the impact of key parameters such as learning rate (\u03b1 \u2208 [0,1)) and discount factor (\u03b3\u2208 [0,1)) on the model's performance and set the optimal values.\nIn the next step, our aim is that an agent will yield a higher average Q-value estimate in the replay buffer with further online learning that leads to policy improvement. We aim to examine the impact of leveraging behavior both individually (Figure 2a) and in combination with online settings using pre-trained Q-values. Initially, we used the existing offline data for the preliminary steps. We then enhanced the Q-values by augmenting the dataset with 100K and 200K timesteps. Figure 2d shows that this online fine-tuning outperforms for any amount of environment interaction, converging faster early in training and also obtaining higher performance (Figures 2b and 2c). Figure 2d confirms that fine-tuning not only maintains the proficiency of pre-trained models but also enhances their performance as training progresses. Additionally, Figure 2 illustrates that the pre-training phase is crucial, with transferred representations significantly outperforming isolated online RL (100K and 200K timesteps). Fine-tuning stabilizes the Q-values and achieves a 44% improvement in performance.\nA key takeaway is that implementing isolated 100K and 200K timesteps reveals that a small dataset can effectively train the agent and reveal process patterns. In total, accelerating online interactions using offline data is vital for operations and business process management, especially where interaction costs are high and simulations can't capture all uncertainties. Lever-"}, {"title": "4.2. Evaluations", "content": "To assess the effectiveness of the proposed approach, we replayed 15,000 traces from the test set of event logs using the optimal policy. Each experience is represented as a tuple $(S_t, a_t, r_t, S_{t+1})$; for an episode $j\\in [1, K]$ at timestamp $t \\in [\\tau_i, \\tau_i + d_i]$, where $d_i$ denotes the duration of the case.\nTwo key performance indicators (KPIs) are used to compare data-driven RL against the current experience.\nSaved time span ($\\delta d$): This KPI measures the difference between the actual duration of a case and the duration of the case when activities are reordered according to the RL optimal policy.\nSaved resources time spent $(\\sum_j \\sum_t d_t)$: This quantifies the time saved by comparing the time spent under the optimal RL policy with the time spent under the actual data. It reflects the efficiency improvements in resource utilization achieved through the policy.\nTo evaluate methods for predicting the best next activity, a Multi-task LSTM-based model has been proposed to optimize key performance indicators (KPIs). Building on the approach by [24], this technique features an offline phase where two models are trained: a process prediction model (MPP) that forecasts upcoming activities and their KPIs using a multi-task deep neu-ral network (DNN), and a candidate selection model (MCS) that employs a nearest-neighbor algorithm to retrieve historically similar action sequences. In the online phase, these models work together to recommend the next best actions in real time, leveraging historical data for"}, {"title": "5. Datasets and pre-processing", "content": "In addition to our case study, we applied our approach to multiple datasets and evaluated this method against the proposed techniques in the literature. This benchmark evaluation provides a robust framework for formulating the problem across various cases and offers a valuable comparison for prescriptive process monitoring. As noted earlier, a system must define the desired outcomes, which can be either categorical or continuous. To ensure comparability, we adopted a similar outcome structure (reward setting) as in previous works, such as [7], aligning the conditions for the comparison. The formulation of the Markov Decision Process (MDP) in each case depends heavily on the properties and characteristics of the datasets.\nIn Table 6, we defined and discussed different types of outcomes and presented the results for all the datasets. The desired outcomes were defined in the following references to maintain consistency for our comparison."}, {"title": "5.1. Evaluations", "content": "One of the critical challenges in evaluation arises from the absence of real-life event logs that encompass all possible scenarios, making it difficult to comprehensively assess model performance. To address this, we designed an experiment where the dataset was split into training and testing sets (80-20%), and different methods were compared based on the similarity between the recommended path and the actual path for desired and undesired outcomes. The evaluation was performed using the average Damerau-Levenshtein distance, a metric that quantifies the minimum number of operations (insertions, deletions, or substitutions) required to transform one process trace into another. A lower Damerau-Levenshtein distance indicates a higher similarity between the traces.\nIn Table 8, we present the average of the distances between the recommended paths and the ground truth for both positive and negative outcomes. Closer alignment with the desired path signifies greater accuracy in replicating optimal process traces, even for instances not included"}, {"title": "6. Conclusion and future work", "content": "Our novel AI approach addresses the challenge of sequence optimization in business process management by introducing a data-driven method and data augmentation techniques. FOR-LAPS as a 5-step framework enhances the reliability and performance of existing Reinforcement Learning models through event log augmentation and offline RL fine-tuning.\nThe proposed approach is particularly applicable to industrial scenarios with highly parallel processes, where identifying the importance of activity execution locations is crucial. We demonstrated the feasibility of this method using a real-life event log, achieving significant improvements in time span and resource time spent by 31% and 23%, respectively. Moreover, FORLAPS outperformed LSTM and PFI models in the literature concerning the defined KPIs. Unlike the previous models, FORLAPS can take continuous process outcomes, which are essential in many business process cases.\nOffline reinforcement learning faces significant challenges, including learning instabilities and poor generalization during deployment. To mitigate these issues, our hybrid RL algorithm combines the strengths of offline pre-training with online fine-tuning, as demonstrated in our experiments. Through comparative analyses, we identified key sources of uncertainty in process flows, which are directly linked to the unique number of activities and the range of possible states in the pre-collected dataset. These metrics provide a reliable measure of uncertainty and help determine the suitability and also stability of datasets for evaluating RL methods and selecting the most effective algorithm.\nAn additional evaluation method was developed and implemented to assess the agent's ability to replicate process patterns that align with desired outcomes while avoiding those lead-ing to negative results. Using the Damerau-Levenshtein distance, our findings showed that FORLAPS effectively captured process behaviors, adhering to positive patterns and avoiding negative traces in its optimal policy and activity recommendations. We evaluated FORLAPS across nine public datasets and an additional dataset featuring diverse scenarios. This evaluation incorporated an innovative event log augmentation approach, which addresses the critical issue of data scarcity\u2014a persistent challenge in training business process management (BPM) models\u2014and represents a substantial advancement in applying AI to BPM."}]}