{"title": "HIERARCHICAL AUTOREGRESSIVE TRANSFORMERS: COMBINING BYTE- AND WORD-LEVEL PROCESSING FOR ROBUST, ADAPTABLE LANGUAGE MODELS", "authors": ["Pit Neitemeier", "Bj\u00f6rn Deiseroth", "Constantin Eichenberg", "Lukas Balles"], "abstract": "Tokenization is a fundamental step in natural language processing, breaking text into units that computational models can process. While learned subword tokenizers have become the de-facto standard, they present challenges such as large vocabularies, limited adaptability to new domains or languages, and sensitivity to spelling errors and variations. To overcome these limitations, we investigate a hierarchical architecture for autoregressive language modelling that combines character-level and word-level processing. It employs a lightweight character-level encoder to convert character sequences into word embeddings, which are then processed by a word-level backbone model and decoded back into characters via a compact character-level decoder. This method retains the sequence compression benefits of word-level tokenization without relying on a rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion parameters, that hierarchical transformers match the downstream task performance of subword-tokenizer-based models while exhibiting significantly greater robustness to input perturbations. Additionally, during continued pretraining on an out-of-domain language, our model trains almost twice as fast, achieves superior performance on the target language, and retains more of its previously learned knowledge. Hierarchical transformers pave the way for NLP systems that are more robust, flexible, and generalizable across languages and domains.", "sections": [{"title": "INTRODUCTION", "content": "Tokenization plays a fundamental role in natural language processing (NLP) as it breaks down text into units that computational models can process. Two fundamental approaches are character-level and word-level tokenization. While character-level tokenization uses the \"atomic\" units of text and enjoys a small vocabulary size, it leads to long sequences with a high computational and memory cost. Conversely, word-level tokenization leads to short sequences but suffers from extremely large vocabulary sizes and the inability to process out-of-vocabulary words.\nSubword tokenization has emerged as a compromise between these two extremes and has become the standard. Common subword tokenizers are trained separately from the model on a reference corpus of text. For example, Byte Pair Encoding (BPE; Gage, 1994; Sennrich et al., 2016) builds a vocabulary starting from individual bytes and iteratively merging adjacent pairs of tokens that occur most frequently in the corpus until the desired vocabulary size is reached. The resulting subword vocabulary leads to good sequence length compression on the reference corpus, while maintaining the ability to handle out-of-vocabulary words using a byte fallback.\nHowever, subword tokenizers come with several downsides. First, contemporary models routinely use vocabulary sizes in the hundreds of thousands, making the corresponding embedding matrices and output heads extremely large. For instance, for the 8B model of the Llama-3 family (Dubey et al., 2024), with a vocabulary size of 128k, embedding and head account for roughly 13% of the model's parameter footprint. Secondly, the tokenizer is fitted in a separate step and not included in the end-to-end learning process of the model. This becomes problematic when a pretrained model"}, {"title": "HIERARCHICAL AUTOREGRESSIVE TRANSFORMERS", "content": "We now introduce our hierarchical architecture, which is a refinement and simplification of similar architectures proposed in prior works, see Section 3. Our approach relies on a splitting rule that partitions text into sequences of words. Specifically, we use UTF-8 bytes\u00b9 as our base alphabet, consisting of VB = 256 distinct values\u00b2, and split the text at Unicode whitespace characters, which are appended to the previous word. A text can then be represented as (w\u00b9, . . ., w\u3125) with w\u00b2 \u2208 [VB]li being a word of length li. Importantly, the splitting rule is the only non-trainable processing step in our method. We argue that, for natural text in (alphabetic) languages, whitespace splitting is adequate. However, our hierarchical architecture is agnostic to the type of splitting rule, allowing for alternatives that may be more appropriate for different languages or domains."}, {"title": "HIERARCHICAL ARCHITECTURE", "content": "Our architecture consists of three main components:\n\u2022 An encoder E: RN\u00d7d \u2192 RN\u00d7d, a bidirectional transformer operating on the character embeddings within each word.\n\u2022 A backbone B: RN\u00d7D \u2192 RN\u00d7D, a causal transformer operating on word embeddings.\n\u2022 A decoder D: RN\u00d7d \u2192 RN\u00d7Vs, a causal transformer with a language modelling head, operating on character level and outputting next-character prediction logits.\nIn addition, we have a character embedding C: [VB] \u2192 Rd and two projection matrices WE \u2208 RDxd and WD \u2208 Rd\u00d7D mapping between the (smaller) character-level dimension d and the (larger) word-level dimension D.\nA document (w\u00b9, ..., w\u0141) is processed by the model as explained in the following and depicted in Figure 1. Following Devlin et al. (2019), each word is prepended with a special token [W] and its characters are embedded via C,\nx = C(w)) \u2208 Rd, x[w] = C([W]) \u2208 Rd.  (1)\nThen, it is passed through the encoder. The output corresponding to the [W] token, i.e., the first entry in the sequence dimension, is selected as a word embedding:\ne\u00b2 = [E(x[w], x\u2081,...,x)]\u2081 \u2208 Rd. (2)\nThe resulting sequence of word embeddings is projected to the backbone dimension, passed through the backbone and projected back to the decoder dimension:\n\u1ebd\u00ba = W\u0118e \u2208 RD, p = [B(\u1ebd\u00b9,\u2026\u2026\u2026,\u1ebd\u00b9)]; \u2208RD, p = W\u266dp\u00b2 \u2208 Rd. (3)\nThe output for the i-th word, p\u00b2, is treated as a predictive word embedding, to be decoded into a sequence of characters matchting the next word. To that end, during training, we concatenate p\u00b2 with the character embeddings of the next word and map them through the decoder, resulting in a sequence of next-character prediction logits:\n1 = [D(p, x+1,...,x\nWe train on the loss\n(4)\nL li+1+1\n\u03a3\u03a3 (1), \u03c9+1), (5)\nwhere L denotes character-level cross-entropy loss and we set the final prediction target of each word to we+1+1 = [W], indicating the end of the word."}, {"title": "INFERENCE", "content": "Inference in our hierarchical architecture proceeds in a nested loop, as shown on the left side of Figure 1. To generate a new word, we pass the context sequence through encoder and backbone to produce a predictive next-word embedding. To materialize the next word in characters, we run an autoregressive loop of the decoder module. When a word is completed, as indicated by the prediction of a [W] token, it is appended to the input and the process is repeated."}, {"title": "COMPUTATIONAL AND MEMORY COST", "content": "We now discuss the computational cost and memory footprint of our proposed architecture, contrast-ing it with a baseline model using a subword tokenizer and a corresponding embedding matrix and language modelling head. We assume the baseline model uses  parameters in its backbone and  in its embedding and head. For the hierarchical model, we assume  parameters in the backbone and  in each of the character-level modules.\nComputational Cost. We present a simplified computational cost analysis under the assumption that the cost of a forward-backward pass through a transformer is proportional to SP, where S is the sequence length and P is the number of non-embedding parameters. This is a standard simplification based on the observation that feed-forward FLOPs dominate attention FLOPs for typical settings. An exact comparison, factoring in attention FLOPs, may be found in Appendix A.2.\nConsider a document of S characters. The computational cost of the two models depends heavily on the length of the sequence passed through the backbone, i.e., the number of words Sw and tokens ST contained in the document, respectively. The baseline model processes a sequence of length ST, passing it through the backbone and the output head, incurring a total cost of\n(6)\nIn the hierarchical architecture, the backbone processes a sequence of length Sw. Additionally, the two character-level models each process the sequence of length S plus an additional Sw word separator tokens for a total cost of\n(7)\nOur experiments will show that small en-/decoder modules are viable. For instance, in our 3B-scale experiment, the computational cost of our two character-level modules roughly equals that of the language modelling head of the baseline. Additionally, words represent a coarser unit than subwords; our pretraining dataset exhibits a ratio of Sw \u2248 0.69ST, see Figure 2 (left). Consequently, for a given cost budget, our hierarchical model will be able to operate with a larger backbone. We will revisit this for our compute-matched experiments in Section 4.\nMemory. For compute-matched models, a hierarchical architecture will have a larger parameter footprint, see Table 2. In terms of activation memory, we have to distinguish between training and inference. During training, all activations have to be stored. Our hierarchical model shrinks the number of activations in the backbone by a factor of Sw/ST and avoids large logit tensors, but operates with a slightly larger backbone and needs to store additional activations in the character-level modules. Since the exact size of the activation memory depends on the caching strategy of the auto-differentiation framework, we forego a detailed comparison.\nInference. Using KV caching (Pope et al., 2023), it is possible to achieve near parity of FLOPs during training and inference. When comparing compute-matched models, we argue that a hier-archical architecture has a modest advantage in wall-clock time performance, see Appendix A.4. Additionally we propose a scheme to further optimise inference performance using cached word embeddings. The relevant quantity for memory consumption at inference time is the size of the KV cache, which can be a bottleneck in high-throughput inference systems. Here, hierarchical models lead to a smaller memory load, as detailed in Appendix A.5."}, {"title": "RELATED WORK", "content": "The granularity of computational models for natural language has been a prominent question in NLP research since its inception (see, e.g., a review by Mielke et al., 2021). For many years, the question of character- vs word-level modelling has dominated the discussion, until subword tokenization methods, such as Byte-Pair Encoding (BPE, Sennrich et al., 2016), have emerged as a middle ground, balancing the flexibility of characters with the semantic coherence of words. Over time, subword tokenizers have become the dominant approach, gaining widespread adoption. The current landscape of NLP largely treats subword tokens as fundamental, indivisible units that are determined through a preprocessing step prior to model training.\nSome prior works have augmented word or subword token embeddings with character-level infor-mation (e.g., Ma et al., 2020; El Boukkouri et al., 2020; Aguilar et al., 2021). More recently, the T-FREE method (Deiseroth et al., 2024) operates at the word level while incorporating character information through specialized embedding and output layers. These approaches have either not tackled generative modelling or still require a fixed vocabulary for generation.\nAnother line of prior work aims to enable character- or byte-level language modelling at contem-porary scales. While some attempts have been made to train purely character-level transformers (Al-Rfou et al., 2019; Choe et al., 2019; Xue et al., 2022) these have ultimately not kept pace with subword-level models. Moving closer to the present work, some authors have presented \u201chybrid\" approaches, e.g., using downsampling mechanisms (Charformer; Tay et al., 2022), or cross-attention with a latent \"bottleneck\" sequence (Perceiver AR; Hawthorne et al., 2022) to internally condense the large sequence lengths generated by character-level models. In the following, we describe in detail the most closely-related works.\nSun et al. (2023) use a hierarchical character-word architecture for BERT-style masked language modelling. As input to the decoder, the authors concatenate the backbone output for the i-th word with the per-character encoder outputs for the same word. We devise a generative variant of this architecture. We introduce a shift by one, concatenating with embeddings of the next word and use the raw character embeddings rather than the encoder outputs, since the bidirectional encoder would otherwise \"leak\" information about future characters. Our work also substantially scales up this modelling paradigm, experimenting with models up to the 7B parameter scale, compared to Sun et al. (2023) who use models around the 100M scale.\nThe MegaByte architecture (Yu et al., 2023) uses a backbone-decoder architecture for generative modelling. Instead of splitting byte sequences into words, MegaByte chunks it into fixed-size patches of subsequent bytes. Their architecture does not use an encoder module; the input to the backbone is simply the concatenation of the embeddings of the bytes within a patch. Note that this restricts the architecture to the use of fixed-size patches. At the decoder, the backbone output is"}, {"title": "EXPERIMENTS", "content": "We proceed with an experimental investigation of the proposed method.\nModels. All models are based on the Llama architecture (Touvron et al., 2023) with a fixed atten-tion head size of 128. For the baseline model and the backbone of our hierarchical architecture, we use Llama's default 1:1 \"aspect ratio\", i.e., the number of heads equals the number of layers. The design of the character-level encoder and decoder modules is discussed in Section 4.1. The baseline model uses a BPE tokenizer with a vocabulary size of 64k, fitted on our pretraining data.\nData. We perform our main experiments using the DCLM-Baseline dataset (Li et al., 2024), which is a curated English-only pretraining dataset. Hyperparameter sweeps and ablations conducted early on in the project used the well-established Fineweb dataset (Penedo et al., 2024). For our continued pretraining experiment, we use the German portion of the Occiglot Fineweb v0.5 dataset (Brack et al., 2024). Dataloading is handled on a byte basis to guarantee that both models get to see the exact same data during training. We enforce a maximum document length of 16, 384 bytes, corresponding to roughly 4k tokens or 2.7k words. We load batches of 1024 16384 bytes, packing together documents of varying lengths with appropriate attention mask reset. In our pretraining experiments, we train for 72k steps, which comes down to a total training set size of roughly 1.2 trillion bytes.\nHyperparameters. We use the AdamW optimiser (Loshchilov & Hutter, 2019) using \u03b2\u2081 = 0.9, \u03b22 = 0.95, \u03b5 = 10-8 and weight decay coefficient \u03bb = 0.1. The learning rate is warmed up over 500 steps followed by a cosine decay to 10% of its peak value. We did not tune learning rates individually for each model but instead opted to use a well-established heuristic, scaling the learning rate inversely proportional to model width, see Appendix B.1.\nEval Metrics. We focus on downstream evaluations as the primary metric for comparison, see Appendix B.4 for a description of our eval suite. In addition, we report a more immediate metric for pretraining performance. Since we compare models making byte-level and subword-level pre-dictions, this requires some extra care. We use accuracy aggregated at the word level, as explained in Appendix B.2."}, {"title": "HIERARCHICAL ARCHITECTURE SWEEP", "content": "Our hierarchical model consists of encoder, backbone, and decoder. Given a total compute budget, we now conduct a series of experiments to determine optimal sizes (number of heads and layers) of these three modules. These experiments used the Fineweb dataset at a budget of 14.4k steps."}, {"title": "COMPUTE-MATCHED MODELS", "content": "For the following architecture comparison, we decided to compare compute-matched models, meaning models that require (on average) the same amount of compute to process a document from the pretraining dataset. Since compute is the primary driver of training and inference cost, we believe this approach ensures a fair comparison between architectures. We first set the sizes for the encoder and decoder modules of our hierar-chical architecture based on the considerations in the previous section. For each scale, we then size the backbone of the hierarchical model to match the compute required by the baseline. For the compute matching, we used the exact computa-"}, {"title": "PRETRAINING RESULTS", "content": "In this section, we compare our hierarchical architecture with a subword tokenizer baseline for pre-training on the DCLM-Baseline dataset. All models are trained from scratch. We compare compute-matched models at three scales, corresponding to 1B, 3B, and 7B baselines. Table 1 presents byte-and word-level accuracy, as well as zero-shot performance across 17 standard downstream tasks. Across scales and evaluation tasks, both architectures perform similarly, with a few notable differ-ences. At the 1B scale, the tokenizer-based model holds a modest advantage on TriviaQA, though this gap disappears at larger scales. On the other hand, the hierarchical model consistently outper-forms the baseline on the Lambada evaluation, with a relative margin of up to 68%.\nComparison to MegaByte At the 1B scale, we also conducted a comparison with MegaByte (Yu et al., 2023), another hierarchical byte-level generative architecture, as detailed in Section 3. We use a model configuration from the original paper, which is compute-matched with our 1B-scale (14 layers in the backbone with hidden dimension D = 2048, 18 layers in the decoder with hidden size d = 1024 and learning rate 2.10-4). The results in Table 1 show that MegaByte underperforms across all but one evaluation tasks. To further investigate, we conducted an ablation combining our hierarchical architecture with MegaByte's fixed 8-byte splitting, labeled as Hierarchical (8-byte split) in the table. This variant significantly improves over MegaByte (e.g., a 2.6 ppt increase in byte accuracy), showing that our hierarchical architecture is more performant even when using the same splitting rule. However, the hierarchical architecture with whitespace splitting still comes out on top, suggesting that a semantically meaningful splitting is a valuable inductive bias for the model."}, {"title": "ROBUSTNESS AGAINST INPUT PERTURBATIONS", "content": "Next, we uate the robustness of hierarchical and baseline models against perturbations of the inputs. We conduct this experiment on a subset of five eval tasks, for which the two architectures showed similar performance. We apply perturbations to the prompt of each item of the dataset and measure the change in average accuracy compared to each model's performance on the original (unperturbed) golden answer. The perturbations include permuting, randomizing, or deleting 10% of the characters per word, as well as changing the prompt to all caps. The results are depicted in Figure 4. We see that the hierarchical model is significantly more robust than the tokenizer-based model across"}, {"title": "ADAPTATION ON CROSS-LINGUAL CONTINUED PRETRAINING", "content": "After pretraining on the English-only DCLM-Baseline dataset, we continue training the 3B-scale models on the German Occiglot dataset (Brack et al., 2024) to test adaption to a shift in data distribution. We re-warm the learning rate to half of its initial value and train for 20k steps with otherwise identical settings as the pretraining runs. We conducted down-stream evaluations every 2000 steps on a set of tasks for which both English and German ver-sions are available, see Appendix B.6. The re-sults are shown in Figure 5. The hierarchical model consistently achieves higher average ac-curacy on the German evaluations while also re-taining better scores on the English tasks. Since the tokenizer operates on a rigid vocabulary, it is unable to adapt to the new domain and must resort to byte fallback or combine tokens in sta-tistically unfounded ways. We attribute the per-formance difference to the resulting larger dis-tribution shift in the input of the tokenizer based model.\nIt is important to note that the models compared here have been compute-matched based on the statistics of the English DCLM dataset. As shown in Figure 2 (right), the word and token statistics shift drastically when switching to the German dataset, with bytes-per-token decreasing substantially due to tokenizer fragmentation. Consequently, the token sequence length grows much larger than"}, {"title": "CONCLUSION", "content": "We presented a hierarchical autoregressive transformer architecture that integrates character-level and word-level processing. Our approach retains the sequence length compression of word-level tokenization, while removing the need for a rigid, predefined vocabulary. Through extensive exper-iments, including models scaled up to 7 billion parameters, we demonstrated that the hierarchical architecture matches the downstream task performance of computed-matched tokenizer-based mod-els, while significantly improving robustness to input perturbations and continued pretraining on out-of-distribution data, such as previously undersampled languages. These findings highlight the potential of hierarchical transformers to enhance flexibility, robustness, and generalization across diverse NLP tasks and domains.\nLimitations. The whitespace splitting we used above is tailored to alphabetic languages rather than logographic languages like Chinese, where characters represent entire words or morphemes. These languages may benefit from a custom splitting rule to group bytes into semantically meaning-ful units. A similar concern holds for domains like mathematical writing or code, where whitespace splitting might not yield an optimal chunking. In follow-up experiments, presented in Appendix C, we obtained promising results using a \u201cuniversal\u201d splitter based on the Unicode standard. Secondly, as discussed above, for a compute-matched model, the hierarchical architecture will have a higher parameter footprint. During inference, this may be offset by a reduced size of the KV cache.\nOutlook. Our work can be extended in various ways. First, one could experiment with different models for encoder and decoder. For example, the small character vocabulary, may facilitate multi-token prediction (Gloeckle et al., 2024) with multiple output heads. Further, with few characters per word, a text diffusion model (e.g., Li et al., 2022) could be used as a decoder. Finally, one could investigate additional levels of hierarchy, such as sentences or paragraphs, which may improve long context generation abilities."}, {"title": "MODEL DETAILS", "content": "Splitting Rule We split at whitespace characters, as per the Unicode standard, which includes spaces, tabs, newlines, et cetera. All consecutive whitespace characters are appended to the previous word. Our approach is agnostic to the type of splitting rule. For instance, in Section 4, we experiment with a fixed-size splitting, as in Yu et al. (2023). Other splitting rules me be adequate for non-alphabetic languages or domains like mathematical writing or code. We leave this to future work.\nEnd of Document We append a final dummy word to the end of each document to indicate its end. This dummy word consists of a single special token [S] and, like every word, is prepended with a [W] token during processing. We treat [S] as a termination token during inference. During training, we omit the final [W] as prediction target for that word, since [S] already indicates termination."}, {"title": "EXACT COMPUTATIONAL COST COMPARISON", "content": "We give a detailed description of the computational cost, extending the simplified analysis presented in Section 2.3. Following common practice, we count the number of multiplications in the operations involved in a forward pass through the model, covering only matrix multiplications and ignoring biases, normalization layers, activation functions and other minor operations.\nConsider a forward pass through a single transformer layer with hidden size D and sequence length S. In a standard Llama architecture (Touvron et al., 2023), this layer has 12D2 parameters. This includes key, query, value and attention dense-out weight matrices of shape D \u00d7 D and three matrices of shape D \u00d7 D for the SwiGLU-MLP. The matrix multiplications in the forward pass require one multiplication for each weight,\nCH(S, D) = 12SD2. (8)\nAdditionally, we now factor in the multiplication of the query and key matrix, as well as the multi-plication of the attention matrix with the value matrix, resulting in an additional cost of\nCattn(S, D) = 2S2D. (9)\nNow consider a document consisting of S characters, which get split into ST tokens and Sw words, respectively. For simplicity, we assume that all words are of the same length S/Sw.\nBaseline The baseline model processes a sequence of length St, passing it through the backbone and the output head, incurring a total cost of\nI backbone Cff(ST, Dbackbone) (Feed-forward)\nbaseline Catrn (ST, Dbackbone) (Attention)\n+ ST Dbackbone VT (LM Head) (10)\nHierarchical Architecture In the hierarchical architecture, the backbone processes a sequence of length Sw. The two byte-level models process each word in the document, each of which is of length 1 + S/Sw due to the appended word separator tokens. Additionally, we have the two linear projections at the intersection of backbone and encoder/decoder, each of which incur cost of  per character. Finally, the encoder has a character-level LM head, which adds cost of  In total\n(Backbone feed-forward)\n(Backbone attention)\n(En/decoder feed-forward) (11)\n(En/decoder attention)\n(Linear projections)\n(Decoder LM head)"}, {"title": "COMPUTE MATCHING", "content": "We randomly sample 10,000 documents from our pretraining dataset. For a given model configu-ration, be it a baseline model or a hierarchical model, we can now approximate its average cost by averaging the cost formulae (Eq. 10 or 11, respectively) over our sample of documents. The cost only depends on the document lengths in characters, tokens, and words.\nThe baseline model parameters are fixed, so we can compute its average cost. For the hierarchical model, encoder and decoder size are fixed and the only variable parameter is the number of heads and layers in the backbone. We compute average cost for possible backbone sizes and choose the size whose average cost matches that of the baseline model as closely as possible. The resulting hierarchical model configurations are listed in Table 2. Since the number of backbone heads/layers is an integer quantity, the matching is not exact, but the relative deviation is smaller than 5% across all configurations."}, {"title": "INFERENCE-TIME PERFORMANCE", "content": "We briefly discuss the inference-time performance of our hierarchical model compared to a compute-matched baseline model. We assume we use KV caching and ignore attention FLOPs. Then the average FLOPs required to generate S bytes is matched\n(12)\nWall-clock time is proportional to cost if all architectures process single tokens, i.e.,\n(13)\nHowever, in an inference setting, the hierarchical architecture has the advantage that the encoder will always ever be queried with entire words, not single characters. Since the wall-clock time of forward pass in KV-cached inference is typically dominated by I/O operations, the encoder will take a similar time to process an entire word as it would to process a single character. Hence, in terms of wall-clock time, we will have\n(14)\nThis advantage could be further expanded with a simple inference-time performance optimisation of our hierarchical model. After training, one could extract and store the word embeddings (i.e., encoder outputs) for the Vw most common words in a reference corpus. One could probably cover > 95% of words with a very modest vocabulary size. For words in this vocabulary, the encoder stage is then replaced with an O(1) lookup. Likewise, for the decoder, one could store predictive word embeddings for the words in the corpus. This match could be used for speculative decoding or even be accepted as is if the match is \"good enough\". We leave the implementation and evaluation of this performance optimisation to future work."}, {"title": "INFERENCE-TIME MEMORY", "content": "The size of the KV cache is proportional to SLD, where S is the sequence length, L is the num-ber of layers and D is the hidden dimension. For the hierarchical model, one would only cache activations in the backbone in any practical setting. Encoder and decoder operate on very small sequence lengths at inference time, where KV caching would not yield wall-clock time speedups, see also Appendix A.4. Hence, we compare"}, {"title": "TRAINING SETTINGS", "content": "Following Dubey et al. (2024), the peak learning rate is set to lr32 = 3\u00b710-4 for the 32 head model and scaled with model size in terms of number of heads as Ir(H) = lr32. For the hierarchical model, we use the number of heads in the backbone; we briefly verified that the heuristic is adequate for the hierarchical architecture. Since it is not tailored to a hierarchical model, there might be room for further improvement."}, {"title": "WORD-LEVEL ACCURACY", "content": "For a given input sequence 11:T and a segment 21:t, we denote the model's prediction as m(x1:t) \u2208 [0,1]|B|, which is a vector of predictive probabilities for the next byte xt+1. Byte-level accuracy is\n (15)\nNow assume the byte sequence is segmented into words, given by a set of indices 81,... sw \u2208 [T] indicating the first character of a word. The next-byte probabilities auto-regressively imply next-word probabilities, for which we can compute a word-level accuracy as\n (16)"}, {"title": "HIERARCHICAL ARCHITECTURE SWEEP", "content": "Aspect Ratio We first ran an experiment concerning the heads:layers aspect ratio in encoder and decoder, wanting to understand whether it may be beneficial to deviate from the 1:1 ratio used in a standard Llama architecture. For this experiment, we tied the sizes of encoder and decoder (Ld = Le, Hd = He) and tested different values for the aspect ratio. We hypothesized that a \u201cwider\" model may be beneficial and tested aspect ratios 1:1, 3:2, and 2:1. The result is depicted in Figure 6. Since the result didn't show any considerable difference between different aspect ratios, we went with 2:1 based on the intuition that the change in hidden size between character-level and word-level modules should be limited when scaling to larger backbones.\nEncoder/Decoder Balance Next, we ran an experiment to decide how encoder and decoder should be sized relative to each other. We used He = Ha = 8 and allocated a total number of Le + La = 8 layers to be distributed between encoder and decoder. As shown in Figure 7, the best word accuracy is achieved by an even number of layers in the two modules. Similar as in Figure 3, byte-level accuracy favors a larger decoder module."}, {"title": "EVALUATION TASKS", "content": "We use a set of established downstream evaluation tasks, implemented in evaluation suites like the Eleuther AI eval harness (Gao et al., 2024). In the following, we give brief descriptions as well as citations. The descriptions are quotes either from the original paper or from Li et al. (2024).\n\u2022 MMLU (Hendrycks et al., 2021) is a 4-way multiple choice question answering dataset that covers 57 different domains and tasks, evaluating both world knowledge and problem solving capabilities."}, {"title": "DETAILS ON ROBUSTNESS EVALUATIONS", "content": "Figure 8 shows the complete robustness results, initially reported in Section 4.4, displayed separately for each eval task."}, {"title": "DETAILS ON CONTINUED PRETRAINING EXPERIMENT", "content": "We average over the following benchmarks, for which we have English and German versions:\n\u2022 HellaSwag, machine translated (Zellers et al., 2019; Pl\u00fcster, 2024)\n\u2022 Arc Challenge, machine translated (Clark et al., 2018; Pl\u00fcster, 2024)\n\u2022 Truthfulqa, machine translated (Lin et al., 2022; Pl\u00fcster, 2024)\n\u2022 Lambada OpenAI, machine translated (Gao et al., 2024)\n\u2022 MMLU, Human translated (MMMLU) (Hendrycks et al., 2021; OpenAI, 2024)"}, {"title": "SPLIT RULE", "content": "The Unicode Standard, in particular Unicode Standard Annex #29, provides \"guidelines for deter-mining default segmentation boundaries between certain significant text elements\", which includes word segmentation. This also covers word boundaries for non-alphabetic languages. We use the uniseg Python package\u00b3 to split text into words according to this standard. In addition, we have found it to be beneficial to split text at punctuation. To improve sequence compression, we merge leading whitespaces and trailing punctuation into words. In the following, we refer to this splitting rule as the Unicode splitter."}, {"title": "PRETRAINING RESULTS", "content": "We repeat our DCLM-Baseline pretraining experiments using the Unicode splitter. All training details stay as described in Section 4. The byte-per-word statistics of the new splitter result in the same flop-matched model sizes described in Section 4.2. The results are shown in Table 5, where we can see that the Unicode splitter outperforms the whitespace splitter on the majority of eval tasks."}, {"title": "CROSS-LINGUAL CONTINUED PRETRAINING ON CHINESE DATA", "content": "Next, to test cross-lingual adaptation to a non-alphabetic language, we perform a continued pretrain-ing on the Skypile dataset (Wei et al., 2023), which is a Chinese pretraining dataset. This experiment has been done using the 3B model scale using 5k steps; all other experimental details match those described in the German experiment presented in Section 4.5. We compare the hierarchical archi-tecture using the Unicode splitter to our tokenizer baseline.\nSince we did not have access to Chinese-language downstream evals, we report bits per byte (bpb)4. The learning curves are depicted in Figure 9 and the final bits per byte on a held-out portion of SykPile is shown in Table 6. We see that the hierarchical architecture reduces bpb more rapidly and reaches significantly better values within the assigned training step budget."}, {"title": "INFERENCE PERFORMANCE ON OUT-OF-DISTRIBUTION DATA", "content": "We also applied the models pretrained on DCLM to out-of-distribution text without any additional training. We use the GitHub Codes, OpenWebMath (Paster et al., 2023), Pile of Law (Henderson et al., 2022), and SkyPile datasets. We report bits per byte in Table 7. Overall, there are no signifi-cant differences between the hierarchial and the tokenizer-based model. However, we also computed inference FLOPs on these out-of-distribution datasets, shown in Table 8. As in our continued pre-training experiments, we see a significant advantage for the hierarchical model."}]}