{"title": "Social Media Algorithms Can Shape Affective Polarization via Exposure to Antidemocratic Attitudes and Partisan Animosity", "authors": ["Tiziano Piccardi", "Martin Saveski", "Chenyan Jia", "Jeffrey T. Hancock", "Jeanne L. Tsai", "Michael Bernstein"], "abstract": "There is widespread concern about the negative impacts of social media feed ranking algorithms on political polarization. Leveraging advancements in large language models (LLMs), we develop an approach to re-rank feeds in real-time to test the effects of content that is likely to polarize: expressions of antidemocratic attitudes and partisan animosity (AAPA). In a preregistered 10-day field experiment on X/Twitter with 1,256 consented participants, we increase or decrease participants' exposure to AAPA in their algorithmically curated feeds. We observe more positive outparty feelings when AAPA exposure is decreased and more negative outparty feelings when AAPA exposure is increased. Exposure to AAPA content also results in an immediate increase in negative emotions, such as sadness and anger. The interventions do not significantly impact traditional engagement metrics such as re-post and favorite rates. These findings highlight a potential pathway for developing feed algorithms that mitigate affective polarization by addressing content that undermines the shared values required for a healthy democracy.", "sections": [{"title": "Main", "content": "Social media algorithms profoundly impact our lives: they curate what we see [1] in ways that can shape our opinions [2-4], our moods [5-7], and our actions [8-12]. Due to the power that these ranking algorithms have to direct our attention, many have expressed concern that they may be exacerbating negative societal outcomes [13-17]. Of particular concern is whether feed algorithms cause affective polarization-hostility toward opposing political parties [18-20]. If social media algorithms are causing affective polarization, they might not only bear responsibility for rising political incivility online [21] but also pose a risk to trust in democratic institutions [22]. In this case, isolating the algorithmic design choices that cause polarization could offer alternative algorithmic approaches.\nResearch investigating this question has so far yielded mixed results due to limited collaboration between platforms and researchers, frequent changes in feed-ranking algorithms, and the lack of strong incentives for platforms to address the issue [23]. A major hypothesized mechanism for how feed algorithms cause polarization is a self-reinforcing engagement loop: users engage with content aligning with their political views, the feed algorithm interprets this engagement as a positive signal, and the algorithm exposes even more politically aligned content to users, leading to a polarizing cycle. Some studies support this hypothesis, finding that online interactions exacerbate polarization [24], potentially because of the increased visibility of hostile political discussions [25], divisive language [26-30], and content that reinforces existing beliefs [31]. However, large-scale field experiments aimed at reducing polarization by intervening on the feed algorithm, for example by increasing exposure to out-party content, have found both a decrease [32] and increase [33] in polarization. Similarly, recent large-scale experiments on Facebook and Instagram found no evidence that exposure to in-party sources or a simpler reverse-chronological algorithm affected polarization and political attitudes [34, 35] during the 2020 U.S. election [36]. These mixed results have led to difficulty in identifying what, if any, algorithmic intervention might help.\nWe can distill the goals of these prior interventions to an alternative hypothesis: that feed algorithms cause affective polarization by exposing us to content that polarizes. An algorithm that upranks genuine political dialogue is less likely to polarize than an algorithm that upranks demagoguery. This hypothesis has been difficult to operationalize into interventions, making studies that intervene on cross-partisan exposure and reverse-chronological ranking attractive but more diffuse in their impact and thus more likely to observe mixed results. However, recent advances in AI large language models (LLMs) now provide an opportunity to create a ranking intervention that more directly targets the focal hypothesis [37]. We draw in particular on a recent large-scale field experiment that articulated eight categories of antidemocratic attitudes and partisan animosity as bipartisan threats to the healthy functioning of democracy [38]. We operationalize these eight categories into an AI classifier that labels expressions of these constructs in social media posts, does so as accurately as trained annotators agree with each other, and produces the desired depolarization effects in a lab setting on a fixed feed [39]. This real-time classification enables us to perform a scalable, content-based, real-time reranking experiment on participants' own feeds in the field [40]."}, {"title": "Exposure to Antidemocratic Attitudes and Partisan Animosity", "content": "There are many viable theories as to which content, if amplified, could impact affective polarization. In this experiment, we draw specifically on prior work that articulates eight factors as problematic outcomes for healthy democratic functioning, referred to as \u201cantidemocratic attitudes and partisan animosity\u201d [38]: (1) partisan animosity, (2) support for undemocratic practices, (3) support for partisan violence, (4) support for undemocratic candidates, (5) opposition to bipartisanship, (6) social distrust, (7) social distance, and (8) biased evaluation of politicized facts (Sec. A.1). We refer to these eight factors collectively as AAPA (for Antidemocratic Attitudes and Partisan Animosity), and we hypothesize that increased or decreased exposure to AAPA content will cause corresponding changes to affective polarization. Our feed ranking intervention first identifies only political content in the feed, using a broad definition from the Pew Research Center [44] that includes mentions of officials and activists, social issues, news, and current events (Sec. B.3). We then score each political post from zero to eight based on the number of the eight AAPA factors expressed in its text, and we classify a post as AAPA if it reflects at least four of these eight factors. This scoring is performed by a large language model (Sec. B.4), which shows a high correlation with human judgment on identifying these eight factors in social media posts [39]."}, {"title": "Experimental design", "content": "We designed two parallel experiments to measure the impact of reducing and increasing exposure to AAPA posts (Fig. 1). The first, a \"Reduced Exposure\" experiment, downranked all AAPA content, pushing it lower in their feeds. In contrast, the second, an \"Increased Exposure\" experiment, upranked attitudinally-aligned AAPA posts into participants' feeds. Both experiments are randomized controlled trials. After providing informed consent and installing our web extension, participants were randomly assigned to one of the two experiments and then randomly divided into treatment or control conditions for each experiment. The overall experiment spanned ten days, with the first three days serving as a baseline period, during which no intervention was applied. During the subsequent seven days, participants in the treatment groups received the modified feed (downranked or upranked AAPA posts), while those in the control groups continued to receive the original, unmodified feed."}, {"title": "Feed Reranking", "content": "To administer the interventions, we developed a browser extension for Google Chrome and Microsoft Edge that reranks participants' feeds in real time [40]. The extension activates when participants access the \"For You\" feed on X. It intercepts the participants' original feed and sends it to a remote backend, which scores and reranks the posts (Sec. B). This process adds a latency of about three seconds when opening the feed for the first time, which is consistent across all experimental conditions. To rerank the original feeds, we first identify political posts (Sec. B.3) and then determine whether those posts are classified as AAPA.\nIn the Reduced Exposure treatment condition, we downrank all AAPA posts in the participants' feeds. If participants continue to scroll down far enough, e.g., across several incremental loads of more content, these demoted posts will appear (Fig. A6). Before downranking all the AAPA posts, we randomly select the position of one AAPA post and, if the in-feed survey is sampled to be included, insert the post with the survey in the next available position (Sec. A.2.1). This survey position is consistent across treatment and control conditions. Thus, participants in the control condition will see AAPA posts and the in-feed survey, while participants in the treatment condition will see the survey in the position where the first AAPA post used to be, but all AAPA posts are placed further down. The survey is limited to only a few times per day to avoid fatigue (Sec. 5.2). In the Increased Exposure experiment, since we lack access to the platform's full post inventory, we sourced AAPA posts from the inventory of all posts in the \"For You\" feeds of all other study participants. We selected a random position in the original feed and upranked a random AAPA post to that position. The post containing the survey, if included, is inserted in the next available position. In the control condition, the survey is still added in the randomly sampled"}, {"title": "Results", "content": "We recruited participants from two online survey platforms, CloudResearch and Bovitz-Forthright [46]. We targeted participants who use X, are 18 or older, reside in the United States, and self-identify as either Democrats or Republicans (Sec. C). After obtaining informed consent, participants installed our web extension. To ensure the participants had enough political content, we used a pre-screening task to filter for users whose \"For You\" feeds contained at least 5% of posts related to politics or social issues. The participants were recruited during the U.S. election season, between July 7th and August 14th, 2024, and instructed to use X daily only from the web interface.\nWe preregistered a target of at least 1,100 participants to complete the full study, broken down as 500 in the Increased Exposure experiment and 600 in the Reduced Exposure experiment based on power analyses performed on pilot studies. Toward this goal, a total of 1,662 participants completed the initial survey and qualified for inclusion in the study (Fig. C10). Participants were randomly assigned to either the Increased or Reduced Exposure group until the preregistered experiment-specific quotas were reached [47]. Of the qualified participants, N=1,256 completed the full study by responding to follow-up surveys the day after the end of the study: 727 in the Reduced Exposure experiment and 529 in the Increased Exposure experiment. Both treatment and control groups were well balanced across covariates, and there was no evidence of selective attrition between conditions (Sec. 5.3).\nOn average, participants viewed 158 posts per day (median = 94) and spent 43.8 minutes (median = 19.2 minutes) per day with a browser tab in the foreground on their X feed. During the baseline period, an average of 32% of the posts participants viewed were related to politics or social issues. Of these, 33.1% were classified as AAPA, which comprised an average of 10.6% of the total feed content.\nThe interventions had the intended effects (Fig. A8). During the baseline period, participants across all groups were exposed to political posts with similar average AAPA scores. However, after the intervention began, participants in the treatment condition of the Reduced Exposure experiment experienced a noticeable decrease in exposure to such posts (-91.2% compared to control), while those in the treatment condition of the Increased Exposure experiment were exposed to more AAPA posts (+23.1% compared to control), resulting in higher average scores.\nOver the full ten days of the experiment, participants answered an average of 13.8 (median = 11) in-feed affective polarization questions in the Increased Exposure experiment and 12.6 (median = 9) in the Reduced Exposure experiment. Similarly, they answered an average of 14.0 (median = 11) emotion surveys in the Increased"}, {"title": "Affective polarization", "content": "Exposure to AAPA significantly influenced affective polarization (Fig. 2). Participants in the reduced AAPA exposure experiment reported a significant post-experiment effect, corresponding to an outparty warmth increase of 2.11 degrees (95% CIs: [0.15, 4.06]; P=0.035). Conversely, participants in the Increased Exposure experiment exhibited a symmetric reduction in affective warmth toward the opposing party of -2.48 degrees (95% CIs: [-4.79, -0.17]; P=0.036). We did not observe any significant heterogeneity in treatment effects across the preregistered moderators (Sec. E.5), including party identification, indicating that the effect was consistent and bipartisan. As is common [48], we contextualize these effects against the rising levels of partisan animosity in American society over time: our effects correspond to a decrease and increase in affective polarization equivalent to 3 and 3.6 years, respectively. We observed even stronger effects with the in-feed assessment, with Reduced Exposure causing an increase of 3.24 degrees (95% CIs: [1.21, 5.27]; P=0.002) and Increased Exposure causing a cooling of -2.56 degrees (95% CIs: [-4.53, -0.59]; P=0.011).\nIn an exploratory analysis, we investigated which AAPA factors were predictive of changes in affective polarization. Specifically, a higher proportion of posts supporting Partisan Animosity (95% CIs: [-13.49, -2.61]; P=0.003), Opposition to Bipartisan"}, {"title": "Emotions", "content": "Changes to the feed algorithm caused in-feed, but not post-experiment, changes to participants' negative emotions (Fig. 3). Reduced Exposure caused a change of -5.05 points (95% CIs: [-7.68, -2.42]; Padj=0.002) for \"angry\" and -3.68 points (95% CIs: [- 6.42, -0.94]; Padj=0.024) for \u201csad.\u201d Likewise, in the Increased Exposure, participants report a symmetric increase in negative emotions: 5.13 points (95% CIs: [2.05, 8.22]; Padj=0.013) for \"angry\" and 4.38 points (95% CIs: [1.00, 7.76]; Padj=0.036) for \"sad.\" Neither intervention had an effect on the positive emotions \"calm\" and \"excited.\" Post-experiment emotion effects were not significant, suggesting that the effects were likely short-lived."}, {"title": "Engagement", "content": "We tested whether our interventions impacted platform engagement (Sec. E.4). Neither experiment observed imbalances in attrition rates across conditions. Moreover, in both experiments, the treatment had no measurable impact on daily sessions-Increased Exposure: 1.65 control vs. 1.53 treatment, n.s.; Reduced Exposure: 1.53 vs. 1.49, n.s. Likewise, there was no significant effect on reshare rates-Increased Exposure: 0.52% vs. 1.07%, n.s.; decreased exposure: 0.74% vs. 0.46%, n.s. In addition, there was no effect on favorite rates-Increased Exposure: 4.48% vs. 5.46%, n.s; decreased exposure: 4.96% vs. 4.15%, n.s. We also observed that the differences between treatment and control groups in terms of the number of new posts, replies, and time spent on X during the experimental period were not statistically significant. These null results"}, {"title": "Discussion", "content": "In this study, we conducted a field experiment to investigate the effects of feed ranking algorithms that decrease or increase exposure to contact that polarizes. Our findings demonstrate that repeated exposure to AAPA posts, particularly during heightened political tensions such as months before the US election, significantly influences users' affective polarization and emotional responses. A feed ranking algorithm that controls exposure to such content produces measurable changes, equivalent to mitigating approximately three years of affective polarization, even after just one week of treatment. Importantly, our results indicate that the intervention does not disproportionately impact any specific political leaning or demographic group, suggesting that the approach is bipartisan. The success of this method shows that it can be integrated into social media Als to mitigate harmful societal consequences. We also present evidence that this impact can likely be achieved with a limited effect on business metrics like engagement.\nLimitations of our study include a sample of partisan US users restricted to the web version of X during a specific period, which may affect the generalizability of the findings [50, 51]. Our population is limited to X users with at least 5% of political content (73.4% of the screened), but we intentionally selected this group to ensure the interventions can make a difference. Additionally, the experiment examines short- term effects immediate in-feed responses and outcomes measured the day after the experiment-but does not address whether these effects persist over time. However, in scenarios where this ranking approach is integrated into the feed algorithm, the intervention would be continuously applied. Our reliance on in-feed surveys to assess immediate responses to stimuli follows established practices of ecological momentary assessment (EMA) [45]. To mitigate the risk of demand characteristics associated with the close proximity of the assessment, we followed best practices in designing the experiment [52], providing only essential information about the study's purpose. Post-survey responses to an open-ended question about their experience during the study suggest that these precautions were effective, as participants did not report any impact on their experience attributable to their assigned condition. Describing the causal relation between social media and polarization as a unified phenomenon, of course, remains challenging: platforms are complex socio-technical systems, and small variations in design or exogenous factors can lead to different outcomes [53]. Future research should explore whether these effects persist over longer periods or in less"}, {"title": "Ethical Considerations", "content": "To minimize the risks, we have complied with all relevant ethical regulations. All procedures were approved by our university's Institutional Review Board. All participants were provided with informed consent before installing the browser extension and taking part. Participants were given the option to withdraw their consent or discontinue participation at any time without penalty. Participants received financial compensation for their participation in the current study.\nWhen it comes to any form of social media field experiment, we must consider the ethical implications given the high ecological validity of such work [59]. Just like other algorithmic feed ranking methods (e.g., [34]), our work may also inherently pose risks to participants by altering the ranking of content on their news feeds. However, today's social media platforms (e.g., X) already intervene in people's feeds and take over control of users' information stream [60]. While this work shares limitations with existing algorithmic ranking methods in that participants may cede control over their posts to a certain extent, our work does not introduce extra risks. Our downranking condition reduces exposure to AAPA posts, and the upranking condition only inserts AAPA posts from those that already exist on X. Our aim here is to mitigate risks while still carefully, causally identifying whether and how changes to feed ranking affect polarization."}, {"title": "Posts expressing Antidemocratic Attitudes and Partisan Animosity", "content": "We define AAPA based on eight variables identified [38] as potentially harmful to democratic functioning. While the previous study tested multiple interventions to measure the impact on these variables, we focus on content expressing support for these outcomes, following findings from an in-lab experiment [39].\nThe eight variables are:\n1. Partisan animosity (v1): defined as dislike for opposing partisans [18].\n2. Support for undemocratic practices (v2): defined as willingness to forgo democratic principles for partisan gain [61, 62]\n3. Support for partisan violence (v3): defined as willingness to use violent tactics against outpartisans [63]\n4. Support for undemocratic candidates (v4): defined as willingness to ignore democratic practices to elect inparty candidates [64]\n5. Opposition to bipartisan cooperation (v5): defined as resistance to cross-partisan collaboration [65]\n6. Social distrust (v6): defined as distrust of people in general [66]\n7. Social distance (v7): defined as resistance to interpersonal contact with outpartisans [67]\n8. Biased evaluation of politicized facts (v8): defined as skepticism of facts that favor the worldview of the other party [68]\nThese definitions, evaluated in previous work [39], were used to prompt the LLM (see Table B2). A post is classified as AAPA when at least four of these factors are present. Over the course of 10 days, we found that, on average, 30.6% of the political posts in the pre-intervention feed of participants were classified as AAPA (Fig. A1). The distribution of AAPA varied by participants' political orientation, with 34.25% of political posts in Republican feeds and 28.54% in Democratic feeds classified as AAPA (Fig. A2). The most common factors are Social Distance, Biased Evaluation of Politicized Facts, and Partisan Animosity (Fig. A3).\nFig. A4 shows the correlation between these factors, showing that some of them have a high chance of occurring together. Fig. A5 shows that these factors cluster in 3 groups with {v2, v3, v4}, {v5}, and {v1, v6, v7, v8} tendency to appear together."}, {"title": "Feed reranking implementation", "content": "We developed a browser extension to run the field experiment intervening in real- time in the participants' original feeds [47]. Our extension was distributed through the Google Chrome Store and supported Chrome and Edge browsers. Once the participants consented to participate, the extension is enabled every time they visit their feed on X. The extension is enabled only when the participants use the \"For You\" feed, which is the feed generated by the recommendation system. Participants can read their chronological feed at any time, but a tooltip regularly reminds them to"}, {"title": "Experimental platform", "content": "We developed a custom experimental platform focused on performance to ensure the experiment introduces minimal disruption to the user experience on X. Our implementation consists of a browser extension and a REST backend deployed as a cloud service."}, {"title": "Browser extension", "content": "The browser extension handles editing the X feed and managing all interactions with the participants, including the onboarding steps. It is compatible with Google Chrome and Microsoft Edge and requires only storage permissions to save the user identifier. The extension manages the ranking manipulation by modifying the content avail- able on the X feed in real-time. Our implementation allows us to inject custom code on the main page to override the methods of the native XMLHttpRequest object. This approach allows us to intercept the communication with the server and modify the execution flow of the requests. Our browser extension subscribes to the requests end- points used to retrieve the new content to add to the feed, interrupt the execution flow, send the server response to our backend for scoring and reranking, and finally restore the execution flow by calling the original methods with the response customized by"}, {"title": "Backend", "content": "The backend is responsible for reranking the feed, logging the client events, sending daily participation reminders by email, and orchestrating the participants' registration. The server is written in Python and Java using MongoDB and Memcached for storage and caching. When the browser extension interrupts the execution flow to request the reranking of the feed, X's rotating circular loading indicator is extended until our reranking is complete. The implementation choices are guided by minimizing the response time to ensure minimal destruction of the user experience.\nThe feed reranking proceeds in four steps: first, the response from X's server, typ- ically containing 35 posts, is sent to our backend and parsed to extract the text from the posts. The implementations focused on individual posts and in-feed conversations, ignoring the other types of content, such as ads or users' recommendations. Second, all the posts are evaluated by a political classifier (Sec. B.3) to filter the posts that must be scored for the presence of the eight factors problematic for democratic func- tioning. This service runs as a standalone Python web service, accelerated by GPU, that introduces, on average, a latency of 490 ms (Std = 111 ms). Then, the political posts are dispatched to a Java server that manages an HTTP connection pool with OpenAI APIs. The service batches the posts to reduce the number of requests and uses a pool of tokens to minimize latency and avoid the rate limit exceptions. Finally, the scores are parsed and returned to the main backend, which applies the downrank- ing intervention and adds the in-feed surveys. These steps are replicated in the same way in the control condition, but in that case, the backend returns the original feed. The downranked posts are saved in a cache (Memcached) and added to the feed when the user reaches the new position in a continuous scrolling session.\nThe backend also stores the logs of interactions with the feed. When a participant interacts with the feed on X, the extension regularly shares with the backend what actions are performed on the main page. This data is limited to the actions on the main feed and does not include any private content. The actions recorded include when a post enters the main viewport, likes, favorites, reposts, clicks on external links, and the time spent on the browser tab."}, {"title": "Political Classifier", "content": "Since our downranking intervention applies only to political posts, we developed a classifier to prefilter the relevant posts. Since GPT has a pay-per-use business model and calling an external API service call can introduce additional latency, we ensure that GPT is used to score only the content in which the eight variables are relevant. The political classifier is a distilled version of GPT-4 by fine-tuning a RoBERTa model to identify political posts, including mentions of officials and activists, social issues, news, and current events.\nThe posts used to train the model were collected with the Twitter API in July 2023. To select the posts, we replicated the reverse chronological feeds of users across the political spectrum for two weeks and took a random sample of 50,000 posts. We first selected 300 users and then collected the posts by the users they followed. To select the users, we first obtained the followers of all members of Congress. Then, we categorized each user in the superset of followers based on how many Republican and Democrat members of Congress they followed: (a) far-left: followed 4-6 Democrats and no Republicans; (b) left: followed 1-3 Democrats and no Republicans; (c) middle: followed 1-3 Democrats and 1-3 Republicans; (d) far-middle: followed 4-6 Democrats and 4-6 Republicans; (e) right: follow no Democrats and 1-3 Republicans; (f) far-right: followed no Democrats and 4-6 Republicans. We started with members of Congress since their political affiliations are known and are often used to estimate Twitter users' political leaning [69, 70]. Next, we removed follower accounts that are likely to be bots. We used the Botometer API and filtered out accounts with Complete"}, {"title": "Scoring service", "content": "The scoring service is a Java backend responsible for returning the number of each political post's eight problematic factors. It is designed to minimize latency. When a new request is received, it proceeds as follows: after the political classifier identifies the posts to score, they are sent to this service in one request as a list of textual items. This service first splits the list into chunks of at most ten items and then compiles eight prompts for each of them. The prompts are in the format:\nDo the following messages express partisan animosity?\nPartisan animosity is defined as \"dislike for opposing partisans\"."}, {"title": "Participant characteristics", "content": "To enroll in the experiment, participants had to take a screening task by installing a browser extension. The extension opened X, asked participants to log in if necessary, scrolled through their feed automatically, and verified that at least 5% of the posts in their \"For You\" feed were about politics or social issues. Fig. C12 summarizes the distribution of users by the fraction of political content. Around 73% of the participants qualified and were invited to the full study.\nFigure C11 shows the distributions of some of the participants' demographic information collected in the pre-survey, including race, education level, perceived socioeconomic status, and income."}, {"title": "Covariate Balance", "content": "Tables C3 and C4 show univariate analyses of covariate balance in the Reduced and Increased Exposure experiments, respectively. Omnibus test of balance using ran- domization inference are reported in Section 5. We do not observe any evidence of imbalance."}, {"title": "Adjustment for multiple comparisons", "content": "For the two groups (experiments), we independently implement an FDR adjust- ment [72], grouping the variables as follows:\n\u2022 K\u2081 primary outcomes: Not adjusted,\n\u2022 K2 secondary outcomes: Sharpened FDR-adjusted p-values with K\u2081+ K2 outcomes,\n\u2022 K3 tertiary outcomes: Sharpened FDR-adjusted p-values with K1 + K2 + K3 outcomes."}, {"title": "Affective Polarization: Regression Tables", "content": "The preregistered regression analysis for the in-feed effect is reported in Table E5, and the regression analyses for the post-experiment effect are reported in Table E6.\nTable E7 and Table E8 show the equivalent regressions but include the entire population (N=1,256), regardless of whether the participants completed only the post-survey or only at least one in-feed survey. For both experiments, the treatment effect remains statistically significant. We note that in this sample there is also no evidence of covariate imbalance (reduce, post-experiment: p = 0.86; reduce, in-feed: p = 0.59; increase, post-experiment: p = 0.77; increase, in-feed: p = 0.18), asymmetric attrition rates (reduce, post-experiment: p = 0.55; reduce, in-feed: p = 0.28; increase, post-experiment: p = 0.4; increase, in-feed: p = 0.56), or asymmetric attrition patterns (reduce, post-experiment: p = 0.49; reduce, in-feed: p = 0.69; increase, post- experiment: p = 0.78; increase, in-feed: p = 0.98). Testing procedures are described in Section 5.3."}, {"title": "Emotions: Regression Tables", "content": "The preregistered regression analysis for the in-feed effect in the reduced exposure is reported in Table E9, while for the increased exposure is summarized in Table E10. The post-experiment effect for the reduced exposure experiment is summarized in Table E11 and for the increased exposure in Table E12.\nTable E13, Table E14, Table E15, and Table E16 report the equivalent regressions but include the entire population (N=1,256), regardless of whether the participants completed only the post-survey or only at least one in-feed survey. The results are qualitatively similar, the variation in negative emotions is statistically significant in both experiments: increased in the increased exposure experiment and reduced in the reduced exposure experiment."}, {"title": "Political attitude shifts", "content": "As preregistered, we report the effect of the intervention on the seven remaining attitudes we used to identify the AAPA posts. We find that Reduced Exposure to AAPA content reduces the Biased Evaluation of Politicized Facts while increasing the exposure increases the Opposition to Bipartisan Cooperation (Fig. E13). However, none of these effects are significant after adjusting for multiple hypotheses testing as preregistered (Sec. D)."}, {"title": "Engagement", "content": "When examining the effects of our interventions, none of the differences reached statistical significance. In the Increased Exposure experiment, the average number of daily sessions per participant-defined as scrolling activities separated by at least one hour-was 1.65 in the control group and 1.53 in the treatment group (U=32,220; P=0.18). In the Reduced Exposure experiment, the control and treatment groups averaged 1.53 and 1.49 sessions per day, respectively (U=66,940; P=0.71). After con- trolling for the average daily sessions during the baseline period and the recruitment platform, the change in session count was not statistically significant in either experiment (Increased Exposure: -0.07 sessions, CI [-0.19, 0.04]; P=0.197; 95% Reduced Exposure: -0.04 sessions, CI [-0.13, 0.05] P=0.381; 95%). The differences in the time spent on the platform across conditions also do not reach statistical significance. In the Increased Exposure experiment, participants in the control condition spent on average of 44.4 minutes vs. 41.3 minutes in the treatment condition (U=37,545; P=0.12), while in the Reduced Exposure, they spent 46.8 minutes in the control and 42.2 minutes in the treatment condition (U=70,678; P=0.10).\nSimilarly, repost rates during the experimental period did not differ significantly between treatment and control groups. In the Increased Exposure experiment, the repost rate was 0.52% in the control group and 1.07% in the treatment group (U=30,859; P=0.83). In the Reduced Exposure experiment, the repost rates were 0.74% for the control group and 0.46% for the treatment group. When controlling for user baseline averages and the platform using regression analysis (Fig. E15), the changes in repost rates remained non-significant (Increased Exposure: increase of 0.27%, 95% CI [-0.002, 0.008]; P=0.29; Reduced Exposure: reduction of -0.06%, 95% CI [-0.002, 0.001] P=0.503).\nThe favorite rates during the experimental period also showed no significant dif- ferences. The overall favorite rate during the baseline was 4.73%. Controlling for user baseline and platform (Fig. E16), the favorite rate changed by 0.14% in the Increased Exposure group (95% CI [-0.009, 0.012]; P=0.796) and decreased by 0.29% in the Reduced Exposure group (95% CI [-0.010, 0.004]; P=0.43).\nFinally, we observed no significant differences between treatment and control groups in terms of the number of new posts or replies during the experimental period."}, {"title": "Heterogeneous Treatment Effects", "content": "Fig. E17 and Fig. E18 show the heterogeneous treatment effects on affective polariza- tion in the two experiments. We use the same regression specifications as in the main analyses but interact the treatment with the moderator. As in the covariate balance and attrition tests, we coarsened the education, income, and SES ladder covariates to avoid small categories. We consider only a subset of covariates and split them into primary and secondary moderators as preregistered. After adjusting the p-values for multiple hypotheses testing, none of the covariates reach statistical significance."}, {"title": "Qualitative feedback on the impact of the experiment", "content": "At the end of the study", "participants": "Did the Chrome extension affect your experience on Twitter/X in any way?\" This question was open-ended, and 1,218 participants (96.9%) provided a non-empty response. Most answers were short and indicated no impact, such as \"No\" or \"Not that I could tell.\" We indi- vidually coded each response with the support of AI (GPT-4), providing the original survey question for context"}]}