{"title": "Efficient Technical Term Translation: A Knowledge Distillation Approach for Parenthetical Terminology Translation", "authors": ["Jiyoon Myung", "Jihyeon Park", "Jungki Son", "Kyungro Lee", "Joohyung Han"], "abstract": "This paper addresses the challenge of accurately translating technical terms, which are crucial for clear communication in specialized fields. We introduce the Parenthetical Terminology Translation (PTT) task, designed to mitigate potential inaccuracies by displaying the original term in parentheses alongside its translation. To implement this approach, we generated a representative PTT dataset using a collaborative approach with large language models and applied knowledge distillation to fine-tune traditional Neural Machine Translation (NMT) models and small-sized Large Language Models (sLMs). Additionally, we developed a novel evaluation metric to assess both overall translation accuracy and the correct parenthetical presentation of terms. Our findings indicate that sLMs did not consistently outperform NMT models, with fine-tuning proving more effective than few-shot prompting, particularly in models with continued pre-training in the target language. These insights contribute to the advancement of more reliable terminology translation methodologies.", "sections": [{"title": "Introduction", "content": "Terminology translation task is essential for understanding documents rich in technical terms, such as academic papers and technical reports. Traditionally, methods in the task have involved identifying term pairs in the source and target languages and using these pairs for training or post-editing purposes. However, challenges arise when there is no precise match for a term in the target language, or when new terms are used inconsistently. For instance, the term \"fine-tuning\" may be variably translated as \" \ud30c\uc778\ud29c\ub2dd\" or \"\ubbf8\uc138\uc870\uc815\" in Korean.\nTo address this, our research proposes a novel approach called Parenthetical Terminology Translation (PTT), which displays the original term in parentheses alongside its translation. This approach aims to mitigate reader confusion, especially when suitable translations are unavailable or translation accuracy is low. Although similar translation strategies using parenthetical form have been suggested in previous studies, effective technical solutions for this approach remain underexplored.\nWith the advent of advanced Large Language Models (LLMs), researchers have started exploring their potential for various tasks, including translation. LLMs can effectively support PTT through simple prompt usage, offering a promising solution for this approach. However, the practical application of LLMs is hindered by their high computational costs and latency, making them less feasible for real-time or large-scale deployment.\nTo mitigate these limitations, this study focuses on achieving the capabilities of LLMs using smaller, traditional Neural Machine Translation (NMT) models and small-sized Language Models (sLMs). We generated a high-quality PTT dataset using LLMs and distilled the knowledge by fine-tuning these smaller models with this dataset. This approach ensures that the benefits of LLMs can be harnessed without incurring high computational costs. Additionally, we evaluated the performance of various models and training methods to optimize model performance and efficiency, particularly for the specialized PTT task.\nOur proposed task extends beyond mere translation accuracy; it also emphasizes the correct presentation of technical terms within parentheses, which is crucial for enhancing reader comprehension. To quantitatively evaluate this aspect, we introduced a novel metric specifically designed to assess the models' ability to accurately and effectively use parenthetical annotations. This metric not only evaluates translation quality but also ensures that technical terms are correctly presented, allowing for a robust comparison of model performance across different architectures and training techniques.\nThus, this paper makes three significant contributions to the field of terminology translation:"}, {"title": "Related Work", "content": "Terminology translation plays a crucial role in ensuring consistency and accuracy in specialized domains like technical and academic documentation. Early approaches, such as rule-based and statistical machine translation, effectively leveraged pre-defined glossaries and translation memories (Melby et al., 1999). While these methods successfully maintain consistency within certain contexts, they often struggle with out-of-domain (OOD) words and ambiguous terms (Och and Ney, 2003). Moreover, these approaches are less effective when dealing with domain-specific or emerging terms not covered by existing resources (Tiedemann, 2010; Tiedemann and Scherrer, 2017).\nTo maintain clarity and precision in academic and technical documentation, it is often necessary to preserve certain terms from the source language. This practice is particularly valuable in cases where the translated term may be unfamiliar to the reader or where retaining the original term is essential for legal or scientific accuracy (Moghadam and Far, 2015; Hasler et al., 2018; Michon et al., 2020). A further strategy to support this practice involves the strategic use of parentheses, where textual additions can help enhance translation quality and consistency through corpus-based improvements (Lin et al., 2008; Huang et al., 2017; Hawamdeh, 2018). Despite its potential benefits, the systematic implementation of this approach remains relatively underexplored in current research.\nRecent studies have highlighted the effectiveness of knowledge distillation in transferring knowledge from large language models (LLMs) to smaller traditional translation models (Li et al., 2024; Enis and Hopkins, 2024). Through this process, datasets generated by a powerful teacher model are distilled into a student model, enabling small-sized models to perform specialized tasks like terminology translation. The multi-agent framework is particularly effective in generating targeted, high-quality datasets for specific tasks (Wu et al., 2023). Within this framework, different agents are assigned specialized roles, such as data generation and evaluation, collectively enhancing the quality and relevance of the resulting dataset. This collaborative process is essential for precise and context-aware data generation, which is crucial for training models to excel in specialized translation tasks."}, {"title": "Data Generation", "content": "To create a high-quality dataset for the Parenthetical Terminology Translation (PTT) task, we employed four collaborative agents\u2014Writer, Translator, Evaluator, and Executor-utilizing the large language models (LLMs) GPT-40-mini or GPT-4-turbo for each agent. Our goal was to generate English sentences containing technical terminologies alongside their Korean translations, with the original English terms included within parentheses. The overall framework is illustrated in Figure 1."}, {"title": "Writer", "content": "The Writer agent was responsible for creating academic English sentences that included the technical terms. To achieve this, we first compiled a comprehensive list of terms to be included in the dataset. Recognizing the rapid emergence of new terminology in the field of artificial intelligence (AI), we focused on terms frequently encountered in AI-related research. To ensure multiple terms could be incorporated into single sentences, we clustered similar domain-specific terms together.\nNext, we utilized the arXiv API to find papers that contained all or some of the terms from each cluster. By including the summary of the most relevant paper in the prompt, we helped the Writer LLM understand the appropriate contexts in which these terms were used. This ensured that the generated sentences were contextually accurate and meaningful.\nTo enhance data diversity, the Writer were tasked with generating sentences where each term appeared either once or in conjunction with other terms. By combining these sentences post-generation, we facilitated the creation of sentences with various characteristics: sentences where terms appear only once, sentences containing different terms together, and sentences where the same term appears more than twice. This diversity allowed us to analyze the performance of PTT from multiple perspectives, ensuring a comprehensive evaluation of the models under different conditions. The complete prompt used for the Writer agent is provided in the Appendix (see Listing 1)."}, {"title": "Translator", "content": "The Translator agent translated the English sentences into Korean, ensuring that each target term was followed by its original English term in parentheses to fulfill the PTT task requirements. To enhance accuracy, we employed the GPT-4 Turbo model, while other agents utilized GPT-40-mini. Additionally, we applied one-shot prompting by providing a relevant example to guide the translation process. This approach helped maintain consistency and precision in the PTT task, ensuring that technical terms were accurately presented within parentheses. The complete prompt used for the Translator agent is provided in the Appendix (see Listing 2)."}, {"title": "Evaluator/Executor", "content": "The Evaluator agent reviewed the translated sentences, scoring them from 0 to 10 based on the accuracy of term usage and overall translation quality. Then, The Executor agent transit the statement 'If the score is less than 8: Response \"translator\". If the score is 8 or greater: Response \"final output\".' If a sentence scored below 8, the Evaluator suggested corrections, prompting the Translator to revise the translation. The Translator would then repeat the translation task until the Executor rated the sentence as \"final output\""}, {"content": "After the automatic data generation process, human reviewers conducted a final quality check to ensure the dataset's reliability."}, {"title": "Out-of-Domain Evaluation Dataset", "content": "To evaluate the generalization ability of models in the PTT task, we generated additional datasets in domains beyond artificial intelligence (AI), specifically targeting biology and physics. The data generation process followed the same methodology as the in-domain training dataset to ensure consistency. In total, we generated 171 paired sentences for biology (subcellular processes), 60 for nanoscale physics, and 168 for high-energy physics. Each domain-specific dataset was curated by referencing relevant academic papers, providing authentic and contextually accurate examples. These out-of-domain datasets allowed for a comprehensive assessment of the models' robustness and adaptability across different specialized fields."}, {"title": "Knowledge Distillation", "content": "In this study, we applied knowledge distillation to fine-tune both traditional neural machine translation (NMT) models and small-sized large language models (sLMs) using the synthetic Parenthetical Terminology Translation (PTT) dataset generated in the previous step. Our goal was to evaluate the effectiveness of distillation techniques across various model architectures, sizes, and training methodologies, offering insights into how distilled models perform in specialized translation tasks."}, {"title": "Fine-tuning Traditional Machine Translation Models", "content": "To evaluate the performance of knowledge distillation on traditional neural machine translation models, we employed several widely used open-source models. We focused on encoder-decoder Transformer-based models that support Korean. Specifically, we tested the following models:\n\n\u2022 mBART50 (Liu et al., 2020) : This multilingual NMT model is pre-trained on monolingual corpora from 50 languages and is fine-tuned for translation tasks. It consists of 611 million parameters.\n\n\u2022 M2M100 (Fan et al., 2020): A large-scale multilingual NMT model trained on 2200 translation directions, enabling many-to-many translation across 100 languages. We tested the base version with 418 million parameters.\n\n\u2022 NLLB-200 (Koishekenov et al., 2023): Known for its extensive language coverage, this model is particularly useful for low-resource languages and inclusive translation services. We tested the distilled version with 600 million parameters."}, {"title": "Fine-tuning small-sized Large Language Models", "content": "To effectively compare performance with NMT models, we also fine-tuned open-source small-sized large language models. Our goal was to evaluate various models from multiple perspectives to gain comprehensive insights into the PTT task. To ensure reproducibility and a broad evaluation, we selected four well-known and high-performing open-source LLMs:\n\n\u2022 Llama 3 (Touvron et al., 2023): The latest iteration of the Llama series, this model further refines the architecture introduced in earlier versions, enhancing its performance on large-scale datasets. We evaluated the 8B and 70B versions in our experiments.\n\n\u2022 Gemma 2 (Team et al., 2024): A next-generation multilingual model, Gemma 2 is designed to deliver high performance across diverse natural language tasks with an emphasis on efficiency. We assess the model by testing three versions: the smallest (2B), a mid-sized variant (9B), and a larger configuration (27B).\n\n\u2022 Qwen 2 (Yang et al., 2024): An updated version of the Qwen series, Qwen 2 is developed with a strong focus on flexibility and adaptability to domain-specific tasks. It offers improved performance and efficiency, particularly in handling complex language modeling challenges. In this study, we analyzed the 1.5B, 7B, and 72B versions.\n\n\u2022 Mistral (Jiang et al., 2023): Mistral is known for its streamlined design and high efficiency in multilingual tasks. We specifically evaluate the 7B version to examine how its architecture balances performance with computational efficiency.\nTo compare pre-trained models with those that have been further fine-tuned specifically for the Korean language, we also tested models that underwent continual pre-training (Ke et al., 2023) in Korean. This approach allowed us to assess the impact of additional language-specific pre-training on the models' performance in the Parenthetical Terminology Translation (PTT) task.\n\n\u2022 beomi/Llama-3-Open-Ko-8B\u00b9: A specialized version of Llama 3 focused on Korean language tasks. This open-source model is fine-tuned to excel in Korean linguistic applications.\n\n\u2022 beomi/Llama-3-KoEn-8B2: A bilingual version of Llama 3 tailored for both Korean and English language tasks. This model is designed to maintain balanced performance across both languages, making it versatile for multilingual applications.\nFurthermore, we explored instruction-tuned versions of the aforementioned models using different training techniques, such as LoRA (Hu et al., 2021) and QLoRA (Dettmers et al., 2023). In addition, we applied few-shot prompting (Brown et al., 2020) to both the instruction-tuned models and a commercial LLM (GPT-40) to compare the effects of fine-tuning versus prompting. This comprehensive evaluation provides valuable insights into how knowledge distillation, combined with various tuning and prompting strategies, can enhance translation accuracy while maintaining efficiency across diverse model architectures."}, {"title": "Custom Evaluation Metric", "content": "This section introduces a novel metric designed specifically for the Parenthetical Terminology Translation (PTT) task, aimed at evaluating not only the accuracy of overall translation but also the correct presentation of the technical terms within parentheses.\nFor each sentence in the dataset, let $T_{Eng}$ represent the list of all technical terms provided in the original English sentence, including duplicates if the same term appears multiple times. Similarly, let $T_{kor}$ represent the list of those terms that are correctly translated into Korean and accompanied by their original English terms in parentheses. We define $|T_{Eng}|$ as the total number of technical terms in the English sentence (including duplicates), and $|T_{kor}|$ as the number of correctly translated terms from $T_{Eng}$ in the Korean sentence. The ratio of these terms is calculated as the weight $W_{terms} = min (\\frac{|T_{kor}|}{|T_{Eng}|}, 1)$. This ratio is capped at 1 to ensure that no penalty is applied if more terms appear correctly in the Korean translation than in the original English sentence. The adjusted metric for the PTT task, $M_{PTT}$, is then computed by multiplying this clipped ratio with the original translation metric M, such that $M_{PTT} = W_{terms} \\times M$. Finally, we average $M_{PTT}$ across all sentence pairs in the dataset to obtain the final evaluation metric.\nWe employed BLEU (Papineni et al., 2002), COMET (Rei et al., 2020), and BERTScore (Zhang et al., 2020) as M to evaluate pure translation performance. The translation metrics are computed after removing the parenthetical terms, ensuring that we assess only the translation's accuracy and fluency. This approach allows us to maintain a focus on both the translation's quality and the correct handling of technical terms within parentheses."}, {"title": "Evaluation", "content": "The results presented provide a comprehensive overview of the quantitative performance of various models and training techniques on the in-domain Parenthetical Terminology Translation (PTT) dataset, while presents results on the out-of-domain dataset. Key observations are summarized as follows:\n1.  The performance comparison between small-sized Large Language Models (sLMs) and Neural Machine Translation (NMT) models reveals that sLMs do not consistently outperform NMT models, even though LLMs are often perceived as more advanced due to their architecture. For instance, mBART50 and M2M100, achieved weighted BLEU scores of 37.52 and 40.05, respectively, with corresponding weight indicate strong PTT performance. These scores were comparable or superior to those achieved by some sLMs, such as the Llama 3 8B and 70B models, which obtained similar weighted BLEU scores but required significantly larger model sizes.\n2.  Within the same sLM families, base models generally slightly outperformed instruction-tuned models on the PTT task. For instance, the Llama 3 8B model with QLoRA achieved a weighted BLEU score of 38.88, while the instruction-tuned version (8B-it) with the same QLORA technique scored slightly lower at 37.84. This trend suggests that instruction-tuned models, which are trained on a broad range of tasks, may not gain a specific advantage for the specialized requirements of the PTT task.\n3.  Applying prompt engineering instead of fine-tuning to instruction-tuned models, using a 1-shot prompting approach, resulted in very poor performance. For example, the Llama 3 8B-it scored only 0.523, and the Gemma 2-9B-it scored 0.342 on weight metric. Even the commercial LLM GPT-40 performed worse than other fine-tuned small models, underscoring the critical importance of fine-tuning for specialized tasks like PTT.\n4.  Models that underwent continued pre-training in the target language (Korean) generally outperformed others, with the Llama-3-KoEn-8B-it achieving the highest score among all models. Although Llama-3-Open-Ko-8B, which was continued pre-trained exclusively in Korean, showed slightly lower performance with a weighted BLEU score of 39.869, it still performed well. This highlights the importance of bilingual proficiency in models for the PTT task, where handling both source and target languages effectively is crucial for success.\n5.  In the in-domain dataset, model size had little impact on performance, with smaller models like Gemma 2 7B even outperforming larger ones like Gemma 2 27B. However, when tested on out-of-domain datasets, all models experienced significant performance drops, but larger models such as Gemma 2 27B or Llama 3 70B showed less decline, indicating better generalization capabilities."}, {"title": "Qualitative Analysis", "content": "1.  As illustrated in Table 4, most of the models, including M2M100, initially demonstrated strong proficiency in the PTT task, particularly in incorporating original terms within parentheses, as indicated by the high weight metrics in the earlier epochs. Over successive training epochs, the model's overall translation skills improved gradually, leading to better performance across all weighted metrics.\n2.  Our analysis highlights a persistent challenge among models in accurately translating less common terms, especially proper nouns."}, {"title": "Conclusion", "content": "In this study, we explored the Parenthetical Terminology Translation (PTT) task, a specialized translation problem that focuses on mitigating potential inaccuracies in term translation by displaying the original technical term in parentheses alongside its translation. To effectively evaluate this approach, we introduced a novel evaluation metric, MPTT, designed to measure both the accuracy of overall translation and the proper parenthetical presentation, ensuring that technical terms are effectively communicated across languages.\nTo generate a high-quality dataset for this task, we utilized a collaborative approach involving Writer, Translator, Evaluator, and Executor agents, supported by large language models (GPT-4). This allowed us to create a diverse and contextually accurate dataset that reflects real-world usage of technical terms in artificial intelligence (AI), biology, and physics. We then applied knowledge distillation techniques to fine-tune both traditional Neural Machine Translation (NMT) models and small-sized Large Language Models (sLMs), comparing their performance across various model architectures, sizes, and training methods.\nOur findings revealed that sLMs did not consistently outperform NMT models, challenging the assumption that more advanced architectures inherently lead to superior performance. Additionally, within the same sLM families, base models slightly outperformed instruction-tuned models, suggesting that broad task training may not offer advantages for specialized tasks like PTT. Fine-tuning proved crucial, as prompt engineering approaches like 1-shot prompting resulted in significantly poorer performance. Moreover, models with continued pre-training in Korean outperformed others, highlighting the importance of bilingual proficiency for the PTT task. While model size had little impact on in-domain performance, larger models demonstrated better generalization on out-of-domain datasets, suggesting they are more versatile and better suited for handling diverse and unfamiliar data. These insights contribute to optimizing models and training techniques for specialized translation tasks, offering practical guidance for future research and applications in terminology translation."}, {"title": "Limitations", "content": "Penalty Mechanism in Evaluation Metrics: The current approach to evaluating PTT performance involves simply multiplying translation metrics by a weight that reflects the presence of correctly parenthesized terms. However, this straightforward multiplication can disproportionately affect the overall performance scores. A more sophisticated penalty mechanism, such as using an exponential function, could provide a more balanced assessment by reducing the impact on the metric scores. Additionally, the current metric does not penalize the model for excessively parenthesizing trivial or unintended terms, which could lead to over-parenthesization. Future work could incorporate penalties for such cases, potentially by introducing concepts of recall and precision to refine the evaluation.\nPotential Bias in the Dataset: The PTT dataset was generated using GPT-4, and the performance metrics were assessed with this dataset as the ground truth. This approach may introduce biases inherent to the GPT-4 model into the dataset, potentially affecting the robustness and generalizability of the models trained on it. To mitigate this, future research should consider generating datasets using a variety of models, ensuring a broader representation of translation styles and reducing the potential for model-specific biases.\nLanguage Scope of the Study: This study focused exclusively on translation into Korean, which limits the generalizability of the findings across different languages. PTT performance might vary significantly with other languages due to differences in linguistic structures and translation challenges. Expanding the study to include translations into multiple languages would enable a more comprehensive analysis of the PTT task and provide insights into how the models perform across different linguistic contexts."}]}