{"title": "RARE-TO-FREQUENT: UNLOCKING COMPOSITIONAL GENERATION POWER OF DIFFUSION MODELS ON RARE CONCEPTS WITH LLM GUIDANCE", "authors": ["Dongmin Park", "Sebin Kim", "Taehong Moon", "Minkyu Kim", "Kangwook Lee", "Jaewoong Cho"], "abstract": "State-of-the-art text-to-image (T2I) diffusion models often struggle to generate rare compositions of concepts, e.g., objects with unusual attributes. In this paper, we show that the compositional generation power of diffusion models on such rare concepts can be significantly enhanced by the Large Language Model (LLM) guidance. We start with empirical and theoretical analysis, demonstrating that exposing frequent concepts relevant to the target rare concepts during the diffusion sampling process yields more accurate concept composition. Based on this, we propose a training-free approach, R2F, that plans and executes the overall rare-to-frequent concept guidance throughout the diffusion inference by leveraging the abundant semantic knowledge in LLMs. Our framework is flexible across any pre-trained diffusion models and LLMs, and can be seamlessly integrated with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly surpasses existing models including SD3.0 and FLUX by up to 28.1%p in T2I alignment. Code is available at https://github.com/krafton-ai/Rare2Frequent.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in text-to-image (T2I) diffusion models have achieved unprecedented success in generating highly realistic and diverse images (Zhang et al., 2023a; Saharia et al., 2022). However, these models often struggle to accurately generate images from rare and complex prompts (Samuel"}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 TEXT-TO-IMAGE DIFFUSION MODELS", "content": "Diffusion models are a promising class of generative models and have shown remarkable success in T2I synthesis (Sohl-Dickstein et al., 2015; Ho et al., 2020; Zhang et al., 2023a). Owing to large-scale datasets and pre-trained text embedding models such as CLIP (Radford et al., 2021), GLIDE (Nichol et al., 2021) and Imagen (Saharia et al., 2022) show diffusion models can understand text semantics at scale and synthesize high-quality images. Latent Diffusion Models (LDMs) (Rombach et al., 2022) improve the training efficiency by changing the diffusion process from pixel to latent space. Recently, more advanced models such as SDXL (Podell et al., 2023), PixArt (Chen et al., 2023), and SD3.0 (Esser et al., 2024) further enhance the T2I synthesis quality by using enhanced datasets (Betker et al., 2023), architectures (Peebles & Xie, 2023), and training schemes (Lipman et al., 2022)."}, {"title": "2.2 COMPOSITIONAL IMAGE GENERATION", "content": "Despite decent advances, recent T2I diffusion models often suffer from image compositionality issues (Rassin et al., 2024; Huang et al., 2023). Many approaches have tried to mitigate this issue based on cross-attention control technique. Some works utilize prior linguistic knowledge for text token-level attention control. StructureDiffusion (Feng et al., 2022), Attend-and-Excite (Chefer et al., 2023), and SynGen (Rassin et al., 2024) control text tokens of different objects to be located in separate attention regions. Another line of work uses additional input conditions. GLIGEN (Li et al., 2023) and ReCo (Yang et al., 2023) propose position-aware adapters attachable to the diffusion backbone, and use regional conditions to locate each object at the corresponding region. ControlNet (Zhang et al., 2023b) and InstanceDiffusion (Wang et al., 2024b) introduce more general adapters that incorporate diverse conditions. While these works have succeeded in advancing compositional generation, they require prior linguistic knowledge or extra conditions, which are hard to prepare for arbitrary text."}, {"title": "2.3 LLM-GROUNDED DIFFUSION", "content": "LLMs have shown promising abilities in language comprehension (Achiam et al., 2023; Zhao et al., 2023; Wang et al., 2024a). Powered by this advance, recent works have attempted to ground LLMs into diffusion models to provide prior knowledge and conditions for compositional image generation. LayoutGPT (Feng et al., 2024), LMD (Lian et al., 2023), and CompAgent (Wang et al., 2024c) use LLMs to decompose a given prompt into sub-prompts per object and extract their corresponding bounding boxes. RPG (Yang et al., 2024) further adopts recaptioning and planning for complementary regional diffusion. ELLA (Hu et al., 2024) use LLMs to dynamically extract timestep-dependent conditions from intricate prompts. Some work utilizes LLMs in iterative image editing (Wu et al., 2024; Gani et al., 2023). While these LLM-grounded diffusion approaches have succeeded in spatial compositions, they still face challenges in non-spatial compositions for rare concepts."}, {"title": "3 RARE-TO-FREQUENT (R2F)", "content": null}, {"title": "3.1 COMPOSITIONAL TEXT-TO-IMAGE GENERATION FOR RARE CONCEPTS", "content": "Problem Setup. Consider a T2I generation model \\( \\theta \\) trained on data distribution \\( P_{data} \\) involving two types of data; (i) image data, denoted by \\( x \\in \\mathbb{R}^d \\); and (ii) text prompt, denoted by \\( c \\). Following the compositional generation literature (Liu et al., 2022), we suppose that a text prompt consists of a composition of concepts as \\( c = \\{C_1, ..., C_n\\} \\), where each \\( c_i \\) denotes a unit of concept, e.g., object, color, etc. For simplicity, we denote 'composition of concepts' as 'concept' hereafter. Because real-world T2I datasets typically exhibit a long-tailed nature (Xu et al., 2023), we naturally assume the"}, {"title": "Theoretical Motivation.", "content": "To provide an insight into how relevant frequent concepts help the rare concept composition, we consider a simple setting where two texts are given: one involving a rare concept \\( c_R \\) (e.g.,\u201cfurry frog\u201d), and another involving a frequent one \\( c_F \\) (e.g., \u201cfurry dog\u201d). Let \\( x \\in \\mathbb{R}^2 \\) be an image representation, where the first and second dimension represent the attribute (e.g., \"furry\") and object (e.g., \u201canimal\u201d), respectively. We assume the ground truth conditional distributions \\( P_{data}(x|c_R) \\) and \\( P_{data}(x|c_F) \\) follow Gaussian distributions \\( \\mathcal{N}(\\mu_R, \\Sigma_R) \\) and \\( \\mathcal{N}(\\mu_F, \\Sigma_F) \\), with \\( \\Sigma_R = \\Sigma_F = I_2 \\) and the first component of \\( \\mu_R \\) and \\( \\mu_F \\) being identical. Here, \\( I_2 \\) denotes the 2 \u00d7 2 identity matrix. Let \\( \\nabla_x \\log p_\\theta(x|c_R) \\) be the estimated score function prameterized by \\( \\theta \\). We assume that the score estimator is given for each concept as:  \\(\\nabla_x \\log p_\\theta(x|c_F) = \\nabla_x \\log \\mathcal{N}(\\mu_F, \\Sigma_F) = \\nabla_x \\log P_{data}(x|c_F)\\); and \\(\\nabla_x \\log p_\\theta(x|c_R) = \\nabla_x \\log \\mathcal{N}(\\mu_R, \\Sigma_R)\\), where \\(\\hat{\\mu}_R = \\mu_R \\) and \\(\\hat{\\Sigma}_R = diag(\\sigma^2, 1)\\). Here, \\( diag(\\sigma^2, 1) \\) denotes the 2 \u00d7 2 diagonal matrix with \\( \\sigma^2 \\) and 1 as its diagonal elements."}, {"title": "3.2 PROPOSED FRAMEWORK: R2F", "content": "Inspired by the theoretical motivation, we propose a training-free framework, called R2F, which leverages LLM to find frequent concepts relevant to rare concepts and use them in the diffusion sampling process. R2F involves a two-stage process: (i) Rare-to-frequent concept mapping that uses LLM to identify and map rare concepts and their relevant yet frequent concepts; and (ii) Alternating concept guidance that iteratively uses prompts involving either rare or frequent objects during the sampling process. Different from Theorem 3.1, we use alternating guidance because it yields more realistic images than the linear composition as it continuously denoises from the actual prompts, which is analyzed in Section 4.4. Figure 4 illustrates the overview of our proposed framework.\nRare-to-Frequent Concept Mapping. To extract proper concept mapping, we use LLM instruction with chain-of-thought prompting (See Appendix B for the full LLM instruction and in-context learning examples). Given a text prompt c, R2F performs the concept mapping process step by step: (1) It decomposes the prompt into m sub-prompts per object, \\( c = \\{c^i\\}_{i=1}^m \\). For each sub-prompt \\( c^i \\), (2) it identifies rare concepts and (3) extracts a frequent concept contextually relevant to the rare concept, making a rare-to-frequent concept mapping \\( \\hat{c}^i = (c^i, c_R^i, c_F^i) \\). Specifically, to find the frequent concept \\( c_F^i \\), it splits each rare concept into the main noun object and its attributes and then replaces the main noun object into another object contextually more fit to the attributes. Additionally, (4) It extracts a visual detail level \\( V^i \\) (an integer value from 1 to 5) needed to draw each rare concept for determining an appropriate stop point of concept guidance, based on prior observation (Hertz et al., 2022) that rough visual features (e.g., shape) are highly affected by diffusion latents at early sampling steps while detailed visual features (e.g., texture) are influenced at the later sampling steps. The final mapped rare-to-frequent concept output is formed as \\( \\hat{c} = \\{(c^i, c_R^i, c_F^i, V^i)\\}_{i=1}^m \\).\nAlternating Concept Guidance. Based on the extracted concept mappings, R2F guides the diffusion inference by alternately exposing rare and frequent concepts throughout T sampling steps. We first construct a scheduled batch of prompts \\( \\mathcal{C}_{T:1} = \\{\\check{c}_T, . . . , \\check{c}_1\\} \\), which are sequentially inputted to the diffusion model. In early steps, \\( \\check{c}_t \\) is reconstructed from either the set of frequent concept \\( \\{c_F^i\\}_{i=1}^m \\) or that of rare concept \\( \\{c_R^i\\}_{i=1}^m \\), alternatively; \\( \\check{c}_t = Reconstruct(\\{c_F^i\\}_{i=1}^m) \\) if \\( (T \u2013 t)\\%2 = 0 \\), and \\( \\check{c}_t = Reconstruct(\\{c_R^i\\}_{i=1}^m) \\) otherwise. The reconstruction process is simply done by substituting the words of the detected concept from the original prompt (See \\( Reconstruct(c_k, c_l) \\) in Figure 4).\nMeanwhile, each frequent concept \\( c_F^i \\) stops being used to reconstruct at different stop points \\( S^i \\) determined by the visual detail level \\( V^i \\). The stop point \\( S^i \\) is obtained by converting a visual detail score \\( V^i \\) in the integer grid [1, 2, 3, 4, 5] into a mapped float value in a grid [0.9, 0.8, 0.6, 0.4, 0.2] and multiply it by the total diffusion step T; if \\( V^i = 1 \\), then \\( S^i = [0.9T] \\). After the stop point \\( S^i \\), the i-th frequent concept is no longer used for reconstruction, so it becomes \\( \\check{c}_t = Reconstruct(\\{c^i\\}_{i=1}^k | c_R^i) \\). This process repeats until all stop points have been passed. After the latest stop point \\( S_{last} \\), the alternating guidance is finished and only the original prompt is inputted; \\( \\check{c}_t = c \\) when \\( t < S_{last} \\). Each reconstructed prompt \\( \\check{c}_t \\) in the scheduled prompt batch \\( \\mathcal{C}_{T:1} \\) is sequentially inputted to generate the diffusion latent \\( z_t \\), as \\( z_{t-1} \u2190 p_\\theta(z_t, \\check{c}_t) \\), leading to the final output image \\( x_0 = vae(z_0) \\)."}, {"title": "3.3 EXTENTION WITH REGION-GUIDED DIFFUSION MODELS: R2F+", "content": "We extend our framework to the general region-guided diffusion approach. The extended framework, named R2F+, has more controllability in generating all rare concepts in the prompt by seamlessly"}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 EXPERIMENT SETTING", "content": "Datasets. We evaluate R2F using one new dataset, RareBench, which we design to validate the generated image quality for rare concepts, and two existing datasets, DVMP (Rassin et al., 2024) and T2I-CompBench (Huang et al., 2023), for general image compositionally. As shown in Table 1, RareBench includes more rare concepts compared to existing benchmarks."}, {"title": "4.2 MAIN RESULTS OF R2F", "content": "RareBench. Table 2 shows the T2I alignment performance of R2F compared to all the baselines on RareBench. Overall, R2F consistently performs the best in both GPT-4o and Human evaluations across all cases of rare concepts. Numerically, R2F outperforms the best baselines for each case from 3.1%p to 28.1%p in GPT-4o evaluation and from 0.6%p to 19.4%p in Human evaluation. Among the baselines, the latest pre-trained diffusion model, SD3.0, tends to achieve higher alignment scores possibly because of the advanced training technique. Interestingly, the regional LLM-grounded diffusion models, LMD and RPG, show worse results than their backbone diffusion SDXL on RareBench, as they are designed to generate objects in specific regions and not designed to tightly bind rare attributes to the object in the same region. Figure 6 illustrates the generated image examples. R2F succeeds in generating images of diverse cases of rare concepts, while maintaining the realism of the images. More visualization results with varying random seeds are in Appendix G, and the rare-to-frequent concept mapping examples extracted by R2F are in Appendix H."}, {"title": "4.3 FLEXIBILITY ACROSS VARIOUS DIFFUSION MODELS AND LLMS", "content": "Effectiveness across Different Diffusion Models. Table 4 shows GPT-4o evaluated T2I alignment performance of R2F combined with different diffusion backbones, SDXL and SD3.0. R2F consistently improves the performance of the backbone models in all cases, regardless of the backbone models used. Also, R2Fsdxl outperforms existing LLM-grounded diffusion baselines based on SDXL in most cases, indicating its genuine effectiveness in leveraging relevant frequent concepts in generating images of rare concepts. The generated images by R2Fsdxl is visualized in Figure 7."}, {"title": "4.4 ABLATION STUDIES", "content": "Efficacy of Alternating Guidance. Figure 8 and Table 6 show the qualitative and quantitative analysis of the R2F's alternating guidance compared to other possible guidance choices. We apply three guidance choices, (1) Linear interpolation (Interpolate) of latents as in Theorem 3.1, and bring the idea of (2) Composable Diffusion (Liu et al., 2022) and (3) Prompt-to-prompt (P2P) (Hertz et al., 2022). Given a pair of rare-frequent concept prompts, Interpolate linearly interpolates the latants of rare and frequent prompts with a = 0.5 and Composable blends the two prompt embeddings and uses it as the input, until the stop points obtained from LLM. P2P first generates a complete image from the frequent concept prompt and then edits it by the rare concept prompt with attention-control.\nOverall, R2F's alternating guidance performs the best in terms of T2I alignment and image quality. This may be because linear interpolation and Composable generates images from blended latents or embeddings, which are not the real inputs that diffusion models have seen in the training phase, generating unusual images, e.g., \u201cA zebra-striped duck\u201d, or blurry images, e.g., \u201cA robotic owl\u201d. Also, since P2P starts editing from the complete image of the frequent concept, it tends to preserve too many features of the frequent concept when generating the original rare concept, e.g., most features of zebra are still alive even after editing it with the prompt \"A zebra-striped duck\".\nEfficacy of Visual-detail-aware Guidance Stop Points. Figure 9 depicts the efficacy of R2F's adaptive visual-detail-aware stop points compared to when using a fixed stop point on RareBench"}, {"title": "4.5 CONTROLLABLE IMAGE GENERATION RESULTS OF R2F+", "content": "Table 7 shows the superiority of R2F+ for controllable image generation on three benchmarks with multi-object cases. Overall, R2F+ mostly performs the best in terms of T2I alignment accuracy, outperforming even R2F by leveraging more detailed layout-guided image generation process as shown in Figure 10. In addition, Figure 11 visualizes the generated images of R2F+ compared to SD3.0. With the proper layouts and their rare-to-frequent concept guidance generated by LLM, R2F+ consistently shows very controllable image generation results, achieving proper attribute binding on the corresponding region of objects. Meanwhile, R2F+ is not able to accurately synthesize images from prompts with 'complex' cases where multi-objects are intertwined in the region. This may be because the training-free layout-guided generation often fails with overlapped bounding boxes (Yang et al., 2024). More analysis with failure cases of R2F+ can be found in Appendix J."}, {"title": "4.6 SUPERIORITY AND COMPATIBILITY OVER PROMPT PARAPHRASING", "content": "Paraphrasing Rule. We use GPT-40 to investigate the effect of paraphrasing in generating rare concepts by asking \u2018I will draw a picture based on the given CAPTION. Please paraphrase it so that I can draw it more easily, while not changing the meaning.\u2019.\nResults. Table 8 shows the superiority and compatibility of R2F over prompt paraphrasing by GPT-40. With prompt paraphrasing, the T2I alignment of SD3.0 is enhanced, but it is not as significant as R2F. Interestingly, as shown in Figure 12, some rare concepts, e.g., \u201cviolin with ax-shaped body\u201d and \u201cdiamond-shaped durian\u201d, are extremely hard to synthesize even after a careful"}, {"title": "5 CONCLUSION", "content": "In this paper, we propose R2F, a novel framework that grounds LLMs and T2I diffusion models for enhanced compositional generation on rare concepts. Based on empirical and theoretical observations that relevant frequent concepts can guide diffusion models for more accurate concept compositions, we use the LLM to extract rare-to-frequent concept mapping, and plan to route the sampling process with alternating concept guidance. Our framework is flexible across any pre-trained diffusion models and LLMs, and we further propose a seamless integration with the region-guided diffusion approaches. Extensive experiments on three datasets, including our newly proposed benchmark, RareBench, containing various prompts with rare compositions of concepts, R2F significantly outperforms existing diffusion baselines including SD3.0 and FLUX."}, {"title": "B LLM INSTRUCTION FOR R2F", "content": "Table 9 and Table 10 detail the full LLM prompt and the in-context examples for R2F, respectively. With the detailed chain-of-thought instruction, we can automatically find rare concepts, generate related frequent concepts, and extract the visual detail level of concepts in one-shot LLM inference."}, {"title": "C LLM INSTRUCTION AND ALGORITHM PSEUDOCODE FOR R2F+", "content": "Table 11 details the complete LLM prompt for R2F+. Since the generation process of R2F+ requires detailed information, including rare-to-frequent concept mappings and region guides, we configure the LLM prompt to generate output in a structured JSON format.\nAlgorithm 1 describes the pseudocode for R2F+. As discussed in Section 3.3, the algorithm consists of three stages: (1) region-aware rare-to-frequent concept mapping, (2) masked latent generation via object-wise R2F, and (3) region-controlled alternating concept guidance."}, {"title": "D DETAILS FOR CONSTRUCTING RAREBENCH", "content": "Overview. RareBench aims to evaluate the T2I model's compositional generation ability for rare concept prompts across single- and multi-objects. For single-object prompts, we categorize visual concept attributes in five cases: (1) property, (2) shape, (3) texture, (4) action, and (5) complex, which is a mixture of the attributes. For multi-object prompts, we categorize the way to combine multiple prompts/concepts with single-object in three cases: (1) concatenation, (2) relation, and (3) complex. To ensure the prompt rareness, we construct the prompt set in a two-stage process: (i) Rare concept composition generation by GPT-40, and (ii) Rare prompt selection by human."}, {"title": "E GPT INSTRUCTION FOR CALCULATING % RARENESS", "content": "Since real-world T2I datasets typically exhibit a long-tailed nature (Xu et al., 2023; Park et al., 2020), they contain many rare concepts (in the tail distributions). To measure whether a prompt contains rare concepts that are very difficult to observe in the real-world, we ask GPT4 with the yes or no binary question using the following instructions. \u201cYou are an assistant to evaluate if the text prompt contains rare concepts that exist infrequently or not in the real world. Evaluate if rare concepts are contained in the text prompt: PROMPT, The answer format should be YES or NO, without any reasoning.\u201d. Formally, the % rareness of each test dataset \\( \\mathcal{C}_{test} \\) is calculated as\\(%Rareness(\\mathcal{C}_{test}) = \\frac{1}{|\\mathcal{C}_{test}|}\\sum_{c \\in \\mathcal{C}_{test}} 1(GPT_{rare}(c) == Yes)\\), where \\(GPT_{rare}(c)\\) is the binary answer of rareness from GPT."}, {"title": "F DETAILS FOR EVALUATION", "content": "GPT-based Evaluation. We leverage GPT-40 to evaluate the image-text alignment between the prompt and the generated image. The evaluation is based on a scoring scale from 1 to 5, where a score of 5 represents a perfect match between the text and the image, and a score of 1 indicates that the generated image completely fails to capture any aspect of the given prompt. Table 13 presents the complete prompt with a full scoring rubric. We convert the original score scale {1, 2, 3, 4, 5} to {0, 25, 50, 75, 100}, which is reflected in the reported results.\nHuman Evaluation. We collect human evaluation scores from ten different participants. For each prompt, all the generated images produced by R2F and baseline approaches are presented simultaneously to each participant. This comparative evaluation allows participants to provide more accurate evaluations by assessing all methods side by side (Sun et al., 2023). During evaluation, the name of all the methods is fully anonymized, and their order is randomly shuffled for each prompt to prevent any bias. Participants follow the same scoring criteria used in the GPT-based evaluation. Score scale is also converted from {1, 2, 3, 4, 5} to {0, 25, 50, 75, 100}."}, {"title": "H EXAMPLES FOR RARE-TO-FREQUENT CONCEPT MAPPING", "content": "Table 14 shows examples for R2F concept mapping generated by GPT-40. All examples are selected from RareBench. By leveraging the state-of-the-art LLM, R2F successfully split the prompt by objects, identify rare concepts, extract their relevant yet more frequent concepts, and their visual detail level to generate images."}, {"title": "I FAILURE RESULTS OF T2I-COMPBENCH'S AUTO-EVALUATION METRICS", "content": "Figure 14 shows the failure results of auto-evaluation metrics in T2I-CompBench. In the left case, while both SD3.0 and R2F generate appropriate images well-following the input prompt 'A big hippopotamus and a small mouse', R2F got very low BLIP score 0.0322. Also, in the right case,"}, {"title": "JMORE VISUALIZATION RESULTS OF R2F+", "content": "Figure 15 shows more visualization examples and failure cases of R2F+ on RareBench. As shown in the first row (i.e., Successful Cases), R2F+ produces highly controllable image generation results when the bounding boxes are non-overlapped and assigned with proper sizes. However, as shown in the second row (i.e., Failure Cases), when the bounding boxes are overlapped or too small, which is usually produced by LLM when the prompt is long and complex, it fails to accurately generate"}]}