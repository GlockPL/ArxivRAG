{"title": "Compositional Risk Minimization", "authors": ["Divyat Mahajan", "Mohammad Pezeshki", "Ioannis Mitliagkas", "Kartik Ahuja", "Pascal Vincent"], "abstract": "In this work, we tackle a challenging and extreme form of subpopulation shift, which is termed compositional shift. Under compositional shifts, some combinations of attributes are totally absent from the training distribution but present in the test distribution. We model the data with flexible additive energy distributions, where each energy term represents an attribute, and derive a simple alternative to empirical risk minimization termed compositional risk minimization (CRM). We first train an additive energy classifier to predict the multiple attributes and then adjust this classifier to tackle compositional shifts. We provide an extensive theoretical analysis of CRM, where we show that our proposal extrapolates to special affine hulls of seen attribute combinations. Empirical evaluations on benchmark datasets confirms the improved robustness of CRM compared to other methods from the literature designed to tackle various forms of subpopulation shifts.", "sections": [{"title": "Introduction", "content": "The ability to make sense of the rich complexity of the sensory world by decomposing it into sets of elementary factors and recomposing these factors in new ways is a hallmark of human intelligence. This capability is typically grouped under the umbrella term compositionality. Compositionality underlies both semantic understanding and the imaginative prowess of humans, enabling robust generalization and extrapolation. For instance, human language allows us to imagine situations we have never seen before, such as \"a blue elephant riding a bicycle on the Moon.\" While most works on compositionality have focused on its generative aspect, i.e., imagination, as seen in diffusion models , compositionality is equally important in discriminative tasks. In these tasks, the goal is to make predictions in novel circumstances that are best described as combinations of circumstances seen before. In this work, we dive into this less-explored realm of compositionality in discriminative tasks.\nWe work with multi-attribute data, where each input (e.g., an image) is associated with multiple categorical attributes, and the task is to predict an attribute or multiple attributes. During training, we observe inputs from only a subset of all possible combinations of individual attributes, and during test we will see novel combinations of attributes never seen at training. Following, we refer to this distribution shift as compositional shift. These distribution shifts can also be viewed as an extreme case of subpopulation shift . Towards the goal of tackling these compositional shifts, we develop an adaptation of naive discriminative Empirical Risk Minimization (ERM) tailored for multi-attribute data under compositional shifts. We term our approach Compositional Risk Minimization (CRM). The foundations of CRM are built on additive energy distributions that are studied in generative compositionality , where each energy term represents one attribute. In CRM, we first train an additive energy classifier to predict all the attributes jointly, and then we adjust this classifier for compositional shifts.\nOur main contributions are as follows:\n\u2022 Theory of discriminative compositional shifts: For the family of additive energy distributions, we prove that additive energy classifiers generalize compositionally to novel combinations of attributes represented by a special mathematical object, which we call discrete affine hull. Our characterization of extrapolation is sharp, i.e., we show that it is not possible to generalize beyond discrete affine hull. We show that the volume of discrete affine hull grows fast in the number of training attribute combinations thus"}, {"title": "Related Works", "content": "Compositional Generalization Compositionality has long been seen as an essential capability on the path to building human-level intelligence. The history of compositionality being too long to cover in detail here, we refer the reader to these surveys. Most prior works have focused on generative aspect of compositionality, where the model needs to recombine individual distinct factors/concepts and generate the final output in the form of text or image. For image generation in particular, a fruitful line of work is rooted in additive energy based models which translates naturally to additive diffusion models. Our present work also leverages an additive energy form, but our focus is on learning classifiers robust under compositional shifts, rather than generative models.\nOn the theoretical side, recently, there has been a growing interest in building provable approaches for compositional generalization. These works study models where the labeling function or the decoder is additive over individual features, and prove generalization guarantees over the Cartesian product of the support of individual features. While these works take promising and insightful first steps for provable compositional guarantees, the assumption of additive deterministic decoders may come as quite restrictive. In particular a given attribute combination can then only correspond to a unique observation, produced by a very limited interaction between generative factors, not to a rich distribution of observations. By contrast an additive energy model can associate an almost arbitrary distribution over observations to a given set of attributes. Based on this more realistic assumption of additive energy, our goal is to develop an approach that provably enables zero-shot"}, {"title": "Problem Setting", "content": "In compositional generalization, we aim to build a classifier that performs well in new contexts that are best described as a novel combination of seen contexts. Consider an input x (e.g., image), this input belongs to a group that is characterized by an attribute vector z = (21,..., zm) (e.g., class label, background label), where zi corresponds to the value of ith attribute. There are m attributes and each attribute zi can take d possible values. So z \u2208 Z with Z = {1, ...,d}m.\nWe use the Waterbirds dataset as the running example. Each image x has two attributes summarized in the attribute vector z = (y, a), where y tells the class of the bird \u2013 Waterbird (WB) or Landbird (LB), and a tells the type of the background \u2013 Water (W) or Land (L). Our training distribution consists of data from three groups \u2013 (WB, W), (LB, L), (LB, W). Our test distribution also consists of points from the remaining group (WB, L) as well. We seek to build class predictors that perform well on such test distributions that contain new groups. This problem setting differs from the standard problem studied in, where we observe data from all the groups but some groups present much more data than the others.\nFormally, let p(x, z) = p(z)p(x|z) denote the train distribution, and q(x, z) = q(z)q(x|z) the test distribution. We denote the support of each attribute component zi under training distribution as Ztrain and the support of z under training distribution as Ztrain. The corresponding supports for the test distribution are denoted as Ztest and Ztest. We define the Cartesian product of marginal support under training as Zx := Ztrain x Ztrain x.. Ztrain\nIn this work, we study compositional shifts that are characterized by:\n1. p(x|z) = q(x|z),\u2200z \u2208 Zx.\n2. Ztest \u2282 ZX."}, {"title": "Additive Energy Distribution", "content": "We assume that p(x|z) is of the form of an additive energy distribution (AED):\n$\\displaystyle p(x|z) = \\frac{1}{Z(z)} exp\\left(-\\sum_{i=1}^m Ei(x, z_i)\\right) \\qquad(1)$\nwhere $Z(z) := \\int exp\\left(-\\sum_{i=1}^m Ei(x, z_i)\\right) dx$ is the partition function that ensures that the probability density p(x|z) integrates to one. Also, the support of p(x|z) is assumed to be Rn, \u2200z \u2208 Z\u00d7.\nWe thus have one energy term Ei associated to each attribute zi. Note that we do not make assumptions on Ei except Z(z) < \u221e, leaving the resulting p(x|z) very flexible. This form is a natural choice to model inputs that must satisfy a conjunction of characteristics (such as being a natural image of a landbird AND having a water background), corresponding to our attributes.\nThere are two lines of work that inspire the choice of additive energy distributions. Firstly, these distributions have been used to enhance compositionality in generative tasks but they have not been used in discriminative compositionality. Secondly, for readers from the causal machine learning community, it may be useful to think of additive energy distributions from the perspective of the independent mechanisms principle. The principle states that the data distribution is composed of independent data generation modules, where the notion of independence refers to algorithmic independence and not statistical independence. In these distributions, we think of energy functions of different attributes as independent functions.\nRecall z = (21,..., zm) is a vector of m categorical attributes that can each take d possible values. We will denote as \u03c3(z) the representation of this attribute vector as a concatenation of m one-hot vectors, i.e.\n$\\sigma(z) = [onehot(z_1),...,onehot(z_m)]$\nThus \u03c3(z) will be a sparse vector of length md containing m ones.\nWe also define a vector valued map $E(x) = [E_1(x, 1), . . ., E_1 (x, d), . . ., E_m(x, 1), . . ., E_m(x, d)]^\\top$ where Ei(x, zi) is the energy term for ith attribute taking the value zi.\nThis allows us to reexpress equation 1 using a simple dot product, denoted (., .):\n$\\displaystyle p(x|z) = \\frac{1}{Z(z)} exp(- (\\sigma(z), E(x))), \\qquad(2)$\nwhere $Z(z) = \\int exp \\left( \u2013 (\u03c3(z), E(x))\\right)dx$ is the partition function."}, {"title": "Provable Compositional Generalization", "content": "Our goal is to learn a distribution \u011d(z|x) that matches the test distribution q(z|x) and predict the attributes at test time in a Bayes optimal manner. If we successfully learn the distribution q(z|x), then we can also predict the individual attributes q(zi|x), e.g., the bird class in Waterbirds dataset, by marginalizing over"}, {"title": "Discrete Affine Hull", "content": "We define the discrete affine hull of a set of attribute vectors $A = {z^{(1)}, ..., z^{(k)}}$ where $z^{(i)} \\in Z$, as:\n$\\displaystyle DAff(A) = \\{x \\in Z \\mid \\exists a \\in \\mathbb{R}^k, \\sigma(z) = \\sum_{i=1}^k a_i\\sigma(z^{(i)}), \\sum_{i=1}^k a_i = 1\\}$\nIn other words, the discrete affine hull of A is the set of all possible attribute vectors whose one-hot encoding is in the (regular) affine hull of the one-hot encodings of the attribute vectors of A. This construct will be used to characterize what new combinations of attributes we can extrapolate to. We now give a simple example to illustrate discrete affine hull.\nLet us revisit the Waterbirds dataset. Suppose we observe data from three out of the four groups. In one-hot encoding, we represent WB as [1,0] and LB as [0,1]. We represent Water as [1,0] and Land as [0,1]. Below we show that the attribute vector WB on L represented as $\\begin{bmatrix}1\\0\\0\\1\\end{bmatrix}$ can be expressed as an affine combination of the remaining three attribute vectors. Based on this, we can conclude that the discrete affine hull of three one-hot concatenated vectors contains all the four possible one-hot concatenations.\n$\\begin{bmatrix}1\\0\\0\\1\\end{bmatrix} = (+1)\\begin{bmatrix}1\\0\\1\\0\\end{bmatrix} + (-1)\\begin{bmatrix}0\\1\\0\\1\\end{bmatrix} + (+1)\\begin{bmatrix}0\\1\\1\\0\\end{bmatrix} \\qquad(3)$\nIn Section B.4, we generalize the above finding and develop a mathematical characterization of discrete affine hulls that leads to an easy recipe to visualize these sets. In the remainder whenever we use affine hull it means discrete affine hull."}, {"title": "Extrapolation of Conditional Density", "content": "We learn a set of conditional probability densities $\\displaystyle \\hat{p}(x|z) = \\frac{exp\\left( - (\\sigma(z), \\hat{E}(x))\\right)}{\\hat{Z}(z)}, \\forall z \\in Z^{\\text{train}}$ by maximizing the likelihood over the training distribution, where $\\hat{E}$ denotes the estimated energy components and $\\hat{Z}$ denotes the estimated partition function. Under perfect maximum likelihood maximization $\\hat{p}(x|z) = p(x|z)$ for all the training groups $z \\in Z^{\\text{train}}$. We can define $\\hat{p}(x|z)$ for all $z \\in Z^{\\times}$ beyond $Z^{\\text{train}}$ in a natural way as follows. For each $z \\in Z^{\\times}$, we have estimated the energy for every individual component $z_i$ denoted $\\hat{E}_i(x, z_i)$. We set $\\hat{Z}(z) = \\int exp\\left( - (\\sigma(z), \\hat{E}(x))\\right) dx$ and the density for each $z \\in Z^{\\times}$, $\\displaystyle \\hat{p}(x|z) = \\frac{1}{\\hat{Z}(z)} exp\\left( - (\\sigma(z), \\hat{E}(x))\\right)$.\nTheorem 1. If the true and learned distribution ($p(\\cdot|z)$ and $\\hat{p}(\\cdot|z)$) are additive energy distributions, then $\\hat{p}(\\cdot|z) = p(\\cdot|z), \\forall z \\in Z^{\\text{train}} \\implies \\hat{p}(\\cdot|z') = p(\\cdot|z'),\\forall z' \\in DAff(Z^{\\text{train}})$.\nThe result above argues that so long as the group z' is in the discrete affine hull of $Z^{\\text{train}}$, the estimated density extrapolates to it.\nProof sketch: Under perfect maximum likelihood maximization $\\hat{p}(x|z) = p(x|z), \\forall z \\in Z^{\\text{train}}$. Replacing these densities by their expressions and taking their log we obtain\n$\\displaystyle (\\sigma(z), \\hat{E}(x)) = (\\sigma(z), E(x)) + C(z),\\forall z \\in Z^{\\text{train}} \\qquad(4)$"}, {"title": "Extrapolation of Discriminative Model", "content": "In Section 4.2, we saw how we could, in principle, obtain a classifier that generalizes under compositional shift, by first training conditional probability density models p(x|z). But high dimensional probability density modeling remains very challenging, and involves dealing with intractable partition functions. It is typically deemed much simpler to learn a discriminative classifier.\nCan we achieve the same extrapolation without having to estimate the entire distribution of x conditional on z? This question brings us to our method, which we refer to as Compositional Risk Minimization (CRM).\nObserve that if we apply Bayes rule to the AED p(x|z) in equation 2, we get\n$\\displaystyle p(z|x) = \\frac{p(x|z)p(z)}{\\sum_{z'\\in Z^{\\text{train}}} p(x|z')p(z')} = \\frac{exp \\left( - (\\sigma(z), E(x)) + log p(z) - log Z(z)\\right)}{\\sum_{z'\\in Z^{\\text{train}}} exp\\left(- (\\sigma(z'), E(x)) + logp(z') \u2013 log Z(z')\\right)}$\nWe thus define our additive energy classifier as follows. To guarantee that we can model this p(z|x), we use a model with the same form. For each z \u2208 Ztrain\n$\\displaystyle \\hat{p}(z|x) = \\frac{exp \\left( - (\\sigma(z), E(x)) + log\\hat{p}(z) - B(z)\\right)}{\\sum_{z'\\in Z^{\\text{train}}} exp\\left(- (\\sigma(z'), E(x)) + log\\hat{p}(z') \u2013 B(z')\\right)} \\qquad(6)$\nwhere $\\hat{p}(z)$ is the empirical estimate of the prior over z, i.e., $ \\hat{p}(z)$, $E : \\mathbb{R}^n \\rightarrow \\mathbb{R}^{md}$ is a function to be learned, bias B is a lookup table containing a learnable offset for each combination of attribute. Given a data point (x, z), loss $l(z, p(\\cdot|x)) = - log \\hat{p}(z|x)$ measures the prediction performance of $p(x)$. The risk is defined as the expected loss as follows"}, {"title": "Algorithm for Compositional Risk Minimization (CRM)", "content": "In a nutshell, CRM consists of: a) training a model of the form of equation 6 by maximum likelihood (equation 8) for trainset group prediction; b) compute extrapolated biases (equation 10); c) infer group probabilities on compositionally shifted test distribution using equation 9. For the case where we have 2 attributes z = (y, a), Figure 1 illustrates a basic architecture using a deep network backbone (x; 0) followed by a linear mapping (matrix W), and Algorithm 1 provides the associated pseudo-code."}, {"title": "Experiments", "content": "We evaluate CRM on widely recognized benchmarks for subpopulation shifts, that have 2 attributes z = (y, a), where y denotes the class label and a denotes the spurious attribute (y and a are correlated). However, the standard split between train and test data mandated in these benchmarks does not actually evaluate robustness to compositional shifts, because both train and test datasets contain all the groups (Ztrain = Ztest = Z\u00d7). Therefore, we repurpose these benchmarks for compositional shifts by discarding samples from one of the groups (z) in the train (and validation) dataset; but we don't change the test dataset, i.e., z \u2209 Ztrain but z \u2208 Ztest. Let us denote the data splits from the standard benchmarks as"}, {"title": "Conclusion", "content": "We provide a novel approach based on flexible additive energy models for compositionality in discriminative tasks. Our proposed CRM approach can provably extrapolate to novel attribute combinations within the discrete affine hull of the training support, where the affine hull grows quickly with the training groups to cover the Cartesian product extension of the training support. Our empirical results demonstrate that the additive energy assumption is sufficiently flexible to yield good classifiers for high-dimensional images, and that the proposed CRM estimator is able to extrapolate to novel combinations in DAff(Ztrain), without having to model high-dimensional p(x|z) nor having to estimate their partition function. CRM is a simple and efficient algorithm that empirically proved consistently more robust to compositional shifts than approaches based on other logit-shifting schemes and GroupDRO."}]}