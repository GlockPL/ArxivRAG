{"title": "EFFECT OF A PROCESS MINING BASED PRE-PROCESSING STEP IN PREDICTION OF THE CRITICAL HEALTH OUTCOMES", "authors": ["Negin Ashrafi", "Armin Abdollahi", "Greg Placencia", "Maryam Pishgar"], "abstract": "Predicting critical health outcomes such as patient mortality and hospital readmission is essential for improving survivability. However, healthcare datasets have many concurrences that create complexities, leading to poor predictions. Consequently, pre-processing the data is crucial to improve its quality. In this study, we use an existing pre-processing algorithm, concatenation, to improve data quality by decreasing the complexity of datasets. Sixteen healthcare datasets were extracted from two databases - MIMIC III and University of Illinois Hospital - converted to the event logs, they were then fed into the concatenation algorithm. The pre-processed event logs were then fed to the Split Miner (SM) algorithm to produce a process model. Process model quality was evaluated before and after concatenation using the following metrics: fitness, precision, F-Measure, and complexity. The pre-processed event logs were also used as inputs to the Decay Replay Mining (DREAM) algorithm to predict critical outcomes. We compared predicted results before and after applying the concatenation algorithm using Area Under the Curve (AUC) and Confidence Intervals (CI). Results indicated that the concatenation algorithm improved the quality of the process models and predictions of the critical health outcomes.", "sections": [{"title": "1 Introduction", "content": "A patient's health records can be understood as a sequence of observations including services performed, diagnoses, or lab measurements [1]. In the process mining field, each of these observations is called an event, and the sequences of observations are referred to as event logs. Process mining analyzes and optimizes processes. The process mining approach has been used widely in healthcare to enhance healthcare processes [2] and to predict critical health outcomes [1, 3, 4, 5].\nReal-life event logs are complex and noisy. Infrequent behaviors and various concurrences generate inefficient and complex process model through process discovery algorithms. Healthcare data exacerbates these issues. For instance, various lab measurements can be taken at the same time causing concurrent relationships among these different lab measurement events. It is therefore critical to pre-process raw event logs to improve their quality, to make the process model more understandable."}, {"title": "2 Preliminaries", "content": "The notations used in this section are adopted from [20].\n2.1 Process Mining\nProcess mining analyzes and optimizes process sequences called event logs. Process mining uses the following steps: process discovery, conformance checking, and process enhancement. The process discovery step is the most important step in process mining. This step uses process discovery algorithms to output a process model, mostly called a Petri Net (PN).\n2.2 Petri Net\nA PN is a mathematical model often used to represent a process. It includes a set of places graphically represented as circles and transitions as rectangles. Transitions correspond to events. The occurrence of a process is modeled by firing transitions. Places can be marked or unmarked. Marked places indicate current process states. A place is marked by putting a dot (token) in the circle corresponding to that event. For a transition to be fired, all its input places must contain a token. If a transition fires, the all tokens are removed from its inputs. Also, each output of the transition receives a token.\n2.3 Event Logs\nAn event $a \\in A$ is an instantaneous change of a process' state where A is the finite set of all possible events. An event a can happen more than once in a provided process. An event instance E is a vector with two attributes at minimum: the name of the associated event a and the corresponding occurrence timestamp, \u03c4. The timestamps 7 of two events cannot be equal. A trace g e G is a finite and ordered sequence of event instances [20].\n2.4 Decay Replay Mining\nDecay Replay Mining (DREAM) [21] is a process mining / deep learning-based approach used to predict the next event in the sequence of a process. DREAM uses places from a process model and extends these places with a time decay function F(T). The time decay function F(T) uses timestamps 7 information as a parameter when replaying event logs L on a process model and produces Timed State Samples (TSS)."}, {"title": "2.5 Timed State Samples", "content": "A TSS will be produced by replaying event logs L on a process model [21]. A TSS, S(T) at time 7 results from concatenating time decay function values F(T), token counts C'(T), and of a marking M(\u03c4):\n$S(T) = F(T) + C(T) + M(T)$  (1)"}, {"title": "2.6 Concatenation Algorithm", "content": "The concatenation algorithm [7] is a pre-processing algorithm that removes some concurrences and self-loops based on a probability algorithm. This improves the quality of event logs and the resulting process model."}, {"title": "2.7 Concurrency Relation", "content": "Given any event logs L, and any two labels l, l' e L, and any two events $e_i, e_j \\in E$, a concurrent relation exists, denoted by (ei||ej), given the following conditions: |l \u2192 l'| > 0 and |l' \u2192 l| > 0."}, {"title": "2.8 Self-loops", "content": "A self-loop exists in the event logs L, if |l \u2192 l| is positive for some event e \u2208 E with \u03bb(e) = l [22]."}, {"title": "2.9 F-Measure", "content": "An F-Measure is an evaluation metric uses to measure the quality of a process model. The F-Measure is a trade-off value between fitness and precision [23]. The Fitness of a process model represents the behavior of the event logs L. Precision shows the capability of a model to produce the behaviors found only in the event logs L. In this work, the alignment-based method is used to measure both fitness and precision per [20, 24]."}, {"title": "2.10 Complexity", "content": "Complexity is an evaluation metric that measures the quality of a process model. Complexity shows how easily a process model can be understood. Several metrics measure the complexity of a model [22], including Control-Flow Complexity (CFC) [23], size of the models [25], and structuredness [25]. CFC shows how many branches are prompted by split gateways in a process model. The size of a process model calculates the numbers of the nodes and arcs in the model. A process model is structured, if, for any split gateway in the process model a corresponding join gateway exits."}, {"title": "2.11 Split Miner", "content": "Split Miner (SM) is a process discovery algorithm that produce a sound PN from the event logs L. It produces a process model with less complexity and higher F-Measure value than other process discovery algorithms. The SM algorithm includes the following steps: First, generate a directly-follow graph and identify short loops. Second, discover the existing concurrency relation between events. Third, utilize filtering by introducing 2 threshold values, to control the filtering process (frequency threshold, \u20ac), the other, \u03b7, to control concurrency relations. Moreover, the algorithm derives choice and concurrency relations by adding split gateways. In the end, the join gateways are discovered. The output of these steps is a BPMN, and it can be converted to a PN through using ProMs' BPMN Miner package [26]. This algorithm is publicly available as a java application [26]."}, {"title": "3 Methodology", "content": "This section focuses on a proposed method for predicting several health outcomes with and without applying a pre- processing step on event logs L. First, feature selection is described. Next, conversion of Electronic Health Records (EHR) to event logs L is explained followed by the pre-processing step. Finally, the DREAM algorithm is introduced to predict health outcomes.\nThe intuition behind this methodology is that applying an appropriate pre-processing step on complex healthcare event logs L decreases complexity of a process model. Hence, it predicts critical health outcomes more accurately. Since healthcare event logs L contain many concurrences and self loops, applying a concatenation algorithm to concurrent events makes the process model simpler and generates more accurate predictions. An overview of the proposed framework is shown in Figure1."}, {"title": "3.1 Feature Selection", "content": "MIMIC III [27] and University of Illinois Hospital (UIH) databases[18] were used for predictions. Due to the nature of our approach (process mining / deep learning approach) every patient must have a medical history available at the time of prediction. Table1 shows the features extracted from MIMIC III and UIH databases by disease category."}, {"title": "3.2 Conversion of EHR to the Event Logs", "content": "Event logs L can be understood as a sequence of events with timestamps 7 associated to events that have occurred. Events represent activities performed on patients such as admission, diagnosis, lab measurements, etc., also known as patients careflows. Each unique event has a corresponding timestamp 7 when it occurred. In this paper, the timestamp \u0442 of comorbidity and artificial events correspond to the discharge time of the corresponding hospital admission. Since events in a trace g occur sequentially, multiple comorbidities or artificial events with the same timestamps 7 are delayed by multiples of 1 ms to retain their order. Table2 summarizes the conversion of the EHR to the event logs L."}, {"title": "3.3 Pre-processing Step", "content": "In Algorithm1, the resultant event logs (L) from the previous step were then used as an input to the concatenation algorithm. Assume that events ei, ej, ek, and el \u2208 L, if (ei \u2192 ej) and (ej \u2192 ei); also (ek \u2192 ei) and (el \u2192 ek), if there is a concurrent relations between ei, ej, ei||ej, and between er, el, ek||el."}, {"title": "Algorithm 1 Event Log Concatenation Algorithm", "content": "Require: Event logs L, threshold p*\nEnsure: Modified event logs L'\n1: Initialize an empty list of concurrent pairs\n2: for all events $e_i, e_j \\in L$ do\n3:   if $(e_i \\rightarrow e_j) \\land (e_j \\rightarrow e_i)$ then\n4:     Add $(e_i||e_j)$ to the list of concurrent pairs\n5:   end if\n6: end for\n7: Initialize an empty list of valid pairs\n8: for all pairs $(e_i||e_j)$ in the list of concurrent pairs do\n9:   if P($e_i \\rightarrow e_j) > p*$ and P($e_j \\rightarrow e_i) > p*$ then\n10:    P($e_i||e_j$) = P($e_i \\rightarrow e_j$) + P($e_j \\rightarrow e_i$)\n11:    Add ($e_i||e_j$, P($e_i||e_j$)) to the list of valid pairs\n12:  end if\n13: end for\n14: Sort the list of valid pairs in descending order of P($e_i||e_j$)\n15: for all pairs $(e_i||e_j)$ in the sorted list of valid pairs do\n16:  Remove $e_i$ and replace $e_j$ with $e_i \\oplus e_j$\n17: end for\n18: for all events $e_i \\in L$ not concatenated do\n19:  if $e_i \\in e_i \\oplus e_j$ then\n20:   Remove $e_i$ and replace with $e_i \\oplus e_j$\n21:  end if\n22: end for\n23: for all events $e_i \\in L$ do\n24:  if $e_i == e_i$ then\n25:   Remove $e_i$\n26:  end if\n27: end for\n28: Feed the modified event logs L' to the SM algorithm for process modeling\n29: Use the resultant process model as input to the DREAM algorithm for prediction\nWe then calculate P($e_i \\rightarrow e_j$) and P($e_j \\rightarrow e_i$); and P($e_k \\rightarrow e_i$) and P($e_i \\rightarrow e_k$). Moreover, threshold value p* is introduced, if P($e_i \\rightarrow e_j$) > p* and P($e_j \\rightarrow e_i$) > p*, the combination is chosen. Similarly, if P($e_k \\rightarrow e_i$) > p* and P($e_i \\rightarrow e_k$) > p*, the combination is chosen for the next step. Additionally, the probabilities of the selected combinations were added and sorted in the descending order, P($e_i||e_j$) = P($e_i \\rightarrow e_j$) + P($e_j \\rightarrow e_i$) also P($e_k||e_i$) = P($e_k \\rightarrow e_i$) + P($e_i \\rightarrow e_k$). Furthermore, if P($e_i||e_j$) = P($e_k||e_i$) a re-position step was applied. After the order of combinations for concatenation is finalized, the concatenation algorithm selects the ordered combinations one by one for the concatenation. Assume that (ei, ej) is chosen for concatenation, event ei is removed and replaced with the concatenated event, $e_i \\oplus e_j$. This step is repeated until all selected combinations are concatenated in the event logs L."}, {"title": "3.4 DREAM algorithm", "content": "The algorithm has three different phases [21]. The first phase is to create a PN model from an event log L and assign a decay function to each PN location fp(T). The discovery log is then replayed, and feature arrays comprising decay function response values, token movement counts, and resource use are extracted. Lastly, we train a Neural Network (NN) to predict the next event, using these feature arrays as training data. To simulate data values that decrease over time, one can use decay functions. Financial domains, physical systems, and population trend modeling are among the common applications for these functions. We assign a linear decay function fp(7) to every place in the PN produced by SM on an event log L."}, {"title": "Algorithm 2 PN Discovery and Decay Function Initialization", "content": "Require: Event log L with traces {$g_1, g_2,..., g_n$}\nRequire: Decay function parameters \u03b1, \u03b2\nEnsure: Discovered PN with initialized decay functions\n1: Step 1: PN Model Discovery\n2: Use SM to discover a PN model from event log L\n3: for each place p in the PN model do\n4:   Associate place p with a linear decay function $f_p(T) = max(\u03b2 - \u03b1 \\cdot \\Delta_p, 0)$\n5: end for\n6: Step 2: Decay Function Enhancement\n7: for each place p in the PN model do\n8:   Initialize the decay function $f_p(7)$ to 0 by setting $\u0394_p = \u221e$\n9: end for\n10: Step 3: Calculate Decay Rates\n11: for each place p in the PN model do\n12:  Calculate the maximum trace duration Amax(L) = $max_{1<i<|L|}(V_{dts} (L_{i,y(L_i)}) - V_{dts} (L_{i,1}))$\n13:  Define $v_p(g)$ as the number of tokens entering place p during replay of trace g\n14:  if maxgEL Up(g) \u2264 1 then\n15:   Set $a_p = \\frac{\u03b2}{A_{max} (L)}$\n16:  else\n17:   Set $a_p = \\frac{\u03b2}{mean_{gEL} d_p (g)}$\n18:  end if\n19: end for\n20: Step 4: Update Decay Functions with Calculated Rates\n21: for each place p in the PN model do\n22:  Update the decay function $f_p(7) = max(\u03b2 \u2013 a_p \\cdot (\u0442 \u2014 \u0442_p), 0)$\n23: end for\n24: Return the discovered PN model with initialized decay functions\nBy using Ap = T - Tp, we can express the time difference between the current time, 7, and the most recent time a token entered place p, Tp. With the help of the two decay function parameters, a and \u00df, we can adjust the significance level. Based on the reactivation durations of a place p, a should ideally be set so that the slope of fp(T) covers the entire range from 3 to 0. In other words, the slope should not be excessively steep for a small Ap such that fp(T) = 0 or too flat for a large Ap such that fp(r) = B. This suggests varying a values with each place in the PN model. In the next step of Algorithm2 we try to estimate these ap values. Using the event log L and the corresponding PN found by the SM on L, we estimate ap. There are a limited number of event instances in each trace g in L. The jth event instance of an event log L's ith trace is referred to by Li,j. A(Li) expresses the number of event instances of the ith trace of the event log L. The value of the attribute d for the event instance E is $va(E)$. When an attribute d is absent from an event instance E, the result is $va(E) = 0$, an empty set. We use dts to represent the attribute timestamp. Amax (L), the Maximum observed trace duration in an event log L, is defined in the algorithm and based on the value of maxg\u2208L Up(g) the value of ap is determined. If the value is less than one then ap will be set to a value that ensures that before the final event"}, {"title": "Algorithm 3 Event Log Replay and Feature Extraction", "content": "Require: Event log L, PN with places P, decay functions fp(7) for each place p \u2208 P\nEnsure: Set of timed state samples S\n1: Initialize counting vector C(T) to zero\n2: for each trace g\u2208 L do\n3:   Reset F(T), C(T), and marking M(\u03c4) to initial states\n4:   for each event $e_i$ in trace g do\n5:   Calculate decay function responses F(r) = {$f_p(t) | p\u2208P$}\n6:   Update C(T) for token movements\n7:   Update PN marking M(7) based on transition fired by $e_i$\n8:   Construct timed state sample S(t) = F(t) + C(T) \u2295 M(T)\n9:   Add S(T) to set of timed state samples S\n10:  end for\n11: end for\n12: Return set of timed state samples S\nAs a result, the slope will not be overly flat or steep. In Algorithm3 F(T) is a vector of decay functions for all places. C(T) shows the number of tokens that have entered each place from time 0 to 7. When a token enters place p and time T the corresponding value in C(T) vector is updated. Here M(T) is a vector representing the marking of a PN. So for each trace g and events in traces, the value of the aforementioned vectors are updated, and then the timed state sample S(T) is constructed by concatenating the vectors. Consequently, a timed state sample, S(T) provides a time-dependent description of a PN process state. It includes data regarding time-based token movements, such as token counts per location (loop information), the current PN state using the marking, and the last time a token entered a place in relation to the current time. In the last step of the algorithm, the S(T) is input to the different NN with different architecture for predicting the next event."}, {"title": "3.5 Prediction", "content": "DREAM was used to predict the critical health outcomes for both pre-processed and non-processed event logs L. DREAM replays the event logs L on a process model and produces time information related to the variables which are called TSS. The TSS, demographic information, and severity scores were fed into the dense NN for prediction. The NN architecture differs by disease but remaine the same for both pre-processed and unprocessed event logs L.\n3.5.1 CAD disease\nTSS are fed into a unique branch of one hidden layer with 200 neurons as shown in Figure2. The Dropout (DO) rate after the first layer was regularized at DO = 0.2. Demographic information was fed into a single hidden layer with 20 neurons and the same dropout rate. These were concatenated into a second layer with 90 neurons and the same dropout rate. The critical outcome which is predicted in this case is the mortality of ICU patients with CAD."}, {"title": "3.5.2 HF disease", "content": "TSS, demographics information, and severity scores were fed separately to three branches each with three hidden layers (Figure3). A batch normalization layer was added after the first hidden layer for each branch with a dropout rate of 0.2. The output layer included a softmax activation function to predict unplanned 30-day readmission of ICU HF patients. The critical outcome predicted in this case was unplanned 30-day readmission of ICU patients with HF."}, {"title": "3.5.3 Diabetes", "content": "Each input layer is fed to an individual hidden layer branch before a hidden layer concatenates the branches and feeds it to two further layers. All hidden layers use a Rectified Linear Unit activation function (ReLU). The branched first hidden layers also have a batch normalization layer. Dropout layers with a rate of 0.4 for regularization are used. The output layer consists of a softmax activation function to output the patient's probability of in-hospital mortality for diabetes patients. This architecture is shown in Figure4."}, {"title": "3.5.4 COVID- 19", "content": "TSS, demographics information and comorbidities were fed separately to two branches where the first branch contained three hidden layers with 90, 50 and 20 neurons respectively (Figure5). The first and second hidden layers each had a dropout layer with a rate of 0.2. The second branch also contained one hidden layer with 5 neurons. The two branches were concatenated to a branch with three hidden layers, containing 90, 50, and 20 neurons respectively. The dropout layer after the second concatenated hidden layer had a rate of 0.3. The output layer used a softmax activation function to predict mortality of COVID- 19 patients every 6-hour during the first 72 hours after hospital admission."}, {"title": "3.5.5 PI", "content": "In Figure6, TSS were fed into two hidden layers. The first had 76 neurons and a subsequent dropout rate of DO = 0.5. The second hidden layer contained 20 neurons. The demographic information was fed into a single hidden layer of 5 neurons. Both layers were concatenated into two further layers with 96 and 10 neurons respectively. Between these two layers was a layer with a dropout rate of DO = 0.5. The outcome predicted in this case was the mortality of ICU patients diagnosed with PI 24 hours after being admitted to the ICU."}, {"title": "4 Evaluation", "content": "This section discusses experimental evaluation of the model to predict the critical health outcomes. Subsection one describes the datasets. Subsection two describes the modeling set up. Subsection three highlights the results."}, {"title": "4.1 Datasets", "content": "Data was obtained from MIMIC III (Medical Information Mart for Intensive Care) [27], a large database containing information relating to patients admitted to Beth Israel Deaconess Medical Center (BIDMC). This data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more [27]. Data was also obtained from UIH, a tertiary, teaching hospital in Chicago. This study was approved by The University of Illinois at Chicago (UIC) Institutional Review Board [18]. A summary of the datasets for each disease category is shown in Table3."}, {"title": "4.2 Setup", "content": "RapidProM was used to run experiments [24] extending RapidMiner with process mining analysis capabilities. A concatenation algorithm's threshold value of p* was used to select the combinations of concurrent events for further concatenation steps.\nThe optimal value of p* was based on the highest F-Measure value. The hyper-parameter optimization was conducted on 16 datasets using steps of 0.1. Of these, 13, found an optimal value of p* = 0.7.\nSM takes two threshold values: \u03b7 and 6. These values were optimized based on the highest F-Measure values using steps of 0.1. Thresholds selected were \u03b7 = 0.4 and \u20ac = 0.1.\nTrain and validation sets are required to discover a process model and train the NN. The validation set is used to select the best model. A test set is then used to evaluate model performance. A summary of the set up for training the NN for each disease category is shown in Table4."}, {"title": "4.3 Results", "content": "In our first experiments without pre-processing, we used the optimal threshold values found for the SM algorithm, plus raw event logs to discover process models. The model was fed to the DREAM algorithm to produce a TSS. The TSS, severity score and demographics were then fed to a NN for prediction.\nFor our second experiments, with pre-processing we applied the concatenation algorithm to raw event logs L using optimal thresholds values for the algorithm to produce modified event logs L. The modified event logs L were then fed to the SM algorithm with the aforementioned optimal threshold values to discover a process model, which was then fed to the DREAM algorithm to produce the TSS. The TSS, severity score and demographics were then fed to a NN for prediction.\nResults of the evaluation are summarized in Table5. The F-Measure is calculated for different types of disease before and after the concatenation algorithm. In Table6, AUC and CI are calculated and in Table7 complexity is measured by evaluating the size of the models, sructuredness and CFC. We ran the Wilcoxon Test to determine whether improvements, observed for evaluation metrics applied after concatenation were statistically significant or not (p = 0.05). Of 6 pre-processed datasets, 9 showed statistically significant differences in AUC scores. All 16 showed statistically significant differences in F-Measures and complexity metrics."}, {"title": "4.4 Discussion", "content": "To reduce concurrences and self-loops of the complex healthcare data, we applied a concatenation algorithm as a pre-processing step to improve data quality to improve the trustworthiness of prediction models for clinicians. We observed significant statistical improvements in AUC values, F-Measure and complexity metrics.\nThis was especially evident for the CAD and COVID-19 datasets which were more complex compared to other datasets. In those cases the predicted results improved by significant statistical amount of 2% for the AUC metric from our experiments.\nOne of the main advantages of a process mining approach is that it discovers a detailed process model which allows clinicians to comprehensively visualize the entire process patients (careflow) go through in a medical system. This is precisely why process mining has been preferred over other traditional machine learning methods. Process mining also accurately models temporal time-related information related to variables and utilizes them effectively as an input to the neural networks when other traditional techniques cannot. Lastly, process mining leverages the extensive and detailed medical history of patients from prior hospital admissions.\nA limitation of the proposed approach is that it requires patients' medical history that small clinics and hospitals might not have. It also excludes patients with no prior hospital admissions like new patients and patients who are not admitted before the current hospital visit. Moreover, hospitals tend not to share patient data across outside hospital networks. Thus, the proposed approach best matches large hospital networks. In addition, the training, validation, and test sets all came from the MIMIC-III and UIH datasets. Therefore, using an independent dataset from a different health center or hospital would better test the performance of the model for future research."}, {"title": "5 Conclusion", "content": "Healthcare data is complex and have many concurrences and loops. This makes predicting healthcare outcomes difficult, because data quality is poor. By using concatenation algorithm as a pre-processing step, data complexity decreased, improving the performance of process discovery algorithms by recommending a probability-based concatenation of events, with concurrent relations. Thus, it can predict critical healthcare outcomes more accurately."}, {"title": "Appendix: Notations", "content": "a Event\nA Finite set of all events\n\u03b1 Decay rate\nap Decay rate for a specfic place p\n\u03b2 Constant parameter of a decay function\nC(T) Token counting vector from time 0 to 7 each element represents the number of tokensthat entered a specific place\nd Attribute\ndts Timestamp attribute\nD Finite set of all possible attributes\nAmax (L) Maximum observed trace duration in an event log L\n\u0394p Difference of 7 and Tp\n\u03b4p(9) function of average time between a token is consumed in place p until a new token is produced in p based on an input trace g\nE Event instance vector\ne An instance of E\n\u03b7 SM threshold which controls concurrency relations\nE SM threshold which controls filtering process\nfp(T) Decay function of place p\nF(T) Decay function response vector\ng Case or trace\nG Finite set of all possible traces\nL Event log which is a set of traces\nLizj jth event instance in the ith trace of an event log L\nl label of each event\n\u03bb function that retrieves the label of each event"}]}