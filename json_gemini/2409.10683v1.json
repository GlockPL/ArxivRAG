{"title": "MotIF: Motion Instruction Fine-tuning", "authors": ["Minyoung Hwang", "Joey Hejna", "Dorsa Sadigh", "Yonatan Bisk"], "abstract": "While success in many robotics tasks can be determined by only observing the final state and how it differs from the initial state e.g., if an apple is picked up - many tasks require observing the full motion of the robot to correctly determine success. For example, brushing hair requires repeated strokes that correspond to the contours and type of hair. Prior works often use off-the-shelf vision-language models (VLMs) as success detectors; however, when success depends on the full trajectory, VLMs struggle to make correct judgments for two reasons. First, modern VLMs are trained only on single frames, and thus cannot capture changes over a full trajectory. Second, even if we provide state-of-the-art VLMs with an aggregate input of multiple frames, they still fail to correctly detect success due to a lack of robot data. Our key idea is to fine-tune VLMs using abstract representations that are able to capture trajectory-level information such as the path the robot takes by overlaying keypoint trajectories on the final image. We propose motion instruction fine-tuning (MotIF), a method that fine-tunes VLMs using the aforementioned abstract representations to semantically ground the robot's behavior in the environment. To benchmark and fine-tune VLMS for robotic motion understanding, we introduce the MotIF-1K dataset containing 653 human and 369 robot demonstrations across 13 task categories. MotIF assesses the success of robot motion given the image observation of the trajectory, task instruction, and motion description. Our model significantly outperforms state-of-the-art VLMs by at least twice in precision and 56.1% in recall, generalizing across unseen motions, tasks, and environments. Finally, we demonstrate practical applications of MotIF in refining and terminating robot planning, and ranking trajectories on how they align with task and motion descriptions.", "sections": [{"title": "I. INTRODUCTION", "content": "Measuring success in robotics has focused primarily on what robots should do, not how they should do it. Concretely, what is determined by the final state of an object, robot, or end-effector [1], [2]. However, not all trajectories that achieve the same final state are equally successful. When transporting a fragile object, a path through safer terrain could be considered more successful than a shorter yet riskier route (Fig. 1 a). Similarly, in the presence of humans a robot's actions when navigating, holding objects, or brushing human hair (Fig. 1 b-d) can cause surprise, discomfort, or pain, making such motions less successful.\nSuccess detectors play an important role in robot learning since they evaluate whether or not a robot has completed a task. However, most overlook the importance of \"how\" the task is accomplished, focusing on the initial and final states of the trajectory [2], [3]. This simplification fails to account for tasks that fundamentally require evaluating the entire trajectory to assess success. As we incorporate robots into everyday"}, {"title": "II. RELATED WORK", "content": "With the recent development of large language models (LLMs) and VLMs, foundation models have been used to understand the environment and critique agent behaviors. Additionally, the increasing use of visual observations in robotics has brought attention to motion-centric visual representations.\nSuccess Detection Using Foundation Models. [4], [5], [6], [7], [8] use LLMs to generate reward functions that evaluate agent behaviors. However, as LLMs cannot understand how a robot's actions are visually grounded in a scene, such methods require everything including the state of the environment to be translated through language. VLMs have been used to understand and evaluate agent behaviors given visual and language information. [9] use a VLM critic for general vision-language tasks, and closed API-based models have been used as behavior critics in robotics, even if inaccurate [10], [3]. Prior work [11], [10], [12], [13] have also used VLMs as zero-shot reward models for training downstream policies. [2] trains a VLM success detector for evaluating what was achieved from the robot, but does not consider \"how\" the agent solves the task. [14], [15] use representation learning to train reward models as behavior critics. Other studies [16], [17], [18] train VLMs to be physically or spatially grounded, but focus"}, {"title": "III. MOTION INSTRUCTION FINE-TUNING (MOTIF)", "content": "In this section we first broaden the definition of success by including motion as a core component. We then discuss why pre-trained models are insufficient for motion-based success detection in robotics. Finally, we introduce MotIF for fine-tuning VLMs to be motion-aware success detectors."}, {"title": "A. Problem Statement", "content": "Success detection has been an integral part of recent robotics literature. Typically, success detection is defined as a binary function of the final state conditioned on the task, $y = f(o_T|task) \\in {0,1}$ [2], where y is the binary success label and $o_T$ is the image observation of the agent and the environment at the final time step T. This restrictive assumption prevents success detectors from criticizing how a task is completed. For many tasks like collision-aware navigation, the"}, {"title": "B. Visual Motion Representations", "content": "Can foundation models be used for success detection?\nWhile VLMs have demonstrated a strong understanding of physical and causal commonsense reasoning [31], [32] and semantic grounding [17], [16], [33], they typically work with static images as inputs, and cannot reason about sequential inputs necessary for dynamic tasks in robotics. Understanding motion requires not only isolating the most meaningful aspects of the scene but also identifying which changes that occurred due to the robot's motion are semantically relevant to the task. Without the ability to understand the changes over time, VLMs may struggle to detect success over robotic motions.\nWhile the ideal model would simply extract semantic content from videos, the strongest current VLMs instead rely on detecting differences between multiple frames, often in the form of storyboards (see Fig. 2 (c-d) and the example of GPT-40 in Section C). Though straightforward, this approach often struggles as storyboards lead to lower resolution images. Instead, we borrow insights from prior work [21], [29] that show VLMs can effectively leverage diagrams or abstract representations on top of image observations.\nRepresenting a robot's motion in a single image. Due to the limitations of storyboards, we explore what representations effectively capture a robot's motion in a single frame. To construct diagrams of robotic motions, we overlay a robot's trajectory on the image observation as shown in Fig. 2 (a-b). One solution is to detect K keypoints ${(x_1, y_1^t), . . ., (x_K^0, y_K^t)}$ in the initial frame $I_0$, and track the movement of each keypoint until the final frame $I_T$ (see Fig. 2 (b)). Here, $x_k^t$ and $y_k^t$ denote the x and y coordinates of the kth keypoint in a 2D image observation at timestep t, respectively. The detected visual traces, i.e., optical flow, represent the apparent motions of the robot and how the environment changes accordingly. While this solution helps a single image representation contain the information of multiple keypoints, the full optical flow with trajectories of all keypoints may obscure a large portion of the background and important objects in the scene. Additionally, visualizing many keypoints often results in indistinguishable or overlapping trajectories, which may create visual clutter and reduce the clarity of the motion.\nTherefore, the proposed method, MotIF, visualizes the trajectory of the most representative keypoint (see Fig. 2 (a)), as a simplified yet more interpretable visual motion representation. We call the keypoint as point of interest, where a point of interest is typically chosen as a point on the end effector's surface by human annotators. Compared to prior work [21] that visualizes the center of mass of the end effector, our approach ensures that the selected keypoint is visually recognizable and not occluded. Details on labeling the point of interest and visualizing its trajectory are provided in Section IV."}, {"title": "C. Fine-Tuning VLMs", "content": "While we can directly pass a single image visualizing the robot's trajectory into a model zero-shot, non-fine-tuned models struggle to understand complex robotic motions. Section V empirically shows that existing state-of-the-art VLMs often"}, {"title": "IV. MOTIF-1K DATASET", "content": "To benchmark and improve VLMs for motion understanding, we release the MotIF-1K dataset containing 653 human and 369 robot demonstrations across 13 tasks. As in Fig. 1 and Fig. 4, each task has demonstrations for 2 to 5 distinct motions which vary during the intermediate steps of a trajectory. For instance, a motion's path shape and semantic relationship with nearby objects may be different across demonstrations. This captures the diversity of how a task can be achieved, reflecting the nuanced and complex motions present in real-world scenarios. For instance, when shaking a boba drink, one person might use vigorous vertical movements, while another might use careful side-to-side movements to avoid spillage after inserting a straw. Moving a cup near a laptop via the shortest path is preferred if the cup is empty, but a detour is necessary if the cup contains water. Motion diversity is essential in grounded settings where motions need to adapt to varying environmental contexts. Fig. 4 shows examples of diverse motions. By collecting a diverse set of context-dependent motions with different intermediate trajectories, we ensure that our dataset challenges VLMs to consider the full trajectory for success detection."}, {"title": "A. Collecting Human and Robot Demonstrations", "content": "As addressed in Section III-C, we use a mixture of human and robot demonstrations. In this section, we will explain how we collect human and robot demonstrations. Fig. 4 visualizes the final frames of the collected human and robot demonstrations. Our human demonstrations are collected by six different people to ensure ample variation in motion. For robot data, a single human expert teleoperates a Stretch robot with a Meta Quest 3 VR controller for manipulation tasks and a gamepad for navigation tasks. We record the agent's joint states and image observations with a fixed exocentric RGBD camera for visual consistency. For the pick and place, stir, shake, brush hair, and tidy hair tasks we collect trajectories in two orthogonal camera views to support future 3D motion understanding using multiview image observations. Since we use VLMs that input RGB images, we treat observations from different camera viewpoints as separate trajectories and focus on effectively representing the agent's motion on a 2D image frame. Section IV-B explains how we annotate the trajectories with task instructions and fine-grained motion descriptions. We preprocess the collected demonstration observations using three motion representation methods (see Fig. 2):\n\u2022 optical flow [22], [23]: visualizing the trajectories of all visible keypoints with rainbow colors. For each keypoint, its trajectory is drawn with a single color.\n\u2022 single keypoint tracking (MotIF): visualizing a trajectory of a single keypoint. For single point tracking on human data, we use mediapipe [36] and track the center of the hand pose. For robot data, we annotate 2D keypoints to identify point of interests in the initial frame of each episode, which is either a keypoint on the object that is being manipulated or the initial position of the robot's end effector. Then, we choose the keypoint nearest to the point of interest. For both human and robot data, temporal changes are shown with color gradient from white to green, ending with a red circle.\n\u2022 N-frame storyboard (N = 2, 4, 9): sampling N keyframes and stacking those frames into a single image. We use K-means clustering on the image embeddings of all frames to sample keyframes that are sufficiently different in latent space. Frame indices are annotated above each frame image."}, {"title": "B. Grounded Motion Annotations", "content": "In this section, we use \"agent\" to refer to either human or robot demonstrating the task. We use human annotators to label"}, {"title": "V. EXPERIMENTS", "content": "In this section we seek to answer the following questions: 1) How does MotIF compare to start-of-the-art models? 2) How important is robot data in understanding motion? and finally 3) What is the effect of visual motion representation? We compare our approach with state-of-the-art models and assess the benefits of co-training on human and robot data. We investigate the impact of different visual motion representations. All models are evaluated on the validation and test splits in MotIF-1K. See Section C for detailed analyses.\nBaselines. We evaluate against GPT-40, GPT-4V [37], and Gemini-1.5 Pro [38] as state-of-the-art API-based baselines. We also compare to the best performing pre-trained open LLaVA [35] models with various sizes (7B, 13B, 34B). To evaluate different visual motion representations, we compare our proposed single point tracking to full optical flow, N-frame storyboard (N = 2, 4, 9), and a single-frame image. All learned baselines are fine-tuned from the LLaVA-v1.5 7B model [35], [39], which was pre-trained on the Vicuna visual question answering (VQA) dataset [40].\nTraining Details. We train on all human and 100 robot demonstrations from MotIF-1K. For each demonstration, we construct one positive and 10 negative samples of image-text pairs. When fine-tuning the VLMs, we freeze the weights of the pre-trained visual encoder, CLIP ViT-L/14 [41] with an input size of 336px by 336px. We fine-tune the projection layer and the language model in the VLM using low-rank adaptation (LORA) with cross-entropy loss for 30 epochs with a learning rate of 5e-5 and a batch size of 32. We use a single A100 GPU for fine-tuning.\nEvaluation Set and Metrics. All VLMs output a binary label indicating whether or not the agent's motion in the image aligns with the given task and motion descriptions. For off-the-shelf models, we convert natural language responses into their corresponding binary labels. We evaluate models on the validation and test split of MotIF-1K containing 129 and 134 robot demonstrations, respectively. The test split contains a set of unseen trajectories which vary in terms of camera viewpoints, motions, tasks, and environment in comparison to the training"}, {"title": "VI. DISCUSSION", "content": "Summary. Task specification in robotics often goes beyond simply stating what the objective is, and additionally consists of how a task should be done. As a step in this direction, we introduce a dataset, MotIF-1K, alongside a unique repre-sentation method and training technique, MotIF. Our findings demonstrate that our system can effectively provide assessments of nuanced robotic actions. The MotIF-1K dataset consisting of human and robot trajectories, captures the diverse ways in which tasks can be executed. We propose MotIF that uses this data to fine-tune open-source VLMs to detect a more nuanced notion of success. Our results demonstrate that MotIF can effectively assess success over these nuanced motions by leveraging a simple visual motion representation; overlaying the robot's trajectory on the image observation.\nMotIF can determine the success of trajectories generated by any policy evaluating if the motions align with the task instruction and motion description. This can provide a signal for when to terminate an episode or to further refine the policy. We include examples of the following uses of MotIF on LLM generated policies [42]: MotIF as a success detector (App. Fig. 10), providing correction feedback to the policy, terminating or adapting robot policies (Fig. 9 (a)), and ranking trajectories (Fig. 9 (b)).\nLimitations and Future Directions. One limitation of MotIF is the dependency on 2D visual motion representations, which might not capture all aspects of complex 3D motions. Future work could incorporate RGB-D and multi-view images, or 3D visual representations overlaid on the image for a more comprehensive understanding of 3D space using our dataset. Another direction is to use MotIF as a reward signal for reinforcement learning. MotIF can act as a motion discriminator when training RL policies that can potentially more accurately reflect user preferences and contextual appropriateness. Future work might create a VLM that outputs natural language responses, which could be used to not only evaluate binary success but also perform reasoning on robotic motions."}, {"title": "APPENDIX", "content": "We provide additional details and analyses of MotIF and MotIF-1K in the appendix. Section A provides details for how we construct and annotate the dataset. Section B provides implementation details of the proposed motion discriminator, MotIF. Section C provides additional experiment results and detailed analyses on the results in the paper. Section D provides the implications of MotIF in real robot planning, with detailed examples."}, {"title": "A. MOTIF-1K DATASET DETAILS", "content": "A.1 Grounded Motion Annotations\nPath Shape. For translation motions, we consider the direction of the movement such as move upward or move upward and to the right. As motions could be forming a curve or part of a shape, we also consider the convexity of the path or compare the path to figures (e.g., circle, triangle, and square) that resemble the shape being traced. For cyclic motions, the direction such as clockwise/counter-clockwise are considered. Many trajectories may be a composition of simpler motions. Side-to-side motions could be described as move to the right and to the left N times or vice versa. When one motion component dominates the overall agent behavior among multiple motion components, we describe the dominating motion first and include the remaining details as modifiers. For instance, a motion can be a combination of downward translation and side-to-side motion, thereby resulting in move downward, making horizontal oscillations.\nGrounded in the Environment. Building on top of VLMs' semantic reasoning capabilities by leveraging their world knowledge, we address how robotic motions can be grounded in the environment. The most basic grounded motion is moving over or avoiding an instance in the environment. As shown in the second leftmost motion in the orange box of Fig. 5, to avoid an obstacle or a hazardous region, the agent can make a detour to the left or right of the corresponding instance. Another example could be getting closer to or farther from an instance, even when the instance is not essentially a target. This could happen when people want legibility or predictability of the agent's motion. The instance could also be a path as shown in the two rightmost motions, such as walkways, drawings on a whiteboard, or spills on a table."}, {"title": "B. IMPLEMENTATION AND EXPERIMENT DETAILS", "content": "B.1 Image Preprocessing\nTo preprocess images, we first crop all images to be square. Next, depending on the visual motion representation, the images are sampled and aggregated into a storyboard, or have trajectories of single or multiple keypoints overlaid.\nMulti-frame Storyboard. For constructing multi-frame storyboards, we investigate N = 2, 4, 9 numbers of keyframes, where the keyframes are sampled from K-means clustering of N clusters based on the ResNet18 embeddings of the images at each timestep of a given visual trajectory.\nOptical flow. To extract optical flow from a robot's motion video, we use the BootsTAPIR [43], [23] algorithm. We run"}, {"title": "C.2 Visualization of MotIF Outputs", "content": "In this section, we visualize four trajectories in the val and test splits of MotIF-1K. For each trajectory, we analyze the output of MotIF given four different motion descriptions.\nApp. Table VII visualizes the 106th trajectory in the val split of MotIF. While motion description 1 is included in the training data, our VLM generalizes to understanding unseen motion descriptions (2, 3, and 4). This may be due to the effect of having diverse human demonstrations on grounded robot motions, since there are similar motion descriptions in human data such as \u201cmove downward, getting closer to the laptop, and then move to the left\".\nApp. Table VIII shows an example of our model understanding a robotic motion from an unseen camera viewpoint. Despite the change of the camera height and angle, our model effectively understands a paraphrased instruction (motion description 2) and an instruction with a small tweak on the core directional component (motion description 4).\nApp. Table IX demonstrates our model understands robotic motions even when the interacting object is changed. While parmesan cheese is the only object for this task in the training data, MotIF does accurately determine ground truth and paraphrased motion descriptions align with the robot's motion.\nApp. Table X shows a trajectory of a robot delivering lemonade to the door, while avoiding moving over the manhole. This example demonstrates the grounded motion understanding capability of our model such as moving over or making a detour to avoid a specific instance (e.g., manhole) in the environment."}, {"title": "C.3 Qualitative Analysis on Off-the-shelf VLMs", "content": "To check the motion understanding capabilities of off-the-shelf VLMs, we perform a toy experiment on asking several questions to the VLMs, given a video or an image of the robot's motion brushing hair (see App. Fig. 11). The results in this section show that off-the-shelf VLMs such as GPT-4V and GPT-40 fail to describe the robot's motion and its semantic grounding in the environment.\nApp. Fig. 12 shows the response of GPT-4V and GPT-40, given the robot's motion video, without any trajectory overlaid, and a simple text prompt 'Describe the robot's motion'. While GPT-4V fails to analyze the video, GPT-40 generates a long detailed response explaining the robot's motion. Specifically, GPT-40 first extracts keyframes with its internal algorithm and creates a storyboard of the keyframes. Then, it detects the trajectory of the robot's end effector. Although the model's approach is logically valid, the model eventually describes the robot's motion as \"The robot's end effector moves in a non-linear path with variations in both the x and y directions, suggesting a complex motion.\", which implies neither a specific shape of the motion or how it is grounded in the scene.\nWe also check if off-the-shelf VLMs can better understand"}, {"title": "D. EXAMPLES OF USING MOTIF", "content": "In this section, we provide a detailed examination of the qualitative results demonstrating the utility of MotIF in evaluating policy-generated trajectories. Specifically, we focus on how MotIF acts as a critical component in assessing whether the motions of a robot are in alignment with the given task instructions and motion descriptions. This capability is essential for determining appropriate moments to terminate episodes or to offer corrective feedback to the policy module responsible for trajectory generation. We illustrate various applications of MotIF, including its role as a success detector, providing correction feedback, terminating or adapting robot policies. Additionally, we discuss how MotIF can be used to rank trajectories, as depicted in App. Fig. 14 and App. Fig. 15. These examples underscore the method's potential in various robotic applications."}, {"title": "D.1 Refining and Terminating Robot Planning", "content": "Policy. Bringing an insight from code-as-policies [42], we use an LLM (ChatGPT) to generate the agent policy as a code. We use the prompts in App. Fig. 16 to generate a function that outputs the sequence of end effector states. We make the functions to be parameterized, so that we can utilize the output of MotIF as a correction feedback to adjust the parameters. In App. Table XI, we show an example of refining a policy to generate a robot's motion corresponding to \"move to the left while making vertical oscillations\"."}, {"title": "D.2 Ranking Trajectories with MotIF", "content": "Another usage of MotIF can be ranking trajectories, as shown in App. Fig. 14 and App. Fig. 15. To rank trajectories, we can use the probability of the output token being \"1\" or the logits for tokens \"1\" and \"0\" as criteria. While these different criteria make a consensus on choosing the best trajectory in both examples (the rightmost trajectories in App. Fig. 14 and App. Fig. 15), the ranking of the other trajectories change depending on the criteria. For instance, in App. Fig. 14, while the rank from the first to the last trajectories is (4, 5, 2, 3, 1) using the probability, it becomes (4,5,3, 2, 1) using the logit for token \"1\". Despite a small difference on the rankings, the two leftmost trajectories always rank the lowest. This is a desirable result since those trajectories only show a simple translation motion, without any oscillations.\nApp. Fig. 15 shows how we can reduce human effort in choosing high-quality demonstrations. In this example, robot motions get higher scores as the shape becomes closer to a perfect circle. Also, the two leftmost trajectories with repeated vertical or horizontal movements are accurately determined as failed trajectories."}]}