{"title": "AN UNDETECTABLE WATERMARK FOR GENERATIVE IMAGE MODELS", "authors": ["Sam Gunn", "Xuandong Zhao", "Dawn Song"], "abstract": "We present the first undetectable watermarking scheme for generative image models. Undetectability ensures that no efficient adversary can distinguish between watermarked and un-watermarked images, even after making many adaptive queries. In particular, an undetectable watermark does not degrade image quality under any efficiently computable metric. Our scheme works by selecting the initial latents of a diffusion model using a pseudorandom error-correcting code (Christ and Gunn, 2024), a strategy which guarantees undetectability and robustness. We experimentally demonstrate that our watermarks are quality-preserving and robust using Stable Diffusion 2.1. Our experiments verify that, in contrast to every prior scheme we tested, our watermark does not degrade image quality. Our experiments also demonstrate robustness: existing watermark removal attacks fail to remove our watermark from images without significantly degrading the quality of the images. Finally, we find that we can robustly encode 512 bits in our watermark, and up to 2500 bits when the images are not subjected to watermark removal attacks. Our code is available at https://github.com/XuandongZhao/PRC-Watermark.", "sections": [{"title": "INTRODUCTION", "content": "As AI-generated content grows increasingly realistic, so does the threat of AI-generated disinformation. AI-generated images have already appeared online in attempts to influence politics (Ryan-Mosley, 2023). Watermarking has the potential to mitigate this issue: If AI providers embed watermarks in generated content, then that content can be flagged using the watermarking key. Recognizing this, governments have begun putting pressure on companies to implement watermarks (Biden, 2023; California State Legislature, 2024; European Union, 2024). However, despite an abundance of available watermarking schemes in the literature, adoption has remained limited (Seetharaman & Barnum, 2023). There are at least a few potential explanations for this.\nFirst, some clients are willing to pay a premium for un-watermarked content. For instance, a student using generative AI for class might find a watermark problematic. Any company implementing a watermark could therefore put itself at a competitive disadvantage.\nSecond, existing watermarking schemes noticeably degrade the quality of generated content. Some schemes guarantee that the distribution of a single response is unchanged, but introduce correlations across generations. While this might be acceptable for small models, it is questionable whether anyone would be willing to use such a watermark for a model that cost over $100 million just to train (Knight, 2023). In other words, given the vast effort put into optimizing models, any observable change in the model's behavior is probably unacceptable.\nUndetectable watermarks. Undetectability, originally defined in the context of watermarking by Christ et al. (2024), addresses both of these issues. For an undetectable watermark, it is computationally infeasible for anyone who doesn't hold the detection key to distinguish generations with the watermark from generations without even if one is allowed to make many adaptive queries."}, {"title": "The PRC watermark.", "content": "In this paper, we introduce the first undetectable\u00b2 watermarking scheme for image generation models. Our scheme works for latent diffusion models (Rombach et al., 2022), with which we generate watermarked images by progressively de-noising initial watermarked noise within the latent space. The key component in our scheme is a pseudorandom error-correcting code, or pseudorandom code (PRC), a cryptographic object introduced by Christ & Gunn (2024). We therefore refer to our scheme as the PRC watermark in this work.\nAt a high level, a PRC allows us to embed a cryptographically pseudorandom pattern that is robustly distributed across the entire latent space, ensuring that the watermark operates at a semantic level. The fact that our watermark is embedded at the semantic level, combined with the PRC's error-correcting properties, makes our watermark highly robust  especially to pixel-level watermark removal attacks (Zhao et al., 2023).\nAdditionally, since the PRC from Christ & Gunn (2024) can be used to encode and decode messages, we can robustly embed large messages within the PRC watermark. While the decoder is somewhat less robust than the detector, the detector can still be effectively used in cases where the decoder fails.\nFinally, the PRC watermark is highly flexible, requiring no additional model training or fine-tuning, and can be seamlessly incorporated into existing diffusion model APIs. It allows the user to independently set the message length and a desired upper bound on the false positive rate (FPR) at the time of watermark key generation. The false positive rate is rigorous, rather than empirical: If the user sets the desired upper bound on the false positive rate to F, then we prove in Theorem 2 that the false positive rate will not exceed F.\nResults. Experiments on quality and detectability are presented in Section 4.2. We emphasize that undetectability theoretically ensures quality preservation, and our scheme is undetectable by the results of Christ & Gunn (2024). Therefore we perform experiments on quality and detectability only to ensure that our scheme is secure enough with our finite choice of parameters.\nWe demonstrate the undetectability of our scheme in three key ways:\n\u2022 We show in Table 1 that the quality, as measured by the FID, CLIP, and Inception Score, are all preserved by the PRC watermark. This is in contrast to every other scheme we tested.\n\u2022 We show in Table 2 that the perceptual variability of responses, as measured by the LPIPS score (Zhang et al., 2019), is preserved under the PRC watermark. This is in contrast to every other comparable scheme we tested.\n\u2022 We show in Figure 2 that an image classifier fails to learn to detect the PRC watermark. The same image classifier quickly learns to detect every other scheme we tested.\nWe demonstrate the robustness of our scheme in Section 4.3. We find that watermark removal attacks fail to remove the PRC watermark without significantly degrading the quality of the image. We test the robustness under ten different types of watermark removal attacks with varying strengths and compare PRC watermark to eight different state-of-the-art watermarking schemes. Among the three watermarking schemes with the lowest impact on image quality,\u00b3 the PRC watermark is the most robust to all attacks."}, {"title": "RELATED WORK", "content": "There is a rich history of digital watermarking techniques, ranging from conventional steganography to modern methods based on generative models. Following the taxonomy in An et al. (2024), watermarking methods are categorized into two types: post-processing and in-processing schemes.\n\u2022 Post-processing schemes embed the watermark after image generation and have been used for decades due to their broad applicability.\n\u2022 In-processing schemes modify the generative model or sampling process to embed the watermark directly in the generated content.\nOur PRC watermark falls under the in-processing category. Note that post-processing watermarks cannot be made undetectable without introducing extra modeling assumptions: One can always distinguish between a fixed image and any modification of it. We refer the reader to surveys (Cox et al., 2008; Wan et al., 2022; An et al., 2024) for more on post-processing methods. Below, we focus on two popular in-processing techniques: Tree-Ring and Gaussian Shading watermarks.\nTree-Ring watermark. Wen et al. (2023) introduced Tree-Ring watermark, the first in-processing watermark that modifies the latent sampling distribution and employs an inverse diffusion process for detection. Our PRC watermark builds on this framework but adopts a different latent distribution. The Tree-Ring watermark works by fixing concentric rings in the Fourier domain of the latent space to be 0. To detect the watermark, one uses DDIM inversion (Song et al., 2021) to estimate the initial latent, and the watermark is considered present if the latent estimate has unusually small values in the watermarked rings. Follow-up works have extended this approach by refining the heuristic latent pattern in the watermarking process (Zhang et al., 2024; Ci et al., 2024). However, under Tree-Ring's strategy, the initial latent significantly deviates from the Gaussian latent distribution, leading to reduced image quality and variability, as shown in Tables 1 and 2. Furthermore, the Tree-Ring watermark is a zero-bit scheme and cannot encode messages. While the Tree-Ring watermark is robust to several attacks, it is highly susceptible to the adversarial surrogate attack since the latent pattern is easy to learn with a neural network. In Figure 5, we find that this attack removes the Tree-Ring watermark with minimal effect on image quality.\nGaussian Shading watermark. The basic Gaussian Shading watermark (Yang et al., 2024) works by choosing a fixed quadrant of latent space as the watermarking key, and only generating images from latents in that quadrant. Detection involves recovering the latent and determining if it lies unusually close to the watermarked quadrant. In their paper, Yang et al. (2024) include a proof that Gaussian Shading has \u201clossless performance.\u201d However, this proof only shows that the distribution of a single watermarked image is the same as that of a single un-watermarked image. Crucially, even standard quality metrics such as the FID (Heusel et al., 2017), CLIP Score (Radford et al., 2021), and Inception Score (Salimans et al., 2016) account for correlations between generated images, so their proof of lossless performance does not guarantee perfect quality under these metrics. Indeed, we find in Table 1 that the Gaussian Shading watermark significantly degrades the FID and Inception Score. We expand on this further by measuring the \"variability\" of watermarked images. Since images under the Gaussian Shading watermark all come from the same quadrant in latent space, we expect that the variability should be reduced. We use the LPIPS perceptual similarity score (Zhang et al., 2018) to measure the diversity among different watermarked images for a fixed prompt. As shown in Table 2, the perceptual similarity between images is significantly higher with the Gaussian Shading watermark, confirming the diminished variability. The diminished variability turns out to be easily observable to the human eye  we show an example of this in Figure 10.\nUndetectability. Undetectable watermarks were initially defined by Christ et al. (2024) in the context of language models. Subsequent to Christ & Gunn (2024), alternative constructions of PRCs have been given by Golowich & Moitra (2024) and Ghentiyala & Guruswami (2024). It would be interesting to see if these PRCs yield improved image watermarks, but we did not investigate this."}, {"title": "METHOD", "content": "3.1 THREAT MODEL\nWe consider a setting where users make queries to a provider, and the provider responds to these queries with images produced by some image generation model. In watermarking, the provider holds a watermarking key that is used to sample from a modified, watermarked distribution over images. Anyone holding the watermarking key can, with high probability, distinguish between samples from the un-watermarked distribution and the watermarked distribution.\nSince the watermark may be undesirable to some users, some of them may attempt to remove the watermark. We are therefore interested in robust watermarks, for which watermark detection still functions even when the image is subjected to a watermark removal attack. We assume that the adversary performing such a removal attack is restricted in two ways. First, the adversary should have weaker capabilities than the provider. If the adversary can generate their own images of equal quality to the provider, then they don't need to engage in watermark removal attacks. Second, we are only interested in adversaries that produce high-quality images after the removal attack. If removal attacks require significantly degrading the quality of the image, then there is incentive to leave the watermark.\nWe are also interested in spoofing attacks, whereby an adversary who doesn't know the watermarking key attempts to add a watermark to an un-watermarked image. We only perform limited experiments on spoofing attacks, so we do not discuss the adversarial capabilities here. However, we note that our techniques, together with the ideas on unforgeable public attribution from Christ & Gunn (2024), immediately yield a scheme that is provably resilient to spoofing attacks.\n3.2 OVERVIEW OF THE PRC WATERMARK\nImage generation and randomness recovery. Before describing our watermarking scheme, let us establish some notation. Let Generate be a randomized algorithm that takes as input (1) a prompt string $\u03c0 \u2208 \u03a3^*$ over some alphabet \u2211 and (2) a standard Gaussian in $R^n$, and produces an output in $R^d$. Our method applies to any such algorithm, but in this work, we are interested in the case that Generate is a generative image model taking prompts in $\u03a3^*$ and initial (random) latents in $R^n$ to images in $R^d$.\nSome of the most popular generative image models today are latent diffusion models (Rombach et al., 2022), which consist of a diffusion model specified by a de-noising neural network $\u03f5$, a (possibly-randomized) function $f_\u03f5$ depending on $\u03f5$, a number of diffusion iterations $T$, and an autoencoder $(E, D)$. For a latent diffusion model, Generate works as follows.\nAlgorithm Generate($\u03c0, z^{(T)})$:\n(1) For $i = T$ down to 1:\n(2) $z^{(i-1)} \u2190 f_\u03f5(\u03c0, z^{(i)}, i)$\n(3) $x \u2190 D(z^{(0)})$\n(4) Output $x$\nIn words, Generate works by starting with a normally distributed latent and iteratively de-noising it. The de-noised latent is then decoded by the autoencoder. In order to produce an image for the prompt using Generate, we use Sample defined as follows."}, {"title": "", "content": "Algorithm Sample($\u03c0$):\n(1) Sample $z^{(T)} \u223c N(0, I_n)$\n(2) Compute $x \u2190 Generate(\u03c0, z^{(T)})$\n(3) Output $x$\nDetection of the watermark will rely on a separate algorithm, Recover, that recovers an approximation of the latent in $R^n$ from a given image $x \u2208 R^d$. For latent diffusion models, the key component in Recover is an inverse diffusion process $\u03b4$ that attempts to invert $\u03f5$ without knowledge of the text prompt $\u03c0$. There is also some (possibly-randomized) function $g_\u03b4$ that determines how $\u03b4$ is used to perform each update.\nAlgorithm Recover(x):\n(1) Compute an initial estimate $z^{(0)} \u2190 E(x)$ of the de-noised latent.\u2075\n(2) For $i = 0$ to $T - 1$:\n(3) $z^{(i+1)} \u2190 g_\u03b4(z^{(i)}, i)$\n(4) Output $z^{(T)}$\nThere has been increasing interest in tracing the diffusion model generative process back (Recover). Diffusion inversion has been important for various applications such as image editing (Hertz et al., 2022) and style transfer (Zhang et al., 2023). A commonly used method for reversing the diffusion process is Denoising Diffusion Implicit Models (DDIM) (Song et al., 2021) inversion, which leverages the formulation of the denoising process in diffusion models as an ordinary differential equation (ODE). However, the result of DDIM inversion, $z^{(T)}$, is an approximation even when the input text is known. For our implementation of Generate, we employ Stable Diffusion with DPM-solvers (Lu et al., 2022) for sampling. In our implementation of Recover, we adopt the exact inversion method proposed in Hong et al. (2023) for more accurate inversion.\nEmbedding and detecting the watermark. Our watermarking scheme works by passing to Generate a vector $\u017e^{(T)}$ which is computationally indistinguishable from a sample from $N(0, I_n)$. To sample $\u017e^{(T)}$, we rely on a cryptographic object called a pseudorandom code (PRC), introduced by Christ & Gunn (2024). A PRC is a keyed error-correction scheme with the property that any polynomial number of encoded messages are jointly indistinguishable from random strings. For watermarking, it suffices to use a zero-bit PRC which only encodes the message '1.' If one wishes to encode long messages in the watermark, we can do this as well; see Appendix C for details on how this is accomplished. For simplicity we focus on the zero-bit case in this section.\nOur PRC consists of four algorithms, given in Appendix B:\n\u2022 PRC.KeyGen($n, F, t$) samples a PRC key $k$, which will also serve as the watermarking key. The parameter $n$ is the block length, which in our case is the dimension of the latent space; $F$ is the desired false positive rate; and $t$ is a parameter which may be increased for improved undetectability at the cost of robustness.\n\u2022 PRC.Encode$_k$ samples a PRC codeword.\n\u2022 PRC.Detect$_k(c)$ tests whether the given string $c$ came from the PRC.\n\u2022 PRC.Decode$_k(c)$ decodes the message from the given string $c$, if it exists. The decoder is slower and less robust than the detector.\nAs our PRC, we use the LDPC construction from Christ & Gunn (2024), modified to handle soft decisions. Essentially, this PRC works by sampling random $t$-sparse parity checks and using noisy solutions to the parity checks as PRC codewords. For appropriate choices of parameters, Christ & Gunn (2024) prove that this distribution is cryptographically pseudorandom. We describe how the PRC works in detail in Appendix B, and we describe our watermarking algorithms in detail in Appendix C."}, {"title": "", "content": "In fact, the algorithm of Hong et al. (2023) further uses gradient descent on $z^{(0)}$ to minimize $||D(z^{(0)})||$, initializing with $z^{(0)} = E(x)$. They call this \u201cdecoder inversion,\u201d and it significantly reduces the recovery error."}, {"title": "", "content": "To set up our robust and undetectable watermark, we simply sample a key $k$ using PRC.KeyGen. To sample a watermarked image, we choose $\u017e^{(T)}$ to be a sample from $N(0, I_n)$ conditioned on having signs chosen according to the PRC and then apply Generate. In more detail, we sample $\u017e^{(T)}$ using the following algorithm.\nAlgorithm PRCWat.Sampler($\u03c0$):\n(1) Sample a PRC codeword $c\u2208 {\u22121,1}^n$ using PRC.Encode($k$)\n(2) Sample $y \u223c N(0, I_n)$\n(3) Let $\u017e^{(T)} \u2208 R^n$ be the vector defined by $\u017e^{(T)}_i = c_i \u00b7 |y_i|$ for all $i \u2208 [n]$\n(4) Compute $x \u2190 Generate(\u03c0, \u017e^{(T)})$\n(5) Output $x$\nFor a full description of the algorithm, see Algorithm 6.\nSince the signs of $z^{(T)} \u223c N(0, I_n)$ are uniformly random, the pseudorandomness of the PRC implies that any polynomial number of samples $\u017e^{(T)}$ in PRCWat.Sample are indistinguishable from samples $z^{(T)} \u223c N(0, I_n)$. As Generate is an efficient algorithm, this yields Theorem 1, which says that our watermarking scheme is undetectable against poly($n$)-time adversaries for latent space of dimension $n$, as long as the underlying PRC is.\nTheorem 1 (Undetectability). Let PRC be any PRC, and let PRCWat.Sample be as defined above. Then for any efficient algorithm $A$ and any $c > 0$,\n$Pr_{k\u223cPRC.KeyGen}[A^{PRCWat.Sample_k}(1^n) = 1] - Pr[A^{Sample}(1^n) = 1] \u2264 \\frac{1}{n^c} + O(n^{-c}).$\nThe notation $A^{O}(1^n)$ means that $A$ is allowed to run in any time that is polynomial in $n$, making an arbitrary number of queries to $O$. For our experiments, we do not strictly adhere to the parameter bounds required for the pseudorandomness proof of Christ & Gunn (2024) to hold; as a result of this and the fact that we use small finite choices of parameters, our scheme should not be used for undetectability-critical applications. See Appendix C.3 for a discussion on this point.\nTo detect the watermark with the watermarking key, we use (roughly) the following algorithm. As long as Recover reproduces a good enough approximation to the latent that was originally used to generate an image, PRCWat.Detect will recognize the watermark.\nAlgorithm PRCWat.Detect$_k(x)$:\n(1) Compute $z^{(T)} \u2190 Recover(x)$\n(2) Let $c$ be the vector of signs of $z^{(T)}$\n(3) Compute result \u2190 PRC.Detect$_k(c)$\n(4) Output result\nFor our actual detector, we use a slightly more complicated algorithm that accounts for the fact that coordinates of $z^{(T)}$ with larger magnitude are more reliable. The complete algorithm is given in Algorithm 7.\nIt turns out that, for low error rates, the PRC from Christ & Gunn (2024) can be used to encode and decode long messages using an algorithm called belief propagation. We can therefore include long messages in our watermark. Our algorithm for decoding the message from an image is PRCWat.Decode, described in Algorithm 8. PRCWat.Decode is slower and less robust than PRCWat.Detect, but we find that it still achieves an interesting level of robustness.\nFinally, our PRC watermark allows the user to set a desired false positive rate, $F$. We prove Theorem 2, which says that our PRC watermark detector has false positive rate at most $F$, in Appendix C.2.\nTheorem 2 (False positive rate). Let $n, t \u2208 N$ and $F > 0$. For any image $x$,\n$Pr_{k\u223cPRCWat.KeyGen(n,F,t)}[PRCWat.Detect_k(x) = True] < F$\nand\n$Pr_{k\u223cPRCWat.KeyGen(n,F,t)}[PRCWat.Decode_k(x) \u2260 None] < F.$"}, {"title": "EXPERIMENTS", "content": "4.1 EXPERIMENT SETUP\nIn our primary experiments, we focus on text-to-image latent diffusion models, utilizing the widely adopted Stable Diffusion framework (Rombach et al., 2022). Specifically, we evaluate the performance of various watermarking schemes using the Stable Diffusion-v2.1 model, a state-of-the-art generative model for high-fidelity image generation. Additionally, we explore applying PRC watermarking to other generative models, as demonstrated with VAE (Kingma & Welling, 2013) models in Appendix D. All images are generated at a resolution of 512x512 with a latent space of 4\u00d764\u00d764. During inference, we apply a classifier-free guidance scale of 3.0 and sample over 50 steps using DPMSolver (Lu et al., 2022). As described in Section 3, we perform diffusion inversion using the exact inversion method from Hong et al. (2023) to obtain the latent variable z(T). In particular, we use 50 inversion steps and an inverse order of 0 to expedite detection, balancing accuracy and computational efficiency. All experiments are conducted on NVIDIA H100 GPUs.\nWatermark baselines. We conduct comparative evaluations against various watermarking schemes, including in-, and post-processing techniques, as defined in Section 2. For post-processing methods, we compare with DwtDct (Al-Haj, 2007), DwtDctSvd (Navas et al., 2008), RivaGAN (Zhang et al., 2019), StegaStamp (Tancik et al., 2020), and SSL Watermark (Fernandez et al., 2022). For in-processing methods, we include a comparison with Stable Signature (Fernandez et al., 2023), Tree-Ring (Wen et al., 2023) and Gaussian Shading (Yang et al., 2024). Most baseline methods are designed to embed multi-bit strings within an image. Specifically, we set 32 bits for DwtDctSvd, RivaGAN, and SSL Watermark; 96 bits for StegaStamp; and 48 bits for Stable Signature. We employ publicly available code for each method, using the default inference and fine-tuning parameters specified in original respective papers for post- and in-processing methods. For Tree-Ring and Gaussian Shading watermarks, we use the same diffusion model and inference parameter settings as those used in PRC. We encode 512 random bits in the PRC watermark. If the decoder is successful, then with high probability, the bits are recovered correctly. Figure 1 illustrates examples of different watermarking schemes applied to a specific text prompt, highlighting the visual impact of each approach."}, {"title": "QUALITY AND DETECTABILITY", "content": "To evaluate the image quality of watermarked images, we compute the Frechet Inception Distance (FID) (Heusel et al., 2017), CLIP Score (Radford et al., 2021), and Inception Score (Salimans et al., 2016) to measure the distance between generated watermarked and un-watermarked images, and between watermarked and real images. For our comparison to real images, we use the MS-COCO-2017 training set; for the comparison to unwatermarked images, we use 8,000 images generated by the un-watermarked diffusion model using prompts from the SDP dataset. We calculate FID and CLIP Scores over five-fold cross-validation and report the mean and standard error. To assess perceptual variability (diversity), we select 10 diverse prompts from the PromptHero website and use different in-processing watermark methods to generate 100 images for each prompt. We calculate perceptual similarity for all image pairs using the LPIPS (Zhang et al., 2019) score, averaging the results over the 10 prompts and reporting the standard error. Higher LPIPS scores indicate better variability for a given prompt. This evaluation is essential since, for image generation tasks, users typically generate multiple images from a single prompt and then select the best one (e.g., Midjourney)."}, {"title": "ROBUSTNESS OF THE DETECTOR", "content": "To comprehensively evaluate the robustness of the PRC watermark and compare it to baseline watermarking methods, we tested nine distinct watermarking techniques against ten different types of attacks. Detailed descriptions of the attack configurations can be found in Appendix A.1.\nThe robustness of the various watermarking methods under these attacks is shown in Figure 5. We evaluated the quality of the attacked images using PSNR, SSIM, and FID metrics, comparing them to the original watermarked images. Notably, the PRC watermark demonstrates high resilience to most attacks. Even under sophisticated attacks, no method successfully reduced the true positive rate (TPR) below 0.99 while keeping the FID score under 70. This demonstrates that current watermark removal techniques struggle to erase our watermark without significantly degrading image quality. For instance, as shown in Figure 5, a JPEG compression attack with a quality factor of 20 only reduced the TPR from 1.0 to 0.94, but the resulting images displayed noticeable blurriness and a loss of detail (see Figure 4). Finally, in Figure 7 we demonstrate increased robustness for $t = 2$. However, for $t = 2$ there exist fast attacks on the undetectability of the PRC watermark, so we do not explore this choice further."}, {"title": "ENCODING LONG MESSAGES IN THE WATERMARK", "content": "The use of a PRC allows us to embed long messages in our watermarks, as described in Appendix C. We find in Figure 11 that the decoder is highly robust for 512-bit messages, although the detector is slightly more robust in this case. We find in Figure 12 that the decoder can reliably recover up to 2500 bits of information if the images are not subjected to removal attacks."}, {"title": "SECURITY OF THE PRC WATERMARK UNDER SPOOFING ATTACKS", "content": "To test the spoofing robustness of different watermarks, we followed the approach in Saberi et al. (2023), aiming to classify non-watermarked images as watermarked (increasing the false positive rate). Spoofing attacks can damage the reputation of generative model developers by falsely attributing watermarks to images. We used a PGD-based (Madry et al., 2018) method similar to that of the surrogate model adversarial attacks, flipping the surrogate model's prediction from un-watermarked to watermarked. Just as with the adversarial surrogate attack, this attack cannot work against any undetectable watermark such as PRC watermark."}, {"title": "POSSIBILITY OF EXTENSION", "content": "The PRC watermark can also be applied to other generative models, particularly those sampling from Gaussian distributions. We have set up a demo experiment working for traditional VAE models, as detailed in Appendix D. We would also be interested to see the PRC watermark applied to emerging generative models such as Flow matching (Lipman et al., 2022); whether or not this is possible hinges only on the existence of a suitable Recover algorithm."}, {"title": "CONCLUSION", "content": "We give a new approach to watermarking for generative image models that incurs no observable shift in the generated image distribution and encodes long messages. We show that these strong guarantees do not preclude strong robustness: Our watermarks achieve robustness that is competitive with state-of-the-art schemes that incur large, observable shifts in the generated image distribution."}, {"title": "THE PSEUDORANDOM CODE", "content": "We use the construction of a PRC from Christ & Gunn (2024), which is secure under the certain-subexponential hardness of LPN. The proof of pseudorandomness, assuming the $2^{w(\\sqrt{n})}$ hardness of LPN, from the technical overview of Christ & Gunn (2024) applies identically here. The PRC works by essentially embedding random parity checks in codewords. The key generation and encoding algorithms are given in Algorithms 1 and 2."}, {"title": "", "content": "Algorithm 1: PRC.KeyGen\nInput: n, message_length, F, t\nOutput: PRC key k\n/* Set parameters\n*/\n1 $\u03bb \u2190 \\lfloor log_2 (t) \\rfloor$;\n2 $\u03b7 \u2190 1 \u2212 2^{\u22121/\u03bb}$;\n3 num_test_bits \u2190 $\\lfloor log_2 (1/F) \\rfloor$;\n4 $k \u2190 message\\_length + \u03bb + num\\_test\\_bits$;\n5 $r \u2190 n \u2212 k \u2212 \u03bb$;\n6 $max\\_bp\\_iter \u2190 \\lfloor log_t n \\rfloor$;\n/* Sample randomness to ensure a low false-positive rate\n*/\n7 Sample uniformly random vectors $otp \u2208 F_2^n$ and $testbits \u2208 (F_2)^{num\\_test\\_bits}$;\n/* Sample generator matrix and parity-check matrix\n*/\n8 Sample a uniformly random matrix $G_0 \u2208 F_2^{(n-r)\u00d7k}$;\n9 for $i \u2208 {1, ..., r}$ do\n10 Sample a random $(t \u2212 1)$-sparse vector $w_i \u2208 F_2^{n-r+i\u22121}$;\n11 $\\begin{bmatrix}G_i\\\\wG_0\\end{bmatrix}$ \u2190 $G_0 \\\\ w G_0$;\n12 $w \u2190 [w, 1,0^{(r-i)}]$; \n13 Let $P$ be the matrix whose rows are $w_1, ..., w_r$, and let $G \u2190 G_r$;\n14 Sample a random permutation $\u03a0 \u2208 F_2^{n\u00d7n}$ and let $P \u2190 PI^{-1}, G \u2190 IG$;\n15 $k \u2190 (n, message\\_length, F, t, \u03bb, \u03b7, num\\_test\\_bits, k, r, max\\_bp\\_iter, otp, testbits, G, P)$; \n16 Output $k$;"}, {"title": "", "content": "Algorithm 2: PRC.Encode\nInput: k, m\nOutput: PRC codeword c\n1 Parse\n$(n, message\\_length, F, t, \u03bb, \u03b7, num\\_test\\_bits, k, r, max\\_bp\\_iter, otp, testbits, G, P) \u2190 k$; \n2 Sample a uniformly random vector $r \u2208 F_2^{n}$; \n3 $y \u2190 (testbits, r, m)$;\n4 Sample $e \u223c Ber(n, \u03b7)$;\n5 $c \u2190 G(y \u2295 e) \u2295 otp$;\n6 Output c;\nSince the work of Christ & Gunn (2024), at least two new constructions of PRCs have been introduced using different assumptions (Golowich & Moitra (2024); Ghentiyala & Guruswami (2024)). It would be interesting to see if any of these new constructions yield image watermarks with improved robustness.\nThe main difference between the PRC used here and the one from the technical overview of Christ & Gunn (2024) is that ours is optimized for our setting by allowing soft decisions on the recovered bits. That is, PRC.Detect takes in not a bit-string but a vector s of values in the interval [-1,1]. If the PRC codeword is c, then $s_i$ should be the expected value $(-1)^{c_i}$ conditioned on the user's observation. We present PRC.Detect in Algorithm 3 and explain how we designed it in Appendix C.1."}, {"title": "", "content": "Algorithm 3: PRC.Detect\nInput: k, s\nOutput: Detection result True or False\n1 Parse\n$(n, message\\_length, F, t, \u03bb, \u03b7, num\\_test\\_bits, k, r, max\\_bp\\_iter, otp, testbits, G, P) \u2190 k$; \n2 For $i \u2208 [n]$, let $s_i \u2190 (\u22121)^{otp_i} \u00b7 (1 \u2212 2\u03b7) \u00b7 s_i$; \n3 For each parity check $w \u2208 P$, let $\\hat{s_w} \u2190 \u03a0_{i\u2208w} s_i$;\n4 $C \u2190 \u03a3_{w\u2208P} log(\\frac{1 + s_w}{2})$; \n5 if\n$\u03a3_{w \u2208 P} log(\\frac{1 + s_w}{2}) \u2265 \u221a C log(\\frac{1}{F}) + \\frac{r}{t} \u03a3_{w\u2208P} log(\\frac{1}{4})$\nthen\n6 Output True;\n7 else\n8 Output False;\nChrist & Gunn (2024) show that any zero-bit PRC (i.e., a PRC with a Detect algorithm but no Decode) can be generically converted to one that encodes information at a linear rate. However, that construction requires increasing the block-length of the PRC, which could harm the practical performance of our watermark. Instead, we use belief propagation with ordered statistics decoding to directly decode the message. Note that belief propagation cannot handle a constant rate of errors if the sparsity is greater than a constant; therefore, this only works when Recover produces an accurate approximation to the initial latent. Still, since our robustness experiments use a small sparsity of t = 3, we find that our decoder functions even when the image is subjected to significant perturbation."}, {"title": "", "content": "Algorithm 4: PRC.Decode\nInput: k", "s\nOutput": "Decoded message m \u2208 {0, 1}k or None\n1 Parse\n$(n, message\\"}]}