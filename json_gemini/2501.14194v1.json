{"title": "ENTER: Event Based Interpretable Reasoning for VideoQA", "authors": ["Hammad Ayyubi", "Junzhang Liu", "Ali Asgarov", "Zaber Hakim", "Najibul Sarker", "Zhecan Wang", "Chia-Wei Tang", "Naveen Reddy Dyava", "Hani Alomari", "Md. Atabuzzaman", "Xudong Lin", "Shih-Fu Chang", "Chris Thomas"], "abstract": "In this paper, we present ENTER, an interpretable Video Question Answering (VideoQA) system based on event graphs. Event graphs convert videos into graphical representations, where video events form the nodes and event-event relationships (temporal/causal/hierarchical) form the edges. This structured representation offers many benefits: 1) Interpretable VideoQA via generated code that parses event-graph; 2) Incorporation of contextual visual information in the reasoning process (code generation) via event graphs; 3) Robust VideoQA via Hierarchical Iterative Update of the event graphs. Existing interpretable VideoQA systems are often top-down, disregarding low-level visual information in the reasoning plan generation, and are brittle. While bottom-up approaches produce responses from visual data, they lack interpretability. Experimental results on NExT-QA, IntentQA, and EgoSchema demonstrate that not only does our method outperform existing top-down approaches while obtaining competitive performance against bottom-up approaches, but more importantly, offers superior interpretability and explainability in the reasoning process.", "sections": [{"title": "1. Introduction", "content": "Video Question Answering (VideoQA) [43, 50] is a challenging task that requires recognizing objects, people, actions, and events, and understanding their relationships' evolution across time. Traditional VideoQA models [35, 39] were trained end-to-end, and while these systems perform strongly, they suffer from a lack of interpretability in their decision process. This lack of transparency undermines trust and reliability and limits their utility when understanding the model's decision-making process is crucial.\nTo address this, modular reasoning approaches [25, 37] break down the reasoning process into interpretable steps,\nmaking the decision-making process more transparent, reliable, and trustworthy. Such properties are vital for applications in high-stakes environments, real-world deployments where human oversight is essential, and improving individual reasoning or perception modules through explicit feedback.\nDespite their promise, current interpretable approaches\nfor VideoQA [25, 37] follow a top-down approach by generating reasoning plans solely from the input question. While their modular structure improves interpretability, it fails to fully utilize the visual modality. Ignoring this visual context during plan generation limits the reasoning system's ability to incorporate contextual clues that are critical for answering complex questions or navigating complex media assets such as long-range videos. Further, these systems are brittle: if the key visual clues are missing from the initial plan, the system is not able to correct the incomplete plan. In contrast, bottom-up approaches [7, 51] directly use visual frames to produce their answer. While some methods [41] select frames based on the context of questions which allows these systems to be contextual and robust, their reasoning process is still opaque since they directly produce the answer from captions using Large Language Models (LLMs).\nTo bridge this gap, we propose ENTER, Event Based Interpretable Reasoning. ENTER combines the interpretability of modular reasoning with the contextual awareness and robustness of bottom-up systems by integrating visual information into the plan generation process. ENTER uses a structured and intermediate representation of videos known as event graphs, as shown in Fig. 1. In these graphs, nodes represent events and edges capture relationships between events, forming a rich structured representation of the events in the video. The event graph, along with the question, is passed to a large language model (LLM) to generate modular, Python-based code that parses the graph. This process allows the system to explicitly model relationships between events and incorporate them into the reasoning process. The generated code is then executed to arrive at the final answer.\nENTER uses a hierarchical approach to enhance graph completeness for answering questions while considering operation cost to maximize efficiency. Initially, ENTER generates a dense graph from available captions, as this is the least costly operation. If the initial graph lacks necessary detail, ENTER enhances the captions and regenerates a denser graph. Only if these steps are insufficient does ENTER introduce multimodal edges, which is a more resource-intensive operation. By doing so, ENTER makes efficient use of computational resources but also enhances the robustness of the graph to mitigate failure cases caused by incomplete graphs.\nENTER's syntactic, interpretable code and event graph provide full transparency into its decision-making process. Event graphs provide ENTER with a powerful representation that captures explicit relationships between events throughout an entire video, enabling it to address questions requiring long-range dependencies. The structured design of these graphs naturally embeds complex semantic relationships, such as causality and temporal order, enabling robust, interpretable reasoning. ENTER combines the clarity and modularity of interpretable models with the contextual awareness of end-to-end systems, offering users a balanced, transparent, and effective reasoning framework.\nWe demonstrate the effectiveness of ENTER through comparable or even better performance than state-of-the-art on the NeXT-QA [43], IntentQA [14], and EgoSchema [24] VideoQA datasets and highlight the benefits of structured event graphs for video reasoning. Our qualitative results highlight the advantages of this representation and illustrate how event-based graphs contribute to more accurate and interpretable reasoning. Finally, our approach allows more focused debugging and easily allows isolation of the root cause of the error from the knowledge representation, the executable reasoning, or the reasoner's use of the extracted knowledge."}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Top-Down vs. Bottom-Up in Video QA", "content": "Existing interpretable VideoQA methods [25, 37, 38, 47, 54] often take a top-down approach generating reasoning steps in response to a specific question. Typically, these steps are structured as programs that use APIs to interact with the video [9, 25, 36, 37] or as chains of thought grounded in visual cues [8]. However, because reasoning steps are generated independently of the visual data, their ability to fully understand video content is constrained. In contrast, bottom-up approaches [2, 7, 11, 15, 19, 23, 39, 41, 51, 53] jointly process both the question and video, allowing for reasoning that integrates richer visual information. For example, methods like LLoVi [51] employ extensive captioning, while VideoAgent [7] iteratively selects keyframes. While these models can incorporate more detailed visual data, they often lack interpretability, typically providing direct answers without explicit reasoning paths. Both top-down and bottom-up approaches face a trade-off between interpretability and the depth of visual information used. To bridge this gap, recent work [4, 7, 20-22, 46] has explored end-to-end methods that aim to integrate visual data while maintaining interpretability. However, these methods still struggle to fully address complex queries, often providing limited reasoning steps or relying on simple visual or commonsense cues."}, {"title": "2.2. Contextual Plan Generation", "content": "Our work is inspired by Neural Module Networks [1], which suggest that complex vision tasks are inherently compositional and should be divided into atomic perceptual units. The recent methods, CodeVQA [36], ViperGPT [37], and VisProg [9] achieve accuracy comparable to end-to-end systems by leveraging GPT-3's [3] and GPT-4's [28] ability to generate code that acts as a plan for answering visual questions through reasoning steps. This approach enables the use of logical operators to manipulate the outputs of visual models, however, they generate the entire reasoning plan in one pass without access to evolving visual context, limiting their adaptability. Similar to VisProg [9], ProgPrompt [34] generates Python-like robot action plans but is more limited in input types. MoReVQA [25] introduces multi-stage plan generation but does not integrate visual context throughout its reasoning process. In contrast, our method can generate and adaptively refine the reasoning plan in response to evolving visual details from complex dynamic scenes in videos."}, {"title": "2.3. Iterative Robustness in VideoQA", "content": "Many existing VideoQA methods [5, 10, 12, 25, 26, 29, 33, 40, 42, 45, 52] focus on question-answering without incorporating a comprehensive error analysis mechanism. This leads to models that are less robust when given complex or ambiguous queries. To solve this, TraveLER [32] iteratively refines its understanding through locating, evaluating, and replanning agents. VidF4 [17] introduces frame-scoring mechanisms for question relevance and inter-frame similarity. VideoInsta [18] applies a self-reflective reasoning approach to enhance information sufficiency, confidence, and temporal balance in VideoQA. VideoTree [41] iteratively refines keyframe selection based on query relevance through a hierarchical, tree-based structure. However, these methods struggle with nuanced temporal and causal relationships in complex videos and have efficiency issues when densely sampling complex scenes. In contrast, our method introduces an iterative hierarchical update mechanism, which systematically refines the event graph by identifying and incorporating missing information through stages of graph densification, dense caption generation, and multimodal information integration. ENTER's self-correction strategy helps ensure a more thorough understanding of the video content when needed which improves the robustness and performance of the VideoQA system."}, {"title": "3. Method", "content": "We propose a modular approach for VideoQA that represents videos with event graphs and generates code to parse the graph interpretably. Starting with a detailed video description, we construct the event graph (Section 3.1), which, along with the input question, directs the generation of a Python-based reasoning plan (Section 3.2). This code is then executed to answer the question or, if details are missing, performs self-correction by iteratively retrieving necessary information (Section 3.3) for improved robustness."}, {"title": "3.1. Event Graph Generation", "content": "To generate event graphs from videos, we first convert the video to captions using Vision Language Model (VLM) and then extract event graphs from captions using LLMs. The two stages leverage the strength of VLM and LLM for each of their respective tasks in the process. In comparison, a single-stage process to convert videos to event graphs suffers from noisy predictions since VLMs were not inherently trained on graph generation tasks."}, {"title": "3.1.1. Caption Generation from Video", "content": "To build our event graph, we generate structured descriptions of video events. Standard captioning methods, like frame-based captioning [52], often lose entity coherence and miss subtle event relationships, limiting their effectiveness for detailed graph construction. Although recent work [31] enhances coreference for people, it lacks depth in capturing complex relationships for generic objects.\nOur approach (shown as \u2460 in Figure 2) emphasizes preserving event relationships, providing detailed descriptions, and maintaining consistent entity coreference. We prompt the model to use relational keywords like \"after,\" \"due to,\" or \"meanwhile\" to capture temporal, causal, and hierarchical links. Entities are described specifically (e.g., \"white Honda Civic\") and consistently referenced (e.g., \"black car with broken window\") throughout. This structured captioning supports event graph generation that links entities and relationships across the video, essential for robust reasoning."}, {"title": "3.1.2. Graph Generation", "content": "To build the event graph (shown as \u2461 in Figure 2), we extract events and their relationships from the caption, where each node represents an event, and directed edges capture intra-event semantic relationships: temporal, causal, and hierarchical. Temporal edges link events occurring sequentially, causal edges connect events with cause-effect relationships, and hierarchical edges denote event containment.\nEach node includes arguments that represent participants or objects involved in the event (e.g., in \"A man driving a car,\" the event \"Drive\" includes \"actor\" as \"man\" and \"vehicle\" as \"car\"). To preserve coreference, we prompt the model for consistent naming of arguments and events across the graph, as discussed in Section 3.1.1. Additionally, the model is prompted to identify all relevant event relationships to produce a comprehensive, cohesive graph that captures the video's full semantic narrative."}, {"title": "3.2. Code Generation and Execution", "content": "To answer questions, we generate Python code that utilizes both the event graph and the textual question. Inspired by ViperGPT's API with image patches [37], we designed an API with Event and EventGraph classes to navigate and query the graph. For example, EventGraph includes a get_child method to retrieve hierarchically related nodes for a given event. The code generator receives an abstract API definition with function signatures and examples. This API and prompt guide the model to produce Python code focused on task-specific logic, executed via Python's interpreter using basic control structures like if/else statements and loops. This abstraction improves efficiency by focusing the model on high-level reasoning.\nTo derive the final answer, we extract the relevant event and arguments from the question, locate the corresponding node in the graph, and traverse its subgraph to collect the necessary details. Unlike other VideoQA methods that may overlook visual features or process video data inefficiently, our approach uses the event graph's compact, text-based representation to integrate visual context efficiently in the code generation process. Code generation and execution correspond to steps \u2462 and \u2464 in Figure 2. In case of multiple choice question, a reasoner (LLM) call is made via select_answer(question, possible_answers, info)."}, {"title": "3.3. Iterative Hierarchical Graph Update", "content": "In cases where the initial event graph lacks the details necessary to answer the question, we perform an Iterative Hierarchical Graph Update process to address information gaps. Missing information can result from limitations in the captioning process, graph generation or mismatches between video and text information (modality gap). Unlike existing modular methods that risk undetected gaps or hallucinated results, our graph-based approach identifies and addresses these gaps through a structured, self-correcting update process. The update mechanism applies a three-stage, layered approach. Starting with low-cost graph consolidation, it progressively adds detail by refining captions and, if needed, incorporating multimodal data. The hierarchical refinement, described below, iteratively refines the event graph with additional layers of information only as needed which improves the system's ability to accurately answer questions, while remaining resource-efficient.\nDenser Graph Generation. If the missing information exists in the captions but is not captured in the graph, we generate a denser graph by reprocessing the captions with a focus on the missing details. We implement the function generate_denser_graph(graph, caption, request), which reinvokes the graph generator using the current graph, captions, and a specific request to include the missing nodes or relations. This stage corresponds to the \u2462 to\u2461 loop in Figure 2.\nDenser Caption Generation. When necessary events or relations are absent from the captions themselves, we generate denser captions by prompting the captioner to include the missing information. The function generate_denser_caption(caption, video, request) reprocesses the video to extract these details. The new information is then integrated into the graph using generate_denser_graph. This stage is represented by the \u2462 to \u2460 loop in Figure 2. We repeat this procedure a fixed number of times or until the relevant events and relations are included before proceeding to the next step.\nMultimodal Information Incorporation. In cases where the denser captions and graph remain insufficient to answer the question, we enhance the graph using multimodal information. Uncertainty in the model's answers can be detected using confidence assessments [48]. Upon detecting uncertainty, we invoke get_new_info(question, request, video), prompting the captioner to revisit the video and retrieve the required information. If gaps remain and are detected by has_sufficient_info, as the last fail-safe, we enhance the graph nodes with relevant video clips to create a multimodal representation. Specifically,\ngenerate_multi_modal_edges (subgraph), iterates through the nodes of the subgraph (Section 3.2) and retrieves node-relevant video clips using a text-to-video retrieval model [27]. The enhanced multimodal subgraph is then passed to a vision-language model to answer the question. This stage corresponds to the \u2463 to \u2462 loop in Figure 2."}, {"title": "4. Experiments", "content": "To validate the effectiveness of our approach, we conduct a series of experiments on the Next-QA [44], IntentQA [13], and EgoSchema [24] datasets. These experiments are designed to evaluate our system's performance on multiple-choice question-answering tasks and provide comparisons against existing methods. In addition to performance metrics, we also focus on the interpretability of our approach. The following sections outline our experimental setup and present the results."}, {"title": "4.1. Experimental Setup", "content": "Our system employs Gemini 1.5 Flash for captioning, graph generation, and code generation, with GPT-4 serving as the reasoner. Accuracy is used as the primary evaluation metric across all the multiple-choice VideoQA datasets.\nNExt-QA[44]: This dataset contains 4996 multiple choice questions, corresponding to 570 videos. The average length of the video clips is 43 seconds. The questions are categorized into three types: Causal (Cau), Temporal (Tem), and Descriptive (Des).\nIntentQA[13]: This dataset specifically focuses on inference-based QA types, including Causal and Temporal questions. It includes two subtypes under Causal\u2014 Causal Why (CW) and Causal How (CH), and two under Temporal\u2014Temporal Previous (TP) and Temporal Next (TN). The dataset contains 2,134 questions corresponding to 567 videos.\nEgoSchema[24]: This dataset has long (180 seconds) egocentric videos with multiple-choice question-answering. It offers two evaluation splits: a hidden test set (Full set) of 5,000 videos evaluated via an external server, and a publicly available validation set (Subset) of 500 videos. We only consider the Full set in our evaluation.\nNEXT-QA and IntentQA are particularly suited for evaluation of our approach as they are event-centric and require strong causal and temporal reasoning. Although EgoSchema focuses on long video understanding rather than event analysis, we include it to demonstrate ENTER's robustness across diverse scenarios.\nBaselines. We evaluate our approach against three categories of baselines: End-to-End, Bottom-Up, and Top-Down/Modular. End-to-End models, such as large vision-language models like Gemini, are typically not interpretable and may be pre-trained on VideoQA datasets. Similarly, Bottom-Up approaches such as VideoTree [41] also lack interpretability. As such, these models are not directly comparable to our method. Our primary comparisons are with Top-Down/Modular methods, which emphasize interpretability. The most direct comparison for our work is against prior modular approaches like MoReVQA [25], which provide transparent reasoning processes similar to our own."}, {"title": "4.2. Experiments Results", "content": "Table 1 summarizes our results. We make the following observations.\nOur approach achieves state-of-the-art performance on NEXT-QA. We improved overall accuracy by 2.8% compared to VideoINSTA [18], the previous best among training-free models. Notably, we surpass all methods on causal and temporal question types, proving the effectiveness of our event graph for event-centric reasoning. Compared to the best modular approach, MoReVQA [25], we achieve a 16.6% improvement on causal questions and a 6.7% improvement on temporal questions. For descriptive questions, we outperform all modular methods and achieve comparable accuracy to the best bottom-up approach, VideoTree [41].\nOur performance on IntentQA is very close to the existing state-of-the-art method. We achieve comparable performance against the best-performing Bottom-Up method. The accuracy of our method slightly lags behind VideoINSTA [18] by 1.3% and LVNet[30] by 0.5%. However, our method offers interpretability, with a small compromise in accuracy. No modular approaches report benchmark scores on the IntentQA dataset, so a direct comparison against those methods was not possible.\nWe outperform all modular approaches on EgoSchema. Our method surpasses all modular approaches on the EgoSchema dataset, with a 1.1% improvement over ProViQ [5] and TraveLER [32]. Although primarily designed for event-centric reasoning, our approach demonstrates stable performance on descriptive questions as well. While bottom-up approaches use richer visual information and thus excel in descriptive tasks, they lack interpretability. In contrast, our approach generates executable code that navigates the video event graph, allowing for transparent reasoning and easy error tracing. This transparency is a key advantage that bottom-up methods cannot provide.\nIn sum, our method surpasses all prior modular approaches on the NExT-QA, IntentQA, and EgoSchema datasets and achieves comparable or superior performance to bottom-up methods on event-centric datasets like NEXT-QA and IntentQA. While bottom-up models perform better on tasks requiring detailed video understanding, such as EgoSchema, our model offers a clear advantage in transparency in its decision-making process\u2014something unachievable with non-interpretable models."}, {"title": "4.3. Ablation", "content": "We conduct ablation studies to demonstrate the effectiveness of our proposed Iterative Hierarchical Graph Update mechanism and compare the behavior of our system vs existing methods under the influence of various reasoner models. Our results show that ensuring the completeness of our event graph is critical to achieving competitive performance with our method, which our dynamic self-correcting mechanism effectively does.\nMultimodal data enhances event graph expressiveness. Incorporating the third stage of our self-correction mechanism enriches subgraph nodes with visual information, compensating for missing information of the corresponding nodes. As shown in Table 2, integrating multimodal data (both visual and textual) boosts performance by 11.3% compared to the baseline and by an additional 3.6% over the second-stage update. This demonstrates the significant impact of combining visual and textual modalities in enhancing the event graph, especially when unimodal nodes lack complete information. Further ablations using a video-only graph and parent-node video clips respectively achieved performance increases of 3.5% and 2.8% over the second-stage mechanism.\nReliable performance across both open-source and proprietary reasoners. To evaluate the robustness of our approach to different reasoning models, we conduct additional ablations on using open-source to proprietary LLMs as the reasoner as shown in Table 3a. The results indicate a 2.5% difference in performance between the open-source Llama 3.1 [6] and the proprietary GPT4. As there is low fluctuation in performance when using LLMs with high differences in caliber, it shows our method remains robust across diverse reasoners. These findings suggest that the final extracted graph is expressive enough across LLM reasoners, pointing to the effectiveness of our proposed mechanism.\nENTER stays consistent, while other models fluctuate across reasoner types. We further ablate against other approaches while keeping an open-source LLM (Llama 3) as the reasoner model across all systems to ensure a fair comparison. As shown in Table 3b, when using the same open-source reasoner, our method overwhelmingly outperforms LloVi [51] and VideoINSTA [18] by 25.9% and 14.2% in NEXT-QA, and 20.1% and 16% in IntentQA respectively. Thus, our model comfortably outperforms other models when open-source LLMs are considered. The table also shows the performance gap (in gray) of each method from its proprietary reasoner-based counterpart. Interestingly, our performance difference between open-source Llama 3 and closed-source GPT4 is only around 2.6% and 2.5% in NExT-QA and IntentQA respectively. Comparing the same for LLoVi [51] and VideoINSTA [18], the score difference is more than 21% and 18% for NExT-QA, and 14% and 19% for IntentQA. Thus existing methods see a huge degradation in performance when switching to open-source reasoners, while our method excels even when access to proprietary models may be limited. This further emphasizes that our method is less dependent on the choice of reasoner compared to existing methods."}, {"title": "5. Qualitative Analysis", "content": "We present two qualitative examples in Figure 3. In the first example (left figure), the task is to count the total number of people in the video. The model iterates through event nodes, identifying human-related arguments, which are then counted by the final LLM reasoner using the simple_query function.\nThe second example (right figure) involves a more complex scenario. The question asks about the dog's action before being petted, which is missed by the captioner. The graph generator linked the man sitting on the chair as the event before petting. As this information dealt with the action of the man rather than the dog, it was insufficient. Consequently, generate_multimodal_edges retrieved the relevant clips. The dog's action, \"Chasing its tail,\" occurring simultaneously with \"Sitting,\" was retrieved, providing the correct answer.\nThe given examples depict the reasoning process employed by our approach, and illustrate its interpretability."}, {"title": "5.1. Error Analysis", "content": "The interpretability offered by our approach enables diagnosis of different types of errors that our model makes and traces the source of the error. From our analysis we categorized the errors into three types primarily.\n\u2022 Error Type: Insufficient Knowledge occurs when the model lacks the necessary knowledge to properly interpret the scene. Figure 4 top example highlights an error caused by the limited knowledge of the captioner. The question, \"How do the two men play the instrument?\" should be answered with \"Roll the handle.\" However, the captioner misidentifies the instrument as \"look like a guitar,\"."}, {"title": "6. Conclusion", "content": "This paper introduced ENTER, a novel interpretable VideoQA system that pioneers the use of event graphs for contextually-aware and transparent video reasoning. ENTER's unique approach of transforming videos into structured event graphs allows it to effectively bridge the gap between the interpretability of modular reasoning and the rich contextual awareness of bottom-up methods. Our experiments on NeXT-QA, IntentQA, and EgoSchema demonstrate that ENTER not only achieves state-of-the-art performance, surpassing prior modular approaches and demonstrating competitive accuracy against leading zero-shot models like Gemini 1.5 Flash, but also offers a significantly more transparent and explainable reasoning process. Moreover, the insights from error analysis provide foundations for future research."}, {"title": "ENTER: Event Based Interpretable Reasoning for VideoQA Supplementary Material", "content": "We provide additional details in this section which further elucidate the claims and contributions of the paper. It's divided into the following sections:\n\u2022 Modular Baselines Performance on IntentQA. (Appendix A)\n\u2022 Ablation on Hierarchical Update Mechanism (Appendix B)\n\u2022 More Qualitative Samples (Appendix C)\n\u2022 More Error Analysis (Appendix D)\n\u2022 Prompts (Appendix E)"}, {"title": "A. Modular Baselines Performance on Inten-tQA", "content": "As the modular baselines didn't have experiments reported on IntentQA, we ran those baselines on IntentQA to further benchmark our performance against them. Specifically, we evaluated ViperGPT and TravLER. The results are presented in Table 5. TravLER and ViperGPT achieved scores of 65.2 and 54.7, respectively, while our method outperformed both with a score of 71.5 with a relative improvement of at least ~10%. This further demonstrates the effectiveness of our approach."}, {"title": "B. Ablation on Hierarchical Update Mechanism", "content": "To evaluate the contribution of each component described in Section 3 to our method's performance, we conducted a detailed analysis of the percentage of data points activating each functionality and their corresponding accuracy. Table 4 presents an ablation study of the three hierarchical update mechanisms, detailing how frequently each component from Section 3 is activated. The \"Base\" component refers to cases where the generated code operates without triggering additional modules like the \"Denser Graph.\" Our results indicate that 63.6% of data points achieve high accuracy without requiring additional visits to the video, demonstrating the baseline capability of our method to address a substantial portion of the dataset effectively. However, when critical information is overlooked during the initial video pass or cannot be adequately represented by unimodal graphs (as revealed in the qualitative error analysis), our method effectively identifies these scenarios and leverages the \"Denser Graph,\" \"Denser Caption,\" or \"Multimodal Graph\" functionalities. It is important to note that the subset activating \"Multimodal Graph\" overlaps with those activating \"Denser Graph\" and \"Denser Caption,\" so the percentages do not sum to 100."}, {"title": "C. Qualitative Analysis", "content": "We further present two more positive examples in Figure 5 to demostrate how the multimodal feature from our model can and event graph provide interpretability\nThe first example (left figure) addresses a question about what caused a girl in a pink dress to move backward. Initially, the captioner and the model was able to construct an event graph and retrieved the event correctly, as shown in the light-orange highlighting. However, while the system identified the \u201cMoving_0\u201d event, it missed the crucial causal information explaining why the girl moved backward, as indicated by the green-highlighted code. To address this gap, the model employed the generate_multimodal_edges function to analyze relevant clips and construct multimodal edges connected to the \u201cMoving_0\u201d event. This analysis revealed the missing causal action: \u201cthe girl dodging the ball,\u201d which led to the correct answer \u201cdodge the ball.\u201d\nThe second example (right figure) demonstrates a different scenario. The question references why the ladies were watching the guy in black; however, neither the event graph nor the generated code contains causal edges or code for retrieving the causal relationship for the event \u201cWatch\u201d, making it insufficient to determine the answer. To resolve this, the model invokes the generate_multimodal_edges function to analyze relevant video clips for additional context. Through this analysis, it successfully identified the missing causal relationships and confirmed the final answer. These examples demonstrate ENTER\u2019s ability to incorporate visual information explicitly to recover from failure cases where information in the text-only event graph proves insufficient.\nFigure 6 illustrates how generate_denser_caption and generate_denser_graph work to mitigate incomplete graph failure cases. The question in the figure asks about an event that occurred after 'speaking' event. However, the initial event graph didn't contain any event with after edge from 'speaking' event. This triggered the generate_denser_caption and generate_denser_graph functions\nto try to incorporate this missing information. In this example, these two function successfully combined to include the missing information in the graph \u2013 the fact that the lady started walking after 'speaking'. This led to the successful prediction of the correct answer."}, {"title": "D. Error Analysis", "content": "To further demonstrate our model\u2019s interpretability capabilities, we analyze more examples of incorrect predictions to trace the source of different types of errors, including Insufficient Knowledge, Missed Information and Inconsistent Referencing."}, {"title": "Insuffcient Knowledge Leads to Incorrect Predictions Despite Correct Reasoning Steps", "content": "The insufficient knowledge error types occurs when the input lacks crucial information needed for accurate scene interpretation. Figure 7 illustrates two examples where the model attempts to identify video locations but fails due to insufficient knowledge.\nIn the top example, although the model successfully generates the correct code to search for location information by traversing nodes in the event graph, it still fails to produce the correct answer due to missing critical information in the graph itself. While the ground truth location is \u201challway\u201d, this information is absent from the captioner's output, resulting in an event graph that lacks any node containing the \u201challway\u201d information. Instead, the event graph erroneously includes \u201cchurch\u201d as a location node. Thus, despite the model's successful retrieval of location-related nodes and correct code execution, it cannot arrive at the correct answer due to this fundamental gap in the input knowledge Similarly, in the second example, the model retrieves location-related nodes including \u201coutdoors\u201d, \u201cbackyard\u201d and \u201cground\u201d. However, none of these nodes correspond to the ground truth location. This again demonstrates how insufficient knowledge provided\nto the model can lead to prediction errors, even when the model's code functions properly."}, {"title": "Missed Information Leads to Incorrect Event Reasoning", "content": "Figure 8 presents two examples where the event graph captures basic events but fails to establish crucial causal relationships, leading to incorrect answers. In the top example, a child plays with toys and turns back at the end of the video to get a cube. While the sequence of events is captured in the graph with a temporal edge from \u201cTurn Around\u201d to \u201cLook back\u201d and a causal edge from \u201cLook back\u201d to \u201cGet\u201d, the event graph misses causal edges connecting the event \u201cTurn Around\u201d to events related to\nthe ground truth answer \u201cGet\u201d. This missing causal relationship makes the model unable to infer the why the baby turned around, resulting in an incorrect prediction. Similarly, in the bottom example, a lion on the left raises its front legs in response to the lion on the right starting to fight. While these basic events are captured in the graph, the model cannot establish causal edges from the event \u201cRaise\u201d to the event \u201cFight\u201d, as the event graph lacks information linking to the true cause, making the model unable to infer the actual reason why the lion raised her front legs.\nInconsistent Referencing of Entities Result in Incorrect Answers Figure 9 shows two examples where errors occur due to inconsistent entity referencing in the event graph. In the top example, the same baby is incorrectly referenced with two different labels: \u201cBaby\u201d and \u201cBaby girl\u201d. This inconsistency leads to the code double counting the number of babies in the video, resulting in an incorrect answer of two instead of one. Similarly, in the bottom example, the event graph generation shows the same referencing error. The video depicts a woman giving a haircut to a boy while another woman holds a camera, but the same child is referenced twice with different labels: \u201cboy\u201d and \u201cYoung boy\u201d. Both examples illustrate how inconsistent entity referencing leads to counting errors in the final analysis."}, {"title": "E. Prompts", "content": ""}, {"title": "Captioner Prompt", "content": "You will be given a video. You need to describe the events in the video. Start your description with the first event happening in the video", "after\" \"then\" \"meanwhile\" etc.; indicate causal relations of events using key words such as \"causing\" etc.; indicate hierarchical relations of events using keywords such as \"by\" \"with\" etc.\nWhen referring to entities, always use descriptive keywords representing their characteristics, such as \"white Honda Civic\", \"man in black suit\", \"chair on the left\", etc. Make sure the references to the entities are consistent, for example, if you mentioned a \"black car with broken window\", refer to it with the same name (\"black car with broken window\") everywhere else you mention it. For entities mentioned in the question, reference them using the same name everywhere else, for example, if the question mentions \"man in black pants\", use \"man in black pants\" to reference that entity.\nYour description will be used to answer the following question": {"Choices": "A. {a0"}, "description": "Two boxers, one in gold-white shorts, and the other one in orange-white shorts, facing each other in a boxing ring; meanwhile, the referee is observing the fight inside the ring; at the same time, a large number of audiences are watching. During the fight, the boxer in gold-white shorts throws a left jab, knocking down the opponent in orange-white shorts. After that, the referee counts to ten while the boxer in gold-white shorts raises his hand in victory; at the"}]}