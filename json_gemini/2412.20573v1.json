{"title": "The intrinsic motivation of reinforcement and imitation learning for sequential tasks", "authors": ["Nguyen Sao Mai"], "abstract": "My work in the field of developmental cognitive robotics aims to devise a new domain bridging between reinforcement learning and imitation learning, with a model of the intrinsic motivation for learning agents to learn with guidance from tutors multiple tasks, including sequential tasks. My main contribution has been to propose a common formulation of intrinsic motivation based on empirical progress for a learning agent to choose automatically its learning curriculum by actively choosing its learning strategy for simple or sequential tasks: which task to learn, between autonomous exploration or imitation learning, between low-level actions or task decomposition, between several tutors. The originality is to design a learner that benefits not only passively from data provided by tutors, but to actively choose when to request tutoring and what and whom to ask. The learner is thus more robust to the quality of the tutoring and learns faster with fewer demonstrations.\nWith my previous advisors, colleagues and my collaborators, we developed the framework of socially guided intrinsic motivation with machine learning algorithms to learn multiple tasks by taking advantage of the generalisability properties of human demonstrations in a passive manner or in an active manner through requests of demonstrations from the best tutor for simple and composing subtasks. The latter relies on a representation of subtask composition proposed for a construction process, which should be refined by representations used for observational processes of analysing human movements and activities of daily living. With the outlook of a language-like communication with the tutor, we investigated the emergence of a symbolic representation of the continuous sensorimotor space and of tasks using intrinsic motivation. These works proposed within the reinforcement learning framework, a reward function for interacting with tutors for multi-task learning. The formulation of this reward unifies interactive learning, multi-task learning and hierarchical learning through the same empirical competence progress measure as the intrinsic motivation to choose the different aspects its learning strategy.\nThe socially guided intrinsic motivation framework could be used for intelligent tutoring systems to be applied in socially assistive robotics for a robot coach for physical rehabilitation of low-back pain patients analysing whole body movements, or for a robot coach for ASD children in an imitation game.\nThe project I wish to develop in cognitive developmental learning pertains both to the reinforcement learning framework of machine learning, to control learning in the perception-action loop of cognitive robotics and the theory of intrinsic motivation from developmental psychology in cognitive science. The objective is to propose new reinforcement learning algorithms for robots to learn by actively interacting with their environment, enabling robots to master control tasks of growing complexity, while extending the current theory of intrinsic motivation towards a unified model of motivations for interacting with tutors. The project relies on two axes: a model of interaction with tutors relying on an emerging representation of tasks by the robot, and a model of motivation of human learners through the use case of a robot coach for physical rehabilitation. This work will enrich the formulation of the motivation for social guidance with various motivational drives of learners to interact with tutors, taking the perspective of communication and emotions. This will also highlight the importance of emerging internal representations for understanding social guidance, thus emphasising the interdependence between autonomous learning and social learning.", "sections": [{"title": "1 APPROACH AND STATE OF THE ART", "content": "My research has been investigating in developmental cognitive robotics [Cangelosi and Schlesinger, 2015; Asada et al., 2009; Lungarella et al., 2003], the motivations of learning agents to interact with teachers, with the outlook of a fully autonomous robot that must continuously learn new tasks and how to perform more complex tasks relying on simpler ones, so as to adapt to the human daily environment and user's needs that change constantly. The paradigm of a permanent learning process is called lifelong learning or continual learning [Ring, 1994].\nMore specifically, learning in an unbounded environment a non-finite number of tasks is referred to as open-ended learning [Doncieux et al., 2018]. Open-ended learning is characterised by multi-task learning, where the set of tasks is not known in advance, including interrelated tasks where easy tasks can be composed into more complex tasks. Thus state, action and task spaces are a priori unbounded in time, space and complexity, thus can be of infinite dimensionality. The challenges of open-ended learning are that (1) the state and action spaces are continuous and high-dimensional, even of infinite dimensionality, and (2) the mappings to learn can be stochastic and redundant.\nIn this chapter, I will describe the developmental learning approach, the challenge and outlook taken by our works, and the relations of our works in active imitation learning with the state of the art."}, {"title": "1.1 DEVELOPMENTAL ROBOTICS", "content": "Uncovering the mechanism babies acquire new skills has been proposed as a key to enable artificial intelligence agents, since Alan Turing suggested :"}, {"title": "1.3 FRAMEWORK", "content": "Let us outline a framework for learning open-ended problems from reinforcement learning and imitation learning."}, {"title": "1.3.1 FORMALISING THE LEARNING PROBLEM", "content": "Let us consider a robot interacting with a non-rewarding environment by performing sequences of motions of unbounded length in order to induce changes in its surroundings.\nEach of these motions is named a primitive action, described by a parametrised function with p parameters: $a \\in A \\subset \\mathbb{R}^p$. We call the continuous multidimensional space A the primitive action space. Our robot can perform sequences of primitive actions.\nThe actions performed by the robot have consequences on its environment, which we call outcomes $\\omega \\in \\Omega$,"}, {"title": "1.3.2 FORMALISING THE IMITATION PROCESS", "content": "Imitation learning can be separated into two broad categories of social learning [Call and Carpenter, 2002], as represented in fig. 1.3.1: the mimicry strategy where the learner copies the observed trajectory $\\phi_1$ with policy parameters $a_4$, and emulation strategy where the learner attempts to replicate goals $\\omega \\in \\Omega$.\nSeveral teachers generate trajectories according to their policies $\\pi_\\kappa$, which are a priori unknown to the learner. A priori, the action space of the teacher is not equal to the action space of the learner. We suppose that $\\pi_\\kappa$ depends on the current state, the goal, but can also be influenced by the query of the learner. In the next sections, we discuss the query process from a learner to a tutor."}, {"title": "1.4 ACTIVE IMITATION LEARNING", "content": "While reinforcement learning and imitation learning have been traditionally opposed  the former being typically a data collection algorithm through exploration of the environment, and the latter typically using a preset dataset of labelled data  we show that both worlds are on the contrary complementary and highlight the merits of merging the two fields. While an increasing stream of machine learning works propose to combine the two paradigms [Price and Boutilier, 1999; Ng and Russell, 2000], and has been named interactive reinforcement learning Thomaz et al. [2005]; Arzate Cruz and Igarashi [2020]. This stream of work includes reinforcement learning from human feedback [Thomaz and Breazeal, 2008; Knox et al., 2013; Christiano et al., 2017] including for large language models [Ouyang et al., 2022]. In most interactive reinforcement learning works, the agent in these works undergoes the interaction passively. We show how a learning agent could optimise its interaction with tutors.\nResearch in developmental psychology indicates that social interaction is not only for social pleasure but can serve for learning and exploration of the environment. While most theories of infant social learning focus on how infants learn whatever and whenever the adults decide to teach them, recent findings suggest"}, {"title": "1.4.1 \u041e\u0420T\u0406MISING WHAT, HOW, WHEN AND WHO TO IMITATE", "content": "In order to define the social interaction that we wish to consider, let us characterise the different possibilities of information flow as reviewed in [Argall et al., 2009; Billard et al., 2007; Schaal et al., 2003; Lopes et al., 2009] with respect to: what, how, when and who to imitate. This categorisation has been introduced in [Dautenhahn and Nehaniv, 2002; Breazeal and Scassellati, 2002]. In this section, we examine the related works studying the closed-loop phenomenon of a learner making queries to a tutor that influence what data are added to its training set. Compared to the review by Zhang et al. [2019] which focuses on the deep reinforcement learning and analyses the state of the art with respect to the types of learning algorithm, we propose to analyse in this section the existing works under the human-robot interaction perspective. As opposed to the more general review in [Ravichandar et al., 2020], we will focus on active choices by the learner on its interaction with teachers, and contrast our works to the state of the art.\nPlease note that the current review does not aim at friendly human-robot interaction, the social rules for a comfortable and natural interaction or goal understanding and intentionality.\nIn this section, we focus on the information from the tutor to the learning agent and its efficiency to convey content by human-robot interaction. Thus we examine what information is given by the tutor.\n\u2022 What to imitate and query? Let us examine the target of the information given by the teacher, or mathematically speaking, the space on which he operates. This can be either the policy, context or outcome spaces, or combinations of them. Merging autonomous exploration with socially guided exploration can optimise the number of data needed for multi-task learning by reusing knowledge across tasks by interpolating for parameterised tasks or using hindsight replay [Nguyen and Oudeyer,"}, {"title": "1.4.2 THE REWARD HYPOTHESIS OF SOCIAL INTERACTION", "content": "Yet, no algorithms have yet proposed a comprehensive framework to optimise all types of social interaction. Most works have focused on addressing what and how to interact. Only a few have addressed when and who to imitate. A few works such as [Nicolescu and Mataric, 2003; Thomaz, 2006] have combined several types of messages such as policy demonstrations, highlight on the context elements, corrective and reward feedback, but they rely on the teacher to optimise the interaction. The learning agent does not actively choose the aspects of this interaction.\nIn active imitation learning, intrinsic motivation based algorithms such as CLIC [Fournier et al., 2019] or SGIM [Nguyen et al., 2021] have proposed a common criteria to optimize what, how, when and who to imitate: the learning progress. These optimisations rely on empirical estimations of the impact of additional data on the learning progress for each source of information, and on the active selection of the information to be queried. The algorithms also optimise the number of data needed for multi-task learning by reusing knowledge across tasks and by querying and combining different types of information, after identifying through learning the relationships between tasks. The learning progress measure can be used to merge autonomous exploration with socially guided exploration, select the best among several teachers, and be robust to imperfect teachers. In particular, choosing aspects of the interaction with the teachers, such as who to imitate and when to imitate can be optimised with, as criteria, the learning progress a teacher can bring to the learner, weighted by the cost of querying a demonstration to the teacher. This criteria can thus be viewed as a reward to interact with teachers.\nWhile reinforcement learning and social interaction seem to live in different frameworks, our review shows that they can be combined. However a common framework is still needed. In reinforcement learning, Sutton and Barto [2018] posited the reward hypothesis as :\n\"all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\" [Sutton and Barto, 2018]\nHowever, this reward hypothesis has only been tested in autonomous learning settings, and has not been extended to settings where the learning agent interacts with teachers, who may be perfect or imperfect. How can a learning agent learn to choose right interaction actions with teachers? Which reward function should shape the learning agent's interactive behaviour ?\nWhile Human-Computer Interaction and Affective computing research postulate that the drive for interaction with others comes from emotions, thus emotional signals can act as an external reward signals, we take the outlook of interaction as an active learning process to gain information, and the others as sources of information, which probability distribution the learner has to model through sampling. Thus, we strive to model the intrinsic motivation of interacting with teachers.\nRecently, some works have proposed a universal reward function for reinforcement learning of social interaction: the learning progress. In [Fournier et al., 2019; Nguyen et al., 2021], the same formulation of the intrinsic motivation is valid both for autonomous exploration and for social guidance, may the demonstrations requested to tutors be low-level policies, goals or decomposition into subgoals. Thus, a common framework is possible by using intrinsic motivation as the common criteria to choose each learning strategy. Our works shows that by considering that each choice of teacher and type of demonstration is a learning strategy, the common criteria to choose a learning strategy and task w can be formulated as $im(\\sigma,\\omega) = \\kappa(\\sigma)*progress(\\sigma,\\omega)$ where progress is the empirical progress measured through the last episodes with strategy $\\sigma$ and goal w, and $\\kappa(\\sigma)$ is the cost of the strategy, representing the availability of teachers, their willingness to interact with the robot\nAt the convergence of active imitation learning and open-ended learning in non-rewarding environments, we describe in the next chapter our multi-task learning algorithms for multiple parametrised tasks and for hierarchical tasks, using intrinsic motivation. In chapter 3, we detail our active imitation learning algorithms. In chapter 4, we describe application projects in socially assistive robotics and assistive technologies. Lastly, chapter 5 sketches future research directions."}, {"title": "2 INTRINSIC MOTIVATION TO LEARN COMPOSITIONAL TASKS", "content": "In this chapter, we describe, for open-ended learning in environments without external rewards, our works to learn with active imitation learning without external rewards from the environment or the teachers, and to solve long-horizon tasks such as compositional tasks, also referred to as sequential tasks. While most works on multi-task learning focus on parametrised tasks or tasks of the same level of complexity, my team examined how to learn compositional tasks, ie tasks can be considered as composed of simpler tasks.\nIndeed, humans, and other animals, cannot only learn simple behaviors and generalise them across various states, but they can also combine simple behaviors to create more complex behaviors. This idea of decomposition and composition of actions into sequences of reusable primitives has been formalised as the motor schemata theory by Arbib [1981]. The theory considers that a set of action primitives might be memorised, to be retrieved and combined by the higher level to generate desired actions. The ability to compositionally combine behaviors is thought to be central to generalized intelligence in humans and a necessary component for artificial intelligent systems. The ability to learn a variety of compositional, long-horizon skills while generalizing to novel concepts remains an open challenge. Long-horizon tasks demand sophisticated exploration strategies and structured reasoning, while generalization requires suitable representations.\nApplying Reinforcement Learning (RL) [Sutton and Barto, 1998] to the complex environments commonly found in robotics is often challenging, due to the high-dimensional and continuous state space and sparse rewards for temporally extended tasks, especially for open-ended learning [Doncieux et al., 2018].\nTo solve open-ended learning, several successive representational redescription processes through"}, {"title": "2.1 MULTI-ARM BANDIT WITH INTRINSIC MOTIVATION", "content": "My team examines how an agent can learn both the policies for simple and sequential tasks and the relationship between tasks for the purpose of transferring knowledge from simpler tasks into sequential tasks and proposes an architectural architecture, Socially Guided Intrinsic Motivation for Sequence of Actions through Hierarchical Tasks (SGIM-SAHT), common to the implementations of algorithms IM-PB, CHIME, SGIM-PB."}, {"title": "2.1.1 FORMALISATION", "content": "We extend the formalisation proposed in Section 1.3.1 for sequential tasks.\nOur robot can perform sequences of primitive actions. Let a compound action be a sequence of any length $n\\in \\mathbb{N}$ primitive actions, and be described by n * p parameters : $a = [a_1,...,a_n] \\in A^n$. Thus the action space exploitable by the robot is a continuous space of infinite dimensionality $A^N \\subset \\mathbb{R}^N$.\nLet us note H the hierarchy of the models used by our robot. His formally defined as a directed graph where each node is a task T and its successors are the components of $\\mathcal{C}_T$. As our robot learns this hierarchy, H varies along time."}, {"title": "2.1.2 ALGORITHMIC ARCHITECTURE", "content": "Socially Guided Intrinsic Motivation for Sequence of Actions through Hierarchical Tasks (SGIM-SAHT), detailed in Algo. 2.1.1 and outlined in Fig. 2.1.2, learns by episodes in which a task T to work on, a goal outcome $\\omega_g \\in \\Omega_T$ and a strategy $\\sigma$ have been selected to optimize progress and according to an interest map (see below). The selected strategy $\\sigma$ applied to the chosen goal outcome $\\omega_g$ chooses a sequence of controllables $l_c = [c_1, ..., c_m]$ as a candidate to reach the $\\omega_g$ (Alg.2.1.1, 1.3)."}, {"title": "2.1.3 LEARN TASK HIERARCHY FROM A STATIC SET OF TASKS", "content": "A first stream of works proposed a representation of task hierarchy as a sequence of parametrised subgoals, in a teleological approach [Csibra, 2003] which considers actions as goal-oriented. As illustrated in Fig. 2.1.4, a task is specified with its parameters and can be decomposed as sequence of two subgoals, which are specified by variables and their parameters. This decomposition can be recursive, providing generalisability to several levels of hierarchy. Learning a task decomposition thus means identifying both subtasks and their parameters.\nLet us consider that the set of tasks is given. The robot needs to choose which are easier to learn first, which are more difficult, and which tasks can be reused as sub-goals for more difficult models: we use knowledge transfer in the context of automatic curriculum learning. To learn the hierarchy H between tasks, we proposed in [Duminy et al., 2018c] an implementation, IM-PB (Intrinsically Motivated Procedure Babbling) based on sequencing and interpolation of subgoals. Learning to draw with a pen and control a game character with joysticks, as illustrated in Fig. 2.1.3, IM-PB uses intrinsic motivation to learn sequential tasks by composing more simple tasks by learning autonomously from a known set of possible decompositions. IM-PB involves an exploration of the possible task decompositions, driven by intrinsic motivation.\nThe results in [Duminy et al., 2018c] report that IM-PB can learn the task decomposition as designed, owing to the task hierarchy representation. IM-PB can then use the learned relationship between tasks to"}, {"title": "2.1.4 SYMBOLIC REPRESENTATIONS OF TASKS", "content": "As I tackle the learning in continuous open-ended environments, the exploration of the continuous high-dimensional sensorimotor space faces the curse of dimensionality. In IM-PB, the curse of dimensionality problem increases with the addition of the space of all possible task decompositions to the exploration. Our experimental setup features a task space that is a composite and continuous space with tasks up to 4 dimensions and the action space is of dimension 14. Our results show that SGIM-PB can handle such complexity to learn hierarchical tasks. However, scaling this algorithm to higher dimensional sensorimotor spaces can prove difficult. Thus, in order to improve the exploration of the task decomposition space, as a means to reduce the size of the task space, I examined whether one can derive a symbolic (ie. discrete) representation of tasks or task decomposition."}, {"title": "2.1.5 LEARN TASK HIERARCHY FROM A DYNAMIC SET OF TASKS : ADAPTIVE LEARNING OF AN AFFORDANCE HIERARCHY THROUGH EXPLORATION", "content": "In the first line of work, I removed an assumption made in the previous studies with SGIM: that the specification of all tasks is known from the beginning by the robot. Thus, the set of all possible task decompositions are known, and mappings only need their parameters to be tuned. In [Manoury et al., 2019b], on the contrary, the learning agent only discovers through its exploration which observables of the environment are controllable and creates a new task to learn. Thus, the robot learns which objects it can control. More concretely, it creates a new neural network to learn the mapping for this task, with the observable as output. To allow quick learning with few data, I considered how to use small size neural networks, the algorithm CHIME selects the appropriate inputs through an evolutionary selection. The criteria for the stochastic selection is the estimated prediction accuracy. Once the input and output of the neural network chosen, the weights are learned by supervised learning with the data collected during intrinsically motivated exploration. Moreover, a task A is added to the set of possible subtasks only when the robot reaches a mastery level for task A above a threshold. Thus, the space of all possible task decompositions grows progressively as the robot masters tasks. For instance, the manipulation of an object of the environment needs to reach a good level of mastery, before this object can be used as a tool to solve more complex tasks. Thus, CHIME implements the notion of tool use and affordance learning [Gibson, 1979] with a mechanism of emergence of tasks and a mechanism for task composition into hierarchically more complex tasks based on the same formulation of intrinsic motivation. While it uses deep neural networks, its originality lies in the proposition of a network which architecture evolves throughout its learning curriculum, enabling it to learn simple tasks quickly. The nested models emerge online, in parallel to the learning by the robot of its automatic curriculum. The results in [Manoury et al., 2019b] show that the robot, owing to the intrinsic motivation heuristic, learns first the easy tasks, before the tasks higher in hierarchy: the reward in controlling its self-position increases first, in controlling the position of a movable object after, and at last pushing an object with another.\nLike SGIM-PB, CHIME implements a temporal abstraction representation of tasks: a task is not defined by its duration but by its goal."}, {"title": "2.2 STAR : GOAL-CONDITIONED HIERARCHICAL LEARNING", "content": "Yet, CHIME still shows limitations for scaling to higher dimensional spaces because each task defines a continuous goal space. This is why in a second line of work, we build upon the framework of hierarchical reinforcement learning (HRL)[Dayan and Hinton, 1992; Sutton et al., 1999], which offers a potential solution for learning long-horizon tasks by training a hierarchy of policies. However, the abstraction used by policies is critical for good performance. Hard-coded abstractions often lack modeling flexibility and are task-specific. Moreover, for long-horizon tasks, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, thus we aimed for ab abstract representation of space. We examined how to discretise online during exploration the continuous goal space using formal methods, in collboration with Sergio Mover, from LIX, Ecole Polytechnique, expert in formal methods. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. In [Zadem et al.,"}, {"title": "2.3 REPRESENTATIONS USED FOR THE ANALYSIS OF HUMAN ACTIVITY OF DAILY LIVING", "content": "In parallel to the previous investigations on representations of sequential tasks for learning agents to build them up, we also examined from the observational point of view, representations used in analysis methods. As the most complex motor activities in terms of compositionality and hierarchical decomposition into subtasks, we inspected our daily tasks: activities of daily living (ADL) include sleeping, eating, cooking, bathing as listed in datasets such as CASAS [Cook et al., 2012] or the ADL definitions provided by Katz [1983]. Indeed, an activity of daily living such as cooking and cleaning can vary greatly from one day to the other depending on the context and goal of the inhabitant. Moreover, ADL are sequential actions of variable length combining several primitive actions with multilevel temporal dependencies. Activities can also be deeply interconnected, underscoring the significance of temporal context in human activity recognition. Each activity can be viewed as a combination of unit actions that are selected and organised for the completion of a temporally distant goal. The activities of daily living share common properties as the sequential actions we considered earlier : the activities can be described as sequences of subtasks as multilevel time dependencies, the subtasks are recruited to complete a goal, and the temporal length and complexity degree of each task is not a-priori bounded. How complex actions such as activities of daily living can be described by hierarchical models ?\nOur investigations first examined the recognition of activities based on video data as part of the project AMUSAAL. We proposed a hierarchical LSTM to classify the human poses in [Devanne et al., 2019], and showed that a hierarchical representation is better suited for activities of daily living."}, {"title": "3 INTRINSIC MOTIVATION TO LEARN WITH TEACHERS", "content": "While in the previous chapter, the learning agent was exploring its environment autonomously, in this chapter, I integrate social guidance in the learning process. I used theories of imitation learning and intrinsic motivation to bridge the paradigms of reinforcement learning and supervised learning, and devise the framework of socially guided intrinsic motivation. I proposed algorithms in the framework of reinforcement learning to enable learning agents to learn multiple parametrised tasks by devising their own learning strategy: they choose actively what do learn, when to learn and thus their own curriculum; and also what, when and whom to imitate.\nFrom the point of view of cognitive science, our work can be seen as a first outline for model of the intrinsic motivation to interact with teachers, and thus a first step towards a model of motivations for social interaction."}, {"title": "3.1 LEARNING A PARAMETRISED TASK", "content": "My PhD research has taken a stance opposite to the current research in intrinsic motivation. While computational models of intrinsic motivation have examined how agents learn by interacting autonomously with its environment to control it without external reward by implementing an intrinsic reward based on its empirical progress, my PhD research has proposed a formulation of the intrinsic motivation of interacting with teachers: what can drive a learning robot to actively interact with teachers? Our proposition is inspired by developmental psychology studies that describe how children can learn by trial and error during their exploration with the environment, and also by requesting their parents' help in an active manner."}, {"title": "3.4 COMPOSITIONAL TASKS WITH ACTIVE IMITATION LEARNING", "content": "With the algorithm SGIM-PB (Socially Guided Intrinsic Motivation with Procedure Babbling), Duminy et al. [2021] extends this intrinsic motivation criteria for requesting from the best teacher either a demonstration of a policy or a task decomposition. It uses the same representation for task decomposition as IM-PB, presented in the previous chapter. The interaction with teachers thus has a supplementary strategy: asking how to decompose a task. SGIM-PB outperforms SGIM-ACTS on the most complex outcome spaces, owing to task decomposition which enables it to learn and exploit the task hierarchy of this experimental setup and previously learned skills."}, {"title": "3.5 SOCIALLY GUIDED INTRINSIC MOTIVATION", "content": "We have presented two algorithms for active imitation learning based on intrinsic motivation. While SGIM-ACTS learns several parametrised tasks by choosing its teachers and the timing of its requests, and SGIM-PB learns uses transfer of knowledge between tasks by learning the relationship between the parametrised tasks with the supplementary strategies of task decomposition by autonomous exploration and task decomposition imitation. The main differences between SGIM-D, SGIM-ACTS and SGIM-PB are outlined and they are contrasted with autonomous learning algorithm IM-PB in Table 3.5.1.\nThe various implementations and experimental setups show that the robot benefits more from exploring directly the policy space for simple tasks, and by trying to decompose into subtasks for sequential tasks. With"}, {"title": "4 APPLICATION TO SOCIALLY ASSISTIVE ROBOTICS", "content": null}, {"title": "4.1 A ROBOT COACH PROPOSING AN AUTOMATIC CURRICULUM", "content": "The algorithms for multi-task learning SGIM, CHIME and GARA automatically design their own learning curriculum, by deciding at each step which task to learn and the appropriate learning strategy. Such a learning curriculum can be used by intelligent tutoring systems for a robot coach to propose exercises personalised to each user. In [Annabi and Nguyen, 2023a] we study an intelligent tutoring system based on intrinsic motivation that chooses exercises to maximise the progress for each student. The efficiency of this intelligent tutoring system relies on the knowledge about the relationship between the exercises, and the relationship between the skills. As with our study on hierarchically sequential tasks, some exercises train high-level skills (for instance multiplication in maths) that require other simpler skills to be mastered before (for instance addition in maths is a prerequisite before learning multiplication). We propose an algorithm to uncover these pre-requisite relationships using causality analysis, based only data of student scores. We therefore obtain a hierarchical relationship between exercises, but this time with respect to pre-requisite requirements.\nThese intelligent tutoring systems can be applied to socially assistive robots to personalise training programs for robot coaches."}, {"title": "4.2 ROBOT COACH FOR PHYSICAL REHABILITATION", "content": null}, {"title": "4.2.1 HUMANOID SOCIALLY ASSISTIVE ROBOT IN A CLINICAL TRIAL", "content": "As part of the Keraal experiment, funded by EU FP-7 ECHORD++, for which I was the scientific coordinator, we proposed a humanoid robot coach for physical rehabilitation. The consortium includes Olivier Remy Neris, director of the rehabilitation department of CHRU Brest; the company Generation Robots; Mathieu Simonnet, specilaist in cognitive ergonomy at LEGO of IMT Atlantique, and Myriam Le Goff-Pronost, health economist at LaTIM, IMT Atlantique. Its goal is to entice motivation in patients while the patients exercises by themselves. This humanoid robot can entice motivation by its embodied presence and by the feedback it can give. In [Devanne and Nguyen, 2017], we applied Gaussian Mixture Model (GMM) on Riemannian manifolds [Zeestraten et al., 2017] for giving feedback after analysing patients' movements for three low-back pain exercises using a single RGB-D camera. In later works, we tested LSTM auto-encoder [Nguyen et al., 2024] and Spatio-Temporal Graph Convolutional Networks [Marusic et al., 2023a] to analyse the whole body movements.\nIn order to design our system and our HRI, we first choose an anthropomorphic robot platform, and led a psychological analysis of the target population in a co-design process [Devanne et al., 2018], in collaboration with Gilles Kermarrec, psychologist at European Research Center for Virtual Reality and Research Center for Education, Learning and Didactics, European University of Brittany. Then, this system has been tested with 21 subjects, including 12 patients in clinical trials in a longitudinal study over daily sessions over three weeks for each patient. The system is assessed with medical criteria that there is non-inferiority of this system compared to classical care with therapists [Blanchard et al., 2022]."}, {"title": "4.2.2 FOUR CHALLENGES FOR A DATASET", "content": "This clinical trial enables us to identity four challenges for robot coaches and publish a medical low-back pain physical rehabilitation dataset for human body movement analysis [Nguyen et al., 2024]. With the aim of rehabilitation using human body movement analysis, intelligent tutoring systems (ITS) need to analyse complex full-body exercises that can involve several parts of the body but not necessarily all parts of the body. The assessment algorithm should be able to understand which parts are important, and what are the ranges of freedom that are acceptable. The ITS should encapsulate the tolerated variance for each joint and time frame. Thus, we have identified 4 challenges in rehabilitation movements analysis:\n1. Rehabilitation motion assessment. The goal is to assess an observed motion sequence by detecting if the rehabilitation exercise is correctly performed or not. Thus each recording is labeled with a global evaluation: correct or incorrect, and also if the error is significant, medium or small.\n2. Rehabilitation error recognition. The goal is to classify the observed error among a set of known errors, so as to explain and give feedback. Each recording has the label of the error and also if the error is significant, medium or small.\n3. Spatial localization of the error. In addition to recognizing the error, the goal is also to identify which body part is responsible of the error to draw the patient's attention to it. Each annotation indicates the body part causing the error.\n4. Temporal localization of the error. The goal is to detect the temporal segment where the detected error occurred along the sequence. Each annotation indicates the time window where the error occurs.\nWhile most rehabilitation datasets [Leightley et al., 2015; Ar and Akgul, 2014; Vakanski et al., 2018; Dolatabadi et al., 2017; Miron et al., 2021; Aung et al., 2016; Capecci et al., 2019] only provide annotations for the rehabilitation motion assessment, we propose a medical dataset recruiting 12 clinical patients carrying out low back-pain rehabilitation exercises over 3 weeks of rehabilitation program. The Keraal dataset is composed of 2622 recordings, including 1881 recordings from patients. Each recording is composed of 3D Kinect skeleton positions and orientations, RGB videos, 2D skeleton data, and medical annotations by two medical annotators to assess the correctness, label and timing errors of each movement part."}, {"title": "4.2.3 PERSONALISATION OF THE COACHING TO DIFFERENT MORPHOLOGIES", "content": "Our current work seeks to personalise this coaching, by taking into account the physical limitations of each person. We proposed to use shared latent variables to translate the movement representation between the robot and the human motions from patients of different morphologies, representing patients who have different biomechanical constraints because of their pathologies or physical differences. In [Devanne and Nguyen, 2019], the common representation was obtained using a shared Gaussian Process Latent Variable Model (shared GP-LVM). Simulating mechanical contraints of joint angles, we could show that the robot could extend the model to these contraints and adapt the assessment of rehabilitation exercises to each patient's physical limitation, by updating the inverse mapping matrix W, as shown in Fig. 4.2.2.\nMore recently, we started investigating how we can learn this common latent space when we do not have recordings of all exercises for the all biomechanical models. Indeed, in the absence of paired data, supervised"}, {"title": "4.3 IMITATION GAME FOR ASD CHILDREN", "content": "In collaboration with the psychiatrist Nathalie Collot-Lavenne from of CHRU Brest, we adapted a program of their daily care for autism spectrum disorder (ASD) children to build a robot coach for imitation games, building on the same movement analysis algorithms.\nIndeed, while autism is an neurodevelopmental condition affecting social interactions and motor capacities, it has been shown that robots and ICT technologies can induce social and cognitive stimultation of children with ASD. This seems to be partly because they are more comfortable with explicit and predictive behaviors. Several studies have shown that imitating ASD children can be efficient in enhancing their social behaviour [Nadel, 2005", "2010": "."}]}