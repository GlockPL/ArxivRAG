{"title": "EMERGENCE OF GLOBALLY ATTRACTING FIXED POINTS IN DEEP NEURAL NETWORKS WITH NONLINEAR ACTIVATIONS", "authors": ["Amir Joudaki", "Thomas Hofmann"], "abstract": "Understanding how neural networks transform input data across layers is fundamental to unraveling their learning and generalization capabilities. Although prior work has used insights from kernel methods to study neural networks, a global analysis of how the similarity between hidden representations evolves across layers remains underexplored. In this paper, we introduce a theoretical framework for the evolution of the kernel sequence, which measures the similarity between the hidden representation for two different inputs. Operating under the mean-field regime, we show that the kernel sequence evolves deterministically via a kernel map, which only depends on the activation function. By expanding activation using Hermite polynomials and using their algebraic properties, we derive an explicit form for kernel map and fully characterize its fixed points. Our analysis reveals that for nonlinear activations, the kernel sequence converges globally to a unique fixed point, which can correspond to orthogonal or similar representations depending on the activation and network architecture. We further extend our results to networks with residual connections and normalization layers, demonstrating similar convergence behaviors. This work provides new insights into the implicit biases of deep neural networks and how architectural choices influence the evolution of representations across layers.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks have revolutionized various fields, from computer vision to natural language processing, due to their remarkable ability to learn complex patterns from data. Understanding the internal mechanisms that govern their learning and generalization capabilities remains a fundamental challenge.\nOne approach to studying these transformations is through the lens of kernel methods. Kernel methods have a long history in machine learning for analyzing relationships between data points in high-dimensional spaces [Sch\u00f6lkopf and Smola, 2002, Smola and Sch\u00f6lkopf, 2004]. They provide a framework for understanding the similarity measures that underpin many learning algorithms. Recent theoretical studies have increasingly focused on analyzing neural networks from the perspective of kernels. The Neural Tangent Kernel (NTK) [Jacot et al., 2018] is a seminal work that provided a way to analyze the training dynamics of infinitely wide neural networks using kernel methods. This perspective has been further explored in various contexts, leading to significant advances in our understanding of neural networks [Lee et al., 2019, Arora et al., 2019, Yang, 2019].\nDespite these advances, an important question remains unexplored: How does the similarity between hidden layer representations evolve across layers, and how is that affected by particular choices of nonlinear functions? Previous work has mainly focused on local behaviors or specific initialization conditions [Saxe et al., 2013, Schoenholz et al., 2017, Pennington et al., 2017]. A comprehensive global analysis of neural kernel sequence fixed points and convergence properties, particularly in the presence of nonlinear activations, is still incomplete."}, {"title": "2 Related works", "content": "The study of deep neural networks through the lens of kernel methods and mean-field theory has garnered significant interest in recent years. The Neural Tangent Kernel (NTK) introduced by Jacot et al. [2018] provided a framework to analyze the training dynamics of infinitely wide neural networks using kernel methods. This perspective was further expanded by Lee et al. [2019] and Arora et al. [2019], who explored the connections between neural networks and Gaussian processes.\nThe propagation of signals in deep networks has been studied in the works of Schoenholz et al. [2017] and Pennington et al. [2017]. Previous studies have also explored the critical role of activation functions in maintaining signal propagation and stable gradient behavior in deep networks [Hayou et al., 2019]. These studies focused on understanding the conditions required for the stable propagation of information and the avoidance of signal amplification or attenuation. However, these analyses often concentrated on local behaviors or specific conditions, leaving a gap in understanding the global evolution of representations across layers.\nHermite polynomials have been used in probability theory and statistics, particularly in the context of Gaussian processes [Williams and Rasmussen, 2006]. Although Poole et al. [2016] and Daniely et al. [2016] have utilized polynomial expansions to analyze neural networks, they do not study global dynamics in neural networks, as presented in this paper. To the best of our knowledge, the only existing work that uses Hermite polynomials in the mean-field regime to study the global dynamics of the kernel is by Joudaki et al. [2023b]. However, this study only covers centered activations, which fail to capture several striking global dynamics covered in this study.\nOur work extends these foundational studies by providing an explicit algebraic framework to analyze the global convergence of kernel sequences in deep networks with nonlinear activations. Using Hermite polynomial expansions, we offer a precise characterization of the kernel map and its fixed points, contributing new insights into the implicit biases of nonlinear activations."}, {"title": "3 Preliminaries", "content": "This section introduces the fundamental concepts, notations, and definitions used throughout this paper. If X and Y are vectors in R\u201d, we use the inner product notation $\\langle X, Y \\rangle_{avg}$ to denote their average inner product = $\\frac{1}{d}\\sum_{i=1}^d X_i Y_i$. We consider a feedforward neural network with L layers and constant layer width d. The network takes an input vector x \u2208 Rd and maps it to an output vector $h^L(x) \u2208 R^d$ through a series of transformations. The hidden representations in each layer l are denoted by $h^l(x)$. The transformation in each layer is composed of a linear transformation followed by a nonlinear activation function \u03c6. We consider the multilayer perceptron (MLP), the hidden representation at layer l is"}, {"title": "4 The mean-field regime", "content": "In this section, we conduct a mean-field analysis of MLP to explore the neural kernel's fixed point behavior as the network depth increases. This approach allows us to gain insight into the global dynamics of neural networks, mainly how the similarity between two input samples evolves as they pass through successive network layers. Now, we can state the mean-field regime for the kernel sequence, stating that in this regime, the sequence becomes deterministic.\nAs will be proven later, this sequence's deterministic transition follows a scalar function defined below.\nDefinition 1. Given two random variables X, Y with covariance p, and activation \u03c6, define the kernel map\u043aas the mapping between the covariance of preactivation and the covariance of post-activations:\n$\\kappa(\\rho) := \\mathbb{E}\\left[\\phi(X)\\phi(Y)\\right], \\quad \\begin{pmatrix} X \\\\ Y \\end{pmatrix} \\sim \\mathcal{N}\\left(0, \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\right)$"}, {"title": "5 Hermite expansion of activation functions", "content": "Hermite polynomials possess completeness and orthogonality under the Gaussian measure. Therefore, any function that is square-integrable with respect to a Gaussian measure can be expressed as a linear combination of Hermite polynomials (see below). The square integrability rules out the possibility of having heavy-tailed postactivations without second moments. This holds for all activations that are used in practice. We use normalized Hermite polynomials and their coefficients.\nDefinition 2. Normalized Hermite polynomials her(x) are defined as follows\n$he_k(x) := \\frac{(-1)^k}{\\sqrt{k!}} e^{\\frac{x^2}{2}} \\frac{d^k}{dx^k} e^{-\\frac{x^2}{2}}$\nAlthough Hermite polynomials have been used in probability theory and statistics, particularly in the context of Gaussian processes [Williams and Rasmussen, 2006], their application in analyzing neural network dynamics provides a novel methodological tool. Previous works, such as Daniely et al. [2016], have utilized polynomial expansions to study neural networks, but our explicit use of Hermite polynomial expansions to derive the kernel map and analyze convergence is a new contribution.\nThe crucial property of Hermite polynomials is their orthogonality:\n$\\mathbb{E}he_k (X)he_l (X) = \\delta_{kl}, \\quad \\quad X \\sim \\mathcal{N}(0, 1)$ \nScaling by 1/$\\sqrt{k!}$ ensures that polynomials form a orthonormal basis. Based on this property, we can define:\nDefinition 3. Given a function \u03c6, square-integrable with respect to the Gaussian measure. Its Hermite expansion is given by:\n$\\phi = \\sum_{k=0}^\\infty c_k he_k, \\quad \\quad c_k \\mathbb{E}[\\phi(X)he_k(X)], \\quad \\quad X \\sim \\mathcal{N}(0, 1)$"}, {"title": "6 Convergence of the kernel with general activations", "content": "In this section, we will show that for any nonlinear activation function, there is a unique fixed point p* that is globally attractive.\nTheorem 1. Given a nonlinear activation function \u03c6 with kernel \u043a such that \u043a(1) = 1. Define the kernel sequence $P_{l+1} = \\kappa(p_l)$, with $p_0 \\in (-1,1)$. Then there is a unique globally contracting fixed point p*, which is necessarily non-negative $p* \\in [0, 1]$. The only other fixed points distinct from p*, could be \u00b11, neither of which is stable. Furthermore, we have the following contraction rate towards p*:\n1.  If \u03ba(0) = 0 then p* = 0 is an attracting with rate\n$\\frac{|p_{l+1}|}{1-|p_{l+1}|} < \\frac{|p_0|}{1-|p_0|}\\alpha, \\qquad \\alpha := \\frac{1}{2 - \\kappa'(0)}$\n2. If \u03ba(0) > 0 and \u043a\u2032(1) < 1, then p* = 1 is attracting, with rate\n$|p_l - 1| \\le |p_0 - 1/\\alpha, \\qquad \\alpha := \\kappa'(1)$.\n3. If \u03ba(0) > 0, and \u043a\u2032(1) = 1 then p* = 1 is attracting with rate\n$\\frac{|p_l-1|}{|1 + p_l|} \\le \\frac{|p_0 - 1|}{|1 + p_0|}$\\alpha, \\qquad \\alpha = 1 - \\kappa(0) - \\kappa'(0)$.\n4. If \u03ba(0) > 0, and \u043a\u2032(1) > 1 then the attracting fixed point is necessarily in the range p* \u2208 (0,1), satisfying \u03ba' (\u03c1*) < 1, for which we have\n$\\frac{|p_l - p^*|}{|1 - p_l|} \\le \\frac{|p_0 - p^*|}{|1 - p_0|}\\alpha, \\qquad \\alpha = max\\{\\frac{1 - \\kappa(0)}{2}, \\kappa'(\\rho^*), \\frac{1 - \\rho^*}{2 - \\kappa'(\\rho^*)}\\}$,\nwhere a < 1.\nImplications. Let us take a step back and review the main takeaway of Theorem 1. Omitting the constants and for sufficiently large depth l, we have\n$\\langle h^l(x), h^l(y) \\rangle_{avg} = \\begin{cases} 0+O(\\alpha^l \\langle x, y \\rangle_{avg}) \\quad case 1 \\\\ 1+O(\\alpha^l \\langle x, y \\rangle_{avg}) \\quad case 2 \\\\ 1 + O (l^{-1}\\langle x, y \\rangle_{avg}) \\quad case 3 \\\\ \\rho^* + O(\\alpha^l \\langle x, y \\rangle_{avg}) \\quad case 4, \\end{cases}$\nwhere $\\langle x, y \\rangle_{avg}$ denotes the input similarly.\nBroadly speaking, we can think of three categories of bias:\n*   Orthogonality bias p* = 0: Implies that as the network becomes deeper, representations are pushed towards orthogonality exponentially fast. We can think of this case as a bias towards independence.\n*   Weak similarity bias p* \u2208 (0, 1): Implies that as the network becomes deeper, the representations form angles between 0 and \u03c0/2. Thus, in this case, the representations are neither completely aligned nor completely independent.\n*   Strong similarity p* = 1: Implies that as the network becomes deeper, representations become more similar or aligned, as indicated by inner products converging to one, exponentially fast in case 2, polynomially in case 3.\nSee section B for a review of commonly used activations, and their biases according to Theorem 1.\nThe bias of activation and normalization layers has been extensively studied in the literature. For example, Daneshmand et al. [2021] show that batch normalization with linear activations makes representations more orthogonal, relying on a technical assumption that is left unproven (see assumption A1). Similarly, Joudaki et al. [2023a] extend this to odd activations yet introduce another technical assumption about the ergodicity of the chain, which is hard to verify or prove (see Assumption 1). Yang et al. [2019] prove the global convergence of the Gram matrix of a network with linear activation and batch normalization (see Corollary F.3) toward a fixed point. However, the authors explain that because they cannot establish such a global convergence for general activations (see page 20, under the paragraph titled"}, {"title": "6.1 A continuous time differential equation view to the kernel dynamics", "content": "One of the most helpful ways to find insights into discrete fixed point iterations is to cast them as a continuous problem. More specifically, consider our fixed point iteration:\n$\\Delta p_l = p_{l+1} - p_l = -p_l + \\sum_{k=0}^\\infty c_k p_l^k$.\nWe can replace this discrete iteration with a continuous time differential equation, which we will refer to as the kernel ODE:\n$dp/dt = -p+\\sum_{k=0}^\\infty c_k p^k.$\n(kernel ODE)\nRecent studies have introduced stochastic models such as the Neural Covariance SDE by Li et al. [2022], which can be viewed as the stochastic analog of kernel ODE. However, kernel ODE will capture the most important parts of the discrete kernel dynamics for our main purpose of characterizing the attracting fixed points. The continuous analog of fixed points is a p* that satisfies p'(p*) = 0. Furthermore, the fixed point is globally attracting if we have p' become negative for p > p* and positive for p < p*. We can say that the fixed point is locally attractive if this property only holds in some small neighborhood of p*.\nWhile the kernel ODE presents an interesting transformation of the problem, it does not necessarily have a closed-form solution in its general form. However, our main strategy is to find worst-case (slowest) scenarios for convergence that"}, {"title": "6.2 Kernel ODE for centered activations", "content": "For the case of centered activation $\\mathbb{E} \\phi(X) = 0$, corresponding to case 1 of Theorem 1, the kernel ODE is given by\n$dp/dt = -(1 - c_1^2)p + \\sum_{k=2}^\\infty c_k p^k$,\nwhere the first term k = 0 is canceled due to the assumption \u043a(0) = $c_0^2$ = 1.\nObserve that p' < 0 when p > 0, and p' > 0 when p < 0. This implies that p(t) \u2192 0 for sufficiently large t. Intuitively, the terms in p' that have the opposite sign of p, contribute to a faster convergence, while terms with a similar sign contribute to a slower convergence. Thus, we can ask what distribution of Hermite coefficients {ck} corresponds to the worst case, slowest convergence. If this is a tractable ODE, we can use its solution to bound the convergence of the original ODE. It turns out that the worst case depends on the positivity of p, which is why we study them separately.\nPositive range. Let us first consider the dynamics when p \u2265 0. In this case, the term corresponding to k = 1 contributes positively to a faster convergence, while terms k \u2265 2 make convergence slower. In light of this observation, a worst-case (slowest possible) convergence rate happens when the positive terms are maximized, which occurs when the weight of all $c_k p^k $is concentrated on the k = 2 term, leading to kernel ODE\n$dp/dt = -(1 - c_1^2)p + (1 - c_1^2)p^2$,\nwhere we used the fact that $\\sum_{k=1}^\\infty c_k^2 = 1$. Finally, we can solve this ODE\n$\\int_{p^0}^{p(t)} \\frac{dp}{|p(t)| - p(t)^2} = -\\int^t (1-c_1^2)dt$\n$\\Rightarrow \\frac{|p(t)|}{|1 - p(t)|} = C exp(-t(1 - c_1^2))$,\nwhere C corresponds to the initial values.\nNegative range. Now, let us assume p < 0. In this case, only the odd terms in $\\sum_{k=2}^\\infty c_k p^k$ contribute to a slowdown in convergence. Thus, the worst case occurs when all the weight of coefficients is concentrated in k = 3, leading to the kernel ODE:\n$dp/dt = -(1 - c_1^2)p + (1 - c_1^2)p^3$\n$\\frac{|p(t)|}{\\sqrt{1 - p^2(t)}} = C' exp(-t(1 - c_1^2))$,\nwhere C' corresponds to the initial values.\nTo summarize, we have obtained:\n$\\begin{cases} \\frac{|p(t)|}{1-p(t)} < C exp(-t(1 - c_1^2)) \\quad p_0 \\in \\mathbb{R}^+ \\\\ \\frac{|p(t)|}{\\sqrt{1 - p^2(t)}} < C' exp(-t(1 - c_1^2)) \\quad p_0 \\in \\mathbb{R}^-. \\end{cases}$\nNow, we can use a numerical inequality $\\sqrt{1 - x^2} > 1 - |x|$ valid for all $x \\in (-1, 1)$, and by solving for the constant, we can construct a joint bound for both cases:\n$\\frac{|p(t)|}{1 - |p(t)|} \\le \\frac{|p_0|}{1 - |p_0|} exp(-t(1 - \\kappa'(0)))$,\nwhere we have also replaced $c_1^2 = \\kappa'(1)$.\nKey insights. The kernel ODE perspective reveals two important insights. First, the appearance of |p|/(1 - |p|) formula in our bound is due to the fact that x/(1 - x) = exp(-ct) is a fundamental solution of the differential equation x' = -cx(1-x). Second, it reveals the importance of the positivity of coefficients in $\\sum_{k=0}^\\infty c_k p^k$, which allowed us to"}, {"title": "7 Extension to normalization layers and residual connections", "content": "In this section, we extend our analysis to MLPs that incorporate normalization layers and residual (skip) connections and examine how they affect convergence."}, {"title": "7.1 Residual connections", "content": "Residual connections, inspired by ResNets [He et al., 2016], help mitigate the vanishing gradient problem in deep networks by allowing gradients to flow directly through skip connections. We consider the MLP with residual strength \u03c4 given by\n$h^l = \\sqrt{1 - \\tau^2} \\phi \\bigg(\\frac{W_l}{\\sqrt{d}} h^{l-1} \\bigg) +\\tau \\frac{P_l}{\\sqrt{d}} h^{l-1}$,\nwhere $W^l$ and $P^l$ are independent weight matrices of dimension $d \\times d$ with entries drawn i.i.d. from a zero-mean, unit-variance distribution, and $\\tau \\in (0, 1)$ modulates the strength of the residual connections (the bigger \u03c4, the stronger residuals will be).\nProposition 2. For an MLP with activation \u03c6 satisfying $\\mathbb{E}\\phi(X)^2$, $X \\sim \\mathcal{N}(0,1)$, and residual parameter \u03c4, we have the residual kernel map\n$\\kappa_{res}(\\rho) = (1 - \\tau^2)\\kappa_{\\phi}(\\rho) + \\tau^2\\rho$,\nwhere $\\kappa_{\\phi}$ denotes kernel map of \u03c6.\nFurthermore, we have: 1) fixed points of $\\kappa_{res}(\\rho)$ are the same as those of $\\kappa_{\\phi}(\\rho)$, 2) p* is a globally attracting fixed point of $\\kappa_{\\phi}$ is a globally attracting fixed point of $\\kappa_{\\phi}$, 3) the convergence of residual kernel map $ \\kappa_{\\phi}$ is monotonically decreasing in r (the stronger residuals, the slower convergence).\nImplications. Proposition 2 reveals that residual connections modify the kernel map by blending the original kernel map with the identity function, weighted by the residual r. This adjustment has several interesting implications. Our analysis here gives a quantitative way of balancing depth with the nonlinearity of activations. For example, Li et al. [2022] show as we 'shape' the negative slope of a leaky ReLU towards identity, it will prevent degenerate covariance in depth. In light of proposition 2 and corollary 2, we can write leaky ReLU as a linear combination of ReLU and identity and conclude that as residual strength increases (r \u2192 1), the convergence rate becomes slower and thus degeneracy happens at deeper layers, which aligns with the prior study. Another interesting byproduct of proposition 2 is that it shows that when r \u2192 1 (highly strong residuals), the kernel ODE becomes an exact model for kernel dynamics (See remark S.1)."}, {"title": "7.2 Normalization layers", "content": "Normalization layers are widely used in deep learning to stabilize and accelerate training. In this section, we focus on two common normalization layers:\n*   Layer Normalization (LN) [Ba et al., 2016]:\n$LN(z) = \\frac{z - \\mu(z)}{\\sigma(z)}$,\nwhere \u03bc(z) is the mean, and \u03c3(z) is the standard deviation of the vector z \u2208 Rd."}, {"title": "8 Discussion", "content": "The power of deep neural networks fundamentally arises from their depth and nonlinearity. Despite substantial empirical evidence demonstrating the importance of deep, nonlinear models, a rigorous theoretical understanding of how they operate and learn remains a significant theoretical challenge. Much of classical mathematical and statistical machinery was developed for linear systems or shallow models [Hastie et al., 2009], and extending these tools to deep, nonlinear architectures poses substantial difficulties. Our work takes a natural first step into this theoretical mystery by providing a rigorous analysis of feedforward networks with nonlinear activations.\nWe showed that viewing activations in the Hermite basis uncovers several strikingly elegant properties of activations. Many architectural choices, such as normalization layers, initialization techniques, and CLT-type convergences, make Gaussian preactivations central to understanding the role of activations. Hermite expansion can be viewed as the Fourier analog for the Gaussian kernel. These facts and our theoretical results, suggest that viewing activations in the Hermite basis is more natural than in the raw signal, analogous to viewing convolutions in the Fourier basis. For example, the fact that the kernel map kis analytic and has a positive power series expansion, i.e., is infinitely smooth, does not require the activation \u03c6 to be smooth or even continuous. Thus, as opposed to many existing analyses that consider smooth and non-smooth cases separately, our theoretical framework gives a unified perspective. As an interesting example, smoothness, which has been observed to facilitate better training dynamics 3[Hayou et al., 2019], appears as a faster decay in Hermite coefficients. Thus, similar to leveraging the Fourier transform for understanding and designing filters, one can aspire to use Hermite analysis for designing and analyzing activation functions.\nOne of the key contributions of our work is the global perspective in analyzing neural networks. Traditional analyses often focus on local structure, such as Jacobian eigenspectrum by Pennington et al. [2018]. While these local studies provide valuable insights, they do not capture the global kernel dynamics. More concretely, real-world inputs to neural networks, such as images of a dog and a cat, are not close enough to each other to be captured in the local changes. Our global approach allows us to study the evolution of representations across layers for any pair of inputs, providing further insights into how deep networks transform inputs with varying degrees of similarity.\nOne of the central assumptions for our approach is operating in the mean-field, i.e., infinite-width regime. While this approach led us to establish numerous elegant properties, it would be worth exploring the differences between finite-width and infinite-width results. This remains an important avenue for future work. Similarly, in the CLT application of Proposition 1, the exact discrepancies between the limited width and the infinite width would deepen our understanding of practical neural networks. Another intriguing endeavor is the potential to extend our framework"}]}