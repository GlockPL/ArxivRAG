{"title": "GRAM: GENERALIZATION IN DEEP RL WITH A ROBUST ADAPTATION MODULE", "authors": ["James Queeney", "Xiaoyi Cai", "Mouhacine Benosman", "Jonathan P. How"], "abstract": "The reliable deployment of deep reinforcement learning in real-world settings requires the ability to generalize across a variety of conditions, including both in-distribution scenarios seen during training as well as novel out-of-distribution scenarios. In this work, we present a framework for dynamics generalization in deep reinforcement learning that unifies these two distinct types of generalization within a single architecture. We introduce a robust adaptation module that provides a mechanism for identifying and reacting to both in-distribution and out-of-distribution environment dynamics, along with a joint training pipeline that combines the goals of in-distribution adaptation and out-of-distribution robustness. Our algorithm GRAM achieves strong generalization performance across in-distribution and out-of-distribution scenarios upon deployment, which we demonstrate on a variety of realistic simulated locomotion tasks with a quadruped robot.", "sections": [{"title": "INTRODUCTION", "content": "Due to the diverse and uncertain nature of real-world settings, generalization is an important capability for the reliable deployment of data-driven, learning-based frameworks such as deep reinforcement learning (RL). Policies trained with deep RL must be capable of generalizing to a variety of different environment dynamics at deployment time, including both familiar training conditions and novel unseen scenarios, as the complex nature of real-world environments makes it difficult to capture all possible variations in the training process.\nExisting approaches to zero-shot dynamics generalization in deep RL have focused on two complementary concepts: adaptation and robustness. Contextual RL techniques (Hallak et al., 2015) learn to identify and adapt to the current environment dynamics to achieve the best performance, but this adaptation is only reliable for the range of in-distribution (ID) scenarios seen during training. Robust RL methods (Nilim & Ghaoui, 2005; Iyengar, 2005), on the other hand, maximize the worst-case performance across a range of possible environment dynamics, providing generalization to out-of-distribution (OOD) scenarios at the cost of conservative performance in ID environments.\nThis work shows how to extract the benefits of these complementary approaches in a unified framework called GRAM: Generalization in deep RL with a Robust Adaptation Module. GRAM achieves generalization to both ID and OOD environment dynamics at deployment time within a single architecture. Our main contributions are as follows:\n1. We introduce a robust adaptation module in Section 4 that provides a mechanism for identifying both ID and OOD environment dynamics within the same architecture. We extend existing contextual RL approaches by using an epistemic neural network (Osband et al., 2023) to incorporate a measure of uncertainty about the environment at deployment time.\n2. We propose a joint training pipeline in Section 5 that combines a teacher-student architecture for learning adaptive ID performance with adversarial RL training for robust OOD performance, resulting in a single unified policy that can achieve both ID and OOD dynamics generalization.\n3. We demonstrate the strong ID and OOD performance of GRAM in Section 7 through comprehensive experiments on realistic simulated locomotion tasks with the Unitree Go2 quadruped robot in Isaac Lab (Mittal et al., 2023)."}, {"title": "RELATED WORK", "content": "Zero-shot generalization in deep RL has received significant attention in recent years (see Kirk et al. 2023 for a survey). The most common formulation of this problem is contextual RL (Hallak et al., 2015), and many studies have demonstrated the importance of leveraging contextual information in deep RL to improve generalization (e.g., Benjamins et al. 2023). For the case of known contexts, both model-free (Beukman et al., 2023) and model-based (Prasanna et al., 2024) architectures have been proposed to effectively incorporate context into training. When the context is unknown at deployment time, it must be inferred from past observations. Some domain randomization methods directly consider a policy conditioned on history (Peng et al., 2018; Tiboni et al., 2024), while self-supervised approaches infer context from history through the use of variational inference (Yang et al., 2020; Chen et al., 2022; Ren et al., 2022), dynamics prediction (Lee et al., 2020b), or separation-based objectives (Luo et al., 2022). Similar techniques have also been applied in the related area of meta RL (Nagabandi et al., 2019; Rakelly et al., 2019; Zintgraf et al., 2020), which require additional learning in the deployment environment for adaptation. Supervised approaches, on the other hand, leverage privileged context information during training, and apply a teacher-student architecture to train a policy that can be deployed using only the history of past observations (Lee et al., 2020a; Kumar et al., 2021; Margolis et al., 2024). All of these methods are designed to adapt across the range of ID contexts seen during training, but are not specifically trained to handle OOD contexts with different environment dynamics. As a result, their ability to generalize is sensitive to the distribution of ID training contexts, and they may not generalize well to OOD contexts.\nRobust RL focuses on generalizing to OOD environments at deployment time by maximizing worst-case performance over a set of transition models (Nilim & Ghaoui, 2005; Iyengar, 2005). Deep RL methods most commonly apply robustness during training through the use of parametric uncertainty or adversarial training. Parametric uncertainty methods consider a range of simulation parameters during training, and optimize for worst-case performance across these environments (Rajeswaran et al., 2017; Mankowitz et al., 2020). Adversarial RL instead applies worst-case perturbations during training to provide robustness to unknown dynamics or disturbances at deployment time. Various types of adversarial interventions have been considered, including external forces (Pinto et al., 2017; Reddi et al., 2024; Xiao et al., 2024) as well as perturbations to actions (Tessler et al., 2019), observations (Zhang et al., 2020), and transitions (Queeney & Benosman, 2023; Queeney et al., 2024). Robust RL methods have also been applied to capture estimation uncertainty (Xie et al., 2022) and context shifts (Lin et al., 2020; Ajay et al., 2022; Zhang et al., 2023) in contextual RL and meta RL. These approaches provide robust generalization to environment dynamics that were not explicitly seen during training, but often sacrifice ID performance to achieve robustness.\nIn this work, we are interested in both ID and OOD generalization. The possibility of different modes at deployment is related to methods that train a collection of policies to select from at deployment time. Many learning-based approaches to safety switch between a task-based policy and a recovery policy in order to guarantee safety at deployment time (Thananjeyan et al., 2021; Wagener et al., 2021; Contreras et al., 2024; He et al., 2024; Sinha et al., 2024). Other methods select between a finite collection of different behaviors based on the deployment environment (Ajay et al., 2022; Chen et al., 2023). Contextual RL can also be viewed as learning a collection of policies for ID adaptation, and our robust adaptation module extends this approach to incorporate a mode for OOD generalization as well."}, {"title": "PROBLEM FORMULATION", "content": "Contextual RL We model the problem of dynamics generalization in deep RL as a Contextual Markov Decision Process (CMDP) (Hallak et al., 2015). A CMDP considers a set of contexts C that define a collection of MDPs {Mc}cec. For each c \u2208 C, we have an MDP given by the tuple Mc = (S, A, pc, r, po, \u03b3), where S is the set of states, A is the set of actions, pc : S\u00d7A\u00d7C \u2192 P(S) is the context-dependent transition model where P(S) represents the space of probability measures over S, r: S \u00d7 A \u2192 R is the reward function, po is the initial state distribution, and y is the discount rate. We focus on the setting where the transition model pe depends on the context c \u2208 C (i.e., varying dynamics), while the reward function r remains the same across contexts (i.e., same task). For a policy and context c\u2208 C, performance is given by the expected total discounted"}, {"title": "ROBUST ADAPTATION MODULE", "content": "We build upon the teacher-student architecture for adaptation in deep RL, which has demonstrated strong performance in complex robotics applications (Lee et al., 2020a; Kumar et al., 2021; Margolis et al., 2024). However, because this approach focuses on adaptation across ID contexts observed during training, its OOD generalization capabilities depend strongly on the relationship between CID and COOD. For OOD contexts with environment dynamics that are not similar to ID training contexts, we show in our experiments that the standard teacher-student architecture can perform poorly. In order to generalize to both ID and OOD contexts at deployment time, we introduce a robust adaptation module that explicitly incorporates a mechanism for identifying and reacting to OOD contexts.\nTeacher-student training The teacher-student approach to generalization in deep RL assumes access to ct \u2208 CID at every timestep throughout training, and leverages this privileged context information to train a teacher policy that can adapt to different contexts. The teacher policy applies a context encoder f : C \u2192 Z that maps the context to a latent feature zt = f(ct), which is then provided as an input to the policy \u03c0 : S \u00d7 Z \u2192 P(A) and critic V\u2122 : S \u00d7 Z \u2192 R. In this work, we consider the latent feature space Z = Rd. The context encoding zt = f(ct), policy \u03c0(at | St, zt), and critic V\u2122 (st, zt) are trained to minimize the average actor-critic RL loss over ID contexts given by\n$C_{RL} =  E_{C~C_{ID}} [L_\\pi(c) + L_V(c)],$\nwhere $L_\\pi(c)$ and $L_V(c)$ represent the policy loss and critic loss, respectively, of a given RL algorithm for the context c ~ CID. In our experiments, we apply Proximal Policy Optimization (PPO) (Schulman et al., 2017) as the RL algorithm. See the Appendix for details.\nNote that the teacher policy cannot be applied at deployment time because it requires privileged information about the context ct in order to compute the latent feature zt. For this reason, RL training is followed by a supervised learning phase where a student policy is trained to imitate the teacher policy using only the recent history of states and actions from the last H timesteps ht = (St\u2212H,Qt\u2212H, ..., st) \u2208 H. In particular, an adaptation module \u00a2 : H \u2192 Z that maps recent history to a latent feature 2t = $(ht) is trained to minimize the loss\n$L_{enc} =  E_{C~C_{ID}} E_{\\tau ~ (\\pi, c)} [||f(c_t) - \\phi(h_t)||^2] =  E_{C~C_{ID}} E_{\\tau ~ (\\pi, c)} [||z_t - \\hat{z_t}||^2],$"}, {"title": "TRAINING FOR ROBUST ADAPTATION", "content": "Our robust adaptation module provides an intuitive structure for achieving both ID and OOD dynamics generalization within a single architecture. In order to accomplish this goal, we jointly train our policy \u03c0(at | St, zt) for adaptive performance in ID environments and robust performance in OOD environments (i.e., when Zt = Zrob). For a given iteration of RL training, we assign each training environment to either ID training or OOD training. This assignment determines how the latent feature vector is calculated, as well as how data collection occurs in the environment. See Figure 2 for an overview.\nData collection Within each iteration, all data for a given training environment is collected according to either standard ID data collection or adversarial OOD data collection, as described in the following paragraphs. This provides temporal consistency when training the policy for adaptive or robust performance, respectively. However, we alternate these assignments between iterations,"}, {"title": "GRAM ALGORITHM", "content": "Together, the robust adaptation module in (8) and the training procedure in Section 5 form our algorithm GRAM. GRAM combines standard ID data collection and adversarial OOD data collection during training to optimize the RL loss\n$L_{RL}^{GRAM} = C_{RL}^{ID} + C_{RL}^{OOD},$\nfollowed by a supervised learning phase to optimize the encoder loss LGRAM. Finally, by applying the robust adaptation module \u00d8GRAM at deployment time, our policy achieves both ID and OOD dynamics generalization within a single unified architecture.\nThere are several key factors that allow GRAM to achieve strong performance in both ID and OOD environments. First, the special robust latent feature Zrob allows us to separate the competing goals of ID adaptation and OOD robustness during training, while still considering a unified policy structure. Second, the robust adaptation module provides a mechanism for identifying and reacting to unreliable latent feature estimates in OOD scenarios at deployment time. Finally, we see in our experiments that the joint training pipeline with mixed data collection provides additional robustness benefits to the policy.\nAs we show in the next section, we found that the use of a teacher-student architecture for ID adaptation and adversarial RL for OOD robustness result in strong performance on the simulated quadruped robot locomotion tasks we consider in our experiments. However, note that it is also possible to apply GRAM with different choices of contextual RL methods for ID adaptation and robust RL methods for OOD generalization, which represents an interesting avenue for future work."}, {"title": "EXPERIMENTS", "content": "We evaluate the performance of GRAM on realistic simulated locomotion tasks with the Unitree Go2 quadruped robot in Isaac Lab (Mittal et al., 2023). The goal of the robot is to track a velocity command vemd = [vcmd, Vy vemd, wmd] \u2208 R\u00b3 provided as input, where vemd, vemd represent target forward and lateral linear velocities, respectively, and womd represents a target yaw angular velocity. For each episode, we uniformly sample a forward linear velocity command between 0.5 and 1.0 meters per second (i.e., vemd ~ U([0.5, 1.0]), vmd = 0) and calculate the yaw angular velocity command womd throughout the episode based on a target heading direction. The simulated quadruped robot can be seen in Figure 2, with the velocity command represented by a green arrow.\nThe policy has access to noisy proprioceptive observations available from standard onboard sensors at every timestep (joint angles, joint velocities, projected gravity, and base angular velocities), and outputs target joint angles at \u2208 R12 for each of the robot's 12 degrees of freedom that are converted to torques by a PD controller with proportional gain Kp = 25 and derivative gain Ka = 0.5. The maximum episode length is 20 seconds with target joint angles processed at 50 Hz, which corresponds to 1,000 timesteps per episode. We consider the reward function used in Margolis et al. (2024), which includes rewards for tracking the velocity command vmd and regularization terms to promote smooth and stable gaits. See the Appendix for additional details.\nWe conduct experiments to investigate several hypotheses related to our algorithm GRAM:\nH1. Existing contextual RL and robust RL methods demonstrate trade-offs between ID and OOD performance that depend critically on the set of contexts CID seen during training.\nH2. GRAM can achieve strong ID and OOD generalization with a single unified policy.\nH3. GRAM identifies ID contexts from OOD contexts at deployment time in a way that automatically adjusts for different choices of CID.\nH4. The unified architecture and joint training pipeline of GRAM outperforms other implementation choices for achieving ID and OOD generalization.\nIn order to test these hypotheses, we compare GRAM to contextual RL and robust RL on two different choices of training sets CID: (i) Base ID and (ii) Base ID + Frozen Joints. First, we consider the Base ID training set described in Table 1. This set represents moderate variations that are commonly used to promote sim-to-real transfer, and do not require significant adaptation to achieve good performance across ID contexts. Next, we consider the same variations shown in Table 1 while also freezing one (or none) of the robot's 12 joints. We refer to this set as Base ID + Frozen Joints, which represents a more diverse set of ID contexts with varying dynamics."}, {"title": "CONCLUSION", "content": "In this work, we have presented a deep RL framework that achieves both ID and OOD dynamics generalization at deployment time within a single architecture. Our algorithm GRAM leverages a robust adaptation module that allows for adaptation in ID contexts, while also identifying OOD environments with a special robust latent feature Zrob. We presented a training pipeline that jointly trains for adaptive ID performance and robust OOD performance, resulting in strong generalization capabilities across a range of realistic simulated locomotion tasks on the Unitree Go2 quadruped robot in Isaac Lab. The ability to achieve ID and OOD generalization within a unified framework is critical for the reliable deployment of deep RL in real-world settings, and GRAM represents an important step towards this goal.\nLimitations and future work Because OOD contexts are unknown during training by definition, the OOD generalization of GRAM depends on how well the robust RL training pipeline captures worst-case OOD dynamics. We applied a single choice of adversary in our experiments that worked well in practice, but it would be interesting to extend GRAM to incorporate different levels of robustness at deployment time. There are also opportunities to apply GRAM with different choices of contextual RL and robust RL techniques, extend our robust adaptation module to incorporate other methods for uncertainty quantification (Gawlikowski et al., 2023), and address other forms of generalization beyond dynamics related to modalities such as vision. Finally, we focused on simulated quadruped locomotion experiments in this work to conduct a comprehensive empirical study of GRAM across a variety of settings. We plan to deploy our GRAM framework in real-world hardware experiments on the Unitree Go2 quadruped robot as part of future work. We are also interested in applying GRAM to other applications where generalization is important, such as contact-rich manipulation tasks and agile quadrotor flight."}, {"title": "IMPLEMENTATION DETAILS", "content": "Task definition For the simulated quadruped robot locomotion tasks we consider in our experiments, we follow standard design choices used in the literature (Rudin et al., 2021; Margolis et al., 2024). See Table 2 for details on the inputs received by our policy, where we consider the default observation noise levels in Isaac Lab (Mittal et al., 2023). Table 3 provides details on the reward"}]}