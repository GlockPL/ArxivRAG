{"title": "Multispectral to Hyperspectral using Pretrained\nFoundational model", "authors": ["Ruben Gonzalez", "Conrad M Albrecht", "Nassim Ait Ali Braham", "Devyani Lambhate", "Joao Lucas de Sousa Almeida", "Paolo Fraccaro", "Benedikt Blumenstiel", "Thomas Brunschwiler", "Ranjini Bangalore"], "abstract": "Hyperspectral imaging provides detailed spectral information, offering\nsignificant potential for monitoring greenhouse gases like CH4 and NO2.\nHowever, its application is constrained by limited spatial coverage and in-\nfrequent revisit times. In contrast, multispectral imaging delivers broader\nspatial and temporal coverage but lacks the spectral granularity required\nfor precise GHG detection. To address these challenges, this study pro-\nposes Spectral and Spatial-Spectral transformer models that reconstructs\nhyperspectral data from multispectral inputs. The models in this paper\nare pretrained on EnMAP and EMIT datasets and fine-tuned on spatio-\ntemporally aligned (Sentinel-2, EnMAP) and (HLS-S30, EMIT) image\npairs respectively. Our model has the potential to enhance atmospheric\nmonitoring by combining the strengths of hyperspectral and multispectral\nimaging systems.", "sections": [{"title": "1 Introduction", "content": "Satellite images are being used to create detailed maps of Earth's surface. These\nmaps can be used for a variety of purposes, including navigation, urban planning,\nand environmental management. Recently there has been a surge in the launch\nof Hyperspectral satellites. Unlike multispectral satellites that capture a few\nbroad bands, hyperspectral satellites capture hundreds of narrow bands, pro-\nviding a much richer spectral fingerprint. Because of its ability to capture finer\nspectral information, it can be used for more precise applications like mineral\nmapping, greenhouse gases (GHGs) mapping, precision agriculture, surveillance,\nwater resource management, and many other tasks. Hyperspectral imaging has\nrevolutionized many fields by enabling a detailed analysis of different matters\nand molecules based on spectral signatures. However, the revisit time of most\nof the hyperspectral satellites are much higher, which can be a limitation for\ntime sensitive observations.\nIn contrast multispectral satellites have lower revisit time and broader spa-\ntial coverage. In this paper, we propose to generate Hyperspectral data from\nMultispectral data, for the periods when hyperspectral data is unavailable using\nfoundational models.\nRecently large-scale Image foundational models like Vision Transformer (ViT)\n[1], Stable Diffusion [2], and DALLE [3] have revolutionized image generation,\nimage classification and segmentation tasks. We have formulated the prob-\nlem of generating hyperspectral data from multispectral data as a precise data\ngeneration case, where we want to generate all hyperspectral bands given the\nmultispectral bands for a satellite Image using a ViT model. The ViT models\nfocuses on capturing spatial relationships but lack the ability to capture spectral\nrelationships. We therefore propose two modified version of vision transformers,\nspecially adapted to capture the spectral and spatial-spectral relationships in\nthe hyperspectral data.\nWe have performed two set of experiments to generate hyperspectral data\nfrom the multispectral data.\n1. Multispectral data from Harmonized Landsat Sentinel-2 (HLS-S30) [4] is\nused to reconstruct the corresponding Earth Surface Mineral Dust Source\nInvestigation (EMIT) [5] hyperspectral bands using a ViT-based model\npre-trained on EMIT data.\n2. Multispectral data from Sentinel-2 (S2) is used to reconstruct the cor-\nresponding Environmental Mapping and Analysis Program (EnMAP) [6]\nusing a ViT-based model pre-trained on EnMAP data."}, {"title": "2 Related work", "content": "Spectral reconstruction bridges the gap between hyperspectral and multispectral\nimaging by enhancing spectral granularity. Early methods, such as dictionary-\nbased approaches [7] and Gaussian processes [8], relied on handcrafted priors but\nwere limited in capturing non-linear spectral-spatial relationships. With the rise\nof deep learning, convolutional neural networks (CNNs) [9, 10] and attention-\nbased transformers [11] have emerged as effective tools, leveraging data-driven\nlearning to improve reconstruction accuracy. Recent approaches, such as hybrid\nattention networks [12], emphasize combining spatial and spectral attention for\nenhanced reconstruction quality.\nSelf-supervised learning (SSL) frameworks, like masked autoencoders (MAE)\n[13], naturally align with spectral reconstruction by leveraging large-scale un-\nlabeled datasets to predict missing spectral information [14]. In hyperspec-\ntral remote sensing, studies such as [15] have demonstrated the effectiveness\nof self-supervised masked image reconstruction for adapting transformers to\nthe unique characteristics of hyperspectral data. By integrating spatial-spectral\nself-attention, spectral positional embeddings, and blockwise patch embeddings,\nthese models achieve significant accuracy improvements, particularly in label-\nscarce scenarios. Building on these insights, our work further explores spectral\nreconstruction with self-supervised transformers tailored for hyperspectral im-\nagery."}, {"title": "3 Data", "content": ""}, {"title": "3.1 Pretraining datastes", "content": "EMIT uses an advanced imaging spectrometer instrument installed on the In-\nternational Space Station that measures a spectrum for every point in the im-\nage. The EMIT data reflects the wavelengths from visible to short infrared\nwavelengths consisting 285 discrete bands, we have used 240 bands for our ex-\nperiments with excluding the bands corresponding to water vapours and the\nfirst four bands. We use the Level 2A EMIT [16] product that contains the\nSurface reflectance derived by screening clouds and correction for atmospheric\neffects. The EMIT dataset used for Pretraining is split into 73,853 train, 4102\nvalidation and 4102 test samples.\nEnMAP is derived from two primary sources. HySpecNet-11k provides\n11,483 hyperspectral images extracted from EnMAP tiles, covering wavelengths\nfrom 420 to 2,450 nm and comprising 202 usable spectral bands after atmo-\nspheric correction [17]. This dataset offers a manageable yet diverse subset\nof hyperspectral data for model training and evaluation. Complementing this,\nthe SpectralEarth dataset comprises 538,974 hyperspectral images derived from\nEnMAP acquisitions between 2022 and 2024. With similar spatial and spectral\ncharacteristics to HySpecNet-11k, SpectralEarth provides extensive geographic\nand temporal coverage, making it well-suited for large-scale pre-training [18]."}, {"title": "3.1.1 Finetuning Datasets", "content": "HLS-S30 and EMIT images from a harmonized surface reflectance product\ncalled HLS-2 (version 2.0) data are downloaded from NASA's land processes\ndistributed active archive center cumulus cloud as optimized GeoTIFFs. The\nSentinel S30 data has been pre-processed to generate and select 128 \u00d7 128 pixel\u00b2\ncloud-free patches. The data is processed to spatio-temporally align the EMIT\nand Sentinel-S30 pairs. This dataset is divided into 20691 train, 3205 validation,\nand 2190 test patches. Sentinel-S30 cloud-free patches with 13 channels were\nutilized to generate the corresponding 240 hyperspectral bands.\nS2 and EnMAP The multispectral imagery is sourced from Sentinel-2,\naligned spatially and temporally with the hyperspectral data of the HySpecNet-\n11k dataset. Level-2A-processed Sentinel-2 images were obtained via Google\nEarth Engine and matched to EnMAP acquisitions within a 3-day window to\nensure comparable surface reflectance values. After cloud filtering, this mapping\nresulted in an 85.32% alignment between EnMAP patches and their correspond-\ning Sentinel-2 images."}, {"title": "4 Pretraining Hyperspectral Foundational Model", "content": ""}, {"title": "4.1 Model Architecture", "content": "We used a very small version of the ViT model based on the MAE approach, a\nsuccessful self-supervised learning method widely used and extended for differ-\nent data types, including videos[19] and multispectral images [20]. The MAE\nreconstructs masked images using an asymmetric encoder-decoder architecture.\nThe input image is divided into non-overlapping patches of the same size, and\na subset of the patches is randomly masked. The encoder receives only the\nunmasked patches generating their latent representation. The decoder then re-\nceives the latent and masked tokens to perform the image reconstruction task\n[13].\nWe have 4 encoder and 2 decoder blocks in our very small ViT model. The\nnumber of heads are (8 and 8) and the embedding dimensions are (768 and 512),\nrespectively, for (the encoder and the decoder). The images are 128 \u00d7 128\u00d7\n240 (H \u00d7 W \u00d7 C) dimensional. We have trained the model with a masking\nratio (r) of 75%. Unlike the loss function used in ViT, which measures the"}, {"title": "4.2 Masking strategies", "content": "The ViT is pre-trained using a masking approach. We have explored two types\nof masking strategies in this paper: spectral and spatial-spectral. The input\nhyperspectral images has 3 dimensions, which are two spatial (H, W) dimensions\nand one spectral dimension (C), where C is the number of channels in the\nhyperspectral data. The spatial masking, where the input (H, W, C) is divided\ninto patches of dimension (p, p, C) has proved to be very beneficial for several\nvision-based models as well as some remote sensing models. The problem with\nspatial masking is that it does not inherently capture spectral relationships. We\ntherefore propose spectral and spectral-spatial masking for band reconstruction."}, {"title": "4.2.1 Spectral Masking", "content": "In Spectral masking strategy, the input (H, W, C) is divided into (p, p, s) patches\nwith C/s band groups. Out of these C/s band groups, we randomly select r% of\nthe band groups to be masked, where r is the masking ratio. If a band group is\nselected to be masked then all the spatial patches (p \u00d7 p), corresponding to the\nselected band groups are masked. We have used band groups, instead of single\nbands here to take care of the increase in computational complexity if single\nbands were used."}, {"title": "4.2.2 Spatial-Spectral Masking", "content": "Similar to Spectral masking the input (H, W, C) is divided into (p, p, s) patches.\nAnd each of the patch of size (p x pxs) is either selected to be masked or\nunmasked randomly based on the masking ratio (r). The pictorial representa-\ntion of all three masking strategies (spatial, spectral and spatial-spectral) are\npresented in the Figure 1."}, {"title": "5 Experiments and Results", "content": ""}, {"title": "5.1 Metrics", "content": "MSE: We have reported the Masked MSE (calculated only over masked pix-\nels), Unmasked MSE (calculated only over unmasked pixels) and the total MSE.\nStructural Similarity Score (SSIM) [21]: SSIM is a metric for evaluat-\ning the quality of digital images\n$SSIM(x, y) = \\frac{(2\\mu_x\\mu_y + C_1) (2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1) (\\sigma_x^2 + \\sigma_y^2 + C_2)}$"}, {"title": "5.2 A comparison of spectral and spatial-spectral pre-trained\nmodels", "content": "The spatial-spectral and spectral models are pre-trained on the EMIT dataset to\nget a comparison between the two masking techniques. The results are reported\nin Table 1. We notice that the model with spectral-spatial masking outperforms\nthe model with spectral masking."}, {"title": "5.3 Reconstruction of Hyperspectral from Multispectral", "content": "After getting sufficient confidence in generating randomly masked EMIT bands,\nwe tried to test the models for Multispectral to Hyperspectral conversion.\nFor this, we have aligned the Sentinel bands with the EMIT and EnMAP\nbands based on the frequencies (if there is more than 60% overlap between the\nfrequencies, then we say there is a match). Wherever the match is found, bands\nare fixed to be the unmasked bands and the rest are the masked bands.\nTable 2 summarizes the experiments for band reconstruction for the two\ndata sets with and without fine-tuning. We have selected Frozen Spa-Spec\nmodel for the HLS-S30-EMIT dataset and Finetuned Spec model for the S2-\nEnMAP dataset to generate the plots in Figure 2 and 3 as they performed\nbest with-respect-to the MSE metric. The frozen version have never seen any\ntrain samples of Sentinel data but still perform considerably well both in terms\nof MSE metric and reconstruction quality which can be seen from Figure 3-\nHLS-S30-EMIT plot. In all the spectral model versions, the results are further\nimproved by fine-tuning. Whereas, with HLS-S30-EMIT dataset, we have seen\nan improvement only in the SSIM metric and not MSE for spatial-spectral\nversion. We have not included the results for spectral-spatial generation for the\nS2-EnMAP dataset as it is already covered in [15]."}, {"title": "6 Conclusion and Future Work", "content": "This study introduced the spectral and spatial-spectral transformer models for\nreconstructing hyperspectral data from limited spectral inputs, demonstrating\naccuracy and practical utility. The model effectively predicted missing spectral\nbands in masked hyperspectral data and enhanced spectral details when applied\nto multispectral inputs.\nOur future plans include using the pre-trained models described in this paper\nfor several downstream tasks like methane detection, CO2 detection, Crop iden-\ntification, Mineral classification, etc. We plan to further generalize the masking\napproach by including a temporal dimension in the masking strategy. We also\nplan to work on multimodal models, which can process data from multiple hy-\nperspectral/multispectral datasets."}]}