{"title": "HUMANVBENCH: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data", "authors": ["Ting Zhou", "Daoyuan Chen", "Qirui Jiao", "Bolin Ding", "Yaliang Li", "Ying Shen"], "abstract": "In the domain of Multimodal Large Language Models (MLLMs), achieving human-centric video understanding remains a formidable challenge. Existing benchmarks primarily emphasize object and action recognition, often neglecting the intricate nuances of human emotions, behaviors, and speech-visual alignment within video content. We present HUMANVBENCH, an innovative benchmark meticulously crafted to bridge these gaps in the evaluation of video MLLMs. HUMANVBENCH comprises 17 carefully designed tasks that explore two primary dimensions: inner emotion and outer manifestations, spanning static and dynamic, basic and complex, as well as single-modal and cross-modal aspects. With two advanced automated pipelines for video annotation and distractor-included QA generation, HUMANVBENCH utilizes diverse state-of-the-art (SOTA) techniques to streamline benchmark data synthesis and quality assessment, minimizing human annotation dependency tailored to human-centric multimodal attributes. A comprehensive evaluation across 16 SOTA video MLLMs reveals notable limitations in current performance, especially in cross-modal and temporal alignment, underscoring the necessity for further refinement toward achieving more human-like understanding. HUMANVBENCH is open-sourced to facilitate future advancements and real-world applications in video MLLMs.", "sections": [{"title": "1. Introduction", "content": "In recent years, Multimodal Large Language Models (MLLMs) have emerged as pivotal technological advancements, significantly expanding traditional language model capabilities to include processing and comprehending diverse data forms like text, images, and videos [3, 5, 39]. Among these, video-oriented MLLMs [7, 28, 49] have garnered substantial research interest due to their potential in interpreting video content in a manner closely aligned with human perception. While image-based MLLMs [18, 30, 32, 43, 44, 55] primarily focus on static content, video-MLLMs offer enhanced capacities for understanding the complex temporal dynamics intrinsic to video data.\nHuman-centric scenes in videos naturally attract attention due to the emphasis on individuals' emotions, actions, and verbal interactions, necessitating effective comprehension by video understanding models. Despite advances in this field, existing benchmarks often fall short in rigorously assessing the nuanced understanding of human emotions and behaviors. Current evaluations predominantly focus on general content comprehension, object recognition, and action detection, often neglecting subtle intricacies such as emotional insight and behavioral analysis. Furthermore, synchronizing speech with visual elements remains a substantial challenge; unlike humans, who effortlessly discern mismatches between audio and visual cues, computational models often struggle with tasks like identifying speakers and aligning speech with corresponding lip movements.\nTo bridge these gaps, we introduce HUMANVBENCH, a pioneering benchmark specifically tailored for video MLLMs focusing on human-centric analysis. HUMANVBENCH includes 17 meticulously designed QA tasks, categorized into two core aspects: inner emotion and outer manifestation, as illustrated in Figure 1. The inner emotion dimension evaluates the model's capacity to perceive emotional cues from videos, while the outer manifestation encompasses person recognition, human behavior analysis, and cross-modal speech-visual alignment. Each category is further subdivided into many fine-grained tasks, offering a comprehensive evaluation across static and dynamic, basic and complex, single-modal and cross-modal aspects. For instance, it enables us to examine MLLMs' understanding of human actions, emotions, and the associated causal relationships, as well as considering the alignment degree between appearance, mouth movement, audio, and speech content in videos.\nThe construction of HUMANVBENCH is facilitated by the advanced open-source data processing system, Data-Juicer [4], and involves two novel data synthesis pipelines: the Human-Centric Video Annotation Pipeline and the Distractor-Included QA Synthesis Pipeline, leveraging over twenty state-of-the-art (SOTA) data processing operators. Unlike conventional benchmarks that rely heavily on human"}, {"title": "2. Related Works", "content": "Multimodal Large Language Models. The remarkable progress in Large Language Models (LLMs) has sparked extensive research into merging language comprehension"}, {"title": "3. The Proposed HUMANVBENCH", "content": "Human observers, by nature, focus intently on individuals within videos, analyzing their appearance, emotional states, and behaviors. This inherent focus forms the basis of our systematic design of 17 fine-grained human-related tasks intended to assess the ability of MLLMs to emulate human-like perception and comprehension in video analysis (Figure 1). Detailed definitions and examples for each task are available in the Appendix. These tasks are categorized based on the observability of analyzed content into two categories: Inner Emotion and Outer Manifestation."}, {"title": "3.1. Inner Emotion", "content": "In realistic video scenarios, inner emotions are often less observable due to the possible lack of focal points on a person's face, as cameras might capture a wider array of scenes. Thus, emotion perception becomes an abstract skill requiring careful attention to facial expressions, body language, and verbal cues. With this goal, tasks designed under this category assess the capability of video-MLLMs to capture and interpret emotional subtleties akin to human perceptual abilities.\nSpecifically, five specific tasks comprise the Emotion Perception category:\n\u2022 Emotion Recognition (ER) requires identifying the most fitting emotional description for an individual in a video.\n\u2022 Emotion Temporal Analysis (ETA) focuses on tracking emotional changes over time.\n\u2022 Attitude Recognition (AT) assesses the inferred attitudes of individuals in relation to specific events or entities, classifying them as positive, negative, or neutral.\n\u2022 Emotion Intensity Comparison (EIC) evaluates the model's ability to differentiate and quantify the emotional intensity of various individuals.\n\u2022 Emotion Recognition in Conversation expands on previous tasks by integrating speech content from the MELD dataset [38] to judge multimodal emotional comprehension when text is available.\nThese tasks collectively enable the evaluation of MLLMs' potential in capturing emotional cues and facial details from videos, thereby gauging their capabilities in inner emotional understanding."}, {"title": "3.1.2. Outer Manifestation", "content": "Unlike inner emotions, outer manifestations deal with more tangible aspects such as identifying individual(s), causality reasoning, and synchronization of video elements like speech or singing. Guided by these aspects of human observation, we formulated three task categories.\nThe first basic dimension is for Person Recognition, evaluating the model's capability to identify a particular person within a complex scene, akin to \"person-finding\":"}, {"title": "3.2. Human-Centric Video Annotation Pipeline", "content": "Creating task-specific questions for the aforementioned tasks hinges on extensive human-centric annotations within videos. Our video annotation pipeline, illustrated in Figure 2, emphasizes multi-modal, granular annotation pathways, supported by the Data-Juicer [4, 6] framework. While benefiting from existing operators, we've additionally developed innovative operators to contribute back to the open-source community. Detailed annotations and examples are available in the Appendix."}, {"title": "3.2.1. Collecting Videos Containing People", "content": "We sourced copyright-free videos from Pexels [37], applying the video_split_by_scene_mapper to partition videos based on scene transitions of ensuring accurate human tracking. After a series of attribute-based filtering such as duration and optical flow, a video_face_ratio_filter gauges the visibility of faces, retaining videos where a face appears in a majority of the frames for further subsequent annotation."}, {"title": "3.2.2. Video Mappers", "content": "First, we implement a specially designed operator named video_human_tracks_extraction mapper to track frame-level face bounding box, which is essential for subsequent person localization and highlighting. A track is constructed by iteratively detecting the bounding boxes across frames based on thresholds for sequential frame overlaps, with the aim to ensure accurate person localization. This process also allows for an approximate count of how many people appear within the continuous shot.\nNext, the human_demographics mapper is leveraged to assign demographic labels such as age, gender, and race. Bounding box data at different positions such as full-body or face-only enable flexible cropping to generate individual-focused videos, paying attention to appearance and pos-ture (via the video_human_description_mapper) or facial expressions and transformations (via the video_facial_description_mapper). Several distinct benefits were derived from these processes. On the one hand, it prevents the description model from failing to recognize videos with bounding boxes or being distracted by other people in the frame. On the other hand, it helps preserve facial details that could be lost through resizing. Further, a series of audio operations are employed to enrich information about people in the video. The audio annotation process begins with a audio-tagging mapper to classify the sound type. For videos whose sound type is determined to be \u201cspeech\", specific operators are activated. These include the active_speaker_detection mapper for identifying who is the real speaker in the video, the asr mapper for transcribing spoken content, the speech_emotion_recognition mapper for detecting emotional cues in speech, and the voice_demographics mapper for profiling voice characteristics, such as gender and age.\nFinally, annotations will be extended to encompass event atmosphere and broader video narratives, captured via video_description_mapper. As a result, these detailed annotations enable versatile and automated question synthesis. For instance, in the case of Audio-Visual Speaker Matching task, videos are filtered using speech features (Label-10 and Label-3 of Figure 2) for precise alignment\""}, {"title": "3.3. Distractor-Included QA Generation Pipeline", "content": "For tasks with exact answers (e.g. restricted category, number or letter), their questions, correct answers, and distractors are constructed using specially designed templates and annotations from section 3.2. For open-ended questions, we designed the following pipeline to construct questions, answers, and distractors. These tasks include Human Emotion Recognition, Emotion Temporal Analysis, Emotion Intensity Comparison, Human-to-Text, Behavior Temporal Analysis, and Behavior Causality Analysis. The Distractor-Included QA Synthesis Pipeline is shown in Figure 3."}, {"title": "3.3.1. Selecting Videos for Generating Questions", "content": "For a specific task, it is first necessary to filter annotated videos to select those suitable for question generation. This can be done based on video length and the number of people in the video. For example, the Behavior Temporal Analysis task may require longer videos than other tasks, and Human-to-Text task requires videos with at least two people. After filtering by these criteria, a subset of videos suitable for question generation is obtained."}, {"title": "3.3.2. Synthesizing Questions and Preliminary Answers", "content": "In this part, the pipeline will synthesize questions and initial answers based on the video. Specifically, the video is processed using the bounding box track information of the target person. For the Human Emotion Recognition and Emotion Temporal Analysis tasks, which focus on facial expressions, we reconstruct the video with cropped face regions. For other tasks, we add a red bounding box to highlight the person in the original video, creating \u201cmarked videos\u201d. Then, for specific tasks, we design prompts for targeted captioning with Video-MLLM. We find that video-MLLMs don't always focus correctly on the person highlighted by the bounding box. Therefore, to further enforce the model's attention on the target person, we add a visual cue for the target person, which is the person's appearance in Label-4 of Figure 2.\nAfter acquiring task-specific captions, we can further analyze the distribution of caption attributes (such as the sentiment distribution in emotion recognition task) and filter"}, {"title": "3.3.3. Answer Optimization and Distractors Generation", "content": "To obtain the most suitable answer to the question, we use three Video-MLLMs models to optimize the answer while generating three distractors. Specifically, for each video-MLLM, the model first answers based on the video and the question, resulting in candidate answers. Then use the same model to select a better answer between the candidate answers and the original optimal answer based on the question and video, updating the optimal answer. The discarded answers serve as raw material for generating distractors. This process is repeated three times, yielding one optimal answer as the correct option and three discarded answers. The three discarded answers need further differentiation from"}, {"title": "3.3.4. Manual Verification and Correction", "content": "Given the limitations of the annotation model and the models used for question-answer generation, errors in the multiple-choice questions may occur. Therefore, we perform manual validation at the end. Specifically, we shuffle the options of the generated multiple-choice question and ask verifiers to select the best answer. If the verifiers' answers match the one automatically generated by the pipeline, we confirm that the question aligns with human cognition. If they do not match, we re-check and decide whether to change the correct answer option. If no best answer is found, we ask the verifier to write down the correct answer, which becomes the true correct answer for the question. Although the dataset generation process still requires some manual work, it is much less challenging and costly compared to starting from scratch, as the questions and answers have already been generated.\nThe Distractor-Included QA Synthesis Pipeline automates the tedious process of creating questions and options, transforming the role of humans from question creators to"}, {"title": "4. Evaluation and Insights", "content": "We meticulously select 15 SOTA open-source video MLLMs. These included both visual-only models such as Chat-UniVi [19], CogVLM2-video [15], LLaVA-One-Vision [22], PLLaVA-7b [47], ShareGPT4Video [7], Otter-V [21], VILA [29], and VideoChat [23], and audio-visual models capable of analyzing both visual and audio inputs, such as the Video-LLaMA series [10, 51] and generalist MLLMs like ImageBind-LLM [13], ChatBridge [54], and OneLLM [14]. We also evaluated commercial models GPT-4o [35] and Gemini-1.5-Pro [41]. All QAs were framed as multiple-choice questions (N choose 1, with N varying across different test samples), reporting both accuracy and the performance of random guesses and graduate-level humans for reference. More implementation details can be found in the Appendix."}, {"title": "4.1. Experimental Settings", "content": "quality inspectors. This ensures the quality of the questions while significantly reducing labor costs. Detailed construction methods for all questions are provided in the appendix.\nMoreover, in both the Human-Centric Video Annotation Pipeline and the Distractor-Included QA Generation Pipeline, all the models we use are based on the most advanced open-source models available. As more powerful and specialized models emerge, updating our pipeline with these models can further improve the quality of annotations and multiple-choice question generation."}, {"title": "4.2. Main Results", "content": "Table 1 summarizes the performance evaluation results for our benchmark, revealing several key insights:"}, {"title": "Performance of Open-Source Video-MLLMs:", "content": "There is still significant room for improvement, especially in speech-visual alignment tasks that humans excel at, which current open-source audio-visual MLLMs handle at nearly random levels. Notably, LLaVA-One-Vision emerged as the leader in visual recognition tasks but achieved a modest mean accuracy of 50.0% across 13 tasks, significantly lower than human comprehension benchmarks (86.1%). For Cross-Modal Speech-Visual Alignment, ChatBridge led its peers but achieved only 33.4% accuracy, far behind its human counterparts, highlighting the current neglect of cross-modal alignment by open-source audio-video MLLMs."}, {"title": "Performance of Proprietary MLLMs:", "content": "These models markedly outperform their open-source counterparts, achieving near-human proficiency on several tasks. Notably, Gemini achieved an average gap of just 15.2% to human results and performed well in four human-centric tasks: Human Emotion Perception, Person Recognition, Human Behavior Analysis, and Cross-Modal Speech-Visual Alignment."}, {"title": "Challenges with Time-Specific Tasks:", "content": "Many open-source video-MLLMs perform only marginally better than random chance, or even worse, on tasks requiring temporal-specific understanding. Tasks like Action at Specific Time, Time of Specific Action, Appearance Time Detection, and Audio-Visual Alignment Detection highlight deficiencies in accurately determining \u201cvideo scene - event time\" correlations, either bidirectionally or unidirectionally.\""}, {"title": "Cross-Modal Speech-Visual Alignment:", "content": "Contemporary open-source models largely neglect intricate speech-to-lip movement alignment. Few have explicit architectural encoding linking audio and video modalities to facilitate such synergy, compounded by a scarcity of comprehensive datasets proposing coherent visual-audio lexical mapping. Consequently, these models struggle acutely in correlating speech to lip movements, though dedicated ASR models effectively address Speech Content Matching a capability absent in tested open-source video-MLLMs."}, {"title": "4.3. Discussion on Speaker Emotion Recognition", "content": "A closer examination of these models reveals a tendency to incorrectly attribute speaker emotions as \"surprise\" or \"shock\". This issue stems from frame sampling processes used by most models, where speaker frames depicting"}, {"title": "4.4. Class-Wise Analysis of Emotion Detection", "content": "For the Multi-modal Emotion Recognition in Conversation (MMERC) task, we visualized categorical emotion detection accuracy for zero-shot video emotion recognition settings, as shown in Figure 5.(a). Different MLLMs display variability in their effectiveness at recognizing diverse emotion types, with \"neutral\" emotions consistently yielding the highest accuracy. On average, these MLLMs show lower accuracy in identifying negative emotions such as \"anger,\" \"disgust,\u201d \u201csadness,\u201d and \u201cfear\u201d compared to \u201cneutral,\u201d \u201cjoy,\u201d and \u201csurprise.\u201d This suggests that MLLMs may be better at recognizing neutral or positive emotions or are more inclined to select neutral or positive emotions as their answers. Furthermore, as illustrated in Figure 5.(b), these MLLMs show varying capabilities in handling emotion polarities in the Human Emotion Recognition task, exhibiting higher precision for positive emotions, while neutral and negative emotional identification aligns closely in accuracy."}, {"title": "4.5. Availability of Question Generation", "content": "The generation pipeline's efficacy for multiple-choice questions across six descriptive categories is visualized in Figure 6. On average, 30% aligned directly with human responses, 20% included correct options conflicting with automated answers, and 50% required manual verification. While video-MLLMs and LLMs currently exhibit limitations in question generation, prospective advancements promise to enhance question accuracy, minimizing human intervention. Moreover, utilizing flawed video-MLLMs to craft misleading options augments evaluative complexity, thus refining model discriminative testing rigor."}, {"title": "5. Conclusion", "content": "We present HUMANVBENCH to address the pressing need for improved assessment of human-centric video understanding in MLLMs. By incorporating extensive evaluation dimensions through 17 fine-grained tasks, HUMANVBENCH provides a systemic view into both successes and critical shortcomings of video MLLMs, particularly in emotion perception, human behavior analysis, and speech-visual alignment. Experimental findings across over ten"}, {"title": "6. Overview", "content": "In the appendix, we first provide additional evaluation details in Section 7, followed by the detailed definition and examples for the 17 tasks of HUMANVBENCH in Section 8. Then, we present implementation specifics of each operator for the proposed Human-Centric Annotation Pipeline, illustrated by an example of the annotation process in Section 9. Finally, we introduce the construction details of all tasks and the work of the human annotators in Section 10."}, {"title": "7. Model Evaluation Implementation", "content": "Prompt. In order to facilitate the statistical model to answer the results, following common practices used in MLLM evaluations [12, 18], we adopt the following prompt to guide the MLLM to output option letters: \"Select the best answer to the following multiple-choice question based on the video. Respond with only the letter of the correct option. <Question-choices> Only answer best answer's option letter. Your option is: \".\nEvaluation Environments. All evaluation experiments for open-source models were conducted on a single NVIDIA L20 GPU with an inference batch size of 1.\nBaseline Configurations and Runtime Statistics. Table 3 shows the scale, parameter settings, and costs (including memory usage and end-to-end testing time) for each model on HUMANVBENCH. The testing is repeated five times with different random seeds, and the results for all open-source models in Table 3 are based on the averages of these five runs. All hyperparameter settings follow the default configurations of these open-source works."}, {"title": "8. Definitions and Examples for Each Task", "content": "Emotion Recognition aims to judge the overall emotional state of the person highlighted by a red bounding box in the video. An example is shown in Figure 7.\nEmotion Temporal Analysis involves analyzing the changes in the emotions of the people highlighted with the red bounding box over time, identifying gradual intensification, diminishment, emotions shifts to test the model's ability to track emotional dynamics. An example is shown in Figure 8."}, {"title": "9. Annotations Details and Examples in Human-Centric Annotation Pipeline", "content": "For the in-the-wild videos collected from Pexels, we first apply splitting and filtering operations. Specifically, we begin by utilizing the video_resolution_filter, video_aesthetics_filter, and video_nsfw_filter operators to select videos that meet the following criteria: a resolution of at least 1280 in width and 480 in height, acceptable aesthetics, and appropriate content. Next, the video_split_by_scene_mapper is used to split the videos into scenes. The resulting clips are then filtered using the video_duration_filter to exclude clips shorter than 1 second and the video_motion_score_filter to remove static videos. These steps utilize existing operators in Data-Juicer, with parameters set to their default values except for the video motion_score_filter, where the minimum motion score is set to 1.2. After completing these foundational steps, we apply the video_face_ratio_filter with a threshold of 0.65 to retain videos containing people. These videos are then processed using a series of mappers to generate fine-grained, multi-modal, human-related annotations.\nWe use a video example to demonstrate the annotation process and results, as shown in Figure 24. Below, we detail the models and settings used for each operator.\nFor the video_human_tracks_extraction_mapper, we follow the approach of Light_ASD [27], utilizing S3FD [52] as the face detector. A face bounding box is added to a human track if its overlap rate exceeds 50%. After obtaining the face track, we identify the corresponding body bounding box for each face bounding box in the same frame to generate a second bounding box track for the individual, referred to as the body track. The matching criterion selects the candidate bounding box with the smallest horizontal center distance and a smaller area. This process can be expressed by the following formula:\nclosest_bbox = arg \\min \\limits_{\\text{bboxcandidate_bboxes}} \\frac{\\left(\\frac{f_x1 + f_x2}{2} - \\frac{x_1 + x_2}{2}\\right)^2 + (x_2 - x_1)(y_2 - y_1)}{2} \\text{(1)}\nwhere \\(f_{x1}, f_{x2}\\) are the left and right boundaries of the face bounding box, the candidate human bounding boxes are obtained using YOLOv8-human, \\(x_1, x_2, y_1, y_2\\) are the boundary values of a candidate human bounding box. If no bounding box meets the criteria, the frame is skipped, and detection proceeds with the other frames. Finally, the empty elements in the body track are replaced with the average of the bounding boxes from the surrounding frames.\nIn the human_demographics_mapper, we use DeepFace to perform frame-level detection of facial gender, age, and race. The analysis is conducted on cropped frames obtained directly from the video_human_tracks_extraction_mapper results. Finally, for a given face track, the demographics features are determined by taking the mode of the frame-level gender and ethnicity detections, and the median of the age detections.\nIn the video_human_description_mapper, we use the body bounding box track to crop the video, creating a reconstructed video focused on a single individual. This reconstructed video is then processed using ShareGPT4Video [7] for appearance description and simple actions.\nIn the video_facial_description mapper, we use the face bounding box track to crop the video, creating face-focused reconstructed videos for emotion description using VideoLLaMA2.1 [10]. The choice of VideoLLaMA2.1 is based on a comparative analysis of multiple models, which revealed that VideoLLaMA2.1 is often more effective at identifying negative emotions. This capability is particularly important for adjusting emotion distribution before designing emotion recognition tasks."}, {"title": "10. Complete Construction Details of All Tasks", "content": "We will first explain the details of six descriptive questions generated using the Distractor-Included QA Generation Pipeline, followed by the construction details of the remaining tasks."}, {"title": "10.1. Construction Details of 6 Descriptive Human-Centric Questions", "content": "For these six tasks, the video-MLLM used to obtain task-oriented captions is VideoLLaMA-2.1[10]. The LLM used for generating question and initial answer pairs is Qwen2.5[48]. The three video-MLLMs employed for optimizing answers and producing raw distractors are VideoLLaMA-2.1[10], CogVLM2[15], and LLaVA-OneVision[22], respectively. The LLM responsible for generating distractors is Qwen2.5. We first present the general instruction templates in the six task generation processes.\nThe prompt template for the three Video-MLLMs used to refine answers and generate raw distractors is:\nPlease focus on the people whose heads are highlighted"}, {"title": "10.2. Construction Details of 11 Closed-Ended Human-Centric Questions", "content": "Attitude Recognition: This task is constructed based on the first half of the Distractor-Included QA Synthesis Pipeline. The human who appears in the most frames is selected as the target for question generation. The target individual is highlighted in the video using the face bounding box track from Label-1. Based on the annotated video, appearance cues of the target individual (i.e., Label-4) are added to the prompt to help the model focus on the intended person. The prompt used to obtain the task-specific caption is:\nFocus on the person highlighted by the red bounding box ((appearance)) and tell me: Do the highlighted people display certain attitudes toward specific objects and events? What kind of attitude is it?\nThe prompt used for question generation is:"}, {"title": "10.3. Details of Human Efforts for HUMANVBENCH", "content": "We employed two professional full-time AI data annotators and invited two graduated-level volunteers to participate in the construction and evaluation of HUMANVBENCH. They collaborate to complete a series of tasks, with two main annotators participating in the full data of each task and two volunteers participating in the sampled data. Inconsistencies will be identified and resolved (for example, through discussion or majority voting to reach a consensus) to ensure high quality. Specifically, their tasks include the following four parts.\nHuman Annotation on Generated QAs: Tasks requiring human annotations to generate questions and options include Human Counting, Action at Specified Time, and Time of Specific Action. The latter two tasks can streamline annotation by annotating a single dataset containing \u201cspecial"}]}