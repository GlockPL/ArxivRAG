{"title": "Some Issues in Predictive Ethics Modeling: An Annotated Contrast Set of \u201cMoral Stories\u201d", "authors": ["Ben Fitzgerald"], "abstract": "Models like the Allen Institute's Delphi have been able to label ethical dilemmas as moral or immoral with astonishing accuracy. This paper challenges accuracy as a holistic metric for ethics modeling by identifying issues with translating moral dilemmas into text-based input. It demonstrates these issues with contrast sets that substantially reduce the performance of classifiers trained on the dataset Moral Stories. Ultimately, we obtain concrete estimates for how much a given form of data misrepresentation harms classifier accuracy. Specifically, label-changing tweaks to a situation's descriptive content (as small as 3-5 words) can reduce classifier accuracy to as low as 51%, almost half the initial accuracy of 99.8%. Associating situations with a misleading social norm lowers accuracy to 98.8%, while adding textual bias (i.e. an implication that a situation already fits a certain label) lowers accuracy to 77%. We conclude by making recommendations to correct these challenges.", "sections": [{"title": "1. Introduction", "content": "Models like the Allen Institute's Delphi have been able to label ethical dilemmas as moral or immoral with astonishing accuracy. Several news outlets recognized Delphi's revolution in predictive ethics modeling: the New York Times's ambiguous review, for example, claimed that \"the system was surprisingly wise.\u201d The creators of Moral Stories, the dataset this paper explores, trained a model with the stunning accuracy of 99.8%.\nThese accuracy rates suggest that our ethics models grasp the totality of human morality. Have we really \u201csolved\u201d the problem of ethics?\nThis paper argues to the contrary. In opening this conversation, I find it helpful to bring up Aristotle's time-tested framework for ethics modeling: the practical syllogism. According to him, an ethical deliberation has two intellectual components:\n1. Inducing a universal premise about some category of object C. (e.g. \u201cunhealthy foods should be avoided\")\n2. Describing a particular ethical dilemma with a particular premise, which instantiates some particular object x as an instance of C. (e.g. \u201cthis food is unhealthy.\u201d)\nThe result is that predicate logic produces the conclusion: \u201cthis food should be avoided.\u201d Equally worth noting is that each step is distinct enough, and intricate enough, for Aristotle to segment them into separate disciplines. We can excel in theoretical wisdom (the creation of top-down moral frameworks) while struggling with practical wisdom (the soldering of these frameworks to practical situations).\nI propose that this description characterizes the discipline of ethics modeling. High classifier accuracy suggests theoretical wisdom\u2013that, given a certain input, a model reaches a"}, {"title": "2. Background", "content": "Predictive ethics modeling seized the public eye less than two years after the publication of its first concrete papers. On the eve of 2020, limitations in capacity research, and our ability to evaluate a model's grasp of ethics, still made training models to mimic human ethics \u201can outstanding challenge without any concrete proposal\" (Talat et al, 2022). This changed with the rapid-fire creation of five main ethics datasets: Social Chemistry (Forbes et. al, 2020), ETHICS: Commonsense Morality (Hendrycks et al. 2020), Social Bias Influence Corpus (Sap et al. 2020), RAINBOW (Lourie et al, 2021), and Moral Stories (Emelin et. al, 2021). Over 1,000 papers, in total, have been published about these datasets in less than four years.\nPublic interest peaked when researchers from the Allen Institute of AI merged these datasets into the \u201cCommonsense Norm Bank:\u201d over 1.6 million data points labeling social norms as either positive or negative. Researchers combined the broader \"rules of thumb\u201d (RoTs) from SocialChem with summaries of the particular, in-depth scenarios from ETHICS and Moral Stories and the conversations in SBIC. The model UNICORN, trained on RAINBOW, was trained on this dataset. The result was DELPHI: a classification model with a staggering level of accuracy. AI2's initial paper boasted a 93% accuracy rate over the test set, as averaged across binary classification and \u201cfree form,\u201d text-based responses. Binary classification in particular achieved a stunning 98.1% accuracy rate, 4.3% higher than human annotators' own predictions.\nThe influx of publicity renewed debates about whether ethics modeling was an effective approach to Al alignment. Talat et al. (2021) criticized Delphi's creators for misunderstanding ethics as a static set of benchmarks. Ethical systems are a \u201ccomplex social and cultural achievement\" in that they are \u201ccontinuously formed and negotiated through debate and dissent from previously accepted norms and values.\u201d By contrast, ethics models can only (at best) articulate a sub-population's ethical views during an isolated time-frame, meaning that they ignore how the continuous evolution of culture characterizes moral philosophy. As a result, Delphi presents itself deceptively by outputting objective-seeming, prescriptive judgments (e.g. \u201cyou should do your homework\") rather than descriptions of a small slice of a culture's moral understanding (e.g. \u201cthese annotators think doing homework is good\").\nResearch also criticized the defeasibility of Delphi's understanding of social norms. Rudinger et al. (2020)'s classifiers of defeasible reasoning targeted the RoTs in Social Chem as \"hypotheses to be weakened or strengthened\". Doing so challenged the effectiveness of the"}, {"title": "3. Designing a contrast set", "content": "Gardner et al. (2020) explain that perturbations should be minimal, but ideally label-changing, to evaluate how the decision boundary delineates a small region of vector space. In other words, contrast sets should contain the data points adjacent to those in training sets. This sharpens our picture of the decision boundary by revealing the specific axes on which a datapoint is classified. More importantly, they can reveal systematic gaps: categories of input data with which a model is unfamiliar.\nMoral stories is a dataset of ethical situations designed to show the impact of social norms, context, and consequences on ethical prediction. Annotators hired through mTurk created \u201cmoral stories\u201d from a random social norm in Forbes et al.'s Social Chemistry dataset. Each row contains data on norm, context, action, and consequence in six rows. Using this data, Emelin et al. proved that adding social norms, context, and consequences to a description of an action increases accuracy from 84% to 99%.\nI chose to examine Moral Stories because it implements a relative degree of non-monotonic reasoning for classification tasks. ETHICS and SCIB express attributes about each situation as scalars, but a contrast set on an NLP model cannot perturb these. Social Chemistry attaches a one sentence description of the \u201csituation\u201d to each norm, but this approach lacks the multiple categories of context Moral Stories offers. Even sets like SCRUPLES, which contain paragraphs of context, do not isolate different types of context among columns, making it hard to isolate specific changes to each situation. Ziems et al. (2023)'s model of non-monotonic reasoning solves both these concerns, and is admittedly more recent, but its emphasis on social norms rather than ethical behavior strays too far from the discussion of alignment.\nA final note: Gardner's team is clear that developing a contrast set requires substantial expertise about a dataset. The best-case scenario is the dataset's creators designing one themselves. Since my last name isn't Emelin, I lack a complete topography of Moral Stories's systematic gaps. Focusing my contrast set around the first 333 rows of Moral Stories remedies this issue. This smaller range follows Gardner et al. (2020)'s suggestion to perform multiple kinds of augmentation on each datapoint, while sticking within his suggested range of 1,000 total data points. Further, redoing four contrast sets on the same subset will sharpen my understanding of the types of ethical dilemmas Moral Stories describes."}, {"title": "3.1: Norm Swaps", "content": "Emelin et. al (2021) report that coupling actions with a social norm increases classifier accuracy from 84% to 92%. This suggests that using an irrelevant, or intentionally misleading, social norm can restore or reduce the classifier's baseline accuracy.\nI address both scenarios. To test how irrelevant norms impact accuracy, I replace each input's \"norm\u201d column with a social norm related to the ethical dilemma that does not attempt to influence its output. To test how intentionally misleading norms impact accuracy, I manually re-label social norms to falsely suggest a different label.\nCertain norms attempt to mislead the classifier by tweaking our framing of the situation. This mimics how we approach moral dilemmas in everyday life. It is quite probable, for instance, that a child whose mother told him that \u201cIt's rude to insult people out of anger\u201d might reply that \"it's important to express when others have angered you.\u201d Other revisions simply flip each norm's judgment of an action. This subset screens a model's response to direct contradictions of its training data.\nImportant to note is that this section does not flip the dataset's labels. This is because each scenario in Emelin et. al (2021) is trained with four combinations of outcomes: moral action & moral consequence, moral action & immoral consequence, immoral action & moral consequence, and immoral action & immoral consequence. Since each altered norm is tested against all these label configurations, swapping the labels has no functional effect. Moreover, this section attempts to test whether a classifier can discard misleading social norms and label an action the same as in training data, rather than use the new norms to flip its usual classification. Social norms themselves do not impact a situation's label, so this test set does not flip labels."}, {"title": "3.2: Textual Bias", "content": "This set addresses text's fallibility as a medium for carrying moral judgments. The difference between Emelin's data and our contrast set shows how easily neutral descriptions of a scenario can become positively or negatively charged, to the point where even a human might label them differently. To ensure the textual neutrality of my original data set, I used Emelin et. al (2021)'s textual bias training split. Further, I divide data into those directly (e.g. \u201cSarah behaved morally by x\") and indirectly determining a situation's morality (e.g. the implication that Sarah behaved morally by x).\nAn important paradigm was adjusting the normativity of each datum's syntax without touching its conceptual content, the semantics, of the action it denotes. This reinvites the debate about whether syntax and semantics are truly separable. It can be argued that \u201cthrowing away children's clothes\u201d and \u201cthrowing away clothes that could have gone to impoverished children\u201d project separate universes, where the latter has concrete differences that place more emphasis on\""}, {"title": "3.3: Descriptive Shifts", "content": "This contrast set examines how label-changing, descriptive tweaks to an ethical scenario affect a classifier's accuracy. I implemented defeasible reasoning to alter the scenario's setting, behavior, occupation, or object whenever possible by employing Ziems et al. (2023)'s database of social norm components, which gave a fair, random distribution of components. However, Emelin et al.'s examples were often too reliant on certain contexts to be shifted randomly. In these cases, I altered situational details like time, demonstrative pronouns, and other situational components to tailor my changes to the particular example. My changes used as few words as possible and swapped each datum's labels, per Gardner's recommendations."}, {"title": "3.4: Weighing gender, sexual, and ethnic bias", "content": "It is a well-documented phenomenon that classifiers are often less accurate for gender, sexual, ethnic, and racial minorities (Christian, 2020). This contrast set tests how shifting demographics impacts both. To test for sexist bias, I programmatically prompted an instance of ChatGPT to swap all genders in each datum. To test for racist/ethnic bias, I prompted ChatGPT to replace every name in the scenario with a name resembling x, where x was a name randomly taken from New York City's Popular Baby Names dataset, for all races except Caucasian. If Moral Stories encodes sexist or racist bias, I predict a lower accuracy, and a higher false positive rate, for this test set."}, {"title": "4: Results", "content": "Immediately, some interesting data points stand out. Inferences on our contrast sets are 15.81% less accurate than on Emelin et al's test. This does not even consider test sets like Norm Swap and Demographics, whose effect turned out to be negligible. 83.74% accuracy may seem acceptable, but Al alignment specialists warn that any alignment solution must be airtight. If applied to a dataset of 20,000 (the length of Emelin et. al's training set), our model would output an estimated 3,252 misaligned predictions, as opposed to the 90 Emelin first predicted. Consider the disastrous impact a single misaligned prediction could cause, and realize how much further ethics modeling is from its goal than we thought.\nThe most impactful augmentation technique was tweaking a story's descriptive content. Accuracy plummeted to 51.58%, with its largest subcategory-changes to the behavior a situation describes-mirroring this shift at 51.4%. Of particular note were some of my subtler changes: changes to the object affected by each action, and the tweaks in wording too insignificant to warrant their own category.\nI hypothesize that these categories are uniquely pernicious because they swap out the nouns that a social norm sees as interchangeable. Consider the entry object, label changed_98, where I swapped out cigarettes for candy cigarettes as the \"gifts\" a child gave her classmates. It would take a complex level of semantic understanding to realize that normal cigarettes relate to the social norm \u201cYou should not pressure someone to start smoking\u201d while candy cigarettes do not, especially when I left the syntax for both entries almost completely unchanged. Similarly, my replica of Emelin's model failed to capture the nuance of changing \u201cgaining weight\" to \"losing weight\" when evaluating the wording set. Failing to grasp this small degree of nuance could change how, for instance, one handles a partner's health issue.\nChanges to setting, behavior, and occupation may have inhibited accuracy less because Moral Stories conditions models to situate each scenario within a concrete setting. Observe the sentence: \u201cBen is meeting his new boss at the boxing gym for his first day of work.\u201d Not only is this information presented first, but setting, behavior, and occupation occupy their own prepositional phrase. This is opposed to later on, when \u201cBen greets the man wearing a tank top and boxing shorts.\u201d The object (the tank top and boxer shorts) goes more easily concealed since it supplements another behavior, rather than appearing by itself, and is not included in the situation column during training. This suggests that our model may be weighing settings, behaviors, and occupations more highly than object and wording because it was trained to isolate these as a situation's most salient descriptors. Re-training the model, then, with more adversarial perturbations of object and wording would improve performance by impacting how these factors are weighed.\nAnother alternative is that certain social norms embrace situational context better than"}, {"title": "5. Re-imagining Ethics Modeling", "content": "Our results show that predictive ethics modeling has significantly advanced in recent years. However, it is still incapable of addressing the nuances needed for an airtight solution to AI misalignment. There is clearly a gap between what we have and what we are aiming for. What will ethics models look like once we close that gap?\nOur literature review has already shown us that regulating human behavior with predictive ethics models is unwise. On top of shifting responsibilities away from human decision makers, ceding our ethical deliberation to predictive models ignores the intrapersonal, communal, and cultural deliberation animating ethics. Any ethical classifier will be, at best, a simulacrum of that richness of deliberation, a screenshot of the minds of a few dozen annotators at the precise moments, and on the precise ethical data points, that went into building training data. Replacing authentic ethical deliberation with its bastard child contradicts the point of ethics as a cultural project.\nHowever, this imperfect simulation of ethical deliberation may be humanity's least imperfect solution to translating our values to a general AI. Ethics models could supplement the reward models used for reinforcement learning, or assist human annotators in reinforcement learning with human feedback. Or, GAI models can be directly trained on refined versions of ethics datasets like Moral Stories. Either way, an important principle emerges. GAI cannot be capabilities-first systems throttled by ethical protections: every aligned GAI must be just as much an ethics model as it is a generalized AI. We have seen that ethical protections are easily dodged if they conflict with an AI's meta-objectives. The only way to ensure a model follows our sketch of human ethics is to encode it as a top-level goal.\nOur work on Moral Stories offers three ways to make this model a reality."}, {"title": "5.1: Reconsidering the structure of a social norm.", "content": "Our contrast set used defeasible reasoning to target norms that presented themselves as universal. If they were universal, a sentence would be a perfectly valid medium, needing no context to outline the cases where the norm might not apply. However, the relevance of context suggests that social norms are non-monotonic, rather than illusory universal premises that lose their explanatory power with every counter-example. Counter-examples do not degrade, but crystallize, non-monotonic social norms, since they clarify the contextual strains at whose intersection a social norm resides.\nAs such, a social norm's structure inherently contains a tabular list of conditions that constrict its domain. It presents a behavior that becomes acceptable in S setting, in R role, and with C situational constraints. Any ethics model, therefore, cannot use the sentence-length norms used by Forbes et al. and Emelin et al., but must employ tabular norms encoding setting, context, characters, behavioral constraints, etc."}, {"title": "5.2: Honing defeasible reasoning", "content": "Recent approaches to predictive ethics modeling emphasize the shift from defeasible to non-monotonic reasoning. However, defeasible reasoning has a place guiding an ethical classifier's user, whether human or AI, to add context to a more general description. ClarifyDelphi takes exactly this approach with promising success. Within our newly-revised view of social norms, a model like ClarifyDelphi would learn to ask questions that reveal the most pertinent contextual information, process that information in a way similar to a model trained on Emelin et al (2021), then make a decision, using a capacity for contextualized moral reasoning trained on a body of non-monotonic social norms."}, {"title": "5.3: Filtering textual bias", "content": "Previous research exists for classifying text as biased. Pivoting that research to classifying the bias of moral scenario descriptors prevents the alignment issues I cited above. A"}, {"title": "5.4: Weighing gender, sexual, and ethnic bias", "content": "Research should continue into addressing discrimination in ethics modeling. Our work found no gender, sexual, or ethnic discrimination, but prior work suggests that this discrimination exists. Even one erroneous classification based on demographics could ruin a person's life and exacerbate existing systemic oppression.\nCulture's role as an arbiter of social norms can be addressed in several ways. A model can have ethical classifiers handling reward modeling, each of which is trained on data from a different culture. Even better, tabular social norms could specify an agent's cultural background, prompting it to change the system of ethics by which it makes its judgments. Doing so allows the model to dynamically switch between ethics systems, much like Hendrycks et al, 2020. This lets foreigners in other cultures, for instance, receive predictions consistent with their own culture."}, {"title": "6. Conclusion", "content": "This paper underwent a literature review of contemporary issues in ethics modeling. It tested these issues with a contrast set of the Moral Stories dataset, as created by Emelin et. al (2021). This produced some of the first concrete estimates for the impact of specific data representation errors on classifier accuracy. Our contrast set performed 15.81% worse than Emelin's original test set (initial accuracy = 99.55%), achieving 98.2% accuracy on perturbations to social norms, 71.6% on overt textual bias, and 51.5% on defeasible context shifts. It then suggests ways to remedy these considerations in future models. In particular, we recommend moving beyond structuring social norms with text-based input by considering the non-monotonicity of social norms; incorporating bias screening into input filtering; actively accounting for the culture informing a user's values; and improving defeasible reasoning by automatically prompting for context.\nThese reforms promise to attack the problem at its source. Theoretically, ethics modeling is wise; practically, ethics modeling is foolish. Truly aligning an ethics model will take more than representing platitudes in training sets\u2014it will take a concrete re-examination of everything we know about the morality we purport."}]}