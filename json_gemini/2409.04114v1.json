{"title": "MULTI-PROGRAMMING LANGUAGE ENSEMBLE FOR CODE GENERATION IN LARGE LANGUAGE MODEL", "authors": ["Tengfei Xue", "Xuefeng Li", "Tahir Azim", "Roman Smirnov", "Jianhui Yu", "Arash Sadrieh", "Babak Pahlavan"], "abstract": "Large language models (LLMs) have significantly improved code generation, particularly in one-pass code generation. However, most existing approaches focus solely on generating code in a single programming language, overlooking the potential of leveraging the multi-language capabilities of LLMs. LLMs have varying patterns of errors across different languages, suggesting that a more robust approach could be developed by leveraging these multi-language outputs. In this study, we propose Multi-Programming Language Ensemble (MPLE), a novel ensemble-based method that utilizes code generation across multiple programming languages to enhance overall performance. By treating each language-specific code generation process as an individual \u201cweak expert\" and effectively integrating their outputs, our method mitigates language-specific errors and biases. This multi-language ensemble strategy leverages the complementary strengths of different programming languages, enabling the model to produce more accurate and robust code. Our approach can be seamlessly integrated with commonly used techniques such as the reflection algorithm and Monte Carlo tree search to improve code generation quality further. Experimental results show that our framework consistently enhances baseline performance by up to 17.92% on existing benchmarks (HumanEval and HumanEval-plus), with a standout result of 96.25% accuracy on the HumanEval benchmark, achieving new state-of-the-art results across various LLM models. The code will be released at https://github.com/NinjaTech- AI/MPLE", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have significantly advanced the field of code generation, demonstrating impressive capabilities in generating syntactically correct and semantically meaningful code across various programming languages (Chen et al., 2021; Li et al., 2022; Austin et al., 2021; Liu et al., 2024). Recent progress has been marked by the ability of these models, such as GPT 4 (Achiam et al., 2023), Llama 3 (Dubey et al., 2024), and Claude 3 (Anthropic, 2024), to produce high-quality code snippets from natural language descriptions, often excelling in specific languages like Python or Java (Li et al., 2023; Roziere et al., 2023; Zhong et al., 2024; Huang et al., 2023; Islam et al., 2024). However, the majority of existing approaches in code generation have primarily focused on a single programming language, neglecting the potential advantages of leveraging multi-language capabilities to enhance the robustness and accuracy of generated code.\nLLMs exhibit varying error patterns across different programming languages due to differences in syntax, semantics, and idiomatic practices (Peng et al., 2024; Zheng et al., 2023; Athiwaratkun et al., 2023; Cassano et al., 2022). For example, an LLM may perform well in Python code generation but generate errors in Java or C++ due to differences in error handling or library usage. These variations indicate that LLMs have language-specific biases, which could be mitigated through a more robust, multi-language approach. By leveraging outputs generated across different programming languages, it is possible to reduce these biases and improve the overall performance of code generation.\nIn this study, we introduce Multi-Programming Language Ensemble (MPLE), a novel ensemble-based method for code generation that harnesses the multi-language capabilities of LLMs. Inspired"}, {"title": "METHODOLOGY", "content": "In this section, we present our Multi-Programming Language Ensemble (MPLE) framework (Fig. 1) for code generation in Large Language Models (LLMs). This approach iteratively refines code by leveraging the strengths of different programming languages, reducing language-specific errors and biases. We integrate this method with reflection algorithms and MCTS to enhance the overall robust- ness and accuracy of the generated code. The following subsections provide a detailed description of the methodology."}, {"title": "PROBLEM FORMULATION", "content": "We follow the problem formulation used in Zhong et al. (2024). We formulate the code generation task as follows: Each sample can be represented as a triplet (Q, Tv, Th), where Q is the code task description, Tv represents the visible test cases, and Th denotes the hidden test cases. At the outset, the LLM is provided with Q and Tv to generate an initial program, Po. The generated program Po is then refined iteratively to produce a sequence of programs {P1, P2, . . . , Pn } until a program passes all the visible tests in Tv. The final output program is denoted as P*. This final program, P*, is then evaluated on the hidden test cases Th to verify its correctness. Notably, Th is used only once (pass@1) and remains hidden during the code generation and refinement process."}, {"title": "FRAMEWORK OVERVIEW", "content": "The proposed MPLE framework (Fig. 1) is designed to utilize the multi-language capabilities of LLMs to improve code generation. The process consists of several steps:\n1. Initial Code Generation: The process begins by prompting the LLM to generate an initial code version Po in a primary programming language Lo based on the given task description Q. This generated code Po is then tested against the visible test cases Tv. If Po passes all visible tests, the code generation process is terminated, and Po is further evaluated on the hidden tests Th to determine the final result.\n2. Multi-Language Sampling and Translation: If Po fails to pass all visible test cases, the framework prompts the LLM to generate a new code version PLi in a different programming language Li (e.g., if Po is in Python, PLi could be generated in Java). The generated code PLi is then translated back into the original programming language Lo to produce a refined version Pi. This refined version is designed to maintain the logical structure of the newly generated code while conforming to the syntax and semantics of the primary language.\n3. Iterative Refinement: The refined code version Pi is tested against the visible test cases Tv. If it passes all tests, the process is terminated, and Pi is evaluated on the hidden tests Th. If Pi fails to pass all visible tests, the framework continues by generating an additional code version PLi+1 in another programming language (e.g., C++). The new version PLi+1 is then translated back into the primary language to produce a further refined version Pi+1. This iterative process continues, utilizing different programming languages, until a code version passes all visible tests or the maximum number of languages (Lmax) is reached.\n4. Ensemble Integration: Throughout the iterations, the ensemble framework integrates the strengths of multiple languages to progressively refine the program. By treating each language-specific code generation as an individual \u201cweak expert,\" the framework combines their outputs to mitigate language-specific errors and biases. This approach leverages the unique strengths of different programming languages, such as differences in syntax, semantics, and idiomatic usage, to produce more robust and accurate code. If no version passes all visible tests within Lmax, the last generated version PLmax is evaluated on the hidden tests Th to determine the final result."}, {"title": "INTEGRATION WITH EXISTING TECHNIQUES", "content": "To further enhance the code generation process, our ensemble framework seamlessly integrates with existing techniques such as the reflection algorithm (Shinn et al., 2024) and Monte Carlo Tree Search (MCTS) (Chaslot et al., 2008). These integrations allow for a more dynamic and iterative refinement of the generated code, ultimately improving the robustness and accuracy of the results.\n\u2022 Reflection Algorithm: The reflection algorithm uses feedback from the execution of visible test cases to iteratively refine the code. Our MPLE framework is integrated into the reflection algorithm by utilizing its iterative refinement process. In each iteration, MPLE generates a code version using multiple programming languages. The code is tested against"}, {"title": "EXPERIMENTS", "content": "We evaluate our proposed MPLE framework on two widely recognized code generation bench- marks: HumanEval (Chen et al., 2021) and HumanEval-plus (Liu et al., 2024). These benchmarks assess the capability of large language models (LLMs) to generate functional code based on textual descriptions.\nHumanEval is designed for text-to-code (Python) generation tasks where the input is a brief passage describing the intended functionality of the program to be generated. The output is then evaluated based on its ability to pass unit tests with specified requirements. HumanEval-plus extends the HumanEval dataset by incorporating a large number of additional valid unit test cases to rigorously evaluate the synthesized code's robustness and correctness."}, {"title": "EXPERIMENTAL SETUP", "content": "We compute Pass@1 accuracy using hidden test cases to assess the performance of the generated code. Pass@1 measures the percentage of tasks for which the model's top output passes all hidden test cases, providing a stringent evaluation metric for the models' capability to generate correct code.\nWe conducted experiments using both proprietary and open-source LLMs:"}, {"title": "METHODS EVALUATED", "content": "We evaluated the performance of the following methods:\n1. Baseline: The model is directly prompted to generate code based on the task description without additional strategies. This serves as a benchmark for comparing the effectiveness of more sophisticated approaches.\n2. MPLE: Our proposed method integrates Java and C++ into Python programming, allow- ing the model to utilize multi-language capabilities for code generation. Java and C++ are selected here because LLMs generally have high performance in these programming lan- guages (Peng et al., 2024; Zheng et al., 2023; Athiwaratkun et al., 2023; Cassano et al., 2022). Note that MPLE is able to integrate with any number of programming languages. The ensemble approach aims to improve code accuracy by leveraging the strengths of mul- tiple programming languages.\n3. MPLE+Reflection: This method combines the proposed MPLE strategy with the reflec- tion algorithm (Shinn et al., 2024), enabling iterative self-correction and refinement. The maximum number of iterations is set to 8, providing the model with multiple opportunities to refine its output based on feedback from visible test cases.\n4. MPLE+MCTS: This method integrates the proposed MPLE strategy with MCTS (Chaslot et al., 2008) to explore the search space of possible code solutions more effectively. The MCTS algorithm runs with a maximum of 8 iterations and 5 nodes each iteration, allowing the model to systematically explore different code generation paths and select the most promising ones."}, {"title": "RESULTS", "content": "The performance results of each method on the HumanEval and HumanEval-plus benchmarks are presented in Tables 1 and 2, respectively. The results demonstrate the impact of each method on Pass@1 accuracy."}, {"title": "CONCLUSION", "content": "In this paper, we propose MPLE, a novel multi-programming language ensemble framework for code generation in Large Language Models (LLMs). Our approach leverages the strengths of mul- tiple programming languages to iteratively refine code generation, thereby enhancing the overall performance and robustness of the models. By integrating strategies such as the reflection algorithm and MCTS with our ensemble framework, we demonstrate significant improvements in Pass@1 accuracy across multiple benchmarks, including HumanEval and HumanEval-plus. The experimental results demonstrate that our method consistently outperforms baseline models, which effectively ex- plores optimal code solutions. Our MPLE approach reduces language-specific errors and harnesses the unique strengths of various programming languages, resulting in more accurate and robust code generation. These findings suggest that combining multi-language ensembles with iterative refine- ment is a promising direction for advancing code generation in LLMs. Our framework can be further developed to address more complex coding tasks and diverse programming environments, contribut- ing to the evolution of AI-driven software development.\nFuture work will focus on integrating more efficient token generation strategies (Xue et al., 2024; Kim et al., 2024) and more advanced inference algorithms (Wang et al., 2024) to further enhance code generation. We also plan to evaluate our approach on a broader range of datasets and real-world challenges (Tian et al., 2024; Jimenez et al., 2024) to assess its generalizability. Additionally, we will explore how to effectively deploy our framework in a production environment, ensuring it meets practical performance and reliability requirements. Currently, our NinjaLLM 3.0 (a fine-tuned and quantized version of llama3.1-405b) has achieved promising scores on HumanEval (93.85%) and HumanEval-plus (86.67%), and we are on the path to further improving its performance."}, {"title": "APPENDIX", "content": ""}, {"title": "APPENDIX", "content": ""}]}