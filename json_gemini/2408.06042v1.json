{"title": "Understanding Byzantine Robustness in Federated Learning with A Black-box Server", "authors": ["Fangyuan Zhao", "Yuexiang Xie", "Xuebin Ren", "Bolin Ding", "Shusen Yang", "Yaliang Li"], "abstract": "Federated learning (FL) becomes vulnerable to Byzantine attacks where some of participators tend to damage the utility or discourage the convergence of the learned model via sending their malicious model updates. Previous works propose to apply robust rules to aggregate updates from participators against different types of Byzantine attacks, while at the same time, attackers can further design advanced Byzantine attack algorithms targeting specific aggregation rule when it is known. In practice, FL systems can involve a black-box server that makes the adopted aggregation rule inaccessible to participants, which can naturally defend or weaken some Byzantine attacks. In this paper, we provide an in-depth understanding on the Byzantine robustness of the FL system with a black-box server. Our investigation demonstrates the improved Byzantine robustness of a black-box server employing a dynamic defense strategy. We provide both empirical evidence and theoretical analysis to reveal that the black-box server can mitigate the worst-case attack impact from a maximum level to an expectation level, which is attributed to the inherent inaccessibility and randomness offered by a black-box server. The source code is available\u00b9 to promote further research in the community.", "sections": [{"title": "Introduction", "content": "Federated learning (FL) [4, 15, 28] enables massive clients to collaboratively train a global model without private data exposure, providing a solution to improving the model utility by sharing learned knowledge in a privacy-preserving manner. In an FL system, a server is expected to periodically aggregate the model updates provided by multiple clients, and then to optimize the global model accordingly. The most widely-used FL algorithm Fedavg [21] takes the mean operator in federated aggregation. However, it has been pointed out by previous studies [2, 11, 30] that the global model learned by Fedavg can be vulnerable when some Byzantine clients tend to send their malicious model updates. The intrinsic reason is that Fedavg equally takes model updates provided by all the clients, including the malicious ones, but the mean operation adopted in federated aggregation might not be robust enough against malicious updates.\nRecent studies [3, 7, 39] propose to apply robust aggregation rules for enhancing the Byzantine robustness of FL systems. For example, Krum [3] suggests filtering outliers in the model updates based on Euclidean distances; Median [39] adopts the statistics median operator rather than the mean operator in federated aggregation. These proposed robust rules make remarkable progress in effectively defending some Byzantine attacks that malicious updates can be generated without additional knowledge of aggregation rules adopted at the server, which are called AGR-agnostic attacks in this study.\nMeanwhile, some advanced attack algorithms [11, 30, 31] are designed targeting specific aggregation rules, which might easily disable the defense and severely damage the utility of the learned global model when the aggregation rules are known. These Byzantine attacks, named AGR-adaptive attacks, aim to optimize malicious updates that can both cheat the adopted robust aggregation rules and incur significant negative impact on the model performance. For example, Fang attack [11] scales the inverse direction of the model updates to generate malicious ones for maximizing the inner product between the aggregate deviation and the perturbation direction.\nIn real-world applications, it is reasonable for FL systems to involve a black-box server that makes the adopted robust aggregation rules inaccessible to attackers, which can violate the assumption that AGR-adaptive attacks rely on, i.e., the knowledge of the aggregation rule is accessible. It fits an intuition that using a black-box server can naturally defend or weaken some Byzantine attacks, but there still remains a gap between intuition and an in-depth understanding of the robustness of the FL system with a black-box server.\nTo fulfill such a gap, we conduct the first study on Byzantine robustness of the FL system with a black-box server, providing both theoretical analysis and empirical observations for in-depth under-standing. Specifically, we conduct the study to answer the question: Compared to white-box server settings (i.e., the behaviors are exposed to malicious clients), how and to what extent a black-box server can enhance the Byzantine robustness of an FL system?\nWe set up an attack and defense model to compare the white-box server and the black-box server scenarios, applying static and dynamic defense strategies, respectively. With the knowledge of the robust aggregation rules, Byzantine attacks on the white-box server can always incur the worst-case impact when malicious clients adopt the targeting Byzantine attack algorithms. We point out that such vulnerability mainly comes from unequal positions in the game of attack and defense under the white-box server setting. In contrast, a black-box server can mitigate such vulnerability as the black-box property takes the server to an equal position with the hidden Byzantine clients.\nWe provide theoretical analysis on robustness and convergence for dynamic defense strategies. Accordingly, we further analyze the improvement of Byzantine robustness when employing a black-box server as opposed to a white-box server, showing that a black-box server can reduce the worst-case attack impact from a maximum level to an expectation level. The reduction essentially sources from the fact that a white-box server consistently provides prior knowledge to malicious clients, enabling them to find an optimal attack strategy, whereas a black-box server can effectively prevent the exposure of any useful information about the defense strategy. We conduct a series of experiments on widely-used datasets, showing that a black-box server can significantly enhance the robustness of the FL systems against various Byzantine attacks by adopting the dynamic defense strategy."}, {"title": "Preliminary", "content": "In this section, we present some preliminary information on federated learning and Byzantine ro-bustness, and formulate the considered Byzantine attacks in this study."}, {"title": "Federated Learning", "content": "Let $F(x; D)$ be the objective function to be minimized where D is the un-derlying data distribution and $x \\in \\mathbb{R}^d$ represents the model parameters. For simplicity, we use F(x) to denote $F(x; D)$ subsequently. $f(x;\\xi)$ is an approximation to F(x) under a federated learning setting in which $\\xi \\sim D$ is an observed data distributed among the clients in an FL course. Each client $c_i$ owns a part of dataset $\\xi_i$ and optimizes an objective function $f_i(x; \\xi_i)$ (also denoted as $f_i(x)$ subsequently). The federated learning task can be formulated as an optimization problem as:\n$\\min_x F(x) = f(x; \\xi) = \\frac{1}{n} \\sum_{i=1}^n f_i(x; \\xi_i)$.                                                                                                                                                                               (1)\nFedavg [23] (Algorithm 1 in Appendix A) is the standard FL algorithm for solving the optimization problem defined in Equation (1), which takes the weighted mean operation over all the local updates from clients to approximate the real model update of F(x) on D."}, {"title": "Byzantine Robustness", "content": "We follow the definition of Byzantine robustness in previous studies [1, 3, 12]. Specifically, given an integer value $0 < h < n/2$ and a real value $\\alpha \\geq 0$, assume that"}, {"title": "Byzantine Robustness in FL with A Black-box Server", "content": "With the aim of building an understanding of Byzantine robustness of FL systems with a black-box server, we first set up an attack and defense model. In an FL system, the central server can choose to apply robust aggregation rules from the candidate set $\\mathcal{S} = \\{ AGR_i, i \\in [M] \\}$, where M denotes the total number of candidate aggregation rules. Correspondingly, for each $AGR_i \\in \\mathcal{S}$, malicious clients can perform AGR-adaptive attack $A_i$ targeting $AGR_i$. The attack $A_i$ achieves the highest attack impact on the performance of $AGR_i$ compared to all other attacks."}, {"title": "Attack and Defense Model", "content": "We follow the definition of Byzantine robustness in previous studies [1, 3, 12]. Specifically, given an integer value 0 < h < n/2 and a real value a \u2265 0, assume that"}, {"title": "Static Defense Strategies", "content": "For a white-box server whose aggregation rule is accessible to malicious clients, it is straightforward that whatever robust aggregation rule $AGR_i$ is chosen by the white-box server, malicious clients can easily perform the attack algorithm $A_i$ accordingly. As a result, the attacks incur the robust level of $\\alpha_{i,i}$, which severely hurt the model performance. The vulnerability of the white-box server is essentially from its unequal position compared to hidden malicious clients in the game of defense and attack, in which any defense strategy of the white-box server is known by malicious clients in prior.\nFor a black-box server whose aggregation rule is inaccessible to malicious clients, the straight-forward strategy for malicious clients is to choose an arbitrary attack algorithm from $A_i \\in A$, and verify which one can incur the maximal negative attack impact. In such cases, whatever the strategy taken by malicious clients, the black-box server can perform no worse than the white-box server in terms of the effect of defense, because the black-box property takes the server to an equal position with hidden malicious clients in the game of defense and attack.\nIn a nutshell, FL systems with a black-box server can naturally weaken Byzantine attacks compared to those with a white-box server. However, although the aggregation rule of the black-box server, say $AGR_i$, is inaccessible, it is still possible that the malicious clients successfully find the optimal attack method $A_i$ by conducting multiple trials with different attack algorithms. As a result, using a static defense strategy might cause a risk that the aggregation rules are identified by the malicious clients. Therefore, black-box servers have better use a dynamic strategy to make the defense more unpredictable, which is introduced in detail below."}, {"title": "Dynamic Defense Strategies", "content": "Motivated by the aforementioned insights, we further consider dynamic defense, which combines diverse robust aggregation rules and incorporates randomness into the aggregation procedure to achieve unpredictability of the defense strategy in an FL system. A direct choice to introduce ran-domness is sampling from a set of robust aggregation rules $\\mathcal{S} = \\{ AGR_i, i \\in [M] \\}$. At each training round, the server samples a robust aggregation rule $AGR_i$ from the candidate set $\\mathcal{S}$ to perform aggregation according to a specific probability distribution $\\mathcal{P} = [p_1, ...,p_M]$. Both benign and malicious clients conduct the local training process and upload their updates following the definition in Equation (2).\nThe above sampling based dynamic defense strategy can easily achieve unpredictability, which ben-efits from (i) the randomness of the aggregation rule adopted in each training round, and (ii) the diversity and inaccessibility of the candidate set of aggregation rules for black-box server settings. Therefore, this dynamic defense strategy can serve as a tool to compare the robustness of white-box and black-box server settings. Here, we first build a theoretical foundation for this dynamic defense strategy in terms of both robust level and convergence performance (the detailed proof can be found in the Appendix B)."}, {"title": "Robustness Analysis", "content": "Let $\\mathcal{B} = \\{ B_1,..., B_h \\}$ be an attack which can successfully attack a subset $\\mathcal{S}'$ of $q$ aggregation rules in $\\mathcal{S}$, W.L.O.G., $\\mathcal{S}' = \\{ AGR_j, j \\in [q] \\}$, in the sense that $\\forall AGR_j \\in \\mathcal{S}'$, it holds that $\\langle AGR_j(\\mathcal{V}, \\mathcal{B}), \\mathcal{V} \\rangle \\leq 0$, then the dynamic defense strategy is Byzantine robust if the probability mass on other $M - q$ aggregation rules satisfies that\n$\\sum_{i \\notin [q]} p_i > \\frac{\\sup_{j \\in [q]} |\\langle Q_j, \\mathcal{V} \\rangle|}{\\sup_{j \\in [q]} |\\langle Q_j, \\mathcal{V} \\rangle| + \\inf_{i \\notin [q]} \\langle Q_i, \\mathcal{V} \\rangle}$,                                                                                                            (7)\nwhere $Q_i = AGR_i(\\mathcal{V}, \\mathcal{B})$, and the robust level is $(h, E_{AGR_i \\sim \\mathcal{P}}[\\alpha_i])$ in expectation.\nTheorem 1 provides a sufficient condition on the candidate set $\\mathcal{S}$ for achieving Byzantine robust-ness. The probability mass on the aggregation rules robust to the attack should be larger than a"}, {"title": "Convergence Analysis", "content": "Let assumptions 1,2,3 and 4 hold, K be the number of sam-pled clients in each round, $h_m$ be the maximal Byzantine client number in each of T rounds of training, $\\{ \\alpha_1,..., \\alpha_M \\}$ be the robust coefficients for aggregation rules in $\\mathcal{S}$ corresponding to $h_m$, $\\mathcal{P}$ be the sampling distribution over $\\mathcal{S}$. If $h_m < K/2$, the learning rate $\\eta$ satisfies that\n$\\eta = \\min \\{\\frac{1}{8L}, \\frac{10}{K-h_m} \\frac{32L(F(x^0) - F^*) + (6 + \\frac{h_m}{K-h_m})G_\\mathcal{H}^2 + 7G_\\mathcal{G}^2}{(8LT)(80L(\\frac{h_m}{K-h_m}G_\\mathcal{H}^2 + G_\\mathcal{G}^2) + 240L E_{AGR_i \\sim \\mathcal{P}}[\\alpha_i]G_\\mathcal{G}^2)} \\}$,                                                                                                                                                           (8)\nand the momentum parameter $\\beta = 1 - \\delta L \\eta$, then it holds that\n$\\frac{1}{T} \\sum_{t=1}^T E||\\nabla F(x^{t-1})||^2 \\leq \\frac{\\frac{6}{K-h_m}32L(F(x^0) - F^*) + \\frac{3}{K-h_m}G_\\mathcal{H}^2 + 3G_\\mathcal{G}^2 + \\frac{2}{T}||\\nabla F(x^0)||^2}{T} \\frac{1}{8L} + \\frac{1}{32L(F(x^0) - F^*)} + (6 + \\frac{h_m}{K-h_m})G_\\mathcal{H}^2 + 7G_\\mathcal{G}^2} E_{AGR_i \\sim \\mathcal{P}}[\\alpha_i]G_\\mathcal{G}^2}$.                                                                                                                                 (9)\nTheorem 2 states that the dynamic defense strategy with an appropriate time-varying learning rate converges to a neighborhood of a first order stationary point in expectation. The radius of this neighborhood (i.e., $\\frac{15}{T} E_{AGR_i \\sim \\mathcal{P}}[\\alpha_i]G_\\mathcal{G}^2$) depends on both the data heterogeneity and the expected robust level. Note that the $h_m$ malicious clients in each round affect the error bound through the term of $E_{AGR_i \\sim \\mathcal{P}}[\\alpha_i]$. As stated in the theorem, the robust level of each aggregation rule in the candidate set is coupled with $h_m$. When $h_m \\rightarrow 0$, each $\\alpha_i$ also tends to be a small value that is only determined by the aggregation bias induced by $AGR_i$. Given the data heterogeneity, the error bound can be reduced by decreasing $E_{AGR_i \\sim \\mathcal{P}}[\\alpha_i]$, which can be achieved by assigning higher sampling weights to the robust aggregation rules. When $G_\\mathcal{H} = 0$, the convergence rate can be simplified to $O(\\sqrt{E_{AGR_i \\sim \\mathcal{P}}[\\alpha_i] + \\frac{K-h_m}{T}})$, matching convergence rate in data homogeneous settings [17].\nFurthermore, we present two representative sampling strategies for instantiating the dynamic defense strategy, including uniform sampling and weighted sampling, which serve as case studies for a better understanding of the Byzantine robustness provided by a black-box server. Specifically, when the server lacks prior knowledge of attacks, a commonly employed practice is uniform sampling. During each round of training, the server randomly samples an aggregation rule from the candidate set $\\mathcal{S}$ following the uniform probability distribution $\\mathcal{P} = [\\frac{1}{M},..., \\frac{1}{M}]$.\nIn cases where the server has prior knowledge, e.g., an approximate model update derived from a root dataset $\\mathcal{D}^0$ [7, 37], it becomes feasible to optimize the sampling strategy. For instance, during the t-th round of training, the server fine-tunes the global model $x_t$ on $\\mathcal{D}^0$ to produce a trustable model update $\\Delta_0$. After that, upon receiving model updates from benign and malicious clients, the server employs every aggregation rule in $\\mathcal{S}$ to combine these updates, and adopts a weighted sampling strategy: $\\mathcal{R} = \\{ R_j, AGR_j \\in \\mathcal{S} \\}$, where R denotes the aggregated results of all M rules, with each update $R_j$ being sampled based on a probability $p_j$ that is proportional to its similarity with the trustable model update. Section 4 contains some empirical studies of these two sampling strategies."}, {"title": "White-box Servers with Dynamic Defense v.s. Black-box Servers with Dynamic Defense", "content": "As discussed in Section 3.3, an inaccessible candidate set $\\mathcal{S}$ of aggregation rules is a crucial charac-teristic of a black-box server in the dynamic defense strategy.\nHowever, a white-box server can also achieve similar unpredictability with an accessible candidate set of aggregation rules, as pointed out by previous study [27]. Here we provide discussions on these two settings to highlight the importance of a black-box server and analyze the extent to which the black-box server can enhance the robustness.\nFor the white-box server setting where the candidate set $\\mathcal{S}$ is accessible to clients, malicious clients can easily conduct local experiments to quantify attack impact $\\alpha_{i,j}$ of each attack $A_i \\in A$ to each"}, {"title": "Experimental Settings", "content": "We conduct a series of experiments to provide empirical observations on the robustness of FL sys-tems with a black-box server that defends against various Byzantine attacks."}, {"title": "Experimental Results", "content": "We conduct a series of experiments to provide empirical observations on the robustness of FL sys-tems with a black-box server that defends against various Byzantine attacks."}, {"title": "Experimental Results", "content": "Figure 6 depicts the attack impacts of three AGR-agnostic attacks to different aggregation algorithms on CIFAR-10 dataset. Similar to the results in Figure 1, as the proportion of malicious clients increases, the negative impacts brought by Byzantine attacks on the models learned without defense become larger, while the performance of models learned with Krum, Median, Trimmedmean, Bulyan, and DDS stays at the same level."}, {"title": "Datasets and Models", "content": "We federally train convnet2 model on FEMNIST [5] and VGG11 [32] model on CIFAR-10 [18], where the CIFAR-10 dataset is split according to a Dirichlet distribution with parameter 0.5.\nWe build up all the experiments based on FederatedScope [38], a developer-friendly FL platform. The total client number is set to 200, and the client sampling ratio in each round is set to 20%. We employ SGD as the optimizer, with the momentum parameters for FEMNIST and CIFAR-10 are 0 and 0.9 respectively. The proportion of malicious clients varies from 2.5% to 10.0%."}, {"title": "Aggregation Rules and Attack Algorithms", "content": "For comparisons, we adopt four widely-used ag-gregation rules as the candidate defense algorithms, including Krum [3], Median [39], Trimmed-mean [39], and Bulyan [14]. Meanwhile, we resort to three AGR-agnostic attacks, i.e., Gaussian attack, label flipping and Lie attack [2], and two effective AGR-adaptive attacks, i.e., Fang at-tack [11] and She attack [30] to conduct experiments on the Byzantine robustness. More details of the adopted aggregation rules and attack algorithms can be found in Appendix C.1."}, {"title": "Compared Defending Strategies", "content": "For the white-box server, there are two strategies: (i) Applying a deterministic aggregation rule that is accessible to malicious clients (i.e., Krum, Median, Trimmed-mean and Bulyan), and (ii) Dynamically sampling an aggregation rule from an accessible candidate set in each training round (as discussed in Section 3.4), denoted as \"White-box Dynamic\" in the figures. For the black-box server, we consider the two dynamic defense strategies that are discussed in Section 3.3, denoted as \u201cBlack-box Dynamic Uniform\u201d and \u201cBlack-box Dynamic Weighted\u201d."}, {"title": "Results and Analysis", "content": "We conduct a series of experiments to provide empirical observations on the robustness of FL sys-tems with a black-box server that defends against various Byzantine attacks."}, {"title": "Attacks", "content": "When the adopted aggregation rules are inaccessible, it is a reasonable and effective solution for malicious clients to perform AGR-agnostic attacks. Therefore we con-duct experiments to study the Byzantine robustness of FL systems with a black-box server against AGR-agnostic attacks. The experimental results are demonstrated in Figure 1. We report the neg-ative impact brought by three AGR-agnostic attacks when the server applies static (i.e. Krum, Median, Trimmedmean and Bulyan) and dynamic defense strategies (i.e., \"White-box Dynamic\", \"Black-box Dynamic Uniform\u201d and \u201cBlack-box Dynamic Weighted\") on FEMNIST dataset. From the figures, we can observe that both static and dynamic defense strategies can achieve competitive performance in defending all the adopted attack algorithms, including Gaussian attack, Label flip-ping, and Lie attack. The similar results on CIFAR-10 dataset are shown in Figure 6 in Appendix C.\nFurther, as the proportion of malicious clients increases, the negative impacts brought by Byzantine attacks on the models learned without defense become larger, while the performance of models learned with both static and dynamic defense strategies stays at the same level. It is worth pointing out that Fedavg (i.e., \u201cNo defense\u201d in the figures) achieves a certain level of Byzantine robustness against the label flipping attack on FEMNIST dataset, which is consistent with previous study [31]. Such robustness is attributed to the client sampling procedure, since some AGR-agnostic attacks rely on a large amount of malicious clients and the attack continuity, while the server only samples a subset of clients (including both malicious and benign clients) in each FL training round."}, {"title": "Adaptive Attacks", "content": "Malicious clients can apply AGR-adaptive attacks to incur the worst-case negative impact on model utility when the aggregation rules are accessible. We conduct exper-iments on FEMNIST and CIFAR-10 datasets to compare the negative impact on FL systems with a white-box server and a black-box server applying dynamic defense strategies, as shown in Fig-ure 2 and 3. For the white-box server, we consider both static and Dynamic (\u201cWhite-box Dynamic\") defense strategies. For both strategies, malicious clients tend to choose a specific attack algorithm according to the knowledge of the attack impacts. For example, when the server applies Krum, malicious clients adopt Fang attack or She attack targeting Krum, denoted as \u201cFang-Krum attack\u201d or \"She-Krum attack\" in the figures, respectively. When the server samples aggregation rules from {Krum, Median, Trimmedmean, Bulyan}, malicious clients adopt the attack that can incur the high-est attack impact from the attack set, e.g., {Fang-Krum, Fang-Median, Fang-Trmean, Fang-Bulyan} for the Fang attack. We set up a black-box server that applies the two dynamic defense strategies discussed in Section 3.3, i.e., \"Black-box Dynamic Uniform\u201d and \u201cBlack-box Dynamic Weighted\", for comparison."}, {"title": "Clients Number", "content": "To further understand the Byzantine robustness brought by a black-box server, we vary the total client number in {200, 500, 1000} and fix the pro-portion of malicious clients as 10%. The experimental results are summarized in Figure 4. From these results, we can conclude that a black-box server which adopts a dynamic defense strategy ef-fectively defends against various Byzantine attacks and outperforms white-box servers which adopt"}, {"title": "Related Works", "content": "Recent studies on robust aggregation rules can be roughly divided into three categories: distance-based rules [3, 6, 13, 33, 34], statistic-based rules [12, 14, 16, 22, 25, 35], and performance-based rules [7, 8, 20, 36]. Distance-based rules discriminate the Byzantine attacks via comparing the distance between the received model updates and then filter out the outliers. For example, Krum [3] sorts the Euclidean distances between updates and selects only updates which are closest to their neighbors. FoolsGold [13] computes the cosine similarity between the updates to determine their contributions in the aggregation. Statistic-based rules exploit some robust statistics to perform aggregation to improve the robustness. For example, Median and Trimmedmean [39] take the coordinate-wise median and the coordinate-wise trimmed mean over model updates as the aggregated result. RESAM [12] demonstrates that the momentum of gradients can help improve the robustness of aggregation. Performance-based rules validate the updates with the help of a clean dataset. For example, FLtrust [7] assigns trust scores for model updates based on the distance from a reliable model update computed on a small clean dataset. Zeno [36] computes scores based on its magnitude and the descendant of the loss function on a small validation set. Besides, there is a line of research demonstrating that robustness can be improved by preprocessing the model updates, e.g., gradients bucketing [16] or splitting [22], before applying the aggregation rules in non-IID settings."}, {"title": "Robustness", "content": "There has been a line of literature [9, 19, 24, 26, 27] build-ing the connection between randomization and adversarial robustness of deep learning. The study in [19] first introduces the differential private [10, 29, 40] randomness into the deep learning pro-cedure, which guarantees any small changes incurred by adversarial examples wouldn't produce an overwhelmingly negative impact on the model performance. Further, researchers [26] prove that any deterministic classifier can be outperformed by a randomized one when evaluated against determin-istic attack strategies, and consider the randomization for both the classifier and the attacker from a game theoretic point of view and confirms that the use of randomization can enhance adversarial robustness [24]. Inspired by previous studies that focus on the robustness of deep learning to ad-versarial examples, we discuss the robustness gain brought by a black-box server against Byzantine updates."}, {"title": "Conclusions", "content": "In this paper, we conduct the first study on Byzantine robustness of the FL system with a black-box server. In particular, we set up an attack and defense model to compare the Byzantine robust-ness between the white-box and black-box server settings, applying both the static and dynamic defense strategies. We provide theoretical analysis for the dynamic defense strategies and accord-ingly analyze the extent to which a black-box server with a dynamic defense strategy can improve"}, {"title": "Conclusions", "content": "In this paper, we conduct the first study on Byzantine robustness of the FL system with a black-box server. In particular, we set up an attack and defense model to compare the Byzantine robust-ness between the white-box and black-box server settings, applying both the static and dynamic defense strategies. We provide theoretical analysis for the dynamic defense strategies and accord-ingly analyze the extent to which a black-box server with a dynamic defense strategy can improve"}, {"title": "Fedavg", "content": "Here we provide the pseudocode of Federated Averaging algorithm (Fedavg). As shown in Algorithm 1, in t-th round of training, the server first samples a set Ct of active clients to participate the training, and broadcasts the current model xt to those clients. Upon receiving xt, each client in Ct trains xt on his/her local data and computes the local update. Together with the local data size, the local update is sent to the central server. After receiving all the local updates, the server takes weighted average over the local updates based on received local data sizes. The result is taken as the final update and used to update the current model x\u00b2. The process proceeds for T rounds to ensure the model convergence."}, {"title": "Detailed Derivations", "content": "In this section, we present the detailed derivations of the robustness (Theorem 1 in the main paper) and conver-gence analysis (Theorem 2) of the proposed dynamic defense strategy (DDS) respectively."}, {"title": "Proof of Theorem 1", "content": "We validate the two conditions in the definition of Byzantine robustness (shown in Section 2) respec-tively."}, {"title": "Proof of Theorem 2", "content": "We basically follow the proof of [16], with main differences in the definition of Byzantine robustness, aggrega-tion strategy, and assumption of the data heterogeneity."}, {"title": "Proof B.2", "content": "Without loss of generality, consider the t-th aggregation, the update of the model parameter xt-1 can be formulated as follows:"}, {"title": "Additional Experimental Settings and Results", "content": "Several widely-used robust aggregation rules are adopted as the candidate defense algorithms:"}, {"title": "Aggregation rules and Attack algorithms", "content": "Several widely-used robust aggregation rules are adopted as the candidate defense algorithms:"}, {"title": "Additional Experimental Results", "content": "Figure 6 depicts the attack impacts of three AGR-agnostic attacks to different aggregation algorithms on CIFAR-10 dataset. Similar to the results in Figure 1, as the proportion of malicious clients increases, the negative impacts brought by Byzantine attacks on the models learned without defense become larger, while the performance of models learned with Krum, Median, Trimmedmean, Bulyan, and DDS stays at the same level."}, {"title": "Gaussian attack", "content": "Malicious clients generate model updates by sampling from a Gaussian distribution N(0, \u03c3\u00b2). In our experiments, \u03c3 = 0.5."}, {"title": "Label flipping", "content": "Malicious clients train the model based on poisoned dataset in which each class label c is flipped into Cn c, where en is the class number."}, {"title": "Lie attack", "content": "Malicious clients compute the average Ax and standard deviation o of available benign updates and generate poisoned updates by Ax = \u2206x + z\u00b7\u03c3, where z is a scaling factor determined by the number of malicious and benign clients."}, {"title": "Fang attack", "content": "For a specific aggregation rule AGR, malicious clients compute the average Ax of the available benign updates and obtain a perturbation vector, w = -sign(\u2206x). Finally, malicious clients optimize to find a scaling factor z so that the poisoned updates Ax + z w can circumvent AGR."}, {"title": "She attack", "content": "For a specific aggregation rule, malicious clients first compute the average Ax of the available benign updates and then generate poisoned updates by Ax + z w, where z is a scaling factor that maximizes the attack impact, w can be {-sign(\u2206x), -std(x), - ||||}."}]}