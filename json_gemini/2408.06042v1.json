{"title": "Understanding Byzantine Robustness in Federated Learning with A Black-box Server", "authors": ["Fangyuan Zhao", "Yuexiang Xie", "Xuebin Ren", "Bolin Ding", "Shusen Yang", "Yaliang Li"], "abstract": "Federated learning (FL) becomes vulnerable to Byzantine attacks where some of participators tend to damage the utility or discourage the convergence of the learned model via sending their malicious model updates. Previous works propose to apply robust rules to aggregate updates from participators against different types of Byzantine attacks, while at the same time, attackers can further design advanced Byzantine attack algorithms targeting specific aggregation rule when it is known. In practice, FL systems can involve a black-box server that makes the adopted aggregation rule inaccessible to participants, which can naturally defend or weaken some Byzantine attacks. In this paper, we provide an in-depth understanding on the Byzantine robustness of the FL system with a black-box server. Our investigation demonstrates the improved Byzantine robustness of a black-box server employing a dynamic defense strategy. We provide both empirical evidence and theoretical analysis to reveal that the black-box server can mitigate the worst-case attack impact from a maximum level to an expectation level, which is attributed to the inherent inaccessibility and randomness offered by a black-box server. The source code is available\u00b9 to promote further research in the community.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) [4, 15, 28] enables massive clients to collaboratively train a global model without private data exposure, providing a solution to improving the model utility by sharing learned knowledge in a privacy-preserving manner. In an FL system, a server is expected to periodically aggregate the model updates provided by multiple clients, and then to optimize the global model accordingly. The most widely-used FL algorithm Fedavg [21] takes the mean operator in federated aggregation. However, it has been pointed out by previous studies [2, 11, 30] that the global model learned by Fedavg can be vulnerable when some Byzantine clients tend to send their malicious model updates. The intrinsic reason is that Fedavg equally takes model updates provided by all the clients, including the malicious ones, but the mean operation adopted in federated aggregation might not be robust enough against malicious updates.\nRecent studies [3, 7, 39] propose to apply robust aggregation rules for enhancing the Byzantine robustness of FL systems. For example, Krum [3] suggests filtering outliers in the model updates based on Euclidean distances; Median [39] adopts the statistics median operator rather than the mean operator in federated aggregation. These proposed robust rules make remarkable progress in effectively defending some Byzantine attacks that malicious updates can be generated without additional knowledge of aggregation rules adopted at the server, which are called AGR-agnostic attacks in this study.\nMeanwhile, some advanced attack algorithms [11, 30, 31] are designed targeting specific aggregation rules, which might easily disable the defense and severely damage the utility of the learned global model when the aggregation rules are known. These Byzantine attacks, named AGR-adaptive attacks, aim to optimize malicious updates that can both cheat the adopted robust aggregation rules and incur significant negative impact on the model performance. For example, Fang attack [11] scales the inverse direction of the model updates to generate malicious ones for maximizing the inner product between the aggregate deviation and the perturbation direction.\nIn real-world applications, it is reasonable for FL systems to involve a black-box server that makes the adopted robust aggregation rules inaccessible to attackers, which can violate the assumption that AGR-adaptive attacks rely on, i.e., the knowledge of the aggregation rule is accessible. It fits an intuition that using a black-box server can naturally defend or weaken some Byzantine attacks, but there still remains a gap between intuition and an in-depth understanding of the robustness of the FL system with a black-box server.\nTo fulfill such a gap, we conduct the first study on Byzantine robustness of the FL system with a black-box server, providing both theoretical analysis and empirical observations for in-depth under-standing. Specifically, we conduct the study to answer the question: Compared to white-box server settings (i.e., the behaviors are exposed to malicious clients), how and to what extent a black-box server can enhance the Byzantine robustness of an FL system?\nWe set up an attack and defense model to compare the white-box server and the black-box server scenarios, applying static and dynamic defense strategies, respectively. With the knowledge of the robust aggregation rules, Byzantine attacks on the white-box server can always incur the worst-case impact when malicious clients adopt the targeting Byzantine attack algorithms. We point out that such vulnerability mainly comes from unequal positions in the game of attack and defense under the white-box server setting. In contrast, a black-box server can mitigate such vulnerability as the black-box property takes the server to an equal position with the hidden Byzantine clients.\nWe provide theoretical analysis on robustness and convergence for dynamic defense strategies. Accordingly, we further analyze the improvement of Byzantine robustness when employing a black-box server as opposed to a white-box server, showing that a black-box server can reduce the worst-case attack impact from a maximum level to an expectation level. The reduction essentially sources from the fact that a white-box server consistently provides prior knowledge to malicious clients, enabling them to find an optimal attack strategy, whereas a black-box server can effectively prevent the exposure of any useful information about the defense strategy. We conduct a series of experiments on widely-used datasets, showing that a black-box server can significantly enhance the robustness of the FL systems against various Byzantine attacks by adopting the dynamic defense strategy."}, {"title": "2 Preliminary", "content": "In this section, we present some preliminary information on federated learning and Byzantine ro-bustness, and formulate the considered Byzantine attacks in this study.\nFederated Learning Let $\\mathcal{F}(x; \\mathcal{D})$ be the objective function to be minimized where $\\mathcal{D}$ is the un-derlying data distribution and $x \\in \\mathbb{R}^d$ represents the model parameters. For simplicity, we use $\\mathcal{F}(x)$ to denote $\\mathcal{F}(x; \\mathcal{D})$ subsequently. $f(x; \\xi)$ is an approximation to $\\mathcal{F}(x)$ under a federated learning setting in which $\\xi \\sim \\mathcal{D}$ is an observed data distributed among the clients in an FL course. Each client $c_i$ owns a part of dataset $\\xi_i$ and optimizes an objective function $f_i(x; \\xi_i)$ (also denoted as $f_i(x)$ subsequently). The federated learning task can be formulated as an optimization problem as:\n$\\min \\mathcal{F}(x) = f(x; \\mathcal{D}) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(x; \\xi_i).$ \t\t(1)\nFedavg [23] (Algorithm 1 in Appendix A) is the standard FL algorithm for solving the optimization problem defined in Equation (1), which takes the weighted mean operation over all the local updates from clients to approximate the real model update of $\\mathcal{F}(x)$ on $\\mathcal{D}$.\nByzantine Robustness We follow the definition of Byzantine robustness in previous studies [1, 3, 12]. Specifically, given an integer value $0 < h < n/2$ and a real value $\\alpha \\ge 0$, assume that"}, {"title": "Byzantine Attacks in FL", "content": "Consider an FL system consisting of one central server and $n$ local clients indexed by $\\{1, ..., n\\}$, among which $h$ clients (indexed by $\\{1, ..., h\\}$) are malicious clients who can communicate with each other (i.e., collusive), and the rest $n-h$ clients are benign workers who can only communicate with the central server. In each round of training, the server first selects a set of clients to distribute the up-to-date global model. Each selected client $c_i$ then conducts the local model training on their own dataset $\\xi_i$ and obtains the model updates $\\Delta x$. After the local training process, each client $c_i$ uploads a vector $v$ to the server with the following rule:\n$v = \\begin{cases} \\Delta x, & i > h; \\\\ b_i, & i \\leq h, \\end{cases}$  \t\t(2)\nwhere $b_i$ denotes a malicious model update that could be generated by colluding with all malicious clients. Under such cases, the mean operations adopted in Fedavg may not well approximate the real model update of $\\mathcal{F}(x)$ on $\\mathcal{D}$.\nAssumptions We give some basic assumptions [16, 17, 22] in theoretical analysis.\nAssumption 1 (Unbiased stochastic gradient) The stochastic gradient $\\nabla f_i(x; \\xi)$ is an unbiased estimator of the local gradient $\\nabla f_i(x)$, where $\\xi$ is the sampled data of client $c_i$ at the $t$-th round,\n$\\mathbb{E}_{\\xi} [\\nabla f_i(x;\\xi)] = \\nabla f_i(x).$ \t\t(3)\nAssumption 2 (Bounded variance) The variance of stochastic gradients is bounded: $\\exists G_i^2 \\in \\mathbb{R}$,\ns.t., $\\mathbb{E}_{\\xi} [||\\nabla f_i(x,\\xi) - \\nabla f_i(x)||^2] \\leq G_i^2$. \t\t(4)\nAssumption 3 (Bounded data heterogeneity) The difference between $\\nabla f_i(x)$ and $\\nabla \\mathcal{F}(x)$ is uni-formly bounded for all benign clients: $\\exists G \\in \\mathbb{R}$,\ns.t., $|\\nabla f_i(x) - \\nabla \\mathcal{F}(x)||^2 \\leq G^2, \\quad \\forall i \\in [n], x \\in \\mathbb{R}^d$. \t\t(5)\nAssumption 4 (L-smoothness) $\\mathcal{F}(x)$ is differentiable and L-smooth:\n$\\mathcal{F}(x) - \\mathcal{F}(x') \\leq (\\nabla \\mathcal{F}(x), x - x') + \\frac{L}{2}||x - x'||^2$.\t\t(6)"}, {"title": "3 Byzantine Robustness in FL with A Black-box Server", "content": "With the aim of building an understanding of Byzantine robustness of FL systems with a black-box server, we first set up an attack and defense model. In an FL system, the central server can choose to apply robust aggregation rules from the candidate set $\\mathcal{S} = \\{\\text{AGR}_i, i \\in [M]\\}$, where $M$ denotes the total number of candidate aggregation rules. Correspondingly, for each $\\text{AGR}_i \\in \\mathcal{S}$, malicious clients can perform AGR-adaptive attack $A_i$ targeting $\\text{AGR}_i$. The attack $A_i$ achieves the highest attack impact on the performance of $\\text{AGR}_i$ compared to all other attacks."}, {"title": "3.2 Static Defense Strategies", "content": "For a white-box server whose aggregation rule is accessible to malicious clients, it is straightforward that whatever robust aggregation rule $\\text{AGR}_i$ is chosen by the white-box server, malicious clients can easily perform the attack algorithm $A_i$ accordingly. As a result, the attacks incur the robust level of $\\alpha_{i,i}$, which severely hurt the model performance. The vulnerability of the white-box server is essentially from its unequal position compared to hidden malicious clients in the game of defense and attack, in which any defense strategy of the white-box server is known by malicious clients in prior.\nFor a black-box server whose aggregation rule is inaccessible to malicious clients, the straight-forward strategy for malicious clients is to choose an arbitrary attack algorithm from $A_i \\in \\mathcal{A}$, and verify which one can incur the maximal negative attack impact. In such cases, whatever the strategy taken by malicious clients, the black-box server can perform no worse than the white-box server in terms of the effect of defense, because the black-box property takes the server to an equal position with hidden malicious clients in the game of defense and attack.\nIn a nutshell, FL systems with a black-box server can naturally weaken Byzantine attacks compared to those with a white-box server. However, although the aggregation rule of the black-box server, say $\\text{AGR}_i$, is inaccessible, it is still possible that the malicious clients successfully find the optimal attack method $A_i$ by conducting multiple trials with different attack algorithms. As a result, using a static defense strategy might cause a risk that the aggregation rules are identified by the malicious clients. Therefore, black-box servers have better use a dynamic strategy to make the defense more unpredictable, which is introduced in detail below."}, {"title": "3.3 Dynamic Defense Strategies", "content": "Motivated by the aforementioned insights, we further consider dynamic defense, which combines diverse robust aggregation rules and incorporates randomness into the aggregation procedure to achieve unpredictability of the defense strategy in an FL system. A direct choice to introduce ran-domness is sampling from a set of robust aggregation rules. Specifically, the central server maintains a candidate set of robust aggregation rules $\\mathcal{S} = \\{\\text{AGR}_i, i \\in [M]\\}$. At each training round, the server samples a robust aggregation rule $\\text{AGR}_i$ from the candidate set $\\mathcal{S}$ to perform aggregation according to a specific probability distribution $P = [p_1, ...,p_M]$. Both benign and malicious clients conduct the local training process and upload their updates following the definition in Equation (2).\nThe above sampling based dynamic defense strategy can easily achieve unpredictability, which ben-efits from (i) the randomness of the aggregation rule adopted in each training round, and (ii) the diversity and inaccessibility of the candidate set of aggregation rules for black-box server settings. Therefore, this dynamic defense strategy can serve as a tool to compare the robustness of white-box and black-box server settings. Here, we first build a theoretical foundation for this dynamic defense strategy in terms of both robust level and convergence performance (the detailed proof can be found in the Appendix B).\nTheorem 1 (Robustness Analysis) Let $\\mathcal{B} = \\{B_1,..., B_h\\}$ be an attack which can successfully attack a subset $\\mathcal{S}'$ of $q$ aggregation rules in $\\mathcal{S}$, W.L.O.G., $\\mathcal{S}' = \\{\\text{AGR}_j, j \\in [q]\\}$, in the sense that $\\forall \\text{AGR}_j \\in \\mathcal{S}'$, it holds that $(\\text{AGR}_i(\\mathcal{V}, \\mathcal{B}), \\mathcal{V}) \\leq 0$, then the dynamic defense strategy is Byzantine robust if the probability mass on other $M - q$ aggregation rules satisfies that\n$\\sum_{i \\notin [q]} p_i > \\frac{\\text{sup}_{j \\in [q]} |<Q_j, \\mathcal{V}>|}{\\text{sup}_{j \\in [q]} |<Q_j, \\mathcal{V}>| + \\text{inf}_{i \\notin [q]} <Q_i, \\mathcal{V}>|}$ \t\t(7)\nwhere $Q_i = \\text{AGR}_i(\\mathcal{V}, \\mathcal{B})$, and the robust level is $(h, \\mathbb{E}_{\\text{AGR}_i \\sim p}[\\alpha_i])$ in expectation.\nTheorem 1 provides a sufficient condition on the candidate set $\\mathcal{S}$ for achieving Byzantine robust-ness. The probability mass on the aggregation rules robust to the attack should be larger than a"}, {"title": "3.4 White-box Servers with Dynamic Defense v.s. Black-box Servers with Dynamic Defense", "content": "As discussed in Section 3.3, an inaccessible candidate set $\\mathcal{S}$ of aggregation rules is a crucial charac-teristic of a black-box server in the dynamic defense strategy.\nHowever, a white-box server can also achieve similar unpredictability with an accessible candidate set of aggregation rules, as pointed out by previous study [27]. Here we provide discussions on these two settings to highlight the importance of a black-box server and analyze the extent to which the black-box server can enhance the robustness.\nFor the white-box server setting where the candidate set $\\mathcal{S}$ is accessible to clients, malicious clients can easily conduct local experiments to quantify attack impact $\\alpha_{i,j}$ of each attack $A_i \\in \\mathcal{A}$ to each"}, {"title": "4 Experiments", "content": "We conduct a series of experiments to provide empirical observations on the robustness of FL sys-tems with a black-box server that defends against various Byzantine attacks."}, {"title": "4.1 Experimental Settings", "content": "Datasets and Models We federally train convnet2 model on FEMNIST [5] and VGG11 [32] model on CIFAR-10 [18], where the CIFAR-10 dataset is split according to a Dirichlet distribution with parameter 0.5.\nWe build up all the experiments based on FederatedScope [38], a developer-friendly FL platform. The total client number is set to 200, and the client sampling ratio in each round is set to 20%. We employ SGD as the optimizer, with the momentum parameters for FEMNIST and CIFAR-10 are 0 and 0.9 respectively. The proportion of malicious clients varies from 2.5% to 10.0%.\nAggregation Rules and Attack Algorithms For comparisons, we adopt four widely-used ag-gregation rules as the candidate defense algorithms, including Krum [3], Median [39], Trimmed-mean [39], and Bulyan [14]. Meanwhile, we resort to three AGR-agnostic attacks, i.e., Gaussian attack, label flipping and Lie attack [2], and two effective AGR-adaptive attacks, i.e., Fang at-tack [11] and She attack [30] to conduct experiments on the Byzantine robustness. More details of the adopted aggregation rules and attack algorithms can be found in Appendix C.1.\nCompared Defending Strategies For the white-box server, there are two strategies: (i) Applying a deterministic aggregation rule that is accessible to malicious clients (i.e., Krum, Median, Trimmed-mean and Bulyan), and (ii) Dynamically sampling an aggregation rule from an accessible candidate set in each training round (as discussed in Section 3.4), denoted as \"White-box Dynamic\" in the figures. For the black-box server, we consider the two dynamic defense strategies that are discussed in Section 3.3, denoted as \u201cBlack-box Dynamic Uniform\" and \u201cBlack-box Dynamic Weighted\"."}, {"title": "4.2 Results and Analysis", "content": "AGR-agnostic Attacks When the adopted aggregation rules are inaccessible, it is a reasonable and effective solution for malicious clients to perform AGR-agnostic attacks. Therefore we conduct experiments to study the Byzantine robustness of FL systems with a black-box server against AGR-agnostic attacks. The experimental results are demonstrated in Figure 1. We report the neg-ative impact brought by three AGR-agnostic attacks when the server applies static (i.e. Krum, Median, Trimmedmean and Bulyan) and dynamic defense strategies (i.e., \"White-box Dynamic\", \"Black-box Dynamic Uniform\u201d and \u201cBlack-box Dynamic Weighted", "No defense\\\" in the figures) achieves a certain level of Byzantine robustness against the label flipping attack on FEMNIST dataset, which is consistent with previous study [31]. Such robustness is attributed to the client sampling procedure, since some AGR-agnostic attacks rely on a large amount of malicious clients and the attack continuity, while the server only samples a subset of clients (including both malicious and benign clients) in each FL training round.\nAGR-adaptive Attacks Malicious clients can apply AGR-adaptive attacks to incur the worst-case negative impact on model utility when the aggregation rules are accessible. We conduct exper-iments on FEMNIST and CIFAR-10 datasets to compare the negative impact on FL systems with a white-box server and a black-box server applying dynamic defense strategies, as shown in Fig-ure 2 and 3. For the white-box server, we consider both static and Dynamic (\u201cWhite-box Dynamic\") defense strategies. For both strategies, malicious clients tend to choose a specific attack algorithm according to the knowledge of the attack impacts. For example, when the server applies Krum, malicious clients adopt Fang attack or She attack targeting Krum, denoted as \u201cFang-Krum attack\" or \\\"She-Krum attack\" in the figures, respectively. When the server samples aggregation rules from {Krum, Median, Trimmedmean, Bulyan}, malicious clients adopt the attack that can incur the high-est attack impact from the attack set, e.g., {Fang-Krum, Fang-Median, Fang-Trmean, Fang-Bulyan} for the Fang attack. We set up a black-box server that applies the two dynamic defense strategies discussed in Section 3.3, i.e., \\\"Black-box Dynamic Uniform": "nd \u201cBlack-box Dynamic Weighted\u201d, for comparison."}, {"title": "5 Related Works", "content": "Robust Aggregation Rules Recent studies on robust aggregation rules can be roughly divided into three categories: distance-based rules [3, 6, 13, 33, 34], statistic-based rules [12, 14, 16, 22, 25, 35], and performance-based rules [7, 8, 20, 36]. Distance-based rules discriminate the Byzantine attacks via comparing the distance between the received model updates and then filter out the outliers. For example, Krum [3] sorts the Euclidean distances between updates and selects only updates which are closest to their neighbors. FoolsGold [13] computes the cosine similarity between the updates to determine their contributions in the aggregation. Statistic-based rules exploit some robust statistics to perform aggregation to improve the robustness. For example, Median and Trimmedmean [39] take the coordinate-wise median and the coordinate-wise trimmed mean over model updates as the aggregated result. RESAM [12] demonstrates that the momentum of gradients can help improve the robustness of aggregation. Performance-based rules validate the updates with the help of a clean dataset. For example, FLtrust [7] assigns trust scores for model updates based on the distance from a reliable model update computed on a small clean dataset. Zeno [36] computes scores based on its magnitude and the descendant of the loss function on a small validation set. Besides, there is a line of research demonstrating that robustness can be improved by preprocessing the model updates, e.g., gradients bucketing [16] or splitting [22], before applying the aggregation rules in non-IID settings.\nRandomization Enhanced Robustness There has been a line of literature [9, 19, 24, 26, 27] build-ing the connection between randomization and adversarial robustness of deep learning. The study in [19] first introduces the differential private [10, 29, 40] randomness into the deep learning pro-cedure, which guarantees any small changes incurred by adversarial examples wouldn't produce an overwhelmingly negative impact on the model performance. Further, researchers [26] prove that any deterministic classifier can be outperformed by a randomized one when evaluated against determin-istic attack strategies, and consider the randomization for both the classifier and the attacker from a game theoretic point of view and confirms that the use of randomization can enhance adversarial robustness [24]. Inspired by previous studies that focus on the robustness of deep learning to ad-versarial examples, we discuss the robustness gain brought by a black-box server against Byzantine updates."}, {"title": "6 Conclusions", "content": "In this paper, we conduct the first study on Byzantine robustness of the FL system with a black-box server. In particular, we set up an attack and defense model to compare the Byzantine robust-ness between the white-box and black-box server settings, applying both the static and dynamic defense strategies. We provide theoretical analysis for the dynamic defense strategies and accord-ingly analyze the extent to which a black-box server with a dynamic defense strategy can improve"}, {"title": "A Fedavg", "content": "Here we provide the pseudocode of Federated Averaging algorithm (Fedavg). As shown in Algorithm 1, in t-th round of training, the server first samples a set $\\mathcal{C}_t$ of active clients to participate the training, and broadcasts the current model $x^t$ to those clients. Upon receiving $x^t$, each client in $\\mathcal{C}_t$ trains $x^t$ on his/her local data and computes the local update. Together with the local data size, the local update is sent to the central server. After receiving all the local updates, the server takes weighted average over the local updates based on received local data sizes. The result is taken as the final update and used to update the current model $x^{t+1}$. The process proceeds for T rounds to ensure the model convergence.\nB Detailed Derivations\nIn this section, we present the detailed derivations of the robustness (Theorem 1 in the main paper) and conver-gence analysis (Theorem 2) of the proposed dynamic defense strategy (DDS) respectively."}, {"title": "B.1 Proof of Theorem 1", "content": "Proof B.1 We validate the two conditions in the definition of Byzantine robustness (shown in Section 2) respec-tively.\nRegarding condition (i), according to the expectation formula, when given a probability distribution satisfying the condition shown in Equation (7), it holds that\n$\\mathbb{E}_{\\text{AGR}}[(Q_i, \\mathcal{V})] = \\sum_{j \\in [q]} p_j. (Q_j, \\mathcal{V}) + p_i (Q_i, \\mathcal{V})$\n$\\mathbb{E}_{\\text{AGR}}[(Q_i, \\mathcal{V})] = \\sum_{j \\in [q]} p_j. (Q_j, \\mathcal{V}) + p_i (Q_i, \\mathcal{V})$"}, {"title": "Proof B.2", "content": "Proof B.2 Without loss of generality, consider the t-th aggregation, the update of the model parameter $x^{t-1}$ can be formulated as follows:\n$x^t = x^{t-1} - \\eta m^t,$\nwhere $m^t$ is the aggregated momentum. For each client $c_i$, the local momentum is computed by\nm^i_t = (1 - \\beta) m_t^{i-1} + \\beta \\nabla f_i(x^{t-1}; \\xi^{t-1}),$\nwhere $m_t^i = \\nabla f_i(x^0; \\xi^0)$. Let $\\eta \\leq \\frac{1}{8L}$, Combining the randomness in the training process and the L-smoothness property of the loss function $\\mathcal{F}$ shown in Assumption 4, we can deduce a descent inequality:"}, {"title": "C Additional Experimental Settings and Results", "content": "Several widely-used robust aggregation rules are adopted as the candidate defense algorithms:\n\u2022 Krum [3]. The server first computes the cumulative Euclidean distance between each update and its h 2 nearest neighbors, and then takes the mean of k updates with the smallest cumulative Euclidean distances. In our experiments, k = 10.\n\u2022 Median [39]. For each j\u2208 [d], the server sorts the j-th parameter of local updates and takes the median as the j-th parameter of the aggregated update.\n\u2022 Trimmedmean [39]. For each j-th parameter, the server removes the largest and smallest \u03b2 of local updates, and takes the mean of the remaining. In our experiments, \u03b2 = 0.2. Trimmedmean is also denoted by Trmean subsequently.\n\u2022 Bulyan [14]. The server first selects m 2h updates using Krum and then finds m 4h parameters closest to the median for each coordinate, and finally computes the mean.\nMeanwhile, the following attack algorithms are considered:"}, {"title": "C.2 Additional Experimental Results", "content": "Figure 6 depicts the attack impacts of three AGR-agnostic attacks to different aggregation algorithms on CIFAR-10 dataset. Similar to the results in Figure 1, as the proportion of malicious clients increases, the negative impacts brought by Byzantine attacks on the models learned without defense become larger, while the performance of models learned with Krum, Median, Trimmedmean, Bulyan, and DDS stays at the same level."}]}