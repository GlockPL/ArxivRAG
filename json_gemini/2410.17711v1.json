{"title": "BEWARE OF CALIBRATION DATA FOR PRUNING LARGE LANGUAGE MODELS", "authors": ["Yixin Ji", "Yang Xiang", "Juntao Li", "Qingrong Xia", "Ping Li", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "abstract": "As large language models (LLMs) are widely applied across various fields, model compression has become increasingly crucial for reducing costs and improving inference efficiency. Post-training pruning is a promising method that does not require resource-intensive iterative training and only needs a small amount of calibration data to assess the importance of parameters. Previous research has primarily focused on designing advanced pruning methods, while different calibration data's impact on pruning performance still lacks systematical exploration. We fill this blank and surprisingly observe that the effects of calibration data even value more than designing advanced pruning strategies, especially for high sparsity. Our preliminary exploration also discloses that using calibration data similar to the training data can yield better performance. As pre-training data is usually inaccessible for advanced LLMs, we further provide a self-generating calibration data synthesis strategy to construct feasible calibration data. We conduct experiments on the recent strong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that the proposed method outperforms commonly used calibration data and can effectively enhance strong pruning methods (e.g., Wanda, OWL).", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, Large Language Models (LLMs) have exhibited remarkable performance and enormous potential in Natural Language Processing (NLP) and Artificial Intelligence (AI) (OpenAI, 2022; 2023; Bubeck et al., 2023; Yang et al., 2023). The success of LLMs is closely tied to scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022): training language models with more parameters, using more data and greater computational resources leads to more powerful capabilities. However, LLMs with more parameters increase the difficulty and cost of deployment and inference. Therefore, much work has been devoted to compressing LLMs to achieve a trade-off between efficiency and performance, such as pruning (Frantar & Alistarh, 2023; Ma et al., 2023; Xia et al., 2024) and quantization (Frantar et al., 2023; Huang et al., 2024; Shao et al., 2024).\nPruning is a model compression technique that has evolved over many years (LeCun et al., 1989) and remains full of potential and challenges. Based on the over-parameterization of neural networks, it aims to remove redundant parameters while minimizing the degradation of model performance. Pruning has been successfully applied to compress small to medium-sized neural networks. Through sparse training (Lee et al., 2019; Frankle & Carbin, 2019; Yuan et al., 2021; Lasby et al., 2024) or pruning-aware training (Sanh et al., 2020; Lagunas et al., 2021; Jiang et al., 2023) methods, it can achieve performance comparable to dense models with a high sparsity ratio (\u226570%). However, these methods require iterative training, which is costly and time-consuming for LLMs with billions of parameters. As a result, post-training pruning that does not require iterative training has become the preferred approach for pruning LLMs.\nThe challenge of post-training pruning is how to perform training-free parameter importance estimation. Frantar & Alistarh (2023) note that simple parameter magnitude-based metrics perform"}, {"title": "2 BACKGROUND", "content": "Model compression is a crucial way to improve inference efficiency by reducing the required memory, including pruning (Frantar & Alistarh, 2023; Sun et al., 2024; Ma et al., 2023; Yin et al., 2024a; Guo et al., 2023; Zhang et al., 2024b; Xia et al., 2024), quantization (Frantar et al., 2023; Xiao et al., 2023; Lin et al., 2024; Huang et al., 2024; Shao et al., 2024), low-rank decomposition (Kaushal et al., 2023; Yuan et al., 2024; Wang et al., 2024; Ji et al., 2024), etc. The enormous memory requirements and inefficient inference speeds for LLMs urgently necessitate model compression. However, many successful model compression methods have required substantial computational resources for re-training, which limits their application for LLMs in low-resource settings. Therefore, post-training compression, which does not require retraining, has become a current research focus."}, {"title": "3 THE IMPACT OF CALIBRATION DATA FOR PRUNING", "content": "Though Williams & Aletras (2024) have noted that calibration data significantly impacts post-training pruning, there exist many open questions. How much does calibration data affect pruning performance? How does the amount of calibration data affect compressed model performance? What data sources are more suitable for calibration? We investigate these questions in this section."}, {"title": "3.1 EXPERIMENTAL DETAILS", "content": "Dense Model To study the impact of data from different sources on post-training pruning methods, we need a comprehensive knowledge of the data used in model training. We select the powerful and fully open-source LLM (including training data), DCLM-7B (Li et al., 2024), as the dense model and conduct post-training pruning with different calibration data on it.\nPost-training Pruning Methods We choose three competitive and representative post-training pruning methods for evaluation: Wanda (Sun et al., 2024), DSnoT (Zhang et al., 2024d) and OWL (Yin et al., 2024b). These methods apply to both unstructured and semi-structured pruning."}, {"title": "3.2 How MUCH DOES CALIBRATION DATA AFFECT PRUNING PERFORMANCE?", "content": "In practical applications, evaluating and comparing the impact of different calibration data on pruned models inevitably consumes time and computational resources. Therefore, we wonder how significant the impact of calibration data is on pruning performance and whether it's worth our effort to seek optimal calibration data in research and practice. We consider different sparsity ratios and sparsity types. Our experiments cover sparsity ratios ranging from 30% to 60%, and at 50% sparsity ratio, we further compare unstructured, 4:8 semi-structured, and 2:4 semi-structured sparsity types."}, {"title": "3.3 IS CALIBRATION DATA FROM DIFFERENT SOURCES EQUALLY ROBUST TO DATA AMOUNT?", "content": "Currently, almost all post-training pruning methods for LLMs have empirically demonstrated robustness in terms of the amount of calibration data they use. Typically, model performance reaches a plateau when the data amount reaches 128, and more calibration data do not lead to additional performance gains. We wonder whether these methods are equally robust to the amount of data for calibration data from different sources. Can certain calibration data that lead to poorer pruned models be improved by increasing the data amount?\nWe perform Wanda and DSnoT pruning on DCLM-7B in the 2:4 semi-structured pruning setting. We randomly sample 64, 128, 256, 512, 1024, and 2048 samples from different data sources as calibration data. We observe that the average performance of pruned models is robust to data amount, regardless of the calibration data source, with fluctuations of only 0.1%-0.2%. Therefore, we cannot expect that increasing the amount of calibration data will narrow the performance gap between different calibration data. Additionally, as the data amount increases, the standard deviation of the pruned model's performance decreases."}, {"title": "3.4 WHAT CALIBRATION DATA IS SUITABLE FOR PRUNING?", "content": "Since the choice of calibration data is crucial and cannot be improved by increasing the amount alone, we have to figure out what calibration data is more suitable for pruning. We propose two reasonable hypotheses: (1) The more similar the calibration data is to the training data of the LLMs, the better the pruning performance. (2) The higher the quality of the calibration data, the better the pruning performance.\nTo verify the hypotheses, we perform three post-training pruning methods on DCLM-7B with various calibration data in the 2:4 semi-structured pruning setting. Therefore, we believe that the similarity of calibration data to the training data has a more significant impact on pruning performance than the quality of the calibration data. Training data or data similar to the training data is better suited as calibration data. We conjecture that this may be due to LLMs learning the patterns in the training data better. Therefore, using data with similar patterns as calibration data during the pruning process can more accurately reflect the importance of model parameters."}, {"title": "4 CALIBRATION DATA SAMPLING METHOD", "content": "In the Section 3, our empirical study of the open-source DCLM-7B model demonstrates that selecting calibration data similar to the training data can yield better pruning performance. However, in practical scenarios, the training data of many LLMs is not publicly available to users. In this section, we will propose the \"self-generating then sampling\" strategy for sampling calibration data when the training data is unavailable. Formally, given a dataset D as the source of calibration data and an LLM M pre-trained on an inaccessible dataset Dt, we aim to sample n instances from D as calibration data D, that has a similar distribution to Dt.\nRecently, Xu et al. (2024b) disclosed that LLMs internalize patterns such as language structure, word distribution, and even commonsense knowledge from the training data during the training process. Due to their auto-regressive nature, LLMs leverage these internalized patterns when predicting the next token, producing the generated text similar to the training data. Thus, we propose using self-generated synthetic data as a proxy for the training data for calibration in post-training pruning. Specifically, for a sample from the source of calibration data D, we truncate the first t tokens as the prefix and then allow the LLM M to generate contextually relevant subsequent content:\n$$x_i \\sim P_M(x_i | x_{<i}), i = t...N.$$\nAfter generating the data, we filter the synthetic data to prevent low-quality generated data from negatively impacting pruning effectiveness. We calculate each generated sample's perplexity and filter the k% samples with the highest perplexity. Higher perplexity indicates that the patterns are not well-fitted by the LLM and may differ significantly from the training data, making them unsuitable as calibration data."}, {"title": "5 EXPERIMENTS", "content": "To evaluate the effectiveness of our proposed calibration data sampling method, we apply it to various LLMs, including DCLM-7B, LLaMA-2-7B, LLaMA-2-13B (Touvron et al., 2023) and LLaMA-3-8B (Dubey et al., 2024). As described in Section 3.1, we use C4, Wikipedia, Slimpajama, and DCLM as baselines for calibration data, employing three post-training pruning methods: Wanda, DSnoT, and OWL, to prune the dense models. In the main experiments, we report performance at the 60% sparsity ratio. We follow previous work to evaluate the compressed LLMs' language modeling and commonsense reasoning capabilities. We do not use the Wikitext2 dataset, which is common in most papers for evaluating language modeling ability, as its similarity to Wikipedia may introduce bias when assessing the impact of different calibration data on language modeling ability. Instead, we choose the Alpaca (Taori et al., 2023) dataset, distinct from all four calibration data sources, as our language modeling test data.\nWhen replicating DSnoT and OWL, we follow the hyperparameter settings detailed in their papers. During the self-generation process, we use Top-k and Top-p sampling to improve the diversity of the generated data. Specifically, we set the p-value to 0.95, the k-value to 50, and the temperature to 0.6 for the DCLM model, and 0.8 temperature for the LLaMA-series model. We apply the repetition penalty of 1.2 to avoid repeatedly generating low-quality fragments. To demonstrate the generalization of our self-generated calibration data, we randomly sample 5,000 examples from the Wikipedia data for generation, as Wikipedia performs poorly among the LLMs we used. In the filtering phase, we eliminate the top 20% of samples based on their perplexity."}, {"title": "5.2 OVERALL PERFORMANCE", "content": "We report main results in Table 2 and Table 5. Overall, our self-generated synthetic calibration data outperforms other baseline calibration data in language modeling and commonsense reasoning tasks and is compatible with different pruning methods. On DCLM-7B, the synthetic calibration data improves performance in commonsense reasoning tasks by an average of 2.2% to 2.6% compared to the original Wikipedia data. Additionally, it surpasses the commonly used C4 calibration data, achieving an average increase of 0.8% to 1.2%. On LLaMA family models, the self-generated synthetic data significantly outperforms the original data, with improvements ranging from approximately 0.9% to 1.1%, and surpasses the C4 data by about 0.3% to 0.5%. Surprisingly, the performance of the self-generated calibration data even exceeds that of calibration data sampled from the DCLM-7B training set, with an average improvement of 0.3% to 0.7%. We think this may be due to certain patterns in the calibration data that LLMs have not adequately learned. Using these patterns as calibration data may misestimate the importance of parameters. In contrast, due to the nature of maximum likelihood training, self-generated calibration data typically generates patterns that LLMs have better learned, thus avoiding using underrepresented patterns as calibration data."}, {"title": "6 DISCUSSION", "content": ""}, {"title": "6.1 IS THE SYNTHETIC CALIBRATION DATA SUITABLE FOR OTHER PRUNING SETTINGS?", "content": "We further validate the effectiveness of self-generated synthetic calibration data across more pruning settings. Table 3 illustrates the commonsense reasoning performance of DCLM-7B during Wanda pruning using different calibration data at unstructured 50% and 65% sparsity ratios, as well as semi-structured 4:8 and 2:4 settings. In all pruning settings, our synthetic calibration data either matches or exceeds the performance of the optimal calibration data from the training set DCLM. Notably, the synthetic data improve performance by approximately 0.8% in the two semi-structured pruning settings. Since semi-structured pruning can achieve practical inference acceleration and advanced GPUs already support 2:4 sparse tensor cores. Thus, we"}, {"title": "6.2 How DOES PREFIX LENGTH AFFECT THE PERFORMANCE OF SYNTHETIC DATA?", "content": "The prefix length during self-generation is a crucial hyperparameter. If the prefix is too short, the synthetic text is likely to be far from the semantics of the original text; if it is too long, the synthetic calibration data may retain excessive patterns from the original text. Therefore, it is essential to explore the selection of prefix length. Our experiments range from 0 to 1024 prefix lengths, where a prefix length of 0 indicates only a special token representing the start of the text. Once there is a prefix, the performance exceeds that of the original calibration data. However, longer prefixes do not yield better results, as performance gradually declines with increased prefix length. The results indicate that using 1 to 4 tokens as a prefix is optimal. This suggests that semantic consistency with the original text is not critical in synthetic calibration data; instead, the key is to avoid retaining patterns that could have negative effects."}, {"title": "6.3 HOW DOES PERPLEXITY-BASED DATA FILTERING AFFECT PRUNING PERFORMANCE?", "content": "After generating synthetic data, we employ a simple perplexity-based method to filter low-quality data. Is this perplexity-based filtering method effective, and what should the filtering rate be? We conduct experiments on the DCLM-7B model. As shown in Table 4, even without any filtering strategy, the synthetic data outperforms the original data. The perplexity-based filtering has proved to be a simple yet effective approach, with the best pruning performance at a filtering rate of 10%-20%. As the filtering rate increases, pruning effectiveness gradually declines, ultimately matching the performance of the unfiltered data. Therefore, we recommend filtering only the outliers based on perplexity, as overly aggressive filtering may compromise the diversity of the calibration data, negatively impacting pruning performance."}, {"title": "6.4 WHETHER SELF-GENERATED SYNTHETIC CALIBRATION DATA IS MORE SIMILAR TO TRAINING DATA?", "content": "In Section 3.4, we assert that data similar to the training data is more suitable as calibration data for post-training pruning. Based on the auto-regressive generation characteristics of LLMs, we propose using self-generated data as an approximation of the training data. But is the self-generated synthetic data truly similar to the model's training data than other calibration data? We use an efficient and effective Min-K%++ method (Zhang et al., 2024a) for measuring. Min-K%++ notes that after maximum likelihood training, the probability distribution of the training data always lies at local maxima along the input dimensions. Therefore, for a given token sequence (x<t, Xt), if the sequence is belong to the training data, the p(x<t, xt) should be higher than that of other candidate tokens in the vocabulary. The Min-K%++ is formulated as follows:\n$$W(x_{<t}, x_t) = \\frac{logp(x_t|x_{<t}) - P_{x_{<t}}}{\\sigma_{x_{<t}}}$$\n$$Min-K^{++}(x) = \\frac{1}{min-k%}\\sum_{x_{<t},x_t \\in Emin-k%} W(x_{<t}, x_t),$$\nwhere $x_{<t}, \\sigma_{x_{<t}}$ is the expectation and standard deviation of the next token's log probability given the prefix $x_{<t}$, respectively. min-k% refers to choosing the bottom k% of subsequences based on scores from the sequence x. Thus, the higher a sample's Min-K%++ score, the more likely it is to appear in the training data. shows the Min-50%++ score distribution of C4, Wikipedia, Slimpajama and our self-generated synthetic data. We can clearly observe that the self-generated synthetic data has higher Min-50%++ scores than the other calibration data. It indicates that the self-generated synthetic calibration data is indeed similar to the training data, confirming the validity of using self-generated data as a proxy for the training data."}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this paper, we highlight the critical role that calibration data plays in post-training pruning for LLMs. Through systematic exploration, we demonstrate that calibration data similar to the original training data leads to superior pruning performance. To address the challenge of inaccessible training data in practical scenarios, we propose a self-generating synthetic calibration data strategy, which effectively samples suitable calibration data for LLMs. Experimental results on the DCLM, LLaMA-2, and LLaMA-3 models demonstrate that our method significantly outperforms existing"}]}