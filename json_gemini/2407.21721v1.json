{"title": "Open-Vocabulary Audio-Visual Semantic Segmentation", "authors": ["Ruohao Guo", "Liao Qu", "Dantong Niu", "Yanyu Qi", "Wenzhen Yue", "Ji Shi", "Bowei Xing", "Xianghua Ying"], "abstract": "Audio-visual semantic segmentation (AVSS) aims to segment and classify sounding objects in videos with acoustic cues. However, most approaches operate on the close-set assumption and only identify pre-defined categories from training data, lacking the generalization ability to detect novel categories in practical applications. In this paper, we introduce a new task: open-vocabulary audio-visual semantic segmentation, extending AVSS task to open-world scenarios beyond the annotated label space. This is a more challenging task that requires recognizing all categories, even those that have never been seen nor heard during training. Moreover, we propose the first open-vocabulary AVSS framework, OV-AVSS, which mainly consists of two parts: 1) a universal sound source localization module to perform audio-visual fusion and locate all potential sounding objects and 2) an open-vocabulary classification module to predict categories with the help of the prior knowledge from large-scale pre-trained vision-language models. To properly evaluate the open-vocabulary AVSS, we split zero-shot training and testing subsets based on the AVSBench-semantic benchmark, namely AVSBench-OV. Extensive experiments demonstrate the strong segmentation and zero-shot generalization ability of our model on all categories. On the AVSBench-OV dataset, OV-AVSS achieves 55.43% mIoU on base categories and 29.14% mIoU on novel categories, exceeding the state-of-the-art zero-shot method by 41.88%/20.61% and open-vocabulary method by 10.2%/11.6%. The code is available at https://github.com/ruohaoguo/ovavss.", "sections": [{"title": "1 Introduction", "content": "Generally speaking, objects can be characterized jointly by their appearance and the sounds they make. By incorporating visual and acoustic information in a collaborative manner, it is beneficial for a better perception and understanding of the concepts of objects. Recently, a novel audio-visual learning task, i.e., audio-visual segmentation (AVS), has been proposed in [34]. The goal of AVS is to output pixel-level maps of sound-emitting objects within video frames. This requires aligning two different modalities and serving audio as prompt to localize and segment visual objects. Different from AVS being tasked with binary foreground segmentation, audio-visual semantic segmentation (AVSS) is to classify sound sources and generate semantic maps associating one category with each pixel. To accomplish the above tasks, many works adopt convolutional- or Transformer-based encoder-decoder architectures to establish audio-visual relationships and segment sounding objects. Despite promising results, these approaches only recognize pre-defined categories in the training set, resulting in poor generalization capacity to novel concepts. In real-world applications, this closed-set paradigm lacks practical value, since it often encounters objects from novel categories during training.\nIf the model has never seen an object and heard its sound, could it still accurately localize sound source in audible videos? Wang et al. [25] present a prompt-based model, GAVS, which attempts to pinpoint sounding objects real-world videos, including novel categories unseen during training. Specifically, they insert Adapter [12] into the large-scale pre-trained segment-anything model (SAM) [13] to construct audio-visual correlations and learn downstream tasks. With the help of the adapter-based fine-tunning technique, GAVS adapts the visual foundational model, i.e., SAM, to the AVS task, and generalize the prior knowledge to unseen objects and different datasets. However, GAVS is designed for binary segmentation, that is, it can only localize visual position of sounding objects without predicting their categories. In addition, GAVS takes only an image along with one-second audio as input and fuse multi-modal in spatial domains, leading to the inability to capture frame-by-frame relationships in temporal domains. Yu et al. [30] explore the feasibility of promoting visual foundational models for the AVS task in a zero-shot setting. They use CLAP [27], a large-scale audio-language pre-trained model, to classify the input audio from open-set categories. Then, the predicted class names are fed into Grounding DINO [18] to obtain box predictions, and these boxes prompt SAM [13] to generate the corresponding masks. Nevertheless, only using audio information to predict the category of sounding objects is weak and inadequate in complex scenarios. For example, humans can imitate cat meows, and cars and airplanes can produce similar engine sounds. Moreover, due to the lack of visual appearances and motion cues, this model tends to segment all objects belonging to the predicted class instead of those sounding objects.\nIn this paper, we propose a novel multi-modal task, i.e., open-vocabulary audio-visual semantic segmentation, which aims at segmenting and classifying sound-emitting objects in videos from open-set categories, as illustrated in Fig. 1. Unlike the above zero-shot AVS methods that generate binary masks, we focus on the semantic segmentation and require the network to assign semantic labels to each pixel in given video frames. Open-vocabulary audio-visual semantic segmentation is a more challenging task, facing the following problems: Audio is a 1D signal that exhibits high information density, meaning that multiple objects could make sound simultaneously and their audio could entangle at any timestamp. Thus, it is not easy to build correct spatial alignment from mixed-source audio to visual content. Video is continuous and only using a single image-audio pair is not optimal. Temporal information and motion cues play an indispensable role in sound source localization and classification. So how to effectively integrate audio and visual features in the temporal dimension? In the open-vocabulary setting, the model is trained on a closed set and then localizes while identifying sounding objects within categories beyond the annotated label space. This calls for the model to possess strong generalization abilities: not only localizing sounding objects, but also suppressing silent objects and background disturbances, such as noise or sounds from objects outside the frame.\nTo address the above problems, we design the following modules. For problem 0, an audio-visual early fusion module is introduced to align audio signals with image features in the spatial dimension. It takes an image feature map from the visual backbone and the corresponding audio embedding from the pre-trained audio encoder as inputs, and then computes bi-directional cross-attention between them. On the one hand, the audio embedding as the query can be enhanced by correlating with image pixels of those semantic-relevant objects. On the other hand, the image feature as the query can be supplemented and activated with audio information. For problem, we present an audio-conditioned Transformer decoder to establish frame-by-frame relationships and capture audio-visual dependencies in the temporal dimension. Specifically, a set of learnable object queries is first fed into the decoder and performs the cross- and self-attention sequentially to obtain object-centric representations, following Mask2Former [2]. Then, these queries extract audio temporal information via a cross-attention layer, where the key elements are the all audio embeddings from the audio-visual early fusion module. For problem, to localize sounding objects"}, {"title": "2 Related Works", "content": "Audio-visual segmentation (AVS) aims to segment the objects that emit sound within each video frame. Recent AVS research primarily focuses on supervised learning on the sparsely annotated AVSBench dataset, which can be divided into FCN-based and Transformer-based approaches. For FCN-based methods, Zhou et al. [34] propose temporal pixel-wise audio-visual interaction (TPAVI), which is a non-local block with cross-modal attention to locate the sound source. ECMVAE [21] employs three latent encoders to achieve latent space factorization, yielding both modality-shared and specific representations to model audio-visual contributions explicitly. In addition, Transformer-based methods accomplish audio-visual fusion and mask decoding with vision Transformer [4]. For example, AVSegFormer [5] uses audio-based channel attention to dynamically adjust visual features, and adopts a DETR-like architecture [1] to segment sounding objects. Liu et al. [17] associate the audio signals with visual information by aligning predicted audio class labels with instance segmentation masks, thereby highlighting sounding objects while suppressing silent ones. Audio-visual semantic segmentation (AVSS) extends the AVS task by additionally predicting the category of each sound source. Due to the semantic entanglement in audio, tackling multi-source AVSS presents a greater challenge. Zhou et al. [33] follow the TPAVI module [34] to perform audio-visual fusion, and then attach a class head for semantic predictions. CATR [15] separately carries out audio-visual interactions in a decoupled manner. Additionally, audio-constrained queries are introduced to select sounding objects during decoding.\nCurrent AVS approaches mainly focus on in-domain and close-set situations. When faced with unseen classes in real-world scenarios, these approaches will no longer be applicable, owing to the limited training data. To overcome the drawback, Wang et al. [25] design an encoder-prompt-decoder framework to improve the generalization of the AVS model by leveraging the prior knowledge of the visual foundation model. Specifically, they use the adapter technique [12] to fine-tune the pre-trained segment-anything model (SAM) [13] and generalize knowledge learned from the training set to unseen objects. Yu et al. [30] employ the large-scale audio-language model, CLAP [27], to predict the category to which the audio belongs. For the single-source and multi-source audio, they select the class names with the highest and the top two highest scores, respectively. The predicted class names are input into Grounding DINO [18] and SAM [13] to sequentially generate box predictions and the corresponding masks. Unlike the above zero-shot methods applied to the binary segmentation, this work focuses on the audio-visual semantic segmentation and exploits the large-scale pre-trained CLIP model [23] to classify sounding objects from novel categories.\nCompared with traditional segmentation methods [9, 32], open-vocabulary segmentation seeks to locate and recognize categories beyond the annotated label space. The main difference between zero-shot learning and open-vocabulary learning is that the latter is capable of leveraging visual-related language vocabulary data [26]. For example, LSeg [14] aligns the image embeddings with the text embeddings of category labels from CLIP text encoder. This allows LSeg to use the knowledge of vision-language models and segment objects that are not pre-defined but depend on the input texts. Recently, more works focus on video instance segmentation and attempt to build an open-vocabulary tracker based on close-set training data. Wang et al. [24] collect a large-scale video dataset, and propose an end-to-end approach, called OV2Seg, for open-vocabulary video instance segmentation. OV2Seg adopts a momentum-updated module to track objects and a CLIP-based classifier for recognizing novel categories. OpenVIS [8] adapts the pre-trained CLIP with instance guidance attention and generates object proposals and the corresponding classes. Moreover, a rollout association mechanism is designed to associate instances of any categories across frames thereby improving tracking performance. In this paper, we extend open-vocabulary learning to audio-visual semantic segmentation: segmenting and classifying sound-producing objects of arbitrary open-set categories simultaneously."}, {"title": "3 Method", "content": "In this section, we present an open-vocabulary audio-visual semantic segmentation framework, namely OV-AVSS, as shown in Fig. 2. In the first stage, the universal sound source localization module (Section 3.3) is applied to predict class-agnostic masks of sounding objects covering all possible classes. In the second stage, the open-vocabulary classifier (Section 3.4) empowered by CLIP [23], is employed to identify the category of each sounding object mask.\nLet $D_{train}$ be a training dataset containing pixel-level annotations for a set of base categories $C_{base}$, open-vocabulary audio-visual semantic segmentation aims to train a model $f_{\\theta}(\\cdot)$ on $D_{train}$, and then test on $D_{test}$ for both base categories $C_{base}$ and novel categories $C_{novel}$ with the help of large language vocabulary knowledge $C_{voc}$. Note that $C_{voc}$ is not strictly required to contain $C_{base}$ or $C_{novel}$, as the language vocabulary may not cover all the class names in the vision or audio data. Given a test video clip $V \\in R^{T\\times H\\times W\\times 3}$ and the corresponding audio $A \\in R^{T}$, $f_{\\theta}(\\cdot)$ is supposed to predict the segmentation mask $\\{m_i\\}_{i=1}^{\\overline{P}} \\in R^{T\\times H\\times W}$ and the category label $c \\in (C_{base} \\cup C_{novel})$ for each sounding object of all the frames:\n$\\left\\{\\{m_1, m_2, . . ., m_{\\overline{P}}\\}, c\\right\\} = f_{\\theta} (V, A)$,\nwhere $\\overline{P}$ is the total number of categories of the sounding objects. In the experiments, we designate the categories used for training as \"base categories\". Categories that fall outside the established base categories are referred to as novel categories.\nGiven an input video sequence containing both visual and audio tracks, we divide it into $T$ non-overlapping audio and visual snippet pairs $\\{V, A\\} = \\{v_i, a_i\\}$, where each snippet is 1-second long and $T$ denotes the number of snippets as well as the video duration.\nFor each audio snippet $a_i$, we use the pre-trained VGGish [6] model to extract the audio feature as $f_a \\in \\mathbb{R}^{C}$, where $C = 128$ is the feature dimension. The VGGish model, a VGG-like 2D convolutional neural network, is pre-trained on AudioSet dataset [6]. Due to the input being 2D audio spectorgram, we first convert the audio $A$ into a mono-waveform at a sampling rate of 16kHz and apply short-time Fourier transform to obtain the mel spectrograms. Then, they are fed into VGGish model and the audio features are extracted as $F_A = \\{f_1, f_2, ..., f_T\\}$. Note that the audio representation is extracted offline and the parameters of VGGish model are frozen during the training process.\nFor each visual snippet $v_i$, we extract image-level hierarchical features using the convolutional-based ResNet-50 [11] or Transformer-based Swin-B [19] backbones. We represent the features as $f_v \\in \\mathbb{R}^{H_k \\times W_k \\times D_k}$, where $(H_k, W_k) = (H, W)/2^{k+1}$, $k = 1, 2, 3, 4$, $(H, W)$ is the resolution of each visual snippet $v_i$ and $k$ is the number of backbone levels. The final visual representation can be formulated as $F_V = \\{f_i^1, f_i^2,...,f_i^4\\}.\nTo enhance the audio features $F_A$ with the image contexts and embed audio information into image features $F_V$ simultaneously, an audio-visual early fusion module is adopted, allowing modality interaction at the early stage (before feature decoding). Specifically, the audio feature $f_i^a$ and multi-level image features $\\{f_i^1, f_i^2, f_i^3\\}$"}, {"title": "3.3 Universal Sound Source Localization", "content": "are linearly projected to the same dimension $C_{av}$ via pointwise convolution and group normalization. Then, we flatten image features and concatenate them into a 1D sequence $s_v$. Finally, a bi-directional cross-attention module ($BCA\\rightarrow V$ and $BCV\\rightarrow A$) is applied to establish audio-visual spatial dependence and obtain cross-modal feature embeddings $f_{AV}, f_{VA}$. This process can be formulated as:\n$f_{AV} = BCA_{V} (f_i^a, s_v) = Softmax\\left( \\frac{f_i^aW_Q \\cdot (s_vW_K)^T}{\\sqrt{d_k}} \\right) \\cdot s_vW_V + f_i^a  \\tag{1}$\n$f_{VA} = BCV_{A}(s_v, f_i^a) = Softmax\\left( \\frac{s_vW_Q \\cdot (f_i^aW_K)^T}{\\sqrt{d_k}} \\right) \\cdot f_i^a + s_v  \\tag{2}$\nwhere $W^Q, W^K, W^V \\in \\mathbb{R}^{C_{av}\\times d_k}$ are learnable parameter matrices. In $BCA\\rightarrow V$, it serves audio feature $f_i^a$ as query and 1D image feature $s_v$ as key/value, and then perform multi-head attention between them. Owing to the image feature $s_v$ being multi-level and processed only once, the audio-visual early fusion module can efficiently leverage both strong semantics and fine-grained details. In $BCV\\rightarrow A$, query is $s_v$ and key/value is $f_i^a$. It treats the audio signals as auxiliary information, and the embedded semantical consistency is used to highlight the corresponding spatial regions.\nGiven the processed image features $f_{VA}$, we feed them into the pixel decoder, a multi-scale deformable attention Transformer [36], to output the enhanced feature $o_{va}$ and high-resolution per-pixel embeddings $p_i$. Considering sound source localization requires learning more accurate audio-object matching relationship, we propose the audio-conditioned Transformer decoder (AudioMaskDec) to incorporate audio information and decode the masks for sound-emitting objects. As shown in Fig. 3, AudioMaskDec consists of five key components: a spatio-temporal cross-attention layer, an audio self-attention layer, an object self-attention layer, an audio-aware cross-attention layer, and a feedforward neural network.\nTo be specific, we first initialize a set of learnable object queries $q \\in \\mathbb{R}^{N\\times C_o}$ supplemented with positional encodings, where $N$ is the number of object queries. Then, the spatio-temporal cross-attention layer computes cross-attention between object queries and all frames' features. This empowers object queries $q'$ with visual semantics and associates them with all potential objects, following previous works [2, 3]. After that, the object queries $q'$ and audio embeddings $\\{f_{AV}\\}_{i=1}^T$ are fed into the object self-attention layer and audio self-attention layer, separately. With the help of two separated self-attention layers, it isolates $q'$ and $\\{f_{AV}\\}_{i=1}^T$ instead of concatenating together as inputs, so as to facilitate object interactions across spatial domains and audio interactions across time domains, respectively. We denote the processed object queries as $q^*$ and audio embedding as $\\{f^*\\}_{i=1}^T$. To better judge when objects emit sound, we introduce an audio-aware cross-attention layer that performs cross-attention between $q^*$ and $\\{f^*\\}_{i=1}^T$, where the former is query and the latter is key/value. In this way, $T$-second audio is embedded into object queries to guide sound source localization temporally. Inspired by [16], the audio-aware cross-attention layer is placed behind spatio-temporal cross-attention layer to avoid forgetting audio information as the decoder layer goes deep.\nTo detect the sounding objects from all categories while ignoring other non-sounding objects, a sound head is attached to the AudioMaskDec, which predicts the binary sounding scores $S \\in \\mathbb{R}^{N\\times 1}$, indicating if a query represents an sounding object:\n$S = Softmax(MLP_{sound} (AudioMaskDec(q)))\\tag{3}$\nDue to the utilization of the sound head instead of a class head, object queries are class-agnostic, enabling our model to detect objects from all categories. This universal object proposal has been widely proved in previous open-vocabulary works [24, 31]. For the mask prediction, the class-agnostic object queries are passed into a mask head and then dot-producted with the per-pixel embeddings $p_i$, yielding the final masks $\\{M_i\\}_{i=1}^N$.\n$M_i = MLP_{mask} (AudioMaskDec(q)) \\cdot p_i, \\forall i \\in [1, T] \\tag{4}"}, {"title": "3.4 Open-Vocabulary Classification", "content": "Once sounding object candidates are localized, we employ the frozen pre-trained vision-language model, i.e., CLIP [23], to classify each candidate. Concretely, the category names with prompt templates are input into the CLIP text encoder $CLIP_{text}$ to generate the text embeddings offline, such as:\n$E_{text} = CLIP_{text}(\\text{\"}This is a photo of a \\{violin\\}\")\\tag{5}$\nwhere the category names contain both base and novel classes. Since CLIP model is trained on full sentences, we ensemble multiple prompt templates to improve classification accuracy. Our list of prompt templates is shown in Appendix.\nIn addition, we crop sounding image regions with the proposed masks $M$ and feed into the CLIP image encoder $CLIP_{image}$ to compute image embeddings $E_{image}$, where $n$ represents the number of sounding objects. Inspired by [7, 8], we first overlay the mask $M$ onto the original image, resulting in the masked images. Then, we compute the object's center coordinate and the length of its longer side. Next, the masked images are cropped into a square-shaped images. Lastly, we resize to $224 \\times 224$ resolution required by the CLIP image encoder, such as ViT-B [4]. Compared with no cropping or simple cropping in [29], this square cropping strategy can avoid objects occupying too small an area of the image and ensure $I$ without distortion.\nAfter acquiring the text embeddings $E_{text} \\in \\mathbb{R}^{cls \\times D_e}$ and image embeddings $E_{image} \\in \\mathbb{R}^{n\\times D_c}$, we compute the similarity matching map $Sim \\in \\mathbb{R}^{n\\times cls}$ between them as:\n$Sim(i, j) = softmax(E_{image}^i \\cdot (E_{text}^j)^T \\cdot \\epsilon)\\tag{6}$\nwhere $cls$ is the number of all categories, including base and novel classes. $\\epsilon$ is the temperature hyper-parameter. The class label of each class-agnostic mask can be obtained by the argmax operation."}, {"title": "3.5 Training and Loss", "content": "In the training phase, suppose there are $K$ sounding objects in an image, we select $K (K < N)$ object queries that best refers to the sounding objects via a bipartite matching strategy. Technologically, we use the sounding scores and mask predictions to compute the assignment cost metrics. The cost metrics is composed of two parts: one for segmentation that includes binary focal loss and dice loss, and the other is binary classification for sounding objects presence. Then the $K$ best matched object queries $\\{q_k\\}_{k=1}^K$ are applied for model training. Given the above matching, each prediction is supervised with a sound score loss and a mask loss. The former is binary cross-entropy loss and the latter consists of a focal loss and a dice loss. The total loss function between the query $\\{q_k\\}_{k=1}^K$ and ground-truth $\\{y_k\\}_{k=1}^K$, can be written as:\n$L(q_k, y_k) = \\lambda_{ce}L_{ce} + \\lambda_{focal}L_{focal} + \\lambda_{dice}L_{dice} \\tag{7}$\nwhere $\\lambda_{ce} = 2.0$, $\\lambda_{focal} = 5.0$, and $\\lambda_{dice} = 5.0$ are the weights to balance the loss function."}, {"title": "4 Experiments", "content": "To facilitate a comprehensive evaluation of OV-AVSS, we partition a novel dataset called AVSBench-OV, derived from AVSBench-Semantic [33], an extension of the initial AVSBench-object dataset [34] for semantic segmentation. AVSBench-Semantic encompasses 70 categories, incorporating both single and multiple sound source scenarios. Each video within AVSBench-Semantic is truncated to either 5 or 10 seconds in duration, with one frame extracted per second. Following the partitions in LVIS [10], we split the categories within AVSBench-OV into 40 base categories representing those seen during training and inherited from frequent and common categories, and 30 novel (unseen and unheard) categories disjoint from the base categories. To ensure consistency, we eliminated sample videos from the training subset whenever a novel category appeared in annotations, thereby restricting the training data exclusively to base categories. The dataset statistics of AVSBench-OV are reported as follows: the training set encompasses 5,184 real-world videos comprising 40,095 frames across 40 base categories, while the validation and test sets consist of 1,240 and 1,490 videos across all 70 categories. For specific category names and corresponding video counts per category, please refer to the Appendix.\nFollowing the previous open-vocabulary semantic segmentation task [22, 28], we adopt the mean intersection-over-union (mIoU) to evaluate the open-vocabulary AVSS performance. We compute overall mIoU, Base mIoU for seen categories, Novel mIoU for unseen categories, along with their harmonic mean (Harmonic). Harmonic is defined as follows:\n$Harmonic = \\frac{2 * Base * Novel}{Base + Novel} \\tag{8}"}, {"title": "4.2 Implementation Details", "content": "We use both ResNet-50 [11] and Swin-B [19] as the image encoders and VGGish [6] as the audio encoder. Parameters in the audio encoder, CLIP image encoder and CLIP text encoder are frozen during the training. For each video clip, we set the total number of video frames $T$ to 5 by default. The number of object queries is set to 100 for all experiments. We train our model on the AVSBench-OV dataset for 82,000 iterations (about 10 epochs) with a batch size of 1. We use the AdamW optimizer [20] and the step learning rate schedule. The initial learning rate is 1e-4 and scaled by a decay factor of 0.1 at the 72,171 iterations. If no other specification, the shorter side of frames are resized to 240, 360 or 480 during training. The experiments are conducted on an NVIDIA Quadro 6000 GPU."}, {"title": "4.3 Comparison to State-of-the-art Methods", "content": "Given that our proposed open-vocabulary AVSS task employs a new division of base and novel categories, there are no baselines with identical training settings for an entirely fair comparison. Therefore, we use baselines from a similar but not identical tasks, specifically, the zero-shot AVS task. First, we consider GAVS [25], which introduces an encoder-prompt-decoder paradigm to enhance the generalization of the AVS model by leveraging the prior knowledge of SAM [13]. However, due to the absence of CLIP or alternative foundation models, GAVS is limited to generate binary masks and cannot be applied to AVSS. Additionally, we compare with Sam4AVS [30], which first employs CLAP [27] to predict categories of the input audio, and then feed into Grounding DINO [18] and SAM for mask prediction. A significant constraint of Sam4AVS stems from its dependency on the CLAP for class identification, which neglects the visual cues. This exclusion can result in inaccuracy classification for sounding objects. Also, Sam4AVS segments all instances of the predicted category, lacking the ability to identify specific sounding source. The original Sam4AVS is used for binary segmentation. We reproduce it and make it suitable for the open-vocabulary AVSS task. As shown in Table 1, our OV-AVSS model surpasses Sam4AVS in all metrics by a significant margin (+41.88% mIoU in base categories, +20.61% mIoU in novel categories, +27.73% harmonic mIoU and +32.34% overall mIoU). These substantial improvements can be attributed to our designed universal sound source localization and open-vocabulary classification modules, which effectively leverages both audio and visual information for accurate segmentation and categorization of both seen and unseen sounding objects.\nAs shown in Table 2, we first compare OV-AVSS with two closed-set trained methods, namely TPAVI [34] and CATR [15]. These methods adhere to the traditional paradigm based on the close-set assumption, which can only predict pre-defined categories that are present in the training set. Consequently, their performance on the novel set yields zero across all metrics. To provide a thorough comparison, we evaluate TPAVI with different backbones, including ResNet-50 and PVT-v2. The results demonstrate that even on the base categories, the performance of these closed-set methods falls short of our approach. Furthermore, we compare OV-AVSS to several state-of-the-art open-vocabulary video instance segmentation methods. These methods do not incorporate audio cues, making it challenging to localize sounding objects in frames. Our proposed OV-AVSS significantly outperforms the best-performing method, CLIP-VIS [35], by +9.772% in base categories, +7.2% in novel categories, +8.84% in harmonic mIoU, and +8.6% in overall mIoU. These results highlight the effectiveness of our approach in leveraging both visual and audio information to achieve superior performance in open-vocabulary audio-visual semantic segmentation."}, {"title": "4.4 Ablation Study", "content": "The setting on the first row of Table 3 serves as a baseline, where no early fusion is applied. While this baseline approach can generate reasonably good results for both base and novel categories, there is still room for improvement. The audio embeddings and processed image features, when used independently, may not capture sufficient semantic information to guide the localization process effectively. To address this limitation, we introduce an early fusion mechanism that allows us to acquire audio-aligned visual features and visual-enriched audio embeddings. The second row in Table 3 presents a straightforward fusion approach, where audio embeddings are broadcast and added to the visual features in a pixel-wise manner. However, it is important to note that this fusion strategy is a one-way process, where the audio embeddings are used to enhance the visual features, but the audio embeddings themselves do not receive any visual priors to aid in sound source localization. To further improve the fusion mechanism, we introduce a bi-directional cross-attention mechanism, where each audio embedding can acquire corresponding semantic information from the visual features. Moreover, the cross-attention operation is much stronger than that of simple one-way summation, as it can adaptively focus on relevant regions in the visual features based on the audio embeddings. This brings extra performance increase as shown in the 4th row of Table 3. This process is performed at multiple levels, allowing the model to incorporate audio information at different scales.\nFor the baseline, we directly feed the object queries into the decoder, disregarding the audio embeddings obtained through our audio-visual early fusion. Due to the pixel-wise audio information incorporated into the image features through early fusion, this approach achieves promising results. However, it fails to fully utilize the temporal audio information, limiting its potential for enhancing performance. To overcome this obstacle, we concatenate the audio embeddings in the temporal dimension and add them to the object queries, similar to positional encoding. While this method yields modest improvements across all categories, it does not fully capture the rich time-dependent correspondence present in the audio modality. We further examine the impact of performing a single cross-attention operation between the object queries and audio embeddings before feeding them into the decoder. This approach yields further improvements in performance, highlighting the importance of effectively integrating audio and visual information. However, compared to our proposed AudioMaskDec, this audio prompting method may suffer from information loss as the decoder depth increases, limiting its ability to fully exploit the audio-visual synergy. In contrast, AudioMaskDec places an audio-aware cross-attention layer behind each spatio-temporal cross-attention layer in the decoder. This design enables the model to iteratively refine the integration of audio and visual features throughout the decoding process. By facilitating the interaction between object queries and audio embeddings at multiple layers, AudioMaskDec ensures that pertinent audio information is retained and effectively utilized before the sound head and mask head. As shown by the results in row 4 of Table 4, this approach further enhances the model's performance.\nPrevious work on zero-shot semantic segmentation [28] has shown that cropping an object's masked region by using its bounding box and resizing to a fixed size (e.g. 224 \u00d7 224) provides a more suitable input representation for CLIP compared to the full image. However, naively resizing the masked region can introduce significant aspect ratio distortion relative to the object's natural proportions. This unnatural stretching may degrade CLIP's zero-shot classification performance. We instead compute the center coordinate and longer side length of the object's bounding box, and extract a square crop around the object center, yielding a distortion-free representation. As shown in Table 5 row 2 and 4, SquareCrop approach outperforms the CropResize by a sizeable margin across all metrics, including a +2.16% gain on base classes, +1.28% on novel classes, +1.65% on harmonic mean, and +1.77% on mIoU. Furthermore, we investigate the impact of using different visual encoders of CLIP. As shown in the 3rd row of Table 5, changing the visual encoder does not yield further improvements in performance. This suggests that the classification effect of CLIP is largely dependent on the segmentation quality of the universal module and the input image quality, rather than the choice of different visual encoder."}, {"title": "5 Conclusion", "content": "This paper introduces a new task of open-vocabulary AVSS with the goal of segmenting and classifying sounding objects of arbitrary categories in a given video. Furthermore, we present the first open-vocabulary AVSS framework, termed OV-AVSS, generalizing from annotated (seen) object classes to other (unseen) categories with the knowledge of CLIP. Quantitative and qualitative experiments on AVSBench-OV dataset show strong zero-shot generalization ability on novel categories unseen during training. We hope that our proposed OV-AVSS model can further promote the generalization study of audio-visual segmentation in zero-shot, open-vocabulary and real-world scenarios. As a pioneering work, the current approach is not perfect. We outline the limitations of the model. 1) Base classes over-fitting: Our model may easily overfit the base ones when both classes share similar appearances or sounds, since these classes are trained with higher confidence scores. 2) Real-world challenges: When audio-visual inconsistency and noisy background disturbances are involved, our model's performance may be limited, leading to inaccurate segmentation results."}]}