{"title": "ENHANCING GRAPH SELF-SUPERVISED LEARNING WITH GRAPH INTERPLAY", "authors": ["Xinjian Zhao", "Wei Pang", "Xiangru Jian", "Yaoyao Xu", "Chaolong Ying", "Tianshu Yu"], "abstract": "Graph self-supervised learning (GSSL) has emerged as a compelling framework for extracting informative representations from graph-structured data without extensive reliance on labeled inputs. In this study, we introduce Graph Interplay (GIP), an innovative and versatile approach that significantly enhances the performance equipped with various existing GSSL methods. To this end, GIP advocates direct graph-level communications by introducing random inter-graph edges within standard batches. Against GIP's simplicity, we further theoretically show that GIP essentially performs a principled manifold separation via combining inter-graph message passing and GSSL, bringing about more structured embedding manifolds and thus benefits a series of downstream tasks. Our empirical study demonstrates that GIP surpasses the performance of prevailing GSSL methods across multiple benchmarks by significant margins, highlighting its potential as a breakthrough approach. Besides, GIP can be readily integrated into a series of GSSL methods and consistently offers additional performance gain. This advancement not only amplifies the capability of GSSL but also potentially sets the stage for a novel graph learning paradigm in a broader sense. GIP is open-sourced at https://github.com/LOGO-CUHKSZ/GIP.", "sections": [{"title": "INTRODUCTION", "content": "Graph-structured data has become increasingly prevalent across a variety of domains, presenting both unique challenges and opportunities for machine learning innovations. The complexity and irregular nature of graph data, characterized by its intricate relationships and diverse structures, necessitate specialized learning approaches. Graph Self-Supervised Learning (GSSL) has emerged as a pivotal strategy in this context (Jin et al., 2020; Liu et al., 2022; Xie et al., 2022; Wu et al., 2021), enabling the utilization of unlabeled graph data effectively in sectors as wide-ranging as molecular property prediction (Rong et al., 2020; Zhang et al., 2021b; Liu et al., 2021), and recommendation systems (Wu et al., 2021; Yu et al., 2022). The strength of GSSL lies in its capacity to autonomously discover complex patterns and structures within data, a process that is inherently valuable in understanding and exploiting the rich connectedness inherent within graph data.\nDespite the promise and advancements in GSSL, much of its development has been influenced by methodologies and ideas borrowed from the domains of computer vision and natural language processing (Chen et al., 2020; He et al., 2020; Devlin et al., 2018). Techniques such as contrastive learning, commonly used loss functions like InfoNCE (Gutmann & Hyv\u00e4rinen, 2010), Jensen-Shannon estimator (JSE) (Nowozin et al., 2016), and Barlow Twins loss (Zbontar et al., 2021), data augmentation strategies (Takahashi et al., 2019; Zhang, 2017), as well as specific architecture designs (Grill et al., 2020; He et al., 2022; Liu et al., 2023), have been adapted to fit the graph learning paradigm (You et al., 2020; Hassani & Khasahmadi, 2020; Bielak et al., 2022; Rong et al., 2019; Wu et al., 2022; Thakoor et al., 2021; Hou et al., 2022; Gong et al., 2024; Zhao et al., 2024). While these adaptations have spurred progress, they often overlook the peculiar and critical characteristics of graph data, such as its non-uniformity, the varying connectivity of different nodes, and the complexity of their relational linkages.\nThe limitations of current GSSL methodologies highlight an urgent need for approaches that are specifically tailored to respect and leverage the unique attributes of graph structures. Conventional methods often fail to tap into the full depth of information available, restricted by their partial adaptation of techniques from other fields. This realization has directed our research toward exploring novel avenues in graph learning that honor the intrinsic properties of graphs more holistically.\nMotivated by these challenges, we have developed Graph Interplay (GIP), a novel conceptual and computational framework designed to enhance the capability of GSSL. GIP introduces an innovative mechanism that integrates random inter-graph edges within batches, facilitating a richer and more dynamic interplay of information across different graphs. This approach is specifically advantageous in the context of GNNs (Graph Neural Networks), which leverage message-passing mechanisms to process graph-structured data. By interconnecting graphs within learning batches, GIP effectively broadens the contextual landscape within which the learning model operates, thus allowing for a more comprehensive understanding of manifold structures across diverse graph examples.\nTheoretically, we show that GIP equipped with GNNs provides a platform for better manifold discovery and separation in the realm of graph data, a critical aspect in enhancing the quality and applicability of learned representations. This theoretical basis underpins the practical benefits of GIP, demonstrating how it offers more discriminating and informative graph representations that are likely to improve performance on downstream tasks. Empirically, we applied GIP to a range of GSSL frameworks and noted significant improvements across multiple benchmarks, as shown in Figure 1. For instance, in challenging graph classification datasets like IMDB-MULTI, the incorporation of GIP elevated the classification accuracy from sub-60% levels to over 90%, showcasing its efficacy and potential as an innovative paradigm in GSSL.\nThe contributions of this paper articulate the core innovations and advancements offered by GIP:\n(I) We introduce Graph Interplay (GIP), a ground-breaking enhancement to graph self-supervised learning that encourages effective inter-graph connectivity for enriched learning experiences. (II) We make a step to provide a theoretical foundation for understanding GIP, elucidating its potential for improved manifold separation within graph domains. (III) We validate the effectiveness of GIP through comprehensive empirical studies across a diverse range of graph-level benchmarks, where GIP has shown remarkable improvements and versatility, significantly elevating the performance metrics of existing GSSL setups."}, {"title": "RELATED WORK", "content": "Graph Self-Supervised Learning (GSSL). GSSL methods can be categorized into Graph Contrastive Learning (GCL) and Graph Predictive Learning (Xie et al., 2022). GCL employs augmentations to create multiple views of the input graph, learning to maximize mutual information between these views for robust and invariant representations. Typically, GCL approaches typically focus on maximizing a lower bound of mutual information using estimators like InfoNCE (Gutmann & Hyv\u00e4rinen, 2010), and JSE (Nowozin et al., 2016). Examples of frameworks utilizing the InfoNCE objective include GRACE (Zhu et al., 2020), GCC (Qiu et al., 2020), and GCA (Zhu et al., 2021b), while MVGRL (Hassani & Khasahmadi, 2020) and InfoGraph (Sun et al., 2019) employ JSE. Predictive learning methods train graph encoders using self-generated labels and prediction heads. These include graph autoencoder-based models like GAE (Kipf & Welling, 2016b), MGAE (Wang et al., 2017), GALA (Park et al., 2019),VGAE (Kipf & Welling, 2016b), and ARGA/ARVGA (Pan et al., 2018), which capture representations through reconstruction. Additionally, models such as S2GRL (Peng et al., 2020) and GROVER (Rong et al., 2020) predict specific statistical properties associated with the graph, further enhancing their ability to learn meaningful representations. Other methods like M3S (Sun et al., 2020) and ICF-GCN (Hu et al., 2021) utilize self-training and node"}, {"title": "METHOD", "content": "In this section, we introduce Graph Interplay (GIP), which is designed to enhance GSSL through direct graph-level communications. We begin by outlining the motivation behind GIP, followed by a detailed description of its core mechanism, as well as its integration with existing GSSL frameworks. Finally, we analyze how GIP achieves a better manifold separation and provide theoretical insights into why GIP leads to more effective graph representations."}, {"title": "MOTIVATION", "content": "GSSL has emerged as a powerful paradigm for learning representations from graph-structured data without relying on explicit labels. However, current GSSL methods face several limitations: (I) Limited Inter-graph Information Exchange: Existing methods typically process graphs independently or rely on indirect interactions through parameter sharing, missing opportunities to leverage broader contextual information across the entire graph set. (II) Inefficient Use of Batch Information: Although graphs are often processed in batches, the structural information within a batch is not fully utilized, leaving the potential for graphs to inform and enhance each other's representations largely untapped. (III) Constrained View Generation: Most existing augmentation techniques focus on intra-graph operations, which may not capture the full spectrum of graph variations present in the data, potentially limiting the model's ability to learn robust and generalizable representations. These limitations collectively restrict the ability of current GSSL methods to fully capture and leverage the rich,"}, {"title": "OVERVIEW", "content": "The GIP process integrates seamlessly with existing GSSL schemes and can be summarized as follows: (I) Batch Sampling: A batch of graphs is sampled from a collection of pre-processed graphs. (II) Inter-graph Edge Addition: GIP randomly adds edges between graphs in the batch, creating two distinct views. These added edges establish message-passing channels between graphs, allowing for information flow across the batch. (III) Representation readout: Each graph in these two views now has access to a broader range of structural information. The GNN encoder and pooling function process this expanded structure, fusing information from both the original graph and the introduced inter-graph interplay. (IV) GSSL-driven Representation Learning: Graph representations from the two views are used to compute pairwise similarity matrices. These matrices serve as input to various GSSL objectives, including contrastive and invariance-keeping reduction methods. This flexibility allows GIP to integrate with a wide range of GSSL methods, guiding the learning process to capture meaningful patterns and relationships within the enriched graph structures. The framework of GIP is outlined in Figure 2."}, {"title": "GRAPH INTERPLAY (GIP)", "content": "To address the limitations of existing GSSL methods, we propose Graph Interplay (GIP), a novel approach that fundamentally reimagines how graphs interact during the self-supervised learning process. GIP transcends the conventional view of graphs as isolated entities, instead conceptualizing them as interconnected components of a larger, dynamic system. The core innovation of GIP lies in its ability to create enhanced views of the graph dataset through the strategic introduction of stochastic inter-graph edges. This process transforms a batch of disparate graphs into a unified, information-rich structure. For frameworks requiring two views, GIP can generate these using two independent probability parameters. Given a batch of graphs $G = {G_1, G_2, ..., G_N}$, where each graph $G_i = (V_i, E_i)$, GIP introduces stochastic inter-graph edges to create an extended edge set:\n$E_{ext} = \\bigcup_{i=1}^{N} E_i \\cup E_{inter}, P((u,v) \\in E_{inter}) = p \\_if u \\in V_i, v \\in V_j, i \\neq j$ (1)\nHere, $E_{ext}$ represents the extended edge set, $E_{inter}$ denotes the set of inter-graph edges, p is the probability of adding an inter-graph edge. For GSSL frameworks that require two views, we can generate these by assigning two independent probabilities $p_1$ and $p_2$, each used to create a separate instance of $E_{ext}$.\nThe GIP-enhanced message passing process operates on this extended graph structure. For each node v, its representation is updated as:\n$h_v^{(l+1)} = UPDATE^{(l)} (h_v^{(l)}, AGGR^{(l)} (MSG^{(l)} (h_u^{(l)}, h_v^{(l)}) : (u, v) \\in E_{ext} ))$ (2)\nIn this equation, $h_v^{(l)}$ denotes the representation of node v at layer l. The function $MSG^{(l)}$ computes the message from a neighbor node u to node v, $AGGR^{(l)}$ aggregates messages from all neighbors, and $UPDATE^{(l)}$ produces the new node representation. This formulation allows each node to assimilate information from a diverse, dynamically generated context spanning multiple graphs, providing a unique perspective on the inter-graph relationships.\nAfter L layers of message passing, we obtain graph-level representations through a pooling operation:\n$h_{G_i} = POOL({h_v|v\\in V_i})$ (3)\nwhere $h_{G_i} \\in R^d$ is the graph-level representation for $G_i$, and POOL is a pooling function that aggregates node representations into a single graph representation."}, {"title": "INTEGRATION WITH GSSL FRAMEWORKS", "content": "The stochastic nature of GIP's inter-graph connections serves a dual purpose. First, it acts as an implicit regularizer, preventing overfitting to specific graph structures. Second, it generates a"}, {"title": "RELATION TO MANIFOLD SEPARATION", "content": "In this section, we formally analyze how GIP enhances manifold separation in the representation space, leading to improved graph representation learning. To bridge the gap between the practical implementation of GIP and our theoretical analysis, we introduce simplifying assumptions and defini-tions that capture the essence of GIP while making the problem mathematically tractable. We consider a set of graphs $S = {G_1, G_2, ..., G_N }$ lying on K underlying manifolds $F = {M_1, M_2, ..., M_K}$ in a high-dimensional space. Each manifold $M_k$ is associated with a probability distribution $P_k$ from which graphs are sampled. This abstraction allows us to model the inherent structure of the graph dataset and analyze how GIP affects the relationships between graphs from the same or different manifolds. To capture the essence of GIP's inter-graph communication mechanism, we propose the following lemma:\nLemma 1 (GIP Transformation). Consider a GNN with n layers (n \u2265 1) used in Graph Interplay (GIP), under the following conditions:\n\u2022 Each layer of the GNN consists of a linear transformation followed by a ReLU activation function.\n\u2022 The pooling operation used to obtain graph-level representations is additive.\nThen the GIP transformation can be equivalently represented as:\n$f_G(G_i) = f(G_i) + \\sum_{j\\neq i} \\alpha_{ij} f(G_j)$ (4)\nwhere $f : G \\rightarrow R^d$ is a GNN encoder, and $a_{ij}$ are learnable parameters representing the strength of interaction between graphs $G_i$ and $G_j$.\nThis formulation abstracts GIP into a more compact form, facilitating our theoretical analysis of its impact on manifold separation. The proof of this lemma can be found in the Appendix G.1. To quantify the effectiveness of GIP in separating manifolds, we introduce the concept of manifold-relevant information $Z_k$ as a random variable for each manifold:\n$Z_k = f_S(G), G \\sim P_k$ (5)\nwhere $P_k$ is the probability distribution over graphs in manifold $M_k$, and $f_S$ denotes the GNN encoder that has been well-trained through standard SSL. This formulation allows us to measure GIP's enhancement in manifold alignment and separation over standard SSL. With these definitions in place, we can now state our main theoretical result:\nTheorem 1 (GIP's Improvement on Manifold Separation). Given the above definitions and assumptions, under the self-supervised learning objective and sufficient training, GIP can achieve better expected manifold separation than SSL:\n$\\frac{E_{G_i \\sim P_k} [I(f_{G}^{(v)}(G_i); Z_k)]}{\\max_{l \\neq k} E_{G_i \\sim P_l} [I(f_{G}^{(v)}(G_i); Z_l)]} > \\frac{E_{G_i \\sim P_k} [I(f_{S}(G_i); Z_k)]}{\\max_{l \\neq k} E_{G_i \\sim P_l} [I(f_{S}(G_i); Z_l)]}, \\nu \\in {1, 2}$ (6)\nwhere $I(\\cdot;\\cdot)$ denotes mutual information and $f_{G}^{(v)}$ represents the GIP embedding function for view $v$."}, {"title": "EXPERIMENT", "content": "In this section, we conducted a comprehensive evaluation of GIP across 12 datasets, where GIP exhibited notable improvements in the majority of datasets. To further elucidate the factors contributing to GIP's performance, we subsequently performed rigorous analytical experiments, providing deeper insights into its underlying mechanisms."}, {"title": "MAIN RESULTS", "content": "Datasets and Protocols We test on multiple graph classification and regression datasets ranging from social networks, and chemical molecules to biological networks. We benchmark our model on the TU Datasets (Morris et al., 2020) and OGB graph property prediction datasets (Hu et al., 2020). For both graph classification and regression tasks, we follow the evaluation protocols established in previous works (Lin et al., 2023; Chen et al., 2024a). Specifically, we first train our model in a self-supervised manner to learn graph representations. Then, we freeze the pre-trained encoder and use it to extract features for downstream tasks. For evaluation, we train a linear classifier or regressor on top of these frozen features and report the performance on the test set. For TU Datasets, we apply 10-fold cross-validation, while for OGB datasets, we use the provided data split. Additional details regarding dataset statistics can be found in the Appendix B.\nSetup and Baselines. We equip GIP with four Graph SSL frameworks: MVGRL (Hassani &\nKhasahmadi, 2020), GRACE (Zhu et al., 2020), G-BT (Bielak et al., 2022), and BGRL (Thakoor et al., 2021) following the previous works (Lin et al., 2023). Using DROPEDGE and ADDEDGE as"}, {"title": "ABLATION STUDY AND ANALYSIS", "content": "Varying GIP probability. To systematically investigate the impact of our proposed Graph Interplay (GIP) mechanism on model performance, we conducted a comprehensive experiment varying the edge addition probabilities ($P_1$, $P_2$) within the GRACE framework. Figure 4 visualizes the results across multiple datasets from the TUDataset collection as 3D surface plots, where the x and y axes represent $p_1$ and $p_2$ respectively, ranging from 0 to 1, and the z-axis represents the achieved accuracy. These visualizations reveal a clear trend: higher proportions of added edges, generally improve model performance, with peak accuracy typically observed when both $p_1$ and $p_2$ approach 1. This finding suggests that facilitating extensive information exchange between graphs significantly enhances the quality of learned representations. For comparison, we conducted similar visualizations for the DROPEDGE and ADDEDGE methods in Appendix D. Interestingly, these baseline approaches showed highly dataset-dependent behaviors with complex, often non-monotonic relationships between edge manipulation probabilities and accuracy. The clear principles governing GIP's performance offer promising and consistent avenues for further theoretical and empirical exploration, potentially leading to even more effective GSSL techniques.\nGIP with deeper GNNs. To further investigate the efficacy of GIP, we conducted extensive experi-ments varying the number of GNN layers in our model. Figure 5 illustrates the performance of GIP compared to baseline graph augmentation methods across different GNN depths on five datasets. The baseline methods include DROPEDGE, ADDEDGE, and Random Walk Sampling (RWS), providing a comprehensive comparison. The results reveal a striking contrast: while GIP consistently benefits from deeper GNN architectures, the baseline methods struggle to leverage increased depth effectively. Specifically, GIP shows a clear upward trend in accuracy as the number of GNN layers increases from 2 to 5 across all datasets, with the most pronounced improvements observed in IMDB-MULTI and IMDB-BINARY. In contrast, baseline methods struggle with increased depth, exhibiting either stagnant performance or degradation, particularly beyond 3 layers. This superior performance of GIP with deeper architectures can be attributed to its ability to effectively utilize expanded receptive fields. As GNN depth increases, the model captures more comprehensive information flows from other graphs, providing richer resources for self-supervised learning and enabling better adjustment of the manifold configuration of learned representations. While conventional methods demonstrate limited effectiveness with deeper architectures, GIP exhibits the potential to unlock the full capacity of deep GNNs in Graph SSL.\nEffect of different starting layers of GIP. To further understand the impact of our Graph Interplay mechanism, we conducted experiments to investigate the effect of applying GIP at different depths within the GNN architecture. In this context, the starting layer refers to the GNN layer from which we begin to apply GIP, with earlier layers using the orig-inal graph topology. Figure 6 illustrates the performance across different starting lay-ers on various datasets. For IMDB-MULTI, we observe slightly better performance when GIP is applied from earlier layers, with a gradual decrease as the starting layer increases. In contrast, IMDB-BINARY shows remarkably stable performance across all starting layers. This stability suggests that for simpler tasks like binary classification, applying GIP at deeper layers is sufficient to achieve good performance. These results indicate that while GIP is generally robust, its optimal application point may vary depending on the complexity of the task, with more complex tasks benefiting from earlier applications of GIP.\nEffect of GIP on learned graph representations. To visually demonstrate the effectiveness of GIP in separating graph manifolds, we employ t-SNE visualizations of pre-trained graph representations on various datasets. Figure 7 showcases the results on IMDB-M and IMDB-B datasets, which showed the largest improvements in downstream tasks, similar trends are observed across other datasets, which we discussed further in Appendix E. We compare DROPEDGE, ADDEDGE, and GIP strategies on both IMDB-M (multi-class) and IMDB-B (binary) datasets. The results demonstrate GIP's superior performance in manifold separation, significantly outperforming the other two methods. For both IMDB-M and IMDB-B, GIP-generated representations exhibit clear class clustering, with points of"}, {"title": "GSSL OBJECTIVE FUNCTION", "content": "This section presents the loss functions of four representative graph self-supervised learning methods for graph-level tasks: GRACE, MVGRL, BGRL, and G-BT. These methods can be categorized into two main approaches: mutual information maximization and redundancy reduction. GIP is implemented within all four frameworks.\nGRACE and MVGRL both aim to maximize mutual information using different estimators. GRACE utilizes an InfoNCE estimator for graph-level representations:\n$L_{GRACE} = -log \\frac{exp(s(f_g(G_i), f_g(G'_i))/\\tau)}{\\sum_{j=1}^{N} exp(s(f_g(G_i), f_g(G'_j))/\\tau)}$ (7)\nwhere $f_g (G_i)$ and $f_g(G'_i)$ are graph embeddings of two views of the same graph, s(, ) is a similarity function, and $ \\tau$ is a temperature parameter.\nMVGRL employs the Jensen-Shannon MI estimator to maximize mutual information between different structural views of graphs:\n$L_{MVGRL} = \\hat{I}^{(JS)} (f_g(G), f_{g'}(G'))$ (8)\nwhere $f_g(G)$ and $f_{g'}(G')$ are graph-level representations from two different views, and $\\hat{I}^{(JS)}$ is the Jensen-Shannon MI estimator defined as:\n$\\hat{I}^{(JS)} (f_g(G), f_{g'}(G')) = E_{(G,G') \\sim p} [log(D(f_g(G), f_{g'}(G'))] + E_{(G,G') \\sim P_{xp}}[log(1-D(f_g(G), f_{g'}(G')))]$ (9)\nHere, D is a discriminator function, and $P$ represents the distribution of graph pairs.\nIn contrast, BGRL and G-BT adopt the redundancy reduction principle. BGRL's loss function is inspired by BYOL and implicitly reduces redundancy through its bootstrapping mechanism:\n$L_{BGRL} = ||sg(f_t(G')) - f_o(G)||^2$ (10)\nwhere $f_t$ and $f_o$ are the target and online networks respectively, G and G' are two augmented views of a graph, and sg denotes stop-gradient.\nG-BT explicitly employs a redundancy reduction objective:\n$L_{G-BT} = \\sum_i (1 - C_{ii})^2 + \\lambda \\sum_{i} \\sum_{j \\neq i} C_{ij}^2$ (11)\nwhere C is the cross-correlation matrix between embeddings of different views, and $ \\lambda$ is a trade-off parameter."}, {"title": "EFFECT OF TWO-BRANCH DROPEDGE/ADDEDGE PARAMETERS", "content": "In this section, we present a detailed analysis of the ADDEDGE and DROPEDGE methods, comparing their performance across various datasets from the TU Dataset collection. As a supplement to Figure 4 in the main body, we analyze the GRACE framework as a case study here. Figures 9b and 9a visualize the results as 3D surface plots, where the x and y axes represent the probabilities of adding or dropping edges, respectively, and the z-axis represents the achieved accuracy.\nThe DROPEDGE method, as shown in Figure 9a, exhibits complex and highly dataset-dependent behavior. Across the six datasets (MUTAG, IMDB-MULTI, IMDB-BINARY, PROTEINS, NCI1, and DD), we observe no consistent optimal probability for edge dropping. Instead, each dataset presents a unique surface with varying patterns of peaks and valleys. For instance, MUTAG shows the highest accuracy when both dropping probabilities are low, while DD exhibits a distinctive pattern where accuracy peaks when one probability is high and the other is low. This variability suggests that the effectiveness of DROPEDGE is strongly influenced by the specific structural characteristics of each dataset. Similarly, the ADDEDGE method, visualized in Figure 9b, demonstrates equally"}, {"title": "ANALYSIS OF THE QUALITY OF THE LEARNED REPRESENTATION", "content": "In this section, we present 2D and 3D visualizations of graph representations pre-trained by GRACE with and without our GIP method. Figure 11 shows t-SNE projections of graph embeddings for three datasets: NCI1, PROTEINS, and DD. For each dataset, we compare three scenarios: DROPEDGE, ADDEDGE, and GIP.\nTaking the NCI1 dataset as an example (subfigures a, b, and c), we observe a high degree of overlap between data points from different manifolds (classes) in the DROPEDGE and ADDEDGE-derived representation distributions. In contrast, GIP significantly reduces this inter-manifold overlap. Although GIP does not produce two entirely separate clusters in the representation space, it is evident that the distributions of the two manifolds have been shifted relative to each other, resulting in improved separation."}, {"title": "CLASS-BASED MANIFOLD SEPARATION PROXY (CMSP)", "content": "To quantitatively evaluate the effectiveness of graph embedding methods in preserving and potentially enhancing the underlying manifold structure, we introduce the Class-based Manifold Separation Proxy (CMSP). This metric is designed to assess how well the embedding method distinguishes between different classes of graphs in the embedded space, serving as a proxy for manifold separation. We base this approach on the assumption that graphs from the same class are likely to lie on or near the same manifold in the high-dimensional space, while graphs from different classes are likely to lie on different manifolds. While we do not have direct access to the true manifold structure, we use class labels as proxies for manifold assignments. This allows us to quantify the degree of separation between these assumed manifolds in the embedding space. The CMSP is particularly relevant for supervised learning tasks such as graph classification, where the goal is to distinguish between different classes of graphs. The CMSP is defined through a series of calculations on the embedded representations. First, we compute the Intra-class Dispersion ($D_k$) for each class k, which\nwe interpret as the dispersion within a manifold:\n$D_k = \\frac{1}{n_k} \\sum_{i \\neq j}^{n_k} ||x_i - x_j||^2$ (12)\nwhere $x_i$ is the embedding vector of the i-th sample in class k, and $n_k$ is the number of samples in class k. We then calculate the Average Intra-class Dispersion ($D_{avg}$) across all K classes:\n$D_{avg} = \\frac{1}{K} \\sum_{k=1}^{K} D_k$ (13)\nTo measure the separation between classes, which we interpret as separation between manifolds, we compute the Inter-class Separation (S) as the average distance between class centroids:\n$S = \\frac{2}{K(K-1)} \\sum_{i<j} ||\\mu_i - \\mu_j||^2$ (14)\nwhere $\\mu_k = \\frac{1}{n_k} \\sum_{i=1}^{n_k} x_i$ is the centroid of class k. Finally, we define the Class-based Manifold Separation Proxy (CMSP) as the ratio of inter-class separation to intra-class dispersion:\n$CMSP = \\frac{S}{D_{avg}}$ (15)\nA higher CMSP value indicates better separation between classes in the embedding space, which we interpret as improved separation between the underlying manifolds. This metric allows for a direct comparison between different embedding methods, capturing their ability to produce representations that preserve and potentially enhance the manifold structure of the data, as approximated by class labels. It's important to note that while we use class labels as proxies for manifold assignments, this approach has limitations. The true manifold structure of the data may be more complex than what is captured by class labels alone. However, in the context of graph classification tasks, where the goal is often to distinguish between different classes of graphs, this approximation provides a practical and interpretable measure of embedding quality and manifold separation."}, {"title": "ENHANCED MANIFOLD SEPARATION IN GRAPH INTERPLAY (GIP)", "content": "G.1 DEFINITIONS AND ASSUMPTIONS\nDefinition 1 (Graph Set and Intrinsic Manifolds). Let $S = {G_1, G_2, ..., G_N }$ be a set of N graphs. Assume these graphs lie on K underlying manifolds $F = {M_1, M_2, ..., M_K}$ in a high-dimensional space. Define the mapping function $ \\mu : G \\rightarrow {1, ..., K}$ that assigns each graph to its corresponding manifold.\nDefinition 2 (Graph Distribution). For each manifold $M_k$, assume there exists a probability distribution $P_k$ from which graphs on $M_k$ are sampled. Let $G \\sim P_k$ denote a graph randomly sampled from manifold $M_k$.\nDefinition 3 (SSL Embedding). Let $f_S : G \\rightarrow R^d$ be the well-trained GNN embedding function obtained through SSL, which maps graphs to a d-dimensional Euclidean space.\nDefinition 4 (Manifold-Relevant Information). For a manifold $M_k$, we define the manifold-relevant information $Z_k$ as a random variable representing the embedding of a graph randomly sampled from $M_k$:\n$Z_k = f_S(G), G \\sim P_k$ (16)\nwhere $P_k$ is the probability distribution over graphs in manifold $M_k$, and $f_S$ is the SSL embedding function.\nLemma 1 (GIP Transformation). Consider a GNN with n layers (n \u2265 1) used in Graph Interplay (GIP), under the following conditions:\n\u2022 Each layer of the GNN consists of a linear transformation followed by a ReLU activation function."}]}