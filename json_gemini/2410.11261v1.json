{"title": "Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix", "authors": ["Yingyu Liang", "Jiangxuan Long", "Zhenmei Shi", "Zhao Song", "Yufa Zhou"], "abstract": "Large Language Models (LLMs) have shown immense potential in enhancing various aspects of our daily lives, from conversational AI to search and AI assistants. However, their growing capabilities come at the cost of extremely large model sizes, making deployment on edge devices challenging due to memory and computational constraints. This paper introduces a novel approach to LLM weight pruning that directly optimizes for approximating the attention matrix, a core component of transformer architectures. Unlike existing methods that focus on linear approximations, our approach accounts for the non-linear nature of the Softmax attention mechanism. We provide theoretical guarantees for the convergence of our Gradient Descent-based optimization method to a near-optimal pruning mask solution. Our preliminary empirical results demonstrate the effectiveness of this approach in maintaining model performance while significantly reducing computational costs. This work establishes a new theoretical foundation for pruning algorithm design in LLMs, potentially paving the way for more efficient LLM inference on resource-constrained devices.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) based on the transformer architecture [VSP+17], including GPT-40 [Ope24a], Claude [Ant24], and OpenAI's recent o1 [Ope24b], have shown immense potential to enhance our daily lives. They revolutionize fields like conversational AI [LCT+24], AI agents [XCG+23, CYL+24], search AI [Ope24b], and AI assistants [MWY+23, ZKAW23, KHC+24, FJL+24]. With their growing capabilities, LLMs are powerful tools shaping the future of technology. However, the current state-of-the-art LLM weights number is extremely large. For instance, the smallest version of Llama 3.1 [LT24] needs 8 billion parameters, which takes more than 16GB GPU memory with half float precision and requires significant inference time. Due to large memory and high computational cost, deploying such models on edge devices such as smartphones becomes challenging.\nTo reduce the LLM model size, many studies work on pruning the LLMs model weights to relax the device memory constraint and minimize response latency. The classical pruning problem in LLMs can be formulated as follows. Given a weight matrix $W \\in \\mathbb{R}^{d \\times d}$ and some calibration data $X \\in \\mathbb{R}^{n \\times d}$, where $n$ is input token length and $d$ is feature dimension, the goal is to find a matrix $W$ under some sparse constraint such that $||XW - XW||$ being small under some norm function. The above formulation has been widely used in many state-of-the-art pruning methods, such as SparseGPT [FA23] and Wanda [SLBK24].\nHowever, the current object functions only focus on the approximation of a linear function $XW$. Their optimal solutions do not have a good approximation to the attention matrix (see Figure 2 for details). Note that the attention mechanism is the kernel module of the transformer architecture. The high-level insight of their bad performance is that the Softmax function is very sensitive to the large positive values of the input due to its exp scaling effect while pruning mask based on linear approximation cannot capture this sensitivity. Thus, in this work, we directly compute the pruning mask on weights to approximate the attention matrix, which is a highly non-linear function, Softmax$(XW X^T) \\in \\mathbb{R}^{n \\times n}$. To the best of our knowledge, this paper is the first work studying attention weight pruning to directly approximate the attention matrix. We provide a theoretical guarantee that optimization based on Gradient Descent (GD) on our loss function can converge to a good pruning mask solution (Theorem 1.3). Furthermore, we preliminarily verified the effectiveness of our method with empirical support (Section 6). Our theoretical foundation may pave the way for more efficient LLM inference on resource-constrained devices.\nIn the following, we introduce some key backgrounds and our contributions in detail."}, {"title": "1.1 Key Background", "content": "We define the attention matrix in self-attention mechanism as below:\nDefinition 1.1 (Attention Matrix). Let $X \\in \\mathbb{R}^{n \\times d}$ be the input. Given query and key weights matrix $W_Q, W_K \\in \\mathbb{R}^{d \\times d}$, we define $W := W_QW_K$. Then, we have the Softmax attention matrix being\nSoftmax$(XW X^T) = D^{-1} exp(XW X^T)$,\nwhere (1) $D := diag(exp(XW X^T) \\cdot 1_n)$, (2) $exp$ denotes the exponential function and is applied entry-wisely, (3) diag$(\\cdot)$ operation takes a vector and outputs a diagonal matrix with the entries of that vector, and (4) $1_n$ denotes the length-$n$ all ones vector.\nFurther, we introduce the problem setup of our Attention Weights Pruning. By selectively reducing the number of non-zero elements in the attention weight matrix $W$ in Definition 1.1, we"}, {"title": "1.2 Our Contributions", "content": "This is the first work studying the Attention Weights Pruning problem, which is an approximation problem to a non-linear function. We provide an algorithm to obtain the near-optimal pruning mask based on Gradient Descent (GD) with convergence guarantee.\nTheorem 1.3 (Main result, informal version of Theorem 4.1). For any $\\epsilon > 0$, our Algorithm 1 can converge to the near-optimal pruning mask for the Attention Weights Pruning problem (Definition 1.2) in $O(d \\cdot poly(n) / \\epsilon)$ time with $O(\\xi + \\epsilon)$ error, where $\\xi$ is a small term depending on intrinsic property of the data and weights."}, {"title": "2 Related Work", "content": "Model compression plays a critical role in improving the efficiency and deployment of large language models (LLMs) [ZLL+23] for its effectiveness in reducing computational overhead while preserving performance. Common compression techniques include quantization [PKL+24, XLS+23, HKM+24], pruning [CJD+21, HABN+21, HCI+21, JCR+22, FA22, FA23, SLBK24, LLSS24, ZDH24, ZZL+24, XZL+23, ACdN+24, CLS+24], and knowledge distillation [HLY+23, SSS23, JCCW23, WKM+23]. Specifically, pruning techniques have been developed extensively, such as unstructured pruning, which removes individual weights [LLSS24, SLBK24], and structured pruning, which eliminates entire components like neurons or attention heads [MLN19, ACdN+24, XGZC24]. [SLBK24] proposed Wanda, a novel unstructured pruning technique that uses weight-activation products to induce up to 50% sparsity in LLMs without retraining, achieving competitive results with significantly lower computational cost. SparseGPT [FA23] introduced a one-shot pruning method that achieves up to 60% sparsity in large GPT-family models with minimal impact on performance. A follow-up work [LLSS24] improved the complexity analysis of SparseGPT, reducing the running time from $O(d^3)$ to $O(d^{2.53})$, enabling faster pruning on LLMs. These techniques together contribute to more scalable and resource-efficient LLMs, maintaining competitive performance while having substantial reductions in computational resources."}, {"title": "2.2 Attention Acceleration", "content": "Attention mechanism has faced criticism due to its quadratic time complexity with respect to context length [VSP+17]. Addressing this criticism, a variety of approaches are employed, including sparse attention [HCI+21, KKF+23, FA23, LLSS24], low-rank approximations [RSW16, LLR16, HSW+22, ZL24, HSK+24], and kernel-based methods [CKNS20, LZ20, DSWZ23, ZHDK23, LSSS24], to reduce computational overhead and improve scalability. [AA22] enable the derivation"}, {"title": "3 Preliminary", "content": "In this section, we introduce some basic concepts and key definitions. In Section 3.1, we introduce some basic notations we use in this paper. In Section 3.2, we provide the definition of attention weights pruning."}, {"title": "3.1 Notations", "content": "For any positive integer $n$, we use $[n]$ to denote set $\\{1,2,\\ldots, n\\}$. For each $a, b \\in \\mathbb{R}^n$, we use $a \\circ b \\in \\mathbb{R}^n$ to denote the Hadamard product, i.e., the $i$-th entry of $(a \\circ b)$ is $a_i b_i$ for all $i \\in [n]$. For $A \\in \\mathbb{R}^{m \\times n}$, let $A_i \\in \\mathbb{R}^n$ denote the $i$-th row and $A_{\\*,j} \\in \\mathbb{R}^m$ denote the $j$-th column of $A$, where $i \\in [m]$ and $j \\in [n]$. We use $exp(A)$ to denote a matrix where $exp(A)_{i,j} := exp(A_{i,j})$ for a matrix $A \\in \\mathbb{R}^{n \\times d}$. We use $||A||_F$ to denote the Frobenius norm of a matrix $A \\in \\mathbb{R}^{n \\times d}$, i.e., $||A||_F := \\sqrt{\\sum_{i \\in [n]} \\sum_{j \\in [d]} |A_{i,j}|^2}$.\nFor a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, $A \\geq 0$ means that $A$ is positive semidefinite (PSD), i.e., for all $x \\in \\mathbb{R}^n$, we have $x^T A x \\geq 0$."}, {"title": "3.2 Attention Weights Pruning", "content": "We aim to determine a near-optimal pruning mask $M$ for the attention weights in a self-attention mechanism. Furthermore, we incorporate causal attention masking\u00b9 into our method to be more aligned with the current decoder-only LLM architecture, while our analysis can be applied to any general attention mask, e.g., block-wise attention mask. To formalize this, we provide the formal definition of causal attention mask and attention weights pruning in this section.\nThe causal attention mask ensures that each token in the sequence can attend only to itself and preceding tokens. Here, we provide the formal definition for the causal attention mask:\nDefinition 3.1 (Causal attention mask, [LSS+24]). We define the causal attention mask as $M_c \\in \\{0,1\\}^{n \\times n}$, where $(M_c)_{i,j} = 1$ if $i \\geq j$ and $(M_c)_{i,j} = 0$ otherwise.\nNow, we incorporate Attention Weights Pruning (see Definition 1.2) with causal attention mask $M_c$.\nDefinition 3.2 (Attention Weights Pruning with Causal Attention Mask). Let $M_c \\in \\{0,1\\}^{n \\times n}$ be the causal attention mask defined in Definition 3.1. Let $A := exp(XW X^T) \\circ M_c$ and $A :="}, {"title": "4 Main Results", "content": "In this section, we provide our main results. We provide an Algorithm 1 for Attention Weights Pruning problem based on Gradient Descent (GD). We also prove the convergence for our GD algorithm in Theorem 4.1.\nTheorem 4.1 (Main result, formal version of Theorem 1.3). Let $M, X, W, D, \\tilde{D}, A, \\tilde{A}, L, L_{attn}$ be defined in Definition 3.2. Assume $XX^T \\geq \\beta I$ and $\\min_{i,j\\in[n]} (D^{-1}\\tilde{A})_{i,j} \\geq \\delta > 0$. Furthermore,"}, {"title": "5 Technique Overview", "content": "In Section 5.1, we introduce some useful tools from previous work. In Section 5.2, we derive the close form of the gradient of Attention Weights Pruning. In Section 5.3, we calculate the Lipschitz constant of that gradient. In Section 5.4, we prove the PL inequality for our loss function."}, {"title": "5.1 Previous Tools on Convergence of GD", "content": "To analyze the convergence behavior of GD for our optimization problem (Definition 3.2), we first introduce the concept of g-proxy, $\\xi$-optimal Polyak-\u0141ojasiewicz(PL) inequality [Pol63, Loj63, KNS16], under which GD will converge:"}, {"title": "5.2 Closed Form of Gradient", "content": "As a first step, we compute the close form of the gradient $\\nabla_M L(M)$. The pruning mask $M$ is inside a non-linear function Softmax, which complicates our calculation. We defer the proof to Section B.\nTheorem 5.3 (Closed form of gradient, informal version of Theorem C.5). Let $L(M)$ be defined in Definition 3.2. Let $p$ be defined in Definition C.1. Let $X \\in \\mathbb{R}^{n \\times d}, M \\in [0,1]^{d \\times d}, W \\in \\mathbb{R}^{d \\times d}$. Then, we have\n$\\frac{dL(M)}{dM} = W \\circ (X^T p X^T) + \\lambda M$.\nBased on Theorem 5.3, we calculate the gradient of the pruning mask from Line 10 to Line 15 in our Algorithm 1."}, {"title": "5.3 Lipschitz of Gradient", "content": "Having obtained the close form of gradient, we proceed to investigate its Lipschitz continuity. We aim to show that the gradient $\\nabla_M L(M)$ is Lipschitz continuous with respect to $M$.\nTheorem 5.4 (Lipschitz of the gradient, informal version of Theorem E.8). Let $R$ be some fixed constant satisfies $R > 1$. Let $X \\in \\mathbb{R}^{n \\times d}, W \\in \\mathbb{R}^{d \\times d}$. We have $||X||_F < R$ and $||W||_F < R$. Let $L(M)$ be defined in Definition 3.2. For $M, M \\in \\mathbb{R}^{d \\times d}$, we have\n$||\\nabla_M L(M) - \\nabla_M L(\\tilde{M})||_F \\leq (\\lambda + 30dn^{7/2}R^6) \\cdot ||M - \\tilde{M}||_F$.\nWe defer the proof to Section E. Establishing the Lipschitz continuity of the gradient satisfies one of the necessary conditions for applying Theorem 5.2. The above theorem implicates that the gradient for $M$ is upper bounded, providing a way to choose step size."}, {"title": "5.4 PL Inequality of Gradient", "content": "Next, we need to verify that our loss function satisfies the PL inequality with appropriate parameters. To complete the verification of the conditions required for convergence, we demonstrate that $L(M)$ satisfies the PL inequality. We show that $\\nabla_M L(M)$ satisfies the PL inequality in this lemma:\nLemma 5.5 (PL inequality, informal version of Lemma F.10). Let $M, X, W, D, \\tilde{D}, A, \\tilde{A}, L, L_{attn}$ be defined in Definition 3.2. Assume that $XX^T \\geq \\beta I$ and $\\min_{i,j\\in[n]} (D^{-1}\\tilde{A})_{i,j} \\geq \\delta > 0$. Also,\nThen, we have\n$\\frac{1}{2} ||\\nabla_M L(M)||^2 \\geq \\mu (2L_{attn}(M) + \\frac{2\\lambda^2}{\\eta^2}||M|| - \\xi)$.\nWe defer the proof to Section F. By confirming that $L(M)$ satisfies the PL inequality and that its gradient is Lipschitz continuous, we then apply Theorem 5.2 to conclude that GD will converge to a solution within our desired error tolerance, and further prove Theorem 4.1.\nTo prove the PL inequality, we also need the following two key Lemmas, which introduce our two assumptions in our Theorem 4.1, $XX^T > \\beta I$ and $\\min_{i,j\\in[n]} (D^{-1}A)_{i,j} \\geq \\delta > 0$.\nLemma 5.6 (Informal version of Lemma F.4). Let $B \\in \\mathbb{R}^{n \\times n}$ and $X \\in \\mathbb{R}^{n \\times d}$. Assume that $XX^T > \\beta I$. Then, we have\n$||X^T B X||_F > \\beta ||B||_F$.\nLemma 5.7 (Informal version of Lemma F.7). Let $B \\in \\mathbb{R}^{n \\times n}$ and each row summation is zero, i.e., $B \\cdot 1_n = 0_n$. Let $B \\in [0,1]^{n \\times n}$ and each row summation is 1, i.e., $B \\cdot 1_n = 1_n$. Assume that $\\min_{i,j\\in[n]} B_{i,j} \\geq \\delta > 0$. Then, we can show\n$||BB - diag((B \\circ B) \\cdot 1_n) B||_F \\geq \\delta \\cdot ||B||_F$."}, {"title": "6 Experiment", "content": "In this section, we discuss the experiments conducted to illustrate the effectiveness of our Algorithm 1. We first introduce our settings in Section 6.1. Then, we present our results in Section 6.2."}, {"title": "6.1 Settings", "content": "Method and evaluation. We implement our method following the pseudocode in Algorithm 1, using NumPy and JAX for acceleration. We evaluate our method on unstructured sparsity, meaning that zeros can occur anywhere within the attention weight matrix $W$. Specifically, we use Definition 3.2 as our loss function, optimizing over the pruning mask $M$ using gradient descent based on the closed-form expression derived in Theorem 5.3. To accelerate convergence, we leverage momentum into the optimization process and fix the momentum parameter at 0.9. After obtaining the optimal pruning mask, we convert $M$ to a binary pruning mask to prune $W$, maintaining sparsity"}, {"title": "7 Conclusion", "content": "This paper introduced a novel approach to LLM weight pruning that directly optimizes for approximating the attention matrix. We provided theoretical guarantees for the convergence of our Gradient Descent-based algorithm to a near-optimal pruning mask solution. Preliminary results"}, {"title": "A Preliminary", "content": "In Section A.1, we introduce some notations we use in this paper. In Section A.2, we provide some basic facts."}, {"title": "A.1 Notations", "content": "For any positive integer $n$, we use $[n]$ to denote set $\\{1,2,\\ldots,n\\}$. For two vectors $x \\in \\mathbb{R}^n$ and $y \\in \\mathbb{R}^n$, we use $\\langle x, y \\rangle$ to denote the inner product between $x, y$, i.e., $\\langle x, y \\rangle = \\sum_{i=1}^n x_i y_i$. For each $a, b \\in \\mathbb{R}^n$, we use $a \\circ b \\in \\mathbb{R}^n$ to denote the Hadamard product, i.e. the $i$-th entry of $(a \\circ b)$ is $a_i b_i$ for all $i \\in [n]$. We use $e_i$ to denote a vector where only $i$-th coordinate is 1, and other entries are 0. We use $1_n$ to denote a length-$n$ vector where all the entries are ones. We use $||x||_p$ to denote the $l_p$ norm of a vector $x \\in \\mathbb{R}^n$, i.e. $||x||_1 := \\sum_{i=1}^n |x_i|$, $||x||_2 := (\\sum_{i=1}^n x_i^2)^{1/2}$, and $||x||_{\\infty} := \\max_{i\\in[n]} x_i$.\nFor $A \\in \\mathbb{R}^{m \\times n}$, let $A_i \\in \\mathbb{R}^n$ denote the $i$-th row and $A_{\\*,j} \\in \\mathbb{R}^m$ denote the $j$-th column of $A$, where $i \\in [m]$ and $j \\in [n]$. For a square matrix $A$, we use $tr[A]$ to denote the trace of $A$, i.e., $tr[A] = \\sum_{i=1}^n A_{i,i}$. For two matrices $X, Y \\in \\mathbb{R}^{m \\times n}$, the standard inner product between matrices is defined by $\\langle X, Y \\rangle := tr[X^T Y]$. We use $exp(A)$ to denote a matrix where $exp(A)_{i,j} := exp(A_{i,j})$ for a matrix $A \\in \\mathbb{R}^{n \\times d}$. For $k > n$, for any matrix $A \\in \\mathbb{R}^{k \\times n}$, we use $||A||$ to denote the spectral norm of $A$, i.e. $||A|| := \\sup_{x\\in \\mathbb{R}^n} \\frac{||Ax||_2}{||x||_2}$. We use $||A||_{\\infty}$ to denote the $l_{\\infty}$ norm of a matrix $A \\in \\mathbb{R}^{n \\times d}$, i.e. $||A||_{\\infty} := \\max_{i\\in[n], j\\in[d]} | A_{i,j}|$. We use $||A||_F$ to denote the Frobenius norm of a matrix $A \\in \\mathbb{R}^{n \\times d}$, i.e. $||A||_F := \\sqrt{\\sum_{i\\in[n]} \\sum_{j\\in[d]} |A_{i,j}|^2}$.\nFor a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, we use $A \\geq 0$ (positive semidefinite (PSD)), if for all $x \\in \\mathbb{R}^n$, we have $x^T A x \\geq 0$. We use $\\lambda_{min}(A)$ and $\\lambda_{max}(A)$ to denote the minimum and the maximum eigenvalue of the square matrix $A$, respectively. Let $A \\in \\mathbb{R}^{n \\times d}$. We use $a := vec(A)$ to denote a length $nd$ vector. We stack rows of $A$ into a column vector, i.e. $vec(A) := [a_1, a_2, ..., a_n]^T$ where $a_i$ is the $i$-th row of $A$, or simply $vec(A)_{j+(i-1)d} := A_{i,j}$ for any $i \\in [n], j \\in [d]$."}, {"title": "A.2 Facts", "content": "Fact A.1 (Indexing). Suppose we have matrices $U \\in \\mathbb{R}^{n \\times m}, V \\in \\mathbb{R}^{m \\times d}$. We define\nThen, we have the following:\nIndexing for one row: $X_i = V U_i^T \\in \\mathbb{R}^d$, i.e. $X_i = U_i V$, for $i \\in [n]$.\nIndexing for one column: $X_{\\*,j} = U V_{\\*,j} \\in \\mathbb{R}^n$ for $j \\in [d]$."}, {"title": "B Gradient Calculation", "content": "In this section, we introduce some definitions we used to compute $d L(M)$. First, we introduce the exponential function.\nDefinition B.1 (Exponential function $u, \\tilde{u}$). If the following condition hold\nWe define $u \\in \\mathbb{R}^{n \\times n}$ as follows\n$u := exp(XW X^T)$.\nWe define $\\tilde{u}(M) \\in \\mathbb{R}^{n \\times n}$ as follows\n$\\tilde{u}(M) := exp(X(M \\circ W) X^T)$.\nWe define $i_0$-th row of $\\tilde{u}(M)$ as follows\n$\\tilde{u}(M)_{i_0} := exp(X(M \\circ W) X^T)_{i_0}$."}, {"title": "B.1 Definitions", "content": "In this section, we introduce some definitions we used to compute $\\frac{d L(M)}{d M}$. First, we introduce the exponential function.\nDefinition B.1 (Exponential function $u, \\tilde{u}$). If the following conditions hold\nLet $X \\in \\mathbb{R}^{n \\times d}$.\nLet $W \\in \\mathbb{R}^{d \\times d}$.\nLet $M \\in [0, 1]^{d \\times d}$.\nLet $i_0 \\in [n]$.\nWe define $u \\in \\mathbb{R}^{n \\times n}$ as follows\n$u := exp(XWX^T)$.\nWe define $\\tilde{u}(M) \\in \\mathbb{R}^{n \\times n}$ as follows\n$\\tilde{u}(M) := exp(X(M \\circ W)X^T)$.\nWe define $i_0$-th row of $\\tilde{u}(M)$ as follows\n$\\tilde{u}(M)_{i_0} := exp(X(M \\circ W)X^T)_{i_0}$."}, {"title": "B.2 Gradient for Each Row of X(M\u3002W)XT", "content": "We introduce the Lemma of gradient for each row of X(M\u3002W)XT.\nLemma B.8. Let $i_1 \\in [d], j_1 \\in [d], i_0 \\in [n]$, we have\nWe can simplify the derivative expression\nwhere the first and second step follows from Fact A.1, the third step follows from Fact A.3.\nWe further compute Eq. (1):\nwhere the first follows from Fact A.4, the second step follows from for any matrix $X$, $X_{i,j} = (X^T)_{j,i}$, the third step follows from Fact A.7, and the fourth step follows from for any matrix $X$, $X_{i,j} = (X^T)_{j,i}$.\nFinally, we have\nwhere the first step follows from Eq. (1) and Eq. (2), and the second step and the third step follows from basic algebra.\nWe introduce the Lemma of gradient for each row of $\\tilde{u}(M)$."}, {"title": "B.3 Gradient for Each Row of $\\tilde(M)$", "content": "Lemma B.9. If the following condition hold:\nLet $\\tilde{u}(M)$ be defined in Definition B.1."}]}