{"title": "CMMaTH: A Chinese Multi-modal Math Skill Evaluation Benchmark for Foundation Models", "authors": ["Zhong-Zhi Li", "Ming-Liang Zhang", "Fei Yin", "Zhi-Long Ji", "Jin-Feng Bai", "Zhen-Ru Pan", "Fan-Hu Zeng", "Jian Xu", "Jia-Xin Zhang", "Cheng-Lin Liu"], "abstract": "Due to the rapid advancements in multimodal large language models, evaluating their multi-modal mathematical capabilities continues to receive wide attention. Despite the datasets like MathVista proposed benchmarks for assessing mathematical capabilities in multimodal scenarios, there is still a lack of corresponding evaluation tools and datasets for fine-grained assessment in the context of K12 education in Chinese language. To systematically evaluate the capability of multimodal large models in solving Chinese multimodal mathematical problems, we propose a Chinese Multi-modal Math Skill Evaluation Benchmark, named CMMaTH, contraining 23k multimodal K12 math related questions, forming the largest Chinese multimodal mathematical problem benchmark to date. CMMaTH questions from elementary to high school levels, provide increased diversity in problem types, solution objectives, visual elements, detailed knowledge points, and standard solution annotations. We have constructed an open-source tool GradeGPT integrated with the CMMaTH dataset, facilitating stable, rapid, and cost-free model evaluation. Our data and code are available.", "sections": [{"title": "1 Introduction", "content": "Large language models(LLMs) excel in various language tasks, while multimodal models effectively handle visual-language problems. They advance natural language processing and computer vision fields, providing powerful solutions for complex tasks. Multimodal large models demonstrate potential as versatile solvers for multimodal problems.\nThe systematic evaluation of large models' performance across various mathematical reasoning scenarios has been a subject of extensive research. GSM8K and MATH(Cobbe et al., 2021; Hendrycks et al., 2021b) assessed the ability in multi-step mathematical reasoning by constructing a high-quality set of elementary school math word problems or various competition mathematics problems. By collecting a diverse set of mathematical problems containing both textual and visual components, Lu et al. (2023); Wang et al. (2024); Zhang et al. (2024b) systematically evaluated the ability of large multimodal models to perceive visual elements and solve corresponding multimodal problems. Shi et al. (2023) constructed a multilingual mathematical reasoning dataset, MGSM, for evaluating the LLM reasoning ability in multilingual environments.\nHowever, in non-English multimodal contexts, especially in Chinese scenarios, there is still a lack of sufficiently detailed and diverse benchmarks for assessing mathematical abilities. To assess the capability of large language models in non-English contexts, Huang et al. (2023) and Zhang et al. (2024a) constructed multidisciplinary Chinese question answering datasets C-Eval and CM-MMU to evaluate the knowledge and reasoning abilities of multimodal large models. However, C-Eval lacks evaluation in multimodal contexts, while CMMMU's dataset has relatively low diversity, consisting of only 540 questions.\nExisting Math benchmarks for answer evaluation can be categorized into two types:Rule-based (Cobbe et al., 2021; Hendrycks et al., 2021b; He et al., 2024) and API-based methods (Lu et al., 2023; Zhang et al., 2024b; Hendrycks et al., 2021a). API-based methods are very costly and time-consuming, and they often result in unstable and inconsistent evaluation results. Rule-based methods, on the other hand, struggle to handle highly diverse contents of benchmarks. Also, it is difficult to maintain handcrafted rules for dynamically updated benchmarks. Current multimodal math benchmark evaluations often resort to multiple-choice or true/false question formats, using rules or API-based LLM to extract options for assessing answers.\nBased on above considerations, we propose a new multimodal mathematical benchmark CMMaTH. Compared to previous benchmarks, our benchmark demonstrates greater diversity, increased depth of reasoning, and finer-grained knowledge annotation for multimodal models to grasp different levels and types of knowledge. We provided and open-sourced a lightweight answer comparator called GradeGPT, designed to compare the consistency between outputs from different LLM/LMMs and standard answers, thus avoiding expensive evaluation costs. Leveraging the CMMaTH dataset and GradeGPT tool, we evaluated mainstream open-source and commercial multimodal large models in Table 4, reporting comprehensive evaluation results along with extensive case analyses. In summary, our paper makes the following contributions:\n\u2022 We introduce the largest high-quality Chinese multimodal mathematics benchmark with the most detailed annotation granularity to date. We also provide an English version of this dataset. The CMMaTH dataset is a dynamically maintained and will be periodically updated.\n\u2022 Compared to previous multimodal mathematical benchmarks, our dataset exhibits great depth of reasoning and diversity. Our benchmark simulates more realistic educational Q&A scenarios, encompassing a wider variety of question types and answer formats. Additionally, we annotate each question with detailed knowledge points and corresponding skills to evaluate the mastery level of current large models.\n\u2022 We build an evaluation assistant named GradeGPT on the CMMaTH dataset, which allows for comparing the proximity of model responses to standard answers and assessing the correctness of results and processes. GradeGPT features lightweight open-source characteristics, avoiding the instability and high costs associated with commercial models.\n\u2022 We conduct a systematic evaluation of existing mainstream multimodal large models, quantitatively and qualitatively comparing with existing models."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Assessment of mathematical abilities", "content": "To evaluate the performance of large models in mathematical reasoning and examine hallucinations during the reasoning process, numerous benchmarks have been proposed for evaluating the mathematical reasoning capabilities of large models. GSM8K(Cobbe et al., 2021) is the first and most widely used mathematical dataset used for large model math evaluation, consisting of 1k math word problem test samples and corresponding answers. The MATH(Hendrycks et al., 2021b) dataset, in comparison to GSM8K, presents a greater challenge in terms of reasoning difficulty. This dataset demands a more profound understanding and intuition in various mathematical domains such as Algebra, Number Theory, and Geometry."}, {"title": "2.2 Large Model Evaluation Tool", "content": "Due to their strong generalization capabilities and extensive world knowledge, large language models have achieved outstanding results in tasks such as machine translation(Zhu et al., 2023), question answering(Kamalloo et al., 2023), dialogue(Duan et al., 2023) and so on by generating text. Evaluating the comprehensive abilities of large models, such as clarity, adherence to instructions, comprehensiveness, formality, and mathematical reasoning ability, has received widespread attention(Ke et al., 2023). Currently, many works opt to use powerful commercial model APIs, such as GPT-4, to assist in evaluating the comprehensive abilities of large models. For instance, MathVista(Lu et al., 2023) and GeoEval(Zhang et al., 2024b) use GPT-4's API to extract correct answers for evaluation. These methods face several challenges: they are costly and time-consuming, and they struggle to keep up with rapid model iterations. Besides, these methods face challenges in terms of consistency and reproducibility(Wang et al., 2023a; Ke et al., 2023).\nRecent methods have proposed using metrics such as BERT score(Zhang et al., 2020) or MAUVE(Pillutla et al., 2021) for evaluation. However, the numerical indicators produced by these methods are difficult to interpret when it comes to the erroneous responses generated by LLM. PandaLM and CritiqueLLM (Wang et al., 2023b; Ke et al., 2023) are similar to our work. They proposed a fine-tuning method based on open-source LLMs, distilling the evaluation capabilities of GPT-3.5 into a series of smaller open-source models. However, they are focused on the automated evaluation of more general text generation tasks, while we are targeting the automated evaluation of responses from large models for multimodal mathematics problems.\nUnlike PandaLM(Wang et al., 2023b) trying to evalution relative conciseness, clarity and so on, our evaluation model, GradeGPT, is a dataset-oriented answer comparator that can provide specific reasons based on the standard answer and a model's response. We distilled the answer comparison capability of GPT-4 using the Cross-Lingual Judge-of-Chain method and enhanced GradeGPT's answer discrimination ability."}, {"title": "3 CMMaTH Dataset", "content": ""}, {"title": "3.1 Overview of CMMATH", "content": "We selected diverse multimodal mathematical problems from a vast pool of K12 educational questions, comprising 23856 items across 13 visual themes, 5 difficulty levels, and encompassing 150 types of knowledge points. More detailed statistical data can be found in Table 1.\nFor the convenience of evaluation, we provide a miniaturized test set of CMMaTH, called CMMaTH-testmin, containing 1500 samples. Test-min retains the diversity of the CMMaTH dataset and shows similar overall performance to the entire CMMaTH dataset. Evaluators can conduct quick tests and generate preliminary analyses based on CMMaTH-testmin."}, {"title": "3.2 Collectioin Guidelines", "content": "We collected a large number of multimodal mathematics questions from a vast K12 educational question bank, including elements such as statistical charts, plane geometry, three-view diagrams, flowcharts, set notation diagrams, etc. The quality and distribution of the data were guided by the following criteria during collection.\n\u2022 Diverse Mathematical Visual Elements. We have collected solutions to multimodal mathematical problems that rely on understanding image content, especially those containing a large amount of Chinese visual content such as text and symbols. Table 2 shows some visual elements subject of CMMaTH.\n\u2022 High relevance to the K12 math knowledge and skill. The annotator, who is well-versed in knowledge, needs to ensure that the multimodal question assesses a specific K-12 mathematics knowledge point during the question collection process. It primarily includes mathematics questions related to K12 education, facilitating the assessment of the application potential of large-scale multimodal capabilities in the field of mathematics education.\n\u2022 High-quality images and answers. During the collection phase, we instruct collectors to disregard multimodal math questions with erroneous symbols or low-quality images (blurry images). Collectors are required to ensure that the collected questions are generally solvable."}, {"title": "3.3 Data Collections", "content": "Collection from Diverse Multimodal Math Sources CMMaTH's data is based on a million-level private database. The private database we used comes from questions collected from the Internet and undergoes rigorous data checking. The project's data has undergone multiple rounds of collection. We first sampled 45,000 multimodal math questions: 14,000 each from elementary, high, and junior high schools. Then, we added 34,000 more questions featuring algorithm block diagrams, statistics, and geometry diagrams to enhance visual diversity.\nData Filtering We filtered out all questions without images in the question stems, including questions with multi-graph reasoning, questions in non-Chinese languages, and questions not relying on visual content to solve. To ensure the quality of the images and text questions, we removed all images whose width and height were less than 100, then used the GPT4 API to score the data quality and filter out questions suspected of being unsolvable and questions with garbled text in the question text.\nData Labeling For K-12 mathematics knowledge points, we have scraped the mathematics section from Jiaoyan Cloud\u00b9 and organized all the knowledge points into a knowledge tree including a total of 5,531 knowledge points. We retained 2,299 knowledge points more relevant to multimodal"}, {"title": "3.4 Comparison with Existing Benchmarks", "content": "The CMMaTH dataset is primarily used to evaluate multimodal reasoning capabilities in K-12 educational scenarios. We compared the current mainstream multimodal mathematical datasets and large model benchmarks in Table 3. Compared to existing multimodal benchmarks and multimodal reasoning benchmarks, the CMMaTH dataset has the following characteristics:\nExtreme Diversity Currently, there is a severe lack of high-quality Chinese multimodal mathematics datasets. MATH-VISION lacks a Chinese component, the MATH-VISTA dataset contains only a small number of Chinese samples, and CMMMU contains only 540 math problems, which are not fine-grained and comprehensive enough. We have included about 23k fine-grained multimodal mathematics assessment samples, covering 13 K12 mathematics visual categories, making it the largest known multimodal Chinese dataset to date.\nReal and High Quality & Multilingual MathVista features a substantial number of problems that are associated with natural and synthetic images. However, these images do not accurately represent the genuine data distribution encountered in K12 mathematics educational settings. OlympiadBench is an Olympiad-level bilingual multimodal benchmark. However, this benchmark is overly challenging and deviates from the application of LMM in real K12 multimodal math scenarios. Additionally, the variety of multimodal visual elements is relatively limited. Instead, we collect multimodal data specifically tailored to the K12 education context. Additionally, MathVista incorporates a significant amount of data from GeoQA and synthetic images, which have relatively poor image quality. Our multimodal visual image elements have all undergone stringent image quality assessments. Unlike CM-MMU, CEval, and CMath, our dataset is a bilingual dataset that considers a large number of Chinese scenes. In addition to the text of the questions being in Chinese, the visual elements related to the questions also contain Chinese text/symbols.\nHigh-quality Fine-grained Annotation and Evaluation Tool Every question in our dataset is meticulously annotated with standardized answers, solutions expressed in natural language, associated multimodal knowledge points, visual element categories, and K-12 grade levels. This fine-grained annotation enables a more nuanced evaluation of multimodal mathematical proficiency within the K-12 educational context. While MathVista and GeoEval rely on GPT-4 for answer extraction and validation, we introduce an open-source model named GradeGPT. GradeGPT stands out by providing a stable, cost-free, and swift accuracy evaluation specifically tailored for the CMMaTH dataset."}, {"title": "4 GradeGPT", "content": "The CMMaTH dataset encompasses a large variety of problem-solving objectives, such as mathematical expressions, multiple-choice options, numerical outcomes, coordinate points, conclusion figures, and correctness assessments. Traditionally, in reasoning or evaluation contexts, problems have been formulated as multiple-choice or true/false questions to facilitate comparison and to simplify the extraction of results. Also, it is difficult to maintain dynamically updated benchmark. Employing API models for evaluation is prohibitively expensive, and the resulting evaluations are not consistently stable, which also hampers the iterative development of models on benchmarks, such as hyperparameter selection."}, {"title": "5 Experiments", "content": "We conducted a series of experiment to evaluate various models on the CMMaTH dataset. We evaluated various LLM/LMM models, including open-source and closed-source models. More model details can be found in Table 13. We employed a method similar to GeoEval and MathVista, generating captions through an GPT4V, and assessed them using MetaMath, and DeepSeekMath equipped with caption information. Our empirical research reveals that even the most advanced models struggle to achieve satisfactory accuracy levels. Furthermore, we conducted an exhaustive error analysis on a sufficiently strong commercial multimodal model, GPT-4V, examining its error distribution and presenting illustrative qualitative examples. Our investigation also revealed that the inclusion of multilingual thought chains does not mitigate the substantial difficulties presented by Chinese multimodal mathematical reasoning scenarios. We postulate that the richness of non-English contextual information contained within the images necessitates models equipped with enhanced multilingual OCR and sophisticated multimodal diagram reasoning capabilities."}, {"title": "5.1 Main Experiments on LLM/LMMS", "content": "We evaluated the results of mainstream multimodal large models and mathematical expert models in Table B. We analyzed the trend of existing large models in descending with problems and conditions, as well as the effectiveness of techniques such as Cross-Lingual Prompting in solving Chinese multi-modal mathematical problems. The experimental in Table 4 results indicates that our data exhibits extremely strong diversity and relatively challenging reasoning depth. Figure 1 and Table 4 shows models such as GPT4V struggle to comprehend our multimodal content and reasoning questions effectively, resulting in significant performance gaps between open-source and proprietary models. In certain rare visual domains, multimodal large models achieve very low reasoning outcomes.\nAccuracy on various question types. We evaluated the accuracy of GPT4V on various target-solving tasks in Figure 4. The results indicate that when solving free-form problems, especially those with more diverse targets such as expressions, coordinates, and conclusion judgments, the multimodal large language model shows poorer performance.\nIs OCR information sufficient for CMMaTH? We also referred to works like MathVista, attempting to use LLMs combined with OCR information from diagrams to assist in mathematical reasoning in Table 4. We found that, in our benchmark, a small amount of OCR information (such as mathematical symbols in diagrams, axis values, and image titles) made it very difficult to complete our multimodal mathematical reasoning tasks. The results indicate that solving problems in CMMaTH requires stronger multimodal mathematical chart capabilities, beyond just OCR.\nK12 Multimodal Knowledge Richness of current LMMs. We systematically evaluated the proficiency of existing multimodal large models in the K12 domain regarding multimodal reasoning skills in Figure 5. The results revealed a significant knowledge gap in existing multimodal K12 educational resources. Compared to other existing LMMs, GPT4V possesses a richer knowledge base, thereby substantially reducing the illusion of reasoning in multimodal mathematical inference."}, {"title": "5.2 Experiments of Cross-language Reason Technology", "content": "We also attempted several multilingual Chain-of-Thought approaches such as En-CoT, CLP(Cross-Lingual Prompting) used by Qin et al. (2023) to observe whether multimodal mathematical problems could be enhanced through context learning techniques without training. The results indicate that multilingual CoT methods face challenges in solving, possibly due to the abundance of Chinese contextual text in the image content, which may necessitate the model to demonstrate excellent cross-lingual OCR capabilities. We have included more details on the implementation of Cross-Lingual Prompting and En-CoT on the CMMaTH dataset in the Table 5."}, {"title": "5.3 Error Analysis", "content": "We conducted a detailed analysis and evaluation of GPT4V on CMMaTH-testmin, categorizing errors into four types: perceptual errors, reasoning errors, calculation errors, and Reject Errors. The error type distribution of GPT4V on CMMaTH is shown in the Figure 6.\nPerception Errors\nPerception Error refers to the model's erroneous interpretation and utilization of diagram content during reasoning. For example, incorrect OCR, misidentification of numerical relationships, geometric relationships, logical relationships, etc.\nReasoning Errors\nReasoning Error are quite common during the solving process. For instance, the model may misinterpret symbols or use incorrect logic or knowledge for inference. The frequency of Reasoning Errors reflects the model's logical and mathematical reasoning capabilities.\nCalculation Errors\nCalculation Error refers to the model performing incorrect mathematical operations, such as writing equations or solving equations incorrectly.\nReject Errors\nReject Error refers to the model's inability to solve a problem that is actually solvable. The frequency of such errors reflects the model's ability to follow instructions."}, {"title": "6 Conclusions", "content": "We introduce CMMaTH, a detailed Chinese math reasoning benchmark with diverse question types, vivid visuals, and complex reasoning. The benchmark includes detailed knowledge points, standard thought processes, and grade levels to measure the mastery of knowledge points in the K-12 multimodal math skill. To evaluate large multimodal models quickly and affordably, we built GradeGPT, an open-source tool for assessing results. Extensive experimental results on CMMaTH manifest the limitations of current models in multilingual, multimodal math reasoning."}, {"title": "Limitation & Potential Impact", "content": "Our dataset CMMaTH, as a multimodal mathematics dataset aimed at the K-12 education sector, can facilitate model evaluation and iteration of multimodal large models in this field, and may promote the research and development of educational artificial intelligence. CMMaTH primarily consists of single-image problems, without considering multi-image contextual reasoning or scenarios requiring auxiliary line drawing and similar tasks. GradeGPT is a result-oriented, relatively coarse reasoning response evaluator. How to construct a process evaluation model for fine-grained assessment of the reasoning ability of large models can continue to be explored in the future."}, {"title": "B Model Generation Details", "content": ""}, {"title": "B.1 Model Weight Version", "content": "We evaluated models on CMMaTH, including open-source models such as LLaVA-v1.5, Deepseek-Math, InternLM-XComposer2-VL, Yi-VL-34B, CogAgent-Chat, MetaMath-70B, LLama-70B, Baichuan-13B and Qwen-14B as well as state-of-the-art commercial models GPT4V. We have listed the parameter versions and the Hugging Face repository names of the open-source models used in Table 12."}, {"title": "B.2 Model Sampling Parameter", "content": "We have listed the corresponding hyperparameters used by the models in Table 13. For API models, we have indicated the corresponding release versions. Models using vLLM for inference are annotated."}, {"title": "B.3 Data quality control", "content": "To ensure the high quality of the final data, we conducted sampling and manual verification. We performed three random samples, each consisting of 500 multimodal samples, to check the data quality and ensure the consistency of the knowledge points and data."}, {"title": "C Prompt Details", "content": ""}, {"title": "C.1 Prompt For Step Response Generation", "content": "When evaluating hallucinations during the assessment process, we use a few-shot prompt format to elicit step-by-step outputs from the model as showed in Table 6."}, {"title": "C.2 Prompt For GradeGPT", "content": "We also listed the prompts used by GradeGPT in Tables 7."}, {"title": "C.3 Prompt For Cross-Lingual Prompting and En-CoT", "content": "We have listed the specific prompts used for En-CoT and Cross-Lingual Prompt during actual execution in Table 11. Unlike the original Cross-Lingual Prompt paper, for experimental simplicity, we only adopted a single-turn format. However, this suffices to illustrate the varying inferential capabilities across different languages in current LMMs."}, {"title": "DCMMaTH Dataset Details", "content": ""}, {"title": "D.1 Data Collection Details", "content": "To more clearly elucidate our data collection process, we have depicted the overall pipeline of data collection in Figure 7."}, {"title": "D.2 Knowledge Point Details", "content": "We provided detailed annotations of knowledge points for our dataset and conducted preliminary clustering of these knowledge points. The distribution of knowledge points in different clusters is as follows: We have formulated a Knowledge Successful Solve Rate(SSR) as a structural metric to gauge the proficiency level of multi-modal extensive models in mastering knowledge points. \\(N_{kn}\\) is the total number of knowledge point of CMMaTH. \\(Acc_{kni}\\) is the \\(Acc_{outcome}\\) of questions about i'th knowledge point. I denotes an indicator function.\n\\(SSR@a = \\frac{\\sum_{i=1}^{N_{kn}} I(Acc_{kni} > a)}{N_{kn}}\\)   (1)\nIt is our contention that a knowledge point can be deemed comprehensively understood only when the accuracy rate of solving problems related to that knowledge point surpasses a predefined threshold, denoted as a. For the purpose of our investigation, we have established a at the values of 0.1, 0.2, 0.3, and 0.6 to demarcate the levels of mastery."}, {"title": "D.3 Characteristics Of Annotators", "content": "We utilized a standard team of four people, who spent two weeks annotating the data. All annotators have a university undergraduate education and are well-versed in basic knowledge of the K12 education field. To ensure quality, each question was verified by at least two people."}, {"title": "E GradeGPT details", "content": ""}, {"title": "E.1 GradeGPT Prompt Detail", "content": "We have listed detailed Fewshot Examples using the GPT4-generated GradeGPT model responses in Table 11. Through this table, you can observe the specific form of the Cross-Lingual-Judge-of-Chain that we have used."}, {"title": "E.2 GradeGPT Performance Metric", "content": "GradeGPT performance evaluation metric is precision in comparison. We constructed a model that responds to a test set containing outputs from various large models (including both correct and incorrect model outputs). Each output is labeled as correct or incorrect based on its result. GradeGPT is tasked with assessing whether the model responses are correct or incorrect, and this performance evaluation metric is a binary classification metric.\n\\(Acc_{outcome} = \\frac{I(GradeGPT(R_i), Overcome_{CT})}{N_{response}} \\times 100\\)   (2)"}, {"title": "E.3 GradeGPT Training Details", "content": "We generated cross-lingual evaluation instruction pairs using the outputs from InternLM-XComposer, LLaVA-v1.5, CogAgent-18B and Yi-VL-34B. These outputs were produced using GPT-4 Fewshot. The generated evaluation instructions were filtered based on specific rules, retaining only those responses from GPT-4 that contained the fields: /. Ultimately, we constructed a cross-lingual format instruction set comprising 56k instruction pairs.\nGradeGPT was trained on 8 H800, with the Qwen-14B-Chat version used as the base model. The model's batch size was set to 16. The learning rate was set to 1e-4, and the gradient accumulation step was set to 16. It was trained for 10 epochs on a 40k bilingual Judge-of-Chain dataset. A detail example of instruction can refer to Figure 9."}, {"title": "E.4 Futher More Ablation Study", "content": "We conducted experiments on a development set comprising outputs from a 0.5k model. The development set was sampled from a subset of 0.5k questions on CMMaTH. Each question was accompanied by answers provided by GPT-4V, GPT-40, and middle school students. Each answer was manually annotated to indicate whether it was correct."}]}