{"title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE", "authors": ["Sichun Wu", "Kazi Injamamul Haque", "Zerrin Yumak"], "abstract": "Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available\u00b9.", "sections": [{"title": "1\nIntroduction", "content": "3D facial animation is not only crucial in film-making and game production but also in a variety of XR applications that involve digital humans. Creating facial animations for 3D characters re-quire lots of manual work from skilled technical artists or relies on performance capture pipelines. Researchers are actively addressing the challenges of 3D animation synthesis, aiming to minimize the manual effort. In particular, the relationship between speech and facial animation has been extensively studied and shown to be a promising direction [12, 29, 48, 61]. However, recent speech-driven 3D facial animation synthesis methods mostly focus on lip-sync and identity control neglecting the role of emotions and emotion control. They are also mostly deterministic methods limiting the generation of diverse facial animations. These limitations are what we address in this paper.\nEarly 3D facial animation generation synthesis methods use procedural approaches [7, 15] by employing linguistic rules to map phonemes to visemes (visual counterparts of phonemes). Despite their artist-friendly features, these models require defining explicit rules and their output is limited to lip-sync only. In recent years, end-to-end deep learning methods have demonstrated their effectiveness in speech-driven 3D facial animation synthesis [12, 18, 23, 29, 47, 51, 56, 61] generalizing to diverse audio inputs and different languages. They rely on 3D vertex-based datasets such as VOCASET [12], Multiface [60] and BIWI [19] with limited size and emotional variations. Lately non-deterministic approaches [2, 37, 48, 50, 62, 66] have been proposed and there is an increasing number of papers that are listed on Arxiv [9, 36, 40, 54]. A few approaches focus on rigged characters and blendshapes such as [5, 39, 48]. Another group of work focus on holistic motion gen-eration including face and body [8, 33, 38, 63]. While there is an increasing number of papers in this area, only a few papers focus on emotion control and emotionally-rich animation generation [14, 29, 41, 66].\nThe work from Karras et al. [29] although focusing on emotions, is based on a small dataset with two actors and there is no explicit emotion control. The closest to our work are EmoTalk [41] and EMOTE [14]. They both rely on 3D datasets constructed from 2D videos with EMOTE having superior visual quality and wider range of emotions. Similar to EMOTE, we opt for using the 3DMEAD dataset as it provides facial animations with various emotions at different intensity levels and for several identities. However, none of these methods are non-deterministic and they cannot gener-ate diverse outputs given the same audio input. A recent method [66] introduces a 4D high-quality dataset with emotion variations, however this dataset and codebase is not publicly available yet.\nThe contributions of our work are enumerated below:\n\u2022 A novel two-stage probabilistic (i.e. non-deterministic) emotion controllable speech-driven 3D facial animation synthesis model based on VQ-VAE, producing diverse yet high-quality facial ani-mations by learning a latent representation of emotional speech animation.\n\u2022 Extensive comparative analysis of our approach with respect to recent non-deterministic approaches using an enhanced list of objective metrics following [62].\n\u2022 Qualitative evaluation and perceptual user study to demonstrate the superior perceptual quality of our model's results compared to the state-of-the-art emotion-control enabled as well as non-deterministic models."}, {"title": "2 Related Work", "content": "This section provides a review of related work on speech-driven 3D facial animation synthesis using deep learning algorithms. Although there is a vast amount of deep learning methods that are used to generate 2D facial animation [28, 49, 64], that is out of the scope of this paper. Another group of work focus on learn-ing representations of facial animation, tracking and reconstruc-tion [13, 16, 22, 31]. 3D speech-driven facial animation methods typically relied on phoneme-based procedural approaches [7, 27] or intermediary representations [52]. With the end-to-end deep learning approaches [12, 18, 23, 29, 47, 51, 56, 61], a new era in 3D speech-driven facial animation started. Although providing promising results, these models are deterministic models limiting the generation of diverse outputs given the same speech input. They also do not provide explicit emotion control while most of them provide identity control. In this paper, we focus on non-deterministic speech-driven and emotion-controllable 3D facial animation syn-thesis. We will present the state-of-the-art in these two categories in the following two subsections."}, {"title": "2.1\nNon-Deterministic 3D Facial Animation", "content": "In recent years, researchers have increasingly focused on the non-deterministic aspects of human motion both for body [1, 4, 11, 53, 63] and facial motion [2, 37, 48, 50, 62, 66] as well as for holistic an-imation [8, 33, 38, 63]. Non-deterministic models are mostly based on Variational Auto Encoders (VAEs), Vector Quantized Variational Auto Encoders (VQ-VAEs) or diffusion models. Compared to Genera-tive Adversarial Networks (GANs), VAE-based and diffusion models stand out by generating diverse outputs by explicitly modeling the underlying data distribution [49]. Unlike traditional VAEs that en-code input into a continuous space, VQ-VAEs represent data using discrete codebook embeddings preventing the posterior collapse problem [58]. Richard et al. [46] propose a Temporal Convolutional Network (TCNN)-based VAE method to drive Codec Avatars [35], while Voice2Face [5] proposes an LSTM-based conditional VAE. MeshTalk [47] learns a categorical latent space while CodeTalker [61] uses VQ-VAE for learning a discrete code space. However, these methods were not explicitly declared non-deterministic and do not provide an evaluation on the diverse outputs generated. Learning to Listen [37] employs a transformer-based VQ-VAE for facial anima-tion synthesis in dyadic conversations and introduces evaluation metrics to assess diversity including diverseness within and across sequences. Yang et al. [62] uses Residual Vector-Quantized (RVQ) codebook achieving improved diversity and high fidelity in facial motion. They provide an extensive benchmarking framework and introduce novel evaluation metrics. FaceDiffuser [48] is the first model to apply diffusion models for the speech-driven 3D facial animation synthesis task. They introduce a new diversity metric that allows the comparison of this model to other deterministic models by measuring variation over identity instead of audio input. Facetalker [3] employs a transformer-based diffusion model and predicts animations of neural parametric head models (NPHMs) [21] offering detailed representations of the human head. They mea-sure diversity using metrics similar to the ones used in the body animation domain [45]. DiffPoseTalk [50] introduces a new dataset that also includes head poses which is constructed from 2D videos"}, {"title": "2.2 Emotion-Controllable 3D Facial Animation", "content": "There are a few papers in this space [14, 29, 41, 66]. Karras et al. [29] proposed an end-to-end method using CNNs aiming to resolve the ambiguity in mapping between audio and face by introducing an additional emotion component to the network. The dataset is based on two actors and cannot handle identity variations. In addi-tion, there is no explicit emotion control but emotions are learned from data. The advantage is that the dataset is collected with a commercial high-end 4D performance capture system. EmoTalk [41] introduces an emotion disentangling encoder to disentangle the emotion and content in the speech using a cross-reconstruction loss. In contrast with Karras et al. [29], they use a dataset that is semantically annotated with emotion labels. Given the emotion and content features, personal style and emotion control features, an emotion-guided multi-head attention decoder generates the out-put motion. Considering the limited availability of emotional 3D audio-visual datasets, EmoTalk addresses this gap by creating their own dataset 3D-ETF using two 2D datasets RAVDESS [34] and HDTF [65]. They use the \"Live Link Face\" application to map input videos to blendshape parameters. They also include a blendshape to FLAME [31] parameters converter which allows to transfer fa-cial expressions across different virtual characters. EMOTE [14] adopts a similar approach to EmoTalk and constructs a new dataset 3DMEAD based on the 2D dataset MEAD [59] including annotated 8 basic emotion types, 3 emotion intensity types per emotion type, and speaker identities. To simplify the problem, they first learn a motion prior based on FLAME parameters changing in time using a temporal VAE. In the audio-driven training stage, they combine audio, identity and emotion features using transformer encoder and decoder structures to infer the motion. In contrast with FaceFormer [18] and CodeTalker [61], they employ a non-autoregressive model for improved efficiency. Although EMOTE does not provide explicit objective and subjective evaluations with respect to EmoTalk, visual results indicates better visual quality. None of these models can produce diverse results in a non-deterministic manner. A recent pa-per Media2Face [66] proposes a two-stage model and a 4D dataset M2F-D. Different from EmoTalk and EMOTE, emotion control is not categorical emotions but rely on CLIP [44] text encoding. In the first stage, a latent space is learned using geometry and expression VAE models. The extracted latent codes are used to augment the dataset with 2D videos such as MEAD [59], RAVDESS [34] and HDTF [65] similar to EmoTalk and EMOTE. In the second stage a transformer-based diffusion model is used. Although Media2Face is essentially a non-deterministic generative model, they do not employ an explicit diversity metric and use only FDD (Upper face Dynamics Deviation) metric similar to CodeTalker [61]. The visual quality of the results are good, however the dataset and codebase is not yet available for direct comparison."}, {"title": "3 Methodology", "content": "In this section, we describe our methodology including a descrip-tion of the dataset, problem formulation, details about the pro-posed model, and two other non-deterministic approaches (VAE and diffusion-based) that are used to compare and evaluate our proposed model. The results and the comparative analysis are then presented in Sec. 4."}, {"title": "3.1 Dataset: 3DMEAD", "content": "3DMEAD dataset is reconstructed from 2D audio-visual dataset MEAD [59]. The 3D reconstruction from the 2D videos was carried out using DECA [20] and MICA [69] methods which was first in-troduced with the EMOTE [14] paper. 3DMEAD dataset includes 3D reconstructions of 47 subjects speaking in English, comprising eight emotions at three intensity levels. The emotion categories include neutral, happy, sad, surprised, fear, disgusted, angry, and contempt. Except for the neutral class, each emotion category has three intensity levels: weak, medium, and strong. Every subject contributes 30 short sentences for seven basic emotions, each ex-pressed at three aforementioned intensity levels, along with an additional 40 sentences with the neutral emotion.\nWe choose 3DMEAD for our experiments as it offers a relatively large-scale, high-quality facial animation data with coverage of diverse emotions. Motion data is sampled at 25 fps. Each frame in the dataset is represented using FLAME [32] 3D Morphable Model (3DMM) parameters $\\{\\beta, \\theta, \\psi\\} \\in \\mathbb{R}^{406}$, where $\\beta \\in \\mathbb{R}^{300}$ denotes the face shape, $\\theta_{jaw} \\in \\mathbb{R}^{3}$ denotes the eular angle rotation (x,y,z) of the jaw bone, $\\theta_{global} \\in \\mathbb{R}^{3}$ denotes the global head pose, and $\\psi \\in \\mathbb{R}^{100}$ denotes the expression parameters. However, similar to EMOTE, we utilize only $\\{\\psi, \\theta_{jaw}\\} \\in \\mathbb{R}^{53}$ for model training, where $\\psi \\in \\mathbb{R}^{50}$ representing the first 50 of the total 100 expression parameters. The original training configuration of EMOTE splits the dataset into training-validation-test sets keeping all sequences per subject while having different subjects in each set. In this way, there is no ground truth data to perform a quantitative evaluation for which EMOTE only conducted a perceptual user study. In contrast, our proposed split keeps a small number of sequences from each training subject for validation and testing allowing comparison of generated samples with respect to the ground truth. Although our training is done on fewer sequences than EMOTE, given the large scale of 3DMEAD dataset, we demonstrate that this split provides sufficient information for effective training and generation of animations perceptually superior in comparison to the EMOTE model. More details about the dataset split can be found in the supplementary material."}, {"title": "3.2\nProblem Formulation", "content": "The task is to generate facial animation sequences based on au-dio and style inputs. To this end, we propose a supervised neural network model training approach to learn from data so that after training, we can predict the facial motion on any arbitrary unseen inputs. To train such a model, we leverage the audio-motion pairs in 3DMEAD dataset. The problem can be formulated as follows-Let $X = \\{X_f\\}_{f=1}^F$ represent a facial animation sequence contain-ing F frames, paired with audio sequence a. Each sequence is also"}, {"title": "3.3 Proposed Model: ProbTalk3D", "content": "Our proposed model ProbTalk3D follows a 2-stage training pro-cess similar to CodeTalker [61] and EMOTE [14] where in the first stage, we learn a motion prior with a motion autoencoder and in the second stage, we train a speech and style conditioned network by leveraging the pretrained HuBERT audio encoder [26] and the motion prior from stage 1. Different from EMOTE [14], our pro-posed model produces diverse outputs non-deterministically with less training data, a less complex and more efficient architecture. The perceptual losses from EMOTE (lip-reading and video-emotion loss) require the ground truth performance videos from the original MEAD dataset, and these involve extra processing that ours does not require. Instead we use quantization and reconstruction losses in the first and second stage."}, {"title": "3.3.1 Stage 1: Motion Autoencoder", "content": "The Motion Autoencoder con-sists of a motion encoder and a decoder aimed to learn a facial motion prior, leveraging the concept of Vector Quantized Varia-tional Autoencoder (VQ-VAE) as laid out in Fig. 2. The Motion En-coder maps the motion input into a latent space during training. We employ transformers, which are proven to effectively capture and learn temporal context. Specifically, the encoder contains a linear projection layer, a 1D convolutional layer, and 6 transformer layers with residual attention and positional encoding. Given an input $X \\in \\mathbb{R}^{F \\times P}$, the Motion Encoder encodes the data to a latent vector $z \\in \\mathbb{R}^{F \\times 256}$. The encoded motion, z undergoes vector quantization and learns a discreet latent embedding codebook, E. We configure E to have 256 latent embeddings with each embedding dimension being 128. This setup implies a codebook size of 256, indicating that the motion is categorized into 256 types and each category is represented by a 128-dimensional vector. From the codebook, we find the embedding that is close to z in terms of distance in the quantization process. This selected embedding is then reshaped to align with the dimension of z, resulting in a quantized latent motion feature, $z'$. More details on the background knowledge of VQ-VAE can be found in the supplementary material. The Motion Decoder consists of a 1D convolutional layer followed by 6 transformer lay-ers. Additionally, a fully connected layer projects the hidden units back to the original input dimension. The Motion Decoder accepts $z'\\in \\mathbb{R}^{N \\times 256}$ as input and processes it to yield $\\hat{X} \\in \\mathbb{R}^{F \\times P}$. The architecture shares similarities with recent VQ-VAE based works [37, 61]. However, unlike these models that autoregressively predict future frames based on previously generated frames for a given se-quence, ours is non-autoregressive similar to EMOTE [14], notably improving training and inference efficiency.\nLoss function. We define the loss function for training stage 1 in Eq.(4). There are three weighted loss terms- (i) $L_{qua}$, that represents the standard quantization loss with codebook loss term and commitment loss term proposed for VQ-VAE models (see Eq.(3)), (ii) $L_{exp}$, that computes the $L_1$ loss between decoded motion and ground truth in terms of the 50 expression parameters, (iii) $L_{jaw}$, that computes the $L_1$ loss between decoded motion and ground truth in terms of the 3 jaw parameters.\n$L_{qua} = ||sg[z] - z'||_2 + \\beta||z - sg[z']||_2$\n$L_{stage1} = \\alpha_{qua}L_{qua} + \\alpha_{exp}^{rec}L_{exp} + \\alpha_{jaw}^{rec} L_{jaw}$\n The weights of the loss function in Eq.(4) are empirically set as follows: $\\alpha_{qua} = 1.5, \\alpha_{exp}^{rec} = 0.5$, and $\\alpha_{jaw}^{rec} = 0.1$."}, {"title": "3.3.2 Stage 2: Speech and Emotion Conditioned", "content": "After learning the motion prior in stage 1, speech and emotion conditioned stage 2 is trained which consists of an audio encoder that encodes the contin-uous raw audio/speech data into discrete hidden representations and fuses the speaking style (i.e. style embedding obtained using Style Vector, C) with the encoded audio information. As shown in Fig.3, the Motion Autoencoder trained in stage 1 is kept frozen in this stage to train the Audio Encoder in a supervised manner so that the latent representation, $z^a$ from the audio encoder closely resembles the latent representation, $z^m$ from the frozen Motion Encoder.\nWe employ pretrained HuBERT [26], a transformer based speech recognition model in our audio encoder, and made appropriate mod-ifications to apply it to our downstream task of facial animation synthesis following [23, 48]. After the last hidden layer of HuBERT, inspired by the aforementioned previous works, we adjust the input representation so it temporally aligns with the paired animation data and add a linear projection layer that projects the encoded hid-den state into a 256-dimensional hidden state to be element-wisely multiplied with the Style Embedding. Style Embedding is obtained using a linear projection of concatenated one-hot vectors of the style annotations (subject ID, emotion, intensity). In our case, the style vector, C is a 43-dimensional vector (as we have 32 subjects, 8 basic emotions, and 3 emotion intensity categories) that is linearly transformed into a 256-dimensional Style Embedding to facilitate the fusion of audio and style information. After fusing the audio information and style information, we introduce a 1D convolutional layer followed by 12 transformer layers to get the latent represen-tation $z^a$ that goes through the frozen motion decoder to obtain the predicted motion, $\\hat{X}$.\nLoss function. The goal in stage 2 training is to get audio latent $z^a$ such that it is as close as possible to the motion latent, $z^m$. Therefore, we construct $L_{lat} = L_1(z^m, z^a)$ that computes the $L_1$ loss between the latent representations. Additionally, we further add the reconstruction losses. The overall loss function is then defined in Eq.(5). The loss weights set for training are $\\lambda_{lat} = 1, \\lambda_{exp}^{rec} = 0.15$, and $\\lambda_{jaw}^{rec} = 0.1$.\n$L_{stage2} = \\lambda_{lat} L_{lat} + \\lambda_{exp}^{rec} L_{exp} + \\lambda_{jaw}^{rec} L_{jaw}$"}, {"title": "3.3.3 Inference", "content": "During inference on unseen audio sequences, the Motion Encoder in the Motion Autoencoder is not used as there is no ground truth motion to encode. The trained Audio Encoder in our model maps input audio to the latent representation z. Sub-sequently, the quantization process extracts an embedding from the learned codebook E, yielding z', which is then passed to the decoder of the Motion Autoencoder to synthesize facial animation, $\\hat{X}$. Style C can be specified to generate a particular subject's speak-ing style with specific emotion class and emotion intensity. The inference process is illustrated in Fig.4. During training iterations, our model is optimized to retrieve the closest learned codebook em-bedding index. We introduce non-deterministic output generation by incorporating a probabilistic sampling process for the codebook embedding index retrieval in the quantization step."}, {"title": "3.3.4 Training Details", "content": "Our proposed model is implemented us-ing the PyTorch Lightning [17] framework and trained on a single"}, {"title": "3.4 Comparison Model: VAE Based", "content": "In order to compare our model with a VAE-based model, we trained a variant of ProbTalk3D by replacing the VQ-VAE structure with a VAE keeping the rest of the architecture the same, ensuring a fair comparison between VAE and VQ-VAE based models. Unlike VQ-VAE which learns a discrete codebook embedding in stage 1 training, VAE learns the mean, $\\mu$ and covariance matrix $\\Sigma$ of the latent space so that it follows a normal distribution $\\phi = N(\\mu, \\Sigma)$.\nSimilar to ProbTalk3D, this model learns the motion prior and in stage 2 the learned mean $\\mu$ and covariance matrix $\\Sigma$ are used and reparameterized for decoding audio and style conditioned fa-cial animation. We use a similar loss function to our proposed model, replacing only the quantization loss term, $L_{qua}$ in Eq.(4) with a Kullback-Leibler (KL) Divergence loss, $L_{KL}$ with $\\alpha_{KL} = 1e^{-4}$, $\\lambda_{exp}^{rec} = 1.5$, $\\lambda_{jaw}^{rec} = 1$."}, {"title": "3.5\nComparison Model: Diffusion Based", "content": "For our comparative analysis, we chose state-of-the-art diffusion based method FaceDiffuser[48]. FaceDiffuser also uses HuBERT as an audio encoder which ensures similarity with respect to our model. In addition to the original model, we train FaceDiffuser on 3DMEAD with a modification of the original model structure in order to incorporate the emotion and intensity categories. We use 3D vertex coordinates instead of FLAME parameters for training FaceDiffuser as our experiments indicate that using low dimen-sional FLAME parameters data does not yield realistic facial ani-mations for this architecture. To obtain the vertex coordinate data, scripts provided by FLAME[32] are utilized to convert temporal FLAME parameters into temporal 3D vertex coordinates. In the re-vised FaceDiffuser model, the style embedding is multiplied earlier in the network with audio features extracted by HuBERT, rather than fusing it later in the network, before the last fully connected layer, as it was done in the original model. The style embedding includes information about subject identity, emotion class, and emo-tion intensity, using the same format as the style vector C in our proposed model, while the original FaceDiffuser only uses one-hot vectors for subject identities. Our experiments show that integrat-ing the style at an earlier stage generates better results. We define two versions: FaceDiffuser - DDPM and FaceDiffuser - DDIM (See Tab.1). The former is trained with Denoising Diffusion Probabilistic Model (DDPM) [24] same as the original FaceDiffuser. We further improve the diffusion sampling efficiency of the modified model by changing the original one with Denoising Diffusion Implicit Model (DDIM) [25]. Aside from these changes, the model structure and hyperparameters are identical to the V-FaceDiffuser in [48]."}, {"title": "4 Results", "content": "In this section, we present the results of our proposed model and evaluate the model quantitatively, qualitatively, and with a per-ceptual user study. In addition to the relevant objective metrics from the literature [48, 61], we use an extensive list of objective metrics following [62]. The non-deterministic generation ability of our model is illustrated using a diversity metric, in line with recent probabilistic models [45, 62]. Furthermore, qualitative eval-uations provide visual demonstrations of animation quality and emotion control aspects. Following that, results of the user study are provided, aiming to evaluate the perceived realism, lip-synchrony, and emotional expressivity of the synthesized animations. Finally, the ablation study showcases the impact of incorporating emotion control into our model."}, {"title": "4.1 Quantitative Evaluation", "content": "We quantitatively evaluate our model performance based on multi-ple metrics. Mean Vertex Error (MVE), Lip Vertex Error (LVE) and Upper Face Dynamics Deviation (FDD) are calculated based on one sample output which are metrics commonly used for evaluating deterministic models. Mean Estimate Error (MEE), CE (Coverage Er-ror) and Diversity are calculated over multiple samples providing a better picture for evaluating non-deterministic models. The widely applied evaluation metrics used by recent models mostly operate in vertex coordinate space. To compare accuracy with these models,"}, {"title": "4.2 Qualitative Evaluation", "content": "For qualitative assessment, we visually compare the generated ani-mations of different models pronouncing specific syllables, focusing on assessing the quality of lip synchronization. The syllables are chosen to assess the model's capability in synthesizing diverse mouth shapes. This includes challenging sounds like the bilabial consonants /p/ and /m/, which require precise lip closure, as well as syllables that require a pout and sounds that demand an open-mouth posture. This quality judgement uses audio sequences that express a neutral emotion, selected from the test set of 3DMEAD.\nTo compare our model against SOTAs, we include animations gen-erated by the modified FaceDiffuser and the deterministic model, EMOTE. Note that we did not retrain EMOTE and utilized the pub-licly available trained model and inference script for motion gener-ation. Fig.5 showcases the results, with the ground truth provided as a reference. It can be seen from the figure that both ProbTalk3D and the VAE-based model closely resemble the ground truth, and achieve proper mouth closure. Inspecting the results of FaceDif-fuser and EMOTE, we observe similar results indicating that our model performs comparable or better with respect to the recent non-deterministic and deterministic models.\nTo illustrate the capability of our model in generating a wide range of facial movements, we adopt the approach used in [61] to present facial motion dynamics through heatmap visualization. We calculate the temporal statistics of adjacent-frame facial motions"}, {"title": "4.3 Perceptual User Study", "content": "We conduct A/B testing to compare our proposed model's results against ground truth and results from SOTA models- FaceDiffuser-DDIM (non-deterministic - trained on 3DMEAD with emotion con-trol) and EMOTE (i.e. deterministic - trained on 3DMEAD with emotion control using the publicly available model). These SOTAS have previously demonstrated superior performance compared to several earlier works. For the user study, we generate 32 videos using each model. These videos cover 8 emotions and different speaking styles, with 4 video sequences generated for each emotion. For the 7 emotions other than neutral, we create 2 videos with medium intensity and 2 with high intensity. Videos under a specific emotion are synthesized using 2 audio sequences from 3DMEAD (with identities unseen during the second stage of training) and 2 in-the-wild audio samples from the VoxMovies [6] dataset for generalizability. Additionally, we generate another set of 32 videos using our model to compare with the ground truth belonging to the unseen test-set of 3DMEAD. The survey is set to randomly choose 1 video pair within 4 pairs for each emotion. This results in a total of 24 video pairs: 8 pairs each comparing our model with- (i) the ground truth, (ii) FaceDiffuser and (iii) EMOTE.\nWe use Qualtrics [43] as the survey tool and recruit participants through Prolific [42], ensuring proper remuneration. Participants are queried about lip synchronization, realism, and emotional ex-pressivity after viewing each video pair, requiring them to select the one they perceive as better. In total, 73 responses are collected, with 4 responses discarded due to failing the attention test, resulting in 69 valid responses. The result of the perceptual study is reported in Tab.2, demonstrating that our model's output is generally less preferred than the ground truth across lip synchronization, realism, and emotional expression. This is expected and in line with other recent works. However, our model outperforms the competitors in all three aspects. The user study demonstrates the superiority of our model over the competitors, showcasing its capability to generate animations with good lip synchronization, high overall face realism, and superior emotional expressivity. More details about the user study can be found in the supplementary material."}, {"title": "4.4 Ablation Study", "content": "We conduct an ablation study to understand whether emotion con-trol enhances synthesized animation quality. We remove the Style Vector input, C, and instead, learn a general representation that is not specific to any of the style vector attributes. The remaining"}, {"title": "5 Discussion and Future Work", "content": "Our model is limited to generate facial animations conditioned on 8 basic emotions and 3 discreet emotion intensities as per ground truth annotations of the dataset. However, human emotion is much more detailed and richer to be controlled by pre-defined categories.\nWe believe that by combining our model with textual descriptions would enable us to learn and control the generation of richer emo-tions rather than relying solely on one-hot vectors for style embed-ding, similar to [4] which was applied in the body motion genera-tion domain. Furthermore, due to the 3DMEAD dataset, our model shares similar limitations as EMOTE [14] as presented in their work that include- (i) not being able to generate eye blinks, (ii) absence of mouth cavity, teeth and tongue animation that effect perception of speech animation (iii) visual artefacts for high-frequency speech as the reconstructed visual data is of low frequency.\nSimilar to our model ProbTalk3D and the VAE variant, we believe a 2-stage diffusion-based model, utilizing the more recent condi-tional diffusion [39] or latent diffusion [10] approaches might prove to be fruitful. Latent diffusion methods have demonstrated success in image generation and are applied in recent models for human motion synthesis [10] for enhanced diversity. More experiments are needed to effectively leverage diffusion based approaches for 3D fa-cial animation synthesis. Moreover, additional datasets can be used for training and analysis to validate our model's generalizability and 4D datasets can be employed for enhanced realism.\nIntuitively, the animation produced by a generative model should resemble human-like facial motion, meaning the diversity between generations should be close to that of the ground truth. If a model can generate a wide range of diverse samples, but they lack meaning, it does not necessarily indicate superior performance. However, to the best of our knowledge, in contrast with body animation datasets [68], no existing facial animation datasets exhibit this kind of natural diversity. Current datasets that include different ways of expressing the same sentence often involve performers being directed to convey specific emotions. To obtain ground truth diversity, we need multiple performances under the same style condition. With this information in the dataset, we can evaluate our models by assessing how closely their diversity matches as observed in ground truth. Future work can focus on constructing datasets that enable this diversity analysis."}, {"title": "6\nConclusion", "content": "In this work, we propose ProbTalk3D, a novel non-deterministic approach for emotion controllable speech-driven 3D facial ani-mation synthesis that outperforms most recent deterministic and non-deterministic methods. Our approach is based on VQ-VAE and trained in 2 stages. In the first stage, we learn a motion prior leveraging a VQ-VAE based motion autoencoder. In the second stage, we train our audio and emotion conditioned 3D facial an-imation synthesis network that takes advantage of the learned motion prior for generation. Extensive evaluations have been con-ducted to validate our model performance. Quantitative evalua-tion results demonstrate that our model achieves results that are comparable to state-of-the-arts while ensuring a wider range of diverse yet high-quality acceptable animation outputs. Further-more, qualitative comparisons with the non-deterministic model-FaceDiffuser[48] and deterministic model- EMOTE[14] show that our approach perform better against state-of-the-art models while being less computationally complex and more efficient. The per-ceptual user study provides further evidence that our model is preferred to both FaceDiffuser and EMOTE on lip synchronization, realism, and emotional expressivity ratings. Our work highlights the necessity of non-deterministic methodologies for generative 3D facial animation by introducing a novel probabilistic model that is capable of generating high-quality yet diverse animations. We hope our findings will inspire new discussions and research directions in generative 3D facial animation landscape."}, {"title": ""}]}