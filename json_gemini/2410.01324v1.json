{"title": "FAIR CLASS-INCREMENTAL LEARNING USING SAMPLE WEIGHTING", "authors": ["Jaeyoung Park", "Minsu Kim", "Steven Euijong Whang"], "abstract": "Model fairness is becoming important in class-incremental learning for Trustwor-thy AI. While accuracy has been a central focus in class-incremental learning, fairness has been relatively understudied. However, na\u00efvely using all the samples of the current task for training results in unfair catastrophic forgetting for certain sensitive groups including classes. We theoretically analyze that forgetting occurs if the average gradient vector of the current task data is in an \"opposite direction\" compared to the average gradient vector of a sensitive group, which means their inner products are negative. We then propose a fair class-incremental learning framework that adjusts the training weights of current task samples to change the direction of the average gradient vector and thus reduce the forgetting of un-derperforming groups and achieve fairness. For various group fairness measures, we formulate optimization problems to minimize the overall losses of sensitive groups while minimizing the disparities among them. We also show the problems can be solved with linear programming and propose an efficient Fairness-aware Sample Weighting (FSW) algorithm. Experiments show that FSW achieves better accuracy-fairness tradeoff results than state-of-the-art approaches on real datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Trustworthy AI is becoming critical in various continual learning applications including autonomous vehicles, personalized recommendations, healthcare monitoring, and more (Liu et al., 2021; Kaur et al., 2023). In particular, it is important to improve model fairness along with accuracy when developing models incrementally in dynamic environments. Unfair model predictions have the potential to undermine the trust and safety in human-related automated systems, especially as observed frequently in the context of continual learning. There are largely three continual learning scenarios (van de Ven &\nTolias, 2019): task-incremental, domain-incremental, and class-incremental learning where the task, domain, or class may change over time, respectively. In this paper, we focus on class-incremental learning, where the objective is to incrementally learn new classes as they appear.\nThe main challenge of class-incremental learning is to learn new classes of data, while not forgetting previously-learned classes (Belouadah et al., 2021; Lange et al., 2022). If we simply fine-tune the model on the new classes only, the model will gradually forget about the previously-learned classes. This phenomenon called catastrophic forgetting (McCloskey & Cohen, 1989; Kirkpatrick et al., 2016) may easily occur in real-world scenarios where the model needs to continuously learn new classes. We cannot simply stop learning new classes to avoid this forgetting either. Instead, we need to have a balance between learning new information and retaining previously-learned knowledge, which is called the stability-plasticity dilemma (Abraham & Robins, 2005; Mermillod et al., 2013; Kim &\nHan, 2023).\nIn this paper, we solve the problem of fair class-incremental learning where the goal is to satisfy var-ious notions of fairness among sensitive groups including classes in addition to classifying accurately. In some scenarios, the class itself can be considered a sensitive attribute, especially in classification tasks where a model produces biased predictions toward a specific group of classes (Truong et al.,\n2023). In continual learning, unfair forgetting may occur if the current task data has similar character-istics to previous data, but belongs to different sensitive groups including classes, which negatively"}, {"title": "2 RELATED WORK", "content": "Class-incremental learning is a challenging type of continual learning where a model continuously learns new tasks, each composed of new disjoint classes, and the goal is to minimize catastrophic forgetting (Mai et al., 2022; Masana et al., 2023). Data replay techniques (Lopez-Paz & Ranzato, 2017; Rebuffi et al., 2017; Chaudhry et al., 2019b) store a small portion of previous data in a buffer to utilize for training and is widely used with other techniques (Zhou et al., 2023a) including knowledge distillation (Rebuffi et al., 2017; Buzzega et al., 2020), model rectification (Wu et al., 2019; Zhao et al., 2020), and dynamic networks (Yan et al., 2021; Wang et al., 2022; Zhou et al., 2023b). Simple buffer sample selection methods such as random or herding-based approaches (Rebuffi et al., 2017) are also commonly used as well. There are also more advanced gradient-based sample selection techniques like GSS (Aljundi et al., 2019) and OCS (Yoon et al., 2022) that manage buffer data to have samples with diverse and representative gradient vectors. All these works do not consider fairness and simply assume that the entire incoming data is used for model training, which may result in unfair forgetting as we show in our experiments.\nModel fairness research mitigates bias by ensuring that a model's performance is equitable across different sensitive groups, thereby preventing discrimination based on race, gender, age, or other sensitive attributes (Mehrabi et al., 2022). Existing model fairness techniques can be categorized as pre-processing (Kamiran & Calders, 2011; Feldman et al., 2015; Calmon et al., 2017; Jiang &\nNachum, 2020), in-processing (Agarwal et al., 2018; Zhang et al., 2018; Cotter et al., 2019; Roh et al.,\n2020), and post-processing (Hardt et al., 2016; Pleiss et al., 2017; Chzhen et al., 2019). However,\nmost of these techniques assume that the training data is given all at once, which may not be realistic.\nA recent line of research addresses model fairness in class-incremental learning where there is a risk of disproportionately forgetting previously-learned sensitive groups including classes, leading to unfairness across different groups. A recent study (He, 2024) addresses the dual imbalance problem involving both inter-task and intra-task imbalance by reweighting gradients. However, the bias is not only caused by the data imbalance, but also by the inherent or acquired characteristics of the data (Mehrabi et al., 2021; Angwin et al., 2022). CLAD (Xu et al., 2024) first discovers imbalanced forgetting between learned classes caused by conflicts in representation and proposes a class-aware disentanglement technique to improve average accuracy. Among the fairness-aware techniques, FaIRL (Chowdhury & Chaturvedi, 2023) supports group fairness measures like demographic parity for class-incremental tasks, but proposes a representation learning method that does not directly optimize the given fairness measure and thus has limitations in improving fairness as we show in our experiments. FairCL (Truong et al., 2023) also addresses fairness in a continual learning setup, but only focuses on resolving the imbalanced class distribution based on the number of pixels of each class in an image for semantic segmentation tasks. In comparison, we support fairness more generally in class-incremental learning by satisfying multiple notions of group fairness for sensitive groups including classes."}, {"title": "3 FRAMEWORK", "content": "In this section, we first theoretically analyze unfair forgetting using gradient vectors of sensitive groups and the current task data. Next, we propose sample weighting to mitigate unfairness by adjusting the average gradient vector of the current task data and provide an efficient algorithm. We use the following notations for class-incremental learning and fairness.\nNotations In class-incremental learning, a model incrementally learns new current task data along with previous buffer data using data replay. Suppose we train a model to incrementally learn L tasks"}, {"title": "3.1 UNFAIR FORGETTING", "content": "Catastrophic forgetting occurs when a model adapts to a new task and exhibits a drastic decrease in performance on previously-learned tasks (Parisi et al., 2019). We take inspiration from GEM (Lopez-Paz & Ranzato, 2017), which theoretically analyzes catastrophic forgetting by utilizing the angle between gradient vectors of data. If the inner products of gradient vectors for previous tasks and the current task are negative (i.e., 90\u00b0 < angle < 180\u00b0), the loss of previous tasks increases after learning the current task. Catastrophic forgetting thus occurs when the gradient vectors of different tasks point in opposite directions. Intuitively, the opposite gradient vectors update the model parameters in conflicting directions, leading to forgetting while learning.\nUsing the notion of catastrophic forgetting, we propose theoretical results for unfair forgetting:\nLemma 1. Denote G as a sensitive group of data composed of features X and true labels y. Also, denote $f_l^{-1}$ as a previous model and $f_l$ as the updated model after training on the current task $T_l$. Let $l$ be any differentiable standard loss function (e.g., cross-entropy loss), and $\u03b7$ be a learning rate. Then, the loss of the sensitive group of data after training with a current task sample $d_i \u2208 T_l$ is approximated as follows:\n$l(f_l, G) = l(f_{l-1}, G) \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, G) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i),$ (1)\nwhere $l(f_l, G)$ is the approximated average loss between model predictions $f_l(X)$ and true labels $y$, whereas $l(f_{l-1}, G)$ is the exact average loss, $\u2207_\u03b8l(f_{l-1}, G)$ is the average gradient vector for the samples in the group of data G, and $\u2207_\u03b8l(f_{l-1}, d_i)$ is the gradient vector for a sample $d_i$, each with respect to the previous model $f_{l-1}$.\nTo define fairness in class-incremental learning with the approximated loss, we adopt the definition of approximate fairness that considers a model to be fair if it has approximately the same error on the positive class, independent of the group membership (Donini et al., 2018). In addition, we use a differentiable loss function instead of the widely used 0-1 loss to implement fairness measures because the approximated loss can be derived using gradients. In practice, we can utilize cross-entropy loss"}, {"title": "3.2 SAMPLE WEIGHTING FOR UNFAIRNESS MITIGATION", "content": "To mitigate unfairness, we propose sample weighting as a way to suppress samples that negatively impact fairness and promote samples that help. Finding the weights is not trivial as there can be many sensitive groups, and even a single sample may improve the fairness of a pair of groups, but worsen the fairness for another pair of groups. Given training weights $w_i$ for the samples in the current task data, the approximated loss of a group G after training is now $l(f_l,G) = l(f_{l-1}, G) \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, G) \u22c5 w_i\u2207_\u03b8l(f_{l-1}, T_l)$.\nWe then formulate an optimization problem to find the weights such that both loss and unfairness are minimized. Here we define Y as the set of all classes and $Y_c$ as the set of classes in the current task. We represent accuracy as the average loss over the current task data and minimize the cost function $L_{acc} = l(f_l, G_{Y_c}) = \\sum_{y\u2208Y_c} l(f_l, G_y) = \\frac{1}{|Y_c||Z|} \\sum_{y\u2208Y_c, z\u2208Z} l(f_l, G_{y,z})$. For fairness, the cost function $L_{fair}$ depends on the group fairness measure as we explain below. We then minimize $L_{fair} + \u03bbL_{acc}$ where $\u03bb$ is a hyperparameter that balances fairness and accuracy.\nEqual Error Rate (EER) This measure (Venkatasubramanian, 2019) is defined as $Pr(\u0177 \u2260 y_1|y = y_1) = Pr(\u0177 \u2260 y_2|y = y_2)$ for $y_1, y_2 \u2208 Y$, where \u0177 is the predicted class and y is the true class. We define the cost function for EER as the average absolute difference between the loss of a class and the average loss of all classes: $L_{EER} = \\frac{1}{|Y|} \\sum_{y\u2208Y} |l(f_l, G_y) \u2013 l(f_l, G_Y)|$. The entire optimization problem is:\n$\\min_{W_i} \\frac{1}{|Y|} \\sum_{y\u2208Y} |l(f_l, G_y) \u2013 l(f_l, G_Y)| + \u03bb \\frac{1}{|Y_c|} \\sum_{y\u2208Y_c} l(f_l, G_y),$ (2)\nwhere $l(f_l, G_y) = l(f_{l-1}, G_y) \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, G_y) \u22c5 w_i\u2207_\u03b8l(f_{l-1}, T_l)$.\nEqualized Odds (EO) This measure (Hardt et al., 2016) is satisfied when sensitive groups have the same accuracy, i.e., $l(f_l, G_{y,z_1}) = l(f_l, G_{y,z_2})$ for $y \u2208 Y$ and $z_1, z_2 \u2208 Z$. We design the cost function for EO as $L_{EO} = \\frac{1}{|Y||Z|} \\sum_{y\u2208Y,z\u2208Z} |l(f_l, G_{y,z}) \u2013 l(f_l, G_y)|$ to compute the EO disparity, and the entire optimization problem is:\n$\\min_{W_i} \\frac{1}{|Y||Z|} \\sum_{y\u2208Y,z\u2208Z} |l(f_l, G_{y,z}) \u2013 l(f_l, G_y)| + \u03bb \\frac{1}{|Y||Z|} \\sum_{y\u2208Y,z\u2208Z} l(f_l, G_{y,z}),$ (3)\nwhere $l(f_l, G_{y,z}) = l(f_{l-1}, G_{y,z}) \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, G_{y,z}) \u22c5 w_i\u2207_\u03b8l(f_{l-1}, T_l)$.\nDemographic Parity (DP) This measure (Feldman et al., 2015) is satisfied by minimizing the difference in positive prediction rates between sensitive groups. Here, we extend the notion of demographic parity to the multi-class setting (Alabdulmohsin et al., 2022; Denis et al., 2023), i.e., $Pr(\u0177 = y|z = z_1) = Pr(\u0177 = y|z = z_2)$ for $y \u2208 Y$ and $z_1, z_2 \u2208 Z$. In the binary setting of Y = Z = {0, 1}, a sufficient condition for demographic parity is suggested using the loss multiplied by the ratios of sensitive groups (Roh et al., 2021). By extending the setting to multi-class, we derive a"}, {"title": "3.3 ALGORITHM", "content": "We describe the overall process of fair class-incremental learning in Alg. 1. For the recently arrived current task data, we first perform fairness-aware sample weighting (FSW) to assign training weights that can help learn new knowledge of the current task while retaining accurate and fair memories of previous tasks (Step 2). Next, we train the model using the current task data with its corresponding weights and stored buffer data of previous tasks (Step 3), where \u03b7 is a learning rate, and \u03c4 is a hyperparameter to balance between them during training. This procedure is repeated until the model converges (Steps 1\u20133). Before moving on to the next task, we employ buffer sample selection to store a small data subset for the current task (Steps 4\u20135). Buffer sample selection can also be done with consideration for fairness, but our experimental observations indicate that selecting representative and diverse samples for the buffer, as previous studies have shown, results in better accuracy and also fairness performance. We thus employ a simple random sampling technique for the buffer sample selection in our framework.\nAlg. 2 shows the fairness-aware sample weighting (FSW) algorithm for the current task data. We first divide both the previous buffer data and the current task data into groups based on each class and sensitive attribute. Next, we compute the average loss and gradient vectors for each group (Steps 1-2), and individual gradient vectors for the current task data (Step 3). To compute gradient vectors, we use the last layer approximation, which only considers the gradients of the model's last layer, that is efficient and known to be reasonable (Katharopoulos & Fleuret, 2018; Ash et al., 2020; Mirzasoleiman et al., 2020). We then solve linear programming to find the optimal sample weights"}, {"title": "4 EXPERIMENTS", "content": "We implement FSW using Python and PyTorch. All evaluations are performed on separate test sets and repeated with five random seeds. We write the average and standard deviation of performance results and run experiments on Intel Xeon Silver 4114 CPUs and NVIDIA TITAN RTX GPUs.\nMetrics We evaluate all methods using accuracy and fairness metrics as in the fair continual learning literature (Chowdhury & Chaturvedi, 2023; Truong et al., 2023).\n\u2022 Average Accuracy: We denote $A_l = \\frac{1}{L} \\sum_{l=1}^L a_{l,t}$ as the accuracy at the $l^{th}$ task, where $a_{l,t}$ is the accuracy of the $t^{th}$ task after learning the $l^{th}$ task. We measure accuracy for each task and then take the average across all tasks to produce the final average accuracy, denoted as $A_l = \\frac{1}{L} \\sum_{l=1}^L A_l$ where L represents the total number of tasks.\n\u2022 Average Fairness: We measure fairness for each task and then take the average across all tasks to produce the final average fairness. We use one of three measures for per-task fairness: (1) Equal Error Rate (EER) disparity, which computes the average difference in test error rates among classes: $\\frac{1}{|Y|} \\sum_{y\u2208Y} |Pr(\u0177\u2260y|y = y)-Pr(\u0177 \u2260 y)|$; (2) Equalized Odds (EO) disparity, which computes the average difference in accuracy among sensitive groups for all classes: $\\frac{1}{|Y||Z|} \\sum_{y\u2208Y,z\u2208Z} | Pr(\u0177 = y|y = y,z = z) - Pr(\u0177 = y|y = y)|$; and (3) Demographic Parity (DP) disparity, which computes the average difference in class prediction ratios among sensitive groups for all classes: $\\frac{1}{|Z|} \\sum_{y\u2208Y,z\u2208Z} | Pr(\u0177 = y|z = z) \u2013 Pr(\u0177 = y)|$. For all measures, low disparity is desirable.\nDatasets We use a total of five datasets as shown in Table 1. We first utilize commonly used benchmarks for continual image classification tasks, which include MNIST and Fashion-MNIST (FMNIST). Here we regard the class as the sensitive attribute and evaluate fairness with EER disparity. We also use multi-class fairness benchmark datasets that have sensitive attributes (Xu et al., 2020; Putzel & Lee, 2022; Churamani et al., 2023; Denis et al., 2023): Biased MNIST, Drug Consumption (DRUG), and BiasBios. We consider background color as the sensitive attribute for Biased MNIST, and gender for DRUG and BiasBios, respectively. We use EO disparity and DP disparity to evaluate"}, {"title": "4.1 ACCURACY AND FAIRNESS RESULTS", "content": "We compare FSW against other baselines on the five datasets with respect to accuracy and corre-sponding fairness metrics as shown in Table 2 and Tables 5\u20137 in Sec. B.6. The results for DP disparity and BiasBios dataset are similar and shown in Sec. B.6. We mark the best and second-best results with bold and underline, respectively, excluding the upper-bound results of Joint Training and the lower-bound results of Fine Tuning. For any method, we store a fixed number of samples per task in a buffer, which may not be identical to its original setup, but necessary for a fair comparison. The detailed sequential performance results for each task are shown in Sec. B.6.\nOverall, FSW achieves better accuracy-fairness tradeoff results compared to the baselines for all the datasets. For the DRUG dataset, although FSW does not achieve the best performance in either accuracy or fairness, FSW shows the best fairness results among the baselines with similar accuracies (e.g., CLAD, GSS, and OCS) and thus has the best accuracy-fairness tradeoff. We observe that FSW sometimes improves model accuracy while enhancing the performance of underperforming"}, {"title": "4.2 SAMPLE WEIGHTING ANALYSIS", "content": "We next analyze how our FSW algorithm weights the current task samples at each task using the Biased MNIST dataset results shown in Fig. 2. The results for the other datasets are similar and shown in Sec. B.7. As the acquired sample weights may change with epochs during training, we show the average weight distribution of sensitive groups over all epochs. Since FSW is not applied to the first task, where the model is trained with only the current task data, we present results starting from the second task. Unlike na\u00efve methods that use all the current task data with equal training weights, FSW usually adjusts different training weights between sensitive groups as shown in Fig. 2. For the Biased MNIST dataset, FSW assigns higher weights on average to the underperforming group (Sensitive group 1 in Fig. 2) compared to the overperforming group (Sensitive group 0 in Fig. 2). We also observe that FSW assigns a weight of zero to a considerable number of samples, indicating that"}, {"title": "4.3 ABLATION STUDY", "content": "To show the effectiveness of FSW on accuracy and fairness performance, we perform an ablation study comparing the performance of using FSW versus using all the current task samples for training with equal weights. Table 3 shows the results for the MNIST, FMNIST, Biased MNIST, and DRUG datasets, while the results for DP disparity and the BiasBios dataset are similar and shown in Sec. B.8. As a result, applying sample weighting to the current task data is necessary to improve fairness while maintaining comparable accuracy."}, {"title": "4.4 INTEGRATING FSW WITH A FAIR POST-PROCESSING METHOD", "content": "In this section, we emphasize the extensibility of FSW by showing how it can be combined with a post-processing method to further improve fairness. We integrate FSW and other existing continual learning methods (iCaRL, CLAD, and OCS) with the state-of-the-art fair post-processing technique in multi-class tasks, e-fair (Denis et al., 2023), as shown in Table 4 and Table 11 in Sec. B.9. Since e-fair only supports DP, we only show DP results using the Biased MNIST, DRUG, and BiasBios datasets. We mark the best and second-best results with bold and underline, respectively, regardless of the application of post-processing. Overall, combining the fair post-processing technique can further improve fairness without degrading accuracy much. In addition, FSW still shows a better accuracy-fairness tradeoff with the combination of the fair post-processing technique, compared to existing continual learning methods."}, {"title": "5 CONCLUSION", "content": "We proposed FSW, a fairness-aware sample weighting algorithm for fair class-incremental learning. Unlike conventional class-incremental learning, we showed how training with all the current task data using equal weights may result in unfair catastrophic forgetting. We theoretically showed that the average gradient vector of the current task data should not solely be in the opposite direction of the average gradient vector of a sensitive group to avoid unfair forgetting. We then proposed FSW as a solution to adjust the average gradient vector of the current task data such that unfairness is mitigated without harming accuracy much. FSW supports various group fairness measures and is efficient as it solves the optimization by converting it into a linear program. In our experiments, FSW outperformed other baselines in terms of fairness while having comparable accuracy across various datasets with different domains."}, {"title": "Ethics Statement", "content": "We anticipate our research will have a positive societal impact by improving fairness in continual learning. However, improving fairness may result in a decrease in accuracy, although we try to minimize the tradeoff. In addition, choosing the right fairness measure can be challenging depending on the application and social context."}, {"title": "Reproducibility Statement", "content": "To ensure the reproducibility of our work, we provide detailed expla-nations of all the theoretical and experimental results throughout the appendix and supplementary material. For the theoretical results, we include complete proofs of all our theorems in the appendix. For the experimental results, we present a thorough description of the datasets used, as well as the experimental settings of model architectures and hyperparameters, in the appendix. In addi-tion, we submit the source code necessary for reproducing our experimental results as a part of the supplementary material."}, {"title": "A APPENDIX \u2013 THEORY", "content": "A.1 THEORETICAL ANALYSIS OF UNFAIRNESS IN CLASS-INCREMENTAL LEARNING\nContinuing from Sec. 3.1, we prove the lemma on the updated loss of a group of data after learning the current task data.\nLemma. Denote G as a sensitive group of data composed of features X and true labels y. Also, denote $f_l^{-1}$ as a previous model and $f_l$ as the updated model after training on the current task $T_l$. Let $l$ be any differentiable standard loss function (e.g., cross-entropy loss), and $\u03b7$ be a learning rate. Then, the loss of the sensitive group of data after training with a current task sample $d_i \u2208 T_l$ is approximated as follows:\n$l(f_l, G) = l(f_{l-1}, G) \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, G) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i),$\nwhere $l(f_l, G)$ is the approximated average loss between model predictions $f_l(X)$ and true labels $y$, whereas $l(f_{l-1}, G)$ is the exact average loss, $\u2207_\u03b8l(f_{l-1}, G)$ is the average gradient vector for the samples in the group of data G, and $\u2207_\u03b8l(f_{l-1}, d_i)$ is the gradient vector for a sample $d_i$, each with respect to the previous model $f_{l-1}$.\nProof. We update the model using gradient descent with the current task sample $d_i \u2208 T_l$ and learning rate \u03b7 as follows:\n$\u03b8 = \u03b8_{l-1} \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, d_i)$.\nUsing the Taylor series approximation,\n$l(f_l, G) = l(f_{l-1}, G) + \u2207_\u03b8l(f_{l-1}, G)(\u03b8 \u2013 \u03b8_{l-1})$\n$= l(f_{l-1}, G) + \u2207_\u03b8l(f_{l-1}, G)(-\u03b7\u2207_\u03b8l(f_{l-1}, d_i))$\n$= l(f_{l-1}, G) \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, G) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i)$.\nIf we update the model using all the current task data $T_l$, the equation is formulated as $l(f_l, G) = l(f_{l-1}, G) \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, G) \u22c5 \u2207_\u03b8l(f_{l-1}, T_l)$. Therefore, if the average gradient vectors of the sensitive group and the current task data have opposite directions, i.e., $\u2207_\u03b8l(f_{l-1}, G) \u22c5 \u2207_\u03b8l(f_{l-1}, T_l) < 0$, learning the current task data increases the loss of the sensitive group data and finally leads to catastrophic forgetting.\nWe next derive a sufficient condition for unfair forgetting.\nTheorem. Denote $G_1$ and $G_2$ as the overperforming and underperforming groups of data that satisfy the following conditions: $l(f_{l-1}, G_1) < l(f_{l-1}, G_2)$ while $\u2207_\u03b8l(f_{l-1}, G_1) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i) > 0$ and $\u2207_\u03b8l(f_{l-1}, G_2) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i) < 0$. Then $|l(f_l, G_1) \u2013 l(f_l, G_2)| > |l(f_{l-1}, G_1) \u2013 l(f_{l-1}, G_2)|$.\nProof. Using the derived equation in the lemma above $l(f_l, G) = l(f_{l-1}, G) \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, G) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i)$, we compute the disparity of losses between the two groups $G_1$ and $G_2$ after the model update as follows:\n$|l(f_l, G_1) \u2013 l(f_l, G_2)| = |(l(f_{l-1}, G_1) \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, G_1) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i))-\n(l(f_{l-1}, G_2) \u2013 \u03b7\u2207_\u03b8l(f_{l-1}, G_2) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i))|$\n$= |(l(f_{l-1}, G_1) \u2013 l(f_{l-1}, G_2))-\n\u03b7(\u2207_\u03b8l(f_{l-1}, G_1) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i) \u2013 \u2207_\u03b8l(f_{l-1}, G_2) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i))|$.\nSince $l(f_{l-1}, G_1) < l(f_{l-1}, G_2)$, it leads to $l(f_{l-1}, G_1) \u2013 l(f_{l-1}, G_2) < 0$. Next, the two assumptions of $\u2207_\u03b8l(f_{l-1}, G_1) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i) > 0$ and $\u2207_\u03b8l(f_{l-1}, G_2) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i) < 0$ make $-\u03b7(\u2207_\u03b8l(f_{l-1}, G_1) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i) \u2013 \u2207_\u03b8l(f_{l-1}, G_2) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i)) < 0$. Since the two terms in the absolute value equation are both negative,\n$|l(f_l, G_1) \u2013 l(f_l, G_2)| = |l(f_{l-1}, G_1) \u2013 l(f_{l-1}, G_2)|+\n| - \u03b7(\u2207_\u03b8l(f_{l-1}, G_1) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i) \u2013 \u2207_\u03b8l(f_{l-1}, G_2) \u22c5 \u2207_\u03b8l(f_{l-1}, d_i))|$\n$> |l(f_{l-1}, G_1) \u2013 l(f_{l-1}, G_2)|$."}, {"title": "A.2 DERIVATION OF A SUFFICIENT CONDITION FOR DEMOGRAPHIC PARITY IN THE MULTI-CLASS SETTING", "content": "Continuing from Sec. 3.2", "m_{y,z}": {"i": "y_i = y", "m_{*,z}": {"i": "z_i = z"}, "follows": "Pr(\u0177 = y|z = z) = Pr(\u0177 = y"}, "follows": "n$Pr(\u0177 = y", "sum_{i": "y_i = y"}, {"follows": "n$Pr(\u0177 = y, y = y_n|z = 0) = \\"}]}