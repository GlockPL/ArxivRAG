{"title": "ATM: IMPROVING MODEL MERGING BY ALTERNATING TUNING AND MERGING", "authors": ["Luca Zhou", "Daniele Solombrino", "Donato Crisostomi", "Maria Sofia Bucarelli", "Fabrizio Silvestri", "Emanuele Rodol\u00e0"], "abstract": "Model merging has recently emerged as a cost-efficient paradigm for multi-task learning. Among current approaches, task arithmetic (Ilharco et al., 2022) stands out for its simplicity and effectiveness. In this paper, we motivate the effective-ness of task vectors by linking them to multi-task gradients. We show that in a single-epoch scenario, task vectors are mathematically equivalent to the gradients obtained via gradient descent in a multi-task setting, and still approximate these gradients in subsequent epochs. Furthermore, we show that task vectors perform optimally when equality is maintained, and their effectiveness is largely driven by the first epoch's gradient. Building on this insight, we propose viewing model merging as a single step in an iterative process that Alternates between Tuning and Merging (ATM). This method acts as a bridge between model merging and multi-task gradient descent, achieving state-of-the-art results with the same data and computational requirements. We extensively evaluate ATM across diverse settings, achieving up to 20% higher accuracy in computer vision and NLP tasks, compared to the best baselines. Finally, we provide both empirical and theoretical support for its effectiveness, demonstrating increased orthogonality between task vectors and proving that ATM minimizes an upper bound on the loss obtained by jointly finetuning all tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "The pretrain-and-finetune paradigm has become the standard for many deep learning tasks, where a model pretrained on large-scale, unlabeled data is adapted to a specific downstream task with minimal tuning. However, when working with multiple tasks, a major drawback is the need to store separate finetuned models for each task. Model merging addresses this challenge by combining task-specific models into a single model capable of handling all tasks. This significantly reduces storage costs, as the unified model's size remains comparable to that of a single-task model, regardless of the number of tasks. Among numerous model merging methods, task arithmetic (Ilharco et al., 2022) stands out for its simplicity and effectiveness. Given a pretrained model 00 and a model \u03b8i finetuned on task ti, the task vector Ti = 0; \u2013 00 is defined as the difference between the finetuned and pretrained weights. For multi-task learning with n tasks, task arithmetic sums the n task vectors, scales the sum with a coefficient a, and adds the resulting vector back to the pretrained model.\nIn this paper, we explain the effectiveness of task arithmetic by linking task vectors to the gradients of the average loss across all tasks. We start from a simple observation: when a model is finetuned for a single epoch using gradient descent (GD), the corresponding task vector is the additive inverse of the loss gradient, scaled by the learning rate. Similarly, the multi-task vector, obtained by summing individual task vectors, is equivalent to the additive inverse of the average loss gradient. Thus, task addition is analogous to performing a GD step on the sum of the average task losses.\nWhen finetuning spans multiple epochs, this equality becomes an approximation, with an error dependent on the learning rate."}, {"title": "2 RELATED WORK", "content": "Mode connectivity and model merging Mode connectivity studies how weights characterize local minima in the loss landscape. Frankle et al. (2020) explored linear mode connectivity in models trained from the same initialization, while Entezari et al. (2022) suggested that all models converge to a shared basin once neuron permutations are resolved. Building on this, permutation-based model merging combines diverse models into a single one, inheriting their capabilities without en-sembling overhead. Singh & Jaggi (2020) introduced an optimal-transport weight-matching method, while Git Re-Basin (Ainsworth et al., 2022) proposed three variants acting both on weights and activations, and REPAIR (Jordan et al., 2023) demonstrated significant barrier reduction through activation renormalization. Most recently, Navon et al. (2023) suggested merging models in the em-bedding space of deep neural networks, while Crisostomi et al. (2024) proposed a cycle-consistent matching procedure for improved merging. When models share the same pretrained initialization, Wortsman et al. (2022) proposed fusing them via a simple average. Jolicoeur-Martineau et al. (2023) proposed to merge models by pushing them towards the population mean to ensure stability. Reg-Mean (Jin et al., 2022) and Fisher-weighted averaging (Matena & Raffel, 2021) fall under the regime of weighted averaging, where the weights are optimized according to some criteria. Daheim et al. (2023) shed light on the positive relation between post-averaging multi-task performance and the gradient mismatch between the constituent models. Finally, Choshen et al. (2022) proposed model merging as a replacement for pretraining; they argue that pretrained checkpoints are not always the optimal starting point for further finetuning, and a model obtained by merging finetuned models can be a better starting point than any of its constituents.\nTask vectors Task vector-based merging (Ilharco et al., 2022) finetunes a pretrained model on dif-ferent tasks to obtain task vectors (differences between finetuned and original checkpoints). Arith-metic operations on these vectors enable forgetting, analogy learning, and multi-task learning. Several works aim to improve task vector merging by reducing task interference (Deep et al., 2024; Wang et al., 2024; Huang et al., 2024). Some methods include sparsifying task vectors or finetuning only lottery tickets (Panda et al., 2024). TIES-merging (Yadav et al., 2023) merges vectors by prun-ing, selecting a unified sign vector, and merging disjointly, while Model Breadcrumbs (Davari & Belilovsky, 2023) prunes both small and large-magnitude weights. DARE Merging (Yu et al., 2023) randomly masks out a portion of weights and scales up the rest. AdaMerging (Yang et al., 2023) optimizes aggregation coefficients, while Yang et al. (2024) proposed task-specific modules for test-time adaptation. Ortiz-Jimenez et al. (2024) introduced the concept of weight disentanglement and recommended finetuning in the tangent space. In contrast to these one-shot methods, we present an iterative model merging approach that progressively refines a base model to achieve improved multi-task performance."}, {"title": "3 TASK VECTORS AS GRADIENTS", "content": "In this section, we show that task vectors are tightly related to the loss gradients over the union of the tasks.\nTheorem 3.1. Let ${{\\theta^{(k)}_t}}_{t \\in T}$ to be a set of models obtained by finetuning the base model $O_{base}$ for k epochs on tasks $T$ using GD with a learning rate $\\eta$, where finetuning task $t \\in T$ minimizes the loss $L_t(\\theta) = \\frac{1}{n_t} \\sum_{i=1}^{n_t} l(x_i, Y_i, \\theta)$. Additionally, let ${{\\tau_t^{(k)}}}_{t \\in T}$ denote the corresponding set of task vectors, with each $\\tau_t^{(k)} = \\theta_t^{(k)} - \\theta_{base}$. Let $\\tau_{T}^{(k)}$ be the multi-task vector $\\tau_{T}^{(k)} = \\sum_{t \\in T} \\tau_t^{(k)}$. Finally, let $0_{MT}^{(k)}$ represent the model obtained by minimizing the combined loss $\\sum_{t=1}^{|T|} L_t$ for k epochs using"}, {"title": "4 ATM: ALTERNATING TUNING AND MERGING", "content": "Building upon the insights of Section 3, we argue that task arithmetic is an approximation to a single GD step over the union of all the tasks. Following this parallel, we advocate taking further update steps iteratively.\nThe overall framework of ATM is depicted in Fig. 1. Specifically, starting from a pretrained check-point as the base model $O_{base}$, we finetune it separately on each task to obtain the first-iteration task vectors ${\\tau_t^{(1)}}_{t\\in T}$. These are then aggregated and added to the base model to form the next-iteration unified model $\\theta_{base}^{(1)}$. The procedure is iterated according to the following equation:\n$\\theta_{base}^{(k+1)} = \\theta_{base}^{(k)} + \\frac{\\alpha}{|T|} \\sum_{t \\in T} \\tau_t^{(k)} \\qquad k = 0,..., K - 1.$\nThe k-th iteration task vector for task t is obtained as $\\tau_t^{(k)} = \\theta_t^{(k)} - \\theta_{base}^{(k)}$, where $\\theta_t^{(k)}$ is a model obtained finetuning the k-th iteration base model $\\theta_{base}^{(k)}$ on task t. The total number of iterations K can be predefined or based on a stop condition.\nIn practice, each iteration of ATM involves finetuning the current base model on all T tasks of interest, thereby obtaining |T| task vectors. These task vectors determine the task-specific directions the current base model should follow in order to attain enhanced performance on the corresponding tasks.\nThe merging step of ATM at a given iteration simply consists of summing the mean current-iteration task vectors to the current base model, although any interference-resolution method in the task vector literature can be integrated. This step is intended to pull the base model closer to the multi-task basin on the loss landscape. Taking the average over T ensures the magnitude of the update remains insensitive to the number of tasks.\nNote that after each iteration, the task vectors of the previous iterations can be safely discarded. Therefore, at any stage of ATM, we only store the current base model and one task vector for each of the |T| tasks, incurring no additional memory requirements."}, {"title": "5 UPPER BOUNDING THE MULTI-TASK LOSS", "content": "In this section, we explore the relationship between the ATM loss, defined as the mean of average losses over all tasks, and the loss of a model trained jointly on all the datasets. Analogously to Section 3, we conduct this analysis under the assumption that GD, rather than SGD, is used for optimizing the model parameters. This simplifying assumption removes the stochasticity introduced by random sampling, enabling a more straightforward analysis while still providing valuable insights into the underlying dynamics of the optimization process. With a slight abuse of notation, we denote with t both the task and its corresponding dataset with cardinality nt. The total number of samples for all tasks is given by $N = \\sum_{t \\in T} n_t$.\nInspired by Daheim et al. (2023), we define the target loss for model merging as $L_{target}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} l(x_i, Y_i, \\theta)$, which is the loss of a model trained jointly on all the datasets. By Theorem 3.1, when merging occurs after one step of finetuning on each dataset, the ATM update is given by:\n$\\theta_{base}^{(k+1)} = \\theta_{base}^{(k)} + \\frac{\\alpha}{|T|} \\sum_{t \\in T} \\tau_t^{(k)} = \\theta_{base} - \\frac{\\alpha\\eta}{|T|} \\sum_{t \\in T} \\nabla L_t(\\theta_{base}) \\text{,}$\nwhich corresponds to performing a GD step over the loss $L_{ATM} = \\frac{1}{|T|} \\sum_{t \\in T} L_t$.\nHaving established that one step of ATM in GD minimizes $L_{ATM}$, a crucial question arises: under what conditions does minimizing $L_{ATM}$ imply the minimization of $L_{target}$? In other words, when can we be certain that optimizing the ATM loss will also minimize the loss associated with training jointly on all datasets?\nTo answer this question, we first note that $L_{ATM}$ is an unweighted average of the individual dataset losses, while the target loss is a weighted average:\n$L_{target}(\\theta) = \\frac{\\sum_{t \\in T} n_t L_t (\\theta)}{\\sum_{t \\in T} N_t}$\nWe now analyze the parameter update from (0(k) to )(k+1). For both ATM and target methods, we denote the change in loss, Lmethod, as \u2206Lmethod = Lmethod(0(k)) \u2013 Lmethod()(k+1)). In the following theorem, we prove that if the drop in ATM loss exceeds a threshold \u03b4, the target loss will also decrease. The value of 8 depends on the size of the largest dataset with a decreasing loss and the smallest dataset with an increasing loss. In particular, if the former dataset is larger than the latter, a reduction in ATM loss reduces the target loss. In practice, this is ensured when the loss is reduced on the largest dataset.\nTheorem 5.1. Let $D = \\{t | \\Delta L_t > 0\\}$ be the set of datasets where the loss decreases after a parameter update, and $I = \\{t | \\Delta L_t < 0\\}$ be the set of datasets where the loss increases or remains unchanged. If the reduction in the ATM loss satisfies $\\Delta L_{ATM} > \\delta$, where\n$\\delta = \\frac{1}{|T|} \\left(1 - \\frac{\\min_{t \\in D} N_t}{\\max_{t \\in I} N_t} \\right) \\sum_{t \\in I} |\\Delta L_t|,$   \nthen the target loss $L_{target}$ will also decrease, i.e., $\\Delta L_{target} > 0$."}, {"title": "6 EXPERIMENTS", "content": "In this section, we compare ATM against several recent baselines across a number of classification tasks in computer vision and NLP."}, {"title": "6.1 EXPERIMENTAL SETTING", "content": "Datasets and Models For computer vision tasks, we test ATM with a ViT-B-16 backbone (Dosovitskiy et al., 2021) and evaluate it on a diverse set of datasets: CIFAR100 (Krizhevsky et al., 2009), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019), GTSRB (Houben et al., 2013), MNIST (Lecun et al., 1998), RESISC45 (Cheng et al., 2017), and SVHN (Netzer et al., 2011). For NLP tasks, we employ RoBERTa-base (Liu, 2019) and BERT-base-uncased (Devlin et al., 2019), evalu-ating them on the GLUE benchmark (Wang et al., 2019)."}, {"title": "6.2 IMPACT OF EPOCH DISTRIBUTION ON PERFORMANCE", "content": "In this experiment, we first establish a fixed compute bud-get of 10 finetuning epochs for each task. Then, we seek the optimal distribution of epochs among different num-bers of ATM iterations. To exemplify, if 10 epochs are distributed among 5 iterations, then in each iteration a task is finetuned for 2 epochs.\nAs depicted in Fig. 5, with a fixed compute budget, max-imizing iterations while minimizing epochs per iteration yields the best results for ATM. This indicates that more gradual updates to the base model are preferable to abrupt ones. Splitting 10 epochs across 10 iterations achieves the highest average accuracy of 89%, outperforming the 1 it-eration of 10 epochs setting (analogous to task arithmetic) by 21%. Based on this, we use the 1 epoch, 10 iterations setting for most subsequent experiments."}, {"title": "6.3 EFFECT OF COMPUTE BUDGET", "content": "We extend the comparison of ATM against baseline methods across different compute budgets. Specifically, we vary the per-task fine-tuning epochs (K) withint and te and compare the average test ac-curacy across tasks. As shown in Figs. 4 and 7, ATM consistently outperforms the baselines, with its advantage growing as the budget increases. ATM surpasses the best baseline by7%, 11%, and 21% for budgets of 2, 4, and 10 epochs, respectively. Detailed results for vision and NLP benchmarks can be found in Appendix B.2.\nInterestingly, while ATM's accuracy improves with more finetuning epochs, the baseline methods exhibit the opposite trend; see Fig. 6. In other words, the more specialized the task-specific models, the lower the performance of the unified multi-task model. ATM avoids this issue by gradually special-izing the intermediate multi-task models."}, {"title": "6.4 COMPARISONS IN ORIGINAL SETTINGS", "content": "Given the available training data, we compare three ATM settings against various baselines under their original settings: (i) ATM finetuned on validation data (valFT ATM) for 10 iterations of 1 epoch, (ii) ATM finetuned on training data for 10 iterations of 1 epoch, and (iii) ATM finetuned on training data to convergence for 30 iterations of 1 epoch. As illustrated in Table 1, all ATM variants significantly outperform the baselines. With a 10-epoch budget and available training data, ATM achieves an average accuracy of 89%, leading by 17% over the best-performing baseline. Without compute restrictions, ATM converges after 30 iterations, achieving a remarkable accuracy of 91%."}, {"title": "6.5 TRAINING-DATA FREE SETTING", "content": "A common realistic constraint is the lack of per-task training data, as finetuned models are often sourced from online repositories. In this section, we assume only the availability of validation data, typically used by baselines for hyperparameter tuning. Unlike the baselines, ATM uses this validation data for finetuning the tasks, leaving hyperparameters untuned.\nWe define this variant as valFT ATM and compare its performance against standard ATM and the baselines, using author-recommended hyperparameters for fair comparisons. As presented in Ta-ble 1, valFT ATM outperforms the best baseline by 10%. While the 7% gap between valFT ATM and ATM is due to the limited finetuning data, valFT ATM performs comparably to ATM on all tasks except EuroSAT. A similar trend is observed on the NLP benchmark."}, {"title": "7 DISCUSSION", "content": ""}, {"title": "7.1 ORTHOGONALITY", "content": "Orthogonality between task vectors has been recommended as a desirable property for multi-task merging (Ilharco et al., 2022). Davari & Belilovsky (2023) adopt pairwise cosine similarity between task vectors as a proxy for task interference. Following this line, we again back the validity of this observation by identifying a positive correlation between ATM performance and task vector orthog-onality. Fig. 9 highlights that as the number of ATM iterations increases, the magnitude of cosine similarity between task vectors tends to shrink, suggesting greater orthogonality as performance im-proves. Furthermore, as depicted in Fig. 10, we find that ATM task vectors exhibit lower-magnitude average cosine similarity compared to the baseline methods."}, {"title": "7.2 TASK PROFICIENCY IS NOT MERGEABILITY", "content": "As supported by figures 6 and 2, task-specific expertise does not imply multi-task performance. We observe that better-performing task-specific models result in worse multi-task models when adopting baseline methods, hinting that downstream performance is not a predictor of post-merging perfor-mance. We speculate that specialized models end up in highly dispersed locations in the parameter space, and merging them abruptly in a one-shot fashion generates a suboptimal multi-task model; this can be observed in Fig. 8, where baseline methods end up all in the same (suboptimal) loss basin. From the perspective of gradient approximation, this degradation can be explained by the fact that vanilla task vectors approximate increasingly noisy multi-task gradients when finetuning occurs for multiple epochs. On the contrary, a lower degree of specialization ensures the task-specific models remain closer to the pretrained checkpoint in the parameter space, as evinced by Ortiz-Jimenez et al. (2024) from the perspective of weight disentanglement in the tangent space, thus leading to less aggressive updates when merging. The above insights translate to less aggressive updates (shorter-norm task vectors) being more amenable to merging. This phenomenon is also consistent with the findings of Lu et al. (2024), explaining this phenomenon by the vanquishing common knowledge due to task-specific finetuning. Again, in view of gradient approximation, fewer epochs of fine-tuning yields a better approximation of the multi-task gradient, with a single epoch achieving the best approximation. Hence, our theoretical explanation generalizes the previous observations on this phenomenon. Capitalizing on this, ATM gradually aggregates task-specific models and updates the base model accordingly, merging less aggressively but over multiple iterations. At each iteration, the ATM task vectors represent the best nudges for the current base model without referring back to the initial pretrained model as all baseline methods do."}, {"title": "7.3 EDUCATED TRAJECTORY", "content": "Task arithmetic performs the aggregation step abruptly in a one-shot fashion over the initial pretrained checkpoint, likely overshooting the multi-task optimum. In ATM, however, the loss landscape is traversed iteration by it-eration as the base model updates, leading to more in-formed nudges toward the multi-task optimum. Fig. 8 de-picts the 2D projection of various checkpoints via PCA. Notably, TIES and breadcrumbs, being post-hoc enhance-ments of task arithmetic, end up around the same basin, whereas ATM takes gradual steps toward a different and better basin, signaling the effectiveness of our novel iter-ative merging paradigm."}, {"title": "7.4 TIME AND MEMORY COMPLEXITIES", "content": "We compare ATM to the baselines in terms of time and memory consumption, assuming the backbone has d parameters. Memory-wise, ATM has no additional require-ments compared to task arithmetic, as task vectors can be deleted after each merge step. The time complex-ity of iteration-k ATM is equivalent to performing task arithmetic k times, as it involves k rounds of finetuning and merging. This time complexity is generally negligi-ble compared to TIES and breadcrumbs, whose dominant overhead originates from pruning task vectors post-hoc according to the magnitude ranking of all weights, with a time complexity of O(d * log(d)). As outlined in Table 2, ATM is faster than TIES and breadcrumbs as long as log(d) is asymptotically greater than k, which is typically the case since k is usually a small constant (e.g., 10)."}, {"title": "7.5 LIMITATIONS OF ITERATIVE MERGING", "content": "While we have demonstrated the benefits of ATM in various settings, its iterative nature has some drawbacks. Specifically, iterative merging does not immediately provide a task representation for new tasks, as it produces a series of K vectors per task. However, it remains the best approach for obtaining a single model that performs well across all tasks. We ensured a fair comparison by using the same compute budget and data requirements, showing that a small validation set suffices for effective use of the framework \u2013 this is standard in the literature, typically used for hyperparameter optimization. Lastly, although ATM resembles gradient descent on the union of tasks, it retains a key advantage of task arithmetic: by generating task vectors independently, it avoids centralizing data, making it suitable for federated settings where data privacy is critical."}, {"title": "8 CONCLUSIONS", "content": "In conclusion, this paper identifies a key limitation of task arithmetic \u2013 overshooting due to abrupt model merging \u2013 and establishes its connection to gradient descent, forming the basis for our pro-posed model merging framework. We present Alternating Tuning and Merging (ATM), an iterative framework that addresses the shortcomings of one-shot merging techniques. By alternating between finetuning and merging, ATM effectively prevents overshooting and enhances multi-task perfor-mance. Extensive experiments on computer vision and NLP benchmarks demonstrate that ATM achieves state-of-the-art accuracy while maintaining computational efficiency comparable to exist-ing baselines. We explain the effectiveness of ATM from the perspective of multi-task gradient approximation. Additionally, our theoretical analysis reveals that ATM optimizes the upper bound of the loss over the union of all the tasks and improves task vector orthogonality. The flexibility of ATM opens numerous future research directions, including the integration of interference-mitigation techniques and further refinements through advancements from the gradient descent literature."}, {"title": "ETHICS STATEMENT", "content": "This research was conducted with a strong commitment to ethical standards in both data usage and experimental methodology. All datasets utilized in this study are publicly available. No personally identifiable information was accessed or used during the course of this research. Additionally, the experiments were designed to ensure fair comparisons across methods. We encourage future work that adheres to these same principles and addresses the broader societal impacts of machine learning."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We are committed to ensuring the reproducibility of our results and have taken steps to facilitate this for the broader research community. The code, datasets, and configurations used for the experiments in this paper are made available via a public repository. We are open to providing further instructions on the usage of our code. The hyperparameters, frameworks, and evaluation metrics have been"}, {"title": "A PROOFS", "content": ""}, {"title": "A.1 PROOFS OF THEOREM 3.1 AND COROLLARY 3.1.1", "content": "In this section, we provide proofs for Theorem 3.1 and Corollary 3.1.1. For clarity, we restate both the theorem and corollary.\nTheorem. Let ${{\\theta^{(k)}_t}}_{t \\in T}$ to be a set of models obtained by finetuning the base model $O_{base}$ for k epochs on tasks $T$ using GD with a learning rate $\\eta$, where finetuning task $t \\in T$ minimizes the loss $L_t(\\theta) = \\frac{1}{n_t} \\sum_{i=1}^{n_t} l(x_i, Y_i, \\theta)$. Additionally, let ${{\\tau_t^{(k)}}}_{t \\in T}$ denote the corresponding set of task vectors, with each $\\tau_t^{(k)} = \\theta_t^{(k)} - \\theta_{base}$. Let $\\tau_{T}^{(k)}$ be the multi-task vector $\\tau_{T}^{(k)} = \\sum_{t \\in T} \\tau_t^{(k)}$. Finally, let $0_{MT}^{(k)}$ represent the model obtained by minimizing the combined loss $\\sum_{t=1}^{|T|} L_t$ for k epochs using"}, {"title": "B ADDITIONAL RESULTS", "content": ""}, {"title": "B.1 FULL RESULTS OVER VARYING COMPUTATIONAL BUDGET", "content": "In the main paper, we pictorially illustrated the multi-task accuracy of the baselines and ATM vari-ants, in the form of radar plots. For deeper analysis, here we report the full results of ATM compared to the baselines, as the computational budget varies for 2, 4, 7, and 10 epochs. See Tables 3, 4, and 5 respectively."}, {"title": "B.2 COSINE SIMILARITY OF EPOCH-WISE GRADIENTS", "content": "We report in Fig. 11 the cosine similarity of gradients for the first 10 epochs over DTD and EuroSAT, as these do not have a marked difference in gradient norms between the first epoch and the remaining ones in Fig. 3a. We see that the alignment of subsequent gradients observed in RESISC45 still holds in DTD, even if with less marked similarities. On the other hand, this does not seem to hold for EuroSAT."}]}