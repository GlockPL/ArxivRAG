{"title": "MELT: Materials-aware Continued Pre-training for Language Model Adaptation to Materials Science", "authors": ["Junho Kim", "Yeachan Kim", "Jun-Hyung Park", "Yerim Oh", "Suho Kim", "SangKeun Lee"], "abstract": "We introduce a novel continued pre-training method, MELT (Materials-aware continued pre-training), specifically designed to efficiently adapt the pre-trained language models (PLMs) for materials science. Unlike previous adaptation strategies that solely focus on constructing domain-specific corpus, MELT comprehensively considers both the corpus and the training strategy, given that materials science corpus has distinct characteristics from other domains. To this end, we first construct a comprehensive materials knowledge base from the scientific corpus by building semantic graphs. Leveraging this extracted knowledge, we integrate a curriculum into the adaptation process that begins with familiar and generalized concepts and progressively moves toward more specialized terms. We conduct extensive experiments across diverse benchmarks to verify the effectiveness and generality of MELT. A comprehensive evaluation convincingly supports the strength of MELT, demonstrating superior performance compared to existing continued pre-training methods. In-depth analysis of MELT also shows that MELT enables PLMs to effectively represent materials entities compared to the existing adaptation methods, thereby highlighting its broad applicability across a wide spectrum of materials science.", "sections": [{"title": "1 Introduction", "content": "Materials science encompasses interdisciplinary studies concerning the behaviors, properties, and applications of materials. Given the vast search space in materials science, deep learning-based approaches have emerged as significant avenues to accelerate the entire research pipeline (Tshitoyan et al., 2019; Weston et al., 2019; Olivetti et al., 2020). Specifically, methods centered on natural language processing (NLP) have provided promising results across a number of materials tasks, such as materials entity recognition (Weston et al., 2019; Friedrich et al., 2020) and synthesis action retrieval (Wang et al., 2022a). However, the limited and scarce nature of datasets in this domain poses substantial challenges for developing models that generalize well across a broad range of materials entities (Song et al., 2023a).\nOne promising approach to addressing this limitation involves adapting the pre-trained language models (PLMs) for materials science by continuously pre-training them on materials science corpus (Gupta et al., 2022; Huang and Cole, 2022). However, these methods have predominantly focused on constructing the domain-specific corpora used in the continued pre-training process, neglecting the training strategies employed in the adaptation process. This oversight can lead to inefficiencies in capturing domain-specific nuances and knowledge. For example, the chemical formulas (e.g., LiCoO2), which are fundamental terms in the materials science field, are typically infrequent words (Figure 1). Therefore, these domain-specific terms are often inadequately captured by the random masking strategy used in previous studies, leading to sub-optimal adaptation results."}, {"title": "2 Related Works", "content": "The increasing number of textual datasets in materials science (e.g., scientific publications, patents) has facilitated the use of NLP-based approaches to address various materials tasks, such as relation classification (Mysore et al., 2019a; Mullick et al., 2024a) and materials entity extraction (Weston et al., 2019; Friedrich et al., 2020). For example, Weston et al. (2019) proposed a bidirectional LSTM tagger for named entity recognition on tags associated with the well-known materials science tetrahedron (i.e., structure, property, processing, performance). Tshitoyan et al. (2019) demonstrated promising results with embedding-based unsupervised methods for understanding chemistry knowledge and chemical properties. Beyond embedding models, Trewartha et al. (2022) introduced PLMs trained on a materials science corpus following the BERT procedure (Devlin et al., 2019). Similarly, Gupta et al. (2022) and Huang and Cole (2022) adapted SciBERT (Beltagy et al., 2019) and BERT (Devlin et al., 2019) to the domains of general materials and battery-specific corpora by additionally training on the domain-specific corpus. Recently, HoneyBee (Song et al., 2023b) suggested the materials domain-specific instruction data to fine-tune the large language models.\nPrevious studies have demonstrated promising results in adapting PLMs to materials science. However, these methods have primarily focused on adapting the corpus for materials science while employing a basic random masking strategy (Devlin et al., 2019). Such a domain-agnostic approach potentially prevents the models from adequately learning about chemical entities and diverse formulas, which often fall into the less frequent tail distributions in word frequency. In contrast, MELT is a tailored approach to materials science, focusing on both the adaptation corpus and learning strategies to adapt PLMs for materials science."}, {"title": "2.2 Continued Pre-training of PLMs", "content": "Building PLMs from scratch requires substantial computational resources; therefore, continued pre-training has garnered significant attention, especially in scientific domains. For example, Gururangan et al. (2020) adapted PLMs trained on general corpora to various domains (e.g., computer science, biomedical) by performing random-based masked language modeling on the target domain corpus. Lin et al. (2021b) performed the domain adaptation by selectively masking entities based on the trained taggers. Subsequently, Wilf et al. (2023) proposed Diff-Masking that utilizes the frequency difference between generic and target domains to perform masked language modeling on domain-specific terms. In addition to the masking strategy, several works studied the catastrophic forgetting"}, {"title": "3 Materials-aware Continued Pre-training", "content": "We elaborate on the proposed continued pre-training method, MELT. The key strategy is to extract the material entities from the scientific papers and inject the extracted knowledge of the materials into the PLMs through a masked language modeling (MLM) objective. To this end, we first extract the materials entities from the materials science corpus. From these entities, we expand the materials knowledge by semantically augmenting the related entities and properties. We then start pre-training on the basic materials entities and progressively move more specialized knowledge in a curriculum manner. The overall procedures are described in Figure 2."}, {"title": "3.1 Continued Pre-training through MLM", "content": "We start by setting the training objective of continued pre-training. Following the promising results of the previous study (Gururangan et al., 2020), we adapt the PLMs to different domains by performing MLM (Devlin et al., 2019) on the domain-specific corpus related to materials science. Specifically, it masks out a small portion of the input sequence (i.e., replace the original word with the special token [mask]) and trains the model to predict the original tokens. Formally, let the set of words to be masked be denoted as G, the MLM training objective is as follows:\n$L(x, \\hat{x}; \\theta) = \\sum_{W_m\\in G,W_m\\in x} log P(y = W_m | x; \\theta)$\nwhere x and \u00ee denote the original and masked input sequences based on the set of masking words G, respectively. In a typical pre-training objective, the set G is randomly determined without considering the importance of words in the target domain. However, such a random strategy makes the PLMs poorly learn the sparse domain-specialized terms (Wilf et al., 2023), as these terms are rarely selected"}, {"title": "3.2 Materials-aware Entity Masking", "content": "Chemical Entity Extraction While the general domain corpus is widely spread across diverse sources and formats (e.g., Wikipedia, News, SNS), the large-scale corpus for materials science mainly exists in the form of scientific publications (e.g., patents, papers). Moreover, it has several distinct characteristics. First, the materials corpus includes a number of chemical formulas (e.g., H2O, LiCoO2), which are crucial components to understanding materials science. For example, in materials benchmarks for token classification, such as named entity recognition, the formulas roughly take up 20% among all entities\u00b2. Secondly, materials corpus frequently include various representations of domain-specific jargon and abbreviations (e.g., \"THF\" for Tetrahydrofuran, \"AZT\" for 3'-azido-3'-deoxythymidine).\nTo extract these terms to learn materials knowledge, we follow the hybrid approach of ChemDataExtractor (Swain and Cole, 2016; Mavracic et al., 2021), utilizing both dictionary-based mapping and CRF taggers to identify the materials entities. The extracted terms include not only elemental and compound names but also materials characteristics such as density, melting point, and electrical conductivity. This wealth of material-aware entity extraction provides a solid foundation for the continued pre-training.\nSemantic Graph for Knowledge Expansion While the various types of material-aware entities from previous extraction phases involve knowledge about materials, they still lack the fundamental concept of materials science about structure-property-processing-performance paradigm within materials science (William and Callister, 1989).\nTo augment these fundamental relations, we construct semantic graphs of materials entities by connecting related terms and finding similar yet missing entities with the seed entities. To this end, we first train a lightweight embedding model (Tshitoyan et al., 2019) on the materials corpus 3. Based on the learned embeddings of materials terms and entities, we leverage the compositional property"}, {"title": "3.3 Curriculum-based Entity Learning", "content": "While continued pre-training with the tailored masking can effectively train the knowledge of materials sciences, learning with specialized entities from the beginning makes the training unstable,"}, {"title": "4 Experiments", "content": "In this section, we verify the efficacy of the MELT. Specifically, we answer the following four questions through extensive experiments and analysis:\nQ1 (Adaptability) Does MELT enable better adaptation than existing methods across diverse benchmarks? (\u00a74.2, \u00a74.4 \u00a74.5)\nQ2 (Transferability) How the extracted materials knowledge from MELT is related and affect to downstream tasks? (\u00a74.3, \u00a74.6, \u00a74.7)\nQ3 (Efficiency) Does MELT offer better efficiency than existing continued pre-training methods for domain adaptation? (\u00a74.8)\nQ4 (Insights) What materials science knowledge is extracted and learned during the continued pre-training? (\u00a74.9)"}, {"title": "4.1 Experimental Setups", "content": "Baselines To confirm the effectiveness of MELT, we mainly compare ours with strong domain adaptation methods for PLMs: DSP (Gururangan et al., 2020), EntityBERT (Lin et al., 2021a), Diff-Masking (Wilf et al., 2023), and DAS (Ke et al., 2023). The detailed experimental setups for baselines are described in Appendix C.1.\nPre-training To adapt the PLMs to the domain of materials science, we leverage the 150K scientific papers related to materials science following the previous work (Gupta et al., 2022). In MELT, we implement iterative curriculum learning, consisting of 10K steps for warm-up training and 10K steps for each subsequent curriculum stage. We set the number of curriculum stages (K) to 3 based on the empirical analysis. Detailed analysis is represented in Appendix F.\nDownstream Tasks and Datasets To demonstrate the diverse aspects of the MELT, we compare the models on both generation and classification tasks. For generation tasks, we evaluate each baseline using the MatSci-NLP dataset (Song et al., 2023a), which comprises seven materials-related tasks (e.g., materials entity recognition, slot filling). Detailed information on each task within MatSci-NLP can be found in Appendix B. For classification tasks, we adopt four different tasks following the previous research (Gupta et al., 2022), which include NER (MatScholar (Weston et al., 2019), SOFC-EXP (Friedrich et al., 2020)), paragraph"}, {"title": "4.2 Main Results", "content": "Table 1 presents the overall performance results on the MatSci-NLP benchmark. The comparison indicates that MELT yields the best adaptation outcomes compared to previous methods. Specifically, MELT enhances the performance of the backbone model by an average of 6.9% and 15.3% in terms of Micro-F1 and Macro-F1 scores, respectively, across all tasks. This indicates the broad applicability of the proposed method to various materials science tasks. It is also noteworthy that domain-agnostic methods (i.e., Diff-Masking, DAS) reveal limited performance improvement compared to the random masking baseline (i.e., DSP (Gururangan et al., 2020)). It underscores the effectiveness of the domain-specific approach in continued pre-training. Overall, the results strongly support the superiority of the proposed method in adapting PLMs to materials science."}, {"title": "4.3 Ablation Study", "content": "We perform ablation studies to confirm whether the components in MELT are crucial in producing better PLMs for materials science. Table 2 shows the ablation results about two components: (i) Materials-aware entity masking (MEM) by building a semantic graph of chemical entities (ii)"}, {"title": "4.4 Effect of the Curriculum Adaptation", "content": "To further investigate the effectiveness of the proposed curriculum learning, we compare ours with the existing curriculum-based MLM strategies: Frequency, Concept, Masking Ratio, and Reverse7. Table 3 presents the comparison results on the MatSci-NLP benchmark. We observe that the curriculum approach in MELT demonstrates superior performance compared to existing curriculum strategies. These results underscore the efficacy of considering node degree in materials semantic graphs when adapting PLMs for materials science."}, {"title": "4.5 Evaluation on Classification Tasks", "content": "To assess the generality of our model, MELT, we evaluate each baseline on four classification tasks following the settings from prior work (Gupta et al., 2022). We present the evaluation results on test sets in Table 4. Similar to the results in generation tasks, MELT outperforms other baselines in most cases. These results demonstrate that PLMs adapted by MELT perform effectively across various classification tasks."}, {"title": "4.6 Masking Relevance to Downstream Tasks", "content": "We additionally analyze how the extracted knowledge relates to the entities of the downstream tasks. Here, we compare the proposed method with the random masking. For evaluation, we aggregate all words with their tags in NER (MatScholar and SOFC), RC, and SF tasks and calculate the over-"}, {"title": "4.7 Effect of Materials-aware Entity Masking", "content": "The extracted knowledge from MELT is centered on chemical entities (the first step of the proposed method), and the benchmark datasets for materials science involve a number of chemical entities. To verify the effectiveness of the entity-centric approach, we further analyze the performance of the fine-tuned model for the classes of the chemical entities (i.e., chemical names or formulas) in the slot-filling task (SOFC-Filling). Figure 4 shows the categorical performance of the entity-based (ours) and random-based approaches. We observe that our MELT achieves superior performance in all materials categories. Specifically, MELT outperforms random-based masking about 25% on the Support materials class. Moreover, our MELT performs better than random masking across a variety of properties. These results indicate that materials-aware entity masking can improve the generalization ability of PLMs by learning from diverse material entities."}, {"title": "4.8 Efficiency of Continued Pre-training", "content": "We analyze the training efficiency of our proposed MELT by comparing the performance of SOFC-"}, {"title": "4.9 Extracted Materials Knowledge", "content": "Our MELT extracts chemical entities from the scientific corpus and expands the knowledge through a materials semantic graph. To validate the constructed knowledge, we sample a part of the graph with the five relation types (i.e., Property, Method, Application, Symmetry label, Descriptor). Figure 6 represents the expansion of material entities (e.g., DP, electrodes, cotunnite) with five relation types from extracted chemical entities in corpora (i.e., FeO3)8. We observe that most extracted entities (e.g., electrodes, Micro Electromechanical Systems) represent the related concepts well, indicating the effectiveness of our automatic knowledge expansion. Moreover, even in cases of entities with incorrect relations (e.g., CaO, Al2O3 with the property relation), these entities are still materials-related words, demonstrating that such entities can be effectively used in the training stage. The overall results highlight the usefulness of the extracted knowledge from MELT. More examples can be found in Appendix G"}, {"title": "5 Conclusion", "content": "In this paper, we have proposed MELT, a novel continued pre-training method to adapt PLMs for materials science. Unlike previous approaches that typically focus on the adaptation corpus, our method comprehensively considers both the corpus and the training strategy in the adaptation process. Specifically, we have constructed the materials knowledge base by considering materials science tetrahedron (William and Callister, 1989). This extracted knowledge is then transferred to the PLMs in a structured, curriculum-based manner. We have conducted extensive experiments across diverse materials science benchmarks. The evaluation results convincingly demonstrate that our tailored approach yields superior performance on downstream tasks compared to domain-agnostic methods. In-depth analysis focusing on generality, efficiency, and insights has further supported the efficacy of MELT in continued pre-training, highlighting its broad value in the field of materials science."}, {"title": "Limitations", "content": "While we have demonstrated that MELT effectively adapts the PLMs on materials science domains, there are several limitations that present valuable opportunities for future research."}, {"title": "Further Analysis on Generation Tasks", "content": "We have mainly focused on improving the efficacy of continued pre-training on information retrieval tasks (e.g., NER, Slot Filling), aligning with previous works (Song et al., 2023a; Trewartha et al., 2022; Gupta et al., 2022). However, the applicability of MELT on further generation tasks for material discovery, such as hypothesis or code generation (Miret and Krishnan, 2024), remains underexplored in this work. Nevertheless, considering the significant performance improvements of MELT achieved on generation benchmarks, MatSci-NLP, we believe that MELT is expected to work well within other generation tasks. We leave the exploration of this direction as promising future research."}, {"title": "Tokenization for Materials Sciences", "content": "We have expanded materials entities and masked them to adapt the PLMs on the materials domain in this work. During this process, we have observed that complex materials terms such as chemical formulas and substances split into multiple subwords (e.g., LiMnO2 tokenized to {LiMn, ##O, ##2}). Such over-tokenization may limit the PLMs to learning meaningful representations in the pre-training stage, which leads to sub-optimal results on the downstream tasks (Kononova et al., 2021). These observations highlight the necessity of materials domain-specific tokenizers for processing complex chemical terms, and we expect that future work in this direction will be another promising avenue for domain adaptation for PLMs."}]}