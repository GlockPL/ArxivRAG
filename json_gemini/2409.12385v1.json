{"title": "Look Through Masks: Towards Masked Face Recognition with De-Occlusion Distillation", "authors": ["Chenyu Li", "Shiming Ge", "Daichi Zhang", "Jia Li"], "abstract": "Many real-world applications today like video surveillance and urban governance need to address the recognition of masked faces, where content replacement by diverse masks often brings in incomplete appearance and ambiguous representation, leading to a sharp drop in accuracy. Inspired by recent progress on amodal perception, we propose to migrate the mechanism of amodal completion for the task of masked face recognition with an end-to-end de-occlusion distillation framework, which consists of two modules. The de-occlusion module applies a generative adversarial network to perform face completion, which recovers the content under the mask and eliminates appearance ambiguity. The distillation module takes a pre-trained general face recognition model as the teacher and transfers its knowledge to train a student for completed faces using massive online synthesized face pairs. Especially, the teacher knowledge is represented with structural relations among instances in multiple orders, which serves as a posterior regularization to enable the adaptation. In this way, the knowledge can be fully distilled and transferred to identify masked faces. Experiments on synthetic and realistic datasets show the efficacy of the proposed approach.", "sections": [{"title": "1 INTRODUCTION", "content": "Human faces in the wild are often occluded by masks, intentionally or unintentionally. The ability to handle recognition towards masked faces is essential for many visual applications, e.g. video surveillance [19] and urban governance [16]. In the last few years, the deep-learning-based face recognition models [3, 47, 49, 50, 52, 61] have been able to achieve or even exceed human-level performance on public benchmarks. This has much attributed to our growing understanding of how our brain may be solving the identity recognition task. However, face recognition under more challenging conditions, such as masked faces, is less characterized. The question is how to represent these masked parts of perceived faces: this is the problem of amodal perception [37].\nTo facilitate recognition towards masked faces, intuitive methods [54, 56] that seek to extract credible features from visible regions permit direct mapping between the input partial observation in the source domain and expected output in the target domain. They usually split the input image into several local parts and predict the possibilities of each being masked, on which the allocated weights for local parts are based. These approaches are supported by biological neural science conclusions that masked objects are selectively perceived as disconnected elements [5, 22].\nRecent researches have proved amodal completion indispensable in perceiving partly-observed objects. In [5] Chen et al. revealed that amodal completion is first manifested in face-selective areas, then implemented through feedback and recurrent processing among different cortical areas. Via attention mechanism and context restraints, many effective algorithms for occluded object recognition have been proposed [20, 52]. However, different from general object or shape recognition, facial identity relies on both shape/view and appearance [4]. The latter involves more complexity and ambiguity when being masked. Given an example, with an image of a man with a face mask, it's rather easy to tell from isolated parts like eyes that there is a man, while difficult to tell his identity. Amodal perception for masked faces is hard to achieve by conventional Deep Neural Networks (DNNs), where completion is done implicitly.\nWe propose to explicitly regularize the amodal completion process. The most related task is face completion. Recent approaches [18, 26, 41, 57] based on Generative Adversarial Networks (GANs) [11], which formulate completion as a conditional image generation problem, have evolved as compelling tools that generate photo-realistic results. However, recently Joe et al. [36] looked into the question that whether generative face completion helps recognition. Experimental results showed that, though completion greatly please biometric systems, the benefit for recognition is limited. We suspect that it arises from the innate selectivity-invariance pattern of CNNs, where higher layers of representation amplify shared aspects and suppress irrelevant variations [23]. General constraints on efficient generative completion naturally lead to an axis representation with strong appearance bias. There exists a clear domain gap between the inexact visual imagery [38] and realistic faces.\nIn this work, inspired by the amodal completion mechanism in the human brain, we propose a novel de-occlusion distillation framework to deal with the task of masked face recognition, as shown in Fig. 1. The model consists of two main modules, de-occlusion and distillation. The de-occlusion module applies a GAN-based face completion network to eliminate the appearance ambiguity and enables the masked face to be perceived as a whole. The attention mechanism is introduced to teach the model to \"look\" at informative areas. Then to subsequently benefit recognition, the distillation module takes a pre-trained general face recognition model as the teacher and adapts its knowledge to completed faces through knowledge distillation. Recently Ge et al. [9] employed identity-centered regularization and gained an effective accuracy boost, which inspired us to exploit in deep generative models rich problem structures and domain knowledge. Assuming the distribution of unmasked faces could provide essential guidance, we represent the teacher knowledge with structural relations among instances. Via enforcing various orders of structural similarities to provide a posterior regularization, the student learns to perform accurate recognition towards completed faces. We evaluate the proposed method on both synthetic masked face datasets (Celeb-A [32] and LFW [17]) and realistic masked face datasets (AR [35]), both showing compelling improvements on recognition accuracy.\nOur main contributions can be summarized as three folds: 1) We propose a novel end-to-end framework for masked face recognition, which first enforces face completion explicitly and then transfer rich domain knowledge from pre-trained general face recognition model via knowledge distillation; 2) We introduce the theory of amodal perception to shed light on the masked face recognition task, and our empirical results echo the theory and 3) We conduct extensive experiments to demonstrate the efficacy of our approach."}, {"title": "2 RELATED WORKS", "content": "2.1 Amodal Perception and Face Completion\nHumans are able to recognize objects even when they are partially occluded by another pattern, so easily that one is usually not even aware of the occlusion. The phenomena of completion of partly occluded shape have been termed \"amodal perception\" [37], since the occluded contours are not seen. Kovacs et al. [22] found that single IT units remain selective for shape outlines under a variety of partial occlusion conditions, physiologically locating where amodal perception happens for the first time. So one last question we care about is: how the occluded contents are represented. In [22] the discrimination performance was found much better when they were familiar with the subjects. This suggests that amodal perception relies heavily on our background knowledge of how the occluded parts of the object (may) look. They also find that the IT cells only respond to selective fragments, and conclude that amodal completion doesn't happen. Chen et al. [5] delves into the time course of amodal completion in face perception. Their results suggest amodal completion is first manifested in face-selective areas, then implemented through feedback and recurrent processing among different cortical areas. Therefore, amodal completion plays an indispensable role in perceiving partly-observed objects.\nFace completion, or inpainting, aims to recover masked or missing regions on faces with visually plausible contents. Traditional exemplar-based approaches [1, 12] searched similar patches as reference for the synthesis of missing regions. While this non-parametric manner achieves good results when similar content is available, the mechanism is not scalable for objects with unique textures, e.g. faces. Recently, the GAN-based architecture has been widely adopted in completion with visually satisfactory results [18, 26, 41, 44, 58]. They usually train an auto-encoder to predict the missing region using a combination of reconstruction loss and adversarial loss. Despite their capacity in recovering high-quality visual patterns, the recognition accuracy gain is still limited [36].\n2.2 Occluded Object Recognition\nPartial occlusions are one of the greatest challenges for many vision tasks, e.g. classification [20], recognition [61], and person re-ID [27]. Various approaches have been proposed to solve the problem, following \"representation\u201d or \u201creconstruction"}, {"title": "3 DE-OCCLUSION DISTILLATION", "content": "In this section, we first provide an overview of our proposed approach, then describe the details of each network component as well as the loss functions.\n3.1 Problem Formulation\nMasked faces are faces that not fully observed. In this section, we dissect the problem of masked face recognition and try to provide simple yet solid explanations for two questions: i) What help does data recovery do? ii) What needs to do after data recovery?\nHere we describe the generative process for partly observed data, following the setting of missing data processing [28]. Let $X \\in R^n$ be a data vector and $M \\in {0, 1}^n$ is a binary mask indicating which entries in X to reveal: $x_d$ is observed if $m_d = 1$ and vise versa.\n$X \\sim p_\\theta(X), \\quad \u041c \\sim p_\\varphi (\u041c|X),$ (1)\nwhere $\\theta$ denotes the parameters of data distribution and $\\varphi$ denotes the parameters of the mask distribution. The mask distribution is usually assumed to depend on the data X. Let $X_o$ denote the observed elements of X, and $X_m$ denote the missing elements according to the mask M. We define the target attribute as $a_t$. In the standard maximum likelihood setting, the unknown parameters are estimated by maximizing the following marginal likelihood, integrating over the unknown missing data values:\n$p(a_t, X_o) = \\int p_\\theta (X_o, X_m) \\cdot p_\\varphi (M|X_o, X_m) \\cdot p_\\gamma(a_t |X_o, X_m) dX_m,$ (2)\nwhere $p_\\gamma (a_t | X_o, X_m)$ is a recognizor that gives prediction with $X_m$ replacing the missing region. Due to the ambiguity introduced by masks, the optimization process involves integration over literally infinite possible $X_m$. Even with the remarkable capacity of well-crafted DNNs, it is difficult to reach convergence. One simple technique is to multiply with an impulse function $\\delta(X_m \u2013 \\hat{X_m})$:\n$\\delta(\\hat{X} \u2013 X_o) = \\begin{cases} \\infty, X = X_0 \\\\ 0, X \\neq X_0 \\end{cases}$ (3)\nThe physical meaning for this operation is to select the best restoration $\\hat{X_m}$ for the missing data based on certain criterions, e.g. the coherence with the observed data regarding the wearing mask, measured by $p_\\theta (X_o, X_m)p_\\varphi (m|X_o, \\hat{X}m)$. In this way, the optimization problem in Eq. 2 can be simplified as:\n$p(a_t, X_o) = \\int p_\\theta (X_o, X_m) \\cdot p_\\varphi (m|X_o, X_m)p_\\gamma(a_t|X_o, X_m) \\cdot \\delta(X_m \u2013 \\hat{X_m}) dX_m$ (4)\n$= p_\\theta(X_o, \\hat{X}m) \\cdot p_\\varphi(m|X_o, \\hat{X}m) \\cdot p_\\gamma(a_t|X_o, \\hat{X}m),$ (5)\n$\\hat{X}m = arg \\max_{Xm} p_\\theta(X_o, X_m) \\cdot p_\\varphi(m|X_o, \\hat{X}m),$ \nwhich turns out to be proportional to the prediction towards the completed data $p_\\gamma(a_t |X_o, \\hat{X}m)$, as the former two terms can be seen as constants once $X_m$ is decided. Via data recovery, we turn the intricate problem in Eq. 2 into two sub-problem: finding the best restoration $\\hat{X}m$ and acquiring accurate prediction $p_\\gamma(a_t |X_o, \\hat{X}m)$. This answers for our first question.\nNaturally we wonder if $p_\\gamma (a_t |X_o, X_m)$ could adopt a pre-trained state-of-the-art recognition model for faces in the wild. Following our former analysis, the optima $\\hat{Xm}$ is obtained by solving a maximum optimization. Numerical solutions for these high-order space concerning optimizations are not available. In practice, we approach"}, {"title": "3.2 Appearance Recovery via Completion", "content": "In the de-occlusion module, we explicitly enforce amodal completion via a generative face completion model. First, it's important to emphasize that amodal perception is manifested in face-selective areas, and masked faces are perceived as disjointed segments. Attention plays a very important role here. We adopt the same architecture as in [57], which consists of a generator for inpainting and two auxiliary discriminators for regularizing from local and global views, with a contextual attention mechanism.\nGenerator Given an image of a masked face and a binary mask indicating the missing regions, the generator G(X, M; WG) aims to generate a photo-realistic result as similar with the ground-truth as possible. To achieve that, a pixel-wise reconstruction loss is employed to penalize the divergence, formulated as:\n$L_G = l_1(X, Y) = l_1(G(X, M; W_G), Y).$ (7)\nLocal and Global Discriminators Two discriminate networks are adopted to identify whether input images are real or fake from global and local views, respectively. The global discriminator takes the whole image as input, while the local one uses the completed region only. Contextual information from local and global views compensate each other, eventually reaching a balance between global consistency and local details. They regularize the generator via local and global adversary losses:\n$L_D = \\min_D \\max_G E[log D_i(Y) + log(1-\\begin{array}{c}G \\\\ D_i\\end{array}(G(X, M); W_G))], i \\in {g,l},$ (8)\nwhere $D_i$ denotes the global discriminator when i = g and the local discriminator when i = l."}, {"title": "Contextual Attention", "content": "The contextual attention layer enables the generator to refer to features from the whole image and to learn long-distance semantic dependencies. It computes the similarity of patches centered in missing pixel (m, n) and observed pixel (p, q):\n$S_{p,q,m,n} = \\frac{X_{p,q} \\cdot X_{m,n}}{`||x_{p,q}||' ||x_{m,n}||''},$ (9)\nwhere $x$ and $x$ denotes the masked and the completed face image, separately. The calculated similarities are then send through a softmax layer to obtain attention score for each pixel $s_{p,q,m,n} = softmax_{m,n}(\\lambda\\cdot S_{p,q,m,n})$, where $\\lambda$ is a constant value. Finally the image contents are reconstructed by performing de-convolution on attention score. The contextual attention layer is differentiable and fully convolutional. Implementation details refer to [57]."}, {"title": "3.3 Identity Recovery via Distillation", "content": "The last stage has recovered missing visual contents via GAN-based face completion. Experiments suggest activations responsible for amodal completion happen in the same place where cells are activated when we visualize objects with our eyes closed [21]. It is easy to accept that between an actual visual stimulus and visual imagery, there exists a non-ignorable domain gap. We here raise our insight that, between the ground truth and the heuristic completion results, there also exists a non-ignorable domain gap. This is consistent with the unsatisfactory performance of generative face completion helping recognition applications. To bridge the gap, we propose to rearrange the identity features via knowledge distillation.\nKnowledge distillation is a widely applied technology to transfer the knowledge from a cumbersome teacher network into a compact counterpart. To be general, the goal function for traditional knowledge distillation can be formulated as:\n$L_c = \\sum_{x_i \\in X} l(t_i, s_i),$ (10)\nwhere $t_i$ and $s_i$ denote the feature representation produced by the teacher and student respectively, with $x_i$ as input; and l denotes specific loss function adopted to penalize the differences.\nTraditional distillation usually focuses on classification tasks, trained with the Cross-Entropy loss. During training, the output class distribution generated by the student is forced to be close to that of the teacher. In this way, the student could obtain better results than directly trained with class labels. The main reason may lie in that probability distribution over classes provided by the teacher's output, reveals relevance information between classes, therefore providing richer knowledge than ground truth labels.\nHowever, the present distillation methods remain limited. Existing distillation methods usually focus on the point-wise similarity between representations of teacher and student. Previous researches [7, 39, 49, 52] have verified that instance relationships can help reduce the intra-class variations and enlarge the inter-class divergences in the feature space. Nevertheless this is rarely considered in distillation. We assume that what constitutes the knowledge is better presented by relations of the learned representations than individuals of those, and the structural distribution of unmasked faces could provide essential guidance for the identity feature rearrangement of completed faces. Besides, point-wise distillation methods usually require the teacher and student to share similar"}, {"title": "network architecture and close data domains.", "content": "Here in the masked face recognition scenario, we seek to distill the rich knowledge about feature distribution for unmasked faces and use them to guide the rearrangement of that of completed faces. The common characteristics we seek here should be the aggregation behaviors, in other words, the instance relationships, which are more robust to network changes and domain shifts.\nLet $f_t (Y; W_t)$ and $f_s (X; W_s)$ be the sub-networks composed by the first several layers of the teacher network $f_t (Y; W_t)$ and the student network $p_s (X; W_s)$, respectively, where Y and X is the corresponding input. $f_t (Y; W_t)$ is the feature extraction backend before the softmax layer for extracting the identity features of unmasked faces, while $p_s (X; W_s)$ denotes the layers before the embedding layer, used to extract features of masked faces. The training process of the student network can be described as transferring the relational structure of the output representation $f_t (Y; W_t)$ to $p_s (X; W_s)$, to improve the final recognition ability of $p_s (X; W_s)$. Let $Y^n$ and $X^n$ denote a set of n-order tuple of unmasked and completed faces respectively, $s_i = p_s(x_i; W_s)$ is the student knowledge gained from a completed face and $t_i = f_t(y_i; W_t)$ is the teacher knowledge distilled from the corresponding ground-truth. The loss function for n-order distillation process can be formulated as\n$L_n = \\sum_{\\begin{smallmatrix} (y_1,..., y_n) \\in Y^n, \\\\ (x_1,...,x_n) \\in X^n \\end{smallmatrix}} l(\\psi(t_1,..., t_n), \\psi(s_1,..., s_n)),$ (11)\nwhere $\\psi$ is a relational potential function that measures relational similarity between given n-tuple of teacher and student models, and l is a loss that penalizes structural difference based on that.\nTo efficiently transfer the relational knowledge, we here introduce relational loss in three orders, enforcing structural similarity in instance-wise, pair-wise and triplet-wise fashion, respectively. Instance-Wise Relational Loss Following the vanilla setting, we enforce instance-wise similarity via punishing the difference\n$L_i = \\sum_{Y_i \\in Y,X_i \\in X} ||t_i - s_i||_1,$ (12)\nwhere $|| ||_1$ loss is chosen instead of $|| ||_2$ loss because it deals better with abnormal points. In this task, besides the gap between teacher and student domain, there also exists great variance within the student domain. Despite better convergence and more robustness, $|| ||_2$ loss would bring unwanted smoothness.\nPair-Wise Relational Loss Recent several works have used pair-wise relational distillation loss in tasks such as image classification [30], image retrieval [59] and semantic segmentation [31]. It is used to transfer pair-wise relations, especially pair-wise similarities in our approach, among instances. We formulate the pair-wise relational knowledge distillation loss as follows:\n$L_p = \\sum_{\\begin{smallmatrix} (Y_i, y_j) \\in Y^2, \\\\ (x_i,j) \\in X^2 \\end{smallmatrix}} l_s (\\psi_p(t_i, t_j), \\psi_p (s_i, s_j)),$ (13)\nwhere $l_s$ is Huber loss, and $ \\psi_p(t_i, t_j) = 1 - ||t_i \u2013 t_j||_2$ is the pair-wise potential function which measures the Euclidean distance between the two instances $t_i$ and $t_j$ in a mini-batch space. \u03bc ="}, {"title": "$\\frac{1}{\\sum_{(y_i, y_j) \\in x^2} ||t_i \u2013 t_j||_2}$ is a normalization factor, which enables relational structures transferring disregarding the difference in space dimensions between source and task field.", "content": "Triplet-Wise Relational Loss The structure within a triplet could provide more strict regularization than that of a pair. Inspired by this, [40] propose a triplet-wise relational distillation loss:\n$L_t = \\sum_{\\begin{smallmatrix} (Y_i, Y_jY_k) \\in Y^3, \\\\ (X_i,x_j,x_k) \\in X \\end{smallmatrix}} l_s (\\psi_t(t_i, t_j, t_k), \\psi_t (s_i, s_j, s_k)),$ (14)\nwhere $l_s$ is Huber loss, and the corresponding triplet-wise potential function which measures the angle formed by the three instances $t_i, t_j$ and $t_k$ in a mini-batch space is formulated as:\n$ \\psi_t (t_i, t_j, t_k) = <\\frac{t_i-t_j}{||t_i - t_j||_2}, \\frac{t_k-t_j}{||t_k - t_j ||_2}>.$ (15)\nThe triplet-wise relational loss transfers relationships of instance embedding by penalizing angular differences. Compared with the pair-wise potential function, the triplet-wise potential function measures structural similarity in a higher-order space, enabling more effective relational knowledge transferring.\nTotal Loss The total loss is therefore formulated as:\n$L = L_{CE} + \\lambda_iL_i + \\lambda_pL_p + \\lambda_tL_t,$ (16)\nwhere $L_{CE}$ is the Cross-Entropy loss between outputs of the teacher and student network, as defined in Eq. 10 when l adopts Cross-Entropy. $L_i, L_p$ and $L_t$ are various orders of relational distillation loss defined in Eq. 12, Eq. 13 and Eq. 14, respectively. The $\u03bb_i, \u03bb_p$ and $\u03bb_t$ are weighting hyper-parameters to balance the loss terms."}, {"title": "3.4 Implementation Details", "content": "We build the de-occlusion module with a generative inpainting network using the same architecture as in [57]. In the distillation module, we employ a pre-trained VGGFace2 [3] model as the teacher. It achieves a very high accuracy of 99.53% on the LFW dataset [17] after alignment. The student network is composed of a ResNet-18 [13] model with a single embedding layer on top.\nOur end-to-end network is implemented based on the deep learning library Pytorch. In the experiments, we set $\u03bb_p = 1.0, \u03bb_t = 2.0$. All models were trained with a mini-batch size of 128. The initial learning rate is lr = 0.1 and decreases to 0.1 times every 24 epochs. When sampling tuples of instances for the relational losses, we simply use all the tuples (pairs or triplets) in the given mini-batch."}, {"title": "4 EXPERIMENTS", "content": "In this section, the proposed de-occlusion distillation framework is systemically evaluated on both synthesized and realistic masked face datasets. We first introduce the experiment setting, then present experimental results on two datasets, finally we conduct ablation studies and discuss the function paradigm of the proposed method.\n4.1 Experiment Setting\nDatasets Our experiments are carried out on three datasets: Celeb-A dataset [32], LFW dataset [17] and AR dataset [35].\nThe Celeb-A dataset consists of 202,599 face images covering 10,177 subjects. Each face image is cropped, roughly aligned by the position of two eyes, nose, and two mouth corners, and rescaled to"}, {"title": "4.2 Results on Synthetic Masked Faces", "content": "In this subsection, we compare the recognition performance for synthetic masked faces of different models. We trained our end-to-end de-occlusion model on Celeb-A, then evaluate comparison accuracy on the LFW dataset. All models extract features of all 6000 face pairs and then computes the cosine similarities between the face pairs. The accuracy is the percentage of correct prediction, where the threshold is decided as the one with the highest accuracy.\nWe trained our network with total loss taking the form as Eq. 16. Several sotas are also presented and compared. GFC [26], GA [57] and IDGAN [9] are all state-of-the-art generative inpainting methods, especially IDGAN is designed and optimized for masked face recognition problem. We equip them with five high-performance"}, {"title": "4.3 Results on Realistic Masked Faces", "content": "We then evaluate the proposed method on the AR dataset, where two variations of occlusions are available. For testing, the masked faces were divided into two subsets, denoted as AR1 and AR2, consisting of faces with sunglasses and scarfs, respectively.\nWe adopted four classic face recognition algorithms and four deep learning recognition models to test the recognition performances on the masked faces and the completed faces. Specifically, the four recognition algorithms are: 1) PCA [51], the typical statistic-based recognition algorithm; 2) Gabor wavelet-based recognition (GW+PCA) [24], using features in the transformed domain; 3) locality projection [14], a manifold-based recognition algorithm; and 4) SR [54], which is a branch of norm-based optimization. The four state-of-the-art deep learning recognition models include VGGFace [39], VGGFace2 [3], SphereFace [29] and ArcFace [7].\nWe completed faces by two tradition methods RPCA [53] and GL [8], as well as two sota generative inpainting methods GFC [26] and GA [57] respectively. All faces are then predicted by all recognizers above, as comparisons with our de-occlusion distillation model. The results are shown in Tab. 1, and our method achieves a higher recognition accuracy. The VGGFace2 and SphereFace model exhibit relatively milder degradation in masked scenarios among the baselines. We take this as evidence that data diversity and structural regularization are beneficial for model robustness."}, {"title": "4.4 Ablation Study", "content": "In this section, we conduct ablation studies to prove efficacy.\nContribution of each loss component. We trained our model with 1)Cross-Entropy(CE) loss only $L_{CE}$, 2) CE loss and pair-wise relational loss $L_{CE} + \u03bb_pL_p$, 3) CE loss, pair-wise and triplet-wise relational loss $L_{CE}+\u03bb_pL_p+\u03bb_tL_t$, 4) CE loss, pair-wise, triplet-wise, and hard instance-wise relational loss $L_{CE}+\u03bb_iL_i+\u03bb_pL_p+\u03bb_tL_t$ and 5)CE loss, pair-wise, triplet-wise, and soft instance-wise relational loss $L_{CE} +\u03bb_iL+\u03bb_pL_p + \u03bb_tL_t$. Fig. 5 shows the trend of loss during training in both the training and evaluation set. Testing accuracy on Celeb-A and LFW share similar trends, and the trend on LFW is less aligned due to the domain gap. From the figure, we noticed that CE loss can well stabilize the training process. However, with CE loss only, the model fails to reach the optima. Our relational distillation loss in various orders exhibits good collaboration with CE loss and leads to the best performance eventually.\nOUR-Hard vs OUR-Soft. It is worth to note in Fig. 5 that OUR-Hard trained with $L_{CE} + \u03bb_iL_i + \u03bb_pL_p + \u03bb_tL_t$ shows apparent over-fitting in later stage, even worse than those trained without instance-wise loss. With a large reduction in parameters and change on model structure, it's reasonable to suspect the student bear considerable instability, especially under perturbation settings like ours. Directly enforcing the features to be exactly the same could be too strict. After we soften the instance-wise loss into identity-centered ensemble loss L, as defined in Eq. 17, the resulting OUR-Soft shows more efficient convergence and finally reach the lowest loss. We believe this discovery is meaningful for more general adaptation tasks, especially with great domain gaps or severe perturbation.\nChoice of completion methods. We then ask whether the choice of completion methods make a difference. We replace the face completion model in the de-occlusion module with 1)GFC [26], 2)GA [57] without contextual attention and 3)IDGAN [9], in comparison with the adopted 4)GA [57] with contextual attention. We trained model with Hard ($L_{CE} + \u03bb_iL_i + \u03bb_pL_p + \u03bb_tL_t$) and Soft"}, {"title": "loss ($L_{CE} + \u03bb_iL + \u03bb_pL_p + \u03bb_tL_t$) and Tab", "content": "2 shows the results on LFW. Bold and underline denote the first and second highest in each column, separately. All models trained with Soft loss perform better than those with Hard loss, which verifies the efficacy of the softening mechanism. Besides, it's evident that the performance of GA [57] with attention stands out alone, while the others are close. We attribute it to the contextual attention mechanism which allows the model to focus on the most relevant and informative areas.\nDistillation vs Fine-tune. Finally, to prove the efficacy of the distillation module, we fine-tune the Arcface model [7], which has a similar size with our student model, on completed faces and make comparisons. The evaluation accuracy on the LFW dataset is reported in the first row of Tab. 2. Our OUR-Soft model surpasses the fine-tuned ArcFace model by 3.27%, which suggest that fine-tuning can hardly come across the semantic gap. Our models instead, learn to recover the identity features under the guidance of pre-trained recognizer, and effectively improve the masked face recognition."}, {"title": "5 CONCLUSION", "content": "Masked face recognition is a problem of wide prospects for applications. Despite great efforts and advancements made over the years, current methods are restrained by incomplete visual content and insufficient identity cues. In this work, we migrate the mechanism of amodal perception and propose a novel de-occlusion distillation framework for efficient masked face recognition. The model first recovers appearance information via a generative face completion based de-occlusion module, and then transfers rich structural knowledge from a high-performance pre-trained general recognizor to train a student model. In this way, the student model learns to recover the missing information both in appearance space and in identity space. By representing knowledge of existing high-performance recognition models with structural relations in various orders, the model is enforced to extract representations with similar aggregation behaviors with those of the teacher. Experimental results show that the amodal completion mechanism is also beneficial for deep neural networks, and our proposed de-occlusion distillation can deal with the masked face recognition task on both synthetic and realistic datasets. In the future, we will work on the establishment of amodal perception for computer vision and further investigation on suitable network architecture."}]}