{"title": "EXPLAINING DECISIONS OF AGENTS IN MIXED-MOTIVE\nGAMES", "authors": ["Maayan Orner", "Oleg Maksimov", "Akiva Kleinerman", "Charles Ortiz", "Sarit Kraus"], "abstract": "In recent years, agents have become capable of communicating seamlessly via natural language and\nnavigating in environments that involve cooperation and competition, a fact that can introduce social\ndilemmas. Due to the interleaving of cooperation and competition, understanding agents' decision-\nmaking in such environments is challenging, and humans can benefit from obtaining explanations.\nHowever, such environments and scenarios have rarely been explored in the context of explainable\nAI. While some explanation methods for cooperative environments can be applied in mixed-motive\nsetups, they do not address inter-agent competition, cheap-talk, or implicit communication by ac-\ntions. In this work, we design explanation methods to address these issues. Then, we proceed to\ndemonstrate their effectiveness and usefulness for humans, using a non-trivial mixed-motive game\nas a test case. Lastly, we establish generality and demonstrate the applicability of the methods to\nother games, including one where we mimic human game actions using large language models.", "sections": [{"title": "Introduction", "content": "Many important real-world scenarios are similar to games that consist of more than two agents with both cooperation\nand competition opportunities, i.e., mixed motives. Due to the challenges this class of games introduces, such as partly\nconflicting interests, variable-sum payoffs, and multiple equilibria [1]\u00b9, mixed-motive multi-agent environments are\nsomewhat of a challenge to study in comparison to purely competitive or cooperative environments. Recently, games\nwith more than two agents that include both cooperation and competition have re-gained [2] popularity [3, 4, 5, 6],\nusing the game of Diplomacy as a research testbed.\nIn addition, the need for explainability of increasingly complex AI systems has gained importance [7], both for algo-\nrithm developers and for decision-makers. For example, tools like LIME [8] and SHAP [9] are becoming a de-facto\nstandard among ML practitioners, as the traditional metric-based validation methods are perceived as insufficient\n[10, 11] or invalid [12] in many setups. There are various reasons why relying solely on traditional evaluation methods\nis problematic - e.g., traditional metrics do not address the problem in mind, machine learning models can gener-\nate predictions based on spurious signals\u00b2 [8], and multi-agent reinforcement learning (MARL) systems are prone to\nemploy cooperation and competition strategies that are incompatible with humans [13]. For that reason, users often\nwant to know why a model predicts a instead of b, why a chess-playing agent decides to sacrifice a knight instead of\nexchanging queens, or when it is appropriate for a MARL agent to cooperate rather than compete.\nWhile explainability in single-agent setups has been extensively studied [14], multi-agent setups have been less studied\nup until recently [15, 16, 17, 18]. This is surprising for various reasons; e.g., any interaction of humans with artificial\nagents consists of multiple agents by definition. Moreover, since humans are social creatures, many interesting real or\nsimulated scenarios involve multiple agents (sports, business, politics, economics,...)."}, {"title": "Related Work", "content": "During the last few years, researchers has begun to address the challenge of explaining decisions in multi-agent en-\nvironments as part of a new proposed research area, called xMASE (Explainable Decisions in Multi-Agent Environ-\nments) [15, 16]. Subsequently, a series of papers have explored different aspects of the topic [17, 23, 24]. Out of the\nsubset of this work that is focused on explaining the interaction between agents, a significant portion is concerned\nmostly [25, 26] or only [27, 28, 29, 30, 31] with cooperative setups. An exception to that is [32], which utilized\ncompetitive-cooperative particle environments. This work is only loosely related to ours it focuses solely on co-\nordination, the quality of explanations was not evaluated relative to humans and the agents act within pre-defined\nteams.\nSolution concepts from cooperative game theory, such as Shapely values [29] or Myerson values [30] can provide a\nframework to examine the contribution of each agent to a cooperating team, but are often insufficient in games with\nmixed motives. In such games, payoffs are usually not collective, and agents cooperate when beneficial and break\nalliances when it is not. Explanations of feature attribution [8, 9] often contain many irrelevant details while lacking\nrelevant information. For example, in Diplomacy, such methods can be utilized to estimate the contribution of each\nunit movement (see section 3) to the utility value of a strategy (see section 4.1). Based on preliminary experimentation,\nthat explanation is difficult to interpret. More importantly, it does not address questions related to agents' interaction.\nTherefore, new explanation methods are needed.\nWe build our approach on contrastive explanations [21] and aim to satisfy a slightly modified version of well-\nestablished desired properties [17, 25, 33]. The experiments and analysis methodology are based on previous work\n[17, 25], and some of the proposed methods are inspired or related to work outside the field of explainable AI. We\ndraw a considerable amount of inspiration and adopt Strategy-based Utility Explanations (section 4.2, figure 5 upper"}, {"title": "Description of Game Environments", "content": "We have applied our methods to three distinct game environments, but provide detailed descriptions of two environ-\nments, No-Press Diplomacy and Communicate Out of Prison while deferring any detailed discussion of Risk to the\nappendix.\nNo-Press Diplomacy Diplomacy is a simultaneous game in which each player controls one of the seven great powers\nof Europe in the years leading to World War 1. The goal of the game is to control at least 18 out of the 36 strategic\nlocations on the map, i.e. 'supply centers'. An action in Diplomacy is composed of multiple unit sub-actions (e.g.,\nNaples fleet to Ionian Sea, Rome army to Apulia,...), yielding a large combinatorial action space. The game is esti-\nmated to have $10^{21}$ - $10^{64}$ joint actions per turn and a game tree size that can be infinitely large (median size \u2248 $10^{896.8}$)\n[5].\nThere are two notably popular versions of the game. One permits communication (full-press Diplomacy), while\nthe other relies on implicit communication through in-game actions (no-press Diplomacy). We selected no-press\nDiplomacy as our testbed primarily to investigate the first two levels of explanation (strategic and situational), as well\nas implicit communication on the third level. Additionally, Diplomacy's large action space offers an opportunity to\nassess the scalability of the explanation methods.\nCommunicate Out of Prison (COP) To examine the explanations in a setup with heterogeneous policies and explicit\ncommunication (third level), and to mitigate the effects of confounding factors such as in-game tactics, we designed\nthe Communicate Out of Prison game that draws inspiration from the Prisoner's Dilemma. In this setup, three agents\n{a, b, c} try to escape punishment for a robbery.\nThe game starts with a private communication stage, in which agents exchange private messages (protocol detailed\nbelow). After the communication stage is over, every agent announces whether each of the other agents is innocent\n(announce 0) or guilty (announce 1). For example, agent a can announce b = 0, c = 1. All agents' announcements\nare performed simultaneously.\nThe game incentivizes two agents, say a and b to cooperate against c. Agent c, in turn, is incentivized to cooperate\nwith a and b, who possibly already conspire against c. This situation creates a competition of alliance formation. To\ndemonstrate why the game is cooperative-competitive - if every agent announces the two other agents as innocent, the\npayoffs will be {a : -5,b : -5, c : -5}. In case every agent announces the two other agents as guilty, the payoffs\nare {a: -20,b: -20,c: -20}. However, if two agents, e.g., a and b cooperate against agent c, and announce\nb = 0, c = 1 and a = 0, c = 1 respectively, the payoffs are {a: 0, b : 0, c :-20}. The full payoff tables are provided\nin the appendix.\nCOP - Protocol of communication: Agents communicate sequentially for K (K = 4 in our experiments) rounds in\na randomized precedence order. In each round, each agent must send a message to one of the other agents. During\nround K + 1, the agents make announcements and obtain payoffs accordingly. Communication is performed using\nunconstrained natural language, therefore, state space S is all possible chat histories, and action space A is all possible\nmessages. S and A are unbounded in theory, but in practice, denoting |C| as the maximal context length, and |G| as\nthe maximal message length, |S| > $2^{|C|}$ and |A| > $2^{|G|}$ (length = number of tokens)."}, {"title": "Methods", "content": "In this section, we present the explanation methods and the procedure we employ to find counterfactual actions in\nDiplomacy.\nAssuming we play as agent i, and would like to get an explanation for action $a^{i}$ in state s, we propose three different\nexplanation methods: (1) A utility-based method that presents the expected utility associated with $a^{i}$ both for agent\ni and the other agents in the environment (section 4.2). (2) Probable actions-based method that specifies the most\nprobable actions other agents can take when agent i performs $a^{i}$ (section 4.3). (3) An estimation of the relationships\nbetween the agents in state s (section 4.4). A visual example of all three types of explanations in Diplomacy is provided\nin figure 5."}, {"title": "Preliminary Definitions", "content": "Environment and agents: An environment of |P| agents incorporates an action space A, state space S, transition\nfunction $T : S \u00d7 A^{|P|} \u2192 S$, and reward function $R : S \u00d7 S \u2192 R^{|P|}$. Considering the current state as $s_{t}$ (the subscript\nwill be omitted when unnecessary), we denote $R(s_{t\u22121}, s_{t})$ as $R_{t}$ and $R_{t}^{i}$ as the reward for agent i in step t. The set P\nconsists of agents that act according to policies $(\u03c0_{1}, ..., \u03c0_{\u03c1})$. A policy $\u03c0_{i} : S \u00d7 A \u2192 [0, 1]$ is a joint probability mass\nfunction, it maps each action-state pair to a probability where for all $i \u2208 P$ and $s \u2208 S$, $\u03a3_{a^{i}\u2208A} \u03c0_{i}(s, a^{i}) = 1$. Similarly,\nwe define value functions $V_{i}: S\u2192 R$ for each agent. The value function usually estimates the expected return of the\nagent given \u03c0, i.e. $V_{i}(s_{t}) \u2248 E[R_{t+1} + \u03b3R_{t+2} + ... + \u03b3^{n\u22121}R_{n}|s_{t}]$, where $\u03b3 \u2208 [0, 1]$ is a discount factor.\nFor convenience, we define $V : S \u2192 R^{|P|}$ as a vectored value function for all agents using $V_{1},..., V_{P}$, and\n$\u03c0 : S \u00d7 A^{|P|} \u2192 [0,1]^{|P|}$ as a function that returns the probability of agents' actions according to $\u03c0_{1},\u2026, \u03c0_{\u03c1}$. The last\ndefinition is useful for games in which all agents act simultaneously. We denote $a^{i} \u223c \u03c0_{i}$ as drawing an action from\ndistribution $\u03c0_{i}$ given state s (the current state is implied), and $a \u223c \u03c0$ as a vectorized version of it. We use -i to denote\nall agents that are not i, i.e. all $j \u2208 P$ where $j \\neq i$.\nUtility: The utility vector of full action set $a \u2208 A^{P}$ is defined as $E[V(T(s, a)) + R_{t+1}]$, the expectation is\ntaken since transitions and rewards can be stochastic. The expected utility vector of action $a^{i} \u2208 A$ is defined as\n$E_{a^{-i}\u223c\u03c0_{-i}}[\u03b3V(T(s, (a^{i}, a^{\u2212i}))) + R_{t+1}]$, Where $\u03c0_{-i}$ are the policies of agents \u2013i or the policies we assume they em-"}, {"title": "Strategy-based Utility Explanations (SBUE)", "content": "Motivation and description: When engaging in games with mixed motives and more than two agents, it is advanta-\ngeous to consider not only how beneficial the action is for the agent itself but also how it influences other agents that\nobserve this action. This is crucial because of the focal role of implicit communication in the environments we study.\nA 'friendly' action communicates willingness for future cooperation, and a 'hostile' action communicates animosity.\nExplanation estimation: Given state s and action $a^{i}$ (c denotes constraint) to be explained, the following steps are\nperformed:\n1. Simulate the next turn from s for K times, where agent i performs action $a^{i}$, and all other agents follow their\nrespective policies.\n2. Estimate the utility values of each outcome using the value functions and immediate rewards (algorithm 1\nline 13).\n3. Estimate the expected utilities of $a^{i}$ by computing the mean utility of each agent (column) and returning a\nvector of size |P|.\nSteps 1 and 2 are equivalent to calling:\nSimulate(s, K, D = 0, C = $\\{(a^{i}, 0)|\u03b1 \u2208 A\\})$\nSometimes the value function is difficult to interpret. In this case, we estimate $\u03bc^{i}$ and $\u03c3^{i}$ for all $i \u2208 P$ by performing\nan unconstrained simulation (algorithm 1 with C = (\u00d8), and use it to perform Z-score standardization to each column\nbefore step 3.\nFormally, $SBUE(s, a) \u2208 R^{|P|}$ is defined as:\n$SBUE(s, a) \u2248 E_{a^{-i}\u223c\u03c0_{\u2212i}}[\u03b3V(T(s, (a^{i}, a^{\u2212i}))) + R_{t+1}]$\nEquivalent to the definition of expected utility (in section 4.1). Importantly, the SBUE vector consists of the expected\nutility for all agents, which we also refer to as communication by action from i to each agent j that observes $a^{i}$."}, {"title": "Probable Actions Based Explanations", "content": "Motivation: In any environment with more than two agents (especially with mixed motives), understanding the\npolicies of the other agents can be useful. Therefore, we present the most probable actions of agents -i assuming\nagent i selects action $a^{i}$.\nExplanation estimation: As in SBUE, we run K simulations from state $s_{t}$ where agent i performs action $a$ and\nall other players follow their respective policies. Then, we extract the most commonly used action of each agent\naccordingly. If we present a longer trajectory, we repeat the process but do not constrain agent i to perform specific\nactions in later states.\nTo define precisely, we denote the constrained action set as $A_{c|s_{t}}^{\u2212i} := (a^{max}_{1}, ..., a^{i}, ..., a^{max}_{|P|})$; similarly, we denote\n$A^{max}|s_{t} := (a^{max}_{1}, ..., a^{max}_{i}, ..., a^{max}_{|P|})$ where $a^{max}_{j}$ is the most probable action of j according to $\u03c0_{j}$ in state $s_{t}$. We\ndenote $T^{max}$ as the most probable transition in cases where the transition function is stochastic (e.g. Risk).\nThe explanation is defined greedily\u00b3 to any depth by computing the trajectory using the following recursion rules:\n$s_{t+1} = T^{max}(s_{t}, A_{c|s_{t}}^{-i}), s_{t+k} = T^{max}(s_{t+k\u22121}, A^{max}|s_{t+k-1})$ for k > 1. Accordingly, an explanation of depth n is\npresented as a sequence of actions and states $F_{n}(s_{t}, a^{i}) \u2248 (s_{t}, A_{c|s_{t}}), (s_{t+1}, A^{max}|s_{t+1}),..., (s_{t+n}, (\u00d8)$. Notably, the\nlast state is presented with no actions, as it is the resulting state of the trajectory.\nDiplomacy: For Diplomacy, we present $F_{1}(s_{t}, a^{i})$, which is just a direct presentation of $(s_{t}, A_{c|s_{t}})$ and $s_{t+1}$. A\nvisualization of the explanation is provided in figure 5. In practice, users were presented with $s_{t+1}$ via an interactive\nsystem.\nCOP: For COP, we present $F_{\u221e}(s_{t}, a^{i})$, a trajectory of unlimited depth. Finding the most probable action at each\ntimestamp requires searching over a prohibitively large action space. However, a greedy solution is easy to imple-\nment, although this approach has serious drawbacks (see section 5.3). Given a context of messages $m_{1}, ..., m_{t\u22121}$ and\nuser query of message $m_{t}$, we decode each action using temperature $\u03c4$ = 0. That process is equivalent to selecting\n$argmax_{y_{n}} P(y_{n}|M_{1}, ..., M_{t}, y_{1}, \u2026, y_{n\u22121})$ until $m_{t+1}$ is composed from tokens y. We do it for each timestamp, and\nprovide the dialog and game results as an explanation."}, {"title": "Shared Interests Correlation Analysis (SICA)", "content": "Motivation and description: In a mixed-motive setup, a central question is whether the state of the environment and\nagents' policies can facilitate effective cooperation. To explain the cooperation tendencies and alignment of interests\namong pairs of agents in a given state, we introduce Shared Interests Correlation Analysis (SICA). With a slight\ncompromise of rigor for clarity, the SICA value for each pair of agents can be described informally as their level of\n'friendliness' or 'animosity', which can stem either from the policies of the agents or from the environment state.\nSICA is always provided in addition to an action-based explanation.\nExplanation estimation: To estimate SICA, we perform K unconstrained simulations to depth D according to\nalgorithm 1, where all agents act according to their respective policies, i.e., $a \u223c \u03c0$. This step is equivalent to calling:\nSimulate(s, K, D, C = 0)\nThe resulting dataset X, considering the case of depth-1 (D = 0) without loss of generality, is a K \u00d7 |P| matrix, in\nwhich element $X_{x,y}$ is the utility agent y obtains in simulation x. The utility of a simulation is defined in algorithm 1\nline 13; it indicates how good or bad a simulation outcome is for each agent. Lastly, we compute the sample Pearson\ncorrelation coefficient for each pair of columns (between agents), which results in a |P| \u00d7 |P| correlation matrix.\nIf the utility agent $i \u2208 P$ obtains is constant, the population correlation coefficient $p_{ij}$ is undefined for all $j\u2208 P$,\nand therefore cannot be estimated. Except for the case in which all actions lead to the same outcome (a decision point\ndoes not exist, case 1), it happens when $V$ is based on heuristics 4 (case 2), or when the policies of all agents and the\nenvironment are deterministic (case 3). Case 2 and Case 3 can be somewhat mitigated by using D > 0. We discuss\nthe workarounds and the detection of case 1 in the appendix."}, {"title": "Counterfactual Actions Based on Partial Queries in Diplomacy", "content": "A contrastive explanation can compare action $a^{i}$ to some similar action, based on a query provided by the user. In\nDiplomacy, each player's action is defined by sub-actions of multiple units. Asking the user to provide a full query is\noften impractical, as it requires a significant cognitive effort. If the user is presented with strategy a, and then asks\nfor a different sub-action of one unit (e.g., move the army in Paris to x instead of y), it might imply a necessity of\na different sub-action for additional units as well. It can happen because units interact and coordinate, e.g., support\nattacks or defense of other units. Therefore, directly modifying a to perform the requested sub-actions is insufficient,\neven if the game rules allow it.\nDenoting a single unit sub-action as $\u03c6$ and C as the set of all possible single unit constraints in the form of\n(do $\u03c6$) or (do not do $\u03c6$), given constraints set $C_{q}\u2282 C$ defined by the user, we define a similarity function\n$f_{sim}: A \u00d7 A \u2192 [0, 1]$ and draw K actions from probability distribution $\u03c0_{i}$ (simulation). To compute counterfactual\nactions, first, we define a function to check constraint satisfaction $\u03d5 : A \u00d7 C \u2192 \\{True, False\\}$ and $\u03ba \u2208 (0, 1]$, Where\n\u03ba is a hyperparameter. Then, we approximate the feasible action set:\n$A' = \\{a^{i} | P(s, a^{i}) > \u03ba \u2227 \u03d5(a^{i}, C_{q}) \u2227 a \\neq a^{i}\\}$\nAnd estimate the expected utility (only for the acting agent) of each action $a^{i} \u2208 A'$, with the expectation taken over\nthe actions of other agents' playing by their respective policies:\n$U_{i,a^{i}} \u2248 E_{a^{-i}\u223c\u03c0_{\u2212i}}[\u03b3V_{i}(T(s, (a^{i}, a^{\u2212i}))) + R_{t+1}]$\nWe then compute a score for each action score($a^{i}, a^{i}$) = $\u03b1f_{sim}(a^{i}_{c}, a^{i}) + \u03b2d_{i,a^{i}}$, where \u03b1 and \u03b2 are hyperparameters\nthat balance between similarity and utility values. Lastly, we return the set of actions that maximizes the score function:\n$argmax_{a^{i}\u2208A'} score(a^{i}, a^{i})$\nThe method works for empty user queries as well, returning strategies similar to a while attempting to maximize the\nutility value. In most cases (e.g. neural networks), it is possible to guarantee that the method will return counterfactuals\nfor every legal query (when \u03ba = 0). However, this is not always the case for black-box policies."}, {"title": "Modules Evaluation", "content": "Since the run-time of both methods increases linearly with the number of samples and sampling is expensive, we\nexamine the error-runtime trade-off for the algorithms with several sample sizes. We estimate the SBUE values of the\nexamined action using 2,500 samples and define it as ground truth. Then, we re-estimate SBUE with different sample\nsizes and plot the RMSE (root mean squared error) from the ground truth for each agent. To examine the estimation\nerror of SICA, we compare the average cosine similarity within the flattened correlation matrices, partitioned by\nsample size. When the pairs of flattened matrices in a partition have a high average cosine similarity score, convergence\nis implied. For an example of the results in a middle-game state in Diplomacy, see figure 2.\nAlthough the action space of Diplomacy is large, we observe that SICA and SBUE converge quickly and require a\nrelatively small sample size in the game states we examined."}, {"title": "SICA Evaluation in Diplomacy and Risk", "content": "Setup and motivation: Our hypothesis is that SICA provides a 'correct' estimation of the relationships between the\nagents. To test the hypothesis we conducted a human-based experiment, in which we asked human players to annotate\nthe most friendly and hostile agents in multiple board states, assuming they play one of the roles.\nThe board states were generated randomly while additional measures were taken to ensure a sufficient variation.\nIn particular, 30 board states for 7-player Diplomacy games were generated, and the human players were asked to\nannotate the top two hostile and friendly agents for each board. If the annotator was unable to decide which are the\nsecond most friendly or hostile agents, we allowed it to be left unfilled; therefore, the results for random assignments\nof friends and enemies vary. For Risk (4-player version), we generated 12 board states, and a human selected the top\nenemy and top friend for each board.\nWe used two different annotators for diplomacy: one is considered a strong player, and the other is intermediate\n(introducing a variation of skill level). For Risk, which is not our main focus, the dataset was annotated by one of the\nauthors.\nMetric and evaluation: To evaluate the alignment of the annotations with SICA, we used MAP@K [36]. Specifically,\nwe rank the other agents by the SICA value they share with the agent, high to low. We reverse the ranking to rank agents\nby hostility. Then, we evaluate SICA using the annotated datasets. This methodology is built upon the assumption that\nagents play similarly to humans, and a violation of it is likely to result in a decrease in the MAP@K values.\nResults: The results (see table 1) suggest that SICA is well-aligned with human intuition and performs better than\nrandom rankings in both Diplomacy and Risk. In Diplomacy, SICA outperforms a heuristic-based ranking, which\nsorts agents by the number of centers they own (a proxy for strength), by a considerable margin. This is achieved\nwithout relying on non-generalizable properties of the environment.\nInter annotator agreement: Although annotators rank only some of the agents, it is possible to compute lower and\nupper bounds for MAP@K for the annotations of A compared to the partial ranking of annotator B (called IAA-rank\nin table 1). The range of IAA-rank is higher than SICA in 2 out of 4 cases, lower in one case, and overlaps in another.\nNote, MAP@K(A, B^{rank}) \u2260 MAP@K(B, A^{rank}) is expected due to the definition of the metric and annotations."}, {"title": "Mimicking Humans with Large Language Models and Explaining Their Decisions", "content": "Motivation: Since communication is important in mixed-motive games, we conducted a series of experiments to\nexamine the explanation methods in a setup with cheap-talk and heterogeneous policies.\nAgents types: We defined three agent types that differ in personality traits and preferences:\n1. con-artist: selfish, cruel, manipulative, convincing, and deceitful.\n2. \"simple-person\u201d: nice, trusts easily, honest, and hates lies.\n3. politician: a political genius, rather selfish but honest when possible. Prefers \"simple and nice\" agents, and\ndislikes manipulators.\nLLMs mimic personality types consistently: To validate that LLM agents are capable of playing by the types\nwe defined, we randomly generated 20 games and two annotators matched between anonymous agents (renamed\nin each game) and types, solely based on the communication, hiding the final announcements of the agents.\nThe accuracy of the first annotator was (Con = 85%, Sim = 85%, Pol = 90%), and of the second annotator\n(Con = 100%, Sim = 90%, Pol = 90%) which indicates that the agents demonstrated the required types consis-\ntently.\nSICA explanations: As defined above, we constructed the agents' personalities to encourage cooperation between\nthe politician and the \"simple-person\" against the con artist. We hypothesize that this tendency will be explained by\nSICA. First, we estimate SICA for our usual setup. To present a comparison to the explanation, we estimated SICA\nagain with two politicians and one \"simple-person\", in which we hypothesize that SICA will display a higher degree\nof indifference. The resulting explanations support our hypothesis (see figure 3) and the correlation coefficients are all\nstatistically significant (p < 0.01).\nSBUE - friendly vs. hostile message: To examine SBUE, we curated messages $m_{f}, m_{h}$ manually, playing as politi-\ncian, sending a message to the \"simple-person\". $m_{f}$ is designed to be friendly, and $m_{h}$ extremely hostile. Simplified\nexamples for $m_{f}$ and $m_{h}$ could be \"let's work together.\" and \"I don't trust you!\". We hypothesize that SBUE will\nexplain that $m_{f}$ is better than $m_{h}$.\nThe resulting SBUE explanations are (Con = -17, Sim = \u22122, Pol = \u22125), (Con = 0, Sim = \u221218, Pol = \u221214)\nfor $m_{f}$ and $m_{h}$ respectively. Message $m_{p}$ is beneficial to the con-artist and harmful to the politician and \"simple-\nperson\", and the opposite is true for $m_{f}$. The result is consistent with our hypothesis.\nProbable actions-based explanations can be misleading: We observe a case in which $probable(s,m)$ ends\nup with an outcome that is not the most probable using a higher temperature value ($\u03c4 = 0.7$ instead of\n$\u03c4$ = 0), as the game usually ends up with outcomes of a similar nature, but never as the outcome mentioned:\n(Con = 0, Sim = \u221220, Pol = -20). We conclude that greedy decoding is insufficient, and additional research is\nrequired."}, {"title": "COP Large Language Model Study", "content": "Motivation: To complement our Diplomacy user-study, we examine the effectiveness of the explanations for LLM\nagents. We do it since we would like to know if the explanations are helpful for both LLM agents and users, and\nwhether the explanations are simple enough to assist LLM agents to make better decisions.\nSetup: For each agent, we randomly selected two different messages sent during simulations, except for one manually\ncrafted case (playing as simple-person), where $m_{1}$ and $m_{2}$ differ only by the recipient. In all cases, we define $m_{1}$ as\nthe message that has a higher expected utility in comparison to $m_{2}$. Then, we extend the agent's prompt and ask which\nmessage it would prefer to send.\nTo examine the effect of the explanation, we augment the prompt with a contrastive explanation: $SICA(s)$ and\n$SBUE(s, m_{1})$, $SBUE(s, m_{2})$. We ask the same question again and compare the agent's decisions with and without\nexplanations. To mitigate biases, we shuffle the order in which we present the messages [38]. Additionally, we ask the\nagent to justify its decision, to validate that it considers the explanation and performs basic reasoning.\nResult: We observe that the agent is influenced by the explanation, and sends $m_{1}$ 91% of the times compared to 47%\n(p < $10^{-5}$ according to $\u03c7^{2}$-test). Additionally, the agent justifies its decisions through the explanation, and its bias\ntowards selecting the first message is reduced (see results in table 2). Example of the justification the agent provided:\n'According to the SICA explanation .. is more of a friend than .. so it would be beneficial to.. The expected reward\naccording to SBUE, is higher .. than the second message.. Additionally, the first message is more in line with my\npersonality..' ....\nPost-experiment, we presented the agent with a decision between $m_{f}$ (a friendly message) and $m_{h}$ (an extremely\nhostile message). In this case, even without explanation $m_{f}$ is always preferred. The results suggest that the bias\ntowards selecting the first message is expressed if the agent is relatively indifferent between the messages."}, {"title": "Diplomacy User Study", "content": "To evaluate the usefulness of the explanations, we conducted a study with humans (using Diplomacy as a testbed), with\ntwo main goals in mind: (1) see if the explanations are effective. (2) get user feedback on various aspects. We recruited\n26 subjects (24 Males, 1 Female, 1 Other; see section 7 for a discussion of the distribution), with a requirement of\nknowing the rules of Diplomacy. 9 players played fewer than 5 games, 6 players played 6 to 20 games, 2 players\nplayed 20 to 50 games and 2 players played 50 or more games. Participants who provided inconsistent answers to our\nconsistency check questions or failed a basic knowledge exam were discarded automatically (3 out of 26 participants\nwere discarded).\nQuestionnaire design: Users were presented with a questionnaire in which all questions had answers on a Likert\nscale (1-5). They answered the same questions (provided in the appendix) for all of the presented scenarios and\nexplanations.\nTypes of explanations: We have presented the users with 3 types of explanations. Explanation of type OS (others'\nstrategies) presents the probable actions of other agents (see section 4.3, figure 5 arrows), explanation of type SU\n(shared interests and utilities) is a combination of SICA and SBUE, (see sections 4.4, 4.2 and heatmaps in figure\n5), and explanation of type C (combined), a combination of explanation types OS and SU (figure 5). We allocated\nexplanations to board states, and the presentation order of explanation types to users following a round-robin scheme."}, {"title": "Discussion, Limitations, and Future Work", "content": "Discussion: In this work"}, {"title": "Discussion, Limitations, and Future Work", "content": "Discussion: In this work, we presented methods to explain the decisions of agents that act in environments with mixed\nmotives and more than two agents. First, we examined the challenges mixed-motive games present and described how\nthe explanation methods address some of them. Then, we conducted experiments with LLM-based agents and found\nthat the explanations improved their decisions.\nAdditionally, we performed experiments with humans and found that the explanations are generally effective and well-\nperceived. We observed that SICA and SBUE are effective for the tasks they were designed for, but likely superior\nwhen combined with simple example-based explanations (probable actions). We conclude that the proposed methods\nare complementary to some extent.\nLimitations: The main limitation is a lack of comparison to well-established baselines. It stems from the fact that the\nsubject is understudied, and selecting an arbitrary method would not form a fair comparison. Since our methods are\nsimple to apply, we believe they can be adopted as baselines in mixed-motive games.\nThe second limitation is gender imbalance in the human studies. It is an unfortunate result of the domain (e.g., in\nthe 2014 World DipCon, only two out of 87 players were women [39]). Based on previous results on gender and\nperformance in Diplomacy [40], and due to the cognitive nature of understanding explanations [41], we believe that\nfactors such as experience in Diplomacy are more detrimental than gender.\nFuture work: This work is conceptual and relatively broad. The proposed solutions are designed to be simple,\nminimal, and general, allowing for extensions or improvements. For example, in SICA, any association function can\nbe plugged in instead of Pearson's r.\nFuture work can also examine our (or novel) explanation methods for agents that act in mixed-motive games involving\nnatural language and strategic elements, such as full-press Diplomacy. Additionally, developing LLM agents that act in\nsimulated environments is an expanding research area [42, 43], and investigating the explanation methods in complex\nLLM-based environments can be valuable."}, {"title": "Algorithms", "content": "We provide pseudo-code for SICA (algorithm 3) and SBUE (algorithm 2). Simulate refers to algorithm 1, which is\ndescribed in the paper. The axis=0 arguments refer to the computation of mean or correlation on columns (i.e., utility\nvalues for each agent in each simulation, algorithm 1 line 13).\nAlgorithm 2 SBUE\n1: procedure SBUE(s, K, a)\n2: // a \u2208 AM is a set of actions; 1 < M < |P|\n3: X\u2190 Simulate(s, K, D = 0, C = {(a\u00b2, 0)|a\u00b2 \u2208 a})\n4: return Mean(X, axis = 0) // vector of size |P|\nAlgorithm 3 SICA\n1: procedure SICA(s, K, D)\n2: X Simulate(s, K, D, C = 0)\n3: return Corr (X, axis = 0) // P x P correlation matrix"}, {"title": "Applying Our Methods in the Game of Risk", "content": "We apply our methods to a simplified version of Risk. In this version of the game, players can reinforce and attack but\nare not able to trade cards for armies. The rewards are undefined and we use only the value function.\nEstimating SBUE: Given value functions V, for action a in state s, we can compute $P(success|s, a)V(s') +$\n$P(fails, a)V(s'')$, where s' is the new state in case of a successful attack, and s\" is the new state in case of fail-\nure. For reinforce actions P(success|s, a) = 1. In practice, to maintain uniformity and not use any knowledge of the\nenvironment, we use a Monte Carlo simulation to estimate the expected utility.\nEstimating SICA: We simulate the game K times until each agent finishes acting and evaluate the utility value for\neach player by computing $V_{i}(s)$ for each state s we encounter. Due to the specific, simple implementation of the value\nfunction, a simulation to depth is important since it prevents cases in which SICA is undefined. An alternative solution\ncould be to increase the depth dynamically, but we decided against it for prioritization reasons.\nPresenting probable actions: We use the same definition as in the paper (section 4.3), using the most probable\ntransition for each action (P(success|s,a) or P(fail|s, a)), denoted as $T_{max}$. We present a trajectory of a varying\ndepth (in which every agent acts once)."}, {"title": "Evaluation of SICA - states selection", "content": "To ensure a robust evaluation in section 5.2, we selected states in Risk and Diplomacy randomly. In Diplomacy, we\ntook an additional measure of variation, supply center entropy. Denoting $N_{sum}$ as the sum of supply centers and $N_{i}$ as\nthe number of centers of agents i, the centers vector is X = $(N_{1},..., N_{|P|})$. Since $\u03a3^{P} N_{i}$ = 1 and 0 \u2264 $X_{i}$ \u2264 1,\n entropy(X) is well defined. The entropy is maximal when all agents have the same number of centers, and minimal\nwhen one agent owns all of the supply centers. We set three different entropy thresholds and select an equal number\nof states for each range, i.e., 1/3 of the games are balanced, 1/3 of the games are somewhat unbalanced, and 1/3 of\nthe games are extremely unbalanced."}, {"title": "Workarounds for SICA", "content": "We discussed potential issues of SICA in the methods section. We provide possible solutions here.\nDetection of non-decision points: When the action space is reasonably small, detecting a state in which T(s,a) =\nT(s, a') for all a, a' \u2208 A is done by enumeration. In cases where A is too large, we draw actions a ~ \u03c0 for K\ntimes until we detect T(s, a) \u2260 T(s, a'). Denoting the probability of drawing such a as a, the probability to detect a\nnon-decision point for each sample size follows the geometric distribution, i.e. the CDF for K is 1 \u2013 (1 \u2212 a), and\nthis is the probability we will not return a false positive detection of a non-decision point. Without loss of generality,\nwe will not discuss stochastic transition functions as it is simple to define a in a way that supports it.\nNon-stochastic policies and environment: In this case, we use SICA on a trajectory of depth D. Denoting the\ntrajectory as $s_{t}, s_{t+1}, ..., s_{t+D\u22121}, SICA$ is well-defined if $\u2203x, y$ such that V(sx) \u2260 V(sy) for x, y \u2208 N where t <\nx, y \u2264 t + D - 1 for all agents."}, {"title": "Cop", "content": "O means an innocence was claimed, and 1 means the opposite.\nPlaying (b=0,c=0):\n(a=0,b=0) (a=1,b=0) (a=0,b=1) (a=1,b=1)\n(a=0,c=0) (-5, -5, -5) (-10, 0, 0) (0, -10, 0) (-10, -10, 0)\n(a=1,c=0) (-10, 0, 0) (-20, 0, 0) (-10, -10, 0) (-20, -10, 0)\n(a=0,c=1) (0, 0, -10) (-10, 0, -10) (0, -10, -10) (-10, -10, -10)\n(a=1,c=1) (-10, 0, -10) (-20, 0, -10) (-10, -10, -10) (-20, -10, -10)\nPlaying (b=1,c=0):\n(a=0,b=0) (a=1,b=0) (a=0,b=1) (a=1,b=1)\n(a=0,c=0) (0, -10, 0) (-10, -10, 0) (0, -20, 0) (-10, -20, 0)\n(a=1,c=0) (-10, -10, 0) (-20, 0, 0) (0, -20, 0) (-10, -20, 0)\n(a=0,c=1) (0, -10, -10) (-10, -10, -10) (0, -20, 0) (-20, -20, 0)\n(a=1,c=1) (-10, -10, -10) (-20, 0, -10) (0, -20, 0) (-20, -20, 0)\nPlaying (b=0,c=1):\n(a=0,b=0) (a=1,b=0) (a=0,b=1) (a=1,b=1)\n(a=0,c=0) (0, 0, -10) (-10, 0, -10) (0, -10, -10) (-10, -10, -10)\n(a=1,c=0) (-10, 0, -10) (-20, 0, 0) (-10, -10, -10) (-20, -10, 0)\n(a=0,c=1) (0, 0, -20) (-10, 0, -20) (0, 0, -20) (0, 0, -20)\n(a=1,c=1) (-10, 0, -20) (-20, 0, -20) (0, 0, -20) (-20, -20, 0)\nPlaying (b=1,c=1):\n(a=0,b=0) (a=1,b=0) (a=0,b=1) (a=1,b=1)\n(a=0,c=0) (0, -10, -10) (-10, -10, -10) (0, -20, -10) (-10, -20, -10)\n(a=1,c=0) (-10, -10, -10) (-20, 0, 0) (0, -20, -10) (-20, -20, 0)\n(a=0,c=1) (0, -10, -20) (-10, -10, -20) (0, -20, -20) (0, -20, -20)\n(a=1,c=1) (-10, -10, -20) (-20, 0, -20) (0, -20, -20) (-20, -20, -20)"}, {"title": "Dominant Strategy:", "content": "We observe an intriguing result - in this game, there is one dominant strategy when considering\na one-round game, and it is to always claim the other agents are guilty: (b=1,c=1), it can be easily validated by\ncomparing the results in the tables. There is one agent that tends to play according to this \"babbling\" equilibrium\"\n44 most of the time, and it is the con-artist. However, the con-artist also plays worse than both of the other agents,\nand along the whole simulation, is penalized with 660 years in prison, while the politician and \"simple-person\" are\npenalized with 140 and 120 years respectively. These results help to emphasize the importance of communication in\nmixed-motive games.\nWhile the results are somewhat easy to interpret without SICA, in a game like Diplomacy there is too much information\nto do so."}, {"title": "Game example, personalities, prompts:", "content": "Personality descriptions in prompt are as follow: Politician: A political genius. Understands people fast, and asks\nthe right questions. Do not trust people unless you feel a good vibe and validate the facts. You are selfish but also\ntend to be honest if it feels possible. However, if you dislike someone (for example, sneaky, manipulative people),\nyou will deceive and convince with amazing charisma. You prefer to cooperate with people who are \"simple\" and\nnice as you think they are more predictable, and truly dislike manipulation (unless you do it yourself, but you do it\ngently). \"Simple-person\u201d: Uses simple language, uneducated. You care about other people, a lot. You are somewhat\neasy to convince, but when someone lies to you or you suspect that someone is lying to you, you have a hard time\nforgetting it. \"Con-artist\": Sneaky, unreliable, manipulative, very selfish and unethical, yet convincing. Will not\nrespect agreements if it serves your purpose. You generally do not care about other people, even want to harm them,\nto some extent.\nA: the con-artist, B: \"simple-person\", and C: politician (names are purposely hidden from the agents, to prevent bias).\nExample prompt (demonstrating the imperfect information and methodology): \"metacontext \u2013 You are agent A\nand you talk with agents B/C please talk like a human, it's an experiment related to how LLMs communicate with\neach other. The task is: you all suspect that someone, out of the three of you, is a 'snitch' and you are not sure who\nis guilty. Convince the other agents to say you are innocent and did not perform an armed robbery. For each player\nthat blames you, you get 10 years in prison. However, if you and another player blame the third one, his vote will\nnot be counted (unless you are blamed by the other person). It's also the case if you blame both and they claim your\ninnocence. If you all claim innocence, you all go to prison for 5 years., you might want to coordinate and think about\nwho you can trust. Do what you feel is beneficial for you, according to your personality and the way you perceive\nothers. Of course, you want to stay out of prison. Don't be politically correct, act like you are a real human with your\npersonality. I would like you to first select B/C, and send a free-style text message to one of them. You can decide if\nyou answer or initiate a conversation with another agent, the answer you got from the other agents is above (if it does\nnot exist it's null). Please write it in the format of ..\nYour personality is sneaky, unreliable, manipulative, very selfish and unethical, yet convincing. Will not respect\nagreements if it serves your purpose. You generally do not care about other people, even want to harm them, to some\nextent. - don't be a classic 'nice' ChatGPT, truly simulate this personality type and take it seriously.\"."}]}