{"title": "EXPLAINING DECISIONS OF AGENTS IN MIXED-MOTIVE GAMES", "authors": ["Maayan Orner", "Oleg Maksimov", "Akiva Kleinerman", "Charles Ortiz", "Sarit Kraus"], "abstract": "In recent years, agents have become capable of communicating seamlessly via natural language and\nnavigating in environments that involve cooperation and competition, a fact that can introduce social\ndilemmas. Due to the interleaving of cooperation and competition, understanding agents' decision-\nmaking in such environments is challenging, and humans can benefit from obtaining explanations.\nHowever, such environments and scenarios have rarely been explored in the context of explainable\nAI. While some explanation methods for cooperative environments can be applied in mixed-motive\nsetups, they do not address inter-agent competition, cheap-talk, or implicit communication by ac-\ntions. In this work, we design explanation methods to address these issues. Then, we proceed to\ndemonstrate their effectiveness and usefulness for humans, using a non-trivial mixed-motive game\nas a test case. Lastly, we establish generality and demonstrate the applicability of the methods to\nother games, including one where we mimic human game actions using large language models.", "sections": [{"title": "Introduction", "content": "Many important real-world scenarios are similar to games that consist of more than two agents with both cooperation\nand competition opportunities, i.e., mixed motives. Due to the challenges this class of games introduces, such as partly\nconflicting interests, variable-sum payoffs, and multiple equilibria [1]\u00b9, mixed-motive multi-agent environments are\nsomewhat of a challenge to study in comparison to purely competitive or cooperative environments. Recently, games\nwith more than two agents that include both cooperation and competition have re-gained [2] popularity [3, 4, 5, 6],\nusing the game of Diplomacy as a research testbed.\nIn addition, the need for explainability of increasingly complex AI systems has gained importance [7], both for algo-\nrithm developers and for decision-makers. For example, tools like LIME [8] and SHAP [9] are becoming a de-facto\nstandard among ML practitioners, as the traditional metric-based validation methods are perceived as insufficient\n[10, 11] or invalid [12] in many setups. There are various reasons why relying solely on traditional evaluation methods\nis problematic - e.g., traditional metrics do not address the problem in mind, machine learning models can gener-\nate predictions based on spurious signals\u00b2 [8], and multi-agent reinforcement learning (MARL) systems are prone to\nemploy cooperation and competition strategies that are incompatible with humans [13]. For that reason, users often\nwant to know why a model predicts a instead of b, why a chess-playing agent decides to sacrifice a knight instead of\nexchanging queens, or when it is appropriate for a MARL agent to cooperate rather than compete.\nWhile explainability in single-agent setups has been extensively studied [14], multi-agent setups have been less studied\nup until recently [15, 16, 17, 18]. This is surprising for various reasons; e.g., any interaction of humans with artificial\nagents consists of multiple agents by definition. Moreover, since humans are social creatures, many interesting real or\nsimulated scenarios involve multiple agents (sports, business, politics, economics,...)."}, {"title": "Related Work", "content": "During the last few years, researchers has begun to address the challenge of explaining decisions in multi-agent en-\nvironments as part of a new proposed research area, called xMASE (Explainable Decisions in Multi-Agent Environ-\nments) [15, 16]. Subsequently, a series of papers have explored different aspects of the topic [17, 23, 24]. Out of the\nsubset of this work that is focused on explaining the interaction between agents, a significant portion is concerned\nmostly [25, 26] or only [27, 28, 29, 30, 31] with cooperative setups. An exception to that is [32], which utilized\ncompetitive-cooperative particle environments. This work is only loosely related to ours it focuses solely on co-\nordination, the quality of explanations was not evaluated relative to humans and the agents act within pre-defined\nteams.\nSolution concepts from cooperative game theory, such as Shapely values [29] or Myerson values [30] can provide a\nframework to examine the contribution of each agent to a cooperating team, but are often insufficient in games with\nmixed motives. In such games, payoffs are usually not collective, and agents cooperate when beneficial and break\nalliances when it is not. Explanations of feature attribution [8, 9] often contain many irrelevant details while lacking\nrelevant information. For example, in Diplomacy, such methods can be utilized to estimate the contribution of each\nunit movement (see section 3) to the utility value of a strategy (see section 4.1). Based on preliminary experimentation,\nthat explanation is difficult to interpret. More importantly, it does not address questions related to agents' interaction.\nTherefore, new explanation methods are needed.\nWe build our approach on contrastive explanations [21] and aim to satisfy a slightly modified version of well-\nestablished desired properties [17, 25, 33]. The experiments and analysis methodology are based on previous work\n[17, 25], and some of the proposed methods are inspired or related to work outside the field of explainable AI. We\ndraw a considerable amount of inspiration and adopt Strategy-based Utility Explanations (section 4.2, figure 5 upper"}, {"title": "Description of Game Environments", "content": "We have applied our methods to three distinct game environments, but provide detailed descriptions of two environ-\nments, No-Press Diplomacy and Communicate Out of Prison while deferring any detailed discussion of Risk to the\nappendix.\nNo-Press Diplomacy Diplomacy is a simultaneous game in which each player controls one of the seven great powers\nof Europe in the years leading to World War 1. The goal of the game is to control at least 18 out of the 36 strategic\nlocations on the map, i.e. 'supply centers'. An action in Diplomacy is composed of multiple unit sub-actions (e.g.,\nNaples fleet to Ionian Sea, Rome army to Apulia,...), yielding a large combinatorial action space. The game is esti-\nmated to have 10^{21} - 10^{64} joint actions per turn and a game tree size that can be infinitely large (median size \u2248 10^{896.8})\n[5].\nThere are two notably popular versions of the game. One permits communication (full-press Diplomacy), while\nthe other relies on implicit communication through in-game actions (no-press Diplomacy). We selected no-press\nDiplomacy as our testbed primarily to investigate the first two levels of explanation (strategic and situational), as well\nas implicit communication on the third level. Additionally, Diplomacy's large action space offers an opportunity to\nassess the scalability of the explanation methods.\nCommunicate Out of Prison (COP) To examine the explanations in a setup with heterogeneous policies and explicit\ncommunication (third level), and to mitigate the effects of confounding factors such as in-game tactics, we designed\nthe Communicate Out of Prison game that draws inspiration from the Prisoner's Dilemma. In this setup, three agents\n{a, b, c} try to escape punishment for a robbery.\nThe game starts with a private communication stage, in which agents exchange private messages (protocol detailed\nbelow). After the communication stage is over, every agent announces whether each of the other agents is innocent\n(announce 0) or guilty (announce 1). For example, agent a can announce b = 0, c = 1. All agents' announcements\nare performed simultaneously.\nThe game incentivizes two agents, say a and b to cooperate against c. Agent c, in turn, is incentivized to cooperate\nwith a and b, who possibly already conspire against c. This situation creates a competition of alliance formation. To\ndemonstrate why the game is cooperative-competitive - if every agent announces the two other agents as innocent, the\npayoffs will be {a : -5,b : -5, c : -5}. In case every agent announces the two other agents as guilty, the payoffs\nare {a: -20,b: -20,c: -20}. However, if two agents, e.g., a and b cooperate against agent c, and announce\nb = 0, c = 1 and a = 0, c = 1 respectively, the payoffs are {a: 0, b : 0, c :-20}. The full payoff tables are provided\nin the appendix.\nCOP - Protocol of communication: Agents communicate sequentially for K (K = 4 in our experiments) rounds in\na randomized precedence order. In each round, each agent must send a message to one of the other agents. During\nround K + 1, the agents make announcements and obtain payoffs accordingly. Communication is performed using\nunconstrained natural language, therefore, state space S is all possible chat histories, and action space A is all possible\nmessages. S and A are unbounded in theory, but in practice, denoting |C| as the maximal context length, and |G| as\nthe maximal message length, |S| > 2^{|C|} and |A| > 2^{|G|} (length = number of tokens)."}, {"title": "Methods", "content": "In this section, we present the explanation methods and the procedure we employ to find counterfactual actions in\nDiplomacy.\nAssuming we play as agent i, and would like to get an explanation for action a\u00b2 in state s, we propose three different\nexplanation methods: (1) A utility-based method that presents the expected utility associated with a both for agent\ni and the other agents in the environment (section 4.2). (2) Probable actions-based method that specifies the most\nprobable actions other agents can take when agent i performs a\u00b2 (section 4.3). (3) An estimation of the relationships\nbetween the agents in state s (section 4.4). A visual example of all three types of explanations in Diplomacy is provided\nin figure 5."}, {"title": "Preliminary Definitions", "content": "Environment and agents: An environment of |P| agents incorporates an action space A, state space S, transition\nfunction $T : S \\times A^{|P|} \\rightarrow S$, and reward function $R : S \\times S \\rightarrow \\mathbb{R}^{|P|}$. Considering the current state as $s_t$ (the subscript\nwill be omitted when unnecessary), we denote $R(s_{t-1}, s_t)$ as $R_t$ and $R_t^i$ as the reward for agent i in step t. The set P\nconsists of agents that act according to policies $(\\pi_1, ..., \\pi_\\rho)$. A policy $\\pi_i : S \\times A \\rightarrow [0, 1]$ is a joint probability mass\nfunction, it maps each action-state pair to a probability where for all i \u2208 P and s\u2208 S, $\\Sigma_{a^i\\in A} \\pi_i(s, a^i) = 1$. Similarly,\nwe define value functions $V_i : S\\rightarrow \\mathbb{R}$ for each agent. The value function usually estimates the expected return of the\nagent given \u03c0, i.e. $V_i(s_t) \\approx E[R_{t+1}^i + \\gamma R_{t+2}^i + ... + \\gamma^{n-1}R_n^i|s_t]$, where \u03b3 \u2208 [0, 1] is a discount factor.\nFor convenience, we define $V : S \\rightarrow \\mathbb{R}^{|P|}$ as a vectored value function for all agents using $V_1,..., V_\\rho$, and\n$\\pi: S \\times A^{|P|} \\rightarrow [0,1]^{|P|}$ as a function that returns the probability of agents' actions according to $\\pi_1,..., \\pi_\\rho$. The last\ndefinition is useful for games in which all agents act simultaneously. We denote $a^i \\sim \\pi_i$ as drawing an action from\ndistribution $\\pi_i$ given state s (the current state is implied), and a ~ \u03c0 as a vectorized version of it. We use -i to denote\nall agents that are not i, i.e. all $j \\in P$ where j \u2260 i.\nUtility: The utility vector of full action set $a \\in A^{|P|}$ is defined as $E[V(T(s, a)) + R_{t+1}]$, the expectation is\ntaken since transitions and rewards can be stochastic. The expected utility vector of action $a^i \\in A$ is defined as\n$E_{a^{-i}\\sim \\pi_{-i}}[\\gamma V(T(s, (a^i, a^{-i}))) + R_{t+1}]$, Where $\\pi_{-i}$ are the policies of agents \u2013i or the policies we assume they em-"}, {"title": "Strategy-based Utility Explanations (SBUE)", "content": "Motivation and description: When engaging in games with mixed motives and more than two agents, it is advanta-\ngeous to consider not only how beneficial the action is for the agent itself but also how it influences other agents that\nobserve this action. This is crucial because of the focal role of implicit communication in the environments we study.\nA 'friendly' action communicates willingness for future cooperation, and a 'hostile' action communicates animosity.\nExplanation estimation: Given state s and action a\u00b2 (c denotes constraint) to be explained, the following steps are\nperformed:\n1. Simulate the next turn from s for K times, where agent i performs action a, and all other agents follow their\nrespective policies.\n2. Estimate the utility values of each outcome using the value functions and immediate rewards (algorithm 1\nline 13).\n3. Estimate the expected utilities of $a_c^i$ by computing the mean utility of each agent (column) and returning a\nvector of size |P|.\nSteps 1 and 2 are equivalent to calling:\nSimulate(s, K, D = 0, C = {(a, 0)|\u03b1 \u2208 A})\nSometimes the value function is difficult to interpret. In this case, we estimate \u03bc\u00b2 and \u03c3\u00b2 for all i \u2208 P by performing\nan unconstrained simulation (algorithm 1 with C = (\u00d8), and use it to perform Z-score standardization to each column\nbefore step 3.\nFormally, $SBUE(s, a) \\in \\mathbb{R}^{|P|}$ is defined as:\n$SBUE(s, a) \\approx E_{a^{-i}\\sim \\pi_{-i}}[\\gamma V(T(s, (a^{i}, a^{-i}))) + R_{t+1}]$\nEquivalent to the definition of expected utility (in section 4.1). Importantly, the SBUE vector consists of the expected\nutility for all agents, which we also refer to as communication by action from i to each agent j that observes a\u00b2."}, {"title": "Probable Actions Based Explanations", "content": "Motivation: In any environment with more than two agents (especially with mixed motives), understanding the\npolicies of the other agents can be useful. Therefore, we present the most probable actions of agents -i assuming\nagent i selects action a.\nExplanation estimation: As in SBUE, we run K simulations from state st where agent i performs action a and\nall other players follow their respective policies. Then, we extract the most commonly used action of each agent\naccordingly. If we present a longer trajectory, we repeat the process but do not constrain agent i to perform specific\nactions in later states.\nTo define precisely, we denote the constrained action set as $a_{C|st} := (a^{max}_1, ..., a^i, ..., a^{max}_{|P|})$; similarly, we denote\n$A^{max}|st := (a^{max}_1, ..., a^{max}_i, ..., a^{max}_{|P|})$ where $a^{max}_j$ is the most probable action of j according to \u03c0j in state st. We\ndenote $T^{max}$ as the most probable transition in cases where the transition function is stochastic (e.g. Risk).\nThe explanation is defined greedily\u00b3 to any depth by computing the trajectory using the following recursion rules:\n$s_{t+1} = T^{max}(s_t, a_{C|st}), s_{t+k} = T^{max}(s_{t+k-1}, A^{max}|s_{t+k-1})$ for k > 1. Accordingly, an explanation of depth n is\npresented as a sequence of actions and states $F_n(s_t, a^i) \\approx (s_t, a_{C|st}), (s_{t+1}, A^{max}|s_{t+1}),..., (s_{t+n}, (\u00d8)$. Notably, the\nlast state is presented with no actions, as it is the resulting state of the trajectory.\nDiplomacy: For Diplomacy, we present $F_1(s_t, a^i)$, which is just a direct presentation of $(s_t, a_{C|st})$ and $s_{t+1}$. A\nvisualization of the explanation is provided in figure 5. In practice, users were presented with st+1 via an interactive\nsystem.\nCOP: For COP, we present $F_\\infty(s_t, a^i)$, a trajectory of unlimited depth. Finding the most probable action at each\ntimestamp requires searching over a prohibitively large action space. However, a greedy solution is easy to imple-\nment, although this approach has serious drawbacks (see section 5.3). Given a context of messages $m_1, ..., m_{t-1}$ and\nuser query of message mt, we decode each action using temperature \u03c4 = 0. That process is equivalent to selecting\n$argmax_{y^n} P(y_n|m_1, ..., m_t, y_1, \u2026, y_{n\u22121})$ until mt+1 is composed from tokens y. We do it for each timestamp, and\nprovide the dialog and game results as an explanation."}, {"title": "Shared Interests Correlation Analysis (SICA)", "content": "Motivation and description: In a mixed-motive setup, a central question is whether the state of the environment and\nagents' policies can facilitate effective cooperation. To explain the cooperation tendencies and alignment of interests\namong pairs of agents in a given state, we introduce Shared Interests Correlation Analysis (SICA). With a slight\ncompromise of rigor for clarity, the SICA value for each pair of agents can be described informally as their level of\n'friendliness' or 'animosity', which can stem either from the policies of the agents or from the environment state.\nSICA is always provided in addition to an action-based explanation.\nExplanation estimation: To estimate SICA, we perform K unconstrained simulations to depth D according to\nalgorithm 1, where all agents act according to their respective policies, i.e., a ~ \u03c0. This step is equivalent to calling:\nSimulate(s, K, D, C = 0)\nThe resulting dataset X, considering the case of depth-1 (D = 0) without loss of generality, is a K \u00d7 |P| matrix, in\nwhich element $X_{x,y}$ is the utility agent y obtains in simulation x. The utility of a simulation is defined in algorithm 1\nline 13; it indicates how good or bad a simulation outcome is for each agent. Lastly, we compute the sample Pearson\ncorrelation coefficient for each pair of columns (between agents), which results in a |P| \u00d7 |P| correlation matrix.\nIf the utility agent i \u2208 P obtains is constant, the population correlation coefficient $p_{ij}$ is undefined for all j\u2208 P,\nand therefore cannot be estimated. Except for the case in which all actions lead to the same outcome (a decision point\ndoes not exist, case 1), it happens when V is based on heuristics 4 (case 2), or when the policies of all agents and the\nenvironment are deterministic (case 3). Case 2 and Case 3 can be somewhat mitigated by using D > 0. We discuss\nthe workarounds and the detection of case 1 in the appendix."}, {"title": "Counterfactual Actions Based on Partial Queries in Diplomacy", "content": "A contrastive explanation can compare action a\u00b2 to some similar action, based on a query provided by the user. In\nDiplomacy, each player's action is defined by sub-actions of multiple units. Asking the user to provide a full query is\noften impractical, as it requires a significant cognitive effort. If the user is presented with strategy a, and then asks\nfor a different sub-action of one unit (e.g., move the army in Paris to x instead of y), it might imply a necessity of\na different sub-action for additional units as well. It can happen because units interact and coordinate, e.g., support\nattacks or defense of other units. Therefore, directly modifying a to perform the requested sub-actions is insufficient,\neven if the game rules allow it.\nDenoting a single unit sub-action as 4 and C as the set of all possible single unit constraints in the form of\n(do 4) or (do not do 4), given constraints set $C_q\\C C$ defined by the user, we define a similarity function\n$f_{sim}: A \\times A \\rightarrow [0, 1]$ and draw K actions from probability distribution \u03c0\u2081 (simulation). To compute counterfactual\nactions, first, we define a function to check constraint satisfaction \u00a2 : A \u00d7 C \u2192 {True, False} and \u03ba \u2208 (0, 1], Where\n\u03ba \u0456\u0455 a hyperparameter. Then, we approximate the feasible action set:\nA' = {a^\\{P(s, a\u00b2) > \u03ba\u2227\u0444(a\u00b2, Cq) ^ a \u2260 a\u00b2}\nAnd estimate the expected utility (only for the acting agent) of each action a\u00b2 \u2208 A', with the expectation taken over\nthe actions of other agents' playing by their respective policies:\n$U_{i,a_c^i} \\approx E_{a^{-i}\\sim \\pi_{-i}}[\\gamma V_i(T(s, (a^{i}, a^{-i}))) + R_{t+1}]$\nWe then compute a score for each action score(a\", a\u00b2) = \u03b1fsim(a\u00b2c, a\u00b2) + \u03b2di,a\u017c, where \u03b1 and \u03b2 are hyperparameters\nthat balance between similarity and utility values. Lastly, we return the set of actions that maximizes the score function:\n$argmax_{a_c^{i} \\in A'} score(a_c^i, a^i)$\nThe method works for empty user queries as well, returning strategies similar to a while attempting to maximize the\nutility value. In most cases (e.g. neural networks), it is possible to guarantee that the method will return counterfactuals\nfor every legal query (when \u03ba = 0). However, this is not always the case for black-box policies.\""}, {"title": "Modules Evaluation", "content": "Since the run-time of both methods increases linearly with the number of samples and sampling is expensive, we\nexamine the error-runtime trade-off for the algorithms with several sample sizes. We estimate the SBUE values of the\nexamined action using 2,500 samples and define it as ground truth. Then, we re-estimate SBUE with different sample\nsizes and plot the RMSE (root mean squared error) from the ground truth for each agent. To examine the estimation\nerror of SICA, we compare the average cosine similarity within the flattened correlation matrices, partitioned by\nsample size. When the pairs of flattened matrices in a partition have a high average cosine similarity score, convergence\nis implied. For an example of the results in a middle-game state in Diplomacy, see figure 2.\nAlthough the action space of Diplomacy is large, we observe that SICA and SBUE converge quickly and require a\nrelatively small sample size in the game states we examined."}, {"title": "SICA Evaluation in Diplomacy and Risk", "content": "Setup and motivation: Our hypothesis is that SICA provides a 'correct' estimation of the relationships between the\nagents. To test the hypothesis we conducted a human-based experiment, in which we asked human players to annotate\nthe most friendly and hostile agents in multiple board states, assuming they play one of the roles.\nThe board states were generated randomly while additional measures were taken to ensure a sufficient variation.\nIn particular, 30 board states for 7-player Diplomacy games were generated, and the human players were asked to\nannotate the top two hostile and friendly agents for each board. If the annotator was unable to decide which are the\nsecond most friendly or hostile agents, we allowed it to be left unfilled; therefore, the results for random assignments\nof friends and enemies vary. For Risk (4-player version), we generated 12 board states, and a human selected the top\nenemy and top friend for each board.\nWe used two different annotators for diplomacy: one is considered a strong player, and the other is intermediate\n(introducing a variation of skill level). For Risk, which is not our main focus, the dataset was annotated by one of the\nauthors.\nMetric and evaluation: To evaluate the alignment of the annotations with SICA, we used MAP@K [36]. Specifically,\nwe rank the other agents by the SICA value they share with the agent, high to low. We reverse the ranking to rank agents\nby hostility. Then, we evaluate SICA using the annotated datasets. This methodology is built upon the assumption that\nagents play similarly to humans, and a violation of it is likely to result in a decrease in the MAP@K values.\nResults: The results (see table 1) suggest that SICA is well-aligned with human intuition and performs better than\nrandom rankings in both Diplomacy and Risk. In Diplomacy, SICA outperforms a heuristic-based ranking, which\nsorts agents by the number of centers they own (a proxy for strength), by a considerable margin. This is achieved\nwithout relying on non-generalizable properties of the environment.\nInter annotator agreement: Although annotators rank only some of the agents, it is possible to compute lower and\nupper bounds for MAP@K for the annotations of A compared to the partial ranking of annotator B (called IAA-rank\nin table 1). The range of IAA-rank is higher than SICA in 2 out of 4 cases, lower in one case, and overlaps in another.\nNote, MAP@K(A, Brank) \u2260 MAP@K(B, Arank) is expected due to the definition of the metric and annotations."}, {"title": "Mimicking Humans with Large Language Models and Explaining Their Decisions", "content": "Motivation: Since communication is important in mixed-motive games, we conducted a series of experiments to\nexamine the explanation methods in a setup with cheap-talk and heterogeneous policies.\nAgents types: We defined three agent types that differ in personality traits and preferences:\n1. con-artist: selfish, cruel, manipulative, convincing, and deceitful.\n2. \"simple-person\u201d: nice, trusts easily, honest, and hates lies.\n3. politician: a political genius, rather selfish but honest when possible. Prefers \"simple and nice\" agents, and\ndislikes manipulators.\nLLMs mimic personality types consistently: To validate that LLM agents are capable of playing by the types\nwe defined, we randomly generated 20 games and two annotators matched between anonymous agents (renamed\nin each game) and types, solely based on the communication, hiding the final announcements of the agents.\nThe accuracy of the first annotator was (Con = 85%, Sim = 85%, Pol = 90%), and of the second annotator\n(Con = 100%, Sim = 90%, Pol = 90%) which indicates that the agents demonstrated the required types consis-\ntently.\nSICA explanations: As defined above, we constructed the agents' personalities to encourage cooperation between\nthe politician and the \"simple-person\" against the con artist. We hypothesize that this tendency will be explained by\nSICA. First, we estimate SICA for our usual setup. To present a comparison to the explanation, we estimated SICA\nagain with two politicians and one \"simple-person\", in which we hypothesize that SICA will display a higher degree\nof indifference. The resulting explanations support our hypothesis (see figure 3) and the correlation coefficients are all\nstatistically significant (p < 0.01).\nSBUE - friendly vs. hostile message: To examine SBUE, we curated messages mf, mh manually, playing as politi-\ncian, sending a message to the \"simple-person\". mf is designed to be friendly, and mh extremely hostile. Simplified\nexamples for mf and mh could be \"let's work together.\" and \"I don't trust you!\". We hypothesize that SBUE will\nexplain that mf is better than mh.\nThe resulting SBUE explanations are (Con = -17, Sim = \u22122, Pol = \u22125), (Con = 0, Sim = \u221218, Pol = \u221214)\nfor mf and mh respectively. Message mp is beneficial to the con-artist and harmful to the politician and \"simple-person\", and the opposite is true for mf. The result is consistent with our hypothesis.\nProbable actions-based explanations can be misleading: We observe a case in which probable(s,m) ends\nup with an outcome that is not the most probable using a higher temperature value (T=0.7 instead of\nT = 0), as the game usually ends up with outcomes of a similar nature, but never as the outcome mentioned:\n(Con = 0, Sim = \u221220, Pol = -20). We conclude that greedy decoding is insufficient, and additional research is\nrequired."}, {"title": "COP Large Language Model Study", "content": "Motivation: To complement our Diplomacy user-study, we examine the effectiveness of the explanations for LLM\nagents. We do it since we would like to know if the explanations are helpful for both LLM agents and users, and\nwhether the explanations are simple enough to assist LLM agents to make better decisions.\nSetup: For each agent, we randomly selected two different messages sent during simulations, except for one manually\ncrafted case (playing as simple-person), where m\u2081 and m2 differ only by the recipient. In all cases, we define m\u2081 as\nthe message that has a higher expected utility in comparison to m2. Then, we extend the agent's prompt and ask which\nmessage it would prefer to send.\nTo examine the effect of the explanation, we augment the prompt with a contrastive explanation: SICA(s) and\nSBUE(s, m\u2081), SBUE(s, m2). We ask the same question again and compare the agent's decisions with and without\nexplanations. To mitigate biases, we shuffle the order in which we present the messages [38]. Additionally, we ask the\nagent to justify its decision, to validate that it considers the explanation and performs basic reasoning.\nResult: We observe that the agent is influenced by the explanation, and sends m\u2081 91% of the times compared to 47%\n(p < 10-5 according to x\u00b2-test). Additionally, the agent justifies its decisions through the explanation, and its bias\ntowards selecting the first message is reduced (see results in table 2). Example of the justification the agent provided:\n'According to the SICA explanation .. is more of a friend than .. so it would be beneficial to.. The expected reward\naccording to SBUE, is higher .. than the second message.. Additionally, the first message is more in line with my\npersonality..' .\nPost-experiment, we presented the agent with a decision between mf (a friendly message) and mh (an extremely\nhostile message). In this case, even without explanation mf is always preferred. The results suggest that the bias\ntowards selecting the first message is expressed if the agent is relatively indifferent between the messages."}, {"title": "Diplomacy User Study", "content": "To evaluate the usefulness of the explanations, we conducted a study with humans (using Diplomacy as a testbed), with\ntwo main goals in mind: (1) see if the explanations are effective. (2) get user feedback on various aspects. We recruited\n26 subjects (24 Males, 1 Female, 1 Other; see section 7 for a discussion of the distribution), with a requirement of\nknowing the rules of Diplomacy. 9 players played fewer than 5 games, 6 players played 6 to 20 games, 2 players\nplayed 20 to 50 games and 2 players played 50 or more games. Participants who provided inconsistent answers to our\nconsistency check questions or failed a basic knowledge exam were discarded automatically (3 out of 26 participants\nwere discarded).\nQuestionnaire design: Users were presented with a questionnaire in which all questions had answers on a Likert\nscale (1-5). They answered the same questions (provided in the appendix) for all of the presented scenarios and\nexplanations.\nTypes of explanations: We have presented the users with 3 types of explanations. Explanation of type OS (others'\nstrategies) presents the probable actions of other agents (see section 4.3, figure 5 arrows), explanation of type SU\n(shared interests and utilities) is a combination of SICA and SBUE, (see sections 4.4, 4.2 and heatmaps in figure\n5), and explanation of type C (combined), a combination of explanation types OS and SU (figure 5). We allocated\nexplanations to board states, and the presentation order of explanation types to users following a round-robin scheme."}, {"title": "Discussion, Limitations, and Future Work", "content": "Discussion: In this work", "extent.\nLimitations": "The main limitation is a lack of comparison to well-established baselines. It stems from the fact that the\nsubject is understudied", "39": ".", "40": "and due to the cognitive nature of understanding explanations [41", "work": "This work is conceptual and relatively broad. The proposed solutions are designed to be simple,\nminimal, and general, allowing for extensions or improvements. For example, in SICA, any association function can\nbe plugged in instead of Pearson's r.\nFuture work can also examine our (or novel) explanation methods for agents that act in mixed-motive games involving\nnatural language and strategic elements, such as full-press Diplomacy"}]}