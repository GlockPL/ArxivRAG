{"title": "DIVE: Taming DINO for Subject-Driven Video Editing", "authors": ["Yi Huang", "Wei Xiong", "He Zhang", "Chaoqi Chen", "Jianzhuang Liu", "Mingfu Yan", "Shifeng Chen"], "abstract": "Building on the success of diffusion models in image generation and editing, video editing has recently gained substantial attention. However, maintaining temporal consistency and motion alignment still remains challenging. To address these issues, this paper proposes DINO-guided Video Editing (DIVE), a framework designed to facilitate subject-driven editing in source videos conditioned on either target text prompts or reference images with specific identities. The core of DIVE lies in leveraging the powerful semantic features extracted from a pretrained DINOv2 model as implicit correspondences to guide the editing process. Specifically, to ensure temporal motion consistency, DIVE employs DINO features to align with the motion trajectory of the source video. For precise subject editing, DIVE incorporates the DINO features of reference images into a pretrained text-to-image model to learn Low-Rank Adaptations (LoRAs), effectively registering the target subject's identity. Extensive experiments on diverse real-world videos demonstrate that our framework can achieve high-quality editing results with robust motion consistency, high-", "sections": [{"title": "1. Introduction", "content": "In recent years, diffusion-based generative models have emerged as a leading approach in the domain of Artificial Intelligence Generated Content (AIGC) due to their outstanding capabilities in capturing data distribution and their inherent resistance to mode collapse [18, 52-55]. Extensive research has shown the exceptional potential of diffusion models in generating diverse and high-quality images conditioned on text prompts, a process commonly referred to as text-to-image (T2I) synthesis [7, 20, 44, 47]. Building upon this success, researchers have extended the application of T2I diffusion models to the field of text-to-video (T2V) generation [11, 19, 21, 38, 51, 64, 68] and editing, aiming to achieve similar advancements in video content creation.\nCompared to image editing [3, 24, 30, 48], video editing [35, 43, 56, 70, 76] presents unique challenges, especially in capturing the motion from source videos and transferring it to edited results while maintaining temporal consistency. Most methods address this challenge by relying on pretrained T2I models, introducing additional temporal layers, motion modules, or unique mechanisms such as spatial-causal attention and temporal attention [12, 28, 35, 65]. These methods typically transfer the source video's motion to the target video by injecting temporal attention maps or features. For instance, Tune-A-Video [65] performs cross frame attention between each frame and anchor frames (e.g., the first and previous frames) to preserve appearance consistency. TokenFlow [12] further explicitly propagates diffusion features based on inter-frame correspondences to improve consistency. However, the stored features in these extra modules or internal blocks may inadvertently retain some source appearance details, leading to a blend of source and target appearances and resulting in visual incoherence. To better disentangle motion from appearance in source videos, some methods utilize video correspondences that carry minimal appearance information, such as flow [6, 33], depth [8, 25, 34], or edge [70, 76] map sequences, for temporal motion modeling. For instance, RAVE [25] utilizes depth maps extracted from source videos, which are then applied to generate target videos through ControlNet [74]. However, due to the high density of these sequences, existing techniques for extracting or estimating them often result in visual content incoherence and flickering. VideoSwap [15] addresses this by observing that a subject's motion trajectory can be effectively represented using a small number of semantic points and utilizes these as sparse video correspondences to align the motion trajectory. Although this approach can accurately align motion trajectories, it requires accurate manual definition of these points in the source videos, which is labor-intensive.\nIn this paper, we aim to establish more robust video correspondences that not only contain minimal appearance information but also can accurately align motion trajectories without manual annotation. Motivated by recent advances in self-supervised learning, we leverage a pretrained DINOv2 model [41], a vision Transformer distilled using a large collection of natural images. DINO features have been demonstrated to have robust and discriminative capabilities in capturing fine-grained semantic information, proving valuable across visual tasks [1, 39, 50]. Additionally, Zhang et al. [73] reveal that DINO features excel at obtaining sparse and accurate matches on image semantic correspondence. However, their application in the video domain has not been fully explored. Based on these insights, we introduce DINO to the video domain and find that DINO features across video frames exhibit high semantic similarity, indicating their potential as robust video correspondences to guide the editing process.\nTo this end, we propose DINO-guided Video Editing (DIVE), a framework that enables subject editing in source videos conditioned on either text prompts or reference images with specific identities, as shown in Figure 1. DIVE consists of three stages: temporal motion modeling, subject identity registration, and inference. In the first stage, we extract the DINO features from the source video and align them into the diffusion feature space through learnable MLPs to serve as motion guidance. In the second stage, we incorporate the DINO features of reference images into a pretrained T2I model to learn Low-Rank Adaptations (LoRAs) for identity guidance. In the final inference stage, we use DDIM inversion to obtain the latent noise of the source video and replace the source subject with the target subject in the text prompt, utilizing both the motion and identity guidance learned from the previous two stages. Our contributions can be summarized as follows:\n\u2022 We demonstrate that tamed DINO features can serve as strong and robust correspondences for capturing temporal motion.\n\u2022 We propose DIVE, a subject-driven video editing framework that leverages DINO features for consistent motion modeling and precise subject identity registration.\n\u2022 Extensive experiments demonstrate that DIVE can achieve high-quality editing results while maintaining temporal consistency and motion alignment."}, {"title": "2. Related Work", "content": "Diffusion models have recently emerged as a powerful approach in various vision tasks, particularly in image generation and editing [3, 23, 40, 44, 74]. Building on their success, video generation and editing with diffusion models"}, {"title": "2.1. Diffusion-Based Video Generation and Editing", "content": "have garnered significant research interest. Early efforts focus primarily on training video diffusion models directly in pixel or latent spaces [2, 19, 21, 61], which require extensive datasets [62, 69]. However, recent studies have shifted towards training-free models to reduce computational costs. For instance, Text2Video-Zero [26] leverages the pretrained Stable Diffusion model for video synthesis, employing the cross-attention mechanism with the first frame to maintain frame consistency.\nAs some focus shifts from video generation to video editing, research has increasingly explored methods that adapt pretrained T2I models, driven by their outstanding generative capabilities [12, 28, 31, 32, 60, 65, 75]. These methods often introduce additional temporal layers, motion modules, or spatial-temporal attention to achieve temporal coherence. For example, Tune-A-Video [65] incorporates a temporal self-attention layer and updates projection matrices in each attention block of a T2I model to enhance temporal modeling. FateZero [43] preserves motion and structural information by storing comprehensive attention maps at each stage of the inversion process, which are then fused during editing. Some other methods use auxiliary sequences as correspondences, such as flow [6, 22, 33], depth [8, 9, 25, 34], or edge [70, 76] maps, to model temporal motion. For instance, FLATTEN [6] integrates optical flow into diffusion models through flow-guided attention, aligning patches from different frames along consistent flow paths. However, these methods often suffer from flickering and content incoherence due to the high density of the auxiliary sequences. To address these issues, VideoSwap [15] employs semantic points as sparse video correspondences to align motion trajectories, but it requires users to manually define these points in source videos. In contrast, our approach leverages the self-supervised DINOv2 features as implicit video correspondences, enabling accurate motion alignment without the need for manual annotation."}, {"title": "2.2. Subject-Driven Image Generation", "content": "Subject-driven image generation aims to create personalized images that consistently preserve the identity and unique characteristics of a given subject, typically guided by a few reference images. Earlier efforts in this domain, such as Textual Inversion [10] and DreamBooth [45], lay the foundation for personalized generation [4, 5, 13, 36, 37, 49, 58, 59, 63, 71, 72]. Textual Inversion [10] achieves this by learning a unique identifier word that represents a new subject, integrating it into the text encoder's dictionary to maintain subject consistency. DreamBooth [45], on the other hand, fine-tunes the entire Imagen [47] model with a few reference images, associating a new, rare word with the specific subject to ensure the generated images retain the subject's identity. To improve the representation of multiple new concepts simultaneously, Custom Diffusion [29] optimizes cross-attention parameters within Stable Diffusion, enabling joint training to combine multiple concepts without compromising subject identity. COMCAT [67] further improves efficiency by introducing a low-rank approximation of attention matrices, significantly reducing storage requirements while maintaining high fidelity in the generated outputs. In addition, methods such as adapters, LoRAs, and their variants [14, 27, 46] have become increasingly popular in subject-driven generation, offering parameter-efficient fine-tuning that allows for precise control over the personalized output. These advancements enable refined control over the generation process, enhancing the precision and accuracy with which the subject's identity is maintained across generated images. In this paper, we aim to register the subject's identity for editing source videos, either conditioned on text prompts or reference images."}, {"title": "3. Method", "content": "In this section, we begin by presenting the motivation behind our use of DINO feature correspondences in Section 3.1. Next, we provide an overview of the DIVE pipeline for subject-driven editing in Section 3.2, where we explain the technical details of each stage."}, {"title": "3.1. Motivation", "content": "A major challenge in subject-driven video editing is maintaining the motion trajectory of the source subject across frames. Existing methods often rely on auxiliary correspondences like optical flow or depth maps [6, 8, 25, 33], but they tend to result in visual flickering due to the high density of these correspondences and can cause mismatches in the subject's parts due to their lack of fine-grained semantics. To address these limitations, we explore DINO features as a more robust form of video correspondence, inspired by recent studies in image representation learning [57, 73].\nTo evaluate the potential of DINO features in video editing, we visualize them across consecutive frames using Principal Component Analysis (PCA). As shown in Figure 2, the DINO features of a moving subject (a kitten) reveal three key advantages. First, they effectively track the subject's motion across frames, consistently following the kitten's trajectory. Second, they contain minimal appearance information, abstracting away fine details like texture and color to focus on the subject's overall structure."}, {"title": "3.2. DIVE", "content": "The overall pipeline of DIVE is presented in Figure 3, designed as a framework for subject-driven video editing that ensures temporal consistency and precise identity preservation. It consists of three primary stages: temporal motion modeling, subject identity registration, and inference, which are detailed in the following sections."}, {"title": "3.2.1. Stage 1: Temporal Motion Modeling", "content": "As illustrated in Figure 3, given a source video V containing N frames, we first encode each frame into successive latent variables $Z = \\{Z_1, Z_2, \u2026, Z_V\\}$ using a VAE encoder, where $Z \\in \\mathbb{R}^{N \\times H \\times W \\times d}$, H and W represent the height and width of each latent variable, and d denotes the dimension of each token. To simulate the diffusion process, we add random Gaussian noise at timestep t to each latent frame, resulting in a noisy sequence $Z_t = \\{z_1^t, z_2^t,\u2026\u2026,z_V^t\\}$, which is then processed by the pretrained T2I model $\\epsilon_\\theta$ (e.g., Stable Diffusion). Additionally, we incorporate motion layers from AnimateDiff [16] into $\\epsilon_\\theta$ to maintain essential temporal consistency across frames. In the U-Net encoder of the inflated T2I model $\\epsilon_{enc}$, we extract intermediate features after each downsample block, represented as:\n$F^l = \\{F_1^l, F_2^l, \u2026, F_N^l\\} = \\epsilon_{enc}^l(Z_t)$.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(1)\nConsequently, the size of each feature $F_i^l$ at downsample stage $l \\in \\{1,2,3,4\\}$ is $N \\times H/2^l \\times W/2^l \\times d_l$, where $d_l$ denotes the channel dimension.\nTo capture the motion of the source video's subject, we use the DINOv2 model to extract semantic features from each frame of V. To avoid interference from background elements, we isolate the foreground subject features $F_d \\in \\mathbb{R}^{N \\times h \\times w \\times c}$ by adaptively generating masks M from the complete features. Specifically, we apply PCA to reduce the dimensionality of the full DINO features to 1, and use thresholding to separate the foreground from the background. Due to the high semantic similarity and appearance sparsity across frames, $F_d$ serves as effective temporal motion guidance. To integrate $F_d$ back into the diffusion space, we introduce a set of learnable MLPs $\\psi = \\{\\psi_l | l \\in \\{1,2,3,4\\}\\}$, following a similar technique used in VideoSwap [15]. Each $\\psi_l$ projects $F_d$ to match the feature dimension of the corresponding $F_i^l$, followed by a resizing operation to align the spatial size to $H/2^l \\times W/2^l$, as represented as:\n$\\tilde{F}_i^l = Resize(\\psi_l(F_d)), l \\in \\{1, 2, 3, 4\\}$.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(2)\nFinally, $\\tilde{F}_i^l$ is added element-wise to the corresponding intermediate feature $F_i^l$ of the U-Net encoder:\n$F_i^l \\gets F_i^l + \\lambda \\tilde{F}_i^l, l \\in \\{1,2,3,4\\}$.\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(3)"}, {"title": "3.2.2. Stage 2: Subject Identity Registration", "content": "Beyond modeling the temporal motion of the source subject in stage 1, it is crucial to capture the target subject's identity for precise subject replacement. In this stage, we use several (usually 3 to 5) reference images of the target subject. Similar to stage 1, we first transform these reference images into the latent space, adding Gaussian noise at timestep t to produce noisy image latents $I_t = \\{I_1^t, \u2026, I_P^t\\}$, where P is the number of images. These noisy latents are then passed through the pretrained T2I model $\\epsilon_\\theta$, from which we extract intermediate features $F_i^l = \\{F_1^l, F_2^l, F_3^l, F_4^l\\} = \\epsilon_{enc}^l(I_t)$ after each downsampling block of the U-Net encoder.\nSince previous studies [57, 73] suggest that Stable Diffusion features primarily capture low-level spatial information, whereas DINO features excel at extracting high-level semantic information and can achieve sparse yet accurate matches. Therefore, we also extract semantic features of the reference images using the DINOv2 model and fuse them with the internal diffusion model features to provide accurate identity guidance in the subsequent inference stage. Specifically, we mask out background elements to focus on the foreground subject as well. The resulting foreground semantic features, $F_d$, are integrated back into the diffusion space through a set of learnable MLPs $\\phi$, yielding projected features $F_s$, similar to Eq. 2. We then fuse $F_s$ with $I_t$ via element-wise addition, as in Eq. 3. To effectively register the target subject's identity, we employ Low-Rank Adaptations (LoRAs), denoted as $\\Delta \\theta$, rather than fine-tuning the entire T2I model. The optimization objective for identity registration is thus formulated as:\n$\\min_{\\Phi, \\Delta \\theta} E_{\\epsilon \\sim N(0,1), t \\sim U(1,T)} [\\vert\\vert \\epsilon - \\epsilon_{\\theta + \\Delta \\theta}(I_t^p, t, \\tilde{c}, \\phi(F_d)) \\odot M \\vert\\vert^2]$,\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(5)\nwhere $\\tilde{c}$ represents the text prompt embedding, and M is a binary mask activating only the target subject's region. Unlike in stage 1, this optimization is applied across all timesteps from t = 1 to t = T. In this way, we register the target subject's identity into the LoRAs by combining high-level semantic features from the DINOv2 model with low-level details from the pretrained T2I model, forming a robust representation of the target subject's characteristics."}, {"title": "3.2.3. Stage 3: Inference", "content": "In the final stage, we synthesize the edited video frames by leveraging the motion and identity guidance learned in the previous stages. First, we perform DDIM inversion on each frame of the source video to obtain the initial latent noise representations, providing a starting point for the editing process. To guide the edited frames with the source video's motion, we apply the motion features $\\tilde{F}_i^l$ learned in stage 1, which are derived from the DINO-extracted semantic features of the source video. These features are added element-wise to the intermediate layers of the U-Net encoder during denoising the steps from T to T/2. The MLPs $\\psi$, already optimized in stage 1 to project DINO-based motion guidance into the diffusion model space, are directly applied in inference mode here, preserving the motion trajectory of the original video. For identity guidance, we use the pretrained LoRAs $\\Delta \\theta$ from stage 2 to integrate the target subject's identity into the model without further feature fusion or additional MLP projections. These learned LoRAs encode the characteristics of the target subject directly within the T2I model, ensuring that the target identity is accurately and consistently represented across frames. To preserve the unedited background in the source video, we apply latent blending using a foreground mask, generated from P2P [17], to merge the latent variables from inversion and denoising at each timestep. Additionally, we replace the source subject word in the text prompt (e.g., \"cat\") with the target subject identity word (e.g., \"dog\") to ensure alignment with the target identity.\nAs the denoising process progresses, the combined motion and identity guidance together work on each frame to produce an edited video that retains the original motion and accurately replaces the subject with the target identity from the reference images."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "For the model architecture, we adopt Stable Diffusion 1.5 from the official Huggingface repository, incorporating the pretrained motion layer from AnimateDiff [16] as the foundational model. For the DINOv2 model, we utilize the ViT-g/14 variant without registers\u00b9.\nWe evaluate our method on a dataset of 30 videos curated from DAVIS [42] and the Internet, covering both objects and animals. Each video sample consists of 16 frames with a temporal stride of 4 to match the temporal window of the motion layer in AnimateDiff. We crop and reisze these videos to two alternate resolutions for editing (H \u00d7 W): 512x 512 and 512 \u00d7 896. For videos at 512 \u00d7 512, we further resize them to 448 x 448 for DINO feature extraction. This preprocessing allows us to match feature"}, {"title": "4.2. Qualitative Comparison", "content": "We compare DIVE against 4 baseline methods: TokenFlow [12], AnyV2V [28], FLATTEN [6], and RAVE [25],"}, {"title": "4.3. Quantitative Comparison", "content": "For quantitative evaluation, we use the metrics proposed in [29, 66], assessing DIVE and the baseline methods across"}, {"title": "4.4. Ablation Study", "content": "Motion Guidance. To validate the effectiveness of DINO features as temporal motion guidance, we remove the DINO feature extraction branch and the learnable MLPs $\\psi$ in Eq. 3, but fine-tune the motion module from AnimateDiff [16]. The editing result, shown in the second row of Figure 6, indicates that although this setup can capture the source motion, it struggles to disentangle the motion from the appearance of the source subject. In contrast, our method can fully edit the subject without introducing elements of the source appearance, as DINO features exhibit high sparsity and semantic consistency across frames (see the last row). Additionally, to further evaluate the effectiveness of DINO features in capturing source motion trajectories, we vary the guidance injection weighting parameter $\\lambda$ in Eq. 3 while keeping the motion module frozen. The comparison in the last three rows of Figure 6 shows that without adequate DINO feature guidance, the motion cannot be consistently maintained, demonstrating the robustness of DINO features as video correspondences for reliable video editing.\nIdentity Guidance. To evaluate the impact of DINO features on identity registration, we perform an ablation study"}, {"title": "5. Conclusion and Limitations", "content": "This paper presents DIVE, a framework for subject-driven video editing that leverages DINO features to achieve reliable motion alignment and precise identity registration. DIVE effectively replaces the subject in a source video with a specified target identity, whether described by reference images or text prompts, while preserving the original motion trajectory and background context. Extensive qualitative and quantitative comparisons demonstrate that DIVE outperforms existing methods in visual quality, motion consistency, and identity alignment, highlighting the potential of DINO to advance video editing.\nDespite its strengths, DIVE has certain limitations. First,"}]}