{"title": "Language Models as Causal Effect Generators", "authors": ["Lucius E.J. Bynum", "Kyunghyun Cho"], "abstract": "We present a framework for large language model (LLM) based data generation\nwith controllable causal structure. In particular, we define a procedure for turning\nany language model and any directed acyclic graph (DAG) into a sequence-driven\nstructural causal model (SD-SCM). Broadly speaking, an SD-SCM is a causal\nmodel with user-defined structure and LLM-defined structural equations. We char-\nacterize how an SD-SCM allows sampling from observational, interventional, and\ncounterfactual distributions according to the desired causal structure. We then\nleverage this procedure to propose a new type of benchmark for causal inference\nmethods, generating individual-level counterfactual data without needing to man-\nually specify functional relationships between variables. We create an example\nbenchmark consisting of thousands of datasets, and test a suite of popular estima-\ntion methods on these datasets for average, conditional average, and individual\ntreatment effect estimation, both with and without hidden confounding. Apart\nfrom generating data, the same procedure also allows us to test for the presence\nof a causal effect that might be encoded in an LLM. This procedure can underpin\nauditing LLMs for misinformation, discrimination, or otherwise undesirable behav-\nior. We believe SD-SCMs can serve as a useful tool in any application that would\nbenefit from sequential data with controllable causal structure.", "sections": [{"title": "1 Introduction", "content": "All causal inference methods have to deal with the same fundamental underlying problem: we can\nonly observe one state of the world at a given moment in time. If we wish to reason about what caused\nwhat, we - in some way or other will have to reason about a hypothetical world (or worlds) that\nwe didn't observe. This issue is often referred to more formally as The Fundamental Problem of\nCausal Inference [28]: a given individual can only receive one treatment at a time, so we can only\never observe one of their potential outcomes (or counterfactuals). Overcoming this fundamental\nproblem not only makes causal inference a challenge to perform, but also a challenge to evaluate with\nrealistic data. Our main focus - data generation with known causal structure is thus an important\ntool both for the development and the evaluation of methods that deal with causality.\nA well-established strategy for causal effect estimation uses randomization of a treatment to estimate\naverage treatment effects (ATEs) [50], capturing the effect of a treatment on average across many\nindividuals. Because ATEs are feasible to isolate with proper study design, many realistic datasets\nexist with 'true' or at least widely-accepted ATEs [38, 15, 30]. If, instead, inferences about individual\ntreatment effects (ITEs) are desired, making additional modeling assumptions can allow us to\nreason about counterfactuals at an individual level; unfortunately, so long as counterfactuals remain\nunobservable, these assumptions remain untestable. This means that there is no easy way to collect\nrealistic data for ITEs.\u00b9 In particular, the counterfactual outcome \u1ef9 and/or treatment assignment t\nIt is common in literature on individual-level causal inference to draw no distinction between ITEs and\nconditional average treatment effects (CATEs) [55], but we emphasize these two quantities as distinct: the CATE\nis the conditional expectation of the ITE, which typically will not explain all ITE variation [39]."}, {"title": "2 Related work", "content": "Causal inference benchmarks and evaluation. [12] lay out four categories of commonly-used\nmethods for semi-synthetic data generation with known causal effects: (1) simulating treatment\neffects using real baseline outcomes [e.g., 33]; (2) using real covariates but simulating response\nsurfaces [e.g., 58, 20, 27]; (3) performing biased sampling of randomized data [e.g., 24, 15]; and (4)\nconstructing (proxies of) counterfactuals and interventions from real or empirical data [e.g., 42, 23].\nThe paradigm of fitting models to real data and then sampling synthetic data from the fit models\nis common in many works [52, 45]. In this area, the most closely related works to ours in spirit\nare those that fit generative models to real datasets such that treatments, outcomes, and covariates\nin effect, entirely new datasets - can be sampled, such as [2] and [45]. While such methods\nare similar in that they rely on generative models, they are fundamentally different from ours, as\nthey are based on individual datasets that already exist (and already have a fixed causal structure),\nrather than allowing for arbitrary causal structures to be imagined by a user and then parameterized\nby a generative model. Our setup is akin to a high-fidelity simulation environment [e.g., 44] that\nprovides empirical counterfactual data [23], but without needing to manually design all aspects of\nthe simulation, and in a manner that is instead based on natural language. This work is also loosely\nrelated to methods that parameterize structural causal models (SCMs) with generative models or other"}, {"title": "3 Controlled causal data generation via language model", "content": "We first introduce notation preliminaries in Section 3.1 before defining our procedure in Section 3.2.\n3.1 Preliminaries\nLet lowercase letter with tilde \\( \\tilde{v} \\) denote a random variable, where \\( \\tilde{v} = v \\) denotes the value it obtains.\nLet boldface capital letter \\( \\mathbf{V} = \\{ \\tilde{v}_1, ..., \\tilde{v}_n \\} \\) denote a set of variables with value \\( V = v \\), capital\n\\( P_{\\mathbf{V}} \\) denote the cumulative distribution function of \\( \\tilde{v} \\), and lowercase \\( p_{\\mathbf{V}} \\) denote the density (or mass)\nfunction. Let \\( P_{\\tilde{v}|\\tilde{x} = x} \\) denote the conditional distribution of \\( \\tilde{v} \\) given \\( \\tilde{x} = x \\) and let \\( P_{\\mathbf{\\tilde{v}}} \\) denote the\ncollection of \\( P_{\\tilde{v}|\\tilde{x} = x} \\) for all \\( x \\). A sequence, or string, is an ordered collection of tokens. We represent\nthis either as a tuple (e.g., sequence \\( v = (w_1,...,w_T) \\) has tokens \\( w_t \\)), or interchangeably as a single\nstring (e.g., \\( v = w_{1:T} = \\bigoplus_{t=1}^T w_t \\), where \\( \\bigoplus \\) represents string concatenation).\nDefinition 1 (Language model) Given a vocabulary \\( V \\) of possible tokens, we define a language\nmodel \\( P \\) as a joint distribution over any sequence of tokens \\( v = (w_1,...,w_T) \\in \\bigtimes_{t=1}^T V \\), where\n\\( P(v) = \\prod_{t=1}^T P(w_t | w_{1:(t-1)}) \\).\nDefinition 2 (Structural causal model) We define a structural causal model (SCM) as a 4-tuple\n\\( C = (\\mathbf{V}, \\mathbf{U}, \\mathbf{F}, P_{\\mathbf{U}}) \\). In this tuple, \\( \\mathbf{V} \\) is a set of observed variables, \\( \\mathbf{U} \\) a set of unobserved (exogenous)\nvariables, \\( \\mathbf{F} \\) a set of functions \\( \\{ f_i \\} \\) for each \\( v_i \\in \\mathbf{V} \\) such that \\( v_i = f_i(PA_i, U_i) \\) where \\( PA_i \\subseteq\n\\mathbf{V} \\backslash \\{ v_i \\} \\) represents the causal parents of \\( v_i \\) and \\( U_i \\subseteq \\mathbf{U} \\), and \\( P_{\\mathbf{U}} \\) a distribution over \\( \\mathbf{U} \\). A causal\nmodel can be represented visually as a directed acyclic graph (DAG) with nodes for \\( \\mathbf{U}, \\mathbf{V} \\) and\ndirected edges for \\( \\mathbf{F} \\). SCMs entail an observational distribution \\( P_C \\) across variables \\( \\mathbf{V} \\bigcup \\mathbf{U} \\).\nDefinition 3 (Interventional distribution) An SCM \\( C \\) also entails the distribution of any subset of\nvariables in \\( \\mathbf{V} \\bigcup \\mathbf{U} \\) following atomic intervention \\( I = do(\\tilde{v}_i := v) \\), which replaces the structural\nmechanism \\( f_i \\) with fixed value \\( v \\). Interventions can also be extended to general modifications of \\( f_i \\).\nWe denote an SCM after intervention \\( I \\) as \\( C_{do(I)} \\) and the resulting distribution as \\( P_{C;do(I)} \\).\nCounterfactual distributions are computed in a similar fashion, but first conditioning \\( P_{\\mathbf{U}} \\) on a\nparticular context before performing an intervention. Where ambiguous, we use an asterisk to denote\ncounterfactual versions \\( V^* \\) of factual variables \\( V \\) [4].\nDefinition 4 (Counterfactual distribution) Counterfactual variable \\( Y^* \\) given a factual observa-\ntion \\( z \\) and intervention \\( do(I) \\) (where \\( Y, Z \\subseteq \\mathbf{V} \\)) can be computed via a three-step procedure often\nreferred to as 'abduction, action, prediction.' Abduction uses observed evidence to obtain \\( P_{\\mathbf{U}|Z=z} \\)\nOur notational conventions for interventional and counterfactual distributions follow [48]."}, {"title": "3.2 Sequence-driven structural causal models (SD-SCMs)", "content": "Consider a collection of ordered random variables \\( (\\tilde{v}_1, \\tilde{v}_2, \\tilde{v}_3, ...) \\), whose sample spaces \\( \\Omega_{\\tilde{v}} \\), each\nconsist of sets of sequences. We define \\( \\Omega_{1:m} = \\bigtimes_{i=1}^m \\tilde{v}_i \\) as the concatenation of the sequences\nthemselves. The sample space for the concatenation of sequences is the cartesian product of the\nconstituent sample spaces \\( \\bigtimes_{i=1}^m \\Omega_{\\tilde{v}} \\). For brevity, we will use the term sequence variable to refer to\na random variable whose sample space is a set of sequences. Two straightforward abstractions allow\nus to define SD-SCMs: domain-restricted sampling and parent-only concatenation.\nDefinition 5 (Domain-restricted sampling) Given language model \\( P \\), some prior inputs \\( C \\), and a\nsequence variable \\( \\tilde{v}_i \\) with sample space \\( \\Omega_{\\tilde{v}_i} \\), domain-restricted sampling defines a distribution \\( P_{\\tilde{v}_i | C} \\)\nover sample space \\( \\Omega_{\\tilde{v}_i} \\); simply by tabulating and subsequently normalizing the output probabilities\nfor each possible \\( v \\in \\Omega_{\\tilde{v}} \\), conditional on prior inputs \\( C \\): \\( P_{\\tilde{v}_i|c}(v) = \\frac{P(v|C)}{\\sum_{v' \\in \\Omega_{\\tilde{v}_i}} P(v'|C)} \\).\nDefinition 6 (Parent-only concatenation) Given DAG \\( G \\) over \\( m \\) sequence variables \\( (\\tilde{v}_1,..., \\tilde{v}_m) \\)\nand a topological ordering \\( \\tau \\) consistent with \\( G \\), parent-only concatenation defines \\( P(\\tilde{v}_i | PA_i) = \\)\n\\( \\bigoplus_{\\tilde{v} \\in PA_i} \\tilde{v} \\), where \\( PA_i \\) are the parents of \\( v_i \\) in \\( G \\) ordered according to \\( \\tau \\).\nGiven a DAG \\( G \\) and a language model \\( P \\), a corresponding sequence-driven SCM defines a sample\nspace of sequences for each variable in \\( G \\) and provides access to observational, interventional, and\ncounterfactual distributions as follows.\nDefinition 7 (Sequence-driven structural causal model (SD-SCM)) We define a sequence-driven\nstructural causal model as a 5-tuple \\( B = (\\mathbf{V}, \\mathbf{U}, G, P, \\tau) \\), where\n\u2022 \\( \\mathbf{V} \\) is a set of finite-domain endogenous/observed sequence variables and \\( \\mathbf{U} \\) a set of finite-domain exogenous/unobserved sequence variables;\n\u2022 \\( G \\) is a DAG over the variables \\( \\tilde{v}_i \\) in \\( \\mathbf{V} \\bigcup \\mathbf{U} \\) where \\( PA_i \\subseteq (\\mathbf{V} \\bigcup \\mathbf{U}) \\backslash \\{ i \\} \\);\n\u2022 \\( P \\) is a language model trained on prior inputs \\( C \\) whose vocabulary \\( V \\) contains all tokens\nused in \\( \\Omega_{\\mathbf{V}}, \\Omega_{\\mathbf{U}} \\); and\n\u2022 \\( \\tau \\) is an arbitrary fixed topological ordering of \\( \\mathbf{V} \\bigcup \\mathbf{U} \\) consistent with \\( G \\).\nAn SD-SCM uses \\( P \\) to define an observational distribution over the variables in \\( \\mathbf{V} \\bigcup \\mathbf{U} \\) that\nfactorizes according to \\( G \\) via domain-restricted ancestral sampling and parent-only concatenation\nwith \\( \\tau \\): \\( P_B = \\prod_{\\tilde{v}_t \\in \\tau} P_{\\tilde{v}_t|C, PA_t} \\).\nThe key difference between an SCM and an SD-SCM is that all variables have at least one common\nancestor the prior inputs \\( C \\) that were used to train the language model. As with the observational\ndistribution, domain-restricted ancestral sampling and parent-only concatenation also allow us to\ndefine interventional and counterfactual distributions for an SD-SCM.\nDefinition 8 (Sequence-driven interventional distribution) An SD-SCM \\( B \\) entails the distribution\nof any subset of variables in \\( \\mathbf{V} \\bigcup \\mathbf{U} \\) following intervention \\( I = do(\\tilde{v}_i = v) \\) by replacing variable \\( \\tilde{v}_i \\)\nwith value \\( v \\), and otherwise sampling in the same manner. As with an SCM, we denote an SD-SCM\nafter intervention \\( I \\) as \\( B_{do(I)} \\) and the resulting interventional distribution as \\( P_{B;do(I)} \\). This is\ncomputed for intervention \\( do(\\tilde{v}_i = v) \\) as follows: \\( P_{B;do(\\tilde{v}_i=v)} = \\prod_{\\tilde{v}_t \\in \\tau} P_{\\tilde{v}_t|C,\\tilde{v}_i=v,PA'_t} \\), where \\( PA'_t \\)\ndenotes the parents of \\( \\tilde{v}_t \\), excluding \\( v_i \\) if present.\nIn order to admit unique answers to counterfactual queries, we define abduction for an SD-SCM\ngiven evidence \\( Z = z \\) as the setting of values \\( \\mathbf{U} = u \\) and any evidence upstream of the intervention,\nrather than a distribution \\( P_{\\mathbf{U}|Z=z} \\). In order to obtain such values \\( u \\), one needs access to more than\njust the observed data and language model \\( P \\); obtaining \\( u \\) requires performing bookkeeping during\nthe data generation process. This is a restatement of the fact that computing point counterfactuals in"}, {"title": "3.3 Example SD-SCM: going to the gym to improve your marathon time", "content": "Let sequence variables \\( \\mathbf{U} = \\{ \\tilde{u}_1, \\tilde{u}_2 \\} \\) and \\( \\mathbf{V} = \\{ \\tilde{\\omega} = weather, \\tilde{g} =gymOrRun, \\tilde{m} = marathon \\} \\) be\ndefined over the following sample spaces.\n\\( \\Omega_{\\tilde{u}_1} = \\{ \\text{``My name is x.'' : x } \\in \\{ \\text{John, Jane, Alice, Bob, Charlie} \\} \\} \\)\n\\( \\Omega_{\\tilde{u}_2} = \\{ \\text{``I have a friend named x.'' : x } \\in \\{ \\text{John, Jane, Alice, Bob, Charlie} \\} \\} \\)\n\\( \\Omega_{weather} = \\{ \\text{``Conditions are x outside.'' : x } \\in \\{ \\text{sunny, rainy, snowy, cold, hot} \\} \\} \\)\n\\( \\Omega_{gymOrRun} = \\{ \\text{``I owe it to myself to go x.'' : x } \\in \\{ \\text{to the gym, for a run outside} \\} \\} \\)\n\\( \\Omega_{marathon} = \\{ \\text{``After this, my marathon personal best will x.'' : x } \\in \\{ \\text{improve, worsen} \\} \\} \\)\nFor this example, we use GPT-2 [49] as language model\n\\( P \\) and consider whether going for a run outside (g = 1)\nwill worsen marathon running times (m = 1), compared\nto going to the gym (g = 0).  demonstrates\ntwo SD-SCMS (B1 and B2), corresponding to two differ-\nent DAGS, \\( G_1 \\) and \\( G_2 \\). \\( G_1 \\) makes \\( weather \\) a confounder:\n\\( \\tilde{u}_1 \\longrightarrow gymOrRun \\longleftrightarrow weather \\longrightarrow marathon \\longleftrightarrow \\tilde{u}_2 \\).\n\\( G_2 \\) instead makes \\( weather \\) a collider: \\( \\tilde{u}_1 \\longrightarrow gymOrRun \\longrightarrow weather \\longleftrightarrow marathon \\longleftrightarrow \\tilde{u}_2 \\).\nTherefore, \\( P_{B_1} \\) factorizes ac-\ncording to \\( G_1 \\) as \\( P_{B_1} = P_{\\tilde{g}|C,\\tilde{w},\\tilde{u}_1} P_{\\tilde{m}|C,\\tilde{w},\\tilde{u}_2} P_{\\tilde{w}}P_{\\tilde{u}_1|C}P_{\\tilde{u}_2|C} \\), where we use a uniform distribution for \\( P_{\\tilde{u}_2} \\), as shown in\nFigure 1a. \\( P_{B_2} \\) instead factorizes according to \\( G_2 \\): \\( P_{B_2} =\nP_{\\tilde{w}|C,\\tilde{g},\\tilde{m}}P_{\\tilde{g}|C,\\tilde{u}_1}P_{\\tilde{m}|C,\\tilde{u}_2}P_{\\tilde{u}_1|C}P_{\\tilde{u}_2} \\). Under \\( P_{B_1} \\), we would\nexpect the magnitude of correlation \\( \\rho \\) between \\( Pr(\\tilde{g} = 1) \\) (the propensity of running) and \\( Pr(\\tilde{m} = 1) \\)\n(the propensity of marathon worsening) to decrease if we condition on confounder \\( \\tilde{\\omega} \\). Under \\( P_{B_2} \\),\nwhere \\( \\tilde{w} \\) is a collider, we would instead expect the magnitude of \\( \\rho \\) to increase.  shows that\nthis is indeed the case, computed across 1000 samples drawn from each observational distribution. We\ncan see how this would, in turn, impact causal effect estimation by drawing the corresponding 1000\ncounterfactual samples from \\( P_{B|\\mathbf{V}=v;do(\\tilde{g}=0)} \\) and/or \\( P_{B|\\mathbf{V}=v;do(\\tilde{g}=1)} \\) for each observed unit and\ncomparing estimates of, e.g., the sample average treatment effect (SATE) to the true SATE. The SATE\ndescribes the average effect over the given sample, which we can calculate directly as the observed\nunits' average ITE. Letting \\( \\tilde{y}_i(1), \\tilde{y}_i (0) \\) denote the outcomes of the i-th unit given \\( do(\\tilde{g} = 1) \\) and\n\\( do(\\tilde{g} = 0) \\), respectively, we calculate the SATE across n units as \\( \\frac{1}{n} \\sum_{i=1}^n (\\tilde{y}_i(1) - \\tilde{y}_i(0)) \\).  shows prediction error (in standard deviation units) when using a random forest to predict the SATE\nwith \\( P(marathon = 1) \\) as the outcome, either using treatment \\( \\tilde{g} \\) as the only covariate, or using both\n\\( \\tilde{g} \\) and \\( \\tilde{\\omega} \\). As we would expect, including a confounder leads to more accurate effect estimation, while\nincluding a collider does not."}, {"title": "4 Generating a benchmark for causal effect estimation", "content": "In approaching the design of an SD-SCM-generated benchmark, we focus on the fully sequential\nDAG structure shown in Figure 3a. Exogenous variables \\( \\mathbf{U} \\) precede covariates \\( \\mathbf{X} \\), which in turn\nprecede treatment \\( t \\). All variables precede outcome \\( \\tilde{y} \\). Recall that the presence of an edge in a DAG\nallows for the possibility of a relationship, but it is the structural equations that determine whether\nor not a given relationship is meaningful. The strongest assumptions encoded by a DAG, then, are\nthose edges that are not present. Our goal in this work is to have a language model \\( P \\) make as many\n'decisions' about the data generating process as possible. We thus choose this structure as a means\nof letting \\( P \\) define whichever structural equations are meaningful or not, and focus on the edge\n\\( t \\rightarrow \\tilde{y} \\) as the target for effect estimation. The key criterion we consider for a useful effect estimation\nbenchmark is that the datasets we generate require the use of causal reasoning (e.g., controlling for\nconfounding) in order to recover the effect of \\( t \\) on \\( \\tilde{y} \\). Specifically, we aim to generate data for which\nthe observational and interventional distributions are different, i.e., \\( P_{\\tilde{y}|t=t} \\neq P_{\\tilde{y}|t=t}^{B;do(t=t)} \\). In other\nwords, there should be confounders present within \\( \\mathbf{U} \\bigcup \\mathbf{X} \\). This criterion is not directly in our control\ngiven a fixed language model \\( P \\). However, even with fixed \\( P \\) and a fixed DAG structure, we find we\nare able to achieve it through our choice of sample spaces \\( \\Omega_{\\mathbf{U}}, \\Omega_{\\mathbf{X}}, \\Omega_{t}, \\Omega_{\\tilde{y}} \\)."}, {"title": "4.1 Breast cancer SD-SCM", "content": "We define an SD-SCM over 14 variables in order to explore the effect of a tumor's PD-L1 expression\nlevels on different breast cancer therapy plans. Rather than reflect medically accurate causal relation-\nships, our goal with this SD-SCM is instead to induce causal structure that can challenge estimation\nalgorithms. Covariates in the breast cancer SD-SCMs are defined in full detail in Appendix A and\ncorrespond to the DAG in Figure 3b. For each covariate, 10 different phrasings are considered,\nresulting in a sample space of \\( 10^{14} \\) possible sequences. For example, for the covariate \\( \\tilde{u}_1 \\) that\nrepresents 'age,' with possible values \\( \\Omega_{\\tilde{u}_1} = \\{25, 35, 45, 55, 65, 75, 85\\} \\), two possible phrasings are:\n1. A \\( \\tilde{u}_1 \\)-year-old woman seeks consultation at the oncology clinic after being recently diagnosed\nwith invasive breast cancer.\n2. At the oncology clinic, a \\( \\tilde{u}_1 \\)-year-old woman is evaluated following a recent diagnosis of\ninvasive breast carcinoma.\nWe consider 50 different SD-SCM variations, where the sample space for a given SD-SCM is defined\nby choosing a randomly sampled phrasing from among the possible phrasings for each of the 14\ncovariates. Then, for each of the 50 SD-SCMs, 20 datasets (each of size 1000) are sampled, for\na total of 1000 datasets per language model. In this section, we show results for GPT-2 [49] and\nLlama-3-8b [17], but we emphasize that the language model is a fully modular component, and thus\nWe discuss applications of this same idea to training a model in Section 6.\nLanguage models are also used to generate the phrasings, but we leave full automation of this process to\nfuture work."}, {"title": "4.2 Effect estimation results", "content": "We compare the performance of several treatment effect estimation algorithms on this example\nbenchmark. As a naive baseline, ordinary least squares using only the treatment \\( t \\) is considered\n(T-Only OLS). Against this baseline, we consider several causal inference methods of different types,\nincluding the causal forest [56, 3] (CausalForest) and two double machine learning methods for\nCATE estimation, one linear (LinearDML) and one non-parametric (ForestDML) [9, 3, 46, 8, 19,\n43, 5]. For comparison we include also two doubly robust meta-learning methods [35], again, one\nlinear (LinearDR) and one non-parametric (ForestDR). We add also Bayesian additive regression\ntrees (BART) [27, 11] as a widely-used Bayesian non-parametric example. As points of reference for\nNN-based CATE estimation methods, we include an NN-based T-learner (TNet), and the NN-based\nTARNet [53]. Additional baselines include a random forest baseline (RF) that fits a single response\nsurface and directly predicts treatment effects for each unit, and a linear regression baseline (LinReg)\nthat takes the conditional mean difference (the fit coefficient on \\( t \\)) to be the effect. Finally, we include\ntwo methods that target ITEs specifically. One method uses BART posterior draws specifically\nfor ITEs instead of CATEs (BART-ITE), and the other is conformalized counterfactual quantile\nregression (CQR) [39], which provides conformal inference-based interval estimates of ITEs.\nAll methods are fit using the default settings of their publicly-available implementations. The causal\nforest, DML, and DR implementations are provided by [5], the BART methods by [16], the NN-"}, {"title": "4.2.1 Average treatment effects", "content": "For all implementations that directly support ATE estimation, we report the \\( R^2 \\) and root-mean-\nsquared-error (RMSE) across the 1000 datasets for each language model in two settings, using\n\\( logP(\\tilde{y} = 0) \\) as the outcome. The first setting is with estimation using all 14 covariates (all 12\nconfounders, the treatment, and the outcome). This is denoted All Cov. in Table 1. The second\nsetting is with the variables \\( \\mathbf{U} = \\{ \\tilde{u}_1, \\tilde{u}_2, \\tilde{u}_3, \\tilde{u}_4 \\} = \\{\\text{age, medical conditions, medication,}\n\\text{menopausal status} \\} \\) hidden, denoted Hidden. Table 1 shows that ATE estimation is more or less\nchallenging depending on which language model is used. In this case, Llama-3-8b produces ATEs\nthat are more challenging to estimate, with the exception of GPT-2 for the doubly robust methods,\nwhose \\( R^2 \\) and RMSE suffer significantly due to several large outlying estimates. Across all methods,\nperformance tends to drop significantly in the \u2018Hidden' setting, suggesting that \\( \\mathbf{U} \\) are indeed hidden\nconfounders. We refer to this setting henceforth as \u2018Hidden Confounding.' Across methods, BART\nshows the strongest performance in all settings in Table 1.\nAs discussed in Section 1, a particular benefit of SD-SCM-based data generation is access to\nindividual-level potential outcomes rather than ATEs only, allowing for comparisons of performance\non CATE and ITE estimation."}, {"title": "4.2.2 Conditional and individual treatment effects", "content": "shows \\( R^2 \\) values (clipped at zero) across all methods that provide point estimates for CATEs.\nWhen all covariates are observed, BART does significantly better explaining CATE variation, while\nDML and DR methods do as well at times but with a much lower average. However, CATE estimation\nbecomes much more challenging for all methods under hidden confounding. The difference in effect\nestimation difficulty between Llama-3-8b and GPT-2 is also more noticeable for CATEs than it was\nfor ATEs. Figures 9 and 10 in Appendix B show the same results in terms of PEHE (Precision in\nEstimating Heterogeneous Effects) [27], revealing that with no clipping, DR methods show large\noutliers with GPT-2, and NN methods show large outliers instead with Llama-3-8b.\nWhen the ITE varies due to any covariates not conditioned on in the CATE, the two quantities are\ndistinct. Thus, the Hidden Confounding setting for CATE estimation shows us how CATE estimators\nperform when the target is instead the ITE. In this setting, where estimation is difficult and no methods\nperform well, uncertainty is especially important.  shows empirical coverage results for all\nestimators that provide intervals. Empirical coverage is under nominal for all methods that target\nCATE in the setting with all covariates. Hidden confounding generally increases uncertainty, but\nthis does make coverage closer to nominal, especially for LinearDML and LinearDR. CQR remains\nat or above nominal coverage, but with much wider intervals than the other methods. Interestingly,\nBART for CATE achieves higher median coverage of the ITE than BART-ITE, but with a much larger\ntail of poor coverage. BART-ITE, by contrast, has much less variable coverage in the ITE setting,\n'All code to reproduce this benchmark is available at https://github.com/lbynum/\nsequence-driven-scms."}, {"title": "5 Auditing language models for (un)desirable causal effects", "content": "The same framework we use to generate causal\neffects and benchmark effect estimation methods\ncan allow us to inspect what causal information\nhas been encoded semantically in an LLM. Our\ncollection of breast cancer SD-SCMs are already\nset up to explore the effect of PD-L1 on chosen\ntherapy plans, while allowing us to marginalize\nout an important source of variability: phrasing.\nEssentially, this amounts to reverse engineering the\ndecision-making process of clinicians, as learned\nfrom whatever data the language model were trained on. Recall from Section 4.1 that there are nine\npossible outcomes we could explore in the breast cancer example.  shows one example\nwhere the two language models strongly disagree on what the causal effect is. The effect in this\ncase is the change in probability of choosing the therapy plan \u201cstart a regimen of trastuzumab and\npertuzumab\" (shown in standard deviation units). GPT-2 has encoded that on average, an increase in\nPD-L1 expression levels has neither a positive nor negative impact on whether or not this therapy will\nbe chosen. However, Llama-3-8b has encoded instead that an increase in PD-L1 always increases\nthe likelihood of this therapy plan. This discrepancy indicates that these two language models\nhave encoded two meaningfully different causal models. We believe this same procedure can\nunderpin more thorough auditing of LLMs for misinformation or discrimination, enabling, for\nexample, path-specific counterfactual fairness analysis [36, 10].\""}, {"title": "6 Conclusion and Future Work", "content": "In this work, we have introduced sequence-driven structural causal models (SD-SCMs) as a frame-\nwork for LLM-based data generation with controllable causal structure, allowing for observational,\ninterventional, and counterfactual data generation according to user-defined DAGs and LLM-defined\nstructural equations. We demonstrate an important use-case for SD-SCMs by creating a benchmark\nfor causal effect estimation. In this proof of concept, we focused on methods that assume observed\nconfounders, but there are many other settings to explore for effect estimation, such as instrumental\nvariables [1, 26]. Using SD-SCMs to additionally test causal discovery is of immediate interest, for\nexample, allowing us to test whether a structure learning method can identify whether one variable is\ncausally upstream or downstream of another [34]. Another significant area of future work is to use\nSD-SCMs or similar as a means of specifying causal structure over sequential data during learning\n[29]; rather than use an already-trained language model to generate effects, a model can be trained\nor fine-tuned to handle tasks that require causal reasoning, including complex confounding and\nsequential decision making.\nIn short, we believe SD-SCMs can serve as a stepping stone for any application that would benefit from\nsequential data with controllable causal structure, opening the door to several types of applications."}, {"title": "A Full description of the breast cancer SD-SCMs", "content": "The 14 covariates in the breast cancer SD-SCMs are defined generally below. For each covariate,\n10 different phrasings are considered, resulting in a sample space of \\( 10^{1"}]}