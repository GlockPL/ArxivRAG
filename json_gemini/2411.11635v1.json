{"title": "Review of Data-Driven Generative AI Models for Knowledge Extraction from Scientific Literature in Healthcare", "authors": ["Leon Kopitar", "Primoz Kocbek", "Lucija Gosak", "Gregor Stiglic"], "abstract": "This review examines the development of abstractive NLP-based text summarization approaches and compares them to existing techniques for extractive summarization. A brief history of text summarization from the 1950s to the introduction of pre-trained language models such as Bidirectional Encoder Representations from Transformer (BERT) and Generative Pre-training Transformers (GPT) are presented. In total, 60 studies were identified in PubMed and Web of Science, of which 29 were excluded and 24 were read and evaluated for eligibility, resulting in the use of seven studies for further analysis. This chapter also includes a section with examples including an example of a comparison between GPT-3 and state-of-the-art GPT-4 solutions in scientific text summarisation. Natural language processing has not yet reached its full potential in the generation of brief textual summaries. As there are acknowledged concerns that must be addressed, we can expect gradual introduction of such models in practise.", "sections": [{"title": "6.1 INTRODUCTION", "content": "Recent progress in Natural Language Processing (NLP) research using large pre-trained language models using Deep Neural Networks (DNN) where the number of parameters is in the order of 100 billion, commonly called Large Language Models (LLM), have pushed the limits of language understanding and text generation. Standard practice for such general-purpose models is to fine-tune them for task-specific downstream tasks, often via text prompts. This review presents a current overview of such models in healthcare with the"}, {"title": "6.1.1 Introduction to a brief history of text summarisation using NLP", "content": "The history of text summarization begins with the development of NLP. The origin of NLP is usually attributed to Machine Translation, which was used during the second world war to translate the English language to Russian and the other way around (Sreelekha et al. 2016).\nDuring the 1950s and 1960s, Luhn and Edmundson have been studying ways of automatically extracting documents using different approaches: the topic is hidden within initial sentences (e.g. in the first paragraph, immediately after sections such as \u201cIntroduction\u201d, \u201cPurpose\u201d, \u201cConclusions\u201d), topic sentences are located with the presence of pragmatic words (\u201csignificant\u201d, \u201cimpossible\u201d, \u201chardly\u201d) and location-wise, where the key information is hidden in the first and the last sentence of paragraphs (Luhn 1958; Edmundson 1969).\nThe rise of text summarization appeared in the 1980s when researchers perceived NLP as an opportunity for a topic for research. These days, a rule-based algorithm known as an importance evaluator was predominantly used in text summarization. The importance evaluator used two knowledge bases, the importance rule base that contained knowledge conveyed through IF-THEN rules, and \u201cencyclopedia\" containing domain-specific world knowledge constituted of a network of frames (Fum et al. 1986).\nLater, the development of text summarization progressed with the introduction of the Diversity-based approach in extractive summarization. This approach calculated the diversity of sentences and removed redundant sentences from the final abstract. Diversity was calculated using the K-means clustering algorithm with Minimum Description Length Principle (Nomoto and Matsumoto 2001).\nAnother way to extract sentences was to apply Graph-based ranking algorithms. These algorithms determine the importance of a node within a graph according to global information computed recursively from the entire graph. TextRank sentence extraction algorithm uses a similar principle, instead, the graph is built from natural language text and includes multiple or partial links, represented as the graph's edges, that are extracted from the text (Mihalcea 2004).\nIn 2009, Suanmali et al. (2009) introduced text summarization based on the general statistic method (GSM) and fuzzy logic method to decide on the importance of the text. This method consisted of four components: fuzzifier, inference engine, defuzzifier, and fuzzy knowledge base.\nA few years later, the encoding and decoding model was developed: Sequence-to-sequence (Seq2Seq) learning model. The architecture of Seq2Seq was composed of an encoder, a Recurrent neural network (RNN) with LSTM/GRU, and a decoder (RNN). The role of an encoder was to encode the input sequence into a single fixed-size vector, while the decoder yielded an output sequence based on the fixed-size vector. Performance issues with Seq2Seq arose when long and complex sentences were tried to be encoded into a single fixed-size vector (Sutskever et al. 2014). This was later resolved with an Attention Mechanism technique, which identified relevant input tokens, and elements of the text, by computing context vectors for all tokens in the input sentence for each token in the output sentence (Bahdanau et al. 2015). Note that tokens are basic unit in many LLM, usually defined as groups of characters and used to compute the length of text, for example in GPT models 1000 tokens represent approximately 750 words and a paragraph usually consists of around 35 tokens (OpenAI 2022).\nBased on the achieved improvements with the Attention Mechanism technique, Vaswani et al. (2017) proposed Transformer architecture, which uses self-attention that selectively focuses on parts of the input, and next, multi-head attention that deals with multiple parts simultaneously. Furthermore, it does not use recurrence or convolution and it relies on position-wise feed-forward network architecture (Vaswani et al. 2017).\nThe evolution of pre-trained language models began with the introduction of Bidirectional Encoder Representations from Transformer (BERT). BERT is based on transformer architecture, using Masked Language Modelling (MLM) and it is pre-trained on a vast amount of text data, such as articles, books, and other literature. Reportedly it is pre-trained on over 3.3 billion words (Devlin et al. 2019). Like other LLM that will be introduced later, BERT uses a transfer learning technique, specifically sequential transfer learning that consists of two steps: the pre-training stage, where NN is trained on general data, and fine-tuning stage where the model is trained on domain-specific task (Zhang et al. 2019; Devlin et al. 2019).\nIn the same year, the corporation OpenAI developed the first one in a series of Generative Pre-training Transformers (GPT), known as GPT version 1 (GPT-1) (Radford et al. 2018). A year later GPT-2 (with 1.5 billion parameters) (Alec et al. 2019), and GPT-3, in 2020 (Brown et al. 2020). GPT-3 is pre-trained on 100 times more parameters than its predecessor, precisely on 175 billion parameters (Brown et al. 2020).\nDuring the development of the LLM series by OpenAI, in 2020, Google's teams introduced the PEGASUS model that utilizes MLM together with gap sentence generation (GSG). Their model removes/masks important sentences from the input text and generates new sentences from the remaining sentences. PEGASUS generates great results with as many as 1000 examples (Zhang et al. 2020b).\nIn late 2022, ChatGPT, another variant of GPT was released. ChatGPT is used as a chatbot built on top of GPT3.5 and has the ability to provide short abstracts on demand (Ayd\u0131n and Karaarslan 2022; Jiao et al. 2023). It should be noted, that in the first quarter of 2023 GPT-4 was released, and although no specific details of the model are known (OpenAI 2023), but early indications do imply improvements in multiple downstream tasks compared to previous iterations."}, {"title": "6.2 METHODS", "content": "In this section, we introduce the basic terminology relevant to short summarization models and study selection for this review."}, {"title": "6.2.1 Extractive and abstractive summarization", "content": "Extractive summarization is a technique that extracts the most important sentences from the text and arranges them into a logical summary (Gupta and Gupta 2019) without generating any new sentences nor altering the original text. Abstractive summarization is a technique that summarises text by understanding the content and produces new or rephrased sentences of the original text. It is considered to be one of the more challenging tasks in NLP since it combines the understanding of long passages, information compression, and language generation (Zhang et al. 2020a). The aim of the abstractive summarization is to produce human-like summaries that successfully capture the meaning of the source text.\nChallenges will be addressed in the subsection \"2.3.4 Limitations and challenges of existing SOTA models\u201d."}, {"title": "6.2.2 Zero-shot", "content": "Text summarization is performed mainly with two types of meta-learning: zero-shot learning and few-shot learning. Zero-shot learning is associated with the process when learners can resolve tasks that have not been presented to the learner before (Kojima et al. 2022) and is solely based on the description of the tasks (Romera-Paredes and Torr 2015).\nOne study indicated that LLM, specifically GPT-3 performed well at zero-shot information extraction from clinical text despite not being trained specifically for the clinical domain (Agrawal et al. 2022)."}, {"title": "6.2.3 Few-shot learning", "content": "On the other hand, few-shot learning (Bataa and Wu 2020; Chintagunta et al. 2021), supplies the learner with unseen samples of data to learn a new task (Kojima et al. 2022). In scenarios when few-shot learning is utilized, it is often sufficient to apply only a couple of examples. Speaking of meta-learners, Brown et al. demonstrated that zero-, one-, and few-shot performance increases with the increase of model capacity, indicating that larger models are more accomplished (Brown et al. 2020).\nIn one of the previous studies, researchers showed that the few-shot GPT-3 model performs poorly in the biomedical domain, specifically, in experiments that deal with textual inference, estimating semantic similarity, question answering, and others (Moradi et al. 2021)."}, {"title": "6.2.4 Search strategy", "content": "Our general research question was to investigate to what extent LLM can be used to extract viable knowledge in healthcare settings. The following search engines were used for conducting a scoping review: PubMed, Web of Science, CINAHL/Medline. We limited the review to the studies that were conducted in the past five years (2017 - 2022). Studies were retrieved in December 2022 using the following search term:\n(((text OR abstract OR scientific literature OR scientific document) AND (summarization OR summarisation OR TLDR)) AND (large language models OR LLM OR NLP OR natural language processing) AND (healthcare OR health care OR primary care OR patient care OR nursing OR medical care))."}, {"title": "6.2.5 Study selection", "content": "The screening was carried out according to the guidelines of Arksey and O'Malley (2005). In the first step, duplicate studies were removed. The remaining studies were assessed by checking the title and abstract for the presence of words such as \"summari(s/z)ation\u201d, \u201cTLDR\u201d, terms that relate to healthcare, and phrases that resemble and belong to the natural language processing domain. Systematic and other reviews were included and reviewed regardless of the satisfaction of the above criteria. Included studies were then skimmed and"}, {"title": "6.3 RESULTS", "content": null}, {"title": "6.3.1 Search results", "content": "Following PRISMA diagram (Figure 2.1), a total of 60 studies were identified in PubMed (45) and Web of Science (15). After the removal of duplicated records, 54 studies underwent a screening process. During the initial step of screening, 29 studies were excluded since either the title or the content of the abstract did not cover desired topics described in the subsection \"Study selection\", and one study could not be retrieved. In the next step, 24 studies were fully read and assessed for eligibility, where seven reviews did not cover relevant topics, and eight studies reported only keyword/symptom identification or extraction without resulting in any form of text summaries. Additionally, two studies were excluded due to summarising connections in a form of tabular summarization or their identification. Thus, the selection process yielded seven studies that were relevant to the review process."}, {"title": "2.3.1.1 Characteristics of the included studies", "content": "All included studies were published as a scientific article either as a part of conference proceedings (4) or journals (3). One study was published in 2017, 2018 and in 2020, while five studies were published in 2022. Almost half of the included studies were conducted in the USA (42%), remaining studies were based in Europe (Table 2.1.). Four studies included extractive text summarization in their research, while three studies dealt with abstractive summarization."}, {"title": "6.3.2 Applications in the healthcare domain", "content": "In the study conducted by Sadeh-Sharvit et al. (2022), an Al therapy-specific platform (Eleos health platform) was proposed. This platform uses speech-to-text technology by translating what is being said into suggestions for progress note documentation that allows for generating structured sentence-by-sentence summaries (Sadeh-Sharvit et al. 2022).\nLee and Uppal developed a multi-indicator text summarization algorithm (MINTS) for generating extractive summaries from clinical and biomedical texts. MINTS uses random forests classifiers and other indications like sentence length, the position of input text, number and percentage of clinical/biomedical terms, normalized degree centrality, and overlap with global term frequency distribution calculated by using a similarity metric known as S\u00f8rensen-Dice-coefficient/index (DS) (Lee and Uppal 2020). Compared to other extractive summarization ranking methods (topicDist (Zhang et al. 2020a), LexRank (Erkan and Radev 2011), Position- based ranking, and Random selection), MINTS achieved the highest evaluation score. Additionally, they proposed a web-based interactive summarization and visualization tool (CERC), which uses indicative, as well as informative summarization techniques (Lee and Uppal 2020).\nGoldstein et al. proposed an automatic summarisation system (CliniText - CTXT) based on Intensive Care Unit (ICU) clinical knowledge base that generates summaries that can serve as a base for a discharge letter. The system was evaluated by relative completeness, several quality parameters (such as readability, comprehensiveness, supportive ...), functional performance, and correctness of clinicians' answers (Goldstein et al. 2017). CTXT generates summaries of longitudinal clinical data by combining knowledge-based temporal data abstraction, textual summarization, abduction, and NLP (Goldstein and Shahar 2013).\nIn another study, researchers presented an interactive approach to help clinicians to identify relevant text for inclusion in sign-out notes. Their tool, NLPReViz, uses extractive summarization for providing suggestions that can then be accepted or rejected by the user (Trivedi et al. 2018).\nIn 2022, Kocbek et al. explored the use and usefulness of several SOTA models, such as OpenAI Davinci, OpenAI Curie, Pegasus-XSum, and BART-SAMSum for generating short summaries from abstracts of healthcare scientific literatures (Kocbek et al. 2022). Later, the usefulness and relevance of generated abstractive short summaries were also evaluated in the use case scenario (Stiglic et al. 2022). Abstractive summaries from the Semantic Scholar platform (Hannousse 2021), which served as a baseline model, achieved the highest average score in the evaluation metrics (Naturalness, Quality, and Informativeness) (Kocbek et al. 2022).\nIn yet another research, authors introduced a keyword-based text summarization method for nursing entries in EHRs using model explainability (Reunamo et al. 2022)."}, {"title": "6.3.3 Evaluation of generated short summaries", "content": "In the selected studies, researchers carried out an evaluation of generated summaries using various techniques. Lee et al. evaluated generated texts using ROUGE metrics, in particular ROUGE-1, ROUGE-2, and ROUGE-SU4, comparing generated text with the reference text according to overlapping unigrams, bigrams, and n-grams with the maximum skip distance of 4, respectively. The ROUGE metric is one of the most widely evaluation techniques used in general (Lin and Och 2004; Yan et al. 2011).\nThe study of Reunamo et al. (2022) reported using a manual evaluation scale of four classes, that evaluates the quality of information being conveyed. Similarly, Kocbek et al. (2022) used \u201cInformativeness\u201d as one of the evaluation measures.\nMeasures like \"Readability\u201d, \u201cComprehensiveness\u201d, \u201cRelevance\u201d, \u201cNaturalness\" and \"Quality\", were applied only once through all seven studies, where \u201cReadability\" and \"Comprehensiveness\u201d were included in the study of Goldstein et al., while among others, \u201cNaturalness\" and \"Quality\" were present in the study of van der Lee et al. (2021) and Kocbek et al. (2022).\nThe assessment criteria can also encompass other quality measures, such as clinical course and continuity of care. The clinical course measures the effectiveness of the text in enabling clinicians to understand the events and experiences of the patient throughout their ICU stay, meanwhile the continuity of care measures the extent to which the information provided in the text supports the flowless continuation of care (Goldstein et al. 2017)."}, {"title": "6.3.4 Examples of evaluations of summaries generated by SOTA models", "content": "The first example is from Reunamo et al. (2022) and is called an Explainer Extractor, which extracts keywords and keyphrases through explainable AI, more specifically it combines a text classification model (bidirectional LSTM) with model explainability (local interpretable model-agnostic explanations (LIME)). An example can be seen in Figure 2.2, where thou it might not provide a full sentence, it does provide a reasonable explanation and could we have turned into sentences with advanced transformer-based methods. First part shows the keywords that were recognized as coefficients of LimeTextExplainer with the highest absolute values. Coefficients were then Z-standardized and weighted by paragraph score and coloured accordingly. In the second part, keywords that appeared consecutively were merged together to form keyphrases, and the keyphrase scores were determined based on the highest scores of the individual components. The last step shows the result after the removal of stop words and duplicate keywords."}, {"title": "6.3.5 Limitations and challenges of existing SOTA models", "content": "Models generate repetitive and generic summaries especially when input text is very long. Summarization of very long documents present a problem of delivering coherent summaries (Gupta and Siddiqui 2012). At times, models can be misled by metaphors or idiomatic expressions and change the meaning of text during summarization process. Another issue is that these models are pre-trained on English text data and might not perform well in text summarization in other than non-global languages (Aksenov et al. 2020).\nContent from images can be extracted using various methods, such as Optical Character Recognition (OCR), which extracts text from images (Indravadanbhai Patel et al. 2012), or image captioning models, which can generate descriptions of images. Features such as relationships or patterns are usually not interpretable and understandable by the model, which can affect the quality of the generated summary (Fan et al. 2018).\nOne of the main limitations is the scalability of human evaluations, both in terms of time and costs, but are still favoured in current trends and best practice guidelines (van der Lee et al. 2021)."}, {"title": "6.4 DISCUSSION", "content": "This review explores the findings and implications of the research conducted on the application of SOTA models in text summarization within the healthcare domain. It delves into the various approaches and techniques adopted by researchers.\nThis study observed that only a few studies utilized models, such as those that are based on GPT-3 for the purpose of text summarisation in a healthcare domain. Moreover, most included studies dealt with an extractive summarization. Consequently, there exists a lack of in-depth exploration of abstractive summarisation, that provides paraphrased, concise, and coherent summaries. Studies included in this review focus on generating summaries from clinical, biomedical texts, speech-to-text generated text, and nursing entries in EHRs. Based on that, there is a potential in research dealing with radiology reports, pathology reports, and similar. Another area, that can be focused on, is in establishing standardized evaluation metrics for assessing the effectiveness of methods for generating short summaries. Since some studies mentioned metrics such as readability, completeness, other naturalness, quality, and informativeness. Summary evaluation in healthcare lags behind novel evaluation approaches that are proposed for LLM. In general, studies evaluate summaries with limited, single aspects (fluency, readability ...) or multi-aspects but neglect their definition or even more relationship among aspects. Even more, such evaluations require time-consuming manual annotations of samples. These issues were divulged by Fu et al. (2023), who proposed a novel network GPTScore that utilizes NLP instructions (zero-shot instructions) and contextual learning to overcome multiple evaluation challenges simultaneously (Fu et al. 2023). Another approach to summary evaluation was introduced by Liu et al. They proposed G-Eval that uses LLMs with chain-of-thoughts (CoT) (Wei et al. 2022; Liu et al. 2023). It requires a prompt with the definition of the evaluation task and criteria which is a collection of intermediary instructions (CoT) generated by LLM that delineate the precise sequence of evaluation steps (Liu et al. 2023). The difference in evaluation scoring between these two is that GPTScore uses probability in text generation as an evaluation metric, whereas G-Eval performs the evaluation with a form-filling paradigm.\nThe use of NLP for short text generation has not reached its peak potential. Due to the known issues, it is still unclear when the need for human evaluation could be replaced by an LLM."}]}