{"title": "PROTECTING FEED-FORWARD NETWORKS FROM ADVERSARIAL\nATTACKS USING PREDICTIVE CODING", "authors": ["Ehsan Ganjidoost", "Jeff Orchard"], "abstract": "An adversarial example is a modified input image designed to cause a Machine Learning (ML) model\nto make a mistake; these perturbations are often invisible or subtle to human observers and highlight\nvulnerabilities in a model\u2019s ability to generalize from its training data. Several adversarial attacks can\ncreate such examples, each with a different perspective, effectiveness, and perceptibility of changes.\nConversely, defending against such adversarial attacks improves the robustness of ML models in\nimage processing and other domains of deep learning. Most defence mechanisms require either a\nlevel of model awareness, changes to the model, or access to a comprehensive set of adversarial\nexamples during training, which is impractical. Another option is to use an auxiliary model in a\npreprocessing manner without changing the primary model. This study presents a practical and\neffective solution \u2013 using predictive coding networks (PCnets) as an auxiliary step for adversarial\ndefence. By seamlessly integrating PCnets into feed-forward networks as a preprocessing step, we\nsubstantially bolster resilience to adversarial perturbations. Our experiments on MNIST and CIFAR10\ndemonstrate the remarkable effectiveness of PCnets in mitigating adversarial examples with about\n82% and 65% improvements in robustness, respectively. The PCnet, trained on a small subset of the\ndataset, leverages its generative nature to effectively counter adversarial efforts, reverting perturbed\nimages closer to their original forms. This innovative approach holds promise for enhancing the\nsecurity and reliability of neural network classifiers in the face of the escalating threat of adversarial\nattacks.", "sections": [{"title": "1 Introduction", "content": "An adversarial example is a modified input intended to cause a machine-learning model to make a mistake. The\nmodifications are often imperceptible or very subtle to human observers. However, predictive coding can reverse such\nalterations due to its perturbation resiliency, providing more robustness against such attacks. This defensive strategy\nagainst adversarial attacks includes generative mechanisms that revert the perturbed images to their original form.\nPredictive coding offers a theoretical framework to support such a defence.\nTo help understand this work, we will briefly discuss adversarial examples, including how to create and defend against\nthem. We will also mention the attack methods we used in our experiments and popular corresponding defence\nstrategies in subsections 1.1.1 and 1.1.2, respectively. We will introduce the predictive coding framework and its\nlearning algorithm in subsection 1.2. We then explain the experiment setups in section 2, and show the results in section\n3, which will be discussed in section 4. At the end, we will summarize our work and point to possible future venues in\nsections 5 and 6, respectively."}, {"title": "1.1 Adversarial Attacks and Defences", "content": "Unlike humans, who robustly interpret visual stimuli, artificial neural networks can be deceived by adversarial attacks\n(ATs), particularly perturbation attacks [1]. These attacks subtly alter an image to trick a well-trained trained feed-forward network (FFnet) used for classification tasks [2, 3]. One standard method to create an adversarial\nexample (AE) that causes the FFnet to misclassify the image as a specific target label is to find a perturbation that\nminimizes the loss function\n$\\argmin_{\\delta\\in \\Delta} l(F_\\theta(x + \\delta), y_t),$\nwhere:\n\u2022 x represents the image,\n\u2022 8 is the perturbation needed to deceive the FFnet when applied to the image, and $||\\delta||_\\infty < \\epsilon$ is enforced.\n\u2022 $\\Delta$ represents allowable perturbations that are visually indistinguishable to humans.\n\u2022 $y_t$ is the 1-hot vector (i.e., $e_t$) corresponding to the target label t.\n\u2022 $F_\\theta(\\cdot)$ is the FFnet model, (i.e., $F_\\theta : x \\rightarrow y \\in \\mathbb{R}^k$, where k is the number of classes),\n\u2022 $\\theta$ represents all parameters defining the model.\n\u2022 l is the cross-entropy loss function.\nThis optimization can be achieved iteratively [4]. Alternatively, instead of deceiving the model by a specific target label,\nthe optimization can be solved for an untargeted attack by maximizing the loss\n$\\argmax_{\\delta\\in \\Delta(x)} l(F_\\theta(x + \\delta), y),$\nfor the given pair (x, y), which can be achieved in one step using methods like the fast gradient sign method (FGSM)\n[5], or other approaches [6, 7, 8, 9, 10, 11]. When testing a well-trained FFnet MNIST classifier (with an accuracy\nof approximately 98%) against FGSM-generated AEs (with \u0454 ~ 0.78%), the adversarial success rate is about 41%.\nTo defend against ATs, augmenting the training dataset with AEs can improve the classifier's resilience, achieving an\naccuracy of approximately 94%. Alternatively, a min-max approach to directly counteract AEs can enhance robustness\nwithin specific perturbation limits [7, 12]."}, {"title": "1.1.1 Creating Adversarial Examples", "content": "Adversarial examples (AEs) are crucial for assessing and improving the robustness of machine learning (ML) models,\nespecially in deep learning. In image data, creating AEs involves making imperceptible changes to the original image to\nmislead the model into misclassifying the image. Various methods are available to generate such AEs, particularly for\ndeceiving deep neural networks. Below, we briefly discuss the main gradient-based techniques relevant to our work\nwhile noting that there are other techniques to create AEs [8, 9, 10, 11].\n\u2022 Fast Gradient Sign Method (FGSM) modifies the input image by computing the loss gradient for the input\nimage and then making a small step in the opposite direction to increase the loss [5].\n\u2022 Basic Iterative Method (BIM), an extension of FGSM, takes multiple small steps while adjusting the direction\nof the perturbation at each step [6].\n\u2022 Projected Gradient Descent (PGD) modifies the input image in multiple iterations with a constraint on the\nperturbation's size. PGD starts from a random point within a small ball (i.e., e-ball) around the original image\nand performs a series of gradient descent steps to maximize the prediction error while ensuring the perturbation\nis smaller than the specified \u20ac [7]."}, {"title": "1.1.2 Defending Against Adversarial Attacks", "content": "Although various adversarial attacks exist, some defence mechanisms attempt to protect ML models against such\nattacks. Here, we review defence strategies for each previously mentioned attack.\n\u2022 For FGSM and BIM/PGD: Adversarial training involves training the model using adversarial and clean\nexamples. It has been particularly effective against gradient-based attacks like FGSM and BIM. Gradient\nmasking attempts to hide or modify gradients so that they are less useful for generating adversarial examples.\nHowever, this method has often been criticized and can be circumvented [13].\n\u2022 For C&W Attack: Some defences estimate the likelihood that input is adversarial using auxiliary models or\nstatistical analyses [4]. Defensive distillation involves training a model to output softened probabilities of\nclasses, making it harder for an attacker to find gradients that can effectively manipulate the model's output\n[13].\nWhile these methods offer some protection against specific types of adversarial attacks, it is essential to note that there\nis no one-size-fits-all solution, and sophisticated or adaptive attackers can circumvent many defences. However, some\ndefence strategies come with a cost, and there is a trade-off between robustness and accuracy [12]. Continued research\nis crucial to improving the robustness of neural networks against these threats."}, {"title": "1.2 Predictive Coding", "content": "Computational neuroscience seeks to understand behavioural and cognitive phenomena at the level of individual neurons\nor networks of neurons. One approach to solving difficult problems, such as adversarial attacks, which do not seem\nto be a problem for the brain, is to explore biologically plausible perception models. The model we will be using is\npredictive coding (PC), a neural model capable of implementing error backpropagation in a biologically plausible\nmanner [18, 19, 20]."}, {"title": "1.2.1 Model Schema and The Learning Algorithms", "content": "The concept of predictive coding suggests that the brain works to minimize prediction error [21]. This model aims\nto improve overall predictions, and all neurons work towards this common objective. In a predictive coding network\n(PCnet), each neuron, or PC unit, consists of a value (v) and an error node (8). These PC units are organized into layers,\nsimilar to artificial neural networks (ANNs), forming PCnets that learn by adjusting parameters to refine predictions\nand reduce errors between layers.\nFor example, in a PCnet, layer i contains vectors vi and \u025bi. Vector vi predicts the values of the\nnext layer, vi-1, using prediction weights Mi-1. The resulting error, Ei\u22121, is then communicated back via correction\nweights Wi-1, allowing vi to improve its predictions."}, {"title": "2 Methodology", "content": "We conducted experiments to evaluate the effectiveness of our defence strategy against adversarial examples (AEs). We\nalso measured the classification performance of the PCnet and FFnets classifier on AEs. First, we trained the models\nto classify the MNIST and CIFAR10 datasets. Then, we subjected the FFnets, including a fully connected network\n(FCnet) and convolutional neural network (CNN), to various adversarial attacks. The experimental setup is illustrated in\nfigure 4, and the classifiers' architectures are detailed in table 2. Note that \"FFnet\" refers to FCnet and CNN in general,\nand we will repeat the same experiments for both architectures."}, {"title": "3 Results", "content": "We trained PCnet on a random subset of the MNIST dataset and trained FFnet (FCnet and CNN models) on the entire\nMNIST dataset. The test accuracy for PCnet, FCnet, and CNN was 74.22%, 92.82%, and 96.50%, respectively. We\nthen partitioned the dataset based on the PCnet classifier: classified (X) and misclassified (X). We used the AT module\nto generate AEs targeting nine incorrect labels per image using the C&W attack and non-targeted AEs using FGSM,\nBIM, and PGD attacks. To measure the accuracy, we collected successful AEs that fooled the FFnet and passed them\nthrough three phases, as shown in figure 4. We conducted the same experiments on the CIFAR10 dataset.\nWe described the classifier models for the MNIST and CIFAR10 datasets in table 2. The ratio of successful AEs relative\nto the attempted attacks for classified and misclassified data for both sets of experiments on the MNIST and CIFAR10\ndatasets are shown in table 4 for all three phases: FFnet(Z), PCnet(Z), FFnet(P).\nAs shown in the figure 6, from top to bottom, through these phases: (1) we cherry-picked AEs, Z, that were 100%\nsuccessful against FCnet; (2) PCnet classified 70% of those AEs, and adjusted Z into P in unclamped mode; (3) this\ntime, FCnet classified 81% of adjusted AEs, P. Similarly, on 100% successful AEs attacking the CNN model, PCnet\nclassified 68% of them, and the CNN model classified 84% of adjusted such AEs. On average, the number of successful\nAEs per class for FCnet is higher than for the CNN model, as the FCnet is simpler and easier to fool. At the same time,\nthe CNN model benefits more from adjusted AEs than FCnet. Similar trends were observed for the CIFAR10 dataset, as\nshown in figure 7, where the CNN model outperformed the FCnet.\nWe also applied the same AT methods to generate \u017d: AEs from the misclassified subset X on attacking FFnet. Then\nwe passed Z through our multi-phase experiment (as shown in figure 4). As PCnet misclassified X in the first place,\nwe did not expect PCnet to perform well on corresponding AEs. However, the results showed that perturbing the AEs"}, {"title": "4 Discussion", "content": "Now, we are going to scrutinize the results. When creating Adversarial Examples (AEs) from a random subset of each\nclass, some classes are more vulnerable than others. Moreover, the vulnerability level for each class might not be\nthe same across different models. For example, as shown in figures 9, 10, about 90% of attacks on FCnet and CNN\nfor class '1' were successful, while the numbers for class \u20180' are less than 60% and 30%, respectively. However, on\naverage, FCnet is more deceivable than CNN. Nevertheless, PCnet can classify many AEs created to deceive either\nmodel, regardless of targeted or non-targeted attacks (as shown in figure 6)."}, {"title": "5 Summary", "content": "Our observations on both correctly classified X and misclassified X data (as shown in figures 6, 8) suggest that using\nthe PCnet reversion process benefits FFnet. These results hold not only for the inputs that the PCnet correctly classified\nbut also for the inputs that the PCnet misclassified.\nThe PCnet's generative nature results in lower classification accuracy. Improving this accuracy would presumably\nfurther improve the robustness of the PCnet reversion process, which means more AEs can be reverted.\nThe PCnet performs well in classifying AEs, but its primary purpose is to enhance the classification performance of\nFFnets without directly accessing them. The modifications made to the PCnet serve as a preprocessing step, ultimately\nimproving the classification accuracy of FFnets."}, {"title": "6 Future Work", "content": "The study highlights the effectiveness of PCnets in defending against adversarial attacks, but there are potential gaps to\nconsider.\n1. Scalability: We conducted the experiments on the MNIST and CIFAR10 datasets, which are relatively simple\ncompared to some high-dimensional real-world datasets. Further research is needed to assess the scalability of\nPCnets to larger and more complex datasets.\n2. Generalization: The study focuses on specific architectures (FC and CNN) and datasets. However, to ensure\ntheir applicability in diverse scenarios, it is essential to investigate the generalization of PCnets across various\nnetwork architectures and datasets.\n3. Adversarial Strength: The study focuses on four major gradient-based adversarial attacks. Further exploration\nis needed to assess the effectiveness of PCnets against more sophisticated adversarial attacks.\n4. Complex PCnet models: For our experiments, we constructed PCnets in a fully connected architecture and\ntrained them by clamping the training dataset without using any specific regularization technique. However, a\nmore complex architecture on PCnet might excel in defending against more complex attacks or datasets.\n5. Computational Overhead: We trained PCnets using only 10% random subset of each dataset; however,\ntraining PCnets was a bottleneck in our experiment pipelines, so the computational cost of PCnet as a defence\nmechanism needs to be evaluated, particularly in real-time or resource-constrained environments.\n6. Ineffectiveness: PCnet effectively defends against C&W attacks. However, more analytical experiments are\nneeded to show why it is ineffective against FGSM, PGD, and BIM attacks.\nAddressing these gaps will further validate the feasibility and effectiveness of PCnets as a defence mechanism against\nadversarial attacks on neural network classifiers."}]}