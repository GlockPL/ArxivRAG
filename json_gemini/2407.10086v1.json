{"title": "Rapid Biomedical Research Classification: The Pandemic PACT Advanced Categorisation Engine", "authors": ["Omid Rohanian", "Mohammadmahdi Nouriborji", "Olena Seminog", "Rodrigo Furst", "Thomas Mendy", "Shanthi Levanita", "Zaharat Kadri-Alab", "Nusrat Jabin", "Daniela Toale", "Georgina Humphreys", "Emilia Antonio", "Adrian Bucher", "Alice Norton", "David A. Clifton"], "abstract": "This paper introduces the Pandemic PACT Advanced Categorisation Engine (PPACE) along with its associated dataset. PPACE is a fine-tuned model developed to automatically classify research abstracts from funded biomedical projects according to WHO-aligned research priorities. This task is crucial for monitoring research trends and identifying gaps in global health preparedness and response. Our approach builds on human-annotated projects, which are allocated one or more categories from a predefined list. A large language model is then used to generate 'rationales' explaining the reasoning behind these annotations. This augmented data, comprising expert annotations and rationales, is subsequently used to fine-tune a smaller, more efficient model. Developed as part of the Pandemic PACT project, which aims to track and analyse research funding and clinical evidence for a wide range of diseases with outbreak potential, PPACE supports informed decision-making by research funders, policymakers, and independent researchers. We introduce and release both the trained model\u00b9 and the instruction-based dataset used for its training\u00b2. Our evaluation shows that PPACE significantly outperforms its baselines. The release of PPACE and its associated dataset offers valuable resources for researchers in multilabel biomedical document classification and supports advancements in aligning biomedical research with key global health priorities.", "sections": [{"title": "1 Introduction", "content": "The surveillance and monitoring of emerging and re-emerging pathogens are vital to global health security. Infectious diseases with the potential to cause pandemics represent a significant threat to public health, economies, and societies worldwide. Efficiently tracking these threats and coordinating research efforts is essential to mitigate their impact. Traditional approaches to research funding and co-ordination during health crises, such as the COVID-19 pandemic, often exhibit limitations including slow activation of research projects, duplication of efforts, and fragmented funding landscapes. These issues highlight the need for improved systems to manage and analyse research activities (Carroll et al., 2021; McLean et al., 2022). \n The rapid identification and response to infectious disease threats require a well-coordinated and systematic approach to research funding and project tracking. Accurate categorisation and analysis of research projects are crucial for identifying trends, understanding research gaps, and ensuring that resources are allocated effectively. This task is inherently complex, given the diverse and interdisciplinary nature of biomedical research (Seminog et al., 2024). \n Artificial intelligence (AI), particularly large language models (LLMs), offers a promising solution to enhance the efficiency and accuracy of research categorisation. LLMs can be fine-tuned to assist in the classification of research abstracts, providing valuable support to human annotators. These models not only streamline the categorisation pro- cess but also generate rationales for their decisions, adding a layer of interpretability and transparency that is essential for gaining the trust of researchers and policymakers. \n This paper introduces the Pandemic PACT Advanced Categorisation Engine (PPACE), a fine-tuned LLM designed to classify biomedical research abstracts according to WHO-aligned research priorities. PPACE leverages human-annotated data and employs generative AI to produce rationales for each classification. By automating the categorisation process, PPACE aims to enhance the monitoring of research trends and the identification of critical gaps in global health preparedness and response. \n In the remainder of the paper, we first provide an overview of the literature on the use of LLMS in biomedical document classification (Section 2). Next, we introduce the Pandemic PACT project which this work builds on, and describe the details of the dataset and the annotation procedure involved in its creation (Section 3). Section 4 will describe the methodology in finetuning the PPACE model, and finally, in Section 5, we present the results and conclude the paper. The contributions of this work are as follows: \n1. We contribute to the task of biomedical document classification by publicly releasing a carefully annotated dataset of research projects (each project containing a title and a PubMed-style abstract) gathered as part of the Pandemic PACT project and further pre-processed to include rationales generated by a 70B LLM. The augmented dataset is formatted as an instruction-based dataset and can be used to train similar models by the research community. \n2. We fine-tune and publicly release an 8B model trained on the aforementioned dataset and make the model weights available publicly. \n3. We perform a range of analyses on the dataset to shed light on the complexities of the data and run a number of evaluations to ensure that the model outperforms the baseline."}, {"title": "2 Biomedical Document Classification and the Use of LLMs", "content": "Biomedical document classification is an active area of research that has attracted considerable attention in recent years (Laza et al., 2011). The PubMed 200k RCT dataset, for instance, focuses on classifying different sections of randomized controlled trial abstracts into categories like objectives, methods, results, and conclusions (Dernoncourt and Lee, 2017). Another notable task in this area is the Hallmarks of Cancer (HoC), which presents a multi-label classification challenge and aims to identify key cancer-related research themes from PubMed abstracts (Baker et al., 2015). The Lit-Covid dataset (Chen et al., 2020; Jimenez Gutierrez et al., 2020), which comprises over 30,000 COVID-19-related articles, each annotated with one or more topics relevant to the pandemic, is another major biomedical document classification benchmark that has been studied in the literature. Automated topic annotation tasks like this can significantly reduce the manual curation burden during pandemics, and the present work can be considered a more generalized effort to automate the classification of biomedical literature into research themes of interest. Additional datasets relevant to biomedical document classification include the BioCreative Corpus III (BC3, Arighi et al., 2011) and TREC (Hersh et al., 2006). Behera et al. (2019) provides an overview of this task and the various deep learning algorithms to address it. \n Large Language Models (LLMs) have become ubiquitous in various text processing and classification tasks, including document classification. Their ability to handle a wide range of text-related tasks makes them particularly appealing for numerous applications. LLMs can be instructed to perform specific tasks via few-shot examples or through fine-tuning with detailed instructions. For biomedical researchers, generative LLMs are especially valuable because they can be interfaced with using natural human language, facilitating more intuitive and effective interactions. \n SciFive (Phan et al., 2021) is a domain-specific T5 model that has been pretrained to address a number of biomedical tasks, including document classification. Rohanian et al. (2023) is the first attempt to use generative language models to address classical biomedical text processing tasks like HoC via instruction tuning. Chen et al. (2023) and Tian et al. (2024) have studied the use of LLMs in a number of biomedical text processing tasks, including document classification, although the focus is mostly on closed-source frozen models like GPT-4. \n Various techniques are observed in the literature regarding the use of LLMs when addressing this task. Several studies use parameter-efficient fine-tuning methods (Hu et al., 2021; Taylor et al., 2024; Jiang et al., 2024) which have become very prevalent due to the ease of use and faster training time they offer. Our work not only employs instruction tuning and LoRA as a parameter-efficient fine-tuning technique, but also draws inspiration from Hsieh et al. (2023) in that it utilises 'rationales' generated by a larger model to augment the labelled dataset and then fine-tunes a smaller one trained on the expanded dataset."}, {"title": "3 Dataset Overview and Sources", "content": "The Pandemic Preparedness Analytical Capacity and Funding Tracking Programme (Pandemic PACT) operates under the auspices of the Global Research Collaboration for Infectious Disease Preparedness (GloPID-R, Norton et al., 2020) at the University of Oxford's Pandemic Sciences Institute. This initiative aims to enhance global response capabilities by tracking and analysing research funding for diseases with pandemic potential and other significant public health threats. By aligning with the WHO priority diseases, Pandemic PACT focuses on dynamic data collection and rigorous analysis to inform critical policy and funding decisions across the health system and public health domains (Norton et al., 2024; Seminog et al., 2024)."}, {"title": "3.2 The Pandemic PACT Funding Tracker", "content": "The Pandemic PACT Funding Tracker is an integral component of this initiative, collecting detailed information on research grants from GloPID-R and UKCDR members since January 2020 and has since expanded to include a much broader set of international funding bodies. This tool maps the alignment of funding to critical research categories and priorities, displaying the data through an interactive dashboard that visualises funding trends"}, {"title": "3.3 Annotation Procedure", "content": "The Pandemic PACT database expands upon the previous database co-developed by UKCDR and GloPID-R as part of the COVID-19 Research Coordination and Learning initiative (COVID CIRCLE1). Pandemic PACT funding data on other diseases and additional COVID-19 research projects are collected either through direct data provision by funders (using a standardised template) or by scraping funder websites. The scraping process is based on search terms including disease-specific keywords, acronyms, virus, and virus family names. For the detailed search protocol, inclusion criteria, and transformation of the COVID CIRCLE data into the new standardised schema, see Seminog et al. (2024). Only grants that include a minimum level of essential information are included, such as grant award or start date, publication date, funder name, grant ID or another form of identifier, and grant title. The data encompass funding information from January 2020 onwards for the relevant diseases. \n While all Pandemic PACT search terms used are in English, it does not exclude grants in other languages. If the search returns any relevant grants in foreign languages, their title and abstracts are translated using Google Translate and then included in the database. All collected data is stored in its original format as retrieved from the funding source, with basic data cleaning procedures performed to remove special characters from data in textual format. \n All collected data are reviewed by a team of trained researchers from broad public health backgrounds to determine their relevance, classified against a research categorisation framework developed under Pandemic PACT, and assigned other relevant tags using manual annotation. The number of team members has varied over time, starting with three, increasing to ten before the launch of the Pandemic PACT tracker, then decreasing to six, and currently stabilising at four. The team size is subject to change based on project needs. Over the first months of the project, Pandemic PACT developed a standard approach to training and preparing new members of the data collection team through a series of training steps. First, they were exposed to tutorials of training material and videos that explained how to interpret data and submit contributions through the online interface. After that, data coders were expected to attend a weekly all-contributor meeting, at which point they started being included in the regular coding allocation. These meetings were used for expanding comprehension of the coding schema and processes, facilitating a collective consensus on interpretations of codes, and effectively probing coding disagreements. \n After data is entered, they are marked as 'unverified' in the back-end database portal used by the Pandemic PACT if any issues arise or if the coder hesitates on how to code them. This flags them for the review process. Conversely, entries are marked as 'complete' if no concerns are raised. To ensure data reliability, Pandemic PACT mandates peer review of all new data by at least two annotators, ensuring each grant undergoes scrutiny and confirmation by an independent coder. In cases of inter-annotator disagreement, discussions are held to reach joint decisions. Alternatively, judgments from a designated coder, such as the Principal Investigator or a more experienced researcher, take precedence over others. Going forward, Pandemic PACT plans to implement a comprehensive approach where initial coding is performed by an LLM, followed by manual verification and final annotation."}, {"title": "3.4 Dataset Description", "content": "Our study employs a carefully selected sample from the Pandemic PACT database. Each row in the data represents a funded research project and includes a title and an abstract which provides a concise description of each project's aims, methods, and potential impacts. The data is randomly divided into an approximate 80/20 split with the number of rows shown in Table 1. \n To gain insights into the training set, we analysed the lengths of the project titles and abstracts as well as the distribution of research categories. The statistics for the lengths of project titles and abstracts are presented in Table 3. During finetuning, to keep the computation manageable, the abstract length is capped at 512 tokens."}, {"title": "3.5 Combined Label Distributions", "content": "We also examined the combined label distributions to understand the most common combinations of research categories assigned to the projects. Figure 2 shows the top 12 most frequent label clusters, indicating that combinations such as Pathogen & Clinical Characterisation in Humans (1, 4), and Policies for Public Health & Secondary Impacts of Disease (9, 10) are prevalent."}, {"title": "3.6 Label Correlations", "content": "Understanding the correlations between different research categories provides insights into interdisciplinary research trends visible in the training set. These correlations highlight how different fields of study intersect, helping us identify areas where models might struggle or easily pick up patterns. \n Apart from the significant correlations mentioned in Figure 2, we also found notable intersections between Epidemiological Studies (Category 3) and Clinical Characterisation in Humans (Category 4), and between Pathogen (Category 1) and Therapeutics Research (Category 6). A properly trained model should be able to detect these patterns while also recognising instances where these correlations do not hold. \n The conditional probabilities for the top five most frequent pairs of research categories are shown in Table 4. For example, the highest conditional probability for the combination (1, 3) is for label 4 at 0.39, and for (1, 4), the highest is label 3 at 0.17. These findings suggest that certain third-label correlations exist, but they are not overwhelmingly strong. \n The heatmap in Figure 3 provides a visual representation of the strength of correlations between different research categories."}, {"title": "4 Methodology", "content": "In this work, we initially use a manually labelled dataset to generate \u2018rationales' for the labels using a Llama-3 70B model. These rationales explain why each category label is chosen, pinpointing the reason by referencing the abstract. We add these rationales to the labels, constructing an expanded dataset that includes both the prompt and the rationales. For details on the prompt template used, refer to Appendix Section B. \n We subsequently explore adapting a smaller model to the classification task using this dataset. The fine-tuned model is expected not only to predict the labels but also to explain its reasoning. Based on insights from McCoy et al. (2023) regarding the limitations of autoregressive language models, we structured the prompt such that the rationales are provided first before the model determines the category labels. This approach ensures that errors from irrelevant categories are not propagated back into the model's outputs. Given the substantial computing resources required to train the full weights of the 8B model, we explored the use of efficient fine-tuning via LoRA (Section 4.2). \n We have chosen to restrict the experiments and the baselines to a single decoder-only transformer. As of this writing, an LLM with around 10 billion parameters is considered relatively small but can be performant enough to rival state-of-the-art. A representative LLM with a good starting performance on this complex task provides us with a foundation to improve upon while avoiding potential saturation. Additionally, a very large model like Llama-3 70B would be impractical for independent researchers to fine-tune or utilise on less powerful machines. Section 4.1 details how this LLM was chosen."}, {"title": "4.1 Adjudicating between Outputs of Candidate LLMS", "content": "In order to fine-tune a smaller model using the augmented dataset of human annotations and rationales generated by the LLama 3-70B, we evaluated several candidate models. The best-performing ones were the Mixtral-8x7B Instruct and the Meta-Llama-3-8B-Instruct. We empirically found that the outputs produced by these models were significantly more relevant compared to other models of similar or smaller size such as Phi3. \n To determine which model to fine-tune for developing PPACE, we conducted a detailed comparison. We randomly selected 10 projects from the dataset and used a few-shot learning template (Section B) to obtain inferences from both Mixtral and Llama3 models, utilising them as frozen language models. Each model's output was then recorded and passed to GPT-4o for adjudication. GPT-4o was tasked with evaluating the responses, comparing them to the available human judgments, and providing a verdict favouring one model output over the other or declaring a tie. \n This adjudication process involved a thorough analysis of each model's outputs against human labels and was further verified by our annotators. The results showed that both models performed well, but Llama-3 was deemed the better model by a small margin. One of the key advantages of Llama-3 was that extracting the output labels from the generated text was significantly easier compared to Mixtral, which occasionally deviated from the specified format, complicating the extraction process. Additionally, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. In other words, Llama-3 also has the advantage of being smaller and more performant for this task. This consistency and ease of label extraction made Llama-3 the preferred choice for fine-tuning PPACE."}, {"title": "4.2 Low-Rank Adaptation (LoRA)", "content": "Low-Rank Adaptation (LoRA, Hu et al., 2021) is a parameter-efficient approach for adapting large pre-trained models without modifying the original weights. This method can be particularly beneficial for maintaining memory efficiency and reducing computational overhead. LoRA introduces two small matrices, A and B, into the transformer architecture, which project the high-dimensional parameter space into a lower-dimensional space and back. The original transformation in a transformer layer, typically a matrix multiplication involving a weight matrix W, is modified as follows: \n $W' = W + \\frac{r}{\\alpha} AB$   (1) \n Here, W is the original weight matrix of the transformer, and W' is the adapted weight matrix. The matrices A and B are of dimensions d\u00d7 r and r \u00d7 d respectively, where d is the dimensionality of W and r is much smaller than d, finally \u03b1 is a hyperparameter for adjusting the learning rate for the trainable weights. This low-rank structure ensures that the number of additional parameters introduced by A and B is significantly lower than the number of parameters in W, leading to substantial savings in terms of memory and computational resources. \n The low-rank projection effectively captures the essential transformations needed for task-specific adaptation while preserving the original model's capabilities. This approach is particularly advantageous when computational resources are limited or when the adaptability needs to be achieved with minimal disturbance to the original model structure, as often required in real-world applications where both efficiency and performance are critical. In practical applications, LoRA has shown to enable effective fine-tuning of large models on specific tasks without the need for extensive retraining of the original parameters."}, {"title": "4.3 Training Strategy and Hyperparameters", "content": "We used Supervised Finetuning (SFT) to adapt the Llama-3 8B model to the classification task. The model was trained for 2 epochs on the training samples using 8 A100 GPUs, with a batch size of 1 per GPU and 4 gradient accumulation steps. The LORA modules were placed in all trainable layers of the self-attention and MLP layers of the Llama model. \n During initial experiments, we found that focusing the loss calculation on the completion tokens (related to the explanations and categories) and ignoring the loss for the prompt tokens improved the model's performance. This approach was more effective than using the autoregressive language modelling loss on all tokens, including the prompt. \n To maximise model performance, we used a beam search decoding strategy with 4 beams for the smaller models, which appeared to markedly improve generation quality and output structure. However, due to computational constraints, beam search was not employed for the 70B version. \n The hyperparameters used for training the model in our experiments are listed in Table 5."}, {"title": "5 Results and Analysis", "content": "Table 6 shows the results of the baseline Llama3-8B model, the larger Llama3-70b model, and the proposed PPACE model, respectively. As can be seen, PPACE outperforms the baselines on all the metrics with the exception of macro-averaged recall. \n The finetuned model demonstrates significant improvements in F1 scores across most categories, indicating the effectiveness of the finetuning process. Notable improvements are seen in categories like 'Infection Prevention & Control', 'Therapeutics', and 'Vaccines', where the finetuned model's precision and F1 scores show substantial gains. Categories with low representation in the dataset, such as 'Capacity Strengthening' and 'Health Systems', see mixed results, with some performance metrics slightly decreasing. This suggests that while finetuning enhances the model's ability to generalise, it may still struggle with categories that have very few examples. Figure 4 shows the changes in F-scores between the fine-tuned and the base model across the different categories, sorted from the least frequent to the most frequent as seen in the test set. \n Overall, the finetuned model generally performs better in terms of precision compared to recall. This trend indicates that the model has become more conservative in its predictions, leading to fewer false positives but potentially more false negatives. Categories like 'Vaccines', 'Public Health' and 'Secondary Impacts' show remarkable improvements in precision and F1 scores, demonstrating PPACE's enhanced capability to identify relevant instances within these categories. The dramatic increase in all metrics for 'Public Health' is particularly noteworthy, with the F-score jumping from 0 to 0.65. However, there remains room for improvement, especially for categories with minimal representation in the dataset. The results highlight the strengths of the finetuning approach while also pointing out the difficulty of the task."}, {"title": "6 Conclusion", "content": "In this work we introduced the Pandemic PACT Advanced Categorisation Engine model or PPACE, a fine-tuned 8B language model for biomedical research classification as part of the Pandemic PACT initiative. PPACE is capable of accurately categorising research abstracts according to WHO-aligned priorities. This tool can be a valuable asset for identifying biomedical research trends and gaps in a multilabel classification scenario. The model was built on a robust foundation of human-annotated data, enhanced with LLM-generated rationales, ensuring that the model's predictions are not only accurate but also interpretable. The use of efficient fine-tuning has enabled us to adapt the model effectively while maintaining computational efficiency. \n Our evaluation demonstrated that PPACE outperforms its baselines, offering significant improvements in the context of multilabel classification. The model and the instruction-based dataset used for training are released oublicly, providing a valuable resource for the research community. These contributions facilitate further advancements in aligning biomedical research with critical global health priorities. \n Looking ahead, the integration of LLMs in the annotation process promises to streamline data collection and categorisation, potentially reducing the burden on human annotators and improving the scalability of such initiatives. The evolution of PPACE can play a crucial role in enhancing the efficiency and effectiveness of global health research, ultimately contributing to better preparedness for future outbreaks."}, {"title": "Limitations", "content": "Our work has several limitations. First, the dataset used for training and evaluation, while extensive, may not encompass the full diversity of biomedical research projects globally, potentially limiting the generalisability of our model for prospective analyses of research in new emerging pathogens. Additionally, the research categories might become outdated at some point, requiring updates and subsequent retraining of the model. Second, some projects can be categorised in different ways, introducing a degree of subjectivity in certain assignments. While our use of human-annotated expert labels aims to minimise this issue, it does not completely eliminate it. \n Furthermore, despite using efficient fine-tuning methods like LoRA, the 8-billion parameter model is still sizable. Researchers with limited computational resources would need reliable GPUs for inference, as running solely on CPU can be very slow. Future iterations of this work will aim to fine-tune smaller models to improve accessibility. \n Lastly, we did not experiment heavily with advanced prompting techniques or invest significant time in crafting the best possible prompts. There is potential for further improvement in the reported results for the frozen language models through optimised prompts, which might narrow the performance gap with the fine-tuned model."}, {"title": "Appendix", "content": "The following is the list of categories used in the prompt along with corresponding explanations of what each category entails: \n1. Pathogen: Natural History, Transmission, and Diagnostics: Development of diagnostic tools, understanding pathogen morphology, genomics, and genotyping, studying immunity, using disease models, and assessing the environmental stability of pathogens. \n2. Animal and Environmental Research & Research on Diseases Vectors: Animal sources, transmission routes, vector biology, and control strategies for vectors. \n3. Epidemiological Studies: Research on disease transmission dynamics, susceptibility, control measure effectiveness, and disease mapping through surveillance and reporting. \n4. Clinical Characterisation and Management in Humans: Prognostic factors for disease severity, disease pathogenesis, supportive care and management, long-term health consequences, and clinical trials for disease management. \n5. Infection Prevention and Control: Research on community restriction measures, barriers and PPE, infection control in healthcare settings, and measures at the human-animal interface. \n6. Therapeutics Research, Development, and Implementation: Pre-clinical studies for therapeutic development, clinical trials for therapeutic safety and efficacy, development of prophylactic treatments, logistics and supply chain management for therapeutics, clinical trial design for therapeutics, and research on adverse events related to therapeutic administration. \n7. Vaccines Research, Development, and Implementation: Pre-clinical studies for vaccine development, clinical trials for vaccine safety and efficacy, logistics and distribution strategies for vaccines, vaccine design and administration, clinical trial design for vaccines, research on adverse events related to immunisation, and characterisation of vaccine-induced immunity. \n8. Research to Inform Ethical Issues: Ethical considerations in research design, ethical issues in public health measures, ethical clinical decision-making, ethical resource allocation, ethical governance, and ethical considerations in social determinants of health. \n9. Policies for Public Health, Disease Control and Community Resilience: Approaches to public health interventions, community engagement, communication and infodemic management, vaccine/therapeutic hesitancy, and policy research and interventions. \n10. Secondary Impacts of Disease, Response, and Control Measures: Indirect health impacts, social impacts, economic impacts, and other secondary impacts such as environmental effects, food security, and infrastructure. \n11. Health Systems Research: Health service delivery, health financing, access to medicines and technologies, health information systems, health leadership and governance, and health workforce management. \n12. Capacity Strengthening: Individual capacity building, institutional capacity strengthening, systemic/environmental components, and cross-cutting activities across all levels of capacity building."}, {"title": "B Prompt Design for Model Inference", "content": "To generate high-quality inferences from the models during the few-shot learning experiments, a carefully constructed prompt was employed. This prompt was designed based on feedback from expert human annotators to ensure precision and discourage spurious associations. Below is the detailed explanation and the code used for generating the prompt. \n The prompt includes guidelines and examples to steer the model towards accurate classification. The guidelines ensure that the model focuses on relevant categories and avoids unnecessary implications or speculative guesses. The examples provided demonstrate the expected structure and reasoning for categorisation. \n The {guideline} variable in the prompt is replaced with the list of research categories mentioned at A, prefaced with the following line: \nWe have a project in the area of biomedical research which we want to classify in terms of the research priorities it relates to. We have 12 possible research priorities and a project can be mapped to 1 or more of these priorities. The following is a guide on what each of these 12 categories are alongside the specific areas that they cover. \nIn the case of fine-tuning, in order to ensure the model is actually learning from the labels rather than relying on extra information in the prompt, we do not use the few-shot examples and omit the extra 5 notes as well. Every other detail in the template above stays the same when finetuning."}, {"title": "C Distribution of Categories in the Test set", "content": ""}]}