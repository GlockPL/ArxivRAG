{"title": "Forecasting of Bitcoin Prices Using Hashrate Features: Wavelet and Deep Stacking approach", "authors": ["Ramin Mousa", "Meysam Afrookhteh", "Hooman Khaloo", "Amir Ali Bengari", "Gholamreza Heidary"], "abstract": "Digital currencies have become popular in the last decade due to their non-\ndependency and decentralized nature. The price of these currencies has seen\na lot of fluctuations at times, which has increased the need for prediction. As\ntheir most popular, Bitcoin (BTC) has become a research hotspot. The main\nchallenge and trend of digital currencies, especially BTC, is price fluctuations,\nwhich require studying the basic price prediction model. This research presents\na classification and regression model based on stack deep learning that uses a\nwavelet to remove noise to predict movements and prices of BTC at different time\nintervals. The proposed model based on the stacking technique uses models based\non deep learning, especially neural networks and transformers, for one, seven,\nthirty and ninety-day forecasting. Three feature selection models, Chi2, RFE\nand Embedded, were also applied to the data in the pre-processing stage. The\nclassification model achieved 63% accuracy for predicting the next day and 64%,\n67% and 82% for predicting the seventh, thirty and ninety days, respectively. For\ndaily price forecasting, the percentage error was reduced to 0.58, while the error\nranged from 2.72% to 2.85% for seven- to ninety-day horizons. These results show\nthat the proposed model performed better than other models in the literature.", "sections": [{"title": "1 Introduction", "content": "Bitcoin is one of the first decentralized digital currencies that banks and individuals do\nnot have any involvement in its control, which has the advantage that no institution\ncan claim its ownership. BTC is highly popular as a pioneer in digital currencies.\nThis money was created in 2009 but became customary in 2017. The price of BTC\nhas risen and fallen several times throughout the year. Many economic institutions\nare interested in predicting the price of Bitcoin. This issue is significant for existing\nor potential investors and for government structures. Therefore, the demand for BTC\nprice prediction mechanism is high[1].\nPrice volatility is essential to intangible digital assets, especially digital currencies.\nThe value of BTC is different from the value of stocks[3]. Different algorithms are\nused in the market for price prediction. However, the factors affecting Bitcoin are\ndifferent, so it is necessary to predict the value of BTC in order to make the right\ninvestment decisions. Unlike the stock market, gold, oil, etc., the price of BTC does\nnot depend on business events or the intervention of any government. Therefore, it\nbecomes necessary to use machine learning and deep learning technology to predict\nits price value. Business with machine learning (ML) and artificial intelligence (AI)\nhas gained attention and attention in the past few years.\nMuch research has been done to predict the changes in BTC. Most researchers\nuse the historical information of BTC to predict its future price using artificial neural\nnetworks (ANN) and ML methods[4][5][6]. Another group of methods uses sentiment\nanalysis and makes predictions using positive or negative comments [7] [8]. Several\nresearchers have also identified influential people in the social network and use their\nopinions as a criterion for prediction[9][10].\nIn this article, two issues are considered for BTC. In the first problem, classification\naims to increase and decrease based on price. For this purpose, the price of BTC was\nwritten into two classes, 0 and 1, based on the following relationship:\nClass = $\\begin{cases}\n1, & \\text{if Nextprice > Currentprice} \\\\\n0, & \\text{if Nextprice <= Currentprice}\n\\end{cases}$ (1)\nIn the second problem, the real price of BTC was considered the forecast value, and\nin this case, the problem was considered a regression problem. Considering these two\nissues, a model based on deep neural networks and stacking technique was considered.\nDifferent models make predictions based on input data after training. This prediction\nis used in a higher-level layer as a selection based on the majority vote in classification\nproblems or averaging in regression problems. Previously reviewed literature has tried\nto use technical data or social networks, while hash rate data is considered the proposed\nmodel's raw input. This research aims to provide a suitable approach for medium-\nterm price forecasting and high/low forecasting for forecast horizons from 7 days to 90\ndays. Also, this research focuses on predicting the daily closing price and short-term\nprices prediction. Our results show that our stacking model are better than the latest\nliterature on daily forecasting and price up/down classification."}, {"title": "2 Related Works", "content": "Cryptocurrencies have attracted considerable awareness, and Bitcoin has been the\nmost notable in terms of price and academic and non-academic research. Since BTC,\nlike other digital currencies, has much volatility in its price, predicting its price is\nchallenging. Although several researchers used traditional economic and statistical\nmethods to discover the driving variables of Bitcoin price, the progress in develop-\ning prediction models for decision-making tools in investment techniques is still in\nthe early stages [11]. Many cryptocurrency price forecasting methods cover objectives\nsuch as forecasting in a one-step(one-hour, one-day, or one-week) approach that can\nbe performed through time series analysis, neural networks(NNs), and machine learn-\ning(ML) algorithms. However, it is important to understand the price trend of BTC\nin the long and middle term. In this section, we examine ML and DL approaches in\npredicting cryptocurrencies.\nIn [12], the authors have turned the primary research focus to extracting the\nfactors or features that affect the BTC price. This research used the combined pre-\ndiction model with double support vector regression as the primary model. Also, the\nXGBoost and random forest algorithms were considered feature selection algorithms.\nA hybrid model with dual support vector regression (TWSVR), support vector regres-\nsion (SVR) and least squares support vector regression (LSSVR) were used to predict\nBitcoin price. To optimize the models, the authors used WOA and PSO algorithms.\nThe models used six combinations: WOA-SVR, WOA-LSSVR, WOA-TWSVR, PSO-\nSVR, PSO-LSSVR, and PSO-TWSVR. The best-reported results combined PSO-\nTWSVR with RF feature selection, achieving R2 = 0.924, MAE=0.032, MSE=0.002,\nRMSE=0.044, and MAPE=0.0441.\nMany types of research have shown a comparison between the approaches available\nin the literature. In [13], the authors have discussed the performance of two LSTM\nand ARIMA models for the short-term prediction of Bitcoin. For this purpose, they\nhave two educational examples from 12/21/2020 0:00 to 12/19/2021 20:30 (using a 24-\nhour time format and date format is MM/DD/YYYY) and a test that starts from 12/\n19/2021 20:40 to 12/21/2021 16: were considered for cross-validation of prediction find-\nings. The ARIMA approach reached MAE=837.77, MAPE=1.79%, RMSE=940.40,\nand Accuracy=98.21%. On the other hand, the LSTM approach reached MAE=126.97,\nMAPE=0.27%, RMSE=151.95, and Accuracy=99.73%, and the results got better.\nUnlike ARIMA, which could only track the Bitcoin price trend, the LSTM model could\npredict direction and value over time. This research demonstrates the stable capacity\nof LSTM to predict Bitcoin price volatility despite the complexity of ARIMA.\nAdvanced deep-learning approaches [14] were used to predict Ether currency. The\nauthors used LSTM, GRU, TCN (Temporal Convolution Network), Hybrid LSTM-\nGRU, Hybrid LSTM-TCN, Hybrid GRU-TCN, and Ensemble that combined the\npredictions of the LSTM, Hybrid LSTM-GRU and Hybrid LSTM-TCN models were\nused for short-term and long-term forecasting. They concluded that similar factors\ninfluence the daily and weekly forecasts. These factors included EMA and MACD\ntechnical indicators, Bitcoin price and Ethereum search volume index on Google. The\nintroduced approaches were used in both regression and classification modes for daily"}, {"title": "3 NETWORK DESCRIPTION", "content": "An overview of the proposed approach deep stacking model is given in Figure 1.\nThis work presents a deep stack learning framework for time series using a prediction\nscheme based on cumulative and average learning to take deep and consistent features\nfor price prediction one step further. The framework consists of two steps:\n1. Data preprocessing using wavelet transform, which is applied to decompose the\nBitcoin price time series and to remove noise\n2. A deep Stacked model to predict overall results.\n3.1 Wavelet transform for time series\nThis study uses Wavelet transform to remove data noise because it can handle\nnon-stationary time series data. The key feature of the wavelet can analyze the fre-\nquency components of time series simultaneously with time compared to the Fourier\ntransform. This study uses the Haar function as the basis wavelet function because it\ncan decompose the time series into the time and frequency domains and significantly"}, {"title": "3.2 Stacked model", "content": "The general purpose of this model is to provide a learning representation that can use\nseveral models in presenting the final results. Each model predicts the input data and\nthe final output is obtained from the average prediction of the models. For example,\nif P\u2081 is the prediction result of model 1, P2 is the prediction result of model 2, and Pn\nis the prediction result of model n, the overall output will be as follows:\nPend = $\\frac{P_1+P_2+P_3 + ... + P_n}{n}$ (14)\nWhile the majority vote was used for the classification model.\nStacked models have been reviewed in the result section because comprehensive\nresearch has been done on traditional machine learning models, the review of these\nmodels has been omitted here, and only deep learning approaches have been used.\nSome of the most important models used in this study have been discussed in the\nfollowing. These models are used in a stacked form, as shown in Figure 1.\nWe have three types of RNN: Long short-term memory (LSTM) cells [29], gated\nrecurrent units (GRUs) [33] and independent RNN(IndRNN). Various versions of these\nunits exist in the literature, so we briefly summarize the ones used here.\n\u2022 LSTM: A standard LSTM cell contains three gates: the forgetting gate(ft), which\ndetermines how much of the previous data will be forgotten; An input gate(it)\nthat evaluates the information to be written to the cell memory. Furthermore,\nthe output gate(ot) that determines how to calculate the output from the current\ninformation[29]:\nft = \u03c3(Weifxt + Ueifht\u22121 + beif) (15)\nit = \u03c3(Weiixt + Ueiiht\u22121 + beii) (16)\nOt = \u03c3(Weioxt + Ueioht\u22121 + bei.) (17)\nCt = ft \u00b0 Ct\u22121 + it \u00b0 (Weicxt + Ueicht\u22121 + beic) (18)\nht = Ot \u00a9 T(Ct) (19)\nwhere n size of input, m size of cell state and output, xt input vector(time\nt, size n * 1), ft forget gate vector(m * 1), it input gate vector(m * 1),\nOt output gate vector (size=m * 1), ht output vector (size=m * 1), ct cell\nstate vector(size=m * 1), [Weif, Weii, Weio, Weic] input gate weight matrices\n(size=m * n), [Ueif, Ueii,Ueio, Ueic] output gate weight matrices (size=m * m),"}, {"title": "3.3 Transformer", "content": "In this section, we will first introduce the transformer network. The linear transformer\n(LT) [30] is a variant of the well-known transformer architecture widely used for NLP\nand other sequence modelling tasks. The primary difference between the linear trans-\nformer and the baseline transformer is in the way the mechanism of attention is applied\nto a sequence."}, {"title": "3.3.1 Input embedding", "content": "RNN-based models use a time pattern with a repeating structure, while the trans-\nformer uses a point-attention mechanism, and timestamps act as local location context.\nTo obtain long-range dependencies, global information such as hierarchical times-\nstamps is required. First, we project the scalar field xti to the vector d model-dim with\nan LSTM network. Second, we use fixed-position embedding to represent the local\ntexture."}, {"title": "4 MATERIAL", "content": "Here, we briefly describe the preparatory material we used for developing the Deep\nStack architecture. In particular, we focus on the BTC dataset used for evaluating our\nsystem and the basic tools used for building our model."}, {"title": "4.1 The Bitcoin Dataset", "content": "To predict the price of Bitcoin, in Figure3, based on the literature review, a model is\npresented that includes the following steps:\n\u2022 Dataset collection: The data set used in this research includes BTC hash rate\ninformation, which is discussed in detail in the data collection section.\n\u2022 Pre-processing: This step is usually the pre-processing of the hash rate data,\nwhich includes the extraction of indicator features and feature selection algorithms."}, {"title": "4.1.1 Preprocessing", "content": "In preprocessing, the linear interpolation method imputes missing value items as much\nas possible. 20% of the data was kept for validation, and 80% was used for training.\nOn the other hand, the Isolation Forest method algorithm [36] was used to control\noutliers."}, {"title": "4.1.2 Feature Selection", "content": "Feature selection (FS) have become an essential part of the learning in machine and\ndeep models for dealing with high-dimensional features. Selecting suitable features can\nimprove the inductive learner in learning speed, generalization capacity, and model\nsimplicity[37]. Dimensionality reduction methods are often divided into FS and feature\nextraction (FE) [38]. On the one hand, feature extraction methods reduce dimensional-\nity by combining key features. Hence, they can construct a set of new features that are\nusually more compact and distinct. These methods are preferred in applications such\nas image analysis, image processing and information retrieval because model accuracy\nis more important than its interpretability [38]. The principles of FS will be explained\nbelow.\nFS is an important part of preprocessing in data-driven systems that improves the\nperformance of learning models. In the FS process, features are repeatedly extracted\nand pruned by different approaches after extraction. First, feature importance is deter-\nmined using an ensemble method based on a random decision forest. In the second\nstep, it examines the reduced feature set for multicollinearity and cross-correlation.\nThe subset of the resulting features has relatively high importance with low cross-\ncorrelation values and no multicollinearity. FS is repeated for each of the three data\ncollection intervals. This extraction method is called the Embedded feature. This\napproach was used with two other FS approaches: Chi2 and RFE. In the following,\nwe will examine two other approaches to feature selection:"}, {"title": "5 EVALUATION", "content": "The deep stack approach discussed in Section III has been evaluated by adopting the\nBTC data set. Here, we describe the implemented validation procedure and we discuss\nthe obtained results."}, {"title": "5.1 Evaluation Procedure", "content": "The model validation procedure used 80% to 20% adjustment for training and testing\ndata. For this purpose, the proposed model was compared with several basic models,\nwhich are listed below:\n\u2022 ANN[2]: The neural network considered by the authors includes hyperparameters:\nOptimizer=Adam\n2(128,128) Hidden layers(neurons)\nlearning rate(LR)=0.08\n500 Epoch\nBatch size(BS)=64\nActivation Function(AF)=Relu\nLoss function (LF)=logcosh\nThe basic article used This network in two regression and classification modes.\n\u2022 SANN)[2]: In this approach, 5 ANN networks were considered with the settings\nmentioned in the ANN approach. SANN consists of 5 individual ANNs that are\nused to train a larger ANN model. Individual models are trained using training data\nwith 5-fold cross-validation; each model is trained with the same configuration in\na separate layer. Since ANNs have random initial weights, each trained ANN gets\ndifferent weights, and this advantage enables them to learn their differences well.\n[2] used this network in two modes : regression and classification.\n\u2022 SVM[2]: This algorithm is a supervised ML model that operates based on the\nidea of separating points using a hyperplane, and in fact, its primary goal is to\nmaximize the margin. In SVM, kernels can be linear or non-linear depending on the\ndata, including radial basis function (RBF), tangent hyperbola, and polynomial.\nThis algorithm can provide predictions with a low error rate for small data sets\nwithout much training. The [2] considers this approach with Gaussian RBF kernel\nfor classification and regression problems.\n\u2022 LSTM [2]: This approach is an RNN network that uses four gates to learn long\nsequences. In the previous section, RNN approaches were discussed. This approach\nis used in both regression and classification modes, and in this research, its regression\nmode was used depending on the type of labels."}, {"title": "5.2 Price Classification", "content": "At first, classification issues were discussed. For this purpose, the price of the next step\nwas set to 1 in case of increase and 0 in case of decrease. Due to having two classes,\nnetwork architectures were considered binary classifications. Table ?? shows the results\nobtained from different feature selection approaches along with the deep stack model.\n+ values indicate the offset of the results in different execution rounds. In interval I,\nthe Chi2 feature selection approach reached an accuracy of 62\u00b11, the highest accuracy\namong the tested models. This approach also reached F1 score = 0.73\u00b10.02 and\nAUC = 0.54\u00b10.02. RFE and Embedded approaches worked the same in this interval.\nThese two approaches were able to achieve accuracy = 1 \u00b1 61, F1 = 0.74 \u00b1 0.01\nand AUC = 0.55 \u00b1 0.01. In interval II, the Embedded approach recorded higher\nresults. This approach was able to achieve accuracy = 1 \u00b1 62, F1 = 0.72 \u00b1 0.02, and\nAUC = 0.55 \u00b10.01. Chi2 and RFE approaches also achieved an accuracy of 60 \u00b1 1\nand 61 \u00b1 1, respectively. In interval III, the best result was obtained by the Chi2\napproach. This approach was able to achieve accuracy = 57\u00b11, F1 = 0.53\u00b10.01 and\nAUC = 0.50\u00b10.01. The two RFE and Embedded approaches achieved 53\u00b11 accuracy\nin these data. The forecasts were slightly different in the 7, 30, and 90-day forecasts. In\n7 days, the RFE approach achieved accuracy = 66\u00b11, F1 = 0.62\u00b10.01 and AUC =\n0.65 \u00b1 0.01, the best result among the tested approaches. The two approaches, Chi2\nand Embedded in this interval, reached the accuracy 63\u00b11. In 30 days, the Embedded\napproach achieved the best result. This approach achieved accuracy = 68\u00b11, and two\nChi2 and RFE approaches achieved an accuracy of 66 \u00b11 and 67\u00b11, respectively. In\n90 days, the proposed approach recorded the best result on Chi2. This approach was\nable to achieve accuracy = 81 \u00b1 1, F1 = 0.77\u00b10.02, and AUC = 0.72 \u00b10.02.\nThe highest average accuracy(AVG%) was obtained by the Chi2 feature selection.\nThis approach was able to reach AVG=64.83. Also, in average F1 and AUC, this\napproach had the highest performance and could reach Average F1=0.67 and Average\nAUC=0.62."}, {"title": "5.3 Price Prediction", "content": "In the regression problems, it was tried to consider the price of BTC as a pre-\ndictor. For this purpose, only one neuron was considered in the last layer of the\npre-institutional network. Table ?? shows the results obtained from the proposed\napproach using feature selection approaches. In interval I, the Chi2 feature selection\napproach reached MAE = 1.21\u00b10.01, RMSE = 1.85\u00b10.02, and MA\u03a1\u0395 = 0.50\u00b10.02,\nwhile the RFE approach reached MAE = 1.31 \u00b1 0.02, RMSE = 1.89 \u00b1 0.02, and\n\u041c\u0410\u0420\u0415 = 0.58\u00b10.02, and the Embedded approach also reached MAE = 1.28\u00b10.03,\nRMSE = 1.87 \u00b1 0.02, AND MAPE = 0.58\u00b10.02. In interval II, the Chi2 approach"}, {"title": "5.4 Comparison with base models", "content": "Table 9 shows the results of the classification models for the six investigated intervals.\nIn the interval I, deep stack performed best in the Acc evaluation criterion and reached\naccuracy=62\u00b11. In this interval, the 1D-CNN+IndRNN approach also obtained bet-\nter results than other comparative approaches and achieved accuracy=61. In Interval\nII, SANN and Transformer achieved 65% accuracy, the highest in this interval. In\ninterval III, the 1DCNN+IndRNN approach achieved accuracy=62, which was the\nhighest accuracy. For 7, 30 and 90 day forecasting, the proposed deep stack approach\nreached the highest accuracy. This approach achieved an accuracy of 63\u00b11,66\u00b11 and"}, {"title": "6 Error Analysis", "content": "In the error analysis, we first deal with the errors of the classification models.\nFigure 4 shows the Accuracy, F1-Score and AUC values for feature selection models."}, {"title": "7 Conclusion", "content": "BTC price modelling is done in two general ways: 1)price increase and decrease, which\nis a classification problem(binary classification), and 2) real price, which is a regression\nproblem. This paper presents a deep stack model that can solve price forecasting prob-\nlems with a very low error rate. However, the classification problem that was a 2-class"}]}