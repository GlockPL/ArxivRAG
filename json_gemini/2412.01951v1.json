{"title": "Self-Improvement in Language Models: The Sharpening Mechanism", "authors": ["Audrey Huang", "Adam Block", "Dylan J. Foster", "Dhruv Rohatgi", "Cyril Zhang", "Max Simchowitz", "Jordan T. Ash", "Akshay Krishnamurthy"], "abstract": "Recent work in language modeling has raised the possibility of self-improvement, where a language\nmodels evaluates and refines its own generations to achieve higher performance without external feedback.\nIt is impossible for this self-improvement to create information that is not already in the model, so why\nshould we expect that this will lead to improved capabilities?\nWe offer a new perspective on the capabilities of self-improvement through a lens we refer to as\nsharpening. Motivated by the observation that language models are often better at verifying response\nquality than they are at generating correct responses, we formalize self-improvement as using the model\nitself as a verifier during post-training in order to \"sharpen\" the model to one placing large mass on\nhigh-quality sequences, thereby amortizing the expensive inference-time computation of generating good\nsequences. We begin by introducing a new statistical framework for sharpening in which the learner\naims to sharpen a pre-trained base policy via sample access, and establish fundamental limits. Then,\nwe analyze two natural families of self-improvement algorithms based on SFT and RLHF. We find\nthat (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage,\nbut (ii) the RLHF-based approach can improve over SFT-based self-improvement by leveraging online\nexploration, bypassing the need for coverage. Finally, we empirically validate the sharpening mechanism\nvia inference-time and amortization experiments. We view these findings as a starting point toward a\nfoundational understanding that can guide the design and evaluation of self-improvement algorithms.", "sections": [{"title": "Introduction", "content": "Contemporary language models are remarkably proficient on a wide range of natural language tasks (Brown\net al., 2020; Ouyang et al., 2022; Touvron et al., 2023; OpenAI, 2023; Google, 2023), but inherit shortcomings\nof the data on which they were trained. A fundamental challenge is to achieve better performance than what\nis directly induced by the distribution of available, human-generated training data. To this end, recent work\n(Huang et al., 2022; Wang et al., 2022; Bai et al., 2022b; Pang et al., 2023; Yuan et al., 2024) has raised\nthe possibility of \u201cself-improvement,\" where a model\u2014typically through forms of self-play or self-training\nin which the model critiques its own generations-learns to improve on its own, without external feedback.\nThis phenomenon is somewhat counterintuitive; at first glance it would seem to disagree with the well-known\ndata-processing inequality (Cover, 1999), which implies that no form of self-training should be able to\ncreate information not already in the model. This motivates the question of why we should expect such\nsupervision-free interventions will lead to stronger reasoning and planning capabilities.\nA dominant hypothesis for why improvement without external feedback might be possible is that models\ncontain \"hidden knowledge\" (Hinton et al., 2015) that is difficult to access. Self-improvement, rather than\ncreating knowledge from nothing, is a means of extracting and distilling this knowledge into a more accessible\nform, and thus is a computational phenomenon rather than a statistical one. While there is a growing body"}, {"title": "1.1 Our Perspective: The Sharpening Mechanism", "content": "In this paper, we posit a potential source of hidden knowledge, and offer a formal perspective on how to\nextract it. Our starting point is the widely observed phenomenon that language models are often better at\nverifying whether responses are correct than they are at generating correct responses (Huang et al., 2022;\nWang et al., 2022; Bai et al., 2022b; Pang et al., 2023; Yuan et al., 2024). This gap may be explained by\nthe theory of computational complexity, which suggests that generating high-quality responses can be less\ncomputationally tractable than verification (Cook, 1971; Levin, 1973; Karp, 1972). In autoregressive language\nmodeling, computing the most likely response for a given prompt is NP-hard in the worst case (Appendix E),\nwhereas the model's likelihood for a given response can be easily evaluated.\nWe view self-improvement as any attempt to narrow this gap, i.e., use the model as its own verifier to improve\ngeneration and sharpen the model toward high-quality responses. Formally, consider a learner with access\nto a base model $\\pi_{\\text{base}}: \\mathcal{X} \\rightarrow \\Delta(\\mathcal{Y})$ representing a conditional distribution that maps a prompt $x \\in \\mathcal{X}$ to a\ndistribution over responses (i.e., $\\pi_{\\text{base}}(y | x)$ is the probability that the model generates the response y given the\nprompt x). We posit that $\\pi_{\\text{base}}$ has already been trained in some manner (e.g., through next-token prediction\nor additional post-training steps such as SFT or RLHF), with the key feature being that $\\pi_{\\text{base}}$ is a good verifier,\nas measured by some self-reward function $r_{\\text{self}}(y | x; \\pi_{\\text{base}})$ measuring model certainty. The self-reward\nfunction is derived purely from the base model $\\pi_{\\text{base}}$, without external supervision or feedback. Examples\ninclude normalized and/or regularized sequence likelihood (Meister et al., 2020), models-as-judges (Zheng et al.,\n2024; Yuan et al., 2024; Wu et al., 2024a; Wang et al., 2024), and model confidence (Wang and Zhou, 2024)."}, {"title": "Sharpening", "content": "We refer to sharpening as any process that tilts $\\pi_{\\text{base}}$ toward responses that are more certain\nin the sense that they enjoy greater self-reward $r_{\\text{self}}$. That is, a sharpened model $\\pi$ is one that\n(approximately) maximizes the self-reward:\n$\\hat{\\pi}(x) \\approx \\underset{y \\in \\mathcal{Y}}{\\text{arg max}} r_{\\text{self}}(y | x; \\pi_{\\text{base}}).$ (1)"}, {"title": "1.2 Contributions", "content": "We initiate the theoretical study of self-improvement via the sharpening mechanism. We disentangle the\nchoice of self-reward from the algorithms used to optimize it, and aim to understand: (i) When and how\ndoes self-training achieve sharpening? (ii) What are the fundamental limits for self-training algorithms?"}, {"title": "Algorithms for sharpening (Section 2)", "content": "The starting point for our work is to consider two natural\nfamilies of self-improvement algorithms based on supervised fine-tuning (SFT) and reinforcement learning\n(RL/RLHF), respectively, SFT-Sharpening and RLHF-Sharpening. Both algorithms amortize the sharpening\nobjective (1) into a dedicated post-training/fine-tuning phase:\n\u2022 SFT-Sharpening filters responses where the self-reward $r_{\\text{self}}(y | x; \\pi_{\\text{base}})$ is large and fine-tunes on the\nresulting dataset, invoking common SFT pipelines (Amini et al., 2024; Sessa et al., 2024; Gui et al.,\n2024; Pace et al., 2024).\n\u2022 RLHF-Sharpening directly applies reinforcement learning techniques (e.g., PPO (Schulman et al., 2017)\nor DPO (Rafailov et al., 2023)) to optimize the self-reward function $r_{\\text{self}}(y | x; \\pi_{\\text{base}})$.\nIn the remainder of the paper, we introduce a theoretical framework to analyze the performance of these\nalgorithms, and validate our findings empirically. Our main contributions are as follows."}, {"title": "Maximum-likelihood sharpening objective (Section 3.1)", "content": "As a concrete proposal for one source of\nhidden knowledge, we focus on self-rewards defined by the model's sequence-level log-probabilities:\n$r_{\\text{self}}(y | x; \\pi_{\\text{base}}) := \\log \\pi_{\\text{base}}(y | x)$ (2)\nThis is a stylized self-reward function, which offers perhaps the simplest objective for self-improvement\nin the absence of external feedback (i.e., purely supervision-free), yet also connects self-improvement to a\nrich body of theoretical computer science literature on computational trade-offs for optimization (inference)\nversus sampling (Appendix B). We view Eq. (2) as a clean and minimal objective that reveals the interplay\nbetween hidden knowledge, computational bottlenecks, and self-improvement in generative model. In spite\nof its simplicity, we show empirically that maximum-likelihood sharpening is already sufficient to achieve\nnon-trivial performance gains over greedy decoding on a range of reasoning tasks with several language models;"}, {"title": "A statistical framework for sharpening (Sections 3.2 and 3.3)", "content": "Though the goal of sharpening is\ncomputational in nature, we recast self-training according to the maximum-likelihood sharpening objective\nEq. (2) as a statistical problem where we aim to produce a model approximating (1) using a polynomial\nnumber of (i) sample prompts $x \\sim \\mu$, (ii) sampling queries of the form $y \\sim \\pi_{\\text{base}}(x)$, and (iii) likelihood\nevaluations of the form $\\pi_{\\text{base}} (y | x)$. Evaluating the efficiency of the algorithm through the number of such\nqueries, this abstraction offers a natural way to evaluate the performance of self-improvement/sharpening\nalgorithms and establish fundamental limits and minimax optimality, similar to the role of information-based\ncomplexity in optimization (Nemirovski et al., 1983; Traub et al., 1988; Raginsky and Rakhlin, 2011; Agarwal\net al., 2012), statistical query complexity in computational learning theory (Blum et al., 1994; Kearns, 1998;\nFeldman, 2012, 2017), and query complexity more broadly. We use our framework to prove new lower bounds\nand fundamental limits which highlight the importance of the base model's coverage (that is, probability\nmass placed on high-quality responses)."}, {"title": "Analysis of sharpening algorithms (Section 4)", "content": "Within our statistical framework for maximum-\nlikelihood sharpening, we show that SFT-Sharpening and RLHF-Sharpening provably converge to sharpened\nmodels, establishing several results:\n\u2022 Optimality of SFT-Sharpening. We show that SFT-Sharpening succeeds at learning a sharpened\nmodel whenever $\\pi_{\\text{base}}$ has sufficient coverage, and is minimax optimal in a worst-case sense. Perhaps\nsurprisingly, we show that a novel variant based on adaptive sampling can bypass this lower bound.\n\u2022 Benefits of RLHF-Sharpening. We show that RLHF-Sharpening also succeeds at learning a sharpened\nmodel and achieves similar performance to SFT-Sharpening when $\\pi_{\\text{base}}$ has sufficient coverage. However,\nwe show that this algorithm can bypass the need for coverage\u2014improving over SFT-Sharpening-by\nleveraging deliberate exploration of the response space."}, {"title": "Empirical investigation (Section 5)", "content": "We empirically explore the extent to which our theoretical\nframework can aid language models in a variety of tasks. We first consider three choices of self-reward,\nincluding maximum-likelihood sharpening, and sharpen via a practical approximation, inference-time best-of-\nN sampling: given a prompt $x \\in \\mathcal{X}$, we draw N responses $Y_1,\\ldots,Y_N \\sim \\pi_{\\text{base}}(\\cdot | x)$ and return the response\n$y = \\text{arg max}_{Y_i} r_{\\text{self}}(Y_i | x)$; this is equivalent to Stiennon et al. (2020); Gao et al. (2023); Yang et al. (2024)\nand is a popular approach in modern deployments. We consider an extensive list of model-dataset pairs and\nfind that sharpening, even with the stylized maximum-likelihood self-reward, often improves performance\nover greedy decoding. We then implement one of our algorithms, SFT-Sharpening, on a subset of these\nmodel-dataset pairs and observe a significant positive effect on performance, indicating that sharpening can\nindeed be amortized. An overview of our inference-time experiments can be found in Figure 1."}, {"title": "1.3 Related Work", "content": "Our work is most directly related to a growing body of empirical research that studies self-training for language\nmodels in a supervision-free setting with no external feedback (Huang et al., 2022; Wang et al., 2022; Bai et al.,\n2022b; Pang et al., 2023; Yuan et al., 2024). The specific algorithms for self-improvement/sharpening we study\ncan be viewed as applications of standard alignment algorithms (Amini et al., 2024; Sessa et al., 2024; Gui\net al., 2024; Pace et al., 2024; Christiano et al., 2017; Bai et al., 2022a; Ouyang et al., 2022; Rafailov et al., 2023)\nwith a specific choice of reward function. However, the maximum likelihood sharpening objective (2) used for\nour theoretical results has been relatively unexplored within the alignment and self-improvement literature.\nOn the theoretical side, current understanding of self-training is limited. One line of work, focusing on the\nself-distillation objective (Hinton et al., 2015) for classification and regression, aims to provide convergence"}, {"title": "2 Sharpening Algorithms for Self-Improvement", "content": "This section introduces the two families of self-improvement algorithms for sharpening that we study. Going\nforward, we omit the dependence of $r_{\\text{self}}$ on $\\pi_{\\text{base}}$ when it is clear from context. We use the notation\n$\\text{arg max}_{\\pi \\in \\Pi}$ or $\\text{arg min}_{\\pi \\in \\Pi}$ to denote exact optimization over a user-specified model class II for theoretical\nresults (Agarwal et al., 2019; Foster and Rakhlin, 2023); empirically, these operations can be implemented by\ntraining a neural network to low loss."}, {"title": "2.1 Self-Improvement through SFT: SFT-Sharpening", "content": "SFT-Sharpening filters responses for which the self-reward $r_{\\text{self}}(y | x)$ is large, and applies standard supervised\nfine-tuning on the resulting dataset (Amini et al., 2024; Sessa et al., 2024; Gui et al., 2024; Pace et al., 2024).\nThis can be viewed as amortizing inference-time sharpening via the effective-but-costly best-of-N sampling\napproach (Brown et al., 2024; Snell et al., 2024; Wu et al., 2024b). Concretely, suppose we have a collection\nof prompts $x_1,\\ldots,x_n$. For each prompt, we sample N responses $Y_{i,1},\\ldots, Y_{i,N} \\sim \\pi_{\\text{base}}(\\cdot|x_i)$, then compute\nthe best-of-N response $y_{\\text{BON}} \\leftarrow \\text{arg max}_{j \\in [N]}\\{r_{\\text{self}}(Y_{i,j} | x_i)\\}$, scoring via the model's self-reward function. We\ncompute the sharpened model via supervised fine-tuning on the best-of-N responses:\n$\\pi_{\\text{BON}} = \\underset{\\pi \\in \\Pi}{\\text{arg max}} \\sum_{i=1}^n \\log \\pi(y_{\\text{BON}} | x_i).$\nSFT-Sharpening is a simple, flexible self-training scheme, and converges to a sharpened model as n, N \u2192 \u221e.\nIn Appendix D, we consider a variant of SFT-Sharpening based on adaptive sampling, which adjusts the\nnumber of sampled responses adaptively for better performance."}, {"title": "2.2 Self-Improvement through RLHF: RLHF-Sharpening", "content": "A drawback of the SFT-Sharpening algorithm is that it may ignore useful information contained in the\nself-reward function $r_{\\text{self}}(y | x)$. Fixing a regularization parameter $\\beta > 0$ throughout, our second class of\nalgorithms solve a KL-regularized reinforcement learning problem in the spirit of RLHF and other alignment\nmethods (Christiano et al., 2017; Bai et al., 2022a; Ouyang et al., 2022; Rafailov et al., 2023). Defining\n$\\mathbb{E}_{\\pi}[\\cdot] = \\mathbb{E}_{x \\sim \\mu, y \\sim \\pi(\\cdot|x)}[\\cdot]$ and $D_{\\text{KL}}(\\pi || \\pi_{\\text{base}}) = \\mathbb{E}_{\\pi} [\\log \\frac{\\pi(y|x)}{\\pi_{\\text{base}}(y|x)}]$, we choose\n$\\hat{\\pi} \\approx \\underset{\\pi \\in \\Pi}{\\text{arg max}} \\{\\mathbb{E}_{\\pi}[r_{\\text{self}}(y | x)] - \\beta D_{\\text{KL}}(\\pi || \\pi_{\\text{base}})\\}$.\nThe exact optimizer $\\pi^{\\diamond} = \\underset{\\pi \\in \\Pi}{\\text{arg max}} \\{\\mathbb{E}_{\\pi}[r_{\\text{self}}(y | x)] - \\beta D_{\\text{KL}}(\\pi || \\pi_{\\text{base}})\\}$ for this objective has the form\n$\\pi(y | x) \\propto \\pi_{\\text{base}}(y | x) \\cdot \\exp(\\beta^{-1}r_{\\text{self}}(y | x))$, (3)\nwhich converges to the solution to the sharpening objective in Eq. (1) as \u03b2 \u2192 0. Thus, Eq. (3) can be seen\nto encourage sharpening."}, {"title": "3 A Statistical Framework for Sharpening", "content": "This section introduces the theoretical framework within which we will analyze the SFT-Sharpening and\nRLHF-Sharpening algorithms. We first introduce the maximum-likelihood sharpening objective as a stylized\nself-reward function, then introduce our statistical framework for sharpening."}, {"title": "3.1 Maximum-Likelihood Sharpening", "content": "Our theoretical results focus on the maximum-likelihood sharpening objective given by\n$r_{\\text{self}}(y|x) := \\log \\pi_{\\text{base}} (y | x)$,,\nwhich we aim to maximize using conditional samples $y \\sim \\pi_{\\text{base}}(x)$ from the base model. This is a simple\nand stylized self-reward function, but we will show that it enjoys a rich theory. In particular, we can restate\nthe problem of sharpening with this self-reward through the lens of amortization.\nCan we efficiently amortize maximum likelihood inference (optimization) for a conditional\ndistribution $\\pi_{\\text{base}}(y|x)$ given access to a sampling oracle that can sample $y \\sim \\pi_{\\text{base}}(\\cdot | x)$ ?\nThe tacit assumption in this framing is that the maximum-likelihood response constitutes a useful form of\nhidden knowledge. Maximum-likelihood sharpening connects the study of self-improvement to a large body\nof research in theoretical computer science demonstrating computational reductions between optimization\n(inference) and sampling (generation) (Kirkpatrick et al., 1983; Lov\u00e1sz and Vempala, 2006; Singh and Vishnoi,\n2014; Ma et al., 2019; Talwar, 2019). Our sharpening framework offers a new learning-theoretic perspective\nby focusing on the problem of amortizing this type of reduction.\nWe evaluate the quality of an approximately sharpened model as follows. Let\n$y^*(x) := \\underset{y \\in \\mathcal{Y}}{\\text{arg max}} \\log \\pi_{\\text{base}} (y | x);$ ,,\nwe interpret $y^*(x) \\subset \\mathcal{Y}$ as a set to accommodate non-unique maximizers, and will write $y^*(x)$ to indicate a\nunique maximizer when it exists (i.e., when $y^*(x) = \\{y^*(x)\\}$)."}, {"title": "Definition 3.1 (Sharpened model)", "content": "We say that a model $\\hat{\\pi}$ is $(\\epsilon, \\delta)$-sharpened relative to $\\pi_{\\text{base}}$ if\n$\\mathbb{P}_{x \\sim \\mu}[\\hat{\\pi}(y^*(x) | x) \\geq 1 - \\delta] \\geq 1 - \\epsilon$.\nThat is, an $(\\epsilon, \\delta)$-sharpened model places at least $1 - \\delta$ mass on arg-max responses on all but an $\\epsilon$-fraction of\nprompts under $\\mu$. For small $\\delta$ and $\\epsilon$, we are guaranteed that $\\hat{\\pi}$ is a high-quality generator: sampling from the\nmodel will produce an arg-max response with high probability for most prompts."}, {"title": "Maximum-likelihood sharpening for autoregressive models", "content": "Though our most general results are\nagnostic to the structure of $\\mathcal{X}$, $\\mathcal{Y}$, and $\\pi_{\\text{base}}$, our primary motivation is the autoregressive setting in which\n$\\mathcal{Y} = V^H$ for a vocabulary space V and sequence length H, and where $\\pi_{\\text{base}}$ has the autoregressive structure\n$\\pi_{\\text{base}}(Y_{1:H} | x) = \\prod_{h=1}^H \\pi_{\\text{base},h}(Y_h | Y_{1:h-1},x)$ for $y = Y_{1:H} \\in \\mathcal{Y}$. We observe that when the response\n$y = (Y_1,\\ldots,Y_H) \\in \\mathcal{Y} = V^H$ is a sequence of tokens, the maximum-likelihood sharpening objective (2)\nsharpens toward the sequence-level arg-max response:\n$\\underset{Y_{1:H}}{\\text{arg max}} \\log \\pi_{\\text{base}} (Y_{1:H} | x)$. (5)\nAlthough somewhat stylized, Eq. (5) is a non-trivial (in general, computationally intractable; see Appendix E)\nsolution concept. We view the sequence-level arg-max as a form of hidden knowledge that cannot necessarily\nbe uncovered through naive sampling or greedy decoding."}, {"title": "Role of $\\delta$ for autoregressive models", "content": "As can be verified through simple examples, beam-search and\ngreedy tokenwise decoding do not return an exact (or even approximate) solution to (5) in general. There\nis one notable exception: If the model has already been sharpened to $\\delta < 1/2$ and the arg-max sequence\nis unique, then greedy decoding will succeed."}, {"title": "Proposition 3.1 (Greedy decoding succeeds for sharpened policies)", "content": "Let $\\pi = \\pi_{1:H}$ be an autoregressive\nmodel defined over response space $\\mathcal{Y} = V^H$. For a given prompt $x \\in \\mathcal{X}$, if $y^*(x) = \\{y^*(x)\\}$ is a singleton and\n$\\pi(y^*(x) | x) > 1/2$, then the greedy decoding strategy that selects\n$\\hat{y}_h = \\underset{Y_h \\in V}{\\text{arg max}} \\pi_h(Y_h | Y_{1},\\ldots, Y_{h-1},x)$\nguarantees that $\\hat{y} = y^*(x)$. This result is tight, in the sense that there exist $\\pi$ with $\\pi(y^*(x) | x) \\leq 1/2$ for\nwhich greedy decoding fails to recover $y^*(x)$.\nThis means that if we start from an un-sharpened model, it can suffice to focus on sharpening to $\\delta < 1/2$."}, {"title": "3.2 Sample Complexity Framework", "content": "As described, sharpening in the sense of Definition 3.1 is a purely computational problem, which makes it\ndifficult to evaluate the quality and optimality of self-improvement algorithms. To address this, we introduce\na novel statistical framework for sharpening, inspired by the success of oracle complexity in optimization\n(Nemirovski et al., 1983; Traub et al., 1988; Raginsky and Rakhlin, 2011; Agarwal et al., 2012) and statistical\nquery complexity in computational learning theory (Blum et al., 1994; Kearns, 1998; Feldman, 2012, 2017)."}, {"title": "Definition 3.2 (Sample-and-evaluate framework)", "content": "In the sample-and-evaluate framework, the algorithm\ndesigner does not have explicit access to the base model $\\pi_{\\text{base}}$. Instead, they access $\\pi_{\\text{base}}$ only through sample-\nand-evaluate queries: The learner is allowed to sample n prompts $x \\sim \\mu$. For each prompt x, they can sample\nN responses $Y_1, Y_2,\\ldots,Y_N \\sim \\pi_{\\text{base}}(\\cdot|x)$ and observe the likelihood $\\pi_{\\text{base}}(Y_i | x)$ for each such response. The\nefficiency, or sample complexity, of the algorithm is measured through the total number of sample-and-evaluate\nqueries $m := n \\cdot N$.\nThis framework can be seen to capture algorithms like SFT-Sharpening and RLHF-Sharpening (implemented\nwith DPO), which only access the base model $\\pi_{\\text{base}}$ through i) sampling responses via $y \\sim \\pi_{\\text{base}}(\\cdot | x)$ (gener-\nation), and ii) evaluating the likelihood $\\pi_{\\text{base}} (y | x)$ (verification) for these responses. We view the sample\ncomplexity $m = n \\cdot N$ as a natural statistical abstraction for the computational complexity of self-improvement\n(a clear parallel to oracle complexity for optimization algorithms), one which is amenable to information-\ntheoretic lower bounds. We will aim to show that, under appropriate assumptions, SFT-Sharpening and\nRLHF-Sharpening can learn an $(\\epsilon, \\delta)$-sharpened model with sample complexity\n$m = \\text{poly}(\\epsilon^{-1},\\delta^{-1}, C_{\\text{prob}})$ ,,\nwhere $C_{\\text{prob}}$ is a potentially problem-dependent constant."}, {"title": "3.3 Fundamental Limits", "content": "Before diving into our analysis of SFT-Sharpening and RLHF-Sharpening in the sample-and-evaluate framework,\nlet us take a brief detour to give a sense for how sample complexity guarantees for sharpening should scale.\nTo this end, we will prove a lower bound or fundamental limit on the sample complexity of any algorithm in\nthe sample-and-evaluate framework.\nIntuitively, the performance of any sharpening algorithm based on sampling should depend on how well the base\nmodel $\\pi_{\\text{base}}$ covers the arg-max response $y^*(x)$. To capture this, we define the following coverage coefficient:\n$C_{\\text{cov}} = \\mathbb{E}_{x \\sim \\mu} [\\frac{1}{\\pi_{\\text{base}} (y^*(x) | x)}].$ (6)\nMore generally, for a model $\\pi$, we define $y^{\\pi}(x) = \\underset{y \\in \\mathcal{Y}}{\\text{arg max}} \\pi(y | x)$ and $C_{\\text{cov}}(\\pi) = \\mathbb{E}_{x \\sim \\mu}[\\pi(y^{\\pi}(x))]$.\nOur main lower bound shows that for worst-case choice of II, the coverage coefficient acts as a lower bound\non the sample complexity of any sharpening algorithm."}, {"title": "Theorem 3.1 (Lower bound for sharpening)", "content": "Fix an integer $d \\geq 1$ and parameters $\\epsilon \\in (0,1)$ and $C > 1$. There\nexists a class of models II such that (i) $\\log |\\Pi| = d(1 + \\log(C\\epsilon^{-1}))$, (ii) $\\text{sup}_{\\pi \\in \\Pi} C_{\\text{cov}}(\\pi) \\leq C$, and (iii) $y^{\\pi}(x)$ is\na singleton for all $\\pi \\in \\Pi, x \\in \\mathcal{X}$. Any sharpening algorithm that achieves $\\mathbb{E}[\\mathbb{P}_{x \\sim \\mu}[\\pi(\\pi_{\\text{base}} (x) | x) > 1/2]] >\n1 - \\epsilon$ for all $\\pi_{\\text{base}} \\in \\Pi$ must collect a total number of samples $m = n \\cdot N$ at least\n$m \\gtrsim \\frac{C\\log |\\Pi|}{\\epsilon^2 \\cdot (1 + \\log(C\\epsilon^{-1}))}$.\nThis result shows that the complexity of any $(\\epsilon, 1/2 - \\delta)$-sharpening algorithm (for $\\delta > 0$) in the sample-\nand-evaluate framework must depend polynomially on the coverage coefficient $C_{\\text{cov}}$, as well as the accuracy\nparameter $\\epsilon$. The lower bound also depends on the expressivity of $\\pi_{\\text{base}}$, as captured by the model class\ncomplexity term $\\log |\\Pi|$. We will show in the sequel that it is possible to match this lower bound. Note\nthat this result also implies a lower bound for the general sharpening problem (i.e., general $r_{\\text{self}}$), since\nmaximum-likelihood sharpening is a special case."}, {"title": "Remark 3.1 (Relaxed notions of sharpening and coverage)", "content": "The notion of coverage in Eq. (6) is somewhat\nstringent, since it requires that $\\pi_{\\text{base}}$ place large mass on $y^*(x)$ on average. In Appendix F, we introduce a more\ngeneral and permissive notion of approximate sharpening (Definition F.1), which allows the model to sharpen\ntoward approximate arg-max responses (in the sense that $\\log \\pi_{\\text{base}}(y | x) \\geq (1 - \\gamma) \\text{max}_{y \\in \\mathcal{Y}} \\log \\pi_{\\text{base}}(y | x)$\nfor an approximation parameter $\\gamma > 0$). This notion of sharpening leads to significantly weaker coverage\nrequirements, and we state generalized versions of all our main results which accommodate this in the appendix."}, {"title": "4 Analysis of Sharpening Algorithms", "content": "Equipped with the sample complexity framework from Section 3, we now prove that the SFT-Sharpening\nand RLHF-Sharpening families of algorithms provably learn a sharpened model for the maximum likelihood\nsharpening objective under natural statistical assumptions.\nThroughout this section, we treat the model class II as a fixed, user-specified parameter. Our results in the\ntradition of statistical learning theory allow for general classes II, and are agnostic to the structure beyond\nstandard generalization arguments."}, {"title": "4.1 Analysis of SFT-Sharpening", "content": "Recall that when we specialize to the maximum-likelihood sharpening self-reward, the SFT-Sharpening\nalgorithm takes the form\n$\\pi^{\\text{BON}} = \\underset{\\pi \\in \\Pi}{\\text{arg max}} \\sum_{i=1}^n \\log \\pi(Y_{\\text{BON}} | x_i),$\nwhere $y_{\\text{BON}} = \\text{arg max}_{j \\in [N]} \\{\\log \\pi_{\\text{base}}(Y_{i,j} | x_i)\\}$ for $Y_{i,1},\\ldots, Y_{i,N} \\sim \\pi_{\\text{base}}(\\cdot | x_i)$.\nTo analyze SFT-Sharpening, we first make a realizability assumption. Let $\\pi_{\\text{BON}}^*(x)$ be the distribution of the\nrandom variable $y_{\\text{BON}}(x) \\sim \\text{arg max}\\{\\log \\pi_{\\text{base}}(Y_i | x) | Y_1,\\ldots, Y_N \\sim \\pi_{\\text{base}}(x)\\}$.\nAssumption 4.1. The model class II satisfies $\\pi_{\\text{BON}}^* \\in \\Pi$.\nOur main guarantee for SFT-Sharpening is as follows."}, {"title": "Theorem 4.1 (Sample complexity of SFT-Sharpening)", "content": "Let $\\rho,\\delta \\in (0,1)$ be given, and suppose we set\n$N = N^*\\log(2\\delta^{-1})$ for a parameter $N^* \\in \\mathbb{N}$. If Assumption 4.1 holds, then for any $n \\in \\mathbb{N}$, SFT-"}]}