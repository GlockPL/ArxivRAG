{"title": "Causal Abstraction Learning based on the Semantic Embedding Principle", "authors": ["Gabriele D'Acunto", "Fabio Massimo Zennaro", "Yorgos Felekis", "Paolo Di Lorenzo"], "abstract": "Structural causal models (SCMs) allow us to investigate complex systems at multiple levels of resolution. The causal abstraction (CA) framework formalizes the mapping between high- and low-level SCMs. We address CA learning in a challenging and realistic setting, where SCMs are inaccessible, interventional data is unavailable, and sample data is misaligned. A key principle of our framework is semantic embedding, formalized as the high-level distribution lying on a subspace of the low-level one. This principle naturally links linear CA to the geometry of the Stiefel manifold. We present a category-theoretic approach to SCMs that enables the learning of a CA by finding a morphism between the low- and high-level probability measures, adhering to the semantic embedding principle. Consequently, we formulate a general CA learning problem. As an application, we solve the latter problem for linear CA; considering Gaussian measures and the Kullback-Leibler divergence as an objective. Given the non-convexity of the learning task, we develop three algorithms building upon existing paradigms for Riemannian optimization. We demonstrate that the proposed methods succeed on both synthetic and real-world brain data with different degrees of prior information about the structure of CA.", "sections": [{"title": "1. Introduction", "content": "Causal modeling and reasoning are key to trustworthy and responsible AI (Ganguly et al., 2023; Rawal et al., 2024; Qi et al., 2024). Structural causal models (SCMs) provide a widely adopted framework for causal reasoning (Pearl, 2009). While canonical causal theory focuses on a single SCM, scientific research often requires multiple representations of the same system at different levels of resolution. For example, biological processes can be studied at the molecular level (e.g., gene expression), cellular level (e.g., metabolic pathways), or organism level (e.g., physiological responses), each offering a different view of the same underlying system. Causal abstraction (CA) theory (Rubenstein et al., 2017; Beckers & Halpern, 2019) formalizes mappings between SCMs at different abstraction levels, enforcing rigorous consistency requirements. This makes CA a powerful tool for transitioning between resolutions, synthesizing causal evidence, and selecting the most parsimonious representation for a given task. However, CAs are unknown in practice, underscoring the need for advancing CA learning from data (Zennaro et al., 2023).\nRelated works. Seminal works on CA have focused on"}, {"title": "2. Background on causality and abstraction", "content": "This section provides the notation and key concepts related to causal modeling and abstraction theory.\nNotation. The set of integers from 1 to n is [n]. The vectors of zeros and ones of size n are $0_n$ and $1_n$. The identity matrix of size n x n is $I_n$. The Frobenius norm is $||A||_F$. The set of positive definite matrices over $R^{n \\times n}$ is $S^{+}$. The Hadamard product is $\\odot$. Function composition is $\\circ$. The domain of a function is $D[\\cdot]$ and its kernel $ker$. Let $M(X^n)$ be the set of Borel measures over $X^n \\subseteq R^n$. Given a measure $\\mu^n \\in M(X^n)$ and a measurable map $\\varphi$, $X^n \\ni x \\mapsto \\varphi(x) \\in X^m$, we denote by $\\varphi_{\\#}(\\mu^n) := \\mu^n(\\varphi^{-1}(x))$ the pushforward measure $\\mu^m \\in M(X^m)$.\nWe now present the standard definition of SCM.\nDefinition 2.1 (SCM, Pearl, 2009). A (Markovian) structural causal model (SCM) $M^n$ is a tuple $(\\mathcal{X},\\mathcal{Z},\\mathcal{F},\\zeta)$, where (i) $\\mathcal{X} = \\{X_1, ..., X_n\\}$ is a set of n endogenous random variables; (ii) $\\mathcal{Z} = \\{Z_1, ..., Z_n\\}$ is a set of n exogenous variables; (iii) $\\mathcal{F}$ is a set of n functional assignments such that $X_i = f_i(P_i, Z_i), \\forall i \\in [n]$, with $P_i \\subseteq \\mathcal{X} \\setminus \\{X_i\\}$; (iv) $\\zeta$ is a product probability measure over independent exogenous variables $\\zeta = \\Pi_{i \\in [n]} \\zeta^{Z_i}$, where $\\zeta^{Z_i} = P(Z_i)$.\nA Markovian SCM induces a directed acyclic graph (DAG) $G_{M^n}$ where the nodes represent the variables $\\mathcal{X}$ and the edges are determined by the structural functions $\\mathcal{F}$; $P_i$ constitutes then the parent set for $X_i$. Furthermore, we can recursively rewrite the set of structural function $\\mathcal{F}$ as a set of mixing functions $\\mathcal{M}$ dependent only on the exogenous variables (cf. App. C). A key feature for studying causality is the possibility of defining interventions on the model:\nDefinition 2.2 (Hard intervention, Pearl, 2009). Given SCM $M^n = (\\mathcal{X},\\mathcal{Z},\\mathcal{F},\\zeta)$, a (hard) intervention $\\iota = do(X' = x')$, $X' \\subseteq \\mathcal{X}$, is an operator that generates a new post-intervention SCM $M^n_{\\iota} = (\\mathcal{X},\\mathcal{Z},\\mathcal{F}_{\\iota},\\zeta)$ by replacing each function $f_i$ for $X_i \\in X'$ with the constant $x \\in x'$. Graphically, an intervention mutilates $G_{M^n}$ by removing all the incoming edges of the variables in $X'$."}, {"title": "3. Category-theory formalization", "content": "Standard category-theoretic formalization of CA (Rischel, 2020; Otsuka & Saigo, 2022) are based on a functorial semantics (Jacobs et al., 2019) approach mapping the graphical structure of causal models (syntax) onto the discrete distributions of individual variables (semantics). Because of our non-assumption (NA2), no knowledge of the structure of an SCM is available in our setting; thus, we propose a formalization mapping a dyadic structure (syntax) onto the exogenous and the endogenous probability measures implied by an SCM (semantics).\nA crucial role in our modelling is that of the mixing functions $\\mathcal{M}$, which express the data generation process as a recursive process from the exogenous functions. This allows us to define an SCM $M^n$ in measure-theoretic terms as a tuple made up of the probability space of exogenous variables $(\\mathcal{U}, \\Sigma_{\\mathcal{U}}, \\zeta)$, the probability space of the endogenous variables $(\\mathcal{V}, \\Sigma_{\\mathcal{V}}, \\chi)$, and a set of measurable functions $\\mathcal{M}$ given by the mixing functions (cf. App. C).\nWe can now rely on this representation to interpret an SCM as a category-theoretic functor from a simple index category Ind, made up only of a source and a sink object and an edge between them, to the category of probability spaces Prob, where objects $(\\mathcal{X}, \\Sigma_\\mathcal{X}, p)$ are probability spaces and morphisms are measurable maps:\nDefinition 3.1 (Category-theoretic SCM). An SCM is a functor $M^n$: Ind$\\rightarrow$ Prob, mapping the source node of Ind to $(\\mathcal{U}, \\Sigma_{\\mathcal{U}}, \\zeta)$, the sink node of Ind to $(\\mathcal{V}, \\Sigma_{\\mathcal{V}}, \\chi)$, and the edge of Ind to the collection $\\mathcal{M}$ of measurable maps.\nApp. B presents basic category-theoretic concepts, whereas App. C.5 deepens Def. 3.1. CA can now be expressed as a natural transformation between two SCMs, as shown in Fig. 2. This formulation has two important features. First, it highlights the role of exogenous variables in a constructive abstraction showing the commutativity of the paths $\\mathcal{M}_h \\circ \\alpha_{\\mathcal{U}}$ and $\\alpha_{\\mathcal{V}} \\circ \\mathcal{M}^l$. Second, morphisms in Prob relates measure spaces, viz. sets equipped with sigma algebras. Consequently, the natural transformation components are measurable maps with dimensionality determined by the cardinality of $\\mathcal{X}^h$ and $\\mathcal{X}^l$. To ease the notation, we will denote $\\alpha(\\mathcal{U}^l, \\Sigma_{\\mathcal{U}}^l)$ by $\\alpha_{\\zeta}$ and $\\alpha(\\mathcal{V}^h, \\Sigma_{\\mathcal{V}}^h)$ by $\\alpha_{\\chi}$. Then, we can formally recast the a-abstraction in Prob.\nDefinition 3.2 (a-abstraction in Prob). Given low-level $M^l$ and high-level $M^h$ SCMs, an abstraction $\\alpha = (R, Q, m, a)$ is a tuple, where: (i) R is the same as in Def. 2.3; (ii) $Q \\subseteq \\mathcal{Z}^l$ is a set of relevant exogenous variables given by the union of the set of exogenous corresponding to the endogenous in R and those corresponding to their ancestors; (iii) $m = (m_{\\zeta}, m_{\\chi})$ is a pair of surjective functions mapping sets, $m_{\\zeta} : Q \\rightarrow \\mathcal{Z}^h$ and $m_{\\chi} : R \\rightarrow \\mathcal{X}^h$, respectively; (iv) $a = (\\alpha_{\\zeta}, \\alpha_{\\chi})$ is a natural transformation made by measurable functions mapping probability spaces, $\\alpha_{\\zeta}$ for the exogenous and $\\alpha_{\\chi}$ for the endogenous, respectively.\nAs Def. 2.3, Def. 3.2 makes no reference to interventional consistency. App. D explains how intervened SCMs and interventional consistency can be represented categorically."}, {"title": "4. Problem formulation", "content": "Within our category-theoretic framework, CA learning amounts to finding the endogenous components $m_{\\chi}$ and $\\alpha_{\\chi}$ from data. We start by formulating a general learning problem working under the non-assumption (NA1)-(NA5), and then decline it to the case of linear CA.\nOur problem formulation relies upon three key ingredients. First, we assume that the data generated by a constructive abstraction adheres to the semantic embedding principle. This principle requires that the CA component $\\alpha_{\\chi}$ admits a right-inverse measurable map.\nDefinition 4.1 (Semantic embedding principle, SEP). Given an a-abstraction as in Def. 3.2, the semantic embedding principle states that $\\alpha_{\\chi}$ has a right-inverse measurable map $\\beta_{\\chi}$, such that $\\alpha_{\\chi} \\circ \\beta_{\\chi} = Id_{(\\mathcal{V}^h, \\Sigma_{\\mathcal{V}}^h, \\chi^h)}$. Hence, it holds\n$\\chi^h = \\alpha_{\\chi} \\circ \\beta_{\\chi}(\\chi^h)$.\n(1)\nThe SEP implies that going from the high-level model $M^h$ to the low-level model $M^l$ and then abstracting back to $M^h$ allows for perfect reconstruction. Notice that SEP only holds in one direction, as suggested by the word embedding; thus, identity on the left inverse is not guaranteed, meaning that the abstraction from the low level to the high level can still shed information, as we would expect in CA.\nSecond, because of the non-assumption (NA3) only observational data is available. Thus, we can not explicitly use interventional consistency information to drive our learning. Only if we identify the true constructive abstraction, we are guaranteed interventional consistency. In trying to learn the abstraction, we leverage (A1), which is met in application domains as discussed in Sec. 1.\nThird, to learn a CA, we look for a distance function quantifying the misalignment between the probability measures $\\chi^l$ and $\\chi^h$, given $\\alpha_{\\chi}$. Since the probability measures belong to spaces of different dimensionality, specifically $R^l$ and $R^h$, we leverage the approach proposed in (Cai & Lim, 2022) to compute the misalignment through an embedding as $D(\\chi^l, \\varphi_{\\#}(\\alpha_{\\chi}(\\chi^l)))$, where D is an information-theoretic metric (e.g., p-Wasserstein) or $\\phi$-divergence (e.g., Kullback-Leibler). Please refer to App. F for more details. We can now pose the following general learning problem:\nProblem 1. (SEP-based CA Learning)\nInput: (i) probability measures $\\chi^l$ and $\\chi^h$; (ii) prior information about $m_{\\chi}$, and (iii) a distance function $D(\\chi^l, \\varphi_{\\#}(\\alpha_{\\chi}(\\chi^l)))$.\nGoal: learn a measurable map $\\alpha_{\\chi}$ such that (i) it belongs to $ker\\, D(\\chi^l, \\varphi_{\\#}(\\alpha_{\\chi}(\\chi^l)))$, (ii) it complies with SEP in Def. 4.1, and (iii) it agrees with the prior information about $m_{\\chi}$.\nThe zeroing of the distance function implies $\\chi^l = \\varphi_{\\#}(\\alpha_{\\chi}(\\chi^l))$, which, together with Eq. (1), yields $\\alpha_{\\chi} \\circ \\beta_{\\chi}(\\chi^h) = \\varphi_{\\#}(\\alpha_{\\chi}(\\chi^l))$. However, despite solving Prob. 1, there is no guarantee that $\\alpha$ coincides with the ground truth CA. In other words, the optimal solution is not unique. For a linear constructive CA, we express $m_{\\chi}$ and $\\alpha_{\\chi}$ as $B \\in \\{0,1\\}^{h \\times l}$ and $V^T \\in R^{h \\times l}$, respectively. In accordance with constructivity, each row of $B$ has a single nonzero entry, and each column has at least one nonzero entry. Importantly, for linear CA, a simple yet principled way to satisfy SEP is via the geometry of the Stiefel manifold:\n$St(l, h) := \\{V \\in R^{l \\times h} | V^TV = I_h\\}$.\n(2)\nThe Stiefel manifold (see App. E for details), is a convenient choice for the following reasons: (i) differently from a generic pseudo-inverse matrix, the orthogonality of $V$ guarantees that the geometry of the high-level space is preserved; (ii) the transpose eases the formulation and ensures numerical stability in optimization. Consequently, we restate SEP for the linear case as follows.\nDefinition 4.2 (Semantic embedding principle, linear case). Given the linear constructive CA, viz. $V^T$, SEP implies that $V \\in St(l, h)$. From Eq. (1) we get $\\chi^h = V_{\\#}(V^T(\\chi^h))$.\nA pictorial representation of Def. 4.2 is provided in Fig. 1. Def. 4.2 shapes our methodology for CA learning, posing it as a Riemannian optimization problem (Boumal, 2023). As an application, in the sequel, we will tackle an implementation of Prob. 1 for the linear constructive case $\\alpha_{\\chi} = V^T$, where (i) $\\chi^h \\sim \\mathcal{N}(0_h, \\Sigma^h)$ and $\\chi^l \\sim \\mathcal{N}(0_l, \\Sigma^l)$; and (ii) $D(\\chi^l, \\varphi_{\\#}(\\alpha_{\\chi}(\\chi^l))) = D_{KL}(\\chi^h || \\varphi_{\\#}(\\chi^l))$ where $D_{KL}$ stands for KL divergence. Specifically,\n$D_{KL} = Tr\\{(\\Sigma^lV V^T)^{-1}\\Sigma^h\\} + log det\\{\\Sigma^lV V^T\\} + C$,\n(3)\nwhere $C$ is a constant term. Additionally, from Eq. (3) it is immediate to see that both $V^*$ and $-V^*$ belong to $ker\\,D_{KL}$. Such an application is highly relevant as it is common to deal in practice with Gaussian measures (or quasi) (D'Acunto et al., 2024); also, in causality, such a measure easily arises from the prominent family of linear models (Bollen, 1989; Shimizu et al., 2006) and is investigated in the CA literature (Keki\u0107 et al., 2023; Massidda et al., 2024). KL divergence is a common choice in ML and statistics, but notice that any distance vanishes when evaluated at the ground truth.\nRemark 1. From Eq. (3), it is immediate to derive a criterion to decide on the existence of a linear constructive CA adhering to SEP. For zero-mean Gaussian measures, the variance provides all the relevant information about the data, and via the eigendecomposition we can compute the eigenvalues quantifying the variance associated with each eigenvector. Thus, a linear constructive CA adhering to SEP"}, {"title": "5. Problem solution", "content": "To solve the nonsmooth and smooth Riemannian problems in Sec. 4, we leverage the following:\nProposition 5.1. Consider the function\n$f(A) = Tr\\{(A \\Sigma^l A^T)^{-1} \\Sigma^h\\} + log det\\{A \\Sigma^l A^T\\}$.\n(7)\nEq. (7) is smooth for $A \\in St(l,h)$. Additionally, define $\\tilde{A} := (A \\Sigma^l A^T)^{-1}$. The gradient of $f(A)$ is\n$\\nabla_A f = 2 (\\Sigma^l A \\tilde{A}) (I_h - \\Sigma^h \\tilde{A})$,\n(8)\n5.1. Solution of the nonsmooth learning problem\nLeveraging Proposition 5.1, we have that Eq. (4) is constituted by a smooth yet nonconvex term, $f(V)$, and a nonsmooth one, $h(V)$. Hence we solve Prob. 2 through two different optimization paradigms for nonsmooth Riemannian optimization: MADMM and ManPG. We term the proposed methods LinSEPAL-ADMM and LinSEPAL-PG, where LinSEPAL stands for Linear Semantic Embedding Principle Abstraction Learner. Next we provide a sketch of the solution and provide the full mathematical derivation in App. H and App. I.\nLinSEPAL-ADMM. The MADMM framework appeals to our setting given the objective function separating into smooth and nonsmooth terms. To derive the LinSEPAL-ADMM iterative algorithm, we proceed as follows. First, the nonsmooth term $h(V)$ is associated with a splitting variable $Y$ to be optimized over $[R^{l \\times h}$, obtaining an equivalent problem formulation (cf. Eq. (P2)). LinSEPAL-ADMM proceeds by iteratively minimizing"}, {"title": "5.2. Solution of the smooth learning problem", "content": "We provide a sketch of the solution below and the full mathematical derivation in App. J. In this case, we want to jointly optimize S and V, both being components of the linear CA, viz. $(B \\odot S \\odot V)^T$. Hence, unlike the nonsmooth case, we constrain to the Stiefel manifold the product $(B \\odot S \\odot V)$."}, {"title": "6. Empirical assessment on synthetic data", "content": "This section provides the empirical assessment of LinSEPAL-ADMM, LinSEPAL-PG and CLinSEPAL with different degrees of prior knowledge, from full (fp) to partial (pp). We monitor four metrics to evaluate the learned CA $V^{\\nabla}$: (i) constructiveness, as required by Def. 4.2; (ii) $D_{KL}$ evaluating the alignment between $\\varphi_{\\#}(\\chi^l)$ and $\\chi^h$; (iii) the"}, {"title": "7. Causal abstraction of brain networks", "content": "To show the practical relevance of our approach, we apply CLinSEPAL to resting-state functional magnetic resonance imaging (rs-fMRI) data, using the dataset from (D'Acunto et al., 2024) (refer to the paper for details on the dataset). The data, publicly released as part of the Human Connectome Project (Smith et al., 2013), comprises recordings from 100 healthy adults with a parcellation scheme that divides the brain into 89 regions of interest (ROIs), K = 44 for each hemisphere plus the shared vermis region.\nWe simulate a first investigating team of neuroscientists taking zero-mean stationary time series for the left hemisphere of the first adult in the dataset. They estimate the data covariance matrix using a Gaussian mixture probability model, viz. $\\Sigma^l \\in R^{l \\times l}$, with l = K + 1, and interpret it as generated by an underlying, unknown, low-level SCM.\nIn a first fp scenario, we imagine a second investigating team that has collected data according to their causal network specified on a coarser parcellation of the same brain in h = 14 macro ROIs. We generate the data for the second team using a ground truth linear CA $B, V^* \\in St(45, 14)$ based on the structural mapping in (D'Acunto et al., 2024), and use the data for estimating the covariance matrix $\\Sigma^h \\in R^{h \\times h}$. In this scenario it is realistic to assume knowledge of B defining how macro ROIs are mapped to ROIs. Then, to align their models, the two groups run CLinSEPAL to recover the abstraction given $\\Sigma^l, \\Sigma^h$ and B. Fig. 7 (in App. L) shows that CLinSEPAL recovers $V^*$.\nIn a second pp scenario, we imagine that the second investigating team has collected data according to a causal network aggregating ROI time series into h = 8 brain functional networks related to different activities (e.g., motor, visual, default mode). Data is generated again through a ground truth linear CA $B, V^* \\in St(45, 8)$ based on groupings in (D'Acunto et al., 2024) and the covariance matrix $\\Sigma^h \\in R^{h \\times h}$ computed. In this scenario, knowledge of B is debatable as different studies in the literature suggest different relations between ROIs and functions; we then express this partial information via uncertainty over B, meaning that some rows of B have more than one entry equal to one. The two groups now run CLinSEPAL using $\\Sigma^l, \\Sigma^h$ and an uncertain B; partial knowledge compounds on an already challenging learning problem due to the high coarse-"}, {"title": "8. Conclusion and future works", "content": "In this work, we addressed the challenge of CA learning in realistic scenarios, abandoning restrictive assumptions (NA1)-(NA5) that limit the applicability of existing methods. We proposed an alternative category-theoretic framework for SCM and CA, and introduced the semantic embedding principle to learn CAs that meaningfully preserve information. We formulated a general CA learning problem grounded in SEP, under a mild assumption of partial prior knowledge about the structure of CA. For the linear CA setting, we showed how SEP links CA to the geometry of the Stiefel manifold; as an application, we tackled the important case of Gaussian measures, with the KL divergence as a measure of alignment between the low- and high-level SCMs. We pursued two different formulations. For the first, a nonsmooth Riemannian learning problem, we devised the LinSEPAL-ADMM and LinSEPAL-PG methods. For the second, a smooth Riemannian learning problem ensuring the constructiveness of the CA, we developed CLinSEPAL. Our empirical assessment on synthetic data confirmed the effectiveness of our methods, and the application to brain data showcased the potential in real-world problems.\nOur work paves the way for several exciting research directions. First, as it emerges from our Gaussian application, linear CAs with different probability measures deserve careful investigation. Second, studying the nonlinear case is a compelling avenue. We believe that deep and reinforcement learning paradigms, such as encoding-decoding and actor-critic architectures, hold promise for modeling nonlinear CA maps. Lastly, we view our work as a foundational step toward observational causal abstraction learning, bridging the gap between CA learning and causal discovery (Spirtes & Zhang, 2016). Our category-theoretic framework underscores the pivotal role of exogenous variables, drawing a path to translate SCM identifiability results into CA identifiability results. This suggests that, in some cases, interventional consistency may be achieved without relying on interventional data."}, {"title": "A. Extended notation for the appendix", "content": "Below is the notation used throughout the appendices. The set of integers from 1 to n is [n]. The vectors of zeros and ones of size n are $0_n$ and $1_n$. The identity matrix of size n\u00d7n is $I_n$. The entry indexed by row i and column j is $a_{ij} = [A]_{ij}$, diag(a) is the diagonal matrix having as diagonal the vector a, while diag(A) is the diagonal of the matrix A. The Frobenious norm is $||A||_F$. The set of positive definite matrices over $R^{n \\times n}$ is $S^{+}$. That of symmetric ones as Sym(p). The column-wise vectorization of a matrix is vec (). The Hadamard product is $\\odot$. Function composition is $\\circ$.\nLet $M(X^n)$ be the set of Borel measures over $X^n \\subseteq R^n$. Given a measure $\\mu^n \\in M(X^n)$ and a measurable map $\\varphi$, $X^n \\ni x \\mapsto \\varphi(x) \\in X^m$, we denote by $\\varphi_{\\#}(\\mu^n) := \\mu^n(\\varphi^{-1}(x))$ the pushforward measure $\\mu^m \\in M(X^m)$. The proximal mapping of h at A is $prox_{\\lambda h}(A) = arg\\,min_V h(V) + 1/(2\\lambda) ||V - A||_F$, $\\lambda \\in R^+$. The Euclidean gradient of a smooth f is $\\nabla f$, while the Riemannian one $\\nabla_R f$. The Euclidean subgradient of a nonsmooth h is $\\partial h$, the Riemannian instead $\\partial_R h$."}, {"title": "B. Category theory essentials", "content": "Below are fundamental definitions and examples that are instrumental in providing the necessary background on category theory to understand our work. For a comprehensive overview of category theory see resources such as (Mac Lane, 2013; Perrone, 2024).\nDefinition B.1 (Category). A category C consists of\n\u2022 A collection of objects, viz. X in C,\n\u2022 A collection of morphisms, viz. f : X \u2192 Y in C;\nsuch that:\n\u2022 Each morphism f has assigned two objects of the category called source and target, respectively,\n\u2022 Each object X has an identity morphism $id_X : X \\rightarrow X$,\n\u2022 Given f: X \u2192 Y and g: Y \u2192 Z, than the composition exists, g \u25e6 f = h : X \u2192 Z.\nThese structures satisfy the following axioms:\n\u2022 (Unitality) \u2200f : X \u2192 Y, $f \\circ id_X = f$ and $id_Y \\circ f = f$;\n\u2022 (Associativity) Given f, g, and h such that the compositions hold, then h\u25e6(g\u25e6f) = (h\u25e6g)\u25e6f.\nExample 1. The following are some notable examples of categories:\n\u2022 Indicate with Poset a partial order set. Poset can be viewed as the category whose objects are the elements p and morphisms are order relations p < p'. Notice that there is at most one morphism between two objects;\n\u2022 Vectr is the category whose objects are real vector spaces and morphisms are linear maps;\n\u2022 Prob is the category whose objects are probability measure spaces and morphisms measurable maps.\nArrows between categories are called functors, defined as follows:\nDefinition B.2 (Functor). Consider C and D categories. A functor F : C \u2192 D consists of the following data:\n\u2022 For each object X in C, an object F(X) in D;\n\u2022 For each object morphism f : X \u2192 Y in C, a morphism F(f) : F(X) \u2192 F(Y) in D;\nsuch that the following axioms hold:\n\u2022 (Unitality) \u2200X in C, F($id_X$)=$id_{F(X)}$. In other words, the identity in C is mapped into the identity in D.\n\u2022 (Compositionality) \u2200f and g in C such that the composition is defined, then F(g \u25e6 f) = F(g) \u25e6 F(f). In other words, the composition in C is mapped into the composition in D.\nTo ease the notation, in the sequel, we use FX and Ff to denote F(X) and F(f), respectively. Finally, we can have arrows"}, {"title": "C. Causality and causal abstraction.", "content": "This section provides additional definitions and examples related to SCMs and the CA framework.\nC.1. Mixing functions\nA set of structural function in a Markovian SCM can be reduced to a set of mixing functions dependent only on the exogenous variables.\nGiven an SCM $M^n$, recall that $\\mathcal{F}$ is a set of n functional assignments which define the values $X_i = f_i(P_i, Z_i), \\forall i \\in [n]$, with $P_i \\subseteq \\mathcal{X} \\setminus \\{X_i\\}$. Denote by $Z_{A_i} \\subseteq \\mathcal{Z} \\setminus \\{Z_i\\}$ the set of exogenous variables corresponding to the ancestors of $X_i$, where $A_i \\subseteq [n] \\setminus \\{i\\}$. According to $\\mathcal{F}$, we can identify a set of mixing functions $\\mathcal{M} = \\{m_1,...,m_n\\}$ such that the values of the endogenous random variables are equivalently expressed as $x_i = \\mathcal{M}_i (\\{z_j\\}_{j\\in A_i}, z_i), \\forall i \\in [n]$.\nFurther, we can also characterize the product probability measure implied by the SCM purely in terms of the exogenous variables, viz. $\\chi_{\\mathcal{X}} = \\Pi_{i \\in [n]} P (X_i|Z_{A_i}, Z_i)$.\nAs an example, consider a causal relation $x_1 \\rightarrow x_2$. In the linear SCM with additive noise (Bollen, 1989; Shimizu et al., 2006) setting we have\n$\\begin{cases}x_1 = z_1, \\\\ x_2 = c_{2,1}x_1+z_2 = c_{2,1}z_1+z_2.\\end{cases}$\n(10)\nAgain, for the post-nonlinear model (Zhang & Hyvarinen, 2012), we get\n$\\begin{cases}x_1 = f_{1,1}(z_1) = m_{1,1}(z_1), \\\\ x_2=f_{2,2}(f_{2,1}(x_1) +z_2) = (f_{2,2} \\circ f_{2,1} \\circ f_{1,1}) (z_1) + f_{2,2}(z_2) = m_{2,1}(z_1) + m_{2,2}(z_2).\\end{cases}$\n(11)\nC.2. Interventional consistency\nA typical requirement imposed on CA maps is that they act in a consistent way with respect to interventions (Rischel, 2020).\nDefinition C.1 (Interventional consistency). Given an a-abstraction between $M^l$ and $M^h$ and a set $\\mathcal{I}$ of hard interventions on $\\mathcal{X}^h \\subseteq \\mathcal{X}^h$, the abstraction is interventionally consistent if, for any intervention in $\\mathcal{I}$ and for every set of target variable $\\mathcal{Y}^h \\subseteq \\mathcal{X}^h \\setminus \\mathcal{X}^h$, the following diagram commutes:"}, {"title": "E. Stiefel manifold", "content": "We now provide a short review of the Stiefel manifold, referring the interested reader to (Absil et al., 2008; Boumal, 2023) for a comprehensive discussion.\nGiven l, h \u2208 N, h < l, the Stiefel manifold is the set of l \u00d7 h matrices with orthonormal columns, mathematically\n$St(l, h) := \\{V \\in R^{l \\times h} | V^TV = I_h\\}$.\n(18)\nConsider the function g : $[R^{l \\times h} \\rightarrow Sym(h), g(V) := V^TV - I_h$. It is well-known that g is a generating function for St(l, h), thus making it an embedded submanifold of $R^{l \\times h}$, with dimension $dim\\,R^{l \\times h} - dim\\,Sym(h) = lh - h(h + 1)/2$. Given a point of the manifold V, the tangent space to St(l, h) can be defined implicitly as the kernel of the differential of g at V,\n$T_V St(l, h) := \\{G \\in R^{l \\times h} | V^TG + G^TV = 0\\}$.\n(19)\nWe consider the Riemannian metric as the restriction of the Eucliden product between two matrices in $R^{l \\times h}$ to St(l, h). Accordingly, given A, B \u2208 $T_V St(l, h)$, we have $(A, B)_V = Tr A^T B$. The tangent space linearizes the manifold around V, then, we can move away from V along the directions in $T_V St(l, h)$. However, to make such a movement smooth along the manifold, we employ the retraction map $R_V (\\cdot) : T_V St(l, h) \\rightarrow St(l, h)$. The retraction has to satisfy the following conditions\n(i) $R_V (0_{l \\times h}) = V$, and (ii) $\\lim_{G\\rightarrow 0_{l \\times h}} \\frac{||R_V (G) - (V + G)||_F}{||G||_F} = 0$.\n(20)"}, {"title": "F. Information-theoretic distance on spaces of different dimensionality", "content": "Two types of distances can be defined as follows using an affine map $\\phi^{\\mathcal{V}"}]}