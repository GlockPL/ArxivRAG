{"title": "Cluster and Predict Latent Patches for Improved Masked Image Modeling", "authors": ["Timoth\u00e9e Darcet", "Federico Baldassarre", "Maxime Oquab", "Julien Mairal", "Piotr Bojanowski"], "abstract": "Masked Image Modeling (MIM) offers a promising approach to self-supervised representation\nlearning, however existing MIM models still lag behind the state-of-the-art. In this paper, we\nsystematically analyze target representations, loss functions, and architectures, to introduce\nCAPI\na novel pure-MIM framework that relies on the prediction of latent clusterings. Our\napproach leverages a clustering-based loss, which is stable to train, and exhibits promising\nscaling properties. Our ViT-L backbone, CAPI, achieves 83.8% accuracy on ImageNet and\n32.1% mIoU on ADE20K with simple linear probes, substantially outperforming previous\nMIM methods and approaching the performance of the current state-of-the-art, DINOv2.\nWe release all our code and models.", "sections": [{"title": "Introduction", "content": "Recent advances in large-scale visual representation learning have established foundation models as a\ncornerstone of modern computer vision. Self-supervised representations have proven particularly effective in\ndomains with limited annotations, such as satellite imagery (Tolan et al., 2024) and medical imaging (Xu\net al., 2024; Vorontsov et al., 2024; Chen et al., 2024; Moutakanni et al., 2024; Dermyer et al., 2025),\nwhile enabling breakthroughs in more fundamental vision tasks like monocular depth estimation (Yang\net al., 2024a; Bochkovskii et al., 2024; Yang et al., 2024b), keypoint matching (Edstedt et al., 2024), and\ntracking (Tumanyan et al., 2025). This shift from small supervised models to large self-supervised generalist\nmodels mirrors the evolution in natural language processing since the publication of BERT (Devlin et al.,\n2018) and GPT (Radford et al., 2018), where large-scale models pretrained on web-scale unlabeled data have\nbecome ubiquitous foundation models.\nHowever, the impressive scalability of language models remains unmatched in vision: the best self-supervised\nvisual encoders contain around one billion parameters (Oquab et al., 2024), hundreds of times smaller than"}, {"title": "Related Work", "content": "Self-supervised representation learning. Self-supervised learning (SSL) is a pre-training paradigm\nin which a model is optimized to solve a pretext task on unlabeled data, often collected at scale. As the\nmodel is not trained to match specific human annotations, the benefit of this type of training is to produce\na generalist model, that can be adapted to solve many different downstream tasks. Depending on the\napplication, these tasks can be solved either by fully fine-tuning the mode, or by using the representations\nextracted with the frozen model. In the SSL literature, some works have focused mainly on full fine-tuning (He\net al., 2022; Huang et al., 2023), while others have shown that frozen representations can reach excellent\nperformance on a wide range of tasks, avoiding costly fine-tuning (Oquab et al., 2024) and generalizing to\nannotation-scarce domains where fine-tuning is not possible (Tolan et al., 2024; Xu et al., 2024). Historically,\nearly work on self-supervised learning focused on hand-crafted pretext tasks such as predicting the rotation\nof an image (Gidaris et al., 2018), the relative position of patches (Doersch et al., 2015) or the color of a\ngrayscale image (Zhang et al., 2016). Subsequent works pushed the field forward with methods based on\nclustering (Caron et al., 2018; 2020; 2021) and contrastive learning (Chen et al., 2020b; He et al., 2020).\nNowadays, the best self-supervised encoder inherits from these families (Oquab et al., 2024) and complements\nthem with a masked image modeling objective (Zhou et al., 2022). However, training with both global\nand MIM objectives can prove difficult, as multiple components can interact negatively\u00b9. In this work, we\nstudy the masked image modeling component in isolation, suggesting improvements to properly stabilize the\noptimization objective in the absence of a global term.\nPixel reconstruction. Learning by predicting a missing part of an image was first proposed in Context\nencoders (Pathak et al., 2016). This was envisioned as conceptually similar to denoising autoencoders (Vincent\net al., 2008), in the case where the noise is a masking process. More recently, the success of masked language\nmodeling (Devlin et al., 2018) and autoregressive pretraining in natural language processing (Radford et al.,\n2018) brought a new wave of interest for transferring these ideas to vision. iGPT (Chen et al., 2020a) was the\nfirst effort to train a transformer (Vaswani et al., 2017) by generating pixels. Chen et al. (2020a) proposed\nrasterizing images to very low resolution, then training for autoregressive next-token prediction. Then, the\nadvent of the ViT (Dosovitskiy et al., 2021) architecture sparked further interest in the field. Following the\ninitial exploration of Dosovitskiy et al. (2021), BeiT (Bao et al., 2021) tried using the quantized latents of a\ndVAE as targets for a masked image pretraining, using the tokenizer from DALLE (Ramesh et al., 2021).\nBeiT has proven useful as an initialization for further fine-tuning, but severely underperformed baselines in\nrepresentation learning. To simplify BeiT, SimMIM, and MAE (Xie et al., 2021; He et al., 2022) concurrently\nproposed using raw pixels as targets. Thanks to a clever encoder/decoder architecture, MAE proved very\nstable and reached interesting representations, despite its simplicity. However, it still fell short of previous\nSSL methods in terms of representation quality: MAE models need to be scaled to a ViT-H size to match the\nlinear probing performance of a 25\u00d7 smaller DINOv1 ViT-S/16(Caron et al., 2021).\nLatent reconstruction. Concurrently to MAE, Zhou et al. (2022) proposed iBOT. To obtain a more\nsemantic tokenization, iBOT used the online output of the model being trained as the reconstruction target\nfor masked image modeling. This led to good representations, and the method was reused to obtain the\ncurrent state-of-the-art (Oquab et al., 2024). However, the iBOT objective was very unstable and required an\nadditional DINO (Caron et al., 2021) loss to stabilize the training. The idea of using online representations as\ntargets was then reused in data2vec (Baevski et al., 2022) and I-JEPA (Assran et al., 2023; Bar et al., 2024).\nI-JEPA in particular proposed reusing the encoder/decoder architecture of MAE and removing the projection\nhead of iBOT, to obtain a more stable objective in the latent space. The improvements in I-JEPA established\na new tradeoff between stability and performance, but it was still both sensitive to hyperparameters (-12\npoints on IN-1k when changing the \"target scale\" from [0.15,0.2] to [0.125, 0.2] (Assran et al., 2023))) and\nweaker than DINOv2 (81.1 on ImageNet-1k, 5.4 below DINOv2 while using a model twice bigger)."}, {"title": "Approach", "content": "At a high level, masked image modeling involves masking a part of the input, feeding the visible region\nto a prediction model, and optimizing it to predict the content of the missing parts. Despite the simple\nformulation, the effectiveness of reconstruction-based methods and the properties of the trained models\nare dramatically influenced by a number of design choices. In this section, we discuss the three aspects of\nmasked image modeling depicted in Figure 2: the type of patch representation used as target (fig. 3), the loss\nformulation (Section 3.1, fig. 4), and the prediction architecture (Section 3.2, fig. 5). Based on our findings,\nwe introduce CAPI, an SSL model that enjoys stable learning and strong representation capabilities.\nOverview of training. In short, our main design choices are: first, we reconstruct images in latent space\nwith a teacher-student framework, following iBOT. Then, we formulate our loss using a clustering component,\ninspired by DeepCluster, SwAV, and DINO, and draw inspiration from the regularization methods they\nintroduce, in particular the Sinkhorn-Knopp (Sinkhorn & Knopp, 1967). Finally, we employ a cross-attention\npredictor model, separate from the encoder, to perform reconstructions, following crossMAE (Fu et al., 2024).\nOur encoder and the predictor are transformers (Dosovitskiy et al., 2021) and, during pre-training, they\noperate in tandem as the student (fig. 1 left). The teacher is an EMA of the encoder. The typical values for\none training iteration using square images of side 224 pixels are:"}, {"title": "Clustering-based loss formulation", "content": "Latent clustering methods such as SwAV and DINO employ a cross-entropy loss between the student and\nteacher output distributions. These distributions, produced by a linear or MLP head, can be seen as soft\ncluster memberships, where the centroids correspond to the prototypes. Replicating the DINO objective,\niBOT proposes to pass the student predictions through an MLP head and to pass the teacher embeddings\nthrough the EMA of this head. But this ignores a specificity of masked image modeling: while in DINO both\ntargets and predictions are [CLS] tokens, in iBOT the targets are patch tokens while the predictions are\nspecial [MSK] tokens. This causes a distribution mismatch: the MLP head of the student is trained with [MSK]\ninputs but is instead applied to regular patch tokens in the teacher. This mismatch would be even stronger in\nan asymmetric architecture as in MAE or I-JEPA, where the targets and predictions come from two different\nnetworks (see fig. 5). Without the stabilizing effect of the DINO loss, the iBOT formulation is unable to\nbootstrap itself and results in trivial representations (see table 1c). Our proposition is simple: we decouple\nthe training of the teacher projection from the student's head by directly learning an online clustering of the\nteacher patch tokens. This way, the training remains stable even in the absence of a stabilizing loss.\nOnline clustering. Inspired by the minibatch k-means algorithm and the SwaV loss, we define our online\nclustering process as follows. Let $X \\in \\mathbb{R}^{n \\times d}$ be the output of the teacher, with row vectors $x_i$ for $i \\in \\{1, ..., n\\}."}, {"title": "Predictor architecture", "content": "Model architecture is another important component in reconstruction-based SSL. Two broad categories are\nwidely used in previous work. BeiT, iBOT, and SimMIM use a fused architecture: a single vision transformer\nthat takes as inputs patches and mask tokens (Fig. 5a). Fused architectures are difficult to train and yield"}, {"title": "Experiments", "content": "In this section, we report empirical evaluations of our model. We describe experimental details and present\nsome ablation studies. Then we discuss whole-image understanding results and dense prediction results."}, {"title": "Experimental setup", "content": "Pretraining dataset. Most methods from the self-supervised learning literature choose to pretrain\non ImageNet-lk. This dataset is usually chosen because of its relatively small size, allowing for easy\nexperimentation, and the ability to compare to existing methods pretrained on it. However, this has led to\nan overspecialization of SSL methods to the type of object-centric images found in ImageNet-1k. Recent\nfoundation models obtain state-of-the-art results by exploiting much larger datasets, such as ImageNet-\n22k (Zhou et al., 2022) and LVD-142M (Oquab et al., 2024). If we are to design a method that can produce\nnew foundation models, we believe it is crucial to design it to be able to handle such large datasets.\nTo this end, we carry out all ablation experiments on ImageNet-22k. It is composed of 14M images from 22k\ncategories taken from the WordNet ontology. Although it is close to ImageNet-1k in nature, its much larger\nsize and diversity make it suitable to train excellent foundation models, as reported by Oquab et al. (2024).\nFor our longer experiments, we train on multiple datasets: ImageNet-1k, for comparability with previous\nworks, ImageNet-22k, to test scaling, Places205, to test training on more diverse and less object-centric data,\nand finally LVD-142M, a large-scale automatically curated dataset used in previous SSL foundation models.\nModel architecture. We do all our experiments with a Vision Transformer (Dosovitskiy et al., 2021)\nof 300M parameters (ViT-L). This architecture is widely used in various computer vision tasks, and most\nbaselines provide a model of comparable size. We equip the vision transformer with registers (Darcet et al.,\n2024). These additional tokens were recently proposed as a way to add an information buffer, which enabled\nthe model to produce smoother feature maps. For the decoder, we use 12 transformer blocks that cross-attend\nto the output of the encoder. This is similar to a standard transformer decoder (Vaswani et al., 2017),\nwith the difference that we do not include self-attention layers. In this decoder, every token is forwarded\nindependently and separately attends to the encoder output. When using a different encoder size, we align\nthe embedding dimension, MLP ratio, and number of attention heads of the decoder to those of the encoder,\nand use a decoder depth equal to half that of the encoder.\nImplementation Details. The learning rate follows a linear warmup followed by a cosine annealing. We\ntruncate out the last 20% of the cosine, as proposed in I-JEPA (Assran et al., 2023). To simplify the choices\nof parameters and schedules, we set the teacher EMA momentum to $\\mu = 1 - lr$, and we set the learning rate\nfor the clustering to half of the backbone learning rate. The impact of the most important hyperparameters\nwill be discussed in section 4.2. All our pretraining hyperparameters are summarized in table 6.\nEvaluation protocol. All the evaluations reported in this paper fall into two categories: image classification\nand semantic segmentation. For all classification tasks, we use an attentive probe (Assran et al., 2023;"}, {"title": "Ablation Studies", "content": "We conduct extensive ablation studies to study the effect of design choices on performance. To make the\nablation study more tractable, we train on the ImageNet-22k dataset for 100k iterations with a patch size of\n16. To provide slightly more robust results, the default setting was run twice with different seeds, and the\nresults of the two runs were averaged. All results are presented in Table 1.\nPredictor architecture. We evaluate the different predictor architectures. The split predictor produces\nbetter representations while training 32% faster than the fused predictor (table 1a). Using pure cross-attention"}, {"title": "Results", "content": "Image classification. We evaluate our model and compare it to state-of-the-art reconstruction-based SSL\nmodels. We run the evaluation on four datasets including object recognition, fine-grained classification, and\nscene recognition. We use ImageNet-1k (Russakovsky et al., 2015), iNaturalist 2021 (Van Horn et al., 2021),\nPlaces205 (Zhou et al., 2017), and SUN397 (Xiao et al., 2010). For ImageNet, we also report OOD robustness\nby running inference on additional test sets: ImageNet-V2 (Recht et al., 2019), ImageNet-ReaL (Beyer et al.,\n2020), ImageNet-A (Hendrycks et al., 2021), and ObjectNet (Barbu et al., 2019). For each model, we resize\nthe image to 224\u00d7224 and collect the patch tokens output by the model. We feed those to an attentive probe\nimplemented as a single layer of multi-head cross-attention with a single query (head size d//64, no residual).\nFor c classes and an embedding size d, the probe contains 2d\u00b2 + (3 + c)d parameters, which are trained with\nAdamW for 10 epochs, selecting the best learning rate for each model/task on a held-out split of the training\nset. More details on the protocol are available in appendix F. All the results are summarized in Table 2."}, {"title": "Additional explorations", "content": "As a final set of experiments, we investigate some additional properties of our model. We investigate its\nrobustness to change of input resolution and try to obtain global representations using the predictor.\nHigh-resolution image understanding. Our model was trained on 224 \u00d7 224 images. To compare with\nthe I-JEPA model trained natively at high resolution, we try to evaluate our model on 448 \u00d7 448 images. In\ntable 4, we see that our model does not require high-resolution training or evaluation to achieve the best\nperformance. Our model trained at 224 and evaluated at 224 outperforms the large I-JEPA model trained at"}, {"title": "Discussion and Concluding Remarks", "content": "In this paper, we have proposed a novel reconstruction-based self-supervised learning algorithm. Our algorithm\nis based on an online clustering of dense features computed with a teacher network. The latent assignments\nare used as targets to train the student. We propose to implement the student as an encoder followed by\na predictor: a cross-attention decoder. The teacher is updated as an EMA of the encoder. The proposed\nalgorithm is simple and allows the training of a state-of-the-art model. Our ViT-L outperforms all available\nreconstruction-based models, including much larger architectures. We have shown promising scaling trends\nuntil the 300M model sizes of the ViT family, opening up a potential for further scaling in future work."}, {"title": "Detailed overview", "content": "In Figure 8, we provide a detailed overview of the complete method, with tensor sizes annotated for a reference\nCAPI VIT-L/14 model."}, {"title": "Loss curve", "content": "We report in Figure 9 the loss curve of our CAPI ViT-L model. After an initial adjustment period, the loss\ntrends smoothly downwards, with no sign of instability or plateauing. Compared to other latent masked\nimage modeling methods such as I-JEPA or iBOT, this trend is reassuring, and might indicate good potential\nfor further scaling."}, {"title": "Blockwise masking strategy", "content": "The so-called \"block masking\" strategy used in many masked image modeling methods is by no means\nstandardizes and can actually refer to several different implementations. The most common block masking\nimplementation was proposed in BeiT (Bao et al., 2021), then reused in iBOT (Zhou et al., 2022) and\nMAE (He et al., 2022). It involves sampling many rectangular regions and doing multiple attempts to\nmask out approximately the right number of patches. Another implementation was proposed in I-JEPA,\nadding multiple constraints on the masks, to obtain a similar multi-block mask. Additionally, some methods\npostprocess the proposed mask to obtain a constant number of masked patches, in order to keep the same\nsequence length in all batch elements.\nIn CAPI, we propose a simpler heuristic: we sample a single rectangular mask, and truncate out the excess\npatches at the lower right end. Conversely, our implementation of inverse block masking is to sample a block\nmask, then simply invert it."}, {"title": "Self-distillation interpretation", "content": "It was observed in DINO (Caron et al., 2021) that the downstream scores of the EMA model were consistently\nhigher than the ones of the online model during training. This led to the interpretation of DINO as a\nself-distillation method, where the EMA model, the \"teacher\" distilled its slightly better representations into\nthe online model, the \"student\". We observe that this interpretation still seems to hold in CAPI, albeit to a\nlesser extent, as evidenced by the comparison of teacher and student performance in Figure 10."}, {"title": "Modified Sinkhorn-Knopp", "content": "We provide the pseudo-code for the standard Sinkhorn-Knopp and for our modified version in Figure 11.\nBoth the original code and the proposed change are very simple. The actual code additionally contains an\ninitial additive shift to prevent numerical instabilities in the exponential, as well as a collective all_reduce\nfor distributed training."}, {"title": "Detailed evaluation protocol", "content": "In all cases, our evaluations are performed with a frozen model, and use only the patch tokens outputted by\nthe vision transformer. The input images are always at resolution 224\u00d7224."}, {"title": "Classification", "content": "The backbone model is kept frozen, and we extract only the patch tokens from its output. On top of these\nfeatures, we train an attentive pooling classifier, consisting of a learned query, two learned k anv v projections,\nand a final projection to the number of classes. The attention is multi-head, with the head dimension being\nfixed at 64 and the number of heads being $\\frac{dmodel}{64}$. This head is optimized with a cross-entropy loss and the"}, {"title": "Compute cost and environmental footprint", "content": "We measure the training of a CAPI VIT-L model to take 180h on 32 A100 GPUs, amounting to 5763 A100\nhours. This consumed around 2651 kWh of electricity, which we estimate to amount to approximately 928\nkgCO2eq. The entire project used 3.75M A100 hours, which we similarly estimate to have emitted 604\ntCO2eq for the electricity consumption. Note that the carbon footprint estimations here are purely scope 2\nestimations, i.e. limited to electricity consumption, and are further limited to the electricity consumption of\nthe GPUs. A full carbon accounting should additionally include many other harder to estimate emissions,\nsuch as the electricity consumption of the other server components and the rest of the datacenter appliances,\nand scope 3 emissions from the component manufacturing, datacenter construction, and their respective\nend-of-life. 2"}, {"title": "List of models used", "content": "We provide in Table 8 the list of all models presented in this paper, along with a unique hash identifier and\nthe relevant hyperparameters. Non-listed hyperparameters are detailed in Table 6. To disambiguate any\npossible unclarities in the presented results, Table 9 provides the mapping from tables and figures to model\nidentifiers."}, {"title": "Visualisations", "content": "In Figures Figure 12 and 13, we provide visualisations of the feature maps of CAPI compared to other\nstate-of-the-art self-supervised vision models."}]}