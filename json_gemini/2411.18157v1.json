{"title": "A survey on cutting-edge relation extraction techniques based on language models", "authors": ["J. Angel Diaz-Garcia", "Julio Amador Diaz Lopez"], "abstract": "This comprehensive survey delves into the latest advancements in Relation Extraction (RE), a pivotal task in natural language processing essential for applications across biomedical, financial, and legal sectors. This study highlights the evolution and current state of RE techniques by analyzing 137 papers presented at the Association for Computational Linguistics (ACL) conferences over the past four years, focusing on models that leverage language models. Our findings underscore the dominance of BERT-based methods in achieving state-of-the-art results for RE while also noting the promising capabilities of emerging large language models (LLMs) like T5, especially in few-shot relation extraction scenarios where they excel in identifying previously unseen relations.", "sections": [{"title": "1 Introduction", "content": "With the rapid growth of data generated and stored on the internet, numerous tools have emerged to process and extract valuable insights from vast information. Given that a significant portion of this data exists as unstructured text, the need for techniques capable of handling such data is evident. The branch of artificial intelligence dedicated to processing text is Natural Language Processing (NLP) [95]. NLP provides practitioners and researchers with a comprehensive toolkit for effectively analyzing unstructured text from diverse sources, including social media [24,18], public health data [43], research papers [123] and digital humanities [102].\nOne of the prominent topics within NLP is Relation Extraction (RE). RE is a task focusing on identifying and extracting the intricate relationships between different entities mentioned in the textual content. Essentially, RE automates discovering connections between words or phrases within a text. For example, in the sentence \"The Eiffel Tower is located in Paris,\" a relation extraction system would automatically identify the relationship \"located in\" between the entities \"Eiffel Tower\" and \"Paris.\" Though this example may seem trivial, it can be extrapolated to other domains with far-reaching implications. In fields such as biomedicine, finance, and law [110], the importance of Re-lation Extraction becomes undeniable, as it enables the automatic discovery of critical connections within vast amounts of unstructured data."}, {"title": "2 Methodology", "content": "To ensure that we focus on cutting-edge applications of RE in natural language processing, we have limited our survey to the most recent papers presented at the Association for Computational Linguistics (ACL) conferences. These conferences are widely recognized as one of the most important and prestigious conferences in the field of NLP. Specifically, we focus on ACL, NAACL, AACL, and EACL. This review was conducted from September 2023 to January 2024. During this period, the most recent conferences were ACL 2023, AACL 2023, and EACL 2023, marking the endpoint of our survey. We started in 2020, the first year with the significant incorporation of language models like BERT, which was introduced at NAACL 2019. It is important to note that ACL is the most prominent conference in our survey, as AACL, NAACL, and EACL are held biennially.\nTo narrow down the papers, we searched for pieces that contained the phrase \"Relation Extraction\" in either the title or abstract. This search resulted in a set of papers screened based on their relevance to our research question and quality. Specifically, we only included papers that presented novel approaches or significant advancements in RE techniques using state-of-the-art lan-guage models. We exclude papers that are not directly related to the traditional problem of RE and venture into novel domains, such as temporal RE [77,19,106] or papers with a particular focus on NER [141]. We also omitted papers that lack a foundation in language models or, at the very least, word embeddings [149,143]. Our final exclusion criterion involves papers suggesting novel encoding techniques that, while intriguing, lack widespread adoption within the research community. For in-stance, the approach presented by [114], which introduces an exciting graph encoding solution, falls into this category. According to our inclusion criteria, the final set comprised 65 papers, classified as research contributions due to their introduction of new models, training strategies, or innovative approaches.\nIn total, we examined 81 papers presented at the last four years' editions of ACL conferences. Following inclusion criteria, the final set comprised 65 papers, classified as research contributions due to their introduction of new models, training strategies, or innovative approaches. Our in-vestigation delved into the language models featured in these papers, with prominent examples including BERT and ROBERTa. Additionally, we scrutinized the diverse range of datasets utilized for model evaluation, encompassing well-known benchmarks like TACRED, NYT10, and DocRED. We examined a total of 56 datasets in our study. It is important to note that, in these instances, datasets are not exclusively confined to proposals in ACL conferences; instead, we have inclusively considered all datasets employed in RE, irrespective of their origin."}, {"title": "3 Background", "content": "This section focuses on the theoretical principles that form the basis of our survey. We aim to provide the reader with the necessary conceptual foundations for RE and Language Models."}, {"title": "3.1 Relation Extraction", "content": "RE is a task in natural language processing that aims to identify and classify the relationships between entities mentioned in the text. This task can be divided into three main components: named entity recognition, relation identification, and relation classification.\nNamed Entity Recognition (NER) is a critical initial step in relation extraction. It involves identifying and categorizing named entities within a text, such as the names of individuals, orga-nizations, locations, and other specific entities [103]. For example, in the sentence \"Apple Inc. is headquartered in Cupertino, California,\u201d NER identifies \u201cApple Inc.\" as an organization and \u201cCu-pertino, California\" as a location. This step is foundational, determining the entities between which relationships will be analyzed.\nRelation identification follows NER and involves detecting pairs of entities related to each other in the text. This task can be accomplished using various techniques, including rule-based approaches or machine learning models [75]. For instance, in the sentence \"Steve Jobs co-founded Apple Inc.,\" relation identification would recognize the entity pair 'Steve Jobs\" and \"Apple Inc.\" and determine their relationship."}, {"title": "3.2 Approaches and challenges in Relation Extraction", "content": "RE poses several challenges that are currently being addressed and require further research. Our study we have identified and we will specifically focus on the advancements in RE within the following areas:\nDocument-level Relation Extraction: In specific scenarios, relationships between entities extend beyond individual sentences, spanning entire documents. Document-level RE tackles this challenge by considering relationships in a broader context. This task is crucial for applications where understanding connections across multiple sentences or document sections is essential for accurate information extraction.\nSentence-level Relation Extraction: Conversely, sentence-level RE focuses on identifying relationships within individual sentences. This level of granularity is relevant for tasks where relationships are localized and can be adequately captured within the confines of a single sen-tence. This task is essential for applications with short-form content or when the relations of interest are contained within discrete textual units.\nMultimodal Relation Extraction: With the increasing prevalence of multimodal data, RE extends beyond textual information to include visual cues. Multimodal RE integrates text and visual data to enhance understanding of relationships. This task is relevant in domains where textual and visual information jointly contribute to the overall context, such as image captions, medical reports, or social media content.\nMultilingual Relation Extraction: In multilingual relation extraction, the challenge lies in extracting relations and training systems in domains where multiple languages are present. This task includes scenarios where documents or texts may contain information in different languages, requiring models to understand and extract relations effectively across language barriers. This task is particularly relevant in diverse linguistic contexts or global applications where information is presented in multiple languages.\nFew-Shot and Low-Resource Relation Extraction: Few-shot learning addresses scenarios with limited labeled data for training RE models. It aims to develop models that generalize well even with minimal labeled examples. In low-resource settings, such as specialized domains like biology or medicine, the scarcity of annotated data poses a significant challenge. The high annotation costs in these domains hinder the availability of large labeled datasets, making it challenging to train accurate RE models using traditional supervised learning methods.\nDistant Relation Extraction: Distant RE involves extracting relations between entities based on distant supervision. In this approach, heuristics or existing knowledge bases automatically label large amounts of data. However, this introduces challenges related to noisy labels and the potential inaccuracies in the automatically generated annotations. Addressing these challenges is crucial for improving the reliability of models trained on distantly supervised data.\nOpen Relation Extraction: OpenRE is dedicated to unveiling previously unknown relation types between entities within open-domain corpora. The primary objective of OpenRE lies in"}, {"title": "3.3 Language Models", "content": "LMs process sequences of tokens, denoted as w = {w1, w2, ..., wn}, by assigning a probability p(w) to the sequence and incorporating information from previous tokens. This is captured mathematically as follows:\np(w) = \\prod_{t} p(w_t | w_{t-1}, w_{t-2}, ..., w_1)\nAlthough p(w) has been traditionally calculated statistically, recently p(w) through neural lan-guage models [10]. However, the methods employed to calculate p(w) vary across applications, diverse architectures from multi-layer perceptrons [78] and convolutional neural networks [22] to recurrent neural networks [147].\nRecently, the introduction of self-attention mechanisms [87] has heralded the development of high-capacity models, with transformer architectures [115] emerging as frontrunners due to their efficient utilization of extensive data, leading to state-of-the-art results. Transformers have revolu-tionized the field of NLP, demonstrating their potential across various downstream applications, such as question answering [84] and text classification [109] or text generation [73]. Within the transformer architecture, there are different types, with two of the most widespread being the encoder-only and the encoder-decoder models [13].\nEncoder-only models are designed to process input data and generate rich, contextualized rep-resentations. These models excel at tasks that involve understanding [56] and analyzing text, such as named entity recognition. The key feature of encoder-only models is their focus on generating deep contextual embeddings of the input sequence, which capture intricate semantic and syntactic information without producing any output sequence.\nOn the other hand, Encoder-decoder models are structured to handle tasks that require both understanding and generation of text. This architecture consists of two distinct components: an encoder and a decoder. The encoder processes the input sequence and generates a set of intermediate representations that encapsulate the input's contextual information. The decoder then takes these representations and generates the final output sequence. This design is particularly effective for tasks that involve complex text generation, such as machine translation [16], where the model translates text from one language to another, and text summarization [62]. The encoder-decoder structure allows for a dynamic interplay between understanding and generation, making it well-suited for applications that require producing coherent and contextually appropriate output based on the input.\nNoteworthy is Bi-directional Encoder Representations from Transformers (BERT) [23]. BERT employs a unique approach that involves learning the probability p(wi) by masking parts of the input sequence and predicting the masked word wi while considering the surrounding context. Specifically, BERT and other contextual transformer-based models aim to compute the probability of a word given its context by estimating:\np(w_i) = p(w_i | w_1, ..., w_{i-1}, w_{i+1},...,w_N),"}, {"title": "4 Datasets for RE", "content": "We have compiled the most comprehensive datasets used for RE to date. We begin by providing brief descriptions of each dataset. Furthermore, in Section 7.3, we describe in more detail the most frequently used datasets for RE. We have categorized datasets into two distinct periods. The first comprises cutting-edge datasets introduced during our review period from 2019 to 2023, while the second includes traditional datasets proposed before this time frame.\nDatasets proposed during the survey period\nDialogRE: A dataset derived from dialogues in the Friends TV series, comprising 10,168 subject-relation-object tuples, covering diverse relations such as 'positive_impression' and 'sib-lings'. The dataset utilizes BERT-base-uncased with special tokens to demarcate subject and object boundaries within dialogues [142].\nBioRED: A biomedical relation extraction dataset featuring multiple entity types (e.g., gene, protein, disease, chemical) and relation pairs (e.g., \u2018gene-disease', \u2018chemical-chemical) at the document level, tagged on a set of 600 PubMed abstracts, comprising 4,178 relations and 13,351 entities [72].\nRe-DocRED: An updated version of the DocRED dataset with re-annotation of 4,053 doc-uments to address and rectify issues with false negatives due to incomplete annotation in the original dataset [105]."}, {"title": "5 Cutting-edge RE techniques based on language models", "content": "We employed a two-fold approach to facilitate a thorough analysis of the field's evolution. First, we conducted a task-based analysis, offering a comprehensive examination of the techniques used to address each specific task in relation extraction. This structured approach provides a detailed understanding of how various methods have been applied and evolved across different RE tasks. Second, we organized our survey into chronological tables by year, conference, task, sub-task, and model type. This segmentation enables us to explore the progression of techniques over time, identify emerging trends, and derive valuable insights from the evolving landscape of RE methods."}, {"title": "5.1 Task analysis of cutting-edge RE", "content": "In this section, we will discuss how new systems and models are addressing the tasks and challenges of relation extraction (RE) that were introduced in Section 3.1. We will particularly emphasize the systems and models that make use of language models (LMs) and large language models (LLMs).\nSentence-level Relation Extraction\nIn[116], Veyseh et al. used a Ordered-Neuron Long-Short Term Memory Networks (ON-LSTM) to infer model-based importance scores for every word within sentences. This paper uses dependency trees to calculate importance scores for sets of words based on syntax. These syntax-based scores are then regulated to align with the model-based importance scores, ushering in a harmonious synergy between syntax and semantics. This approach achieved state-of-the-art performance levels on three distinguished RE benchmark datasets: ACE 2005, related to news; SPOUSE with only one entity type and relation, related to whether two entities regarding people are married; and SciERC, related to scientific abstracts. They used BERT to obtain pre-trained word embeddings for the sentences. Specifically, they use the hidden vectors in the last layer of the BERT-base model (with 768 dimensions) to obtain the pre-trained word embeddings for the sentences. They fixed BERT in the experiments and compared the performance of their proposed model with BERT embeddings against other models that used word2vec embeddings. In this paper, word embeddings achieve state-of-the-art results, so it is not surprising that there will be a stream in 2020 that harnesses the potential of word embeddings.\nIn [142], Yu et al. introduced DialogRE, a dataset derived from dialogues in the Friends TV series. The dataset contains 10,168 subject-relation-object tuples, encompassing a wide range of relations such as \"positive_impression\" and \"siblings\". The paper harnesses BERT-base-uncased, employing"}, {"title": "5.2 Observations", "content": "Regarding models, we identified four main groups: those employing LSTMs, those utilizing CNNS and graph-based approaches, and the largest group leveraging transformers like BERT. It is impor-tant to note that many works incorporate language models like BERT at some stage for encoding, with classification often performed using other techniques, such as LSTMs. However, in this section, we categorize each work based on its primary component. The presence or absence of BERT as an encoder or as part of the main RE component is detailed in Table 8.\nAdditionally, we observed that not all works adhere to the entire three-stage RE pipeline, with some focusing solely on entity detection or classifying pre-tagged entities within a dataset. To present our findings concisely, we have organized all reviewed works into tables categorized by conference and year, offering a comprehensive year-conference-task perspective on the cutting-edge developments in RE. It is also important to note that many reviewed papers do not explicitly specify how many stages of the relation extraction process they perform. We have inferred this information based on their models, methodologies, and reported results in such cases.\nOne of the key insights from the trend analysis is related to the models used. In the first two years of conferences, 2020 and 2022, we observe a trend toward using various models, including CNNs and LSTMs, though they are less prevalent. By 2022 and 2023, however, the focus has shifted to transformer-based techniques, highlighting their growing dominance and effectiveness.\nWhen examining the subtasks, it is clear that relation identification is the least frequently addressed. Most datasets are tagged with pre-identified relations and are used primarily for relation classification. This subtask, the primary focus in the literature, remains the most relevant component of relation extraction techniques.\nExamining the tasks and challenges within relation extraction reveals some compelling insights. Specifically, document-level relation extraction has gained significant prominence in the past two"}, {"title": "6 Large Language Models vs language models in RE", "content": "We reviewed various baseline approaches in the literature and compared the effectiveness of different language models for encoding relations. We survey and analyze existing studies that use various language models, including GPT-3, T5, and traditional models like BERT or ROBERTa. Our aim is two-fold: firstly, to assess the relative performance of different model categories in the challenging task of RE, and secondly, to evaluate whether the emergence of Large Language Models represents a significant advancement over traditional models in performance. Our study delves into Few-Shot relation extraction, document-level relation extraction, and traditional relation extraction.\nWe have selected the top five models for each benchmark dataset associated with these tasks: FewRel, DocRed, and TACRED. This approach allows us to capture various methodologies and"}, {"title": "7 Discussion", "content": "In our descriptive analysis, w have noticed a growing interest in using language models to solve Relation Extraction (RE) problems. As is shown in Figure 2, the number of publications on this topic has been consistently increasing, with the most recent year reaching the highest point. This continued growth shows that the field is active and encourages more research and development.\nIt is important to consider that some conferences occur biennially. We created a year-conference graph in Figure 3 to address this. Although the data for 2023 appears underrepresented compared to 2022, with 3 and 2 conferences, the overall trend remains upward.\nIn terms of application areas, our analysis reveals that relation extraction has found application in diverse domains, including media, academia, economics, and even unconventional realms such as heritage conservation and mathematics. This broad applicability underscores the technique's usefulness and emphasizes the need for new systems capable of autonomously identifying novel relations in an ever-changing landscape."}, {"title": "7.1 RQ1: What are the challenges of RE that are being solved by systems that leverage language models?", "content": "We have identified areas for improvement and further exploration in the following challenges:\nDocument-level RE: While progress has been made in sentence-level RE, document-level RE remains challenging. Extending the capabilities of language models to capture relationships and context across entire documents deserves further attention. With their ability to capture broader contexts, large language models present a promising avenue for exploration in this area.\nMultimodal RE: Integrating information from diverse modalities, such as text and images, poses a distinct challenge. Enhancing language models to extract relations from multimodal data effectively requires fusing models for text and pictures. Despite some progress, there is still room for improvement, especially in benchmark datasets.\nMultilingual RE: Handling relations in languages not encountered during training or extract-ing relations with no training examples remains challenging. While many language models can generalize to other languages, their performance may still need to be bettered. Developing sys-tems and training strategies, especially in transfer learning, is essential to address this challenge effectively.\nFew-Shot and Low-Resource Relation Extraction: This category encompasses areas with a scarcity of training examples, such as medical or biological datasets, and relations in tradi-tional domains but with unseen relations. Large language models can offer valuable solutions in these domains, leveraging their ability to generate new content for training strategies and learn complex relations and contexts. Significant improvements have been observed in distant supervision RE, where automatically labeled datasets are created from knowledge bases, and large language models can contribute to further enhancements.\nOpen Relation Extraction: Developing techniques to identify and characterize relations with-out predefined categories is an area that requires further investigation. Unsupervised techniques, such as clustering and large language models, hold promise in addressing this challenge."}, {"title": "7.2 RQ2: What are the most commonly used language models for the RE problem?", "content": "It is evident that BERT stands out as the predominant model across the literature, featuring prominently in 45 different papers, constituting almost 55% of the recent papers about extraction. The second most widely used model is ROBERTa achieving state-of-the-art results, specially in areas like document-level relation extraction. When examining the percentage of papers that incorporate or at least mention large language models such as T5 or GPT in their methodology, we find that only 8.5% of papers leverage these language models.\nBERT is one of the oldest models studied in this survey. Its popularity may be influenced by the fact that it was one of the first language models widely adopted. To address this potential bias, we performed an analysis that normalizes the number of times a paper references a model by the number of years since model release. The formula used is tu/y, where tu represents the frequency of use as shown in Table 8, and y denotes the number of years from model release to the end of the survey in 2023. In case of BERT, was released in 2018 so, y = 2023 \u2013 2018, 5. This approach yields a factor of 9, which remains higher than newer models like T5 or GPT.\nThis underscores the prevalence of BERT-based methods being the most widespread in most literature. The preference for BERT-based models in the realm of RE can be attributed to their inherent characteristics that align seamlessly with the nature of RE systems:\nBERT is pre-trained using two objectives: MLM and NSP. The MLM aspect predicts missing words in a sentence, while NSP helps the model understand relationships between consecutive"}, {"title": "7.3 RQ3: Which datasets are used as benchmarks for RE using language models?", "content": "Table 10 presents the utilization of datasets in ACL conferences. Addressing RQ3 directly, the most prevalent benchmark datasets for RE have been TACRED, DocRed, and FewRel. Examining datasets with over ten recent cutting-edge RE research mentions reveals an intriguing pattern.\nFirstly, in traditional sentence-level RE, TACRED serves as the most robust benchmark. Re-searchers are actively exploring new techniques, including comparisons between joint models and pipeline approaches, methods for sentence encoding, and advancements in accurately marking the tails and heads of relations for each model using this dataset.\nSecondly, the domain of document-level relation extraction is gaining significant attention. Do-cRed, as a dataset, is instrumental in testing novel approaches and assessing recent advances. Researchers leverage this dataset to compare against baselines, fostering the ongoing evolution of methodologies in document-level RE."}, {"title": "7.4 RQ4: Are new large language models like GPT or T5 useful in RE versus widespread models such as BERT?", "content": "To address RQ4, we analyzed the performance of various models on three benchmark datasets in Section 6 and reviewed the most widespread models used in Table 8. The results shed light on the effectiveness of new large language models like GPT and T5 compared to well-established models like BERT.\nDespite their promising capabilities, large language models, including GPT and T5, are used less frequently than models such as ROBERTa and BERT. This phenomenon could be attributed to the inherent suitability of these language models for downstream tasks like chatbots, where their remarkable ability to capture extensive contextual information is more prominently leveraged. The current landscape suggests that large language models do not play a central role in advancing state-of-the-art performance in extraction tasks. However, it is essential to note that the field is dynamic, and the influence of these models may evolve with ongoing research and advancements."}, {"title": "7.5 Synthesis", "content": "We have identified two primary methods of RE: joint and pipeline. Joint models are designed to simultaneously identify entities and relationships, capturing the inherent interdependence between these two tasks. In contrast, the pipeline approach follows a sequential process, starting with NER to identify entities, followed by at least a classification stage for these recognized entities. The pipeline approach can involve up to three stages, with an additional step of relation identification between NER and relation classification. While this two-step or three-step process allows for more specialized handling of each task, it also introduces the risk of error accumulation. In recent years, joint approaches have gained more attention, mainly due to concerns that the pipeline method is prone to error propagation.\nFuture trends in the realm of new language models, including sub-models and large language models, point toward increasingly complex and intricate models with more parameters. In the context of RE, these advanced models, with their extended context windows, will be particularly valuable for document-level tasks. They can maintain entire documents within their context, improving the handling of extensive information. Furthermore, in multilingual and multimodal settings, the vast context capabilities of these LLMs will enhance understanding across different languages and modalities.\nBefore concluding this section, it is essential to address this research's ethical considerations and global impact. All reviewed papers adhere to ethical standards by utilizing anonymized or publicly available data, ensuring their contributions to society are made responsibly. They are committed to minimizing bias and avoiding potential ethical issues, reflecting a conscientious approach to research and its broader implications. Finally, the environmental impact is minimal, considering the CO2 emissions and energy consumption associated with the research discussed."}, {"title": "8 Conclusion", "content": "This paper reviews cutting-edge relation extraction, explicitly focusing on models leveraging lan-guage models. Our analysis encompasses 137 papers, spanning dataset proposals, novel techniques, and excluded studies.\nOur research focuses on 65 papers, which we have rigorously categorized into eight sub-tasks reflecting the latest advances in RE. From these papers, we have extracted crucial insights into training strategies, novel methodologies, and the utilization of language models, which have emerged as a cornerstone of contemporary RE research.\nOur findings highlight the dominance of BERT-based approaches to extraction, underscoring their efficacy in achieving state-of-the-art results. Notably, BERT has emerged as the most widely utilized model for extraction to date. Additionally, emerging large language models like T5 exhibit"}]}