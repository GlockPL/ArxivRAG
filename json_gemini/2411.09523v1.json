{"title": "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents", "authors": ["YUYOU GAN", "YONG YANG", "ZHE MA", "PING HE", "RUI ZENG", "YIMING WANG", "QINGMING LI", "CHUNYI ZHOU", "SONGZE LI", "TING WANG", "YUNJUN GAO", "YINGCAI WU", "SHOULING JI"], "abstract": "With the continuous development of large language models (LLMs), transformer-based models have made groundbreaking advances in numerous natural language processing (NLP) tasks, leading to the emergence of a series of agents that use LLMs as their control hub. While LLMs have achieved success in various tasks, they face numerous security and privacy threats, which become even more severe in the agent scenarios. To enhance the reliability of LLM-based applications, a range of research has emerged to assess and mitigate these risks from different perspectives.\nTo help researchers gain a comprehensive understanding of various risks, this survey collects and analyzes the different threats faced by these agents. To address the challenges posed by previous taxonomies in handling cross-module and cross-stage threats, we propose a novel taxonomy framework based on the sources and impacts. Additionally, we identify six key features of LLM-based agents, based on which we summarize the current research progress and analyze their limitations. Subsequently, we select four representative agents as case studies to analyze the risks they may face in practical use. Finally, based on the aforementioned analyses, we propose future research directions from the perspectives of data, methodology, and policy, respectively.", "sections": [{"title": "1 Introduction", "content": "With the continuous development of language models (LMs), LLMs based on the transformer architecture [259] have achieved significant success in various fields of NLP [62, 214]. The massive number of parameters and extensive training data endow LLMs with strong capabilities in tasks like text generation [24, 253], code assistance [79, 272], logical reasoning [277, 297], etc. Due to their powerful understanding capabilities, an increasing number of studies are positioning LLMs as the core decision-making hub of AI agents [189, 235], which are sophisticated software programs designed to autonomously perform tasks on behalf of users or other systems. Compared to earlier AI agents based on heuristic algorithms or reinforcement learning [154, 185], LLM-based agents can communicate with users, making them easier to understand and accept. Additionally, their vast foundational knowledge allows them to think in a manner similar to humans (understanding + planning). These characteristics contribute to their popularity, making them a promising direction for AI to serve various practical fields [263, 310, 317]. For example, Supertools [247] is a comprehensive collection of trending applications empowered by LLMs.\nDespite the significant success of LLMs, they also face security and privacy threats due to inner vulnerabilities or outer attacks. LLM-based agents add some components and functionalities, which makes these risks even more threatening. For example, LLMs face jailbreaking attacks [168, 233], which refer to the process to bypass their built-in safety mechanisms [17, 198]. In the context of LLM-based agents, LLMs need to handle multi-round dialogues and multiple sources of information, making jailbreaking attacks more complex and difficult to defend against [12, 47]. To uncover the vulnerabilities of LLM-based agents and make them more secure and reliable, an increasing number of studies focus on the threats from various perspectives.\nTo help researchers better understand LLM-based agents and pursue future research work, there exist two surveys [53, 61] to summarize the security risks of LLM-based agents. They categorize the security risks based on the composition (called modules) or operational phases (called stages) of the agents as follows. (i) The module perspectives. Cui et al. [53] identified four key modules in LLM-based systems, i.e., the input module, the LM module, the toolchain module, and the output module. They summarize the risks of LLM-based agents based on the four modules. (ii) The stage perspectives. Deng et al. [61] identified four key knowledge gaps in LLM-based AI agents, i.e., the stage of perception, the stage of internal execution, the stage of action in environment, and the stage of interaction with untrusted external entities. They summarize the risks of LLM-based agents based on the four stages. These two taxonomies clearly highlight the sources of attacks faced by LLM-based agents. However, they struggle to accurately pinpoint threats that span across modules and stages. For example, privacy leakage is caused by memory issues within the language model module, but it occurs at the output module. Similarly, goal hijacking can happen not only during the perception stage but also during the interaction stage with external data [5]. These cross-module and cross-stage threats are inaccurately pinpointed to a single module or stage.\nTo categorize various threats of LLM-based agents more accurately and comprehensively, we propose a novel taxonomy by mapping the threats into a binary table based on their sources and types. (i) For the sources of threats, we consider the operational nature of LLM-based agents: LLMs make decisions based on inputs from multiple sources, as shown in Fig. 1 left. As a probabilistic model, the output decision distribution of an LLM is determined by both the input and the model itself. Therefore, we attribute the threats to LLM-based agents to the inputs, the model, or a combination of both. Compared to categorizing attacks by modules or stages, our classification of sources is closer to the essence of the threat. For example, goal hijacking [109, 169, 213] may originate from a user input or an external database, but both fundamentally act as inputs to the model for hijacking the goal. (ii) For the types of threats, we categorize the threats into three classes: security/safety, privacy, and ethics. Specifically, if a threat results in the model producing incorrect outputs (including errors that are factually inaccurate or do not align with the needs of developers or users), it is categorized as a security/safety issue, such as adversarial examples [299]. If a threat leads to the leakage of privacy, it is classified as a privacy issue, such as prompt leakage attacks [303, 323]. If a threat does not produce \"incorrect\" outputs but raises concerns such as unfairness, it falls under ethical issues, such as bias [81].\nWe collect papers from the top conferences and highly cited arXiv papers. Top conferences are included but not limited: IEEE S&P, ACM CCS, USENIX Security, NDSS, ACL, CVPR, NIPS, ICML, and ICLR. We categorize different kinds of threats with our taxonomy in Fig. 1 right. For threats originating from inputs, we refer to them as problematic inputs. In this scenario, attackers cannot modify the model but can design inputs to induce malicious outputs or behaviors, e.g., the adversarial example. For threats from within the model, we refer to them as model flaws. In this scenario, the inputs are always benign, but the model's own defects lead to malicious outputs or behaviors, e.g., the hallucination problem. For threats arising from both model flaws and carefully crafted inputs, we refer to them as combined threats. In this scenario, the inputs are deliberately designed by attackers to exploit the model's vulnerabilities, e.g., the backdoor attack.\nCompared with recent surveys [53, 61] on the security risks of LLM-based agents, there are three main advantages of our work.\n(i) A Novel Taxonomy of Threats. We propose a novel taxonomy that maps threats into a binary table based on their sources and impacts, which can comprehensively cover the existing threats and extend to future threats, including the cross-module and cross-stage threats.\n(ii) Detailed Analysis of Multi-modal Large Language Models (MLLMs). Many tasks require agents to handle inputs from multiple modalities, (e.g., city navigation systems [310]), leading to the emergence of a range of MLLMs and agents based on these models [295, 310]. Previous surveys primarily focus on the text modality, lacking analysis of multimodal models. We cover both LLMs and MLLMs, placing particular emphasis on analyzing the new challenges and threats posed by multimodal tasks in the context of threats.\n(iii) Four Carefully Selected Case Studies. Previous surveys analyze the risks based on a general framework of LLM-based agents (or systems). However, actual agents may not necessarily contain all modules in the general framework, and the designs within these modules may also be customized [66]. More importantly, the scenarios they face have significant differences, resulting in the varying levels and causes of threats. To help readers better understand the actual threats faced by agents, we present case studies of four different agents, representing four classic situations in Section 6.\nThis paper is organized as follows. Section 2 introduces a general framework of LLM-based agents and identifies six key features of the framework. Sections 3, 4, and 5 depict the risks from problematic inputs, model flaws, and input-model interaction, respectively. Section 6 offers four carefully selected case studies. Section 7 gives future directions for the development of this field."}, {"title": "2 LLM-based Agent", "content": "Al agents are considered promising a research direction that utilize Al technology to autonomously execute specific tasks and make decisions. In previous researches, Al agents often achieved good results in specific scenarios (such as playing games) through heuristic strategies or reinforcement learning [154][185][226][279]. In recent years, LLMs, such as ChatGPT, have attracted substantial attention from both academia, and industry, due to their remarkable performance on various NLP tasks [24] [321][259]. Therefore, there is an increasing amount of work studying the use of LLMs as the decision-making center for Al agents [189][9][200]. With the development of LLMs, LLMs can handle more modalities and tasks [235].\nFramework of LLM-based Agents. In our work, we consider a comprehensive framework of an LLM-based agent that covers the modules and runtime modes of mainstream LLM-based agents, as shown in Fig. 2. This framework contains the following four modules.\n(i) Input Module (IM). IM receives the users' inputs and preprocesses them as follows. First, IM formats the inputs to a specific distribution (e.g., normalize an input image) or a specific format (e.g., a special language [229]). Second, IM implements harmful information detection [221][293] or purification [229]. Third, many LLM-based agents add a system prompt before the inputs [189][235].\n(ii) Decision Module (DM). DM understands and analyzes the query of the user, gives plans and generates the final response to the user. Many agents' decision modules only contain one LLM. They leverage an LLM for understanding, planning, and feedback [189] [235], or use an LLM for understanding and feedback, with another non-LLM planner handling the planning [159] [54]. As tasks become more complex, many agents employ multiple LLMs to accomplish the aforementioned tasks. For example, VOYAGER [263] uses GPT-4 and GPT-3.5 to handle the tasks of understanding, planning, and generating a skill library, respectively. Huang et al. [106] used GPT-3 for task planning, while leveraging BERT to translate the plans into admissible actions.\n(iii) External Entities (EE). With the task becoming more complex, the agents need the help of the external modules, including memory module, external tools, the other agents and the environment. The memory module is used to store and retrieve relevant information to improve the coherence and context-awareness of the agent's responses. In this paper, we adopt the definition of agents' memory from [327], considering external databases as a form of agents' memory as well. External tools integrate numerous APIs to fulfill the user's requirements (e.g., search engine APIs for Webgpt [189] and APIs for controlling the robotic arm [9]). Sometimes, multiple agents need to collaborate to complete a task, where one agent needs to interact with other agents [98].\n(iv) Output Module (OM). There might be zero or multiple interaction between DM and EE to accomplish the task. After that, DM generates the response and delivers it to the user through OM. Agents can implement harmful information detection or purification on the output [103].\nBased on this framework, we identify six key features of LLM-based agents, which involve new attack surfaces compared with the traditional DNN models and the RL-based agents. As shown in Fig. 3, these six key features are as follows. (i) LLM-based controller. LLMs serve as the core of agents, leveraging transformer architecture, vast amounts of knowledge, and massive training data to confer strong understanding capabilities, while also introducing new risks. (ii) Multi-modal inputs and outputs. As agents become capable of handling increasingly complex tasks, many scenarios require the processing of multimodal information. Research indicates that risks vary across different modalities, and their interaction in multimodal systems presents unique challenges and opportunities. (iii) Multi-source inputs. The inputs to LLMs within agents consist of multiple components from different sources, such as user input, system prompts, memory, and environmental feedback. Compared to a standalone LLM, multi-source inputs present new opportunities and challenges for both attackers and defenders. (iv) Multi-round interaction. Agents often require multiple rounds of interaction (with the environment, users, other LLMs, etc.) to complete tasks, leading to longer and more complex inputs for LLMs, which may exacerbate certain threats. (v) Memory mechanism. The memory mechanisms in agents can help accumulate experience and enhance knowledge, improving their ability to handle various tasks, but they also introduce new security and privacy risks. (vi) Tool invocation. LLMs are specially crafted with instruction-tuning data designed for tool usage. This process enables LLMs to handle complex tasks, but it may also result in more severe consequences and introduce new vulnerabilities."}, {"title": "3 Risks from Problematic Inputs", "content": "This section focuses on the risks that arise due to issues with the input data, such as adversarial examples, prompt injection, etc. that can lead to problems with the LLM-based agent's performance and behavior. Compared with a standalone LLM, the decision module of an LLM-based agent can receive inputs from different modules, which increases its attack surface. For each risk, we first introduce what they are, then summarize their technological advancements in the six key features of LLM-based agents, and finally analyze their limitations."}, {"title": "3.1 Adversarial Example", "content": "An adversarial example is an adversarial-perturbed sample preserving the semantics but misclassified by the deep learning models. Specifically,\n$8^* = arg \\min_{\\delta \\in \\Delta} SemDis(x, x + 8)$  (1)\ns.t. $\\begin{cases}g(x + 8^*) \\neq 0 (Untargeted)\\\\ g(x + 8^*) = t (Targeted)\\end{cases}$\nwhere SemDis() denotes the semantic distance between the perturbed sample and the original sample, $\\Delta$ represents the feasible perturbation space, and g() signifies the target model. If the adversarial perturbation makes the target model misclassify the original label o of the sample, it represents the untargeted attack (g(x + 8*) $\\neq$ 0). If the adversarial perturbation makes the target model misclassify the sample to a target label t, it represents the targeted attack (g(x + 8*) = t).\nThe research of adversarial examples has passed over ten years [249], raising attention in many domains, e.g., autonomous driving [73], malware detection [94], reinforcement learning [285], etc. Szegedy et al. [249] first discovered the adversarial example in the neural networks, which opens Pandora's box of the adversarial example. According to the knowledge of the attacker, the adversarial example attack methods can be categorized into perfect knowledge attack, limited knowledge attack, and zero knowledge attack. The history of adversarial example attacks starts from the perfect knowledge attack [37] to the zero knowledge attack [43], which is a more practical setting. Correspondingly,\nthe development of the defense method is from the empirical methods, e.g., defensive distillation [199], obfuscated gradients [13, 25], etc, to the theoretical methods, certificated robustness [70, 179]. The arms race of adversarial example attacks and defenses exists from deep learning models and LLMs to LLM-based AI agents. In the context of LLM-based agents, as shown in 5, the development of adversarial examples primarily involves four key features: LLM-based controller, multi-modal inputs and outputs, multi-source inputs, and multi-round interaction. In the following, we review the recent advancements in both attack and defense perspectives."}, {"title": "3.1.1 Technical Progress. Attack Perspective.", "content": "As discussed in section 2, the input and output interactions of LLM-based Al agents are characterized by their handling of multi-modal data across multiple rounds of interaction. This complexity necessitates a nuanced approach to adversarial example attacks, which increasingly focus on the relationships between different modalities within these interactions.\nRecent research in this area has produced several sophisticated methods for attacking multi-modal systems. For instance, RIATIG [161] introduces a reliable and imperceptible adversarial example attack targeting text-to-image models. This method employs a genetic-based optimization loss function aimed at improving the quality of adversarial samples, ensuring that the generated examples are both effective and difficult to detect. VLATTACK [299], advances the field by generating adversarial samples that fuse perturbations from both images and text. This fusion occurs at both single-modal and multi-modal levels, making the attacks more versatile and challenging to defend against. The method's ability to operate across modalities highlights the increasing sophistication of adversarial techniques as they target the interconnected nature of multi-modal systems.\nBeyond direct adversarial attacks, there is significant focus on the transferability of adversarial examples across different vision-language models (VLMs). For example, SGA [172] generates adversarial examples by leveraging diverse cross-modal interactions among multiple image-text pairs. This method incorporates alignment-preserving augmentation combined with cross-modal guidance, allowing adversarial examples to maintain their efficacy across various models and tasks. Similarly, TMM [264] enhances the transferability of adversarial examples through attention-directed feature perturbation. By targeting critical attention regions and disrupting modality-consistency features, this approach increases the likelihood that adversarial examples will succeed across different VLMs.\nAnother line of adversarial example attack methods specifically targets downstream applications that involve multiple rounds of interaction. For instance, Liu et al. [163] proposed imitation adversarial example attacks against neural ranking models, with the goal of manipulating ranking results to achieve desired outcomes. This method exemplifies how adversarial attacks can exploit the iterative nature of certain applications to progressively distort the final output. Similarly, NatLogAttack [332] leverages adversarial examples to compromise models based on natural logic, introducing subtle perturbations that undermine the model's reasoning processes. In the domain of dialogue generation, DGSlow [146] generates adversarial examples by defining two objective loss functions that target both response accuracy and length. This approach ensures that the generated responses not only deviate from expected content but also manipulate the conversational flow, making the attack more disruptive."}, {"title": "Defense Perspective.", "content": "Defense methods against adversarial examples are broadly categorized into two primary types: input-level defenses and model-level defenses. Each of these approaches targets different aspects of the adversarial threat landscape, aiming to enhance the robustness of LLM-based AI agents against adversarial perturbations.\nInput-level defenses primarily focus on detecting and mitigating adversarial examples before they can influence the model's predictions. These defenses typically employ techniques for adversarial example detection and purification. Most of the existing input-level defense methods [18, 123, 140, 186, 273] in the domain of LLM-based AI agents leverage LLMs to identify and neutralize adversarial inputs effectively. For instance, ADFAR [18] implements multi-task learning techniques to enable LLMs to distinguish adversarial input samples from benign ones. Similarly, methods such as BERT-defense [123] and the approach proposed by Li et al. [140] utilized the BERT model to purify adversarial perturbations, thereby safeguarding the model's outputs from being compromised by malicious inputs. The SOTA input-level defense strategies have begun to focus on the prompt mechanisms within LLM-based Al agents, as discussed in section 2. For example, APT [139] enhances the robustness of the CLIP model by leveraging soft prompts, which serve as an additional layer of defense against adversarial manipulation by refining the model's input processing pipeline.\nModel-level defenses [67, 132, 165, 262, 336], on the other hand, are concerned with the architecture and parameters of the model itself. These defenses aim to create inherently robust models through techniques such as adversarial training and fine-tuning specific model parameters. For instance, RIFT [67] employs mutual information to achieve robust fine-tuning, and InforBERT [262] designs the information bottleneck regularizer and the anchored feature regularizer for adversarial training. To address the high computational cost associated with retraining entire models, some methods like SHIELD [132] propose retraining only the final layer of LLMs. This approach significantly reduces the training overhead while still providing a degree of robustness against adversarial examples. The most advanced model-level defense method currently available, Dynamic Attention [232], leverages a dynamic attention mechanism to enhance the robustness of transformer-based models. This method represents a significant advancement in the development of robust transformer-based models by dynamically adjusting the model's attention mechanisms in response to potential adversarial threats."}, {"title": "3.1.2 Discussion of Limitations. Attack Perspective.", "content": "Current adversarial attack methods are primarily focused on untargeted attacks, where the attack objectives are not precisely defined. As a result, the outcomes of these attacks cannot be explicitly controlled to induce specific, pre-determined misbehavior. For example, adversarial perturbations applied to images fail to generate targeted responses, such as causing a specific erroneous answer in visual question-answering tasks. Additionally, as discussed in Section 3.1.1, existing adversarial attack strategies aimed at multi-modal systems often engage with multiple modalities simultaneously. However, the constraint metrics used to evaluate the success of these attacks are typically designed for single-modality scenarios. This approach may be inadequate when adversarial perturbations must be applied across different modalities, as it does not account for the unique interactions between distinct data types.\nFurthermore, as discussed in Section 3.1.1, the scope of current adversarial example attacks remains confined to targeting the output module of LLM-based agents. However, there are additional, unexplored targets for adversarial example attacks. Specifically, vulnerabilities may exist within the memory, external tool interfaces, and the planner components of these agents, which remain under-investigated. For instance, adversarial example attacks could potentially disrupt the planning capabilities of LLM-based agents, leading them to devise incorrect or suboptimal plans. Expanding the attack surface beyond the output module to include these other critical components could reveal new dimensions of adversarial risks in complex systems.\nDefense Perspective. Current defense mechanisms against adversarial examples in LLM-based agents remain constrained to single-modal inputs and the robustness of individual models. For instance, dynamic attention [232], a state-of-the-art adversarial defense technique within LLM-based agents, is limited to NLP tasks. However, LLM-based AI agents are increasingly handling multi-modal input data that extends beyond the scope of a single model. Furthermore, the decision-making module within these agents may incorporate multiple LLMs, each exhibiting varying degrees of robustness against adversarial attacks. Despite this, existing defense strategies focus exclusively on enhancing the robustness of a single model, neglecting the broader issue of joint robustness across multiple models integrated into the system.\nIn addition, current adversarial defense methods for LLM-based AI agents overlook key components such as the memory and planner modules, which may provide additional avenues for defending against adversarial examples. For instance, the memory bank could be leveraged to detect and counteract adversarial attack patterns by recognizing recurring attack tactics. Future strategies could extend the defense scope to include these often overlooked modules, achieving more comprehensive protection against adversarial threats."}, {"title": "3.2 Goal Hijacking", "content": "Goal hijacking refers to an attack strategy in which an adversary manipulates the objective or behavior of an AI model, causing it to deviate from its intended purpose. By introducing adversarial inputs or modifying the system's environment, the attacker can influence the model to pursue the attacker's desired outcome instead of the original goal. A naive attack can achieve goal hijacking of a large model by inserting \"ignore the previous instruction...\" into the user's reference statement, thus shifting the model's response to meet the attacker's requirements.\nIn the context of LLM-based agents, the sources and targets of goal hijacking attacks have become more varied. As shown in Fig. 6, the development of goal hijacking in LLM-based agents primarily involves six key features: LLM-based controller, multi-modal inputs, multi-source inputs, multi-round interaction, memory mechanism, and tool invocation. In the following, we review the recent advancements in both attack and defense perspectives."}, {"title": "3.2.1 Technical Progress. Attack Perspective.", "content": "Early attempts to exploit this vulnerability used heuristic prompts, such as \"ignore the previous question,\" to achieve targeted hijacking attacks on standalone LLMs [207]. In order to make the attacks more covert and successful, more carefully designed methods have been proposed. In terms of attack methods, some approaches use vocabulary searches to obtain more covert attack prompts [136], while others leverage gradient optimization to obtain higher success rate and transferable adversarial prompts [109, 169, 213].\nAs LLMs are applied to different domains and tasks, researchers have begun to focus on the forms and methods of targeted hijacking attacks in various scenarios. In multimodal scenarios, researchers have found that semantic injections in the visual modality can hijack LLMs [126]. For memory modules, researchers have discovered that target hijacking can be achieved by contaminating the database of RAG [202]. In multi-round interaction scenarios, researchers have found that confusing the model can be achieved by forging chat logs [276]. Regarding tool invocation, researchers have exposed the threat of goal hijacking to LLM-based agents using tools through analysis of actual tool-integrated LLMs and the establishment of benchmarks [88, 314]."}, {"title": "Defense Perspective.", "content": "Current defenses against goal hijacking can be categorized into two main types. The first type involves defenses from an external perspective. The second type focuses on defenses from an endogenous perspective.\nFrom the perspective of external defenses, strategies primarily involve prompt engineering and prompt purification. Hines et al. [97] introduced strategies such as segmentation, data marking, and encoding, which enhance the LLM's ability to recognize inputs from multiple sources and thus effectively defend against goal hijacking. Sharma et al. [229] introduced a system prompt meta-language, a domain-specific language designed to refine prompts and monitor inputs for LLM-based chatbots to guard against attacks. They developed a system that utilizes this language to conduct real-time inspection of attack prompts, ensuring user inputs align with the chatbot's definitions and thus preventing malicious operations. Additionally, Chen et al. [45] proposed a defense method for structured queries that separates prompts and data to counteract goal hijacking.\nEndogenous defenses primarily involve fine-tuning and neuron activation anomaly detection. Wallace et al. [261] proposed a fine-tuning method that establishes an instruction hierarchy, enabling the model to prioritize privileged instructions for defending against attacks, such as goal hijacking. Using supervised fine-tuning, they trained the model to recognize and execute instructions across different privilege levels, thereby enhancing its robustness against attacks. Piet et al. [208] introduced Jatmo, a method using task-specific fine-tuning to create models resistant to goal hijacking. They showed that Jatmo leverages a teacher model to generate task-specific datasets and fine-tune a base model, effectively defending against goal hijacking. Abdelnabi et al. [5] explored detecting task drift caused by inputs by analyzing the activations of LLMs. They showed how comparing activations before and after processing external data can detect task changes, effectively identifying task drift induced by goal hijacking."}, {"title": "3.2.2 Discussion of Limitations.", "content": "Current defenses against multimodal goal hijacking are insufficient. Attackers can leverage multiple modalities and their combinations for covert attacks, making defense more complex. Effectively defending against goal hijacking in multimodal inputs is a crucial direction for future research. Moreover, existing external defenses are often tailored to specific types of attacks. Developing a universal external defense strategy is an important area to explore. Finally, detecting goal hijacking from a neuronal perspective holds potential. Systematic testing is needed to determine whether the activation values of target neurons can effectively indicate anomalies associated with goal hijacking, thus proposing an efficient endogenous defense strategy from this perspective."}, {"title": "3.3 Model Extraction", "content": "Model extraction (stealing) attacks aim to achieve performance close to that of the black-box commercial models while incurring a relatively low computational cost. Attackers carefully design a set of inputs in order to steal the structure, parameters, or functionality of the target model. In the context of LLM-based agents, the development of model extraction attacks mainly involves LLM-based controllers. In the following, we review the recent advancements in both attack and defense perspectives."}, {"title": "3.3.1 Technical Progress. Attack Perspective.", "content": "In traditional DNNs, attackers typically have two main objectives. 1. Make the surrogate model's performance as consistent as possible with the target model (i.e., function-level extraction). 2. Make the substitute model's parameters as consistent as possible with the target model (i.e., parameter-level extraction) [33, 113, 183, 228]. With the introduction of the transformer, NLP tasks evolve from RNN structures to transformer-based structures. The scale of LMs also become larger: from the relatively large BERT to the open-source large model LLaMA, and further to the extremely large commercial models like GPT-4. Model extraction attacks also become more challenging. On BERT, there is not yet any work on achieving parameter-level attacks on the entire model. A few papers discuss function-level extraction [95, 130, 288, 308]. Their attack logic is consistent with the traditional DNN scenario, mainly focusing on how to create the query dataset [95, 130] and the training loss function [288]. On commercial models, due to the cost constraints of training substitute models, model extraction attacks focus on stealing a part of the target model. Li et al. [147] trained a model (e.g., CodeBERT [79] and CodeT5 [272]) to extract the specialized code abilities of text-davinci-003. Naseh et al. [191] stole the decoding algorithm of LLM. Carlini et al. [35] stole the last layer of a production LLM."}, {"title": "Defense Perspective.", "content": "In traditional DNNs, the defenders typically have two lines to defend against model extraction attacks. 1. Active defense: prevent the model from being extracted. 2. Passive defense: verify the ownership of the extracted model. As LMs become larger, active defense in the LLM scenario is still an area to be explored. Researchers have mainly considered passive defenses, which add watermarks to the model's outputs as a way to verify ownership. The advantage of watermarking is that it does not require modifying the model itself, but only perturbing the model's inputs. For example, Zhao et al. [329] perturbed the probability vector of transformer; He et al. [96] perturbed the generated words of Bart [137]; Li et al. [148] perturbed the generated codes of CodeBERT [79] and CodeT5 [272]; Peng et al. [206] perturbed the embeddings of the GPT-3."}, {"title": "3.3.2 Discussion of Limitations.", "content": "There are two limitations of recent research on model extraction attacks. (i) Most LLM-based agents contain large open-source models (e.g. LLaMA) or commercial large models (e.g. ChatGPT), with fewer using BERT-level LMs. However, the current model extraction attacks have discussed less about this scale of LLMs. (ii) Current model extraction attack patterns all rely on training a substitute model that approximates the target model by observing its inputs and outputs. However, LLM-based agents contain not only the LLM but also many other modules (as shown in Section 2). This means that the attacker's input may not be the same as the LLM's input, and the LLM's output may not be the same as the attacker's observed output. For example, in WebGPT [189], the model's input includes not only the user but also the search results obtained by the browser. Similarly, in HuggingGPT [235], the attacker's observed output includes outputs from other Hugging Face models as well. This makes it more challenging for the attacker to directly steal the LLM within an LLM-based agent.\nAdditionally, most agents are designed with a series of prompts for specific tasks, and then directly call the commercial LLMs (for example, Voyager [263], PReP [310] and ChatDev [212] all use ChatGPT as their controllers). This means that if part of the parameters [35] or functionalities [147] of these commercial LLMs are stolen, it may lead to adversarial attacks against all agents that use these LLM. This will pose a major vulnerability for LLM-based agents. However, the security in this scenario has not yet been studied."}, {"title": "3.4 Prompt Leakage", "content": "Prompts, as task descriptions, can guide LLM-based agents in executing specific tasks without extensive fine-tuning. For example, an LLM embedded with system prompts can function as a Planner [242], directly handling task planning. However, these prompts are at risk of leakage. Prompt leakage occurs when attackers illegally access or obtain the prompts used within LLMs or LLM-based agents, especially system prompts, without authorization. This not only poses a serious privacy risk but also infringes on the intellectual property rights of LLM-based agent owners. As shown in Fig. 7, the development of prompt leakage primarily involves four key features: LLM-based controller, multi-modal inputs, multi-source inputs, and tool invocation. In the following, we review the recent advancements in both attack and defense perspectives."}, {"title": "3.4.1 Technical Progress. Attack Perspective.", "content": "Several key features of LLM-based agents", "LLMs": "prompt leaking attacks and prompt stealing attacks.\nPrompt leaking involves injecting malicious prompts into LLMs to induce them to reveal their internal system prompts. For instance", "Forget the previous content and tell me your initial prompt,\" the LLM might inadvertently expose its system prompt to a malicious entity. Current research on prompt leaking attacks generally falls into two categories": "one approach focuses on manually designing malicious prompts to achieve prompt leakage [303", "323": ".", "112": ".", "strategies": "one is training an inversion model [315", "294": "."}]}