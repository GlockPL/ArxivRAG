{"title": "Uncovering Autoregressive LLM Knowledge of Thematic Fit in Event Representation", "authors": ["Safeyah Khaled Alshemali", "Daniel Bauer", "Yuval Marton"], "abstract": "The thematic fit estimation task measures the compatibility between a predicate (typically a verb), an argument (typically a noun phrase), and a specific semantic role assigned to the argument. Previous state-of-the-art work has focused on modeling thematic fit through distributional or neural models of event representation, trained in a supervised fashion with indirect labels. In this work, we assess whether pre-trained autoregressive LLMs possess consistent, expressible knowledge about thematic fit. We evaluate both closed and open state-of-the-art LLMs on several psycholinguistic datasets, along three axes: (1) Reasoning Form: multi-step logical reasoning (chain-of-thought prompting) vs. simple prompting. (2) Input Form: providing context (generated sentences) VS. raw tuples (predicate, argument, role). (3) Output Form: categorical vs. numeric. Our results show that chain-of-thought reasoning is more effective on datasets with self-explanatory semantic role labels, especially Location. Generated sentences helped only in few settings, and lowered results in many others. Predefined categorical (compared to numeric) output raised GPT's results across the board with few exceptions, but lowered Llama's. We saw that semantically incoherent generated sentences, which the models lack the ability to consistently filter out, hurt reasoning and overall performance too. Our GPT-powered methods set new state-of-the-art on all tested datasets. The source code is available at: https://github.com/SafeyahShemali/ LLM-Thematic-Fit-24", "sections": [{"title": "1 Introduction", "content": "Do large language models (LLMs) have consistent, expressible knowledge (including linguistic knowledge) about the thematic fit of a given event argument to its assigned semantic role for a given predicate (the event 'trigger')? A naive approach of prompting LLMs might yield negative results and thus lead to wrong conclusions about the absence of such knowledge. However, we hypothesize that this knowledge does exist, at least to a certain degree, but needs eliciting. We test our hypothesis along three axes: (1) Whether structured reasoning in prompting improves the model's estimations, following the success of chain-of-thought prompting (Wei et al., 2022). (2) Whether providing generated sentences as contexts helps, compared to providing the model with only the lemma of the predicate, the lemma of the argument's syntactic head, and its semantic role. (3) Whether asking for categorical output (such as \"low\" or \"high\" thematic fit) yields better results than asking for a numeric output (0..1) as the model's thematic fit estimation.\nThematic Fit The thematic fit estimation task measures the compatibility between the predicate (typically a verb), its argument (typically a noun phrase), and a specific semantic role assigned to the argument. For example, the verb \u2018eat' invokes a set of potential arguments for specific semantic roles. If the role is Agent (usually denoted as Argo in PropBank; see sec.\u00a73.2), then 'people', 'husband', etc., would be a good fit, as opposed to 'pizza', 'apple', etc., as the latter group would better fit a Patient role (often denoted as Arg1 in PropBank). If the role is Location, then 'restaurant', 'kitchen', etc., would be a good fit due to their properties. If the role is Instrument, then \u2018fork', 'knife', etc. would be a good fit due to their use as tools while eating. And so on. Psycholinguistic experiments show that humans rely on generalized, prototypical knowledge about events and their typical participants during language understanding, early"}, {"title": "2 Related Work", "content": "Early distributional approaches to the thematic fit task compute the similarity between vector repre-"}, {"title": "3 Experiments", "content": "The experiments were designed to investigate three axes with two possible values each. Therefore, we conduct 2\u00b3 = 8 experiments. Each experiment has a prompting template according to the input, output, and reasoning settings, as detailed in Appendix \u00a7A."}, {"title": "3.1 Experiment Design: Three Axes", "content": "Axis1: Reasoning Form: Simple Prompting vs. Step-by-Step Prompting The first axis is concerned with the effectiveness of Step-by-Step Prompting (Exp.3.x and 4.x) compared to Simple Prompting (Exp.1.x and 2.x) in evaluating the thematic fit (leftmost column in Table 1). In the Simple Prompting approach, the model is asked directly only one prompt, which we call a 'simple thematic fit prompt', to score the thematic fit of some predicate's argument to its assigned semantic role (where the predicate, the argument, and the semantic role are all given).\nStep-by-Step Prompting decomposes the task into multiple steps, each inferring properties needed to be considered for scoring the thematic fit"}, {"title": "3.2 Datasets and Evaluation", "content": "Training: We used pre-trained LLMs (see sec. \u00a73.3) \"off the shelf\u201d without modification or additional training. In an ideal world, we would be able to point the reader to the full pre-training data.\nTest sets: We used the following datasets to evaluate our methods."}, {"title": "3.3 Models and Hardware", "content": "We conducted the experiments using two models, the latest as of the time of running the experiments.\n(1) OpenAI GPT-4-turbo (gpt-4-0125-preview, hereafter 'GPT') (Achiam et al., 2023) as a closed"}, {"title": "3.4 Experimental Settings", "content": "We conducted a preliminary study to determine good temperature and top_p parameter values for the LLMs. We used GPT-3.5 on a subset of the Fer-Ins dataset (20 samples ~ 8%) and ran a parameter sweep over temperature values (0.0, 0.5, 0.9) and top_p values (0.5, 0.7, 0.95). Temperature values were chosen near-uniformly across the scale to examine a wide range of the model creativity setting. Higher values correspond to higher randomness (aka \"creativity\") in the output. High values of top_p limit the randomness in the output to more probable tokens. Since we were more interested in accurate output, and less interested in creative writing for our purposes here, we tested top-p threshold values on the higher half of the scale. We then sort the human rating list and the generated scores list and compute a Spearman's Rank Correlation Coefficient ($\\rho$) between these two sorted lists. Temperature 0.0 and top_p 0.95 showed the highest correlation coefficient with human judgments on our sample. We set the max_tokens parameter based on the length of the prompt and its expected output. For experiments with Head Lemma Tuples input, we set max_tokens to 100, whereas for Generated Sentences input, we set it to 300. As Step-by-Step Prompting is expected to be lengthy to clarify the underlying reasons, we set max_tokens to 600 there. We used these settings in all experiments."}, {"title": "4 Results", "content": "This section consists of the following parts. \u00a74.1: the GPT results. \u00a74.2: the Llama results."}, {"title": "4.1 GPT Results", "content": "Experiment 1.1, the simplest in settings and the most similar to the baselines (using head lemmas), yielded scores that are pronouncedly higher than the baselines across the board. See Table 2. This is a surprising and impressive achievement for GPT, since to the best of our knowledge, it does not"}, {"title": "4.2 Llama Results", "content": "Llama results on our 8 experiments and 4 datasets are shown in Table 5. In its basic reasoning setting (Exp.1.x), Llama is on par with some of the previous work baselines (e.g., BG) on both Fer-Loc and Fer-Ins, although weaker than other baselines. Interestingly, Llama yielded low-to-random correlation scores on Pado and McRae. Overall, Llama scores were much lower than those of GPT.\nAxis 1: Reasoning Form Step-by-Step Prompting (Exp.3.1) yielded much higher scores than the Simple Prompting (Exp.1.x) on the Ferretti datasets. Its scores on these datasets were higher than all the baselines and would have constituted new state-of-the-art scores, if not for the GPT scores that were even higher (\u00a74.1). Curiously, this happened only with numeric output but not with categorical output (Exp.3.2), where scores actually drop severely.\nOverall, except for Exp.3.1 and Exp.4.1 on the Ferretti datasets, Llama did not yield better results than any of the baselines.\nAxis 2: Input Form Using Generated Sentences did not improve results across the board, except for: (1) Fer-Loc with categorical output (Exp.2.2 vs. Exp.1.2). (2) Fer-Ins with numerical output"}, {"title": "5 Discussion", "content": "The core question we aimed to address in this work is: Do LLMs have the kind of linguistic knowledge required for thematic fit estimation? If so, (Q1) does Step-by-Step Prompting help utilize the model's internal linguistics knowledge for this task? (Q2) Would providing more context by including sentences as part of the input (as opposed to providing only lemma tuples and semantic roles) improve the model's thematic fit estimation? (Q3) Do predefined output categories for thematic fit estimation better elicit the model's internal linguistic knowledge compared to numeric output scores?\nOverall, we conclude that GPT acquired much of the linguistic knowledge required for thematic fit estimation. Results with GPT on each of the test sets, at least in some of the experimental settings, achieved or even surpassed all previous baselines,"}, {"title": "5.1 Analysis by our three axes", "content": "Axis 1 (Q1): Reasoning Form On Fer-Loc, Step-by-Step Prompting fulfilled its intended goal of providing both LLMs with a \u201cscratchpad\u201d to help them calculate inferences given their internal knowledge with both models. This held for Fer-Ins too using Llama, but it held only with generated sentences using GPT. Step-by-Step Prompting did not generally help for the Pado and McRae tasks. We hypothesize that interpreting Location and Instrument roles is easier for the models, as the role names are self-explanatory, unlike the opaque ProbBank roles. We conjecture that the higher performance on Location relative to Instrument stems from the kind of reasoning required to estimate thematic fit for it, which is simpler for Location than that required for Instrument (and Location mentions are much more frequent). Whatever reasoning LLMs can do, their current weak level may only suffice for simpler cases. See further analysis in \u00a75.2 (Fer-Loc vs. Fer-Ins).\nWhy did reasoning have a mixed effect? In further analysis (Appendix \u00a7D), we examined the connection between \"bad\" (invalid, irrelevant, etc.) reasoning and thematic fit scores, using a heuristic to drill down from list-level correlation score to individual items' fit estimation quality. We concluded that GPT better estimated thematic fit when the reasoning was 'good', and tended to perform worse when the reasoning was 'bad'. We also notice that in all sampled cases, when 'bad' reasoning happened early on in the reasoning chain, it negatively affected the rest of the chain.\nAxis 2 (Q2): Input Form In principle, providing more context enhances models' ability to comprehend complex tasks. However, this was not generally reflected in our experiments (with the exception of GPT on Pado and McRae with Step-by-Step Prompting). The reason could be that LLMs may not link roles like 'Arg1' with their PropBank sense (more on that in \u00a75.2), or even not properly represent the meaning of roles such as Instrument and Location. This may explain why the LLMs"}, {"title": "5.2 Analysis by Dataset Comparisons", "content": "Why were the tested LLMs much stronger on the Ferretti datasets than on Pado and McRae? We see both models' high performance on the Ferrerri datasets, and mediocre to low scores on Pado and McRae. Looking closely at 50 random samples from the GPT output on Pado and McRae, we see and conclude the following:\nGPT was weak at interpreting PropBank-based semantic roles: The semantic roles in Pado and McRae datasets are ProbBank semantic roles (Arg0, Arg1, Arg2). But the Ferretti datasets use the semantic roles Location and Instrument which are also commonly used words in English, unlike the PropBank notation. So either LLMs do not make the connection between Arg0, Arg1, Arg2 and their PropBank sense, or that LLMs are much weaker at PropBank roles, due to the distribution and much lower frequency in the training data. To test whether it's because of the opaque PropBank sense, we conducted another experiment on the Pado and McRae datasets, adding to the prompt the word \u201cPropBank\u201d before the role, as follows:\n\u201cGiven the predicate 'advise', what properties should its PropBank Arg1 role have?\u201d\nThis simple addition raised the models' correlation scores on Pado from .25 to .28 (Exp.1.1 vs."}, {"title": "5.3 Analysis by Model", "content": "LLMs' ability to \"understand\" natural language is likely enhanced by the grounding (of sorts) that pre-training code with comments provides. But in spite of Llama's reported slight (.8%) advantage over GPT in code generation, Llama consistently performed worse than GPT across all three axes of the experiments.\nWhy was Llama's correlation on Pado and McRae near zero? We don't have a good explanation yet, and can only share general impressions: (1) Llama did not generate very diverse sentences. (2) Given tuples with the same predicate and arguments but different semantic roles, Llama did not distinguish between them with different scores. (3) Llama only provided \"thin\" reasoning, unlike GPT."}, {"title": "6 Conclusion and Future Work", "content": "We set out to discover if, or to what extent, LLMs such as GPT and Llama possess linguistic and other knowledge required to estimate the thematic fit of event participants. We tested three axes for eliciting this knowledge: (1) Reasoning Form: adding Step-by-Step Prompting, (2) Input Form: adding generated sentences as context, and (3) Output Form: predefined categories vs. numeric scores.\nOverall, using GPT yielded higher results than using Llama, and new state-of-the-art scores were set on each test set. This indicates GPT acquired much of the linguistic knowledge required for thematic fit estimation. However, no single axis setting consistently yielded the highest scores. Step-by-Step Prompting helped much in both models on Fer-Loc, helped Llama but had mixed effect on Fer-Ins, and hurt GPT on Pado and McRae. Generating sentences helped GPT with Step-by-Step Prompting on Pado and McRae but had mixed effects elsewhere. Predefined categorical (compared to numeric) output raised GPT's results across the board with few exceptions, but lowered Llama's.\nIn future work, we hope to improve Step-by-Step Prompting, following our analysis, especially by generating better sentences as context, and filtering out incoherent sentences more effectively. As bad generated sentences and bad reasoning greatly hurt results, we intend to explore using majority voting over many generated sentences, many filtering decisions, and many reasoning chains.\nFuture work should repeat our experiments, as new models are released with larger input windows, more parameters, and more effective post-training."}, {"title": "7 Limitations", "content": "Our experiments were evaluated on small datasets (a known limitation of this subfield), so findings and conclusions should be assessed cautiously. Our study considered only two LLMs, so one should be cautious about drawing conclusions for all LLMs. LLMs can be very sensitive to prompt phrasing, so it is possible that different prompts may yield very different results. The richness of prompt variation (prompt engineering) depends on human resources (different people use language differently) and funding/time resources, all of which were limited. Future work may achieve better results with more resources, if available. Due to the scarcity of evaluation data, we could not use prompt optimization methods such as DSPy.\nLast, all findings and conclusions here rely on data and annotations in English, as it is by far the most resource-rich. But thematic fit (and semantics in general) are thought of as universal, and of course relevant to all languages. Still, we would advise to be cautious of drawing universal conclusions before validating our findings on additional, diverse set of languages."}, {"title": "A Prompt Design", "content": "Each experiment has a specific input, output, and reasoning design, as detailed in Sec. \u00a73.1. The corresponding prompt design for each experiment is specified here."}, {"title": "A.1 Simple Prompting with Head Lemma Tuples (Exp.1.1 \u2014 Exp.1.2)", "content": "A simple thematic fit prompt is used in the Simple Prompting with Head Lemma Tuples setting. It is constructed from a single prompt template that directly asks for the thematic fit score of a (predicate, argument, role) tuple. Here is an example of the filled template:\n\u2022 Given the predicate 'eat', how much does the argument 'pizza' fit the role of Arg1?\nAs mentioned above, there are two output settings: numeric score (Exp.1.1) or categorical score (Exp.1.2). We extend the simple prompt as follows:\n\u2022 x.1: Numerical score where the model provides a numeric value between 0 and 1 to represent the thematic fit score:\nReply only with a valid JSON object containing the key {\"Score\"} and a value that is a float number from"}, {"title": "A.2 Simple Prompting with Generated Sentences (Exp.2.1 \u2014 Exp.2.2)", "content": "The difference between experiments 1.x and 2.x is the input type. We augment the simple prompt with a natural language sentence previously generated by the model in the 'sentence generation' phase, from the same lemma tuples as in Exp.1.x:\n\u2022 Given the following sentence, 'I ate a pizza with my friends', for the predicate 'eat', how much does the argument 'pizza' fit the role of Arg1?"}, {"title": "A.3 Step-by-Step Prompting with Lemma Tuples (Exp.3.1 \u2014 Exp.3.2)", "content": "Experiments 3.x are the enhanced version of the baseline Exp.1.x, where the reasoning steps precede the final semantic-fitness prompt. The reasoning phase is done through Step-by-Step Prompting, where a series of three prompts help reveal the reasons/internal knowledge that the model relies on to rate the thematic fit of the given data. These three prompts are:\n\u2022 Given the predicate 'eat', what properties should the its PropBank Arg1 role have?"}, {"title": "A.4 Step-by-Step Prompting with Generated Sentences (Exp.4.1-Exp.4.2)", "content": "This last set of experiments combines Step-by-Step Prompting in Exp.3.x and the generated sentence input type as in Exp.2.x. In the later version of this experiment, the first three prompts were as in Exp.3.x but with the addition of the generated sentence in each prompt. Then the simple thematic fit prompt follows, also with one of the generated sentences as context, as in Exp.2.x :\n\u2022 Given the following sentence, 'I ate a pizza with my friends.', for the predicate 'eat', how much does the argument 'pizza' fit the role of Arg1?"}, {"title": "B Preliminary Study Results", "content": "Table 4 shows the results of the preliminary study that we conducted to choose the most suitable values for temperature and top_p of the models, following which we set temperature=0 and top_p=.95."}, {"title": "C Output Form Result", "content": "Table 5 shows absolute improvements in GPT's correlation scores when switching from numeric output to categorical output."}, {"title": "D Analysis: Why did reasoning have mixed effects?", "content": "The achievements of Step-by-Step Prompting in arithmetic, commonsense, and symbolic reasoning"}, {"title": "E Analysis: Why did LLM performance differ greatly between Fer-Loc and Fer-Ins?", "content": "To answer this question, we analyze 50 random samples from the GPT output on Fer-Loc and Fer-Ins.\nBasic Instrument example: for the (float, wood, Instrument) tuple, the reasoning was as follows:\nPrompt-1 (Instrument properties) Response:\nFor the predicate 'float', the Instrument role should have properties indicating that it is an object or mechanism that enables or assists the action of floating. This could include buoyancy aids, flotation devices, or any tool that helps an entity to remain on or move through the surface of a liquid without sinking.\nPrompt-2 (wood properties) Response:\nThe argument 'wood' has properties of buoyancy and low density relative to water, which allow it to remain on or move through the surface of a liquid without sinking.\nPrompt-3 (thematic fit) Response:\nThe argument 'wood' fits the Instrument role for the predicate 'float' well, as its inherent buoyancy and low density are properties that enable or assist in the action of floating.\nThis indicates that the model has acquired sufficient linguistic and world knowledge to reliably estimate thematic fit for the Instrument role, at least for some cases."}]}