{"title": "LLMR: Knowledge Distillation with a Large Language Model-Induced Reward", "authors": ["Dongheng Li", "Yongchang Hao", "Lili Mou"], "abstract": "Large language models have become increasingly popular and demonstrated remarkable performance in various natural language processing (NLP) tasks. However, these models are typically computationally expensive and difficult to be deployed in resource-constrained environments. In this paper, we propose LLMR, a novel knowledge distillation (KD) method based on a reward function induced from large language models. We conducted experiments on multiple datasets in the dialogue generation and summarization tasks. Empirical results demonstrate that our LLMR approach consistently outperforms traditional KD methods on different tasks and datasets.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have achieved remarkable performance in various text generation tasks, such as summarization (Ahmed and Devanbu, 2022; Nair et al., 2023) and dialogue systems (Deng et al., 2023; Cao et al., 2020). Moreover, this can be accomplished in a zero-shot manner, that is, a user enters a natural language prompt (e.g., \"Summarize the following text\u201d) and the LLM will generate a desired output for the task (Brown et al., 2020).\nHowever, LLMs also present significant challenges. For example, the GPT-3 model has 175 billion parameters, which is resource-intensive, requiring significant computing power and memory. This might hinder real-world applications in resource-constrained environments.\nTherefore, knowledge distillation (KD; Hinton et al., 2015) becomes an increasingly important research direction for LLMs (Gu et al., 2024; Wu et al., 2023; Hsieh et al., 2023), where the goal is to transfer the knowledge of LLM (called a teacher) to a smaller and more efficient model (called a student). Conventionally, this is accomplished by training the student from the teacher's predicted sentences or distributions (Kim and Rush, 2016). However, it has inherent limitations: during training, the student learns to predict the next word based on the teacher's previous predictions, whereas during inference, the student has to do so based on its own previous predictions. Such a discrepancy is known as exposure bias, and often leads to a performance degradation (Chiang and Chen, 2021; Ranzato et al., 2016).\nIn this paper, we propose a novel knowledge distilling method, based on reinforcement learning with a Large Language Model-induced Reward (dubbed LLMR). Instead of directly training from LLM's output, we first induce a q-value function from the LLM's policy (predicted probabilities) based on a widely adopted assumption (Chan and van der Schaar, 2021; Ramachandran and Amir, 2007; Ziebart et al., 2008), and then further induce a reward function based on the Bellman optimality equation (Sutton et al., 1999); this process follows our recent theoretical analysis between policies and rewards (Hao et al., 2022). The induced reward function is subsequently used to distill LLM's knowledge into the student, achieved by sampling a candidate sequence from the student-predicted distributions and evaluating it with the LLM-induced reward for policy gradient learning (Williams, 1992). In this way, our proposed LLMR distilling approach allows the student model to explore on its own during KD in a reinforcement learning (RL) fashion, thus alleviating the exposure bias problem.\nWe conducted experiments on two text generation tasks: dialogue generation and text summarization. Empirical results show that our LLMR approach largely outperforms traditional KD based on cross-entropy loss. We further quantitatively analyzed the exposure bias of the student models, verifying that RL indeed alleviates exposure bias arising during the KD process."}, {"title": "Related Work", "content": "Knowledge distillation (KD) is effective in reducing the computing and memory demands of large neural networks while retaining high performance. Common KD approaches include matching output distributions (Hinton et al., 2015; Wu et al., 2023) and matching intermediate-layer representations (Romero et al., 2015; Polino et al., 2018; Sun et al., 2019).\nKD has been applied to the sequence level for distilling text generation models (Kim and Rush, 2016; Wen et al., 2024) and autoregressive language models (West et al., 2022). Typically, the student learns from the teacher step by step with a cross-entropy loss, but such an approach may suffer from exposure bias (Ranzato et al., 2016). Researchers have proposed reverse Kullback-Leibler (Tu et al., 2020; Gu et al., 2024) and generalized f-divergence (Wen et al., 2023b) losses, which involve student sampling but still follow the spirit of traditional KD pushing the student's distribution to the teacher's step by step. In our LLMR method, on the other hand, the teacher only scores a student-sampled sequence, which allows more exploration during the KD process.\nReinforcement learning (RL) has been widely used for text generation, especially for alleviating exposure bias (Ranzato et al., 2016; Gu et al., 2024). A key design choice is the reward function, which in previous work is often given by task heuristics with groundtruth sequences (Sokolov et al., 2016; Pang and He, 2021) or trained reward models (Bahdanau et al., 2017; Paulus et al., 2018). Our LLMR method follows previous theoretical work (Hao et al., 2022), but directly induces a reward function from a pretrained LLM in a principled and task-agnostic manner."}, {"title": "Approach", "content": "Problem Formulation. Knowledge distillation (KD) aims to transfer the knowledge of a teacher model to a student. Although the student can solely learn a task from a parallel corpus \\(D_p = \\{(x^{(i)},y^{(i)})\\}_1^n\\), it is argued that the teacher's predicted distribution contains more knowledge than an annotated label y (Hinton et al., 2015). Kim and Rush (2016) propose SeqKD and minimize a Kullbuck-Leibler loss, equivalent to minimizing a cross-entropy loss, at the sequence level between a teacher \\(p\\) and a student \\(q_\\theta\\) by \\(J_{SeqKD} = E_{y \\sim p}[-\\log q_\\theta(y|x)]\\). In practice, the expectation over the sentence space is intractable. To tackle this, they use a hard sequence \\(y\\) generated by beam search on the teacher model as an approximation: \\(\\hat{J}_{SeqKD} = -\\log q_\\theta(\\hat{y}|x)\\).\nIn our work shown in Figure 1, we prompt a large language model (LLM) and treat it as the teacher. However, we do not follow the common KD that minimizes the divergence between LLM's probability \\(P_{LLM}\\) and the student \\(q_\\theta\\). Instead, we propose to induce a reward function \\(R_{LLM}\\) from \\(P_{LLM}\\) and adopt reinforcement learning for KD with objective:\nmaximize \\(E_{y \\sim q_\\theta} [R_{LLM}(y)]\\) \t(1)\nOur approach alleviates the exposure bias problem (Chiang and Chen, 2021; Ranzato et al., 2016) in traditional KD, where the student is fed with the teacher's predicted prefix during training, but only has access to its own partial prediction during inference. By contrast, our RL-based KD allows the student to explore with its own predicted sequence, shown by \\(y \\sim q_\\theta\\) in (1), which bridges the gap between training and inference.\nIn the rest of this section, we will introduce the reward \\(R_{LLM}\\) and the optimization of (1) in detail. Inducing Reward from LLMs. We propose to induce a reward function from large language models (LLMs) for RL-based KD, inspired by the theoretical analysis that links policies (predicted probabilities) and reward functions (Hao et al., 2022). In our work, we design an intuitive prompt to obtain the LLM's policy for reward induction.\nConsider a task T and an input sentence x. We formulate a prompt as \\(pmt(x)\\). In fact, the prompt depends on the task of interest, and in our experiments, two common text generation tasks are considered: summarization (Ahmed and Devanbu, 2022; Nair et al., 2023) and dialogue generation (Deng et al., 2023; Cao et al., 2020). Our prompts are\n\\(pmt_{sum}(x)\\) = \"Summarize [ x ]:\"\n\\(pmt_{dialog}(x)\\) = \"The dialogue response of [ x ] is:\"\nwhere x is the original input sentence and the square brackets are delimiters specifying the input boundaries.\nGiven a candidate output \\(y = (y_1,\\dots, y_T)\\), our goal is to induce a reward function \\(R_{LLM}(y)\\) that evaluates the \"goodness\" of \\(y\\). This requires modeling text generation as a Markov decision process (MDP), where an action is the prediction of the next word and a state is the partially predicted sequence in addition to the prompt. The state transition is a deterministic process that simply appends the newly generated word to the previous state.\nOur reward induction starts by querying an LLM in a step-by-step fashion to obtain the next word probability \\(P_{LLM}(y_t|y_{<t}, pmt(x))\\). Notice that we do not let the LLM generate outputs during RL-based KD, but the prefix \\(y_{<t}\\) and the next word \\(y_t\\) are from the student-sampled sequence. The role of LLM is to predict its probability and to induce a reward for y."}, {"title": null, "content": "With the next-word probability, we are able to induce a q-value function for step t, which indicates the goodness of an action, i.e., the word \\(y_t\\), at the state \\((y_{<t}, pmt(x))\\). The q-value induction process is based on the common assumption (Chan and van der Schaar, 2021; Ramachandran and Amir, 2007; Ziebart et al., 2008) that an action is taken stochastically based on a Boltzmann distribution induced by q-values:\n\\(P_{LLM}(y_t|y_{<t}, pmt(x)) = \\frac{exp\\{q-val(y_t; y_{<t})\\}}{\\sum_{y'} exp\\{q-val(y'; y_{<t})\\}}\\) \t(2)\nwhere the q-value function also depends on \\(pmt(x)\\) but is omitted for simplicity.\nIn other words, the assumption implies that a higher-valued action will be taken with a larger probability, which makes much sense in practice. Moreover, the resemblance between (2) and a softmax function suggests that we may directly take the LLM's logit (pre-softmax value) \\(\\text{logit}_{LLM}\\) as the q-value in the MDP modeling.\nThe final step of reward induction is based on Bellman optimality (Degris et al., 2012; Sutton and Barto, 2018), which derives an optimal q-value function from a reward. We follow the practice of inverse reinforcement learning (Ramachandran and Amir, 2007; Ziebart et al., 2008; Chan and van der Schaar, 2021) and use Bellman optimality in an opposite way to derive a reward \\(R_{LLM}\\) from the q-value function in (2):\n\\(R_{LLM}(y_t; y_{<t}) = q-\\text{val}(y_t; y_{<t}) - \\max_{y'} q-\\text{val}(y'; y_{<t+1})\\) \t(3)\nIn this way, our derived reward \\(R_{LLM}\\) evaluates the appropriateness of a word \\(y_t\\) at every step given its context \\(y_{<t}\\). That is to say, such a reward function is dense as opposed to various other heuristic rewards (e.g., BLEU scores) that only come at the end of a sequence (Wu et al., 2017). The overall reward induction process follows our previous work (Hao et al., 2022), but this paper extends it to a new scenario. Hao et al. (2022) train a sequence-to-sequence network in a supervised manner on a parallel corpus and perform semi-supervised learning on non-parallel corpora. Our paper shows that a reward function can be derived directly from a pretrained LLM and helps various text generation tasks, which is a new insight, especially in the LLM era.\nReinforcement Learning-Based KD. Our derived reward function allows us to perform reinforcement learning (RL) for KD. Specifically, a sequence is sampled from the student's prediction, given by \\(y \\sim q_\\theta\\). Then, each word \\(y_t\\) in y is evaluated by the induced reward function (3), and our total reward of the sequence is\n\\(R_{LLM}(y) = \\sum_t R_{LLM}(y_t; y_{<t})\\) \t(4)\nwhich is our objective to maximize, as shown in Eqn. (1).\nSince the parameter \\(\\theta\\) occurs during the sampling process, the gradient cannot be obtained by backpropagation, and RL is required to train \\(\\theta\\) in a trial-and-error manner. In NLP, the REINFORCE method is commonly used (Ranzato et al., 2016; Wang et al., 2020), where the gradient is given by\n\\(\\nabla_\\theta E_{\\pi_\\theta} [\\sum_t R_{LLM}(y_t; y_{<t})] = E_{\\pi_\\theta} [\\sum_t G_t(y) \\log \\pi_\\theta (y_t; y_{<t})]\\)\nwhere \\(G_t(y)\\) is known as the gain in the RL literature, being the accumulated reward from step t, given by \\(G_t(y) := \\sum_{\\tau>t} R_{LLM}(y_\\tau; y_{<\\tau})\\).\nOverall, our RL-based KD differs from traditional sequence-level KD, where the teacher teaches unilaterally with its own prediction, i.e., \\(y \\sim P_{LLM}\\). Instead, we allow the student to generate its own prediction, and the LLM teaches by evaluating the \"goodness\" of the student's output. In this way, our approach alleviates the exposure bias problem, as the student is aware of its own partial prediction during training. Compared with classic RL-based text generation, we do not require heuristically designed reward functions (Bahdanau et al., 2017; Shen et al., 2016) or human feedback reward models (Ouyang et al., 2022; Ziegler et al., 2019)."}, {"title": "Experiments", "content": "Setups. We evaluated our approach on two text generation tasks with three datasets: DailyDialog (Li et al., 2017) and OpenSubtitles (Lison and Tiedemann, 2016) for dialogue generation, as well as CNN/DailyMail (See et al., 2017; Hermann et al., 2015) for summarization. In particular, dialogue datasets tend to have sample-overlapping issues between training and test sets, and we used the cleaned version (Wen et al., 2022) for rigorous experimentation.\nOur teacher was a T0-3B model (Sanh et al., 2022) and the student was T5-Base with 220 million parameters (Raffel et al., 2020). Since our RL-based KD requires meaningful sampling from the student, we performed pre-distillation by the standard cross-entropy loss (Kim and Rush, 2016), which is common in KD research (Wen et al., 2023b; Shleifer and Rush, 2020) and shows our method provides add-on improvement.\nIt should be emphasized that our work addresses unsupervised KD, where the training process only used unlabeled input sentences without groundtruth references. During validation and test phases, the ground truths were used in the standard evaluation metrics: BLEU (Papineni et al., 2002) for dialogue generation and ROUGE (Lin, 2004) for summarization."}, {"title": "Main Results", "content": "Main Results. Table 1 presents the performance of our model and baselines. As seen, the teacher model (Row 1) achieves decent performance in these tasks. The results are slightly lower than, or comparable to, those of supervised methods reported in previous literature, for example, 8.96 BLEU2 for Daily Dialog (Hao et al., 2022) and 39.5 ROUGE-1 for CNN/DailyMail (Vaswani et al., 2017). This is understandable because our teacher is directly prompted for the tasks without finetuning. On the other hand, prompting the student (Row 2) does not yield meaningful performance, which is consistent with the findings of the scaling effect (Kim and Rush, 2016; Hinton et al., 2015; Wen et al., 2023b). The strong teacher and weak student jointly set up a reasonable foundation for our distillation research.\nRows 3-7 present the performance of different distilling methods, showing that KD can indeed transfer the teacher's knowledge into the student. Among different KD methods, SeqKD (Kim and Rush, 2016) employs hard samples to train the student, and achieves close performance to the teacher; in particular, it surpasses the teacher on DailyDialog, which can be interpreted by smoothing the noise of the teacher (an un-finetuned prompting system). We also experimented with soft distillation based on various f-divergence functions, including Kullback-Leibler (KL), Reverse KL (RKL), and Jenson-Shannon (JS) divergences (Wen et al., 2023b). As seen, the results are not fully consistent, although JS tends to perform better in general.\nOur LLMR (Row 7) performs reinforcement learning based on a reward function induced from the teacher model. It achieves superior performance across all the metrics and datasets, consistently demonstrating the effectiveness of our approach.\nDiversity Analysis. The diversity of output text is considered an important aspect of text generation systems (Li et al., 2016; Wen et al., 2023a). We evaluated the diversity of competing models by the standard distinct n-gram measures (Li et al., 2016; Pang and He, 2021; Ji et al., 2023), given by\nDistinct-n = \\(\\frac{\\text{Number of unique n-grams}}{\\text{Total number of n-grams}}\\)\nAs seen in Table 2, the KL loss achieves low distinct scores, which is consistent with previous evidence that the KL training makes the model generate dull and short utterances (Wei et al., 2019; Wen et al., 2023a). By contrast, our LLMR yields much higher distinct scores, which verifies that our RL mechanism allows the model to explore different regions of the sentence space, leading to much more diverse output.\nExposure Bias Analysis. As mentioned in \u00a71, our LLMR adopts RL and is supposed to alleviate exposure bias during KD. We quantify the amount of exposure bias by adapting a recently established measure, Excess Error Percentage (ExError%, Arora et al., 2022). In our scenario, ExError% is defined by\nExError%(l) = \\(\\frac{D_S(l) - D_t(l)}{D_t(l)} \\times 100\\%\\)\nHere, \\(D(l)\\) stands for the accumulated Kullback-Leibler (KL) divergence between the teacher and student, when the models follow the student's trajectory up to the (l - 1)th step:\n\\(D_s(l) = \\sum_{t=1}^l E_{\\substack{y\\_{<t} \\sim q_\\theta(y|x) \\\\ y_t \\sim p(y\\_{<t},x)}} \\log \\frac{p(y_t|y\\_{<t},x)}{q_\\theta(y_t|y\\_{<t},x)}\\)\nwhereas \\(D_t(l)\\) is the KL divergence when the models follow the teacher's trajectory up to the (l - 1)th step:\n\\(D_t(l) = \\sum_{t=1}^l E_{\\substack{y\\_{<t} \\sim p(y|x) \\\\ y_t \\sim p(y\\_{<t},x)}} \\log \\frac{p(y_t|y\\_{<t},x)}{q_\\theta(y_t|y\\_{<t},x)}\\)\nOverall, ExError% measures the percentage of excess error when the models follow the student's trajectory, compared with following the teacher's trajectory. Typically, ExError% is positive and a higher value indicates more exposure bias. It can go over 100% because the KL divergence is not upper bounded.\nAs seen in Figure 2, KL- and RKL-based KD methods yield high exposure bias, which is expected as the KL and RKL divergence functions are asymmetric and do not push the student to the teacher well. The JS divergence is symmetric and JS-based KD requires both teacher and student samplings. Its ExError% remains low at the beginning, but grows when the sequence becomes longer. Our LLMR approach employs RL training and achieves low ExError% throughout different lengths. The experiment confirms our approach alleviates exposure bias and explains the performance improvement in main results."}, {"title": "Conclusion", "content": "In this paper, we propose a novel knowledge distillation method, called LLMR, based on a large language model-induced reward function. Experiments on dialogue generation and text summarization show that our approach outperforms previous KD methods in terms of various metrics. We also conducted a detailed analysis to verify that our reinforcement learning-based method indeed alleviates the exposure bias problem present in common KD approaches."}]}