{"title": "Fine-Tuning and Evaluating Open-Source Large Language Models for the Army Domain", "authors": ["MAJ Daniel C. Ruiz", "John Sell"], "abstract": "In recent years, the widespread adoption of Large Language Models (LLMs) has sparked interest in their potential for application within the military domain. However, the current generation of LLMs demonstrate sub-optimal performance on Army use cases, due to the prevalence of domain-specific vocabulary and jargon. In order to fully leverage LLMs in-domain, many organizations have turned to fine-tuning to circumvent the prohibitive costs involved in training new LLMs from scratch. In light of this trend, we explore the viability of adapting open-source LLMs for usage in the Army domain in order to address their existing lack of domain-specificity. Our investigations have resulted in the creation of three distinct generations of TRACLM, a family of LLMs fine-tuned by The Research and Analysis Center (TRAC), Army Futures Command (AFC). Through continuous refinement of our training pipeline, each successive iteration of TRACLM displayed improved capabilities when applied to Army tasks and use cases. Furthermore, throughout our fine-tuning experiments, we recognized the need for an evaluation framework that objectively quantifies the Army domain-specific knowledge of LLMs. To address this, we developed MilBench, an extensible software framework that efficiently evaluates the Army knowledge of a given LLM using tasks derived from doctrine and assessments. We share preliminary results, models, methods, and recommendations on the creation of TRACLM and MilBench. Our work significantly informs the development of LLM technology across the DoD and augments senior leader decisions with respect to artificial intelligence integration.", "sections": [{"title": "Introduction", "content": null}, {"title": "Background", "content": "In the fast-paced fields of machine learning (ML) and natural language processing (NLP), LLMs have garnered worldwide attention as a highly transformative technology with virtually limitless applications across multiple domains. Popularized by high-impact commercial LLMs like OpenAI's ChatGPT [31], Google's Gemini [40], and Anthropic's Claude [2], LLMs have become household terms due to their uncanny ability to emulate human-like understanding and advanced problem- solving abilities. Concurrent with the rise of LLMs from leading technology companies, open-source LLMs such as BLOOM [47], LLaMA [41], and Mistral [19] have made advanced artificial intelligence freely available to researchers for application to highly specialized domains, including the U.S. Army. The advent of these powerful open-source baseline models is particularly significant, due to the well- known prohibitive cost of training an LLM from scratch [23]. Given the existence of permissively licensed open-source LLMs, researchers and small organizations with limited resources have turned to fine-tuning in order to adapt LLMs to domains and tasks of interest. Albeit a generic term with"}, {"title": "Problem Definition", "content": "During TRAC's support for Project Convergence (PC) Capstone Four (C4) in October 2022, we first observed that an Army-specific fine-tuned LLM could benefit Army analytics. PC is a large-scale Army experiment that seeks to inform the acquisitions process by testing the compatibility and interoperability of various in-development technologies across key use cases. In recent years, TRAC provided support for PC C4 in the form of in-stride analysis, where daily snapshots of experiment output are collected and refined in near-real-time to inform long-tail analytical efforts and senior leader decisions. In support of the in-stride analysis mission, TRAC successfully leveraged T0 [38], a 3B-parameter open-source LLM, to provide sentiment analysis, topic modeling, and summarization of unstructured documents on classified networks. While T0 performed admirably in many cases, it became increasingly clear to TRAC analysts that model output faltered when it was asked to process text full of Army jargon and technical descriptions of Army equipment. Furthermore, swapping TO for a commercial LLM solution would have been infeasible, both due to the classified operating environment of PC and a lack of options in the pre-ChatGPT era. Even if today's near-SOTA LLMS existed at the time, they would still suffer from a lack of domain specificity, due to being trained largely on generic internet-sourced text [26]. In light of this problematic capability gap, we scoped our research questions as follows:\n\nCan fine-tuning effectively inject domain-specific Army knowledge into open-source LLMs?\nHow can we validate that a fine-tuned LLM has successfully acquired target knowledge?"}, {"title": "Contribution", "content": "In response to our problem statement and research questions, this paper documents techniques, results, and lessons learned in both the development and evaluation of TRACLM and MilBench. Contributions include:\nA detailed road-map for fine-tuning three TRACLM versions, highlighting techniques that improved model performance with each iteration.\nA technical description of MilBench, an Army-built modular LLM evaluation framework with high potential for broad application across the DoD.\nEvaluation results of TRACLM and other open-source models using MilBench tasks and general-purpose benchmarks.\nDescriptions of TRACLM & MilBench technical limitations, safety and security considera- tions, and recommendations for future work."}, {"title": "Related Work", "content": null}, {"title": "Army Research", "content": "To our knowledge, the TRACLM project is the first and only attempt to fine-tune open-source LLMs for the Army domain at the scale and scope discussed in this paper. While small-scale academic experiments from the Army Research Lab (ARL) and Artificial Intelligence Integration Center (AI2C) are known to have surveyed the LLM training design space, these efforts were largely exploratory"}, {"title": "Academia & Industry Research", "content": "Despite the evident lack of Army fine-tuning examples before the TRACLM project, there has been no shortage of domain adaptation experiments in academia and industry. Domain-specific knowledge has been encoded into baseline LLMs successfully in numerous sectors, including the medical domain [15], financial domain [44], and other highly specialized domains such as silicon chip design [24]. The success of such endeavors strongly suggests that open-source LLMs can also be calibrated for Army, military, and defense-related utilization through effective fine-tuning. Additionally, in our observation, LLM fine-tuning research generally supports either one or both of two interrelated goals: improving the performance of smaller open-source LLMs through innovative training techniques, or democratizing access to LLMs by making fine-tuning more efficient. These goals are interrelated because boosting smaller LLM performance and fine-tuning efficiency reduces hardware requirements for researchers, thus tempering a reliance on commercial, closed-source near-SOTA LLMs."}, {"title": "Improving Small LLM Performance", "content": "Researchers have discovered a number of techniques in recent years that help close the gap between LLMs in the 3-7B parameter range and much larger, near-SOTA LLMs with hundreds of billions or even trillions of parameters [27]. Many of these techniques informed our training procedures and experiments for TRACLM. These include optimal learning rates based on published LLM scaling laws [18], leveraging synthetic training data derived from near-SOTA LLMs [39], and teaching LLMs to follow instructions by converting raw domain text into questions and answers [5]. Furthermore, recent open-source research has popularized model alignment techniques like direct preference optimization (DPO) [34] and reinforced token optimization (RTO) [48]. These techniques are notable because they are viable alternatives to reinforcement learning through human feedback (RLHF) [3], thus reducing the need for active human participation in the training loop."}, {"title": "Democratizing LLM Fine-tuning", "content": "In service of increasing fine-tuning efficiency, particularly in terms of memory requirements, parameter efficient fine-tuning (PEFT) methods have become the norm [16]. Among these methods, Dettmers et. al's Quantized Low Rank Adapters (QLORA) [8] deserves special mention for its introduction of three novel quantization techniques. When applied in tandem, these techniques enable researchers to fine-tune 70B-parameter LLMs (approximately 280GB models in 32-bit precision) on consumer-grade graphics processing units (GPUs) with minimal performance degradation. While TRAC's local hardware allowed us to fully fine-tune LLMs in the 3-7B parameter range without the need for quantization, projects like QLoRA and, more recently, model merging [12] suggest a near future where Army fine-tunes in the 70B+ parameter range are well within the realm of possibility. If this trend continues, we may reach an end-state where fine-tuning hardware requirements become virtually negligible, allowing any DoD organization to create LLMs for bespoke purposes on demand without relying on commercial solutions."}, {"title": "Statement of Assumptions", "content": "Given our own observations and the related work outlined above, our approach for the TRACLM + MilBench project relied upon the following assumptions:\n\nThe unclassified subset of Army doctrine provides sufficient domain-specific knowledge to noticeably improve the performance of a pre-trained LLM through fine-tuning.\nDue to linguistic capabilities LLMs acquire through pre-training, minimal pre-processing of Army doctrine would be required to fine-tune a given LLM.\nOpen-source LLMs, as opposed to commercial alternatives, are already performant enough to warrant fine-tuning on domain-specific data.\nAs we demonstrate below, these assumptions are both validated and challenged throughout the course of our research."}, {"title": "TRACLM", "content": "The TRACLM project is a first-of-its-kind effort to create a fine-tuned LLM intended for broad usage across the Army. Over the course of several months, TRAC has run extensive experiments on local hardware in partnership with the Naval Postgraduate School (NPS), resulting in three distinct generations of TRACLM models. Below, we detail the acquisition and preparation of training data utilized for each generation, the training pipeline for each model, and a discussion on subjective and quantitative evaluations of the models' Army-domain performance (see Section 5: Results & Discussion). The training pipeline section is further broken down to capture distinguishing features of each generation, with special emphasis on techniques leading to improved performance over anterior generations."}, {"title": "Training Data", "content": "It is well-known among statisticians, data scientists, and machine learning specialists that data quality is paramount to creating a successful model. Furthermore, any model developed without sufficient data to accurately represent a domain or problem space will be inherently unreliable. Thus, from the onset of the TRACLM project, we sought a training corpus that was accessible, plentiful, and representative of the general Army domain. Army doctrine and related publications fit these criteria well. Due to the broad availability of Army publications over the open internet, we discovered that downloading and extracting the text from the unclassified subset of Army doctrine was near-trivial to automate with simple Python scripts. In March of 2023, and again in April of 2024, we accumulated over 4,300 unclassified documents from the Army Publishing Directorate (APD) website [28]. Many of these documents were hundreds of pages long, resulting in an 80M+ token corpus of high-quality Army-domain specific raw text (see Table 4 in Section 11: Appendix for a full breakdown of TRACLM training corpus contents). For comparison, it is important to note that training LLMs in the 3-7B parameter range from scratch requires trillions of tokens for optimal convergence [18]. While the size of our APD corpus paled in comparison, our working assumption was that it represented the domain well enough that fine-tuning would prove effective.\nAfter downloading the documents from APD, we prioritized preprocessing before fine-tuning an LLM. Preprocessing is essential to cleanse the text of extraneous information and eliminate artifacts introduced by automated text extraction, such as extra whitespace and unnecessary symbols, which can impair the training process. Our preprocessing workflow evolved through successive TRACLM generations. Initially, we employed minimal preprocessing, followed by the integration of hard- coded rules to identify high-value data, and ultimately transformed the training corpus into synthetic questions and answers. Individual TRACLM generation preprocessing steps are documented at the beginning of each pipeline subsection."}, {"title": "Training Pipeline", "content": "The following subsections dissect TRACLM training pipelines into four distinct areas of discus- sion: data preprocessing, notable training hyperparameters, hardware utilization, and miscellaneous notes. For TRACLM-v1, the Trainer class from the HuggingFace Transformers [46] Python library"}, {"title": "TRACLM-v1", "content": "Data preprocessing for TRACLM-v1 was intentionally minimal, given our initial assumption that rigorous data cleaning was unnecessary given the base LLM's pretraining. Only two basic NLP rules were applied to our corpus before training: all text was converted to lowercase, and all leading or trailing continuous whitespace around sentences was converted to singular spaces. The base model we used for fine-tuning was Together Computer's RedPajama-INCITE-Base-3B-v1 [7]. This model was selected for its reputable source, permissive Apache 2.0 license, and comparable size to the open-source model we utilized in support of PC C4 in 2022. TRACLM-v1 was fine-tuned on our minimally preprocessed APD corpus for a single epoch using a batch size of one, 16 gradient accumulation steps, and an AdamW optimizer [32]4. The uninterrupted training run lasted 18 hours, and training was conducted in collaboration with the NPS Department of Defense Analysis for access to a 16x NVIDIA V100 GPU cluster."}, {"title": "TRACLM-v2", "content": "For TRACLM-v2, we implemented a number of changes to our training pipeline aimed at overcoming TRACLM-v1's performance shortfalls (see Section 5: Results & Discussion). For data preprocessing, we incorporated additional basic NLP steps and custom rules to remove less relevant data from the corpus. Concretely, we excluded the tables of contents, references, and acknowledgments typically found at the beginning of documents, due to their sparse domain-specific knowledge and limited value for LLM training. These changes were made based on the observation that semantic relevance in our APD corpus varies across each document. We also upgraded our base model from RedPajama-INCITE-Base-3B-v1 to Meta's Llama-2-7b [41]. This decision was made based on the general understanding that leveraging an increased parameter count from a more recently released open-source LLM would better tap into emergent capabilities [45]. Lastly, perhaps the most impactful change to the TRACLM-v2 training pipeline was the addition of a second stage of fine-tuning after the continued pretraining stage. This \"instruction-tuning\" stage is meant to convert a raw LLM6 into a chatbot that responds well to questions and general instructions [38]. For instruction"}, {"title": "TRACLM-v3", "content": "Initially, we were satisfied with TRACLM-v2's domain-specific knowledge and instruction- following capabilities, but we soon realized it struggled with answering follow-up questions. We developed a working hypothesis that this capability shortfall was derived from two factors, namely, that our instruction-tuning dataset was not large enough and, more importantly, the instruction-tuning dataset did not contain any domain-specific examples. TRACLM-v3, our current flagship model, thus offers one critical innovation over version 2's training pipeline. To create a larger and more domain oriented instruction tuning dataset, we converted our APD corpus into approximately 500,000 question and answer pairs, and shuffled them together with an open-source dataset comprised of approximately 300,000 pairs. We leveraged synthetic example generation utilizing a larger, near-SOTA, open-source LLM: MistralAI's Mixtral-8x7B-Instruct-v0.1 [20]. In recent months, LLM training via synthetic examples has emerged as a promising approach in data-constrained circumstances [25]; thus, we sought to employ related techniques for TRACLM-v3. Our abbreviated algorithm for question and answer generation via prompt engineering is represented below:\n\nAlgorithm 1 Q&A Generation and Evaluation Workflow\nSplit raw corpus into chunks\nfor each chunk do\nfor each category do\nattempts 0\nrepeat\nPrompt LLM for Q&A within category\nPrompt LLM to evaluate Q&A\nattempts attempts +1\nuntil quality threshold met or attempts = 10\nif quality threshold not met then\nLabel chunk as <unsuitable for conversion>\nend if\nend for\nend for\nDrop chunks labeled <unsuitable for conversion> from the final dataset"}, {"title": "MilBench", "content": "Just as evaluation is essential to calibrate the accuracy and effectiveness of a weapon system, quantitative performance evaluation is essential for successful LLM fine-tuning. Throughout the first two fine-tuning iterations of TRACLM, we measured its performance qualitatively by observing the LLM's response to a set of standardized prompts. While this qualitative evaluation revealed an apparent improvement in performance from TRACLM-v1 to TRACLM-v2, it did not provide the detailed performance characterization necessary to understand how fine-tuning impacted TRACLM's performance across various domain-specific tasks. Furthermore, our previous evaluation method was also ill-suited to assess whether fine-tuning led to decreased performance on general tasks when compared to the base models, sans fine-tuning.\nLLM evaluation has received significant academic and community interest, with HuggingFace's Open LLM Leaderboard [4] and EleutherAI's Language Model Evaluation Harness (LEH) [11] being notable examples of comprehensive performance leaderboards and evaluation frameworks, respectively. Given that LEH is open-source with an MIT license, we considered leveraging it to evaluate TRACLM. However, we found its complexities went beyond our use case and opted to build our own framework, called MilBench, that leveraged an open-source dataset format such that our tasks would be interoperable with LEH, and our framework would be compatible with community-made datasets.\nMilBench is a collection of benchmarking datasets aggregated and maintained by TRAC and a modular software framework that enables organizations of all sizes to easily evaluate and assess LLMs at scale. In this section, we describe each of MilBench's three components: MilBench Datasets, MilBench Evaluation Harness (MEH), and MilBench Server. We highlight how MilBench can be an"}, {"title": "MilBench Datasets", "content": "To quantitatively evaluate TRACLM and other LLMs, we sought a methodology and datasets that provided meaningful performance metrics and LLM compatibility. While there exist many methods of evaluating LLMs, several of the widely-used benchmarks are comprised of multiple-choice questions and answers10 that are used to prompt a model with a question and ask it to respond with the letter of the correct response. Using multiple-choice questions provides meaningful performance metrics as we can compare the scores of an LLM to that of a human taking the same test. Multiple-choice questions are also faster and cheaper to evaluate compared to questions that require longer responses, as the LLM usually needs to generate only one token to provide a ready-to-evaluate response.\nMilBench Datasets is a repository of datasets, referred to as tasks, that are designed to evaluate LLM performance in the military domain. The current repository contains four tasks derived from those presented by Hallapy et al. in MilGLUE [14] and one task derived from Army officer multiple-choice tests (CATB). As mentioned above, MilBench supports any multiple-choice dataset in the HuggingFace Datasets format, to include common benchmarks like MMLU [17]. Though we will limit further discussion of non-Army datasets, we emphasize that this standard format enables any organization to create their own evaluation datasets for use within MilBench."}, {"title": "MilGLUE-Derived Tasks", "content": "MilGLUE [14] presents evaluation datasets that are tailored for Bidirectional Encoder Rep- resentations from Transformers (BERT) [9] models. Thus, raw MilGLUE examples are neither conversational in nature nor formatted as multiple-choice questions. However, this does not present a problem for our use case as it is common practice when evaluating LLMs to wrap the dataset entries with conversational cues as part of a task definition within the evaluation framework. The MilGLUE dataset corpus required further curation due to its size, nearly 1,000,000 records for some datasets, which we deemed too expansive for large-scale LLM evaluation due to limited compute and time resources. We selected four of the MilGLUE datasets to curate for use with MilBench, as shown in Table 1 and denoted in bold."}, {"title": "CATB Task", "content": "The CATB task is of special mention, as it enables a user-friendly form of evaluation that is easily understood by virtually all uniformed service members. The intent for CATB is to aggregate Army multiple-choice questions, starting with the United States Military Academy (USMA) military science program and continuing through increasingly advanced Army professional military education (PME). Upon evaluating an LLM with the CATB task, it will become possible to \"rank\" LLMs based on their performance across this broad range of exams. To date, we have collected USMA Department of Military Instruction (DMI) questions through official releases, and Army Command and General Staff College (CGSC) questions from online study guides containing outdated test questions [33]. Future agreements, which are in progress at the time of writing, will add content from the Army's Basic Officer Leader Course (BOLC), Captain's Career Course (CCC), and other specialized PME sources. In its current form, despite lacking diverse sets of questions from various sources, the CATB task has still proven effective for informing our fine-tuning process as a means of validating the improved domain-specific knowledge of each subsequent model version."}, {"title": "MilBench Evaluation Harness", "content": "MEH is a modular evaluation framework written in Python that serves as a test proctor for LLMs hosted on a HuggingFace Text Generation Inference (TGI) server, or behind any OpenAI-compatible application programming interface (API). We chose to have MilBench offload interaction with LLMs to inference servers as this enables MEH to concurrently evaluate a greater number of models than its host machine could run simultaneous inference on. The harness can be configured to administer an arbitrary set of tasks, so long as the associated datasets are in the HuggingFace Datasets format, a format used by all common LLM evaluation datasets. As of writing, MilBench exclusively supports evaluation using tasks comprised of multiple-choice questions. These tasks are defined using a YAML"}, {"title": "MilBench Server", "content": "To add to the functionality and utility of MEH, we then targeted capabilities beyond those enabled by a command line interface (CLI). These included easy initiation of evaluations from a remote computer, and providing a way to track, compare, and audit evaluation results. MilBench Server, a web API wrapper around MEH, was created to enable remote and distributed management of MEH evaluations. MilBench Server provides an easy-to-use RESTful14 API to start, view, and audit LLM evaluations. MilBench Server stores logs of every evaluation to enable leaderboard visualizations, as"}, {"title": "Results & Discussion", "content": "In this section, we report and discuss the performance of TRACLM models, both against each other and comparable open-source models in the 3-7B parameter range. We begin by quantifying the domain-specific knowledge of each generation of TRACLM, to demonstrate a progressive growth in knowledge as our data preprocessing and training pipeline evolved. Then, we compare the capabilities of our latest and most performant model, TRACLM-v3, against competing models via both quantitative and qualitative benchmarking."}, {"title": "Comparing TRACLM Generations", "content": "With each successive version of TRACLM, our prevailing goal was to fine-tune models that clearly outperform the preceding version in terms of domain-specific knowledge. While this difference in capability can be observed through basic interaction with each model, the reliability of this form of subjective analysis plateaus as models become more performant. Therefore, a tool enabling quantifiable comparison of LLMs against concrete and domain-specific benchmarks became essential to measure the increasing innate knowledge of our models. As discussed in Section 4, MilBench was developed to fulfil this critical role. For the purpose of our analysis here, we provide the following table and corresponding radar plot. Note the tasks labeled CATB through SSB are Army-specific benchmarks, while MMLU [17] and TQA [22] are open-source benchmarks meant to assess LLM general knowledge and truthfulness, respectively. The latter two benchmarks are included by default in MilBench to enable researchers to identify shortfalls in general capabilities while calibrating models for specialized domains."}, {"title": "TRACLM-v3 Performance Analysis", "content": "TRACLM-v3's domain adaptation is evident when contrasted against well-known open-source models, both quantitatively and qualitatively. Table 3 uses MilBench to compare TRACLM-v3 against three instruction-tuned variants of popular 7B-parameter LLMs, and one mixture-of-experts LLM with 56B parameters. Despite the 8x increase in parameter count of Mixtral-8x7B-Instruct-v0.1, TRACLM-v3 still manages to outperform it on 3/5 of the domain-specific tasks, heavily supporting the efficacy of the TRACLM-v3 training corpus and pipeline."}, {"title": "Conclusion", "content": "In this paper, we introduced TRACLM, a first-of-its-kind series of LLMs fine-tuned on unclassified Army doctrine for general application in the Army domain. We shared our methodology and techniques, assumptions, analysis, results, and best practices on military domain LLM calibration based on our experiences for the benefit of the Army analytical community. Additionally, we introduced MilBench, a modular and model-agnostic evaluation framework leveraging custom Army- specific benchmarks to assess any open-source, locally-hosted LLMs. MilBench possesses high impact potential across the DoD, because it enables any government organization to incorporate quantitative LLM evaluations into their generative AI acquisition pipelines, thus enhancing senior leaders' ability to make informed decisions about commercial partnerships vs. in-house development."}, {"title": "Limitations", "content": "As a snapshot of a continuous research project, TRACLM-v3 suffers from some limitations despite the capabilities demonstrated throughout this paper. Firstly, TRACLM-v3's context window mirrors that of the base model: 4096 tokens. Effectively, this means model performance will swiftly diminish when asked to process sequences longer than this maximum, which renders certain use cases (e.g. long transcript summarization) intractable until future versions. Second, and closely related, we have observed through qualitative evaluations that TRACLM-v3's rate of hallucination increases as the context limit is approached. We theorize this effect is influenced not only by the limited context window, but also by the often direct and to-the-point tendencies of Army communications, suggesting that TRACLM-v3 \"prefers\" to arrive at its main points and terminate generation as soon as possible. Third, TRACLM-v3 is prone to false attribution regarding its creators and affiliation. Concretely, when prompted with certain verbiage, TRACLM-v3 will confidently state it is an \"LLM created by OpenAI\". This is due to the presence of such statements in the open-source subset of the TRACLM-v3 training corpus that were not removed prior to training. Lastly, the aforementioned limitations of TRACLM-v3 are amplified for TRACLM-v2, due to the less sophisticated nature of the latter's training pipeline. Also of note, TRACLM-v1 is severely limited by its classification as a raw LLM, meaning it is incapable of following any instructions reliably.\nMilBench suffers from similar issues to other benchmarks and evaluation frameworks. The CATB dataset has been observed to have a small portion of its questions reference contextual information that would have been present in the test's original form, but is not present in the benchmark dataset. Additionally, CATB includes questions that use strings of underscores that vary in length to represent \"fill-in-the-blank\" questions. We theorize that such inconsistent and implicit question formatting may introduce uncertainty into LLM response correctness. Records in the MilGLUE tasks have been noted to contain data that is either grammatically flawed, or that suffers from formatting issues. For example, the paraphrase task has at least one record where the source paragraph contains a full URL, but the proposed paraphrase has a malformed URL where all special characters are removed. Such dataset issues introduce some amount of error into evaluation results, as missing context or malformed text may lead to multiple responses being plausible. MEH is further limited by only supporting the filtered top_k evaluation strategy, which is only applicable to questions whose response can be expected to be a single token. Due to these limitations, the MilBench tasks are not yet finalized as there remains work to further curate and clean the datasets. While this does mean that an LLM's score on a given task is subject to change, we do not anticipate the datasets changing enough to invalidate our preliminary results. Additionally, the initial findings will continue to provide valuable insight to the fine-tuning process."}, {"title": "Safety & Security Considerations", "content": "Despite TRACLM-v3's clear ability to generate accurate, Army-specific text, it is still a proba- bilistic model, and thus is capable of producing false information with little to no effort by the user. Thus, researchers employing TRACLM for their own experiments should verify all outputs. The creation of the TRACLM series of models constitutes academic research in partnership with the Naval Postgraduate School. The purpose of this research is to inform future DoD experimentation regarding the development and application of domain-specific LLMs. While experiments involving direct application of this model to downstream military tasks are encouraged, extreme caution should be used before production-level integration into Army systems. Regarding classification risks, TRACLM is trained on exclusively unclassified and publicly released training data. Therefore, we deem the risk of TRACLM producing higher classification content very unlikely. However, classification by compilation is a risk associated with all LLMs that is of unique concern to the DoD. Thus, we invite researchers from across the analytical community to leverage TRACLM in their future studies to help quantify LLM classification risks with concrete findings.\nAlthough we intend for MilBench to be leveraged across the DoD to evaluate LLMs, it is important that the evaluation datasets remain close-hold. Exposure of the raw datasets would enable LLMs to be trained on the exams themselves, which would lead to greater performance scores but would render those scores meaningless as evaluations are only valid if the models have not previously seen the questions and answers. This may theoretically prevent MilBench from evaluating commercial LLMs unless it could be guaranteed that no inference logs were persisted during evaluation. Additionally,"}, {"title": "Future Work", "content": "Future work associated with the TRACLM project aims to produce more capable model versions using larger base models with longer context windows. Furthermore, we plan to conduct extensive experiments to conclusively inform the preceding discussions regarding two-stage (e.g. continued pretraining followed by instruction-tuning) vs. single-stage (e.g. instruction-tuning only) fine-tuning. We will also continue to improve the TRACLM training corpus by including data from additional sources (e.g. other Army PME courses and authoritative documents maintained by each of the Army's Centers of Excellence) and address aforementioned attribution issues with open-source instruction- tuning datasets. Lastly, we'll explore the value of model alignment via newer techniques like DPO, which will require the creation of additional Army-specific training examples derived from recorded interactions with prior TRACLM versions.\nFuture work on MilBench includes the collection of more domain-specific evaluations from Army officer PME, Army enlisted basic and advanced individual training (AIT), and Army enlisted PME such that LLM performance could be loosely categorized into existing tiers of service member perfor- mance15. We also plan to further sanitize and finalize our domain-specific benchmarks. Continued work on MEH aims to add support for evaluation strategies that can consider more than a single token response, and we plan to add support for evaluating a model's performance on a specific task, such as retrieval-augmented-generation. Finally, we plan to add robust multi-user support to MilBench Server to enable broader access to its evaluation capability."}, {"title": "TRACLM-v3", "content": "Initially, we were satisfied with TRACLM-v2's domain-specific knowledge and instruction- following capabilities, but we soon realized it struggled with answering follow-up questions. We developed a working hypothesis that this capability shortfall was derived from two factors, namely, that our instruction-tuning dataset was not large enough and, more importantly, the instruction-tuning dataset did not contain any domain-specific examples. TRACLM-v3, our current flagship model, thus offers one critical innovation over version 2's training pipeline. To create a larger and more domain oriented instruction tuning dataset, we converted our APD corpus into approximately 500,000 question and answer pairs, and shuffled them together with an open-source dataset comprised of approximately 300,000 pairs. We leveraged synthetic example generation utilizing a larger, near-SOTA, open-source LLM: MistralAI's Mixtral-8x7B-Instruct-v0.1 [20]. In recent months, LLM training via synthetic examples has emerged as a promising approach in data-constrained circumstances [25]; thus, we sought to employ related techniques for TRACLM-v3. Our abbreviated algorithm for question and answer generation via prompt engineering is represented below:\n\nSplit raw corpus into chunks\nfor each chunk do\nfor each category do\nattempts 0\nrepeat\nPrompt LLM for Q&A within category\nPrompt LLM to evaluate Q&A\nattempts attempts +1\nuntil quality threshold met or attempts = 10\nif quality threshold not met then\nLabel chunk as <unsuitable for conversion>\nend if\nend for\nend for\nDrop chunks labeled <unsuitable for conversion> from the final dataset"}, {"title": "MilBench Datasets", "content": "To quantitatively evaluate TRACLM and other LLMs, we sought a methodology and datasets that provided meaningful performance metrics and LLM compatibility. While there exist many methods of evaluating LLMs, several of the widely-used benchmarks are comprised of multiple-choice questions and answers10 that are used to prompt a model with a question and ask it to respond with the letter of the correct response. Using multiple-choice questions provides meaningful performance metrics as we can compare the scores of an LLM to that of a human taking the same test. Multiple-choice questions are also faster and cheaper to evaluate compared to questions that require longer responses, as the LLM usually needs to generate only one token to provide a ready-to-evaluate response.\nMilBench Datasets is a repository of datasets, referred to as tasks, that are designed to evaluate LLM performance in the military domain. The current repository contains four tasks derived from those presented by Hallapy et al. in MilGLUE [14] and one task derived from Army officer multiple-choice tests (CATB). As mentioned above, MilBench supports any multiple-choice dataset in the HuggingFace Datasets format, to include common benchmarks like MMLU [17]. Though we will limit further discussion of non-Army datasets, we emphasize that this standard format enables any organization to create their own evaluation datasets for use within MilBench."}, {"title": "MilGLUE-Derived Tasks", "content": "MilGLUE [14", "9": "models. Thus, raw MilGLUE examples are neither conversational in nature nor formatted as multiple-choice questions. However, this does not present a problem for our use case as it is common practice when evaluating LLMs to wrap the dataset entries with conversational cues as part of a task definition within the evaluation framework. The MilGLUE dataset corpus required further curation due to its size, nearly 1,000,000 records for some datasets, which we deemed too expansive for large-scale LLM evaluation due to limited compute and time resources. We selected four of the MilGLUE datasets to curate for use with MilBench, as shown in Table 1"}]}