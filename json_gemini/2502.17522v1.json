{"title": "Spectral Theory for Edge Pruning in\nAsynchronous Recurrent Graph Neural\nNetworks", "authors": ["Nicolas Bessone"], "abstract": "Graph Neural Networks (GNNs) have emerged as a power-\nful tool for learning on graph-structured data, finding applications in\nnumerous domains including social network analysis and molecular bi-\nology. Within this broad category, Asynchronous Recurrent Graph Neu-\nral Networks (ARGNNs) stand out for their ability to capture complex\ndependencies in dynamic graphs, resembling living organisms' intricate\nand adaptive nature. However, their complexity often leads to large and\ncomputationally expensive models. Therefore, pruning unnecessary edges\nbecomes crucial for enhancing efficiency without significantly compromis-\ning performance. This paper presents a dynamic pruning method based\non graph spectral theory, leveraging the imaginary component of the\neigenvalues of the network graph's Laplacian.", "sections": [{"title": "Introduction", "content": "Modern deep neural networks are characterized by enormous model sizes, requir-\ning considerable computational power and storage resources. To mitigate these\ndemands, network pruning techniques are frequently used to decrease memory\nusage and shorten training time.\nThe Lottery Ticket Hypothesis [5] suggests that within a randomly initialized\ndense network, a sparse sub-network can be trained using the original weights\nto achieve performance comparable to that of the full network. Building on this\nidea, pruning network parameters has become a popular compression technique,\noperating on the premise that many parameters are often redundant and can\nbe removed without significantly impacting the network's performance. How-\never, identifying these redundant connections (weights) is challenging due to the\ncomplexity of the network [2,8], where millions of mathematical operations are\nperformed, and the contributions of all weights are combined to generate the\nfinal output.\nDynamic pruning [10] involves adjusting the network's topology in response\nto changing conditions. In a decentralized and asynchronous setting, these changes"}, {"title": null, "content": "are difficult to track and respond to in real time. The network must adapt with-\nout centralized coordination, which can lead to issues such as network partition-\ning or inefficiencies if pruning decisions are not well-coordinated.\nIn a decentralized network, where there is no central authority or single point\nof control, each node operates independently and has only partial information\nabout the overall network state and its current performance. This makes it dif-\nficult to implement and coordinate pruning decisions consistently across the\nnetwork. Each node must make local decisions based on limited information,\nwhich can lead to inconsistencies, where some nodes might prune connections\nthat other nodes still rely on. In an asynchronous network, nodes do not operate\nunder a global clock. Messages between nodes can be delayed, lost, or arrive\nout of order. This lack of synchronization exacerbates the difficulty of dynamic\npruning, as nodes might make pruning decisions based on outdated or incomplete\ninformation.\nThis paper introduces a dynamic pruning methodology grounded in spec-\ntral theory, a branch of mathematics that focuses on the study of the spectrum\n-eigenvalues and eigenvectors of operators, particularly linear operators on\nvector spaces, often within the context of functional analysis. Although the lit-\nerature on pruning methodologies is extensive and well-documented [3,8,12], the\napplication of spectral theory to enhance pruning techniques appears, to the best\nof my knowledge, to be unexplored.\nThis work represents a preliminary exploration into the potential of spectral\ntheory to identify and capture unnecessary connections within a network during\ntraining. It aims to initiate a discussion on spectral theory's broader applications\nand implications in enhancing pruning techniques. By leveraging the principles of\nspectral analysis, this study seeks to uncover new avenues for optimizing neural\nnetwork architectures, contributing to the ongoing research on advanced pruning\nmethodologies."}, {"title": "A Fully Decentralized Asynchronous Recurrent Graph\nNeural Network", "content": "Asynchronous Recurrent Graph Neural Networks (ARGNNs) represent a sub-\nclass of RGNNs where the nodes update their states asynchronously. This asyn-\nchronous updating mechanism more closely mimics many real-world systems\nwhere entities do not act in a synchronized manner [4]. The challenge with\nARGNNs lies in their potential computational inefficiency and complexity due\nto the large number of edges that need to be processed at each time step [2].\nARGNNs operate effectively in decentralized networks where nodes lack\nglobal knowledge. In these settings, computations rely solely on local informa-\ntion within each node's neighborhood. This decentralized approach mirrors the\nadaptive behavior observed in living organisms, allowing GNNs to capture com-\nplex dependencies in dynamic graphs without requiring global coordination or\noversight [6]. This characteristic is particularly advantageous in scenarios such as"}, {"title": null, "content": "social network analysis and molecular biology, where graphs evolve dynamically\nand maintaining global knowledge is impractical or infeasible.\nIn this work, a decentralized implementation of a Neural Network (NN) was\nutilized. During each update step, the nodes execute their behaviors in an arbi-\ntrary order, this behavior consists in updating its value according to an activa-\ntion function, followed by a parameters optimization based on a local gradient\ndescent. The activation function f of each node n is computed based on the\naggregation of its inputs and their respective weights. This forward computation\nis expressed as:\n$$f(n) = Vn = \\frac{1}{1+e^{-(V_{n_1} W_{n_1})}}$$\nwhere $V_{n_1}$ is the vector of input values, $W_{n_1}$ is the vector of edge weights to the\ninput nodes of n. Following its activation, the node will compute its local error\ngradient $V_{en}$:\n$$V_{en} = (V_{eno} W_{no}) f'(n)$$\nwhere $V_{eno}$ is the vector of error gradients of output nodes of n while $W_{no}$ is\nthe vector of edge weights to the output nodes of n, and $f'(n)$ is the derivative of\nthe activation function (Eq. 1), evaluated at its current state. In the case of the\noutput node (the one utilized to evaluate the performance of the network), the\ngradient error is computed as the difference between the expected value and the\ncurrent value V of the node. This output node is the only node that introduces\nan error signal in the system, which is later propagated through the network as\nindicated in Eq. 2, this local gradient of error is finally used in a gradient descent\noperation to adjust the weights of its input nodes edges $W_{n_1}$:\n$$W_n \\leftarrow W_n - V_{en}.V_n$$"}, {"title": "Graph Spectral Theory and Pruning", "content": "Graph spectral theory provides a robust mathematical framework for analyzing\nthe structural properties of graphs. The eigenvalues and eigenvectors of the graph\nLaplacian matrix, which encodes the connectivity information of the graph, play\na pivotal role in various graph algorithms [11,9]. These eigenvalues and eigenvec-\ntors encapsulate important properties such as coupling, oscillation frequencies,\nconnectivity, clustering tendencies [7], and the presence of bottlenecks within\nthe graph [9]. The novel aspect of the presented pruning method focuses on the\nimaginary part of the eigenvalues of the Laplacian matrix. Unlike the real parts,\nwhich are often associated with the global structural properties of the graph,\nthe imaginary parts can reveal subtle patterns related to edge redundancy and\ncommunity structure.\nThe pruning strategy leverages spectral graph theory, which helps capture\nimportant structural and dynamic properties of the network through the eigen-\nvalues of its local Laplacian matrix. Each node computes the Laplacian matrix"}, {"title": null, "content": "based on its local connections, and the eigenvalues of this matrix give insight\ninto the node's interactions with its neighbors.\nThe key insight here is the relationship between eigenvalues with opposite\nimaginary parts. Imaginary components in the eigenvalues typically arise when\nthere is oscillatory or cyclic behavior in the local network, often indicating some\nform of dynamic coupling between the nodes. When two nodes in a graph have\neigenvalues with equal magnitudes but opposite imaginary components, it sug-\ngests that their behaviors are symmetrically coupled meaning the actions or\nstates of one node have a mirrored impact on the other.\nThis symmetry implies redundancy: if the interaction between two nodes\ncan be described by symmetric dynamics, one of these interactions can often be\npruned without significantly altering the overall behavior of the system. Specifi-\ncally, one of the weights can be expressed as a linear transformation of the other.\nFurthermore, if the dynamics between two nodes are independent, their corre-\nsponding eigenvalues will be zero. By removing the edge between nodes with\nopposite imaginary eigenvalues, we reduce the number of trainable parameters\nwhile preserving the essential properties of the graph.\nThe use of eigenvalues as a pruning criterion ensures that the graph retains\nits core dynamic and structural characteristics, even as unnecessary edges are\nremoved. This selective pruning balances model complexity and expressiveness,\nenabling efficient training without sacrificing important relationships within the\nnetwork."}, {"title": "Degree Matrix and Weighted Degree Matrix", "content": "Before delving into the pruning algorithm, it is essential to highlight a few impor-\ntant points regarding the Degree matrix. Each node's pruning decision is based\non the eigenvalues of the Laplacian matrix corresponding to its neighborhood,\nexpressed as follows:\n$$Ln = Dn - An \\odot Wn$$\nbeing Dn the local Degree matrix, An the neighborhood adjacency matrix, and\nWn the corresponding weight matrix of the node n, \\odot represents the Hadamard\nproduct (element-wise product).\nThe Degree matrix Dn captures the number of edges in the node's neighbor-\nhood. In an undirected graph, the degree matrix Dis a diagonal matrix where\neach diagonal entry Dii represents the degree of the node i, which is the number\nof edges connected to it. In this case, D captures the number of connections each\nnode has.\nIn directed graphs, the concept of degree is split into in-degrees and out-\ndegrees: Din is a diagonal matrix where each diagonal entry Dii represents the\nin-degree of node i, and Dout is a diagonal matrix where each diagonal entry Dii\nrepresents the out-degree of node i. The degree matrix D for a directed graph\nis computed as $D = D_{in} + D_{out}$. Here, D combines both incoming and outgoing\nconnections, capturing the total degree in terms of both input and output edges."}, {"title": null, "content": "In weighted graphs, each edge has an associated weight. In this setting, the\ndegree matrices need to account for these weights: in $D_{win}$, for each node i, the\ndiagonal entry $D_{ii}$ is the sum of the weights of all edges directed towards node i,\nand in $D_{wout}$, for each node i, the diagonal entry $D_{ii}$ is the sum of the weights\nof all edges directed away from node i. In this context, the total degree matrix\nis given by: $D_w = D_{win} + DWout$.\nThe Laplacian matrix is a fundamental construct that reveals important\nproperties about the structure of a graph. In the context of optimizing edge\nweights within an ARGNN and identifying redundant couplings, our goal is to\nconstruct a Laplacian matrix that effectively represents the relationships between\nthese edge weights. We will assess the performance of our pruning strategy using\ntwo types of Degree matrix: the directed Degree matrix $D = D_{in} + D_{out}$ and the\nweighted Degree matrix $D_w = D_{win} + DWout$ The computation of the Degree\nmatrix $D_w$ may result in negative or zero values on its diagonal for certain\nweight configurations. In contrast, the directed Degree matrix D ensures positive\nvalues on its diagonal, with zero indicating an isolated node. The motivation for\nexploring both configurations of D is to explore different ways to construct the\nLaplacian matrix that may result in different pruning performances."}, {"title": "Pruning Decision", "content": "After the parametric optimization shown in Eq. 3, each node will compute the\nLaplacian matrix of its neighborhood expressed in Eq. 4. Let $\\lambda_n$ be the vector\nof eigenvalues of the local Laplacian matrix of the node n:\n$$\\lambda_n = eig(L_n) = [\\lambda_0, \\lambda_1, ..., \\lambda_m]$$\nCompute the imaginary parts of these eigenvalues as:\n$$\\Im(\\lambda_n) = [\\Im(\\lambda_0), \\Im(\\lambda_1), ..., \\Im(\\lambda_m)]$$\nLet a be defined as the maximum imaginary component among the eigenvalues:\n$$a = max (\\Im(X)) > 0$$\nSuppose there exists another eigenvalue $\\lambda_k$ with an imaginary component b\nsuch that b = -a. Let $n_a$ and $n_b$ be the nodes associated with the eigenval-\nues having imaginary components a and b, respectively. The edge between the\nnodes $n_a$ and $n_b$, specifically the directed edge $n_a \\leftarrow n_b$, is removed if it exists.\nImplementation and reproduction kit available at [1].\nIf two nodes have eigenvalues with opposite imaginary parts in their local\nLaplacians, it typically points to a form of structural or dynamic symmetry of\ncoupled behaviors. Utilizing this insight in pruning allows for maintaining essen-\ntial properties of the graph while reducing the amount of trainable parameters."}, {"title": "Experiments", "content": "To assess the efficacy of the proposed approach, experiments were conducted on\nlogic gates including AND, OR, and XOR. These experiments were carried out\nwith the defined Degree matrix D and Dw, and without pruning to establish a\nbaseline. The experimental setup involved circuits with 2 input nodes, 1 output\nnode, and a variable number of hidden nodes. The initial graph was set to be\nfully connected, except for the input nodes which act as placeholders and do not\nhave input from any other node. The weights of the connections were randomly\ninitialized with a uniform distribution between -1 and 1.\nDuring the experiments, each logic gate was subjected to 2000 random input\nconfigurations. The nodes within the circuits executed their operations 10 times\nper input, with the sequence of execution randomized for each input set. This\nupdate frequency ensured that each input configuration was processed multiple\ntimes to evaluate the robustness and efficiency of the proposed pruning mecha-\nnism. After the training process, the error was computed as the mean error of\neach input configuration of each gate.\nFigure 3 illustrates the experimental results. A notable initial observation\nis that the pruning mechanism has minimal impact on the resultant error of\nthe neural network, this means that the proposed method does not over-prune\nthe network. Regarding the number of edges pruned it can be noticed that the\ndegree mode Dw has a consistently superior pruning capability than D."}, {"title": "Pruning Timing", "content": "In network pruning, the timing of pruning refers to when the pruning is executed.\nAccording to [2], pruning can be categorized into three main types: pruning\nbefore training, pruning during training (with dynamic pruning falling under this"}, {"title": null, "content": "category), and pruning after training. In dynamic pruning, early pruning during\nthe initial stages of training involves removing parameters before the model fully\nconverges. This approach can lead to a more efficient training process and may\nserve as a form of regularization, encouraging the model to learn robust features.\nHowever, early pruning also carries the risk of prematurely removing important\nparameters, which could destabilize the learning process and hinder the model's\nconvergence.\nThis experiment investigates the timing of pruning within the proposed\nmethodology under both modes of the Degree matrix. Figure 4 shows the re-\nsult of the experiment, the initialization of the graph follows the same criteria\nas the previous experiment. Notably, in the Dw mode, pruning occurs earlier\ncompared to the D mode. Although both models ultimately converge to similar\nvalues, the D mode demonstrates a slightly faster convergence. This could be\nattributed to the larger parameter space in the D mode, where it is easier to\nfind a combination of parameters that successfully solves the task. In contrast,\nsmaller models have a reduced number of parametric combinations, making the\ntask of finding a suitable configuration more challenging.\nThis experiment underscores that varying the composition of the Degree\nmatrix, and consequently the Laplacian matrix, can lead to distinct pruning\nbehaviors. These differences can be strategically advantageous for specific tasks,\nsuggesting that the choice of matrix composition should be carefully considered\nin the context of the desired outcomes. By tailoring the Degree and Laplacian"}, {"title": "Discussion", "content": "The experimental results from Section 4.1, visualized in Fig. 3, indicate that\nwhile the total number of pruned edges increases as more hidden nodes are\nadded to the network, the proportion of pruned edges decreases with the grow-\ning number of hidden nodes. At first glance, this might suggest a limitation\nin the scalability of the methodology. However, this issue may not lie in the\nmethod itself, but rather in the pruning criterion. Recall that pruning occurs\nwhen two nodes have imaginary components of their eigenvalues that are equal\nand opposite. This method exclusively identifies binary couplings (i.e., direct\ninteractions between two nodes), but as the network grows, higher-order cou-\nplings-interactions involving more than two nodes become more prevalent.\nIn networks with more hidden nodes, the likelihood of finding simple pairs of\ncoupled nodes decreases, while more complex coupling patterns emerge, where\noscillations are no longer confined to one-to-one relationships. This shift towards\nhigher-order interactions requires more sophisticated methods to capture and"}, {"title": null, "content": "prune these dynamics effectively. The current pruning decision process, while\nuseful for testing the usefulness of spectral properties for pruning strategies,\nis a simplified approach. Future iterations should explore techniques that can\ndetect and handle these multi-node interactions, potentially leveraging more ad-\nvanced eigenvalue-based metrics or introducing new criteria that capture the full\nspectrum of node couplings. By addressing these complexities, we can improve\nthe pruning process to better accommodate larger and more intricate networks,\nensuring that we prune redundant connections without overlooking the subtle\nrelationships that develop as the system scales.\nIn addition, in our current pruning strategy, we focus solely on the imaginary\ncomponent of the eigenvalues, ignoring both real components (shown in Fig. 2)\nand eigenvectors. While this approach provides a baseline for evaluating spec-\ntral properties, it overlooks the potential richness of the full spectrum. Future\nstrategies could benefit from leveraging both real and imaginary components,\ncapturing more intricate structural and dynamic properties of the graph. By\nrefining these spectral insights, we can develop more sophisticated and efficient\npruning techniques that adapt to the complexity of larger networks, ultimately\nimproving both performance and scalability.\nRegarding the composition of the Laplacian matrix, the results from both ex-\nperiments reveal key insights into the impact of the degree modes (Dw and D)\non pruning effectiveness and network performance. The Dw mode consistently\nprunes more edges than the D mode, showcasing its superior pruning capability\nacross both experiments. In terms of timing, pruning occurs earlier in the Dw\nmode compared to the D mode, yet both modes eventually converge to similar\noutcomes. The slight difference in convergence speed, with D being faster, may\nresult from the larger parameter space in the D mode, which offers more combi-\nnations to solve the task. Conversely, the Dw mode's reduced parameter space\nmay present more difficulty in finding an optimal configuration, though it is still\neffective in the long term.\nDifferent compositions of the Laplacian matrix can be used to adjust pruning\ntiming, tailoring it to the needs of each task. For instance, a composition like Dw,\nwhich favors earlier pruning, could enhance efficiency in large, overparameterized"}, {"title": "Conclusion", "content": "The proposed approach leverages the spectral properties of local graph struc-\ntures to devise a fully decentralized dynamic pruning strategy for asynchronous\nrecurrent graph neural networks. This strategy was evaluated across different\nconfigurations of the local Laplacian matrix and network sizes, investigating how\nthese configurations influence performance. The results highlight the promise of\napplying spectral theory to neural network pruning. Although the task appears\nstraightforward, this research introduces innovative methods that settle the way\nfor exploring new techniques to enhance structural efficiency among decentral-\nized nodes while maintaining network performance.\nThis work excludes other significant spectral properties, such as eigenvectors\nand the real components of eigenvalues, which could potentially contribute to\nmore refined pruning strategies. While the current results are promising, future\nresearch could benefit from incorporating these additional spectral features to\nfurther optimize and expand upon the use of spectral theory in pruning neural\nnetworks."}]}