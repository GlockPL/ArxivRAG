{"title": "From PINNs to PIKANS: Recent Advances in Physics-Informed Machine Learning", "authors": ["Juan Diego Toscano", "Vivek Oommen", "Alan John Varghese", "Zongren Zou", "Nazanin Ahmadi Daryakenari", "Chenxi Wu", "George Em Karniadakis"], "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a key tool in Scientific Machine Learning since their introduction in 2017, enabling the efficient solution of ordinary and partial differential equations using sparse measurements. Over the past few years, significant advancements have been made in the training and optimization of PINNS, covering aspects such as network architectures, adaptive refinement, domain decomposition, and the use of adaptive weights and activation functions. A notable recent development is the Physics-Informed Kolmogorov-Arnold Networks (PIKANS), which leverage a representation model originally proposed by Kolmogorov in 1957, offering a promising alternative to traditional PINNs. In this review, we provide a comprehensive overview of the latest advancements in PINNs, focusing on improvements in network design, feature expansion, optimization techniques, uncertainty quantification, and theoretical insights. We also survey key applications across a range of fields, including biomedicine, fluid and solid mechanics, geophysics, dynamical systems, heat transfer, chemical engineering, and beyond. Finally, we review computational frameworks and software tools developed by both academia and industry to support PI\u039d\u039d research and applications.", "sections": [{"title": "1. Introduction", "content": "The finite element method (FEM) has been the cornerstone of Computational Science and Engineering (CSE) in the last few decades but it was viewed with skepticism when the first published works appeared in the early 1960s. Despite their success in academic research and industrial applications, FEM cannot easily assimilate measured data unless elaborate data assimilation methods are employed that render large-scale computations prohibitively expensive. FEM and other conventional numerical methods are effective in solving well-posed problem with full knowledge of the boundary and initial conditions as well as all material parameters. Unfortunately, in practical applications, there are always gaps in such a setting and arbitrary assumptions have to be made, e.g. to assume the thermal boundary conditions at the walls in power electronics cooling applications. This may lead to erroneous results as in such a problem of interest is the highest temperature or the highest heat flux that is typically located at the surface where erroneous assumptions are employed. What may be available instead are a few sparse thermocouple measurements either on the surface or inside the domain of interest. Unfortunately, current numerical methods like FEM cannot utilize such measurements effectively and hence important experimental information for the system is lost. On the other hand, neural networks are trained based on data of any fidelity or any modality so data assimilation is a natural process in such settings.\nPhysics-Informed Neural Networks (PINNs) were developed to address precisely this need, considering different simulation scenarios where there is some knowledge of the governing physical laws but not complete knowledge, and there exist some sparse measurements for some of the state variables but not for all. Hence, PINNs provide a framework to encode physical laws in neural networks [1] and resolve the disconnect between traditional physically grounded mathematical models and modern purely data-driven methods. Specifically, PINNs incorporate the governing laws by having an additional 'residual' loss term in the objective function that enforces the underlying PDE as a soft constraint. They are effective in solving both forward and inverse problems across all scientific domains. PINNs can incorporate sparse and noisy data, making them effective in scenarios where acquiring accurate measurements can be difficult or expensive. A key innovation in PINNs is the use of automatic differentiation based on computational graphs that leads to accurate treatment of the differential operators employed in conservation laws but most importantly removes the tyranny of elaborate mesh generation that is time consuming and limits solution accuracy.\nSince the original two papers appeared on the arXiv in 2017 [2, 3] and the subsequent publication of a combined paper in 2019 [1], there has been great excitement in the CSE community and very important advances on many aspects of the method have been proposed by research groups from around the world and across all scientific domains. At the time of this writing, there have already been over 11000 citations of [1], with many studies investigating the applicability of PINNs across different scientific domains while other studies proposing algorithmic improvements aimed at addressing the limitations of the original formulation. In the current review paper, we provide a compilation of most of the major algorithmic developments and present a non-exhaustive list of applications of PINNs across different disciplines. A comprehensive timeline of some of the important papers about PINNs is presented in the Appendix.\nWhile existing reviews, such as those by [4-7] summarize key aspects of PINNs, our paper differentiates itself by providing a more extensive overview of the latest algorithmic developments and by covering a broader range of applications of PINNs across scientific disciplines. Reviews by [4] and [5] focus primarily on the methodology and applications of PINNs in various domains, with less emphasis on recent algorithmic improvements. The review by [7] provides a concise overview of PINNs and their extensions, with an example on data-driven discovery of equations, but does not dive deep into applications of PINNs. The review in [6] includes a discussion of algorithmic developments, but limits the scope of their discussion on applications to thermal management and computational fluid dynamics. Additionally, several reviews focus on specific domains of application. For example, [8] and [9] review the use of PINNS in fluid dynamics, while [10] focuses on applications within power systems. In contrast, [11] conducted a bibliometric analysis of 120 research articles, highlighting key publication trends, highly cited authors and leading countries in PINN research.\nThe structure of the paper is shown schematically in Fig. 1. In Section 2 we outline the general framework of Physics-Informed Machine Learning. Section 3 provides a comprehensive summary of the major techniques aimed at improving PINNs. In Section 4 we provide an overview of the diverse applications of PINNs. Section 5 focuses on uncertainty quantification methods in PINNs. In Section 6, we summarize the developments in the theory behind PINNs. Section 7 reviews the various computational frameworks and software. Finally, in Section 8, we provide a discussion and future outlook."}, {"title": "2. Physics-Informed Machine Learning (PIML)", "content": "Physics-Informed Machine Learning (PIML) has emerged as a powerful alternative to traditional numerical methods for solving partial differential equations (PDEs) in both forward and inverse problems. PIML was first introduced in a series of papers by Raissi, Perdikaris, and Karniadakis [12] based on Gaussian processes regression (GPR); see also the patent by the same authors [13]. In this paper, however, we will review the subsequent development of PIML using neural networks and automatic differentiation, starting with the two papers from 2017 on the arXiv [2, 3], which were combined into a single paper later in [1]. It is worth noting that earlier papers by [14, 15] attempted to solve PDEs (forward problems) but without any data fusion or automatic differentiation. The PIML we present in this paper employs a representation model, namely a multilayer perceptron (MLP) or a Kolmogorov-Arnold Network (KAN), to approximate the solution of ordinary or partial differential equations (ODEs/PDEs) and match any given data and constraints by minimizing a loss function comprised of multiple terms. In particular, this loss function is designed to fit observable data or other physical or mathematical constraints while enforcing the underlying physics, e.g., conservation laws [1, 16].\nUnlike traditional numerical methods, most PIML models do not rely on predefined grids or meshes, allowing them to handle complex geometries and high-dimensional problems efficiently. By leveraging automatic differentiation, PIML models compute derivatives accurately without discretization, seamlessly integrating governing physical laws with data. This flexibility allows PIML models to approximate solutions from partial information, making them optimal for uncovering hidden parameters [1], as well as reconstructing [17] or inferring hidden fields [18] from real-world data. Moreover, PIML models are well-suited for handling high-dimensional PDEs [19], coupled systems [20, 21], stochastic differential equations [22], and fractional PDEs [23], all while maintaining scalability through parallelization on modern hardware such as GPUs [24]."}, {"title": "3 Algorithmic Developments of PIML", "content": "From the PIML framework outlined in Section 2, we can identify three key components: (1) a representation model to approximate the solution, (2) a governing equation (such as an ODE or PDE), and (3) an optimization process that minimizes a multi-objective loss function to find the optimal learnable parameters, see Fig. 2. Ongoing research has greatly enhanced PIML's baseline performance through various methods targeting these three areas, namely, modifications to the representation model, advancements in the treatment of the governing equation, and optimization process improvements."}, {"title": "3.1. Representation Model Modifications", "content": "When the representation model is defined using an MLP, the PIML formulation is referred to as physics-informed neural networks (PINNs) [1]. PINNs utilize MLPs to approximate the solutions of an ODE/PDE system ($\\hat{u} = {\\hat{u}_1,..., \\hat{u}_p}$}) by leveraging the network's ability to model complex nonlinear functions. The approximated solution ($u = {u_1, ..., u_p}$) using MLPs can be mathematically expressed as:\n$u(\\theta, x) = MLP(\\theta, x)$  (3)\n$= \\sigma \\Big(W^{(L)} (W^{(L-1)} ... \\sigma(W^{(1)}x + b^{(1)}) ... + b^{(L-1)}) + b^{(L)}\\Big)$ (4)\nwhere ($x = {x_1,..., x_n}$}) are inputs and $\\theta = {W^{(1)}, b^{(1)}}$ are the trainable parameters of the network; $W^{(1)}$ and $b^{(1)}$ denote the weights and biases of the l-th layer, respectively. The network consists of $L$ layers, and $\\sigma$ denotes a suitable activation function. The output of each layer serves as the input for the subsequent layer, culminating in the final output u.\nThe ability of MLPs to approximate virtually any continuous function on compact subsets of $R^n$ is supported by the Universal Approximation Theorem [27]. This theorem underpins the effectiveness of neural networks in modeling complex nonlinear relationships.\nBuilding on the foundational work in [1], many studies have explored enhancing the expressiveness of representation models in PINNs through various strategies. These include input and output normalization, feature expansions, hard constraint encoding, model decompositions, and architectural modifications. Each of these strategies aims to improve the network's ability to capture the underlying physics of the problem more accurately and efficiently."}, {"title": "3.1.1. Input/Output Transformations", "content": "One of the most straightforward ways to improve the stability and accuracy of a representation model Mis by transforming the model inputs $x \\in R^n$ or outputs $u \\in R^p$ using suitable mappings, $I(\\cdot)$and $\\Gamma(\\cdot)$ (see Fig. 3). Under this reformulation, the solutions of an ODE/PDE ($\\hat{u}$) can be approximated as:\n$\\hat{u}(x) \\approx u(\\theta, x) = \\Gamma(M(\\theta, I(x))),$  (5)\nwhere $u = {u_1, ..., u_p}$}) are the approximated solutions from the representation model M, and $I : R^n \\rightarrow R^m$isa mapping that transforms the multivariate inputs and enhances the model's expressivity. The choice of I often depends on the activation function $\\sigma$, as different activation functions are more effective over specific input domains. For example, Cai et al. [37] proposed normalizing inputs to the range [-1,1] when using sin(\u00b7) or tanh(\u00b7) activation functions. Similarly, Raissi et al. [18] recommended normalizing inputs based on their mean and standard deviation when using the swish(\u00b7) activation function."}, {"title": "Feature Expansions", "content": "These modifications aim to address some fundamental weaknesses in the conventional PIML formulation, particularly in learning high-frequency functions, known as spectral bias [41, 42], and capturing other complex relations [30]. To mitigate these issues, researchers have modified the baseline model formulation by transforming its input $x \\in R^n$ into an expanded input $I(x) \\in R^m$, which is then fed into the representation model as shown in Fig. 3. Several expansion maps have been explored, with their selection typically based on the specific problem. For example, Wang et al. [30] proposed using random Fourier features and demonstrated that this modification helps to mitigate spectral bias. Other types of expansions, such as polynomials, exponentials, Chebyshev polynomials, and even gated recurrent units (GRU), have also been explored or proposed in previous studies [43\u201348]."}, {"title": "Hard Constraints", "content": "Training in PIML involves minimizing an objective function that often combines multiple constraints, significantly complicating the optimization process [49]. Research has shown that improper enforcement of boundary conditions can degrade both the performance and stability of neural network training [28, 36, 50]. Thus, accurately imposing boundary conditions is crucial for improving model reliability and efficiency. In particular, Zeinhofer et al. [51] theoretically demonstrated that hard constraints can lead to lower error estimates in linear problems. One practical way to address these challenges is by embedding boundary conditions directly into the model's structure, either through input/output transformations or specialized architectures, thereby simplifying the optimization and enhancing the overall performance."}, {"title": "Dirichlet Boundary Conditions", "content": "Several methods have been developed to enforce Dirichlet boundary conditions exactly in PIML problems. Berrone et al. [52] introduced the Nitsche's method, which applies a variational approach to enforce these conditions. Another systematic technique is the Theory of Functional Connections, which imposes constraints through functional connections, as detailed by Leake et al. [53]. On the other hand, hPINNs utilize penalty methods and the augmented Lagrangian approach to impose hard constraints, providing a flexible framework for handling various boundary conditions [54]. Notably, Sukumar et al. [35] introduced Approximate Distance Functions (ADF), which impose boundary conditions through output transformations. Under the ADF framework, the constrained expression for Dirichlet boundary conditions is represented as:\n$\\hat{u}(x) \\approx u(\\theta, x) = \\Gamma(M(\\theta, I(x))) = g(x) + \\phi(x)M(\\theta, I(x)),$ (6)\nwhere $\\Gamma(\\cdot)$ transform the network output in terms of a function that satisfies the solution $\\hat{u}$ along the boundaries $g(\\cdot)$ and a composite distance function $\\phi(\\cdot)$ that equals zero when evaluated on the boundaries. If the boundary is composed of $M$ partitions, denoted as $[S_1, ..., S_M]$, the composite distance function for Dirichlet boundary conditions can be expressed as $\\phi(\\phi_1,\\phi_2,...,\\phi_M) = \\Pi_{i=1}^{M} \\phi_i$, where $[\\phi_1,...,\\phi_M]$ are the individual distance functions. Notice that if $x \\in S_i$, then $\\phi_i(x) = 0$, ensuring that the neural network approximation exactly satisfies the boundary conditions, i.e., $u(x) = g(x)$."}, {"title": "Periodic Boundary Conditions", "content": "On the other hand, this type of boundary conditions can be strictly enforced as hard constraints by selecting a suitable input transformation $I(x)$ (see Fig. 3). For instance, the periodic nature of a smooth univariate function $u(x)$ can be encoded into a model using a one-dimensional Fourier feature embedding, $I(x) = [1, cos(\\omega_x x), sin(\\omega_x x), . . ., cos(m\\omega_x x), sin(m\\omega_x x)]$. Dong et al. [28] demonstrated that any representation model, such as $u(\\theta, I(x))$, is periodic along the x coordinate when using this Fourier feature embedding. Similar expansion maps have been explored for higher dimensions by Wang et al. [55].\nOther methods to impose hard constraints for periodic boundary conditions include using hPINNs, which employ the penalty and augmented Lagrangian methods to enforce such constraints [54]. Additionally, hybrid approaches that combine various techniques for the exact imposition of periodic boundary conditions have been investigated [56]."}, {"title": "Complex Constraints", "content": "Complex constraints can be addressed through specialized architectures that incorporate domain-specific knowledge into the learning process. One such approach is the use of Divergence-Free Networks, which apply an appropriate output transformation to ensure that the learned vector fields satisfy divergence-free conditions (see Fig.3), as required in certain fluid dynamics applications[36, 49, 57]. Zeinhofer et al.[51] theoretically demonstrated that enforcing divergence-free constraints leads to improved error estimates in linear problems. Another example is SympNets, a specialized architecture for identifying Hamiltonian systems from data, which preserves the symplectic structure through different Jacobian matrix factorization techniques[58]. Finally [59] proposed a method that uses the theory of functional connections to exactly enforce the data constraints in inverse problems."}, {"title": "3.1.2. Architectures", "content": "The original PIML formulation uses an MLP as the representation model [1]. The building blocks of an MLP are perceptrons, which define a layer l and can be defined as follows:\n$z^{(l)} = \\sigma^{(l)} \\Big(W^{(l)} z^{(l-1)} + b^{(l)}\\Big),$  (7)\nwhere $z^{(l)} = {z_1^{(l)}, ..., z_j^{(l)}}$ are the outputs of layer l, $\\sigma^{(l)}$ is a non-linear activation function (e.g., sigmoid(\u00b7), tanh(\u00b7), sin(\u00b7), etc.), and $W^{(l)}$ and $b^{(l)}$ are trainable parameters that linearly transform the inputs $z^{(l-1)} = {z_1^{(l-1)}, ..., z_j^{(l-1)}}$.  Several approaches have been proposed to improve the perceptron's capabilities. For instance, Jagtap et al. [34, 60, 61] proposed modifying the activation function in as $\\sigma^{(l)} (\\cdot) = \\sum_j a_i \\sigma(f_j.)$, where $a_i$ and $f_j^{(l)}$ are trainable parameters. The authors showed both theoretically and empirically that these modifications significantly improve model performance.\nOther approaches proposed modifying the weight matrix $W^{(l)}$; for instance, [18, 32] proposed decomposing $W^{(l)}$ into its magnitude and its direction via weight normalization described as $W^{(l)} = g \\frac{v^{(l)}}{||v^{(l)}||^2}$. This re-parameterization speeds up the model convergence with minimal computational overhead [32]. Similarly, [33] proposed weight factorization $W^{(l)} = diag(s^{(l)}) \\cdot V^{(l)}$, where $s^{(l)}$ are trainable parameters. The authors experimentally and theoretically showed that this reparameterization significantly improves the model performance [33]."}, {"title": "Other Representation Models", "content": "One natural extension to other representation models, given their similarity to MLPs, is Kolmogorov-Arnold Networks (KANs) [62], which were introduced as PIKANs in [16]. Each PIKAN layer can be described as follows:\n$z_i^{(l)} = \\phi\\Big(\\sum_{j=1}^{H} \\Phi_{i,j} (z_j^{(l-1)}) \\Big),$  (8)\nwhere $z^{(l-1)} = {z_1^{(l-1)},..., z_j^{(l-1)}}$ is the multivariate input, $H$ denotes the number of neurons and $\\Phi_{ij}$ are the outer and $\\phi_{i,j}$ are the inner univariate functions. The specific form of $\\phi(\\cdot)$ and $\\Phi(\\cdot)$ defines the types of KAN architectures. Among these variations, [16] introduced cPIKANs, which use Chebyshev polynomials as inner and outer univariate functions defined as $\\phi(\\zeta, \\theta) = w_n \\sum_d C_n T_n(tanh(\\zeta))$, where are the inputs, $\\theta = (w_n, C_n)$ are trainable parameters, d is the degree and $T_n$ is the n-th order Chebyshev polynomial, defined recursively as $T_n(\\zeta) = 2\\zeta T_{n-1}(\\zeta) + T_{n-2}(\\zeta)$ [63]. The authors found that this stable representation is more robust to noise [16] and can lead to improved performance with fewer parameters [16, 49] than MLPs. Subsequent studies extended the KAN framework and developed new architectures for PIML problems [48, 49, 64\u201370].\nOther representation models have also been explored, including convolutional neural networks (CNNs) [71], Hermite spline CNNs [72], generative adversarial neural networks (GANs) [73, 74], spiking neural networks [75], transformers [76], long short-term memory (LSTM) networks [77, 78], information-botleneck inspired architectures [79] and reinforcement learning models [80\u201382]."}, {"title": "Residual Connections", "content": "Another approach to improve the PIML architecture performance is by adding residual connections (see Fig. 3), which enable accurate calculation of high-order derivatives. The general formulation for a single additive skip connection is defined as follows:\n$z^{(l)} (z^{(l-1)}, \\theta) = M^{(l)}(z^{(l-1)}, \\theta) + z^{(l-1)},$  (9)\nwhere $z^{(l)} = {z_1^{(l)}, ..., z_j^{(l)}}$ are the outputs of layer l, $M^{(l)}$ is a layer of the representation model (e.g., MLP layer, KAN layer), and $z^{(l-1)} = {x_1^{(l-1)},...,x_j^{(l-1)}}$ are the inputs to layer l. Several studies have explored the advantages of incorporating skip connections via addition or multiplication. Wang et al. [36] pioneered this approach by introducing a method referred to as modified MLP, where two single-layer MLPs project the model inputs (x) into a high-dimensional feature space, which is then used to update the remaining hidden layers via element-wise multiplication and addition. Other approaches introduce multiplicative connections between every layer [83]. Finally, the modified MLP was further improved by incorporating adaptive residual connections that incorporate a new learnable parameter that controls the contribution of the deeper layers with respect to the input [31]. This improved architecture enables the use of deeper networks without compromising the accuracy of PIML problems."}, {"title": "Model Decomposition", "content": "Further improvements can be achieved by splitting the network into several components. Cho et al. [84] proposed separable PINNs (sPINNs), which utilize separate sub-networks to approximate the desired solution.Under this formulation, the solution of a PDE is approximated as $\\hat{u}(x_1,\u2026, x_d) = \\sum_{i=1}^{p} \\prod_{j=1}^{d} M_j(x_i, \\theta_i)$, where ${M_j(x_i, \\theta_i)}_{i=1}^p$ are the outputs univariate representation models that encodes each dimension $x_i$ into a p-dimensional space. This decomposition addressed the curse-of-dimensionality and was introduced to the deep learning community as tensor neural networks in [85]. While [85] focused on improving accuracy through enhanced numerical integration methods, [84] prioritized computational efficiency, achieving up to 60-times speedups for high-dimensional problems, such as the 3D Helmholtz Equation and the 4D Navier-Stokes Equation. This formulation has been further explored and improved in subsequent studies [19, 86, 87].\nOther types of decompositions have also been explored for inverse problems [59, 88]. For instance, [88] extended the negative log-likelihood (NLL) framework [89] for linear PDEs in PIML, enabling the quantification of aleatoric uncertainty and improving model performance. The authors assumed that the observed data was corrupted by noise, so they decomposed the desired solution into mean fields and fluctuations as u =\n\u016b+ u'. Then they used independent models to learn the data-driven mean fields \u016b and their corresponding fluctuations, standard deviations u' by using the NLL criterion [89\u2013 91]. On the other hand, [59] proposed a method to simultaneously reconstruct flow states and determine particle properties from Lagrangian particle tracking (LPT) using a neural network as a flow model and a data-constrained polynomial as a particle model."}, {"title": "3.2 Governing Equations", "content": "PIML aims to obtain a representation model u that adheres to the governing equations. In the original study by [1], the ODE/PDE and boundary conditions (BCs) are enforced by iteratively minimizing the strong-form residuals from the governing equations. The ODE/PDE residuals $r_e(u, \\theta)$ and boundary conditions (or initial conditions) $r_B(u, \\theta)$ are defined as:\n$r_e(x, \\theta) = F_{\\tau}[u](x, \\theta) \u2212 f(x),  x \\in \\Omega_E$, (10)\n$r_B(x, \\theta) = B_{\\tau}[u](x, \\theta) \u2212 b(x),  x \\in \\Omega_B$. (11)\nThese residuals quantify the extent to which the approximation u satisfies the ODE/PDE and boundary constraints specified in Eq. 1. If $r_e = 0$ and $r_B = 0$, the approximated solution satisfies the PDE and BCs exactly.\nFor inverse problems, it is necessary to incorporate additional observations within the domain. The disagreement between the observations and predictions can be quan-"}, {"title": "3.2.1. Derivative Calculation", "content": "To enforce governing laws, it is necessary to compute the spatial and temporal derivatives of the approximated solution in order to construct and penalize PDE residuals. In the original formulation by [1], these derivatives were computed exactly using automatic differentiation (AD). AD leverages the fact that all numerical computations are ultimately compositions of a finite set of elementary operations, for which derivatives are known [25, 98]. However, AD significantly increases computational cost due to the need for calculating and multiplying gradients at each layer, which can become inaccurate for higher-order derivatives [31] and infeasible for fractional operators or high-dimensional problems [19]. To address these challenges, several studies have explored alternatives to or enhancements of backpropagation."}, {"title": "Alternative Differentiation Methods", "content": "[92] proposed approximating derivatives using finite differences, which speeds up computation; however, this method relies on a predefined grid, limiting its broader applicability. Other approaches [93, 99] involve predicting derivatives as additional network outputs and learning the relationship through an auxiliary loss function. To handle fractional derivatives, several studies have employed Monte Carlo methods [23, 100\u2013104]."}, {"title": "High-Dimensions", "content": "One of the main advantages of PIML methods is their ability to handle high-dimensional problems [98, 105]. However, computing derivatives with AD becomes particularly challenging in such cases since the requirements for derivative calculation increase with the number of dimensions. Alternative approaches to AD have been proposed, particularly for high-dimensional problems. For instance, [106] introduced a Gaussian-smoothed model with Stein's identity to parameterize PINNs, bypassing backpropagation and accelerating convergence. Additionally, [19] proposed Stochastic Dimension Gradient Descent (SDGD), a method that decomposes the gradient of PDEs and PINN residuals into components corresponding to different dimensions. During each training iteration, a subset of these dimensional components is randomly sampled, resulting in a highly efficient approach that enables solving PDES with up to 100,000 dimensions."}, {"title": "3.2.2. ODE/PDE Reformulations", "content": "To enhance the performance of PIML models, several studies have proposed reformulations of the ODE/PDEs."}, {"title": "Non-dimensionalization", "content": "As discussed in [107], one of the simplest and most effective ways to improve model performance is through non-dimensionalizing the governing equations. In this approach, the inputs and predicted outputs are scaled using characteristic units, helping to identify important non-dimensional numbers (e.g., Reynolds, Peclet, Prandtl, Rayleigh) that characterize the solution's behavior. Additionally, choosing appropriate characteristic units can control the magnitude of the inputs and outputs, which is crucial for stabilizing the training process. Several studies have successfully approximated solutions to PDEs in their non-dimensional form [17, 18, 20, 36, 37, 49, 88, 108]."}, {"title": "Equivalent and Auxiliary Formulations", "content": "Another approach to improving model performance involves transforming the governing equations into an equivalent form that simplifies the optimization problem. For example, Wang et al. [36] and subsequent studies [57] solved the Navier-Stokes equations using the streamfunction formulation, which inherently satisfies the conservation of mass, thereby reducing the number of constraints to optimize. Similarly, Jin et al. [20] reformulated the Navier-Stokes equations into their vorticity formulation, achieving better performance. Basir et al. [109] introduced an auxiliary vorticity variable, which lowered the order of the Stokes equations, further simplifying the problem. In some cases, these reformulations are necessary to achieve an acceptable solution. For instance, Toscano et al. [49] reformulated the Rayleigh-B\u00e9nard equations using the vorticity formulation, eliminating the pressure dependence and enabling the inference of temperature from sparse turbulent velocity data. Similarly, Wang et al. [108] reformulated the Navier-Stokes equations with an entropy-viscosity method, allowing for the approximation of solutions at high Reynolds numbers."}, {"title": "3.2.3. Differential Operator Variations", "content": "The PIML approach is flexible enough to solve several types of problems even with multiple solutions; for instance, Huang et al. [94] proposed homotopy physics-informed neural networks (HomPINNs) for solving multiple solutions of nonlinear elliptic differential equations. This flexibility allows handling residuals in their strong or weak form and obtaining approximate solutions from different types of operators, leading to various PIML extensions."}, {"title": "Variational Methods", "content": "Several studies have proposed weakly enforcing the PDE and boundary constraints by solving the problem in its variational form. This approach, known as variational PINNs (vPINNs), was introduced by [110]. Similar to the Deep Ritz Method [111], vPINNs compute weighted integrals of the residuals by projecting them onto a suitable space of test functions V [97]. In the variational form, the residuals are represented as:\n$R_{e,j}(u) = \\int_{\\Omega} r_e(x, \\theta)v_j dx,$\n$R_{b,j}(u) = \\int_{\\Omega_B} r_b(x, \\theta)v_j dx,$\nwhere $v_j$ represents a chosen test function. Ideally, the exact solution is obtained when all residuals are identically zero [97]. Various types of test functions have been proposed, including Dirac-delta functions [96], global [110], piece-wise [112] polynomials, and non-overlapping functions [97]. The vPINN formulation has also been explored in the context of mesh-free methods [113], variable coefficients [114], volume-weighted methods [115], and has been optimized for computational efficiency [116, 117]."}, {"title": "Fractional Differential Equations", "content": "Another extension involves fractional operators, giving rise to the fractional PINNs (fPINNs) framework, first introduced by [23]. Several studies have explored and expanded upon the fPINN formulation [100\u2013104, 118]. For example, [102] proposed a Monte Carlo-based method to solve fractional partial differential equations on irregular domains. Furthermore, [103] provided a theoretical analysis to estimate the training and generalization errors for the \u03c8-Caputo type fractional PDE. Lastly, [104] extended the fPINN framework to overcome the curse of dimensionality in fractional and tempered fractional PDEs."}, {"title": "Stochastic Differential Equations", "content": "Stochastic Differential Equations (SDEs) have also been explored within the PIML framework. For instance, [73] utilized GANs as a representation model and applied automatic differentiation to encode the governing laws for solving SDEs. Similarly, [119] combined spectral dynamically orthogonal (DO) and dynamically biorthogonal (BO) methods with PIML to develop two novel PINN approaches for solving time-dependent SDEs. Furthermore, [98] extended the PIML formulation to solve high-dimensional SDEs; see also [120] for more recent PINN algorithms for high-dimensional Fokker-Planck and Hamilton-Jacobi-Bellman equations."}, {"title": "3.3 Optimization Process", "content": "Training a PIML model involves solving an optimization problem that enables a representation model to approximate the solution of a governing equation. The model parameters are learned by optimizing a loss function (i.e., objective function), which minimizes the residuals (i.e., Eqns. 10, 11, and 12) of the governing equations, boundary conditions, and data observations. This loss function is minimized using an optimization algorithm, often referred to as an \"optimizer.\u201d Based on this description, the optimization process in PIML can be broken down into three main subcomponents: the optimization problem, the loss (i.e., objective) function, and the optimizer."}, {"title": "3.3.1. Optimization Problem", "content": "The optimization problem in PIML can be summarized as minimizing a multi-objective loss function that encourages the model to satisfy constraints related to boundary conditions, governing equations, and potentially observational data. To simplify this problem, several studies have sought to reduce the number of constraints. For instance, in systems of PDEs, [21] proposed using numerical solvers to provide easily accessible high-fidelity data, using the PIML model primarily to uncover hidden fields that are difficult to recover with traditional methods. Other approaches aim to simplify the problem through domain decomposition, learning from low-fidelity data, and sequential training strategies."}, {"title": "Domain Decomposition", "content": "The baseline approach for domain decomposition was introduced in the extended PINNs (XPINNs) framework [128] and can be summarized as follows. First, the training domain \u03a9 is divided into N smaller subdomains $\u03a9_i \u2282 \u03a9$. Then, N PIML submodels are trained to approximate the solution on each subdomain"}, {"title": "Sequential Training", "content": "Sequential training can be viewed as a form of \u201cproblem decomposition,\" where the model learns or satisfies objectives sequentially. Ahmadi et al. [138] proposed a novel method for addressing specific challenges in inverse problems, where the values of constant parameters change at specific times and the system experiences abrupt spikes due to sudden input changes. In this case, it is essential to train the model on sequential intervals, which are strategically chosen to capture the spikes observed in the system. For example, in the CMINNs method, the tumor growth model following drug administration exhibits spikes that cannot be effectively represented without decomposition. Furthermore, domain decomposition is necessary for the inference of piecewise-constant parameter values over continuous intervals, enhancing the ability to capture variations in drug efficacy and providing insights into tolerance phenomena in multi-dose administration."}, {"title": "Time"}]}