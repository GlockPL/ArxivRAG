{"title": "PiPa++: Towards Unification of Domain Adaptive Semantic Segmentation via Self-supervised Learning", "authors": ["Mu Chen", "Zhedong Zheng", "Yi Yang"], "abstract": "Unsupervised domain adaptive segmentation aims to improve the segmentation accuracy of models on target domains without relying on labeled data from those domains. This approach is crucial when labeled target domain data is scarce or unavailable. It seeks to align the feature representations of the source domain (where labeled data is available) and the target domain (where only unlabeled data is present), thus enabling the model to generalize well to the target domain. Current image- and video-level domain adaptation have been addressed using different and specialized frameworks, training strategies and optimizations despite their underlying connections. In this paper, we propose a unified framework PiPa++, which leverages the core idea of \"comparing\" to (1) explicitly encourage learning of discriminative pixel-wise features with intraclass compactness and inter-class separability, (2) promote the robust feature learning of the identical patch against different contexts or fluctuations, and (3) enable the learning of temporal continuity under dynamic environments. With the designed task-smart contrastive sampling strategy, PiPa++ enables the mining of more informative training samples according to the task demand. Extensive experiments demonstrate the effectiveness of our method on both image-level and video-level domain adaption benchmarks. Moreover, the proposed method is compatible with other UDA approaches to further improve the performance without introducing extra parameters.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, deep neural networks (DNNs) have made significant advancements in the field of computer vision. By processing vast amounts of data, DNNs effectively extract discriminative features and capture subtle patterns from complex visual data, excelling in various visual tasks. This progress has greatly propelled the computer vision applications [5]\u2013[9], leading to unprecedented achievements in scene understanding tasks such as semantic segmentation, object detection, and scene graph generation. However, deep neural networks are data-hungry and typically require abundant training datasets to be properly trained. For dense prediction tasks such as semantic segmentation, pixel-level annotations are required [2], [10]. In video scenarios, it is even more demanding [11], as dense annotations are needed for each video frame, which entails a prohibitively expensive and time-consuming annotation process. In real-world scenarios, such detailed annotation is hard to meet. To address the shortage of training data, one straightforward idea is to access the abundant synthetic data and the corresponding pixel-level annotations generated by computer graphics. However, there exist domain gaps between synthetic images and real-world images in terms of illumination, weather, and camera parameters. To minimize such a gap, researchers resort to unsupervised domain adaptation (UDA) to transfer the knowledge from labeled source-domain data to the unlabeled target-domain environment.\nPrevailing UDA methods are typically designed with highly tailored training paradigms and optimization strategies for static and dynamic scenes. In static scenes, the core research focus is on aligning the source domain and target domain using distribution matching methods such as adversarial training [12]\u2013[15] and self-training [16]\u2013[19]. In dynamic scenes, existing methods [20]\u2013[23] usually leverage optical flow estimation and prediction propagation to bridge distribution shifts across different video domains. While image-level UDA and video-level UDA methods are rapidly improving the state-of-the-art for their benchmarks, they are overwhelmingly task-specific. These methods' specialized designs cannot conceptually generalize to each other, leading to duplicated research and hardware optimization efforts for both tasks. To address this fragmentation, a natural question arises: what must a model possess to effectively tackle both static and dynamic scenarios? Ideally, it should not only 1) attain strong spatial contextual relations but also 2) be well-designed to address temporal continuity at the same time. Prior self-supervised learning methods and their core idea of \"learning to compare\" have achieved tremendous success in various applications, and they hold significant potential for scene understanding tasks. This success motivates us to rethink the de facto training paradigms of image- and video-semantic segmentation under domain shift. Pixel-level contrast, aside from its basic function of comparing similar classes to distinguish them from other categories to make the networks understand the semantic context better during the training process, should encompass two additional layers of understanding for a dynamic scene to form a good representation space. On one hand, it needs to be aware of the spatial context to prevent totally ignoring the contexts in pixel-level comparisons. On the other hand, humans use \"comparison\" to leverage semantic structures of preceding frames to aid in understanding the current frame. Hence, contrastive learning should enable comparison across frames to achieve temporal continuity and implicit cross-frame correspondences.\nWith the above insight, we present a novel method, namely PiPa++, which takes advantage of self-supervised learning to naturally achieve a unified architecture by leveraging spatial and temporal representation learning. PiPa++ (1) fully exploits pixel-level contrastive learning to aggregate spatial contextual information and passes the learned spatial information from the source domain to the target domain. (2) Recognizing the rich temporal relationships that can serve as valuable clues for representation learning, it wisely uses the distinguished semantic concepts in historical frames to aid the recognition of the current frame while maintaining temporal continuity. The choice of contrastive samples is crucial for contrastive learning [8], [24]. PiPa++ designs scenario-smart sample mining strategies. For static scenarios, PiPa++ exploits samples from the entire dataset by maintaining a memory bank for each class, thus fully leveraging the dataset's implicit diverse semantic structure to achieve a better representation space. In multi-frame dynamic scenes, PiPa++ restricts the choice of contrastive samples to a limited number of essential frames to prevent temporal inconsistency caused by frames from long distances. To evaluate the performance of our method, we conduct extensive experiments by following previous works on the widely used benchmarks GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes for image-level UDA, and SYNTHIA-Seq\u2192 Cityscapes-Seq, VIPER \u2192 Cityscapes-Seq for video-level UDA. We achieve notable improvements across these benchmarks (shown in Figure 1). Moreover, our method is compatible with other UDA approaches to further improve performance without introducing extra parameters.\nOur main contributions are as follows:\n(1) We introduce PiPa++, a unified framework that tackles both image-level and video-level DA under an identical architecture by leveraging self-supervised spatial and temporal representation learning.\n(2) For static environment, different from existing works on inter-domain alignment, we focus on mining domain-invariant knowledge from the original domain in a self-supervised manner. The proposed Pixel- and Patch-wise self-supervised learning framework harnesses both pixel- and patch-wise consistency against different contexts, which is well-aligned with the segmentation task. For dynamic environment, we maintain temporal continuity without introducing optical flow calculation or additional neural network modules, streamlining the complex pipelines adopted by existing methods.\n(3) We propose task-smart sampling strategies to capture more informative samples in both static and dynamic scenarios to aid contrastive learning.\n(4) Our self-supervised learning framework does not require extra annotations and is compatible with other existing UDA frameworks. The effectiveness of PiPa++ has been tested by extensive ablation studies, and it achieves competitive accuracy on four commonly used UDA benchmarks in image- and video- level.\nPiPa++ is an extension of our previous conference version, PiPa [25]. Compared to the previous version, this work introduces the following new content:\n(1) PiPa++ extends the image-level PiPa to a unified architecture that tackles both image-level and video DA tasks without introducing additional parameters. Further experiments are conducted on two widely adopted video-level DA benchmarks.\n(2) PiPa++ proposes a task-smart sampling strategy to further boost the performance of PiPa.\n(3) To verify the proposed method as a general framework that is orthogonal to existing methods and scalable to different scenarios, we conduct experiments involving multi-source domain adaptation, Clear-to-Adverse-Weather adaptation, and cross-city adaptation. In each case, we observe consistent improvements."}, {"title": "II. RELATED WORK", "content": "This work is related to existing work on unsupervised domain adaptation, and contrastive learning.\na) Unsupervised Domain Adaptation: Pioneering image-level UDA works [12], [26] propose to transfer the visual style of the source domain data to the target domain using Cycle-GAN [27]. Later UDA methods can mainly be grouped into two categories according to the technical routes: adversarial training [14], [15], [19], [28]\u2013[31] and self-training [16], [17], [32]\u2013[36]. Adversarial training methods aim to learn domain-invariant knowledge based on adversarial domain alignment. For instance, Tsai et al. [14] and Luo et al. [15] learn domain-invariant representations based on a min-max adversarial optimization game. However, as shown in [37], unstable adversarial training methods usually lead to suboptimal performance. Another line of work harnesses self-training to create pseudo labels for the target domain data using the model trained by labeled source domain data. Pseudo labels can be pre-computed either offline [16], [19] or generated online [25], [35], [38], [39]. Due to considerable discrepancies in data distributions between two domains, pseudo labels inevitably contains noise. To decrease the influence of faculty labels, Zou et al. [16], [17] adopts pseudo labels with high confidence.\nTaking one step further, Zheng et al. [40] conducts the domain alignment to create reliable pseudo labels. Furthermore, some variants leverage specialized sampling [32] and uncertainty [18] to learn from the noisy pseudo labels. In addition to the two mainstream practices mentioned above, researchers also conducted extensive attempts such as entropy minimization [30], [41], image translation [42], [43], Graph Network [44] and combining adversarial training and self-training [37], [45], [46]. Source-free domain adaptation, although a relatively recent concept, has been extensively studied across various fields [47]-[50]. Recently, Pan et al. [51] minimizes the intra-domain discrepancy by separating the target domain into an easy and hard split using an entropy-based ranking function. Yan et al. [52] conducts the inter-domain adaptation between the source and target domain by treating each pixel as an instance.\nPrior video-level UDA methods [20]\u2013[22] have explored various strategies to tackle domain shifts in video segmentation. Guan et al. [20] made the first attempt at video-to-video domain adaptive semantic segmentation, in which both cross-domain and intra-domain temporal consistencies are considered to regularize the learning. Wu et al. [21] introduced a novel approach that does not rely on videos for the source domain but instead uses images, leveraging a proxy network to produce pseudo labels for target predictions. Cho et al. [53] extended image domain adaptation strategies to video using temporal information by proposing Cross-domain Moving Object Mixing (CMOM) and Feature Alignment with Temporal Context (FATC). Xing et al. [23] proposed a method named Temporal Pseudo Supervision (TPS) that explores consistency training in spatiotemporal space for effective domain adaptive video segmentation. Different from the above-mentioned works that focus on either image-level or video-level tasks and design-specific architectures, training, and optimization strategies, we utilize the idea of self-supervised learning to design a unified architecture. The proposed method is orthogonal with the above-mentioned approaches, and thus is complementary with existing ones to further boost the result.\nb) Contrastive Learning: Contrastive learning is one of the most prominent unsupervised representation learning methods [54]\u2013[58], which contrasts similar (positive) data pairs against dissimilar (negative) pairs, thus learning discriminative feature representations. For instance, Wu et al. [55] learn feature representations at the instance level. He et al. [58] match encoded features to a dynamic dictionary which is updated with a momentum strategy. Chen et al. [57] proposes to engender negative samples from large mini-batches. In the domain adaptative image classification, contrastive learning is utilized to align feature space of different domains [59], [60]. A few recent studies utilize contrastive learning to improve the performance of semantic segmentation task [24], [61]\u2013[65]. For example, Wang et al. [61] have designed and optimized a self-supervised learning framework for better visual pre-training. Gansbeke et al. [62] applies contrastive learning between features from different saliency masks in an unsupervised setting. Recently, Huang et al. [66] tackles UDA by considering instance contrastive learning as a dictionary look-up operation, allowing learning of category-discriminative feature representations. Xie et al. [67] presents a semantic prototype-based contrastive learning method for fine-grained class alignment. Other works explore contrastive learning either in a fully supervised manner [24], [64] or in a semi-supervised manner [68]\u2013[70]. For example, Wang et al. [24] uses pixel contrast in a fully supervised manner in semantic segmentation. But most methods above either target image-wise instance separation or tend to learn pixel correspondence alone. Different from existing works, we introduce a multi-grained self-supervised learning framework to formulate pixel- and patch-wise contrast in a similar format but at different effect regions. The unified self-supervised learning on both pixel and patch are complementary to each other, and can mine the domain-invariant context feature. Existing works [8], [71], [72] have also attempted to explore temporal knowledge in video segmentation tasks through contrastive learning. However, unlike them, our approach is conducted in an unsupervised manner within the unsupervised domain adaptation (DA) context."}, {"title": "III. METHOD", "content": "In this section, we first introduce the problem definition and conventional segmentation losses for semantic segmentation domain adaptation. Then we shed light on the proposed component of our framework PiPa, i.e., Pixel-wise Contrast and Patch-wise Contrast, both of which work on local regions to mine the inherent contextual structures, which is designed for static scenes. Next, we extend PiPa to dynamic scenes to naturally achieve temporal continuity, resulting in PiPa++. We finally also raise a discussion on the mechanism of the proposed method.\nProblem Definition. As shown in Figure 2, given the source-domain synthetic data \\(X^{S} = \\{x_{u}\\}_{u=1}^{U}\\) labeled by \\(Y^{S} = \\{y_{u}\\}_{u=1}^{U}\\) and the unlabelled target-domain real-world data \\(X^{T} = \\{x_{v}\\}_{v=1}^{V}\\), where U and V are the numbers of images in the source and target domain, respectively. The label \\(Y^{S}\\) belongs to C categories. Domain adaptive semantic segmentation intends to learn a mapping function that projects the input data \\(X^{T}\\) to the segmentation prediction \\(Y^{T}\\) in the target domain.\nBasic Segmentation Losses. Similar to existing works [17], [40], we learn the basic source-domain knowledge by adopting the segmentation loss on the source domain as:\n\\[L_{ce} = E[-\\mathbf{p} \\log h_{cls}(g_{\\theta}(x^{s}))],\\]\nwhere \\(\\mathbf{p}\\) is the one-hot vector of the label \\(y\\), and the value \\(\\mathbf{p}(c)\\) equals to 1 if \\(c == y\\) otherwise 0. We harness the visual backbone \\(g_{\\theta}\\), and 2-layer multilayer perceptrons (MLPs) \\(h_{cls}\\) for segmentation category prediction.\nTo mine the knowledge from the target domain, we generate pseudo labels \\(\\widehat{Y}^{T} = \\{\\hat{y}_{v}\\}_{v=1}^{V}\\) for the target domain data \\(X^{T}\\) by a teacher network \\(g_{\\bar{\\theta}}\\) [35], [73], where \\(\\hat{y}_{v} = \\text{argmax}(h_{cls}(g_{\\bar{\\theta}}(x^{T}_{v})))\\). In practice, the teacher network \\(g_{\\bar{\\theta}}\\) is set as the exponential moving average of the weights of the student network \\(g_{\\theta}\\) after each training iteration [37], [74]. Considering that there are no labels for the target-domain data, the network \\(g_{\\theta}\\) is trained on the pseudo label \\(\\hat{y}^{T}\\) generated by the teacher model \\(g_{\\bar{\\theta}}\\). Therefore, the segmentation loss can be formulated as:\n\\[L_{ce}^{T} = E[-\\mathbf{p_{\\hat{y}^{T}}} \\log h_{cls}(g_{\\theta}(x^{T}))],\\]\nwhere \\(\\mathbf{p_{\\hat{y}^{T}}}\\) is the one-hot vector of the pseudo label \\(\\hat{Y}^{T}\\). We observe that pseudo labels inevitably introduce noise considering the data distribution discrepancy between two domains. Therefore, we set a threshold that only the pixels whose prediction confidence is higher than the threshold are accounted for the loss. In practice, we also follow [35], [38] to mix images from both domains to facilitate stable training. Specifically, the label \\(\\hat{y}^{Mix}\\) is generated by copying the random 50% categories in \\(y^{s}\\) and pasting such class areas to the target-domain pseudo label \\(\\hat{y}^{T}\\). Similarly, we also paste the corresponding pixel area in \\(x^{s}\\) to the target-domain input \\(x^{T}\\) as \\(x^{Mix}\\). Therefore, the target-domain segmentation loss is updated as:\n\\[L_{ce} = E[-\\mathbf{p_{\\hat{y}^{Mix}}} \\log h_{cls}(g_{\\theta}(x^{Mix}))],\\]\nwhere \\(\\mathbf{p_{\\hat{y}^{Mix}}}\\) is the probability vector of the mixed label \\(\\hat{Y}_{Mix}\\). Since we deploy the copy-and-paste strategy instead of the conventional mixup [75], the mixed labels are still one-hot.\nMulti-grained Contrast in Different Effect Regions. We note that the above-mentioned segmentation loss does not explicitly consider the inherent context within the image, which is crucial to the local-focused segmentation task. Therefore, we study the feasibility of self-supervised learning in mining intra-domain knowledge for domain adaptive semantic segmentation tasks. In this work, we revisit the current pixel-wise contrast in semantic segmentation [24] and explore the joint training mechanism of contrastive learning on both pixel- and patch-level effect regions. To this end, we introduce a unified multi-grained contrast including patch-wise contrast to enhance the consistency within a local patch.\nIn the pixel-wise effect region, given the labels of each pixel \\(y^{s}\\), we regard image pixels of the same class C as positive samples and the rest pixels in \\(x^{s}\\) belonging to the other classes are the negative samples. The pixel-wise contrastive loss can be derived as:\n\\[L_{Pixel} = \\sum_{C(i)=C(j)} log \\frac{r(e_i, e_j)}{\\sum_{k=1}^{N_{pixel}} r(e_i, e_k)},\\]\nwhere \\(e\\) is the feature map extracted by the projection head \\(e = h_{pixel}g_{\\theta}(x)\\), and \\(N_{pixel}\\) is the number of pixels. \\(e_i\\) denotes the i-th feature on the feature map e. r denotes the similarity between the two pixel features. In particular, we deploy the exponential cosine similarity \\(r(e_i,e_j) = \\exp(s(e_i, e_j) / \\tau)\\), where s is cosine similarity between two pixel features \\(e_i\\) and \\(e_j\\), and \\(\\tau\\) is the temperature. As shown in Figure 2, with the guide of pixel-wise contrastive loss, the pixel embeddings of the same class are pulled close and those of the other classes are pushed apart, which promotes intra-class compactness and inter-class separability. In the patch-wise effect region, in particular, given unlabeled target image \\(x\\), we also leverage the network \\(g_{\\theta}\\) to extract the feature map of two partially overlapping patches. The cropped examples are shown at the bottom of Figure 2. We deploy an independent head \\(h_{patch}\\) with 2-layer MLPs to further project the output feature maps to the embedding space for comparison. As shown in Figure 2(b), overlapping region O1 and O2 denote the same green area in the original image. In practice, we first randomly select the region O and then sample two neighbor patches M covering O. We use M to denote the entire patch including O. We argue that the output features of the overlapping region should be invariant to the contexts. Therefore, we encourage that each feature in \\(O_1\\) to be consistent with the corresponding feature of the same location in \\(O_2\\). Similar to pixel-wise contrast, as shown in Figure 2 module (b), we regard two features at the same position of \\(O_1\\) and \\(O_2\\) as positive pair, and any two features in \\(M_1\\) and \\(M_2\\) at different positions of the original image are treated as a negative pair. Given a target-domain input \\(x\\), the patch-wise contrast loss can be formulated as:\n\\[L_{Patch} = \\sum_{O_1(i)=O_2(j)} log \\frac{r(f_i, f_j)}{\\sum_{k=1}^{N_{patch}} r(f_i, f_k)},\\]\nwhere \\(f\\) is the feature map extracted by the projection head \\(f = h_{patch}g_{\\theta}(x)\\), and \\(N_{patch}\\) is the number of pixels in \\(M_1 \\cup M_2\\). i is the pixel index in the patch \\(M_1\\), and j is for \\(M_2\\). \\(O_1(i)\\) denotes the location in the overlapping region \\(O_1\\). \\(O_1(i) = O_2(j)\\) denotes i and j are the same pixel (location) in the original image, as shown in Figure 2(b). \\(f_i\\) denotes i-th feature in the map. Similarly, r denotes the exponential function of the cosine similarity as the one in pixel contrast. It is worth noting that we also enlarge the sample pool. In practice, the rest feature \\(f_k\\) not only comes from the union set \\(M_1 \\cup M_2\\), but also from other training images within the current batch, which is further explained in task-smart sampling part below.\nTemporal contrastive learning. Aggregating temporal information is crucial for dynamic scenes. Existing video-level UDA methods typically learn spatial and temporal information in the source domain and transfer them to the target domain. For instance, Cho et al. [53] utilized moving objects and their temporal information from the source domain, mixing them into the background of the target domain. This allows the model to observe the motion of source domain objects within the target domain's background, thereby learning better spatiotemporal features. This cross-domain mixing strategy helps enhance the model's temporal continuity and segmentation performance in the target domain. However, this method requires task-specific design, limiting its flexibility. Other works enhance temporal continuity in the target domain by utilizing motion information between video frames, but the introduction of optical modules incurs significant computational overhead. To simplify the cumbersome pipelines in domain adaptive video semantic segmentation and achieve a unified framework, we extend PiPa [25] to PiPa++, whose core idea is to extend PiPa's capability of capturing context information through multi-grained contrastive learning. PiPa++ leavrages cross-frame temporal contrast to learn temporal continuity in the target domain. As shown in Figure 3(c), when selecting a key frame \\(f_{key}\\) during training, a reference frame from its temporal neighborhood is also randomly selected, which is denoted as \\(f_{ref}\\). Typically, in two adjacent or closely spaced frames, the temporal information remains consistent, as evidented by [71], [72], [76], [77]. Following similar idea of patch-wise contrastive learning, we crop adjacent frames \\(f_{key}\\) and \\(f_{ref}\\) to two overlapping patches. Then we adopt another lightweight projection head \\(h_{temp}\\) with 2-layer MLPs to project the output feature maps of the two adjacent frames to the embedding space for comparison. The cropped patches are \\(f_{t_{key}}\\) and \\(f_{t_{ref}}\\) after projection. In video scenario, a pixel at \\(f_{t_{ref}}\\) is defined as positive if it is at the same location at \\(f_{t_{key}}\\) and all other pixels inside \\(f_{t_{ref}}\\) are considered negative. To maintain the contextual structure between to frames and hence, realizes temporal continuity, the positive samples are pulled together and negative samples are pushed away:\n\\[L_{Temp} = \\sum_{i=j} log \\frac{r(f_{key_i}, f_{ref_j})}{\\sum_{k=1}^{N_{temp}} r(f_{key_r}, f_{ref_k})},\\]\nwhere \\(f_{key_i}, f_{ref_j}\\) and \\(f_{ref_k}\\) are pixel features extracted by the temporal projection head \\(h_{temp}\\), denoting key pixel sample, reference positive and reference negative samples, respectively. \\(N_{temp}\\) is the number of pixels in \\(f_{t_{key}} \\cup f_{t_{ref}}\\, i is the pixel index in the key frame patch, and j is for the reference frame patch. i = j denotes that i and j are the same pixel (location) in the original image. \\(f_i\\) denotes the i-th feature in the map. Similarly, r denotes the exponential function of the cosine similarity, as used in patch-wise contrast.\nTask-smart Sampling. Prior works has validated that the choice of contrastive sampling significantly enhances the discriminating power of representation space [8], [24], [78]. In our case, informative samples differ in static and dynamic scenes, leading to the formation of a task-smart sampling strategy. In static scenes, to obtain a large number of highly structured pixel training samples, we utilize a semantic-aware memory bank to store not just training samples from a single image, but rich semantic information extracted from the same batch, and even the whole dataset. This design allows us to access more representative data samples obtained from the entire dataset during each training step. For dynamic scenes, the data usually comes in the form of long video sequences, and temporal continuity typically remains consistent only between adjacent frames. Over time, the objects in the scene and their contextual structures usually undergo significant changes. Including such data in our contrastive framework would lead to performance degradation. Therefore, we select positive and negative samples only from a range of 1 to 3 frames. Further experimental validation are provided in the ablation study section.\nTotal Loss. The overall training objective is the combination of pixel-level cross-entropy loss and the proposed PiPa++:\n\\[L_{total} = L_{ce} + L_{ce}^{T} + \\alpha L_{Pixel} + \\beta L_{Patch} + \\gamma L_{Temp},\\]\nwhere \\(\\alpha, \\beta\\), and \\(\\gamma\\) are the weights for pixel-wise contrast \\(L_{Pixel}\\), patch-wise contrast \\(L_{Patch}\\), and temporal contrast \\(L_{Temp}\\) respectively. Temporal contrast \\(L_{Temp}\\) is not applied at static scene.\nDiscussion. 1. Correlation between Pixel and Patch Contrast. Both pixel and patch contrast are derived from instance-level contrastive learning and share a common underlying idea, i.e., contrast, but they work at different effect regions, i.e., pixel-wise and patch-wise. The pixel contrast explores the pixel-to-pixel category correlation over the whole image, while patch-wise contrast imposes regularization on the semantic patches from a local perspective. Therefore, the two kinds of contrast are complementary and can work in a unified way to mine the intra-domain inherent context within the data. 2. What is the advantage of the proposed framework? Traditional UDA methods focus on learning shared inter-domain knowledge. Differently, we are motivated by the objectives of UDA semantic segmentation in a bottom-up manner, and thus leverage rich pixel correlations in the training data to facilitate intra-domain knowledge learning. By explicitly regularizing the feature space via PiPa, we enable the model to explore the inherent intra-domain context in a self-supervised setting, i.e., pixel-wise and patch-wise, without extra parameters or annotations. Therefore, PiPa could be effortlessly incorporated into existing UDA approaches to achieve better results without extra overhead during testing. 3. Difference from conventional contrastive learning. Conventional contrastive learning methods typically tend to perform contrast in the instance or pixel level alone [24], [55], [66]. We formulate pixel- and patch-wise contrast in a similar format but focus on the local effect regions within the images, which is well aligned with the local-focused segmentation task. We show that the proposed local contrast, i.e., pixel- and patch-wise contrasts, regularizes the domain adaptation training and guides the model to shed more light on the intra-domain context. Our experiment also verifies this point that pixel- and patch-wise contrast facilitates smooth edges between different categories and yields a higher accuracy on small objects. 4. Why PiPa++ does not perform temporal contrastive learning on source domain. In general, for static scenes, the contextual information learned from the source domain needs to be passed to the target domain to achieve good performance in the target domain. However, for temporal information, PiPa++ focuses more on learning cross-frame continuity. This is mainly achieved through comparison between key and reference frames. PiPa++ learns temporal information solely within the target domain's video sequences without the need to introduce additional networks such as optical flow. Thus, it achieves a unified network that simultaneously handles both image and video domain adaptation."}, {"title": "A. Implementation Details", "content": "Structure Details. Following recent SOTA UDA setting [38], [64], [73], our network consists of a SegFormer MiT-B5 backbone [38], [79] pretrained on ImageNet-1k [80] and several MLP-based heads, i.e., \\(h_{cls}, h_{pixel}\\) and \\(h_{patch}\\), which contains two fully-connected (fc) layers and ReLU activation between two fc layers. Note that the self-supervised projection heads \\(h_{pixel}\\) and \\(h_{patch}\\) are only applied at training time and are removed during inference, which does not introduce extra computational costs in deployment.\nImplementation details. We train the network with batch size 2 for 60k iterations with a single NVIDIA RTX 6000 GPU. We adopt AdamW [81] as the optimizer, a learning rate of \\(6 \\times 10^{-5}\\), a linear learning rate warmup of 1.5k iterations and the weight decay of 0.01. Following [64], [73], the input image is resized to 1280 \u00d7 720 for GTA and 1280 \u00d7 760 for SYNTHIA, with a random crop size of 640 \u00d7 640. For the patch-wise contrast, we randomly resize the input images by a ratio between 0.5 and 2, and then randomly crop two patches of the size 720 \u00d7 720 from the resized image and ensure the Intersection-over-Union(IoU) value of the two patches between 0.1 and 1. We utilize the same data augmentation e.g., color jitter, Gaussian blur and ClassMix [82] and empirically set pseudo labels threshold 0.968 following [35]. The exponential moving average parameter of the teacher network is 0.999. The hyperparameters of the loss function are chosen empirically \\(\\alpha = \\beta = \\gamma = 0.1\\)."}, {"title": "IV. EXPERIMENTS", "content": "A. Results for Image-baesd UDA\nDatasets. We evaluate the proposed method on image-based UDA benchmarks GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes"}, {"title": "V. CONCLUSION AND FURTHER DISCUSSIONS", "content": "In this paper, we presented PiPa++, a unified framework for unsupervised domain adaptive semantic segmentation that leverages self-supervised learning techniques to bridge the gap between image- and video-level domain adaptation. Our approach integrates pixel-wise, patch-wise, and temporal contrastive learning to capture both spatial and temporal contextual information, enabling robust feature representation and improved segmentation performance across different domains. The extensive experiments conducted on both image-based and video-based UDA benchmarks demonstrate the effectiveness and versatility of PiPa++. For image-based tasks, PiPa++ achieves significant performance gains by constructing an inside-image representation space and extending it to the entire training dataset through a semantic-aware memory bank. For video-based tasks, the proposed task-smart sampling strategy ensures the selection of informative samples, maintaining temporal consistency and enhancing segmentation accuracy. Future work will focus on exploring more advanced contrastive learning techniques and further enhancing the scalability of PiPa++ to handle larger and more diverse datasets. Additionally, we plan to investigate the integration of PiPa++ with other vision tasks, such as object detection and instance segmentation, to develop a more comprehensive and unified framework for visual understanding."}]}