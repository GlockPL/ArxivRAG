{"title": "Revisit Mixture Models for Multi-Agent Simulation: Experimental Study within a Unified Framework", "authors": ["Longzhong Lin", "Xuewu Lin", "Kechun Xu", "Haojian Lu", "Lichao Huang", "Rong Xiong", "Yue Wang"], "abstract": "Simulation plays a crucial role in assessing autonomous driving systems, where the generation of realistic multi-agent behaviors is a key aspect. In multi-agent simulation, the primary challenges include behavioral multimodality and closed-loop distributional shifts. In this study, we revisit mixture models for generating multimodal agent behaviors, which can cover the mainstream methods including continuous mixture models and GPT-like discrete models. Furthermore, we introduce a closed-loop sample generation approach tailored for mixture models to mitigate distributional shifts. Within the unified mixture model (UniMM) framework, we recognize critical configurations from both model and data perspectives. We conduct a systematic examination of various model configurations, including positive component matching, continuous regression, prediction horizon, and the number of components. Moreover, our investigation into the data configuration highlights the pivotal role of closed-loop samples in achieving realistic simulations. To extend the benefits of closed-loop samples across a broader range of mixture models, we further address the shortcut learning and off-policy learning issues. Leveraging insights from our exploration, the distinct variants proposed within the UniMM framework, including discrete, anchor-free, and anchor-based models, all achieve state-of-the-art performance on the WOSAC benchmark.", "sections": [{"title": "I. INTRODUCTION", "content": "Simulation facilitates the assessment of autonomous driving systems in a safe, controllable, and cost-effective manner. One key to narrowing the gap between simulated and real-world environments is the generation of human-like multi-agent behaviors [1]. In recent years, many studies [1]\u2013[5] have employed data-driven approaches to mimic the behavior of human traffic participants. These imitative simulations typically learn a behavior model from real-world driving datasets, which generates future states for each agent based on map information and historical states. Then the behavior model is iteratively run in an autoregressive manner to generate simulated scenarios. The main challenges in achieving realistic simulation involve capturing the multimodality of agent behaviors and addressing distributional shifts in closed-loop rollouts. Multimodality means that an agent may execute one of many underlying possible behaviors. Distributional shifts refer to situations where the behavior model, when unrolled in a closed-loop manner, could encounter states rarely visited during training, thereby resulting in compounding errors.\nRecovering the multimodality of agent behaviors from real-world datasets has been extensively studied in the field of motion prediction [6]\u2013[10]. State-of-the-art methods [11]\u2013[15] predominantly employ mixture models, like Gaussian Mixture Models (GMMs), to represent multimodal future motions. Consequently, a bunch of works [16]\u2013[18] have adopted analogous mixture models as behavior models for multi-agent simulation. These mixture models are usually trained to predict over a relatively long horizon, using a winner-takes-all continuous regression loss and a classification term based on the positive mixture component. For identifying positive components, both anchor-based and anchor-free matching paradigms are commonly utilized. More recently, inspired by the success of GPTs [19], [20], a growing number of studies on multi-agent simulation [21]\u2013[24] have sought to discretize agent trajectories into motion tokens and apply a next-token prediction task, where the behavior model predicts a categorical distribution over a large number of motion modalities. Overall, the GPT-like discrete models have empirically excelled in generating realistic simulations, outperforming continuous mixture models adapted from motion prediction.\nIn fact, we note that GPT-like models with categorical distributions can also be viewed as mixture models, where each mixture component represents a discrete category. The motion tokens, in this context, are analogous to anchors, each linked to a specific mixture component. As mixture models, the leading GPT-like methods [23], [24] are essentially anchor-based models devoid of continuous regression, typically with a large number of components and a short prediction horizon. Therefore, the distinction in model design between the aforementioned discrete and continuous models can be understood as different model configuration choices within the framework of mixture models. Given this perspective, we wonder Q1: Can the differences in model configurations fully account for the performance gap between existing continuous and discrete mixture models?\nIn addition to behavioral multimodality, another major challenge of multi-agent simulation lies in distributional shifts during closed-loop rollouts. In time series modeling, DaD [25] mitigates distributional shifts by leveraging model predictions to modify ground truth inputs in training samples. TrafficSim [1] employs an analogous principle for multi-agent simulation with a CVAE-based behavior model, where ground truth states are autoregressively replaced with posterior predictions from the CVAE, forming closed-loop samples. To discretize ground truth trajectories, GPT-like methods also transform the training samples, via a tokenization process. This motion tokenization often applies a rolling matching strategy to reduce discretization errors across the full sequence, where the continuous states are iteratively substituted by matched motion tokens [21], [23]. The above methods all infuse training samples with model-related features through rollouts. Observing this similarity in data processing, we hypothesize H1: Motion tokenization with rolling matching is equivalent to the generation of closed-loop samples. If the hypothesis holds, both being mixture models, GPT-like discrete methods can be seen as adopting a closed-loop data configuration, while existing continuous models directly utilize the original data. This leads us to ask Q2: Is the data configuration involving closed-loop samples responsible for the performance gap between continuous and discrete mixture models? Moreover, we inquire Q3: How might such a data configuration be extended to benefit general mixture models?\nAs shown in Fig. 1, to investigate the aforementioned hypothesis and questions, we review the general paradigm of mixture models for multi-agent simulation, seeking to discuss methods derived from different inspirations within a unified framework. Subsequently, we highlight several critical model configurations, including positive component matching, continuous regression, prediction horizon, and number of components. Benefiting from the unified framework, we systematically examine mixture models across these diverse configurations. Furthermore, to explore the data configuration involving closed-loop samples, we propose a closed-loop sample generation approach for general mixture models, drawing on the philosophy of DaD [25] and TrafficSim [1]. When it comes to GPT-like models, namely anchor-based models without continuous regression, we confirm H1, showing that the above closed-loop sample generation is equivalent to motion tokenization with rolling matching. We thus identify a commonly overlooked disparity between existing continuous and discrete mixture models: discrete models naturally train on closed-loop samples introduced by tokenization, while their continuous counterparts do not.\nOur experiments refine and extend the findings on mixture models, offering deeper empirical insights into multi-agent simulation. Firstly, in response to Q1, while model configurations are important, they cannot fully explain the performance gap between continuous and discrete mixture models. Adopting a model setting distinct from GPT-like discrete models also has the potential to achieve realistic simulation. Furthermore, we provide an affirmative answer to Q2, revealing that training with closed-loop samples is key to attaining superior simulation performance. Proceeding to Q3, in an effort to enable closed-loop samples to benefit a wide spectrum of mixture models, we identify and address the shortcut learning issue and the off-policy learning problem. Based on the above exploration, we propose several distinct variants within the unified mixture model (UniMM) framework, all achieving state-of-the-art performance on the Waymo Open Sim Agents Challenge (WOSAC) benchmark [26]. In particular, the anchor-free model with 6 components proves competitive, alleviating past concerns that a limited number of components may fail to capture behavioral multimodality [21], [23]. With designs that leverage anchors, the anchor-based continuous model attains efficiency comparable to its discrete counterpart while exhibiting better effectiveness. Therefore, it is still advisable to consider continuous modeling in multi-agent simulation."}, {"title": "II. RELATED WORK", "content": "Simulation provides a safe, controllable, and cost-effective environment for assessing autonomous driving systems. Earlier simulators [27]\u2013[30] utilized heuristic-based policies [31]\u2013[33] to model agent behaviors, which often struggle to capture the complexity of real-world scenarios. Benefiting from the availability of real-world driving datasets [34], [35], learning-based imitative methods [1]\u2013[5] have demonstrated superiority in generating human-like multi-agent simulations. The Waymo Open Sim Agents Challenge (WOSAC) [26], based on the"}, {"title": "III. PROBLEM FORMULATION", "content": "Generally, the generation of multi-agent simulations is factorized into an autoregressive sequential process:\n\\(P(S_{0:T}|C_0) = \\prod_{t\\in \\{0,1,2,...,T-\\tau\\}} p(S_{t:t+\\tau}|S_{0:t}, C_0),\\) (1)\nwhere \\(S_{0:T}\\) represents the simulated scenario, specifically the state sequence of all agents from time 0 to T, with T being the total simulation duration. \\(C_0\\) denotes the initial scenario context, including map information and historical agent states before time 0. \u03c4 represents the update interval of the simulation.\nWith a sufficiently small update interval \u03c4, each agent can be considered to independently take actions, given adequate historical context [21], [49]:\n\\(p(S_{t:t+\\tau}|S_{0:t}, C_0) = \\prod_{n=1}^{N} p(S_{t:t+\\tau}|S_{0:t}, C_0, n),\\) (2)\nwhere \\(S_{t:t+\\tau}\\) represents the state sequence of the n-th agent from time t to t+\u03c4, and N denotes the number of agents in the scenario. In practice, we adopt the widely used setting of \u03c4 = 0.5s [23], [24], which aligns with real-world driving [50]."}, {"title": "B. Mixture Model for Behavioral Multimodality", "content": "To generate realistic multi-agent simulations, data-driven approaches typically learn a behavior model:\n\\(\\pi_{\\theta} (Y|X),\\) where \\( \\begin{cases} X := (S_{0:t}, C_0, n) \\\\ Y := S_{t:t+T_{pred}} \\end{cases}\\) (3)\nThe behavior model \\(\\pi_{\\theta}\\) predicts Y, the state trajectory of the n-th agent over the prediction horizon \\(T_{pred}\\), given X which includes the historical agent states, the initial scenario context, and the agent identifier. For modeling multimodal agent behaviors, mixture models are a suitable choice [15]:\n\\(\\pi_{\\theta} (Y|X) = \\sum_{k=1}^{K} q_{\\theta}(Z = k|X) m_{\\theta}(Y|Z = k, X),\\) (4)"}, {"title": "C. Distributional Shifts in Closed-Loop Simulation", "content": "As shown in Fig. 2, if the behavior model \\(\\pi_{\\theta}\\) is naively trained on open-loop samples, directly derived from splits of ground truth trajectories \\((x = (s_{0:t}, C_0, n), y) \\sim D\\), it could be prone to distributional shifts and suffer from compounding errors in closed-loop simulation [48]. Specifically, when \\(\\pi_{\\theta}\\) is unrolled autoregressively during simulation:\n\\(S_{t:t+T_{pred}} \\sim \\pi_{\\theta} (S_{t:t+T_{pred}}|\\hat{S}_{0:t}, C_0, n),\\) \\( \\hat{S}_{t:t+\\tau} = \\{ \\hat{S}_{t:t+\\tau} \\}_{n=1}^{N},\\) (10)\nthe model may encounter novel input states \\(\\hat{S}_{0:t}\\) induced by its previous suboptimal predictions, which differ from the observed states \\(S_{0:t}\\) during training. As the input states \\(\\hat{S}_{0:t}\\) deviate from the training data, the prediction errors increase, which in turn exacerbates the distributional shift of the subsequent input states \\(\\hat{S}_{0:t+\\tau}\\), leading to compounding errors.\n1) Data Configuration: As proposed in DaD [25], utilizing closed-loop samples \\((x_{cl}, y) \\sim D_{cl}\\) can effectively mitigate distributional shifts:\n\\(\\max_{\\theta} E_{(x_{cl},y) \\sim D_{cl}} [\\log \\pi_{\\theta} (y|x_{cl})],\\)(11)\nwhere the ground truth input x in Eq. 5 is substituted with the closed-loop input \\(x_{cl}\\), generated from the corresponding autoregressive predictions. However, DaD [25] focuses solely on unimodal, single-step prediction models. To extend this principle to mixture models for multimodal and long-horizon behavior modeling, we further investigate the analogous data configuration within the unified mixture model framework in Section V, centering on:\n\u2022 Closed-loop samples \\((x_{cl}, y) \\sim D_{cl}\\) in Eq. 11, derived by transforming open-loop samples \\((x, y) \\sim D\\)."}, {"title": "IV. MODEL CONFIGURATIONS", "content": "For selecting the positive component \\(z^*\\) in Eq. 8, mixture models can be categorized into two primary paradigms: anchor-free and anchor-based matching.\n1) Anchor-Free Matching: As demonstrated in Fig. 3(a), anchor-free methods [12], [18] designate the component \\(z^*\\), whose predicted trajectory \\(\\mu_{z^*}(x;\\theta)\\) is closest to the ground truth y, as positive:\n\\(z^* = \\arg \\min_{k} d(\\mu_k(x;\\theta), y),\\)(12)\nwhere \\(d(\\cdot,\\cdot)\\) computes the distance between two trajectories, and \\(\\mu_k(x;\\theta) := E_{Y\\sim m_{\\theta} (Y|Z=k,X=x)} [Y]\\) represents the predicted trajectory corresponding to component k.\n2) Anchor-Based Matching: In anchor-based models [15], [16], [21], the state anchors \\(\\{A_k(x)\\}_{k=1}^{K}\\) are each associated with a specific component. As illustrated in Fig. 3(b), the positive component is determined as the one corresponding to the anchor closest to ground truth:\n\\(z^* = \\arg \\min_{k} d(A_k(x), y).\\)(13)\n\\(A_k(x)\\) means that anchors can be predefined in spaces like agent-centric trajectories [23], and then transformed into the same state space as the ground truth y using state information in x. Additionally, the distribution of anchors can be influenced by information such as the agent category [11] in x.\n3) Anchor-Free vs. Anchor-Based: According to prior research on motion prediction [14], mixture models employing different matching paradigms exhibit distinct tendencies. Anchor-free models, due to their flexible predictions for each component, make it hard for \\(q_{\\theta}\\) to distinguish the component closest to the ground truth. Thus, their performance relies more heavily on the regression capability of \\(m_{\\theta}\\). In anchor-based models, the introduction of stably distributed anchors significantly alleviates the difficulty for \\(q_{\\theta}\\) in selecting the correct positive component, and these models tend to generate trajectories around the anchors. So their effectiveness is more dependent on the classification performance of \\(q_{\\theta}\\).\nIn this study, we implement both anchor-free and anchor-based models under relatively consistent and fair conditions, enabling a direct demonstration of the characteristic differences between the two matching paradigms in the context of multi-agent simulation."}, {"title": "B. Continuous Regression", "content": "For anchor-based mixture models described in Section IV-A, if the component distribution \\(m_{\\theta}\\) is chosen as:\n\\(m_{\\theta}(Y|Z = k, X = x) = \\mathbb{I}(Y = A_k(x)),\\)(14)\nwhich means the state anchor \\(A_k(x)\\) is directly used as the predicted trajectory for component k. At this point, the component distribution is no longer dependent on trainable parameters \u03b8. Hence, the regression term in the training objective (Eq. 9) is redundant, leaving only the classification loss:\n\\(\\max_{\\theta} E_{(x,y) \\sim D} [-KL[\\hat{q}^*(z)||q_{\\theta}(z|x)]] \\Rightarrow \\min_{\\theta} E_{(x,y) \\sim D} [L_{CE} (\\mathbb{I}(z = z^*),q_{\\theta}(z|x))],\\)(15)\nwhere \\(L_{CE}\\) denotes the cross-entropy loss.\nThe above setting fully aligns with GPT-like models with categorical distributions [21]\u2013[24]. Thus, GPT-like discrete models are essentially anchor-based mixture models, devoid of the trainable component distribution and its corresponding continuous regression. Among previous works, the discrete models often outperform those with continuous regression. However, as shown in Table I, these methods also exhibit notable differences in other configurations.\nIn our study, we evaluate anchor-based models, both with and without continuous regression, under consistent configurations aligned with the leading discrete models, to validate whether incorporating continuous regression provides a performance advantage."}, {"title": "C. Prediction Horizon", "content": "As indicated in previous works [1], [18], [22], training behavior models with a longer prediction horizon, \\(T_{pred}\\) (Eq. 3), may enhance spatio-temporal interaction reasoning, helping agents become more robust to distributional shifts and generate realistic behaviors. However, these studies merely provide a preliminary demonstration of the benefits associated with increasing the prediction horizon.\nIn our experiments, by systematically examining a broad range of \\(T_{pred}\\) under diverse conditions, we observe a more comprehensive trend in the variation of simulation realism as the prediction horizon increases, along with the interactions between the prediction horizon and other configurations."}, {"title": "D. Number of Components", "content": "Intuitively, the number of components, K (Eq. 4), reflects the mixture model's ability to represent complex distributions. Some prior works [18] have also demonstrated the positive impact of using more mixture components. As shown in Table I, discrete models typically have a much larger number of components, which is commonly considered an important factor behind their superior performance [21], [23].\nIn the experiments, we systematically investigate mixture models with varying component numbers. Through the related model architecture design, which will be discussed in Section VI-B, anchor-based mixture models with continuous regression are enabled to scale up the number of components K as efficiently as those employing categorical distributions. It is observed that anchor-based models continue to benefit from the growth of K, whereas the situation differs for anchor-free models. Notably, we find that anchor-free models with 6 components can also achieve highly competitive performance, based on the use of closed-loop data samples to be introduced in Section V."}, {"title": "V. DATA CONFIGURATION", "content": "This section focuses on the utilization of closed-loop samples. We begin by introducing the closed-loop sample generation method proposed for general mixture models (Section V-A). Next, we discuss the potential challenges associated with training on closed-loop samples, including the shortcut learning issue (Section V-B) and the off-policy learning problem (Section V-C). Finally, we propose an approximate posterior policy (Section V-D) to accelerate the closed-loop sample generation for anchor-based models."}, {"title": "Closed-Loop Sample Generation", "content": "Inspired by DaD [25] and TrafficSim [1], we propose a closed-loop sample generation method tailored for mixture models to mitigate distributional shifts. Briefly, given an open-loop sample (x = (so:t, Co, n), y) from the dataset D (Eq. 5), the behavior model \\(\\pi_{\\theta}\\) is unrolled to transform the ground truth input states \\(s_{0:t}\\) into the closed-loop input states \\(\\hat{s}_{0:t}\\), thereby obtaining a closed-loop sample that incorporates the predicted behaviors of \\(\\pi_{\\theta}\\):\n\\((x_{cl} := (\\hat{s}_{0:t}, C_0, n), y) \\sim D_{cl}.\\)(16)\nThe specific closed-loop sample generation process is illustrated in Fig. 4. Starting from the initial ground truth states \\(\\hat{s}_{0:t}\\), a posterior policy \\(\\pi_{post}\\) is autoregressively applied:\n\\(\\mu_{post}(x_h, S_{h:h+T_{post}}; \\theta) := \\mu_{z_{post}}(x;h; \\theta),\\)(17)\nwhere the posterior plan \\(\\mu_{z_{post}}(x;h; \\theta)\\) for time h is the predicted trajectory corresponding to the component \\(z_{post}\\) of \\(\\pi_{\\theta}\\), based on the previously generated closed-loop input states in x = \\((s_{0:n}, C_0,n)\\). Similar to the positive component matching in Section IV-A, the posterior component \\(z_{post}\\) is the one that best matches the ground truth \\(s_{h:h+T_{post}}\\) over the posterior planning horizon \\(T_{post}\\):\n\\(z_{post} \\arg \\min_k \\begin{cases} d_{T_{post}} (\\mu_k(x;h; \\theta), S_{h:h+T_{post}}), \\text{anchor-free} \\\\ d_{T_{post}} (A_k(x_h), S_{h:h+T_{post}}), \\text{anchor-based} \\end{cases}\\)(18)\nHere, the subscript of \\(d_{T_{post}}\\) highlights the distance is computed over the shared time interval \\(T_{post}\\), which may be shorter than \\(T_{pred}\\) in subsequent discussions.\nThe posterior plans of each agent are then executed to generate the subsequent closed-loop states, and the replanning frequency is aligned with the simulation update interval \u03c4:\n\\(\\hat{s}_{h:h+T_{pred}} = \\mu_{post} (x_h, S_{h:h+T_{post}}; \\theta),\\\\ S_{h:h+\\tau} = \\{\\hat{s}_{h:h+\\tau} \\}_{n=1}^{N}\\)(19)\nFollowing Eq. 11, at each optimization step during training, the model \\(\\pi_{\\theta}\\) first generates a batch of closed-loop samples through the above process, and then updates its parameters \u03b8 based on these samples. Such closed-loop samples, while attempting to stay close to the ground truth, introduce the model's generated behaviors into the input \\(x_{cl}\\). As a result, the input states observed during training more closely resemble those encountered by the model in closed-loop simulation.\nWhen the above closed-loop sample generation is applied to GPT-like discrete models, namely anchor-based mixture models without continuous regression, we can discover that:\n\u2022 In GPT-like discrete models [21], [23], agent motion tokenization based on rolling matching is equivalent to closed-loop sample generation for discrete mixture models (H1).\nTherefore, most GPT-like models with categorical distributions actually train on the closed-loop samples naturally introduced by tokenization, as shown in Table I. In contrast, for mixture models with continuous regression [16], [18], although some works mention closed-loop training, they mostly refer to using an RNN-based motion decoder, while the model is in fact trained on open-loop samples. Hence, the use of closed-loop samples is a commonly overlooked disparity between existing continuous and discrete mixture models, which could serve as a significant factor in explaining the performance gap."}, {"title": "Shortcut Learning Issue", "content": "Considering the goal of making input states more consistent between training and closed-loop simulation, the posterior planning horizon \\(T_{post}\\) is by default set equal to the prediction horizon \\(T_{pred}\\). However, when \\(T_{pred}\\) exceeds the update interval \u03c4, meaning that the planning horizon of \\(\\pi_{post}\\) is greater than its replanning interval (\\(T_{post} > \\tau\\)), the generated closed-loop input \\(\\hat{S}_{cl}\\) will incorporate information from the ground truth output y. As shown in Fig. 4(c), the closed-loop states \\(\\hat{s}_{2\\tau:3\\tau}\\) are derived based on \\(\\hat{s}_{2\\tau:2\\tau+T_{post}}\\) that overlap with the ground truth output. This may lead the model \\(\\pi_{\\theta}\\) to learn a shortcut, which could hinder its ability to generate realistic behaviors in closed-loop simulation.\nIn the experiments, we attempt to set the posterior planning horizon equal to the update interval (\\(T_{post} = \\tau\\)) for resolving the potential shortcut learning issue."}, {"title": "Off-Policy Learning Problem", "content": "If one seeks to leverage a longer prediction horizon (\\(T_{pred} > \\tau\\)), the aforementioned alignment between the posterior planning horizon and the update interval (\\(T_{post} = \\tau\\)) will introduce a misalignment between \\(T_{post}\\) and \\(T_{pred}\\). Upon closer inspection, this misalignment is primarily reflected in the fact that the behavior model \\(\\pi_{\\theta}\\) is trained to select the positive component \\(z^*\\) over \\(T_{pred}\\) (Eq. 12 and Eq. 13), while the posterior policy \\(\\pi_{post}\\) selects the posterior component \\(z_{post}\\) over \\(T_{post}\\) (Eq. 18). This is analogous to the off-policy problem in Reinforcement Learning (RL), where a mismatch between the data collection policy and the training policy leads to distributional shifts. Here, the off-policyness is mainly manifested in the disparity between the component selection horizons of \\(\\pi_{\\theta}\\) and \\(\\pi_{post}\\).\nIn fact, the component selection horizon of \\(\\pi_{\\theta}\\), namely the positive matching horizon \\(T_{z^*}\\), can differ from the prediction horizon \\(T_{pred}\\) by extending Eq. 12 and Eq. 13 as follows:\n\\(z^* = \\arg \\min_k \\begin{cases} d_{\\tau_{z^*}} (\\mu_k(\\hat{x}_t; \\theta), y), \\text{anchor-free} \\\\ d_{\\tau_{z^*}} (A_k(\\hat{x}_t), y), \\text{anchor-based} \\end{cases}\\)(20)\nSimilar to Eq. 18, \\(d_{\\tau^*}\\) represents the distance calculated over the first \\(\\tau_z\\) time interval of the trajectories. This indicates that the positive component \\(z^*\\) is selected over the horizon \\(\\tau_{z^*}\\), while \\(\\pi_{\\theta}\\) could still generate trajectories over a longer \\(T_{pred}\\). In our experiments, we utilize the alignment between \\(T_{z^*}\\) and \\(T_{post}\\) to study the off-policy learning problem."}, {"title": "Approximate Posterior Policy", "content": "Except for discrete models, closed-loop sample generation involves iterative inference with the model \\(\\pi_{\\theta}\\), which could take a relatively long time. According to previous work [14], the behavior patterns generated by anchor-based models can be reflected in their anchors. Therefore, we propose an approximate posterior policy for anchor-based models:\n\\(\\pi_{post} (x_h, S_{h:h+T_{post}}; \\theta) \\approx A_{z_{post}} (\\hat{x}_h),\\)(21)\nwhere the anchor \\(A_{z_{post}} (\\hat{x}_h)\\) rather than the predicted trajectory of the component \\(z_{post}\\) is applied. In this way, the generation of closed-loop samples involves only predefined anchors, eliminating the need for \\(\\pi_{\\theta}\\) computation and significantly reducing the required time.\nIn the experiments, we validate the efficiency and effectiveness of the above approximate posterior policy proposed for anchor-based mixture models."}, {"title": "VI. MODEL ARCHITECTURE", "content": "The model architecture employed in our experiments, as demonstrated in Fig. 5, consists of a context encoder and a motion decoder. The context encoder (Section VI-A) can process information from multiple agents across multiple time steps in parallel. The encoded embedding of the n-th agent at time t can be viewed as the extracted features from the corresponding scene context \\((S_{0:t}, C_0, n)\\). This agent embedding is then passed into the motion decoder to generate multimodal future motions for the n-th agent, starting from time t, along with the confidence scores for each component. The motion decoders (Section VI-B) for the anchor-free and anchor-based models are separately designed. Particularly, for anchor-based models with continuous regression, our decoder design enables scaling up the number of components as efficiently as in those without continuous regression."}, {"title": "Context Encoder", "content": "To efficiently process information from multiple agents across multiple time steps simultaneously, we adopt a symmetric scene context encoding based on query-centric attention [13], [15]. Specifically, for each scene element, such as a map polyline or an agent tracklet, the embedding is derived in its local reference frame. When modeling interactions between scene elements, their relative positional encodings are integrated into the corresponding attention operations. Following existing works [18], [23], we apply map self-attention within the map encoder, as well as the factorized attention containing temporal, agent-map and agent-agent attention (Fig. 5), to obtain agent embeddings enriched with diverse spatio-temporal features. During closed-loop simulation, thanks to the symmetric encoding [13], we can reuse previously derived embeddings to incrementally encode newly generated agent motions for faster inference, akin to the KV cache in LLMs [20]."}, {"title": "Motion Decoder", "content": "Since the agent embeddings derived from the symmetric encoder contain spatio-temporal information in their local reference frame, the motion decoder treats each embedding equivalently and outputs trajectories in the corresponding local coordinate system. Without loss of generality, focusing on the processing of an individual agent embedding, we next introduce the decoder designs for anchor-free and anchor-based models respectively."}, {"title": "Anchor-Free Model", "content": "For anchor-free models, similar to previous methods [18], each learnable query is linked to a specific component. Every component query is fused with the agent embedding, which is then used to generate the corresponding trajectory along with its confidence score, as depicted in Fig. 5. In the above process, the computational overhead of each part increases at least linearly with the number of components. Besides, most existing anchor-based models with continuous regression [15] adopt a decoder structure akin to anchor-free models, with the main difference being that component queries are generated using anchors. It thus seems challenging for general continuous mixture models to scale up the number of components, as noted by methods employing discrete distributions [23]. In fact, for anchor-based models, the above situation has the potential to change, as will be discussed next."}, {"title": "Anchor-Based Model", "content": "For anchor-based models, we first generate anchor trajectories for each agent category by clustering the training data (Fig. 6), as done in previous works [16], [21]. In this context, for a specific agent category, the anchor corresponding to each component index is actually well-defined. Additionally, the confidence scores in anchor-based models assess the congruence between the ground truth and anchors, rather than predicted trajectories. Therefore, we can directly use the agent embedding to predict a categorical distribution that represents the scores for each anchor, as shown in Fig. 5. When employing continuous regression, we only need to select a single anchor, either the one associated with the positive component or one sampled based on the scores, and generate its corresponding trajectory. If continuous regression is not applied, the model is equivalent to one that uses a discrete distribution [23]. While preserving the features inherent to anchor-based models, the above design markedly mitigates the increase in decoder computational cost as the number of components grows. Such a design also unifies continuous and discrete anchor-based models, thereby facilitating the investigation of continuous regression."}, {"title": "VII. EXPERIMENTS", "content": "In the experiments, we begin by systematically examining the effects of varying prediction horizons (Section VII-B1) and component numbers (Section VII-B2) for both anchor-free and anchor-based models. The models in the above experiments (Section VII-B) are all trained on the original open-loop samples to ensure consistency in the training data. Next, we select the optimal prediction horizons and component numbers for the anchor-free and anchor-based models respectively, to explore training with closed-loop samples (Section VII-C). Specifically, we investigate the shortcut learning issue (Section VII-C1) and the off-policy learning problem (Section VII-C2). Additionally, for anchor-based models, we validate the effectiveness of the proposed approximate posterior policy (Section VII-D) and examine the performance gain introduced by continuous regression (Section VII-C4). From the above exploratory experiments, we select the best configurations that include the anchor-free model as well as anchor-based models with and without continuous regression, evaluating their performance on the WOSAC [26] benchmark (Section VII-D)."}, {"title": "Experimental Setup", "content": "We perform experiments on the large-scale Waymo Open Motion Dataset (WOMD) [35], which includes 487k training, 44k validation, and 44k testing scenes. Each scene contains an HD map and up to 128 agents, spans 9 seconds, and is sampled at a frequency of 10 Hz, with 1 second of historical data and 8 seconds of future trajectories. For efficiently conducting the exploratory experiments in Section VII-B and Section VII-C, we train models on 20% of the scenes uniformly sampled from the training set, which is empirically found to exhibit a similar distribution to the full training set [15]. In Section VII-D, the models evaluated on the benchmark are trained on the full WOMD training set."}, {"title": "Metrics", "content": "For multi-agent simulation, we adopt the evaluation metrics from the Waymo Open Sim Agents Challenge (WOSAC) [26", "35": "including minADE (Minimum Average Displacement Error), minFDE (Minimum Final Displacement Error), MR (Miss Rate), and mAP (Mean Average Precision). The motion prediction metrics are computed on multiple 8-second future trajectories of interested agents, given 1 second of history. We apply the official evaluation tool locally to calculate the metrics on the full WOMD validation"}]}