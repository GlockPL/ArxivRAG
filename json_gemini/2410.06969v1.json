{"title": "DLGNet: Hyperedge Classification through Directed Line Graphs for Chemical Reactions", "authors": ["Stefano Fiorini", "Stefano Coniglio", "Pietro Morerio", "Giulia M. Bovolenta", "Michele Ciavotta", "Michele Parrinello", "Alessio Del Bue"], "abstract": "Graphs and hypergraphs provide powerful abstractions for modeling interactions among a set of entities of interest and have been attracting a growing interest in the literature thanks to many successful applications in several fields. In particular, they are rapidly expanding in domains such as chemistry and biology, especially in the areas of drug discovery and molecule generation. One of the areas witnessing the fasted growth is the chemical reactions field, where chemical reactions can be naturally encoded as directed hyperedges of a hypergraph. In this paper, we address the chemical reaction classification problem by introducing the notion of a Directed Line Graph (DLG) associated with a given directed hypergraph. On top of it, we build the Directed Line Graph Network (DLGNet), the first spectral-based Graph Neural Network (GNN) expressly designed to operate on a hypergraph via its DLG transformation. The foundation of DLGNet is a novel Hermitian matrix, the Directed Line Graph Laplacian Ln, which compactly encodes the directionality of the interactions taking place within the directed hyperedges of the hypergraph thanks to the DLG representation. Ln enjoys many desirable properties, including admitting an eigenvalue decomposition and being positive semidefinite, which make it well-suited for being adopted within a spectral-based GNN. Through extensive experiments on chemical reaction datasets, we show that DLGNet significantly outperforms the existing approaches, achieving on a collection of real-world datasets an average relative-percentage-difference improvement of 33.01%, with a maximum improvement of 37.71%.", "sections": [{"title": "1 Introduction", "content": "In recent years, ground-breaking research in the graph-learning literature has been prompted by seminal works on GNNs such as Scarselli et al. [2009], Micheli [2009], Li et al. [2016], Kipf and Welling [2017], Veli\u010dkovi\u0107 et al. [2018]. However, representing data solely through graphs, either undirected or directed, can be limiting in many real-world applications where more complex relationships exist.\nIn such cases, generalizations of graphs known as hypergraphs, which allow for higher-order (group) relationships among the vertices, have emerged as powerful alternatives. Hypergraphs extend the traditional concept of a graph by allowing hyperedges to connect an arbitrary number of nodes, thereby capturing both pairwise (dyadic) and group-wise (polyadic) interactions [Schaub et al., 2021]. This has naturally led to a new stream of research devoted to the investigation of Hypergraph Neural Networks (HNNs) [Feng et al., 2019, Chien et al., 2021, Huang and Yang, 2021, Wang et al., 2023a,b].\nAmong many successful applications, graph and hypergraph representations have recently been applied in chemistry and biology to address various tasks such as drug discovery [Bongini et al., 2021], molecule generation [Hoogeboom et al., 2022], and protein interaction modeling [Jha et al., 2022]. Several graph-based representations have also been developed and employed for the study of chemical reactions, which has applications in areas such as reaction engineering, retrosynthetic pathway design, and reaction feasibility evaluations. In particular, retrosynthetic modeling, where a synthetic route is designed starting from the desired product and analyzed backward, benefits greatly from accurate reaction type identification. This capability enables the elimination of unfeasible pathways, thereby streamlining the discovery of efficient routes for chemical production. This is particularly important in industries such as pharmaceutical and material sciences, where optimizing synthetic routes can lead to significant cost savings and innovation. A similar situation holds, in reaction feasibility analysis, where predicting the likelihood of a reaction's success based on the molecular inputs is essential for designing scalable and efficient processes.\nOne of the most relevant techniques to model reactions is the directed graph [Fialkowski et al., 2005], where molecules are represented as nodes and the chemical reactions are represented as directed edges from reactants to products. Despite its popularity, such a directed graph model suffers from some key limitations. In particular, modeling each reaction as a collection of individual directed edges between each reactant-product pair fails to fully capture the complexity of multi-reactant or multi-product reactions, which are key to many important applications [Restrepo, 2022, Garcia-Chung et al., 2023]. To mitigate this issue, Chang [2024] proposed a hypergraph representation in which molecules are nodes and each reaction is captured by a hyperedge. However, this model lacks a mechanism to represent the directionality of a reaction, thus failing to capture the reactant/product relationship within it. As a further attempt, Restrepo [2023] introduced a directed hypergraph representation which is able to model both the chemical reactions structure and their directionality, where directed hyperedges model the directional interactions between reagents (heads) and products (tails), better capturing the full complexity of chemical reactions. Let us note that this literature only focuses on modeling reaction structures without considering any form of hypergraph learning methods. We set ourselves out to developing one in this paper.\nIn contrast to prior studies that address node classification or link prediction tasks [Dong et al., 2020, Wang et al., 2023b, Zhao et al., 2024], in this work we tackle the reaction classification problem (i.e., the problem of predicting the reaction type of a given set of reactants and products) as a hyperedge classification task."}, {"title": "Main Contributions of This Work", "content": "\u2022 We introduce the first formal definition of a directed line graph associated with a directed hypergraph H: the Directed Line Graph DLG(H).\n\u2022 We propose the Directed Line Graph Laplacian Ln, a Hermitian matrix that captures both directed and undirected relationships between the hyperedges of a directed hypergraph via its DLG. We also prove that LN possesses many desirable spectral properties.\n\u2022 We introduce DLGNet, the first spectral-based Graph Neural Network specifically designed to operate on directed line graphs associated with directed hypergraphs by directly con-volving hyperedge features rather than node features.\n\u2022 We perform an extensive experimental evaluation on the chemical reaction-classification task on real-world datasets. Our results highlight the advantages of our approach compared to other methods presented in the literature."}, {"title": "2 Background", "content": "An undirected hypergraph is defined as an ordered pair H = (V, E), with n := |V| and m := |E|, where V is the set of vertices (or nodes) and E C 2V \\ {} is the (nonempty) set of hyperedges. The weights of the hyperedges are stored in the diagonal matrix W \u2208 Rm\u00d7m, where we is the weight of hyperedge e \u2208 E (in the unweighted case we have W = I). The vertex degree du and hyperedge degree de are defined as du := \u2211e\u2208E:u\u2208e |We| for u \u2208 V, and de := |e| for e \u2208 E. These degrees are stored in two diagonal matrices Dv \u2208 Rn\u00d7n and De \u2208 Rm\u00d7m. In the case of 2-uniform hypergraphs, the matrix A \u2208 Rn\u00d7n is defined such that Auv = we for each e = {u, v} \u2208 E and Auv = 0 otherwise; we refer to it as the adjacency matrix of the graph. Hypergraphs where d(e) = k for some k \u2208 N for all e \u2208 E are called k-uniform. Graphs are 2-uniform hypergraphs. Following Gallo et al. [1993], we define a directed hypergraph H as a hypergraph where each hyperedge e \u2208 E is partitioned in a head set H(e) and a tail set T(e). If T(e) is empty, e is an undirected hyperedge.\nThe relationship between vertices and hyperedges in a undirected hypergraph H is classically represented via an incidence matrix B of size |V| \u00d7 |E|, where B is defined as:\n$B_{ve} = \\begin{cases} 1 & \\text{if } v \\in e \\\\ 0 & \\text{otherwise} \\end{cases} \\quad v \\in V, e \\in E.$\nFrom the incidence matrix B, one can derive the Signless Laplacian Matrix Q as well as its normalized version QN [Chung and Graham, 1997]:\n$Q := BWB^T$   $Q_N := D_V^{-1/2} B \\,\\, W \\,\\, D_e^{-1} \\,\\, B^T \\,\\, D_V^{-1/2}$"}, {"title": "2.1 Graph Fourier and Graph Convolutions", "content": "Let L be a generic Laplacian matrix of a given 2-uniform hypergraph H. We assume that L admits an eigenvalue decomposition L = UAU*, where U \u2208 Cn\u00d7n represents (in its columns) the eigenvectors, U* is its conjugate transpose, and A \u2208 Rn\u00d7n is the diagonal matrix containing the eigenvalues. Let x \u2208 Cn be a graph signal, i.e., a complex-valued function x : V \u2192 Cn of the vertices of H. We define x = F(x) = U*x as the graph Fourier transform of x and F-1(x) = U\u00ee its inverse transform. The convolution y x between x and another graph signal y \u2208 Cn, acting as a filter, in the vertex space is defined in the frequency space as y\u00ae x = Udiag(U*y)U*x. Letting \u0176 := U\u011cU* with G := diag(U*y), we can write y \u00ae x as the linear operator \u0176x. See Shuman et al. [2013] for more details.\nIn the context of GNNs, explicitly learning y as a non-parametric filter presents two significant limitations. Firstly, computing the eigenvalue decomposition of L can be computationally too expensive [Kipf and Welling, 2017]. Secondly, explicitly learning y requires a number of parameters proportional to the input size, which becomes inefficient for high-dimensional tasks [Defferrard et al., 2016]. To address these issues, the GNN literature commonly employs filters where the graph Fourier transform is approximated as a degree-K polynomial of A, with K kept small for computational efficiency. For further details, we refer the reader to Kipf and Welling [2017], Defferrard et al. [2016], Huang et al. [2024]. This leads to a so-called localized filter, thanks to which the output (i.e., filtered) signal at a vertex u \u2208 V is a linear combination of the input signals within K edges of u [Shuman et al., 2013]. By employing various polynomial filters and setting K = 1 (as commonly employed in the literature), such as Chebyshev polynomials as in Hammond et al. [2011], Kipf and Welling [2017] or power monomials as used by Singh and Chen [2022], one obtains a parametric family of linear operators with two learnable parameters, 00 and 01:\n$\\hat{Y} := \\theta_0 I + \\theta_1 L.$"}, {"title": "3 The Directed Line Graph and Its Laplacian", "content": "The line graph L(H) of a generic undirected hypergraph H is classically defined as the undirected graph whose vertex set is the hyperedge set of H. In L(H), two vertices i, j are adjacent i.e., L(H) contains the edge {i, j}\u2014if and only if their corresponding hyperedges i, j have a nonempty intersection [Tyshkevich and Zverovich, 1998]. By construction, L(H) is a 2-uniform graph. Its adjacency matrix is defined as:\n$A(L(H)) := Q - W D_e,$\nwhere Q := BTB is, by construction, the Signless Laplacian of L(H). 2 The normalized version of Q and the corresponding normalized Laplacian are defined as:\n$\\tilde{Q}:= W^{\\frac{1}{2}} B B^T W^{\\frac{1}{2}}$   $Q_N := D^{\\frac{-1}{2}} W^{\\frac{1}{2}} B^T D_B^{-1} B W^{\\frac{1}{2}} D^{\\frac{-1}{2}}$  $L_N := I - Q_N.$\nNotice that, from equation 2, one can define the weighted version of B as BW \u00bd. The definitions in equation 6 rely on the same matrix, but transposed.\nTo the best of our knowledge, the literature does not offer any formal definition for the line graph associated with a (weighted) directed hypergraph H (it does only for the undirected case). The availability of such a definition could be crucial for tasks where the hyperedge direction is important."}, {"title": "Definition 1. The Directed Line Graph DLG(H)", "content": "of a directed hypergraph H is a 2-uniform hypergraph whose vertex set corresponds to the hyperedge set of H and whose adjacency matrix is the following complex-valued skew-symmetric matrix:\n$A(DLG(H)) = W^{\\frac{1}{2}} B^* B W^{\\frac{1}{2}} - W D_e.$\nUsing equation 8 of definition 1 and equations 5\u20136, we obtain the following formulas for the normalized Signless Laplacian QN and the normalized Laplacian LN of DLG, which we refer to by Signless Directed Line-Graph and Directed Line Graph Laplacian:\n$Q_N := D^{\\frac{-1}{2}} W^{\\frac{1}{2}} B^* D_B^{-1} B W^{\\frac{1}{2}} D^{\\frac{-1}{2}}$\n $L_N := I - Q_N.$\nTo better understand how \u013d\u0147 encodes the directionality of H, we illustrate its definition in scalar form for a pair of hyperedges i, j \u2208 E (which correspond to vertices in DLG(H)):\n$\nL_N(i,j) = \\begin{cases}\n    1 - \\frac{\\sum_{u \\in i} \\delta_u}{\\sqrt{\\delta_i \\delta_j}} w_i & i = j \\\\\n    -\\sqrt{w_i w_j} \\bigg( \\sum_{u \\in H(i) \\cap H(j)} \\frac{1}{\\delta_u} +  \\sum_{u \\in T(i) \\cap T(j)} \\frac{1}{\\delta_u}  - i \\sum_{u \\in H(i) \\cap T(j)} \\frac{1}{\\delta_u} + i \\sum_{u \\in T(i) \\cap H(j)} \\frac{1}{\\delta_u}  \\bigg) & i \\neq j\n\\end{cases}\n$\nWhen i = j, we are in the self-loop part of the equation and Ln(ij) weights hyperedge i proportionally to its weight w\u2081 and inversely proportionally to its density and the density of its nodes. When i \u2260 j, Ln(ij)'s value depends on the interactions between the hyperedges of H (which correspond to the nodes of DGL(H)). Let u \u2208 V be a node and i, j \u2208 E be two hyperedges in the hypergraph H. If u belongs to the head set of both the hyperedges (i.e., u \u2208 H(i) \u2229 H(j)) or to the tail set of both (i.e., u \u2208 T(i) \u2229T(j)), its contribution to the real part of Lv (ij), R(\u013dn (ij)), is negative. For the undirected line graph associated with an undirected hypergraph, this is the only contribution, consistent with the behavior of LN (as described in equation 6). If u takes opposite roles in hyperedges i and j, i.e, it belongs to the head set in i and to the tail set in j or vice versa, it contributes to the imaginary part of LN, I(\u013d\u0147 (ij), negatively when u \u2208 H(i)\u2229T(j), and positively when u \u2208 T(i) \u2229 H(j). Consequently I(Ln(ij)) coincides with the net contribution of all the vertices that are shared between the hyperedges i and j. An example illustrating the construction of LN for a directed line graph associated with a directed hypergraph is provided in Appendix F. Let us point out that the behavior of Directed Line Graph Laplacian differs from every (to the best of our knowledge) Laplacian matrix previously proposed in literature (see Appendix A for more details).\nWith the following theorem, we show that Ly is a generalization of LN (defined in equation 6) from the undirected to the directed case:\nTheorem 1. If H is undirected (i.e., H = H), L\u2116 = LN and QN = Qn holds."}, {"title": "4 The Directed Line Graph Network (DLGNet)", "content": "The properties of the proposed Laplacian make it possible to derive a well-defined spectral convolution operator from it. In this work, this operator is integrated into the Directed Line Graph Network (DLGNet). Specifically, based on equation 4, by setting L = Ly, the convolution operator is defined as \u0176x = 001 + 0\u2081LN. The advantage of adopting two parameters 00, 01 within DLGNet's localized filter is explained by the following result:\nThis shows that DLGNet, by selecting appropriate values for 00 and 01, can leverage either \u013d\u0147 or QN as convolution operator to maximize the performance on the task at hand.\nWe define X \u2208 Cmxco as a co-dimensional graph signal (a graph signal with co input channels), which we compactly represent as a matrix. This matrix serves as the feature matrix of the hyperedges of H which we construct from the feature matrix of the nodes X' \u2208 Cnxco of H. Specifically, inspired by the operation used in the reduction component for graph pooling [Grattarola et al., 2022], we define the feature matrix for the vertices of DGL(H) as X = B*X'. This approach combines features through summation, based on the topology defined by B. See Appendix D for more details.\nIn our network, the scalar parameters 00 and 0\u2081 are subsumed by two operators \u03980, \u04e8\u2081 \u2208 CCOXC which we use to carry out a linear transformation on the feature matrix X. A similar transformation, which can also increase or decrease the number of channels of X, is adopted in other GNNs such as MagNet [Zhang et al., 2021]. DLGNet features l convolutional layers. The output Z \u2208 Cmxc' of any such layer adheres to the following equation:\n$Z(X) = \\phi \\left( IX \\Theta_0 + L_N X \\Theta_1 \\right),$\nwhere \u03d5 is the activation function. Following [Fiorini et al., 2023, 2024], DLGNet employs a complex ReLU where \u03d5(z) = z if R(z) > 0 and \u03d5(z) = 0 otherwise, with z \u2208 C. DLGNet also utilizes a"}, {"title": "5 Experimental results", "content": "We present three real-world datasets, the baseline models, and the results on the chemical reaction classification task, where we predict the reaction type based on a given set of molecules."}, {"title": "5.1 Datasets", "content": "We test DLGNet on the most common organic chemistry reaction classes, namely a variety of chemical transformations that are fundamental to both research and industrial chemistry. Those include molecular rearrangements, such as the interconversion (substitution) or the elimination of molecular substituents, as well as the introduction of specific functional groups (e.g., acyl, alkyl, or aryl groups) in a chemical compound. Other important reactions classes involve the formation of certain bond-types (e.g., carbon-carbon: C\u2013C) or structures (e.g., heterocyclic compounds), the change in the oxidation state of a molecular species (oxidation-reduction processes), and the protection/deprotection of functional groups, allowing to temporarily block a specific reactive site at a certain step in a synthetic route. For our study, we rely on a standard dataset (Dataset-1) and additionally construct two new ones (Dataset-2 and Dataset-3):-see Figure 2."}, {"title": "5.2 Baselines and Experimental Details", "content": "We evaluate the performance of DLGNet against 12 state-of-the-art (baseline) methods: i) Undirected Hypergraph Neural Networks (HNNs): HGNN [Feng et al., 2019], HCHA\u00b3 [Bai et al., 2021], HCHA with the attention mechanism [Bai et al., 2021], HNHN [Dong et al., 2020], UniGCNII [Huang and Yang, 2021], HyperDN [Tudisco et al., 2021], AllDeepSets [Chien et al., 2021], AllSetTrans-former [Chien et al., 2021], LEGCN Yang et al. [2022], ED-HNN [Wang et al., 2023a], and Phe-nomNN [Wang et al., 2023b]; ii) Directed HNN: DHM [Zhao et al., 2024]. Since all the competitors operate directly on the undirected or directed hypergraph, we apply the feature transfer operation X = B*X' described in Section 3 (more details in Appendix D) after the convolutional layers. After this step, each method is equipped with l linear layers. The hyperparameters of these baselines and of our proposed model are selected via grid search (see Appendix D). The datasets are split into 50% for training, 25% for validation, and 25% for testing. The experiments are conducted with"}, {"title": "5.3 Results", "content": "Quantitative. The F1-score along with the relative standard deviation across different methods, datasets, and folds is presented in Table 1. The results show that, across the three datasets, DLGNet achieves an average additive performance improvement over the best-performing competitor of approximately 23.51 percentage points. In terms of Relative Percentage Difference (RPD)4, we have an average RPD improvement of 33.01%. DLGNet achieves the best improvement on Dataset-3, with an average RPD improvement of approximately 37.71% and an average additive improvement of 31.65 percentage points. A clear trend emerges: HNNs-based methods designed for undirected hypergraphs consistently underperform compared to DHM, which is the only method specifically designed for handling directed hypergraphs. Crucially, our proposed DLGNet, which operates on the directed line graph, surpasses all the competitors in performance, incuding DHM.\nQualitative. To gain deeper insights into the capability of DLGNet of classifying different reaction types, we analyze the confusion matrices for Dataset-1 and Dataset-2. The results of this analysis are presented in Figure 4 and Figure 5 in Appendix E. The confusion matrix for Dataset-1 reveals that, while most of the classes are predicted extremely well, e.g., Protection and Functional group addition reactions (accuracy of 88% and 77%, respectively), some are predicted not as well, e.g., Functional group interconversion (41%). To better understand this behavior, we conducted a thorough inspection of the structural features of Dataset-1's components, selecting several elements from pairs of classes among which the model yields the highest uncertainty. Two example cases are reported in Figure 3. Overall, our analysis reveals that the pair of classes which are subject to the higher degree of confusion are, structurally, highly similar, which well explains the poorer performance that DLGNet achieves on them, as we illustrate in the following. The left panel illustrates the mislabeling of Class 9 (Functional group interconversions, correctly predicted in 41% of the cases) with Class 7 (Reductions, incorrectly predicted in 14% of the cases), while the right panel presents an example of Class 4 (Heterocycle formations, correctly predicted in 44% of the cases) with Class 1 (Arylations, incorrectly predicted 30% of the cases). Notably, in these examples, both the main backbone structure of the molecules and the substituent groups (the segments affected by the reactive process, highlighted in the figure) exhibit a high degree of similarity between the two classes. In the left panel, the reactants of both classes present a 6-carbon ring (in grey) as well as a iodine substituent (in purple). The atoms composing the highlighted groups are also of the same types. On the other hand, in the right panel, the majority of the constituent parts of the products are in common between the two classes. Specifically, despite the outcome of Class 4 is the formation of a heterocycle, i.e., a hexagonal ring containing a heteroatom (nitrogen, in blue), such a geometrical feature is also present in Class 1"}, {"title": "6 Conclusions", "content": "We introduced the Directed Line Graph Network (DLGNet), the first spectral GNN specifically de-signed to operate on directed line graphs associated with directed hypergraphs by directly convolving hyperedge features. DLGNet leverages a novel complex-valued Laplacian matrix, the Directed Line"}, {"title": "A Properties of Our Proposed Laplacian", "content": "This section contains the proofs of the theorems and corollaries reported in the main paper.\nTheorem 1. If H is undirected (i.e., H = H), L\u2116 = LN and QN = QNn holds.\nProof. Since H = (V, E) is an undirected hypergraph, B is binary and only takes values 0 and 1 (rather than being ternary and taking values 0, 1, -i), defining an undirected line graph L(H). In particular, for each edge e \u2208 E we have Bue = 1 if either u \u2208 H(e) or u \u2208 T(e) and Bue = 0 otherwise. Consequently, the directed incident matrix B is identical to the non-directed incidence matrix B, i.e., B = B. Thus, by construction, LN = Ly and QN = QN \u2022\nTheorem 2. Letting 1 be the indicator function, the Euclidean norm induced by \u013d\u0147 of a complex-valued signal x = a + ib \u2208 Cn reads:\n$\\frac{1}{2} \\sum_{i,j \\in E} \\sum_{u \\in V} \\frac{\\delta(u)}{\\sqrt{\\delta(i) \\delta(j)}} w(i) w(j) \\Biggl(  \\Biggl( \\Biggl( \\frac{a_i}{\\delta(i)} - \\frac{a_j}{\\delta(j)}  \\Biggr) ^2 + \\Biggl( \\frac{b_i}{\\delta(i)} - \\frac{b_j}{\\delta(j)}  \\Biggr) ^2 \\Biggr) 1_{u \\in H(i) \\cap H(j) \\cup T(i) \\cap T(j)} \\\\ +   \\Biggl( \\Biggl( \\frac{a_i}{\\delta(i)} + \\frac{b_j}{\\delta(j)}  \\Biggr) ^2 + \\Biggl( \\frac{a_j}{\\delta(j)} - \\frac{b_i}{\\delta(i)}  \\Biggr) ^2 \\Biggr) 1_{u \\in H(i) \\cap T(j)} \\\\ + \\Biggl( \\Biggl( \\frac{a_j}{\\delta(j)} + \\frac{b_i}{\\delta(i)}  \\Biggr) ^2 + \\Biggl( \\frac{a_i}{\\delta(i)} - \\frac{b_j}{\\delta(j)}  \\Biggr) ^2 \\Biggr) 1_{u \\in T(i) \\cap H(j)} \\Biggr) w(j).$\nProof.\n$\\sum_i x_i^* L_N x_i = \\sum_i x_i x_i - \\sum_i  \\sum_{u \\in i} \\sum_j \\frac{1}{\\sqrt{\\delta_i \\delta_j}}  w(i)^{\\frac{1}{2}} B(u, i)^* B(u, j) w(j)^{\\frac{1}{2}} x_i^* x_j = \\\\\\  \\sum_i  w(i)  - \\sum_{i,j}  \\sum_{u \\in V} \\frac{1}{\\sqrt{\\delta_i \\delta_j}} w(i)^{\\frac{1}{2}} B(u, i)^* B(u, j) w(j)^{\\frac{1}{2}} x_i^* x_j = \\\\\\   \\sum_{u \\in V}  \\frac{1}{d(u)} \\Biggl(  \\sum_{i,j \\in E} w(i)^{\\frac{1}{2}}  B(u, i)^* B(u, j) w(j)^{\\frac{1}{2}} x_i^* x_j \\Biggr)= \\\\\\  \\sum_{u \\in V}  \\frac{1}{d(u)}  \\Biggl(  \\sum_{i \\neq j}  w(i)^{\\frac{1}{2}}  B(u, i)^* B(u, j) w(j)^{\\frac{1}{2}}  \\frac{x_i^* x_i}{\\sqrt{\\delta(i)  \\delta(j)}} + B(u, j)^* B(u, i) \\frac{x_j x_i}{\\sqrt{\\delta(j) \\delta(i)}} + B(u, i)^* B(u, j) \\frac{x_i^* x_j}{\\sqrt{\\delta(i)  \\delta(j)}} + B(u, j)^* B(u, i) \\frac{x_j x_i}{\\sqrt{\\delta(j) \\delta(i)}} \\Biggr) w(j).$\nWe proceed by analyzing the three possible cases for the summand.\nB Properties of DLGNet\nThese operations are used in the equation of the convolution operator and the equations are analyzed."}, {"title": "B Complexity of DLGNet", "content": "The detailed calculations for the (inference) complexity of DLGNet are as follows.\n1. The Directed Line Graph Laplacian Ly is constructed in time O(m\u00b2n), where the factor n is due to the need for computing the product between two columns of B (i.e., two rows of B*) to calculate each entry of \u013d\u0147. After L\u0145 has been computed, the convolution matrix \u0176 \u2208 Cm\u00d7m is constructed in time O(m\u00b2). Note that such a construction is carried out entirely in pre-processing and is not required at inference time.\n2. Constructing the feature matrix X = B*X' requires O(mnco) elementary operations.\n3. Each of the l convolutional layers of DLGNet requires O(m\u00b2c + mc\u00b2 + mc) = O(m\u00b2c + mc\u00b2) elementary operations across 3 steps. Let Xl-1 be the input matrix to layer l = 1,..., l. The operations that are carried out are the following ones.\n(a) LN is multiplied by the hyperedge-feature matrix Xl\u22121 \u2208 Cm\u00d7c, obtaining Pl\u00b9 \u2208 Cmxc in time O(m\u00b2c) (we assume, for simplicity, that matrix multiplications takes cubic time);\n(b) The matrices Plo = IXl\u22121 = Xl\u22121 and Pl1 are multiplied by the weight matrices \u0398\u03bf, \u0398\u2081 \u2208 Rcxc (respectively), obtaining the intermediate matrices Pl01, Pl11 \u2208 Cnxc in time O(mc\u00b2).\n(c) The matrices Pl01 and Pl11 are additioned in time O(mc) to obtain Pl2.\n(d) The activation function & is applied component wise to Pl2 in time O(mc), resulting in the output matrix X\u00b9 \u2208 Cm\u00d7c of the l-th convolutional layer.\n4. The unwind operator transforms Xl (the output of the last convolutional layer l) into the matrix U\u00ba \u2208 Rn\u00d72c in linear time O(mc).\n5. Call Us-1 the input matrix to each linear layer of index s = 1,..., S. The application of the s-th linear layer to Us\u22121 \u2208 Cm\u00d7c' requires multiplying Us\u22121 by a weight matrix Ms \u2208 Ccxc' (where c' is the number of channels from which and into which the feature vector of each node is projected). This is done in time O(mc\u00b2).\n6. In the last linear layer of index S, the input matrix US\u22121 \u2208 Rm\u00d7c' is projected into the output matrix O \u2208 Rm\u00d7d in time O(nc'd).\n7. The application of the Softmax activation function takes linear time O(md).\nWe deduce an overall complexity of O(mnco)+O(l(m\u00b2c+mc\u00b2)+mc+(S-1)(mc'\u00b2)+mc'd+md). Assuming O(c) = O(c') = O(d) = \u0113, such a complexity coincides with O(l(m\u00b2c) + (l+S)(mc\u00b2))."}, {"title": "F From a Directed Hypergraph to the Directed Line Graph Laplacian", "content": "To illustrate the construction of the directed line graph and the associated Directed Line Graph Laplacian, consider a directed hypergraph H = (V, E) where the vertex set is V = {a, b, c, d, e} and the hyperedge set is E = {e1, 62, 63}. The incidence relationships are defined as follows:\n\u2022 H(e\u2081) = {b,c}, T(e\u2081) = {a},"}]}