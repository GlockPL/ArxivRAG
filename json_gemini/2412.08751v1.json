{"title": "Sampling-based Continuous Optimization with Coupled Variables for RNA Design", "authors": ["Wei Yu Tang", "Ning Dai", "Tianshuo Zhou", "David H. Mathews", "Liang Huang"], "abstract": "The task of RNA design given a target structure aims to find a sequence that can fold into that structure. It is a computationally hard problem where some version(s) have been proven to be NP-hard. As a result, heuristic methods such as local search have been popular for this task, but by only exploring a fixed number of candidates. They can not keep up with the exponential growth of the design space, and often perform poorly on longer and harder-to-design structures. We instead formulate these discrete problems as continuous optimization, which starts with a distribution over all possible candidate sequences, and uses gradient descent to improve the expectation of an objective function. We define novel distributions based on coupled variables to rule out invalid sequences given the target structure and to model the correlation between nucleotides. To make it universally applicable to any objective function, we use sampling to approximate the expected objective function, to estimate the gradient, and to select the final candidate. Compared to the state-of-the-art methods, our work consistently outperforms them in key metrics such as Boltzmann probability, ensemble defect, and energy gap, especially on long and hard-to-design puzzles in the Eterna100 benchmark.", "sections": [{"title": "1. RNA Design as Discrete Optimization", "content": "An RNA sequence x of length n is specified as a string of base nucleotides x1X2...Xn, where xi \u2208 N (N\u2261 {A, C, G, U} is the set of nucleotides) for i = 1,2,..., n. A pseudoknot-free secondary structure for sequence x is a well-balanced dot-bracket string y = y1y2... yn where yi = \".\" indicates that xi is unpaired, and yi = \"(\" indicates that xi is paired with some downstream xj and yi = \")\" indicates that xi is paired with some upstream xj. The set of unpaired indices is denoted unpaired(y) and the set of paired indices pairs(y). For example, if x = CCCAAAGGG and y = (((...))), we have unpaired(y) = {4,5,6} and pairs(y) = {(1,9), (2, 8), (3, 7)}. We assume each base-pair is a Watson-Crick-Franklin or wobble pair, i.e., \u2200(i,j) \u2208 pairs(y), xixj \u2208 P where P\u2286 {CG, GC, AU, UA, GU, UG}."}, {"title": "1.A. RNA Folding.", "content": "The ensemble of an RNA sequence x is the set of all possible secondary structures of x, denoted as y(x). In thermodynamic RNA folding models, Gibbs free energy change \u0394G\u00b0(x, y) is used to characterize the stability of y \u2208 y(x). The lower the free energy \u0394G\u00b0(x, y), the more stable the secondary structure y for x. The structure with the minimum free energy is the most stable structure in the ensemble, i.e., MFE structure,\n\n MFE(x)= argmin \u0394G\u00b0(x, y).\ny\u2208y(x)  [1]\nNote that for most methods for secondary structure prediction, ties for argmin are broken arbitrarily when there are multiple lowest free energy structures. This issue was often neglected in the literature, but it deserves clarification here. To be precise, we define\n MFEs(x) = {y | \u0394G\u00b0(x,y) = min \u0394G\u00b0(x, y')}  [2]\ny'\u2208y(x)\nto be the set of MFE structures for x. When it is a singleton set, we say x has a unique MFE (uMFE) structure.\nThe partition function sums the contribution of all structures in an ensemble:\n Q(x) = \u2211e-\u0394G\u00b0(x,y)/RT, [3]\ny\u2208y(x)\nwhere R is the molar gas constant and T is the absolute temperature. Accordingly, the equilibrium probability of a sequence x folding into a structure y is defined as\n p(y | x) = e-\u0394G\u00b0(x,y)/RT/Q(x) [4]"}, {"title": "1.B. RNA Design as Inverse Folding.", "content": "Given a target structure y*, RNA design aims to find a suitable RNA sequence x that can naturally and easily fold into y*, within the design space X(y*) of all valid sequences for y*:\n X(y*) = {x \u2208 N|y* | \u2200(i, j) \u2208 pairs(y*), xixj\u2208 P} [5]\nBut there are different ways to quantify how \"naturally\" or \"easily\" x folds into y*, which we categorize into two broad groups: (a) MFE-based and (b) ensemble-based criteria.\n\nMFE criteria and uMFE criteria Sequence x is said to be an MFE solution of y* if y is one of the MFE structures of x:\n y* \u2208 MFEs(x)  [6]\nOr equivalently, \u2200y \u2260 y*, \u0394G\u00b0(x, y) < \u0394G\u00b0(x, y*). As a stricter criteria, sequence x is said to be a uMFE solution of y* if y is the unique MFE structure of x, or equivalently:\n \u2200y \u2260 y*, \u0394G\u00b0(x, y) < \u0394G\u00b0(x, y*) [7]\nTo search for an MFE or uMFE solution, we can start from a random sequence x and gradually update it to minimize one of the following metrics:\n\u2022 structural distance d(y*, MFE(x)), where d(,) is a standard distance metric between two secondary structures, returning the number of differently folded nucleotides:\n d(y, y') = |y| - 2. |pairs(y) \u2229 pairs(y')|\n - unpaired(y) \u2229 unpaired(y')|. [8]"}, {"title": "\\u2022 free energy gap \u0394\u0394G\u00b0(x, y), which is the difference between the free energies of y* and MFE(x):", "content": "\u0394\u0394G\u00b0(x, y*) = \u0394G\u00b0(x, y*) \u2013 \u0394G\u00b0(x, MFE(x))\n = AG\u00b0(x, y*) \u2013 min AG\u00b0(x, y) \u2265 0 [9]\ny\u2208y(x)\nClearly, when d(y*, MFE(x)) or \u0394\u0394G\u00b0(x, y*) reaches 0, we have an MFE solution.\nEnsemble-based criteria: Boltzmann probability and ensemble defect However, the above two criteria only consider MFE structures, and neglect the other competing structures. Even if y* is the unique MFE structure, there could still be many highly competitive structures that are very close in energy to y*; see design x in Fig. 1(a) for an example. As a result, the Boltzmann probability p(y* | x) could still be arbitrarily small due to competition, which means x is highly unlikely to fold into y* in equilibrium (see Fig. 1(b)). So a better criteria is to look at the whole Boltzmann ensemble to minimize the competition from alternative structures. We consider two such metrics:"}, {"title": "\\u2022 conditional (i.e., Boltzmann) probability p(y* | x).", "content": "Since each x has exponentially many possible structures in the ensemble, this probability can be arbitrarily small. So for numerical stability, we minimize the negative log probability - log p(y* | x) instead."}, {"title": "\\u2022 ensemble defect ED (x, y*), which is the expected structural distance between y* and all structures in the ensemble", "content": "(18). This metric not only considers competition, but also how (dis)similar the competing structures are from y*; we want to penalize highly competitive structures that are very different from y*. The value of ensemble defect can be normalized to between 0 and 1, known as normalized ensemble defect (NED):\nNED(x, y) = 1/|x| Ey~p(x) d(y*, Y)\n =1/|x| \u2211p(yx) d(y*,y). [10]\ny\u2208y(x)\nBy plugging in Eq. 8 and some simplifications (18, 19), we get\nNED(x,y)= 1- \u2211  pi,j - 1/|x|\u2211qi, [11]\n(i,j) \u2208pairs(y*)\n j\u2208unpaired(y*)\nwhere pij is the base-pairing probability of nucleotides i and j, while qj = 1-\u2211i Pij is the probability of j being unpaired. NED(x, y) can also be further decomposed into the sum of positional defect (ei):\n\nNED(x, y) = 1/|x| \u2211 \u2208(x,y), [12]\n1<=i<=|x|\nwhere\n \u2208i =\n1-qi, if i \u2208 unpaired(y*);\n1-Pij ,if (i, j) \u2208 pairs(y*) for some j > i;\n(1-pji, if (j, i) \u2208 pairs(y*) for some j < i. [13]"}, {"title": "Now we can formulate the RNA design problem as optimizing some objective function f(x, y) over the design space X(y*):", "content": "x* = argmin f(x, y)\nxEX(y*) [14]\nwhere the objective function can be one of these four:\n f(x, y) = d(MFE(x), y*), structural distance\n \u0394\u0394G\u00b0(x, y*) , free energy gap\n - log p(y* | x) , conditional probability\n NED(x, y*) , ensemble defect. [15]"}, {"title": "2. RNA Design as Continuous Optimization", "content": "However, it is well known that the above discrete optimization formulation is hard to optimize. For any target structure, the RNA design space is exponentially large:\n |X(y*)| = 4|unpaired(y*)| . 6|pairs(y*)| [16]\nBut most commonly used local search methods (20-23) considers only one (or a few) candidate sequence in each step and only modifies one (or a few) nucleotides, which seems highly inefficient in exploring the exponentially large design space.\nCan we instead modify all positions of the candidate sequence in each step, or consider all candidate sequences simultaneously and promote the better ones? Here we replace the discrete representation of a single candidate sequence by a probability distribution py* (x) over all possible sequences x in X(y*). Essentially, we propose the following continuous relaxation of the optimization problem minimizing the following objective function:\n J = Ex~py* (:) [f(x, y*)] = \u2211py* (x) f(x, y*) [17]\nx\u2208X(y*)\nThis new objective function is to find a distribution py* (\u00b7) of RNA candidates whose expectation of the objective function f(x, y*) is minimized. If the probability mass concentrates on only one sequence, then this new relaxed objective degenerates to the original discrete objective.\nThe way of modeling the probability distribution py* (.) over the design space X(y*) could potentially affect the complexity of the optimization and the convergence of the final solution. We aim to find a method that can represent the design space efficiently while being easy to manage."}, {"title": "2.A. Independent distributions (v0).", "content": "The most obvious modeling of py(x) is to use an independent distribution over N = {A, C, G, U} for each position, so that the distribution over sequences is simply the product of individual distributions:\n py(x) = \u03a0 pi(xi) [18]\ni\nThis is the same distribution in previous work (16). However, this distribution is simplistic and overlooks the fact that each base-pair (i, j) \u2208 y requires xixj to be one of the 6 possible pairs in P, which is impossible with independent variables (4 \u00d7 4 = 16 choices for xixj). As a result, the domain of this p(.) distribution is all possible sequences N|y| (of size 4|y|) rather than the set X(y) of valid sequences (of"}, {"title": "2.B. Coupled variables for pairs (v1).", "content": "In order to model the dependencies between paired positions, we separate the positions into two groups: the set of unpaired indices, denoted unpaired(y), and the set of paired indices, pairs(y). We now factorize the joint distribution of the entire sequence as\n p(x) = \u03a0 p(xi) \u03a0 pij (xixj)\ni\u2208unpaired(y)\n (i,j) Epairs(y) where p() is the local distribution (over N) for unpaired position i, and p() is the local distribution (over 6 choices in P) for paired positions (i, j). This is the first distribution over the set of valid sequences X(y) for a given y.\nAs an example, consider Fig. 2(a). Here y = (.(...)...), so unpaired(y*) = {2, 4, 5, 6, 8, 9, 10} and pairs(y*) = {(1,11), (3,7)}. The probability distribution of the design space is factorized as:\n p(x) = p2(x2)\u00b7p\u2084(x4)\u00b7p(x5)\u00b7p(x6)\u00b7p(x8) \n\u00b7pg (19) Pro (110) \u00b7 PP,11 (X1X11) \u00b7 P37(X3X7) Therefore, for the particular design in Fig. 2(a),\nPu (CAUAAGACAUG) = P(A)p(A)p(A)p(G)p(C) \nP(A) Pro (U) P1,11 (CG) P37(UA)"}, {"title": "2.C. Coupled variables for terminal mismatches (v2 and v3).", "content": "The next two versions (v2 and v3) are refinements of the above v1. First, we note that in the standard energy models (Turner rules (24)), there are terminal mismatches lookup tables. For example, for a hairpin loop defined by the pair (i, j), the first and last nucleotide of the loop, xi+1 and xj-1, are the terminal mismatch, and will be looked up together in the energy table (such as 14 and 16 in Fig. 2(a)). Therefore, it is better to make a coupled variable over N\u00b2 (4 \u00d7 4 = 16 choices) for each terminal mismatch position-pair:\n p(x)= \u03a0 p(xi) \u03a0 pij (xixj) \u03a0 pij (xixj) Moreover, there is a special case that deserves our\ni\u2208unpaired(y)  (i,j)Epairs(y)  (i,j) Emismatches(y)\nattention. Let us consider the 1-by-3 internal loop in Fig. 2."}, {"title": "For such 1-by-x (x > 1) internal loops, there is exactly one unpaired nucleotide on one of the two branches, and that single unpaired nucleotide (x2 in our example) is included in two mismatches (x2 and 18 on one side and 12 and 110 on the other).", "content": "Therefore, it is better to model all these three nucleotides together in a coupled variable over N\u00b3 (43 = 64 choices), which we call a \"trimismatch\":\np(x) = \u03a0 p(xi) \u03a0 pij (xixj)\ni\u2208unpaired(y)\n (i,j) Epairs(y)\n \u03a0 pij (xixj) \u03a0 pijk (XiXjXk) , tm=  (i,j)\u2208mismatches(y)  (i,j,k) Etrimismatches(y)\nNow for the example structure y* in Fig. 2(a), our final joint distribution is:\np(x) = p(x5)p(x9) P11 (x1x11) P37(X3X7)\n\u2022P4,6(X4X6) P2,8,10 (X2X8X10)"}, {"title": "3. Sampling for Objective Evaluation, Gradient Estimation, and Design Space Exploration", "content": "Given the complexity and variety of RNA design problem settings, a method that can seamlessly switch between various objective functions f(x, y) is desirable. Even though we factorize py* (.) into many tractable local distributions, without making any assumptions or requirements about the structure of f(x, y), the exact calculation of the expectation J = Ex~py*(\u00b7) [f(x, y*)] is generally intractable. Therefore, we employ gradient descent for optimization and adopt random sampling for estimating the objective and its gradients."}, {"title": "3.A. Sampling for Objective Evaluation.", "content": "We approximate the expectation by averaging over a set of samples S from the distribution:\n I = Expy* (:) [f(x, y*)] \u2248 1/|S| \u2211 f(x,y)\nxES\nwhere S = {x(1) ~ py* (\u00b7)}1 |S|\n=1\nTheoretically, as |S|\u2192 \u221e, this approximation will converge to the true expectation."}, {"title": "3.B. Sampling for Gradient Estimation.", "content": "Next, we derive the true gradient of J with respect to py* (.) as:\n\u2207py* J =\u2207py* Ex~py*(\u00b7) [f(x, y*)]\n=\u2207py* \u2211 py* (x) f(x, y*) (def. of expectation)\nxEX(y*)\n=\u2211 py* \u2207py* (x) f(x, y) (linearity of \u2207)\nxEX(y*)\n=\u2211 \u2207py* (x) f(x, y) =\u2211 \u2207py* (x) log py* (x) f (x, y*) (log f(x))'=0/f(x)\nxEX(y*)\n py* (x)\n \u03a3 py* (x)\nxEX(y*)\n= Ex~py* (:) [\u2207py* log py* (x) f(x, y)] (def. of expectation)"}, {"title": "We again use sampling to estimate the above expectation and derive the approximate gradient for step t:", "content": "\u2207\nPy* J(t) \u2248 1/|S|\u2211 \u2207py* log py* (x) f (x, y*) [19]\nxES\nN\nS(t) \u2190 {x(1) ~p()}1 [20]\n=1 We then use the approximate gradient to update the distribution (see Sec. 4 for details):\n\n(t+1)\nPy* \u2190 update (P, Py J(t)) [21]"}, {"title": "3.C. Sampling-based Design Space Exploration.", "content": "At the end of this continuous optimization, we still need to return a single sequence from the distribution, i.e., an \"integral solution\". This can be done by \"rounding\" if the distribution is close to one-hot, or more generally by taking the sequence with the highest probability in the final distribution"}, {"title": "x* = argmax py* (x) For example, for independent distributions (v0), since each position is isolated, we simply take the best nucleotide for each position: x = argmaxa\u2208N Pi(a).", "content": "But for the coupled variable distribution (v1), for each unpaired position i \u2208 unpaired(y*), we take x = argmaxa\u2208N P (a) same as in v0, and for each paired position-pair (i, j) \u2208 pairs(y*), we take the best pair out of the six pair types: xx = argmaxabep Pj (ab).\nHowever, this max-probability sequence is not necessarily the best sequence in terms of the objective function, since the distribution is often not perfectly aligned with objective function. Here we use an alternative approach that simply takes the best sample in terms of the objective function out of all samples collected in the optimization process:\n [22]  x(t) = argmin f(x, y*) [23]\nxES(t)   x* = argmin x(t) [24]\nt\nThis method, which we call \"sampling-based candidate exploitation\", outperforms the max-probability solution, because the samples offer much more diversity in the exploration of the distribution than a single sequence."}, {"title": "4. Parameterization and Optimization", "content": "Now we turn to the question of how to parameterize the factorized distribution py* (\u00b7) as py* (; \u0398). The first method (Sec. 4.A) simply uses \u0398 as raw probabilities, but the update of \u0398 needs to result in probabilities, leading to a harder constrained optimization problem. The second method (Sec. 4.B) models the distribution implicitly by applying softmax on \u0398, resulting in a simpler unconstrained optimization problem."}, {"title": "4.A. Method 1: Direct Parameterization and Constrained Optimization.", "content": "The obvious way of parameterization is to use explicit probabilities. For each unpaired position i, we use a non-negative parameter vector (0A, 0C, 0G, 0U) which sums to 1 as the probability distribution over nucleotides:\n \u2200a \u2208 N, p(a; \u03b8) \u2254 a [25]\nSimilarly for each paired position (i, j), we use a non-negative parameter vector (0GG), which sums to 1, and we have\n \u2200abe P, p (ab; \u03b8) \u2254 i,j,ab\nThe cases for mismatches and trimismatches are also similar. The whole parameter set \u0398 includes all parameter vectors:\n \u0398 ={0 | i \u2208 unpaired(y*)}\n\u222a {0i,j | (i, j) \u2208 pairs(y*)}\nU {0; | (i, j) \u2208 mismatches(y*)}\nU {0tm,k | (i, j, k) \u2208 trimismatches(y*)}\nwhere each \u2208 is a distribution, i.e.,\n\u03b8\u03b1 \u2208 \u03b8, \u03b8\u03b1 \u2208 [0, 1], and \u03a3\u03b8\u03c0\u03b5\u03b8 \u03b8\u03b1 = 1."}, {"title": "Now we can formulate the RNA design problem as a constrained optimization", "content": "min J(0)\n\u0398\ns.t. each is a distribution. [26]\nTo solve this constrained optimization problem, we use the Projected Gradient Descent (PGD) method (25). At each step t, we first perform a gradient descent (with learning rate \u03b1):\n \u00d4 \u2190 \u04e8 \u2212 a\u2207\u0473J (\u0398) [27]\nwhere the gradient components are computed individually for each parameter vector:\nVeJ(0) = {J(0) | \u03b8 \u2208 \u03b8}\n20\nFor example, for an unpaired position i, we have:\nDJ(\u0398)\n20\n= (\u042d\u0413(\u04e8), \u042d\u0413(\u04e8), \u042d\u0413(\u04e8), \u042d\u0413(\u04e8))\nA iC i,G iU\nThe first component can be estimated using Eq. 19 as follows:\nDJ(\u0398) 1\nA\ns i,A\n2\nXES\nlogy*(x;) f(x, y) [28]\n,A Expanding the term log py* (x;\u0398), the gradient can be simplified (details provided in Sec. S1.A) as:\ndlog p y* (x;)\nDJ(\u0398) 1\niA\n=\u03a3 [iA f(x, y*) [29]\nS\nXES xia=A After the gradient update (Eq. 27), we then project \u04e8 back onto the set of valid distributions. For each in \u0398, we project it back to the probability simplex by finding the vector in the simplex that is closest (in 12 norm) to 0:\n\u0398' \u2190{proj(0) | e\u2208} proj(6)=argmin || \u2013 0||\ns.t. O is a distribution."}, {"title": "4.B. Method 2: Softmax Parameterization and Unconstrained Optimization.", "content": "An alternative approach to the optimization problem is to introduce a parametrization that naturally enforces the required normalization for a valid distribution, thus converting the problem into an unconstrained optimization problem. This approach eliminates the need for performing gradient projection at each step. A common choice for achieving this normalization is the softmax function, which inherently converts a set of real numbers into a valid probability distribution.\nInstead of using a parameter vector as a distribution explicitly, now we model a distribution implicitly using softmax and our new parameter vector  (0A, 0C, 0G, 0U) no longer sums to 1; instead we have:\n\u2200a\u2208 N, p(a; 0) \u2254 exp(0)/\u03a3exp(0) [30]\n\u03b1\nwhere a can be any real number. The softmax function ensures that each p(0) forms a valid distribution without explicitly imposing this as a constraint. This definition can be extended for other parameter vectors, and  k as follows:\n \u2200ab E P, pj (ab; 0) \u2254 exp(0jab)/\u03a3exp(0.a'b')\na'b'e\n \u2200ab \u2208 N\u00b2, pj (ab;0) \u2254 exp(0mj,ab)/\u03a3exp(@mj,a'b')\na'b'\u2208N2\n tm abc \u2208 N\u00b3, pk (abc; 0) \u2254 exp(0m,k,abc)/ Ea'b'c'e exp(ka'b'c')"}, {"title": "With this new parametrization, the optimization problem becomes an unconstrained problem:", "content": "min J(0) [31]\n\u0398\nSince the constraints have been naturally embedded into the problem formulation through the softmax function, we can directly apply the vanilla gradient descent algorithm to solve this optimization problem. The gradient can be updated by:\n\u0398' \u2190 \u0398 \u2013 \u03b1\u2207\u04e9I(0) where a is the learning rate."}, {"title": "Due to the softmax parametrization, the specific form of the gradient VeJ() differs from that in the constrained optimization problem.", "content": "Using the chain rule, we express it as:\ndlog py* (x;)\n0A\ns dlog py* (x;)\n\u03a3\na\u0395N\ndp(\u03b1; 0)\n(Eq. 29)\ndp(\u03b1; 0) \u03b1.\u0398A\ns(Eq. 33) [32] where the first partial derivative on the right hand side is identical to the case of direct parameterization above (Sec. 4.A; Eq. 29), but the second partial derivative, which used to be 1 in direct parameterization, is now the gradient of the softmax function (see Fig. 4 and Sec. S1.B for details):\ndp(\u03b1; 0)\n0A\n= p(\u03b1; 0) (1 [a = A] \u2013 p (\u0391; 0)) [33]"}, {"title": "So the gradient for the softmax parameterization is:", "content": "dlog py* (x;)\n0 2A\ns E\nxia\n\u03a3\u03a3 \u03a3\na\u0395N\nxia=A\n[p(\u03b1; 0) (1 [a = A] \u2013 p(\u0391; 0))]\nWe run the gradient decent step until the changes in the value of the objective function J() become sufficiently small (see Sec. 6), indicating that the solution has converged. Algorithm 1 outlines the procedure of both constraint and unconstraint optimization approach."}, {"title": "5. Related Work", "content": "Although Matthies et al. (16) also used continuous optimization for RNA design, our approach is vastly different from and substantially outperforms theirs in both scalability and design quality (by all metrics).\n\u2022 First, their sequence distribution is a simple product of independent distributions for each position (same as our distribution v0 in Sec. 2.A) which is ill-suited for the RNA design problem for two reasons: (a) that distribution includes exponentially many illegal sequences for any input structure due to pair violations and (b) that distribution does not explicitly model the"}, {"title": "covariance between paired positions.", "content": "Instead, we use coupled variables for paired and mismatch positions (our distributions v1, v2, and v3 in Secs. 2.B-2.C), which rules out invalid sequences and explicitly models the dependencies between correlated positions.\n\u2022 Second, our sampling framework can work with arbitrary objective functions while their work is specifically designed for one such function, the Boltzmann probability.\n\u2022 Third, our unbiased sampling yields an unbiased approximation to the expectation of an arbitrary objective function over the distribution of sequences. For example, for the case of Boltzmann probability, our sampling results in an unbiased approximation of the expected Boltzmann probability, which converges to the true expectation as the sample size increases:\nS\nepy* | x) = Exp(y* | x)] [34]\nxES\nBy contrast, they optimize a different objective (in red below) that deviates from the true expectation of Boltzmann probability with a bias (E[X/Y] \u2260 E[X]/E[Y]):\nEx[e-AG\u00b0(x,y*)/RT] Ea -e-AG\u00b0(x,y*)/RT ]\n \u03a3 [Q(x)] \u2248 Ea [p(y* | x)] [35]\n\u2022 Fourth, our sampling-based approach is much more efficient: it scales to the longest structures in the Eterna100 benchmark (400 nt) while their work only scaled to structures up to 50 nt long."}, {"title": "6. Evaluation Results on Eterna100 Dataset", "content": "The Eterna100 dataset (26) is a widely used benchmark for evaluating RNA design programs. It contains 100 secondary structures (i.e., \"puzzles\") of up to 400 nucleotides, varying in design difficulty from simple hairpins to intricate multiloop structures. We evaluated this work against three baselines using this dataset: SAMFEO (4), \u039d\u0395\u039c\u039f (6), and Matthies et al. (16). To compare their performance, we used the following metrics:\n1. Average p(y* | x) across all puzzles;\n2. Geometric mean of p(y* | x) across all puzzles except those 18 that are proven to be undesignable (in the sense that there is no uMFE solution) by our previous work (27, 28); these puzzles have extremely low p(y* | x) which bias the geometric mean towards 0;\n3. Average NED(x, y) across all puzzles;\n4. Average d(MFE(x), y*) across all puzzles;\n5. Average AAG\u00b0(x, y) across all puzzles;\n6. Number of puzzles in which an MFE solution is found;\n7. Number of puzzles in which a uMFE solution is found."}, {"title": "Sampling-based Continuous Optimization (This Work)", "content": "By default, our method samples 2500 sequences at each step. The number of steps is adaptive to each puzzle, specifically the run stops after 50 steps in which the objective function does not improved and the total number of steps is limited to 2000. Our main program is implemented in C++ and utilizes OpenMP for parallelization.\nThe default initial learning rate is set to 0.01, which works well for all puzzles under the softmax parameterization. However, we observe that the direct parameterization (projection) requires smaller learning rates as puzzle lengths increases; otherwise, the objective value does not improve. Therefore, we apply adaptive learning rate decay for the projection method. Additionally, we implement momentumbased optimizers: Adam (29) for softmax and Nesterov accelerate gradient (30) for projection.\nWe adopt three types of initialization:\n\u2022 Uniform: Each parameter is set to uniform distribution.\n\u2022 Targeted: Assigns 100% A for unpaired, 50% CG and GC for base pairs, and uniform distribution for mismatches and trimismatches.\n\u2022 e-Targeted: A combination of targeted and uniform distribution defined by e targeted + (1-6) uniform.\nFor the projection method, we use both uniform and targeted initializations. For the softmax method, we use uniform and e-targeted initializations with e = 0.75. The final solutions are selected from the best out of both initializations.\nWe use our previous work, Linear Partition (19), to compute the partition function and base-pairing probability in linear time with beam pruning. We use beam size b = 250 for optimizing p(y* | x), and b = 100 for optimizing NED(x, y*). It is only when optimizing for p(y* | x), we use a larger beam size because LinearPartition tends to underapproximate the partition function, resulting in p(y* | x) > 1. For optimizing d(MFE(x), y*) and AAG\u00b0(x, y), we fold the sequences using LinearFold (31) with b = 100.\nAt each step, we record the best sample among the 2,500 samples. After completing all steps, we reevaluate the recorded samples using ViennaRNA 2.0 (32), selecting the best sequence for each metric. The values reported in this paper are thus, unaffected by the approximation error from beam search.\nBaseline 1: SAMFEO SAMFEO is an iterative approach that selects a few nucleotides to mutate by (a) sampling positions based on positional defects and (b) utilizing structural information (4). Similar to our work, SAMFEO is a general approach that can work with any objective function f(x, y*). In their paper, they optimize for two ensemble objectives: 1-p(y*x) and NED(x, y*). SAMFEO is run five times on each puzzle under their default settings with 5000 steps, and we report the best solution obtained from these five runs.\nBaseline 2: NEMO NEMO uses Nested Monte Carlo Search with domain-specific knowledge to solve puzzles (6). It maximizes a scoring function defined by:\nscore(x, y*) = K(1 + \u0394\u0394G\u00b0(x, y*))-sign(K) [36]\nwhere\nK-1 = BPD(MFE(x), y*) [37] 2|pairs(y*)|"}, {"title": "This work (softmax) achieves arithmetic mean p(y* | x) of 0.594, outperforming SAMFEO by 0.013.", "content": "In terms of the geometric mean of p(y* | x) without undesignable puzzles, our method performs better than other baselines by a wider margin. We obtained geometric mean of 0.512, surpassing the second-place method, SAMFEO, by 0.345. This indicates that our method is much more effective at designing solutions for longer and harder-to-design puzzles with lower p(y* | x) values.\nTo our surprise, optimizing for p(y* | x) yields excellent solutions for other"}]}