{"title": "THE GEOMETRY OF CONCEPTS: SPARSE AUTOENCODER FEATURE STRUCTURE", "authors": ["Yuxiao Li", "Eric J. Michaud", "David D. Baek", "Joshua Engels", "Xiaoqing Sun", "Max Tegmark"], "abstract": "Sparse autoencoders have recently produced dictionaries of high-dimensional vectors corresponding to the universe of concepts represented by large language models. We find that this concept universe has interesting structure at three levels: 1) The \u201catomic\u201d small-scale structure contains \"crystals\" whose faces are parallelograms or trapezoids, generalizing well-known examples such as (man:woman::king:queen). We find that the quality of such parallelograms and associated function vectors improves greatly when projecting out global distractor directions such as word length, which is efficiently done with linear discriminant analysis. 2) The \u201cbrain\u201d intermediate-scale structure has significant spatial modularity; for example, math and code features form a \u201clobe\u201d akin to functional lobes seen in neural fMRI images. We quantify the spatial locality of these lobes with multiple metrics and find that clusters of co-occurring features, at coarse enough scale, also cluster together spatially far more than one would expect if feature geometry were random. 3) The \"galaxy\" scale large-scale structure of the feature point cloud is not isotropic, but instead has a power law of eigenvalues with steepest slope in middle layers. We also quantify how the clustering entropy depends on the layer.", "sections": [{"title": "1 INTRODUCTION", "content": "The past year has seen a breakthrough in understanding how large language models work: sparse autoencoders have discovered large numbers of points (\u201cfeatures\") in their activation space that can be interpreted as concepts (Huben et al., 2023; Bricken et al., 2023). Such SAE point clouds have recently been made publicly available (Lieberum et al., 2024), so it is timely to study their structure at various scales.\u00b9 This is the goal of the present paper, focusing on three separate spatial scales. In Section 3, we investigate if the \u201catomic\u201d small-scale structure contains \u201ccrystals\u201d whose faces are parallelograms or trapezoids, generalizing well-known examples such as (man:woman::king:queen). In Section 4, we test if the \u201cbrain\" intermediate-scale structure has functional modularity akin to biological brains. In Section 5, we study the \"galaxy\" scale large-scale structure of the feature point cloud, testing whether it is more interestingly shaped and clustered than an isosropic Gaussian distribution."}, {"title": "2 RELATED WORK", "content": "SAE feature structure: Sparse autoencoders have relatively recently garned attention as an approach for discovering interpretable language model features without supervision, with relatively few works examining SAE feature structure. Bricken et al. (2023) and Templeton et al. (2024) both visualized SAE features with UMAP projections and noticed that features tend to group together in"}, {"title": "3 \"\u0410\u0442\u043e\u043c\" SCALE: CRYSTAL STRUCTURE", "content": "In this section, we search for what we term crystal structure in the point cloud of SAE features. By this we mean geometric structure reflecting semantic relations between concepts, generalizing the classic example of (a, b, c, d)=(man,woman,king,queen) forming an approximate parallelogram where b - a \u2248 d \u2013 c. This can be interpreted in terms of two function vectors b - a and d - c that turn male entities female and turn entities royal, respectively. We also search for trapezoids with only one pair of parallel edges b - a \u2248 d - c (corresponding to only one function vector);.\nWe search for crystals by computing all pairwise difference vectors and clustering them, which should result in a cluster corresponding to each function vector. Any pair of difference vectors in a cluster should form a trapezoid or parallelogram, depending on whether the difference vectors are normalized or not before clustering (or, equivalently, whether we quantify similarity between two difference vectors via Euclidean distance or cosine similarity).\nOur initial search for SAE crystals found mostly noise. To investigate why, we focused our attention on Layers 0 (the token embedding) and 1, where many SAE features correspond to single words. We then studied Gemma2-2b residual stream activations for previously reported word \u2192 word function vectors from the dataset of (Todd et al., 2023), which clarified the problem. Figure 1 illustrates that candidate crystal quadruplets are typically far from being parallelograms or trapezoids. This is consistent with multiple papers pointing out that (man,woman,king,queen) is not an accurate parallelogram either."}, {"title": "4 \"BRAIN\" SCALE: MESO-SCALE MODULAR STRUCTURE", "content": "We now zoom out and look for larger-scale structure. In particular, we investigate if functionally similar groups of SAE features (which tend to fire together) are also geometrically similar, forming \"lobes\" in the activation space.\nIn animal brains, such functional groups are well-known clusters in the 3D space where neurons are located. For example, Broca's area is involved in speech production, the auditory cortex processes sound, and the amygdala is primarily associated with processing emotions. We are curious whether we can find analogous functional modularity in the SAE space.\nWe test a variety of methods for automatically discovering such functional \u201clobes\" and for quantifying if they are spatially modular. We define a lobe partition as a partition of the point cloud into k subsets (\"lobes\") that are computed without positional information. Instead, we identify such lobes based on then being functionally related, specifically, tending to fire together within a document.\nTo automatically identify functional lobes, we first compute a histogram of SAE feature co-occurrences. We take gemma-2-2b and pass documents from The Pile Gao et al. (2020) through it. In this section, we report results with a layer 12 residual stream SAE with 16k features and average L0 of 41. For this SAE, we record the features that fire (we count a feature as firing if its hidden activation > 1). Features are counted as co-occurring if they both fire within the same"}, {"title": "5 \"GALAXY\" SCALE: LARGE-SCALE POINT CLOUD STRUCTURE", "content": "In this section, we zoom out further and study the \"galaxy\" scale structure of the point cloud, mainly its overall shape and clustering, analogously to how astronomers study galaxy shapes and substruc-ture.\nThe simple null hypothesis that we try to rule out is that the point cloud is simply drawn from an isotropic multivariate Gaussian distribution. Figure 6 visually suggests that the cloud is not quite round even in its three first principal components, with some principal axes slightly wider than others akin to a human brain."}, {"title": "5.1 SHAPE ANALYSIS", "content": "Figure 6 (left) quantifies this by showing the eigenvalues of the point cloud's covariance matrix in decreasing order, revealing that they are not constant, but appear to fall off according to a power law. To test whether this surprising power law is significant, the figure compares it with the corresponding eigenvalue spectrum for a point cloud drawn from an isotropic Gaussian distribution, which is seen to be much flatter and consistent with the analytic prediction: the covariance matrix of N random vectors from a multivariate Gaussian distribution follow a Wishart Distribution, which is well studied in random matrix theory. Since the abrupt dropoff seen for the smallest eigenvalues is caused by limited data and vanishes in the limit N \u2192 \u221e, we dimensionally reduce the point clound to its 100 largest principal componends for all subsequent analysis in this section. In other words, the point cloud has the shape of a \"fractal cucumber\", whose width in successive dimensions falls off like a power law. We find such power law scaling is significantly less prominent for activations than for SAE features; it will be interesting for further work to investigate its origins."}, {"title": "5.2 CLUSTERING ANALYSIS", "content": "Clustering of galaxies or microscopic particles is often quantified in terms of a power spectrum or correlation function. This is complicated for our very high-dimensional data, since the underlying density of varies with radius and, for a high-dimensional Gaussian distribution, is strongly concen-trated around a relatively thin spherical shell. For this reason, we instand quantify clustering by estimating the entropy of the distribution that the point cloud is assumed to be sampled from. We estimate the entropy H from our SAE feature point cloud using on the k-th nearest neighbor (k-NN) method Dasarathy (1991); Kozachenko & Leonenko (1987), computed as follows,\n$H_{features} = \\frac{1}{n} \\sum_{i=1}^{n} - \\log(r_i + \\delta) + \\log(n - 1) - \\Psi $ (1)\nwhere $r_i$ is the distance to the k-th nearest neighbor for point i, and d is the dimensionality of the point cloud; n is the number of points; the constant \u03a8 is the digamma term from the k-NN estimation. As a baseline, the Gaussian entropy represents the maximum possible entropy for a given covariance matrix. For a Gaussian distribution with the same covariance matrix, the entropy computed as:\n$H_{gauss} = \\frac{d}{2} (1 + \\log(2\\pi)) + \\sum_{i=1}^{d} \\log(\\lambda_i)$ (2)\nwhere $\\lambda_i$ are the eigenvalues of the covariance matrix. We define the clustering entropy (often referred to as \u201cnegentropy\u201d in physics as $H_{gauss} \u2013 H$, i.e., how much lower the entropy is than its maximum allowed value. Figure 8 shows the estimated clustering entropy across different layers. We see that the SAE point cloud is strongly clustered, particulary in the middle layers. In future work, it will be interesting to examine whether these variations depend mainly on the prominence of crystals or lobes in different layers, or have an altogether different origin."}, {"title": "6 CONCLUSION", "content": "In this paper, we have found that the concept universe of SAE point clouds has interesting structures at three levels: 1) The \"atomic\" small-scale structure contains \"crystals\" whose faces are parallelo-grams or trapezoids, generalizing well-known examples such as (man:woman::king:queen), and get revealed when projecting out semantically irrelevant distractor features. 2) The \u201cbrain\u201d intermediate-scale structure has significant spatial modularity; for example, math and code features form a \"lobe\" akin to functional lobes seen in neural fMRI images. 3) The \u201cgalaxy\" scale large-scale structure of the feature point cloud is not isotropic, but instead has a power law of eigenvalues with steepest slope in middle layers. We hope that our findings serve as a stepping stone toward deeper understanding of SAE features and the workings of large language models."}, {"title": "A ADDITIONAL INFORMATION ON BRAIN LOBES", "content": null}, {"title": "A.1 Co-OCCURRENCE MEASURES", "content": "Definitions of co-occurrence based affinity measures: Let nij be the number of times features i and j co-occur. Let m11 be number of times i and j co-occur, moo be number of times i and j both do not occur, m10 be number of times i occurs but j does not, m\u2081. be number of times i occurs and j either occurs or not, and so on. Then,\nJaccard similarity, Jaccard (1908):\n$J_{ij} = \\frac{n_{ij}}{|i \\cup j|} = \\frac{n_{ij}}{n_{ii} + n_{jj} - n_{ij}}$\nDice score, Dice (1945):\n$DSC_{ij} = \\frac{2n_{ij}}{|i| + |j|} = \\frac{2n_{ij}}{n_{ii} + n_{jj}}$\nOverlap coefficient:\n$overlap_{ij} = \\frac{|i \\cap j|}{min (|i|, |j|)} = \\frac{n_{ij}}{min (n_{ii}, n_{jj})}$\nSimple matching coefficient:\n$SMC_{ij} = \\frac{m_{00} + m_{11}}{m_{00} + m_{11} + m_{01} + m_{10}}$\nPhi coefficient, Yule (1912):\n$\\phi_{ij} = \\frac{m_{11}m_{00} - m_{10}m_{01}}{\\sqrt{m_{1.}m_{0.}m_{.1}m_{.0}}}$"}, {"title": "B UNDERSTANDING PRINCIPAL COMPONENTS IN DIFFERENCE SPACE", "content": "Figure 10 shows that the first principal component encodes mainly the length difference between two words' last tokens in Gemma-2-2b Layer 0."}, {"title": "C BREAKING DOWN SAE VECTORS BY PCA COMPONENT", "content": "An additional investigation of structure we undertake is quantifying how SAE vectors are distributed throughout the PCA components of the activations vectors. To do this, we define a PCA score:\n$PCA score(feature_i) = \\frac{1}{n} \\sum_{i}^{n} i * (pca@feature_i)^2$\nThis metric is a weighted sum between 0 and 1 measuring approximately where in the PCA each SAE feature lies. In Figure 12, we plot this metric on a single Gemma Scope SAE (the results look similar on all Gemma Scope SAEs), and we see that there is an intriguing dip into earlier PCA features in the last third of SAE features."}]}