{"title": "Democratizing MLLMs in Healthcare: TinyLLaVA-Med for Efficient Healthcare Diagnostics in Resource-Constrained Settings", "authors": ["Aya El Mir", "Lukelo Thadei Luoga", "Boyuan Chen", "Muhammad Abdullah Hanif", "Muhammad Shafique"], "abstract": "Deploying Multi-Modal Large Language Models (MLLMs) in healthcare is hindered by their high computational demands and significant memory requirements, which are particularly challenging for resource-constrained devices like the Nvidia Jetson Xavier. This problem is particularly evident in remote medical settings where advanced diagnostics are needed but resources are limited. In this paper, we introduce an optimization method for the general-purpose MLLM, TinyLLaVA, which we have adapted and renamed TinyLLaVA-Med. This adaptation involves instruction-tuning and fine-tuning TinyLLaVA on a medical dataset by drawing inspiration from the LLaVA-Med training pipeline. Our approach successfully minimizes computational complexity and power consumption, with TinyLLaVA-Med operating at 18.9W and using 11.9GB of memory, while achieving accuracies of 64.54% on VQA-RAD and 70.70% on SLAKE for closed-ended questions. Therefore, TinyLLaVA-Med achieves deployment viability in hardware-constrained environments with low computational resources, maintaining essential functionalities and delivering accuracies close to state-of-the-art models.", "sections": [{"title": "1 INTRODUCTION", "content": "The transformative potential of AI in healthcare is increasingly recognized, primarily for enhancing diagnostic accuracy and personalizing care [1] [2] [3] [4]. In healthcare, a domain characterized by diverse data forms such as medical images, textual reports, and real-time sensor data, AI technologies that can effectively handle and utilize this multimodal information are crucial [5]. These technologies not only improve clinical decision-making but also enable comprehensive patient management, thus optimizing health outcomes. Moreover, AI applications extend from reducing routine administrative burdens to supporting complex diagnostic processes, thereby increasing healthcare delivery efficiency and patient-centered care. [6]\nIn response to the critical need for AI technologies that can handle multimodal data in healthcare, several multi-modal large language models (MLLMs) like LLaVA-Med [7], Med-PaLM [8], Med-flamingo, [9], PubMedCLIP [10] and BiomedCLIP [11] have been proposed. These MLLMs integrate Large Language Models (LLMs) with Vision Encoders, thus possessing capabilities that extend beyond textual understanding and analysis to include image processing capabilities. This enables them to simultaneously interpret both textual data and medical images, facilitating more accurate and comprehensive diagnostics and decision-making in healthcare. By rapidly processing and synthesizing diverse data types, these models can significantly advance patient care, enabling quicker, more precise diagnoses and personalized treatment plans, thus, transforming healthcare into a more efficient, effective, and patient-centered service [5] [6].\nHowever, the deployment of these models in practical settings is constrained by their large size and substantial computational requirements. This becomes a significant barrier in resource-limited environments, typical in remote or underserved areas, limiting access to state-of-the-art AI medical technologies. These regions often lack high-performance computing (HPC) facilities and powerful GPUs needed to run large multimodal models, which typically require substantial memory and computational resources. By proposing a model with significantly fewer parameters, we make it feasible to run on less powerful hardware, such as embedded devices like the Nvidia Jetson Xavier. This reduction in resource requirements makes advanced AI diagnostics more accessible in these regions, bridging the gap between technological capability and accessibility where it is most needed.\nOur work proposes TinyLLaVA-Med, a compact multi-modal large language model (MLLM) developed by fine-tuning the general-purpose TinyLLaVA on medical datasets using the training framework of LLaVA-Med MLLM. TinyLLaVA-Med MLLM is designed to be deployable on embedded systems with low computational power, such as the Nvidia Jetson Xavier. While existing studies have shown that smaller multimodal large language models (MLLMs) like MoE-TinyMed [12] can achieve or even surpass the accuracy of larger models in medical settings, they did not focus on the practical deployment of these models on resource-constrained devices. Our research not only confirms that TinyLLaVA-Med attains high accuracy but also extends these findings by demonstrating the practical deployment of this model on embedded devices. This step showcases the potential of implementing advanced AI-driven medical diag-"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Understanding the current state of the art in Multimodal Large Language Models (MLLMs) used in healthcare is crucial for identifying gaps and opportunities for innovation. This section highlights significant developments in MLLMS that enhance medical diagnostics and patient care, leveraging diverse data forms such as text and images. We also detail the architecture and training methodology of TinyLLaVA, a model designed to bring these advanced capabilities to resource-constrained environments, illustrating our project's basis and its alignment with leading practices in the field."}, {"title": "2.1 Multimodal Large Language Models in Healthcare", "content": "Recent advancements in multimodal large language models (MLLMs) have significantly enhanced medical diagnostics and patient care. These models integrate diverse data types, including text and images, to boost both the accuracy and efficiency of diagnostics. MLLMs frequently contain three key components: a pre-trained modality encoder, a pre-trained large language model (LLM), and a modality interface. The modality encoder, often a convolutional neural network (CNN) or transformer-based model, processes visual data such as X-ray, MRI, and CT scan images, extracting and enhancing detailed features. The LLM handles text generation, interpreting and producing medical reports, patient records, and research articles, drawing from extensive medical literature to provide precise, context-rich interpretations. The modality interface ensures seamless integration of these components, using techniques like cross-modal attention mechanisms to align text and image data effectively [5] [7] [10] [8]. This structure allows MLLMs to produce comprehensive and accurate diagnostic conclusions, improving patient outcomes by integrating detailed image analysis with contextual textual information. Leading examples of Multimodal Large Language Models (MLLMs) in healthcare include LLaVA-Med, which leverages a large-scale biomedical dataset for conversational support on biomedical images [7], and Med-PaLM 2, renowned for its physician-level accuracy in medical question answering due to extensive domain-specific fine-tuning [8]. Med-Flamingo introduces adaptability through few-shot learning, proficiently managing real-time medical visual question answering [9]. Additionally, BiomedCLIP and PubMedCLIP excel in biomedical image-text pair analysis, significantly enhancing diagnostic precision with their specialized training datasets [11] [10]. These MLLMs can be used in real-world scenarios to provide clinical decision support in medical fields such as radiology and pathology, thereby showcasing their potential to improve diagnostic accuracy and patient outcomes [5]."}, {"title": "2.2 TinyLLAVA", "content": "The TinyLLaVA model represents a significant advancement in the field of Multimodal Small Language Models (MSLMs), developed to offer a cost-effective and computationally efficient alternative to larger models without compromising on performance. Illustrated in Figure 1, the architecture integrates three primary components: a vision encoder V, a small-scale language model Fe, and a connector P. The vision encoder processes images into visual patch features, while the language model handles textual data to generate responses. The connector aligns these visual and textual elements within the embedding space, facilitating coherent multimodal interaction. TinyLLaVA was trained using a unique approach that involves two primary stages: pre-training and supervised fine-tuning. During pre-training, the model was trained to align the vision and text information in the embedding space using image-caption style data formats. This stage was crucial for preparing the model's layers to handle real-world data by aligning different modalities effectively. The supervised fine-tuning stage then utilized image-text pair data in a multi-turn conversation format, optimizing the model's responses to be contextually relevant and accurate. This model not only supports efficient processing but also maintains competitive performance, making it ideal for applications requiring robust multimodal understanding in resource-limited settings [13]."}, {"title": "3 METHODOLOGY", "content": "The methodology to adapt the TinyLLaVA model for medical applications involved a sequential approach, beginning with instruction-tuning and downstream fine-tuning and ending with deployment on an embedded device. Figure 2 outlines the entire process."}, {"title": "3.1 Instruction-Tuning", "content": "Beginning with the pretrained general-purpose TinyLLaVA model, our first step was to adapt it to interpret and process multimodal medical data that integrates text and imagery. This adaptation drew inspiration from the LLaVA-Med model's approach [7], which involved tuning a pretrained model specifically for medical applications. The Instruction Tuning stage mirrored the LLaVA-Med's second"}, {"title": "3.2 Fine-tuning to Downstream Datasets", "content": "Following instruction tuning, TinyLLaVA-Med underwent downstream fine-tuning on specialized biomedical Visual Question Answering (VQA) datasets, such as VQA-RAD [16] and SLAKE [17]. These datasets are critical for evaluating the effectiveness of our training pipeline, as they contain both open-ended and close-ended medical questions, serving as a benchmark for assessing the model's performance. This fine-tuning step helped to attain a highly accurate and dataset-specific TinyLLaVA-model."}, {"title": "3.3 Deployment on Embedded Device", "content": "The final step involved deploying TinyLLaVA-Med on the Nvidia Jetson Xavier, an embedded device chosen for its balance of computational power and energy efficiency suitable for real-time applications in healthcare. This deployment tested the model's operational effectiveness, particularly its ability to process data swiftly and accurately in an embedded system environment, thereby confirming its readiness for practical medical use.\nEach phase of this methodology not only refined TinyLLaVA's capabilities but also ensured that the final model, TinyLLaVA-Med, was robust and efficient enough to function in resource-limited healthcare environments."}, {"title": "4 RESULTS", "content": ""}, {"title": "4.1 Datasets", "content": "In the instruction-tuning stage, our TinyLLaVA-Med leverages the LLaVA-Med [7] open-sourced dataset, containing 60K image-text pairs from five major imaging domains, including Chest X-ray, MRI, Histology, Gross pathology, and CT. This dataset provided a core foundation for our work.\nOur initial task involved downloading and extracting images from PMC-15M articles to ensure all necessary images were included in our dataset. This process ensured that our model would effectively learn from the well-structured and detailed multimodal biomedical dataset."}, {"title": "4.2 Evaluation Metrics for TinyLLaVA-Med", "content": "To assess whether TinyLLaVA-Med can reach the required medical accuracy and be successfully deployable on the Nvidia Jetson Xavier board, we employ the following metrics. These are designed to measure both the model's effectiveness in medical applications and its operational efficiency within the hardware constraints of the Jetson Xavier, ensuring that it performs optimally in real-world healthcare environments.\n1) Medical Capability Metrics\nWe evaluate the diagnostic capabilities of TinyLLaVA-Med using the VQA-RAD and SLAKE datasets, both specialized biomedical Visual Questioning (VQA) benchmarks in healthcare. The VQA-RAD dataset includes 315 radiology images and 3515 QA pairs covering various body parts and question types like abnormality and modality [16], while the SLAKE dataset contains 642 images and over 7000 QA pairs annotated by physicians, featuring semantic segmentation masks and enhancements from an external medical knowledge graph [17]. These datasets allow for a comprehensive assessment of TinyLLaVA-Med's performance in medical image understanding and question answering, using metrics such as recall for open-ended questions and accuracy for closed-ended questions to gauge the model's ability across different medical scenarios.\n2) Hardware Deployment Evaluation Metrics\na) GPU Utilization Efficiency\nOur goal is to optimize GPU utilization on the Nvidia Jetson Xavier to nearly 100% capacity. High utilization indicates that the model maximizes the available computational resources, which is crucial for efficient operation and rapid response times needed in real-time healthcare applications. Low utilization may suggest that the model either requires minimal computation or is not fully optimized for the hardware."}, {"title": "4.3 Results", "content": "1) Instruction-Tuning Training Stage Results\nThe instruction tuning stage involved end-to-end fine-tuning where the model was adjusted to follow specific instructions and perform tasks within a conversational medical context. During the stage, we monitored the training loss as seen in Figure 3.\n2) Hardware Performance Results\nTo assess the operational efficiency of TinyLLaVA-Med, we conducted hardware performance monitoring of the Nvidia Jetson AGX Xavier during the inference of TinyLLaVA-Med. The following metrics were noted and are summarized in Table 1:\n\u2022 GPU Utilization: Our model achieved a GPU utilization rate of 62%. Typically, higher utilization closer to 100% is preferable for maximizing computational resources, especially in performance-critical applications like real-time healthcare diagnostics. However, the 62% rate suggests that while the system is not overloaded, there may be room to further optimize the model to use the available GPU resources more effectively.\n\u2022 Power Efficiency: The power consumption was measured at 18.9W, which falls within the operational power range of 10W-30W set for the Nvidia Jetson Xavier. This confirms the model's efficient power usage.\n\u2022 Memory Footprint: Memory utilization was optimized to 11.9GB out of 30.3GB RAM and 1.1GB out of 4.2GB GPU memory. This optimization reflects a significant reduction in memory usage while maintaining robust model performance.\nThese results validate our model's design and optimization processes, confirming that TinyLLaVA-Med not only meets but exceeds the necessary operational standards for effective deployment in medical settings. This ensures efficient data processing without compromising the performance required for real-time medical diagnostics.\n3) Comparison with State-of-the-Art\nTo position TinyLLaVA-Med in the current landscape of multimodal large language models (MLLMs), we compare its performance against several leading models such as LLaVA-Med (Llama7B and Vicuna7B) and TinyMoE-Med variants. These models represent the state-of-the-art in combining textual and visual data to address complex question answering tasks in medical domains, making them relevant benchmarks for our evaluation. As seen in Table 2, The model displayed varied performance, excelling particularly in closed-ended questions with accuracies reaching 70.70% in SLAKE and 64.54% in VQA-RAD, which suggests its proficiency in scenarios requiring definitive binary answers. In comparison, open-ended question handling proved more challenging, with lower recall rates of 61.62% and 29.85% respectively on SLAKE and VQA-RAD.\nDespite its lower accuracy in open-ended questions compared to other models, TinyLLaVA-Med's performance in closed-ended scenarios highlights its potential for specific applications where concise, binary outputs are required. The modest gap in performance between TinyLLaVA-Med and larger models such as LLaVA-Med and TinyMoE-Med illustrates the feasibility of achieving a balance between model size and accuracy. This balance is critical for deploying efficient yet capable models on platforms with limited computational resources like the Nvidia Jetson Xavier, indicating promising avenues for optimization and targeted application in real-world settings.\nThese results shows the potential to refine TinyLLaVA-Med further, enhancing its capability for open-ended questions while maintaining its efficiency for closed-ended tasks. Optimizing this balance can make TinyLLaVA-Med a prac-"}, {"title": "5 DISCUSSION", "content": "This project addresses the deployment of Multimodal Large Language Models (MLLMs) in healthcare, particularly the adaptation required for embedded systems such as Nvidia Jetson Xavier/Orin. It is crucial to understand the implications and limitations of our work and to explore pathways that further the democratization of MLLMs in the healthcare domain.\nLack of Benchmark for MLLMs Optimization in embedded systems: The field of deploying MLLMs in healthcare on embedded systems is relatively unexplored, with traditional MLLMs typically targeting well-resourced environments that prioritize maximum accuracy and throughput. As a result, we faced a challenge in identifying existing benchmarks to effectively assess the optimizations of MLLMs for resource-constrained development. Consequently, we relied on the maximum capabilities of our environment, the Nvidia Jetson Xavier, as the benchmark. However, the work of other models like MoE-TinyMed [12] suggests that this area is beginning to be more thoroughly explored. Potentially, our work could also serve as a foundational effort for establishing benchmarks in this field.\nPerformance Metrics Considered: In developing TinyLLaVA-Med, we utilized a dual-focused set of performance metrics that cater not only to the technical demands of deployment on constrained devices but also to the requirements of medical diagnostics. These metrics include GPU utilization efficiency, energy consumption, memory footprint, and medical diagnostic accuracy. This comprehensive framework ensures that TinyLLaVA-Med is not just an effective doctor assistant MLLM but also a practical deployable tool for healthcare professionals, capable of operating effectively within the limited resources typical of remote or underserved areas. Our balanced approach ensures that the model delivers reliable medical insights with optimal power and resource efficiency, making sophisticated medical AI technology both effective and accessible."}, {"title": "6 CONCLUSION", "content": "Artificial Intelligence (AI) in healthcare has revolutionized the way medical professionals diagnose and treat diseases. In this field, our work with TinyLLaVA-Med addresses the critical need for advanced healthcare technologies that are accessible in low-resource settings, especially in remote and underserved areas. By successfully deploying TinyLLaVA-Med on the Nvidia Jetson Xavier, we demonstrate that it is feasible to implement sophisticated Multi-Modal Large Language Models (MLLMs) with limited computational resources without compromising diagnostic effectiveness. Our model, fine-tuned on specialized medical datasets and rigorously benchmarked on VQA-RAD and SLAKE datasets, proves that high diagnostic accuracy can be sustained even on low-performance computing platforms. This enables real-time, reliable medical decision-making capabilities in regions where advanced healthcare technology was previously inaccessible. By optimizing the performance to meet the constraints of embedded systems, TinyLLaVA-Med not only enhances healthcare delivery but also democratizes access to life-saving diagnostics."}]}