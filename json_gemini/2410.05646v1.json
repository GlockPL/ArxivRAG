{"title": "Score-Based Variational Inference for Inverse Problems", "authors": ["Zhipeng Xue", "Penghao Cai", "Xiaojun Yuan", "Xiqi Gao"], "abstract": "Existing diffusion-based methods for inverse problems sample from the posterior using score functions and accept the generated random samples as solutions. In applications that posterior mean is preferred, we have to generate multiple samples from the posterior which is time-consuming. In this work, by analyzing the probability density evolution of the conditional reverse diffusion process, we prove that the posterior mean can be achieved by tracking the mean of each reverse diffusion step. Based on that, we establish a framework termed reverse mean propagation (RMP) that targets the posterior mean directly. We show that RMP can be implemented by solving a variational inference problem, which can be further decomposed as minimizing a reverse KL divergence at each reverse step. We further develop an algorithm that optimizes the reverse KL divergence with natural gradient descent using score functions and propagates the mean at each reverse step. Experiments demonstrate the validity of the theory of our framework and show that our algorithm outperforms state-of-the-art algorithms on reconstruction performance with lower computational complexity in various inverse problems.", "sections": [{"title": "1 Introduction", "content": "Diffusion models [22, 25, 5, 23, 19] have shown impressive performance for image generation. For diffusion models such as diffusion denoising probability model (DDPM) [5] and denoising score matching with Langevin dynamics (SMLD) [25], the essential part is the learning of score functions of data distributions with large datasets. By approximating score functions with neural networks such as U-Net [20, 25], the prior of complex data distributions can be learned implicitly which encourages many applications. Inverse problems aim to recover an unknown state x from observation y, which is fundamental to various research areas such as wireless communication, image processing and natural language processing. Recent works [6, 9, 23, 26, 3, 8, 15, 2, 12, 13] have shown that diffusion models can be used for solving inverse problems since the prior of data distribution is learned implicitly with score functions.\nBased on Bayes' rule, diffusion models are used for the generation of data from the posterior distribution with score functions of data and likelihood, and thus can be applied in solving reverse problems. The main difficulty of applying diffusion models to solving inverse problems is the calculation of likelihood score. SNIPS [9] and DDRM [8] are proposed to solve noisy linear inverse problems with diffusion process in the spectral domain. In these methods, the measurement and data to be estimated are transformed into the spectral domain via singular value decomposition (SVD), and the conditional score can be calculated with SVD explicitly. In [15], the authors propose to approximate the likelihood score by a noise-perturbed pseudo-likelihood score which has a closed form under certain assumptions and can be efficiently calculated using SVD for noisy linear inverse problems. MCG [3] circumvents the calculation of likelihood score by projections onto the measurement constrained manifold. In DPS [2], the Laplace method is used for the approximation of the likelihood score for general inverse problems. Instead of directly approximating the likelihood score, a variational inference based method termed RED-Diff [13] optimizes the reconstruction loss with score matching regularization.\nThe works mentioned above mainly focus on generating random samples from the posterior distribu-tion rather than finding a statistical estimate such as the posterior mean, i.e. the MMSE estimation in Bayesian inference, which is preferred in many applications [10]. To get the posterior mean, we have to sample from the posterior multiple times and average out which is time-consuming. In this work, we show that the evolution of the conditional probability of the reverse diffusion process can be characterized by the reverse mean and covariance, and the posterior mean can be achieved by tracking the mean of each conditional reverse diffusion step. Base on this, we propose a variational inference based framework that minimizes the reverse KL divergence and propagate the reverse mean at each reverse step, referred to as Reverse Mean Propagation (RMP). By implementing RMP with stochastic natural gradient descent and approximating the gradient with score functions, we devise a practical algorithm for inverse problems. Instead of generating samples by running algorithms multiple times, RMP outputs an estimate which is closed to the posterior mean directly and thus can reduce complexity greatly. We demonstrate the validity of the theory of RMP by a toy example as illustrated in Fig 1 and show that by adopting the RMP algorithm, we achieve a large improvement over existing state-of-the-art algorithms on various image reconstruction tasks."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Inverse Problems", "content": "An inverse problem is defined as the estimation of an unknown state or a latent $x \\in \\mathbb{R}^{N\\times1}$ from measurement $y \\in \\mathbb{R}^{M\\times1}$. Specifically, the measurement process can be described by a measurement operator $A: \\mathbb{R}^{N\\times1} \\rightarrow \\mathbb{R}^{M\\times1}$, and the final output is a noisy version of the measurement:\n$y = A(x_0) + w_0$                                                                                              (1)\nwhere $w_0 \\in \\mathbb{R}^{M\\times1}$ is the measurement noise. Usually the measurement noise is assumed to be zero mean Gaussian with variance $\\epsilon^2I$. Other noise models may also apply. In linear inverse problems, the measurement function A is linear and can be represented by a linear transform $A \\in \\mathbb{R}^{M\\times N}$. We focus on inverse problems with known A in this paper."}, {"title": "2.2 Variational Inference", "content": "The inverse problem (1) can be formulated as a Bayesian estimation problem. The posterior distri-bution of x is given by $p(x_0|y) = \\frac{p(y|x_0)p(x_0)}{p(y)}$, where $p(x_0)$ is the prior distribution of $x_0$ and $p(y|x_0)$ is the conditional distribution of y given $x_0$. The posterior mean, i.e., the MMSE estimator can be employed for the estimation of $x_0$. However, the posterior $p(x_0|y)$ is intractable in general since the prior $p(x_0)$ and the likelihood $p(y|x_0)$ may be very complicated in real applications. An alternative way is to find an approximation of the posterior as in variational inference (VI). VI introduces a distribution $q(x_0|y)$ and maximizes a lower bound of the log probability of marginal $p(y)$:\n$\\log p(y) = \\log \\int q(x_0|y) \\frac{p(y|x_0)p(x_0)}{q(x_0|y)} dx_0\\\\\n\\ge \\int q_{\\phi}(x_0|y) \\log \\frac{p(y, x_0)}{q_{\\phi}(x_0|y)} dx = -F_{\\phi}(y)\\\\\n= -KL(q_{\\phi}(x_0|y)||p(x_0|y)) + \\log p(y)$                                                                  (2)\nwhere the inequality is obtained by using the Jensen's inequality. The lower bound is referred as the evidence lower bound (ELBO) or the negative of free energy $F(y)$. It is worth noting that maximizing ELBO is equivalent to minimizing the KL divergence between $q_{\\phi}(x_0|y)$ and $p(x_0|y)$ as shown in the last line of (2). Many methods have been proposed for the optimization of (2) such as mean field VI [1], black box VI [17], stochastic VI [11] and normalizing flow VI [18]. However, these methods are difficult to be applied to real applications since prior $p(x_0)$ is complicated and is usually learned by neural networks. It is shown that learning the distribution of high dimensional data through score matching [28] directly is inaccurate since the existance of low density data regions [25]. Also, perturbing data with Gaussian noise makes the data distribution more amenable to learn [25] which is the core of score-based generative model."}, {"title": "2.3 Score-Based Generative Models", "content": "Score-based generative models or diffusion models generate samples of a data distribution from the reverse process of a diffusion process. The diffusion process is also called the forward process where Gaussian noise is added gradually to the original data distribution until the noisy data are approximately Gaussian-distributed. More specifically, the diffusion process is a Markov process with joint probability of its latent states $\\{x_k\\}_{k=0}^T$ given by\n$p(x_{T:0}) = p(x_0) \\prod_{k=0}^{T-1} p(x_{k+1}|x_k)$.                                                                            (3)\nTwo classes of widely studied diffusion models, i.e., the variance preserving (VP) diffusion model [5] and the variance exploding (VE) diffusion model [25] are distinguished by Markov diffusion kernel $p(x_{k+1}|x_k)$. For VE diffusion, $p(x_{k+1}|x_k) = \\mathcal{N}(x_{k+1}; x_k, (\\sigma_{k+1}^2 - \\sigma_k^2)I)$, where $\\sigma_T^2 > \\sigma_{T-1}^2 > \\cdots > \\sigma_1^2 > \\sigma_0^2 = 0$, and for VP diffusion, $p(x_{k+1}|x_k) = \\mathcal{N}(x_{k+1}; \\sqrt{1 - \\beta_{k+1}}x_k, \\beta_{k+1}I)$ where $\\beta_T > \\beta_{T-1} > \\cdots > \\beta_1 > \\beta_0 = 0$. The learning of score function $\\nabla_{x_t} \\log p(x_k)$ is essential to the sample generation for both diffusion models, and thus such models are called score-based generative models. In VP diffusion models, a variational reverse process is leaned to minimize the KL divergence between forward and reverse process. The score functions of perturbed data distributions are trained with a variational bound. Samples are generated from the learned reverse process with ancestral sampling method. In VE diffusion models, the score functions of perturbed data distributions are learned with a score network using denosing score matching [28]. Samples from the desired distribution are generated with Langevin dynamics method."}, {"title": "3 Diffusion Process and Posterior Estimation", "content": "The measurement y in (1) and diffusion states $\\{x_k\\}_{k=0}^T$ in (3) form a new Markov chain $y \\rightarrow x_0 \\rightarrow x_1 \\rightarrow x_T$ and the reverse conditional $p_k(x_k|x_{k+1}, y)$ is given by $p_k(x_k|x_{k+1}, y) = \\frac{p(x_{k+1}|x_k)p(x_k|y)}{p(x_{k+1}|y)}, \\forall k = 0,\\dots T-1$. In this part, we focus on the property of reverse conditional $p_k(x_k|x_{k+1}, y)$.\n$p_k(x_k|x_{k+1}, y)$. We relate the Markov chain $\\{x_k\\}_{k=0}^T$ to continuous stochastic process $\\{x_t\\}_{t=0}^T$ by letting $x_k = x_{t=k\\Delta t}$, where $\\Delta t = \\frac{T}{T+1}$. Then, the discrete diffusion process (3) becomes a continuous process in the limit $\\Delta t \\rightarrow 0$. We have the following results in the limit of $\\Delta t \\rightarrow 0$.\nProposition 1 For diffusion models with forward process (3), the reverse conditional $p_k(x_k|x_{k+1}, y), \\forall k = 0\\dots T-1$, is Gaussian when $\\Delta t \\rightarrow 0$. For VE and VP diffusion, the mean and covariance of $p_k(x_k|x_{k+1}, y)$ are tractable with mean given by\n$\\mu_k(x_{k+1}, y) = V_{k,1}x_{k+1} + V_{k,2}\\mathbb{E}_{p(x_0|y)}[x_0]$                                                                   (4)\nwhere $V_{k,1} = (\\sigma_k^2I + C_{x_0})(\\sigma_{k+1}^2I + C_{x_0})^{-1}$ and $V_{k,2} = (\\sigma_{k+1}^2 - \\sigma_k^2)(\\sigma_{k+1}^2I + C_{x_0})^{-1}$ for VE diffusion, and $V_{k,1} = \\sqrt{\\frac{\\bar{\\alpha}_{k+1}}{\\bar{\\alpha}_k}}((1 - \\bar{\\alpha}_k)I + \\bar{\\alpha}_k C_{x_0})((1 - \\bar{\\alpha}_{k+1})I + \\bar{\\alpha}_{k+1} C_{x_0})^{-1}$ and $V_{k,2} = \\sqrt{\\bar{\\alpha}_k}(1 - \\bar{\\alpha}_{k+1})((1 - \\bar{\\alpha}_{k+1})I + \\bar{\\alpha}_{k+1} C_{x_0})^{-1}$ for VP diffusion. $\\mathbb{E}_{p(x_0|y)}[x_0]$ and $C_{x_0}$ are the mean and covariance of $p(x_0|y)$ respectively. $\\bar{\\alpha}_k = \\prod_{i=0}^{k} \\alpha_i, \\alpha_i = 1 - \\beta_i$. The covariance of $p_k(x_k|x_{k+1}, y)$ for VE and VP diffusion models are given respectively by\n$C_{k,VE} = (\\sigma_{k+1}^2 - \\sigma_k^2)(\\sigma_k^2I + C_{x_0})(\\sigma_{k+1}^2I + C_{x_0})^{-1}$ \n$C_{k,VP} = \\frac{\\beta_{k+1}}{1 - \\beta_{k+1}}((1 - \\bar{\\alpha}_k)I + \\bar{\\alpha}_k C_{x_0})\\left(\\frac{\\beta_{k+1}}{1 - \\beta_{k+1}}((1 - \\bar{\\alpha}_{k+1})I + \\bar{\\alpha}_{k+1} C_{x_0})\\right)^{-1}$                                  (5)\nProposition 1 generalizes the Gaussian property of $p_t(x_t|x_{t+\\Delta t})$ to the conditional case $p_t(x_t|x_{t+\\Delta t}, y)$ when $\\Delta t \\rightarrow 0$. The essential is that the reverse diffusion process can also be expressed by a reverse SDE [27]. Based on Proposition 1, we obtain the following main result.\nDefinition 1 The reverse mean propagation chain of a diffusion process is defined as\n$\\mu_T \\rightarrow \\mu_{T-1}(x_T = \\mu_T, y) \\rightarrow \\cdots \\rightarrow \\mu_1(x_2 = \\mu_2, y) \\rightarrow \\mu_0(x_1 = \\mu_1, y)$                                                        (6)\nwhere $\\mu_k(x_{k+1} = \\mu_{k+1}, y)$ is the mean of $p_k(x_k|x_{k+1} = \\mu_{k+1}, y), \\forall k = 0,\\dots ,T-1$, and $\\mu_T$ and $\\mu_0$ are the initial point and the end point of the reverse chain respectively.\nTheorem 1 For VE diffusion, when $\\Delta t \\rightarrow 0$, the end point of the reverse chain, i.e., $\\mu_0$ is given by\n$\\mu_0 = (\\sigma_0^2I + C_{x_0})(\\sigma_T^2I + C_{x_0})^{-1}\\mu_T + (\\sigma_T^2 - \\sigma_0^2)(\\sigma_T^2I + C_{x_0})^{-1}\\mathbb{E}_{p(x_0|y)}[x_0]$                                          (7)\nand $\\mu_0 \\rightarrow \\mathbb{E}_{p(x_0|y)}[x_0]$ as $\\sigma_T \\rightarrow \\infty$. For VP diffusion, when $\\Delta t \\rightarrow 0$, $\\mu_0$ is given by\n$\\mu_0 = \\sqrt{\\frac{\\bar{\\alpha}_T}{\\bar{\\alpha}_0}}((1 - \\bar{\\alpha}_0)I + \\bar{\\alpha}_0C_{x_0})((1 - \\bar{\\alpha}_T)I + \\bar{\\alpha}_T C_{x_0})^{-1}\\mu_T\\\\n+ (1 - \\bar{\\alpha}_T)((1 - \\bar{\\alpha}_T)I + \\bar{\\alpha}_T C_{x_0})^{-1}\\mathbb{E}_{p(x_0|y)}[x_0]$                                                           (8)\nand $\\mu_0 \\rightarrow \\mathbb{E}_{p(x_0|y)}[x_0]$ as $\\bar{\\alpha}_T \\rightarrow 0$.\nAccording to Theorem 1, the posterior mean can be obtained by tracking the mean at each reverse step. By calculating the reverse mean and following the reverse chain in (6), we get a posterior estimation framework termed Reverse Mean Propagation (RMP) for inverse problems as presented in Algorithm 1. We note that when the initial point of the reverse chain $\\mu_T$ and y are given, the reverse chain is deterministic and converges to the posterior mean $\\mathbb{E}_{p(x_0|y)}[x_0]$."}, {"title": "4 Score-Based Variational Inference", "content": "In this section, we propose a score-based variational inference method to implement the RMP framework. We show that tracking the mean of the reverse process of each step can be formulated as a sequential of variational inference problems which we prove to be equivalent to a variational inference problem for all latent variables. We solve the variational inference by stochastic natural gradient descent with approximations that simplify the calculation."}, {"title": "4.1 RMP as Variational Inference", "content": "In Section 3, we present the RMP framework based on the reverse diffusion process. However, in practice, the reverse mean in RMP cannot be calculated using (4) since $\\mathbb{E}_{p(x_0|y)}[x_0]$ and $C_{x_0}$ are unknown. We now show that the RMP framework can be implemented using variational inference. Instead of applying variational inference on the conditional posterior of $x_0$ as in (2), we focus on the joint conditional posterior of $\\{x_k\\}_{k=0}^T$, i.e., $p(x_{0:T}|y)$, which includes all the latent variables in the diffusion process. The variational reverse process with joint conditional is defined by\n$q(x_{0:T}|y) = q(x_T|y) \\prod_{k=0}^{T-1} q_k(x_k|x_{k+1}, y),$                                                                    (9)\nwhere $q_k(x_k|x_{k+1}, y) = \\mathcal{N}(x_k; \\mu_k(x_{k+1}, y), C_k(x_{k+1}, y)), \\forall k = 0 : T-1$. We set $q(x_T|y) = \\mathcal{N}(x_T; 0, I)$. The variational reverse process is chosen as a Markov chain since the reverse of the diffusion process (3) is a Markov process. The KL divergence between variational joint posterior $q(x_{0:T}|y)$ and and joint posterior $p(x_{0:T}|y)$ is given by\n$KL(q||p) = \\int q(x_{0:T}|y) \\log \\frac{q(x_{0:T}|y)}{p(x_{0:T}|y)} dx_{0:T}$                                                                  (10)\nwhere the forward joint posterior $p(x_{0:T}|y) = p(x_T|y) \\prod_{k=0}^{T-1} p(x_k|x_{k+1:T}, y)$. For VE diffusion $p(x_T|y) = \\mathcal{N}(x_T; 0, \\sigma_{T+1}^2I)$ and for VP diffusion $p(x_T|y) = \\mathcal{N}(x_T; 0, I)$. The following proposition simplifies the minimization of $KL(q||p)$ with proof given in Appendix C.\nProposition 2 For a diffusion process with forward process (3), the KL divergence between varia-tional $q(x_{0:T}|y)$ and joint posterior $p(x_{0:T}|y)$ defined in (10) equals\n$KL(q||p) = \\sum_{k=T-1}^{0} \\int q(x_{k+1}|y) \\int q_k(x_k|x_{k+1}, y) \\log \\frac{q_k(x_k|x_{k+1}, y)}{p_k(x_k|x_{k+1}, y)} dx_k dx_{k+1}$,                                                       (11)\nand the minimization of $KL(q||p)$ is equivalent to the minimization of\n$KL(q_k||p_k) = \\int q(x_{k+1}|y) \\int q_k(x_k|x_{k+1}, y) \\log \\frac{q_k(x_k|x_{k+1}, y)}{p_k(x_k|x_{k+1}, y)} dx_k , \\forall k = 0,\\dots ,T-1$.                                                     (12)\nAccording to Proposition 2, we can minimize the KL divergence between q and p by minimizing the KL divergence $KL(q_k||p_k)$, i.e., solve the following VI problem at each reverse step k:\n$q^* = \\underset{q_k}{arg \\min} KL(q_k||p_k), \\forall k = 0,\\dots ,T-1$.                                                     (13)\nBy propagating the mean of $q_k$ and solving problem (13) at each reverse step k, $k = 0,\\dots ,T-1$, we can approximate the RMP framework in Algorithm 1 based on variational inference, as detailed below."}, {"title": "4.2 Variational Inference by Natural Gradient Descent", "content": "For $q_k(x_k|x_{k+1}, y) = \\mathcal{N}(x_k; \\mu_k, \\Lambda_k^{-1}I)$, the KL divergence between $q_k$ and $p_k$ is given by\n$KL(q_k||p_k) = \\int q_k(x_k|x_{k+1}, y) \\log \\frac{q_k(x_k|x_{k+1}, y)}{p_k(x_k|x_{k+1}, y)} dx_k\\\\\n= -\\frac{N}{2} \\log(2\\pi/\\Lambda_k) - \\mathbb{E}_{q_k}[\\log p_k(x_k|x_{k+1}, y)]$.                                                   (14)\nA common practice to optimize $KL(q_k||p_k)$ is to update variational parameters $\\phi_k = \\{\\mu_k, \\Lambda_k\\}$ using mini-batch stochastic gradient descent which involves the calculation of $\\nabla_{\\phi_k} KL(q_k||p_k)$. Since $q_k$ is Gaussian, the gradient of $\\mathbb{E}_{q_k}[\\log p_k(x_k|x_{k+1}, y)]$ in (14) with respect to variational parameters $\\phi_k = \\{\\mu_k, \\Lambda_k\\}$ have simple forms [16] which are given by:\n$\\nabla_{\\mu_k} \\mathbb{E}_{q_k}[\\log p_k(x_k|x_{k+1}, y)] = \\mathbb{E}_{q_k}[\\nabla_{x_k} \\log p_k(x_k|x_{k+1}, y)]$\n$\\nabla_{\\Lambda_k} \\mathbb{E}_{q_k}[\\log p_k(x_k|x_{k+1}, y)] = -\\frac{1}{2} \\mathbb{E}_{q_k}[Tr(\\nabla_{x_k}^2 \\log p_k(x_k|x_{k+1}, y)]$.                                                      (15)\nwhere $\\nabla_{x_k} \\log p_k(x_k|x_{k+1}, y)$ and $\\nabla_{x_k}^2 \\log p_k(x_k|x_{k+1}, y)$ are the gradient and Hessian of $\\log p_k(x_k|x_{k+1}, y)$ respectively, and $Tr(\\cdot)$ returns the trace of the input matrix.\nAs a special case of steepest descent, gradient descent updates parameter that lies in the Euclidean space. However, our objective is to optimize parameters that represent a distribution, it makes sense to take the steepest descent direction in the distribution space. As in natural gradient descent [14], the parameter to be optimized lies on a Riemannian manifold and we choose the steepest descent direction along that manifold. Thus, we choose KL-divergence as the metric of distribution space and take steepest descent in this space. For KL-divergence metric, the natural gradient of parameter $\\phi$ of a loss function $L = \\mathbb{E}_{q_{\\phi}}[h(x)]$ is defined as $\\nabla_{\\phi}L = F_{\\phi}^{-1}\\nabla_{\\phi}L$ [14], where $F_{\\phi}$ is the Fisher information matrix of $\\phi$ given by the variance of the gradient of log probability of parameter, i.e., $Cov_{q_{\\phi}}[\\nabla_{\\phi}\\log q_{\\phi}(x)]$. For $q(x) = \\mathcal{N}(x; \\mu, \\Sigma) = \\mathcal{N}(x; \\mu, \\Lambda^{-1}I)$, the Fisher information matrices of mean and precision are given respectively by $F_{\\mu} = \\Lambda I$ and $F_{\\Lambda} = \\Lambda^{-2}I$. Thus, the natural gradients of parameters $\\{\\mu, \\Lambda\\}$ have concise forms given by\n$\\bar{\\nabla}_{\\mu}L = F_{\\mu}^{-1}\\mathbb{E}_q[h(x)] = \\Lambda^{-1}\\mathbb{E}_q[\\nabla_x h(x)]$\n$\\bar{\\nabla}_{\\Lambda}L = F_{\\Lambda}^{-1}\\nabla_{\\Lambda}\\mathbb{E}_q[h(x)] = -\\mathbb{E}_q[Tr(\\nabla_x^2 h(x))]$.                                                                                                        (16)\nFollowing the natural gradient given in (16), we update the variational parameters $\\phi_k = \\{\\mu_k, \\Lambda_k = \\nu_k^{-1}I\\}$ of loss function in (14) using natural gradient descent (NGD) as\n$\\mu_k \\leftarrow \\mu_k - s_1 \\Lambda_k^{-1}\\nabla_{\\mu_k}KL(q_k||p_k) = \\mu_k + s_1 \\Lambda_1\\mathbb{E}_{q_k}[\\nabla_{x_k}\\log p_k(x_k|x_{k+1}, y)]$\n$\\Lambda_k \\leftarrow \\Lambda_k - 2s_2 \\Lambda \\nabla_{\\Lambda_k}^*KL(q_k||p_k) = \\Lambda_k - s_2(N\\Lambda + \\mathbb{E}_{q_k}[Tr(\\nabla_{x_k}^2 \\log p_k(x_k|x_{k+1}, y))])$                                             (17)\nwhere $s_1$ and $s_2$ are step sizes. We obtain a stochastic NGD update when the expectations of gradient and Hessian matrix in (17) are approximated by sample mean:\n$\\mu_k^{(i+1)} = \\mu_k^{(i)} + s_1 (\\Lambda_k^{(i)} \\frac{1}{L}\\sum_{l=1}^L  {\\nabla_{x_k} log p_k(x_k|x_{k+1},y)|_{x_k = x_k^{(l)}} })$\n$\\Lambda_k^{(i+1)} = \\Lambda_k^{(i)} - s_2 (N\\Lambda + \\frac{1}{L} \\sum_{l=1}^L {Tr(\\nabla_{x_k}^2 log p_k(x_k|x_{k+1},y))|_{x_k = x_k^{(l)}}} )$                                               (18)\nwhere L is the number of samples. The stochastic update of parameters of $q_k$ converges to the local minima of KL divergence $KL(q_k||p_k)$ which is a Gaussian approximation of the posterior $p_k(x_k|x_{k+1}, y)$. It is worth noting that we choose stochastic NGD since it achieves a good perfor-mance and parameters involved are easy to tune in our experiments. Other optimization methods may be applied. In stochastic NGD update (18), the gradient and Hessian of $\\log p_k(x_k|x_{k+1}, y)$ are required. We next introduce some approximations to simplify the calculation."}, {"title": "4.3 Score-Based Gradient Calculation", "content": "From Bayes' rule, the score of reverse conditional $p_k(x_k|x_{k+1}, y) = \\frac{p(x_{k+1}|x_k)p(x_k|y)}{p(x_{k+1}|y)}$ involved in (18) is given by $\\nabla_{x_k} \\log p_k(x_k|x_{k+1}, y) = \\nabla_{x_k} \\log p(x_{k+1}|x_k) + \\nabla_{x_k} \\log p(y|x_k) + \\nabla_{x_k} \\log p(x_k)$, where $\\nabla_{x_k} \\log p(x_k)$ is the noisy score function which can be approximated by a well-trained score network $s_{\\theta}(x_k, \\sigma_k)$ and $\\nabla_{x_k} \\log p(x_{k+1}|x_k)$ can be calculated explicitly for both VE and VP diffusion models. For VE diffusion $\\nabla_{x_k} \\log p(x_{k+1} x_k) = \\frac{x_{k+1}-x_k}{\\sigma_{k+1}^2 - \\sigma_k^2}$, and for VP diffusion $\\nabla_{x_k} \\log p(x_{k+1} x_k) = \\frac{\\sqrt{1-\\beta_{k+1}}}{\\beta_{k+1}} \\frac{x_{k+1}}{\\sqrt{1-\\beta_{k+1}}} + \\frac{1-\\sqrt{1-\\beta_{k+1}}}}{\\beta_{k+1}} x_k$. However, the likelihood score, i.e., the gradient of logarithm conditional $\\nabla_{x_k} \\log p(y|x_k)$ is hard to handle in general. For linear inverse problems, several SVD based approximations of $\\nabla_{x_k} \\log p(y|x_k)$ are proposed in [9, 8, 15] for linear measurements and Gaussian approximation for general measurements are discussed in [24]. In [2], the authors propose the following approximation that can be applied for general measurements:\n$\\log p(y|x_k) \\approx \\log p(y|x_0(x_k))$                                                                                                   (19)\nwhere $x_0(x_k)$ is the MMSE estimate of $x_0$. For VE diffusion, according to the Tweedie formula:\n$x_0(x_k) = \\mathbb{E}_{p(x_0|x_k)}[x_0] = x_k + \\sigma_k^2\\nabla_{x_k} \\log p(x_k)$.                                                                            (20)\nSimilarly, for VP diffusion, the MMSE estimation of $x_0$ given $x_k$ is\n$x_0(x_k) = \\mathbb{E}_{p(x_0|x_k)}[x_0] = \\frac{1}{\\sqrt{\\bar{\\alpha}_k}}(x_k + (1 - \\bar{\\alpha}_k)\\nabla_{x_k} \\log p(x_k))$.                                                                           (21)\nThe approximation error of (19) can be quantified with Jenson's Gap as given in [2]. We choose the likelihood approximation of (19) in our implementation, other approximation methods can be applied to RMP as discussed in Appendix. As a conclusion, the gradient is calculated as\n$\\nabla_{x_k} \\log p_k(x_k|x_{k+1}, y) \\approx \\nabla_{x_k} \\log p(x_{k+1}|x_k) + \\gamma_k\\nabla_{x_k} \\log p(y|x_0(x_k)) + s_{\\theta}(x_k, \\sigma_k)$.                                                (22)\nwhere the parameter $\\gamma_k$ is added to balance the approximated likelihood score and prior score. We set $\\gamma_k = \\zeta \\frac{\\log p(y|x_0(x_k))||_2}{||s_{\\theta}(x_k, \\sigma_k)||_2}$ where $\\zeta$ is a hyper parameter to be tuned for different problems. The idea behind the strategy is that we should always keep a balance between the data score and the likelihood score."}, {"title": "4.4 Fixed Precision Update", "content": "In the update (18), the Hessian matrix $\\nabla_{x_k}^2 \\log p_k(x_k|x_{k+1}, y)$ is difficult to acquire in general. Also, the complexity involved in the calculation of Hessian may prevent the application of the algorithm. Thus, we introduce an approximation that does not require the calculation of Hessian. According to the Proposition 1, the update of precision $\\Lambda_k^{(i+1)}$ converges to the precision of $p_k(x_k|x_{k+1}, y)$. Thus, we can fix the update of $\\Lambda_k^{(i+1)}$ in Algorithm 1 to the precision of $p_k(x_k|x_{k+1}, y)$ and only update $\\mu_k$ at each step. According to Proposition 1, for VE diffusion model"}]}