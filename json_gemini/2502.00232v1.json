{"title": "A HYBRID RANDOM FOREST AND CNN FRAMEWORK FOR TILE-WISE OIL-WATER CLASSIFICATION IN HYPERSPECTRAL IMAGES", "authors": ["Mehdi Nickzamir", "S. Mohammad Sheikh Ahmadi"], "abstract": "This paper presents a novel hybrid Random Forest and Convolutional Neural Network (CNN) framework for oil-water classification in hyperspectral images (HSI). To address the challenge of preserving spatial context, we divided the images into smaller, non-overlapping tiles, which served as the basis for training, validation, and testing. Random Forest demonstrated strong performance in pixel-wise classification, outperforming models such as XGBoost, Attention-Based U-Net, and HybridSN. However, Random Forest loses spatial context, limiting its ability to fully exploit the spatial relationships in hyperspectral data. To improve performance, a CNN was trained on the probability maps generated by the Random Forest, leveraging the CNN's capacity to incorporate spatial context. The hybrid approach achieved 7.6% improvement in recall (to 0.85), 2.4% improvement in F1 score (to 0.84), and 0.54% improvement in AUC (to 0.99) compared to the baseline. These results highlight the effectiveness of combining probabilistic outputs with spatial feature learning for context-aware analysis of hyperspectral images.", "sections": [{"title": "Introduction", "content": "Oil spills have severe ecological, economic, and health consequences. In marine ecosystems, they threaten aquatic species, contaminate seafood, and disrupt the food chain, posing risks to human health. Additionally, oil spills impose financial and reputational damage on companies through fines and legal liabilitiesMoussaoui and Idelhakkar [2023]. Timely detection is critical to mitigate these effects and prevent further damage.\nHyperspectral imaging (HSI) is a powerful tool for detecting oil spills, capturing detailed spectral information across hundreds of wavelengths. Unlike traditional RGB images, HSI identifies unique spectral \u201cfingerprints\u201d of oil spills, even when they are invisible to the naked eye. However, its effective use faces challenges, including high costs of airborne data collection, noise from environmental factors, and the need for robust models capable of handling high-dimensional data.\nData scarcity is another major hurdle, as obtaining labeled hyperspectral datasets is costly and time-intensive. While unsupervised methods avoid the need for labeled data, they often underperform compared to supervised techniques. Supervised methods like Random Forest excel at pixel-wise classification by leveraging spectral features but fail to incorporate spatial context, which is essential for producing coherent predictions across images.\nTo address these challenges, we propose a hybrid framework that combines Random Forest and Convolutional Neural Networks (CNNs). Random Forest handles pixel-wise classification, while CNNs capture spatial dependencies by"}, {"title": "Related Works", "content": "Oil-water classification using hyperspectral images (HSI) has gained increasing attention due to its environmental significance, particularly in detecting and monitoring oil spills. This section focuses on recent advancements in HSI classification, with an emphasis on works closely aligned with the Hyperspectral Oil Spill Detection (HOSD) dataset, introduced by Puhong Duan.Duan et al. [2023]\nThe HOSD dataset, contains 18 hyperspectral images captured by the ARVIS sensor during the Deepwater Horizon oil spill. These images cover a spectral range of 365 nm to 2500 nm and are accompanied by reference maps that classify each pixel as either oil or non-oil. It is a valuable resource for advancing oil-water classification research.\nEarly work on the HOSD dataset explored unsupervised detection techniques for pixel level classification, such as the isolation forest (iForest)-based framework proposed by Duan et al. [2023]. This method employed a Gaussian statistical model to preprocess noisy spectral bands, followed by dimensionality reduction using kernel principal component analysis (KPCA). Probabilistic outputs from the iForest were refined using clustering algorithms and a support vector machine (SVM), achieving competitive image-wise classification accuracy. While effective, the approach did not integrate spatial relationships, which are critical for context-aware analysis in oil-water classification Duan et al. [2023].\nTo address the limitations of pixel-wise methods, multiscale spectral-spatial learning frameworks have emerged as a promising direction for HSI classification. A notable contribution in this area is the multiscale spectral-spatial CNN (HyMSCN), which introduced an image-based classification framework designed to improve processing efficiency by integrating features from multiple receptive fields Xu et al. [2021]. Unlike patch-based approaches, this method minimized redundancy in testing and effectively fused multiscale features to enhance classification accuracy. While the approach achieved strong performance on general-purpose hyperspectral datasets, it was not tailored for domain-specific datasets like HOSD, highlighting the need for specialized frameworks that address the unique spectral and spatial characteristics of oil spill imagery Xu et al. [2021].\nBuilding upon the strengths of spectral-spatial integration, some efforts have emphasized the importance of contextual learning in HSI classification. The contextual CNN(HybridSN) proposed by Zhang et al. [2020] demonstrated how a multi-scale convolutional filter bank could effectively exploit spatio-spectral relationships, producing a unified feature map for accurate pixel-wise classification. By leveraging deeper and wider architectures, this approach achieved high-ranking performance on standard datasets, such as Indian Pines and Salinas, underscoring the potential of contextual learning in enhancing classification accuracy Zhang et al. [2020].\nWhile these approaches provide valuable insights, our method simplifies the training process by adopting a CNN on 2D probabilistic images generated by Random Forest rather than directly processing 3D hyperspectral data. This hybrid framework bridges the gap between probabilistic modeling and spatial feature learning, achieving superior oil-water classification performance on the HOSD dataset. By combining the strengths of Random Forest in both pixel-level and tile-wise classification with CNNs' capacity to incorporate spatial context, the proposed approach offers a practical and efficient solution for context-aware analysis of hyperspectral images."}, {"title": "Data Preprocessing", "content": ""}, {"title": "Dataset", "content": "The dataset used in this study is the publicly available Hyperspectral Oil Spill Detection (HOSD) dataset Duan et al. [2023], which contains hyperspectral images documenting the Deepwater Horizon oil spill. The dataset has 18 images showing spectra from 365 nm to 2500 nm, labeled as oil or non-oil. The spatial resolution of the images varies due to different flight altitudes during data collection, as shown in Table 1. The dataset's class imbalance (95% water, 5% oil) and the complexity of hyperspectral data present unique challenges, as depicted in Figure 1.\nThe preprocessing steps applied to address these challenges are summarized in the pipeline shown in Figure 3. These steps include noisy channel removal, normalization, dimensionality reduction using PCA, tiling, and data augmentation. Each of these processes is detailed in the following subsections."}, {"title": "Noisy Channel Removal and Normalization", "content": "To improve dataset quality and facilitate dimensionality reduction, we carefully inspected each hyperspectral channel to detect and remove noisy ones. Noisy channels, characterized by low variance and lack of discernible patterns, fail to provide meaningful information for oil spill detection. Figure 2 illustrates examples of noisy and normal channels in the dataset.\nTo ensure consistency across the dataset, noisy channels were identified by intersecting channels marked as noisy across all images. This process resulted in the removal of 31 channels, as listed below:\n[103, 106, 107, 108, 109, 110, 111, 112, 113, 114, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 221, 222, 223].\nAfter removing noisy channels, normalization was applied to ensure that all channels contributed equally to variance. Figure 4 highlights the disparity in value ranges across channels, necessitating normalization."}, {"title": "Dimensionality Reduction", "content": "Although noisy channels were removed during preprocessing, the dataset remained high-dimensional due to the large number of spectral channels, posing challenges for computational efficiency and increasing the risk of overfitting.\nTo address this, Principal Component Analysis (PCA) was applied, reducing the dataset to 32 principal components while preserving 99% of its variance, as shown in Figure 5. This reduction significantly simplified the dataset, retaining essential spectral information and improving its representation of meaningful features. Notably, the removal of noisy channels reduced the number of components required to explain 99% of the variance from 60 to 32, highlighting the benefits of preprocessing."}, {"title": "Tiling and Data Splitting into Train, Validation, and Test Sets", "content": "To ensure robust training and evaluation, we divided each hyperspectral image into smaller, non-overlapping tiles of size 64*64, instead of assigning entire images to specific splits. This approach avoided overfitting, as the model was not confined to learning the context of some images while leaving others unrepresented.\nAfter tiling, the tiles were shuffled, ensuring a fair distribution of data across the training, validation, and test sets. The shuffled tiles were then allocated as follows: 60% for training, 20% for validation, and 20% for testing. This method distributed spatial contexts from different regions of the dataset into each subset.\nFor edge tiles, which contained partially empty spaces due to the dimensions of the original images, we applied a padding strategy. These empty spaces were filled with a constant value calculated as the average reflectance of water for the corresponding channels in the affected tile. This ensured data consistency while preserving the integrity of the dataset."}, {"title": "Data Augmentation", "content": "To address the challenge of data scarcity, despite the dataset being relatively larger, in terms of number of images, compared to those used in similar studies, we applied effective data augmentation techniques: rotation, flipping, and Gaussian noise. The primary objective of these augmentations was to increase the diversity of the dataset while ensuring that no artificial patterns or anomalies were introduced, which could otherwise interfere with the model's learning process.\nAn interesting and deliberate choice in our augmentation strategy was to exclude water-only tiles from augmentation. This decision was driven by the significant class imbalance in the dataset, where approximately 95% of the labels represented water and only 5% represented oil spills before augmentation. By focusing augmentation efforts solely on tiles containing oil spills, we aimed to improve the balance within the training set and enhance the model's ability to recognize minority-class patterns. As a result, the proportion of oil tiles in the training set increased to approximately 10%, compared to 5% in the validation and test sets, which remained representative of the real-world distribution.\nThe final dataset distribution of tiles is as follows:\n\u2022 Training set: 6,804 tiles\n\u2022 Validation set: 759 tiles\n\u2022 Test set: 780 tiles"}, {"title": "Methodology", "content": ""}, {"title": "Model Choice", "content": "In designing our hybrid framework, we carefully selected two complementary models: Random Forest (RF) and Convolutional Neural Networks (CNNs), each addressing specific challenges of hyperspectral image (HSI) analysis.\nRandom Forest was chosen as the primary model due to its robustness in handling high-dimensional data and its ability to process noisy data effectively. Our dataset, despite being reduced is still high dimensional, and RF's ensemble-based architecture excels in isolating meaningful features from such complex data without requiring extensive parameter tuning. Additionally, RF performs well even with relatively small datasets, which is a critical factor given the scarcity of labeled HSI datasets.\nConversely, CNNs were incorporated to address a key limitation of Random Forest: the lack of spatial awareness. RF operates on individual pixels without considering relationships between neighboring pixels, which can lead to fragmented or inconsistent predictions. CNNs, with their convolutional layers, are adept at capturing spatial context, enabling the model to learn patterns and structures that span multiple pixels. This capability is particularly important for HSIs, where spatial features often complement spectral information in distinguishing oil spills from water.\nThe hybrid framework leverages the strengths of both models, combining RF's ability to handle high-dimensional noisy data with CNN's capacity to refine predictions by incorporating spatial context. An overview of the hybrid framework is illustrated in Figure 6, which shows how the probabilistic outputs of RF are utilized as inputs for CNN to produce refined predictions."}, {"title": "Training the Random Forest Model", "content": "The dataset was flattened to ensure compatibility with the Random Forest (RF) model. Each pixel, represented by 32 principal components was treated as an independent data point. The RF model was trained to perform pixel-wise classification, predicting the likelihood of each pixel belonging to the \"oil\u201d or \u201cnon-oil\u201d class.\nRandom Forest was configured with the following hyperparameters:\n\u2022 Number of Trees: 100 ensures a balance between model accuracy and computational efficiency.\n\u2022 Random State: 42 ensures reproducibility of results.\n\u2022 Max Features: sqrt uses the square root of the number of features for splitting, a commonly effective default.\nthe model achieved reliable results with the baseline hyperparameters, demonstrating its robustness. However, due to resource constraints, we did not perform hyperparameter fine-tuning. The dataset's size and memory requirements posed a challenge, as RF's ensemble nature involves maintaining multiple decision trees in memory simultaneously."}, {"title": "CNN Architecutre and Training", "content": ""}, {"title": "Architecture", "content": "In the second phase of our hybrid framework, we employ a Convolutional Neural Network (CNN) to refine the probabilistic predictions generated by the Random Forest (RF) model.\nThe CNN architecture consists of three main components:\n1. Downsampling Path (Encoder): The encoder extracts hierarchical features from the input probability maps using convolutional layers with ReLU activations and same padding to retain spatial dimensions. Max-pooling is applied to progressively reduce spatial resolution, allowing the network to capture broader contextual information.\n2. Upsampling Path (Decoder): The decoder restores the spatial resolution of feature maps using transposed convolutional layers. Skip connections directly link encoder and decoder features, enabling the integration of low-level spatial details with high-level abstractions. This enhances the refinement of predictions.\n3. Output Layer: A final convolutional layer with a sigmoid activation function produces a refined probability map, where each pixel represents the likelihood of belonging to the \"oil\" class."}, {"title": "Training", "content": "The trained RF model generated probabilistic predictions for the validation and test sets, which were then reshaped into 64\u00d764 tiles matching the CNN's input format. To ensure unbiased training, the validation set's prediction maps were split into a new training and validation set, with 80% allocated for training and 20% for validation. This approach prevented the CNN from training on biased probability maps. Since the RF model had already seen the original training data, using its predictions from that set would have created overly optimistic maps too closely aligned with the ground truth. Instead, RF predictions on the validation set provided unbiased and realistic probability maps that were unseen during the RF model's training. By training the CNN on these representative predictions, the model learned to refine its outputs effectively. The best-performing model was selected based on its validation AUC score. To mitigate overfitting, training was limited to 50 epochs due to the small number of tiles, which increased the risk of memorization."}, {"title": "Metrics", "content": "To assess the performance of our hybrid framework, we used five metrics: Accuracy, Recall, Precision, F1-Score, and AUC-ROC. However, due to the severe imbalance in the dataset (95% water and 5% oil), we prioritized F1-Score and AUC over accuracy, as the latter can be misleading in such scenarios. For example, a model predicting all labels as \"water\" would achieve a 95% accuracy but fail entirely to detect oil.\nF1-Score, the harmonic mean of precision and recall, was emphasized because it balances the trade-off between detecting oil pixels (recall) and minimizing false positives (precision). The AUC-ROC, which measures the model's ability to distinguish between classes across various thresholds, complements F1-Score by providing an overall measure of classification performance.\nThe formulas for the key metrics are as follows:\nPrecision = $\\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Positives (FP)}}$ (2)"}, {"title": "", "content": "Recall = $\\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}}$ (3)\nF1-Score = 2$\\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision + Recall}}$ (4)\nAUC = $\\int_{0}^{1} TPR(FPR) \\, d(FPR)$ (5)\nWhere:\n\u2022 TPR is the True Positive Rate, defined as $\\frac{TP}{TP+FN}$.\n\u2022 FPR is the False Positive Rate, defined as $\\frac{FP}{FP+TN}$.\nBy focusing on F1-Score and AUC, we ensured that the evaluation reflected the model's ability to balance precision and recall while effectively detecting the minority class (oil) in the highly imbalanced dataset."}, {"title": "Experiments and Results", "content": "In this section, we present the results of various models tested on the Hyperspectral Oil Spill Detection (HOSD) dataset. The primary goal of these experiments was to evaluate the performance of different methods, including Random Forest (RF), XGBoost (XGB), HybridSN, and Attention-based U-Net, and to compare them with our hybrid RF+CNN approach."}, {"title": "Baseline and Comparison Models", "content": "Baseline Model (Random Forest): Random Forest served as our baseline model due to its robustness in handling high-dimensional data and small datasets. It provided a strong starting point for pixel-wise classification, demonstrating high accuracy and acceptable F1 and AUC scores despite its lack of spatial awareness.\nXGBoost (XGB): We tested XGBoost as an alternative ensemble learning method, but its performance was slightly lower than RF. XGB demonstrated effective feature handling but did not significantly outperform RF in terms of F1 and AUC.\nAttention-Based U-Net: To evaluate whether a complex spatial-spectral model could perform better, we tested an attention-based U-Net. However, its performance was suboptimal due to the small dataset size, leading to overfitting and reduced generalization.\nHybridSN: HybridSN, a spectral-spatial deep learning model proposed in prior researches, was also tested. While it outperformed the attention-based U-Net, it underperformed the Random Forest baseline model.\nProposed Hybrid RF+CNN: Our hybrid RF+CNN model was designed to combine the strengths of both RF and CNNs. This approach not only outperformed the baseline but also demonstrated improved consistency across tiles compared to the base model."}, {"title": "Results", "content": "The performance of all models is summarized in Table 2. While RF achieved competitive results, our hybrid RF+CNN approach outperformed it in key metrics such as F1 and AUC, demonstrating the effectiveness of incorporating spatial context."}, {"title": "Consistency Analysis", "content": "In addition to average performance metrics, we evaluated the consistency of predictions across tiles using F1 distributions. Analyzing tile-wise prediction histograms is important because average metrics, such as overall F1 score, can sometimes mask inconsistencies. A model may perform exceptionally well in certain tiles while failing in others, resulting in a high average performance but poor reliability in specific contexts. By examining the distribution of F1 scores, we gain deeper insights into how often the model struggles to make accurate predictions, enabling us to identify and address the contexts where the model is less effective.This analysis is particularly critical for hyperspectral oil spill detection, where environmental conditions and data quality vary widely across regions. Figure 7 illustrates the distribution of F1 scores for tiles predicted by the RF baseline and the RF+CNN hybrid approach.\nBaseline RF: Approximately 32% of tiles achieved F1 scores below 0.7.\nHybrid RF+CNN: With the hybrid approach, the proportion of tiles with F1 scores below 0.7 dropped to 24%.\nThe decrease in tiles with low F1 scores in the hybrid model shows how well RF's ability to handle spectral features and CNN's spatial context awareness work together. This improvement is vital for real-world applications, where consistent performance in different environmental conditions is key to reliable oil spill detection."}, {"title": "Conclusion", "content": "This study introduced a hybrid framework combining Random Forest (RF) and a Convolutional Neural Network (CNN) for hyperspectral oil spill detection. By leveraging RF's robustness in handling high-dimensional, noisy data and CNN's ability to capture spatial context, the framework effectively addressed the challenges of class imbalance and limited data.\nExperimental results demonstrated that the hybrid RF+CNN model consistently outperformed other tested methods, including XGBoost, HybridSN, and attention-based U-Net, achieving higher F1-Score and AUC\u2014key metrics for imbalanced datasets. Additionally, the hybrid model showed improved consistency across tiles, significantly reducing the proportion of low F1 scores compared to the RF baseline.\nThis framework highlights the potential of combining spectral and spatial features for effective hyperspectral image classification. Future work could explore the integration of advanced spatial-spectral techniques or domain-specific data augmentation strategies to further enhance the model's robustness and generalization. Moreover, scaling this approach to larger datasets or real-time monitoring scenarios could unlock new possibilities for environmental applications and remote sensing."}]}