{"title": "Skeleton-based Action Recognition with Non-linear Dependency Modeling and Hilbert-Schmidt Independence Criterion", "authors": ["Haipeng Chen", "Yuheng Yang", "Yingda Lyu"], "abstract": "Human skeleton-based action recognition has long been an indispensable aspect of artificial intelligence. Current state-of-the-art methods tend to consider only the dependencies between connected skeletal joints, limiting their ability to capture non-linear dependencies between physically distant joints. Moreover, most existing approaches distinguish action classes by estimating the probability density of motion representations, yet the high-dimensional nature of human motions invokes inherent difficulties in accomplishing such measurements. In this paper, we seek to tackle these challenges from two directions: (1) We propose a novel dependency refinement approach that explicitly models dependencies between any pair of joints, effectively transcending the limitations imposed by joint distance. (2) We further propose a framework that utilizes the Hilbert-Schmidt Independence Criterion to differentiate action classes without being affected by data dimensionality, and mathematically derive learning objectives guaranteeing precise recognition. Empirically, our approach sets the state-of-the-art performance on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA datasets.", "sections": [{"title": "Introduction", "content": "Endowing machines with the ability to perceive and recognize human behaviors is very much coveted for various applications ranging from virtual reality to security monitoring (Liu et al. 2022b; Wu et al. 2024b). Consequently, action recognition has attracted much interest, particularly for methods that rely on skeletal data, which is robust against environmental noise and viewpoint changes.\nRecent skeleton-based approaches (Yan et al. 2018; Shi et al. 2019a,b) tend to employ Graph Convolutional Networks (GCNs) to model human motion patterns since the hierarchical and tree-like graph structure naturally in the human skeleton. For instance, (Xu et al. 2022) attempts to design sophisticated adjacency matrices, seeking to pursue more nuanced modeling of spatial joint dependencies. (Chi et al. 2022; Zhou et al. 2023) put their efforts into learning discriminative motion features from the skeleton sequence. (Yang et al. 2023) adopts an information-theoretic objective to fully mine task-relevant information while reducing task-irrelevant nuisances.\nUnfortunately, after a systematic investigation of prior works, we observe that they still suffer from inaccurate action recognition. We conjecture that the reasons are two folds. (1) First, current methods (Yan et al. 2018; Li et al. 2019) typically model human motions by performing graph convolutions on pre-defined skeleton graphs. However, these approaches exhibit a severe limitation as they consider only the dependency between physically connected joints while ignoring non-linear dependencies between geometrically separated joints. Although several works (Xu et al. 2022; Song et al. 2022) tend to address this through hierarchical graphs or scaling graphs, they still struggle to effectively model non-linear dependencies between distant joints due to the limited receptive field of graph convolutions. (2) Second, given the motion feature representation, existing approaches (Chi et al. 2022; Yang et al. 2023; Zhou et al. 2023; Huang et al. 2023) typically estimate its probability density in conjunction with the category label to identify the action class. A higher probability density indicates a stronger likelihood of the motion feature belonging to the correct class. Nevertheless, a motion sequence is characterized as a time series of skeletal poses, each of which translates to the positions of all joints, resulting in a high-dimensional representation. Estimating the probability density of these high-dimensional representations in the raw Euclidean space introduces unnecessary training difficulty, which leads to reduced accuracy in action classification.\nIn this paper, we embrace two key components to tackle these challenges. Technically, (i) We propose a dependency refinement method that strips the skeletal structure down to dynamically zoom in on the joint-to-joint dependencies of pairwise skeletal joints. Specifically, setting aside the skeletal structure, we explicitly model each pair of joint dependencies with a Gaussian correlation function. By adjusting the kernel width in the Gaussian correlation function, we could fine-grainedly control the influence of distance on joint dependencies. These dependencies are then used to adaptively refine the initial skeletal graph, enabling precise human motion modeling. We also employ an ensemble of networks trained with different kernel widths, seeking to improve the comprehensiveness and accuracy of action recognition. (ii) Based on the aforementioned method, we further propose a novel framework that leverages the Hilbert-Schmidt Independence Criterion (HSIC) for facilitating action recognition. First, we utilize a Hilbert kernel function to map high-dimensional motion features from the straightforward Euclidean space to a Hilbert space, as illustrated in Fig. 1. In this space, the HSIC mathematically evaluates the statistical dependence between these features and the corresponding action labels. Second, we theoretically derive the learning objectives, guaranteeing the efficacy of the final action classification. We would like to point out that, since HSIC is defined in terms of kernel method, the process of distinguishing action classes operates in a dimension-independent manner.\nThereafter, we conduct extensive experiments on three popular benchmark datasets, namely the NTU RGB+D 60 dataset, the NTU RGB+D 120 dataset, and the Northwestern-UCLA dataset. The empirical results show that our approach consistently and significantly outperforms state-of-the-art performance. To summarize, our key contributions are as follows:\n\u2022 A dependency refinement method is presented to comprehensively learn the relationships between joints, which simultaneously considers the skeletal connection between adjacent joints and the non-linear dependencies between distant joints.\n\u2022 We propose a novel action recognition framework, which effectively distinguishes action classes among high-dimensional motion feature representations while deriving learning objectives to ensure the efficacy of the action classification.\n\u2022 Our method outperforms existing approaches and achieves state-of-the-art performance on three popular benchmark datasets. The implementations have been released, hoping to facilitate future research."}, {"title": "2 Related Work", "content": "Skeleton-based action recognition. Previous methods tackle the skeleton-based action recognition task by utilizing Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs) (Liu et al. 2016, 2019b), which unfortunately ignore the inherent relationships between joints.\nRecently, there has been an increasing interest in developing Graph Convolutional Networks (GCNs) (Yan et al. 2018) since the advantages in dealing with irregular graphical structures. FR-Head (Zhou et al. 2023) introduces an auxiliary feature refinement head to acquire discriminative representations of skeletons, aiding in distinguishing ambiguous actions. Stream-GCN (Yang et al. 2023) engages the mutual information loss to maximize the task-relevant information while minimizing the task-irrelevant nuisances for facilitating the action recognition. SkeletonGCL (Huang et al. 2023) introduces graph contrast learning to explore the cross-sequence global context in a fully supervised setting.\nHilbert-Schmidt independence criterion. The Hilbert-Schmidt Independence Criterion (HSIC) is a statistical measure used to assess the independence between two random variables (Gretton et al. 2005). HSIC maps input data into feature vectors and computes their product to evaluate correlation. Its strength lies in uncovering intricate data correlations within a Reproducing Kernel Hilbert Space (RKHS) without being constrained by the dimensionality of the input data (Bertsimas and Koduri 2022). For instance, HSIC-InfoGAN (Liu et al. 2022a) learns unsupervised disentangled representations by directly optimizing the Hilbert-Schmidt Independence Criterion (HSIC) loss, eliminating the need for an auxiliary network. To the best of our knowledge, there is no related research on the utilization of HSIC in the skeleton-based action recognition task."}, {"title": "3 Methodology", "content": "Notations. Generally, the human skeleton can be regarded as an interlinked structure comprised of joints and bones. We denote the set of joints as a set of nodes $V = \\{v_1,..., v_N\\}$. Additionally, we represent the set of bones as a set of edges $E$, which can be formulated as an adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$. Hence, we conveniently depict a human skeleton as a graph $G = (V, E)$. An action can be represented as a sequence of skeleton poses $X = (X_1,X_2, ..., X_T)$, where $x_t$ denotes the skeleton pose at time $t$ and $T$ is the number of frames. Since $x_t \\in \\mathbb{R}^{N \\times C}$, where $N$ is the number of joints, $C$ denotes the dimension of a joint, the 3D motion sequence $X$ can be formulated as a feature tensor $X \\in \\mathbb{R}^{T \\times N \\times C}$. Presented with a 3D skeleton motion sequence $X$, we aim to accurately predict its action class label $y$.\nNon-linear Dependency Modeling\nCurrent works (Shi et al. 2019b; Liu et al. 2020; Cheng et al. 2020a) typically learn human motions by modeling the dependencies between adjacent joints. Unfortunately, these methods ignore the relationships between geometrically separated joints. While some approaches introduce adaptive graphs or hierarchical graphs (Lee et al. 2023; Wu et al. 2024a) to mitigate such problems, they are known to have difficulties in capturing the non-linear dependencies between distant joints due to the limited receptive field of graph convolutions. Motivated by these insights, we propose a novel dependency refinement method designed to overcome the limitations imposed by joint distance, allowing for precise human motion modeling. In what follows, we will delve into the specifics of our approach."}, {"title": "implementation:", "content": "$X^{(s)} = \\sigma \\big( \\hat{D}^{-\\frac{1}{2}} (A_c + I) \\hat{D}^{-\\frac{1}{2}} X W \\big),$ (4)\nwhere $A_c = A+WR$, $W$ denotes the learnable parameters for the dependency, $R \\in \\mathbb{R}^{N \\times N \\times C'}$ denotes the dependency matrix consists of $r_{ij}$, and $A_c$ is the novel adjacency matrix obtained in a broadcast manner."}, {"title": "Distinguishing Action Classes For Motion Feature Representations", "content": "Many existing methods (Yang et al. 2023; Huang et al. 2023; Zhou et al. 2023) advocate for distinguishing action classes by estimating probability density among multiple motion feature representations. However, factors in the motion sequence, such as time series, 3D joint coordinates, and the number of joints, result in a high-dimensional feature representation. In such a high-dimensional space, feature representations tend to be sparsely distributed, leaving much of the sampling space unfilled, which makes it difficult to accurately classify the action classes (Majdara and Nooshabadi 2022).\nTo tackle this problem, we propose a framework based on the Hilbert-Schmidt Independence Criterion. As shown in Fig. 3, our action recognition framework includes a base model for generating motion features and an auxiliary model for producing auxiliary motion information. By incorporating the auxiliary motion information into the motion features, we could explicitly enhance their discriminative ability. These enhanced features are then mapped from Euclidean space to Hilbert space using a Hilbert Reproducing Kernel Function, transforming them into kernel matrices. Finally, we utilize HSIC values derived from these kernel matrices to identify action classes and mathematically formulate training objectives, ensuring effective learning without being hindered by high dimensionality.\nFormally, our approach initiates by performing joint dependency refinement. Then, the base model is trained to encode the motion sequence $X$ into the motion feature $z$. Meanwhile, the auxiliary model encodes $X$ into $\\tilde{z}$, which is then fed into the classifier to predict motion information $\\tilde{y}$. Thereafter, we incorporate $\\tilde{y}$ into $z$ to obtain the enhanced feature $\\hat{z}$ and utilize the Mat\u00e9rn covariance function as the Hilbert Reproducing Kernel Function to map $\\hat{z}$ into Hilbert Space. The kernel function is:\n$K_\\eta(z_u, z_w) = \\frac{\\alpha}{2^{\\eta-1}\\Gamma(\\eta)} \\big(\\frac{\\sqrt{2\\eta} r}{l}\\big)^\\eta K_\\eta \\big(\\frac{\\sqrt{2\\eta} r}{l}\\big), (5)$\nwhere $\\eta > 0$ is the kernel order, $F_\\eta$ is the modified Bessel function of the second kind, $\\alpha > 0$ and $l > 0$ denote the amplitude and length scale, $\\Gamma(\\eta)$ is the normalization factor, and $r = || z_u - z_w||_2$. $\\{(z_u, z_w)\\}_{u,w=1}^n$ denote $u$th and $w$th feature samples. The particular case where $\\eta = 2$ is probably the most commomly used kernel (Williams and Rasmussen 2006):\n$k_\\eta(z_u, z_w) = \\alpha \\big(1 + \\frac{\\sqrt{3}r}{l} \\big) \\exp(-\\frac{\\sqrt{3}r}{l}).$ (6)"}, {"title": "Content", "content": "Subsequently, we could obtain the kernel matrix $K_z$, which is defined as follows:\n$K_z = \\begin{pmatrix}\nk_\\eta(\\hat{z}_1, \\hat{z}_1) & k_\\eta(\\hat{z}_1, \\hat{z}_n) \\\\\nk_\\eta(\\hat{z}_2, \\hat{z}_1) & k_\\eta(\\hat{z}_2, \\hat{z}_n) \\\\\n\\vdots & \\vdots \\\\\nk_\\eta(\\hat{z}_n, \\hat{z}_1) & k_\\eta(\\hat{z}_n, \\hat{z}_n)\n\\end{pmatrix}, (7)$\nwhere each entry $k_\\eta(z_u, z_w)$ in Equation (7) denotes the pair-wise kernel value between $z_u$ and $z_w$. Finally, by centering the kernel matrix as described in Equation (7), we calculate the HSIC value, which helps improve the classification efficacy for $\\hat{z}$:\n$HSIC(\\hat{z}, y) = \\frac{1}{(n-1)^2} tr(K_\\hat{z} H K_y H), (8)$\nwhere $H = I - \\frac{1}{n} 11^T$ is the centering matrix, $tr(\\cdot)$ denotes the trace operation, $y$ is the class label, and $K_y$ denotes the kernel matrix of $y$. It is important to note that the HSIC value computation, achieved through the inner product of kernel matrices, is inherently dimension-agnostic. Thus we can derive the learning objective combined with Equation (8):\n$L_H = L_{cls} + HSIC(\\hat{z}, y), (9)$\nwhere $L_H$ is the HSIC-based learning objective and $L_{cls}$ denotes the empirical classification loss (Chi et al. 2022).\nUp to now, we have arrived at the HSIC-based learning objective in Equation (9). Besides this, we further engage in the distillation loss to supervise the alignment between the base model and the auxiliary model. Specifically, to supervise the knowledge distillation from the auxiliary model to the base model, we adopt the distillation method of (Yun et al. 2020). Our distillation loss is defined as:\n$L_D = KL(\\sigma(f(z)/P)||\\sigma(f_\\theta(\\tilde{z})/P)), (10)$\nwhere $KL$ is Kullback-Leibler Divergence, $\\sigma$ is the softmax activation, $f(\\cdot)$ represents the logit of each model, $\\theta$ and $\\phi$ denote the parameters of the base model and the auxiliary model, and $P > 0$ is the distillation temperature. On the grounds of distillation loss, we incorporate explicit supervision for the auxiliary model using the cross-entropy loss $L_{CE}$. The entire training objective of our framework is defined as follows:\n$L_{Total} = L_H + L_{CE} + L_D.$ (11)\nMulti-Stream Ensemble\nAs mentioned earlier, the Gaussian kernel width can regulate the influence of distance on joint dependencies. Therefore, we advocate for training the proposed frameworks with multiple kernel widths to finely capture the non-linear dependencies in different action types. Specifically, a large kernel width is more suitable for modeling actions highlighting distant joint collaborations, such as the \u201cKicking\u201d action. In contrast, a small kernel width is adaptable for modeling the dependencies between adjacent joints, such as the localized relationship between the head and neck joints during the \"Sneeze\" action. To this end, we propose feeding the frameworks with joint and bone inputs, each of which is"}, {"title": ""}]}