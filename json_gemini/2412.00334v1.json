{"title": "EFTVIT: Efficient Federated Training of Vision Transformers with Masked Images on Resource-Constrained Edge Devices", "authors": ["Meihan Wu", "Tao Chang", "Cui Miao", "Jie Zhou", "Chun Li", "Xiangyu Xu", "Ming Li", "Xiaodong Wang"], "abstract": "Federated learning research has recently shifted from Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs) due to their superior capacity. ViTs training demands higher computational resources due to the lack of 2D inductive biases inherent in CNNs. However, efficient federated training of ViTs on resource-constrained edge devices remains unexplored in the community. In this paper, we propose EFTViT, a hierarchical federated framework that leverages masked images to enable efficient, full-parameter training on resource-constrained edge devices, offering substantial benefits for learning on heterogeneous data. In general, we patchify images and randomly mask a portion of the patches, observing that excluding them from training has minimal impact on performance while substantially reducing computation costs and enhancing data content privacy protection. Specifically, EFTViT comprises a series of lightweight local modules and a larger global module, updated independently on clients and the central server, respectively. The local modules are trained on masked image patches, while the global module is trained on intermediate patch features uploaded from the local client, balanced through a proposed median sampling strategy to erase client data distribution privacy. We analyze the computational complexity and privacy protection of EFTViT. Extensive experiments on popular benchmarks show that EFTViT achieves up to 28.17% accuracy improvement, reduces local training computational cost by up to 2.8x, and cuts local training time by up to 4.4\u00d7 compared to existing methods.", "sections": [{"title": "1. Introduction", "content": "Federated Learning (FL) targets to enable collaborative training across multiple data distributed among different clients while prioritizing data privacy protection [21, 24, 25]. Early research on FL primarily concentrates on Convolutional Neural Networks (CNNs) [1, 20, 22]. Recently, the focus has increasingly shifted toward Vision Transformers (ViTs) [8], whose self-attention mechanisms excel at capturing long-range correspondences within images, achieving state-of-the-art performance across visual problems, e.g., object recognition [8], detection [6, 13], and semantic segmentation [40]. Despite their impressive capabilities, training ViTs generally incurs significantly higher computational costs and longer training times due to the lack of spatial inductive biases within images [3, 30], making it prohibitively challenging for resource-constrained edge devices.\nIn the CNN era, the resource-constrained FL problem has been explored by some researchers. The workflow of these methods is summarized in Figure 1a. Typically, model-heterogeneous methods [1, 4, 23, 37] train models of"}, {"title": "2. Related Works", "content": "2.1. General Federated Learning\nFederated learning is a decentralized machine learning approach that enhances privacy by training models directly on client devices, only transmitting model parameters to a central server. Most studies focus on addressing data heterogeneity [11, 17, 20, 22] and privacy protection [2, 5, 27] in FL. For instance, FedProx [22] adds a proximal term to optimize the local updates for addressing data heterogeneity. Regarding privacy protection, Asad et al. [2] apply homomorphic encryption to FL, enabling clients to encrypt their local models using private keys. Shi et al. [27] propose a FL method with differential privacy (DP). However, these works rely on the ideal assumption that clients have sufficient resources to handle model training process.\n2.2. Federated Learning on Edge Devices\nFederated learning approaches on resource-constrained clients can be categorized into federated distillation"}, {"title": "2.3. Parameter-Efficient Fine-Tuning", "content": "When dealing with transformer-based complex models, Parameter-Efficient Fine-Tuning (PEFT) [14, 16, 36] provides a practical solution for efficiently adapting pre-trained models across the various downstream tasks, which can reduce storage and computation costs by fixing most pre-trained parameters and fine-tuning only a small subset [10]. Several studies [29, 38] have explored using different PEFT techniques to assess performance improvements and resource savings in federated systems. However, the limited fine-tuning of parameters in PEFT inevitably constrains the adaptability of pre-trained models to new tasks, potentially resulting in suboptimal performance in federated systems with data heterogeneity."}, {"title": "3. Efficient Federated Learning with Masked Images", "content": "3.1. Problem Definition\nWe employ supervised classification tasks distributed across K clients to formulate our problem. Each client k possesses a dataset \\(D_k := (X_k, Y_k)\\), where \\(X_k \\in \\mathbb{R}^{N_k \\times d_k}\\) denotes the data samples and \\(Y_k \\in \\mathbb{R}^{N_k \\times c_k}\\) represents their corresponding labels. Here, \\(N_k\\) represents the number of data points, \\(d_k\\) denotes the input dimension, and \\(c_k\\) indicates the number of classes.\n3.2. Overview\nAs illustrated in Figure 3, EFTViT employs hierarchical training across clients and the central server to enable privacy-preserving and efficient collaborative learning. Each client includes a local module with M Transformer layers, a shared global module with N Transformer layers, and a classification head. The local module and classification head are trained on each client with unmasked image patches \\(X_p\\), enabling efficient local training and generating patch features that represent local knowledge. To safeguard data distribution privacy, a median sampling strategy is applied on each client to create a balanced patch features"}, {"title": "3.3. Training with Masked Images", "content": "To enable efficient local training on resource-constrained clients, we present a patch-wise optimization strategy. Firstly, each input image is divided into a sequence of regular, non-overlapping patches, which are randomly masked at a ratio \\(r_m\\). The remaining unmasked patches, denoted as \\(X_p\\), are then used to train our framework. We define the patch features obtained by the local module on the client k \\(M_k(\\phi_k; \\cdot)\\) as \\(H_k = M_k(\\phi_k; X_p)\\), where \\(X_p = Mask\\_D(X_k)\\) and \\(Mask\\_D(X_k)\\) is the operation of randomly masking image patches from \\(X_k\\) and discarding the selected patches. To preserve patch ordering for ViTs, the positional embeddings [28] of the remaining patches are retained. This is inspired by the internal redundancy of images and reduces the amount of data that the model needs to process, thereby lowering computational complexity. Additionally, these patch features \\(H_k\\) make it pretty challenging to reconstruct the original images since they are encoded from a very small portion of each image, inherently providing EFTViT with a content privacy advantage. Notably, the entire images are adopted for the inference on each client."}, {"title": "3.4. Data Distribution Protection with Median Sampling", "content": "To enhance privacy in EFTViT, we propose a median sampling strategy to generate a balanced patch features dataset \\(D_H\\) on each client. It aims to ensure that the generated patch features on each client contain an equal number of samples for each class, thereby preventing the leakage of statistical information or user preferences when uploaded to the central server. Imbalanced data distribution on clients is a common issue in federated learning, and the median, being less sensitive to extreme values, is well-suited for addressing this challenge. Our median sampling strategy uses the median of class sample counts on each client to differentiate between minority and majority classes. It then applies oversampling to increase samples of minority classes and downsampling to reduce samples of majority classes. Specifically, for minority class samples, all patch features generated across multiple local training epochs are retained, whereas for majority class samples, only patch features from the final epoch are preserved. Next, downsampling is applied to reduce the number of samples in each class to the median. Empirically, we find that increasing the sampling threshold adds to computation costs but does not significantly improve final performance."}, {"title": "3.5. Hierarchical Training Paradigm", "content": "To effectively reduce the computational burden on clients without compromising performance, we propose a new hierarchical training strategy for ViTs that minimizes the number of trainable parameters on the clients. As aforementioned, our ViT models comprise a collection of lightweight local modules, a shared large global module and a classification head.\nTraining on Clients. On the client k, the local module \\(M_k(\\phi_k; \\cdot)\\) is responsible for mapping image patches \\(X_p\\) into patch features \\(H_p\\), while the global module \\(M_k(\\omega; \\cdot)\\) encodes \\(H_p\\) into representation vectors \\(H_r\\). The final classification head \\(M_k(\\theta_k; \\cdot)\\) transforms the representation vectors \\(H_r\\) to match the number of classes. Only the parameters of the local module and classification head are trainable, while the parameters of the global module remain frozen and are iteratively updated via downloads from the server. For the client k, the loss function used in local training is defined as\n\\[\\mathcal{L}(\\phi_k, \\theta_k) = \\sum_{i=1}^{c_k} p(y = i) \\log(M_k(\\phi_k, \\omega, \\theta_k; X_p)), \\quad (1)\\]\nwhere \\(c_k\\) is the number of classes in client k, and \\(p(y = i)\\) is the probability distribution of label i. The parameters \\(\\phi_k\\), \\(\\omega\\), \\(\\theta_k\\) are from the local module, global module, and classification head, respectively. Therefore, the optimization objective is to minimize\n\\[\\min_{\\Phi_k, \\theta_k} \\mathbb{E}_{X_p \\sim D_K} [\\mathcal{L}(M_k(\\phi_k, \\omega, \\theta_k; X_p), Y_k)], \\quad (2)\\]\nwhere \\(\\Phi_k\\) and \\(\\theta_k\\) are trainable.\nTraining on Server. The server aggregates heterogeneous knowledge from clients to learn universal representations across diverse tasks. The global module \\(M_s(\\omega; \\cdot)\\) and classification head \\(M_s(\\theta; \\cdot)\\) are trained using the balanced patch features dataset uploaded from participating clients in the latest training round. The loss function can be formulated as\n\\[\\mathcal{L}_s(\\omega, \\theta) = \\sum_{i=1}^C p(y = i) \\log(M_s(\\omega, \\theta; H_p)) \\quad (3)\\]\nwhere C is the total number of classes, and \\(p(y = i)\\) is the probability distribution of label i on the data. The optimization objective on the server is to minimize\n\\[\\min_{\\omega, \\theta} \\mathbb{E}_{H_p \\sim D_H} [\\mathcal{L}_s(M_s(\\omega, \\theta; H_p), Y)], \\quad (4)\\]\nwhere \\(H_p\\) and Y represent respective patch features and labels uploaded from clients."}, {"title": "3.6. Collaborative Algorithms", "content": "The overall workflow of our EFTViT is shown in Algorithm 1 and Algorithm 2. At the start of each round t, the server will randomly choose a proportion P from K clients to participate in training. Each client updates the parameters of its global module and classification head with those received from the server, and then initiates local training. The"}, {"title": "3.7. Privacy & Complexity Analysis", "content": "Data Content Privacy. Contrary to previous beliefs, recent studies show that exchanging intermediate features during federated learning training is safer than sharing gradients. This is because attackers only have access to evolving feature maps rather than the final, fully trained maps, making data reconstruction attacks more challenging\nData Distribution Privacy. To protect user statistical information and preferences, our patch features are balanced via the proposed median sampling strategy on clients, ensuring an equal number of samples for each class. Additionally, our strategy is orthogonal to other privacy protection methods, such as Differential Privacy [9], which can be seamlessly integrated into EFTViT to offer enhanced protection against attacks.\nComplexity. Given a ViT model, let (h, w) represent the resolution of original image, (p, p) represent the resolution of each image patch, \\(n = h*w/p^2\\) be the resulting number, d be the latent vector size, and \\(N_T\\) represent the number of Transformer layers. To simplify the calculation, we assume that size of Q, K and V is \\(n \\times d\\). Each client model has \\(N_T\\) Transformer layers, divided into M layers for local module and N layers for global module. The model trains on \\((1 - r_m)\\) of the image patches, where \\(r_m\\) is the masking ratio. The time cost for forward propagation on the client is \\(O(5 \\cdot M \\cdot (1-r_m) \\cdot n \\cdot d^2 + 2 \\cdot N_T \\cdot (1-r_m)^2 \\cdot n^2 \\cdot d)\\). As the parameters of the N Transformer layers in the global module are frozen, the backward propagation time cost is \\(O(10 \\cdot (N_T - N) \\cdot (1-r_m) \\cdot n \\cdot d^2 + 4 \\cdot (N_T - N) \\cdot (1-r_m)^2 \\cdot n^2 \\cdot d)\\). Therefore, the overall time complexity in the client training stage is \\(O((15N_T - 10N) \\cdot (1 - r_m) \\cdot n \\cdot d^2 + (6N_T - 4N) \\cdot (1-r_m)^2 \\cdot n^2 \\cdot d)\\). As N approaches \\(N_T\\) and \\(r_m\\) ap-"}, {"title": "4. Experiments", "content": "4.1. Datasets\nTo comprehensively evaluate EFTViT, we conduct experiments on two widely used federated learning datasets, CIFAR-10 [18] and CIFAR-100 [18], as well as a more challenging datasets, UC Merced Land-Use [34], for remote sensing. CIFAR-10 and CIFAR-100 datasets each contain 60,000 color images. CIFAR-10 is organized into 10 classes, with 6,000 images per class (5,000 for training and 1,000 for testing), while CIFAR-100 has 100 classes, with 600 images per class (500 for training and 100 for testing). UC Merced Land-Use dataset contains 21 land-use classes, e.g., agricultural, forest, freeway, beach, and other classes, each with 100 images (80 for training and 20 for testing). We partition samples to all clients following a Dirichlet distribution \\(Dir_n(\\beta)\\) with a concentration parameter \\(\\beta\\), setting \\(\\beta = {0.1,1}\\) to simulate high or low levels of heterogeneity.\n4.2. Implementations\nWe use ViT-B [8] pre-trained on ImageNet-21K [26] as the backbone of our framework. The input images are resized to 224 x 224 with a patch size of 16 \u00d7 16. During training, data augmentation techniques such as random cropping, flipping, and brightness adjustment are applied. Following federated learning practices, we set the number of clients to 100, with a client selection ratio P = 0.1. The AdamW optimizer is used with an initial learning rate of 5 \u00d7 10-5, weight decay of 0.05, and a cosine annealing learning rate schedule with warm-up. We use a batch size of 32 for both training and testing on each client. All experiments are conducted on a single NVIDIA GeForce RTX 3090 GPU. In each round, clients train for 5 epochs locally, while the server performs an additional 2 epochs. The framework is trained for a total of 200 rounds, requiring approximately 24 hours.\n4.3. Comparison with State-of-the-Art Methods\nGiven the lack of studies on training ViTs on resource-constrained clients, we adapt the FEDBFPT approach [33],"}, {"title": "4.4. Ablation Study", "content": "We conduct extensive ablation experiments to investigate the key components of our approach.\nEffect of Masking Ratio. The masking ratio \\(r_m\\) determines the number of masked image patches. A smaller \\(r_m\\) reduces the amount of input data, thus lowering computational requirements during model training. Table 4 provides the GFLOPs for various masking rates, demonstrating that increasing the masking ratio significantly reduces GFLOPs. However, increasing the masking ratio also affects overall performance. We evaluate the effect of different masking rates for EFTViT. Figure 5 shows the results of EFTViT with varying masking ratios on CIFAR-100 at \\(\\beta\\) = 0.1. Results indicate that EFTViT can support a wide masking ratio range. When the masking ratio increases from 0% to 75%, the accuracy remains larger than 90%. However, the performance decreases heavily when the masking ratio exceeds 75%. Therefore, we select a masking ratio of 75% to strike\na balance between accuracy and computational efficiency.\nEffect of Layer Number M in Local Module. The layer number M determines the trainable parameter division between clients and the server, affecting the computational load of clients and final performance. Table 5 presents the number of trainable parameters (# Params) in each client and the corresponding accuracy achieved by the model for different values of M. The results show that M has minimal impact on the testing accuracy, showcasing the superior robustness of our EFTViT w.r.t. client resources. Given the higher computational cost of a large M on clients and the accuracy decrease, we select M = 2 as the default setting.\nEffect of Sampling Threshold. As elaborated in Section 3.4, the sampling threshold determines the number of balanced patch features to upload for server training. Therefore, a higher threshold increases the training cost on the server. We investigate the impact of utilizing median or higher sampling thresholds in EFTViT, as shown in Figure 6. Results indicate that increasing the threshold provides minimal performance improvements. To enhance the computational efficiency on the server, we select the median as the threshold in our method."}, {"title": "5. Conclusion", "content": "In this work, we propose a hierarchical federated framework, EFTVIT, designed for efficient training on resource-constrained edge devices and handling heterogeneous data effectively. EFTViT reduces client computation by leveraging masked images with an appropriate masking ratio,"}]}