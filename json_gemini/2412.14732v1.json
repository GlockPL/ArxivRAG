{"title": "Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools", "authors": ["James Prather", "Juho Leinonen", "Natalie Kiesler", "Jamie Gorson Benario", "Sam Lau", "Stephen MacNeil", "Narges Norouzi", "Simone Opel", "Vee Pettit", "Leo Porter", "Brent N. Reeves", "Jaromir Savelka", "David H. Smith IV", "Sven Strickroth", "Daniel Zingaro"], "abstract": "Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community.", "sections": [{"title": "1 Introduction", "content": "Computing education is undergoing a seismic shift due to the advances in generative AI (GenAI) [22, 36, 111]. Beginning in early 2022, computing education researchers showed that these models had incredible accuracy solving programming problems and exam questions in multiple courses and contexts [6, 40, 41, 48, 68, 119, 125, 126]. Other early work focused on the ways in which GenAI can provide support to computing educators [16, 37, 81, 82, 94, 124, 127]. Others were quick to raise concerns about potential threats to education, such as over-reliance, bias in the models, and educational misconduct [17, 80, 111, 114]. Computing instructors at the K-12 level are also struggling with integrating GenAI into their curricula [14, 43, 147]. K-12 teachers outside of computing education are having similar conversations to those occurring in higher education [14, 18, 99, 109, 116, 121]. Public perception of GenAI is mixed, partially because it is a \u201cblack box\" and that lack of transparency often increases fear [21], which is one reason why GenAI should be designed to increase transparency to end users [142].\nHowever, the discussion has largely moved from threats, challenges, and opportunities [17, 62, 104, 135, 145] to questions of practical adoption [93, 112]. The 2023 ITiCSE working group on GenAI summarized the activity within the computing education community and suggested that the next step is to determine reliable and safe ways to implement it into computing curricula [111]. An essay released along with the 2023 ACM/IEEE Computing Curricula suggested ways in which this could be accomplished based on preliminary data from a few studies and painted an optimistic picture of the future [16]. Indeed, many within the community are calling AI integration just another step in the advancement of educational technology [1]. Others are discussing how GenAI will change programming competencies and skills in the future [65, 132]. New assessment methods [61] and ways to measure user interaction with GenAI [98] are required, and although some are arguing for making assignments \u201cLLM-proof\u201d [20], which seems to be an impractical goal given the rapid improvement in these models.\nAlthough some have advocated for banning GenAI entirely, that also seems impractical given its free availability to students outside the classroom [80]. Furthermore, professional developers are also discussing the role that GenAI will play within their work [8, 44, 75, 84, 141, 155], lending credibility to the idea that students must be prepared for using it after university. Therefore, thoughtful integration and scaffolding appears to be the way forward [36]. However, there is a lack of helpful and clear terminology to discuss the kinds of classroom interventions conducted so far.\nTo this end, our report attempts to distinguish the following use cases: (1) instructors teaching students about using GenAI tools in order to write code, and (2) instructors using GenAI tools to support the teaching of their course via help-seeking bots, code feedback, assignment creation, etc.\nThe first category is by far the most extensive. Researchers were quick to show that GenAI could automatically enhance programming error messages [82], which was followed with two large-scale replications showing a direct benefit to students [143, 153]. Indeed, GenAI can provide other kinds of advanced and customized help and qualitative feedback to students working on computing assignments [10, 11, 67, 81, 89, 94]. Other more recent work, such as the Harvard CS50 course, has focused on providing programming tutoring and help to students at scale through TA chatbots [87]. Researchers are only beginning to define how these should be designed, implemented, and evaluated [35]. The same applies to understanding how students actually use GenAI in authentic course settings, for example, in introductory programming courses [69, 128, 129], and advanced computing courses [42].\nInstructors can also use GenAI to create customized and unique assignments tailored to student interests [88, 124] as well as generate other educational content [38, 138, 146]. Yet, this second category of GenAI usage in instruction is the least studied to date, possibly because many instructors have not yet thoughtfully integrated GenAI directly into their programming instruction [39, 45, 151]. However, studies thus far are showing mixed results. Some are pointing to its introduction as a way to equip students to move faster through the curriculum than was ever possible before [148]. Others are claiming that using GenAI has no negative effects on student learning outcomes [159]. However, some early work when GitHub Copilot was first released showed that students would flail and wander during programming tasks due to that tool's constant interruptions [114]. Other more recent work has now shown that GenAI can significantly undercut student critical thinking during code writing and debugging tasks for those students who over-rely on it, decreasing their grades overall [56]. Similarly, in an observational study, researchers found that although a larger percentage of students are able to complete programming tasks with the aid of GenAI tools, students faced new metacognitive challenges [113].\nAs evidenced by the discussion above and our systematic literature review below, the literature on GenAI in computing education is expanding rapidly. With so much happening so quickly, it is difficult to know what has been done, why it is being done, what works, and where this is all headed. We therefore attempt to capture the zeitgeist of the present moment in computing history by defining terms, ordering and summarizing all of this for the reader."}, {"title": "1.1 Goals", "content": "With all of the above in mind, this report addresses the following overarching research goals:\n(1a) How are instructors incorporating GenAI into teaching computing?\n(1b) And why are they making these choices?\n(2a) How have the expectations towards skills in software development changed with the advent of GenAI?\n(2b) Which computing competencies are required in the future, according to teachers and industry professionals?\nWe address these overarching goals via several fine-grained research questions, and various methods: a systematic literature review, a survey study with educators and software developers in industry, and qualitative interviews with computing educators, computing researchers, and GenAI tool creators."}, {"title": "1.2 Contributions", "content": "This working group report describes how and why computing instructors have chosen to integrate (or not integrate) GenAI into their courses, and what they expect with regard to future developments in curricula and industry usage of GenAI tools. We identify the current state-of-the-art by presenting the following deliverables:\n(1) Systematic Literature Review (Section 2 and 3): We review the existing literature on GenAI tools in computing education (through May 23, 2024) and present the studies in which educators report on (a) evidence of GenAI in computing education research, (b) educators using GenAI tools in their teaching practices, and (c) the rationale of educators to incorporate GenAI tools in a certain way.\n(2) Evaluating Instructional Practices to Teach Students How to Use GenAI Tools (Section 4 and 5): We gather current integration practices through an international survey of computing instructors. The survey also focused on the tools, policies, motivational aspects, and the impact of GenAI on the competencies students require to succeed \u2013 from an educator's perspective.\n(3) Evaluating Instructors' Use of GenAI Tools (Section 4 and 5): The survey of educators also revealed the use of GenAI tools by these educators, for example, to create tools that would support students.\n(4) Evaluating Instructors' Perspectives on Learning Outcomes and Future Developments (Section 4 and 5): To capture the impact of GenAI tools on actual student outcomes, we conducted semi-structured interviews with instructors. The results reveal the disruptive character of GenAI tools in terms of the potential benefits and limitations of GenAI in computing education.\n(5) Exploring the Industry's Experience (Section 4 and 5): Another contribution of this report is the integration of the industry perspective, their experiences and usage patterns of GenAI tools. This encompasses policies, motivations, and expectations regarding the competencies future developers will need."}, {"title": "1.3 Structure of the Report", "content": "The structure of this working group report is as follows. To address related work, the authors present the systematic search and review of prior studies on how and why computing educators have integrated GenAI tools into their teaching practices. The methods of the literature review are presented in Section 2 and the respective results of the systematic literature review are presented in Section 3.\nThe second major component of this working group report captures the perspective of both educators and software developers. We do so by presenting our methodology (Section 4), consisting of an international survey of computing educators, a series of qualitative interviews with computing educators, and an aligned survey of software developers to gather current industry practices and perspectives. As a part of the methodology, we briefly introduce prior work, fine-grained research questions, the process of developing the survey and interview questions, and the data analysis approaches we applied.\nWe present the results of our mixed-methods approach (i. e., surveys with educators and developers, and interviews with educators) in Section 5 by triangulating the different data sources for every research question. In Section 6, we discuss the most interesting results, and how they build and extend prior work.\nIn Section 7, we summarize the threats to validity of the applied methodology, before concluding our work in Section 8, and presenting pathways for future work (Section 9)."}, {"title": "2 Systematic Literature Review: Methods", "content": "The systematic literature review (SLR) aims to identify how instructors are integrating generative Al into computing classrooms. The goal is to extend the many prior interview and survey studies that have looked at students' perceptions of how generative AI tools are used [7, 49, 80, 161] to focus instead on what is actually being done within classroom settings. Therefore the goal is to focus on pedagogies, tools, and classroom interventions that feature empirical data about students' experiences with those classroom interventions. Specifically, we investigate the following three research questions:\nSLR-RQ1. How can the reported evidence of Generative AI in CER be summarized?\nSLR-RQ2. How is generative Al being incorporated into teaching?\nSLR-RQ3. What are the motivations behind incorporating GenAI tools into teaching?\nSLR-RQ1 and SLR-RQ2 relate to the overarching research goal (1a), while SLR-RQ3 relates to the overarching research goal (1b). Figure 1 illustrates the overall process of the whole systematic literature review. We follow the literature review best practices by Kitchenham and Brereton [72]. We first searched through various databases using a search string. Then, we checked for the inclusion of reference papers that should be found to confirm the quality of the search string. This was followed up by doing a title/abstract scan for relevance. After the title/abstract scan, we read the full papers and started extracting relevant information from them. In this stage, some papers were still rejected if they did not pass the inclusion/exclusion criteria. In the end, we had a set of 71 papers that passed the criteria and for which we extracted data. The included papers are listed in Table 1."}, {"title": "2.1 Search String Construction", "content": "The research team iteratively constructed a search string aimed at including papers that matched our research interests according to four categories: 1) the domain of the work was computer science or computer engineering education, 2) the topic was related to the use of Generative AI (GenAI), 3) it aligned with the working group focus on the impact of GenAI on pedagogy or teaching tools, and 4) it included the use of empirical methods. The literature review team met several times to discuss various keyword permutations and ultimately arrived at the following sub-search string for each of the four categories:\nDomain: \"Computer science education\u201d OR \u201cComputing education\" OR \"CS education\u201d OR \u201cCSEd\u201d OR \u201cCER\u201d OR \u201cComputing students\" OR \"Computing instructors\u201d OR \u201cCS students\" OR \"CS instructors\" OR \"Computer engineering education\" OR \"Programming education\" OR \"Introductory programming\"\nTopic: \"Large language model\" OR \"Large language models\" OR \"LLM\" OR \"LLMS\" OR \"Generative AI\" OR \"ChatGPT\" OR \"GPT3\" OR \"GPT4\" OR \"Multimodal model\" OR \"Multimodal models\" OR \"Gemini\" OR \"Claude\" OR \"GenAI\" OR \"GPT-4\" OR \"GPT-3.5\u201d OR \u201cGPT-3\u201d OR \u201cCopilot\" OR \"Language model\" OR \"Language models\" OR \"Generative pre-trained transformer\"\nWorking Group Focus: \"Pedagogy\" OR \"Pedagogies\" OR \"Classroom\" OR \"Student\" OR \"Students\" OR \"Teaching approach\" OR \"Teaching tools\"\nMethod: \"Qualitative\u201d OR \u201cQuantitative\u201d OR \u201cPerceptions\" OR \"Investigating\" OR \"Exploratory\u201d OR \u201cSurvey\u201d OR \u201cInterview\" OR \"Experiment\" OR \"Focus group\"\nThe full search string was constructed by combining each of these categories to ensure a paper contained at least one search term per category. As such, the full search string is as follows:\nSearch String = Domain AND Topic AND Working Group Focus AND Method\nWe decided to look for papers in a total of five different databases to ensure comprehensive coverage of work. The chosen databases were ASEE Peer, arXiv, Scopus, ACM Digital Library, and IEEE Xplore.\nThe final search resulted in 1536 papers after duplicate removals. The search was done on May 23rd, 2024, which is the cut-off date for included articles. The breakdown of the number of papers from each database prior to the removals is included in Table 2."}, {"title": "2.2 Paper Filtering Process", "content": "Balancing the need to broadly include relevant papers while ensuring the central goals of the research remain in focus is a common challenge in SLRs. To address this, we developed exclusion criteria that were applied repeatedly throughout the filtering process, ensuring that only high-quality papers focusing on the use of generative Al in pedagogical practice were included in our final analysis.\nA primary criterion for inclusion in the SLR was that the papers prominently featured some form of generative AI. This included studies where students used existing generative AI tools or where such tools were integrated into pedagogical practices. Papers that focused on teaching students about generative AI or its associated ethical aspects were also included.\nWe included only papers that focused on computing education at the tertiary level. Papers must have featured student participants of some kind and could not focus solely on K-12 or professional developers. However, papers that featured both tertiary students and K-12 or professional developers were included. The reason to focus on tertiary students was that these students are being trained to become computing professionals in the near future.\nIn keeping with the goal of evaluating robust interventions within computing education, we excluded shorter papers, such as posters, demos, and other shorter formats. Since the typical length of papers in the field of computing education is six pages or more in a double-column format, we used this lower limit to exclude less comprehensive studies.\nBased on these goals, we applied the following exclusion criteria at each stage of the filtering process:\n(1) Not GenAI: Papers that did not include a generative AI component were excluded. Papers that simply provide implications for generative AI were also not included.\n(2) Not Computing Education: Papers that were not related to computing education were excluded. For example, papers primarily focused on professional developers were excluded.\n(3) No Human Evidence: The paper did not contain empirical data or included only an expert evaluation.\n(4) No Intervention: Papers that did not feature a classroom intervention were excluded. However, user studies with students were included.\n(5) Exclusively K-12: Papers where the participants were exclusively K-12 were excluded.\n(6) Too Short: Papers were excluded if they were under 5 pages for double-column articles and under 8 pages for single-column articles.\n(7) Not an Article: We excluded conference proceedings' front matters, white papers, or other non-research content.\nIn the first phase, we had two reviewers evaluate the title and abstract of the paper for inclusion. If either of the reviewers thought that the paper was relevant, it was chosen for the next stage extraction."}, {"title": "2.3 Extraction", "content": "For all the papers that were not excluded in the title/abstract scanning phase, we thoroughly read the full paper. A paper could still be rejected at this stage if it did not pass the inclusion/exclusion criteria. To consistently systematically extract the content from each paper, we developed a structured form shown in Appendix F to extract information for all papers that were included in the review. Four researchers extracted the content from the final set of 71 papers."}, {"title": "3 Systematic Literature Review: Results", "content": "In this section, we present the results of the systematic literature review. They are based on the information extracted from the final set of 71 papers that resulted from the process described in the previous section."}, {"title": "3.1 Descriptive Statistics", "content": "From the evaluation form we collected a variety of descriptive data that relates to: 1) the authors and the students they evaluated, 2) the characteristics of the studies they conducted, 3) the types of courses in which these studies took place, and 4) the custom tools that have been developed.\nCourse Information. A variety of courses were used to study AI tools. We saw 26 upper division courses including three masters' level courses. CS1 was studied in 20 papers. Eight papers included more than one course. Examples of other courses were Human Computer Interaction, Software Modeling, and Embedded Systems.\nAuthor and Student Information. As shown in Figure 2a, the locations of the authors' institutions varied widely with the largest proportion of articles having authors from the United States (34%) and New Zealand (14%). Additionally, they were primarily from academic institutions (n=67) with very few coming from industry or involving industry-academic collaborations (n=2), or just from industry (n=1). As might be expected, the majority of studies took place at one or more of the authors' institutions which results in the distribution of student populations that were investigated looking quite similar to the one for authors (Figure 2b)."}, {"title": "3.2 How Is Generative AI Being Incorporated into Teaching?", "content": "In the extraction form, we had an open-ended question on \"How Instructors Incorporate Generative AI into Teaching Computing?\", which directly maps to our SLR-RQ2.\nTo analyze the results for this question, three authors thematically analyzed the open-ended responses and came up with initial tags. They discussed the initial tags and then combined them into definitive tags along three axes: tool type, purpose from the student's point of view, and whether students received guidance on how to use generative AI.\nOur tags for each axis are below.\n\u2022 Tool type: general purpose (e. g., ChatGPT), task-specific (e. g., GitHub Copilot), instructor-provided guardrails (e. g., CodeHelp [86], Promptly [33], CodeAid [60], CodeTailor [50]).\n\u2022 Purpose: hints, debug, learning resources, writing code, teacher training, code comprehension, code review, teach GenAI, motivation, UML, multiple, not specified.\n\u2022 Guidance on GenAI use: yes, no, unclear.\nFor tool type, we categorized tools as general purpose (such as ChatGPT), whether the tool is task-specific (i. e., the tool is meant for a specific task, but not education-focused, such as GitHub Copilot), and whether there are instructor-provided guardrails (i. e., there were pedagogical guardrails or other constraints in the tool).\nFor purpose from the student's point of view, we looked at the tasks that students worked on with the support of GenAI. During the thematic analysis, we came up with the following categories.\n\u2022 Writing code \u2013 GenAI was used for code writing support. For example, CodeTailor helps students write code and interact with Parsons problems [50].\n\u2022 Code comprehension - GenAI was used to teach code comprehension. For example, Gilt provides scaffolding contextualized to select sections of students' code [100].\n\u2022 Hints - GenAI was used to produce hints for students. For example, next-step hints [120].\n\u2022 Learning resources GenAI was used to create or improve learning resources that could be used by other students too. For example, students were instructed to generate analogies using LLMs [19].\n\u2022 Teach generative AI \u2013 the article describes an approach or tool to teach students how to use GenAI. For example, Promptly provides scaffolding for students to learn how to prompt GenAI [33].\n\u2022 Multiple tags GenAI was used for multiple purposes. For example, Choudhuri et al. investigated students' use of GenAI for multiple different software engineering tasks such as debugging and refactoring [25].\n\u2022 Debug \u2013 GenAI was used for debugging help, such as by explaining error messages [131].\n\u2022 Code review - GenAI was used for code review. For example, by integrating it into an assignment submission system [29].\n\u2022 UML \u2013 GenAI was used to assist in generating UML diagrams. This was done by C\u00e1mara et al. [30].\n\u2022 Teacher training \u2013 GenAI was used for teacher or teaching assistant training. GPTeach creates LLM agents that act as students to train new TAs [96].\n\u2022 Motivation - GenAI was used to increase student motivation. This was done by Moore et al. in a narrative-based learnersourcing platform [97].\n\u2022 Not specified \u2013 the purpose of GenAI was not specified in the paper or was unclear.\nFor guidance on GenAI use, we categorized each paper as a \"yes\", 'no\", or \"unclear\u201d (i. e., whether students received guidance or instructions on how to use generative AI).\nThe results of this analysis are presented in Table 5. Based on the findings, most studies focused on using generative AI for writing code, code comprehension, hints, and generating learning resources. Slightly under half (32/71) of the studies used tools with instructor-provided guardrails, for example, a custom tool with pedagogical guardrails. However, most commonly (34/71), studies used a general purpose generative AI tool such as ChatGPT. In the majority of studies (49/70), it was not reported that students would have been instructed on how to use generative AI.\nWe also looked at the type of evidence used, categorizing it into perceptual (e. g., opinions of activity) or behavioral (e. g., correctness of produced code). This was contrasted with the nature of the findings, which were categorized as positive, negative, mixed, or neutral. The results of this analysis are presented in Table 6. The results suggest that the majority of studies found positive results, with studies using behavioral evidence slightly more likely to report positive findings. 54% of studies with perceptual evidence reported positive findings whereas 69% of studies with behavioral and 70% of studies with both types of evidence reported positive findings."}, {"title": "3.3 What Are the Motivations Behind Incorporating GenAI Tools into Teaching?", "content": "In the extraction form, we had an open-ended question on \u201cAnd why do they incorporate GenAI tools that way?\", which directly maps to our SLR-RQ3. In the previous section, we outlined how instructors are incorporating generative Al into their teaching. Part of the why is overlapping - the ways it is incorporated often tell about the why. Thus, we here focus on in benefit of whom it is being integrated. We tagged each of the included papers with \u201cstudents\u201d, \u201cinstructors\" or \"both\". Out of the 71 papers, in 59 GenAI was incorporated in benefit of students. In 5 papers it was in benefit of teachers. In 7 papers it was in benefit of both students and teachers.\""}, {"title": "3.4 Recommendations for Incorporating GenAI", "content": "To analyze the effectiveness of incorporating generative Al into computing classrooms, we cross-tabulated the type of tool and guidance on GenAI use provided to students with the nature of the findings (positive, negative, mixed or neutral). The results of this analysis are reported in Table 7. The main result that can be seen from the table is that when there is no guidance on using GenAI from the instructor, it is recommended to use a tool that includes instructor-provided guardrails, e. g., pedagogical guardrails. When students are not provided guidance and use a general purpose generative Al model (e. g., ChatGPT), only 55% (12/22) of studies found positive results. However, even without guidance, if the tool included instructor-provided guardrails, positive results were found in 73% (19/26) of studies. When students are given instructions on how to use generative AI, whether the tool has built-in pedagogical guardrails does not seem to matter as much - studies that used general purpose tools found positive results in 58% (7/12) of cases and studies that used instructor-guardrailed tools found positive results in 67% (4/6) of cases.\nIn a similar vein, we cross-tabulated the nature of the findings with the task that GenAI was used for to examine what computing education tasks might benefit most from generative AI. The results of this analysis are reported in Table 8. There are differences between tasks in whether findings of the studies have been positive or not. For code writing, 58 % (15/26) studies reported positive results, whereas for code comprehension, 80% (8/10) of studies reported positive results. According to our results, hint generation is an area where generative AI could still improve, since only half of the studies (4/8) reported positive results. Better results have been observed for generating learning resources, where 86 % (6/7) studies found positive results."}, {"title": "4 Educator and Developer Views: A Mixed-Methods Approach", "content": "Computing educators want to prepare students for a successful career in software development. It is thus important to understand the experiences of software developers in addition to educators' perspectives to evaluate how different the two perspectives are. For this reason, we conducted a survey study with both target groups, and an interview study with educators.\nThrough the educator survey study, we aimed to gather a large sample of educator perspectives on whether (and how) they incorporate GenAI in their classroom, their motivations for this decision, and how they see competencies changing for programming education. We also surveyed developers to build a landscape of how GenAI tools are currently being used in industry settings and how developers view the changing competencies required for programming. The larger sample size of the survey also allows us to begin to explore equity-related questions related to student access and exposure to GenAI tools.\nThrough the interview study, we aimed to gather first-hand accounts, and deeper insights from educators on changes to their classroom as a result of emerging GenAI tools. We interviewed educators who teach students how to use GenAI tools to engage with class materials as well as how to use such tools in their professional careers. We also included educators who took a deliberate (explicit) stand to disallow the use of GenAI tools in their classes. Further, we interviewed developers of LLM-powered educational tools as well as researchers focused on the use of GenAI in computing education.\nIn this section, we first summarize prior studies that similarly gathered perspectives of educators and developers. Then we present the ten research questions that we answer through both the surveys and interviews. Next, we describe the applied methodology."}, {"title": "4.1 Prior Studies of Educators' and Developers' Perceptions", "content": "Computing educators' perspectives on GenAI tools have been reported in several prior studies. For example, Chan and Lee [23] surveyed 184 educators and 399 students, primarily from Hong Kong but across different disciplines. They focused on the perceptions, experience, and knowledge about GenAI, and compared educator and student responses. They found that the educators seemed more concerned about students' over-reliance and ethical issues and were skeptical towards GenAI tools and their capabilities. The need for policies and guidelines ensuring academic integrity and equitable learning conditions was yet another outcome [23].\nAmani et al. [5] also investigated students' and instructors' perceptions towards GenAI through two surveys at Texas A&M university. The goal of the instructor survey was to capture how GenAI affected their recent courses and how they think students should use respective tools. One important finding is that the responses from 243 staff members emphasize the need for teaching practices to adapt [5]. Another survey of educators was conducted by Prather et al. [111]. The 57 respondents elaborated on their perceptions, experience, usage, course policies, expectations, and beliefs, indicating that educators should stay abreast of the technological developments. The results also highlight the need to provide guidance for students regarding the ethical use of GenAI.\nIn addition to these surveys, a number of recent research studies used interviews to gather the instructor perspective on the impact of GenAI on Computing education research and practice [80, 111, 118, 132, 154, 161]. For example, Lau and Guo conducted interviews with 20 instructors about their intentions to adapt their teaching to emerging GenAI tools (e. g., ChatGPT and Copilot) [80]. Wang et al. [154] also interviewed 11 instructors about their perceptions. Despite sharing concerns about over-reliance and misuse of GenAI tools, the instructors did not have plans to adapt their courses due to the lack of effective strategies at the time. Another example is the interview study with 40 instructors by Rajabi et al. [118], which showed that educators were cautious about banning AI tools, because students would find ways of using them regardless. At the same time, instructors seemed concerned about increasing anxiety among students by focusing too heavily on exams, which is a well-known issue [64, 79, 92]. A recent interview study with educators was conducted by Sheard et al. [132]. They focused on educators' current practices, concerns, and planned adaptations relating to these tools. They found, for example, that educators appreciate that the tools can be a source of support for students, but that these tools may lead to students missing out on learning. Finally, Zastudil et al. [161] conducted an interview study comparing the perspectives of 6 instructors and 12 students, finding alignment in their concerns about over-reliance and misalignment in their motivations and relative knowledge about generative AI.\nWhile there has been some work in studying how developers can use GenAI tools to enhance their workflow, little has been done to understand the perceptions of how software developers see the practice changing. Specifically, one study found that software developers are using GenAI tools at many points in the programming process, including creating, modifying, debugging, and explaining code [15]. Recent work reports that GenAI automated code generation increases developer productivity [83], and GenAI tools are particularly useful for assisting in resolving technical issues [28]. Typically, productivity is measured in terms of the acceptance rates of coding suggestions [163].\nIn summary, we identified several survey and interview studies about the perceptions of educators regarding the prospective adoption of courses, learning objectives, assessments, or institutional policies, but not the actual adoption. Moreover, we found fewer studies focused on developers. In our survey study, we focus on both instructors and developers, to understand the perspectives of experts both within the learning environment and within the industry."}, {"title": "4.2 Research Questions", "content": "Based on the overarching research goals (see Section 1), the systematic literature review, and the identified gap on actual integration practices in education and industry, we constructed the following research questions guiding our work:\nRQ1 Policies: What are the existing policies and practices around using GenAI in Computing courses? (1b)\nRQ2 Instruction-use: How are instructors teaching students about using GenAI tools in order to program? (1a)\nRQ3 Instruction-support: How are instructors using generative Al tools to support the teaching of their courses? (1a)\nRQ4 Motivations: Why are instructors making these choices around using GenAI? (1b)\nRQ5 Tools: What kinds of GenAI tools are emerging in Computing education? (1a)\nRQ6 Perceived Outcomes: How has GenAI impacted instructor perceptions of student outcomes (compared to before)? (2a)\nRQ7 Industry usage: How are industry developers using GenAI tools? (2a)\nRQ8 Competencies: How has GenAI impacted desired student competencies? (2a, 2b)\nRQ9 Equity: How has GenAI impacted student equity? (1a, 1b, 2b)\nRQ10 Future: How is GenAI shaping the future of Computing education? (2b)\nWe intentionally divided RQ2 and RQ3 into two distinct research questions as the Working Group believes it is valuable to emphasize that GenAI tools are used in the classroom and also impacting course learning outcomes. Specifically, RQ2 focuses on investigating how educators prepare students for the future of programming where GenAI tools are available. This may include instructing them about GenAI capabilities for programming or allowing GenAI tools to be used by students as a method to teach students how to effectively incorporate tools into their workflow. RQ2 is in contrast to RQ3 which explores how educators incorporate GenAI tools into the classroom and their workflow. This could include using GenAI tools for grading, assignment creation, student feedback, or providing help (like a TA-bot)."}, {"title": "4.3 Mixed-Methods", "content": "To address the research questions regarding educators' and software developers' viewpoints, we employed a mixed-methods approach."}, {"title": "4.4 Survey Development", "content": "To address our research questions", "surveys": "an educator survey and a developer survey.\nTo design the questionnaires, we collaborated within our group to draft survey questions for each overarching research question, including closed-ended, open-ended, and rating scale question types. As a part of this process, we reviewed the literature (e. g., [111"}]}