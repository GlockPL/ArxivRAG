{"title": "An Adversarial Learning Approach to Irregular Time-Series Forecasting", "authors": ["Heejeong Nam", "Jihyun Kim", "Jimin Yeom"], "abstract": "Forecasting irregular time series presents significant challenges due to two key\nissues: the vulnerability of models to mean regression, driven by the noisy and\ncomplex nature of the data, and the limitations of traditional error-based evaluation\nmetrics, which fail to capture meaningful patterns and penalize unrealistic forecasts.\nThese problems result in forecasts which are often misaligned with human intuition.\nTo tackle these challenges, we propose an adversarial learning framework with a\ndeep analysis of adversarial components. Specifically, we emphasize the importance\nof balancing the modeling of global distribution (overall patterns) and transition\ndynamics (localized temporal changes) to better capture the nuances of irregular\ntime series. Overall, this research provides practical insights for improving models\nand evaluation metrics, and pioneers the application of adversarial learning in the\ndomain of irregular time-series forecasting.", "sections": [{"title": "Introduction", "content": "Irregular time series, characterized by significant variations in inter-arrival times and quantities,\npose unique challenges in analysis and forecasting. Unlike stationary and regular time series, which\nhave seen substantial advancements in both methodologies and applications [1], the exploration\nof irregular time series remains limited due to their inherent low interpretability. Forecasting such\ndata is particularly challenging for two primary reasons\u2014one stemming from the inadequacies of\nevaluation metrics and the other from the limitations of models, with both factors compounding\neach other. The first challenge lies in the widespread reliance on error-based metrics in forecasting,\nsuch as MAPE (mean absolute percentage error), which are ill-suited for capturing the unique\ncharacteristics of lumpy or intermittent patterns [2, 3]. These metrics fail to penalize the unrealistic\nforecasts often produced by statistical models, while simultaneously overlooking the potential of\nmodels that accurately identify underlying patterns but exhibit minor temporal shifts. The second\nchallenge stems from the mean regression problem faced by forecasting models. This issue primarily\narises due to the inherently noisy nature of irregular time series, which often lack clear trends or\nseasonality, making them especially prone to this problem. Paradoxically, under commonly used\nmetrics like MAPE, the mean regression problem is not penalized but often rewarded, leading to\nmodels that fail to align with human intuition. Fig. 1 provides a clear example where statistical\nmodels or models with mean regression problem deviate notably from intuitive expectations, despite\nachieving high scores with MAPE-based evaluations. Similar cases in real world datasets are detailed\nin Appendix A. While fields such as large language models and image generation have made\nsignificant strides in producing outputs that align with human intuition, traditional forecasting models\nand evaluation metrics lag behind, especially in irregular time-series. To address this issue, we\npropose an adversarial learning framework aimed at bridging the gap between forecasting outputs\nand intuitive expectations. Adversarial learning, widely recognized for its applications in time-series"}, {"title": "Related Works", "content": "Adversarial learning in time series forecasting. Adversarial learning is often adopted in time\nseries generation and forecasting [7, 8, 6, 9, 10, 5, 11]. However, despite its widespread use, a\nthorough analysis of the role of adversarial learning in the time-series domain remains lacking. The\nunidirectional nature of sequential data introduces unique challenges, particularly in forecasting\nrather than in generation. Hence, it is crucial to revisit the objective of using adversarial components\nin forecasting, given the differing nature of these tasks. The earlier works including C-RNN-GAN [5]\nsimply replaced the generator and discriminator with LSTM networks. On the other hand, RCGAN\n[12] conditioned on additional input instead of previous outputs, while it still utilized recurrent units\nfor both the generator and discriminator. TimeGAN [6] employed both supervised and adversarial\nobjectives to address the mean regression problem and the lack of temporal dynamic consideration,\nrespectively. Alongside other works [13, 14], TimeGAN integrated recurrent networks into both the\ngenerator and discriminator. GT-GAN [7], while not focused on forecasting, employs GRU-ODE [15]\nto construct the discriminator for generative purposes. AST [8] is the first adversarial model designed\nfor forecasting, and it incorporates adversarial learning to prevent error accumulation from the\nautoregressive nature of predictions. Thus, the discriminator module consists of non-recursive layers\nand competes with the forecaster, which is expected to eliminate error accumulation. TrendGCN\n[9] employs a Graph Convolutional Network as a forecaster and utilized two discriminators, one for\ncapturing spatial relations and the other for temporal relations.\nIrregular time series forecasting. Irregular and sparse time-series forecasting is challenging as\nit is often characterized by multiple inter-arrival times, with some additionally distinguished by\nsignificant variations in quantity between these intervals. Although statistical methods (e.g., Croston\n[16]), machine learning approaches (e.g., SVR [17]), and deep learning techniques (e.g., LSTM\n[18]) have been applied to these problems [3], it remains unclear which method is the most suitable."}, {"title": "Rethinking Adversarial Components in Irregular Time-Series Forecasting", "content": "Let's assume we have M distinct time series, each spanning a time period indexed by t = 1, . . ., \u03a4. \u03a4\u03bf\nforecast, models utilize historical data of length P, represented by {xim}m=1 for each ith time series.\nThe forecasting horizon is set to L, and our objective is to accurately predict the values {yin}h=1\nGiven a forecasting model F : RP \u2192 RL, we express our forecasting process as {yin}n=1 =\nF({xim}m=1). In this paper, we only considered a global forecasting model across the different\ntime series i = 1, . . ., M within the same dataset, i.e., the observational space is X \u2208 RM\u00d7T while\nX = {xi,1:P}1 and Y = {Yi,1:L}\\\u2081 represent the sets of historical data and corresponding target\nvalues, respectively. Below, we specify the meaning of irregular time-series in our work."}, {"title": "Problem Formulation", "content": "Let's assume we have M distinct time series, each spanning a time period indexed by t = 1, . . ., \u03a4. \u03a4\u03bf\nforecast, models utilize historical data of length P, represented by {xim}m=1 for each ith time series.\nThe forecasting horizon is set to L, and our objective is to accurately predict the values {yin}h=1\nGiven a forecasting model F : RP \u2192 RL, we express our forecasting process as {yin}n=1 =\nF({xim}m=1). In this paper, we only considered a global forecasting model across the different\ntime series i = 1, . . ., M within the same dataset, i.e., the observational space is X \u2208 RM\u00d7T while\nX = {xi,1:P}1 and Y = {Yi,1:L}\\\u2081 represent the sets of historical data and corresponding target\nvalues, respectively. Below, we specify the meaning of irregular time-series in our work."}, {"title": "Irregular Time-Series", "content": "We define irregular time series by their characteristic\nvariability in inter-arrival times and further subcategorize them based on variability in quantity,\ndrawing on the work of Syntetos et al. [23]. Irregular time series are identified by using the ADI\n(Average Demand Interval), a metric that quantifies regularity over time by calculating the average\ninterval between successive non-zero entries. Following established conventions [3, 23, 24], we adopt\na threshold of 1.32 for ADI (ADI > 1.32). Irregular time series can be further classified into two\nsubcategories\u2014intermittent and lumpy\u2014based on the CV\u00b2 metric, which measures the variability in\nnon-zero quantities. We use a threshold of 0.49 for CV2, as suggested in previous studies [3, 23, 24]."}, {"title": "Adversarial Components", "content": "Discriminators with non-recursive layers treat irregular time-series\nas vectors without considering temporal relationships, similar to the approach used in image transfer\nor generation [25, 26]. For a fixed forecaster F, the non-recursive discriminator Dnr(y, 0d) outputs\na scalar which indicates the probability of input vector y originating from X. We expect Dnr to\noperate as discribed in (1), and the global minimum is achieved if and only when PF = px."}, {"title": null, "content": "$\nDnr(Y1:L) =\n\\frac{PX (Y1:L)}{PX(Y1:L) + PF(Y1:L)}\n(1\n$"}, {"title": "Recurvise Discriminator", "content": "A discriminator with recursive layers Dr functions similarly to a chain\nof multiple PCL modules [27]. The PCL framework constructs two samples of vectors as shown in"}, {"title": null, "content": "$\n\u03c5(\u03b7) =\n( \\frac{Yn-1}{Yn} )and \u03c5* (\u03b7) =\n( \\frac{Y*n-1}{Y*n} )\n(2)\n$"}, {"title": null, "content": "$\nDr(v) =\n\\frac{1}{L}\n\u03a3\nBn(hn(v\u00b9), hn(v\u00b2))\n(3)\n$"}, {"title": null, "content": "$\nMAPE\n=\n\\frac{1}{ML}\n\u03a3\u03a3\n\\frac{Yin-Yin}{Yin}\n(4)\n$"}, {"title": null, "content": "$\nSMAPE =\n\\frac{2}{LM}\n2\n\\frac{Yin-Yin}{Yin + Yin}\n(5)\n$"}, {"title": null, "content": "$\nSPEC =\n\u03a3\u03a3\u03a3\\frac{1}{ML} ((n - m + 1) \u00b7 max(0; 01 \u00b7 min(yim; \u03a3Yik - \u03a3fij);\n(6)\n$"}, {"title": null, "content": "$\nRMSE =\n\\frac{1}{M}\u221a \u03a3\u03a3\\frac{(Yin - Yin)\u00b2}{n}\n(7)\n$"}, {"title": "Baseline models", "content": "Croston [16]: A method specifically designed for intermittent and lumpy time series forecasting.\nIt decomposes demand into occurrence and size, making it effective for irregular patterns in\ntime-series data.\nARIMA [20]: A widely-used statistical model that combines autoregressive (AR) and moving\naverage (MA) components with differencing to handle non-stationarity. It is effective for\ncapturing linear temporal dependencies in regular time series but struggles with highly\nirregular patterns.\nADIDA (Aggregate-Disaggregate Intermittent Demand Approach) [29]: A technique tailored\nfor intermittent time series. It aggregates demand over fixed intervals to smooth irregular\npatterns and then applies standard forecasting methods to generate predictions.\nMLP (Multilayer Perceptron): A feedforward neural network model that learns non-linear\nrelationships between input and output. While flexible, it may require careful tuning for\neffective performance on time series with irregular patterns.\nRNN (Recurrent Neural Network) [30]: A neural network model with feedback connections\nthat capture temporal dependencies in sequential data. RNNs are powerful for regular time\nseries but can struggle with long-term dependencies or highly intermittent patterns.\nLSTM (Long Short-Term Memory) [18]: A specialized type of RNN designed to overcome\nthe vanishing gradient problem, making it suitable for learning long-term dependencies\nin sequential data. LSTM can handle some degree of irregularity but requires significant\ncomputational resources."}, {"title": "Experiment details", "content": "We first set look-back period and forecasting horizon for each dataset. We follow setting of kaggle\ncompetition for M5 dataset, thus both P and L are set to 28. AUTO and RAF are having L of 6,\nwhile P is set to 6 and 18 respectively. AUTO is having shorter look-back period due to the lack of\nhistorical data, as we only have 24 timestamp through out whole dataset.\nIn our experiments, we train encoder and discriminator jointly. Throughout all configurations, we\ntrained for 100 epochs and hyperparameter tuning has been done in logarithmic scale. We selected\nour best model by using MAPE which can be considered as convention, and we observed how the\nmodels behave throughout in other metrics. All seeds in the experiment is set to 0 and we set a batch\nsize to 256. During training, we first update parameters in encoder followed by discriminator."}]}