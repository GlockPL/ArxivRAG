{"title": "SustainDC - Benchmarking for Sustainable Data Center Control", "authors": ["Avisek Naug", "Antonio Guillen", "Ricardo Luna", "Vineet Gundechat", "Desik Rengarajan", "Sahand Ghorbanpour", "Sajad Mousavi", "Ashwin Ramesh Babu", "Dejan Markovikj", "Lekhapriya D Kashyap", "Soumyendu Sarkar"], "abstract": "Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant amounts of energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities for improvement of data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for the development and benchmarking of advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.", "sections": [{"title": "Introduction", "content": "One of the growing areas of energy and carbon footprint (CFP) can be traced to cloud data centers (DCs). The increased use of cloud resources for batch workloads related to Al model training, multimodal data storage and processing, or interactive workloads like streaming services, hosting websites have prompted enterprise clients to construct numerous data centers. Governments and regulatory bodies are increasingly focusing on environmental sustainability and imposing stricter regulations to reduce carbon emissions. This has prompted industry-wide initiatives to adopt more intelligent DC control approaches. This paper presents SustainDC, a sustainable DC Multi-Agent Reinforcement Learning (MARL) set of environments. SustainDC helps promote and prioritize sustainability and facilitates an ecosystem of Al researchers with a platform to contribute to a more environmentally responsible DC.\nThe main contributions of this paper are the following:\n\u2022 A highly customizable collection of environments related to Data Center (DC) operation that can be used to benchmark energy and carbon footprint for different DC designs. The ability to subclass models across the spectrum of DC components like workloads, specification of individual servers, to cooling towers, allows the user to test fine-grained design choices.\n\u2022 The environments are wrapped in the Gymnasium Env class, lending it itself to benchmarking of different control strategies for optimizing energy, carbon footprint, and related metrics.\n\u2022 Supports MARL controllers with both homogeneous and heterogeneous agents, as well as non-ML controllers. With the given environments, we undertake extensive studies to show the benefits and disadvantages of a collection of multi-agent approaches.\n\u2022 SustainDC allows the user to perform reward shaping to help run ablation studies on different parts of the DC for performance optimization of the different areas.\nCode, licenses, and instructions of SustainDC can be found on GitHub\u00b2. Documentation can be found here \u00b3."}, {"title": "Related Work", "content": "Recent advancements in the field of Reinforcement Learning (RL) have led to an increased focus on optimizing energy consumption in areas such as building and DC management. This has resulted in the development of several environments for RL applications. CityLearn (V\u00e1zquez-Canteli et al., 2019) is an open-source platform that supports single and MARL strategies for energy coordination and demand response in urban environments. Energym (Scharnhorst et al., 2021), RL-Testbed (Moriyama et al., 2018) and Sinergym (Jim\u00e9nez-Raboso et al., 2021) were developed as RL wrappers that facilitate communication between Python and EnergyPlus, enabling RL evaluation on the collection of buildings modeled in EnergyPlus. SustainGym Yeh et al. (2023) is one of the latest suite of general purpose RL tasks for evaluation of sustainability, simulating electric vehicle charging scheduling and battery storage bidding in electricity markets.\nMost of the above-mentioned works use EnergyPlus (Crawley et al., 2000) or, Modelica Wetter et al. (2014), which were primarily designed for modeling thermo-fluid interactions with traditional analytic control with little focus on Deep Learning applications. The APIs provided in these works only allow sampling actions in a model free manner, lacking an easy approach to customization or re-parameterization of system behavior. This is due to the fact that most of the works have a set of pre-compiled binaries (e.g. FMUs in Modelica) or fine-tuned spline functions (in EnergyPlus) to simulate nominal behavior. Furthermore, there is a significant bottleneck in using these precompiled environments from Energyplus or Modelica for Python based RL applications due to latency associated with cross-platform interactions, versioning issues in traditional compilers for EnergyPlus and Modelica, unavailability of open source compilers and libraries for executing certain applications.\nSustainDC allows users to simulate the electrical and thermo-fluid behavior of large DCs directly in Python. Unlike other environments that rely on precompiled binaries or external tools, SustainDC is easily end-user customizable and fast. It enables the design, configuration, and control benchmarking of DCs with a focus on sustainability. This provides the ML community with a new benchmark environments specifically for Heterogeneous MARL in the context of DC operations, allowing for extensive goal-oriented customization of the MDP transition function, state space, actions space, and rewards."}, {"title": "Data Center Operational Model", "content": "Figure 1 shows the typical components of a DC operation modeled in SustainDC. Workloads are uploaded to the DC from a proxy client. For non-interactive batch workloads, a fraction of these jobs can be flexible or delayed to different time periods. This creates a scheduling problem where workloads can be delayed to some time of the day when Grid Carbon Intensity (CI) is lower.\nNext, as the servers (IT system) in the DC process these workloads, they generate heat, which needs to be removed. Hence, a complex HVAC system comprising multiple components is set up to cool the IT system. As shown in Figure 2, the warm air leaves the servers and rises up by convection. Due to the forced draft of the HVAC fan, this warm air enters the Computer Room Air Handler (CRAH) (shown using red arrows) where it is cooled down to an optimal setpoint by a heat exchange process using a \"primary\" chilled water loop. The chilled air is then sent back to the IT room using a plenum located underneath the DC (shown using blue arrows). The warm water then goes back to the Chiller where this heat is transferred by another heat exchange process to the \"secondary\" chilled water loop that carries the heat to a Cooling Tower. The cooling tower fan operates at a certain speed to reject the heat from the secondary chilled water to the outside environment. This speed, and hence the energy consumption of the fan, is a function of the inlet temperature of the secondary chilled water loop at the cooling tower, the required setpoint for the outlet temperature, the outside air temperature, and humidity. Thus, depending on the external Weather and Workload processed, the IT and cooling system consume Grid Energy. Choosing the optimal cooling setpoint for the CRAH has the potential to reduce the carbon footprint of the DC. Furthermore, the optimal cooling setpoint also influences the energy efficiency of the servers (Sun et al. (2021)).\nOften, larger DCs include onsite Battery Banks. Batteries can be charged from the grid during low CI periods. During higher CI periods, they provide auxiliary energy to the DC. This creates an opportunity to solve a decision problem where we optimally choose the time intervals during which the battery charges versus when it is used to provide auxiliary energy.\nWe observed that these three control problems are related in a cascading manner. This motivates the development of testbeds or environments where we can test different multiagent control approaches to reduce the carbon footprint and other associated sustainability metrics of interest."}, {"title": "SustainDC environment overview", "content": "We provide a high-level overview of the SustainDC in Figure 3. We identify the three main environments developed in Python and highlight their individual components, customization capabilities, and associated control problems. The Workload Environment models and controls the execution and scheduling of delay-tolerant workloads within the DC. Inside the Data Center Environment, the servers housed in the IT room cabinets process these workloads. It simulates the electric and thermal-fluid behavior, and the resulting heat generated from running these jobs is conducted to the outside environment by means of a set of HVAC cooling components. The Battery Environment simulates charging from the grid during off-peak hours and provides auxiliary energy to the DC during peak grid carbon intensity periods. The detailed physics-based implementation of the individual environments is provided in the supplemental document. The customization can be completely"}, {"title": "Workload Environment", "content": "The Workload Environment (EnvLS) manages the execution and scheduling of delayable workloads within the DC. It does this by streaming workload traces (measured in FLOPs of compute) over a specified period. SustainDC includes a collection of open-source workload traces from Alibaba Alibaba Group (2017) and Google Google (2019) data centers. Users can customize this component by adding new workload traces to the appropriate folder (data/Workload) or specifying a path to existing traces in the dc_config.json.\nCertain workloads are flexible, meaning they can be rescheduled within an allowable time horizon. These tasks, such as update tasks or backup tasks, do not need to be executed immediately and can be delayed based on their urgency or Service-Level Agreement (SLA). This flexibility allows the workload to be shifted to periods when the grid's CI is lower, thereby reducing the DC's overall carbon footprint (CFP).\nUsers can also customize the CI data. By default, we provide a one-year CI file for the following states: Arizona, California, Georgia, Illinois, New York, Texas, Virginia, and Washington. These locations were selected because they are where most data centers are situated. The carbon intensity files are extracted from eia.gov (https://api.eia.gov/bulk/EBA.zip) and located in the folder data/CarbonIntensity.\nLet $B_t$ be the instantaneous DC workload trace at time t, with X% of the load being rescheduled up to N simulation steps into the future. The goal of an RL agent (Agents) is to observe the current time of day ($S_{Ct}$), the current and forecast grid CI data ($Cl_{t...t+L}$), and the amount of rescheduled workload left ($D_t$). Based on these observations, the agent decides an action $A_{ls,t}$ (as shown in Table 1) to reschedule the flexible component of $B_t$, minimizing the net CFP over N steps."}, {"title": "Data Center Environment", "content": "The Data Center environment (EnvDC) includes an exhaustive set of models and associated specifications that can be configured. For IT level design, SustainDC allows users to define the dimensions of the IT Room, the arrangement of the server cabinets (including the number of rows and cabinets"}, {"title": "Battery Environment", "content": "The Battery Environment (EnvBAT) is based on models for charging and discharging batteries, such as $f_{charging}(BatSoc, \\delta t)$ from Acun et al. (2023). Parameters for these components, as well as the battery capacity, can be specified in dc_config.json.\nThe goal of the RL agent (AgentBAT) is to manage the battery's state of charge (BatSoct) effectively. Based on the net energy consumption (Ehvac + Eit) from the Data Center environment, the time of day (SCt), the battery's state of charge (BatSoct), and the forecast grid CI data (CIt...t+L), the agent must decide on an action Abat,t (as shown in Table 1). The actions include whether to charge the battery from the grid, do nothing, or provide auxiliary energy to the data center, all aimed at minimizing the overall carbon footprint."}, {"title": "Heterogeneous Multi Agent Control Problem", "content": "While SustainDC allows the user to solve the individual control problems for the three environments, the goal of the paper is to establish a multi-agent control benchmark which allows us to jointly optimize the CFP by considering the joint actions of all the three agents (AgentLS, AgentDC, and AgentBAT). The sequence of operations for the joint multi-agent and Multi-Environment functions can be represented as:\n$Agents: (SC_t \\times CI_t \\times D_t \\times B_t) \\rightarrow A_{ls,t}$ (1)\n$Agent_{DC} :(SC_t \\times t_{db} \\times t_{room} \\times E_{hvac} \\times E_{it} \\times CI_t) \\rightarrow A_{dc,t}$ (2)\n$Agent_{BAT} :(SC_t \\times Bat\\_SoC \\times CI_t) \\rightarrow A_{bat,t}$ (3)\n$Env_{LS} :(B_t \\times A_{ls,t}) \\rightarrow B^\\prime_t$ (4)\n$Env_{DC} :(B^\\prime_t \\times t_{db} \\times t_{room} \\times A_{dc,t}) \\rightarrow (E_{hvac}, E_{it})$ (5)\n$Env_{BAT} :(Bat\\_SoC \\times A_{bat,t}) \\rightarrow (Bat\\_SoC, E_{bat})$ (6)\n$CFP_t =(E_{hvac} + E_{it} + E_{bat}) \\times CI_t$ (7)"}, {"title": "Rewards", "content": "While CFP reduction is the default objective in SustainDC, the reward formulation is highly customizable and allows users to consider other objectives, including total energy usage and total operating cost across all the DC components and water usage. Primarily, we consider the following default rewards for the three environments (EnvLs, EnvDC, EnvBAT)\n$(\nr_{LS}, r_{DC}, r_{BAT}) = (CFP_t+ LSPenalty), -(E_{hvac,t} + E_{it,t}), -(CFP_t)$\nHere, $LSP_{enalty}$ is a penalty attributed to the Load Shifting Agent (Agents) in the Workload Environment (EnvLS) if it fails to assign the flexible workloads that were supposed to be rescheduled within the time horizon N. This implies that if Dt is positive at the end of a horizon N, we assign $LSP_{enalty}$. The details for evaluating $LSP_{enalty}$ is discussed in the supplemental document. The user can choose to use any other reward formulation by subclassing the base reward class inside utils/reward_creator.py.\nBased on the individual rewards, we can formulate an independent or a collaborative reward structure where each agent gets partial feedback in the form of rewards from the other agent-environment pair. The collaborative feedback reward formulation for each agent is formulated as:\n$R_{LS} = a * r_{Ls} + (1 \u2212 a)/2 * r_{DC} + (1 \u2212 a)/2 * r_{BAT}$\n$R_{DC} = (1 - a)/2 * r_{Ls} + a * r_{DC} + (1 - a)/2 * r_{BAT}$\n$R_{BAT} = (1 - a)/2 * r_{Ls} + (1 \u2212 a)/2 * r_{DC} + a* r_{BAT}$\nHere a is the weighting parameter. The reward-sharing mechanism allows the agents to estimate the feedback from their actions in other environments for independent critic multiagent RL algorithms (e.g. IPPO de Witt et al. (2020)). For example, the adjusted CPU Load Bt affects the data center energy demand, Ecool + Eit which in turn affects the battery optimizer's decision to charge or discharge resulting in a particular net CO2 Footprint. Hence, we decided to investigate a collaborative reward structure. We conduct a set of ablation experiments with different values of a to understand whether performance can be via reward sharing mechanism."}, {"title": "Evaluation Metrics and Experimental Settings", "content": "We consider five metrics when evaluating different RL approaches on SustainDC. $CO_2 footprint$ (CFP) indicates the cumulative carbon footprint of the DC operation over the period of evaluation. HVAC Energy refers to the amount of energy consumed across the DC cooling components including the chiller, pumps and cooling tower. IT energy refers to the amount of energy consumed across the DC servers. Water Usage refers to the chilled water that is recirculated through the cooling system. In certain DCs, the availability of chilled water from a central plant is limited, and efficient utilization of this resource helps lower the water footprint of the DC. Task Queue: In our approach, since we reschedule workloads over time, this metric keeps track of how many FLOPs of compute are accumulating that need to be rescheduled at a later time period under lower CI. Higher values of Task Queue indicate worse SLAs in DCs.\nWe ran the experiments on an Intel(R) Xeon(R) Platinum 8470 server with 104 CPUs using 4 threads per training agent. All the hyperparameter settings for the benchmark experiments are provided in the supplemental document. The codebase and documentation are already linked with the paper."}, {"title": "Benchmarking Algorithms on SustainDC", "content": "The purpose of SustainDC is to understand the benefits jointly optimizing the Workload, Data Center and Battery Environments for reducing the operating CFP of a DC. To investigate this idea, first, we consider the net operating CFP for instances where we only evaluate with a trained RL agent for one of the SustainDC environment while employing the baseline methods (B*) for the other environments. The baseline method (BLS) for the Workload Environment (EnvLS) simply implies that no workload is moved over the horizon, which is the current standard in the majority of the DCs. For the Data Center Environment (EnvDC), we consider the industry standard ASHRAE Guideline 36 as the baseline (BDC) Zhang et al. (2022). For the Battery Environment (EnvBAT), we consider a real time adaptation of the method in Acun et al. (2023) as the baseline (BBAT ). Next, we perform ablations on the collaborative reward parameter a. Finally, then benchmark variants of the multi-agent RL approach. This includes multiagent PPO Schulman et al. (2017) with independent critic associated with each actor network (IPPO) de Witt et al. (2020), a centralized critic that has access to states and actions from other MDPs (MAPPO) Yu et al. (2022). Given the heterogeneous nature of the action and observation spaces for SustainDC, we also benchmark several Heterogeneous multi-agent RL (HARL) Zhong et al. (2024) approaches including HAPPO (Heterogeneous Agent PPO), HAA2C (Heterogeneous Agent Advantage Actor Critic), HAD3QN (Heterogeneous Agent Dueling Double Deep Q Network) and HASAC (Heterogeneous Agent Soft Actor Critic).\nIn Figure 4, we compare the relative performance of different RL algorithms on a radar chart based on the evaluation metrics in Section 5. Since reporting absolute values may not convey much sense, we plot the relative differences in RL agent performances, which provides insights in to the pros and cons of each approach. (We will provide the absolute values for these benchmark experiments in the supplementary document in tabular format.) Hence, we normalize the absolute values of the metrics of interest for each benchmarking approach with respect to the average and standard deviation. We consider the lowest values towards the periphery of the radar chart and higher values towards the center. Hence, the larger the area of each approach on the radar chart, the better is the approach w.r.t the metrics of interest."}, {"title": "Single vs multi-agent Benchmarks", "content": "Figure 4a compares the relative performance of a single RL agent vs multi-agent RL benchmarks to motivate the use of a MARL approach for sustainable DC operation. Among single RL agent"}, {"title": "Reward Ablation on a", "content": "Figure 4b, shows the relative differences in performance when considering collaborative reward components. We considered 2 values of a at the extremes to indicate no collaboration (a= 1.0) and relying only on the rewards of other agents (a=0.1). An intermediate value of a=0.8 was chosen based on similar work on reward-based collaborative approach in Sarkar et al. (2023). The improvement in setting a = 0.8 shows that considering rewards from other agents can improve performance w.r.t. no collaboration (a = 1.0) especially in a partially observable MDP."}, {"title": "Multiagent Benchmarks", "content": "We evaluated and compared the relative performances of different MARL that includes PPO with independent actor critics (IPPO, a = 0.8), centralized critic PPO (MAPPO), heterogeneous multiagent PPO (HAPPO), HAA2C, HAD3QN and HASAC. Figures 4c, 4d, 4e, 4f show the relative performance of these approaches for data centers located in New York, Georgia, and California and Arizona. We observed a clear trend where PPO based shared actor critic methods (MAPPO, HAPPO) outperform their independent agent counterpart IPPO. On closer investigation of the actions, we found that while IPPO is able to reduce HVAC and IT energy, the battery agent is not able to schedule when to charge from the grid and discharge optimally to meet data center demand. Among MAPPO, HAPPO and HA2C, the HAPPO was consistently able to perform better (except Georgia). Among the off-policy methods (HAD3QN and HASAC), there is a significant variance in performance across different regions, with HASAC outperforming all others in Arizona. We have not been able to fully understand the overall reason for these variations which can be partially attributed to weather and carbon intensity related changes. We will continue to investigate this changes in future work."}, {"title": "Limitations", "content": "The absence of an oracle that already knows the best results possible for the different environments makes it difficult to quantify the threshold for performance compared to simpler environments. Also, for computational speed in RL, we used reduced order models for certain components like pumps and cooling towers. Also, we could not exhaustably tune the hyperparameters for all the networks."}, {"title": "Conclusion", "content": "In this paper, we introduced SustainDC, a MARL benchmarking environments developed completely in Python. It allows the user to solve problems related to sustainable, cost and energy efficient data center operations. SustainDC allows the customization of every aspect of the data center. It allows a great degree of customization w.r.t. the RL reward design, which is an important problem that we invite other researchers to collaborate on using SustainDC. We benchmark an extensive collection of single and multiagent RL algorithms on SustainDC across multiple geographical locations and compare their performance. This can provide researchers with the background to sustainably control data centers using reinforcement learning. This can also be used to benchmark hierarchical RL algorithms given the complexity and constraints of this environment rooted in a real-world system."}]}