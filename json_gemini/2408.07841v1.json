{"title": "SustainDC - Benchmarking for Sustainable Data\nCenter Control", "authors": ["Avisek Naug", "Antonio Guillen", "Ricardo Luna", "Vineet Gundechat", "Desik Rengarajan", "Sahand Ghorbanpour", "Sajad Mousavi", "Ashwin Ramesh Babu", "Dejan Markovikj", "Lekhapriya D Kashyap", "Soumyendu Sarkar"], "abstract": "Machine learning has driven an exponential increase in computational demand,\nleading to massive data centers that consume significant amounts of energy and\ncontribute to climate change. This makes sustainable data center control a pri-\nority. In this paper, we introduce SustainDC, a set of Python environments for\nbenchmarking multi-agent reinforcement learning (MARL) algorithms for data\ncenters (DC). SustainDC supports custom DC configurations and tasks such as\nworkload scheduling, cooling optimization, and auxiliary battery management,\nwith multiple agents managing these operations while accounting for the effects of\neach other. We evaluate various MARL algorithms on SustainDC, showing their\nperformance across diverse DC designs, locations, weather conditions, grid carbon\nintensity, and workload requirements. Our results highlight significant opportu-\nnities for improvement of data center operations using MARL algorithms. Given\nthe increasing use of DC due to AI, SustainDC provides a crucial platform for the\ndevelopment and benchmarking of advanced algorithms essential for achieving\nsustainable computing and addressing other heterogeneous real-world challenges.", "sections": [{"title": "Introduction", "content": "One of the growing areas of energy and carbon footprint (CFP) can be traced to cloud data centers\n(DCs). The increased use of cloud resources for batch workloads related to Al model training,\nmultimodal data storage and processing, or interactive workloads like streaming services, hosting\nwebsites have prompted enterprise clients to construct numerous data centers. Governments and\nregulatory bodies are increasingly focusing on environmental sustainability and imposing stricter\nregulations to reduce carbon emissions. This has prompted industry-wide initiatives to adopt more\nintelligent DC control approaches. This paper presents SustainDC, a sustainable DC Multi-Agent\nReinforcement Learning (MARL) set of environments. SustainDC helps promote and prioritize\nsustainability and facilitates an ecosystem of Al researchers with a platform to contribute to a more\nenvironmentally responsible DC.\nThe main contributions of this paper are the following:\n\u2022 A highly customizable collection of environments related to Data Center (DC) operation that\ncan be used to benchmark energy and carbon footprint for different DC designs. The ability\nto subclass models across the spectrum of DC components like workloads, specification of\nindividual servers, to cooling towers, allows the user to test fine-grained design choices.\n\u2022 The environments are wrapped in the Gymnasium Env class, lending it itself to benchmarking\nof different control strategies for optimizing energy, carbon footprint, and related metrics.\n\u2022 Supports MARL controllers with both homogeneous and heterogeneous agents, as well as\nnon-ML controllers. With the given environments, we undertake extensive studies to show\nthe benefits and disadvantages of a collection of multi-agent approaches.\n\u2022 SustainDC allows the user to perform reward shaping to help run ablation studies on different\nparts of the DC for performance optimization of the different areas.\nCode, licenses, and instructions of SustainDC can be found on GitHub\u00b2. Documentation can be found\nhere 3."}, {"title": "Related Work", "content": "Recent advancements in the field of Reinforcement Learning (RL) have led to an increased focus on\noptimizing energy consumption in areas such as building and DC management. This has resulted in the\ndevelopment of several environments for RL applications. CityLearn (V\u00e1zquez-Canteli et al., 2019)\nis an open-source platform that supports single and MARL strategies for energy coordination and\ndemand response in urban environments. Energym (Scharnhorst et al., 2021), RL-Testbed (Moriyama\net al., 2018) and Sinergym (Jim\u00e9nez-Raboso et al., 2021) were developed as RL wrappers that\nfacilitate communication between Python and EnergyPlus, enabling RL evaluation on the collection\nof buildings modeled in EnergyPlus. SustainGym Yeh et al. (2023) is one of the latest suite of general\npurpose RL tasks for evaluation of sustainability, simulating electric vehicle charging scheduling and\nbattery storage bidding in electricity markets.\nMost of the above-mentioned works use EnergyPlus (Crawley et al., 2000) or, Modelica Wetter\net al. (2014), which were primarily designed for modeling thermo-fluid interactions with traditional\nanalytic control with little focus on Deep Learning applications. The APIs provided in these works\nonly allow sampling actions in a model free manner, lacking an easy approach to customization or\nre-parameterization of system behavior. This is due to the fact that most of the works have a set of\npre-compiled binaries (e.g. FMUs in Modelica) or fine-tuned spline functions (in EnergyPlus) to\nsimulate nominal behavior. Furthermore, there is a significant bottleneck in using these precompiled\nenvironments from Energyplus or Modelica for Python based RL applications due to latency associ-\nated with cross-platform interactions, versioning issues in traditional compilers for EnergyPlus and\nModelica, unavailability of open source compilers and libraries for executing certain applications.\nSustainDC allows users to simulate the electrical and thermo-fluid behavior of large DCs directly in\nPython. Unlike other environments that rely on precompiled binaries or external tools, SustainDC is\neasily end-user customizable and fast. It enables the design, configuration, and control benchmarking\nof DCs with a focus on sustainability. This provides the ML community with a new benchmark\nenvironments specifically for Heterogeneous MARL in the context of DC operations, allowing for\nextensive goal-oriented customization of the MDP transition function, state space, actions space, and\nrewards."}, {"title": "Data Center Operational Model", "content": "Figure 1 shows the typical components of a DC operation modeled in SustainDC. Workloads are\nuploaded to the DC from a proxy client. For non-interactive batch workloads, a fraction of these\njobs can be flexible or delayed to different time periods. This creates a scheduling problem where\nworkloads can be delayed to some time of the day when Grid Carbon Intensity (CI) is lower.\nNext, as the servers (IT system) in the DC process these workloads, they generate heat, which needs\nto be removed. Hence, a complex HVAC system comprising multiple components is set up to cool\nthe IT system. As shown in Figure 2, the warm air leaves the servers and rises up by convection. Due\nto the forced draft of the HVAC fan, this warm air enters the Computer Room Air Handler (CRAH)\n(shown using red arrows) where it is cooled down to an optimal setpoint by a heat exchange process\nusing a \"primary\" chilled water loop. The chilled air is then sent back to the IT room using a plenum\nlocated underneath the DC (shown using blue arrows). The warm water then goes back to the Chiller\nwhere this heat is transferred by another heat exchange process to the \"secondary\" chilled water loop\nthat carries the heat to a Cooling Tower. The cooling tower fan operates at a certain speed to reject\nthe heat from the secondary chilled water to the outside environment. This speed, and hence the\nenergy consumption of the fan, is a function of the inlet temperature of the secondary chilled water\nloop at the cooling tower, the required setpoint for the outlet temperature, the outside air temperature,\nand humidity. Thus, depending on the external Weather and Workload processed, the IT and cooling\nsystem consume Grid Energy. Choosing the optimal cooling setpoint for the CRAH has the potential\nto reduce the carbon footprint of the DC. Furthermore, the optimal cooling setpoint also influences\nthe energy efficiency of the servers (Sun et al. (2021)).\nOften, larger DCs include onsite Battery Banks. Batteries can be charged from the grid during low\nCI periods. During higher CI periods, they provide auxiliary energy to the DC. This creates an\nopportunity to solve a decision problem where we optimally choose the time intervals during which\nthe battery charges versus when it is used to provide auxiliary energy.\nWe observed that these three control problems are related in a cascading manner. This motivates the\ndevelopment of testbeds or environments where we can test different multiagent control approaches\nto reduce the carbon footprint and other associated sustainability metrics of interest."}, {"title": "SustainDC environment overview", "content": "We provide a high-level overview of the SustainDC in Figure 3. We identify the three main environ-\nments developed in Python and highlight their individual components, customization capabilities,\nand associated control problems. The Workload Environment models and controls the execution\nand scheduling of delay-tolerant workloads within the DC. Inside the Data Center Environment,\nthe servers housed in the IT room cabinets process these workloads. It simulates the electric and\nthermal-fluid behavior, and the resulting heat generated from running these jobs is conducted to the\noutside environment by means of a set of HVAC cooling components. The Battery Environment\nsimulates charging from the grid during off-peak hours and provides auxiliary energy to the DC\nduring peak grid carbon intensity periods. The detailed physics-based implementation of the individ-\nual environments is provided in the supplemental document. The customization can be completely"}, {"title": "Workload Environment", "content": "The Workload Environment (EnvLS) manages the execution and scheduling of delayable workloads\nwithin the DC. It does this by streaming workload traces (measured in FLOPs of compute) over\na specified period. SustainDC includes a collection of open-source workload traces from Alibaba\nAlibaba Group (2017) and Google Google (2019) data centers. Users can customize this component\nby adding new workload traces to the appropriate folder (data/Workload) or specifying a path to\nexisting traces in the dc_config.json.\nCertain workloads are flexible, meaning they can be rescheduled within an allowable time horizon.\nThese tasks, such as update tasks or backup tasks, do not need to be executed immediately and can\nbe delayed based on their urgency or Service-Level Agreement (SLA). This flexibility allows the\nworkload to be shifted to periods when the grid's CI is lower, thereby reducing the DC's overall\ncarbon footprint (CFP).\nUsers can also customize the CI data. By default, we provide a one-year CI file for the following\nstates: Arizona, California, Georgia, Illinois, New York, Texas, Virginia, and Washington. These\nlocations were selected because they are where most data centers are situated. The carbon intensity\nfiles are extracted from eia.gov (https://api.eia.gov/bulk/EBA.zip) and located in the folder\ndata/CarbonIntensity.\nLet \\(B_t\\) be the instantaneous DC workload trace at time t, with X% of the load being rescheduled up\nto N simulation steps into the future. The goal of an RL agent (Agents) is to observe the current\ntime of day (\\(S_{Ct}\\)), the current and forecast grid CI data (\\(Cl_{t...t+L}\\)), and the amount of\nrescheduled workload left (\\(D_t\\)). Based on these observations, the agent decides an action \\(A_{ls,t}\\) (as shown in Table\n1) to reschedule the flexible component of \\(B_t\\), minimizing the net CFP over N steps."}, {"title": "Data Center Environment", "content": "The Data Center environment (EnvDC) includes an exhaustive set of models and associated specifi-\ncations that can be configured. For IT level design, SustainDC allows users to define the dimensions\nof the IT Room, the arrangement of the server cabinets (including the number of rows and cabinets"}, {"title": "Battery Environment", "content": "The Battery Environment (EnvBAT) is based on models for charging and discharging batteries, such\nas \\(f_{charging}(BatSoc, \u03b4t)\\) from Acun et al. (2023). Parameters for these components, as well as the\nbattery capacity, can be specified in dc_config.json.\nThe goal of the RL agent (AgentBAT) is to manage the battery's state of charge (BatSoct) effectively.\nBased on the net energy consumption (Ehvac + Eit) from the Data Center environment, the time\nof day (SCt), the battery's state of charge (BatSoct), and the forecast grid CI data (CIt...t+L), the\nagent must decide on an action Abat,t (as shown in Table 1). The actions include whether to charge\nthe battery from the grid, do nothing, or provide auxiliary energy to the data center, all aimed at\nminimizing the overall carbon footprint."}, {"title": "Heterogeneous Multi Agent Control Problem", "content": "While SustainDC allows the user to solve the individual control problems for the three environments,\nthe goal of the paper is to establish a multi-agent control benchmark which allows us to jointly\noptimize the CFP by considering the joint actions of all the three agents (AgentLS, AgentDC, and\nAgentBAT). The sequence of operations for the joint multi-agent and Multi-Environment functions\ncan be represented as:\n\\(Agent_{LS}: (SC_t \u00d7 CI_t \u00d7 D_t \u00d7 B_t) \u2192 A_{ls,t}\\)  (1)\n\\(Agent_{DC} :(SC_t \u00d7 t_{db} \u00d7 t_{room} \u00d7 E_{hvac} \u00d7 E_{it} \u00d7 CI_t) \u2192 A_{dc,t}\\)  (2)\n\\(Agent_{BAT} : (SC_t\u00d7 Bat\\_SoC \u00d7 CI_t) \u2192 A_{bat,t}\\)  (3)\n\\(Env_{LS} :(B_t \u00d7 A_{ls,t}) \u2192 B^t\\)  (4)\n\\(Env_{DC} :(B^t \u00d7 t_{ab} \u00d7 t_{room} \u00d7 A_{dc,t}) \u2192 (E_{hvac}, E_{it})\\)  (5)\n\\(Env_{BAT} :(Bat\\_SoC \u00d7 A_{bat,t}) \u2192 (Bat\\_SoC, E_{bat})\\)  (6)\n\\(CFP_t =(E_{hvac} + E_{it} + E_{bat}) \u00d7 CI_t\\)  (7)"}, {"title": "Rewards", "content": "While CFP reduction is the default objective in SustainDC, the reward formulation is highly\ncustomizable and allows users to consider other objectives, including total energy usage and total\noperating cost across all the DC components and water usage. Primarily, we consider the following\ndefault rewards for the three environments (EnvLs, EnvDC, EnvBAT)\n\\((r_{LS}, r_{DC}, r_{BAT}) =\n(CFP_t+ LS_{Penalty}), -(E_{hvac,t} + E_{it,t}), -(CFP_+)\\)\nHere, \\(LS_{Penalty}\\) is a penalty attributed to the Load Shifting Agent (AgentLS) in the Workload\nEnvironment (EnvLS) if it fails to assign the flexible workloads that were supposed to be rescheduled\nwithin the time horizon N. This implies that if Dt is positive at the end of a horizon N, we assign\n\\(LS_{Penalty}\\). The details for evaluating \\(LS_{Penalty}\\) is discussed in the supplemental document. The\nuser can choose to use any other reward formulation by subclassing the base reward class inside\nutils/reward_creator.py.\nBased on the individual rewards, we can formulate an independent or a collaborative reward structure\nwhere each agent gets partial feedback in the form of rewards from the other agent-environment pair.\nThe collaborative feedback reward formulation for each agent is formulated as:\n\\(R_{LS} = a * r_{Ls} + (1 \u2212 a)/2 * r_{DC} + (1 \u2212 a)/2 * r_{BAT}\\)\n\\(R_{DC} = (1 - a)/2 * r_{Ls} + a * r_{DC} + (1 - a)/2 * r_{BAT}\\)\n\\(R_{BAT} = (1 - a)/2 * r_{Ls} + (1 \u2212 a)/2 * r_{DC} + a* r_{BAT}\\)\nHere a is the weighting parameter. The reward-sharing mechanism allows the agents to estimate the\nfeedback from their actions in other environments for independent critic multiagent RL algorithms (e.g.\nIPPO de Witt et al. (2020)). For example, the adjusted CPU Load Bt affects the data center energy\ndemand, Ecool + Eit which in turn affects the battery optimizer's decision to charge or discharge\nresulting in a particular net CO2 Footprint. Hence, we decided to investigate a collaborative reward\nstructure. We conduct a set of ablation experiments with different values of a to understand whether\nperformance can be via reward sharing mechanism."}, {"title": "Evaluation Metrics and Experimental Settings", "content": "We consider five metrics when evaluating different RL approaches on SustainDC. CO2 footprint\n(CFP) indicates the cumulative carbon footprint of the DC operation over the period of evaluation.\nHVAC Energy refers to the amount of energy consumed across the DC cooling components including\nthe chiller, pumps and cooling tower. IT energy refers to the amount of energy consumed across\nthe DC servers. Water Usage refers to the chilled water that is recirculated through the cooling\nsystem. In certain DCs, the availability of chilled water from a central plant is limited, and efficient\nutilization of this resource helps lower the water footprint of the DC. Task Queue: In our approach,\nsince we reschedule workloads over time, this metric keeps track of how many FLOPs of compute\nare accumulating that need to be rescheduled at a later time period under lower CI. Higher values of\nTask Queue indicate worse SLAs in DCs.\nWe ran the experiments on an Intel(R) Xeon(R) Platinum 8470 server with 104 CPUs using 4 threads\nper training agent. All the hyperparameter settings for the benchmark experiments are provided in\nthe supplemental document. The codebase and documentation are already linked with the paper."}, {"title": "Benchmarking Algorithms on SustainDC", "content": "The purpose of SustainDC is to understand the benefits jointly optimizing the Workload, Data Center\nand Battery Environments for reducing the operating CFP of a DC. To investigate this idea, first, we\nconsider the net operating CFP for instances where we only evaluate with a trained RL agent for one\nof the SustainDC environment while employing the baseline methods (B*) for the other environments.\nThe baseline method (BLS) for the Workload Environment (EnvLS) simply implies that no workload\nis moved over the horizon, which is the current standard in the majority of the DCs. For the Data\nCenter Environment (EnvDC), we consider the industry standard ASHRAE Guideline 36 as the\nbaseline (BDC) Zhang et al. (2022). For the Battery Environment (EnvBAT), we consider a real time\nadaptation of the method in Acun et al. (2023) as the baseline (BBAT ). Next, we perform ablations\non the collaborative reward parameter a. Finally, then benchmark variants of the multi-agent RL\napproach. This includes multiagent PPO Schulman et al. (2017) with independent critic associated\nwith each actor network (IPPO) de Witt et al. (2020), a centralized critic that has access to states and\nactions from other MDPs (MAPPO) Yu et al. (2022). Given the heterogeneous nature of the action\nand observation spaces for SustainDC, we also benchmark several Heterogeneous multi-agent RL\n(HARL) Zhong et al. (2024) approaches including HAPPO (Heterogeneous Agent PPO), HAA2C\n(Heterogeneous Agent Advantage Actor Critic), HAD3QN (Heterogeneous Agent Dueling Double\nDeep Q Network) and HASAC (Heterogeneous Agent Soft Actor Critic).\nIn Figure 4, we compare the relative performance of different RL algorithms on a radar chart based\non the evaluation metrics in Section 5. Since reporting absolute values may not convey much sense,\nwe plot the relative differences in RL agent performances, which provides insights in to the pros and\ncons of each approach. (We will provide the absolute values for these benchmark experiments in the\nsupplementary document in tabular format.) Hence, we normalize the absolute values of the metrics\nof interest for each benchmarking approach with respect to the average and standard deviation. We\nconsider the lowest values towards the periphery of the radar chart and higher values towards the\ncenter. Hence, the larger the area of each approach on the radar chart, the better is the approach w.r.t\nthe metrics of interest."}, {"title": "Single vs multi-agent Benchmarks", "content": "Figure 4a compares the relative performance of a single RL agent vs multi-agent RL benchmarks\nto motivate the use of a MARL approach for sustainable DC operation. Among single RL agent"}, {"title": "Limitations", "content": "The absence of an oracle that already knows the best results possible for the different environments\nmakes it difficult to quantify the threshold for performance compared to simpler environments. Also,\nfor computational speed in RL, we used reduced order models for certain components like pumps\nand cooling towers. Also, we could not exhaustably tune the hyperparameters for all the networks."}, {"title": "Conclusion", "content": "In this paper, we introduced SustainDC, a MARL benchmarking environments developed completely\nin Python. It allows the user to solve problems related to sustainable, cost and energy efficient data\ncenter operations. SustainDC allows the customization of every aspect of the data center. It allows a\ngreat degree of customization w.r.t. the RL reward design, which is an important problem that we\ninvite other researchers to collaborate on using SustainDC. We benchmark an extensive collection\nof single and multiagent RL algorithms on SustainDC across multiple geographical locations and\ncompare their performance. This can provide researchers with the background to sustainably control\ndata centers using reinforcement learning. This can also be used to benchmark hierarchical RL\nalgorithms given the complexity and constraints of this environment rooted in a real-world system."}]}