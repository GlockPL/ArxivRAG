{"title": "Multi-tool Integration Application for Math Reasoning Using Large Language Model", "authors": ["Zhihua Duan", "Jialin Wang"], "abstract": "Abstract-Mathematical reasoning is an important research direction in the field of artificial intelligence. This article proposes a novel multi tool application framework for mathematical reasoning, aiming to achieve more comprehensive and accurate mathematical reasoning by utilizing the collaborative effect of large language models (LLMs) and multiple external tools. Firstly, use a Math Tool to perform basic mathematical calculations during the inference process through interaction with LLM. Secondly, Code Tool can generate code fragments that comply with syntax rules and execute them, providing support for complex mathematical problems. Then, through the iterative reasoning of the CoT Tool, the logical coherence and accuracy of mathematical reasoning are enhanced. Ultimately, by using self consistency tools to select the final answer based on different parameters, the consistency and reliability of reasoning are improved. Through the synergistic effect of these tools, the framework has achieved significant performance improvement in mathematical reasoning tasks. We conducted experiments on the NumGLUE Task 4 test set, which includes 220 mathematical reasoning fill in the blank questions. The experimental results showed that, based on Math Tool, Code Tool, and CoT Tool, in Task 4 task,our method achieved an accuracy of 89.09,compared with the GPT3+FewShot baseline, Few Shot+ERNIE-4.0+self consistency improved by 49.09%, and compared with fine-tuning the Fine tuning baseline, Few Shot+ERNIE-4.0+self consistency improved by 52.29%", "sections": [{"title": "I. INTRODUCTION", "content": "Mathematical reasoning is an important field in artificial intelligence research, which solves complex mathematical problems through deduction and reasoning under the guidance of logic and mathematical rules. However, for computers, conducting mathematical reasoning remains a challenging task. In recent years, with the rapid development of large language models, utilizing their powerful language generation and comprehension abilities to assist mathematical reasoning has become a new research direction.\nRecent research has focused on improving the mathematical reasoning ability of Large Language Models (LLMs). By introducing Chain of Thinking (CoT) prompts, LLM has made progress in mathematical reasoning tasks. CoT prompts guide LLM to gradually solve problems, improving the accuracy and interpretability of reasoning. However, there are still some problems and limitations when dealing with scenarios such as common sense reasoning, formal logic, and algebraic compu- tation. Current research is still focused on simple arithmetic reasoning, and for more complex mathematical concepts and problems, Further research is needed to expand the scope and ability of mathematical reasoning. This article aims to propose a novel multi tool application framework for mathematical reasoning, utilizing a large language model driven approach and combining the collaborative effects of multiple external tools to achieve more comprehensive and accurate mathemat- ical reasoning.\nAs shown in Figure 1, our framework utilizes various external tools such as Math Tool, Code Tool, CoT Tool, and self consistency tools in the inference process through a large language model to provide diverse inference support.\nThe unique contribution of this paper lies in the implementa- tion of a self-consistency tool. As shown in Figure 2, based on the parameter configuration, the mathematical calculator, code executor, and thought chain tool are sequentially selected to obtain answers. If all three tools are used simultaneously, the answer with the highest occurrence count is chosen as the final answer. If each answer appears only once, the answer from the code is given priority based on the configured priority."}, {"title": "II. RELATED WORK", "content": "In mathematical reasoning tasks, the MultiTool CoT frame- work combines multiple external tools such as calculators and knowledge retrievers, significantly improving the performance of large language models in digital reasoning tasks [1]. Math- Prompt technology improves the performance of large lan- guage models on arithmetic problems by generating multiple algebraic expressions or Python functions to solve the same mathematical problem [2]. The use of prompt based learning paradigms can improve the performance of information extrac- tion tasks. CodeIE proposes a method to convert structured output into code form and uses a code generation language model to perform named entity recognition and relationship extraction tasks [3]. NumGLUE is a multitasking benchmark used to evaluate the performance of artificial intelligence sys- tems on eight different tasks, promoting cross task knowledge sharing [4]. MathWorld is a graph based semantic formalism"}, {"title": "III. METHODS", "content": "The mathematical reasoning multi tool application we pro- pose is an interactive framework that allows LLM to use multiple external tools during the reasoning process: Math Tool, Code Tool, Cot Tool, and self consistency Tool.\nIn Math Tool, the symbols used in prompts have little impact on model performance, which may be counterintuitive, but patterns as a means of enhancing task understanding [8] will prompt the model to generate correct output. Most importantly, text and patterns form a symbiotic relationship and play an important role in mathematical reasoning. Text helps generate useful patterns, The Math Tool is shown in Table I. such as extracting mathematical patterns, which enhance task under- standing and enable language models to generate text that helps solve tasks. The success of Math Tool is attributed to the interaction between text and patterns, applying extracted symbols to mathematical patterns. This is of great significance for further improving and optimizing the application of large language models.\nCode Tool is a Python code execution function, as shown in Table II. Its main function is to call Baidu Big Model Service to generate code snippets that comply with syntax rules based on user input prompts. The tool first retrieves Python function text by calling Baidu's Big Model service, and dynamically executes the code using the built-in function exec(). The exec() function is capable of executing complex Python statements, receiving Python code stored in strings or objects, and returning the processed answer, which is the result of the function execution.\nThe CoT Tool, as shown in Table III. Its function is to infer based on the input thinking chain prompt words by calling Baidu's big model service to obtain the result of thinking chain inference. This tool uses iterative reasoning to gradually extract the final answer from the reasoning text by calling Baidu's big model service again.\nThe self consistency tool implements a decision system that selects different answers based on given parameters. If the self consistency feature is enabled, the system will call three different tools: Math Tool), Code Tool, and CoT Tool. Firstly, the system will call the Math Tool, Code Tool, and CoT Tool to obtain three answers respectively, and add these answers to a list. Then, the system will count the number of times each answer appears in the list and select the answer with the most occurrences as the final answer. If each answer only appears once, the answer with the highest priority will be selected as the final answer based on the pre-set priority."}, {"title": "IV. EXPERIMENT", "content": "NumGLUE is a multitasking dataset consisting of 8 dif- ferent tasks. Task 1 is common sense+arithmetic, Task 2 is domain specific knowledge+arithmetic, Task 3 is common sense+quantitative, Task 4 is fill in the blank, Task 5 is reading comprehension+explicit numerical reasoning, Task 6 is reading comprehension+implicit numerical reasoning, Task 7 is quantitative natural language reasoning, and Task 8 is arithmetic problem. These tasks involve common sense, do- main specific knowledge, and quantitative reasoning Different aspects such as fill in the blank questions and reading com- prehension. Through this dataset, the performance of different models on various tasks can be evaluated.\nAs shown in Table IV, Task 4 is a Fill in the blank dataset, which retrieves questions from an arithmetic question bank [9] [10] [11],and converts them into the format of fill in the blank questions. Require the generation of correct fill in the blank answers based on the given context, and provide understanding and answers to mathematical problems through fill in the blank questions.\nThis dataset consists of three parts: training set, validation set, and test set. There are 770 samples in the training set, 110 samples in the validation set, and 220 samples in the test set. This article uses Few Shot+LLms to directly test 220 samples from the test set.\nB. Method comparison\nAs shown in Table V, ERNIE-4.0 has achieved good per- formance through self consistency constraints. Under the self consistency constraint, if the results of each tool appear the same number of times, the reasoning answer of the thought chain CoT is prioritized, and the performance of ERNIE-4.0 reaches 75.9. When obtaining the results of the generated code function first, the performance further improves to 80.45. In the case of ERNIE-4.0, by combining Math Tool, CoT Tool, and Code Tool to prioritize obtaining the results of Math Tool, the performance of ERNIE-4.0 reached an impressive 89.09. Compared to the GPT3+FewShot baseline, ERNIE-4.0 improved by 49.09% (=89.09-40). Compared to fine-tuning the Fine tuning baseline, ERNIE-4.0 improved by 52.29% (=89.09-36.8)"}, {"title": "V. CONCLUSION", "content": "This study successfully implemented a multi tool appli- cation framework for mathematical reasoning based on a large language model, which utilizes multiple external tools during the reasoning process, including Math Tool, Code Tool, and CoT Tool. Math Tool can perform basic mathematical calculations, code executor tools can generate code fragments that conform to syntax rules and execute them, and CoT Tool obtain the results of the inference chain through iterative reasoning. The synergistic effect of these external tools has enabled our framework to perform well in mathematical rea- soning tasks. The design of this framework is universal and can be applied to various tasks by extending more external tools. Future work can further explore and optimize the selection and integration of external tools within the framework to improve inference efficiency and performance, and apply the framework to a wider range of fields and practical scenarios."}]}