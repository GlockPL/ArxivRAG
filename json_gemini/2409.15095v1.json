{"title": "Zero-Cost Whole-Body Teleoperation for Mobile Manipulation", "authors": ["Daniel Honerkamp", "Harsh Mahesheka", "Jan Ole von Hartz", "Tim Welschehold", "Abhinav Valada"], "abstract": "Demonstration data plays a key role in learning complex behaviors and training robotic foundation models. While effective control interfaces exist for static manipulators, data collection remains cumbersome and time intensive for mobile manipulators due to their large number of degrees of freedom. While specialized hardware, avatars, or motion tracking can enable whole-body control, these approaches are either expensive, robot-specific, or suffer from the embodiment mismatch between robot and human demonstrator. In this work, we present MoMa-Teleop, a novel teleoperation method that delegates the base motions to a reinforcement learning agent, leaving the operator to focus fully on the task-relevant end-effector motions. This enables whole-body teleoperation of mobile manipulators with zero additional hardware or setup costs via standard interfaces such as joysticks or hand guidance. Moreover, the operator is not bound to a tracked workspace and can move freely with the robot over spatially extended tasks. We demonstrate that our approach results in a significant reduction in task completion time across a variety of robots and tasks. As the generated data covers diverse whole-body motions without embodiment mismatch, it enables efficient imitation learning. By focusing on task-specific end-effector motions, our approach learns skills that transfer to unseen settings, such as new obstacles or changed object positions, from as little as five demonstrations. We make code and videos available at http://moma-teleop.cs.uni-freiburg.de.", "sections": [{"title": "I. INTRODUCTION", "content": "While robots have reached the hardware capabilities to tackle a wide range of household tasks, generating and executing such motions remains an open problem. The efficient collection of diverse robotic data has become a key factor in teaching such motions via imitation learning [1]\u2014[5]. Although a wide variety of interfaces, teleoperation methods, and kinesthetic teaching approaches exist for static manipulators, collecting demonstrations for mobile manipulation platforms is still challenging. Their large number of degrees of freedom (DoF) often overwhelm standard input methods such as joysticks and keyboards or lead to a large cognitive load, trying to coordinate all the necessary buttons and joysticks. While motion tracking systems [6]-[9] and exoskeletons [4], [10]\u2013[12] provide more intuitive interfaces, they are confronted with the correspondence problem if the morphology of robot and human do not match. Furthermore, exoskeletons are highly specialized, expensive equipment, and tracking-based methods restrict the operator to staying within the tracked area, not allowing them to move freely with the mobile robot and having to operate from afar."}, {"title": "II. RELATED WORK", "content": "Teleoperation for mobile manipulation faces the difficulty of operating a large number of degrees of freedom. As a result, large data collection efforts have separated base and arm navigation [22]-[25], thereby unable to focus on more complex mobile manipulation tasks that require base and arm coordination. While mouse and keyboard [9], [26] or mobile phones [16] can be used to send commands, generating coordinated motions for all degrees of freedom simultaneously becomes highly challenging. Thus, current approaches commonly resort back to separated base and arm motions [16], [20], [23]. Joystick teleoperation configurations commonly use shoulder buttons to switch between control modes, overcrowding the functionality of the buttons, as shown in Fig. S.1.\nExoskeletons [4], [10]-[12], [17] or sophisticated avatars [27], [28] can reduce the embodiment correspondence problem, by constraining the human motions. However, they are expensive, robot specific, and cannot make use of robot kinematics that exceed human motions. While motion capture systems [6]-[8] have been successfully used to map human motions to whole-body motions for mobile manipulators, this requires specialized hardware and leads to a correspondence problem if the morphology of robot and human do not match. While keypoint tracking from RGB-D data can alleviate the need for expensive hardware [9], it still has to deal with the correspondence problem and may suffer from less precise estimation. More generally, the operator must remain in the tracked workspace and cannot lead the robot over spatially extended tasks. VR interfaces [9], [18], [29]-[31], especially with integrated joystick functionalities, offer enough flexibility to provide simultaneous base and end-effector commands without overwhelming the user. However, they still require hardware, a camera, and a tracking setup. Approaches without external tracking setup still resort to restricting commands such as only allowing base translation but not end-effector translation [20].\nA number of approaches do not track full motions but infer specific function parameters [31], use predefined gait sequences [18], or match to movement primitives [29]. In contrast, we fully delegate the control of the remaining body parts to a reinforcement learning agent, such that the user only has to focus on the end-effector. While a number of previous approaches have focused on pure end-effector poses, they collect them with human-carried end-effectors [10], [21], making it infeasible to collect further robot states or to actually teleoperate a full robot.\nKinesthetic teaching, in which the passive joints of the robot are physically moved by humans, avoids any embodiment mismatches. While it is very efficient for static manipulation [3], [32], it is not feasible to physically move full mobile robot platforms. Though disturbance observer models have been used to enable some degree of compliance on humanoid robots [33]. For mobile manipulators, Zhao et al. [19] combine a whole-body impedance controller with kinesthetic teaching for the end-effector, with adaptive stiffness for locomotion and manipulation modes. Xing et al. [34] use admittance and nullspace control to teach carrying heavy objects. However, as these controllers have no awareness of their environment, they are unable to avoid collisions. In contrast, our approach is able to avoid collisions and position itself anticipatory for the continuation of the end-effector motions."}, {"title": "III. MOMA-TELEOP", "content": "We aim to reduce the complexity of operating mobile manipulators via existing, standard interfaces. To do so, we develop three components: a user interface that takes in 6-DoF signals, an inference module that translates these signals into end-effector motions and a base agent that moves the robot's base to support the desired end-effector motions. An overview of the proposed approach is depicted in Fig. 2. The result is a modular whole-body teleoperation system that reduces the complexity for the operator to pure end-effector control."}, {"title": "A. Background: Learning Feasible Base Motions", "content": "We use our previously developed N2M\u00b2 approach to decouple end-effector motions from the remaining joint motions and delegate these motions to a reinforcement learning agent for the base of the robot [13], [14]. This agent, shown in blue in Fig. 2, receives a desired end-effector motion, consisting of translation and orientation velocities $v_{ee}$ to the next desired pose as well as a more distant end-effector subgoal $g$ in the form of a 6-DoF pose in the base frame of the robot that indicates the longer-term plan. The agent then generates velocities $v_{b}$, $u_{torso}$ for the base and torso of the robot and uses inverse kinematics for the remaining arm joints, thereby completing the whole-body motions. It also learns to regularize the speed at which the end-effector motions are executed through a scaling factor $||V_{ee}||$. Based on a local occupancy map, it learns to avoid obstacles. At test time, the agent generalizes to unseen end-effector motions and can dynamically react to static and dynamic obstacles, which was demonstrated in a number of works using these policies [5], [35]. It is this ability to enable arbitrary, unseen end-effector motions that we leverage in this work."}, {"title": "B. Interfaces", "content": "Given the base agent, the human operator is tasked with generating 6-DoF velocities for the end-effector. We implement methods for a range of common, low-cost interfaces, including joysticks and hand guidance. However, our approach is compatible with any modality that can generate such a signal. Importantly, these interfaces are extremely low-cost and mobile, without any workspace restrictions of cameras or tracking systems for the operator.\nJoystick: As we reduce the required inputs to 6-DoF for the end-effector, we can comfortably serve all inputs simultaneously on a standard Dualshock3 joystick, shown in Fig. S.1. We use the left and right controller sticks together with two shoulder buttons for translation and orientation changes. These commands are applied in the frame of the wrist camera that is streamed to the user. Two additional buttons enable opening and closing of the gripper. Lastly, we add a button to switch to a higher-precision mode with smaller end-effector velocities, as discussed in Sec. III-C below. This results in a reduction from 18 buttons in default teleoperation down to 10 buttons that can all be operated simultaneously.\nHand Guidance: In this mode, a human can physically guide the manipulator arm to kinesthetically teach the robot. The physical guidance has the particular benefit of being able to demonstrate specific wrenches for contact-rich tasks. With our approach, we are able to extend this method from static arms to mobile manipulators. The operator moves the end-effector of the robot and we detect changes in translation and orientation of the end-effector as motion signals for the base agent. The gripper can be opened and closed via a button on the end-effector. To ensure safety, a deadman switch on the end-effector immediately stops the robot if it is released. As this requires the human operator to move next to the robot, avoiding collisions is essential for safe operation. The base agent detects the human as an obstacle in its LiDAR scan and reacts immediately to the human's movements. We verify this capability in our experiments in Sec. IV-A.\nArbitrary input modalities: While we provide implementations for joystick and hand guidance, our approach can be used with any input modality that can generate 6-DoF input signals, such as VR devices or SpaceMouses."}, {"title": "C. Inferring End-Effector Motions", "content": "After getting pose and velocity signals from the teleoperation interface, we transform these measurements into end-effector translation and orientation deltas consisting of a 3D velocity vector $u_{signal}$ and the change in orientation converted to a unit quaternion $q_{signal}$. To enable the reinforcement learning agent to make good decisions, we extrapolate these deltas into an end-effector motion $m_{ee}$. This motion consists of a vector of 6D end-effector poses, spaced at a fixed resolution of $d_{restraining} = 0.1m$ over a distance of up to $d_{g} = 1.5 m$ into the future (shorter if close to the final goal). From this motion, the agent infers the next desired end-effector velocities and the last pose as a subgoal. We update the inferred end-effector motions at high frequency from the latest user inputs, enabling quick reaction to changes\u00b9.\n1) User Signal: In the following we describe how we infer these directional and translation signals $u_{signal}$ and $q_{signal}$ from different interfaces.\nJoystick: We directly map the pressed joystick buttons to"}, {"title": "2) Functional form", "content": "Next, we transform the user signals to a longer end-effector motion to communicate the user intentions to the base agent. The end-effector motions that the robot has to execute locally follow linear motions and smooth curves. To achieve this, we chose a form of linear dynamic system to extrapolate the inferred signals, enabling the execution of arbitrary motions. In particular, we extrapolate the velocities by integrating a dynamic linear system to infer the end-effector motion $m_{ee} = [e_{et+i} \\in 1, ..., T]$:\n$e_{et+1} = (e_{epos} + q_{signal} * u_{signal}, q_{signal} * e_{eq})$.\nWe run this system for $T = max(||u_{signal}|| * \\frac{d_{g}}{d_{restraining}},5)$ steps, thereby matching the planning horizon $d_{g} = 1.5m$ used during training and producing shorter plans for small (unnormalized) velocity signals $||V_{signal}||$. Visualizations of the resulting motions are shown in Sec. S.4 and the video."}, {"title": "D. RL Base Agent", "content": "The resulting motions $m_{ee}$ are then provided to a pretrained N2M\u00b2 base agent, where we replace the end-effector motion module used during training with the one above. The agent is active whenever the human operator generates a signal and immediately pauses whenever the signal stops (all joystick buttons or hand guidance deadman switch released)."}, {"title": "IV. TELEOPERATION EXPERIMENTS", "content": "Robots: The HSR robot has an omnidirectional base and a 5-DoF arm, including a torso lift joint, resulting in 8-DoF. The FMM robot consists of an omnidirectional Ridgeback base with a lifting column and a Franka 7-DoF Arm, resulting in 11 DoF overall.\nTasks: We evaluate the approaches on a wide range of tasks that cover a diverse set of motions, contact-rich manipulation as well as operation in narrow spaces. We start all experiments from a default start configuration and fixed start position. To demonstrate the compatibility with different input modalities, we evaluate the HSR in teleoperation mode and the FMM with kinesthetic teaching via hand guidance. The tasks are described in Fig. 3 and Sec. S.3.\nBaselines: We compare our approach against a diverse set of approaches based on different input modalities. We focus on methods with comparably low setup costs to our approach. Joystick: On the HSR, we use the Dualshock3 joystick teleoperation package that was developed for the robot, shown in Fig. S.1. It uses buttons to directly send joint commands for the arms. Shoulder buttons serve as switches to change between base and arm commands. As a result, it is not possible to simultaneously send base and arm commands. Hand Guidance: On the FMM, we use static hand guidance control for the arm of the robot, combined with control of the base and lift joint via the joystick.\nVision Tracking [9] tracks human motions with an RGB-D camera and translates torso movements and relative hand movements to robot motions, using inverse kinematics to calculate the connecting arm joints.\nVR Tracking [9] translates motions from virtual reality handheld controllers to end-effector motions. We implement the approach with HTC Vision Pro with two lighthouse towers. For the FMM we set the Franka arm to a low stiffness, enabling it to safely make contact with the objects. Higher stiffness resulted in hardware safety violations upon contact.\nMetrics: We compare the average success rate (SR) over the attempted tasks. Failures can stem from reaching safety limits or collisions with the environment. Moreover, we measure the average completion time for the task, averaged over successful executions only."}, {"title": "A. Evaluation", "content": "We execute all methods five times per task, resulting in 20 episodes per robot and method. The evaluations are conducted by an operator with several hours of experience in all methods. The results are reported in Tab. II and shown in the video. We find that the static operation methods via joystick or hand guidance, as well as our approach, are able to complete all tasks successfully. The tracking approaches are efficient on tasks such as Pick & Place or Microwave with the fastest completion time in the former. However, for tasks that require higher precision or movement over larger distances and rotations, we find that the tracking approaches becomes too imprecise. Particular difficulties include the limited operator workspace, operation from the tracked area afar and the embodiment mismatch between operator and robot, see Sec. S.6 for additional details. As this resulted in frequent safety limit violations and emergency stops of the robots, we abstain from evaluating them on the remaining tasks to ensure the safety of the equipment.\nPure joystick operation is very robust and can efficiently complete tasks such as opening a microwave or door in which we can keep the relative end-effector pose constant and use pure base motion for translation and yaw changes. However, tasks such as opening the toolbox that requires backward movement together with arm translation and pitch changes of the end-effector become tedious, involving numerous switches between base and arm motions. Similarly, for static hand guidance, tasks such as opening a door become cumbersome, as they require repeated base repositioning. Moving the base while in contact with the door risks triggering safety limits of the Franka arm, requiring to release the door handle, move the base, then re-grasp. In contrast, we found MoMa-Teleop to enable continuous operation during these tasks. The human can operate directly next to the robot and the base agent is compliantly moving the base under consideration of the robot's kinematics and obstacles. Cleaning the table, we found hand guidance based methods to result in good tracking of the pattern with constant contact of the table. In contrast, with tracking based methods it was difficult to keep contact through the full scrubbing motion. Overall, we found that MoMa-Teleop facilitated substantially faster and more continuous task executions across tasks, robots and input modalities."}, {"title": "B. User Study", "content": "To evaluate the easy of use of the approaches for new users, we conduct a user study on the FMM robot for the Door Outwards and Fridge P&P tasks. We recruit six participants. Each participant receives a short, five minute introduction to each approach and is then given a practice attempt at the task. The user then completes three episodes for the best baseline, hand guidance, and our approach for each task. We change the order of the task and approach that each user starts with evenly and reverse the order of approaches for the second task. We instruct the users not to move the base while grasping an articulated object, as we found this to easily trigger safety limits of the arm. The results are reported in Fig. 4.\nWe found large differences in user behaviors, strategies and confindence. A particular challenge posed the understanding of joint limits, resulting in occasional failures with the arm joints locking for safety in both approaches, with an overall success rate of 91.7% for both approaches. The completion times confirm the relative results of an expert user. We find particularly large differences in the door opening task, which requires to follow specific motions over the large opening radius and, as a result, repeated base repositioning without a mobile base. Differences in the fridge task are less pronounced. As the fridge door can be opened from a static position, efficient base placement can complete the task with a single repositioning. However, even in such a more static task, users achieved an efficiency improvement of over 12% with our approach. One user found a particularly efficient strategy, outperforming the expert in both approaches on the fridge task, taking 34.3s with hand guidance and 25.7s with MoMa-Teleop, even pulling the user average below the expert value. Overall, MoMa-Teleop reduced average completion time by almost 40%."}, {"title": "V. IMITATION LEARNING", "content": "To learn end-effector motions from the collected demonstrations, we leverage TAPAS-GMM [15], a state-of-the-art imitation learning method based on Gaussian Mixture Models (GMM). We use the time-based variant of TAPAS-GMM, which models the gripper action and end-effector pose $e_{et}$ across time in multiple task-relevant coordinate frames. To this end, TAPAS-GMM first segments long-horizon tasks, such as open the drawer, into a series of shorter skills, such as grasp the handle and pull the handle. It then uses DINO features [37] to extract a set of object keypoints [38] from the robot's Intel RealSense D435 wrist camera. Subsequently, it automatically selects the relevant keypoints per skill and fits the set of demonstrations from the perspective of coordinate frames attached to the selected keypoints. During inference, these per-frame models are joined using the current keypoint poses to generate a combined model in the world frame. We then predict a full end-effector trajectory and step through it as long as the current end-effector pose is close enough to the last prediction. Otherwise, we repeat the last pose command.\nWe construct two policies: Whole-Body jointly fits the GMM to the recorded end-effector and base poses and uses inverse kinematics to solve for arm and torso joint position commands while tracking the base and end-effector motions. EE only models the gripper action, and end-effector poses $e_{et}$ and uses the same learned N2M\u00b2 base agent to convert the learned end-effector motions to whole-body motions.\nThis system enables us to rapidly learn new mobile manipulation tasks from only five demonstrations. The combined data collection with MoMa-Teleop and fitting of the models with TAPAS-GMM takes less than ten minutes in total."}, {"title": "A. Data Quality", "content": "We evaluate both policies across three tasks with hand guidance on the FMM robot: Clean Table, Door Outwards and an additional Open Drawer task. We collect five demonstrations with both the Hand Guidance + Joystick and MoMa-Teleop methods, then execute each policy for ten episodes per task. The results are presented in Tab. III.\nWe find that the we can learn robust motions from static hand guidance data for tasks where a consistent teleoperation strategy exists, such as Open Drawer or Clean Table. For tasks that require large base motions and repositioning, the resulting trajectories are more complex and exhibit greater variance. For Door Outwards, the handle needs to be released and the base repositioned, which happens at different times and positions for different trajectories, rendering the trajectories difficult to model. Consequently, end-effector, gripper, and base actions, are not temporally aligned across trajectories, making the policy mix up parts of the motions due to the more complex data distribution. Accurately fitting such data would require significantly more demonstrations. We experienced the same issue on the Clean Table task, when collecting data as a standard user would without first deciding on a consistent base positioning strategy. This resulted in a policy that is not sufficiently following the desired trajectory and struggling to coordinate base and end-effector.\nIn contrast, the data from MoMa-Teleop leads to smooth and consistent end-effector motions independent of the teleoperator's proficiency, as it removes the decision about base placements and allows to complete mobile manipulation motions without regrasping. Using its data, we are able to learn both successful pure end-effector motions as well as whole-body motions from few demonstrations due the reduced coordination effort required from the end-effector policy. Its data resulted in shorter trajectories and lower execution times for both policies, as the end-effector motions are always focusing on the task, in contrast the separated base movements of the static hand guidance data results in unnecessary end-effector movements while the arm is idle on top of the moving base. The remaining failures stem mostly from the accumulated noise of depth sensors, keypoint estimation and whole-body motions, resulting in insufficiently precise grasping."}, {"title": "B. Generalization", "content": "We further evaluate the policies' ability to generalize to new contexts. The keypoint and task-parameterized motions are object-centric, enabling direct transfer to different positioning of the objects. As such, the learned end-effector motions transfer directly to new contexts, with the base agent enabling the kinematic feasibility of the trajectory. In contrast, the whole-body policy jointly models the base motions and end-effector motions. Consequently, they are mutually dependent, for example due to the kinematic limits of the robot. Thus, they do not easily generalize to new contexts, such as a changed height of the drawer. Similarly, new obstacles would require the whole-body model to learn simultaneous obstacle avoidance across a wide range of different obstacle configurations. As such, both the components would require a lot of additional training data.\nTo evaluate this, we adapt the tasks with common scenarios, as they might occur in a household: we place the drawer at a different height and add a new obstacle at three different positions in front of the table to clean. The scenarios are shown in Sec. S.3 and the results are shown in Tab. III. We find that the whole-body policy does not to generalize to these scenarios, failing to reach the required end-effector poses from the learned base movement and colliding with the obstacles. In contrast, the EE policy directly adapts to these scenarios, with no drop in performance."}, {"title": "VI. CONCLUSION", "content": "We introduced a novel teleoperation approach for whole-body mobile manipulation from existing control modalities, at no additional cost. Our approach scales to extended spatial tasks as it requires no tracking of the operator or the robot. The method enables rapid execution and data collection across a wide range of tasks, including contact-rich manipulation. Combined with recent task-parameterized GMMs, we deployed the same system for autonomous execution of the learned tasks, generalizing to new situations from as little as five demonstrations. We made the code publicly available to facilitate future research."}]}