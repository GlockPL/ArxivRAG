{"title": "FLANEC: EXPLORING FLAN-T5 FOR POST-ASR ERROR CORRECTION", "authors": ["Moreno La Quatra", "Valerio Mario Salerno", "Yu Tsao", "Sabato Marco Siniscalchi"], "abstract": "In this paper, we present an encoder-decoder model leveraging Flan-T5 for post-Automatic Speech Recognition (ASR) Generative Speech Error Correction (GenSEC), and we refer to it as FlanEC. We explore its application within the GenSEC framework to enhance ASR outputs by mapping n-best hypotheses into a single output sentence. By utilizing n-best lists from ASR models, we aim to improve the linguistic correctness, accuracy, and grammaticality of final ASR transcriptions. Specifically, we investigate whether scaling the training data and incorporating diverse datasets can lead to significant improvements in post-ASR error correction. We evaluate FlanEC using the HyPoradise dataset, providing a comprehensive analysis of the model's effectiveness in this domain. Furthermore, we assess the proposed approach under different settings to evaluate model scalability and efficiency, offering valuable insights into the potential of instruction-tuned encoder-decoder models for this task.", "sections": [{"title": "1. INTRODUCTION", "content": "Generative Speech Error Correction (GenSEC) focuses on correcting linguistic errors in transcriptions of spoken language. In Automatic Speech Recognition (ASR) systems, GenSEC acts as an effective post-processing step to enhance the accuracy [1, 2]. This step is crucial when ASR systems are used as inputs to downstream applications such as machine translation or information retrieval, where linguistic correctness is essential for maintaining the quality and reliability of the final output [3]. While modern end-to-end ASR systems have significantly improved transcription accuracy, their primary focus on acoustic modeling often results in neglecting grammatical error correction [4]. These systems excel at capturing acoustic cues and accurately transcribing spoken words. However, the inherent challenges of speech-to-text conversion, including variability in speech patterns, accents, and background noise, can lead to disfluencies, repetitions, and other speech irregularities that compromise the correctness of the final text. Therefore, ASR outputs may require additional post-processing to address linguistic errors and improve the overall quality of the transcription. The primary objective of GenSEC is to correct these errors by exploiting the context provided by n-best hypotheses generated by ASR models and leveraging external linguistic knowledge to produce a more accurate transcription. An n-best list, in this context, is an ordered list of the top n possible transcriptions generated by an ASR model, each with its associated confidence score. Recent advancements in natural language processing (NLP) have shown that large language models (LLMs) can be effectively adapted for various tasks. In this context, the Flan-T5 model [5] has demonstrated strong performance through instruction tuning, a process that specifically trains a single model to perform a diverse set of tasks by following specific instructions. This method significantly enhances the model's generalization capabilities, enabling it to adapt to specific tasks with minimal adjustments. By leveraging Flan-T5's performance in understanding and generating natural language [6], we aim to address the inherent challenges of post-ASR correction.\nThis paper presents our system for Task 1 of the GenSEC Grand Challenge, which focuses on post-ASR correction using large language models (LLMs). We propose FlanEC (Flan-T5 for Error Correction), an encoder-decoder model built on Flan-T5 [5]. Our contributions are as follows:\n\u2022 We introduce FlanEC, an encoder-decoder model specifically instructed for GenSEC, trained at various scales from 250 million to 3 billion parameters.\n\u2022 We evaluate FlanEC using the HyPoradise dataset [7], providing a comprehensive overview of its capabilities across different ASR domains.\n\u2022 We explore the impact of scaling training data across all subsets of the HyPoradise dataset to assess whether training a single model on diverse domains can enhance performance across different subsets.\n\u2022 We investigate the effectiveness of efficient model adaptation (i.e., LoRA [8]) versus full fine-tuning (FT) to assess to which extent it can provide comparable performance to full fine-tuning in our task."}, {"title": "2. POST-ASR ERROR CORRECTION", "content": "Correcting linguistic errors is a fundamental challenge in NLP and has traditionally focused on written text, with significant advancements documented over the years [9]. LLMs have shown remarkable capabilities in understanding and generating natural language, making them well-suited for GenSEC tasks [10, 11, 5]. By integrating general knowledge and linguistic context, LLMs as a post-processing step can leverage the strengths of both ASR and NLP systems to enhance transcription accuracy."}, {"title": "2.1. Generative Speech Error Correction", "content": "GenSEC is an emerging field dedicated to enhancing the accuracy of ASR outputs by leveraging text-only information. It aims to generate a single correct transcription given one or more ASR hypotheses, each potentially accompanied by a confidence score. Previous works have demonstrated that integrating language models as an additional step on top of ASR models can significantly improve the quality of ASR transcriptions [1] with RNN-based LMs showing better performance than n-gram LMs [2]. Recent advancements in LLMS have further corroborated this hypothesis, showing that specific prompts, together with fine-tuning, can enhance the quality of the final transcriptions [12]. By combining the technical strengths of ASR systems with the advanced linguistic processing power of LLMs, GenSEC can lead to transcriptions that are more accurate and linguistically coherent.\nIn this context, the HyPoradise dataset [7] serves as a comprehensive benchmark for post-ASR error correction tasks, spanning several ASR domains to assess GenSEC models under various conditions. The dataset provides a standardized framework for assessing the effectiveness of different approaches to post-ASR error correction, emphasizing the linguistic aspects of the task without requiring additional acoustic modeling or alignment. Each spoken utterance within the dataset is accompanied by an n-best list of hypotheses generated by an ASR model (e.g., [4]), along with the corresponding reference transcription. Our work aims at exploring the potential of encoder-decoder language models to enhance GenSEC tasks using the HyPoradise dataset."}, {"title": "2.2. LLMs in GenSEC", "content": "Instruction tuning has highlighted the adaptability of LLMs, enabling them to effectively shift between different tasks by leveraging explicit instructions [13]. This approach enhances the model's ability to handle diverse NLP challenges efficiently. In-context learning [14, 15, 16] is a notable capability of LLMs; it allows models to learn directly from examples provided within a prompt, adapting to perform specific tasks using only the information given in the input. The emerging abilities of LLMs in this domain demonstrate their potential for handling a wide range of linguistic tasks [17]. In the context of GenSEC, LLMs are used to generate corrections for ASR hypotheses, simultaneously leveraging the context provided by the ASR outputs and the linguistic knowledge embedded in the model [18, 19]. Recent studies [20], however, have demonstrated that adapting model parameters outperforms in-context learning across various model scales. Thus, we focus on using an open-source model to tune parameters for post-ASR error correction.\nWhile most LLMs are decoder-only, we focus on an encoder-decoder architecture that is open-source, available in various sizes, and capable of effectively following instructions. Flan-T5 [5] meets these criteria and has been pre-trained to solve 1,836 diverse tasks, significantly improving T5 [21] ability to generalize across different, even unseen, instructions. These characteristics make Flan-T5 particularly well-suited for GenSEC tasks, where the model must adapt to a range of linguistic contexts to generate accurate transcriptions by exploiting the context provided through ASR hypotheses. Flan-T5 has demonstrated superior performance in handling instruction-based tasks, even when compared to decoder-only LLMs of similar parameter scale, as evidenced by its results in the InstructEval benchmark [22]."}, {"title": "3. FLANEC", "content": "The proposed FlanEC is an encoder-decoder model built on Flan-T5, specifically instructed for GenSEC tasks. We define a specific instruction template and use it to create FlanEC using HyPoradise dataset and leveraging the n-best lists generated by an ASR model."}, {"title": "3.1. Task prompt", "content": "The prompt for FlanEC is designed to instruct the model to generate a single correct transcription from a set of ASR hypotheses. To this end, the HyPoradise dataset, denoted as D, is used. It is composed of different subsets D1, D2, ..., Dm.\nA sample sij \u2208 Di, namely the j-th sample in the subset Di, consists of (i) the n-best list of ASR hypotheses Hij = {hij1, hij2,..., hijn}, and (ii) the corresponding reference transcription rij. FlanEC is trained to map Hij \u2192 rij by generating a single output written sentence that aligns with the reference transcription.\nThe task template is designed to be consistent with the original Flan-T5 approach, using natural language instructions without specific special tokens or tags [5]. The following prompt is intended to guide the model in generating the correct transcription from the n-best list:\nInput: Generate the correct transcription for the following n-best list of ASR hypotheses:\nhij1\nhij2\n...\nhijn\nOutput: rij\nWhere i denotes the subset index and j denotes the sample index within the subset. In our experiments, we use n = 5"}, {"title": "3.2. Fine-tuning strategy", "content": "In line with Flan-T5, we evaluate two training settings:\n\u2022 SD: Single Dataset, where we train a separate model on each subset of the HyPoradise dataset (e.g., ATIS, Tedlium3, etc.).\n\u2022 CD: Cumulative Dataset, where we train a single model on the combined subsets of the HyPoradise dataset.\nIn SD, we train a separate model on each Di \u2208 D, whereas in CD, we train a single model on the union of all subsets, \u222ai=1m Di. While SD has been the standard approach for training GenSEC models [7], in CD settings, we aim to create an omnibus model capable of generalizing across different ASR domains. Our goal is to determine if FlanEC can generalize by leveraging multiple domains, similar to how Flan-T5 generalizes across different tasks. By combining diverse datasets such as ATIS (Airline Travel Information Systems), Tedlium3 (transcripts from TED talks), Switchboard (telephone conversation transcripts), and other training subsets in the HyPoradise dataset, we create a more varied training set. This approach utilizes the diversity of the training data to enhance the model's ability to correct errors across various ASR subsets. We hypothesize that training a single model on these diverse datasets will lead to better generalization and improved performance across multiple ASR domains. By learning from the combined data, the model can benefit from shared knowledge, potentially enhancing its overall performance.\nTo explore efficient fine-tuning methods, we also employ LoRA [8], which has shown promising results in adapting LLMs to specific tasks. LoRA involves training adapters on top of a pre-trained model, allowing for cost-effective fine-tuning without altering the original model's parameters. These adapters consist of additional, task-specific parameters trained to adapt the model to the target task. While LoRA is computationally efficient, it may not always match the performance of full fine-tuning (FT). We investigate this trade-off in the experimental section by training FlanEC using both LoRA and FT and comparatively evaluating their performance."}, {"title": "4. EXPERIMENTS", "content": "We evaluate FlanEC on the HyPoradise dataset, focusing on post-ASR error correction across different ASR domains. We adapt the model at various scales, ranging from 250 million to 3 billion parameters, to assess the impact of scaling model parameters on GenSEC performance. We compare the model's performance under different training settings, including SD and CD, to evaluate its generalization capabilities across diverse ASR domains. Additionally, we explore the effectiveness of LoRA versus full fine-tuning (FT) to assess the model's scalability and efficiency in the GenSEC task\u00b9. We refer to adaptation as the process of updating model parameters, whether through the LoRA method or full fine-tuning."}, {"title": "4.1. HyPoradise Dataset", "content": "Task 1 of the GenSEC Grand Challenge focuses on post-ASR error correction using the HyPoradise dataset [7]. This benchmark is designed to explore LLMs' capabilities in correcting ASR errors across various domains. It includes eight different subsets, each representing a distinct domain:\n1. WSJ [24]: Business news and financial data readings.\n2. ATIS [25]: Airline Travel Information Systems queries.\n3. CHiME-4 [26]: Noisy speech data from different environments, part of the CHiME-4 challenge.\n4. Tedlium-3 [27]: Transcripts from TED talks.\n5. CV-accent [28]: Accented speech data from the English portion of the Common Voice dataset.\n6. SwitchBoard [29]: Telephone conversation transcripts.\n7. LRS2 [30]: Audio clips from BBC programs.\n8. CORAAL [31]: Interviews containing accented speech from regional varieties of African American English.\nThe diversity of the data ensures that models are evaluated across a wide range of ASR domains, providing a comprehensive assessment of their capabilities. Each subset contains both training and test data, comprising spoken utterances, n-best lists generated by an ASR model, and reference transcriptions. In our experiments, we train FlanEC-CD on \u222ai=1m Di and evaluate its performance on the test subsets of the HyPoradise dataset. For FlanEC-SD, we train a separate model on each subset Di \u2208 D and evaluate its performance on the corresponding test subset."}, {"title": "4.2. Experimental Setup", "content": "We train FlanEC in three different sizes: base (250M parameters), large (800M parameters), and extra-large (3B parameters). The models are fine-tuned using the pre-trained Flan-T5 checkpoint, with the number of parameters determined by the model size. We experiment with both LoRA and full fine-tuning (FT) to evaluate the model's performance under different training settings. In the case of LoRA, we use adapter layers for all fully connected layers available in the corresponding models. In all cases, we used the AdamW optimizer [32] and an effective batch size of 16.\nWhen performing full fine-tuning, we use a maximum learning rate of 5 \u00d7 10-5. For LoRA adaptation, we use a higher learning rate of 1 \u00d7 10-4 [8]. In both cases, we use a linear learning rate scheduler with a warm-up phase. Specifically, the learning rate is linearly increased from zero to the maximum value over the first 10% of the total training steps (warm-up phase). After the warm-up phase, the learning rate is linearly decreased back to zero for the remainder of the training. For FlanEC-CD, we set the maximum number of training epochs to 2, given the higher amount of training data and computational resources required for training on the combined dataset. For FlanEC-SD, we train each model for up to 10 epochs, following established best practices. In all cases, the best model is selected based on the validation WER and used for evaluation on the test subsets of the HyPoradise dataset. All experiments are conducted on 2 NVIDIA A100 GPUs with 80GB of memory each. We use the Hugging Face Transformers library [33] for model training and evaluation.\nEvaluation Metrics: We evaluate the models using the Word Error Rate (WER) metric, which measures the percentage of words in the model's output that differ from the reference transcription. We follow the implementation provided in the original HyPoradise repository\u00b2 to have consistent results."}, {"title": "5. RESULTS AND DISCUSSION", "content": "In this section, we present and analyze the performance of FlanEC on the HyPoradise dataset, focusing on post-ASR error correction across different ASR domains. The results are summarized in Table 2, where we compare the WER scores of FlanEC under different training settings, model sizes, and fine-tuning strategies. For an overall comparison, we have included the average WER across all datasets in the last row of Table 2. For a comprehensive evaluation, we also incorporate baseline results from the original HyPoradise [7] experiments, which are obtained using ASR without error correction. In addition, we evaluate two LLM-based approaches without fine-tuning: (1) Flan-T5 in a zero-shot (ZS) setting, using the 3B parameter model provided with the target sample and its 5-best hypotheses, and (2) in-context learning (ICL) using ChatGPT (gpt-3.5-turbo-0125), where the model also receives 5 training examples, each containing n-best lists and transcriptions. These training examples are selected based on the similarity of their first hypotheses to the target sample. We did not evaluate Flan-T5 in the ICL setting because prior research [34] has shown that encoder-decoder models struggle with ICL and even exhibit decreased performance when provided with in-context examples. These general-purpose LLM-based approaches, without task-specific adaptation, provide a broader context for evaluating post-ASR correction performance.\nSingle Dataset Models (SD): FlanEC-SD results indicate that scaling up the model size generally leads to improvements in WER. For instance, the 3B parameter model achieves the best performance on multiple datasets such as ATIS, CHiME-4, and CV-accent, with significant WER reductions compared to smaller models. The base model (250M parameters) trained with LoRA achieves better WER scores than the FT counterpart on the majority of subsets. This may be due to the base model's limited capacity, which can lead to overfitting when fine-tuning all model parameters using only a specific subset of data. For example, on SwitchBoard, the LoRA-adapted base model achieves a WER of 15.0%, compared to 17.2% for the fully fine-tuned counterpart."}, {"title": "Cumulative Dataset Models (CD):", "content": "The 3B FlanEC-CD model with full fine-tuning achieves the best overall performance in 4 out of 8 datasets, with significant WER reductions in domains such as WSJ, CHiME-4, and SwitchBoard. This indicates that certain domains may benefit from training on a cumulative dataset, leveraging the diverse linguistic patterns and contexts provided by multiple ASR domains. Consistent with SD results, full fine-tuning (FT) outperforms LoRA across all datasets. This is likely due to the scale of training data, which helps mitigate overfitting and allows the model to fully exploit its parameter capacity. While larger models generally deliver better performance, the 800M model often provides the best tradeoff between performance and computational complexity. It achieves substantial improvements over the base model while maintaining a lower computational overhead compared to the 3B model.\nLoRA vs. Full Fine-tuning: Comparing LoRA and full fine-tuning (FT) strategies, FT consistently shows better performance across most datasets. The only exception occurs at smaller model sizes in the SD setting, where LoRA outperforms FT on the majority of the subsets, possibly because the limited model capacity leads to overfitting when all parameters are fine-tuned. LoRA mitigates this by adapting only a subset of parameters, reducing the risk of overfitting and task forgetting, especially in low-resource scenarios. However, most of the considered subsets contain around 4,000 samples or more, except for CORAAL, which has approximately 2,000 training examples. The results highlight that while LoRA is computationally efficient, GenSEC tasks significantly benefit from full model fine-tuning. In the CD settings, the 3B FlanEC FT model achieves an average improvement of 0.64 \u00b1 0.46 absolute WER points compared to its LoRA counterpart. This highlights the advantages of full fine-tuning, especially as the scale of training datasets increases.\nModel Size and Training Strategy: Our results clearly show that scaling up the model size and training on a combination of all training subsets with full fine-tuning can enhance the model's accuracy in post-ASR error correction. The 3B FlanEC-CD model stands out as the best-performing model across several subsets. The diverse nature of the HyPoradise dataset allows us to evaluate the generalization capabilities of FlanEC across various ASR domains. The results indicate that FlanEC-CD demonstrates robust generalization across different types of ASR data, from noisy environments (CHIME-4) to conversational speech (SwitchBoard).\nHowever, consistent with previous findings [7], the model struggles with the CORAAL subset, with no model achieving a WER below the baseline. This aligns with existing research [35, 36, 37], which identifies acoustic modeling as the primary challenge for African-American language in ASR systems. As shown in Table 1, this dataset has the highest percentage of tokens in the reference transcriptions that are absent in any of the n-best hypotheses, with more than 90% of references lacking an exact match with any of the n-best hypotheses, likely contributing to its difficulty. Although larger models seem to mitigate the issue to some extent, the results suggest that further research is needed to address the challenges posed by this specific subset. Those findings emphasize the effectiveness of larger models and cumulative training datasets in enhancing post-ASR error correction, while also highlighting areas for improvement in handling highly challenging subsets like CORAAL.\nComparison with Additional Baselines: When evaluating LLM-based approaches without fine-tuning, we observe distinct patterns. First, Flan-T5 in the zero-shot (ZS) setting does not improve upon the baseline results. Although Flan-T5 is instruction-tuned, it struggles to generalize to the post-ASR correction task without any task-specific tuning, highlighting the limitations of using a model in a zero-shot configuration for this task. On the other hand, in-context learning (ICL) with ChatGPT performs better than the baseline in 4 out of 8 cases, demonstrating the generalization capabilities of LLMS like ChatGPT, even when only provided with similar training examples rather than explicit fine-tuning. However, it is important to note that neither Flan-T5 (ZS) nor ChatGPT with ICL are specifically trained for this task; they are simply used as general-purpose LLMs applied to post-ASR correction. In contrast, FlanEC models, which are explicitly fine-tuned for the task, consistently outperform both Flan-T5 (ZS) and ChatGPT (ICL), demonstrating the need of task-specific adaptation to achieve optimal performance in this context."}, {"title": "6. CONCLUSIONS", "content": "We introduced FlanEC, an encoder-decoder model based on Flan-T5, for post-ASR error correction. Extensive experiments on the HyPoradise dataset demonstrate that FlanEC-CD, trained on a combined dataset, generally outperforms single dataset models, especially when fully fine-tuned. This suggests the potential for creating a general-purpose model capable of effective error correction across various ASR domains. Larger models showed significant performance gains, highlighting the importance of model scaling and diverse training data. While LoRA offers efficiency, full fine-tuning provides, on average, better results. Despite improvements, challenges with datasets like CORAAL remain, indicating areas for further research. We plan to expand our analysis to larger models (e.g., 11B parameters) to fully understand the impact of scaling and to further develop a versatile post-ASR correction model."}]}