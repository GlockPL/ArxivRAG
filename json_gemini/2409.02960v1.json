{"title": "Managing multiple agents by automatically adjusting incentives", "authors": ["Shunichi Akatsuka", "Yaemi Teramoto", "Aaron Courville"], "abstract": "In the coming years, AI agents will be used for making more complex decisions, including in situations involving many different groups of people. One big challenge is that AI agent tends to act in its own interest, unlike humans who often think about what will be the best for everyone in the long run. In this paper, we explore a method to get self-interested agents to work towards goals that benefit society as a whole. We propose a method to add a manager agent to mediate agent interactions by assigning incentives to certain actions. We tested our method with a supply-chain management problem and showed that this framework (1) increases the raw reward by 22.2%, (2) increases the agents' reward by 23.8%, and (3) increases the manager's reward by 20.1%.", "sections": [{"title": "1 Introduction", "content": "The rise of AI and machine learning techniques is changing how society operates, as we see more scenarios where AI and machine learning agents play important roles. From automating routine tasks to enabling sophisticated data analysis, schedule optimization, interactive chatbots, and even robotic manipulations, AI is reshaping the landscape of work and everyday life. In the coming years, these AI agents will be used for making more complex decisions, including in situations involving many different groups of people. One big challenge is that AI agent tends to act in its own interest, unlike humans who often think about what will be the best for everyone in the long run. This raises the question: How can we get self-interested agent to work towards goals that benefit society as a whole?\nGame theory offers a powerful framework to analyze these dynamics. At its core, the challenge is to foster cooperation among self-interested agents in a scenario that has general-sum payoff structure. Our goal is to get self-interested agents to cooperate in general-sum games, in a scalable way. We believe that deep reinforcement learning (RL) is one of the candidates, as it has been shown to reach the human expert level in complex decision-making such as Atari and Go. Multi-agent RL methods have been successful in some cooperation games like StarCraft. However, relatively little effort has been made to apply RL in general-sum games.\nOne recent approach is to this is to make the agent aware that other agents are also learning at the same time. LOLA agent optimizes its policy with the assumption"}, {"title": "2 Proposed Method", "content": ""}, {"title": "2.1 General Multi-Agent Reinforcement Learning (MARL)", "content": "A reinforcement learning agent is modeled to take sequential actions in an environment formulated as a Markov Decision Process (MDP). An MDP is defined by the tuple < S, A, P,r,y >, where S is the state space, A is the action space, P is the transition probability, r is the reward function, and y \u2208 [0, 1) is the discount factor. In an MDP environment, an agent observes a state $s_t$ and executes action $a_t$ at timestep t. In the next timestep t + 1, the environment shifts to a new state $s_{t+1}$ at a probability of P = Pr($s_{t+1}$ | $s_t$, $a_t$) and the agent receives a reward r($s_t$, $a_t$, $s_{t+1}$). The goal of the agent is to find a policy $\u03c0$ = Pr($a_t$ | $s_t$) that maximizes the discounted total reward J, defined by\n$J = \\sum_t\\gamma^t r(s_t, a_t, s_{t+1}).$ (1)\nIn a multi-agent problem, the environment is a multi-agent MDP, which is defined similarly to the MDP with multiple agents taking actions. In this paper, we focus on a Markov Game , where all the agents take actions simultaneously at each step. A Markov Game is defined by the tuple < N, S, {$A^i$}, P, {$r^i$}, \u03b3 >, where N is the number of agents in the environment, S is the joint state space for all the agents, and $A^i$ is the action space for agent i. The transition function P and the reward function $r^i$ are defined for the joint state and action spaces. The goal of agent i is to find a policy $\u03c0^i$ = Pr($a^i$ | s) that maximizes its own discounted reward $J^i$, defined by\n$J^i = \\sum_t \\gamma^t r^i(s_t, a_t, s_{t+1}).$ (2)"}, {"title": "2.2 MARL with a manager", "content": "Figure 1 shows the overall concept of our method. In our framework, we add another agent called the manager, which is shown at the top of the figure. At timestep t, the manager observes the state $s^M_t$ and selects an action $a^M_t$ according to its policy $\u03c0^M(s^M_t)$. This action works as an auxiliary state element $\u015d^i_t$ and supplies auxiliary rewards $r\u0302^i_t$ for agent i, such that\n$a^M_t := [\u015d^1_t,...,\u015d^i_t,...,\u015d^N_t,r\u0302^1_t,...,r\u0302^i_t,...,r\u0302^N_t] \\sim \u03c0^M(s^M_t),$ (3)\nwhere N is the number of agents. In other words, the manager tries to control the agents by showing auxiliary states $\u015d^i_t$ and paying incentives $r\u0302^i_t$ to the agents. The manager ultimately wants to maximize the sum of the (raw) rewards of all the agents while keeping the paid-out incentive as low as possible, thus the objective function of the manager $J^M$ will become\n$J^M = \\sum_t\\gamma^t {\\sum_i(r^i_t - r\u0302^i_t)},$ (4)"}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 The supply-chain optimization problem", "content": "We tested our method with a supply-chain optimization problem. We used a simple supply chain shown in Figure 2a with two suppliers, three factories, and three retailers. The factories can purchase parts from either of the two suppliers, but the retailers can only purchase items from a specific factory.\nAn environment step consists of seven days, as shown in Figure 2b. At the beginning of each step (DAY1), the factories place orders to the suppliers. The suppliers produce the parts and deliver them to the factories after several days, depending on the number of orders and the capacity of the suppliers. The factories assemble the parts to create items, which will take another day. At every step on DAY2, the factories receive orders from the retailers. The factories fulfill the orders by shipping the items to the retailers as early as possible (DAY3~DAY7), and only the items shipped on the earliest possible day (DAY3) are considered as on time. When the number of items that can be shipped is smaller than the number of orders, the remaining orders will be pooled as back orders, which need to be fulfilled in the subsequent steps. If the number of produced items is larger than the number of orders, then the remaining items will be stored at each factory as stock, which can be used to fulfill future orders.\nThe players in this environment are the factories. They decide how many parts to buy from each supplier, to maximize their rewards. The factory has two objectives, to maximize its own profit and to fulfill the retailer's orders on time as a whole. We assume that each supplier has a different price and production capacity per day. The factories want to buy the parts from cheaper suppliers, but if they all buy from the same supplier, the number of orders surpasses its capacity and causes delays in parts delivery. This will decrease the number of items shipped to the retailers on time. This is where the general-sum-game characteristic shows up some factories should order more from the expensive supplier for timely shipping; no factory would want to do so as this reduces profit."}, {"title": "3.2 RL formalization", "content": "We set up an RL problem with one agent assigned to each of the three factories. The action of agent i is to place orders with the two suppliers, thus $a_i$ is a two-dimensional integer vector. We defined the reward of agent i as $r^i = w_p r^{p,i} + w_{OFR} r^{OFR}$, where $r^{p,i}$ is the profit of agent i, $r^{OFR}$ is the Order Fulfillment Ratio defined in the next paragraph, and $w_P$, $w_{OFR}$ are the weight factors.\nThe profit reward is defined as\n$r^{p,i} = \\frac{1}{C_{p,norm}} {N_{ship, i} \\cdot p_{item} - \\sum_s a^s_t \\cdot p_{parts,s} - I_t \\cdot p_{inventory} - C_{p, offset}},$ (6)\nwhere $N^{ship,i}_t$ is the number of items shipped, $I_t$ is the number of inventories at time t, $a^s_t$ is the s-th component of the agent action vector $a_i$, $p_{item}$ is the selling price of the item, $p_{parts,s}$ is the price of the parts from supplier s, and $p_{inventory}$ is the penalty fee imposed to each inventory item. The index s runs through all the suppliers, i.e. s = 0, 1. The constants $N_P$ and $C_P$ are the normalization factor and offset parameter, respectively, to normalize the reward range in to approximately [0, 1] per step.\nThe Order Fulfillment Ratio (OFR) is defined by [Number of orders on time]/[Number of total orders]. This is an important metric that affects customer satisfaction and efficiency of the supply chain. We consider a scenario where the OFR is calculated for the supply chain as a whole, and there is a target value $T_{OFR}$ for the OFR. Then we define the OFR reward $r^{OFR}$ as\n$r^{OFR}_t = \\begin{cases} 1, & \\text{if OFR > } T_{OFR} \\\\ 0, & \\text{otherwise.} \\end{cases}$ (7)\nWe defined the auxiliary state $\u015d^i_t$ as a two-dimensional vector with values in range [0, 1], and the auxiliary reward as $r\u0302^i_t = \u015d^i_{t-1} \\cdot a^i_{t-1}$, which is the inner product of the auxiliary state and the agent's action at the previous step. This means that the incentives are given to factory i for buying the items from suppliers, proportional to the auxiliary state $\u015d^i_t$. With this definition, the auxiliary reward is calculated from the auxiliary state and thus the manager's action is only to select the auxiliary states,\n$a^M_t := [\u015d^1_t, \u015d^2_t, \u015d^3_t] \\sim \u03c0^M(s^M_t).$ (8)\nThe raw observation for an agent is a 175-dimension vector, which includes 105 variables on the suppliers' status, 45 variables on the factory's status, 25 variables on future demand estimates, and 1 variable that indicates the current timestep. The observation space for the manager is a 531-dimension vector, which includes the observation of all three factories plus a 6-dimensional vector of the agent's actions in the previous step."}, {"title": "3.3 Training and Evaluation", "content": "Both the agents and the manager are trained with DDPG . The actor and the critic networks for the agents and the manager have two fully-connected hidden layers with 128 nodes each. The outputs of the actor networks are converted to the range [0, 1] with a tanh function. The agents' actions are further converted to integers in the range [0, 99] by multiplying the output by 100 and rounding down. We train the agents with two frameworks: a na\u00efve MARL, where there is no manager, and our proposed framework with the manager. In both cases, we train the agents and the manager with 500 episodes and with 10 different random seeds.\nWe take the final 25 episodes of the training to evaluate the performance. We evaluate the mean scores of the agents and the manager, as well as their standard deviations."}, {"title": "4 Results", "content": "Figure 3 shows the average reward of the factories during the training. Figure 3a shows the plot for the setup without the manager taking action, and Figure 3b shows the plot with the manager. These plots show that, with the manager, the profit reward shown in blue decreases but the OFR reward increases more than that, which improves the final score, shown in red. Note that the decreasing score at the beginning of the training in Figure 3b is caused by the manager randomly incentivizing the factories, and quickly decreasing the incentives.\nThe mean scores of the last 25 training episodes are shown in Table 2. Without the manager, the average reward of the factories was 0.505, and with the manager, it improved to 0.625,"}, {"title": "5 Conclusion and Discussion", "content": "In this paper, we tackled the problem of making self-interested agents cooperate in a multi-agent general-sum game environment. We proposed a method to add an agent called a manager in a multi-agent environment to mediate agent interactions by assigning incentives to certain actions. We tested our method with a supply-chain management problem and showed that it increases the total profit of all the players.\nAlthough our experiments show that the manager's policy can be obtained with a simple reinforcement learning framework, one limitation is that we assume the agents are na\u00efve learning RL agents. This is a strong constraint, because in reality, these agents may be more clever and try to exploit the manager, or sometimes can be less reasonable and stick to their original policy. It is important to consider how the agent architectures impact the performance of our method."}, {"title": "Data Availability Statement", "content": "The data that support the findings of this study are available on request from the authors. Requests for data should be addressed to [Author Name] at [Author's Email Address]."}, {"title": "Conflict of Inetest Statement", "content": "On behalf of all authors, the corresponding author states that there is no conflict of interest."}]}