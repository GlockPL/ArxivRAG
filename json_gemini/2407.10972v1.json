{"title": "VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation", "authors": ["Bocheng Zou", "Mu Cai", "Jianrui Zhang", "Yong Jae Lee"], "abstract": "In the realm of vision models, the primary mode of representation is using pixels to rasterize the visual world. Yet this is not always the best or unique way to represent visual content, especially for designers and artists who depict the world using geometry primitives such as polygons. Vector graphics (VG), on the other hand, offer a textual representation of visual content, which can be more concise and powerful for content like cartoons or sketches. Recent studies have shown promising results on processing vector graphics with capable Large Language Models (LLMs). However, such works focus solely on qualitative results, understanding, or a specific type of vector graphics. We propose VGBench, a comprehensive benchmark for LLMs on handling vector graphics through diverse aspects, including (a) both visual understanding and generation, (b) evaluation of various vector graphics formats, (c) diverse question types, (d) wide range of prompting techniques, (e) under multiple LLMs. Evaluating on our collected 4279 understanding and 5845 generation samples, we find that LLMs show strong capability on both aspects while exhibiting less desirable performance on low-level formats (SVG). Both data and evaluation pipeline will be open-sourced at https://vgbench.github.io.", "sections": [{"title": "1 Introduction", "content": "Current vision models are mostly built on pixels, rasterizing the visual world into a matrix representation. Such rasterziation represents diverse visual content with equally sized elements. But pixels are not the only way to represent the visual world. For simple content such as cartoons or sketches, a different representation using explicit geometry primitives can be more concise and beneficial. Vector graphics offer such a textual representation for visual content via geometry primitives, e.g., circles and polygons, as shown in Figure 1 (a). Vector graphics have been critical for designers and artists since the geometry primitives can be easily manipulated. Vector representations include Scalable Vector Graphics (SVG), TikZ, Graphviz, etc.\nVector Graphics vector representations make it possible to conduct visual understanding and generation with LLMs such as GPT-4 (OpenAI, 2023b). Recent studies (Bubeck et al., 2023; Cai et al., 2023; Rodriguez et al., 2023) showcase LLMs' superior capability across different perspectives. However, those works either (1) only show qualitative results (Bubeck et al., 2023), (2) only study vector graphics understanding (Wang et al., 2024a) and not generation, or (3) only study one specific type of vector graphics such as SVG (Cai et al., 2023; Wang et al., 2024a; Rodriguez et al., 2023) or TikZ (Belouadi et al., 2023). Therefore, the community lacks a comprehensive LLM benchmark for vector graphics.\nIn this paper, we propose VGBench to comprehensively evaluate LLMs' vector graphics processing capabilities via different aspects: VGBench (1) includes both visual understanding (VGQA) and generation (VGen); (2) evaluates diverse vector graphics formats such as SVG, TikZ, and Graphviz; (3) covers a set of taxonomies from low-level vision to high-level semantics, from color, shape, to category and advanced reasoning questions such as usage and the relation between objects; (4) adopts a variety of prompting techniques, such as zero-shot prediction, chain-of-thought reasoning, in-context learning, etc.; and (5) evaluates diverse LLMs including GPT-4 (OpenAI, 2023b), GPT-3.5 (OpenAI, 2023a), Llama-3-8B-Instruct, and Llama-3-70B-Instruct (Meta, 2024).\nWe collect 4279 high-quality visual question-answer (QA) pairs for vector graphics (VG) understanding and 5845 VG-caption pairs for vector graphics generation. The vector graphics code is collected from existing datasets and the Internet."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Vector Graphics", "content": "Vector graphics represent images using basic geometric elements like points, lines, and curves, rather than pixels. This method offers an alternative to raster graphics, providing advantages such as infinite scalability without losing detail and easy human manipulation.\nThere are a variety of vector graphics formats, such as SVG (Quint, 2003), TikZ (Mertz and Slough, 2007) and Graphviz (Gansner, 2009). SVG format defines 14 functional areas or feature sets and represents graphics by recording basic information associated to these primitives, such as their coordination and scales, in an XML file. TikZ format defines some commands to build basic geometric elements and is mainly used with IATEX. In practice, third-party packages are also commonly used with TikZ to build more diverse images. Graphviz (Gansner, 2009) is a vector graphics format that focuses on representing different kinds of graphs. In this paper, we explore the said three kinds of vector graphics to provide a thorough and comprehensive analysis regarding the reasoning capabilities of LLMs on vector graphics."}, {"title": "2.2 Evaluation for Image Understanding and Generation", "content": "Works on Image Understanding are mainly based on raster images. VQA (Antol et al., 2015) first introduced the task of free-form and open-ended Visual Question Answering and evalauted existing LSTM-CNN based methods. CLIP (Radford et al., 2021) introduces two encoders for both texts and images to achieve an aligned representation to serve as a baseline for many image understanding tasks. LLaVA (Liu et al., 2024) and LLaMA-Adapter (Zhang et al., 2023) propose approaches to solve general-purpose visual and language understanding problems based on large language models.\nWhile vector graphics can usually be converted to a raster image easily (Gharachorloo et al., 1989), there are few works that try to directly understand the vector graphics format. (Jiang et al., 2021) explores such a way using graph neural networks. (Wang et al., 2024b) utilizes large language models to understand vector graphics. In our work, we utilize multiple prompting methods, to be mentioned in the following section, to evaluate different LLMs' vector graphics understanding capabilities by prompting them with the vector graphics code directly.\nMost machine learning based image generation models aim to generate raster images (Kingma and Welling, 2013; Goodfellow et al., 2020; Ho et al., 2020; Ramesh et al., 2021). Some research focus on generating vector graphics in text format. Many works generate vector graphics from a raster image (Diebel, 2008; Xia et al., 2009; Ha and Eck, 2017; Ma et al., 2022). Leveraging language models, some try to generate text representing vector graphics directly (Carlier et al., 2020; Wu et al., 2023; Rodriguez et al., 2023). In our work, we provide a different approach to evaluate vector graphics generation via leveraging competent multimodal models such as GPT4-V (OpenAI, 2023b) to generate a detailed caption from a rasterized image of a vector graphics object, based on which other LLMs will be generating vector graphics code for the same object during evaluation. We argue that models like GPT4-V can provide high-quality captions for us to automate part of the evaluation process."}, {"title": "2.3 Prompting Techniques for Large Language Models", "content": "A variety of prompting strategies have been proven capable of boosting the performance of LLMs, such as GPT4 (Achiam et al., 2023). Few-shot learning (Brown et al., 2020b) requires the user to give a few examples of the task to the LLM, while Chain of Thought (Wei et al., 2022) instructs the LLM to think step by step to achieve higher performance. In-context learning (Brown et al., 2020a) provides few-shot examples at inference time, and shows strong performance boost without updating the model's parameters. In this paper, we broadly evaluate LLMs' vector graphic understanding capability by employing the aforementioned prompting techniques."}, {"title": "3 Tasks and Experiments", "content": "We first introduce the source of our vector graphics images in Sec. 3.1, and then describe the experiment settings in Sec. 3.2. After that, we detail our tasks, benchmark creation, evaluation pipeline and results for vector graphics understanding and generation in Sec. 3.3 and Sec. 3.4, respectively. Finally, we provide in-depth analyses on the performance under different LLMs, different sequence lengths, and reasoning processes in Sec. 3.5."}, {"title": "3.1 Vector Graphics Data Collection", "content": "We collect vector graphics samples for both understanding tasks and generation tasks from a variety of sources. For samples in SVG format, we collect them from a large-scale SVG repository.\u00b9 We sample the TikZ format vector graphics code from the DaTikZ dataset (Belouadi et al., 2023). We sample the Graphviz code used to build our dataset by crawling GitHub.\u00b2"}, {"title": "3.2 Experiment Settings", "content": "Vector Graphics Types Here we consider three major types of vector graphics: Scalable Vector Graphics (SVG), TikZ, and Graphviz. SVG is exceptionally versatile and suitable for web applications, allowing for detailed graphical representations that scale infinitely without loss of quality. This enables SVGs to theoretically represent any visual content including complex animations and interactive elements. TikZ, in contrast, is specifically tailored for creating high-precision scientific illustrations within LaTeX documents, offering a comprehensive suite of tools for detailed diagrammatic representations; it encompasses a broad spectrum of high-level semantics such as \"circuit diagrams, complex mathematical illustrations, and structured diagrams\". Graphviz, on the other hand, belongs to the family of automated graph drawing tools, which are optimized for generating diagrams from abstract descriptions and data structures, making it ideal for visualizing hierarchical information, such as state machines, organizational charts, and network infrastructures.\nLanguage Models We primarily use GPT-4 (1106 version) (OpenAI, 2023b) as the medium for vector graphics understanding and generation. This is because GPT-4 shows superior language reasoning and generation capabilities, as previously mentioned in Section 2.3. We also evaluate GPT-3.5 (OpenAI, 2023a) and other open-source LLMs that are highly capable, including Llama-3-70B-Instruct (Meta, 2024) and Llama-3-8B-Instruct."}, {"title": "3.3 VGQA: Vector Graphics Understanding Benchmark", "content": "Tasks VGQA is designed to evaluate models' vector graphics understanding capability. We systematically design a range of tasks based on the nature of each vector graphics category, aiming at a comprehensive evaluation across different semantic levels. For SVG, we design three types of questions: color, category, and usage; for TikZ, we use concept, counting, and relations as types of questions; while for Graphviz, we design layout, domain, and relations.\nBenchmark Creation and Evaluation We employ a semi-automatic benchmark curation pipeline for VGQA, as shown in Figure 3. Specifically, we render code representing vector graphics into a PNG image before leveraging GPT-4V (OpenAI, 2023b) to generate the 4-choice question-answer candidates. Then, human annotators with rich vision-linguistic knowledge make binary annotations to mark whether both the question and the answer of a candidate are rational, correct and belong to that specific type. Our approach brings several benefits: (i) annotation cost is greatly reduced due to GPT-4V's low API cost; (ii) GPT-4V is one of the most competitive LMMs that can provide high quality candidates; (iii) the human filtering process ensures the correctness of the final vector graphics understanding benchmark.\nFinally, we collect 4279 samples in total. The word distribution of answers in the VGQA dataset is illustrated. Specifically, we have 2228, 1139, and 912 samples for SVG, TikZ, and Graphviz, respectively. After an LLM makes responses to the vector graphics questions, we compare the final responses with the ground-truth answers to compute accuracy.\nResults Evaluation results of VGQA under GPT-4 (OpenAI, 2023b) are shown in Table 3. Several interesting findings arise from the results:\nGPT-4 generally shows strong vector graphics understanding capability. In the zero-shot setting, GPT-4 shows non-trivial accuracy far beyond random accuracy (25%) across all categories. Specifically, GPT-4 shows strong performance in TikZ, with an average accuracy of 78%.\nGPT-4 shows stronger performance in high-level vector graphics language (e.g., TikZ, Graphviz) compared to low-level vector graphics language SVG. In either zero-shot, few-shot, or Chain-of-Thought settings, TikZ and Graphviz show at least 17% better performance than SVG. As a reminder, TikZ and Graphviz are fundamentally different from SVG in terms of the semantic levels, as SVG is composed of geometry primitives while TikZ and Graphviz contain high-level semantics such as \u201cabove", "below": "explicit representation of nodes and edges, etc.\nChain of Thought (CoT) and In-Context Learning (ICL) show some performance improvements for some tasks, but not significant. CoT and ICL show ~7% performance boost for SVG which owns lowest performance among three formats. Yet CoT and ICL show no benefits for TikZ and limited improvements for Graphviz, where GPT-4 already obtains ~83% accuracy under TikZ and Graphviz.\nDifferent vector-graphic formats show diverse behaviors upon question types. For SVG, GPT-4 struggles at high-level questions and receives ~50% accuracy on category and reasoning types, while in TikZ and Graphviz, GPT-4 shows decent performance across all types of questions. This again demonstrates that GPT-4 shows inferior performance in low-level vector graphics tasks, especially on tasks related to reasoning."}, {"title": "3.4 VGen: Vector Graphics Generation Benchmark", "content": "Tasks We introduce VGen, a benchmark evaluating LLMs' vector-graphics generation capability. We use text to vector-graphics (T2VG) generation to test an LLM's ability to generate vector graphics code conditioned on a text prompt.\nBenchmark Creation and Evaluation Again we evaluate on three vector graphics formats: SVG, TikZ and Graphviz. First, we obtain captions for each vector graphics image by leveraging GPT-4V (OpenAI, 2023b) over its rasterized image. Then we prompt the LLM to generate the vector graphics code corresponding to the caption."}, {"title": "3.5 In Depth Analysis", "content": "Impact of Different LLMs We next perform experiments over a variety of large language models, including GPT-4, GPT-3.5, Llama-3-70B-Instruct (Meta, 2024) and Llama-3-8B-Instruct. Results are shown in Table 7. The results show that GPT-4 has the best VG understanding ability over vector graphics among those models while Llama-3-70B shows better performance than GPT-3.5.\nImpact of Vector Graphics Sequence Length We next study the influence of the length of the vector graphics on vector graphics understanding. Results for GPT-4 are shown in Table 8, where GPT-4 shows consistent performance across different length groups. Specifically, low-level vector graphics format such as SVG is most sensitive to the length. When the length increases, the understanding performance on SVG decreases steadily, while the understanding performance on other high level format remains stable. Another noticeable finding is that questions requiring complex reasoning, such as Usage in SVG or Relation in Graphviz, suffer more from the increasing sequence length.\nCan LLMs Reason over Vector Graphics? The reasoning process of GPT-4 under the CoT setting is shown in Figure 7. Results show that GPT-4 can detect the key information over those samples, such as \"two large, similar shapes that could represent the eyepieces ...\", for correct reasoning."}, {"title": "4 Conclusion", "content": "Our study unveils new insights into the capabilities of LLMs in understanding and generating vector graphics. We discovered that LLMs demonstrate decent vector graphics understanding in TikZ, Graphviz, and SVGs, with a particular strength in understanding vector graphics code with higher-level semantics. We also found that LLMs often exhibit strong vector graphics generation capabilities. Interestingly, advanced prompting techniques can significantly improve performance for low-level formats such as SVG, and while GPT-4 had the strongest performance, open-source models like Llama-3-70B show competitive performance. Our work lays a groundwork for future studies into LLMs' vector graphics understanding and generation benchmarking, and we hope it will inspire further efforts to enhance these capabilities. We will release our benchmark dataset and evaluation pipeline."}, {"title": "5 Limitations", "content": "We acknowledge that one cannot systematically evaluate the behavior of the closed-source models we employed, namely GPT-4, GPT-35-Turbo, and GPT-4V. Besides, more evaluations on recent LLMs can be conducted, which can provide more supporting experiments on LLMs' behavior on vector graphics understanding and generation.\nFurthermore, recent works propose more prompting techniques such as Tree of Thoughts (ToT) (Yao et al., 2024) and Everything of Thoughts (XoT) (Ding et al., 2023). Incorporating these prompting techniques could further enhance our study."}, {"title": "6 Appendix", "content": ""}, {"title": "6.1 The specific prompt we used", "content": ""}, {"title": "6.1.1 Prompts used to build the dataset", "content": "Question Generation System prompt: The system prompts used to generate questions are different for different types of vector graphics and different types of questions. See the code in supplemental material for details.\nUser prompt: The caption of this image is {caption}, generate the json according to the instruction.\n<IMAGE>\nCaption Generation System prompt: Generate a detailed caption for the given image. The reader of your caption should be able to replicate this picture.\nUser prompt: <IMAGE>"}, {"title": "6.1.2 Prompts used to evaluate the models's understanding ability", "content": "Zero-shot System prompt: I will present a {format} code. Please answer my questions only based on code. Answer and only answer the letter corresponding to the correct option. Do not add any additional comment in your response\nUser prompt: \"{code}\". Given this image, answer {question}. Options are {options}\nFew-shot System prompt: I will present a {format} code. Please answer my questions only based on code. Answer and only answer the letter corresponding to the correct option. Do not add any additional comment in your response. For your reference, I will give you some examples.\nUser prompt: This is an example, the code is: {code}\nUser prompt: Given this image, answer {few_shot_sample_question}. Options are {few_shot_sample_options}\nSimulated assistant prompt: {few_shot_sample_answer}\nRepeat the last three prompts for three times, each time pass a different samples.\nUser prompt: \"{code}\". Given this image, answer {question}. Options are {options}"}, {"title": "6.1.3 Prompts used to evaluate models' generation ability", "content": "System prompt: Generate a {format} based on the caption below. You should output the compilable code without any additional information.\nUser prompt: {caption}"}, {"title": "6.2 Data distribution", "content": "We include the distribution of VGQA grouped by each vector graphic category in Figure 8, each in itself grouped by the specific question categories we assigned."}, {"title": "6.3 Detailed examples for reasoning", "content": "We include the full version of the three example conversations previously put in Figure 7 now in Figure 9. The three conversations show how we only input the vector graphics code, exhibit the question, ask the model to consider each question carefully, and finally make its best choice."}, {"title": "6.4 Llama variants used in this paper", "content": "We evaluated Llama's variants, Llama-3-8B-Instruct-262k and Llama-3-70B-Instruct-Gradient-262k in this paper because they have extended context length."}, {"title": "6.5 Human filtering", "content": "The authors of this study, proficient in English with extensive research experience in vision-language learning, perform the vector graphics QA filtering."}, {"title": "6.6 Programs and Data Release", "content": "Our code and data is included in the supplementary materials."}]}