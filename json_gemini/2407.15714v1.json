{"title": "Mamba meets crack segmentation", "authors": ["Zhili He", "Yu-Hsing Wang"], "abstract": "Cracks pose safety risks to infrastructure that cannot be overlooked and serve as a vital indicator of structural\nperformance deterioration. Enhancing the precision and efficiency of crack segmentation is a prominent research\ntopic. The prevailing architectures in existing crack segmentation networks predominantly consist of convolutional\nneural networks (CNNs) or Transformers. Nonetheless, CNNs exhibit a deficiency in global modeling capability,\nhindering their representation to entire crack features. Transformers can capture long-range dependencies but suffer\nfrom high and quadratic complexity. Recently, the emergence of a novel architecture, Mamba, has garnered\nconsiderable attention due to its linear spatial and computational complexity, as well as its powerful global\nperception capability. This study explores the representation capabilities of Mamba to crack features. Specifically,\nthis paper uncovers the intrinsic connection between Mamba and the attention mechanism, providing a profound\ninsight\u2014an attention perspective\u2014into interpreting Mamba and devising a novel Mamba module following the\nprinciples governing attention blocks, namely CrackMamba. We compare CrackMamba with the most prominent\nvisual Mamba modules, Vim and Vmamba, on two datasets comprising asphalt pavement and concrete pavement\ncracks, and steel cracks, respectively. The quantitative results demonstrate that CrackMamba stands out as the sole\nMamba block consistently enhancing the baseline model's performance across all evaluation measures, while\nreducing its parameters and computational costs. Moreover, this paper substantiates that Mamba can achieve global\nreceptive fields through both theoretical analysis and visual interpretability. The discoveries of this study offer a\ndual contribution. First, as a plug-and-play and simple yet effective Mamba module, CrackMamba exhibits immense\npotential for integration into various crack segmentation models. Second, the proposed innovative Mamba design\nconcept, integrating Mamba with the attention mechanism, holds significant reference value for all Mamba-based\ncomputer vision models, not limited to crack segmentation networks, as investigated in this study.", "sections": [{"title": "1. Introduction", "content": "Defects inevitably occur, accumulate, and propagate during the service life of civil infrastructure due to the intricate\nworking environment. These defects not only pose significant safety hazards to structures but also threaten their\nservice life. Among the various forms of structural defects, cracks are a primary manifestation and a pivotal indicator\nof structural performance degradation, demanding meticulous attention [1].\nRoutine crack inspection serves as an effective and widely employed method for mastering crack information\nof infrastructure. Presently, the predominant mode of inspection is manual visual examination conducted onsite, a\npractice that is both time-consuming and labor-intensive. Moreover, the subjective nature of human assessment\nintroduces a notable level of uncertainty and inconsistency in the inspection outcomes [2]. Recognizing these\nlimitations, there is a growing interest among researchers and practitioners in integrating cutting-edge artificial\nintelligence (AI) and computer vision (CV) technologies into crack inspection practices to achieve intelligent,\nefficient, and precise crack identification [3][4][5]. These AI-driven crack identification methodologies are typically\ncategorized into three distinct groups based on the nature of the researched CV tasks: image-level crack\nclassification, box-level crack detection, and pixel-level crack segmentation. Notably, He et al. [2] have identified\ncrack segmentation as the most appropriate and accurate approach for representing crack features, thus, the research\nfocus of this paper is crack segmentation.\nInitially, crack segmentation models were constructed using convolutional neural networks (CNNs) [1][6][7].\nHowever, considering that on the one hand the ability of CNNs to capture long-distance dependencies is so limited\ndue to the inherent locality of convolutions, and on the other hand, crack pixels are typically dispersed globally on\ncrack images in their length directions, it is challenging for CNN-based crack segmentation networks to\ncomprehensively model the entirety of crack features. In contrast, a newer architecture than CNNs, Transformers,\nbased on self-attention [8][9], inherently excels in modeling long-term context information. Consequently,\nTransformers have been integrated into crack segmentation networks either to replace CNNs [10] or to work with\nthem [2], bringing the networks global perceptual capabilities. The frameworks incorporating Transformers [2] [10]\nhave demonstrated the benefits of effectively modeling long-range context features in crack segmentation tasks,\nfurther underscoring the necessity for crack segmentation networks to possess global modeling capabilities. Despite\nthe advantages, the computational and spatial complexity of Transformers grows quadratically with the length of\ninput sequences, presenting an efficiency bottleneck. As a result, researchers have endeavored to refine the\narchitecture of Transformer to reduce computational and spatial costs without compromising its global perception\nabilities [11][12]. Various efficient modules have been proposed, such as Sparse Transformers [13], Linear attention\n[14], and FlashAttention [15]. However, these advanced modules only optimize the architecture of the vanilla\nTransformer without fundamentally innovating its core structure, namely, the scaled dot product self-attention.\nGiven the well-established maturity and widely proven generalization and robustness capabilities of the vanilla\nTransformer, researchers and engineers still tend to favor the conventional Transformer architecture. The newer and\nmore intricate Transformer structures, despite their novelty, have not yet emerged as the mainstream choice due to\nthis preference.\nRecently, in the realm of one-dimensional (1D) sequence modeling, an architectural innovation known as\nMamba [16], coming from state space models (SSMs) [17][18] rather than the above-mentioned self-attention, has\ngarnered significant attention because Mamba's powerful global modeling capabilities, state-of-the-art (SOTA)\nperformance across diverse tasks encompassing multiple modalities, such as natural language processing (NLP),\naudio modeling, and genomics modeling, and more importantly, its linear complexity. Notably, pioneering work by\nZhu et al. [19] and Liu et al. [20] has introduced Mamba into the domain of computer vision, resulting in the\ndevelopment of the Vision Mamba (Vim) model [19] and the VMamba model [20], respectively. These studies have\ndemonstrated the conspicuous success of Mamba in learning general visual representation and global features of\nimages. Specifically, Vim and VMamba have not only surpassed many well-established vision networks in\nperformance but have also exhibited faster processing speeds and lower memory and computing resource\nrequirements. Building on these achievements, a range of well-crafted models based on Vim or VMamba have been\neffectively deployed in specific downstream tasks of CV, such as Mamba-UNet [21] for medical image analysis and\nRS3Mamba [22] for remote sensing image segmentation. The series of developments underscore Mamba's\nversatility and immense potential to supplant CNNs and Transformers to be the next generation foundational\narchitecture of CV. However, despite these advancements, the application of Mamba in the context of crack images\nremains unexplored. This poses an intriguing question regarding Mamba's ability to effectively represent and\ncomprehend crack features. Addressing this question is of paramount importance, as successful utilization of\nMamba in crack data analysis could revolutionize the field of crack identification, propelling it into a new stage of\nresearch and innovation.\nTherefore, this study investigates the representation ability of Mamba to crack data. Initially, a classical and\ninfluential crack segmentation model, CrackSeU [1], is selected, along with two representative and publicly\navailable crack datasets encompassing cracks from distinct domains. The reason of choosing the datasets is to\nexamine if Mamba can consistently improve performance of the model to the cracks from different domains. The\nfirst dataset, Deepcrack [1][41], encompasses asphalt pavement cracks and concrete pavement cracks, while the\nsecond dataset, Steelcrack [2], comprises cracks in steel structures. Subsequently, Vim and VMamba are integrated\ninto CrackSeU, and the enhanced networks are trained and evaluated on the two crack datasets. Unfortunately, the\nexperimental outcomes in Subsection 5.1 reveal that Vim and VMamba cannot consistently enhance and, in some\ncases, even diminish the model's performance. Upon thorough investigation, it is discovered that the essence of\nMamba and the attention mechanism [1][23] is unified in dynamic parameters, therefore, Mamba can be viewed as\na type of attention. Given the universal success of the attention mechanism in crack identification [1][23][24][25],\nthis study redesign the visual Mamba modules to propose the CrackMamba block, which aligns with the attention\nmodules. Notably, the experimental results in Subsection 5.1 demonstrate that CrackMamba stands out as the sole\nMamba module capable of consistently enhancing the crack recognition performance of CrackSeU across both\ndatasets. Furthermore, CrackSeU combined with CrackMamba (CrackSeU + CrackMamba) requires fewer\nparameters and lower computational resources compared to the standard CrackSeU model. From the insights gained\nthrough this research, several key conclusions can be drawn: (1) the performance of Mamba modules is highly\nrelated to the tasks, and Vim and VMamba blocks are proved unsuitable for crack segmentation tasks; (2) the\nstructural composition of Mamba blocks is crucial and significantly influences the performance of Mamba-based\nAl models, therefore, it is necessary to design the structures of the Mamba modules tailored to specific tasks; (3)\nthe innovative perspective introduced, known as the attention perspective, serves as a valuable guide in shaping\nMamba blocks and also deepens the comprehension of Mamba; and (4) it is believed that the proposed Mamba\ndesign concept, that designing Mamba modules aligning with attention modules, holds enlightening significance\nfor a wide array of CV tasks which either have employed or are considering the utilization of visual Mamba models,\nnot limited to the crack identification problem, as explored in this study. Moreover, this paper demonstrates that\nMamba can bring global receptive fields from both theoretical and visual interpretability standpoints.\nTo summarize, the principal contributions of this research are outlined as follows:\n(1) Mamba and visual Mamba, including Vim and VMamba, are introduced into the domain of crack segmentation.\nDetailed illustration and analysis, often overlooked in existing literature related to Mamba, are provided in the\nappendices to ensure clarity for the civil engineering audience.\n(2) This research offers a heightened and profound perspective for comprehending Mamba, termed the attention\nperspective. Subsequently, a novel, straightforward, yet highly effective Mamba module, named CrackMamba,\nis designed following the principles of attention modules.\n(3) Vim, VMamba, and CrackMamba are evaluated on two representative crack datasets, with the findings\nindicating that CrackMamba is the only Mamba block to consistently enhance crack segmentation performance.\nAdditionally, CrackMamba reduces the number of parameters and computational complexity of the baseline\nmodel. These outcomes not only validate the effectiveness of CrackMamba and emphasize its substantial\napplication potential in crack segmentation models, but also confirm the success of the proposed attention\nperspective and highlight the importance of designing Mamba blocks that incorporate the principles of the\nattention mechanism, which offers significant reference value for various Mamba-based CV models, not solely\nlimited to the crack segmentation frameworks investigated by this study."}, {"title": "2. Mamba", "content": "Notations. Let us introduce the notation rules adopted in this paper to avoid confusion. Following [26], matrices or\ntensors in neural network are denoted by non-tilt boldface uppercase letters, such as A and X. Vectors are denoted\nby non-tilt boldface lowercase letters, such as x and u. Scalars are represented by tilt non-bold lower-case letters,\nsuch as x and t. Additionally, vectors are all column vectors. For instance, x = [x0, x1, ..., xL\u22121]T is adopted to\nrepresent the vector x formed by L scalars.\n2.1. Vanilla Mamba\nThe evolution of state space models (SSMs) unfolds through two pivotal iterations: Structured state space sequence\nmodels, also known as Structured SSMs or S4 models [17] [18], progress into Selective SSMs, referred to as\nSSMs+Selection or S6 models (aka Mamba) [16]. These iterations are elaborated upon in the subsequent contents.\nThe basic idea of S4 models comes from the state space equations in the modern control theory domain, and\nthey are adopted to describe a linear time invariance (LTI) system:\n$\\dot{x}(t) = Ax(t) + Bu(t)$,\n$y(t) = Cx(t) + Du(t)$,\nFor a first-order system, the parameters in the above equations are scalars. Here, we only consider second-order or\nhigher-order systems. Hence, x(t) represents the state vector of the system, u(t) is the system input, and y(t)\ndenotes the system output. A, B, C, and D are the state matrices used to describe the system. Since this system is\na time invariance system, A, B, C, and D are all constant matrices for any time t.\nSSMs mainly make three modifications to the above state space equations. First, SSMs simplify (1b) and omit\nD in (1b) (or equivalently, D = 0) [18]. This simplification is reasonable because Du(t) can be viewed as a\nshortcut connection in neural networks and can be computed easily. Second, the input and output are assumed to be\nthe 1D continuous signals. Third, the notations are slightly changed. Thus, the state space equations in the domain\nof SSMs can be defined as\n$\\dot{h}(t) = Ah(t) + Bx(t)$,\n$y(t) = Ch(t)$,\nwhere the hidden state variable is changed to h(t) \u2208 RN\u00d71, and N denotes the dimension of the state vector. The\nsystem input is x(t), and y(t) still denotes the output, with t representing the continuous time index. Clearly, A \u2208\nRN\u00d7N, B \u2208 RN\u00d71, and C \u2208 R1\u00d7N based on the matrix operation rule. For the subsequent content, the symbols'\nmeanings are consistent with Eqs. (2a) and (2b) unless specified otherwise. It is worth noting that the research core\nof all SSMs is how to effectively represent Eqs. (2a) and (2b) and effectively implement them in AI models and\ncomputing hardware.\nS4 models first discretize the above state space equations since the calculations in the neural work are always\ndiscrete:\nhk = Ahk-1 + Bxk,\nyk = Chk\nAt this time, Eqs. (3a) and (3b) describe the mapping between two discrete signals (or sequences): x \u2208 RL\u00d71 =\n[x0, x1, ..., xL\u22121] \u21d2 y \u2208 RL\u00d71 = [y0, y1, ..., yL-1]T, where L denotes the length of the sequences. In the above\nequations, k \u2208 {0,1,..., L \u2212 1} denotes the discrete index, and A\u2208 RN\u00d7N, B\u2208 RN\u00d71, and \u0108\u2208 R1\u00d7N are the\ndiscrete parameters corresponding to the continuous parameters A, B and C, and they can typically be expressed\nas\nA = fA (\u2206, A), B = fB(\u2206, A, B), and C = C.\nThe most straightforward discretization method is the Euler discretization method (see Appendix A. 1), however, it\nis not precise enough. Thus, S4 models adopt a more advanced method, the bilinear discretization method, where\nA, B, and C can be further formulated as\nA = (I \u2212 \u2206/2 \u00b7 A)\u22121 (I + \u2206/2 \u00b7 A),\nB = (I \u2212 \u2206/2 \u00b7 A)\u22121\u2206B,\nand\nC = C,\nwhere \u2206\u2208 R+ represents the time step size of discretization, and I \u2208 {0,1}N\u00d7N is the identity matrix. The proof of\nthe bilinear discretization method is provided in Appendix A.2. Second, S4 models leverage the high-order\npolynomial projection operator (HiPPO) Matrix to initialize A to handle long-range dependencies, and S4 models\nfind that in contrast to random initialization, the HiPPO Matrix initialization can improve the performance on the\nsequential MNIST classification benchmark from 60% to 98% [17][18]. HiPPO Matrix can be defined by the\nfollowing formula:\nHiPPO Matrix Aij =\n{\n(2i + 1)1/2(2j + 1)1/2_ if i > j\ni+1\n0\nif i = j. i, j\u2208 [0,1, ..., N \u2212 1].\nif i < j\nThird, S4 models combine Eqs. (3a) and (3b) for the entire sequence:\ny = K \u2297 x,\nK = (CB, CAB, ..., CAL-1B),\nwhere \u2297 denotes convolution operation, K\u2208 RL is the one-dimensional (1D) convolutional kernel. The\ncombination process is detailed in Appendix B. This shows that S4 models can be transferred into the convolutional\nform so that they can be trained in parallel like CNNs, and the explanation of parallel training is provided in\nAppendix C.1. In practice, S4 models pre-compute Krylov matrix Kr \u2208 RN\u00d7L of the convolutional kernel K [17]:\nKr = (B, AB, ..., AL-1B).\nSubsequently, the kernel K can be calculated through a matrix multiplication: K = CKr. Finally, Eq. (9a) is\nexecuted in parallel. Besides, it is evident that S4 models can intrinsically capture long-term dependencies and bring\nglobal perception without the intricate hierarchical stacking structures found in CNNs. The aforementioned details\ndescribe the fundamental concept underlying S4 models.\nFurther, it becomes apparent that the primary computational burden of S4 models lies in computing the latent\nstate Kr, which necessitates L consecutive matrix multiplications by A, involving O(N\u00b2L) multiplication\noperations and O(NL) space complexity [18]. The detailed complexity statements are provided in Appendix D.\nThis substantial computational load constitutes the fundamental bottleneck of the na\u00efve S4 models. Subsequent\nadvancements in S4 models pay attention to how to improve computational efficiency. For example, Gu et al. [18]\nhave discovered that A can be transformed into a diagonal plus low-rank (DPLR) form, and this technique can\nreduce the computational complexity to \u00d5(N + L) and the space complexity to O(N + L), respectively, where\n\u00d5(1) denotes the cost of a single Cauchy matrix-vector multiplication.\nGu and Dao [16] observe that conventional S4 models are all inherently linear time invariant and lack the\ncapability to selectively focus on or disregard particular inputs because the parameters K are not related to inputs\nonce SSMs are well-trained. Therefore, to address this limitation, they introduce a selection mechanism into S4\nmodels, giving rise to Mamba (referred to as SSMs+Selection). The core idea of the selection mechanism lies in\nparameterizing A, B and C in an input-dependent manner, enabling the new model to dynamically select valuable\ninformation and filter out irrelevant data indefinitely based on the inputs. Specifically, Mamba models B, C, and\nA as the functions of the input sequence x:\nB = SB(x), C = Sc(x), \u2206= \u03c4\u25b3 (Parameters + S\u25b3(x)),\nwhere SB(x) = LinearN(x) and Sc(x) = LinearN(x), with LinearN representing a linear and parameterized\nprojection to dimension N. In the neural network domain, this is also named a fully connected layer (FCL). The\nsymbol \u03c4\u25b3 represents the Softplus activation functions, and S\u2081 = BroadcastN (LinearN (x)), which is inspired by\nthe recurrent neural network (RNN) gating mechanism [16]. Regarding the matrix A in Eq. (2a), it remains input-\ninvariant and is initialized using the HiPPO Matrix for simplicity. This is due to A = fA(\u2206, A) and B =\nfB(\u2206, A, B) (refer to Eq. (4)), where selectivity in A and B guarantees selectivity in A and B. Subsequently, in\nthe Mamba paper [16], Gu and Dao claim that they employ the zero-order hold (ZOH) technique to discretize the\nstate space equations provided in Eqs. (2a) and (2b). The new parametric representations of A, B, and C are\nA = exp(\u0394\u0391),\nB = (\u0394\u0391)-1 (exp(\u0394\u0391) \u2013 I) \u00b7 \u0394\u0392,\nand C = C. The proof of Eqs. (12) and (13) is included in Appendix A.3. However, in their official code\nimplementation, they choose a simplified version of the ZOH discretization method to facilitate computation\nwithout affecting empirical performance*:\nA = exp(\u0394\u0391),\n\u0392 = \u0394\u0392,\nand C = C. The analysis of the approximation version is given in Appendix A.4. It is critical to stress that the\nsimplified discretization representation is adopted by almost all the Mamba-based research. \u2020 The preceding\ndiscussions constitute the first contribution of the Mamba paper [16], i.e., the Mamba model.\nThe second contribution lies in the design of a straightforward and homogenous module incorporating the\nMamba model, drawing inspiration from the gated multi-layer perceptron (MLP). The module is referred to as the\nvanilla Mamba block in this study and is depicted in Fig. 1 (a), where Gu and Dao choose SiLU or Swish as the\nnonlinear activation function. The core component of the vanilla Mamba block is the SSM module which executes\nthe aforementioned Mamba model, which is also known as the SSMs+Selection mechanism, and henceforth denoted\nas SSM of Fig. 1 (a) for brevity. Recognizing the complexity of Mamba's computations, we illustrate the internal\noperational procedures of the SSM module in Fig. 1 (b) to facilitate readers' comprehension. The default scanning\ndirection is the forward path, progressing from x0 to xL\u22121, as depicted by the direction of the Mamba snake in Fig.\n1 (b)."}, {"title": "2.2. Mamba in computer vision", "content": "Vim and VMamba take the lead to explore the effectiveness of Mamba in the field of CV, standing out as the two\nmost prominent visual Mamba models. Thus, this study solely investigates these models for the domain of Mamba\nin computer vision.\nGiven that the scanning mechanism in the vanilla Mamba block operates along a 1D sequence (from x0 to\nxL\u22121), which contradicts the two-dimensional (2D) image space, Vim and VMamba have adopted the same solution\nas that of Vision Transformer (ViT) [27]. Specifically, first, represent a 2D image as M\u2208RC\u00d7H\u00d7W, where C, H,\nand W denote the channel, height, and width, respectively. Subsequently, the image is partitioned into numerous\nsmall fixed-size patches along the height and width dimensions, with each patch having a spatial resolution of\n((hp, wp)). This results in the shape of each patch being [RC\u00d7hp\u00d7wp, and the total number of patches being N =\n(HW)/(hpwp). Next, each patch is flattened into a 1D sequence, transforming its shape to R(C\u00d7hpxwp).\nConsequently, the original image is reshaped into Mp \u2208 RN\u00d7(C\u00d7hpxwp), where Mp can be viewed as a 1D\nsequence comprising N elements and each element with a dimension of (C \u00d7 hp \u00d7 wp). Specifically, Mp [i], i \u2208\n{0,1,..., N \u2013 1} corresponds to xr in Eq. (3a). With the acquisition of the input 1D sequence, theoretically, we can\ndirect use Mamba to model global dependencies. However, to enhance the handling of spatial context information\ninherent in visual data, Vim and VMamba have refined the vanilla Mamba block, with detailed enhancements\nintroduced below.\nVim imitates the vanilla Mamba block to design the vision Mamba (Vim) block, as depicted in Fig. 2 (a). In\ncontrast to the vanilla Mamba block (refer to Fig. 1 (a)), Vim designs a bidirectional scanning strategy illustrated in\nFig. 2 (b), encompassing both a forward and a backward route. Vim executes a singular SSM operation (see Fig. 1\n(b)) for each route. Leveraging this bidirectional scanning strategy, each element Mp[i] in the sequence\namalgamates information not only from Mp[0] to Mp[i] (forward information) but also from Mp [N - 1] to\nMp[i] (backward information). Consequently, Vim can model long-range dependency relationships more\neffectively than the vanilla Mamba block. Moreover, the symbol L in Fig. 2 (a) denotes the embedded count of Vim\nblocks, with L set to 2 in the experiments conducted in this paper.\nVMamba introduces an innovative extension by broadening the bidirectional scan into a cross scan,\nencompassing four distinct scanning paths, as illustrated in Fig. 3 (a). Notably, the top two routes in Fig. 3 (a)\ncorrespond to the forward and backward routes in Vim. Subsequently, VMamba designs a 2D-Selective-Scan (SS2D)\nmodule to actualize the cross scan. The SS2D module consists of two primary steps: initially, SS2D conducts a\nsingle SSM operation (as depicted in Fig. 1 (b)) for each scanning route, followed by summing the outcomes from\nthe four scanning paths to yield its output. Moreover, VMamba designs two blocks that incorporate SS2D as the\nfundamental components that make up visual Mamba networks: the vanilla Visual State Space (VSS) block and the\nVSS block. The conceptualization of the vanilla VSS block draws inspiration from the vanilla Mamba block (refer\nto Fig. 1 (a)), with the key distinction being the substitution of the SSM module with SS2D, as shown in Fig. 3 (b).\nFurthermore, the tremendous and widespread successes of ViT and its variants in computer vision demonstrate the\neffectiveness of the structure of the Transformer encoder in ViT. Thus, the VSS block mimics the Transformer\nencoder. Specifically, the VSS block replaces the multi-head self-attention (MHSA) in the Transformer encoder\nwith the SS2D block, as illustrated in Fig. 3 (c).\nIn the experimental section (Section 5), we select the three Mamba blocks, namely, the Vim block, the vanilla\nVSS block, and the VSS block to explore their representation capabilities to crack features. Theoretically, both the\nvanilla VSS block and the VSS block are anticipated to outperform the Vim block. This expectation arises from the\nSS2D module within these blocks, which incorporates four diverse scanning routes, bringing a more comprehensive\ninformation interaction and a richer spatial context information compared to the Vim block's forward and backward\nscanning paths. The experimental evidence presented in Table 2 and Table 4 robustly substantiates this theoretical\nconjecture.\nNevertheless, it is crucial to highlight that the experimental outcomes further reveal the three visual Mamba\nblocks cannot bring consistent positive impact to the segmentation model across distinct crack datasets. For instance,\non the Deepcrack dataset, the Vim block demonstrates a decrease in performance (see Table 2), while on the\nSteelcrack dataset, both the Vim block and the VSS block exhibit a detrimental effect on model performance, and\nthe vanilla VSS block showcases almost negligible impact (refer to Table 4). This observation contradicts the\nconclusions drawn in prior studies such as Vim [19], VMamba [20], Mamba-UNet [21] (utilizing the vanilla VSS\nblock), or RS3Mamba [22] (also employing the vanillza VSS block), where Mamba consistently achieved\nremarkable performance for not only general visual representation tasks but also specific domains like medical\nimage data and remote sensing images. Given Mamba's track record of achieving SOTA performance in various\nmodels and tasks, this study posits that the ineffectiveness of the three Mamba blocks in the crack segmentation\ntask is not inherently linked to Mamba itself, but rather to the specific structures of these blocks. Consequently, a\nthorough examination of the essence of Mamba is conducted, leading to the proposal of a novel block named\nCrackMamba for crack segmentation, which is detailed in the subsequent subsection."}, {"title": "2.3. Mamba block in crack segmentation", "content": "2.3.1.\nMotivation: an attention perspective\nFirst, let us rethink Mamba from a higher and more profound perspective. The fundamental enhancement\nintroduced by Mamba is its selection mechanism compared to the S4 models. This mechanism makes the parameters\nin Mamba dynamic and input-dependent, thereby eliminating the time-invariance characteristic. Essentially, this\ndynamic parameterization empowers Mamba-based models to intelligently select relevant information while\nfiltering out worthless data, enhancing the model's representation power and overall performance [16].\nNext, let us reflect on the concept of human attention, which plays an essential role in human perception. For\nhuman attention, individuals can actively allocate their valuable attention to subjects of interest while diminishing\nfocus on the objects they are less interested in [23]. The attention mechanism in the deep learning domain is inspired\nby this innate human ability and replicates this functionality. Specifically, the attention mechanism increases or\ndecreases the weights of the specific regions within feature tensors to accentuate or dampen the regions. As for how\nto ensure the regions where the weights are increased are indeed meaningful and significant, and the increased scale\nof the parameters (vice versa), the core strategy is to make the attention parameters input-dependent and dynamic\n[29], enabling the models to autonomously capture the underlying relationships between the attention parameters\nand the inputs.\nUpon merging Mamba with the attention mechanism, a profound realization emerges: the essence of both is\nthe input-dependent parameters. Consequently, essentially, Mamba can be viewed as a kind of attention mechanism.\nNotably, the attention mechanism has been extensively leveraged in the crack segmentation task, consistently\nenhancing the representation power to crack features and elevating identification performance of segmentation\nmodels [1][2][23][24][25]. This observation leads to the hypothesis that if the design of the Mamba block aligns\nwith the principles governing attention blocks, it could potentially achieve consistent improvements to baseline\nsegmentation models. Motivated by this notion, we introduce an attention block-like Mamba block, termed\nCrackMamba, elaborated upon in the following subsection.\n2.3.2.\nDesign of CrackMamba\nIn attention blocks, a pivotal element is the attention map (AM), which contains the relative importance information\namong different regions within the feature maps. Taking the three most representative attention blocks as an example,\nnamely, SENet [29] (illustrated in Fig. 4 (a)), CBAM [30] (depicted in Fig. 4 (b)), and parallel attention module\n(PAM) [23] (shown in Fig. 4 (c)), their AMs are derived by normalizing the feature maps to a range of (0,1) through\na Sigmoid function. Subsequently, another crucial feature of the attention blocks is that the outputs of these attention\nmodules are obtained through element-wise multiplication of the original feature maps and the AMs. Therefore, the\nprimary enhancement introduced by CrackMamba, in contrast to the conventional Mamba blocks, lies in the\nincorporation of AM. Besides, the outcome of CrackMamba also entails element-wise multiplication of the original\nfeature map with the AM, as depicted in Fig. 4 (d).\nThe design of the specific structure of CrackMamba is twofold. First, considering that the vanilla VSS block\nstands out as the sole Mamba block that does not weaken model performance among the three 2D Mamba blocks\n(as discussed in Subsection 2.2), CrackMamba is constructed upon the foundation of the vanilla VSS block and also\nincorporates a DW Conv and an SS2D module. Second, drawing inspiration from the success of the parallel attention\nmodule (PAM) in the crack segmentation task [1] and its broader applications in the civil engineering domain such\nas bridge defect inspection [23], building structure identification [31], and structural health inspection [32],\nCrackMamba's design is influenced by the architecture of PAM. Specifically, in the domain of crack segmentation,\nthe specific manifestation of PAM is the feature fusion module (FFM) within CrackSeU, with its core part illustrated\nin Fig. 4 (c). Hence, CrackMamba adopts a structure similar to that of FFM, featuring two branches and a skip\nconnection. Besides, referring to the output composition of FFM, the ultimate output of CrackMamba entails the\nelement-wise addition of the combination of the two branches and the skip connection, as shown in Fig. 4 (c) and\nFig. 4 (d).\nThe quantitative assessments of crack identification in Section 5 unequivocally demonstrate that CrackMamba\nconsistently enhances the baseline model's performance across both datasets, affirming the effectiveness of\nCrackMamba. Moreover, benefiting from the linear spatial and computational complexity of Mamba, CrackMamba-\nbased CrackSeU-B has fewer parameters and reduced computational complexity compared to the original\nCrackSeU-B (as evidenced in Table 2 and Table 4). Therefore, as a simple yet effective and plug-and-play module,\nCrackMamba exhibits immense potential for widespread application in crack identification. Furthermore, the\nconcept of integrating Mamba with the attention mechanism introduces a fresh paradigm for designing Mamba\nblocks, offering the potential to innovate existing Mamba blocks, thereby enhancing the overall performance of\nMamba-based models across various computer vision tasks."}, {"title": "3. Crack segmentation network and crack datasets", "content": "3.1. Crack segmentation network\n3.1.1.\nCrack Segmentation U-shape network (CrackSeU)\nIn order to assess the effectiveness of Mamba to crack identification", "1": ".", "7": [33], "34": "indicating that summarized patterns and insights from CrackSeU are\nuniversally applicable to a broad spectrum of crack segmentation networks. Second", "35": "U-Net++ [36", "37": "to multi-level fusion. This multi-level fusion strategy enables more effective and thorough\nmulti-scale integration of crack features and facilitates intra-network information flow"}, {"1": ".", "configurations": "CrackSeU-B, CrackSeU-M, and CrackSeU-L,\ndenoting base, middle, and large volume models, respectively. Considering that CrackSeU-B achieves the best\nbalance between performance and computational complexity, CrackSeU-B is chosen as the final research object.\nFor simplicity, in this paper, CrackSeU refers to CrackSeU-B unless specified otherwise.\n3.1.2.\nIntegrate CrackSeU-B and Mamba\nNext, a key challenge is how to integrate CrackSeU-B and Mamba. In this paper, we do not additionally design new\nfancy architectures for integration, instead, we simply"}]}