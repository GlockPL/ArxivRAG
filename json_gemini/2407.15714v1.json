{"title": "Mamba meets crack segmentation", "authors": ["Zhili He", "Yu-Hsing Wang"], "abstract": "Cracks pose safety risks to infrastructure that cannot be overlooked and serve as a vital indicator of structural performance deterioration. Enhancing the precision and efficiency of crack segmentation is a prominent research topic. The prevailing architectures in existing crack segmentation networks predominantly consist of convolutional neural networks (CNNs) or Transformers. Nonetheless, CNNs exhibit a deficiency in global modeling capability, hindering their representation to entire crack features. Transformers can capture long-range dependencies but suffer from high and quadratic complexity. Recently, the emergence of a novel architecture, Mamba, has garnered considerable attention due to its linear spatial and computational complexity, as well as its powerful global perception capability. This study explores the representation capabilities of Mamba to crack features. Specifically, this paper uncovers the intrinsic connection between Mamba and the attention mechanism, providing a profound insight an attention perspective\u2014into interpreting Mamba and devising a novel Mamba module following the principles governing attention blocks, namely CrackMamba. We compare CrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on two datasets comprising asphalt pavement and concrete pavement cracks, and steel cracks, respectively. The quantitative results demonstrate that CrackMamba stands out as the sole Mamba block consistently enhancing the baseline model's performance across all evaluation measures, while reducing its parameters and computational costs. Moreover, this paper substantiates that Mamba can achieve global receptive fields through both theoretical analysis and visual interpretability. The discoveries of this study offer a dual contribution. First, as a plug-and-play and simple yet effective Mamba module, CrackMamba exhibits immense potential for integration into various crack segmentation models. Second, the proposed innovative Mamba design concept, integrating Mamba with the attention mechanism, holds significant reference value for all Mamba-based computer vision models, not limited to crack segmentation networks, as investigated in this study.", "sections": [{"title": "1. Introduction", "content": "Defects inevitably occur, accumulate, and propagate during the service life of civil infrastructure due to the intricate working environment. These defects not only pose significant safety hazards to structures but also threaten their service life. Among the various forms of structural defects, cracks are a primary manifestation and a pivotal indicator of structural performance degradation, demanding meticulous attention [1].\nRoutine crack inspection serves as an effective and widely employed method for mastering crack information of infrastructure. Presently, the predominant mode of inspection is manual visual examination conducted onsite, a practice that is both time-consuming and labor-intensive. Moreover, the subjective nature of human assessment introduces a notable level of uncertainty and inconsistency in the inspection outcomes [2]. Recognizing these limitations, there is a growing interest among researchers and practitioners in integrating cutting-edge artificial intelligence (AI) and computer vision (CV) technologies into crack inspection practices to achieve intelligent, efficient, and precise crack identification [3][4][5]. These AI-driven crack identification methodologies are typically categorized into three distinct groups based on the nature of the researched CV tasks: image-level crack classification, box-level crack detection, and pixel-level crack segmentation. Notably, He et al. [2] have identified crack segmentation as the most appropriate and accurate approach for representing crack features, thus, the research focus of this paper is crack segmentation.\nInitially, crack segmentation models were constructed using convolutional neural networks (CNNs) [1][6][7]. However, considering that on the one hand the ability of CNNs to capture long-distance dependencies is so limited due to the inherent locality of convolutions, and on the other hand, crack pixels are typically dispersed globally on crack images in their length directions, it is challenging for CNN-based crack segmentation networks to comprehensively model the entirety of crack features. In contrast, a newer architecture than CNNs, Transformers, based on self-attention [8][9], inherently excels in modeling long-term context information. Consequently, Transformers have been integrated into crack segmentation networks either to replace CNNs [10] or to work with them [2], bringing the networks global perceptual capabilities. The frameworks incorporating Transformers [2] [10] have demonstrated the benefits of effectively modeling long-range context features in crack segmentation tasks, further underscoring the necessity for crack segmentation networks to possess global modeling capabilities. Despite the advantages, the computational and spatial complexity of Transformers grows quadratically with the length of input sequences, presenting an efficiency bottleneck. As a result, researchers have endeavored to refine the architecture of Transformer to reduce computational and spatial costs without compromising its global perception abilities [11][12]. Various efficient modules have been proposed, such as Sparse Transformers [13], Linear attention [14], and FlashAttention [15]. However, these advanced modules only optimize the architecture of the vanilla Transformer without fundamentally innovating its core structure, namely, the scaled dot product self-attention. Given the well-established maturity and widely proven generalization and robustness capabilities of the vanilla Transformer, researchers and engineers still tend to favor the conventional Transformer architecture. The newer and more intricate Transformer structures, despite their novelty, have not yet emerged as the mainstream choice due to this preference.\nRecently, in the realm of one-dimensional (1D) sequence modeling, an architectural innovation known as Mamba [16], coming from state space models (SSMs) [17][18] rather than the above-mentioned self-attention, has garnered significant attention because Mamba's powerful global modeling capabilities, state-of-the-art (SOTA) performance across diverse tasks encompassing multiple modalities, such as natural language processing (NLP), audio modeling, and genomics modeling, and more importantly, its linear complexity. Notably, pioneering work by Zhu et al. [19] and Liu et al. [20] has introduced Mamba into the domain of computer vision, resulting in the development of the Vision Mamba (Vim) model [19] and the VMamba model [20], respectively. These studies have demonstrated the conspicuous success of Mamba in learning general visual representation and global features of images. Specifically, Vim and VMamba have not only surpassed many well-established vision networks in performance but have also exhibited faster processing speeds and lower memory and computing resource requirements. Building on these achievements, a range of well-crafted models based on Vim or VMamba have been effectively deployed in specific downstream tasks of CV, such as Mamba-UNet [21] for medical image analysis and RS3Mamba [22] for remote sensing image segmentation. The series of developments underscore Mamba's versatility and immense potential to supplant CNNs and Transformers to be the next generation foundational architecture of CV. However, despite these advancements, the application of Mamba in the context of crack images remains unexplored. This poses an intriguing question regarding Mamba's ability to effectively represent and comprehend crack features. Addressing this question is of paramount importance, as successful utilization of Mamba in crack data analysis could revolutionize the field of crack identification, propelling it into a new stage of research and innovation.\nTherefore, this study investigates the representation ability of Mamba to crack data. Initially, a classical and influential crack segmentation model, CrackSeU [1], is selected, along with two representative and publicly available crack datasets encompassing cracks from distinct domains. The reason of choosing the datasets is to examine if Mamba can consistently improve performance of the model to the cracks from different domains. The first dataset, Deepcrack [1][41], encompasses asphalt pavement cracks and concrete pavement cracks, while the second dataset, Steelcrack [2], comprises cracks in steel structures. Subsequently, Vim and VMamba are integrated into CrackSeU, and the enhanced networks are trained and evaluated on the two crack datasets. Unfortunately, the experimental outcomes in Subsection 5.1 reveal that Vim and VMamba cannot consistently enhance and, in some cases, even diminish the model's performance. Upon thorough investigation, it is discovered that the essence of Mamba and the attention mechanism [1][23] is unified in dynamic parameters, therefore, Mamba can be viewed as a type of attention. Given the universal success of the attention mechanism in crack identification [1][23][24][25], this study redesign the visual Mamba modules to propose the CrackMamba block, which aligns with the attention modules. Notably, the experimental results in Subsection 5.1 demonstrate that CrackMamba stands out as the sole Mamba module capable of consistently enhancing the crack recognition performance of CrackSeU across both datasets. Furthermore, CrackSeU combined with CrackMamba (CrackSeU + CrackMamba) requires fewer parameters and lower computational resources compared to the standard CrackSeU model. From the insights gained through this research, several key conclusions can be drawn: (1) the performance of Mamba modules is highly related to the tasks, and Vim and VMamba blocks are proved unsuitable for crack segmentation tasks; (2) the structural composition of Mamba blocks is crucial and significantly influences the performance of Mamba-based Al models, therefore, it is necessary to design the structures of the Mamba modules tailored to specific tasks; (3) the innovative perspective introduced, known as the attention perspective, serves as a valuable guide in shaping Mamba blocks and also deepens the comprehension of Mamba; and (4) it is believed that the proposed Mamba design concept, that designing Mamba modules aligning with attention modules, holds enlightening significance for a wide array of CV tasks which either have employed or are considering the utilization of visual Mamba models, not limited to the crack identification problem, as explored in this study. Moreover, this paper demonstrates that Mamba can bring global receptive fields from both theoretical and visual interpretability standpoints.\nTo summarize, the principal contributions of this research are outlined as follows:\n(1) Mamba and visual Mamba, including Vim and VMamba, are introduced into the domain of crack segmentation. Detailed illustration and analysis, often overlooked in existing literature related to Mamba, are provided in the appendices to ensure clarity for the civil engineering audience.\n(2) This research offers a heightened and profound perspective for comprehending Mamba, termed the attention perspective. Subsequently, a novel, straightforward, yet highly effective Mamba module, named CrackMamba, is designed following the principles of attention modules.\n(3) Vim, VMamba, and CrackMamba are evaluated on two representative crack datasets, with the findings indicating that CrackMamba is the only Mamba block to consistently enhance crack segmentation performance. Additionally, CrackMamba reduces the number of parameters and computational complexity of the baseline model. These outcomes not only validate the effectiveness of CrackMamba and emphasize its substantial application potential in crack segmentation models, but also confirm the success of the proposed attention perspective and highlight the importance of designing Mamba blocks that incorporate the principles of the attention mechanism, which offers significant reference value for various Mamba-based CV models, not solely limited to the crack segmentation frameworks investigated by this study.\n(4) This paper demonstrates the global modeling capability of Mamba through both the theoretical analysis and visual interpretability perspectives.\nThe structure of this paper is outlined as follows: Section 2 offers detailed descriptions of Mamba, visual Mamba, and CrackMamba. Section 3 presents the adopted segmentation model, the integration of Mamba and the segmentation network, and the crack datasets. Implementation details, encompassing the training policy and the evaluation measures, are thoroughly discussed in Section 4. Section 5 delves into the quantitative experimental results and provides an in-depth examination of the global receptive field of Mamba. Finally, Section 6 concludes the study and offers suggestions for future research endeavors. The implementation code for CrackMamba and a tutorial on installing Mamba are openly accessible at https://github.com/hzlbbfrog/CrackMamba."}, {"title": "2. Mamba", "content": "Notations. Let us introduce the notation rules adopted in this paper to avoid confusion. Following [26], matrices or tensors in neural network are denoted by non-tilt boldface uppercase letters, such as A and X. Vectors are denoted by non-tilt boldface lowercase letters, such as x and u. Scalars are represented by tilt non-bold lower-case letters, such as x and t. Additionally, vectors are all column vectors. For instance,  x = [x_0, x_1, ..., x_{L-1}]^T is adopted to represent the vector x formed by L scalars."}, {"title": "2.1. Vanilla Mamba", "content": "The evolution of state space models (SSMs) unfolds through two pivotal iterations: Structured state space sequence models, also known as Structured SSMs or S4 models [17] [18], progress into Selective SSMs, referred to as SSMs+Selection or S6 models (aka Mamba) [16]. These iterations are elaborated upon in the subsequent contents.\nThe basic idea of S4 models comes from the state space equations in the modern control theory domain, and they are adopted to describe a linear time invariance (LTI) system:\n\\dot{x}(t) = Ax(t) + Bu(t), \\tag{1a}\ny(t) = Cx(t) + Du(t), \\tag{1b}\nFor a first-order system, the parameters in the above equations are scalars. Here, we only consider second-order or higher-order systems. Hence, x(t) represents the state vector of the system, u(t) is the system input, and y(t) denotes the system output. A, B, C, and D are the state matrices used to describe the system. Since this system is a time invariance system, A, B, C, and D are all constant matrices for any time t.\nSSMs mainly make three modifications to the above state space equations. First, SSMs simplify (1b) and omit D in (1b) (or equivalently, D = 0) [18]. This simplification is reasonable because  Du(t) can be viewed as a shortcut connection in neural networks and can be computed easily. Second, the input and output are assumed to be the 1D continuous signals. Third, the notations are slightly changed. Thus, the state space equations in the domain of SSMs can be defined as\n\\dot{h}(t) = Ah(t) + Bx(t), \\tag{2a}\ny(t) = Ch(t), \\tag{2b}\nwhere the hidden state variable is changed to h(t) \u2208 \\mathbb{R}^{N\\times 1}, and N denotes the dimension of the state vector. The system input is x(t), and y(t) still denotes the output, with t representing the continuous time index. Clearly,  A \u2208 \\mathbb{R}^{N\\times N}, B \u2208 \\mathbb{R}^{N\\times 1}, and C \u2208 \\mathbb{R}^{1\\times N} based on the matrix operation rule. For the subsequent content, the symbols' meanings are consistent with Eqs. (2a) and (2b) unless specified otherwise. It is worth noting that the research core of all SSMs is how to effectively represent Eqs. (2a) and (2b) and effectively implement them in AI models and computing hardware.\nS4 models first discretize the above state space equations since the calculations in the neural work are always discrete:\nh_k = Ah_{k-1} + Bx_k, \\tag{3a}\ny_k = Ch_k \\tag{3b}\nAt this time, Eqs. (3a) and (3b) describe the mapping between two discrete signals (or sequences):  x \u2208 \\mathbb{R}^{L\\times 1} = [x_0, x_1, ..., x_{L-1}] \\Rightarrow y \u2208 \\mathbb{R}^{L\\times 1} = [y_0, y_1, \u2026, y_{L-1}]^T, where L denotes the length of the sequences. In the above equations,  k \u2208 {0,1,..., L \u2212 1} denotes the discrete index, and A \u2208 \\mathbb{R}^{N\\times N}, B \u2208 \\mathbb{R}^{N\\times 1}, and \\hat{C} \u2208 \\mathbb{R}^{1\\times N} are the discrete parameters corresponding to the continuous parameters A, B and C, and they can typically be expressed as\n\\hat{A} = f_A(\\Delta, A), B = f_B(\\Delta, A, B), and \\hat{C} = C. \\tag{4}\nThe most straightforward discretization method is the Euler discretization method (see Appendix A. 1), however, it is not precise enough. Thus, S4 models adopt a more advanced method, the bilinear discretization method, where A, B, and C can be further formulated as\n\\hat{A} = (I - \\Delta / 2 \\cdot A)^{-1} (I + \\Delta / 2 \\cdot A), \\tag{5}\nB = (I - \\Delta / 2 \\cdot A)^{-1} \\Delta B, \\tag{6}\nand\n\\hat{C} = C, \\tag{7}\nwhere  \\Delta \u2208 \\mathbb{R}^+ represents the time step size of discretization, and  I \u2208 {0,1}^{N\\times N} is the identity matrix. The proof of the bilinear discretization method is provided in Appendix A.2. Second, S4 models leverage the high-order polynomial projection operator (HiPPO) Matrix to initialize A to handle long-range dependencies, and S4 models find that in contrast to random initialization, the HiPPO Matrix initialization can improve the performance on the sequential MNIST classification benchmark from 60% to 98% [17][18]. HiPPO Matrix can be defined by the following formula:\nHiPPO \\ Matrix\\ A_{ij} = \\begin{cases}\n(2i + 1)^{1/2}(2j + 1)^{1/2} & if\\ i > j \\\\\ni+1 & if\\ i = j. i, j \u2208 [0,1, ..., N - 1].\\\\\n0 & if\\ i < j\n\\end{cases} \\tag{8}\nThird, S4 models combine Eqs. (3a) and (3b) for the entire sequence:\ny = K \\ast x, \\tag{9a}\nK = (CB, CAB, ..., CA^{L-1}B), \\tag{9b}\nwhere \\ast denotes convolution operation,  K \u2208 \\mathbb{R}^{L} is the one-dimensional (1D) convolutional kernel. The combination process is detailed in Appendix B. This shows that S4 models can be transferred into the convolutional form so that they can be trained in parallel like CNNs, and the explanation of parallel training is provided in Appendix C.1. In practice, S4 models pre-compute Krylov matrix K_r \u2208 \\mathbb{R}^{N\\times L} of the convolutional kernel K [17]:\nK_r = (B, AB, ..., A^{L-1}B). \\tag{10}\nSubsequently, the kernel K can be calculated through a matrix multiplication: K = CK_r. Finally, Eq. (9a) is executed in parallel. Besides, it is evident that S4 models can intrinsically capture long-term dependencies and bring global perception without the intricate hierarchical stacking structures found in CNNs. The aforementioned details describe the fundamental concept underlying S4 models.\nFurther, it becomes apparent that the primary computational burden of S4 models lies in computing the latent state K_r, which necessitates L consecutive matrix multiplications by A, involving O(N^2L) multiplication operations and O(NL) space complexity [18]. The detailed complexity statements are provided in Appendix D. This substantial computational load constitutes the fundamental bottleneck of the na\u00efve S4 models. Subsequent advancements in S4 models pay attention to how to improve computational efficiency. For example, Gu et al. [18] have discovered that A can be transformed into a diagonal plus low-rank (DPLR) form, and this technique can reduce the computational complexity to \\widetilde{O}(N + L) and the space complexity to O(N + L), respectively, where \\widetilde{O}(1) denotes the cost of a single Cauchy matrix-vector multiplication.\nGu and Dao [16] observe that conventional S4 models are all inherently linear time invariant and lack the capability to selectively focus on or disregard particular inputs because the parameters K are not related to inputs once SSMs are well-trained. Therefore, to address this limitation, they introduce a selection mechanism into S4 models, giving rise to Mamba (referred to as SSMs+Selection). The core idea of the selection mechanism lies in parameterizing A, B and C in an input-dependent manner, enabling the new model to dynamically select valuable information and filter out irrelevant data indefinitely based on the inputs. Specifically, Mamba models B, C, and  \\Delta as the functions of the input sequence x:\nB = S_B(x), C = S_C(x),  \\Delta= \\tau_{\\Delta} (Parameters + S_{\\Delta}(x)), \\tag{11}\nwhere S_B(x) = Linear_N(x) and S_C(x) = Linear_N(x), with Linear_N representing a linear and parameterized projection to dimension N. In the neural network domain, this is also named a fully connected layer (FCL). The symbol  \\tau_{\\Delta} represents the Softplus activation functions, and  S_{\\Delta} = Broadcast_N(Linear_N(x)), which is inspired by the recurrent neural network (RNN) gating mechanism [16]. Regarding the matrix A in Eq. (2a), it remains input-invariant and is initialized using the HiPPO Matrix for simplicity. This is due to  \\hat{A} = f_A(\\Delta, A) and B = f_B(\\Delta, A, B) (refer to Eq. (4)), where selectivity in A and B guarantees selectivity in A and B. Subsequently, in the Mamba paper [16], Gu and Dao claim that they employ the zero-order hold (ZOH) technique to discretize the state space equations provided in Eqs. (2a) and (2b). The new parametric representations of A, B, and C are\n\\hat{A} = exp(\\Delta A), \\tag{12}\nB = (\\Delta A)^{-1} (exp(\\Delta A) \u2013 I) \\cdot \\Delta B, \\tag{13}\nand  \\hat{C} = C. The proof of Eqs. (12) and (13) is included in Appendix A.3. However, in their official code implementation, they choose a simplified version of the ZOH discretization method to facilitate computation without affecting empirical performance*:\n\\hat{A} = exp(\\Delta A), \\tag{14}\nB = \\Delta B, \\tag{15}\nand  \\hat{C} = C. The analysis of the approximation version is given in Appendix A.4. It is critical to stress that the simplified discretization representation is adopted by almost all the Mamba-based research.  The preceding discussions constitute the first contribution of the Mamba paper [16], i.e., the Mamba model.\nThe second contribution lies in the design of a straightforward and homogenous module incorporating the Mamba model, drawing inspiration from the gated multi-layer perceptron (MLP). The module is referred to as the vanilla Mamba block in this study and is depicted in Fig. 1 (a), where Gu and Dao choose SiLU or Swish as the nonlinear activation function. The core component of the vanilla Mamba block is the SSM module which executes the aforementioned Mamba model, which is also known as the SSMs+Selection mechanism, and henceforth denoted as SSM of Fig. 1 (a) for brevity. Recognizing the complexity of Mamba's computations, we illustrate the internal operational procedures of the SSM module in Fig. 1 (b) to facilitate readers' comprehension. The default scanning direction is the forward path, progressing from x0 to xL\u22121, as depicted by the direction of the Mamba snake in Fig. 1 (b)."}, {"title": "2.2. Mamba in computer vision", "content": "Nevertheless, the prowess of Transformer lies in its versatility across various modalities, encompassing not only sequential information like text and audio but also visual elements such as images and videos. Therefore, researchers have promptly integrated Mamba into the realm of computer vision (CV), which is detailed in the subsequent subsection.\nVim and VMamba take the lead to explore the effectiveness of Mamba in the field of CV, standing out as the two most prominent visual Mamba models. Thus, this study solely investigates these models for the domain of Mamba in computer vision.\nGiven that the scanning mechanism in the vanilla Mamba block operates along a 1D sequence (from x0 to xL-1), which contradicts the two-dimensional (2D) image space, Vim and VMamba have adopted the same solution as that of Vision Transformer (ViT) [27]. Specifically, first, represent a 2D image as M \u2208 \\mathbb{R}^{C\\times H\\times W}, where C, H, and W denote the channel, height, and width, respectively. Subsequently, the image is partitioned into numerous small fixed-size patches along the height and width dimensions, with each patch having a spatial resolution of ((h_p, w_p)). This results in the shape of each patch being [\\mathbb{R}^{C\\times h_p\\times w_p}, and the total number of patches being N = (HW)/(h_p w_p). Next, each patch is flattened into a 1D sequence, transforming its shape to \\mathbb{R}^{(C\\times h_p\\times w_p)}. Consequently, the original image is reshaped into M_p \u2208 \\mathbb{R}^{N\\times(C\\times h_p\\times w_p)}, where M_p can be viewed as a 1D sequence comprising N elements and each element with a dimension of (C \u00d7 h_p \u00d7 w_p). Specifically,  M_p[i], i \u2208 {0,1,..., N \u2013 1} corresponds to x_i in Eq. (3a). With the acquisition of the input 1D sequence, theoretically, we can direct use Mamba to model global dependencies. However, to enhance the handling of spatial context information inherent in visual data, Vim and VMamba have refined the vanilla Mamba block, with detailed enhancements introduced below.\nVim imitates the vanilla Mamba block to design the vision Mamba (Vim) block, as depicted in Fig. 2 (a). In contrast to the vanilla Mamba block (refer to Fig. 1 (a)), Vim designs a bidirectional scanning strategy illustrated in Fig. 2 (b), encompassing both a forward and a backward route. Vim executes a singular SSM operation (see Fig. 1 (b)) for each route. Leveraging this bidirectional scanning strategy, each element Mp[i] in the sequence amalgamates information not only from Mp[0] to Mp[i] (forward information) but also from Mp[N - 1] to Mp[i] (backward information). Consequently, Vim can model long-range dependency relationships more effectively than the vanilla Mamba block. Moreover, the symbol L in Fig. 2 (a) denotes the embedded count of Vim blocks, with L set to 2 in the experiments conducted in this paper.\nVMamba introduces an innovative extension by broadening the bidirectional scan into a cross scan, encompassing four distinct scanning paths, as illustrated in Fig. 3 (a). Notably, the top two routes in Fig. 3 (a) correspond to the forward and backward routes in Vim. Subsequently, VMamba designs a 2D-Selective-Scan (SS2D) module to actualize the cross scan. The SS2D module consists of two primary steps: initially, SS2D conducts a single SSM operation (as depicted in Fig. 1 (b)) for each scanning route, followed by summing the outcomes from the four scanning paths to yield its output. Moreover, VMamba designs two blocks that incorporate SS2D as the fundamental components that make up visual Mamba networks: the vanilla Visual State Space (VSS) block and the VSS block. The conceptualization of the vanilla VSS block draws inspiration from the vanilla Mamba block (refer to Fig. 1 (a)), with the key distinction being the substitution of the SSM module with SS2D, as shown in Fig. 3 (b). Furthermore, the tremendous and widespread successes of ViT and its variants in computer vision demonstrate the effectiveness of the structure of the Transformer encoder in ViT. Thus, the VSS block mimics the Transformer encoder. Specifically, the VSS block replaces the multi-head self-attention (MHSA) in the Transformer encoder with the SS2D block, as illustrated in Fig. 3 (c).\nNevertheless, it is crucial to highlight that the experimental outcomes further reveal the three visual Mamba blocks cannot bring consistent positive impact to the segmentation model across distinct crack datasets. For instance, on the Deepcrack dataset, the Vim block demonstrates a decrease in performance (see Table 2), while on the Steelcrack dataset, both the Vim block and the VSS block exhibit a detrimental effect on model performance, and the vanilla VSS block showcases almost negligible impact (refer to Table 4). This observation contradicts the conclusions drawn in prior studies such as Vim [19], VMamba [20], Mamba-UNet [21] (utilizing the vanilla VSS block), or RS3Mamba [22] (also employing the vanillza VSS block), where Mamba consistently achieved remarkable performance for not only general visual representation tasks but also specific domains like medical image data and remote sensing images. Given Mamba's track record of achieving SOTA performance in various models and tasks, this study posits that the ineffectiveness of the three Mamba blocks in the crack segmentation task is not inherently linked to Mamba itself, but rather to the specific structures of these blocks. Consequently, a thorough examination of the essence of Mamba is conducted, leading to the proposal of a novel block named CrackMamba for crack segmentation, which is detailed in the subsequent subsection."}, {"title": "2.3. Mamba block in crack segmentation", "content": ""}, {"title": "2.3.1. Motivation: an attention perspective", "content": "First, let us rethink Mamba from a higher and more profound perspective. The fundamental enhancement introduced by Mamba is its selection mechanism compared to the S4 models. This mechanism makes the parameters in Mamba dynamic and input-dependent, thereby eliminating the time-invariance characteristic. Essentially, this dynamic parameterization empowers Mamba-based models to intelligently select relevant information while filtering out worthless data, enhancing the model's representation power and overall performance [16].\nNext, let us reflect on the concept of human attention, which plays an essential role in human perception. For human attention, individuals can actively allocate their valuable attention to subjects of interest while diminishing focus on the objects they are less interested in [23]. The attention mechanism in the deep learning domain is inspired by this innate human ability and replicates this functionality. Specifically, the attention mechanism increases or decreases the weights of the specific regions within feature tensors to accentuate or dampen the regions. As for how to ensure the regions where the weights are increased are indeed meaningful and significant, and the increased scale of the parameters (vice versa), the core strategy is to make the attention parameters input-dependent and dynamic [29], enabling the models to autonomously capture the underlying relationships between the attention parameters and the inputs.\nUpon merging Mamba with the attention mechanism, a profound realization emerges: the essence of both is the input-dependent parameters. Consequently, essentially, Mamba can be viewed as a kind of attention mechanism. Notably, the attention mechanism has been extensively leveraged in the crack segmentation task, consistently enhancing the representation power to crack features and elevating identification performance of segmentation models [1][2][23][24][25]. This observation leads to the hypothesis that if the design of the Mamba block aligns with the principles governing attention blocks, it could potentially achieve consistent improvements to baseline segmentation models. Motivated by this notion, we introduce an attention block-like Mamba block, termed CrackMamba, elaborated upon in the following subsection."}, {"title": "2.3.2. Design of CrackMamba", "content": "In attention blocks, a pivotal element is the attention map (AM), which contains the relative importance information among different regions within the feature maps. Taking the three most representative attention blocks as an example, namely, SENet [29] (illustrated in Fig. 4 (a)), CBAM [30] (depicted in Fig. 4 (b)), and parallel attention module (PAM) [23] (shown in Fig. 4 (c)), their AMs are derived by normalizing the feature maps to a range of (0,1) through a Sigmoid function. Subsequently, another crucial feature of the attention blocks is that the outputs of these attention modules are obtained through element-wise multiplication of the original feature maps and the AMs. Therefore, the primary enhancement introduced by CrackMamba, in contrast to the conventional Mamba blocks, lies in the incorporation of AM. Besides, the outcome of CrackMamba also entails element-wise multiplication of the original feature map with the AM, as depicted in Fig. 4 (d).\nThe design of the specific structure of CrackMamba is twofold. First, considering that the vanilla VSS block stands out as the sole Mamba block that does not weaken model performance among the three 2D Mamba blocks (as discussed in Subsection 2.2), CrackMamba is constructed upon the foundation of the vanilla VSS block and also incorporates a DW Conv and an SS2D module. Second, drawing inspiration from the success of the parallel attention module (PAM) in the crack segmentation task [1] and its broader applications in the civil engineering domain such as bridge defect inspection [23], building structure identification [31], and structural health inspection [32], CrackMamba's design is influenced by the architecture of PAM. Specifically, in the domain of crack segmentation, the specific manifestation of PAM is the feature fusion module (FFM) within CrackSeU, with its core part illustrated in Fig. 4 (c). Hence, CrackMamba adopts a structure similar to that of FFM, featuring two branches and a skip connection. Besides, referring to the output composition of FFM, the ultimate output of CrackMamba entails the element-wise addition of the combination of the two branches and the skip connection, as shown in Fig. 4 (c) and Fig. 4 (d).\nThe quantitative assessments of crack identification in Section 5 unequivocally demonstrate that CrackMamba consistently enhances the baseline model's performance across both datasets, affirming the effectiveness of CrackMamba. Moreover, benefiting from the linear spatial and computational complexity of Mamba, CrackMamba-based CrackSeU-B has fewer parameters and reduced computational complexity compared to the original CrackSeU-B (as evidenced in Table 2 and Table 4). Therefore, as a simple yet effective and plug-and-play module, CrackMamba exhibits immense potential for widespread application in crack identification. Furthermore, the concept of integrating Mamba with the attention mechanism introduces a fresh paradigm for designing Mamba blocks, offering the potential to innovate existing Mamba blocks, thereby enhancing the overall performance of Mamba-based models across various computer vision tasks."}, {"title": "3. Crack segmentation network and crack datasets", "content": ""}, {"title": "3.1. Crack segmentation network", "content": ""}, {"title": "3.1.1. Crack Segmentation U-shape network (CrackSeU)", "content": "In order to assess the effectiveness of Mamba to crack identification, we choose a representative crack segmentation network from a recent publication in a top-tier journa, the Crack Segmentation U-shape network (CrackSeU) [1]. This choice is guided by two primary considerations.\nFirst, CrackSeU is built on the U-structure which is the most representative and the most successful structure in image segmentation networks due to its efficient feature fusion mechanism. In crack segmentation networks, the U-structure is a common presence [7][33][34], indicating that summarized patterns and insights from CrackSeU are universally applicable to a broad spectrum of crack segmentation networks. Second, the key innovation of CrackSeU lies in extending the two-level fusion paradigm of conventional U-shape networks like U-Net [35], U-Net++ [36], and Attention U-Net [37] to multi-level fusion. This multi-level fusion strategy enables more effective and thorough multi-scale integration of crack features and facilitates intra-network information flow, resulting in more superior crack segmentation performance compared to other U-shape networks [1]. In view of this, it is meaningful to investigate whether Mamba can further elevate the performance of this potent model. If Mamba proves successful in enhancing CrackSeU, it signifies the potential to deliver similar or even greater advancements in the performance of other ordinary U-shape segmentation networks.\nCrackSeU consists of five encoder blocks (Stage 1~Stage 5), four upsampling blocks (UBk, k = 2,3,4,5), three feature fusion modules (FFMk, k = 1,2,3), one side output block (SOB) and one output block (OB). Based on the difference in FFM's volume, CrackSeU has three configurations: CrackSeU-B, CrackSeU-M, and CrackSeU-L, denoting base, middle, and large volume models, respectively. Considering that CrackSeU-B achieves the best balance between performance and computational complexity, CrackSeU-B is chosen as the final research object. For simplicity, in this paper, CrackSeU refers to CrackSeU-B unless specified otherwise."}, {"title": "3.1.2```json\n{\n        \"title\": \"3.1.2. Integrate CrackSeU-B and Mamba\"", "content": "Next, a key challenge is how to integrate CrackSeU-B and Mamba. In this paper, we do not additionally design new fancy architectures for integration, instead, we simply substitute the original convolutional blocks in Stage 2~Stage 5 of CrackSeU-B with Mamba blocks, and the schematic diagram is shown in Fig. 5. The rationale behind this arrangement is that we want to investigate the potential and generalization of Mamba in crack identification. Clearly, if researchers conduct optimization on the structure of the integration module, the performance of the new model can be improved, however, it becomes challenging to discern whether the improvements come from Mamba or the well-crafted integration module. Moreover, a complicated module may weaken the integrated model's generalization ability, as the benefits of an elaborate structure could diminish with alterations in the training pipeline or the baseline model. Besides, it is important to highlight that the numbers of channels vary among the input features of each stage within the original CrackSeU-B. Nevertheless, the Mamba blocks are all plug-and-play modules, that preserve the shapes of input feature. To solve the contradiction, a point-wise convolution (PW Conv) is incorporated to align the output features of the Mamba blocks with the channel configurations of CrackSeU-B."}, {"title": "3.1.3. Loss function", "content": "To achieve fair comparisons, the loss functions adopted in this paper are consistent with the original loss functions training CrackSeU. The total loss function \\(L_{\\text{total}}\\) is a mixed loss function:\n\\[L_{\\text{total}} = \\alpha_1 \\times \\frac{1}{B} \\sum_{i=1}^{B} L_{\\text{BCE}}(P^i, G^i) + \\alpha_2 \\times \\frac{1}{B} \\sum_{i=1}^{B} L_{\\text{Dice}}(P_\\text{side}^i, G_\\text{side}^i) + \\alpha_3 \\times \\frac{1}{B} \\sum_{i=1}^{B} L_{\\text{CBCE}}(P^i, G^i), \\tag{14}\\]\nwhere \\(B\\) is the batch size. \\(\\alpha_1\\), \\(\\alpha_2\\) and \\(\\alpha_3\\) are three hyperparameters for regulating the impact of different sub-loss functions, and they are set to 1, 1 and 0.1, respectively. \\(P^i \\in (0,1)^{1\\times H \\times W}\\) and \\(G^i \\in \\{0,1\\}^{1\\times H \\times W}\\) denote the predicted probability of the \\(i\\)-th image in a mini-batch and the corresponding ground-truth label. \\(P_\\text{side}^i \\in (0,1)^{1\\times H/2 \\times W/2}\\) and \\(G_\\text{side}^i \\in [0,1]^{1\\times H/2 \\times W/2}\\) represent the \\(i\\)-th side output image and the corresponding human label. It is important to note that the image size of the side output is half of the final output, as shown in Fig. 5. \\(L_{\\text{BCE}}\\) and \\(L_{\\text{Dice}}\\) are two binary cross-entropy (BCE) loss functions and they can be formulated as follows:\n\\[L_{\\text{BCE}} = - \\frac{1}{HW} \\sum_{(h,w)} [G(h, w) \\times \\log(P(h, w)) + (1 - G(h, w)) \\times \\log(1 - P(h, w))], \\tag{15}\\]\nand\n\\[L_{\\text{BCE}}^{\\text{side}} = - \\frac{1}{HW} \\sum_{(h',w')} [G_\\text{side}(h', w') \\times \\log(P_\\text{side}(h', w')) + (1 - G_\\text{side}(h', w')) \\times \\log(1 - P_\\text{side}(h', w'))], \\tag{16}\\]\nwhere \\(h \\in [1, H]\\), \\(w \\in [1, W]\\), \\(h' \\in [1, H/2]\\), and \\(w' \\in [1, W/2]\\). \\(L_{\\text{Dice}}\\) is a Dice loss function, which is effective to alleviate class-imbalance problems. Considering that cracks are always subtle, thus, crack/non-crack pixels are of a highly imbalanced distribution, the Dice loss function is introduced and can be defined as\n\\[L_{\\text{Dice}} = 1 - \\frac{2\\sum_{(h,w)} [P(h, w) \\times G(h, w)] + \\epsilon}{\\sum_{(h,w)} [P(h, w)] + \\sum_{(h,w)} [G(h, w)] + \\epsilon}, \\tag{17}\\]\nwhere \\(\\epsilon\\) is a Laplace smoothing item to avoid zero division and is set to \\(1 \\times 10^{-4}\\)."}, {"title": "3.2. Crack datasets", "content": "The academic community has open-sourced numerous influential crack datasets, and these datasets usually can be categorized into two groups based on the crack type: asphalt or concrete pavement crack datasets and steel structure crack datasets. The former category accounts for the vast majority of the proportion due to its accessibility and includes CrackTree206 [38], CFD [39], CRACK500 [40], and Deepcrack [41]. In contrast, the latter category currently comprises only one publicly available dataset, namely Steelcrack [2].\nTo comprehensively evaluate the impact of Mamba on crack identification accuracy, we select a representative dataset from each category. The first dataset chosen is Deepcrack, which comprises 537 crack images, along with the corresponding pixel-level labels. The crack images are captured in multiple pavement scenes, which can be divided into two categories: asphalt pavements and concrete pavements. The training set of Deepcrack consists of 300 pairs of crack images and labels, while the remaining 237 pairs of images and labels form the test set. The pixel resolutions of the images are 544 \u00d7 384 or 384 \u00d7 544. Since the number of images in the original training set of Deepcrack is so limited, we conduct the same data augmentation strategies for the training set as those in [1]. Initially, all images are horizontally and vertically flipped, as well as rotated by 90\u00b0, 180\u00b0, and 270\u00b0. Subsequently, the image resolutions are resized to 480 \u00d7 480 to account for changes in aspect ratio due to rotation transformation. A random cropping strategy is then implemented, with a fixed crop size of 384 \u00d7 384. The details of the augmented Deepcrack dataset are outlined in Table 1. The second dataset selected is Steelcrack, where all the crack images are directly captured from various engineering projects of steel structures, such as Humen Bridge and Nanjing Second Yangtze River Bridge. Steelcrack also provides the corresponding meticulous manual labels for all images. All the images and labels in Steelcrack maintain a consistent pixel resolution of 512 \u00d7 512. Steelcrack consists of 3300 training images, 525 images for validation, and 530 images in the test set. As the original Steelcrack dataset has already undergone data augmentation, we do not conduct further data augmentation anymore. The specifics of the Steelcrack dataset are presented in Table 1 below."}, {"title": "4. Details of implementation", "content": ""}, {"title": "4.1. Training and inference environment", "content": "The hardware used for training and inference includes an Intel Xeon E5-2697 @ 2.70 GHz CPU, 3 \u00d7 Nvidia GeForce RTX 2080Ti GPUs, and RAM is 256 GB. The main software environment: the Ubuntu 18.04 operating system, CUDA 11.6.2, CUDNN 8.6.0, and Python 3.8.15. The deep learning framework employs version 1.12.1 of PyTorch developed by Meta.\nThe package versions associated with Mamba are as follows: the version of causal-conv1d is 1.1.1. The Mamba utilized in this study is a hybrid of versions 1.2.0 and 1.1.1 sourced from Vim*. Due to installation bugs frequently encountered, we find this relatively complex installation pathway after multiple attempts. In our openly accessible code repository (https://github.com/hzlbbfrog/CrackMamba), we have provided a detailed tutorial about the installation process to bolster credibility and reproducibility."}, {"title": "4.2. Training policy", "content": "For the training policies of the two datasets, we follow the settings in the paper [1] and [2], respectively. Specifically, the training policy of Deepcrack follows [1], where the optimizer is the Adam optimizer, and the hyper-parameters are kept to the default values provided by PyTorch, i.e., \u03b2 = (0.9,0.999), \u03b5 = 10-8 and weight decay = 0. The learning rate is fixed at 9 \u00d7 10-4. The networks are all trained for 80 epochs before inference, and the batch size is set to 12. The Steelcrack's training policy follows the configuration in [2], where the optimizer is still the Adam optimizer, and the hyper-parameters are not changed. The learning rate is 6 \u00d7 10-3, and the batch size is set to 9. We determine the best model by training the networks for 70 epochs.\nIt is worth noting that for fair and standardized comparisons and replicability, for the same dataset, the training pipeline across all the models are maintained consistently."}, {"title": "4.3. Evaluation metrics", "content": "To comprehensively and thoroughly validate and assess the performance of the models, two types of metrics are employed following [2], namely, effect and efficiency metrics, as summarized below."}, {"title": "4.3.1. Effect metrics", "content": "The effect metrics encompass two key indicators: the mean image-wise intersection over union (mi IoU) and the mean image-wise Dice coefficient (mi Dice). Mi IoU is a discrete measure that emphasizes pixel-level similarity, while mi Dice serves as a continuous evaluation metric and is inclined to explore image-level performance [9]. Both metrics exhibit a positive correlation with model performance, with higher values indicating a closer resemblance between the prediction and the ground truth label.\nSpecifically, mi IoU is defined by the following formula:\n\\[\\text{mi IoU} = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\text{TP}_n + \\epsilon}{\\text{FN}_n + \\text{FP}_n + \\text{TP}_n + \\epsilon} \\times 100\\%, \\tag{18}\\]\nwhere N denotes the number of images to be evaluated. For the n-th image, \\(TP_n\\) represents the number of true positive pixels in the image, \\(FN_n\\) denotes the number of false negative pixels, and \\(FP_n\\) is the number of false positive pixels. \\(\\epsilon\\) is a constant with a small value to prevent the denominator from being zero and is set to 10-6. The other effect measure, mi Dice, can be obtained by the following equation:\n\\[\\text{mi Dice} = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{2|P_n \\cap G_n| + \\epsilon}{|P_n| + |G_n| + \\epsilon} \\times 100\\%, \\tag{19}\\]\nwhere, \\(P_n \\in (0,1)^{1\\times H \\times W}\\) denotes the n-th prediction, and \\(G_n \\in \\{0,1\\}^{1\\times H \\times W}\\) is the corresponding n-th ground-truth."}, {"title": "4.3.2. Efficiency metrics", "content": "Two efficiency metrics are employed to assess the computational complexity of models. The first indicator counts the number of parameters in a model and is denoted as #Param. The second index is multiply-accumulate operations (MACs), which is utilized to evaluate the computational expenses of models. To ensure consistency with prior studies in [1][2], we use the open-sourced tool Ptflops$ to compute the two metrics. It is important to note that both #Param. and MACs are inversely correlated with model efficiency; hence, lower #Param. or MACs indicate higher computational efficiency."}, {"title": "5. Experiments", "content": "Extensive experiments have been conducted to verify the effectiveness of the proposed method. Specifically, detailed quantitative comparisons are presented in Subsections 5.1, and Subsections 5.2 provides visualization results of the effective receptive field (ERF) of the models."}, {"title": "5.1. Quantitative evaluation", "content": "As introduced in Section 2, we have chosen four visual Mamba blocks: the Vim block, the vanilla VSS block, the VSS block, and the proposed CrackMamba block to investigate their influence on crack feature comprehension. Subsequently, we substitute the convolutional blocks in the four stages of CrackSeU-B with these Mamba blocks individually and proceed to train the new Mamba-based models on the Deepcrack and Steelcrack datasets (refer to Section 3).\nThe quantitative evaluation of the results obtained from the four Mamba-based CrackSeU-B models and the baseline model (namely, CrackSeU-B) on the Deepcrack dataset is presented in Table 2. From the table, several key conclusions can be drawn: (1) CrackMamba achieves the best segmentation outcomes compared to the other three Mamba blocks and the baseline model; and (2) All Mamba blocks contribute to a reduction in parameters (i.e., the #Param.) and computational complexity (i.e., the MACs) of the baseline model, attributed to the superior efficiency of Mamba. Additionally, Table 3 provides a comparison of results between various representative segmentation networks and CrackSeU-B + CrackMamba. The comparison reveals that CrackSeU-B + CrackMamba attains the SOTA performance across all effect measures with minimal parameter quantity and computational load. Subsequently, Table 4 showcases the quantitative evaluation results of the Mamba-based CrackSeU-B models and the baseline model on the Steelcrack dataset. By combining the results from Table 2 and Table 4, it becomes evident that CrackMamba stands out as the sole Mamba block consistently delivering remarkable positive enhancements to the baseline model. Furthermore, Table 5 displays the evaluation outcomes of prominent segmentation frameworks alongside the designed CrackSeU-B + CrackMamba model. The performance comparison demonstrates that CrackSeU-B + CrackMamba ranks second only to the latest BGCrack, with marginally lower mi IoU and mi Dice values compared to BGCrack (the mi IoU and the mi Dice of CrackSeU-B + CrackMamba are 0.90 and 0.45 less than those of BGCrack, respectively). Notably, CrackSeU-B + CrackMamba surpasses all other models across all effect metrics. It is crucial to emphasize that as presented in Table 5, in terms of efficiency metrics, CrackSeU-B + CrackMamba uses the fewest parameters (79% of BGCrack) and the lowest computational demands (63% of BGCrack). Therefore, CrackSeU-B + CrackMamba actually emerges as a synthetic optimal model."}, {"title": "5.2. Visual explainability", "content": "As delineated in Section 2, the fundamental role of Mamba is to model global dependency relationships just like Transformer, and Section 2 elaborates on why Mamba can achieve global receptive fields through theoretical calculations. This subsection delves into the global modeling ability of Mamba from the perspective of visual interpretability. Specifically, this study employs the visualization tool proposed in RepLKNet [47] to visualize the effective receptive fields (ERFs) of feature maps output from Stage 2 to Stage 5. The visualized outcomes are presented in Fig. 6 and Fig. 7. It is crucial to emphasize that in a visualization image of ERF, the value assigned to each pixel denotes the contribution of the unit (occupying the same spatial position as the pixel in the visualization image) from the input image to the central point in the output feature map [47]. In essence, a higher value indicates a more substantial contribution of the unit to the central point. In other words, the higher the value, the stronger the perception of the central point in the output feature map to the spatial position of the input image. Further details on ERFs and their visualization images are expounded in Appendix E. Unlike Ding et al. [47], who randomly selected 50 images from ImageNet to obtain the visualization images of ERFs, we utilize all 3300 images in the training set of Steelcrack to yield more precise visualization outcomes.\nThe visualization outcomes of the original CrackSeU-B are depicted in Fig. 6. From the visualization results, it is evident that CrackSeU-B can enlarge the ERFs of feature maps through training. However, post-training, the ERFs remain confined and fail to encompass a global scope. In Fig. 7, the visualized ERFs of the feature maps coming from the four stages in CrackSeU-B + CrackMamba are presented. Combining Fig. 6 and Fig. 7, a notable observation emerges: while the ERFs of CrackSeU-B + CrackMamba are initially smaller than those of CrackSeU-B before training, the former undergo significant expansion during training, ultimately achieving a global perception. This empirical evidence provides a factual basis for the global modelling capability of Mamba.\nMoreover, the visualization images of ERFs post-training in Fig. 7 reveals that the pixels within the cross-shaped region surrounding the central area exhibit higher values compared to other regions. This observation indicates that the central points of the output feature maps possess a stronger perception towards the cross-shaped areas. This empirical finding aligns with theoretical analysis, which is illustrated below. The pivotal module within CrackMamba is the SS2D module, which includes four scanning routes, and the output of SS2D entails the addition of results from these four routes, as depicted in Fig. 8 (a). Consequently, the central block in Fig. 8 (a) can assimilate information from all other blocks simultaneously, which bring the central block global modeling capability. Additionally, the state space equations presented in Eqs. (3a) and (3a) are the recurrent form, where the essence lies in compressing sequence information into the latent state h. Fig. 8 (b) restates the state space equations, revealing that for an output yk, newer inputs (such as xk and xk\u22121) undergo lower compression rates, while older inputs (like x0 and x1) experience higher compression rates. As a result, newer inputs exert a more substantial contribution or impact on the output. In other words, in the visualization image of ERF, the values of the pixels sharing the same spatial position as the newer inputs (namely, their indexes closer to the output index) are larger. In Fig. 8 (a), we have highlighted the blocks closer to the central blocks in blue across the four routes. Combining the four routes, we can see that the closer blocks form a cross-shaped areas, therefore, the cross-shaped areas have larger values in the visualization image of ERF (refer to Fig. 7). Notably, for the output yk, the input with the same index, xk, holds the greatest impact to it, thereby attributing the central area with the highest value in a visualization image of ERF. This conclusion is also consistent with the observation from the visualization results depicted in Fig. 7.\nIn conclusion, the visualization results align seamlessly with the theoretical analysis."}, {"title": "6. Conclusions", "content": "Cracks serve as pivotal indicators of deterioration of structural performance, making the enhancement of crack segmentation precision a significant research topic. CNNs and Transformers have always been the mainstream architectures in crack segmentation networks. However, CNNs lack essential global modelling ability required for effectively capturing crack features. On the other hand, Transformers can capture long-range dependencies but come with high computational complexity and significant GPU memory requirements. Recently, the emergence of a novel architecture, Mamba, with its linear complexity and powerful global perception, has attracted considerable attention and demonstrated success across multiple CV tasks. However, its capability in learning visual representations of crack data remains unexplored. Thus, this study takes the lead to the investigation of integrating Mamba into the crack segmentation task. The principal findings and conclusions of this study are outlined below:\nFirst, this study offers a comprehensive exposition on Mamba and vision Mamba, including Vim and VMamba as the most representative visual Mamba modules, while delving into technical details typically omitted in existing Mamba literature, aiming to facilitate understanding within the civil engineering community. Moreover, this paper introduces Vim and VMamba into the domain of crack segmentation. Specifically, these visual Mamba modules are integrated into a prominent crack segmentation model, CrackSeU, and evaluated on two diverse crack datasets, consisting of the crack images from different infrastructure domains. Surprisingly, the experimental outcomes reveal that Vim and VMamba fail to consistently enhance and, in some cases, even diminish the baseline model's performance. Subsequently, we provide a deeper insight, an attention perspective, to interpret Mamba and then design a novel Mamba module following the principles of the attention mechanism, CrackMamba. The quantitative evaluations demonstrate that CrackMamba consistently and effectively enhances the performance of the baseline model while concurrently reducing parameters and computational overhead. Serving as a plug-and-play and simple yet effective Mamba module, CrackMamba showcases enormous potential for integration into various crack segmentation models. Furthermore, the innovative Mamba design concept of combining Mamba and the attention mechanism offers significant reference value for all Mamba-based CV models, extending beyond the scope of crack segmentation networks. Last, this paper demonstrates that Mamba significantly enlarges the receptive fields of baseline models and can achieve global perception, from both the perspectives of theoretical analysis and visual explainability.\nStill, further studies are recommended in the following areas. First, it is noteworthy that the Mamba-based networks in this study represent a straightforward integration of Mamba blocks with CrackSeU. Hence, researchers are encouraged to delve into further optimization of Mamba blocks and the integration methodology with crack segmentation networks to elevate performance levels. Second, this study highlights the advantageous impact of Mamba on the crack identification task. We hope this research can inspire researchers to explore more possibilities regarding the integration of Mamba within civil engineering, for example, the identification of other structural defects like spalling [23], rebar exposure, and efflorescence [48], as well as the analysis of time sequence data, such as the analysis of seismic response [49] or wind-induced vibration [50], considering that Mamba is initially proposed to sequence modeling tasks."}, {"title": "Appendix A Discretization method of ordinary differential equations", "content": ""}, {"title": "A.1 Euler discretization method", "content": "Let's restate the continuous state space equations Eqs. (2a) and (2b) for convenience\n\\dot{h}(t) = Ah(t) + Bx(t), \\tag{Ala}\ny(t) = Ch(t), \\tag{A1b}\nFirst, we introduce the most straightforward discretization method, the differential discretization method, also named the Euler discretization method, to convey the fundamental concept of discretizing ordinary differential equations (ODEs).\nThe differential discretization method. Based on the definition of derivative, we can first obtain\n\\dot{h}(t) = \\lim_{\\Delta \\to 0} \\frac{h(t + \\Delta) - h(t)}{\\Delta}, \\tag{A2}\nwhere \\(\\Delta \\in \\mathbb{R}^+\\) denotes the step size. Next, we let \\(t_k = t\\) and \\(t_{k+1} = t + \\Delta\\). The above equation can be transformed into\n\\dot{h}(t_k) = \\lim_{\\Delta \\to 0} \\frac{h(t_{k+1}) - h(t_k)}{\\Delta}. \\tag{A3}\nThen, we can get an approximate expression of \\(\\dot{h}(t_k)\\):\n\\dot{h}(t_k) \\approx \\frac{h(t_{k+1}) - h(t_k)}{\\Delta}. \\tag{A4}\nPlug the above formulation into Eq. (Ala):"}, {"title": "Change its form:", "content": "\\frac{h(t_{k+1}) - h(t_k)}{\\Delta} = Ah(t_k) + Bx(t_k). \\tag{A5}\nh(t_{k+1}) - h(t_k) = \\Delta Ah(t_k) + \\Delta Bx(t_k). \\tag{A6}\nSubsequently, we can obtain the representation of \\(h(t_{k+1})\\):\nh(t_{k+1}) = (\\Delta A + I)h(t_k) + \\Delta Bx(t_k), \\tag{A7}\nwhere I represents the identity matrix. We utilize \\(h_k\\) to denoted \\(h(t_k)\\) and \\(x_k\\) to represent \\(x(t_k)\\), then the above equation can be changed into\nh_{k+1} = (\\Delta A + I)h_k + \\Delta Bx_k.\\\nTherefore, recalling Eqs. (3a) and (3b), we have \\(\\hat{A} = \\Delta A + I\\), \\(\\hat{B} = \\Delta B\\), and \\(\\hat{C} = C\\). \\tag{A8}\nThe Euler discretization method is straightforward enough; however, the drawback is that its error is relatively large. Therefore, S4 models utilize the bilinear discretization method."}, {"title": "A.2 Bilinear discretization method", "content": "The bilinear discretization method. Unlike the differential discretization method, which is built on the derivative, this method starts with integration. First, for a function \\(h(t)\\), its integration within an interval \\([t_k, t_{k+1}]\\) can be approximatively formulated as\n\\int_{t_k}^{t_{k+1}} h(t) dt \\approx \\frac{1}{2} [h(t_k) + h(t_{k+1})](t_{k+1} - t_k). \\tag{A9}\nThen, we can substitute h(t) with \\(\\dot{h}(t)\\):\n\\int_{t_k}^{t_{k+1}} \\dot{h}(t) dt \\approx \\frac{1}{2} [\\dot{h}(t_k) + \\dot{h}(t_{k+1})](t_{k+1} - t_k). \\tag{A10}\nWhen \\(t_{k+1}\\) close to \\(t_k\\), we can think\n\\int_{t_k}^{t_{k+1}} \\dot{h}(t) dt \\approx \\frac{1}{2} [\\dot{h}(t_k) + \\dot{h}(t_{k+1})](t_{k+1} - t_k). \\tag{A11}\nAdditionally, we know\n\\int_{t_k}^{t_{k+1}} \\dot{h}(t) dt = h(t) \\vert_{t_k}^{t_{k+1}} = h(t_{k+1}) - h(t_k). \\tag{A12}\nCombining the above two equations:\n\\frac{1}{2} [\\dot{h}(t_k) + \\dot{h}(t_{k+1})](t_{k+1}-t_k) = h(t_{k+1}) - h(t_k). \\tag{A13}\nBased on Eq. (Ala), we can obtain\n\\dot{h}(t_k) = Ah(t_k) + Bx(t_k), \\tag{A14a}\n\\dot{h}(t_{k+1}) = Ah(t_{k+1}) + Bx(t_{k+1}). \\tag{A14b}\nEmbed Eqs. (A14a) and (A14b) into Eq. (A13):\n\\frac{1}{2} [Ah(t_k) + Bx(t_k) + Ah(t_{k+1}) + Bx(t_{k+1})](t_{k+1} - t_k) = h(t_{k+1}) \u2013 h(t_k). \\tag{A15}\nLet \\(\\Delta= t_{k+1} - t_k\\), then\nh(t_{k+1}) - h(t_k) - \\frac{\\Delta}{2} Ah(t_k) + Bx(t_k) + Ah(t_{k+1}) + Bx(t_{k+1}). \\tag{A16}\nNamely,\n(I-\\frac{\\Delta}{2}A) h(t_{k+1}) = (I+\\frac{\\Delta}{2}A) h(t_k) + \\frac{\\Delta}{2}Bx(t_k) + \\frac{\\Delta}{2}Bx(t_{k+1}). \\tag{A17}\nWhen A is small enough, we can think \\(x(t_k) \\approx x(t_{k+1})\\). Thus, the above equation can be simplified as"}, {"title": "Change its form:", "content": "(I-\\frac{\\Delta}{2}A) h(t_{k+1}) = (I+\\frac{\\Delta}{2}A) h(t_k) + \\Delta Bx(t_k). \\tag{A18}\n\\hat{h}_{k+1} = (I-\\frac{\\Delta}{2}A)^{-1} (I+\\frac{\\Delta}{2}A) h_k + (I-\\frac{\\Delta}{2}A)^{-1} Bx(t_k). \\tag{A19}\nWe utilize \\(h_k\\) to denoted \\(h(t_k)\\) and \\(x_k\\) to represent \\(x(t_k)\\), then the above equation can be changed into\n\\hat{h}_{k+1} = (I-\\frac{\\Delta}{2}A)^{-1} (I+\\frac{\\Delta}{2}A) h_k + (I-\\frac{\\Delta}{2}A)^{-1} Bx(t_k). \\tag{A20}\nTherefore, recalling Eqs. (3a) and (3b), we have \\(\\hat{A} = (I \u2013 \\Delta/2 \\cdot A)^{-1}(I + \\Delta/2 \\cdot A)\\), \\(\\hat{B} = (I \u2212 \\Delta/2 \\cdot A)^{-1}\\Delta B\\), and \\(\\hat{C} = C\\)."}, {"title": "A.3 Zero-order hold (ZOH) discretization method", "content": "Before introducing the derivation, a key theorem is proved first.\nLemma 1. For A \u2208 \\mathbb{R}^{N\\times N}, and t is a scalar, we have\nAe^{At} = e^{At}A.\nProof. First, let us review the Taylor series expansion:\ne^{at} = 1 + \\frac{at}{1!} + \\frac{(at)^2}{2!} + \\frac{(at)^3}{3!} +\u2026, \\tag{A21}\nwhere a is a constant scalar. Then, if the coefficient of t is a constant matrix, the above equation is changed to\ne^{At} = I + \\frac{At}{1!} + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} +\u2026, \\tag{A22}\nwhere A \u2208 \\mathbb{R}^{N\\times N}, and I \u2208 {0,1}^{N\\times N} is an identity matrix.\nTherefore,\nAe^{At} = A (I + \\frac{At}{1!} + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} +\u2026)\n= AI + \\frac{A^2t}{1!} + \\frac{A^3t^2}{2!} + \\frac{A^4t^3}{3!} +\u2026, \\tag{A23}\nand\ne^{At}A = (I + \\frac{At}{1!} + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} +\u2026) A\n= IA + \\frac{A^2t}{1!} + \\frac{A^3t^2}{2!} + \\frac{A^4t^3}{3!} +\u2026, \\tag{A24}\nBecause I is an identity matrix, AI = IA. Combining Eq. (A23) and Eq. (A24), we have Ae^{At} = e^{At}A.\nTheorem 1. The solution of the ODE formulated by Eq. (Ala) is\nh(t) = e^{A(t-t_0)}h(t_0) + \\int_{t_0}^{t} e^{A(t-\\tau)} Bx (\\tau)d\\tau. \\tag{A25}\nProof. It is important to note that the proof of Theorem 1 can be found in modern control theory or linear system textbooks. Frist, let us multiply both sides of Eq. (Ala) by \\(e^{-At}\\):\ne^{-At}\\dot{h}(t) = e^{-At} Ah(t) + e^{-At}Bx(t), \\tag{A26}\ne^{-At}\\dot{h}(t) + e^{-At}(-A)h(t) = e^{-At}Bx(t),\nThen,\nRecalling Lemma 1, we have \\(e^{-At}(-A) = (-A)e^{-At}\\). Thus,"}, {"title": "We have known", "content": "e^{-At"}, "dot{h}(t) + (-A)e^{-At}h(t) = e^{-At}Bx(t), \\tag{A27}\n\\frac{d(e^{-At}h(t))}{dt} = e^{-At}\\dot{h}(t) + \\frac{d(e^{-At})}{dt} h(t)\n=e^{-At}\\dot{h}(t) + (-A)e^{-At}h(t). \\tag{A28}\nPlug Eq. (A28) into Eq. (A27):\n\\frac{d(e^{-At}h(t))}{dt} =e^{-At} Bx (t). \\tag{A29}\nTherefore,\n\\int_{t_0}^{t} \\frac{d(e^{-A\\tau}h(\\tau))}{d\\tau} dr = \\int_{t_0}^{t} e^{-A\\tau}Bx(\\tau) dr. \\tag{A30}\nFurther,\ne^{-A\\tau}h(\\tau) \\vert_{t_0}^{t} = e^{-At}h(t) - e^{-At_0} h(t_0)\n\\int_{t_0}^{t} e^{-A\\tau}Bx(\\tau) dr. \\tag{A31}\nTransposition of terms:\ne^{-At}h(t) = e^{-At_0} h(t_0) + \\int_{t_0}^{t} e"]}