{"title": "Your Turn: Real-World Turning Angle Estimation for Parkinson's Disease Severity Assessment", "authors": ["Qiushuo Cheng", "Catherine Morgan", "Arindam Sikdar", "Alessandro Masullo", "Alan Whone", "Majid Mirmehdi"], "abstract": "People with Parkinson's Disease (PD) often experience progressively worsening gait, including changes in how they turn around, as the disease progresses. Existing clinical rating tools are not capable of capturing hour-by-hour variations of PD symptoms, as they are confined to brief assessments within clinic settings, leaving gait performance outside these controlled environments unaccounted for. Measuring real-world gait turning angles continuously and passively is a component step towards using gait characteristics as sensitive indicators of disease progression in PD. This paper presents a deep learning-based approach to automatically quantify turning angles by extracting 3D skeletons from videos and calculating the rotation of hip and knee joints. We utilise state-of-the-art human pose estimation models, Fastpose and Strided Transformer, on a total of 1386 turning video clips from 24 subjects (12 people with PD and 12 healthy control volunteers), trimmed from a PD dataset of unscripted free-living videos in a home-like setting (Turn-REMAP). We also curate a turning video dataset, Turn-H3.6M, from the public Human3.6M human pose benchmark with 3D ground truth, to further validate our method. Previous gait research has primarily taken place in clinics or laboratories evaluating scripted gait outcomes, but this work focuses on real-world settings where complexities exist, such as baggy clothing and poor lighting. Due to difficulties in obtaining accurate ground truth data in a free-living setting, we quantise the angle into the nearest bin 45\u00b0 based on the manual labelling of expert clinicians. Our method achieves a turning calculation accuracy of 41.6%, a Mean Absolute Error (MAE) of 34.7\u00b0, and a weighted precision (WPrec) of 68.3% for Turn-REMAP. On Turn-H3.6M, it achieves an accuracy of 73.5%, an MAE of 18.5\u00b0, and a WPrec of 86.2%. This is the first work to explore the use of single monocular camera data to quantify turns by PD patients in a home setting. All data and models are publicly available, providing a baseline for turning parameter measurement to promote future PD gait research.", "sections": [{"title": "Introduction", "content": "Parkinson's disease (PD) is a progressive neurodegenerative movement disorder, characterised by symp-toms such as slowness of movement and gait dysfunction [1] which fluctuate across the day but progress slowly over the years [2]. Currently, treatment of PD relies on therapies which improve symptoms. There are no treatments available which modify the course of the underlying disease (so-called disease-modifying treatments, or DMTs), despite there being multiple putative DMTs showing promise in laboratory studies [3]. One reason for the slow development of DMTs is the dearth of sensitive, frequent, objective biomarkers to enhance the current gold-standard clinical rating scale [4] to measure the progression of PD. This gold-standard clinical rating scale, the Movement Disorders Society-sponsored revision of the Unified Parkinson's Disease Rating Scale (MDS-UPDRS) [4], includes subjective questionnaires concerning gait and mobility ex-periences, along with clinicians' ratings of scripted activities performed by the participants. The assessments typically occur within clinical settings over short durations, offering only a \"snapshot\" of symptoms which vary on an hourly basis. It also has limitations including its non-linear and discontinuous scoring system, the inter-rater variability [5] and Hawthorne effect [6] of being observed on how someone mobilises [7, 8]. Gait and turning abnormalities are common features of PD with over half of the patients reporting difficulties with turning [9] \u2013 when someone moves round on their axis while upright, changing the direction they face. Turning changes associated with PD include the 'en bloc' phenomenon where upper and lower body segments turn simultaneously [10], a longer duration of turn, less accurate turn completion, a narrower base of support [11] and the use of 'step turns' rather than 'pivot turns' [12]. More than 40% of daily steps are during turning [13] and turning abnormalities can predispose to falls, thus turning parameters could potentially be used as measures predicting the time to falls in a patient with PD [9]. Furthermore, if a fall happens during turning, it is up to 8 times more likely to result in a hip fracture [14]. In unmedicated early-stage PD, gait parameters from turning are more sensitive to change compared to straight-ahead gait outcomes [15], making measuring aspects of turns potentially of specific use in clinical trials of disease-modifying interventions which typically recruit recently diagnosed patients [16]. People with PD turn differently when being watched by a clinician [17], so measuring turning passively in a real-world setting could give information about mobility not captured by face-to-face assessments in the clinic.\nBeing able to measure the angle of turn therefore could be very helpful in PD assessment, for use in clinical trials and clinical practice. Turning angle alone provides useful insight into the progression of the disease: people with PD take larger angles of turn when they are taking medications, compared to when they withhold their symptom-improving therapies [19]. Turning in gait also comprises other potential measurable elements including foot strike angle, arm swing and turn speed. Calculating the changes of angle over time could help to analyse and interpret these metrics of turning. Previous work shows that the number of turning steps and the turn duration, from unplanned and pre-planned turns, can distinguish between PD medication states (whether someone takes or withholds medication) [17]. Turning speed can be used to differentiate between healthy control and PD participants [20]. These turning parameters correlate strongly with the"}, {"title": "Literature review", "content": "In this section, we consider related literature in the two most pertinent aspects of our work, i.e. sensor-based turning angle estimation and human pose estimation in gait analysis\nTurning angle estimation \u2013 To acquire objective quantitative turning parameters for human motion analysis, inertial sensors which consist of gyroscopes and accelerometers [25, 26, 20, 27, 22, 28, 29, 30, 31] and floor pressure sensors [32, 33, 34, 23] have been well-explored over the years. Many algorithms using inertial sensors placed on shoes or belts have been validated with gold-standard motion capture systems or human raters with reported accuracy on a sub-degree level [25, 26, 22] but limited to a laboratory environment on the scripted turning course with few predefined turns. Additionally, even though sensors give nearly ground truth readings under these restricted conditions, they require digital devices to be worn on the body of the participant which raises issues of acceptability [35] and usability [36]. The portable wearable sensors for gait evaluation are power-thirsty and have limited memory storage space, and therefore there are significant burdens for both participants and professionals to replace, recharge, re-configure and transfer data manually. This hinders the generalisation ability of their proposed methods to different patient cohorts, especially in the free-living environment where it is hard to control every relevant factor, like the imperfect use and configuration of wearables. It has been shown that sensor-based algorithms evaluating gait translate poorly from laboratory to home [37]. Furthermore, several papers have demonstrated that people mobilise differently in the laboratory compared to home settings [7, 27, 8, 29].\nAnother inherent limitation of wearable-based methods is that they can only provide kinematics param-eters on a few body parts, rather than a holistic view of the position and orientation of the entire body. Furthermore, it is shown in [31] that placing wearables on different locations of the body (head, neck, lower"}, {"title": "Datasets", "content": "Our proposed turning angle measurement approach is evaluated on the turning scenes of the recently released free-living dataset, REMAP [18], and a curated dataset extracted from the public pose estimation benchmark Human3.6M [24]. In this section, we discuss the details of the video data and how our annotations enable quantitative evaluation of our method.\nTurn-REMAP \u2013 REMAP [18] includes PD and healthy participants engaging in actions, such as sit-to-stand transitions or walking turns within a home environment. These specific actions were recorded during free-living, undirected situations, as well as formal clinical evaluations. We present Turn-REMAP, a subset of this data comprising all its turning actions, loosely-scripted and spontaneous . The video data is collected using Microsoft Kinect wall-mounted cameras installed on the ground floor (communal areas) of a test-bed house [64] which captured red-green-blue (RGB) and depth data 2-3 hours daily (during daylight hours at times when participants were at home). The acceptability of using such high-resolution video recordings for validation purposes in home settings in PD has been studied in [65, 66]. The dataset contains 12 spousal/parent-child/friend-friend pairs (24 participants in total) living freely in this sensor-embedded smart home for five days at a time. Each pair consists of one person with PD and one person who was a healthy control volunteer (HC). This pairing was chosen to enable PD vs HC comparison, for safety reasons and also to increase the naturalistic social behaviour (particularly amongst the spousal pairs who already lived together). Of the 24 participants, five females and seven males have PD. The average age of the participants is 60.25 (PD 61.25, Control 59.25) and the average time since PD diagnosis for the person with PD is 11.3 years (range 0.5-19).\nThe RGB videos were watched post-hoc by medical doctors who had undertaken training in the MDS-UPDRS rating score, including gait parameter evaluation. Two clinicians watched up to 4 simultaneously captured video files at a time using ELAN software [67] to manually annotate the videos to the nearest"}, {"title": "Methodology", "content": "In this section, we provide a detailed description of our proposed framework. Our overall pipeline has two major processes (Figure 4): 3D human joints estimation and turning angle calculation.\nFollowing monocular video as input, we apply Fast Pose (FP) [72] to estimate joint locations and lift the 2D skeleton series into 3D using the Strided Transformer (ST) [73]. With 3D skeletons, we compute the turning angle as the rotation of joints after being projected onto the horizontal plane. A continuous value of the angle is determined and further quantised into the nearest 45\u00b0 bin, which is then compared to the clinician's annotation.\n3D human joints estimation - Our approach comprises a two-stage framework where we first detect 2D human joint locations in each frame of the video sequence and then reconstruct them in 3D space based on the spatial-temporal knowledge extracted from the temporal 2D skeletons series using a deep learning model. Another way of estimating 3D human pose from videos is to use a single deep learning model to infer the 3D coordinates from the RGB pixels directly in an end-to-end manner [74, 75]. However, a more loosely coupled pipeline is chosen over end-to-end frameworks as it has been shown to achieve higher accuracy with significantly lower computational cost on almost all of the benchmarks for human pose estimation [76, 73, 77].\nTo detect the 2D body joints in each video frame, we apply FastPose [72] as the 2D keypoints detector. The keypoints detector maps input video frames $V \\in R^{T*W*H*3}$, into frames of 2D keypoint coordinates $K \\in R^{T*J*2}$. T is the number of frames of the video, W and H are the width and height of each frame and J = 17 is the number of joints (keypoints) in our skeleton, following the skeleton model from Human3.6M [24].\nFastPose uses a top-down framework, which detects the human object from the frames and estimates the joint coordinates in the form of a heatmap within a bounding box. The model utilises the classical ResNet [55] as the image feature extraction backbone, and then uses upsampling modules [78] and 1D convolution to generate heatmaps to represent the probability of each pixel being a human joint. FastPose outputs a"}, {"title": "Experiments", "content": "Implementation and Evaluation \u2013 The experiment is conducted in PyTorch on one single NVIDIA 4060Ti GPU and a 12-core AMD Ryzen 5 5500 CPU. In our pipeline, the utilised FastPose model is trained on the MSCOCO pose estimation dataset [81] and the Strided Transformer [73] is trained on Human3.6M [24], following the standard set-up of related literature [76, 77]. These models are not optimised or fine-tuned to our free-living videos.\nWe evaluate our proposed method via three key metrics: accuracy, Mean Absolute Error (MAE) in degrees, and weighted precision (WPrec). Accuracy assesses the proportion of predicted angles that correctly fall into their respective bins, showing the categorical correctness of our predictions. MAE is calculated as the average of the absolute difference between the predicted values for angles, as well as speed in Turn-H3.6M, against ground truth. WPrec measures the percentage of true positive predictions for all positive predictions across angle bins, weighted by each bin's sample size [82]. For example, if a turn is predicted as 90\u00b0, WPrec indicates the probability that the actual turn is 90\u00b0.\nResults on Turn-REMAP \u2013 We compared the predicted turning angle against the clinician's anno-tations for Turn-REMAP. Based on the rotation of hip and knee joints, our method correctly estimates the angle for 41.6% of all the turns on average, with an overall MAE of 34.7\u00b0 and WPrec of 68.3% across 1386 videos."}, {"title": "Ablations", "content": "The accurate detection of 2D skeleton keypoints in each frame of our input clips is an important contrib-utor to the overall accuracy of our method. Another fundamental concern is which single or combination of 'body parts' should be engaged for the computation of the turning angle. We investigate these two issues in our ablation study.\nThe effect of different 2D keypoints \u2013 We investigate how various 2D keypoint detectors impact the performance of the turning angle estimation in Turn-H3.6M. We applied SimplePose [83], HRNet [79] and FastPose [72] as prospective 2D keypoint detectors respectively and evaluated their performance in"}, {"title": "Discussion", "content": "Previous methods for turning analysis have been developed primarily for laboratory or clinical settings to evaluate scripted activities [23, 86, 87, 88]. In Turn-REMAP, we record gait videos in a home-like, unobtrusive environment with PD and control subjects, and provide quantitative evaluations on the accuracy and estimation errors of turning angles during free-living activities. Pham et al. [22] also measured turning in a free-living environment, however, their method measured turning angles from IMUs alone, while our method is video-based. Pham et al. [22] recorded videos to manually validate their estimated results and report an overall error of 0.06\u00b0, but we contend that estimating turning angles at an accurate enough resolution to achieve such low error measurements by examining videos with the naked eye is unreliable. Some other IMU-based studies [26, 28, 89] have also extended their methodology to home environments, but none of these studies validated the measurement accuracy in the free-living setting."}, {"title": "Conclusion and Future Work", "content": "Continuously and automatically measuring turning characteristics in a free-living environment could enhance the current clinical rating scale by capturing the true motor symptoms which fluctuate hour by hour. This study is the first effort to detect the fine-grained angle of turn in gait using video data where people are unscripted and in a home setting. In this paper, we introduced the Turn-REMAP and Turn-H3.6M datasets. Turn-REMAP is the first dataset of free-living turning movements that includes clinician-annotated, quantised turning angle ground truth for both PD patients and control subjects across various scenarios and locations. Turn-H3.6M is derived from the lab-based, large-scale 3D pose benchmark known as Human3.6M, curated specifically for turning data analysis. To estimate the turning angle of a subject in raw RGB videos, we utilised a deep learning framework to reconstruct human joints in 3D space. We then proposed a turning angle calculation approach based on joint rotation. Our framework was applied to the unique Turn-REMAP dataset and further validated on Turn-H3.6M.\nWhile the accuracy of our models may not yet allow their application in the real world, they nevertheless establish a previously non-existent baseline and offer valuable insights for future video-based research in"}, {"title": "Conflict of Interest Statement", "content": "The authors have no conflict of interest in this work."}, {"title": "Author contributions", "content": "QC: Conceptualisation; Data curation; Formal analysis; Investigation; Methodology; Validation; Visual-isation; Writing - original draft; and Writing - review & editing. CM: Resources; Data curation; writing original draft preparation. Data curation; Formal analysis; Investigation; Supervision; Methodology; writ-ing - original draft; and Writing - review & editing. AS: Conceptualisation; Data curation; Methodology. AM and AW: Supervision; Project administration. MM: Supervision; Project administration; Methodology; Writing review & editing"}]}