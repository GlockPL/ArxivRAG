{"title": "Generating Synthetic Electronic Health Record (EHR) Data: A Review with Benchmarking", "authors": ["Xingran Chen", "Zhenke Wu", "Xu Shi", "Hyunghoon Cho", "Bhramar Mukherjee"], "abstract": "Objectives: To conduct a scoping review of existing approaches for synthetic Electronic Health Records (EHR) data generation, to benchmark major methods with proposed open-source software, and to offer recommendations for practitioners.\nMaterials and Methods: We search three academic databases for our scoping review. Methods are benchmarked on open-source EHR datasets, Medical Information Mart for Intensive Care III and IV (MIMIC-III/IV). Seven existing methods covering major categories and two baseline methods are implemented and compared. Evaluation metrics concern data fidelity, downstream utility, privacy protection, and computational cost.\nResults: 42 studies are identified and classified into five categories. Seven open-source methods covering all categories are selected, trained on MIMIC-III, and evaluated on MIMIC-III or MIMIC-IV for transportability considerations. Among them, Generative Adversarial Network (GAN)-based methods demonstrate competitive performance in fidelity and utility on MIMIC-III; rule-based methods excel in privacy protection. Similar findings are observed on MIMIC-IV, except that GAN-based methods further outperform the baseline methods in preserving fidelity. A Python package, \u201cSynthEHRella", "https": "github.com/chenxran/synthEHRella.\nDiscussion: Method choice is governed by the relative importance of the evaluation metrics in downstream use cases. We provide a decision tree to guide the choice among the benchmarked methods.\nConclusion: GAN-based methods excel when distributional shifts exist between the training and testing populations. Otherwise, CorGAN and MedGAN are most suitable for association modeling and predictive modeling, respectively. Future research should prioritize enhancing fidelity of the synthetic data while controlling privacy exposure, and comprehensive benchmarking of longitudinal or conditional generation methods.\nKeywords: Confidentiality, Diffusion Models, Generative Adversarial Network, Generative AI, Phecode, Privacy, Scoping Review, Synthetic EHR, Transformer, Variational Auto-Encoder.", "sections": [{"title": "Introduction", "content": "Electronic Health Records (EHRs) store patients' health information collected via encounters with health systems in a digital format, consisting of both structured and unstructured data. EHRs are often linked to biorepositories that enable molecular profiling of blood or tissue samples. Such rich repositories that integrate disease phenotypes, multi-omics markers, medical images, medications, and laboratory results have become a critical resource for research in biomedicine [1, 2] and public health [3-5]. For researchers in medical informatics, bioinformatics, statistics, computer science, and other quantitative fields, EHR-linked biobanks [6\u20138] are valuable for empirical evaluations of analytical methods using multi-modal data [2, 9-11]. However, few real-world EHR data that are publicly available exist due to patient consent requirements and patient privacy considerations. Even when large databases are publicly available for research, when it comes to evaluation of methods through simulation studies, one needs repeated replicates of realistic EHR data.\nSynthetic EHR data generation has emerged as a promising solution to unlock enormous research and educational potential of real-world healthcare data while safeguarding data confidentiality. First, high-quality realistic synthetic EHR data enable the development and evaluation of analytical tools with less privacy concerns [12-16]. Second, for minority sub-populations, where the risk of reidentification is higher, using privacy-preserving synthetic data is ethicaly more defensible [17-19]. Third, synthetic EHR generation facilitates experimenting with the size of the real training and testing data over multiple replicates for accuracy and uncertainty assessment of methods [20, 21]. Finally, synthetic data provide an ideal environment for training healthcare professionals, data scientists and other researchers in EHR usage, clinical workflows, and data analysis without the risk of exposing real patient information [22, 23].\nWith the rapid growth in the literature on synthetic EHR generation (Figure S1), numerous review and discussion papers have focused on either recent methodological developments or on the drawbacks of existing evaluation metrics [24-28]. However, there is a notable lack of review papers with comprehensive benchmarking of existing methodologies using benchmark datasets. The latest benchmarking paper [29], while making valuable contributions to evaluating Generative Adversarial Network (GAN)-based [30] synthetic EHR generation methods, has two major limitations. First, it evaluated exclusively the methods that are built around GAN, falling short of offering a more comprehensive comparison against alternative approaches in other categories we identified in Section 3.1. Second, the benchmarking was based on closed-source data, limiting the reproducibility and the feasibility for evaluating future methods by other researchers. Finally, with an explosion in latest generative artificial intelligence (GenAI) methods, including large language models (LLMs) and diffusion models, a more comprehensive and updated review, evaluation, and benchmarking is needed.\nIn this paper, we address this knowledge gap by conducting a scoping review of the extant literature on synthetic EHR generation methods and provide a comprehensive benchmarking of seven current approaches and two baseline approaches. Our study provides a holistic evaluation of synthetic EHR generation methods using multiple evaluation metrics, with open-source datasets to ensure reproducibility and accessibility for future research. We also"}, {"title": "Method", "content": "then discuss data source in Section 2.2, and introduce the evaluation procedure, including statement of the evaluation problem, settings, and metrics in Section 2.3. Numerical studies on the effect of varying input training dataset sizes or the generated synthetic dataset sizes are presented in Section 2.4. In Section 3, we present our findings. Specifically, we summarize our results for literature review in Section 3.1; we describe the summary statistics of the data used for benchmarking in Section 3.2; we then present our main findings on evaluation and numerical studies in Section 3.3 and Section 3.4, respectively. We introduce the open-source SynthEHRella package in Section 3.5. In Section 4, we interpret our findings, present a decision tree based on our current evaluation results to offer recommendations for practitioners, discuss challenges of existing methods, outline areas for future methods development, and limitations of this work. The paper concludes with remarks in Section 5."}, {"title": "Literature Review", "content": null}, {"title": "Search Procedure", "content": "In the spirit of a scoping review, we conducted a search on May 5th, 2024 to identify published and preprint articles in English that propose methods for generating synthetic EHR data. The search keywords include (a) synthetic EHR, (b) synthetic EHR generation, and (c) synthetic electronic health records. The search was performed across three academic databases: Google Scholar, PubMed, and Semantic Scholar. An additional search verification was conducted on August 30th, 2024 to identify updated literature.\nAll search results underwent a rigorous screening by the authorship team to exclude irrelevant or unqualified papers. First, studies that did not focus on pipeline development of synthetic EHR generation are removed. Second, papers without an available codebase for reproducing or generating synthetic EHR data were excluded from the consideration of benchmarking. In our final slate, we chose seven distinct methods from each of the five categories that are frequently present in the literature. We also added two baseline methods, one based on directly resampling from the real dataset not intended for privacy-sensitive use but for anchoring the values of the performance metrics, and another based on generating binary phecodes using independent Bernoulli distributions with the mean equal to the marginal phenotype prevalence of each phecode. Detailed descriptions of these methods and the baselines are provided in Section 3.1."}, {"title": "Benchmarking Datasets", "content": "In this work, the evaluation is carried out on two open-source large-scale Intensive Care Unit (ICU)-based and emergency department (ED)-based\u00b2 benchmark EHR datasets described as follows:"}, {"title": "Evaluation", "content": null}, {"title": "Statement of Evaluation Task", "content": "In this study, we focus on the generation of cross-sectional diagnostic events data, following the capabilities of most publicly-available codebases. ICU-based (and ED-based) EHR datasets, such as MIMIC-III and MIMIC-IV, have a longitudinal nature, where each individual patient may have multiple encounters. To this end, we convert all longitudinal records into a cross-sectional format by aggregating each patient's records into a single representation. Formally, consider an EHR dataset with cross-sectional format $\\mathcal{D} = {\\mathbf{x}_1,...,\\mathbf{x}_N}$, where each entry $\\mathbf{x}_i = (x_i^{(1)},...,x_i^{(K)})$ is a $K$-dimensional binary vector representing the presence/absence of diseases coded by a specific coding system (e.g., ICD-9). Each dimension $x_i^{(k)} = 1$ if the $k$-th disease is diagnosed at least once in all visits of $i$-th patient, $k = 1,\u2026, K$, $i = 1,\u2026\u2026, N$. The goal of synthetic EHR generation is to produce a synthetic dataset of size $M$, $\\mathcal{D}_{syn} = {\\mathbf{x}_{i,syn}}_{i=1}^M$, where each sample in $\\mathcal{D}_{syn}$ can be viewed as samples drawn from the same distribution as those in the real dataset of size $N$ denoted as $\\mathcal{D}_{real} = {\\mathbf{x}_{i,real}}_{i=1}^N$, while ensuring privacy protection for patients in $\\mathcal{D}_{real}$. We represent the datasets by two $N \\times K$"}, {"title": "Performance Evaluation Metrics", "content": "We evaluate the quality of synthetic EHR generated by all the selected methods and baselines on both MIMIC-III and MIMIC-IV from three widely adopted perspectives: 1) fidelity for evaluating how closely the distribution of synthetic EHR data matches with real EHR data; 2) downstream utility for measuring the performance gap in downstream analyses compared to real data; 3) privacy exposure for assessing how well the synthetic EHR generation methods protect patient information. Additionally, we also evaluate the selected methods' computational efficiency. The evaluation metrics are presented below.\nFidelity To evaluate whether the generative algorithms capture phenotype-level prevalence information, we first evaluate the dimension-wise distributional discrepancy between the real and the synthetic datasets. We compute the Maximum Mean Discrepancy (MMD) to evaluate the absolute difference in prevalence. Similarly, we use Root Mean Squared Percentage Error (RMSPE) and Mean Absolute Percentage Error (MAPE) for assessing the relative difference in prevalence. These metrics are computed as follows:\n$\\mu_{syn}^{(k)} = \\frac{1}{M} \\sum_{i=1}^M x_{i,syn}^{(k)} $\n$\\mu_{real}^{(k)} = \\frac{1}{N} \\sum_{i=1}^N x_{i,real}^{(k)} $\n$k = 1, ..., K$;\nMMD = $\\max_{1<k<K} | \\mu_{syn}^{(k)} - \\mu_{real}^{(k)} |$;\nRMSPE = $100 \\times \\sqrt{\\frac{1}{K} \\sum_{i=1}^K \\left(\\frac{\\mu_{syn}^{(i)} - \\mu_{real}^{(i)}}{\\mu_{real}^{(i)}}\\right)^2}$;\nMAPE = $\\frac{100}{K} \\sum_{i=1}^K \\left|\\frac{\\mu_{syn}^{(i)} - \\mu_{real}^{(i)}}{\\mu_{real}^{(i)}}\\right|$\nAll the RSMPE values are divided by a factor of 100 for ease of presentation.\nIn addition to quantitative evaluation, we also present visualizations to compare the distribution characteristics between the real and the synthetic EHR data. Specifically, we use a boxplot to show the dimension-wise prevalence using each method, and another boxplot to show the distribution of the number of unique phecodes per patient in the data generated by each method. A scatterplot is included to compare the dimension-wise prevalence between real and synthetic EHR data, with each dot representing a phecode.\nWe are also interested in evaluating whether generative models preserve pair-wise correlations between the phecodes. To this end, we calculate the Pearson correlation matrix following [29, 43] for both real and synthetic datasets and compute the correlation Frobenius distance (CFD):\n$\\mathbf{\\Sigma}_{real} = Corr(\\mathbf{X}_{real}), \\mathbf{\\Sigma}_{syn} = Corr(\\mathbf{X}_{syn}),$\nCFD = $|| \\mathbf{\\Sigma}_{real} - \\mathbf{\\Sigma}_{syn} ||_F = \\sqrt{\\sum_{k=1}^K \\sum_{k'=1}^K (a_{kk'}^{real} - a_{kk'}^{syn})^2}$,\nwhere $a_{kk'}^{real}$ and $a_{kk'}^{syn}$ represents the $(k, k')$-th element in correlation matrices $\\mathbf{\\Sigma}_{real}$ and $\\mathbf{\\Sigma}_{syn}$, respectively. In addition, we visualize element-wise difference of correlation between the real and the synthetic data via boxplots; 100,000 pairs of entries from the upper triangle of the two correlation matrices are sampled and shown in the boxplots.\nIn addition, considering that each disease dimension consists of binary data indicating occurrence, we present another metric based on the Frobenius distance between co-occurrence matrices for the real and the synthetic datasets. The co-occurrence matrices Frobenius distance (COFD) is calculated as follows:\n$\\mathbf{B}_{real} = \\mathbf{X}_{real} \\mathbf{X}_{real}^T, \\quad \\mathbf{B}_{syn} = \\mathbf{X}_{syn} \\mathbf{X}_{syn}^T,$\nCOFD = $|| \\mathbf{B}_{real} - \\mathbf{B}_{syn} ||_F = \\sqrt{\\sum_{k=1}^K \\sum_{k'=1}^K (b_{kk'}^{real} - b_{kk'}^{syn})^2}$,\nwhere $b_{kk'}^{real}$ indicates the number of patients having both the $k$-th and $k'$-th phenotypes in the real data; similarly, $b_{kk'}^{syn}$ for the synthetic data. All the COFD values are divided by a factor of 1000 for ease of presentation."}, {"title": "Utility", "content": "Finally, we assess whether we can distinguish the synthetic and the real data used to train the synthetic data generation method, referred to as discriminative prediction. We use both the real and the synthetic data to train a logistic regression model for predicting whether a given sample in the combined data is real or synthetic. We validate the model via 5-fold cross-validation. We report the Area Under the Curve (denoted as AUC) and Accuracy (denoted as ACC) of the classifier, where lower values indicate higher resemblance of the synthetic data to the real data.\nUtility A primary objective of generating synthetic EHR data is to replace the use of real data in downstream applications, thereby protecting patient privacy without compromising data utility. To this end, we evaluate the utility of synthetic data by examining their performance in both analytical and predictive tasks.\nFirstly, analytical utility assesses the capability of the synthetic data in providing inferential results that approximate those produced based on the real data. To this end, we focus on the analytical task of estimating associations between binary phenotypes $x_i^{(k)}$ and $x_i^{(k')}$ by fitting a logistic regression model with the $k$-th phenotype as the outcome, and the $k'$-th phenotype as the explanatory variable. The point estimates and the 95% confidence intervals of regression coefficients (i.e., log-odds ratios) of the predictor phenotype are reported. A larger overlap in the 95% Confidence Intervals (CIs) between those obtained based on the real data and the synthetic data indicates higher analytical utility of the synthetic data generation method.\nSecondly, predictive utility measures the capability of the synthetic data in training machine learning (ML) methods for downstream use. To evaluate existing methods' predictive utility, a binary classification task is constructed, where the $k$-th phenotype $x_i^{(k)}$ is treated as the outcome with the remaining phenotypes in $x_i$ as predictors. In this work, we list three scenarios commonly used in the literature [29, 44] that concern different combinations of the sources of data used for training and testing a ML method. All the testing are based on the real data for objective evaluations. Suppose we split the real dataset into training and testing subsets (4:1 ratio). All the three scenarios are visualized in Figure S2, with detailed descriptions as follows:\n1. \u201cTrain on Synthetic, Test on Real"}, {"title": "Privacy", "content": "showing whether synthetic data has deleterious or beneficial effect when added to a real data;\n3. \u201cTrain on Real and Test on Real\u201d(TRTR). This scenario does not use synthetic data. The ML model is trained on the training data split, and find the test error on the testing data split. This provides the baseline in predictive utility evaluation.\nFor each of the above three scenarios, we calculate the AUC and ACC of the classifiers. Because TRTR is a real data based approach, we use it as a baseline and report the differences: $AUC_{TSTR} \u2212 AUC_{TRTR}$ and $AUC_{TSRTR} \u2212 AUC_{TRTR}$, and similarly for ACC. Higher values indicate better utility of the synthetic data.\nIn this study, analytical utility evaluations are conducted under two tasks. In the first task, whether a patient gets any type of cancer (phecodeX starting with \u201cCA\u201d) is treated as the outcome, and Obesity (EM_236) is treated as the predictor; for the second task, Diabete (EM_202) is treated as the outcome, and Hypertension (CV_401) is treated as the predictor. See Table 2 for prevalences of any type of cancers, obesity, hypertension, and diabetes. For predictive utility evaluation, Hypertension (CV_401) is treated as the outcome, because it is the most prevalent phenotype in both MIMIC-III and MIMIC-IV cohorts; the remaining phenotypes are treated as predictors. The logistic regression model is treated as our ML models, with results of more advanced ML methods, including Random Forest [46], Gradient Boosting [47], XGBoost [48], LightGBM [49], and K-Nearest Neighbours (K-NN), reported in Supplementary Materials.\nPrivacy First, we use membership inference risk (MIR) [50] to assess the risk that an attacker, who gains access to a patient's complete medical record, could determine whether this patient is included in the training dataset. A subject's membership to the training dataset may be sensitive because the patient may wish to keep this information private from potential attackers. For example, the training data may be based on all the HIV-positive patients treated in a regional facility. Latest work of membership inference attack using synthetic data proposed to conduct privacy attack via an attacker model that can detect local overfitting which occurs if the synthetic data density evaluated at the real data record has a high value; this typically occurs when the real data record is close to the bulk of synthetic data [51]. We compute the minimum Euclidean distance between each real medical record and the synthetic EHR dataset:\n$d_i = \\min_{\\mathbf{x} \\in \\mathcal{D}_{syn}}dist (\\mathbf{x}_{i,real}, \\mathbf{x}),$\nwhere $\\mathbf{x}_{i,real}$ is a record from the real dataset. We further provide an average characterization by reporting the mean and median of these minimum distances as the dataset-level risk of membership inference if all the subjects in the real data may be attacked:\nMIR_{mean} = \\frac{1}{N} \\sum_{i=1}^N d_i, \\quad MIR_{median} = median(d_1,...,d_N),"}, {"title": "Computational Cost", "content": "where higher values indicate lower risks of membership inference attack. Note that unlike [29, 43] where $d_i$ is also calculated, we do not further specify a threshold to classify into successful or failed attacks and calculate the corresponding F1 score to evaluate the membership inference risk of the synthetic data, because the evaluation results may be sensitive to the subjective threshold value.\nThe scenario differs from the discriminitive prediction in the fidelity evaluation in that 1) unlike the use of both the real and the synthetic data, membership inference uses only the synthetic data and a real medical record of the subject being attacked; 2) unlike the binary label of real or synthetic data, membership inference concerns the binary label of whether the subject being attacked is a member in the real data curated by the facility; and 3) unlike the AUC and ACC metric for discriminative prediction, membership inference risk here is evaluated by how close is the real medical record of the attacked individual to the synthetic data.\nWhen $\\mathbf{x}_{i,real}$ belongs to MIMIC-III, $d_i$ measures the risk of identifying the membership of subject $i$ in MIMIC-III based on the synthetic data. When $\\mathbf{x}_{i,real}$ belongs to MIMIC-IV, $d_i$ measures the risk of identifying whether subject $i$ is a member of MIMIC-III which remains meaningful because MIMIC-III and MIMIC-IV datasets overlapped in the periods of data collection (2008-2012), hence containing overlapping subjects. The MIRmean and MIRmedian then provide an average characterization of this distance assuming all the patients in MIMIC-IV may subject to membership inference attacks.\nIn addition, we assess attribute inference risk (AIR) [52] which occurs when attackers who have access to partial information about a patient's real medical records could infer the missing attributes based on the synthetic data. To evaluate AIR, we select the top 10 balanced (prevalence closest to 0.5) and top 10 imbalanced diseases as unknown attributes simultaneously to be inferred. We applied a 1-Nearest Neighbor (1-NN) approach to match each real record with the most similar record in the synthetic dataset based on the remaining known attributes. The corresponding features in the matched synthetic records are considered as the prediction of the missing attributes in the real records. The F1 score is reported to measure how well the masked attributes can be predicted, where a lower value indicates a lower risk of attribute inference attack.\nComputational Cost\nBeyond the above three metrics for assessing the quality of the synthetic data, the computational cost of generating synthetic EHR datasets is also an important factor during methods selection. This is especially true if a large amount of synthetic data are needed for evaluating frequentist performance of analytic and prediction methods. To this end, we evaluate the time needed for all the selected methods in synthesizing 100 EHR samples. The evaluations are conducted on a multi-node cluster equipped with Intel(R) Xeon(R) Gold 6148 CPUs with a total of 40 physical cores. A 16GB Tesla V100 GPU is used for running deep generative models when necessary."}, {"title": "Numerical Studies: Effectiveness of Varying N and M", "content": "Importantly, we explore the effect by varying the size of training (N) and synthetic (M) data on the quality of synthetic EHR data. We address two questions. First, given a fixed amount of training data, how many synthetic samples can a generative model produce before the benefits of adding one additional synthetic sample diminish? Although deep generative models such as GANs, VAEs, and diffusion models can theoretically generate infinite samples, the amount of novel information does not increase proportionally with the size of the synthetic dataset, as it is constrained by the finite training data. Second, how does the size of the training data impact the quality of the generated synthetic data, given a fixed number of synthetic samples?\nTo explore these questions, we select the overall best-performing methods from our benchmarking study. For the first question, we train the selected method and generate M = 1K,2K, 5K, 10K, 20K, 50K, 100K, 200K, 500K synthetic samples. We evaluate the generated data using three metrics: MMD for fidelity, TSTR for utility (AUC is reported), and AIR for privacy. For the second question, we train the selected method with N = 1K, 2K, 5K, 10K, 15K, 20K, 25K, 30K, 35K, 40K real samples and generate 50K synthetic samples; we evaluate them with the same metrics. The results are visualized to illustrate how these metrics evolve under different values of M and N."}, {"title": "Results", "content": null}, {"title": "Summary of Literature Review", "content": "Our literature search identified 107 publications based on keyword searches, from which 42 studies are found to propose methodologies for generating synthetic EHRs that are relevant to our work. Table 1 lists all these relevant studies, organized by the year of publication. Based on the literature review, we classify most existing methods for synthetic EHR generation into five primary categories:\n\u2022 Rule-based method: These approaches generate synthetic EHR data using predefined algorithms and rules, often incorporating real-world disease prevalence and national census data as hyperparameters to simulate disease occurrence and population demographics. Rule-based methods were the dominant approach to generate synthetic EHRs prior to the emergence of deep generative models. From our review, 6 out of 42 methods are rule-based methods [53-57].\n\u2022 GAN-based method: With the rise of deep learning, GANs have become widely adopted for synthetic EHR generation. [58] and [59] are among the first to introduce GANs for this purpose, with subsequent studies focusing on improving the capability of GAN-based models and addressing privacy concerns. Based on the scoping review, 18 out of 42 methods are GAN-based methods [17, 22, 42, 44, 60-70]."}, {"title": "Methods Selected for Benchmarking", "content": "\u2022 VAE-based method: As another strand of deep generative modeling that are popular in the artificial intelligence (AI) community, VAEs are applied to learn latent embedding from the real EHR datasets and generate synthetic EHR data based on this learned structure [18]. Additionally, [41] extends VAEs for longitudinal EHR data generation. Our review shows that 3 out of 42 methods apply VAEs to generate synthetic EHR data.\n\u2022 Transformer-based method: Transformers [32], known for their ability to process sequential data [71] and their success in natural language processing [72, 73], have also been explored for longitudinal synthetic EHR generation. In our literature review, 4 out of 42 methods learn to generate synthetic EHR data with the transformer structures. [74] are the first to train transformers on real EHR data for this purpose. Moreover, [75] explores the potential of Large Language Models (LLMs) trained on natural texts in synthesizing EHR data.\n\u2022 Diffusion-based method: Most recently, diffusion models [33, 34], which have demonstrated strong performance in image and video generation [76-78], have been applied for synthetic EHR generation. 8 out of the 42 studies have proposed diffusion-based models for this purpose [43, 79-85].\nIn addition to the five main categories above, conventional statistical methods were also used to generate synthetic EHR data. For example, [86] proposed an algorithm based on Gibbs sampling to generate synthetic EHR data; [87] introduced Bayesian Network (BN) in synthetic EHR generation. Furthermore, other neural network structures were also explored in synthetic EHR generation [88].\nBased on these literature, we identify three key functionalities that are critical for the design of synthetic data generation algorithms:\nGeneration conditional on specific variables: Supporting synthetic conditional EHR generation, which produces data based on specific contexts, is critical for downstream applications that require subgroup analyses or the inclusion of socio-demographic factors. Our literature review reveals that 24 of the 42 studies cover conditional generation. For example, [59] generates synthetic data based on specific labels to augment downstream predictive applications; [17] generates data conditioning on specific socio-demographic concepts to overcome small data size challenges in training; [18] generates synthetic data given specific medical conditions of interest for further scientific investigation.\nGeneration of repeated visits over time: As EHR datasets include patients with multiple visits in nature, it is important to capture the longitudinal patterns to generate realistic synthetic timped-stamped data. Of the 42 reviewed studies, 23 incorporate temporal information for longitudinal generation. For example, Synthea [89] generates synthetic EHR longitudinally by generating synthetic patients and simulate their encounters across their lifespans; TimeDiff [68] and PromptEHR [74], on the other hand, applied neural networks with architectures designed to model sequential data to simulate longitudinal EHR."}, {"title": "Summary Statistics of Benchmark Datasets", "content": "Generation of multiple modalities of data: EHR datasets are inherently multimodal, containing various types of medical encounters such as diagnoses, medications, procedures, chart events, clinical notes, and laboratory results. Understanding which modalities each method can generate is crucial for effective data generation. In this study, we focus on events that could be characterized by coding systems (i.e., diagnoses, medications, procedures) and summarize if existing methods could model those events. As shown in Table 1, 16 of the 18 existing studies that evaluate their methods on MIMIC-III primarily focus on diagnostic events; [41, 60, 64, 75, 83] additionally include medication or procedures in their modeling; [58, 74, 85, 90] consider all the three types of events in their modeling.\nMethods Selected for Benchmarking\nDespite the recent development of synthetic EHR methods, 25 out of 42 studies remain closed-source at the time of this scoping review, as highlighted in Table 1. The lack of accessible codebases poses challenges for fair comparisons across methods and limits the broader use of synthetic EHRs in downstream applications. To conduct benchmarking, we select seven generation methods that provide publicly available and user-friendly codebases. These methods cover the five primary categories discussed above. Specifically, for rule-based methods, we select Plasmode [54] and Synthea [89]; for GAN-based methods, we choose MedGAN [58] and CorGAN [64]; for VAE-based methods, we use a baseline implemented by CorGAN's authors due to the lack of easily reproducible codebases in existing VAE-based methods; for transformer-based methods, we select PromptEHR [74]; and for diffusion-based methods, we choose EHRDiff [43]. For all the existing methods, we follow their default settings, except necessary minor changes for debugging.\nIn addition to the methods selected from existing literature, we include two simple baselines for comparison. The first baseline, which we refer to as Prevalence-based Random, randomly generates synthetic EHR data based on the estimated marginal phenotype prevalences without taking phenotype correlation into account. For the second baseline, we construct synthetic EHR data by simply bootstrapping the real EHR dataset; we refer to this method as Resample. Note that while the Resample baseline is impractical for synthetic data generation when privacy concern exists, it serves as a valuable reference by providing an upper bound of the fidelity and utility capability of the existing synthetic EHR generation methods, as it closely resemble the real data. To account for possible loss of information during code transformation (Section 2.3.1), we compute phenotype prevalence based on the MIMIC-III data in ICD-9 system for Prevalence-based Random to generate synthetic data. Similarly, we bootstrap real data under the same coding system for Resample.\nSummary Statistics of Benchmark Datasets\nTable 2 presents the summary statistics for the MIMIC-III and MIMIC-IV datasets. Overall, the socio-demographic distributions are largely similar between MIMIC-III and MIMIC-IV, except that MIMIC-III additionally includes 7,874 neonates in the dataset. More than half of the patients in MIMIC-III have been diagnosed with cancer (52.9%), higher than"}, {"title": "Evaluation Results", "content": "the prevalence in MIMIC-IV (40.2%). The prevalence of mental health conditions, such as anxiety and depression, is higher in MIMIC-IV (13.0% and 20.8%, respectively) compared to MIMIC-III (11.0% and 16.0%, respectively). Whereas diabetes and hypertension are more common in the MIMIC-III cohort (25.4% and 47.6%, respectively) compared to the MIMIC-IV cohort (21.7% and 46.0%, respectively). The prevalence of obesity, on the other hand, is higher in the MIMIC-IV cohort (12.6%) than in the MIMIC-III cohort (5.4%). Regarding EHR characteristics, both MIMIC-III and MIMIC-IV exhibit right-skewed distributions for the number of encounters per person, unique phecodes per person, and length of follow-up. The median numbers of encounters per person for both datasets are 1.0, indicating that over half of patients in both data visited ICU or ED (in MIMIC-IV) only once during the data collection period. The mean number of encounters is higher in MIMIC-IV (2.4 visits) than that in MIMIC-III (1.3 visits). In terms of the length of follow-up, significant zero-inflation issues are observed for both datasets, with a median of 0 year of follow-up. This is consistent with our previous findings that half of patients have only one hospital admission during data collection period. The average length of follow-up, on the other hand, is higher in MIMIC-IV (1.2 years) than that in MIMIC-III (0.3 years). The average number of unique phecodes per patient is greater in MIMIC-III (31.3 phecodes) than in MIMIC-IV (26.0 phecodes), although MIMIC-IV has a higher maximum number of unique phecodes for a single patient (260 phecodes) than MIMIC-III (203 phecodes).\nEvaluation Results\nIn this subsection, we present the evaluation results of the selected methods and baselines. By default, we use the entire MIMIC-III dataset to train synthetic data generation methods. We evaluate the quality of the synthetic data by comparing against the real MIMIC-III data or against MIMIC-IV for transportability considerations."}, {"title": "Fidelity", "content": null}, {"title": "Utility", "content": "Figure 1 shows the boxplots of the marginal prevalences in the real and the synthetic EHR data. The Prevalence-based Random baseline, along with MedGAN, CorGAN, and EHRDiff effectively preserve the distribution information of marginal prevalence of MIMIC-III. In contrast, Plasmode, Synthea, and VAE tend to consistently underestimate the prevalence compared to the real dataset. In addition, the MIMIC-IV dataset exhibits a similar distribution in the estimated prevalence compared to MIMIC-III, with a median of the prevalences across all phenotypes slightly lower than that in MIMIC-III.\nFigure 2 displays the number of unique phecodes per patient in EHR data. CorGAN shows similar distributions to MIMIC-III, whereas most of the remaining methods tend to underestimate the number of unique phecodes per patient. Conversely, PromptEHR consistently overestimate the number of unique phecodes, and Prevalence-based Random shows higher values at 25th and 50th quantiles in the number of phecodes per patient compared to MIMIC-III.\nFigure S3 compares the phecode-wise prevalence between the synthetic data and the"}, {"title": "Privacy", "content": "real data (MIMIC-III). Prevalence-based Random achieves"}]}