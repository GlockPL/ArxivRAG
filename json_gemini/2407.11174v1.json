{"title": "iHuman: Instant Animatable Digital Humans\nFrom Monocular Videos", "authors": ["Pramish Paudel", "Anubhav Khanal", "Ajad Chhatkuli", "Danda Pani Paudel", "Jyoti Tandukar"], "abstract": "Personalized 3D avatars require an animatable representa-\ntion of digital humans. Doing so instantly from monocular videos offers\nscalability to broad class of users and wide-scale applications. In this\npaper, we present a fast, simple, yet effective method for creating ani-\nmatable 3D digital humans from monocular videos. Our method utilizes\nthe efficiency of Gaussian splatting to model both 3D geometry and ap-\npearance. However, we observed that naively optimizing Gaussian splats\nresults in inaccurate geometry, thereby leading to poor animations.\nThis work achieves and illustrates the need of accurate 3D mesh-type\nmodelling of the human body for animatable digitization through Gaus-\nsian splats. This is achieved by developing a novel pipeline that benefits", "sections": [{"title": "1 Introduction", "content": "Instant and accurate creation of personalized 3D avatars is highly sought-after\nfor digital human representation, to enable vast applications in virtual reality\n(VR), augmented reality (AR), gaming, and telepresence. A key component in\nthis regard is the animatable representation [12, 19,56]. On the other hand, re-\nconstructing animatable digital humans instantly from monocular videos can\nimmediately facilitate wide-scale applications serving a broad class users. The\nmost existing monocular video based methods focus on either the real-time ren-\ndering solutions [1] (using long training/reconstruction time), or only mesh-level\nreconstruction [15] without the possibility of realistic re-rendering under the\nchange in pose. These solutions eventually hinder the broad-scale applicability,\nwhich we aim to address in this paper by developing a novel method for instant\nand accurate modelling of animatable digital humans from monocular videos.\nIn recent year, building on the re-\nmarkable success of representing radi-\nance fields implicitly (i.e. NeRF [45]),\nseveral methods [8-10,13,14,16,18,18,\n21-23, 32, 32, 37, 40, 42, 48, 57, 58, 61,\n64, 65, 69, 71, 73, 75, 76, 79, 82, 84, 87],\nhave been developed to capture high-\nfidelity humans from multiple frames\nof videos. However, the high compu-\ntational demand of the volume ren-\ndering in the NeRF-based methods [63, 77,85] creates a major bottleneck for\nthe aimed instant animatable digitization. Therefore, some very recent meth-\nods [33,35,36,80] have been developed by leveraging the rendering efficiency of\nthe Gaussian Splats [27]. However, these methods do not meet some or all of the\nrequired criteria in capturing (i) from monocular videos; (ii) in instant manner;\n(iii) animatable avatar; and (iv) high quality re-rendering under change in pose.\nIn this paper, we propose an efficient pipeline to convert a monocular video,\nwith known pose, to animatable digital humans instantly (training time on par\nwith that of capture) in few seconds, using Gaussian splats based modelling."}, {"title": "2 Related Work", "content": "Mesh based reconstruction methods. Most methods that represent the human\nbody as a mesh make use of SMPL [43] or other parametric body models [11,50,\n55]. Methods in this category predict the parameters for the parametric body\nmodel either by regression [25,49,54] or by optimization [7]. Kolotouros et al. [31]\nand similar methods [39,46] directly regress the 3D vertices. Although the out-\nput meshes here can be animated they do not contain the clothing details and\npersonalized facial features. Methods which extend the parametric body model\nwith a deformation layer [2-5] can model clothing as well but are unable to\naccurately model personalized geometric details.\nImplicit functions based approaches. Implicit functions based reconstruction\nmethods [44, 45, 52, 68, 78] use an MLP to learn an implicit function such as\noccupancy, signed distance fields or density fields to describe geometry. They\ncan represent and render the geometric details of static scenes but suffer from\nhigh training time. Anim-NeRF [8] and other similar methods [9,10,16,18,19,21,\n23, 37, 40, 56, 58, 69, 71, 73] extend NERF to dynamic scenes by using SMPL [43]\nguided deformations between the observed space and a static canonical space\nallowing for explicit control. Instant Avatar [22] and similar approaches [24,83]\nuse [66] to speed up the training time but still have high memory requirements.\nGaussian Splat based approaches. Recently introduced 3D gaussian Splatting(3D-\nGS) [28] uses 3D Gaussians and its projections to represent a static scene. 3D-GS\nachieves significantly faster training and rendering time over NERF-based ap-\nproaches. Recent works [20,30,33,36,38,47,51,60,86] extend 3D-GS to represent\ndynamic scenes using SMPL guided deformations. They produce an animat-\nable representation of the human body at speed compared to previous methods."}, {"title": "3 Method", "content": "Provided a monocular video sequence with a dynamic human and the body\nposes, our goal is to generate a personalized colored mesh 3D model of a subject\nconsisting of body shape, hair and clothing geometry, and underlying skeleton.\nGiven an n frame video sequence $(I_t)_{t=1}^n$ of a single subject in front of a fixed\ncamera (camera pose and intrinsics), along with the respective body poses ${\\theta_t}$ we output a personalized animatable representation of the human subject. The\nkeyword 'animatable' implies that we should be able to render the underlying\nrepresentations in novel body poses ${\\theta_t'}$. Additionally, we want to complete the\nchallenging training process in seconds, in favor of scalability.\nWe achieve our goal of obtaining an animatable 3D human using 3D Gaussian\nSplatting (3D-GS) [17,27]. Below we explain the 3D-GS and its deformations as\npreliminaries in \u00a73.1. We then introduce our iHuman representation and describe\nthe details of our method. iHuman consists of initializing 3D-GS in the canonical\nSMPL pose, see \u00a73.2. We bind each 3D Gaussian to a triangle face as described\nin \u00a73.3. We then proceed onto deforming the 3D-GS consistent to the posed\nspace, corresponding to the real image in \u00a73.4. Taking advantage of explicit 2D\nGaussians embedded in 3D [17,67], we encode normal for each Gaussian in \u00a73.5."}, {"title": "3.1 Preliminaries", "content": "3D Gaussian Splatting. 3D Gaussian Splatting (3D-GS) has recently become\nthe state-of-the-art tool for novel view synthesis. Important for our applica-\ntion, different from NERF, 3D-GS also uses explicit 3D representation using\nanisotropic 3D Gaussians.\nA 3D Gaussian can be written in terms of its full 3D covariance matrix\n$\\Sigma \\in \\mathbb{R}^{3\\times 3}$, $\\Sigma \\geq 0$ and position in space $y \\in \\mathbb{R}^3$ along with center $x \\in \\mathbb{R}^3$.\n$$G(y) = exp\\left(-\\frac{1}{2} (y-x)^T \\Sigma^{-1}(y - x)\\right).$$\n(1)"}, {"title": "3.2 Gaussian Human Template Model", "content": "Our iHuman approach uses a Gaussian template model on the canonical pose of\nthe standard SMPL shape [43]. We denote this canonical mesh as M composed\nof vertices $V_c = \\{v_0, v_1, ..., v_m\\}$ and triangles $F = \\{i_x\\}$, thus $M = (V_c, F)$. We\nthen bind the Gaussians in 3D to the canonical mesh M as described in Eq. (2).\nThis process is described in \u00a73.3, where each Gaussian is tied to a specific face,\ni.e., triangle $i_x$. We thus obtain the Gaussian Splat representation for the subject\nin the canonical SMPL pose by extending Eq. (2) as follows:\n$$G_{skinned} = \\{x, q, S, a_o, SH, \\omega, \\delta v,i_x\\}.$$\n(6)\nIn contrast to Eq. (2), in Eq. (6), each Gaussian center x is in fact the centroid of\na triangle $i_x$. Additionally, we introduce new parameters where w is the skinning\nweights obtained from the standard parametric body model [43]. Importantly,\n$\\delta v$ is the vertex displacement for the canonical shape vertex v to the clothed\nsubject shape $v'$:\n$$v' = v + \\delta v.$$\n(7)\nThe displacement vectors $\\delta v$ are obtained vertex-wise from a continuous Hash\nEncoder whose output is fed to a 3-layer MLP (multi-linear perceptron). We\ndenote this as:\n$$\\delta v = f_s(h(v))$$\n(8)\nwhere $f$ is a 3-layer MLP and $h(.)$ is the hash encoder similar to the instant\nNGP [66].\nAfter the displacements, we bind the 3D Gaussians to the parent triangle $i_x$\nby centering it on the centroid of the face $i_x$ at x. The rotation q and scale S\nare 2D rotation and scale as explained in 3.3. Each triangle center x for $i_x$ is\nobtained as,\n$$x = \\frac{i_x[1] + i_x[2] + i_x[3]}{3}.$$\n(9)"}, {"title": "3.4 3D Gaussian Deformation", "content": "Together with the Gaussian binding and the template model described in \u00a73.2,\n\u00a73.3, we are able to precisely represent a human surface in the canonical pose.\nIn this section, we deform the Gaussian Splat model in order to represent any\npose of a human subject. Given the input pose $\\theta_t$, we achieve the deformation\nusing forward linear blend skinning [34].\nThus, we compute the transformation of each vertex $v'$ in posed space with\nblend skinning $w(\\theta_t)$. The transformation of each point $v$ is calculated with blend\nskinning $w(\\theta_t)$ and target bone transformation $B(\\theta_t) = \\{B_1(\\theta_t), ..., B_{nb}(\\theta_t)\\}$.\nThe skinning weight field is defined as:\n$$w(v_c) = \\{w_1,..., w_{nb}\\}$$\n(10)\nwhere $v'$ is a point in canonical space and $n_b$ is the number of bones."}, {"title": "3.5 Normal Map from 3D Gaussian", "content": "Gaussian Splats are generally optimized using RGB photometric loss [27, 59].\nHowever, we note that this approach results in a poor mesh, low on details. Our\ngoal is to compute details of human surface, e.g., facial attributes, wrinkles and\nhair [See Supp]. To that end, we take advantage of two crucial facts:\n1. We have explicit representation of vertices($v_t$) and faces ($i_x$) available for\neach Gaussian to obtain its normal without ambiguity from Eq. (13).\n2. SOTA methods like ECON [61,74,75] rely on normal map prediction from\nRGB to produce SOTA results.\nOne can therefore use the depth gradient Depth in order to compute the\nsurface normals. However, such measurements tend to inherently noisy as it relies\non the alpha blending of the gaussians which can introduce noise [see Supp]. On\nthe other hand, the normal image $I_n$ should also equal to the aligned normals\nobtained from the posed vertices {$\\upsilon_p$}. We first compute the mesh/Gaussian\nnormals using Eq. (13).\n$$\\hat{n} = \\frac{(\\upsilon_p[i_x[1]] - \\upsilon_p[i_x[0]])\\times (\\upsilon_p[i_x[2]] - \\upsilon_p[i_x[0]]}{\\|(\\upsilon_p[i_x[1]] - \\upsilon_p[i_x[0]]) \\times (\\upsilon_p[i_x[2]] - \\upsilon_p[i_x[0]])\\|}$$\n(13)\nwhere $\\upsilon_p[i_x[j]]$ refers to the j-th posed vertex of the triangle in the face $i_x$.\nIn order to obtain the normal map image from the estimated normals of\nEq. (13), we again make use of the Gaussian splatting rasterizer. In order to\npreserve smoothness and accuracy, the Gaussian Splatting rasterizer already\nprovides a highly efficient approach for normal map computation. For that pur-\npose, we encode the normal $\\hat{n}$ into a second spherical harmonics function SH of\ndegree 0 by representing the components of the normal as: $\\hat{n}_x, \\hat{n}_y$ and $\\hat{n}_z$ related"}, {"title": "3.6 Training", "content": "Given a set of training images and input poses, we learn our Gaussian Human\nTemplate Model iHuman by optimizing the following objective function:\n$$L = L_{rgb} + L_{normal} + L_{reg}$$\n(15)\nwhere $L_{rgb}$ is the photometric loss, $L_{normal}$ is the normal map loss and $L_{reg}$\nis the 3D regularization term for normal consistency. We employ a combination\nof $l_1$ and D-SSIM term Eq. (16) for both the $L_{rgb}$ and the $L_{normal}$, with the\nhyperparameter $\\lambda \\approx 0.2$:\n$$L = (1 - \\lambda)L_1 + \\lambda L_{DSSIM} .$$\n(16)"}, {"title": "4 Experiments", "content": "4.1 Implementation Details\nWe use PyTorch [53] for the implementation and we choose Adam [29] as the\noptimizer. We conduct all experiments on a single NVIDIA RTX 4090. We use\nstandard skinned human body template model, SMPL [43] as initial mesh tem-\nplate and also use its blend skinning weights. We upsample the mesh to obtain\n165K faces in order to initialize our model. To obtain the ground truth normal\nmaps for normal supervision, we use same pix2pixHD [70] network as used in\nPIFuHD [61]. Our method runs at 20 iterations per second (optimization on 20\nimages in 1 second for 1 epoch) during training with > 100 fps during inference.\n4.2 Datasets and Baselines\nDatasets. We conduct experiments on 3 different datasets.\nPeopleSnapshot [3]. It comprises of various monocular RGB videos of differ-\nent subjects recorded in natural settings. In these videos, individuals assume\nan A-pose and rotate in place facing a stationary camera. We follow the same\nevaluation protocol as Instant Avatar [22] by training our model with the pose\nparameters optimized by Anim-NeRF [8]. We keep the poses frozen throughout\ntraining for a fair comparison."}, {"title": "5 Conclusion", "content": "In this work, we proposed a new method to obtain high fidelity animatable hu-\nman model in record time. We obtain state-of-the-art performance in limited\ncomputational budget. To that end, we used mesh binded Gaussians, explicit\nnormal rasterization and optimization through normal supervision providing\nfast and accurate results. Through experiments, we also illustrate the need of\naccurate surface representation, while using Gaussian splats, for faithful render-\ning under the change in pose. In the future, we intend to model the temporal\nsmoothness of Gaussian parameters for video frames, that can potentially im-\nprove optimization speed and solution even further."}]}