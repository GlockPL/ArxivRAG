{"title": "iHuman: Instant Animatable Digital Humans\nFrom Monocular Videos", "authors": ["Pramish Paudel", "Anubhav Khanal", "Ajad Chhatkuli", "Danda Pani Paudel", "Jyoti Tandukar"], "abstract": "Personalized 3D avatars require an animatable representa-tion of digital humans. Doing so instantly from monocular videos offersscalability to broad class of users and wide-scale applications. In thispaper, we present a fast, simple, yet effective method for creating ani-matable 3D digital humans from monocular videos. Our method utilizesthe efficiency of Gaussian splatting to model both 3D geometry and ap-pearance. However, we observed that naively optimizing Gaussian splatsresults in inaccurate geometry, thereby leading to poor animations.This work achieves and illustrates the need of accurate 3D mesh-typemodelling of the human body for animatable digitization through Gaus-sian splats. This is achieved by developing a novel pipeline that benefits", "sections": [{"title": "1 Introduction", "content": "Instant and accurate creation of personalized 3D avatars is highly sought-after\nfor digital human representation, to enable vast applications in virtual reality\n(VR), augmented reality (AR), gaming, and telepresence. A key component in\nthis regard is the animatable representation [12, 19, 56]. On the other hand, re-\nconstructing animatable digital humans instantly from monocular videos can\nimmediately facilitate wide-scale applications serving a broad class users. The\nmost existing monocular video based methods focus on either the real-time ren-\ndering solutions [1] (using long training/reconstruction time), or only mesh-level\nreconstruction [15] without the possibility of realistic re-rendering under the\nchange in pose. These solutions eventually hinder the broad-scale applicability,\nwhich we aim to address in this paper by developing a novel method for instant\nand accurate modelling of animatable digital humans from monocular videos.\nIn recent year, building on the re-\nmarkable success of representing radi-\nance fields implicitly (i.e. NeRF [45]),\nseveral methods [8\u201310, 13, 14, 16, 18, 18,\n21\u201323, 32, 32, 37, 40, 42, 48, 57, 58, 61,\n64, 65, 69, 71, 73, 75, 76, 79, 82, 84, 87],\nhave been developed to capture high-\nfidelity humans from multiple frames\nof videos. However, the high compu-\ntational demand of the volume ren-\ndering in the NeRF-based methods [63, 77, 85] creates a major bottleneck for\nthe aimed instant animatable digitization. Therefore, some very recent meth-\nods [33, 35, 36, 80] have been developed by leveraging the rendering efficiency of\nthe Gaussian Splats [27]. However, these methods do not meet some or all of the\nrequired criteria in capturing (i) from monocular videos; (ii) in instant manner;\n(iii) animatable avatar; and (iv) high quality re-rendering under change in pose.\nIn this paper, we propose an efficient pipeline to convert a monocular video,\nwith known pose, to animatable digital humans instantly (training time on par\nwith that of capture) in few seconds, using Gaussian splats based modelling."}, {"title": "2 Related Work", "content": "Mesh based reconstruction methods. Most methods that represent the human\nbody as a mesh make use of SMPL [43] or other parametric body models [11, 50,\n55]. Methods in this category predict the parameters for the parametric body\nmodel either by regression [25, 49, 54] or by optimization [7]. Kolotouros et al. [31]\nand similar methods [39, 46] directly regress the 3D vertices. Although the out-\nput meshes here can be animated they do not contain the clothing details and\npersonalized facial features. Methods which extend the parametric body model\nwith a deformation layer [2\u20135] can model clothing as well but are unable to\naccurately model personalized geometric details.\nImplicit functions based approaches. Implicit functions based reconstruction\nmethods [44, 45, 52, 68, 78] use an MLP to learn an implicit function such as\noccupancy, signed distance fields or density fields to describe geometry. They\ncan represent and render the geometric details of static scenes but suffer from\nhigh training time. Anim-NeRF [8] and other similar methods [9, 10, 16, 18, 19, 21,\n23, 37, 40, 56, 58, 69, 71, 73] extend NERF to dynamic scenes by using SMPL [43]\nguided deformations between the observed space and a static canonical space\nallowing for explicit control. Instant Avatar [22] and similar approaches [24, 83]\nuse [66] to speed up the training time but still have high memory requirements.\nGaussian Splat based approaches. Recently introduced 3D gaussian Splatting(3D-\nGS) [28] uses 3D Gaussians and its projections to represent a static scene. 3D-GS\nachieves significantly faster training and rendering time over NERF-based ap-\nproaches. Recent works [20, 30, 33, 36, 38, 47, 51, 60, 86] extend 3D-GS to represent\ndynamic scenes using SMPL guided deformations. They produce an animat-\nable representation of the human body at speed compared to previous methods."}, {"title": "3 Method", "content": "Provided a monocular video sequence with a dynamic human and the body\nposes, our goal is to generate a personalized colored mesh 3D model of a subject\nconsisting of body shape, hair and clothing geometry, and underlying skeleton.\nGiven an n frame video sequence {$\\mathbf{I}_t$}$_{t=1}^n$ of a single subject in front of a fixed\ncamera (camera pose and intrinsics), along with the respective body poses {$\\mathbf{\\theta}_t$}\nwe output a personalized animatable representation of the human subject. The\nkeyword 'animatable' implies that we should be able to render the underlying\nrepresentations in novel body poses {$\\mathbf{\\theta}'_t$}. Additionally, we want to complete the\nchallenging training process in seconds, in favor of scalability.\nWe achieve our goal of obtaining an animatable 3D human using 3D Gaussian\nSplatting (3D-GS) [17, 27]. Below we explain the 3D-GS and its deformations as\npreliminaries in \u00a73.1. We then introduce our iHuman representation and describe\nthe details of our method. iHuman consists of initializing 3D-GS in the canonical\nSMPL pose, see \u00a73.2. We bind each 3D Gaussian to a triangle face as described\nin \u00a73.3. We then proceed onto deforming the 3D-GS consistent to the posed\nspace, corresponding to the real image in \u00a73.4. Taking advantage of explicit 2D\nGaussians embedded in 3D [17, 67], we encode normal for each Gaussian in \u00a73.5."}, {"title": "3.1 Preliminaries", "content": "3D Gaussian Splatting. 3D Gaussian Splatting (3D-GS) has recently become\nthe state-of-the-art tool for novel view synthesis. Important for our applica-\ntion, different from NERF, 3D-GS also uses explicit 3D representation using\nanisotropic 3D Gaussians.\nA 3D Gaussian can be written in terms of its full 3D covariance matrix\n$\\mathbf{\\Sigma} \\in \\mathbb{R}^{3\\times 3}$, $\\mathbf{\\Sigma} \\geq 0$ and position in space $\\mathbf{y} \\in \\mathbb{R}^3$ along with center $\\mathbf{x} \\in \\mathbb{R}^3$.\n\n$G(\\mathbf{y}) = \\exp \\Big(-\\frac{1}{2} (\\mathbf{y}-\\mathbf{x})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{y} - \\mathbf{x})\\Big).$"}, {"title": "3.2 Gaussian Human Template Model", "content": "Our iHuman approach uses a Gaussian template model on the canonical pose of\nthe standard SMPL shape [43]. We denote this canonical mesh as M composed\nof vertices $\\mathbf{V}_c = {\\mathbf{v}_0, \\mathbf{v}_1, ..., \\mathbf{v}_m}$ and triangles $\\mathbf{F} = {i_x}$, thus M = ($\\mathbf{V}_c$, $\\mathbf{F}$). We\nthen bind the Gaussians in 3D to the canonical mesh M as described in Eq. (2).\nThis process is described in \u00a73.3, where each Gaussian is tied to a specific face,\ni.e., triangle $i_x$. We thus obtain the Gaussian Splat representation for the subject\nin the canonical SMPL pose by extending Eq. (2) as follows:\n\n$\\mathbf{G}_{skinned} = {\\mathbf{x}, \\mathbf{q}, \\mathbf{S}, \\alpha_o, \\mathbf{SH}, \\omega, \\delta\\mathbf{v},i_x\\}$.\n\nIn contrast to Eq. (2), in Eq. (6), each Gaussian center $\\mathbf{x}$ is in fact the centroid of\na triangle $i_x$. Additionally, we introduce new parameters where $\\omega$ is the skinning\nweights obtained from the standard parametric body model [43]. Importantly,\n$\\delta\\mathbf{v}$ is the vertex displacement for the canonical shape vertex $\\mathbf{v}$ to the clothed\nsubject shape $\\mathbf{v}'$:\n\n$\\mathbf{v}' = \\mathbf{v} + \\delta\\mathbf{v}.$\n\nThe displacement vectors $\\delta\\mathbf{v}$ are obtained vertex-wise from a continuous Hash\nEncoder whose output is fed to a 3-layer MLP (multi-linear perceptron). We\ndenote this as:\n\n$\\delta\\mathbf{v} = f_s(\\mathbf{h}(\\mathbf{v}))$\n\nwhere $f_s$ is a 3-layer MLP and $\\mathbf{h}(.)$ is the hash encoder similar to the instant\nNGP [66].\nAfter the displacements, we bind the 3D Gaussians to the parent triangle $i_x$\nby centering it on the centroid of the face $i_x$ at $\\mathbf{x}$. The rotation $\\mathbf{q}$ and scale $\\mathbf{S}$\nare 2D rotation and scale as explained in 3.3. Each triangle center $\\mathbf{x}$ for $i_x$ is\nobtained as,\n\n$\\mathbf{x} = \\frac{i_x[1] + i_x[2] + i_x[3]}{3}.$"}, {"title": "3.3 Binding Gaussians To Mesh Surface", "content": "In the original works, the Gaussian splats are initialized on the point clouds out-\nput by a Structure-from-Motion (SfM) method such as COLMAP [62]. A recent\nwork SuGaR [17] proposes using surface aligned Gaussians for the optimization,\nsuch that one of the axes of the Gaussian covariance $\\mathbf{\\Sigma}$ is aligned to the surface\nnormal $\\mathbf{n}_i$, with the corresponding scale as $\\epsilon$. In iHuman, we have the advantage\nof mesh initialization through the SMPL canonical model. Therefore, we propose\nto align all Gaussians according to the following steps:\n\n1. Compute the surface normal $\\mathbf{n}_i$ for each face $i_x$.\n2. For each Gaussian, assign $\\mathbf{n}_i$ as one of the directions of its covariance matrix\n$\\mathbf{\\Sigma}$, with the corresponding scale in $\\mathbf{S}$, i.e., $S_3 = \\epsilon$. In practice we keep\n$\\epsilon$ = 1mm, an extremely small value.\n3. Assign the other two directions according to the major directions of the\ntriangular face.\nAs a consequence of having a 2D Gaussian, we further reduce the learnable\nparameters required for obtaining the posed as well as canonical human mesh.\nFor each Gaussian we can directly set $S_3 = \\epsilon$ and the quaternion is reduced to\na complex number of a single degree of freedom in order to keep the Gaussian\naligned with the triangle."}, {"title": "3.4 3D Gaussian Deformation", "content": "Together with the Gaussian binding and the template model described in \u00a73.2,\n\u00a73.3, we are able to precisely represent a human surface in the canonical pose.\nIn this section, we deform the Gaussian Splat model in order to represent any\npose of a human subject. Given the input pose {$\\mathbf{\\theta}_t$}, we achieve the deformation\nusing forward linear blend skinning [34].\nThus, we compute the transformation of each vertex $\\mathbf{v}'$ in posed space with\nblend skinning $\\mathbf{w}(\\theta_t)$. The transformation of each point $\\mathbf{v}$ is calculated with blend\nskinning $\\mathbf{w}(\\theta_t)$ and target bone transformation $\\mathbf{B}(\\theta_t) = {\\mathbf{B}_1(\\theta_t), ..., \\mathbf{B}_{n_b}(\\theta_t)}$.\nThe skinning weight field is defined as:\n\n$\\mathbf{w}(\\mathbf{v}_c) = {w_1,..., w_{n_b}}$\n\nwhere $\\mathbf{v}'$ is a point in canonical space and $n_b$ is the number of bones."}, {"title": "3.5 Normal Map from 3D Gaussian", "content": "Gaussian Splats are generally optimized using RGB photometric loss [27, 59].\nHowever, we note that this approach results in a poor mesh, low on details. Our\ngoal is to compute details of human surface, e.g., facial attributes, wrinkles and\nhair [See Supp]. To that end, we take advantage of two crucial facts:\n\n1. We have explicit representation of vertices($\\mathbf{v}'_t$) and faces ($i_x$) available for\neach Gaussian to obtain its normal without ambiguity from Eq. (13).\n2. SOTA methods like ECON [61, 74, 75] rely on normal map prediction from\nRGB to produce SOTA results.\nOne can therefore use the depth gradient $\\mathbf{Depth}$ in order to compute the\nsurface normals. However, such measurements tend to inherently noisy as it relies\non the alpha blending of the gaussians which can introduce noise [see Supp]. On\nthe other hand, the normal image $\\mathbf{I}_n$ should also equal to the aligned normals\nobtained from the posed vertices {$\\mathbf{v}_p$}. We first compute the mesh/Gaussian\nnormals using Eq. (13).\n\n$\\mathbf{n} = \\frac{(\\mathbf{v}_{p[i_x[1]]} - \\mathbf{v}_{p[i_x[0]]}) \\times (\\mathbf{v}_{p[i_x[2]]} - \\mathbf{v}_{p[i_x[0]]})}{\\|(\\mathbf{v}_{p[i_x[1]]} - \\mathbf{v}_{p[i_x[0]]}) \\times (\\mathbf{v}_{p[i_x[2]]} - \\mathbf{v}_{p[i_x[0]]})\\|}$\n\nwhere $\\mathbf{v}_{p[i_x[j]]}$ refers to the j-th posed vertex of the triangle in the face $i_x$.\nIn order to obtain the normal map image from the estimated normals of\nEq. (13), we again make use of the Gaussian splatting rasterizer. In order to\npreserve smoothness and accuracy, the Gaussian Splatting rasterizer already\nprovides a highly efficient approach for normal map computation. For that pur-\npose, we encode the normal $\\hat{\\mathbf{n}}$ into a second spherical harmonics function $SH_n$ of\ndegree 0 by representing the components of the normal as: $\\hat{n}_x$, $\\hat{n}_y$ and $\\hat{n}_z$ related"}, {"title": "3.6 Training", "content": "Given a set of training images and input poses, we learn our Gaussian Human\nTemplate Model iHuman by optimizing the following objective function:\n\n$\\mathcal{L} = \\mathcal{L}_{rgb} + \\mathcal{L}_{normal} + \\mathcal{L}_{reg}$\n\nwhere $\\mathcal{L}_{rgb}$ is the photometric loss, $\\mathcal{L}_{normal}$ is the normal map loss and $\\mathcal{L}_{reg}$\nis the 3D regularization term for normal consistency. We employ a combination\nof $l_1$ and D-SSIM term Eq. (16) for both the $\\mathcal{L}_{rgb}$ and the $\\mathcal{L}_{normal}$, with the\nhyperparameter $\\lambda$ = 0.2:\n\n$\\mathcal{L} = (1 - \\lambda)\\mathcal{L}_1 + \\lambda\\mathcal{L}_{DSSIM}.$"}, {"title": "4 Experiments", "content": "We use PyTorch [53] for the implementation and we choose Adam [29] as the\noptimizer. We conduct all experiments on a single NVIDIA RTX 4090. We use\nstandard skinned human body template model, SMPL [43] as initial mesh tem-\nplate and also use its blend skinning weights. We upsample the mesh to obtain\n165K faces in order to initialize our model. To obtain the ground truth normal\nmaps for normal supervision, we use same pix2pixHD [70] network as used in\nPIFuHD [61]. Our method runs at 20 iterations per second (optimization on 20\nimages in 1 second for 1 epoch) during training with > 100 fps during inference."}, {"title": "5 Conclusion", "content": "In this work, we proposed a new method to obtain high fidelity animatable hu-\nman model in record time. We obtain state-of-the-art performance in limited\ncomputational budget. To that end, we used mesh binded Gaussians, explicit\nnormal rasterization and optimization through normal supervision providing\nfast and accurate results. Through experiments, we also illustrate the need of\naccurate surface representation, while using Gaussian splats, for faithful render-\ning under the change in pose. In the future, we intend to model the temporal\nsmoothness of Gaussian parameters for video frames, that can potentially im-\nprove optimization speed and solution even further."}, {"title": "1 Implementation Details", "content": "A single scene reconstructed using 3D gaussian splatting contains a very large\nnumber of gaussians. Backpropagating the gradients to every gaussians in every\niteration results in poor performance due to which we need an efficient sampling\nstrategy is needed to select only a few gaussians to be updated every iteration.\nInspired by [?] we encode the color and displacement for each gaussian using a\nmulti-resolution hash encoder.\nThe color and displacement encoder include a hash encoder followed by a fully-\nfused MLPs implemented using the tiny-cuda-nn framework [?]. The imple-\nmented hash encoder has 16 levels, hash table size of $2^{17}$ with 4 features per\nentry in the table with a base resolution of 4 and resolution growth factor of 1.5.\nThe fully fused MLPs consist of 2 hidden layer with 64 neurons per layer and\nuses relu as the activation function. Both the color and displacement encoder\ntake the gaussian position as input and produce a color and displacement value\nof the same dimension as the input."}, {"title": "1.2 Binding Gaussian To Mesh Surface", "content": "As we explained in Section 3.3, we bind Gaussians at the centroid of the trian-\ngular face. Given face $i_x$, the centroid is given by:\n\n$\\mathbf{x} = \\frac{i_x[1] + i_x[2] + i_x[3]}{3}.$\n\nThrough this equation, we always maintain the position of Gaussian at the\ncentroid of the face $i_x$. And, we can directly optimize the vertices $V_c$ of canonical\nmesh $\\mathbf{M} = (\\mathbf{V}_c, \\mathbf{F})$.\nSimilar to SuGaR [?], we parameterize the 3D rotation of the Gaussians\nwith only 2 parameters by encoding the rotation in complex 2D rotation form\nwith $(x + iy)$. We limit their rotation to local 2D triangular face plane. We now\nexplain how we convert the local 2D complex rotation to 3D rotation required\nfor Gaussian Rasterizer."}, {"title": "1.3 Training", "content": "For the PeopleSnapshot [?] and Multi-Garment Dataset [?], we use the same hy-\nper parameter across all the subjects. We use Adam [?] optimizer for parameter\noptimization. Each 3D Gaussian is defined by a center (x), scale (S), opacity\n(a), rotation (q), spherical harmonics (SH) and blend weights (w). We don't\noptimize a, q and w. We keep a = 1. We optimize the joints position (J) of\nthe canonical skeleton. For learning good geometric details, the number of 3D\nGaussians should be enough to model the geometry. We use template mesh of\nabout 220K triangular faces and same number of 3D Gaussians are initialized\nfor PeopleSnapshot and Multi Garment Dataset.\nWe divide the training into three stages where we prioritize learning geomet-\nric information in the earlier stage and color information in the later stages. The\nfirst stage lasts till 4th epoch, then the second stage starts from 4th epoch and"}, {"title": "2 Additional Qualitative Results", "content": "3D Mesh Reconstruction. We show 3D mesh reconstruction of GART [?],\nAnim-NeRF [?] and our method on our synthetic Multi-Garment Dataset in\nFig. 1. We show more qualitative results of mesh reconstruction of the proposed\nmethod on PeopleSnapshot in Fig. 2.\nIn Fig. 4, we show reconstructed normal map image. The quality of normal\nmap image being close to the ground truth normal map also shows good quality\nand fidelity of the reconstructed mesh.\nWe show some qualitative results of 3D Mesh reconstruction on UBC-Fashion\ndataset in Fig. 3.\nNovel View Synthesis. Compared to other SoTA methods, our method\nachieves novel view with less artifacts in less time and less number of input\nsequences 5.\nAs shown in Fig. 6, iHuman achieves good quality novel view synthesis even\nbeing trained with only 6 number of views.\nWe show novel pose synthesis results in Fig. 7 on PeopleSnapshot [?] of the\nsubjects trained with only 20 input views."}, {"title": "3 Challenging Cases", "content": "UBC-Fashion dataset [?] contains subjects in long clothing that undergoes de-\nformation. As shown in Fig. 8, with heavy clothing deformation, though the re-\nconstructed view looks good, there are some geometric implausible views. Even\nwith heavy deforming scene, the reconstructed mesh doesn't contain floating\nartifacts."}]}