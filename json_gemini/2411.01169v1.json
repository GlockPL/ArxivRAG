{"title": "Bi-Level Graph Structure Learning for Next POI Recommendation", "authors": ["Liang Wang", "Shu Wu", "Qiang Liu", "Yanqiao Zhu", "Xiang Tao", "Mengdi Zhang", "Liang Wang"], "abstract": "Next point-of-interest (POI) recommendation aims to predict a user's next destination based on sequential check-in history\nand a set of POI candidates. Graph neural networks (GNNs) have demonstrated a remarkable capability in this endeavor by exploiting\nthe extensive global collaborative signals present among POls. However, most of the existing graph-based approaches construct graph\nstructures based on pre-defined heuristics, failing to consider inherent hierarchical structures of POI features such as geographical\nlocations and visiting peaks, or suffering from noisy and incomplete structures in graphs. To address the aforementioned issues, this\npaper presents a novel Bi-level Graph Structure Learning (BiGSL) for next POI recommendation. BiGSL first learns a hierarchical\ngraph structure to capture the fine-to-coarse connectivity between POls and prototypes, and then uses a pairwise learning module to\ndynamically infer relationships between POI pairs and prototype pairs. Based on the learned bi-level graphs, our model then employs a\nmulti-relational graph network that considers both POI- and prototype-level neighbors, resulting in improved POI representations. Our\nbi-level structure learning scheme is more robust to data noise and incompleteness, and improves the exploration ability for\nrecommendation by alleviating sparsity issues. Experimental results on three real-world datasets demonstrate the superiority of our\nmodel over existing state-of-the-art methods, with a significant improvement in recommendation accuracy and exploration\nperformance.", "sections": [{"title": "INTRODUCTION", "content": "HE emergence of location-based social networks has\nbrought to light a subject of great interest to both re\nsearchers and service providers alike: next point-of-interest\n(POI) recommendation. This task seeks to comprehend the\ntemporal nature of a user's preferences by analyzing their\nhistorical check-in sequences and then make predictions\nabout the next POIs that they are most likely to visit. Such\ninsights can be used to improve both the user experience as\nwell as the service provider's services.\nGraph-based methods have been widely used in POI\nrecommendation due to their capability of modeling global\ncollaborative relationships of POIs across users. These meth\nods typically involve two stages: (1) the construction of a\ntopology graph based on POI features and (2) the learning\nof POI representations based on the constructed graph.\nDepending on the type of information to be used, such a\ngraph may be built by taking into consideration the spatial\ninformation of POIs, such as distance intervals [1], [2], [3]\nor grid regions [4], as well as temporal features from users'\nsequential check-in data, such as the average time intervals\nbetween consecutive visits [1] or the Jaccard similarity of\ntime slot sets [4]. Additionally, it is also common to model\ntransitions between POIs based on the number or frequency\nof consecutive visits between each POI [3], [5], [6]. After the\ngraph is built, graph neural networks (GNNs) are used to\nlearn the POI representations by aggregating information\nfrom the neighborhood of the nodes. These POI representa-\ntions are then used to further learn users' preferences from\nthe sequences of visited POIs and rank candidate POIs for\nproducing recommendations.\nDespite their success, existing graph-based methods for\nPOI recommendation suffer from various limitations.\nFirstly, previous methods construct graphs based solely\non local neighborhoods, disregarding the valuable hier\narchical structures of POIs. Hierarchical structure means\nthat fine-grained POIs can be divided into coarse-grained\ngroups, and POIs within the same group have similar\ngroup characteristics in some aspects. These hierarchical\nstructures have been proven to improve recommendations\nby mitigating the sparsity issues [7], [8], [9] and improving\nthe exploration ability [4], [10]. Some previous methods\nhandle hierarchical information by employing multi-task\nlearning [4] or designing hierarchical encoders [11], how\never, the ingenious combination of the advantages of GNNs\nand hierarchical structures remains unexplored.\nSecondly, the graph structure of these existing methods\nis usually fixed during training, determined by pre-defined"}, {"title": "RELATED WORK", "content": "In this section, we succinctly review existing studies for next\nPOI recommendation and graph structure learning.\n2.1 Next POI Recommendation\nNext POI recommendation aims to infer the users' dynamic\npreferences and predict where the user will go next, given\nthe historical check-ins and a set of POI candidates.\nRecurrent neural networks (RNNs) and self-attention\nhave shown promising performance in handling sequential\ndata, hence they have been widely used as the backbone\nof the next POI recommenders [15], [16], [17], [18]. Some\nstudies are dedicated to capturing sequential dependencies\nin sequences to model the dynamic user preferences [19],\n[20], [21], [22]. On the other hand, POI features and his\ntorical check-ins contain rich collaborative signals, such as\nspatial location, visited time, and frequency of consecutive\nvisits, and are therefore leveraged to make more effective\nrecommendations from sparse data. Early works introduce\nthese collaborative signals directly into the backbone, such\nas computing transition matrices or gates in RNNs [23], [24],\n[25], [26] and attention maps in self-attention [27].\nRecently, it has been found that these collaborative\nsignals can be represented by graphs and the GNNs can\nbe employed to effectively capture the correlations among\nPOIs [3], [12], [13]. For example, STP-UDGAT [1] constructs\nthree types of POI graphs based on the spatial distance,\ntime interval, and consecutive visiting, so as to learn user\npreferences in different views. GETNext [6] introduces a\nglobal trajectory graph to better leverage the extensive col\nlaborative signals from different users. HMT-GRN [4] con\nstructs global spatio-temporal graphs to model collaborative"}, {"title": "Graph Structure Learning", "content": "Although GNNs have achieved superior performance in\nanalyzing graph-structured data, most GNNs are highly\nsensitive to the quality of graph structures and usually\nrequire a credible graph structure that is hard to construct\nin real-world applications [30]. Given that GNNs recursively\naggregate information from neighborhoods to update node\nembeddings, the iterative nature of this process has conse\nquential cascading effects. Small noise in a graph will be\npropagated to the neighboring nodes, subsequently affect\ning the embeddings of numerous other nodes [14], [31]. Re\ncently, considerable literature has arisen around the theme\nof graph structure learning (GSL), which targets at jointly\nlearning an optimized graph structure and correspond\ning representations. Existing GSL methods can be roughly\ngrouped into three categories: metric learning that models\nor refines the edge weights by measuring the similarity\nbetween node representations [32], [33], [34], probabilistic\nmodeling that models the probability distribution of edges\nand then samples a graph from this distribution [30], [35],\n[36], and direct optimization that treats the graph adjacency\nmatrix as parameters and optimizes it directly along with\nGNN parameters [37], [38], [39].\nAlthough these studies leverage graph structure learning\nto refine the graph structures, they are not tailored for next\nPOI recommendation. They only learn pairwise relation\nships between nodes and lack consideration on meaningful\nhierarchical structure. NCL [40] extracts coarse-grained pro\ntotypes for cross-granularity contrastive learning in graph\ncollaborative filtering, inspiring us to construct hierarchical\ngraph structures using prototypes. Distinct from previous\ngraph structure learning methods, we not only adaptively\nlearn pairwise relationships between POIs to suppress the\npotential noise, but also construct hierarchical structures to\nfurther enrich the neighborhoods of POIs by extracting POI\nprototypes, so as to capture the global relationships between\nPOIs effectively and provide accurate recommendations."}, {"title": "PRELIMINARIES AND PROBLEM STATEMENT", "content": "Let $U = \\{u_1,u_2,...,u_m\\}$ and $L = \\{l_1,l_2,...,l_N\\}$ denote\nthe set of M users and N POIs, respectively.\nDefinition 1. Check-in. A check-in record is represented\nin a tuple $(l_{t_i}, loct_i, timet_i)$, in which $l_{t_i}$ is the POI visited\non time step $t_i$ with its location coordinates $loct_i$, and $timet_i$\nis the timestamp.\nDefinition 2. Check-in Sequence. Each user $u_m$ has\na chronologically ordered historical check-in sequence $s_m =$\n$\\{(l_{t_1}, loct_1, timet_1), (l_{t_2}, loct_2, timet_2), ..., (l_{t_i}, loct_i, timet_i)\\}$,\nwhere $l_{t_i}$ is the last POI visited.\nDefinition 3. POI Feature. Apart from the POI IDs,\nPOIs typically possess several primitive features, including\nspatial features (latitude and longitude), temporal features\n(the time distribution of check-ins), transition features (the\ndistribution of consecutive check-ins between POIs), and\ncategory features, among others. These features reveal the\nsimilarity or collaborative relationships among POIs, mak\ning them useful in constructing graphs for existing graph\nbased next POI recommendation methods, following pre\ndefined rules. For instance, a spatial graph can be con\nstructed by dividing grids according to latitude and lon\ngitude or calculating distances, while a temporal graph can\nbe constructed based on temporal feature similarity.\nDefinition 4. Feature View. To effectively capture col\nlaborative signals embedded in different primitive features,\nwe adopt a multi-view framework. In each feature view,\nwe model the information provided by only one type of\nprimitive features and then fuse the information from all\nviews. We denote the set of feature views, which also repre\nsents the set of POI primitive feature types, as V. In feature\nview $v \\in V$, the primitive features of POIs are denoted\nby $X_v \\in \\mathbb{R}^{N \\times d_v}$, where $d_v$ represents the dimension of\nthe primitive feature. The i-th row, $x_i \\in \\mathbb{R}^{d_v}$, denotes the\nprimitive feature of POI $l_i$. For instance, in the spatial feature\nview, $spatial_i = (latitude_i, longitude_i)$, and $d_{spatial} = 2$.\nProblem 1. Next POI Recommendation. Next POI rec\nommendation takes a user's historical check-in sequence $s_m$\nand the POI candidate set L as input to generate a ranked\nPOI list for the next time step $t_{i+1}$, where the next visited\nPOI $l_{t_{i+1}}$ should be highly ranked."}, {"title": "THE PROPOSED BIGSL MODEL", "content": "This section elaborates on our proposed BiGSL. We first\nintroduce the base recommender backbone. Then, we intro\nduce four main components in our model: (1) hierarchical\nstructure learning that infers the hierarchical structure by\ngrouping POIs into different clusters and extracting proto\ntypes, (2) pairwise structure learning that adaptively infers\nrelationships between POIs or prototypes, (3) a novel multi\nrelational graph attention network that can fully exploit\nlearned hierarchical graphs, and (4) contrastive multiview\nfusion that computes view-shared and view-specific repre\nsentations to facilitate information fusion. Finally, we ex\nplain how to optimize our model and provide a complexity\nanalysis. The overall framework of BiGSL is shown in Fig. 2.\n4.1\nBackbone Recommender\nThe key components of our proposed BiGSL are model\nagnostic and can be plugged into any sequential recommen\ndation model. To show the effectiveness of our approach, we\nchoose a simple yet effective recommender as the backbone,\nwhich contains only an embedding layer, an LSTM layer\nand a dense layer with softmax normalization."}, {"title": "Hierarchical Structure Learning", "content": "The embedding layer offers dense POI ID embeddings\nand user ID embeddings. We describe a POI $l_i$ (a user $u_m$)\nwith an embedding vector $l_i \\in \\mathbb{R}^{d_2}$ ($u_m \\in \\mathbb{R}^{d_2}$), where $d_2$\ndenotes the embedding size.\nWe first use the LSTM layer to learn the user's dynamic\npreference from the historical check-in sequence $s_m$:\n$ht_i = LSTM (s_m),$\n(1)\nwhere LSTM(\u00b7) represents the LSTM layer, and $ht_i \\in \\mathbb{R}^{d_3}$ is\nthe hidden representation of user $u_m$'s historical check-ins.\nNext, we compute the conditional probability of next\nPOI distribution and rank all POIs to make personalized\nrecommendation:\n$y = Softmax (W (ht_i ||u_m)),$\n(2)\nwhere $y \\in \\mathbb{R}^N$ is the predicted conditional probability\ndistribution regarding $t_{i+1}$, and $\\hat{y_i} = P(l_i|s_m)$ is the proba\nbility that the next POI is $l_i$ given the historical sequence\n$s_m$. $U_m \\in \\mathbb{R}^{d_2}$ is the trainable user ID embedding to\nintroduce personalization, $||$ is the vector concatenation,\nand $W\\in \\mathbb{R}^{N\\times(d_2+d_3)}$ is the learnable weight matrix. The\nsoftmax function is performed to compute the conditional\nprobability of the next POI distribution. Finally, we can sort\nPOIs in descending order of conditional probability and get\na ranked POI list.\n4.2 Hierarchical Structure Learning\nBased on the aforementioned backbone, we point out that\nexisting methods, which employ conventional GNNs on\nmanually constructed graphs to encode the interaction re\nlationship between POIs, cannot model the hierarchical\ninformation well. To address this concern, we propose a\nhierarchical structure learning method to embed the hier\narchical information in the graph structures explicitly.\nTo model the hierarchical nature in POI features, we\npropose a hierarchical structure learning method. We design\na hierarchical structure learning objective to group POIs into\ndifferent clusters and extract prototypes. Roughly speaking,\nprototypes can be regarded as the center of clusters that\nrepresent a group of semantically similar POIs.\nFormally, the goal of graph structure learning is to max\nimize the following log-likelihood function:\n$L = \\sum_{l_i\\in L} log p (z_i | \\Theta, X)$\n$= \\sum_{l_i\\in L} log \\sum_{c_j\\in C}p (z_i, c_j | \\Theta, X),$\n(3)\nwhere $\\Theta$ is learnable parameters of model, X is primitive\nfeatures of POIs, $z_i \\in \\mathbb{R}^{d_2}$ is the learned structure embed\nding of POI $l_i$ that will be used to construct the graph\nstructure. C is the set of cluster centroids, and we use\n$K = |C|$ to denote the number of clusters. The objective\nin Eq. (3) is hard to optimize directly because $z_i, c_j$ are both\nfree variables. Therefore, we introduce its tractable lower\nbound by Jensen's inequality:\n$L = \\sum_{l_i\\in L} log \\sum_{c_j\\in C}Q (c_j | z_i)\\frac{p (z_i, c_j | \\Theta, X)}{Q (c_j|z_i)}$\n$\\geq \\sum_{l_i \\in L} \\sum_{c_j \\in C}Q (c_j | z_i) log \\frac{p (z_i, c_j | \\Theta, X)}{Q (c_j|z_i)},$\n(4)\nwhere $Q (c_j | z_i)$ denotes the distribution of latent variable\n$c_j$ when $z_i$ is observed. The goal of graph structure learning\ncan be reformulated to maximize the function over $z_i$ when\n$Q (c_j z_i)$ is estimated. Since the cluster centroids are latent,\nwe introduce the Expectation\u2013Maximization (EM) algorithm\nto formulate the optimization process.\nIn the E-step, $z_i$ is fixed and $Q (c_j | z_i)$ can be estimated\nby K-Means algorithm over all $z_i$. The distribution is esti\nmated by a hard indicator $Q (c_k | z_i) = 1$ if POI $l_i$ belongs\nto k-th cluster, and $Q (c_j | z_i) = 0$ for other centroids $c_j$.\nIn the M-step, we fix $Q (c_j | z_i)$ and optimize $z_i$. By in\ntroducing hard indicator $Q (c_j | z_i)$, maximizing the lower\nbound in Eq. (4) yields a loss function:\n$L_{HSL} = - \\sum_{l_i\\in L} \\sum_{c_j \\in C}Q (c_j | z_i) log p (z_i, c_j | \\Theta, X),$\n(5)"}, {"title": "Pairwise Structure Learning", "content": "Taking inspiration from NCL [40], we assume that the\ndistribution of POIs is isotropic Gaussian over their corre\nsponding clusters and each Gaussian distribution has the\nsame variance. Therefore, the loss function can be written\nas:\n$L_{HSL} = -\\sum_{l_i\\in L} log \\frac{exp(-(z_i-c_k)^T \\cdot (z_i-c_k)/2\\sigma^2)}{\\sum_{c_j\\in C}exp(- (z_i - c_j)^T \\cdot (z_i - c_j) /2\\sigma^2)}$\n$= -\\sum_{l_i\\in L} log \\frac{exp (z_i^T c_k / T_1)}{\\sum_{c_j\\in C}exp (z_i^T c_j / T_1)},$\n(6)\nwhere $c_k$ is the centroid of the cluster to which POI $l_i$\nbelongs, and $2\\sigma^2$ is represented by a temperature coefficient\n$T_1$. Since $z_i$ and $c_j$ have been $l_2$-normalized in advance, we\ncan leverage $(z_i \u2013 c_j)^T \u00b7 (z_i \u2013 c_j) = 2 \u2013 2z_i^T c_j$ to simplify\nthe loss function.\nThis objective suggests that in the M-step, the struc\nture embedding of each POI and its corresponding cluster\ncentroid should be as close as possible. We achieve this\nby iteratively conducting K-Means in the E-step and min\nimizing $L_{HSL}$ in the M-step. With the above objective, the\nhierarchical nature in POI features can be captured in the\nstructure embedding and the prototypes can be obtained by\naveraging POI representations in each cluster. The prototype\nwill be used to represent the coarse-grained semantics of\nclusters and to extend the neighborhoods of POI nodes.\nCompared to heuristic grouping approaches, such as\ngrouping temporal features by date, our clustering-based\nmethod enables the adaptive uncovering of intricate hier\narchical structures in POI features, which may be irregular\nand not aligned with pre-defined heuristic rules.\nAfter hierarchical structure learning, we can define an\nadjacency matrix $A^{Hier} \\in \\mathbb{R}^{N\\times K}$ between POIs and proto\ntypes based on the hard indicator:\n$A_{ij}^{Hier} = Q(c_j|z_i) = \\{\n    1, l_i \\text{ belongs to j-th cluster},\n    0, \\text{ otherwise}.\n    \\},$\n(7)\n4.3 Pairwise Structure Learning\nAlthough hierarchical structure information has been ex\nplicitly captured, the GNNs are still susceptible to the\npresence of data noise and incompleteness in the graph\nstructures. Traditional rule-based graph construction relies\non pre-defined rules or assumptions, inevitably leading\nto noisy and missing edges. In contrast, graph structure\nlearning methods can alleviate these issues by automatically\nidentifying patterns and inferring the topological structure.\nTo address the issue of noisy and incomplete graphs,\nwe adopt a deep graph structure learning method to learn\npairwise relationships between POIs adaptively:\n$z_i = W_2\\sigma (W_1x_i + b_1) + b_2,$\n(8)\n$A_{ij}^{POI} = \\frac{z_i^T z_j}{\\| z_i \\| \\| z_j \\|},$\n(9)\nwhere $x_i \\in \\mathbb{R}^{d_1}$ is the primitive feature (e.g., latitude and\nlongitude in the spatial view) of POI $l_i$, which is used\nto construct graphs, $z_i \\in \\mathbb{R}^{d_2}$ is the structure embedding\ntransformed by learnable parameters $W_1 \\in \\mathbb{R}^{d_2\\times d_1}, W_2\\in$"}, {"title": "Multi-Relational Graph Attention Network", "content": "$\\mathbb{R}^{d_2\\times d_2}, b_1, b_2 \\in \\mathbb{R}^{d_2}$. $A^{POI} \\in \\mathbb{R}^{N\\times N}$ is the adjacency matrix\nof POI graph. To refine $A^{POI}$ into a sparse and normalized\nadjacency matrix, we also conduct e-neighborhood sparsifi\ncation and normalization post-processing, which are widely\nused in graph structure learning [41], [42], [43].\nDistinct from the previous methods that only consider\nthe resemblance between POIs, we also capture the re\nlationships between coarse-grained prototypes to further\nexplore the hierarchical structure. Following the pairwise\nstructure learning defined in Eq. (8) and (9), we take the\ncluster centroids as the primitive features to construct the\nconnections between the prototypes:\n$\\hat{c_i} = W_4\\sigma (W_3c_i + b_3) + b_4,$\n(10)\n$A_{ij}^{Proto} = \\frac{\\hat{c_i}^T \\hat{c_j}}{\\| \\hat{c_i} \\| \\| \\hat{c_j} \\|},$\n(11)\nFollowing bi-level graph structure learning, we obtained\nbi-level graphs that encompass both fine-grained POI-level\ninformation and coarse-grained prototype-level informa\ntion. The prototype-level information unveils the collective\ncharacteristics of similar POIs. A conventional graph can\nbe symbolized as $G = (V, A)$, where V represents the\nset of nodes, and A denotes the adjacency matrix. We\npresent the formalized notation for a bi-level graph as\n$G = (V_{POI},V_{Proto}, A^{POI}, A^{Proto}, A^{Hier})$, where $V_{POI}$ and\n$V_{Proto}$ respectively signify the sets of POI nodes and proto\ntype nodes, and $A^{POI} \\in \\mathbb{R}^{N\\times N}, A^{Proto} \\in \\mathbb{R}^{K\\times K}, A^{Hier} \\in$\n$\\mathbb{R}^{N\\times K}$ are the three adjacency matrices learned through bi\nlevel graph structure learning. These matrices portray the\ninterconnections between POI-POI, prototype-prototype,\nand POI-prototype, respectively.\n4.4 Multi-Relational Graph Attention Network\nThe hierarchical and pairwise structure learning methods\nproduce high-quality hierarchical graphs, which not only\nare confronted with less noise and missing information, but\nalso manifest the hierarchical information in POI features\nthrough the graph topology.\nTo fully exploit the learned hierarchical graphs, we de\nsign a multi-relational graph attention network. First, we\ndefine the neighborhood of each POI node based on the\nhierarchical graph. As shown in Fig. 3, we define three types\nof neighbor nodes. As \u201c\u2297\u201d is the target node to be updated,\n\u201c\u2297\" represents the connected POI nodes that are most\nsemantically similar to the target node, \u201cA\u201d is the prototype"}, {"title": "Contrastive Multiview Fusion", "content": "of cluster that the target node belongs to", "A": "epresents 2-hop prototype neighbors that\nprovide information about more potential similar groups.\nThus", "nodes": "R = \\{\\text{\u2297-\u2297"}, "text{\u2297-A},\\text{\u2297-A}\\}$. Then,\nthe graph representation learning is defined as:\n$a_{ij} = \\frac{exp (\\phi (a_1^T [W_r l_i||W_r l_j"], "graph\nstructure": "n$s_{ij} = \\{\n    \\text{POI}_{ij} \\quad j \\text{ is POI neighbor},\n    A_{ij}^{Hier} \\quad j \\text{is 1-hop prototype neighbor},\n    A_{pj}^{Proto} \\quad j \\text{is 2-hop prototype neighbor},\n    \\},$\n(14)\nwhere $l_i$ belongs to the p-th cluster when j is 2-hop proto\ntype neighbor.\n4.5\nContrastive Multiview Fusion\nIn POI recommendation, multiple features are widely used\nto construct graphs, such as spatial graph, temporal graph,\nand transition graph. We claim that each feature provides a\nfeature view. In each view, a set of POI representations can\nbe obtained by conducting graph representation learning.\nPOI representations derived from multiple views convey\nboth shared and complementary information. However, ex\nisting methods for fusing representations tend to rely on\nsimple techniques such as summation or concatenation. As\na result, important informative features may be overlooked.\nIn deep learning, linear disentanglement has been used\nto obtain distinguishable and generalizable representa\ntions [44], [45]. To fully exploit the information embedded in\nmultiple views, we propose a contrastive multiview fusion\nmethod that captures both view-shared and view-specific\ninformation. Each POI representation is decomposed into\nview-shared and view-specific parts. View-shared informa\ntion denotes the common characteristics of POIs across all\nviews. Extracting view-shared information aids in construct-"}