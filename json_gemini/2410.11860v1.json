{"title": "Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task", "authors": ["Chengyuan Xu", "Kuo-Chin Lien", "Tobias H\u00f6llerer"], "abstract": "When designing an AI-assisted decision-making system, there is often a tradeoff between precision and recall in the Al's recommendations. We argue that careful exploitation of this tradeoff can harness the complementary strengths in the human-AI collaboration to significantly improve team performance. We investigate a real-world video anonymization task for which recall is paramount and more costly to improve. We analyze the performance of 78 professional annotators working with a) no AI assistance, b) a high-precision \"restrained\" AI, and c) a high-recall \"zealous\" AI in over 3,466 person-hours of annotation work. In comparison, the zealous Al helps human teammates achieve significantly shorter task completion time and higher recall. In a follow-up study, we remove Al assistance for everyone and find negative training effects on annotators trained with the restrained AI. These findings and our analysis point to important implications for the design of AI assistance in recall-demanding scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine-learning-based artificial intelligence (AI) systems have exceeded human performance in certain applications. But in high-stakes domains where fully-autonomous Al is not at peak performance or not permitted, such as in clinical decision-making [7, 9, 49, 53, 56] or driver assistance [11, 13, 22], forming a human-AI team is a viable strategy to improve both efficiency and accuracy. AI can provide recommendations while human users maintain agency and control over the final decisions. Studies have shown the human-Al team is expected to achieve \"complementary team performance\" - the team performance being better than either one alone [2, 5, 61]. But there are more questions than answers on which exact factors in the Al system affect the team performance and how.\nBansal et al. recently showed in simplified binary classification problems that the most accurate AI is not necessarily the best teammate unless it helps to improve the team utility [2]. But how about in more complex problems where the AI teammate is not simply better or worse for its accuracy? For example, in many computer vision problems, people determine the best-performing algorithms based on combination metrics such as the F1 score [14, 47], which can be broken down into two metrics - precision and recall [16, 33, 34]. Researchers can either balance the two metrics or prioritize one over the other to identify the best model for their application [20, 40]. Two AI systems can have the same F1 score but provide very different recommendations with different measures of recall (see a, b in Figure 1). The tradeoff between precision and recall puts them on different parts of the same F1 isoline (see Figure 1 c). Without additional context, one might argue that there is no better or worse between these two Als.\nIn order to capitalize on complementary strengths of humans and Al when presented with tradeoffs in AI precision and recall, we need to be able to answer two questions: 1) for a given task, can we clearly identify if either precision or recall is more important than the other, and 2) independent of importance, is it vastly easier or harder for humans to improve either precision or recall.\nConsider for example a pedestrian detection task in a driver assistance system: prioritizing the detection model towards either precision or recall will hurt the other. Human instinct tells us the risk of a missing detection could be lethal, so we should tune the AI system to prioritize recall, i.e., towards a \"zealous\" AI that provides more detections (recommendations), even the low-confidence ones, at the risk of more false positive errors. In this context, the opposite \"restrained\" AI would only provide high-confidence detections and prioritize precision, but at the risk of more false negative errors.\nIn this work, we investigate how a high-recall zealous AI and a high-precision restrained AI can affect human-Al team performance in a real-world scenario. Compared to, say testing pedestrian detec- tors on the road, video anonymization is a similar but easier-to-test recall-demanding task. We set up a face annotation task for personally identifiable information (PII) protection that blurs human faces in a real-world video dataset [23]. PII protection is a critical task with increasing demand for both ethical research and abiding by regulatory requirements\u00b9. Similar to pedestrian detection, where the cost of a missing detection is very high, one unlabeled face in a single frame can reveal a person's identity in the entire video, if not the entire dataset.\nThis paper focuses on the common yet critical human-AI collab- oration setting, in which recall is more important than precision. As for our second question, \"is it vastly easier or harder for humans to improve either precision or recall?\", an in-depth analysis of the video annotation workflow shows that improving recall is more costly than precision in this task since it is much harder for human"}, {"title": "2 RELATED WORK", "content": "Factors affecting human-Al team performance. While human- Al teams have been studied extensively from various perspectives like in crowdsourcing settings [25, 37], computer vision tasks [25, 44, 53], high-stakes tasks [3, 4, 44, 63], and real-world tasks [1, 25, 31, 42, 44, 49, 55], we still have more questions than an- swers on exactly which factors affect team performance and how. Researchers have looked into factors like users' mental models [3, 10], user expectations [28, 58, 62], cognitive biases [45], model updates during collaboration [4], model accuracy [2, 58], model interpretability or explanations [5, 6, 21, 26, 36, 46, 54], as well as the tradeoff between accuracy and interpretability [9]. Study- ing user's trust and appropriate or inappropriate reliance on AI [7, 30, 35, 41, 59, 63] is another important direction.\nThis paper is aligned with works that focused on the tradeoff between precision and recall in AI recommendations and its effect on team performance. Kay et al. [28] introduced the acceptability of accuracy as a new measure and survey instrument to connect classifier evaluation to users' subjective perception of accuracy. Kocielnik et al. [29] compared two 50%-accurate AI-powered sched- uling assistants one avoids false positive errors, and one avoids false negative. This is a similar design as for our restrained and zealous Als - their study found that false positive errors are more acceptable by participants, which corroborates the overall better performance we observed in the zealous Al group, who also dealt with more false positive errors.\nBalancing precision and recall to compare two real-world AI sys- tems in a human-AI collaboration task is not easy, previous works derived insight from hypothetical systems or manually balanced recommendations [28, 29]. In this work, we provide a real-world user study by observing how 78 professional users would interact with two high-performance face tracking AI systems that are tuned to truthfully portray the realistic tradeoff between high-precision and high-recall on a recent egocentric video dataset.\nFace detection. The annotation platform we used has a built-in face detector, RetinaFace [18], integrated for autonomous work- flows. Our literature search found RetinaFace remains a top-ranking method on the WIDER FACE benchmark [57]. Because more recent methods do not provide significant performance improvement, we continue to use RetinaFace as a consistent baseline to compare with our algorithmic improvements in tracking.\nMulti-object tracking. In the AI-assisted face annotation task, the Al teammate provides annotation recommendations for users to re- view. Conventionally a face detector provides per-frame face bound- ing boxes and a multi-object tracking (MOT) algorithm produces continuous tracks of the same object across frames. This is known as tracking-by-detection. Recent MOT methods like TransTrack [48], DETR [8], Deformable DETR [65], TrackFormer [38], and TransMOT [12] etc. all move toward the end-to-end Transformer- based [50] architecture. However, these black-box MOTs share the same drawback as they are designed for fully-autonomous settings. Similar to Caruana et al's observation that modular system pro- vides better transparency [9], the two-part tracking-by-detection frameworks actually provide us the interpretability and flexibility to steer the output recommendations as needed, so we can produce restrained and zealous AI recommendations for comparison. We reviewed state-of-the-art methods in related multi-object tracking benchmarks [15, 39, 51] in search of a multi-object tracker suitable for a human-in-the-loop annotation workflow. ByteTrack [64] is a conventional tracker that outperforms numerous Transformer- based trackers mentioned earlier.\nVideo annotation. While there are various public video annota- tion platforms or tools to choose from [19, 27, 52, 60], we use a proprietary video annotation tool to gain access to professional"}, {"title": "3 ALGORITHM CHOICES AND PILOT STUDIES", "content": "3.1 Precision and recall in multi-object tracking\nPrecision, recall, and F1 are important performance metrics that can describe the characteristics of a model and are central concepts in this work and other human-AI research [28, 29]. Specifically, in the context of annotating and tracking faces with bounding boxes in videos:\nPrecision = $\\frac{TP}{TP + FP} = \\frac{Face boxes correctly drawn}{All boxes drawn by the user (or the AI)}$     (1)\nRecall = $\\frac{TP}{TP + FN} = \\frac{Face boxes correctly drawn}{All ground truth face boxes}$           (2)\nF1 = $\\frac{2 \\cdot precision \\cdot recall}{precision + recall}$                                                                      (3)\nwhere the TPs are true positives, face boxes that were correctly drawn. The FPs are false positives, boxes drawn by the AI or user which did not match real faces properly. The FNs are false negatives, where there is a real face, but the box is missing.\nThe F1 score is the harmonic mean of the precision and recall (Equation 3). We visually introduced the concept of this function using three methods that have the same F1 score in Figure 1 (c). Put simply, a video pre-annotated by a high-recall method (zealous AI) would have more false-positive boxes \u2013 the user will make more rejections but add fewer missing boxes. A video pre-annotated by the high-precision method (restrained AI) would provide mostly correct boxes but the user will need to add more missing boxes.\nWe are interested in how users will perform differently given restrained or zealous Al recommendations in an Al-assisted face annotation task. While it is easy to generate high-precision annota- tions by simply avoiding low-confidence detections, it is hard for trackers to produce high-recall results while maintaining a simi- larly high F1 score at the same time. This motivates us to propose a tracking algorithm that pushes recall to the limit, but aims to main- tain a similar level of F1 score. We take advantage of the fact that our tracking results will be reviewed by human annotators, allowing us to make targeted optimizations. We test our ideas of a user-friendly tracker with professional annotators through pilot studies. Observing how users work with trackers allows us to further improve the algorithm."}, {"title": "3.2 Pilot studies", "content": "We conducted two pilot studies to observe how professional data an- notators work with Al recommendations. Annotators were tasked to draw bounding boxes around potentially moving or blurred faces of any size in a 1,200-frame video sequence of a busy shopping scene in both sessions (similar to hard videos in the formal study). We provided training material on how to review recommendations from the AI for the face annotation job. The annotation tool user interface is shown in Figure 2. With their consent, we recorded their screens to keep track of mouse movements and other user habits. Each session included ten different users with above-average expe- rience. Both pilot studies concluded with a survey about experiment design and their experience. The two pilot sessions were spaced two weeks apart to test algorithm and design improvements.\nUsers' screen recordings helped us observe the following user habits and behaviors that are not possible to be identified solely from the results:\nCertain bad recommendations cost most of the human review efforts. Following the Pareto Principle [24], annotators in fact spent most of their time and effort amending a small fraction of AI recommendations. The tiny bounding boxes (see ex- amples of three tiny faces in Figure 2), duplicate detections (often clustered), and temporally sparse detections (short tracks) are the most costly recommendations. Addressing these issues allows annotators to have better continuity in their workflow.\nModel explanation should not increase task complexity. Ini- tially, we offered model explanations using \"Certain\" and \"Uncertain\" labels based on the face detector's confidence, hoping this can assist users' decision-making. But video recordings and user feedback revealed that the extra infor- mation in fact increased the task complexity and caused unnecessary confusion. This design was eventually not con- sidered in the formal experiment.\nObserving how human annotators review AI recommendations (bounding box pre-annotations) in multi-object detection and track- ing tasks inspired us to break the complex workflow into three fundamental user actions: accept, reject, or solve, each coming with a higher cost in time. Figure 3 explains each action's time complexity. We can connect these three actions with our two main objectives (time and recall) to make a simple deduction to iden- tify the human-Al complementary strengths in this task:\n1 reject improves precision and solve improves recall. A cor- rect accept improves both.\n2 It takes the AI constant time to solve additional cases (give more recommendations) with a downside of more false- positive boxes for humans to reject.\n3 Humans are faster at rejecting a false-positive (incorrect) box than to solve a false-negative (missing) box.\n4 We also know recall is more important than precision in video anonymization tasks.\n5 Thus, a clear path to better human-AI team perfor- mance is to delegate more solve actions to the AI, so the human's overall effort is reduced by doing more easy rejecting and only solving the most challenging faces."}, {"title": "3.3 The false-positive-robust (FPR) tracker", "content": "We adopted a tracking-by-detection system to produce face pre- annotations (Section 2), the two-part system design allows us to feed the same per-frame face detection from RetinaFace [18] to different downstream multi-object trackers like the ByteTrack [64] or our own designs for a fair comparison. Learning from our pilot studies observations, we propose the false-positive-robust (FPR) tracker that specifically provides user-friendly annotation recom- mendations. We use the following unconventional strategies to design the FPR tracker that can take overwhelmingly noisy de- tections with a high false positive rate as input but outputs \"clean\" tracks for a human-in-the-loop workflow:\nTo improve the Al's recall, we apply an extremely low threshold (t \u2265 0.01, t \u2208 [0, 1]) on the face detector's con- fidence score to keep any potentially useful detected boxes. This is not a viable solution for Autonomous Al systems but we are working in conjunction with a human.\nThe consequence of such a low face detector threshold is clusters of overlapping boxes on small faces. Our solution: for each cluster, we perform non-maximum suppression [43] by only keeping the single bounding box with the highest confidence score because in most cases they are duplicate detections on one true face. This step also improves the AI recommendations' precision.\nFinally, based on our observation that the majority of tem- porally sparse detections are false positives induced by the low threshold, we remove any tracks that are shorter than m consecutive frames so they do not interrupt users' continuity. We used m = 10 in the FPR tracker. Although some true-positive faces are also removed, users are much faster at solving an unlabeled face from scratch than filling the gaps between temporally sparse detections.\nTo design the experiment, we also need a restrained AI that generates recommendations of similar performance (F1 score) but with high precision. This is done by using only the high-confidence (t \u2265 0.8, t \u2208 [0, 1]) face detections with ByteTrack. To ensure fair comparison and reduce moving parts in our systems, we use the same face detection model RetinaFace [18] for both AI teammates. It is the two different (fully transparent) trackers we apply that push the Al recommendations towards either high-precision or high-recall (Figure 2).\nNote that we were only able to optimize the FPR tracker and ByteTrack through pilot studies because the ground truth data was not available for the 36 videos used in the user study. After the study, we aggregated the annotations from all 78 participants (2,780 submissions in total) to form an expert-reviewed consensus to serve as the ground truth. It turns out the zealous AI recommendations (FPR tracker) yielded an F1 score of 90.9% and the restrained AI (ByteTrack) had an F1 score of 93.4%. While the two Als did not pro- vide identical initial performance for their human teammates, we achieved the goal of two distinctive high-recall and high-precision Als (Figure 5). The performance gap also provided us additional evi- dence to support our previous deduction on the zealous AI being the superior choice for this task, which we will discuss in Section 5.1."}, {"title": "4 EXPERIMENTS", "content": "In this work, we aim to investigate how restrained and zealous AI recommendations will affect human-AI team performance. We are also curious if the collaboration experience with an AI teammate can affect users' skills, should they lose access to Al assistance in the future. We design a two-part empirical study to test the restrained and zealous Als in a recall-demanding high-stakes task."}, {"title": "4.1 The task and data.", "content": "Face annotation for video anonymization is a perfect example of recall-demanding tasks a missing face in a single frame can re- veal a person's identity in the entire video. The high-stakes nature requires humans to annotate or verify every frame, yet the man- ual process will become the throughput bottleneck. The tedious process and long hours may also fatigue annotators and cause a decline in quality. In addition, because the task of locating faces requires no specific training or domain expertise, it should help the generalizability of our observations to other AI-assisted annotation tasks or even to other recall-demanding human-Al collaboration tasks.\nIn our human-AI collaboration setting, the AI teammate provides recommendations in the form of bounding boxes (see examples in Figure 2), and a user reviews each of the Al's pre-annotations to make one of the three decisions shown in Figure 3. We evaluate users' performance on the two most important metrics for face anonymization: task completion time and recall.\nTo test different AI recommendations in a real-world setting, we curate 36 first-person videos from a large-scale egocentric video dataset Ego4D [23]. Privacy has always been a major concern for datasets collecting human activities so first-person videos are ideal for this study. The videos we selected include various indoor social activities that are suitable for benchmarking face detection and annotation tasks. Each video clip is 30 seconds long, or 900 frames. We estimate each video takes about 30 minutes to one hour to fully annotate, depending on its difficulty.\nThe different annotation methods (without or with different AI recommendations) adopted by the three treatment groups are the first level of independent variables that we will discuss in the next section. The second level of independent variables that can affect users' performance is the difficulty of the videos. We divide the videos into Easy, Medium, and Hard categories based on the average number of people one needs to track simultaneously in non-empty frames (see examples in Figure 4). We also considered factors like scene diversity, bounding box size, and camera movement intensity that affect the annotation difficulty in a more subtle way. Based on this overall difficulty ranking distribution, we ensure Part 1 and Part 2 videos are not only similar in content but also consistent in annotation difficulty.\nWe generate the bounding box ground truth by aggregating the crowd's annotations to reach a consensus, which is further reviewed and refined by a domain expert. We used an equal number of manual and AI-assisted submissions for each video to generate an unbiased ground truth.\nOn task completion time, annotators are advised to finish each video without taking breaks longer than five minutes but we still need to reject outlier video completion times caused by a known limitation of the annotation tool - the timer continues if an ongoing task window was left idle, or the timer will reset if the annotator continues from previously saved progress. We adopted median absolute deviation (MAD) [32] by comparing each video's completion time within each group to reject 420 out of 2780 (15.11%) completed videos, including completion times that are less than six minutes (the minimum time needed to verify each frame) or longer than median + 3* MAD. The rejected videos also include all 36 submissions from one particular problematic user, see Section 6.2."}, {"title": "4.2 Participants and three treatment groups.", "content": "A total of 78 in-house professional data annotators completed our study. It is important to note that in this project they are paid at their regular hourly rate, so participants are not motivated by compensation to work faster.\nIn the between-subjects experiment, participants were evenly split into three 26-people treatment groups to annotate identical sets"}, {"title": "4.3 Experiment procedure of the two-part study.", "content": "Before beginning the study, we organized a video conference train- ing session with each treatment group to calibrate the task back- ground and requirements. All participants were also asked to review the instruction text and a training video on the landing page. Previ- ous pilot study users become supervisors in each group to ensure all participants have finished the training and the surveys before processing to the next step. We also created three instant messag- ing (IM) groups to answer questions and send out reminders when necessary. The overall procedure can be summarized as follows:\nTraining \u2192 Survey 0 \u2192\nPart 1 (24 videos, different methods) \u2192 Survey 1 \u2192\nPart 2 (12 videos, same method) \u2192 Survey 2\nIn Part 1, all participants from Groups A, B, and C each annotated 24 videos using different methods. For each annotator, the videos were assigned in random order by the annotation platform. We also reminded all participants to avoid taking breaks longer than five minutes before finishing a video, so the timing is more accurate. Depending on the method and individual pace, it took all groups"}, {"title": "5 RESULTS", "content": "In this section, we present our study results and analysis by an- swering each research question presented in Section 1. For statisti- cal analysis, we ran one-way ANOVA or one-way Welch ANOVA tests, depending on the underlying assumptions being satisfied, fol- lowed by Pairwise Tukey-HSD or Games-Howell post-hoc tests, re- spectively. To examine interactions between factors, we conducted two-way ANOVAs followed by Pairwise Tukey-HSD or Bonferroni- corrected post-hoc tests. We adopted Type III sums of squares in ANOVA to address unbalanced data.\nResearch questions Q1, Q2, and Q3 focus on results from Part 1 of the study (Figures 5, 6, 8, and 10a), in which Groups B and C collaborated with restrained and zealous Als. Question Q4 focuses on results from Part 2 (Figures 7, 9, and 10b) to examine how the prior human-AI collaboration experience could affect the users."}, {"title": "5.1 Q1: Can the human-AI teams achieve\n\"complementary team performance\" in this task?", "content": "Bansal et al.[5] defines complementary team performance as the human-Al team performance exceeding both the human-only and AI-only performance.\nFigure 5 shows the two human-AI teams B & C reached compara- ble F1 scores of 96.9% & 96.8%, respectively, significantly better than the human-only Group A that reached 94.5% (Welch F2,1151 = 18.2, p < 0.0001). Both human-AI teams improved F1 accuracy and recall significantly compared to their human-only counterpart.\nBecause the high-stakes nature of this task rules out autonomous AI as a viable option, we really only need to compare the human-AI team performance with human-only performance in Part 1 of our study. However, to verify complementary team performance, we also verify that the two human-AI teams achieved higher perfor- mance in terms of F1 scores and recall than their respective Al's initial standalone performance.\nComparing each human-Al team with their perspective AI team- mates' initial performance - Group B annotators improved the restrained Al from 93.4% to 96.9% (Welch F1,1228 = 178, p < 0.0001), Group C annotators improved the zealous AI from 90.9% to 96.8% (Welch F1,837 = 169, p < 0.0001). Both human-Al teams improved significantly from their respective AI teammate's solo performance.\nIt is understandable that Bansal et al. only considered accuracy and did not compare task completion time in complementary per- formance, since the human-AI teamwork will undoubtedly add more time than Al alone. As we discussed, task completion times directly affect the operation cost as people are paid at an hourly rate, making it a critical metric for annotation tasks, so we addi- tionally compare the human-Al teams' task completion times with the human-only team.\nWe saw overall significant differences between all three groups on task completion time (Welch F2,1039 = 48.6, p < 0.0001), as shown in Figure 6, left. As a baseline, on average it took 1.05 hours for Group A to manually annotate a 30-second video of 900 frames. Group B took a significantly shorter time of 0.91 hours (Games- Howell p < 0.001) to review the restrained AI recommendations. Group C only used 0.73 hours to review zealous Al's recommen- dations, also significantly shorter than the human-only Group A (Games-Howell p < 0.0001).\nIt is also worth noting that Group C, the zealous human-AI team, had an overall significantly worse starting point than Group B in terms of F1 score: 90.9% vs. 93.4% (Welch F1,854 = 35.32, p < 0.0001) as shown in Figure 5. However, annotators working with the zealous Al managed to achieve a significantly higher improvement in F1 score of +5.9% vs. +3.5% (Welch F1,934 = 45.02, p < 0.0001) in significantly less time! This disadvantage for Group C provided the opportunity to demonstrate that our deduction in Section 3.3 was correct a human-Al team can do better in both time and quality (in terms of F1 improvement) by asking the human to reject more false positives and only solve the most challenging faces, i.e., the high-recall zealous AI.\nIn summary, we have not only verified complementary team performance on accuracy, but also showed human-AI teams could achieve significantly shorter task completions time in a real-world case study."}, {"title": "5.2 Q2: Which Al helps annotators be more\nefficient, i.e. save time?", "content": "We mentioned that the professional annotators are paid at their fixed hourly rate in this task, which means 1) they are not necessarily motivated to work faster, and 2) from the business perspective, their task completion time directly impacts operation costs. We discussed in Section 5.1 that overall, both human-AI teams have significantly shortened task completion time compared to the baseline Group A (Figure 6 left). Specifically, the zealous Al recommendations help annotators use 20% less time than the restrained Al recommendations with statistical significance (0.73 hours vs. 0.91 hours, Games-Howell p < 0.0001).\nVideo difficulty. Figure 6 (middle) plots task time by video diffi- culty and saw a significant interaction between group and video dif- ficulty on task completion time (ANOVA F4,1577 = 5.37, p < 0.0001, n = 0.016, small). Specifically, Group C which reviewed zealous AI recommendations used significantly less time than both Group A and B in medium videos (Bonferroni p < 0.0001 & p < 0.0001), as well as in hard videos (Bonferroni p < 0.0001 & p < 0.01). But no significant difference was found for easy videos among the three groups.\nThis observation matches very well with our expectations to different video difficulties: the built-in linear interpolation tool for manual annotation is very efficient in tracking a single face con- tinuously, but AI recommendations can dramatically reduce task time when tracking multiple faces simultaneously in medium and hard videos. This finding allows the system designer to optimize efficiency further: if we know a certain portion of the data has one or fewer people in each frame, it would be reasonable to bypass the AI pre-annotation to save on the GPU budget.\nUser expertise. When solely considering the user expertise factor, we were surprised that veteran workers are overall significantly slower than novice workers in both parts of the study (Welch, Part 1: F1,1380 85.6, p < 0.0001, Part 2: F1,665 = 22.2, p < 0.0001)! However, if we consider how people are paid, this result would be a reasonable optimization given the incentives \u2013 veteran work- ers know the acceptable work pace, so they do not need to work"}, {"title": "5.3 Q3: Which Al helps annotators achieve\nhigher recall?", "content": "From the F1 scores in Figure 5 we know that both AI-assisted meth- ods yield significantly higher-quality annotations than the baseline method (compared in Section 5.1), yet we saw no clear winner between the two human-Al teams. Because recall is paramount in video anonymization tasks, we analyze Group B and C's recall performance in detail.\nFigure 8 shows that Group C, the annotators who reviewed zeal- ous Al recommendations, have an overall significant advantage over Group B, which reviewed restrained AI recommendations (Games- Howell p < 0.01). Interestingly, we noticed a visible shorter tail in Group C's recall distribution in hard videos (Figure 8, right). This observation matches the very nature of zealous AI - giving more recommendations, even low-confidence ones, so the human team- mate is less likely to miss a face. This strategy is especially effective in hard videos because tracking too many faces simultaneously pushes the user's attention to its limit. Zealous Al's superfluous recommendations allow the user to focus on the action of reject, rather than searching for missing faces and then solve.\nTaking user expertise into account, Figure 10 (a) reveals that while both Als improved the veterans' recall performance com- pared to the baseline Group A (Bonferroni A/B: p < 0.0001, A/C p < 0.0001), for novice workers, we only saw a significant advan- tage of Group C over Group A (Bonferroni p < 0.048). It corrobo- rates our previous finding on completion time that \"the zealous AI can consistently improve both novice and veteran annotators\" and extends the statement to higher recalls percentages as well."}, {"title": "5.4 Q4: Will collaborating with an AI improve\nor hurt user skills?", "content": "Should the annotators lose access to their Al teammates in the fu- ture, how will they perform? While we are interested in improving human-Al team performance, we should also seriously consider how the prior human-AI collaboration experience would affect people's skills in the long run before deploying a new system. To find out, we removed AI recommendations from Groups B and C in Part 2, so all groups now work with the manual tool that they have always been using for other projects. It took most annotators two to three weeks to complete Part 1 of the study. For the sake of interpreting the results of Part 2, we can consider this period a training period and their performance in Part 2 showcasing the effect of this medium-term training effort.\nBoth Groups B & C collaborated with their perspective AI team- mates for 2-3 weeks, but the restrained-AI-trained annotators in Group B performed worse than their peers in different ways the novice workers were significantly slower than both A & C, especially in hard videos. The veteran workers' annotations had lower recall percentages than the zealous-AI-trained workers in Group C.\nCompletion time. Figure 7 shows the task completion time of Part 2's 12 new videos without Al recommendations. In all video difficulties, Group C, annotators who previously worked with the zealous AI in Part 1, managed to finish as quickly as Group A, the annotators who were trained using the very manual method now in deployment for all groups. It shows that training with zealous AI recommendations does not negatively affect users' task completion time on subsequent manual tasks.\nHowever, we were surprised to see that Group B annotators trained with the restrained AI became overall significantly slower than Groups A & C (Tukey-HSD A/B: p < 0.021, B/C: p < 0.01), and more specifically in hard videos (Bonferroni A/B: p < 0.044, B/C: p < 0.013). Figure 7 (right) shows that the effects stem mainly from the novice users (Bonferroni A/B: p < 0.0001, B/C: p < 0.01)."}, {"title": "6 DISCUSSION", "content": "6.1 The key to forming a strong human-Al team\nWe propose the restrained AI and the zealous AI to depict the tradeoff between precision and recall as two characteristics that have the potential of becoming advantages in human-AI teams if used properly. By actually using the annotation tools and watching annotators' screens for many hours, we observed that annotators need much less effort in improving precision than recall in a model- assisted annotation task, i.e., rejecting an incorrect box is much"}, {"title": "6.2 Can Al teammates set the quality lower\nbound in a crowdsourcing setting?", "content": "We identified and rejected a single veteran user who submitted the majority of the low-quality annotations. This is an unexpected yet not surprising finding in a crowdsourcing setting: when paid at a flat hourly rate, people are not necessarily motivated to work faster. When lacking a quality-based performance evaluation mech- anism, people are not necessarily motivated to push for \"better- than-sufficient\" quality.\nHowever, could there be other users not making an effort in Groups B or C as well but not being identified? Because the two Als have pre-annotated the videos in decent quality (F1 > 90%), it's hard to tell if someone is actually happy with the Al's recommendations or is not pushing for even better quality.\nWhat we know for sure is that such low-quality submissions, intentional or unintentional, will certainly appear in other real- world crowdsourcing tasks. However, in absence of ground truth, we won't be able to identify them in a real-world setting. It is also very costly to identify bad submissions - ImageNet asks 10 votes for each image [17", "33": "."}]}