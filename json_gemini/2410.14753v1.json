{"title": "Collaboratively adding new knowledge to an LLM", "authors": ["Rhui Dih Lee", "Laura Wynter"], "abstract": "We address the question of how to successively add new knowledge to an LLM\nwhilst retaining previously-added knowledge. We consider two settings, semi-\ncooperative and fully-cooperative. Overall, LoRA performs better in most cases\nthan full-fine tuning of all parameters when both new knowledge acquisition and\nretention of old, including recent, knowledge are taken into account. In the semi-\ncooperative setting, where datasets are not available after training, MOE mixing,\nmodel merging, and LoRA-based orthogonal subspace sequential learning, using\na small weight on the orthogonality term, perform well. In the fully-cooperative\nsetting where datasets remain available, joint training and sequential training with\nreplay are both effective approaches with LoRA training generally preferable to\nfull fine-tuning. The codes needed to reproduce the results are provided in an open\nsource repository.", "sections": [{"title": "Introduction", "content": "With the availability of open weight large language models (LLM) such as the popular llama\nmodels Touvron et al. [2023], along with easy-to-use libraries for fine-tuning such as Hugging Face\nTransformers Wolf et al. [2020], Face, training an LLM on one's own data has become a standard\npractice. Typically, each use case results in a different fine-tuning of the source model, resulting in\nmany tuned models obtained from the same base model.\nLLM tuning aims to give new knowledge or skills to the LLM. The LLM should not, however, lose\nits ability to perform well on the tasks it was previously trained for, including tasks pertaining to\nthe the general knowledge from pre-training and skills like instruction following. This tug-of-war is\nfurther amplified when the LLM is successively subjected to more tuning, over time. While one can\nreturn to the base model for each successive tuning, users would like to be able to leverage on recent\nknowledge from previous tuning, and further tune from there.\nThe options available for tuning the base model are essentially limited to full fine-tuning (FFT) the\nmodel or using Parameter-Efficient Fine Tuning (PEFT). In the former case, the user is left with\nmultiple copies of a potentially large LLM, each tailored to a different use case. In the latter case,\nthe user can maintain one copy of the base model and a set of smaller adapters, e.g. LoRA adapters,\none for each use case. The choice as to whether to use FFT or PEFT in this setting then resolves\naround the quality of the tuned models obtained versus the computational resources needed to store\nthe resulting models.\nIf, however, the user wishes to reap benefits from recently-acquired knowledge, in addition to the new\nknowledge that the user will impart in training the model, then further options become available. A\nterminology used in the pre-LLM era for this is \"continual learning\". In continual learning, a model\nis sequentially subjected to training on new data, over time, without reverting back to the original\nbase model.\nThe risk inherent in subjecting a base model to sequential training on different domains is that the\nold knowledge from pre-training the model, as well as the recently-acquired knowledge from earlier"}, {"title": "Related Work", "content": "Full fine-tuning a large language model (LLM) involves updating potentially all of its parameters.\nThe seminal work of Hu et al. [2021] introduced LoRA, i.e., Low-Rank Adaptation, a form of\nparameter-efficient fine-tuning (PEFT), which demonstrated that remarkably good results can be\nachieved while training only a very small number of parameters associated with the linear layers of\nthe LLM. The success of LoRA paved the way for more efficient training of LLMs.\nIn parallel, however, the development and widespread use of LoRA also led to questions about\nwhether fine-tuning such a small number of parameters from the base model, instead of all the\nparameters, would help to reduce the degradation of old knowledge that can arise from training. In\nBiderman et al. [2024], the authors discuss that question in detail. They conclude that LoRA results\nin less knowledge acquisition than full fine-tuning (FFT), but that LoRA better retains the base model\nperformance from old knowledge.\nThe authors of Unsloth, however, claim that the LoRA results in the paper Biderman et al. [2024]\nwere not as good as they could be due to the choice of hyper-parameters used. Specifically, they claim\nthat the implementation of LoRA used in Biderman et al. [2024], which only trained the attention,\nand up and down matrices, and not the gate projection matrix, performed poorly because the latter\nis important in obtaining good results. Secondly, they claim that the LoRA training should have\nincluded the lm_head and embed_tokens to take into account domain data distribution shift. They\nstate, however, that one must use a smaller learning rate for the lm_head and embed_tokens, and show\nthat training those layers is very sensitive and can result in worse performance in some cases. They\nalso point out that the LoRA rank used in the code tests was $r = 256$ with an $\\alpha$ of 32, whereas for the\nmath tests, $r = 256$ and $\\alpha = 2r$. They claim that the higher $\\alpha$ in the presence of large rank explains\nthe better math results and should also have been used in the code tests. The results of Biderman et al.\n[2024], Unsloth are important in assessing the benefits and disadvantages of full fine-tuning versus\nLORA PEFT. However, we are concerned with how to successively add new knowledge to a model\nwhile benefiting from the knowledge added already by earlier fine-tuning. In this context either FFT\nor PEFT can be used.\nA suite of datasets for training and testing was provided in Wang et al. [2023b]. The authors analyse\nmodel performance across numerous models looking primarily at sequential FFT, sequential LoRA\ntraining, and sequential FFT with replay. However, the results provided in the paper are mixed and\nsomewhat conflicting.\nFinally, we mention a related work on LoRA training for successive tasks which seeks to drive\nthe adapter to be orthogonal to previously-trained adapters, called o-lora Wang et al. [2023a]. We\nexamine whether this technique is of value in the setting where a user does not have access to datasets\nused in previous training."}, {"title": "Methods", "content": "We consider both full fine tuning, or FFT, and LoRA tuning. For sequential FFT, the previous training\ncheckpoint must be available. For sequential LoRA, the previous adapters, along with the full base\nmodel, must be available.\nIn the semi-collaborative setting, datasets from previous training are not available. Sequential training,\nwhether FFT or LoRA, can be performed. Orthogonal-subspace o-lora Wang et al. [2023a] is a\nfeasible method as well, whereby successive LoRA adapters are encouraged to be orthogonal to\npreceding adapters by modifying the loss function to be:\n$\\sum_{x,y \\in D_t} \\log p_\\theta(y|x) + \\lambda \\sum_{i=1}^{t-1} L_{orth}(A_i, A_t)$ (1)\nwhere\n$L_{orth} (A_i, A_t) = \\sum_{j,k} ||O_{i,t} [j, k]||^2,$ (2)\nsuch that $O_{i,t} = A_i^T A_t = 0$ where $O_{i,t}[j, k]$ denotes the element at the jth row and kth column of\n$O_{i,t}$. This method aims to reduce interference between successive LoRA training, postulated to be\nuseful when the tasks of each training are quite different from each other. Note that the orthogonality\nterm $L_{orth}$ in the loss function acts as a soft constraint, and modifies the optimization of the CE loss.\nAs such, we expect that the learning ability of the model trained using o-lora will be reduced in order\nto achieve the reduced interference with previously-trained adapters.\nIn addition, two other options are available which do not require access to the datasets used in training\nthe previous models: namely, model merging and MOE model mixing. Model merging refers to\naveraging the parameters of the model, and a set of methods that extends simple averaging to handle\nvarious cases including parameters with opposite signs. The reader may refer to the survey paper\nYang et al. [2024] on model merging for more details. In this work, we consider simple parameter\naveraging as well as the merging method TIES and a combination of TIES Yadav et al. [2023] and\nDARE Yu et al. [2024a].\nMOE model mixing is yet a different method to combine already-trained models without requiring\naccess to the source data used for training them. Model mixing takes one trained model as the base,\nand several other trained models as experts. The experts contribute the FFN layers (and potentially\nalso the attention layers) in a parallel manner and routers are added to route tokens to a subset of the\nexpert modules. The architecture is the same as that of Mixtral Jiang et al. [2024] without requiring\ntraining of the expert modules or the base. Similarly, a Mixture of Experts (MOE) can consist of a set\nof trained LORA adapters with a router used to route tokens to a subset of the expert LoRA modules.\nThe authors of Lee et al. [2024] propose several variants for MOE model mixing, some of which\nrequire datasets for training the routers, and others which do not, and enable both FFT and LORA\nversions of the MOE. We also consider both FFT and LORA MOE here. Note that the datasets for\ntraining the routers need not be the same as those used in training the models. We use the open source\nrepository https://github.ibm.com/Rhui-Dih-Lee/moetify for both the FFT and LORA MOE.\nA distinction to note between the FFT and LoRA MOE is that the FFT MOE becomes large as the\nnumber of experts increases. Although inference time remains nearly constant as the number of\nexperts increases since a \"top-K\" routing policy is generally used, the memory required to store the\nmodel increases with number of experts. In our experiments, for this reason, we are limited to a\nFFT MOE of 4 (out of the possible 7) experts due to memory requirements of including more expert\nmodules, where as the LORA MOE uses all of the expert modules.\nIn the fully-collaborative setting, all of the datasets used previously for training the model remain\navailable. In this setting, we consider joint training as an alternative to sequential training. Finally\nfor the fully-collaborative setting, we consider sequential FFT and sequential LoRA training with\nreplay in addition to standard sequential training. Replay is defined as adding a percentage, in our\ncase, 10%, of all previous task data into the current training. Note then that the dataset grows with\neach successive training. Note also that orthogonal subspace learning, o-lora, is no longer appropriate\nsince previous tasks' dataset are used in joint and sequential with replay training."}, {"title": "Experimental Results", "content": "We make use of the TRACE Wang et al. [2023b] data as part of our experimental setup. The TRACE\nrepository includes 8 tasks, each of which has a 5K-example dataset for training and a 2K-example\ndataset for testing. The tasks include ScienceQA, which comprises elementary and high school\nscience question-answer pairs, FOMC, a financial hawkish-dovish classification task, MeetingBank,\nis a city council meeting summarization task, and Py150 a python code-completion task. It also\nincludes two multi-lingual tasks, namely cstance, a Chinese-language zero-shot stance detection task\nfrom Chinese social media postings, and 20Minuten, a German news text simplification task.\nThe TRACE dataset also includes two math tasks that we do not use, making use instead of MetaMath\nYu et al. [2024b], a much larger 395K-example dataset for math problem solving, and GSM8K Cobbe\net al. [2021] for testing math solving-ability. Hence we consider in total 7 tasks, 6 from TRACE and\na math task using a much larger dataset than those in TRACE.\nWe also make use of evaluation tests from the LM Eval Harness Gao et al. [2024], which includes\ntests such as MMLU, that reflect the ability of the model to perform well on old knowledge, including\nthe knowledge from pre-training. Experiments are performed on one node with 8 A100-80GB GPU.\nWe use llama3 (meta-llama/Meta-Llama-3-8B) Meta for all tests reported here. The experiments\ncan be reproduced using the code provided in this repository."}, {"title": "One-Round Model Training", "content": "We first consider individual, or one-round, training. In this case, all training starts from the base\nmodel. Figures 1 illustrates the quality of the output after one-round training on the new knowledge\nthe model is trained with. Observe that science question-answer and meeting summarization perform\nbetter when the model is fully fine-tuned. On the other hand, on cstance, the Chinese task, and python\ncode completion the LoRA training performed better. The differences, however, in new knowledge\nacquisition are not very large between FFT and LoRA training, as shown in the last set of bars on the\nright which provide the average performance.\nOn the other hand, as shown in Figure 2, old knowledge retention suffers considerably when the\ntraining modifies potentially all parameters using full fine-tuning. We take each of the one-round-trained models and assess old knowledge retention through the LM eval harness Gao et al. [2024]\nsuite. Then, we average the results on a per-test basis over the one-round-trained models. We see\nold knowledge degradation notably on the tasks MMLU and CommonsenseQA which degrade more\nafter FFT than after LoRA training. The final set of bars provides an average over the tests of the\nmodel-average scores."}, {"title": "Semi-Collaborative Model Training", "content": "Turning next to semi-collaborative, sequential training. In this case, the datasets of previous training\nare not available to users, but the trained models and LoRA adapters remain available. As such, the\nuser can start with the checkpoint from the previously-trained model and train from that tuned model\ninstead of the base model.\nIt is clear that the order of training will have an impact on the resulting performance at any point in the\nchain of successive training. However, in practice, one may not be able to choose which checkpoint\nto start from and even if one could choose, the number of choices would become unwieldy as the\nnumber of successive training increases. For this reason, we do not explore the impact of task-order\non the quality of the output. Instead, we maintain the same order as the authors of the TRACE dataset\nWang et al. [2023b] and keep the order constant. The order of training is thus cstance, fomc, meeting,\npy150, science, 20minuten, and finally the very large math training dataset MetaMath.\nWe first examine the quality of old knowledge retention with successive training over the 7 datasets\nand tasks. Figure 3 summarises the results after all 7 rounds of training are complete. As we saw\nin one-round training, FFT, after successive training, experiences clearly more degradation of old\nknowledge than LoRA training."}, {"title": "Fully-Cooperative Model Training", "content": "In the fully-cooperative setting, one has access to the datasets used in training the already-trained\nmodels. Thus, joint training over all datasets, and sequential training with replay, in which a small"}, {"title": "Conclusion", "content": "We analyze collaboratively adding new knowledge to a large language model. In the semi-cooperative\nsetting where datasets are not available after training, MOE mixing, model merging, and LoRA-based\northogonal subspace sequential learning perform well. It is preferable to use a small weight on the\northogonality term in the loss function to mitigate the reduction in CE loss optimization that comes\nfrom the presence of that term. MOE model mixing can mix trained LoRA adapters or FFT networks.\nBoth perform very well but LoRA MOE Mixed models remain of modest size while FFT MOE\nMixed models are limited by the considerable memory requirement as the number of experts grows.\nLORA overall performs better in most cases than full-fine tuning of all parameters when both new"}, {"title": "Appendix", "content": "We provide additional results here. In Figure 9 we examine the final gain from the base model after 7\nrounds of training in the semi-cooperative setting in terms of new and recently-acquired knowledge.\nThe analogous results on old knowledge are provided in Figure 10."}]}