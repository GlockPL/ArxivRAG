{"title": "Technology as uncharted territory: Contextual integrity and the notion of AI as new ethical ground", "authors": ["Alexander M. Mussgnug"], "abstract": "Recent research illustrates how AI can be developed and deployed in a manner detached from the concrete social context of application. By abstracting from the contexts of AI application, practitioners also disengage from the distinct normative structures that govern them. Building upon Helen Nissenbaum's framework of contextual integrity, I illustrate how disregard for contextual norms can threaten the integrity of a context with often decisive ethical implications. I argue that efforts to promote responsible and ethical AI can inadvertently contribute to and seemingly legitimize this disregard for established contextual norms. Echoing a persistent undercurrent in technology ethics of understanding emerging technologies as uncharted moral territory, certain approaches to AI ethics can promote a notion of AI as a novel and distinct realm for ethical deliberation, norm setting, and virtue cultivation. This narrative of AI as new ethical ground, however, can come at the expense of practitioners, policymakers and ethicists engaging with already established norms and virtues that were gradually cultivated to promote successful and responsible practice within concrete social contexts. In response, I question the current narrow prioritization in Al ethics of moral innovation over moral preservation. Engaging also with emerging foundation models, I advocate for a moderately conservative approach to the ethics of AI that prioritizes the responsible and considered integration of AI within established social contexts and their respective normative structures.", "sections": [{"title": "I Introduction", "content": "AI is employed in a wide range of contexts - scientists leverage AI in their research, media outlets use AI in journalism, and doctors adopt AI in their diagnostic practice. Yet many have noted how AI systems are often developed and deployed in a manner that prioritizes abstract technical considerations which are disconnected from the concrete context of application. This also results in limited engagement with established norms that govern these contexts. When AI applications disregard entrenched norms, they can threaten the integrity of social contexts with often disastrous consequences. For example, medical AI applications can defy domain-specific privacy expectations by selling sensitive patient data, AI predictions can corrupt scientific reliability by undermining disciplinary evidential norms, and AI-generated journalism can erode already limited public trust in news outlets by skipping journalistic best practices.\nThis paper argues that efforts to promote responsible and ethical AI can inadvertently contribute to and seemingly legitimize this disregard for established contextual norms. Echoing a persistent undercurrent in technology ethics of understanding emerging technologies as uncharted moral territory, certain approaches to AI ethics can promote a notion of AI as a novel and distinct realm for ethical deliberation, norm setting, and virtue cultivation. This narrative of AI as new ethical ground, however, can come at the expense of practitioners, policymakers, and ethicists engaging with already established norms and virtues that were gradually cultivated to promote successful and responsible practice within concrete social contexts. In response, this paper questions the current prioritization in AI ethics of moral innovation over moral conservation.\nMy argument proceeds in four parts. Building upon Helen Nissenbaum's framework of contextual integrity, section two illustrates how AI practitioners' disregard for cultivated contextual norms can threaten the very integrity of contexts such as mental health care or international development. Section three outlines how a tendency to understand novel technologies as uncharted ethical territory exacerbates this dynamic by playing into and seemingly legitimizing disregard for contextual norms. In"}, {"title": "II AI Practice and Contextual Integrity", "content": "Several scholars have noted how AI development and deployment can take place in a manner detached from the concrete domain of application. Birhane et al. (2022), for instance, have surveyed influential machine learning conference publications and shown how abstract technical considerations often dominate over the social and ethical dimensions of applications. Mussgnug (2022) has outlined how approaching supervised machine learning generically as a class of \"prediction problems\u201d can circumvent engagement with context-specific considerations. The abstractive practices of AI development are also the motivating concern for sociotechnical scholarship on AI. Sociotechnical research problematizes how AI systems are often narrowly conceived in technical terms without acknowledgement of how AI operates through the complex entanglement of social and technological aspects (Kudina & van de Poel, 2024; Selbst et al., 2019).\nThis section focuses on how abstracting from the concrete domains of application also leads practitioners to disengage from the contextual norms that govern those domains. This disregard for contextual norms can threaten the very integrity of a context with decisive ethical implications. In exploring this dynamic, I draw from Helen Nissenbaum's influential framework of contextual integrity, originally proposed to make sense of privacy debates following the popularization of internet services.\nNissenbaum contends that existing accounts of privacy (in particular the distinction between public and private information) fail to track people's expectations and concerns. In response, she proposes the notion of contextual integrity, not as a definition of privacy but as a framework to help us understand the source of public objection to emerging informational practices and as an initial yardstick for their potential legitimacy. Her approach rests upon the observation that \u201cfinely calibrated systems of social norms, or rules, govern the flow of personal information in distinct social contexts\u201d (Nissenbaum, 2009, p. 3). These contextual informational norms have been cultivated over time and in ways sensitive to the particularities of the context in order to \"define and sustain essential activities and key relationships and interests, protect people and groups against harm, and balance the distribution of power\u201d (ibid, p.3). Where information technologies defy these established informational norms, Nissenbaum speaks of a prima facie violation of contextual integrity.\nA prima facie violation of contextual integrity is not sufficient for something to be deemed morally objectionable. Entrenched norms can be misguided or harmful and the implementation of novel technologies in an existing context might call us to question them. Sometimes we might employ new technologies precisely with the intention to overhaul existing moral customs. Nissenbaum concedes that we might challenge entrenched norms in favor of nonconforming practices, \u201cwhen the latter are shown more effective in supporting or promoting respective [context-based] values, ends, and purposes.\u201d (2009, p. 181). At the same time, the framework of contextual integrity relies on a measured conservatism grounded in an acknowledgement that established informational norms often reflect the cultivated"}, {"title": "II.1 Mental Health Care", "content": "Mental health care is governed by a particularly rich set of context-specific norms. Many of those norms are inherited from clinical care and biomedical research more generally. These include those codified in law, such as informational norms pertaining to the treatment and storage of \"protected health information\u201d (HIPAA, Pub. L. No. 104-191) or ethical principles laid out in official documents, such as the Nuremberg Code (US v. Brandt, 1947) or the Belmont Report (1978). These principles were developed in response to egregious abuse in German concentration camps or during the Tuskegee Syphilis Study and provide a standard against which practitioners today can be held responsible. In addition, professional organizations such as the American Psychological Association (2017) or the Royal College of Psychiatrists (2014, 2017) have adopted designated codes of ethics, which attempt to outline the values, virtues, and best practices of mental health care in particular. Psychological and psychiatric practice has, over the past century, also established epistemic and methodological standards. For instance, the Diagnostic and Statistical Manual of Mental Disorders and the International Classification of Diseases attempt to provide taxonomies of psychological and psychiatric conditions and benchmarks for their diagnosis.\nThese and other contextual norms do not guarantee that mental health care always proceeds smoothly and ethically. Far from it: best practices can be misguided, principles leave much leeway for (mis)interpretation, values and incentives can be misaligned. Nonetheless, these context-specific procedures, principles, virtues, standards, and best practices have often been put in place for good reason. They coordinate actors and their expectations. And their considered adherence in line with the cultivated skill and practical wisdom of practitioners generally leaves mental health care better off than general dismissal or ignorance of such norms. In what follows, I briefly outline how AI applications can breach informational norms and best practices with decisive implications for the integrity of mental health care.\nMental health chatbots and privacy:\nNumerous companies have developed AI chatbots promising prompt mental health advice, at a time where qualified therapists are costly and in short supply. Meanwhile, sensitive data from applications where users discuss their most personal circumstances and struggles is shared rather widely, for instance for targeted marketing. An extensive report by the Mozilla Foundation (2022) describes these practices as nothing less than a \u201cdata harvesting bonanza.\u201d Even the nonprofit organization Crisis Text Line has faced scrutiny for sharing user data with its for-profit spin off customer service AI (Levine, 2022).\nThe notion of contextual integrity provides one way of understanding these privacy breaches. Providers introduce AI chatbots, promising solutions and services comparable to traditional therapy and support. Sharing information widely with commercial actors and without meaningful informed consent, however, breaches the informational norms that govern therapeutic relationships in the mental health context. In the same vein, privacy regulations that intend to protect sensitive health care data such as HIPAA are not enforced when it comes to mental health AI systems (Subbian et al., 2021). As AI applications breach entrenched informational norms and the respective expectations regarding privacy that patients have, they threaten the integrity of mental health care. They erode trust, cause harm and spark justified social outcry.\nAI applications and diagnostic best practice\nAI applications in mental health care also flout diagnostic best practices. Key to psychiatric practice is the process of differential diagnosis. Mental health professionals iteratively eliminate possible conditions before settling on a diagnosis to ensure that no condition is missed or misattributed (First, 2024; Frances, 2013). When diagnosing a patient, such methodological best practices must be paired with skillful judgment. Practitioners must weigh between the access to care and insight that an early diagnosis enables and the potential reinforcing effects of labeling (Becker, 1963) or the risk of a misdiagnosis (Frances, 2013, p. 6). Clinicians must consider how a patient's culture shapes their response to a diagnosis or how differences in lived experiences between them and their patients play out in the diagnostic relationship (American Psychiatric Association, 2013, p. 14). Diagnostic practice is, thus, not just an exercise of checking off symptoms. Instead, it demands a great deal of situational awareness and care (Kroll & Mason, 2021)."}, {"title": "II.2 International Development", "content": "Within international development, moral considerations are discussed in development ethics. And while new macroeconomic strategies (Lipton, 1992), new understandings of poverty (Sen, 1979), and a greater appreciation of colonial legacies (e.g., Langan, 2018) continue to shape the normative landscape of international development, we can also find a considerable body of shared values and norms (Drydyk, 2011) at times codified in codes of ethics (IEDC, 2005).\nA central practice in the context of international development is poverty measurement. Over the past century, poverty measurement practice has developed an increasingly rich set of epistemic, methodological, and normative standards. Development practitioners recognize that choices in measurement design and use are inherently normative (Alkire et al., 2015, Chapter 6). Thus, experts engaged in poverty measurement stress the importance of explicitly recognizing motivations that underlie the design and use of poverty metrics and flagging the limitations of their metrics (Ravallion, 2016, pp. 188-190). Relatedly, best practices regarding stakeholder engagement, principled measurement design, and the validation of poverty measurements (Alkire et al., 2015, Chapter 8; Ravallion, 2016, Chapter 5.5) are intended to ensure that poverty measurements hold up to a certain epistemic standard. While such norms are not always lived up to entirely, even as aspirations they guide and coordinate development research and policy.\nAI poverty predictions and methodological norms\nUnder the banner of \"data for development\u201d machine models are trained on satellite and mobile network data to estimate poverty metrics at low cost and high levels of granularity. In contrast to traditional poverty measurement practice, however, machine learning practitioners commonly fail to validate the metrics their models are trained upon or test the robustness of their estimations (Mussgnug, 2022). Moreover, developers often neglect to emphasize the distinction between the concept of poverty and its operationalization in any given measurement. Doing so collapses the space wherein critical conceptual and normative deliberation take place. In contrast to researchers involved in traditional poverty measurements, machine learning practitioners often do not justify design choices of the measurements they predict nor explicitly lay out how and with which limitations any given metric tries to capture a particular notion of poverty (Mussgnug, forthcoming).\nAs machine learning applications disregard the methodological norms of poverty measurement, their predictions can fall short of established epistemic standards, providing estimations of questionable evidential standing. The concern is not only speculative. Empirical research demonstrates how AI predictions can fail to track short-term changes in livelihood outcomes (Kondmann & Zhu, 2020), are trained on questionable proxies (Brown et al., 2016), and underestimate cases of extreme poverty (Ratledge et al., 2022). The recent proliferation of cheap AI poverty predictions thus risks compromising the (already imperfect) integrity and reliability of development research. As a result, development resources might be distributed unfairly with potentially disastrous consequences for the communities affected.\nAI poverty predictions and norms of interpretability\nAI predictions might threaten the integrity of international development in a second way. As outlined, poverty measurement practice strives towards rendering transparent design choices in the development of poverty indices. Only when design choices are made explicit and measurements are interpretable can these be considered critically and adopted responsibly for a given purpose. Interpretability is not only important for development practitioners but also for affected communities whose acceptance of interventions and regulations might depend critically on understanding the poverty measurements underlying aid distribution and policymaking. Deep learning models employed for the prediction of poverty metrics, however, can be opaque (Boge, 2022) and, thus, lack interpretability and explainability (Hall et al., 2022). When aid is distributed based on opaque machine learning predictions, affected communities are no longer able to comprehend aid decisions. Consequently, they might lose their ability to meaningfully challenge distributive choices. If expectations regarding transparency and explainability are not met, the use of opaque machine learning models can erode already fragile trust and agency in the international development context."}, {"title": "II.3 AI Practice and Contextual Integrity", "content": "These examples served to highlight how AI systems can breach not only informational norms but also context-specific epistemic standards, best practices, established procedures, and other contextual norms. As outlined before, violations of entrenched norms are not necessarily morally or epistemically objectionable as in the cases discussed here. However, respect for the collective and cumulative wisdom and practical experience that these norms can reflect calls for a measured conservatism. Contextual norms have often been cultivated over time to promote central epistemic and ethical aims, mitigate potential harm and coordinate actors and their expectations. The cases presented illustrate how the violation of contextual norms and their respective expectations by AI systems can threaten the integrity of social contexts and, as a result, erode trust, inflict harm, and spark justified social outcry. Arguably, the implications are far-reaching also because AI is often employed in contexts such as international development and mental health care whose moral and epistemic integrity is already very fragile.\nThe point is not to endorse the uncritical adoption of entrenched contextual norms but to underscore the decisive implications of AI's lacking engagement with or even a blanket disregard for established contextual norms. Deviations from such entrenched norms can but also must be justified"}, {"title": "III AI Ethics and the notion of AI as uncharted moral territory", "content": "Much research goes into the ethical design and use of AI systems sometimes in ways that closely engage with the concrete domains of AI applications. At the same time, certain prominent efforts to promote ethical AI can inadvertently contribute to and seemingly legitimize disregard for established contextual norms by endorsing an understanding of AI as a novel and distinct realm for ethical deliberation, norm setting, and virtue cultivation.\nApproaches to AI ethics as the exploration of uncharted ethical territory echo a long-standing current in the philosophy of technology. In the 1980s and 1990s, scholars such as James H. Moor (1985) explicitly defended computer ethics as a distinct field of ethical inquiry. Philosophical debate regarding the foundations of computer ethics, however, soon moved on from examining the legitimacy of computer-related issues as a distinct domain of applied ethics. Already assuming the existence of computer ethics as a distinct sphere of moral inquiry, scholars focused on whether the challenges posed by computers necessitate their own radically unique moral philosophy or can be delt with through existing ethical frameworks (for an overview of the debate see Floridi & Sanders, 2002; Tavani, 2010). A similar debate emerged as attention shifted from computer ethics to the internet or \"cyberspace\u201d as the next ethical frontier (Tavani, 2005).\nThe notion of computer and internet ethics as the exploration of uncharted moral territory did not go unchallenged. Donald Gotterbarn, for instance, questioned the tendency to liberally subsume normative considerations under the concept of computer ethics. He remarks rather strikingly: \u201cIf these are tales about computer ethics simply because they involve the use of a computer, then my use of a scalpel to rob someone is a problem of medical ethics\u201d (1991, p. 26). Commenting on the regulation of the internet, Helen Nissenbaum calls us to resist \u201cthe suggestion that, with regard to privacy, the Net is virgin territory where it falls to the parties to construct terms of engagement for each transaction\u201d (2011, p. 45). Such criticism, however, did little to turn the tide on the notion of emerging technologies as distinct and novel moral territory. Instead, such an understanding of emerging technologies became a persistent undercurrent in the philosophy of technology \u2014 spanning from the ethics of computers and the internet to certain strands of AI ethics today."}, {"title": "III.1 AI Principlism", "content": "Consider, for instance, the case of AI principlism. Principlism denotes an approach to AI ethics that prioritizes the formulation and enactment of high-level principles for responsible AI development and use. Principlism emerged as a prominent early framework in AI ethics due to the proliferation of guidelines wherein governments, private corporations, and the third sector self-commit to high-level AI principles. A recent review article surveys no less than two hundred such documents (Corr\u00eaa et al., 2023) which converge chiefly around themes of transparency, reliability, justice, privacy, and accountability (ibid; see also Fjeld et al., 2020; Hagendorff, 2020; Jobin et al., 2019).\nPrinciplism takes inspiration from other fields of applied ethics, and in particular biomedical ethics, where scholarly and practical moral deliberation often begins from high-level principles of clinical care (e.g., Beauchamp & Childress, 2013). Underlying AI principlism, thus, is the very idea that an approach to ethics based on principles can be translated from the context of clinical health care to the \u201ccontext of AI.\u201d In this way AI principlism is grounded in and can promote the notion that \u201cArtificial Intelligence\" \u2014 its development and deployment \u2014 somehow constitutes its own moral territory that calls for academics, regulators, and practitioners to cultivate, from the ground up, new designated moral principles.\""}, {"title": "III.2 AI Fairness and translational AI Ethics", "content": "A particularly influential line of criticism focuses on the actionability of AI principles. Critics note a disconnect between what AI guidelines offer and what developers would find useful \u2014 namely a way to translate principles into practice. In the words of Morley et al. (2023, p. 411), the failure of principlism can be attributed in part to the fact that in the form of overarching principles \u201cAI ethics theory remains highly abstract, and of limited practical applicability to those actually responsible for designing algorithms and AI systems.\u201d Thilo Hagendorff finds evidence for a lack of engagement with AI practice not only in the absence of on-the-ground technical tools and methods for implementing principles but also in the lack of technical specifics in the language of these guidelines (Hagendorff, 2020, p. 111).\nThe proposed solution to this issue lies in engaging AI principles with technical practice. Ethicists and AI practitioners must collaborate in developing a more fine grained taxonomy (Morley et al., 2020), technical explanations (Hagendorff, 2020, p. 111), and a comprehensive set of standardized and readily-deployable tools that can help translate principles into technical practice (Morley et al., 2023). In answering the call for more technical specificity, researchers have put a particular emphasis on engineering technical methods for designing fairer AI models. Early research on AI fairness promised to develop universal mathematical operationalizations of fairness and off-the-shelf statistical methods to mitigate discrimination against relevant subgroups. This approach to AI fairness, however, has come under criticism for its understanding of fairness as a technical property of an AI model itself rather than a property of the social (or sociotechnical) systems within which AI is employed. By focusing only on AI as a technical system, early research on AI fairness failed to acknowledge how different notions of fairness are operative in different social contexts and how fairness concerns not machine learning predictions"}, {"title": "III.3 AI Ethics and Moral Philosophy", "content": "The notion of AI as its own distinct ethical territory can also be encountered in certain scholarship applying general frameworks from moral and political philosophy to AI ethics. For instance, some moral philosophers (often aligned with the effective altruism movement) have embraced versions of utilitarianism as a means to map out the novel moral territory of AI (Srinivasan, 2015). The notion of AI as a distinct moral domain is illustrated, for example, in an article by \u0160t\u011bp\u00e1n Cvik which disentangles approaches of utilitarianism and their respective challenges \u201cin the context of artificial intelligence\u201d (2022, p. 291, emphasis added).\nOthers have criticized the utilitarian approach to AI ethics and relied instead, for instance, on Rawls' theory of justice (for an overview see Bay, 2023). Hereby it is important to distinguish those that leverage Rawlsian ethics to explore and assess the role of AI in relation to public institutions or the basic structure of society (e.g., Gabriel, 2022; Grace & Bamford, 2020) from those that employ Rawls' theory of justice to the \u201crealm of AI.\u201d As an example of the latter, Salla Westerstrand (2024) proposes to ground ethical AI guidelines in Rawls' theory of justice. Applications of Rawlsian ethics to AI in the context of public institutions might be interpreted as an attempt to integrate AI within existing or aspirational normative structures in the realm of public institutions. Those attempting to pioneer Rawls' theory of"}, {"title": "III.4 AI as Uncharted Ethical Territory and Contextual Integrity", "content": "Al principlism, technical approaches to AI fairness, certain translational research, or some attempts to mobilize general moral philosophies for \u2018the context of AI' are not the only instances in AI ethics that illustrate a tendency to understand AI as a virgin moral domain. A similar inclination can be observed in approaches to virtue ethics that emphasize the cultivation of new technomoral virtues (c.f., Vallor, 2016) without engaging existing contextual standards of excellence, or can be encountered in pursuits of generic AI safety certifications that fail to reference entrenched contextual norms cultivated to promote reliable practice in particular areas (Corr\u00eaa & M\u00f6nig, 2024; Winter et al., 2021).\nGranted, some approaches in AI ethics are not concerned with the moral implications of AI more broadly but focus explicitly and narrowly on the professional context of AI development. For instance, a small fraction of guidelines are framed specifically as ethical principles of AI development (e.g., Blackman, 2020) and some scholars explore the role of professional norms in AI engineering (e.g., Gasser & Schmitt, 2020) or which ethical and epistemic virtues should guide AI practitioners (e.g., Hagendorff, 2022). Embracing the professionalization of AI development and instilling in practitioners a professional identity centered first and foremost around the shared technical dimensions of their work might, in its own ways, contribute to disengagement with the diverse contexts of AI application. Addressing AI ethics primarily as an occupational ethics also faces other problems. For instance, fostering responsible AI requires not only a professional ethics for individual developers but also a broader organizational ethics (Mittelstadt, 2019). Moreover, the emergence of no-code AI platforms renders AI development accessible even to laymen and ethical concerns often center around the widespread use of AI by non-experts, which are outside the focus of occupational AI ethics. For these and other reasons, approaches to AI ethics exclusively as an occupational ethics only make up a small share of the scholarship.\nIn any case, this section does not issue a blanket critique of the current state of AI ethics. Instead, it aims to illustrate how a tendency to approach emerging technologies as uncharted moral territories extends from early research on computer and internet ethics to certain prominent strands of AI ethics today. My central claim is that this notion of AI as uncharted ethical territory can play into AI practitioners' disregard for entrenched contextual norms and, as a result, contribute to AI applications threatening contextual integrity. It does so in at least two ways. First, by shifting the focus of ethical debate narrowly toward moral innovation and, second, by seemingly legitimizing disregard for existing contextual normative structures.\nThe notion of AI as its own distinct uncharted normative ground can lead AI ethics to exclusively or disproportionately focus on moral innovation. Much emphasis is being placed on creating and promoting new principles, new virtues, new epistemic standards, new methodological best practices, and new norms. This approach, however, can come at the expense of ethicists engaging with the established norms that already govern the many diverse contexts of AI application. An understanding of AI as uncharted moral territory can, thus, lead scholars to deliberate about the ethical implications and governance of AI without first thoroughly taking stock of the distinctive normative structures already in place in various domains of AI application. In these cases, AI ethicists act little different from some ecologists and development experts upheaving or \u201cinnovating\u201d existing agricultural systems in the Global South without understanding and respecting how indigenous practices of cultivation have often gradually developed in ways attuned to the particularities of their land and society (c.f., Belay & Mugambe, 2021)."}, {"title": "IV Integrative AI Ethics", "content": "This raises the question to what extent AI ethics requires a reorientation - a shift in emphasis from prioritizing the advancement of new AI-centered principles, norms, and virtues to the responsible and considered integration of AI within established social contexts and their respective normative structures. Such an integrative AI ethics emphatically identifies AI applications as first and foremost elements within existing contexts of practice as AI applications within rather than to mental health care, international development, agriculture, or education.\nAn integrative approach to AI ethics does not dispute that technology-specific considerations are central to AI ethics, nor does it entirely deny the need for cross-contextual moral theorizing and ethical governance. But it calls for a shift in emphasis grounded in the acknowledgement that, as many have noted, the moral implications of AI often play out differently in the radically heterogenous contexts within which Al finds application. Adopting the words of Thilo Hagendorff (2020, pp. 114\u2013115), the goal is to cultivate an AI ethics that deals less with Al as such."}, {"title": "IV.1 Objections", "content": "My call to refocus AI ethics along contextual norms may provoke concerns regarding its feasibility and implications. In what follows, I seek to briefly address four possible objections.\nFirst, one might be concerned that this approach results in a problematic degree of conservatism in the moral debate around AI. I believe that such a concern is warranted. Thus, it is important to stress that integrative AI ethics calls for a measured and conditional conservatism. It calls for a prima facie respect for entrenched contextual norms, not deference. It does not deny that existing norms can be misguided and that the introduction of AI tools might challenge existing rationales. As mentioned, we might sometimes employ new technologies precisely with the intention to overhaul existing customs. In many areas, the introduction of AI technologies provides us with the timely opportunity to reconsider objectionable conventions and norms often based on racism, sexism, classism, and colonial legacies. If developed prudently, AI can help us pursue more equitable futures and greater human flourishing. Integrative AI ethics emphasizes that for those efforts to be most impactful, they must first be justified with reference to entrenched contextual norms and their shortcomings, as well as promoted in ways coordinated with a context's wider normative structure.\nSecond, and relatedly, one might worry that an integrative AI ethics is bound to be too descriptive, lacking a certain degree of normativity central to AI ethics a concern that has been raised with respect to STS scholarship (Mason-Wilkes, 2024) or the empirical turn in the philosophy of technology (Scharff, 2012). I believe such an objection is somewhat misguided. An integrative approach to AI ethics might indeed take strong normative stances on the ethical and socially relevant dimensions of AI. It differs only with respect to the source of such normativity. It argues that normative direction should come to a lesser extent from philosophers' moral innovation and, instead, underscores the cultivated wisdom and accumulated moral and practical experience already embodied in existing contextual norms as a significant source of normativity.\nThird, one might wonder whether the recent popularization of foundation models such as large language models (LLMs) poses challenges to a context-centered approach to the ethics and governance of AI. Foundation models are trained on a large corpus of unlabeled data and can be used (often with minimal fine-tuning) for a wide range of purposes across diverse contexts (European Union, 2023). Thus, the question emerges which contextual norms should guide the development and deployment of foundation models?\nRather than pose challenges to integrative AI ethics, I believe that the latter's focus on contextual norms can help bring out clearly the broad responsibilities that are associated with the development and deployment of foundation models. The fact that foundation models are commonly employed across contexts does not absolve practitioners from contextual norms. Instead, it highlights that practitioners must engage with the respective norms that govern any given context in which foundation models find application. If foundation models are used in education, health care, finance, or the legal context, then systems must respect (or practitioners must substantially justify their deviation from) that context's normative structures. Such is clearly a tall order but not unreasonable.\nLet us, only for the purpose of this argument, buy into the anthropomorphizing narrative of foundation models such as LLMs approaching human or superhuman-level artificial general intelligence (c.f., Y Combinator, 2024). Human actors often engage in a wide range of social contexts. One might work in the education, financial, medical, or legal sector, meet friends at their home, go to church on Sundays, engage in politics, and so on. This does not absolve us from the distinct norms that govern these contexts but renders us accountable to them. It requires us to navigate them thoughtfully, adapting our behavior to align with the specific expectations, responsibilities, and ethical standards each context entails. We should ask no less of foundation models seemingly approaching human-level capabilities.\nHow exactly to engineer and deploy foundation models to fulfill this task is undoubtedly a difficult question. Practitioners might design and implement models with mechanisms that allow them to adapt to diverse contexts, fine-tune distinct context-specific models, or prohibit certain uses of foundation models. Recent efforts highlight that we likely need a combination of these and other strategies (Deng et al., 2024; Longpre et al., 2024). Integrative AI ethics underscores that central to this emerging moral debate should not be the innovation of norms for the new \u201crealm of foundation models\" but engagement with the normative structures governing the diverse contexts of their application.\""}, {"title": "V Conclusion", "content": "Recent research illustrates how AI development and deployment can happen in a manner detached from the concrete social context of application. This paper has emphasized how, by abstracting from the contexts of AI application, practitioners also disengage from the distinct normative structures that govern them. This disregard for contextual norms can threaten the integrity of a context with often decisive ethical implications. Certain prominent efforts to promote responsible and ethical AI can inadvertently contribute to and seemingly legitimize this disregard for established contextual norms. Echoing a persistent undercurrent in technology ethics of understanding emerging technologies as uncharted moral territory, current approaches to AI ethics can promote a notion of AI as a novel and distinct realm for ethical deliberation, norm setting, and virtue cultivation. A narrative of AI as virgin moral territory, however, can come at the expense of practitioners, policymakers and ethicists engaging with already"}, {"title": "Declarations", "content": "The author declares no competing interests."}]}