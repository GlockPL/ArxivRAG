{"title": "Recent Advances of Multimodal Continual Learning: A Comprehensive Survey", "authors": ["Dianzhi Yu", "Xinni Zhang", "Yankai Chen", "Aiwei Liu", "Yifei Zhang", "Philip S. Yu", "Irwin King"], "abstract": "Continual learning (CL) aims to empower machine learning models to learn continually from new data, while building upon previously acquired knowledge without forgetting. As machine learning models have evolved from small to large pre-trained architectures, and from supporting unimodal to multimodal data, multimodal continual learning (MMCL) methods have recently emerged. The primary challenge of MMCL is that it goes beyond a simple stacking of unimodal CL methods, as such straightforward approaches often yield unsatisfactory performance. In this work, we present the first comprehensive survey on MMCL. We provide essential background knowledge and MMCL settings, as well as a structured taxonomy of MMCL methods. We categorize existing MMCL methods into four categories, i.e., regularization-based, architecture-based, replay-based, and prompt-based methods, explaining their methodologies and highlighting their key innovations. Additionally, to prompt further research in this field, we summarize open MMCL datasets and benchmarks, and discuss several promising future directions for investigation and development. We have also created a GitHub repository for indexing relevant MMCL papers and open resources available at https://github.com/LucyDYu/Awesome-Multimodal-Continual-Learning.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, machine learning (ML) has achieved significant advancements, contributing to the resolution of a wide range of practical problems. In conventional settings, most ML models operate within the so-called \"single-episode\" paradigm, being trained on static and single datasets, while evaluated under the independent and identically distributed (i.i.d.) assumption [1]. However, this \"single-episode\" paradigm may not equip the trained models with the capability to adapt to new data or perform new tasks, failing to align with the aspiration of developing intelligent agents for dynamically evolving environments. To address this issue, the ML community is motivated to develop continual learning (CL), also known as lifelong learning or incremental learning, which trains models incrementally on new tasks and maintains early knowledge without requiring full-data retraining [2-5].\n\nThe main challenge of CL is catastrophic forgetting: a phenomenon that when tasks are trained sequentially, training on the new task greatly disrupts performance on previously learned tasks [6, 7], as unconstrained fine-tuning drives parameters moving far from the old optimal state [8]. CL aims to develop learning systems capable of continuous knowledge acquisition while retaining previously learned information. Such a process essentially imitates the cognitive flexibility observed in biological brains, which continually learn diverse skills throughout the human lifespan [9]. By enabling models to adapt to new tasks without forgetting, CL offers clear advantages in terms of resource and time efficiency compared to the traditional approach of exhaustive model retraining on full task datasets. Furthermore, due to issues of storage limitations, privacy concerns, etc., the potential inaccessibility of historical training data makes full-data training unfeasible, further highlighting the efficiency and effectiveness of CL in memorizing former knowledge and acquiring up-to-date one from dynamic environments.\n\nDespite significant progress in CL, most efforts have been devoted to a single data modality, such as vision [10\u201313], language [14-16], graph [17, 18], or audio [19]. This unimodal focus overlooks the multimodal nature of real-world environments, which are inherently complex and composed of diverse data modalities rather than a single one. With the rapid growth of such multimodal data, e.g., data proliferation of images, texts, and videos on platforms like Meta and TikTok, it is imperative to develop AI systems capable of learning continually from multimodal sources, hence the rise of the multimodal continual learning (MMCL) setting. These MMCL systems need to effectively integrate and process various multimodal data streams [20, 21] while also managing to preserve previously acquired knowledge. More importantly, this MMCL setting better mimics the process of learning and integrating information across different modalities in human biological systems, ultimately enhancing the overall perception and cognitive capabilities when dealing with real-world complexities [22, 23]. Illustrations of unimodal CL and MMCL are provided in Fig. 1.\n\nChallenges of MMCL. In spite of the connection between conventional unimodal CL and MMCL, the challenges of MMCL extend beyond a simple stacking of CL\u00b9 methods on multimodal data. Such straightforward attempts have been demonstrated to yield suboptimal performance [31-33]. Concretely, as illustrated in Fig. 2, in addition to the existing challenge of catastrophic forgetting in CL, the multimodal nature of MMCL introduces the following four challenges. These challenges not only stand alone but may also exacerbate the catastrophic forgetting issue:\n\nChallenge 1 (Modality Imbalance). Modality imbalance refers to the uneven processing or representation of different modalities within a multimodal system, which manifests at both the data and parameter levels. At the data level, the data availability of different modalities"}, {"title": "2 PRELIMINARIES", "content": "In this section, we introduce the setup for MMCL, including notations, basic formulation, distinct learning scenarios, and widely used evaluation metrics.\n\n2.1 Notations\n\nWe use bold lowercase, bold uppercase, and calligraphy letters for vectors, matrices, and sets, respectively. We list the key notations in Table 1.\n\n2.2 Basic Formulation\n\nIn this section, we introduce the basic formulation of CL and MMCL. Definitions 1 and 2 define task sequence and CL,"}, {"title": "Definition 1 (Task Sequence).", "content": "Let Xt and Yt denote the input data and the data label of the t-th task, respectively. The dataset of the t-th task, denoted as Dt, is defined as:\n\nDt = {(xt,i, Yt,i) : i \u2208 N, 1 \u2264 i \u2264 Nt},"}, {"title": "Definition 2 (Continual Learning (CL)).", "content": "Given a task sequence TS of size T, we consider the t-th task (1 < t < T) as a new task so far. Continual learning is the setting that, for each such task, the model is trained only on data Dt (or with very limited access to previous datasets {D1,D2,\u2026\u2026,Dt\u22121} in a more relaxed setting). The objective is to learn the new task while maintaining performance on old tasks to overcome catastrophic forgetting. Specifically, given an unseen test sample x \u2208 X from any trained tasks, the trained model f: X \u2192 Y should perform well in inferring the label y = f(x) \u2208 Y [55]. Remark. The performance of the model is evaluated using metrics described in Section 2.4. The difficulty of CL stems from the fact that datasets have dynamic distributions, i.e., Vi, j \u2208 T, i \u2260 j \u21d2 p(X\u2081) \u2260 p(Xj) [5]."}, {"title": "2.3 Multimodal Continual Learning Scenarios", "content": "In MMCL, the learning process varies in terms of modalities, data distribution, and task identity availability, resulting in five different MMCL scenarios. We first introduce three scenarios that originate in conventional CL but can be inclusive in MMCL:"}, {"title": "Scenario 1 (Class-incremental Learning (CIL)).", "content": "For i \u2260 j, Di and Dj have different input distributions and data label spaces, i.e., p(Xi) \u2260 p(Xj) ^ Yi \u2260 Vj [5]. Task identities are not available in testing. The model should be able to perform classification for all seen classes. The model may need to infer the task-ID at test time to determine the possible classes of a test sample [4]. Note that in the conventional CIL setting, the data label spaces of tasks are disjoint, i.e., Vi \u2260 j, V; NY; = \u00d8 [5]; however, in a more generalized CIL setting, the data label spaces may overlap, i.e., \u2203i \u2260 j, Vi \u2229 V; \u2260 0 [23]."}, {"title": "Scenario 2 (Domain-incremental Learning (DIL)).", "content": "For i \u2260 j, Di and Dj have different input distributions but the same label space, i.e., p(Xi) \u2260 p(Xj) ^ Y\u2081 = Vj [5]. Task identities are not required. Identifying the task is unnecessary for the model because of the same label space of all tasks [4]."}, {"title": "Scenario 3 (Task-incremental Learning (TIL)).", "content": "For i \u2260 j, Di and Dj have different input distributions and label spaces, i.e., p(Xi) \u2260 p(Xj) ^ Vi \u2260 V; [5]. Task identities are available in testing. The model needs to learn the tasks, and with the task-ID received at test time, it knows which task needs to be performed [4]."}, {"title": "2.4 Evaluation Metrics", "content": "To evaluate the model performance in MMCL, various metrics are proposed. In the single-task case, the performance evaluation metrics may vary depending on different task types. For instance, these metrics may include accuracy for classification [42], BLEU-4 for text generation [69], Recall for retrieval tasks [68], etc. Based on these single-task evaluation metrics, we introduce common metrics for multiple tasks. Let at,i \u2208 [0, 1] be the model performance on the test set of the i-th task, after the model is trained progressively from task 1 to task t [5]. (1) Average Performance (A). The average performance at the t-th task is defined as: At = 1/t \u2211_{i=1}^{t} at,i. (3) (2) Forgetting Measures (F) [76]. Forgetting is quantified as the difference between the \"maximum\" knowledge and the current knowledge of a task during the continual learning process. Let fi \u2208 [-1,1] be the forgetting measure of the i-th task (i < t), after the model is progressively trained from task 1 to task t: f_i = max_{s\u2208{1,...,t-1}} {as,i - at,i}, Vi < t. (4) The average forgetting at the t-th task is defined as: Ft = 1/(t-1) \u2211_{i=1}^{t-1} f_i (5) (3) Backward Transfer (BWT) [5, 77]. The difference at,i - ai,i measures the influence of a task t on a previous task i (i < t). For the t-th task, backward transfer measures its average influence on the performance of all previous tasks. BWT+ = 1/(t-1) \u2211_{i=1}^{t-1} at,i - ai,i (6) (4) Forward Transfer (FWT) [77]. Let b\u2081 be the performance of the i-th task with random initialization. The difference"}, {"title": "3 METHODOLOGY", "content": "In this section, we present a taxonomy of MMCL methods. Figure 5 categorizes MMCL methods into four types, which we elaborate in the subsections below. We summarize detailed properties of MMCL methods in Table 2 and the representative architectures of MMCL methods in Fig. 6. Note that Table 2 and Fig. 6 focus on methods of vision and language modalities, and methods of other modalities are summarized in Table 3. To ensure readability, we first introduce classical unimodal CL methods, as they are either the predecessors of various MMCL methods or are extensively compared in MMCL works."}, {"title": "3.1 Regularization-based Approach", "content": "Since the free movement of parameters in training causes catastrophic forgetting [8], regularization-based methods are motivated to add constraints on the parameters. Depending on how constraints are imposed, regularization-based methods are divided into two sub-directions: explicit regularization and implicit regularization. We summarize the representative architectures of explicit and implicit regularization-based methods in Fig. 6a."}, {"title": "3.1.1 Explicit Regularization", "content": "Explicit regularization methods directly assign importance to parameters and penalize them differently when they deviate from the previously found solution. Let Lsingle,t be the loss in the single task setting when the model is learning the t-th task. The continual loss is then defined as Lt = Lsingle,t + \u03bbELE,t, where LE,t is the regularization term and the hyperparameter \u03bbE balances the learning of new tasks and avoiding forgetting. LE,t can be formulated as follows [11]: LE,t = \u2211_i bi (\u03b8_i - \u03b8_{t-1,i})^2, where \u03b8i and \u03b8t\u22121,i denote the i-th element in \u03b8 and \u03b8t\u22121 respectively, and bi indicates the corresponding importance. Representative Unimodal Models. EWC [11] utilizes the diagonal of the Fisher information matrix as the term bi in Equation (9) and accumulates multiple regularization terms for the previously found solutions of previous tasks. This method restricts parameter changes that are crucial for previous tasks, while allowing greater flexibility for less significant parameters. EWC is used extensively in unimodal and multimodal works for performance comparison because it is effective and model-agnostic. Several followed-up works like EWCH [78] and online EWC [79] have been proposed to further enhance the efficacy and efficiency of EWC by employing a single regularization term instead of multiple terms. Multimodal Models. In the MMCL setting, TIR [66] leverages BLIP2 [80] and InstrutBLIP [81] as base MM models to handle multimodal data. Based on importance measures from existing methods like EWC [11], TIR proposes to calculate task similarity scores between the new task and old tasks to obtain adaptive weights for parameter regularization, facilitating long-term continual learning."}, {"title": "3.1.2 Implicit Regularization", "content": "Instead of storing one or all optimal states of previous tasks (like EWC [11]) and assigning weights to individual parameters, implicit regularization methods typically focus on minimizing the model's output for previously learned tasks, thereby reducing the risk of forgetting. Unlike explicit regularization, where parameter changes are directly penalized, implicit regularization methods impose the penalty only when parameter changes ultimately lead to alterations of the model outputs. Thus, compared to explicit regularization, implicit regularization methods allow parameters to change more freely. Methods in this category typically utilize knowledge distillation (KD) [85], which matches the output between a teacher model (the previous-task trained model) and a student model (the current model) [5]. Specifically, the output may be confined to the model logits (final layer output, i.e., logits-based KD), or it may be extended to the intermediate features, i.e., feature-based KD and feature pairwise relations, i.e., relation-based KD. Therefore, KD enables the student model to learn logits, feature distribution, and pairwise relations similar to those of the teacher model [86, 87]. When the model is learning the t-th task, the continual loss is expressed as Lt = Lsingle,t+\u03bbILI,t, where LI,t is the regularization term and the hyperparameter \u03bbI is used for loss balancing. LI,t incorporates KD and can be formulated as follows [10, 67]: LI,t = LKD(Yt-1, Yt)= \u2211\u2212yt\u22121,i log yt,i cross-entropy loss ||yt-1 - yt ||2 L2 loss, where yt-1 and yt are the results of one data sample outputted from the model before and after training on the t-th task. yt-1,i and yt,i denote the i-th element in yt\u22121 and yt, respectively. Representative Unimodal Models. LwF [10] is a classical regularization-based CL work that incorporates the KD design. It calculates the output of old tasks using new data before training on a new task. During the learning process for the new task, the model minimizes the changes in the outputs of previous tasks using the KD loss. This strategy"}, {"title": "3.2 Architecture-based Approach", "content": "Architecture-based methods employ an intuitive and direct strategy to learn tasks, by enabling different model parameters to cope with different tasks. Regularization-based methods share all parameters to learn tasks, making them prone to inter-task interference [5]: an issue where remembering old tasks greatly interferes with learning a new task, leading to decreased performance, when the forward knowledge transfer is negative [88]. In contrast, architecture-based methods reduce inter-task interference by incorporating task-specific components. Depending on the model designs, architecture-based methods are categorized into two types: fixed architecture and dynamic architecture. We provide an overview of representative architectures of fixed and dynamic architecture-based methods in Fig. 6b."}, {"title": "3.2.1 Fixed Architecture", "content": "Fixed architecture methods aim to reduce inter-task interference and mitigate forgetting, by utilizing different portions of parameters for individual tasks. Techniques like hard or soft parameter masking are often employed to achieve such task-specific parameter allocation within fixed architectures. Representative Unimodal Models. HAT [89] learns near-binary attention vectors for masking, enabling the activation or deactivation of units across different tasks. Based on the obtained mask, a subset of parameters remains static during training, which helps maintain early knowledge. Multimodal Models. RATT [69] is an early MMCL work for image captioning. It leverages a pre-trained CNN to"}, {"title": "3.2.2 Dynamic Architecture", "content": "Dynamic architecture methods adapt the model structure as new tasks are introduced, typically through expansion by adding new modules. Unlike methods that operate on fixed models, dynamic architecture methods are usually able to increase model capacity with each new task, thereby ensuring that performance is not ultimately constrained by the initial capacity [40]. It is worth noting that, if the model has task-specific components and receives the task-ID during testing (the TIL scenario), the primary objective remains to avoid forgetting; however, the model should also effectively learn shared knowledge across tasks and balance performance with computational complexity [4]. Representative Unimodal Models. An early work, namely Progressive Network [90], initializes a new network for each new task. This strategy is explicitly designed to prevent the forgetting of previously learned tasks. It facilitates knowledge transfer by employing lateral connections to leverage previously acquired features. Multimodal Models. In the MMCL setting, some methods design dynamic modules based on multimodal base models. For instance, both MoE-Adapters4CL [42] and CLAP [41] use CLIP as the base model. When a new task is introduced, one straightforward strategy is to directly add a new module into the network to learn new knowledge, i.e., direct task-based. A more sophisticated approach is to design a mechanism that adaptively determines how to modify the network for learning new knowledge while maintaining computation efficiency, i.e., in an adaptive task-based manner. In addition, the model may change its structure when a new modality is incorporated along with a task, i.e., modality-based. This highlights a clear distinction between MMCL and conventional CL, as this strategy is only applicable to multiple modalities. Therefore, we group methods based on their architecture modification mechanisms (direct task-based, adaptive task-based, and modality-based) in the subsequent paragraphs."}, {"title": "3.3 Replay-based Approach", "content": "Replay-based methods utilize an episodic memory buffer to replay historical instances, such as data samples, from previous tasks, helping to maintain early knowledge while learning new tasks. This approach of replaying instances avoids the rigid constraints of regularization-based methods and circumvents the complexity of dynamically modifying network architectures in architecture-based methods. Depending on the mechanisms to obtain these replay instances, replay-based methods are divided into two sub-directions: direct replay and pseudo replay. When learning the t-th task, the episodic memory M\u2081 will be combined with the incoming data Dt. The loss function can be expressed as: Lt = 1/|DtUMt| \u2211_{(xi,Yi)\u2208(DtUMt)} l(f(xi), Yi). We depict the representative architectures of direct and pseudo replay-based methods in Fig. 6c."}, {"title": "3.3.1 Direct Replay", "content": "This approach usually stores a small number of old training instances in episodic memory. Due to the limited capacity of memory storage, the key to these methods lies in how to select the representative data samples. Representative Unimodal Models. Early studies of unimodal direct replay methods have focused on selecting samples based on some heuristic strategies. For instance, Reservoir Sampling [95] randomly chooses raw samples. iCaRL [12] employs a herding mechanism based on feature representations to ensure class balance. ER-MIR [96] selects samples that have a large influence on loss change. Subsequent work primarily focuses on exploring other selection strategies [17, 18, 97-99] or optimizing memory storage [96, 100]. Multimodal Models. With multimodal data, an intuitive implementation involves directly selecting and replaying samples from various modalities. For instance, following the sampling strategies from [101] and [95], VQACL [25] and SAMM [23] both select multimodal samples randomly. Experimental results from SAMM [23] demonstrate that, compared to unimodal replay, multimodal replay significantly enhances the plasticity and stability of the model, thereby achieving a superior stability-plasticity trade-off. Direct replay methods can be naturally integrated with KD, ensuring that the model maintains consistency in various aspects of the old data before and after model updates [5]. To ensure consistency at the representation level, TAM-CL [71] utilizes a memory buffer to store a small percentage of the training dataset. It then computes the KD loss between the outputs of the last self-attention block from the current student model and the earlier teacher model. This strategy helps to constrain distribution shifts. In terms of consistency in cross-modal interactions, KDR [72] utilizes KD to regulate the cross-modal similarity matrix, thereby enhancing the consolidation of cross-modal knowledge."}, {"title": "3.3.2 Pseudo Replay", "content": "To avoid additional storage requirements and privacy concerns in direct replay methods, pseudo replay has recently gained attention. This approach involves the use of a generative model to learn the data distribution from previous stages and then replay generated data at the current stage. Representative Unimodal Models. DGR [102] is a pioneer unimodal work that trains a GAN [103] to generate data samples, which are then replayed during the current model training to retain the previously learned knowledge. Subsequent research expands this strategy by exploring a variety of generative models [104-106] to enhance replay fidelity and scope. Additionally, some studies shift the focus to the feature level [107, 108], aiming to reinforce feature representations to counteract the issue of forgetting. Multimodal Models. With datasets that include various modalities, generating highly correlated data tuples, such as image-question-answer triplets that are both detailed and accurately labeled, usually poses significant challenges. To address these difficulties, some studies have focused on generating either substitute or partial data. For instance, SGP [63] maintains scene graphs, which are graphical representations of images, and incorporates a language model for pseudo replay. IncCLIP [73] emphasizes pseudo text replay through the generation of negative texts conditioned on images, which helps better preserve learned knowledge. In addition, efforts like FGVIRs [32] and AID [36] specifically tackle issues of modality imbalance. They employ pseudo-representation and pseudo-prototype replay strategies to enhance classifier discriminability. They address the inherent challenges in multimodal learning environments where maintaining balance across different types of data is crucial. Summary. As shown in Fig. 6c, within the MMCL setting, both direct and pseudo replay methods offer greater flexibility in selecting replay data, as they may opt to replay one or multiple modalities based on the specific design of the model. Moreover, the replay strategy may be tailored to apply separately to each modality or involve interactions between modalities."}, {"title": "3.4 Prompt-based Approach", "content": "With the rapid development of large models and their application in the CL setting, prompt-based methods have recently emerged to better utilize the rich knowledge acquired during pre-training. These methods offer the advantage of requiring minimal model adjustments and reducing the need for extensive fine-tuning, unlike previous methods that often require significant fine-tuning or architectural modifications. The paradigm of prompt-based methods involves modifying the input by applying a few prompt parameters in a continuous space, allowing the model to retain its original knowledge while learning additional task-specific information. Consequently, they are inherently capable of addressing Challenge 3: high computational costs, and Challenge 4: degradation of pre-trained zero-shot capability in the MMCL setting. We present the representative architecture of prompt-based methods in Fig. 6d. Representative Unimodal Models. Early unimodal CL studies primarily concentrate on designing prompt architectures that effectively integrate both general and specific knowledge [5]. L2P [55] utilizes a prompt pool shared across all tasks, from which only the most relevant prompts are selected for each input sample during training or inference. In contrast, DualPrompt [109] creates two distinct sets of prompt spaces, accommodating both task-invariant and task-specific prompts. Multimodal Models. Existing multimodal prompt-based works vary in their prompt design strategies, such as shared prompts (Fwd-Prompt [44]), task-specific prompts (S-liPrompts [75]), and layer-specific prompts (CPE-CLIP [43] and TRIPLET [74]). Moreover, these approaches also place greater emphasis on designing prompts that cater to different modalities. For instance, S-liPrompts introduce a joint language-image prompting scheme that enables the image-end transformer to seamlessly adapt to new domains, while enhancing the language-end transformer's ability to capture more semantic information. Meanwhile, CPE-CLIP and TRIPLET focus more on modality fusion: CPE-CLIP connects language and vision prompts by explicitly defining vision prompts as a function of language prompts, while TRIPLET proposes decoupled prompts and prompt interaction strategies to model the complex modality interactions. Summary. We summarize the key architecture of prompt-based methods in Fig. 6d. In the MMCL setting, prompt-based methods may choose to modify the input and learn prompts for the encoders of modalities and/or the modality interaction component. Depending on the model design, these methods may also facilitate interactions between prompts across different modalities."}, {"title": "4 DATASETS AND BENCHMARKS", "content": "In this section, we provide an overview of current datasets and benchmarks in MMCL. A majority of MMCL datasets are adapted from well-known datasets that are initially designed for non-CL tasks, and researchers often either utilize multiple datasets or partition a single dataset into multiple subsets to simulate tasks in the MMCL setting [39]. In addition, there exist several datasets that are dedicated to MMCL, such as P9D [68] and UESTC-MMEA-CL [39]. Table 4 summarizes MMCL benchmarks covering various CL scenarios, modalities, and task types. We introduce them as follows if codes are publicly accessible."}, {"title": "4.1 Benchmarking on an Original Dataset", "content": "In this section, we summarize two dedicated MMCL datasets. Zhu et al. [68] utilize E-commerce data to construct the first vision-language continual pre-training dataset P9D and establish the VLCP benchmark for cross-modal retrieval and multimodal retrieval. P9D contains more than one million image-text pairs of real products and is partitioned into 9 tasks by industrial categories. Xu et al. [39] collect video and sensor data from ten participants wearing smart glasses. They construct the dataset UESTC-MMEA-CL, the first multimodal dataset for continual egocentric activity recognition, with modalities of vision, acceleration, and gyroscope. They also establish a benchmark, CEAR, with three baseline CL methods, namely EWC [11], LwF [10] and iCaRL [12]. Results demonstrate that replay-based iCaRL is more effective in alleviating forgetting than replay-free methods EWC and LwF. Nonetheless, exploring replay-free strategies remains promising and important, as replay-based methods are not always applicable due to considerations such as privacy concerns [39]. Xu et al. [39] use TBW [110]-like midfusion to fuse multimodal features, achieving better results than using single modality data in the non-CL setting. However, in the MMCL setting, the performance with multimodal data (vision and acceleration) is inferior to that with unimodal data (vision), even with CL methods incorporated. These results highlight the necessity for further research in MMCL methods to improve the fusion of modality information while preventing forgetting."}, {"title": "4.2 Benchmarking on Several Datasets", "content": "We outline three benchmarks that employ various datasets as tasks in the MMCL framework. CLIMB [31] benchmarks with four vision-language tasks (VQAv2 [62], NLVR2 [111], SNLI-VE [64], and VCR [112]), five language-only tasks (IMDb [65], SST-2 [113], HellaSwag [114], CommonsenseQA [115], and PIQA [116]) and four vision-only tasks (ImageNet-1000 [117], iNaturalist2019 [118], Places365 [119], and MS-COCO object detection [120]). CLIMB treats each task as a classification task and consists of two phases within the CL process. In upstream continual learning, the model is trained on vision-language tasks with various candidate CL algorithms. In downstream low-shot transfer, after training on the i-th upstream task and saving checkpoints, for each task of the training data of the remaining upstream tasks and unimodal tasks, the model is fine-tuned on the checkpoints with a fraction of the task data. The CLIMB benchmark results demonstrate that common CL algorithms (ER [101], EWC [11]) are able to alleviate forgetting. However, they may hurt downstream task learning, compared to direct fine-tuning. These results underscore the need for further research on MMCL methods. CLOVE [63] splits data from GQA [27] into six subsets representing different"}, {"title": "4.3 Benchmarking on a Partitioned Dataset", "content": "A benchmark can partition one dataset into multiple subsets to simulate tasks in the MMCL setting, and there are three benchmarks of this kind. The IMNER benchmark [34] utilizes the Twitter-2017 MNER dataset (constructed by [130] and preprocessed by [131]) and splits it by categories to simulate the CIL scenario. The IMRE benchmark [34] partitions the MEGA MRE dataset [132] into 10 subsets for the CIL scenario. MMCL [23] is a benchmark that contains audio and visual modalities for classification. It partitions the VGGSound dataset [133] to simulate CIL and DIL scenarios."}, {"title": "5 FUTURE DIRECTIONS", "content": "With the rapid advancement of multimodal models, MMCL has become an active and promising research topic. In this section, we outline several future directions for further exploration and research."}, {"title": "5.1 Improved Modality Quantity & Quality", "content": "Our summarization in Table 3 reveals that only a few MMCL methods focus on modalities other than vision and language. Therefore, there is huge space for further research on incorporating more modalities. Similarly, developing benchmarks for more modalities is important for this field. Moreover, modalities are not limited to those listed in Table 3 and may include biosensors [134], genetics [135], and others [136], thereby enhancing support for emerging challenges, in fields such as AI for science research. With the introduction of more modalities, it will be increasingly imperative to address data-level modality imbalance, i.e., Challenge 1, which, as shown in Table 2 and Table 3, has been addressed by only a few MMCL methods. Furthermore, due to the discrepancy among distributions and quality of different modalities, the modality with better performance may dominate optimization, leaving other modalities under-optimized [35]. Hence, addressing parameter-level modality imbalance is also crucial. Developing specific strategies to balance modalities helps mitigate the forgetting issue [32], making it a promising research direction."}, {"title": "5.2 Better Modality Interaction Strategies", "content": "As we have just mentioned, there are only a few MMCL methods that incorporate more than two modalities. Modality interaction, especially modality alignment, may be more complicated with three or more modalities, i.e., Challenge 2. Furthermore, many existing MMCL methods simply fuse modalities within neural architectures without a deeper understanding or analysis of their mutual influence on learning. Thus, it will be interesting and promising to measure such inter-modality influence [37, 137] for more fine-grained multimodal interaction."}, {"title": "5.3 Parameter-efficient Fine-tuning MMCL Methods", "content": "Parameter-efficient fine-tuning (PEFT) methods offer an effective solution to optimize training costs, i.e., addressing Challenge 3, by reducing the number of trainable parameters while achieving comparable or better performance than full-parameter fine-tuning to the large models [91, 138]. While prompt-based methods are parameter-efficient, in Table 2, we observe that only MoE-Adapters4CL [42] utilizes PEFT methods. CLAP [41] also mentions this as its future work. Therefore, employing PEFT methods to reduce training costs for MMCL methods is a worthy direction. Furthermore, beyond the straightforward application of existing PEFT methods, a promising direction is to propose new PEFT methods specifically for the MMCL setting, and to seamlessly integrate them with other MMCL techniques."}, {"title": "5.4 Better Pre-trained MM Knowledge Maintenance", "content": "As many MMCL methods are armed with powerful MM backbones, it is naturally desirable to memorize their pre-trained knowledge during training. Forgetting pre-trained knowledge may significantly hurt future task performance [44, 45]. We observe that few methods in Table 2, aside from prompt-based ones, explicitly prioritize maintaining pre-trained knowledge, i.e., addressing Challenge 4, as one of their key goals. Moreover, this is particularly challenging for replay-based methods that usually rely on quick adaptation to old data samples for knowledge retention. However, for certain pre-trained models like CLIP, the pre-trained data is private [45], which makes the target difficult yet promising for future research."}, {"title": "5.5 Prompt-based MMCL Methods", "content": "As discussed in Section 3.4, prompt-based MMCL methods effectively address Challenge 3: high computational costs, and Challenge 4: degradation of pre-trained zero-shot capability. However, as shown in Table 2, we note that prompt-based MMCL methods are currently the least represented category. Recently, prompt learning techniques are gaining traction in the non-CL setting for multimodal models [139, 140]. Moreover, there are popular prompt tuning methods that combine learning with high-quality templates [141]. Extending these methods to the MMCL setting facilitates the efficient and effective utilization of pre-trained models. Given that the prompt-based category is still in its infancy, there is significant potential for further research and development."}, {"title": "5.6 Trustworthy Multimodal Continual Learning", "content": "With people paying more attention to privacy and governments imposing more related regulations, the demand for trustworthy models is escalating. Techniques such as"}, {"title": "6 CONCLUSION", "content": "In this work, we present an up-to-date multimodal continual learning survey. We provide a structured taxonomy of MMCL methods, essential background knowledge, a summary of datasets and benchmarks, and discuss two novel MMCL scenarios for further study. We categorize existing MMCL works into four categories, i.e., regularization-based, architecture-based, replay-based, and prompt-based methods, with detailed subcategories described. We also provide representative architecture illustrations for all categories. Our detailed review highlights the key features and innovations of these MMCL methods. Additionally, we discuss promising future research directions in this rapidly evolving field, offering discussions on potential areas for further investigation and exploration. We anticipate that the development of MMCL will further enhance models to exhibit more human-like capabilities. This enhancement includes the ability to process multiple modalities at the input"}]}