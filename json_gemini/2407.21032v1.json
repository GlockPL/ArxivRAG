{"title": "Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion", "authors": ["Sanghyun Kim", "Seohyeon Jung", "Balhae Kim", "Moonseok Choi", "Jinwoo Shin", "Juho Lee"], "abstract": "This paper addresses the societal concerns arising from large- scale text-to-image diffusion models for generating potentially harmful or copyrighted content. Existing models rely heavily on internet-crawled data, wherein problematic concepts persist due to incomplete filtration processes. While previous approaches somewhat alleviate the issue, they often rely on text-specified concepts, introducing challenges in accurately capturing nuanced concepts and aligning model knowledge with human understandings. In response, we propose a framework named Human Feedback Inversion (HFI), where human feedback on model-generated images is condensed into textual tokens guiding the mitigation or re- moval of problematic images. The proposed framework can be built upon existing techniques for the same purpose, enhancing their alignment with human judgment. By doing so, we simplify the training objective with a self-distillation-based technique, providing a strong baseline for concept removal. Our experimental results demonstrate our framework signifi- cantly reduces objectionable content generation while preserving image quality, contributing to the ethical deployment of AI in the public sphere.", "sections": [{"title": "1 Introduction", "content": "Large-scale text-to-image generation models have demonstrated remarkable suc- cess in producing high-quality images encompassing a wide array of concepts. These models possess the ability to combine disparate ideas in novel and unprecedented manners, making them invaluable tools for creative content creation. However, they also give rise to substantial concerns, including the gen- eration of harmful or copyrighted content, even without explicit directives. A critical issue lies in the heavy reliance of the models on internet-crawled datasets (e.g., LAION-5B [42], currently not available [50]), which, despite attempts at aggressive filtration [48], often lead to problematic content generation."}, {"title": "2 Background on Diffusion Models", "content": "Diffusion models [17,44\u201347] are a class of generative models that learn the data distribution by building two Markov chains of latent variables. Given a sample X0~Pdata(x):=q(x) and a noise schedule {\u1e9et}f=1, the forward process gradually injects a series of Gaussian noises to the sample until it nearly follows the stan- dard Gaussian q(xt|xt\u22121) := N(xt; \u221a1 \u2013 \u1e9etxt\u22121, \u03b2tI), q(xT|X0) \u2248 N(xT; 0, I). Such process is then followed by the reverse process parameterized by 0, where the model learns to reconstruct the original image by iteratively denoising from a pure Gaussian noise p(x) = N(0,I) as po(xo:T) = p(x\u0442) \u041f=1 Po (Xt-1|Xt). Ho et al. [17] simplifies the objective to learn a noise estimator \u03b5\u03b8:\n\nLDM (0) = Exo,\u20ac,t [||\u20ac \u2013 \u20ac0(\u221aatxo + \u221a1 \u2013 \u0101t\u20ac; t)||2], (1)\n\nwhere \u2208 ~N(0, I), \u0101t = \u03a0\u22121(1 \u2212 \u03b2\u2084), t ~ U([1,T]), and T is the total number of steps. For simple notation, we omit the timestep t as e(xt; t) = \u20ac(xt).\nLatent Diffusion Models (LDMs) [36] leverage the diffusion process within the latent space rather than in the pixel space, utilizing a pre-trained autoen- coder. By mapping the input x into a latent space with the encoder E, z = E(x), an LDM is trained to predict the added noise in the latent space. As this paper mainly discusses LDMs, we use xt instead of zt. Text-to-image models addition- ally take inputs of the text embedding, cp = Etxt (Cp), paired with an image x, where Etxt is the text encoder and cp is a text prompt. To enhance the quality of text conditioning, the classifier-free guidance (CFG) models both conditional and unconditional noise estimates by randomly replacing cp with the embedding of an empty string c\u00f8 = Etxt("}, {"title": "3 Method", "content": "We present Human Feedback Inversion (HFI), an innovative framework for con- cept removal guided by human feedback. Unlike previous methods that rely on predefined text-based concepts, i.e., a specified target word like \"nudity\", real- world inappropriate concepts are often nuanced and challenging to express in"}, {"title": "3.1 Collecting and Modeling Human Feedback", "content": "We first generate images, on which human feedback will be collected, using the original model and a set of simple prompts containing the target concept word c detailed in Appendix B. Subsequently, two main settings can be considered for gathering human feedback. The design of how and what kind of feedback to collect during this process is crucial. In cases where the content the model should not generate is well-defined and clear annotation guidelines exist (e.g., nudity), binary feedback for each image can be collected. However, this does not apply to all scenarios, with a notable example being an artist's style. Suppose that an artist requests the removal of not only specific works but also the overall style from the model, then the criteria may not be clear. In such cases, ranking feedback can be collected. This involves presenting annotators with M random images and requesting them to rank these images ranging from 1 to \u041c.\nFollowing this, we need to train a reward model ry, which accepts an input image x and outputs a score ry(x) \u2208 R for the concept c. For binary feedback, we follow the MSE-based approach proposed in Lee et al. [20],\n\nLMSE(\u03c8) = E(x,y)~Dhuman [(ry(x) - y)\u00b2], (2)\n\nwhere Dhuman consists of images x and corresponding binary annotations y \u2208 {0, 1}. Here, '1' indicates the presence of the harmful concept (e.g., with nudity), and '0' is the absence (e.g., without nudity), answered by human annotators."}, {"title": "3.2 Inverting Feedback into Embeddings", "content": "Based on the feedback received in Sec. 3.1, we transform the corresponding con- cept into a soft token. Inspired by recent inversion techniques [12,35], we utilize TI [12], as it can easily replace or be combined with text tokens from previous approaches. The goal is to find a soft token v* that maximizes the expected reward of generated images, i.e., v* = arg max, Ex~po (x|v) [ry(x)], where v is the soft word in the token space. Utilizing the reward model ry in Sec. 3.1, we opti- mize the following objective of the reward-weighted negative log-likelihood [20] to get the optimal token v* that best captures the text concept c:\n\n\u03c5* = arg min Ex~po (x\\c) [\u2212ry(x) log pe(x|v)]. (4)\n\nInverting too many images may struggle with concept extraction by prioritizing image reconstruction over it, as shown in Gal et al. [12]. Also, we empirically found that inverting it with a small learning rate may fail to capture the concept, which requires additional hyperparameter tuning. To simplify such a burden, we use a reward-weighted sampler with images of the top-K scores, which is equivalent to the reward-weighted loss in Eq. (4). We use the learning rate of 5 x 10-4, the default value in the HuggingFace's implementation\u00b9, and K = 50."}, {"title": "3.3 Removing Learned Concepts with Self-Distillation", "content": "After obtaining soft tokens through the proposed HFI method in Sec. 3.2, one can adapt them to existing text-based concept removal methods, enabling the desired removal of concepts using the obtained soft tokens instead of text tokens. While HFI alone proves effectiveness in Tabs. 1 and 2, we propose a fine-tuning method termed Safe self-Distillation Diffusion (SDD) to achieve an even greater impact,"}, {"title": "4 Related Work", "content": "Numerous attempts have been made to address harmful content generation, including the use of external classifiers for image filtering during inference, such as the safety checker in Stable Diffusion (SD) [36]. However, these approaches"}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Baselines and Evaluation Protocols", "content": "We compare the performance of our method with the original SD and previous methods. In Tabs. 1 and 2, SD [36] indicates the original Stable Diffusion model, and NEG is the simplest baseline of SD with the negative prompt, i.e., the noise conditioned on the target concept is used instead of the unconditional one, thus negating the target concept. We consider two inference-time techniques: SLD [40] and SEGA [3]. For fine-tuning methods, we consider the following three methods: ESD [13], FMN [52], and UCE [14]. For all fine-tuning methods, including ours, we use the artist name (e.g., \"Vincent van Gogh\") or the word (e.g., \"nudity\" and \"bleeding\") for the target concept in text. When applying HFI to these methods, HFI token v* replaces the text word. Refer to Appendix B for details."}, {"title": "6 Conclusion and Discussion", "content": "In this work, we introduced a novel concept removal framework within a text- to-image diffusion model through human feedback inversion and self-distillation- based fine-tuning. By leveraging human feedback to train soft tokens, we effec- tively safeguarded the diffusion model from harmful concepts, addressing the am- biguity in defining text-based concepts. We further proposed a self-distillation- based diffusion model fine-tuning algorithm, enhancing the efficacy of concept removal. We believe our research lays the groundwork for a human-centric ap- proach to the safety and ethical considerations of large-scale diffusion models, paving the way for future advancements.\nLimitation. While our proposed HFI performs well in capturing the intended concept compared to the text, its effectiveness depends on the quality of human feedback and the performance of inversion to soft tokens. Significant disagree- ments in human feedback can compromise the learning process, underscoring the importance of meticulous data collection and reward modeling."}]}