{"title": "Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models", "authors": ["Avanika Narayan", "Dan Biderman", "Sabri Eyuboglu", "Avner May", "Scott Linderman", "James Zou", "Christopher R\u00e9"], "abstract": "We investigate an emerging setup in which a small, on-device language model (LM) with access to local data communicates with a frontier, cloud-hosted LM to solve real-world tasks involving financial, medical, and scientific reasoning over long documents. Can a local-remote collaboration reduce cloud inference costs while preserving quality? First, we consider a na\u00efve collaboration protocol where the local and remote models simply chat back and forth. Because only the local model reads the full context, this protocol achieves a 30.4\u00d7 reduction in remote costs, but recovers only 87% of the performance of the frontier model. We identify two key limitations of this protocol: the local model struggles to (1) follow the remote model's multi-step instructions and (2) reason over long contexts. Motivated by these observations, we study an extension of this protocol, coined MINIONS, in which the remote model decomposes the task into easier subtasks over shorter chunks of the document, that are executed locally in parallel. MINIONS reduces costs by 5.7\u00d7 on average while recovering 97.9% of the performance of the remote model alone. Our analysis reveals several key design choices that influence the trade-off between cost and performance in local-remote systems.", "sections": [{"title": "Introduction", "content": "Today's cloud-hosted frontier Language Models (LMs) can perform data-intensive reasoning: they can generate and refactor code across entire repositories and make decisions based on financial, legal, and medical documents. However, accessing these models is expensive: processing a standard million-token code repository with OpenAI's o1 API costs > $15 per query. At the same time, smaller LMs (1-8B parameters) are rapidly improving and can now run on personal computers (Ollama, llama.cpp) and smartphones (Mehta et al., 2024; Yi et al., 2024; Xu et al., 2024). Yet, today, these small, on-device LMs are used mostly for simple tasks such as tone adjustment and text completion (Gunter et al., 2024). They do not play a role in data-intensive reasoning tasks.\nInspired by the growing literature on multi-agent systems (Wang et al., 2024; Guo et al., 2024), in this work we ask: how can a small, on-device LM collaborate with a frontier LM in the cloud to reduce inference costs on data-intensive reasoning tasks? In particular, we study the communication protocols that govern how the two LMs talk to each other, focusing on the tradeoff between cost and accuracy. To mimic realistic use cases, we study tasks that involve varying levels of reasoning over large volumes of medical, financial, and academic data (Islam et al., 2023; Adams et al., 2024; Dasigi et al., 2021)."}, {"title": "Related Work", "content": "See Appendix A for an extended discussion of related work.\nThis study is inspired by a large body of work that explores how to combine multiple LMs and tools to improve quality and reduce cost of cloud workloads. These include:\n\u2022 Retrieval-Augmented Generation (RAG)\nRAG workflows append retrieved chunks to a large LM's prompt (Lewis et al., 2020; Karpukhin et al., 2020; Lee et al., 2019), mitigating hallucinations and externalizing knowledge. We differ by assigning arbitrary subtasks to a local LM, which can execute longer or more complex instructions on chunks before sending only the final extracted content. This helps reduce communication costs more than purely retrieving and passing raw text.\n\u2022 Multi-LLM collaboration and routing A growing body of work explores multi-agent or multi-LLM systems (Guo et al., 2024; Wang et al., 2024) and model-routing (Chen et al., 2023, 2024a). Typically, these either combine multiple large models or choose one LM from a \"menu\" of models for the entire task. In contrast, we explicitly study a two-model collaboration where the smaller local LM handles extensive on-device context, while the larger remote LM is called on selectively, reducing cloud inference costs.\n\u2022 Compound LM systems A broader line of research integrates LMs with retrieval modules, tool use, or orchestrators (Saad-Falcon et al., 2024; Khattab et al., 2023). While they optimize accuracy or adapt prompts, they do not usually focus on asymmetric edge-cloud costs or local parallelization."}, {"title": "Preliminaries", "content": "We study the tradeoff between the quality of a local-remote system and the cost of running it. We first outline the problem setup and then provide details on how we measure accuracy and cost.\nProblem setup We study language tasks that involve a context c (e.g. a long document), a query q against that context, and a ground-truth answer y (see (1) in Figure 1).\nMeasuring quality We evaluate the performance of S on a dataset $D = \\{(c_i, q_i, y_i)\\}_{i=1}^N$, via a scoring metric s(\u0177i, yi). Here, s(,) is binary (correct/incorrect) and we report accuracy. As baselines, we compare S to \u0177remote ~ RemoteLM(c, q) and \u0177local ~ LocalLM(c, q)."}, {"title": "Minion: A na\u00efve communication protocol", "content": "In this section, we describe MINION, a baseline local-remote communication protocol, which implements a simple free-form conversation between LocalLM and RemoteLM.\nIt begins with system prompts for both models informing them of the query q and that they will be collaborating with another model to answer it (see (3) in Figure 1). Crucially, the system prompt for the LocalLM includes the full context c while the system prompt for the RemoteLM does not. After the system prompts, the two models chat back and forth with one another until the RemoteLM provides a final answer to the query. See Appendix D.1 for a detailed description of the MINION protocol.\nWe compare MINION to a baseline where RemoteLM is given the full context and the query. Excitingly, MINION reduces RemoteLM costs by 38.13\u00d7, 31.3\u00d7, and 20.9\u00d7 on FINANCEBENCH, LONGHEALTH and QASPER, respectively (see Section 6.1 for dataset details). Averaged across these datasets, it closes 87.0% of the quality gap between RemoteLM and LocalLM operating alone.\nTo further close the gap, we analyze MINION conversations and find that in unconstrained chat, RemoteLM often gives LocalLM complicated instructions over long contexts. Appendix E.2 presents micro-experiments illustrating LocalLM's struggles with these instructions:\n1. LocalLM struggles to handle multi-step instructions. Using GPT-40, we generate instructions with varying numbers of sub-parts. We then show splitting sub-parts into separate requests leads to a 56 point performance improvement (see Figure 3).\n2. LocalLM struggles to reason across long contexts. We show how increasing context length from < 1K to > 65K tokens can decrease performance by 13% on a simple extraction instruction (see Figure 3).\nPut simply, these smaller LMs are currently better equipped to answer simple queries on shorter contexts."}, {"title": "Minions: A decomposition-based communication protocol", "content": "Motivated by these observations, we introduce MINIONS, a simple extension of the na\u00efve communication protocol discussed in section 4. MINIONS uses a divide-and-conquer strategy where the RemoteLM decomposes the task into simpler jobs that can be run in parallel (see (4) in Figure 1). Throughout this section we will continue with the example task introduced in Section 3."}, {"title": "Protocol description", "content": "MINIONS protocol is a loop over three steps:\n1. Job preparation on remote. RemoteLM writes code that generates a list of job specifications for LocalLM (see 4(a) in Figure 1).\n2. Job execution and filtering locally. The job specifications are executed locally with the LocalLM, and outputs are filtered (see 4(b) in Figure 1).\n3. Job aggregation on remote. The remote model receives the filtered outputs and decides whether to output an answer or begin another iteration (see 4(c) in Figure 1).\nStep 1: Job preparation on remote. In this step, the RemoteLM generates a list of jobs that the LocalLM will run in parallel. A job is a specification of a subtask, which can be converted into a prompt and sent to the local model. More precisely, a job, t, is a context-instruction pair $t^{(i)} = (\\tilde{c}^{(i)}, \\tilde{q}^{(i)})$. We denote a list of jobs with T = [t(1), t(2), ...]\nTo avoid reading the entire context, we have the remote model generate a Python function, f(c, T), that accepts the full task context c and jobs from the last iteration \u00ee and outputs a new list of jobs T. Specifically, we prompt RemoteLM with the task query q and instruction prompt pdecompose: $f(,) ~ RemoteLM(q, P_{decompose})$. Then, on-device, the function is executed with the context c as the argument producing a list of jobs T = f(c, T).\nThis strategy, which builds on work using LMs to generate code for information extraction (Arora et al., 2023; Li et al., 2023), allows us to decouple the number of unique jobs from the number tokens generated by the cloud model. For example, the code below, which is an abbreviated version of a function that was generated by the cloud model, is less than fifteen lines but can generate hundreds of jobs."}, {"title": "Protocol hyper-parameters", "content": "MINIONS has three hyper-parameters: choice of RemoteLM and LocalLM (model choice), job preparation strategy (scale of parallel workloads on-device), and looping strategy (sequential communication protocol).\nModel choice. Different model sizes (e.g. 3B vs. 8B), families (e.g. QWEN2.5 vs. LLAMA), and generations (e.g. 3.1 vs. 3.2) can be used for both the LocalLM and the RemoteLM.\nScale of parallel workload on-device. MINIONS has three knobs for increasing the degree of task decomposition and thus, workload parallelization: (1) number of tasks per round (i.e. \u201cExtract the ARR for Q1 of 2014\"), (2) number of samples per task (i.e. number of generations created with LocalLM, \u2265 1), and (3) chunk size (i.e. chunk by page, chunk by paragraph, etc; smaller chunks will send more information to cloud). These parameters are configured by RemoteLM.\nSequential communication protocol. In practice, it is important to cap the number of times MINIONS can loop. After the maximum number of rounds, the synthesis prompt is modified to force the model to produce a final answer. The choice of this maximum affects accuracy and cost. The strategy for maintaining context between rounds (simple retries vs. scratchpads) is another important hyperparameter.\""}, {"title": "Results", "content": "Here, we analyze how the design of MINIONS affects cost and quality. Our main takeaways are:\n\u2022 On average across three datasets, MINIONS can recover 97.9% of the performance of remote-only systems while spending 5.7\u00d7 less;\n\u2022 We identify protocol hyper-parameters that let us flexibly trade-off cost and quality;\n\u2022 As local models grow stronger, MINIONS becomes increasingly cost-effective.\nWe structure our analysis around three core design choices:\n1. Model choice How does the choice of local and remote model effect cost and quality? We examine different model types and sizes for LocalLM and RemoteLM in Section 6.2.\n2. Scaling parallel workloads on-device How should we structure parallel workloads on the local device to maximize performance and minimize cost? We highlight how scaling the local workloads can improve performance (Section 6.3) and study the effects on cost.\n3. Sequential communication protocol Can multiple rounds of communication improve quality? At what cost? We explore this trade-off in Section 6.4.\nOur findings are detailed in Sections 6.2, 6.3, and 6.4. Finally, in Section 6.5 we discuss the relationship between retrieval augemented generation and local-remote compute."}, {"title": "Experimental setup", "content": "Datasets and models We evaluate MINIONS on three benchmarks that are well suited for data-intensive reasoning: FINANCEBENCH, LONGHEALTH, and QASPER. FINANCEBENCH tests financial document understanding with complex reasoning over reports. LONGHEALTH focuses on tracking and interpreting longitudinal health records. QASPER assesses question answering over dense scientific papers. See Appendix B.0.1 for details. We use two open-source model families (LLAMA, QWEN2.5) as LocalLM and GPT-40 as RemoteLM (details in Appendix B.0.2)."}, {"title": "Model choice", "content": "This section explores the model requirements and generalization capabilities of MINIONS, examining the local model sizes necessary for effective collaboration, the sensitivity of the communication protocol across different local-remote model pairings, and the longitudinal evolution of MINIONS' performance with advances in model capabilities over time.\nWhat size does LocalLM have to be in order to be effective in Minions? Our results demonstrate that MINIONS starts being competitive with RemoteLM-only baseline at the 3B parameter model scale. When considering both the QWEN2.5 and LLAMA model families running locally, at 1B scale, MINIONS recovers 49.5% of the GPT-40-only baseline performance, 3B scale recovers 93.4% and 8B recovers 97.9% accuracy (see Table 1 for more details).\nHow does the capacity of LocalLM affect the cost-accuracy tradeoff? In our system, LocalLM implicitly acts as an information encoder, optimizing the Information Bottleneck objective (Tishby et al., 2000) by compressing input context while preserving predictive information (see Appendix D.2). To measure this, we analyze the tradeoff between remote \"prefill\" tokens (fewer tokens indicate greater compression) and accuracy (higher accuracy means better retention). Figure 4 shows that as LocalLM size increases, representations become more compressed and accurate, improving Information Bottleneck values. Larger LocalLM models trade local FLOPs for communication, with 7-8B models being 1.53\u00d7 more token-efficient than 1B models. Additionally, the QWEN2.5 family follows a different tradeoff than LLAMA, yielding more compressed representations. This suggests that as small LMs improve, local-remote systems will become increasingly cost-efficient.\nIs Minions sensitive to different local/remote pairs? We ask whether the communication protocol in MINIONS is invariant to changing the model types (i.e. LLAMA vs QWEN2.5 locally and LLAMA vs GPT-40 remotely). Our results indicate that MINIONS performs similarly with different local-remote LM combinations (see the Table 1): varying the LocalLM from QWEN2.5 to LLAMA-3.2, results in performances within \u00b1.05 performance points (see Table 1). Furthermore, we find that holding the LocalLM fixed as LLAMA-3.2-3B and varying RemoteLM from GPT-40 to LLAMA-3.3-70B leads to similar overall performances within \u00b1 0.07 points (see Table 2 in Appendix).\nHow have local / remote model capabilities changed over time, and what effects do they have on Minions? In Table 3, we provide a retrospective analysis demonstrating how the quality of MINIONS would have changed with model releases over time. From 2023 to 2025, the average performance of MINIONS with the best models available has improved from 0.26 to 0.66 (see Table 3 in Appendix). Interestingly, it was only in July 2024 - with the release of GPT4-TURBO and LLAMA-3.1-8B - that MINIONS could have come within 12% of the best frontier model performance at the time (see Table 3 in Appendix)."}, {"title": "Scaling parallel workloads on-device", "content": "In MINIONS, there are three levers for maximizing local compute resources through parallelized, batched processing: (1) number of tasks per round, (2) number of samples taken per task, and (3) number of chunks. We ablate each, showing their impact on performance. We find that (1) and (3) are more cost effective ways of increasing performance.\nHow does the number of tasks per round affect performance? Increasing tasks per round proxies task decomposition, with more sub-tasks enhancing decomposition. Raising tasks from 1 to 16 boosts performance by up to 14 points but doubles RemoteLM prefill costs. Optimal task count varies by query and model, but exceeding 16 reduces performance.\nHow does scaling local samples affect performance? We explore whether increased sampling at an individual {task, context} level improves performance. Increased sampling enables us to better utilize the available compute resources while improving task-level accuracy (Brown et al., 2024). Our results indicate that increasing the number samples from 1 to 32 can improve performance on average 7.4 points, but comes at the cost of 5\u00d7 the RemoteLM prefill costs. This being said, increasing sampling beyond 16 starts hurting task performance as the noise across samples is too large for the remote model to effectively distill the correct answer (Kuratov et al., 2024).\nWhat effect does chunk size have on downstream performance? We test whether increasing local utilization by using more chunks per task improves performance. Our results indicate that increasing # of chunks per task (by decreasing the number of \"pages\" per chunk from 100 to 5) leads to an 11.7 point accuracy lift. However, this lift comes with a 2.41\u00d7 increase in RemoteLM prefill costs."}, {"title": "Scaling sequential communication", "content": "Both the MINION and MINIONS communication protocols feature sequential communication: they allow for multiple rounds of exchange between the local and remote models.\nDoes performance improve as we increase the maximum number of rounds? At what cost? We vary the maximum communication rounds and find it is correlated with accuracy and cost (see fig. 4). By simply increasing the maximum number of rounds in MINION from 1 to 5, we enable a 8.5-point lift in average accuracy across the three tasks (with LLAMA-3.2 on-device). However, this accuracy improvement comes at a cost: each additional round of communication increases the cost by $0.006 per query while boosting accuracy by 4.2 points.\nHow should we maintain context between Minions rounds? We experiment with two sequential protocol strategies: (1) simple retries and (2) scratchpad. See Section 5 for details of these strategies. As shown in Figure 7, both strategies show consistent increases in both accuracy and cost when increasing the maximum number of rounds, with the scratchpad strategy achieving a slightly better cost-accuracy tradeoff. Notably, each additional round of communication with the scratchpad strategy leads to a larger improvement in accuracy (6.1 accuracy points) which are mostly offset by larger increases in cost (8.6 dollars)."}, {"title": "Retrieval-Augmented Generation in the Context of Local-Remote Compute", "content": "We further investigate the interplay between local-remote compute paradigms (e.g., MINIONS) and retrieval-augmented generation (RAG), analyzing their complementary strengths and trade-offs in data-intensive reasoning tasks. Through empirical evaluations, we examine these methods on financial document extraction (FINANCEBENCH) and long-document summarization (BOOOOKSCORE). See Appendix E.3 for a more detailed treatment."}, {"title": "Comparison of Minions and RAG on FinanceBench", "content": "RAG performs well for financial document analysis, where relevant information is found in specific sections. In Figure 8 (left), we compare MINIONS, MINION, and RAG using BM25 and OpenAI's text-embedding-3-small embeddings (Robertson & Zaragoza, 2009; Neelakantan et al., 2022). A chunk size of 1000 characters balances retrieval accuracy and efficiency (Figure 8 (center)).\nAdjusting the number of retrieved chunks allows RAG to optimize quality and cost. When BM25-based RAG retrieves 50+ chunks, it surpasses the performance of a fully context-fed remote model. However, RAG does not match the cost-effectiveness of MINION. When compared to MINIONS, RAG using OpenAI embeddings achieves similar cost-quality trade-offs but may overlook nuanced financial signals across sections."}, {"title": "Comparison of Minions and RAG on Summarization Tasks", "content": "Unlike FINANCEBENCH, RAG struggles with summarization due to its reliance on retrieval. Unlike the financial parsing tasks, summarization requires reasoning over information dispersed across the document. We evaluate MINIONS, RAG (with BM25 andEmbedding retrievals), and a GPT-40-only baseline on BOOOOKSCORE (Chang et al., 2023), a long novel summarization dataset."}, {"title": "Evaluation", "content": "\u2022 Qualitative Analysis: As shown in App. Table 8, MINIONS generates summaries with richer entity mentions and more coherent narratives than RAG. It is also 9.3\u00d7 more token-efficient than GPT-40-only (11,500 vs. 108,185 tokens).\n\u2022 Quantitative Analysis: Using CLAUDE-3.5-SONNET for evaluation, MINIONS achieves near-parity with GPT-40-only, while RAG-based methods underperform (Table 7 in Appendix)."}, {"title": "Discussion", "content": "As local models continue to improve, studying MINIONS will provide valuable insights into evolving workload distribution and the growing role of local compute. As local models continue to improve, systems like MINIONS will become increasingly valuable across a range of important workloads.\nIn this work, we explore two protocols - MINION and MINIONS- for collaboration between on-device and cloud LMs. Our results demonstrate that it is possible to reduce the cost of cloud computing workloads 5-26x by enabling remote LMs to effectively communicate with local LMs and delegate work to them. We explore the broader implications as well as the research opportunities this approach unlocks.\nUser experience Soon, commodity hardware-laptops, smartphones, and IoT devices-will feature powerful GPUs, enabling always-on local models for complex tasks like code refactoring, document analysis, and retrieval. Additionally, this advancement is expected to reduce users' reliance on API-based cloud LMs, leading to lower operational costs."}, {"title": "Local-remote model co-design", "content": "MINIONS demonstrates the promise of \"collaboration\" between local and remote LMs. Future work could advance this approach in two directions. First, the LMs we investigated were trained independently and therefore might not know each others capabilities and limitations. Communication can be made more efficient by training these models for collaboration. Second, model co-design could enable going beyond natural language as the modality of communication; models can exchange more compressed real-valued representations."}, {"title": "Improvement of Minions over time", "content": "Our analysis in Section E.1 highlights the rapid advancements in local model capabilities over the past 15 months. As local-remote capabilities continue to evolve, studying MINIONS will provide valuable insights into shifts in workload distribution and the increasing utilization of local compute resources."}, {"title": "Extended Discussion of Experimental Setup", "content": "In this section we provide additional details on dataset preparation. In order to extend the context length of the problems in LONGHEALTH and QASPER, we make a few modification to the dataset.\nFinanceBench We filter the original FINANCEBENCH to include only the numerical reasoning, resulting in a dataset of length 64. Each sample has an average context length of 142.9K(\u00b179224.32).\nLongHealth In the original instantiation of the LONGHEALTH dataset, each question is paired with a set of medical documents corresponding to a single patient. To increase the complexity of the dataset, we include medical documents from 10 other patients in the context. We evaluate over the entire dataset (400 problems) for results reported in Table 1. Each sample has an average context length of 120.1K(\u00b11,237) tokens. For all ablations in Section 6, we use a fixed subset of 128 problems.\nQASPER Similarly, in the QASPER dataset, the original dataset provides questions that are associated with a single scientific paper. In order to increase complexity, we include 10 other papers in the context. We evaluate over a random subset of 500 problems for results reported in Table 1. Each sample has an average context length of 54281 tokens (\u00b12403). For all ablations in Section 6, we use a fixed subset of 128 problems."}, {"title": "Model Details", "content": "Local Models. For QWEN2.5 we use the following models: QWEN2.5-1.5-Instruct, QWEN2.5-3B-Instruct, QWEN2.5-7B-Instruct. For LLAMA, we use the following models: LLAMA-3.2-1B-Instruct, LLAMA-3.2-3B-Instruct, LLAMA-3.1-8B-Instruct.\nRemote Models. We use GPT-40 and LLAMA-3.2-70B-Instruct, LLAMA-3.1-70B-Instruct\nAll \"local-only\u201d and \u201cremote-only\" experiments are run with temperature of 0.2. For all MINIONS experiments run in Table 1, we run the RemoteLM with a temperature of 0.0 and LocalLM with a temperature of 0.2 for FINANCEBENCH and 0.00001 for QASPER and LONGHEALTH."}, {"title": "Extended Discussion of Cost Model", "content": "Here, we explain in detail the costs of the different communication protocols discussed in this paper-remote-only, MINION, and MINIONS-with a strong focus on the latency of these methods. This section is organized as follows:\n\u2022 Section C.1: We review background on language model inference, to motivate our cost and latency models.\n\u2022 Section C.2: We present mathematical models for the latency of the remote-only, MINION, and MINIONS protocols.\n\u2022 Section C.3: We present Proposition C.1, an upper bound on the total latency of MINIONS, relative to that of the remote-only model, demonstrating that MINIONS is not much slower than the naive approach of performing the full query in the cloud. As an example, we show that a Llama-8B model on a GTX-4090 GPU collaborating via MINIONS with a Llama-405B model on a 8\u00d7H100 server is at most 4.75\u00d7 slower than the remote-only protocol."}, {"title": "Background on language model inference", "content": "Language model inference consists of a sequence of forward passes through a model, one for prefill (i.e. input) followed by one for each additional token generated (i.e. output). At low/medium batch sizes, each forward pass after prefill is I/O bound, meaning the time it takes to load weights from memory exceeds the time it takes to actually compute the output. As the batch size increases, the computational cost of the forward pass eventually exceeds the I/O cost. Strikingly, for most models and hardware, this happens at a batch size"}, {"title": "Latency models for all protocols: Remote-only, Minion, MinionS", "content": "We now model the latency of each of these protocols (remote-only, MINION, MINIONS). We will then use these results in the following section to upper bound the latency of MINIONS by a scalar multiple of the latency of the remote-only protocol.\nFirst, we introduce the following assumptions and notation:\n\u2022 We assume we have a local GPU (e.g. RTX-4090) with peak compute $F_l$ (flops/sec), and peak bandwidth $M_l$ (bytes/sec), and a remote GPU (e.g. H100) with peak compute $F_r$ (flops/sec), and peak bandwidth $M_r$ (bytes/sec),\n\u2022 We also assume for now simple transformer architectures for both the local and remote models:\nLocalLM: $L_l$ layers, each with $8d_l^2$ params in MLP (Up/down projections each of size $d_l \\times 4d_l$, and 4d parameters in the $W_{Q,K,v,o}$ projections. The total memory required for the (non-embedding/LM head) parameters is thus $P_l = 2 \\cdot 12L_l d_l^2$. For simplicity, we assume the memory for the LM head is small relative to $P_l$.\nRemoteLM: Equivalent architecture to the LocalLM, but with $L_r$ layers, $d_r$ hidden dimension, and $P_r$ total non-embedding/LM-head parameter memory (again assumed to be much greater than the number of LM head parameters).\n\u2022 We model the number of input/output tokens of each protocol as follows, letting n denote the number of tokens in the original document:\nRemote-only: $n$ prefill tokens and $n_{out}$ decode tokens. Note that we assume-here and below that the number of tokens in the query is negligible relative to n. We assume $n \\gg n_{out}$ so we can effectively ignore the KV-cache load time for the output tokens.\nMinion: For LocalLM, we assume $n$ prefill tokens and $n_{out}$ decode tokens. For RemoteLM, we assume $n_{out}'$ prefill tokens, and $n_{out}$ decode tokens. In the case of multiple rounds of communication, the KV cache for the document can be stored to avoid recomputation.\nMinions: For LocalLM, we assume $n/c$ prefill tokens per chunk (c chunks total), and $n_{out}'$ decode tokens per job (though we assume only p fraction of output jobs do not abstain). For RemoteLM, we assume $J \\cdot n_{out}' p$ prefill tokens, and $n_{out}$ decode tokens, letting J = cks denote the total number of jobs in MINIONS (c chunks, k instructions, s samples). In the case of multiple rounds of communication, the KV cache for each document chunk can be stored to avoid recomputation.\n\u2022 Throughout, we use the fact that a [m\u00d7n] [n\u00d7k] matmul takes 2 mnk flops, and assume model parameters are stored in half-precision (2 bytes/param).\nWe are now ready to present the latency models for the three protocols (remote-only, MINION, MINIONS)."}, {"title": "Remote-only", "content": "\u2022 Prefill: We are compute bound, so time is approximately given by total-flops/$F_r$. We can break down total-flops into the matmuls (MLP up/down projections, and QKVO operations) and attention operations.\nMatmuls: $2\\cdot 12n d_r^2$ per layer. Equivalent to a $[n \\times d_r] \\cdot [d_r \\times 12d]$ matmul.\nAttention: $2\\cdot n^2d_r$ per layer. Equivalent to $[n \\times d_r].[d_r\\times n]$ matmul.\nTime: $L_r (24nd_r^2 + 2n^2d_r)/F_r = (nP_r + 2L_rd_rn^2)/F_r$.\n\u2022 Decode: We are memory bound (batch size 1 for Minion), so time is approximately given by total_memory/$M_r$ per decode step. We can break down total_memory into model parameters and KV cache.\nModel parameters: $2\\cdot 12d_r^2$ bytes per layer.\nKV-cache: $2\\cdot 2nd_r$ bytes per layer (K and V are each [n \u00d7 $d_r$] matrices).\nTime: $L_rn_{out} (24d_r^2+4nd_r)/M_r = n_{out}(P_r+4L_rd_rn)/M_r$.\nTotal time is given by the sum of prefill and decode times:\n$T_{remote} = \\frac{nP_r+2L_rd_rn^2}{F_r} + \\frac{n_{out} (P_r+4L_rd_rn)}{M_r}$"}, {"title": "Minion", "content": "The latency of the LocalLM in the MINION protocol can be modeled equivalently to the latency of the remote only protocol, but replacing the remote parameters with the corresponding local ones. Thus, total local latency is:\n$T_{MINION}^{local} = \\frac{nP_l + 2L_ld_ln^2}{F_l} + \\frac{n_{out}(P_l+4L_ld_ln)}{M_l}$\nThe total remote latency can also be expressed using these same equations, but with $n_{out}'$ prefill tokens, and $n_{out}$ decode tokens.\n$T_{remote}^{MINION} = \\frac{n_{out}' P_r+2L_rd_r(n_{out}')^2}{F_r} + \\frac{n_{out} (P_r+4L_rd_rn_{out}')}{M_r}$"}, {"title": "MinionS", "content": "The LocalLM latency of the MINIONS protocol has some important differences from the MINION protocol-the prefill computation avoids cross-chunk attention (which saves time)", "below": "n\u2022 Prefill: We are compute bound", "operations.\nMatmuls": 2, "d_l": "cdot [d_l \\times 12d_l", "n/c$).\nAttention": 2}, {"d_l": "cdot [d_l \\times n_c", "matmuls.\nTime": "L_l\\cdot (24nd_l^2 + 2n^2d_l/c)/F = (nP_l + 2L_ld_ln^2/c)/F$.\n\u2022 Decode: We will now assume we are compute bound during decode, because we have many jobs (ks) per chunk, and many chunks (c) per document, which we can batch together. Thus, time is approximately given by total_flops/$F_l$ per decode step. We can break down total-flops into matmuls and attention"}]}