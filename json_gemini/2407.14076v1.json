{"title": "Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field", "authors": ["Tobias Kerner"], "abstract": "Abstract-There are many cases where LLMs are used for specific tasks in a single domain. These usually require less general, but more domain-specific knowledge. Highly capable, general-purpose state-of-the-art language models like GPT-4 or Claude-3-opus can often be used for such tasks, but they are very large and cannot be run locally, even if they were not proprietary. This can be a problem when working with sensitive data. This paper focuses on domain-specific and mixed-domain pretraining as potentially more efficient methods than general pretraining for specialized language models. We will take a look at work related to domain-specific pretraining, specifically in the medical area, and compare benchmark results of specialized language models to general-purpose language models.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have gained increasing popularity in recent times. It is not unusual to see companies employ them as assistants on websites or employ them directly in workflows like the development of internal IT systems. They also have growing relevance in specialized fields like medicine. LLMs could, for example, assist doctors with diagnoses or explain complicated medical situations to patients in simple language. For models to be used reliably, it is important for them to perform well in the domains of their intended usage. High-performance general-purpose models currently include GPT-4 and claude-3-opus, which can be applied to most tasks with usually good results.\nHowever, due to their large size and most state-of-the-art (SOTA) general-purpose models being proprietary and only accessible through an API, it is generally not possible to run these LLMs locally. This is a big problem when reliability and privacy are the main concerns. When working with sensitive patient data or with an unreliable internet connection, using an external API is often not feasible.\nA possible solution to this problem would be to use smaller, specialized LLMs that perform well on tasks in a certain domain and whose performance in other domains is irrelevant. Small models benefit from comparatively fast inference times, lower latency, and less expensive training. Their reasonably small size allows them to be hosted on local, consumer-grade hardware. Due to their lower parameter count and limited learning capacity, small models inherently perform worse than large models, given similar training conditions such as the amount and quality of data used for training. Focusing the training on a specific domain and ignoring capabilities in all other domains lets us circumvent this limitation to some degree by not forcing the model to learn irrelevant or out-of-domain information and allowing it to fully focus on memorizing domain-related information.\nBut how can we create specialized models, and can they really be as good as large general-purpose models in their domain? To answer these questions, we will take a look at different methods of training LLMs, domain-specific datasets and compare the benchmark results of specialized models to general-purpose models in benchmarks related to their domain. This paper will focus on the medical domain for all examples and comparisons."}, {"title": "II. PRETRAINING", "content": "Pretraining is the most fundamental step in creating in-telligent and capable LLMs. During this training step, the model learns the structure of natural language and tries to memorize as much of the training data as possible. For causal language models, for example, this happens by inputting token sequences into the model and letting it predict the next token in the sequence. The closer the prediction is to the true next token in the sequence, the smaller the loss. The loss is used to adjust the weights of the model, in order to minimize the loss. When the model learns the contents of the data and improves its predictions, the loss decreases. Pretraining teaches the model general knowledge and language understanding, but not to answer in a chat-like way, as many end-users are familiar with. For this usage, a further pretraining step is often necessary, where the model is trained to reply in a certain format. Usually, finetuning impacts the during pretraining learned knowledge only slightly, since finetuning is a relatively short process compared to pretraining and should not change the models weights a lot."}, {"title": "A. General Pretraining", "content": "General pretraining is considered the default method for training large language models and works well in most cases. There are many large datasets that can be used for general pretraining without much modification. They contain enough varied data to teach the model general language understanding and general knowledge, even in multiple languages. During training, the model will try to remember as much of the knowl-edge in the provided data as possible in order to minimize its training loss. Large models may be able to learn most of the data, but smaller models will struggle with this due to their inherent parameter limit. This causes them to memorize all information in the data a little bit, but nothing well. However, in the context of training a specialized model, there are ways to circumvent this issue."}, {"title": "B. Domain-Specific Pretraining", "content": "If the model will be used for tasks in a specific domain only, we can mostly avoid this issue by simply removing everything from the data that is not related to the target domain, leaving us with a domain-specific dataset. When training a medical LLM for example, it is to be expected that a lack of training data on poems or music, for example, should not affect its performance on medical-related tasks. This is only possible if there is enough data left in the domain-specific dataset for the model to get an understanding of natural language from it while not overfitting [1].\nThe assumption is that the less irrelevant information the model has to learn, the better it can learn the relevant infor-mation. This should make it possible to decrease model size and accelerate training without a drastic decrease in domain-related task performance. Even though a reduced parameter count will decrease the model's complexity and likely limit its ability to understand very complex data structures, this should not be an issue for simpler tasks or tasks that rely mostly on knowledge. An example of this in the medical field would be the task of translating a detailed medical report into simple language for the patient to understand, where the model has to know the concept and meaning of medical terms in order to rephrase the report."}, {"title": "C. Mixed-Domain Pretraining", "content": "Mixed-domain pretraining, also called continued pretrain-ing, can be used if there is not enough data in the domain-specific dataset for the model to gain an understanding of natural language from it. We can first train the model on a general-purpose dataset. This will enable the model to learn natural language and, as mentioned before, a bit of all the different domains in the data. This model with a general understanding of language can be used as a base for continued pretraining on the domain-specific dataset. Since this dataset will likely not include many of the things that the model has memorized previously, training on it will result in the model gradually forgetting most of them, which we can ignore when pretraining for domain-specific tasks. If physical resources are limited and training an LLM from scratch is not feasible, this method can also be applied using a third-party general LLM as a base for continued pretraining.\ncontinued pretraining on domain-specific data after training on general data can lead to strong improvements of domain-specific models compared to pretraining on general data only. It sets a stronger knowledge foundation for in-domain fine-tuning. Their experiments show that small models struggle with large amounts of knowledge and perform better on data specialized in a single domain. [2]\nHowever, if there is enough domain-specific data for domain-specific pretraining without general pretraining before, performance on domain-specific tasks may be better with domain-specific pretraining than mixed-domain pretraining. While transfer-learning by pretraining on general data is useful for models specialized on domains with low amounts of data, it could be harmful for domain-specific models like medical models. This may be because biomedical texts are substantially different from general texts or different domains, so the transfer-learning might actually have a negative effect. [1]\nThis negative effect may arise from the embedding layer and the first few layers in the model not benefiting from an early focus on domain-related data, which could enable more efficient processing of complex domain-related data in further layers."}, {"title": "III. DATASETS", "content": "There are many publicly available datasets that can be used for pretraining LLMs, including general datasets as well as domain-specific ones. They are created by collecting data from web scraping, books, articles, or other text-based sources and then processing / cleaning that data. Their sizes can range from a few kilobytes to terabytes."}, {"title": "A. General Datasets", "content": "General-purpose datasets contain various types of content and tend to be a lot larger than domain-specific ones. Their extensive size allows the model to gain a solid understanding of language, as well as broad general knowledge.\nIt is to note that CommonCrawl-data is used partially in many other datasets, such as RefinedWeb [6]. It contains an extensive collection of raw scraped websites, which can be filtered, cleaned, and processed for the creation of new, refined datasets. A large part of the raw data gets filtered out during language identification, during which they keep only English texts, and removal of duplicates. [6]"}, {"title": "B. Medical Datasets Overview", "content": "There are publicly available domain-specific datasets as well. They can be used to further refine a model's knowledge and understanding in a specific area.\nSome of the well-known medical datasets include the fol-lowing: MeDAL consists of PubMed-abstracts with around 3 abbreviations per abstract. The authors goal was to create a dataset that teaches LLMs to handle medical-related abbre-viations better [7]. MedDialog contains English and Chinese dialogues. The English dialogues have exactly 2 utterances per dialogue, with an average length of 86.5 tokens per utterance. The Chinese dialogues are longer, with an average of 3.3 utterances per dialogue up to a maximum of 198 utterances in one dialogue and an average of 55.6 tokens per utterance [8]. MedQA consists of English and Chinese question-answer pairs, collected from medical board examinations in the USA, China, and Taiwan [9]. PubMedQA contains question-answer pairs that are based on PubMed abstracts. It is split into 1k expert-annotated, 61.2k unlabeled, and 211.3k artificially gen-erated QA-pairs [10]. MedMCQA is designed to imitate real-world medical entrance exam questions. It contains questions in natural language with four possible answers, one correct answer, and an explanation for each [11]."}, {"title": "C. Creating Domain-Specific Datasets", "content": "One of the biggest problems regarding training language models is the amount of high-quality data available. For large-scale domain-specific training, we will likely need more domain-related data than is available. For some domains, there may not even be a publicly available dataset yet, or it does not fulfill some requirements. In this case, we would have to create our own dataset. There are several steps we have to take for creating our own domain-specific dataset, including the acquisition of raw data, filtering and cleaning it, and processing it into a format suitable for training.\nFor acquiring raw data, we can either collect it ourselves by means of webscraping, using a dump of already scraped data such as CommonCrawl, or collecting multiple datasets that are at least slightly related to our domain. Large-scale webscraping is a time- and resource-intensive task, so it is recommended to consider using data from CommonCrawl instead. The team behind it releases regular updates to the dataset with newly scraped websites. Due to its extensive size, it is more than likely to contain data related to our domain. [3]\nAssuming we chose to use CommonCrawl as the source for our raw data, we can now filter it for data related to our domain, which would be medicine in our case. One way to do this is by iterating through the scraped websites and looking for medicine-related keywords, saving the websites containing them separately. Other methods to filter for relevant data include Named Entity Recognition, topic modeling (using techniques such as Latent Dirichlet Analysis) or using a pretrained language model to tell us if a text is of medical nature or not. These methods can yield better results than simple keyword-matching, but are more complex. The best fil-tering can likely be achieved by using a combination multiple methods.\nAfter collecting enough in-domain data, it will have to be cleaned. This means removing duplicate sites, as well as noise and irrelevant data embedded inside the relevant text. Noise can, for example, be caused by websites inserting advertisements in the middle of their articles. Keeping this would be harmful to our training since it would likely distract or confuse the model and hence has to be removed.\nThis cleaned data containing medical text could be used for training now, but we can further refine it by creating question-answer pairs for it. This can be especially useful for teaching causality and the relationship between concepts and entities in the text. For this, there are tools like Augmentoolkit. Provided with data in text format, it splits the data into chunks and employs an LLM to extract suitable questions and the corresponding answers from these chunks [12]. Due to the nature of language models, it is not guaranteed that all extracted question-answer pairs are correct, even assuming that the provided data was flawless. But this disadvantage would likely be negligible in comparison to the potential benefit of training on data in this format. Of course, we can first train our LLM on the normal cleaned data and then use this refined question-answer data later for finetuning."}, {"title": "IV. PERFORMANCE OF SPECIALIZED LLMS", "content": "As mentioned previously, specialized LLMs are usually smaller than general-purpose LLMs. They are trained on less out-of-domain data, speeding up training time and reducing compute cost. In the following, we will take a look at medical LLMs trained with domain-specific pretraining and mixed-domain pretraining and compare their performance to general-purpose LLMs of equal or larger size in medical-related tasks."}, {"title": "A. Domain-Specific Pretrained Models", "content": "A well-known medicine-focused model is PubMedBERT. It is based on the BERT architecture [13] and trained on 14 mil-lion PubMed-abstracts with a total length of 3.1 billion words. The model was trained from scratch, only using the BERT architecture but not the pretrained weights of the original BERT model. Its tokenizer uses vocabulary newly generated with the WordPiece algorithm, based on the PubMed-data that was also used for training. [1]\nBioMedLM is a 2.7B model trained exclusively on biomed-ical text, including PubMed-abstracts and full articles. When finetuned to reply in the format required by a specific task, BioMedLM is competitive with much larger models, as shown by its score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam. Even though BioMedLM only has 2.7B parameters, it beats GPT-3.5 with 175B pa-rameters on MedMCQA and is on par with it in the MedQA and PubMedQA benchmarks. However, it seems to struggle against larger models in MMLU general medical knowledge benchmarks. [14]\nApollo is a family of small LLMs ranging from 0.5B to 7B parameters in size. It is trained using a custom multilingual medical dataset called ApolloCorpora. This dataset is based on books, papers, and encyclopedias related to the medical domain, as well as doctor-patient dialogues with a total of 2.5B training tokens. [15]"}, {"title": "B. Mixed-Domain Pretrained", "content": "HEAL is based on LLaMA2-13B, applying continued pre-training to the weights of the original model with 14.89B tokens of medical data. It surpasses GPT-4 on PubMedQA, with a score closely behind Med-PaLM-2. It seems to struggle on the MedQA benchmark, with only a small improvement over the base LLaMA2-13B and quite behind another LLaMA-13B-based model. It is worth noting that its score of 47.2 on MedQA is around the same level as GPT-3.5. [16], [17]"}, {"title": "C. Benchmark Comparison", "content": "As shown in table 2, the specialized LLMs perform well in in-domain tasks with respect to their size when compared to not domain-specifically trained models. Especially BioMedLM shows strong performance, beating GPT-3.5 in MedMCQA, PubMedQA, and MedQA with only 2.7B parameters. The Apollo models perform good as well, with Apollo-7B improv-ing upon BioMedLM's MMLU score, slightly beating GPT-3.5. This is surprising, considering it was trained on 2.5B tokens of data only, compared to BioMedLM's 300B tokens. With a size of just 0.5B parameters, Apollo-0.5B reaches solid scores in MedMCQA and MMLU, beating the about 14 times larger LLaMA-2-7B. Interestingly, both GPT-3.5 and GPT-4 perform worse on PubMedQA with 5-shot than with 0-shot,"}, {"title": "D. Comparing Model-Size to Benchmark Score", "content": "Given a high-quality dataset, a model's performance seems to be directly related to its parameter count. According to our visualization in table 3, especially the MedMCQA benchmark,"}, {"title": "E. Further Resource-Optimization", "content": "In the context of our initial question, we asked if LLMs small enough to be run locally on consumer-grade hardware can match the performance of large, general-purpose models that cannot be run locally in certain tasks. We should be able to run a model with 7B parameters on local hardware with around 16GB of VRAM, assuming the weights are saved as (b)float16. But a further reduction in memory requirements and increase in inference speed are possible by quantization of the model's weights.\nCurrent popular quantization formats include gguf, exl2, awq, and more. Gguf allows for some of the model's layers to be loaded into cpu-memory if the model does not fully fit into gpu-memory, at the cost of inference speed due to slower inference on the cpu. While the original Apollo-7B with (b)float16 weights would require slightly less than 16GB of gpu-memory due to each of the slightly more than 7 billion weights each taking up 16 bits / 2 bytes, the Q8_K quantized version only takes up 9.2GB. Most commonly used quantizations go down to Q4, which stores each weight with an average of 4 bits. The fewer bits are used to store weights, the stronger the degradation of the model's performance becomes.\nBelow 4-bit quantization, especially for smaller models (7B and below), the precision is often too low to be of reliable use. The Q4_K_S and Q4_K_M quantized versions of Apollo-7B take up only 5.1 and 5.4 GB of memory, respectively. The difference between the K_S and the K_M versions is that the K_M version employs a less strict quantization, with some more performance-impacting layers being kept at higher quanitization levels, resulting in a slightly higher memory requirement. Some weights are not quantized at all in the gguf model, such as those of normalization layers. Because of this, we do not see a perfect 50% reduction in memory requirements when switching from the 16-bit model to the 8-bit model. Although the gguf-quantized models huggingface-page does not specifically state the precision changes of the model in the quantized versions, the 8-bit gguf quantization is generally considered to have a precision very similar to the original model, making it a viable and, in some cases, even better-suited option than the 16-bit model. [22]"}, {"title": "V. SUMMARY", "content": "Depending on the use case and available resources, domain-specific and mixed-domain pretraining can be a viable and preferable alternative to general pretraining. With a low amount of in-domain data available, mixed-domain pretraining on a large general dataset and then switching to the small in-domain dataset will likely result in a better-performing model than training on a small domain-specific dataset only. These specialized models can have a much lower parameter count but still perform impressively well on in-domain tasks compared to larger, general pretrained models. This makes it possible to run them on consumer-grade hardware, enabling independent local inference. Therefore, domain-specific pretrained LLMs can be a good alternative to api-based SOTA general-purpose models in some cases, such as supporting symptom analysis and rephrasing documents in a medical setting."}]}