{"title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models", "authors": ["Christoforus Yoga Haryanto"], "abstract": "This paper introduces LLAssist, an open-source tool designed to streamline literature reviews in academic research. In an era of exponential growth in scientific publications, researchers face mounting challenges in efficiently processing vast volumes of literature. LLAssist addresses this issue by leveraging Large Language Models (LLMs) and Natural Language Processing (NLP) techniques to automate key aspects of the review process. Specifically, it extracts important information from research articles and evaluates their relevance to user-defined research questions. The goal of LLAssist is to significantly reduce the time and effort required for comprehensive literature reviews, allowing researchers to focus more on analyzing and synthesizing information rather than on initial screening tasks. By automating parts of the literature review workflow, LLAssist aims to help researchers manage the growing volume of academic publications more efficiently.", "sections": [{"title": "1 INTRODUCTION", "content": "The landscape of academic research is undergoing a dramatic transformation, driven by an unprecedented surge in scientific publications. Identifying relevant work, once a manageable task, has evolved into a time-consuming and often overwhelming process [Khan et al., 2003, Davis et al., 2014]. The exponential growth of scientific publications [Silva J\u00fanior and Dutra, 2021, Ioannidis et al., 2021], the need to screen hundreds or thousands of articles, and pressure for rapid-evidence gathering present significant challenges for researchers, potentially compromising research quality [McDermott et al., 2024, Glasziou et al., 2020, Whiting et al., 2016, Page et al., 2021, Wallace et al., 2010].\nIn response to these mounting challenges, the research community has begun exploring automated solutions to assist in the systematic literature review process [Silva J\u00fanior and Dutra, 2021, Wallace et al., 2010, Tsafnat et al., 2018, Bannach-Brown et al., 2019]. Recently, there has been a growing interest in evaluating the potential of Large Language Models (LLMs) for this purpose [Agarwal et al., 2024, Joos et al., 2024, Susnjak, 2023]. These advanced AI systems, with their ability to understand and generate human-like text, offer promising avenues for automating various aspects of the literature review process.\nA notable contribution to this emerging field comes from Joos et al. [2024], who recently published an extensive evaluation of using LLMs in enhancing the screening process, with results indicating promising potential for reducing human workload. Inspired by these findings, we present LLAssist, a prototype automation tool based on LLM technology. In its current phase, LLAssist is designed as a lightweight and straightforward solution to efficiently process large volumes of articles, extract key information, and assess relevance to specific research questions, thereby streamlining the literature review process. Eventually, we aim LLAssist to be one of the key components for a human-friendly, automated knowledge base and research platform.\nBy using a predetermined process of key semantics extraction, relevance estimation, and must-read determination, LLAssist provides a simple building block for researchers to expand from the idea. This design choice allows for better adaptability to different research domains and provides researchers with more transparent and interpretable results. It is important to note that while LLAssist shows promise in its current form, it represents an initial step towards more sophisticated literature review automation. The simplicity of its design is intentional, allowing researchers to understand, modify, and build upon its core functionalities as needed for their specific research contexts."}, {"title": "2 METHODOLOGY", "content": "Our methodology addresses the primary research question: How can automation of screening using large language models improve the efficiency and effectiveness of systematic literature reviews in the face of exponentially growing scientific publications?\nIt consists of two main parts: 1. the design and implementation of LLAssist, and 2. the experimental evaluation of its performance.\n2.1 LLAssist: An LLM-based Literature Screening Tool\n2.1.1 Data Input\nThe program accepts two primary inputs: a CSV file containing tabular metadata and abstracts of research articles, and a text file listing the research questions of interest. While the current implementation parses the file programmatically, the progress of LLM may allow it to process tabular data reliably despite the current challenges [Dong and Wang, 2024].\n2.1.2 Article Processing\nFor each article in the input CSV, LLAssist performs the following steps:\nKey Semantics Extraction The NLPService extracts topics, entities, and keywords from the article's title and abstract. The implementation sends an extraction prompt with the title and abstract to the LLM.\nRelevance Estimation For each research question provided, LLAssist estimates the article's relevance using the following criteria:\n1. Binary Relevance Decision and Score: A binary TRUE/FALSE and a numerical score (0-1) to indicate how closely the article aligns with the research question,\n2. Binary Contribution Decision and Score: A binary TRUE/FALSE and a numerical score (0-1) to assess the article's potential contribution in answering the research question,\n3. Relevance Reasoning: A brief explanation of why the article is considered relevant.\n4. Contribution Reasoning: A justification for the estimated contribution of the article.\nThe current implementation sends the title, abstract, and key semantics previously extracted. An article is considered relevant or contributing if its score exceeds 0.7. This threshold can be adjusted based on the researcher's needs.\nMust-Read Determination Based on the relevance and contribution scores across all research questions, LLAssist determines whether an article is a \"must-read.\" Currently, this is implemented by using the logical OR operation on all the RQ relevance and contribution thresholds. This binary classification helps researchers prioritize their reading list and conclude the flow.\nOutput Generation LLAssist provides two types of output: 1. a JSON file containing detailed information for each processed article, including extracted semantics, relevance scores, and reasoning, and 2. a CSV file presenting the same information in a tabular format, suitable for further analysis or import into other tools. Necessitating other analysis and tools is deliberate to ensure that the human-in-the-loop principle is adhered to by maintaining the visibility of the process [Bu\u00e7inca et al., 2021, Vasconcelos et al., 2023, Bansal et al., 2021].\n2.2 Experimental Evaluation\n2.2.1 Data Collection\nTo evaluate the effectiveness of LLAssist in streamlining the literature review process for LLM applications in cybersecurity, we conducted two separate experiments using manual sampling of publications from datasets from different sources:"}, {"title": "2.2.2 Research Questions", "content": "We formulated four key research questions to guide our automated analysis:\n\u2022 RQ1: How are Large Language Models (LLMs) being utilized to enhance threat detection and analysis in cybersecurity applications?\n\u2022 RQ2: What are the potential risks and vulnerabilities introduced by integrating LLMs into cybersecurity tools and frameworks?\n\u2022 RQ3: How effective are LLM-based approaches in generating and detecting adversarial examples or evasive malware compared to traditional methods?\n\u2022 RQ4: What ethical considerations and privacy concerns arise from using LLMs to analyze and process sensitive cybersecurity data?"}, {"title": "2.2.3 Automated Analysis", "content": "We processed each paper through LLAssist, which performed the following tasks: 1. Extract key semantics (topics, entities, and keywords) from the title and abstract. 2. Evaluate the relevance of each paper to our research questions. 3. Provide relevance and contribution scores (0-1 scale) for each research question. 4. Generate reasoning for relevance and contribution assessments."}, {"title": "2.2.4 Evaluation Metrics", "content": "We assessed the performance of LLAssist based on i) consistency of evaluations across papers, ii) accuracy in matching papers to relevant research questions, and iii) ability to provide meaningful insights and reasoning.\nAdditionally, we are using several different LLM backends: Llama 3:8B, Gemma 2:9B, GPT-3.5-turbo-0125, and GPT-40-2024-05-13 to allow data comparison."}, {"title": "2.2.5 Preliminary Nature of Evaluation", "content": "It is important to note that the assessment of accuracy in matching papers to relevant research questions and the ability to provide meaningful insights and reasoning was conducted in an uncontrolled environment. Knowing that LLM results can be helpful yet inaccurate, we expect the researchers to use LLAssist as a lightweight filtering enhancement tool while following existing methodologies such as PRISMA [Page et al., 2021, Susnjak, 2023]."}, {"title": "3 TECHNICAL IMPLEMENTATION", "content": "The program is designed to work with various LLM providers, including local models (e.g., Ollama Llama 3, Ollama Gemma 2) and cloud-based models (e.g., OpenAI's GPT-3.5 and GPT-4). This flexibility allows researchers to choose models based on their specific requirements, such as processing speed, accuracy, or data privacy concerns\nLLAssist is implemented in C# and utilizes the following key components: 1. Microsoft.SemanticKernel: For interaction with LLM using OpenAI-compatible API, 2. CsvHelper: For reading input and writing output CSV files, and 3. Microsoft.Extensions.Logging: For logging and debugging purposes.\nUnlike many existing research tools, LLAssist is developed in C#, a static and strongly typed language, offering benefits such as improved performance, early error detection, and enhanced maintainability [Nanz and Furia, 2015], suitable for further integration into a bigger enterprise system. Additionally, the system will write the CSV output file incrementally to ensure better durability in case of a crash. The sequence diagram can be seen in Figure 1."}, {"title": "4 EXPERIMENT RESULTS", "content": "There are two parts to this experiments: 1. A small dataset test using search term a: IES, term b: SS, term c: SM. 2. A large dataset test using search term d: SL. The small dataset test uses 4 LLMs: Gemma 2, GPT-3.5, GPT-40, and Llama 3 and the large dataset test only use 2 LLMs: Gemma 2 and Llama 3, both provisioned locally using Ollama on RTX 3090. Time and cost data are measured indirectly from file time and API usage."}, {"title": "4.1 Small Dataset Test", "content": "The small dataset verified the functionality of the system with the following key results:"}, {"title": "4.1.1 Key Semantics Extraction", "content": "LLAssist successfully identified relevant topics, entities, and keywords for each paper, aligning well with the author-provided keywords and terms for all LLM backends. As this data is currently used as the input to the LLM prompt, there is no controlled measurement done in this experiment. Result data can be reviewed in the CSV. Also, note that the metadata for the classifier is generated by the same LLM."}, {"title": "4.1.2 Binary Relevance Decision and Score Distribution", "content": "The binary relevance decision and relevance score distribution obtained experiment are shown in Figure 2. Further, the summary of binary relevance and binary contribution decisions are shown in Table 1.\n1. Gemma 2:9B shows a reasonable distribution of binary relevance classification and relevance score. It tends to give a strong binary decision and classification score compared to all other LLMs with variation across research questions, indicating sensitivity to different topics."}, {"title": "4.1.3 Must-read vs. Discard Ratio", "content": "All small dataset test runs show a relatively low discard ratio, which is expected due to the specificity of the search term, broad research questions, and the low dataset diversity. A comparison between each test run can be seen in Figure 3."}, {"title": "4.1.4 Reasoning Quality", "content": "A manual review of the system provided coherent explanations for its relevance and contribution assessments, offering insights into why each paper was or wasn't considered relevant to each research question. There is no controlled experiment done to measure quantitatively. The LLMs have to output their reasoning to help the researchers in manually discriminating the articles [Vasconcelos et al., 2023] and can be also part of cognitive forcing functions to be the checkpoint before downstream processing [Bu\u00e7inca et al., 2021]."}, {"title": "4.2 Large Dataset Test", "content": "Table 1 shows two types of binary decisions made by LLAssist: the binary relevance and the binary contribution indicator. Referring to the row id: SL-Gemma2 which contains the result of running large dataset test using Gemma 2:9B, the analysis of the larger Scopus dataset (2,576 articles) revealed key insights:\n1. Trend in Relevance: There's a notable increase in potentially relevant articles from 2020 to 2023, with a peak in 2023 (869 articles, 117 must-read). The slight decrease in 2024 reflects the mid-year data collection cut-off rather than a decline in research quality. Additionally, identifying the most relevant articles accurately is more important than finding a large number of potentially relevant articles.\n2. Research Question Specifics: RQ2 (risks and vulnerabilities of LLMs in cybersecurity) consistently has the highest number of relevant and contributing articles across years. It indicates it's likely the most well-defined or central question. RQ1 (LLMs for threat detection) shows a sharp increase in relevance from 2022 to 2023. RQ3 (LLMs for adversarial examples/evasive malware) and RQ4 (ethical considerations) have fewer articles but show an upward trend, indicating more specialized areas of increasing importance.\n3. Must-Read vs. Contributing Articles: While 324 articles (12.6%) are identified as must-read, only 100 articles (3.9%) are classified as potentially contributing. This suggests that LLAssist are more selective in identifying articles that directly contribute to answering the research questions, implying reduced time to manually read the abstract.\n4. Year-over-Year Growth: The number of potentially relevant articles increased significantly from 2020 to 2023, indicating growing research interest in the field. 2023 stands out as a pivotal year with the highest numbers of potentially high-quality and relevant publications across all categories.\n5. Research Question Focus: RQ2 consistently receives the most attention, suggesting that potential risks and vulnerabilities of LLMs in cybersecurity are a primary concern in the field. RQ4, focusing on ethical considerations, shows the least but growing number of relevant articles, particularly from 2022 onward."}, {"title": "5 ANALYSIS AND DISCUSSION", "content": "5.1 Overall Performance\nLLAssist effectively identifies relevant papers, works with various LLM backends, and significantly reduces manual screening time. On the other hand, the system did not utilize all available metadata (e.g., publication year, citation counts) in its relevance assessments, which could have provided additional context. Also, different LLMs behave differently, necessitating more precise prompt tuning. The analysis was also limited to titles and abstracts, potentially missing relevant information contained in the full text of the papers."}, {"title": "5.2 Time and Cost Efficiency", "content": "LLAssist's throughput varies across models and dataset sizes. It processes datasets of 17-37 articles in under 10 minutes, 115 articles in 20-50 minutes, and 2,576 articles in 10-11 hours. Among the models tested, GPT40 emerges as the slowest, processing articles in 24-29 seconds on average. Llama3 is the fastest, consistently quick at 10-11 seconds per article. Gemma2 and GPT35 offer similar speeds, averaging 12-14 seconds for each article processed. The per-article processing times remain consistent across dataset sizes, indicating scalability. This is a significant improvement over human performance [Wallace et al., 2010, Joos et al., 2024].\nCost-wise, GPT-40 is the most expensive at approximately $3.16 per 100 articles while GPT-3.5 offers a more budget-friendly option at about $0.22 per 100 articles. Meanwhile, both Gemma 2 and Llama 3 do not have a set cost due to the ability to run locally without cloud services. Notably, the high discrimination ability of Gemma 2 may help researchers to do the initial screening of many articles without relying on cloud services."}, {"title": "6 FUTURE WORK", "content": "LLAssist's limitations include dependence on LLM quality and input formatting, focus on titles and abstracts, and potential misalignment with human judgment. Future work should aims to incorporate full-text analysis, implement feedback mechanisms, and develop domain-specific models for improved accuracy."}, {"title": "7 CONCLUSION", "content": "In conclusion, LLAssist demonstrated promising capabilities in automating the initial stages of a literature review. Its ability to quickly process and categorize papers offers valuable support to researchers. However, there is room for improvement in utilizing more of the available metadata and fine-tuning the relevance criteria to better differentiate between highly relevant and marginally relevant papers. By leveraging Large Language Models, it offers researchers a valuable tool to efficiently process large volumes of academic literature. While not a replacement for human judgment, LLAssist can significantly reduce the time spent on initial screening and help researchers focus their efforts on the most promising and relevant articles for their research questions, allowing researchers to focus on high-quality work, to achieve higher productivity across expertise and industry."}, {"title": "8 AVAILABILITY", "content": "The source code for LLAssist is freely available at https://github.com/cyharyanto/llassist. We encourage researchers to use, modify, and contribute to this tool to further advance the efficiency of academic literature reviews across various disciplines."}]}