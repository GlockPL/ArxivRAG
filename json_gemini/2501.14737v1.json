{"title": "EvalSVA: Multi-Agent Evaluators for Next-Gen Software Vulnerability Assessment", "authors": ["Xin-Cheng Wen", "Jiaxin Ye", "Cuiyun Gao", "Lianwei Wu", "Qing Liao"], "abstract": "Software Vulnerability (SV) assessment is a crucial process of determining different aspects of SVs (e.g., attack vectors and scope) for developers to effectively prioritize efforts in vulnerability mitigation. It presents a challenging and laborious process due to the complexity of SVs and the scarcity of labeled data. To mitigate the above challenges, we introduce EvalSVA, a multi-agent evaluators team to autonomously deliberate and evaluate various aspects of SV assessment. Specifically, we propose a multi-agent-based framework to simulate vulnerability assessment strategies in real-world scenarios, which employs multiple Large Language Models (LLMs) into an integrated group to enhance the effectiveness of SV assessment in the limited data. We also design diverse communication strategies to autonomously discuss and assess different aspects of SV. Furthermore, we construct a multi-lingual SV assessment dataset based on the new standard of CVSS, comprising 699, 888, and 1,310 vulnerability-related commits in C++, Python, and Java, respectively. Our experimental results demonstrate that EvalSVA averagely outperforms the 44.12% accuracy and 43.29% F1 for SV assessment compared with the previous methods. It shows that EvalSVA offers a human-like process and generates both reason and answer for SV assessment. EvalSVA can also aid human experts in SV assessment, which provides more explanation and details for SV assessment.", "sections": [{"title": "Introduction", "content": "Software Vulnerabilities (SVs) are mostly caused by insecure code that can be exploited to attack software systems (Dissanayake et al. 2022; Khan and Parkinson 2018), and further cause security issues such as systems susceptible to cyber-attacks, and data leakage problems (Le, Chen, and Babar 2023). In the past ten years, the number of SVs has been increasing rapidly (Smyth 2017), rising from 5,697 in 2013 to 29,065 in 2023 (Statista 2024). Therefore, SV assessment is a crucial yet challenging problem in security.\nThe expert-based Common Vulnerability Scoring System (CVSS) (CVS 2024a) is a widely adopted framework for assessing SVs. CVSS provides metrics to quantify the exploitability, impact, and severity metrics of SVs (CVS 2024c; Foreman. 2019). Such procedures are labor-intensive and suffer from inefficiencies due to the complexity of vulnerabilities (Bilge and Dumitras 2012; Feutrill et al. 2018). Traditional automated approaches for SV assessment, primarily reliant on user-submitted SV reports, are hampered by substantial delays-over 82% of reports are filed more than 30 days post initial detection (Thung et al. 2012). Recent studies aim to automate assess SV via commits (Le et al. 2021; Zhou et al. 2021), significantly reducing reliance on manual expert evaluations and accelerating the assessment process.\nHowever, the existing methods still pose several major challenges that need to be addressed: Firstly, the existing methods depend on extensive labeled data, which is difficult to evolve in practice. Specifically, the CVSS framework updates rapidly, evolving from CVSS v2 to v3, and subsequently to v3.1 (CVS 2024d,b,c). It is time-consuming for experts to furnish high-quality assessments in new standards. For instance, the National Vulnerability Database (NVD) (NIST 2024) and the Common Vulnerabilities and Exposures (CVE) (CVE 2024) lists maintained by Mend (White-Source 2023) only contains 699 complete vulnerability entries for C++ from 2013 to 2023. Consequently, the labeled data present difficulties in industry and limit practical value in real-world scenarios, potentially leading to unreliable performance of existing methods. Second, the previous commit-level SV assessment studies have not started to use the new standards (CVS 2024c), which incorporate additional metrics (e.g., Scope and User Interaction) to enhance the complexity of vulnerability and become the current standard in industry. Additionally, most of the existing techniques solely predict SV scores of CVSS. They provide no idea about how the vulnerability assessment is derived from the input, making the results difficult to interpret and verify.\nTo mitigate the above challenges, we propose a multi-agent EVALuators team to autonomously deliberate and evaluate various aspects for Software Vulnerability Assessment, called EvalSVA. Specifically, we propose a multi-agent-based framework to simulate vulnerability assessment strategies in real-world scenarios, which employs multiple Large Language Models (LLMs) into an integrated group to enhance the effectiveness of SV assessment in limited data. We also design diverse communication strategies to autonomously discuss, which conduct comprehensive processes and assess different aspects of SV. Moreover, to testify the potential of the multi-agent framework in the real-world scenario, we construct the first multi-lingual vulnerability assess-"}, {"title": "Methodology", "content": "In this section, we elaborate on the overview of EvalSVA by first introducing the SV assessment task formulation and then the proposed multi-agent evaluators."}, {"title": "Software Vulnerability Assessment Formulation", "content": "Common vulnerability scoring system. The CVSS has emerged as the definitive framework for evaluating the severity of SVs. In this paper, we first employ CVSS v3.1 for the commit-level SV assessment. In this paper, we focus on the prediction of Base Metrics due to their broader applicability. These metrics encapsulate the intrinsic attributes of a vulnerability that remain constant over time and across different user environments;\nSV assessment task formulation. As shown in Figure 1(a), a vulnerability-related commit can be denoted by the input $X$ (the template of input $X$ are shown in Appendix) and SV tasks can be performed in all metrics simultaneously. The goal of EvalSVA is to learn a mapping $F : X \\rightarrow Y$ from input $X$ to the output signals $Y$. Specifically, the output signals for SV assessment tasks can be broadly classified into three aspects: Exploitability, Scope, and Impact. As shown in Figure 1(b), the output signals consists of the security of Attack Vector (AV) $Y_{AV}$, Attack Complexity (AC) $Y_{AC}$, Privileges Required (PR) $Y_{PR}$ and User Interaction (UI) $Y_{UI}$ for exploitability aspect, the security of Scope Change (S) $y_{S}$ for scope aspect, and the security of Confidentiality (C) $y_{C}$, Integrity (I) $y_{I}$ and Availability (A) $y_{A}$ for impact aspect. We then briefly introduce each task of the SV assessment in the CVSS v3.1 as follows:\n(1) Exploitability: The exploitability reflects the properties of the vulnerability that lead to a successful attack. In this paper, we use the four metrics to represent the vulnerability exploitability, including AV, AC, PR, and UI. Specifically, the AV metric reflects the attack path by which vulnerability exploitation is possible. AC metric describes the difficulty of conditions beyond the attacker's control to exploit the vulnerability. PR metric assesses the level of authority or access rights that an attacker must acquire to successfully exploit the vulnerability. UI metric distinguishes between vulnerabilities that can be exploited solely by attackers and those requiring involvement from a separate user process. For example, Figure 1(a)'s original code (shaded in red) contains"}, {"title": "Multi Agent Evaluators", "content": "Multi Agents and Software Vulnerability Assessment\nVarious studies (Gao et al. 2023; Peng et al. 2023; Deng et al. 2024) have shown that LLM-based methods are utilized to boost interpretability and practical values behind the classical supervised-based method. Despite the capability of a single LLM to handle a wide range of tasks across multiple domains (Zheng et al. 2023; Imran, Chatterjee, and Damevski 2023), it continues to encounter significant challenges in SV assessment. This is primarily due to assessing the severity of vulnerabilities entails a complex and consequential process (Croft et al. 2021), which typically requires collaboration among multiple experts rather than relying solely on individual assessments. These complex situations make it difficult for an existing single LLM to perform well in SV assessment. Inspired by the recent advance in multi-agent methods has demonstrated its effectiveness (Li et al. 2023a; Liang et al. 2023; Huang et al. 2024), we design the first multi-agent-based framework for effectively SV assessment, where the agents interact and communicate within a collaborative environment, aiming to emulate the interaction and collaboration strategies in real-world scenarios (Karpinska, Akoury, and Iyyer 2021). We elaborate on the two components in EvalSVA including vulnerability expert agents and communication strategy."}, {"title": "Component", "content": "We provide the details of each component's role and functionality in this section.\n1. Vulnerability Expert Agents. Vulnerability expert agents for evaluators constitute a critical component in EvalSVA, where each individual LLM is regarded as an expert agent for SV assessment tasks. For each task related to SV assessment, we meticulously craft unique prompts tailored to the specific requirements of the task. Each LLM is tasked with evaluating the severity of a vulnerability-related commit and subsequently providing a detailed explanation. The responses generated by all agents are preserved within the chat history. This archive of interactions enables subsequent evaluators in future rounds of assessment from prior communications, which mirrors the real-world interactions for SV assessment. It is worth mentioning that each agent evaluates all aspects of the same commit, employing different prompts tailored to specific tasks.\n2. Communication Strategy. Another pivotal challenge involves leveraging references from previous expert analyses to construct new prompts that facilitate further exploration by agents. As previously discussed, assessing the multifaceted aspects of vulnerabilities is an intricate and critical process, we are more concerned with how to refer to other expert responses and interpretations for further SV assessment. As shown in Figure 2, we explore four distinct communication strategies to emulate the processes in the real-world scenarios for SV assessment.\n(1) Referencing the preceding one expert. Each expert agent constructs its response based on the input from the immediately preceding expert, except the initial agent. We incorporate only the prior agent's response into the current agent's conversational history. It prevents excessive past interactions from influencing present SV assessment results.\n(2) Referencing the previous communication. The expert agents sequentially generate their responses in a predetermined order. This procedure involves concatenating all previous responses into the chat history to construct the assistant's prompt for the next agents. This approach simulates the written communication for SV assessment in the real world, where experts access all prior information and make their judgments accordingly.\n(3) Simultaneous assessment. Every expert agent cannot reference the responses of other experts from the current round but may consider the responses from all experts in the previous round. This method minimizes the dependency of an agent on the responses of other experts and mitigates the influences that could arise from sequential order.\n(4) Summarizer assessment. Building on the strategy (3), each round additionally augments a summarizer, which synthesizes the responses of all experts within the current round"}, {"title": "Experiments", "content": "We construct a new benchmark and evaluate the benefits of EvalSVA, intending to understand the following questions:\nQ1: How does multi-agent of EvalSVA compare to the single-agent for SV assessment?\nQ2: How does EvalSVA perform on different communication strategies for SV assessment?\nQ3: What are the impacts of expert numbers and communication rounds in the EvalSVA?"}, {"title": "Data Preparation", "content": "Securing high-quality datasets comprising vulnerability-related commits for SV assessment is a formidable challenge, necessitating the demand for qualified expertise.\nData Collection: Our initial step involved acquiring open-source vulnerabilities from Mend (WhiteSource 2023), which provides extensive vulnerability entries contributed by a community of experts. For each identified vulnerability entry, we extracted security-related commits (i.e., patches) from platforms such as GitHub, Android, and Chrome, recording their associated project and commit messages.\nData Filter: To ensure the relevance and accuracy of our dataset, we employed a filtering methodology to select commits based on two essential criteria: (1) All SV assessment labels must be complete, and (2) The labels for SV assessments must conform to the evaluation standards established by CVSS V3.1. Additionally, we utilized time-based splits for testing the EvalSVA, aiming to closely mimic real-world scenarios where future unseen data is not available.\nAs presented in Table 1, we have gathered 699, 888, and 1,310 vulnerability-related commits in C++, Python, and Java, respectively. They are collected according to the CVSS v3.1 standard and encompass 105, 129, and 159 types of vulnerabilities across the 160, 307, and 366 projects, respectively."}, {"title": "Dataset Evaluation", "content": "The previous study (Croft, Babar, and Kholoosi 2023) has demonstrated that vulnerability datasets often exhibit quality problems. Therefore, we conducted an evaluation of our dataset in comparison with existing datasets, despite the absence of specific datasets dedicated to vulnerability assessment. Specifically, we randomly select 20 examples from"}, {"title": "Baselines", "content": "We primarily focus on few-shot-based methods for SV assessment. This is due to the insufficiency of labeled data for CVSS v3.1 available in programming languages such as C++, Java, and Python. Despite the limited data, these languages pose significant vulnerability threats.\nWe use the Yin et al. (Yin, Ni, and Wang 2024a) method as baseline, which directly involves a single LLM to generate a response for the given commit (i.e., Single). This approach tests the LLM's ability for SV assessment. For the LLMs utilized in EvalSVA, we have selected ChatGPT (ChatGPT 2022) and GPT-4 (OpenAI 2023), given their robust capabilities in handling code-related tasks."}, {"title": "Evaluation Metrics", "content": "In this paper, we employ the evaluation framework delineated by the CVSS v3.1 for SV assessment results derived from various methods. Specifically, we compute the Accuracy (i.e., Acc), which quantifies the ratio of accurately classified instances to the total number of instances, and calculate the F1 score (i.e., F1) to evaluate issues of class imbalance situation."}, {"title": "EvalSVA Results", "content": "As illustrated in Table 3, these LLM-based approach tasks are to achieve consistency with the SV assessment results of human experts in the CVSS v3.1 framework. Our findings reveal that: (1) SV assessment is an arduous task for a single agent. Existing single-based LLMs perform poorly across all metrics SV assessment with commit input, with average performances as low as 48.50% and 34.73% on the accuracy and F1 metrics, respectively. This underscores the complexity and difficulty of SV assessment for the single LLM. (2) Superior performance of EvalSVA. EvalSVA significantly enhances the performance of the SV assessment process, achieving higher alignment with human preference compared to single-agent-based methods. Specifically, the multi-agent-based method improves the F1 by 53.71% for ChatGPT and 32.88% for GPT-4. This demonstrates EvalSVA's advanced ability to evaluate the different aspects of SV assessment. (3) GPT-4 can aid human experts in SV assessment. The ChatGPT method shows more substantial improvements in"}, {"title": "Communication Strategy", "content": "To answer Q2, we propose four different communication strategies termed as preceding one expert, previous communication, simultaneous assessment, and summarizer assessment for the SV assessment task. We experiment with these strategies in Python and the detailed results are described in Table 4. The remaining experiment results of Java and C++ are presented in Appendix. Our observations indicate that (1) Employing either communication strategy proves advantageous for SV assessment. Integrating a multi-agent strategy with ChatGPT results in an improvement of 8.83% and 8.07% in accuracy and F1 score, demonstrating the effectiveness of the communication strategy methodology, respectively. (2) The efficacy of distinct communication strategies should be tailored to the tasks. Communication strategies exhibit varying performance depending on the task configuration, which can be attributed to the inherent nature of these tasks. For instance, the evaluation of attack complexity and user interaction typically falls under binary classification, whereas the impact aspect (including confidentiality, integrity, and availability) requires multi-classification. This underscores the necessity of adopting task-specific communication strategies in the development of SV assessment methods. (3) The superior performance of preceding one expert strategy for most metrics. Preceding one expert strategy demonstrates superior performance in four tasks, yielding significant F1"}, {"title": "Expert Numbers and Communication Rounds", "content": "To answer Q3, we conduct the experiment to study the influence of different expert numbers and communication rounds for assessing vulnerability.\nExpert Numbers. The number of experts should be selected as medium (2-3). As illustrated in Figure 3 (a)-(b), the correlation between the number of experts and performance demonstrates a pattern of initial improvement followed by a subsequent decrease, with the optimal performance occurring when the number of experts is 2-3. This suggests that diverse expert roles enhance the model's comprehension of SV assessments, aligning with findings reported by (Du et al. 2023; Chan et al. 2023). Furthermore, it indicates that an excessive number of experts involved in the decision-making process may misguide the LLM-based method decisions, potentially due to the extended context length.\nCommunication Rounds. Multiple rounds of communication are required to facilitate the model's understanding of vulnerability due to its lack of domain-specific knowledge. However, communication across numerous rounds does not necessarily even result in a decline. This could be attributed to the fact that excessively long contexts are detrimental to the model's ability to effectively process the task of SV assessment. It is noteworthy that different tasks may necessitate varying numbers of communication rounds. For instance, the PR exhibits optimal performance after three rounds, while AV reaches peak performance in the first rounds. These findings underscore the need for a more sophisticated appreciation of the balance between the number of communication rounds and the specific task to optimize performance."}, {"title": "Discussion", "content": "Case Study\nFigure 4 is a vulnerability example from CVE-2023-46502 (Detail 2024b), which uses the EvalSVA to evaluate the Attack Complexity. The vulnerability arises from the improper configuration of DocumentBuilderFactory (shaded in brown in Figure 4), which allows XML external entity attacks (i.e., CWE-611 (CWE 2024a)). We observe that there initially exists a discrepancy in opinions between the different agents during the first round of responses. Then, a consensus is reached in the subsequent round. This case mirrors real-world situations where multiple experts assess a single vulnerability. Specifically, EvalSVA demonstrates several human-like decision-making processes observed in the industry. (1) Opinions diversity: Initially, Expert 1 and Expert 2 present differing judgments when assessing the same vulnerability commit. This diversity broadens the perspective and encompasses a more comprehensive range of considerations in SV assessment. (2) Revision: Upon considering the viewpoints of other experts, Expert 1 learns from different aspects and revises its previously erroneous judgment. This indicates that EvalSVA, when informed by the perspectives of multiple experts, possesses the capability to revise. (3) Interpretability: Each expert provides explanations for their assessments. This practice aligns with industry standards set by FIRST (fir 2024), which mandates that CVSS must adhere to documented guidelines and include both the scoring vector and a detailed rationale, enabling others to understand the derivation of the scores. Previous methods (Le et al. 2021; Li et al. 2023b) often provided scores without the explanations needed for comprehensive SV assessment. (4) Evolutionary adaptation: EvalSVA can be adapted to different versions of SV assessment systems based on the prompts. Unlike prior works, EvalSVA swiftly integrates current version-specific domain knowledge to conduct SV assessments without training, demonstrating its agility and relevance in evolving systems."}, {"title": "Limitation", "content": "Transferability on other types of SV assessment. In this paper, we only focus on SV assessment with commit input and CVSS v3.1 standard, excluding SV and bug report-based methods. However, EvalSVA can be generalized to other types of SV assessment and other expert-based SV assessment standards. In the future, we intend to explore the efficacy of EvalSVA regarding the upgrade of the assessment system.\nConstraints of domain knowledge in prompts. For the context limited of LLMs, EvalSVA only contains the prompt-based domain knowledge and chat history to facilitate the SV assessment. In the future, we will explore more expert-based examples as prompts for the LLM-based SV assessment."}, {"title": "Related Work", "content": "Public security databases, such as the NVD (NIST 2024), and expert-based scoring systems, such as the CVSS (CVS 2023) have been pivotal in furnishing detailed datasets for SV. In recent years, the CVSS framework has witnessed significant enhancements (Feutrill et al. 2018), evolving from v2 (CVS 2024d) to v3.0 (CVS 2024b), and subsequently to v3.1 (CVS 2024c). Specifically, the existing methods can be broadly divided into two aspects: SV report-based and commit-based methods. The majority of existing methods for automated SV assessment depend on SV reports (i.e., SV reported-based methods) (Han et al. 2017; Lamkanfi et al. 2010; Le, Sabir, and Babar 2019; Spanos and Angelis 2018) from the NVD. These methods typically focus on predicting either a single metric (Fu et al. 2024; Kudjo et al. 2019; Wang et al. 2019) or a set of metrics associated with the CVSS (Le and Babar 2022; Yamamoto, Miyamoto, and Nakayama 2015; Ognawala et al. 2018). For instance, Han et al. (Han et al. 2017) introduced a Convolutional Neural Network-based method to automate and predict the overall severity rating by analyzing SV descriptions. However, these user-submitted SV reports often exhibit significant delays (Thung et al. 2012; Sawadogo et al. 2021; Bosu and Carver 2012; Thongtanunam et al. 2015), potentially exceeding 1000 days. To expedite SV assessment and reduce the extensive labor required by human experts for evaluations, In addition, the recent research also explored the potential of commit-based methods (Le et al. 2021; Zhou et al. 2022; Li et al. 2023b; Yin, Ni, and Wang 2024b). This type of method involves utilizing commit changes to assess all aspects of SVs. For instance, Le et al. (Le et al. 2021) introduced DeepCVA, a model that applies multi-task learning to perform commit-based SV assessment. Li et al. (Li et al. 2023b) proposed a neural framework dedicated to SV detection and assessment simultaneously.\nConsidering that SV assessment systems are subject to continuous evolution or require customization tailored to specific application contexts, it is imperative for a practical framework to demonstrate effectiveness in limited labels. In this paper, we propose a multi-agent framework for SV assessment to tackle the challenge of lacking data labels under the new standards. Furthermore, we construct a new dataset that includes CVSS v3.1 assessment metrics across multi-lingual programming languages."}, {"title": "Conclusion", "content": "In this paper, we propose the first multi-agent-based framework EvalSVA to simulate vulnerability assessment strategies in real-world scenarios. Furthermore, we construct the first multi-lingual SV assessment dataset based on the new standard of CVSS, comprising 699, 888, and 1,310 vulnerability-related commits in C++, Python, and Java, respectively, which can serve as a foundation dataset for future research. We emphasize the necessity of developing multi-agent evaluators for SV assessment due to the continuous evolution of CVSS. Our experimental results confirm the effectiveness of EvalSVA, especially in scenarios with limited labeled data. We also find that EvalSVA offers a human-like process, providing both rationale and responses for SV assessment. This"}]}