{"title": "Fast and Modular Autonomy Software for Autonomous Racing Vehicles", "authors": ["Andrew Saba", "Aderotimi Adetunji", "Adam Johnson", "Aadi Kothari", "Matthew Sivaprakasam", "Joshua Spisak", "Prem Bharatia", "Arjun Chauhan", "Brendan Duff Jr.", "Noah Gasparro", "Charles King", "Ryan Larkin", "Brian Mao", "Micah Nye", "Anjali Parashar", "Joseph Attias", "Aurimas Balciunas", "Austin Brown", "Chris Chang", "Ming Gao", "Cindy Heredia", "Andrew Keats", "Jose Lavariega", "William Muckelroy III", "Andre Slavescu", "Nickolas Stathas", "Nayana Suvarna", "Chuan Tian Zhang", "Sebastian Scherer", "Deva Ramanan"], "abstract": "Autonomous motorsports aim to replicate the human racecar driver with software and sensors. As in traditional motorsports, Autonomous Racing Vehicles (ARVs) are pushed to their handling limits in multi-agent scenarios at extremely high (\u2265 150mph) speeds. This Operational Design Domain (ODD) presents unique challenges across the autonomy stack. The Indy Autonomous Challenge (IAC) is an international competition aiming to advance autonomous vehicle development through ARV competitions. While far from challenging what a human racecar driver can do, the IAC is pushing the state of the art by facilitating full-sized ARV competitions. This paper details the MIT-Pitt-RW Team's approach to autonomous racing in the IAC. In this work, we present our modular and fast approach to agent detection, motion planning and controls to create an autonomy stack. We also provide analysis of the performance of the software stack in single and multi-agent scenarios for rapid deployment in a fast-paced competition environment. We also cover what did and did not work when deployed on a physical system (the Dallara AV-21 platform) and potential improvements to address these shortcomings. Finally, we convey lessons learned and discuss limitations and future directions for improvement.", "sections": [{"title": "1 Introduction", "content": "Historically, motorsports have been a venue for advancing automotive technology in the name of competition and brand recognition. Teams develop increasingly sophisticated technologies to shave off seconds from lap times. Over time, technology and lessons learned from car racing have been commercialized and adopted in standard passenger vehicles. With the advent of Autonomous Vehicle (AV) technology, motorsports are poised to play a similar role in its development. Autonomous Racing Vehicle (ARV) leagues, such as the Indy Autonomous Challenge (IAC) and Roborace, are challenging software, not drivers, to operate a vehicle at the performance limit.\nApart from motorsports, AVs are starting to be adopted for public use [1]. These conventional AVs are either specially retrofitted passenger vehicles, typically deployed in urban and suburban environments, or tractor-trailers, customized for and deployed in long-haul, highway, and interstate environments. In all of these domains, safety under all conditions and circumstances is vital; however, no matter the size and scope of any test program, edge cases, by the very nature of their rarity and difficulty, will continue to challenge safety verification and necessitate further development.\nThe three broad tasks commonly associated with software for AVs and ARVs are Sense, Think, and Act.\nSense uses sensors to measure the state of the environment. Sensors in ARVs detect and track the opponent and measure the specific gravity and angular rate of the ego vehicle. In the case of a conventional AV, sensors detect and track pedestrians and other vehicles on the road. However, unlike an ARV, AVs typically localize themselves onto High Definition (HD) Maps, which act as a strong prior to understand the environment, rules, and semantics of the road. For an ARV, prior information about the track bounds, banking, and theoretical maximum dynamic limits can be pre-computed for locations along the track. ARV sensors must have fast processing to handle conditions akin to highways, such as unexpected events, previously unseen agents, or high speeds. While the environment of an ARV has fewer actors at any given time, those actors are capable of extremely high (over 20m/s\u00b2) accelerations and speeds. This demands low latency and long-range perception to allow other modules ample time to react. Lastly, high speeds and accelerations introduce noise and unique physical challenges to sensors that often require robust software solutions.\nThink processes sensor information into a prediction of how the environment will evolve, ultimately deciding the best course of action to take next. For example, in the case of an ARV, it may be whether or not an opponent is attempting an overtake and whether or not it should defend against it. However, for an AV, the question may be whether or not it is safe to take a left turn. In both scenarios, significant uncertainty exists in how the world will evolve into the future, complicating decision-making. However, in an ARV, an additional layer of uncertainty is considered because agents are capable of very high accelerations (over 20m/s\u00b2) and are operating in direct competition with one another. Higher speeds often mean that over a given distance traveled, there are fewer measurements of the other agent's motion, meaning that a prediction must be made in less time with less information. Additionally, predictions can become obsolete rapidly, as\nAct executes the actions that were decided upon. For both ARVs and AVs, this involves taking the sequence of actions decided upon (typically a target space-time trajectory) and determining the optimal set of control commands to follow it safely. A traditional AV must often navigate complicated and crowded environments, react quickly to commands, and handle changing road and environmental conditions, such as snow and ice. Safely navigating while slipping on snow and ice is still an open area of research, as it requires advanced vehicle dynamics modeling and control techniques [2]. While they are not driven in snow, racecars are pushed to their handling limits, to the point that simple kinematic and dynamic models begin to fall apart, much like in slippery conditions. The highly non-linear tire dynamics, aerodynamic interactions, effects of track temperature and surface, and more begin to break down the assumptions made in these models. Additionally, these effects are not constant, changing as the race progresses. For example, a worn-out tire on a racecar is like a typical passenger vehicle trying to drive through snow: both are prone to slipping at any moment. Any controller navigating an ARV at the physical limits of handling must understand and account for these dynamics to balance high performance and safety."}, {"title": "1.1 Related work", "content": "The call for advancing autonomous vehicle technology has been present since the early twenty-first century when the Defense Advanced Research Projects Agency (DARPA) launched the 2004 and 2005 DARPA Grand Challenges [3], [4]. These challenges demonstrated some of the capabilities of AVs. The teams in the Grand Challenge autonomously navigated across southern Nevada on a 132-mile course of rugged desert terrain. Succeeding the Grand Challenges was the 2007 DARPA Urban Challenge [5], which introduced a time-based competition focused on city driving. This competition maintained the competitive nature of completing a course, but focused on navigating an urban environment. Each team needed to stop at stop signs, yield for oncoming traffic, complete U-turns, and obey all other traffic laws. These challenges were the first full-scale autonomous racing competitions and laid the groundwork for future AV research and development.\nSince then, there have been several autonomous racing competitions, such as Formula Student [8], [9], Rob-"}, {"title": "1.2 Overview & Key Takeaways", "content": "Our approach follows two main themes: modularity and speed. We have developed every portion of our software stack (hereinafter referred to as \"the stack\") to be stand-alone, allowing for replacing modules as requirements change. Since very little prior work existed with racing at the speeds the competition demands, it is challenging to develop a one-size-fits-all approach. There is a high level of uncertainty because of the many unknowns regarding how sensors or the vehicle will behave at higher speeds. Additionally, with a fast-paced competition and prototype hardware, requirements change day-to-day, necessitating frequent modifications to core functionality.\nSecondly, we define our approach by its speed. When racing at high speeds, algorithms must finish execution quickly and deterministically, i.e., sudden high execution times can be disastrous if they lead to instability. For example, our motion controller uses a dynamics motion model to generate an optimal feedback policy that is cheap and fast to compute, allowing for a high rate of execution with little deviation, which is vital for navigating at very high speeds and accelerations. However, our approach does not sacrifice quality to achieve its speed and efficiency; instead, the key challenge has been to choose algorithms intelligently and design efficient architectures around them.\nIn this work, we present a detailed description of our approach and system design for a full ARV software stack for the Indy Autonomous Challenge (IAC). We will also elaborate on successes, failures, and lessons learned during extensive field testing on oval race tracks over two competition seasons. Finally, we will provide insights in our design process across the whole stack and results from this approach. Overall, our stack demonstrates the following capabilities:\n\u2022 Stable trajectory tracking at speeds over 150mph while maintaining reasonable lateral deviations from the desired trajectory\n\u2022 Reliably detecting and tracking an opponent ARV at over 100m away, even at high speeds (i.e.\n> 125mph)\n\u2022 Safely passing and trailing an opponent ARV vehicle at high speeds (i.e. \u2265 125mph)"}, {"title": "2 The Competition", "content": "The MIT-Pitt-RW autonomous racing team is a team of students forming one of the nine teams that have successfully qualified to participate in the Indy Autonomous Challenge (IAC). The IAC is a global competition in which university teams compete to develop software for a standardized ARV platform named the Dallara AV-21. To date, there have been two seasons with two physical installments each. The first installment was a single-agent, fastest-lap competition, and the following installments have been two-agent passing competitions. These installments have all been on oval super-speedways which has dictated the strategies for developing the stack. Before the in-person installments, there were multiple simulation practice events and a simulation competition, where the teams verified their software in single\nand multi-agent scenarios. All instalments of the competition have been supervised and executed by \"race control\", managed by the IAC. During competition, the race flags and team roles are remotely controlled by race control, allowing for minimal human intervention during the race.\nThe multi-agent passing competition [32] assigns one competitor the \"attacker\" role and the other the \"defender\" role. During each lap, the defender is remotely assigned a speed at which the attacker must pass the defender within two laps. If the pass is successful, the roles are exchanged and the defender's speed is incrementally increased (125mph, 135mph, etc.). A pass is complete once the attacker gains its position in front of the defender with a longitudinal gap of at least 30m. If an attacker fails to pass at a certain speed, the roles are exchanged. The winner of the round is determined once one of the attackers cannot complete a pass. If both teams cannot complete the pass, the round ends in a draw. \nThere are two factors the attacker must consider while deciding to make a pass: safety and dynamic limitations. The attacker must maintain safe lateral and longitudinal separation from the defender at all times. Additionally, when considering a pass, the attacker must ensure its trajectory will keep the car within the\ndynamic limitations of the vehicle and not result in loss of control. Combining the two, the attacker must ensure that the accelerations and decelerations are timed appropriately to stay within the dynamic limits of the vehicle. As the passing competition progresses, it becomes more difficult for the attacker to exceed the defender's speed, particularly in corners of the track.\nIn addition to these considerations for the attacker, the defender can make passing more difficult for the attacker by adjusting their position within their lane, as long as they maintain rule compliance. For example, if the defender moves outwards, the attacker has to travel more distance to complete the pass. A winning strategy for an attacker is to maintain the minimum allowed distance to the defender and to initiate the pass whenever the attacker can maneuver it safely. Completing a pass is also further complicated if the attacker starts the pass too late; they may get trapped too far out on the outer lane into the corners, thereby increasing the distance they need to cover. Prediction and motion forecasting of the opponent agent is imperative to make intelligent strategic decisions.\nIn this work, we present our approach to the Indy Autonomous Challenge (IAC) for both the 2021-22 and 2022-23 Seasons (\"Season One\" and \"Season Two\u201d, respectively). The results shown here are from the the Las Vegas event in Season One and the Texas and Las Vegas events in Season Two. The overall approach was the same for all four events, but more mature and better tested by the Season Two, evidenced by the more than doubling of our highest achieved speed from 69mph to over 150mph. This multi-season evaluation provides a unique perspective into a continual and evolving engineering and testing process, with numerous lessons learned along the way."}, {"title": "2.1 AV-21 platform", "content": "The Dallara AV-21 is the official vehicle of the Indy Autonomous Challenge (IAC). Every competitor must use the same hardware, including vehicle setup, autonomy sensors, and compute. The vehicle is a modified version of the Indy Lights IL-15 chassis, retrofitted with a package of automated vehicle sensors, drive by wire, and compute. The engine is a 4 Piston Racing-built Honda K20C. Sensors onboard the AV-21 include 3 Luminar Hydra LiDARs, 3 Aptiv Medium Range Radars, 2 NovAtel PwrPak7D-E1 GNSS, and 6 Mako G-319 Cameras. In total, these sensors provide redundant 360\u00b0 coverage and over 200m of sensing range.\nBetween Seasons One and Two of the IAC, the AV-21 underwent a hardware refresh that included the ad-dition of a VN-310 Vectornav GNSS system and an update to the main compute platform. In Season One, the main compute was an ADLINK AVA-3501 with an 8 core, 16 thread Intel Xeon CPU and an NVIDIA Quadro RTX 8000 GPU. In Season Two, a dSPACE AUTERA AutoBox with a 12 core, 24 thread Intel Xeon CPU and an RTX A5000 NVIDIA GPU served as the main compute platform. The AutoBox provided many\nadvantages over the ADLINK, including automotive-grade ruggedness, higher available networking band-width, and CAN channels built into the computer. However, these automotive features came at the cost of slower single threaded performance, which necessitated critical engineering design decisions to accommodate all critical software pieces onto a single computer."}, {"title": "3 Approach", "content": "Our software architecture follows a typical, standard autonomy software design, with localization, perception, tracking, prediction and motion planning, and controls. The Robot Operating System (ROS), specifically ROS 2 Galactic, is used for communication between each process, or node. Various libraries and frameworks are utilized from ROS for visualization, math utilities, communication, and more. All modules run asynchronously with one another, usually on a preset frequency, except for perception and portions of localization, which are driven by sensor data arrival."}, {"title": "3.2 Perception", "content": ""}, {"title": "3.2.1 Challenges and Requirements", "content": "The AV-21 is capable of very high accelerations (greater than 20m/s\u00b2) and speeds (greater than 180mph), meaning LiDAR or camera frame-to-frame movement can be significant. Additionally, there are no existing data sets for detecting AV-21s and little data showing the performance of sensors, such as LiDARS and cameras, at our target speeds. As a result, initially, data-driven approaches were not feasible, and the performance at higher speeds could not be immediately evaluated."}, {"title": "3.2.2 Overview of Approach", "content": "Final perception stack: shows our final perception stack's decoupled and multi-modal approach to accurately detecting and localizing all other agents on the track, which notably, no longer makes use of clustering. On the AV-21 Platform , there is an assortment of LiDARs, cameras, and radars, each with advantages and disadvantages. For example, cameras alone do not give an accurate depth estimation but can operate at a much higher frame rate (up to 75Hz) and resolution (2064 \u00d7 960) than LiDARs. A camera-based detection pipeline can provide a higher frequency update on our belief of the world and has the potential to see other agents from further away. To best exploit the sensors' strengths and for robustness and redundancy, our perception stack uses each sensor independently and in parallel and feeds all localized detections to our tracking stack."}, {"title": "3.2.3 Camera", "content": "For full coverage and maximum range, the two front-facing cameras utilize a narrow lens to improve far-field resolution. The four remaining cameras use a wider field of view (FOV) to provide 360\u00b0 coverage around the vehicle. Due to the lower effort required to label 2D bounding boxes, YOLO v5 [37] was chosen for our initial approach. The model was trained on a custom, hand-labeled data set of other AV-21 vehicles, with images taken from onboard our vehicle. Because the model outputs 2D bounding boxes, other assumptions and processing is required to provide a 3D pose of the other agents. By exploiting the fact that the size and shape of the vehicles are known, we can estimate a depth from the 2D bounding boxes from the model by using a standard pinhole optics model [38]:\nDepth = \\frac{(Height_{known} * f)}{Height_{pixels}} \taga (1)\nwhere $f$ is the calibrated focal length of the camera, $Height_{known}$ is the known height of the vehicle in meters, and $Height_{pixels}$ is the detected height of the detected vehicle in pixels. This monocular algorithm yields accurate results for mid/far-field detections; however, the error increases proportionally with the real-world distance between the camera and the other agent. While far-field detections (> 100m) tend to be less accurate, the additional sensor modalities, including LiDAR, cannot see as far as the camera with nearly the exact resolution and fidelity, so some measurement is better than none. As the other agent gets into the LiDAR operating range, we refine the estimates using these detections, and our confidence in the agent's position increases. The unique long-range capability of the camera perception pipeline can provide motion planning more time to respond to agents in our path."}, {"title": "3.2.4 Point Pillars", "content": "The AV-21 platform has three Luminar Hydra LiDARs[39] positioned in a triangular fashion. Each LiDAR has a field of view (FOV) of 120\u00b0, together allowing for 360\u00b0 coverage around the vehicle. Each LiDAR is capable of excellent coverage at over 100m, thereby providing an over 200m radius circle of coverage around the track. Since the track is only so wide, this cloud is cropped further to being 200m \u00d7 40m.\nNumerous Deep Learning methods of object detection using LiDARs have shown promising results, such as VoxelNet [40], PointRCNN [41], SECOND [42], and others. Low-latency inference and accurate detections are of the utmost importance for our use case of high-speed autonomous racing. For this reason, PointPillars\n[43] serves as our primary detection method, capable of reliably detecting vehicles at ranges up to 100m away. The birds-eye-view projection and 2D convolutions used within PointPillars allow for the removal of computationally expensive and time-consuming sparse 3D convolutions performed by other LiDAR networks.\nFigure 13 shows the outcomes of PointPillars. To reduce processing time, our PointPillars implementation is single-sweep, meaning we do not accumulate scans over time before running inference. Additionally, to further simplify the pipeline, inference is done directly on the raw scans, after down-sampling and applying a crop. We explicitly chose to not compensate for distortion caused by the ego vehicle's motion. Based on the data observed and practical considerations within the larger stack, motion compensation was deemed not worth the additionally complexity and processing time required. The scanning rate (~ 50ms from top to bottom) is faster than other LiDARs, which results in less distortion. Additionally, the LiDAR data is"}, {"title": "3.2.5 Data Collection, Labeling, & Training", "content": "No dataset exists that contains AV-21s racing head-to-head. Adequately training PointPillars required developing a large and robust dataset. Initially, data was collected in simulation, which helped developed an initial model. The simulation environment did not match the vehicle setup perfectly. In particular, while the range and coverage were similar, the point cloud was less dense than in real life. Interestingly, we found that the initial model trained off of this data transferred to detecting AV-21s on real data, especially at longer ranges, where the cloud is less dense.\nWith an initial model, it was now possible to do \"auto-labeling\", where the model is used to generate new labels that are then hand-verified by a human annotator. Because the model often provides a detection that is close to ground truth, the workload on the human annotator is reduced. Additionally, by using the existing model to label more data, labels can be focused on the areas where the model performed most poorly."}, {"title": "3.2.6 Discussion: Strengths, Limitations, and Future Work", "content": "Given the prototypical nature of the AV-21 platform, the sensor plate must be disassembled every time the autonomy components need servicing. As a result, the extrinsic calibration between the cameras and the LiDARs changes frequently. This is less of an issue with the LiDARs, as they are all firmly fastened to the same aluminum plate. Additionally, the extreme operating conditions of the AV-21 platform (i.e. high speeds and accelerations) also necessitate re-calibrating the sensors regularly, even if the sensor plate has not been removed. Small extrinsic calibration errors can lead to very large projection errors, especially for distant objects. This problem is not exclusive to ARVs and is an active area of research [45][46]. Future work will center on streamlining the calibration process and developing systems that are less brittle to small errors.\nFinally, due to a severe crash less than 72 hours before the competition in Season Two at Las Vegas, the camera detection pipeline was disabled for the competition events. With the focus being repairing the AV-21 vehicle, no time was available to properly calibrate the sensors and the potential for projection errors outweighed the benefits. Because of the modular design of the perception stack, it was trivial to make such a drastic change. In a fast-paced competition environment, this modularity and flexibility proved paramount in allowing the vehicle to operate during the competition. While the redundancy and peak performance of"}, {"title": "3.3 Tracking", "content": ""}, {"title": "3.3.1 Challenges and Requirements", "content": "Tracking within an ARV software stack serves to provide downstream tasks with a single belief of the states of other agents within the world. Different perception modalities capture different portions of a given agent's state space. For example, the monocular camera perception provides a noisy estimate of an agent's position, but cannot accurately predict its orientation. Our LiDAR perception produces full pose estimates of other agents, but currently does not infer the agent's velocity. While using only one of these detection methods will yield a belief that is severely limited by the outlined weaknesses, the effective fusion of both can result in each modality compensating for the drawbacks of the other.\nOur implementation allows for the fusion of multiple sensing modalities in a straightforward manner, and serves to provide downstream planning tasks with the state of all perceived agents. Our decoupled approach to perception requires our tracking stack to meet the following requirements:\n1.  Incorporate all modalities from perception, including LiDAR and monocular camera detections\n2.  Estimate positions, velocities, and orientations in the world of all tracked agents\n3.  Provide a precise and accurate state estimate of the opponent agents\n4.  Provide a consistent measure of the uncertainty of the agents' state estimates\n5.  Be robust to false positives, missed detections, and drop-outs from one or more sensor modalities\nFinally, tracking must perform all of the above while ensuring as little additional latency as possible, handling measurements from perception asynchronously and out of order, and compensating for any delay between sensor measurement and tracking."}, {"title": "3.3.2 Overview of Approach", "content": "Our approach consists of three main components: Filtering, Association, and Fusion. Filtering removes outliers. Association determines whether or not a detection is of a previously seen agent. Fusion is incor-porating new measurements of agents' states. In order to minimize processing latency within the tracking stack, well-researched, efficient algorithms are leveraged for each module. \nFiltering: Detections filtered by a confidence threshold, a hyper-parameter within our tracking stack, tuned empirically by analyzing the false positives and associated confidence produced by perception. Additionally, any detection that falls outside of the track bounds is ignored. The combination of these two filtering steps helps to ensure that only valid detections are processed and used to generate tracked agents.\nAssociation: AB3DMOT [47] provides the data association module utilized by tracking stack. By employing two computationally efficient algorithms, the Hungarian algorithm for data matching [48] and the Kalman filter [49] for fusion and prediction, the authors demonstrate strong results on multiple open-source data sets while also providing high-frequency predictions. In practice, we observed that the Hungarian algorithm with Euclidean distance often resulted in poor data association, especially during temporary sensor drop-out. Therefore, our implementation uses simple greedy matching with the Mahalanobis distance [50], which performed better in testing.\nTrack births and deaths: To reduce the probability of false positives becoming valid tracks, a new potential track is instantiated (born) only after two detections (from successive sweeps) are associated with it. This hyper-parameter provides a means to balance between the quality and confidence of tracks and end to end latency in reacting to other agents. Finally, any tracks that have not had a detection associated with it within the last five seconds are also removed (killed) to prevent stale tracks from influencing future associations.\nFusion: Once detections have been associated with an existing tracked agent, or have been repeatedly observed and classified as a new agent, we begin tracking the agent using fused multi-modal perception outputs. Again, we utilize a modified version of [47] as the Kalman filter for performing sensor fusion. Since incoming detections from the camera perception pipeline have already been projected into a 3-dimensional position and transformed into a common frame, both LiDAR and camera measurements can be used to update the internal Kalman filter for a given tracked agent. In this way, sensor fusion becomes a simple task that can be asynchronous across the two modalities, and the states of tracked agents can be published at the receipt of each incoming detection."}, {"title": "3.3.3 Discussion, Limitations, and Future Work", "content": "The tracking pipeline meets all requirements and is sufficiently accurately and performant to handle the IAC Passing Competition. Our modular design was especially important when the camera perception was disabled on race day. Given these successes, however, our system has not been robustly tested against multiple agents, specifically agents that are close together (i.e. \u2264 5m). In traditional motorsports, humans drive aggressively in close proximity to one another. While the competition format is far from this style of racing, future works will need to handle such operating domains in order to challenge professional drivers. Multiple agents in close proximity are more difficult to track, due to higher association ambiguity and occlusions.\nFinally, a Kalman filter will be replaced by an Extended Kalman filter (EKF) to enable a non-linear motion model. With an EKF and a better motion model (i.e. constant curvature), predictions of agent tracks will be more accurate, which is especially important during periods of infrequent detections (i.e. the other agent is in the blind-spot produced by the rear wing of the vehicle)."}, {"title": "3.4 Planning", "content": ""}, {"title": "3.4.1 Overview", "content": "We developed a fast, modular motion planning stack capable of predicting agent behavior and safely trailing and passing other agents. The path planner identifies the agent's position at points in the track where it is most optimal to pass and extrapolates its position in the following lap. We have based our path planning approach on a set of primitive behaviors from which higher-level strategies can select. These primitive behaviors include opponent trailing, raceline following, and lane switching. This modularity allows for a clean separation between high-level decision-making and trajectory selection and generation.\nWe also take advantage of knowing the track geometry and develop a set of strong priors in the form of a trajectory bank. Offline, trajectories are generated for various lanes along the track, providing different levels of clearance from the inner and outer track boundaries. Online, the planner selects the best trajectory from the bank based on the selected action primitive."}, {"title": "3.4.2 Offline Trajectory Generation", "content": "The raceline generation is a two-step process that begins with defining a set of waypoints on the track. Afterward, splines are interpolated on these waypoints to ensure continuity. This process relies heavily on the manual selection of waypoints at apexes to properly leverage spline properties.\nFor the overtaking competition, waypoints were selected manually from two lanes. We consider a lane to be defined as a path with both an inner and outer boundary and derive a center-line equidistant from the two boundaries. We obtain these two lanes by dividing the width of the track in half along the full length. Within these lanes, the process of manual sampling begins. A series of N waypoints, (q1,..., qv), are used to interpolate closed Spiro splines [51]."}, {"title": "3.4.3 Online Action Selection", "content": "The multi-agent passing competition sets two roles for the competitors: \"defender\" and \"attacker\". As defined in Section 2, the attacker passes the defender that is maintaining a raceline within the inside lane of the track. The attacker is also responsible for maintaining a safe distance from the defender. We define a set of Action Primitives to encode these maneuvers:\n\u2022 Maintain:\nMaintain the current raceline at a given speed.\n\u2022 Trail:\nMaintain a fixed distance behind the opponent vehicle on the current raceline.\n\u2022 Safe Merge:\nMerge between arbitrary lanes safely (i.e., avoid collisions with other agents).\nSelection of the primitives is dependent on the current role (defender or attacker) and the current Track Condition (i.e., Green, Yellow, Red, Waving Green), which are both defined by flags sent to the vehicle from Race Control.\nAttacker:\n\u2022 Under a Green Flag, the Attacker must close the gap with the defender (Trail)\n\u2022 Under a Waving Green Flag, the Attacker may initiate the pass (Safe Merge)\n\u2022 Under a Waving Green Flag and after the Attacker has passed the opponent by at least 30m, the Attacker must \"Close the Door\" by merging back to the inside lane (Safe Merge)\n\u2022 Under a Green Flag, the Attacker has the freedom to take any lane, but may not begin passing until explicitly allowed (Trail or Safe Merge)\nDefender:\n\u2022 Under a Green Flag, the Defender must move to the inside lane and maintain the speed set for that round (Maintain)\nWhile this logic is simple for the passing competition, the use of these primitives can scale to more complex logic. For example, rewards and costs could be assigned to each of these primitives which are then utilized by a search-based planner or a reinforcement-learning algorithm to estimate expected rewards and costs by taking a set of actions and to determine the best solution given the scenario. By structuring the planner around a modular set of action primitives, we can explore multiple solutions to the behavioral decision-making problem very easily. Additionally, it is easy to add more primitives in the future when necessary."}, {"title": "3.4.4 Online Raceline Merging", "content": "Merging between inner and outer racelines (i.e. \u201cSafe merging\") is accomplished by first calculating the closest pose on the inner raceline $q^{in}_{k}$ corresponding to every pose on the outer raceline $q^{out}_{k}$. Next, given a start and a final interpolation pose, $q^{in}$ and $q^{out}$, as well as the corresponding twist, the spline optimization formulation in Section 3.2 of [54] is used to generate a minimum jerk merging trajectory. Time intervals $h_{k}$ between consecutive time stamps can be decreased as desired to generate a sufficiently smooth raceline between $q^{in}$ and $q^{out}$."}, {"title": "3.4.5 Results and Discussion", "content": "displays a comprehensive flowchart of the mentioned approach and highlights the offline and online computations. Tasks performed by individual components of the planner are agnostic to the actions performed by other components. This includes adding or removing additional raceline primitives as well as action primitives, making this architecture highly modular. Future work on the planner includes automating the process of waypoint selection based on given inside and outside lines, which would consider varying track curvature. Finally, this approach is very fast, efficient, and able to easily execute at 20Hz (and capable of a much higher execution rate). Action selection, safe-merge trajectory interpolation, and velocity profiling are all very inexpensive to compute online."}, {"title": "3.5 Controls", "content": ""}, {"title": "3.5.1 Overview of Approach", "content": "Our controls architecture was designed for speed and modularity. To achieve this, we decompose our control task of path tracking into three tasks lateral control, longitudinal control, and gear selection. Lateral tracking generates a steering angle such that the vehicle converges to the path. The longitudinal controller maintains a particular longitudinal velocity. The gear-shifting controller is responsible for selecting the optimal gear for maintaining the current vehicle speed. The majority of the content in this section will focus on the lateral tracking element of the vehicle's controller. Finally, the work presented here is a continuation of our previous work in [55]."}, {"title": "Longitudinal Control and Gear Shifting", "content": "The throttle and brake control utilizes a simple P control scheme paired with a feed-forward term to account for drag. This is expressed within Algorithm 1 line 6, which returns a command value. This command is then interpreted as either a throttle or brake command depending on whether the command is positive or negative, using a scaling factor on the brake signal to account for differences in magnitude between the throttle and brake. Smoothing on the throttle and brake signals prevents instantaneous acceleration or deceleration to help prevent vehicle instability. Algorithm 1 summarizes this algorithm.\nThe gear-shifting strategy is based on a simple lookup table where the optimal gear is computed given the current velocity of the vehicle. This gearing table was generated based on a model of the engine and track parameters to maximize torque, the computation of which is outside the scope of this paper."}, {"title": "Lateral LQR Control", "content": "The lateral controller is built around a Linear-Quadratic Regulator (LQR) that generates an optimal feedback policy based on a nominal vehicle dynamics model. LQR was chosen because (1) it is inexpensive and fast to compute the desired controls online and (2) it is optimal given the vehicle dynamics model. However, one downside is that LQR does not reason about actuation constraints and only considers the error concerning the reference at the current time step. However, the vehicle primarily drives on an oval race track where only a few degrees of steering are required even for the most aggressive maneuvers. Hence, LQR can stably control the vehicle and meet the reference tracking requirements to safely avoid the other agent, track barriers, and maintain the desired trajectory.\nGiven a continuous-time linear system of the form in Equation 2 a quadratic cost function such as Equation 3 can be defined which is constrained by Equation 2. Equation 3 also has the constraints that $Q = Q^T > 0$ and $R = R^T > 0$.\nAn optimal feedback policy can be derived such that the cost function in Equation 3 is minimized over an"}, {"title": "Longitudinal Tracking"}]}