{"title": "Fast and Modular Autonomy Software for Autonomous Racing Vehicles", "authors": ["Andrew Saba", "Aderotimi Adetunji", "Adam Johnson", "Aadi Kothari", "Matthew Sivaprakasam", "Joshua Spisak", "Prem Bharatia", "Arjun Chauhan", "Brendan Duff Jr.", "Noah Gasparro", "Charles King", "Ryan Larkin", "Brian Mao", "Micah Nye", "Anjali Parashar", "Joseph Attias", "Aurimas Balciunas", "Austin Brown", "Chris Chang", "Ming Gao", "Cindy Heredia", "Andrew Keats", "Jose Lavariega", "William Muckelroy III", "Andre Slavescu", "Nickolas Stathas", "Nayana Suvarna", "Chuan Tian Zhang", "Sebastian Scherer", "Deva Ramanan"], "abstract": "Autonomous motorsports aim to replicate the human racecar driver with software and sensors. As in traditional motorsports, Autonomous Racing Vehicles (ARVs) are pushed to their handling limits in multi-agent scenarios at extremely high (\u2265 150mph) speeds. This Operational Design Domain (ODD) presents unique challenges across the autonomy stack. The Indy Autonomous Challenge (IAC) is an international competition aiming to advance autonomous vehicle development through ARV competitions. While far from challenging what a human racecar driver can do, the IAC is pushing the state of the art by facilitating full-sized ARV competitions. This paper details the MIT-Pitt-RW Team's approach to autonomous racing in the IAC. In this work, we present our modular and fast approach to agent detection, motion planning and controls to create an autonomy stack. We also provide analysis of the performance of the software stack in single and multi-agent scenarios for rapid deployment in a fast-paced competition environment. We also cover what did and did not work when deployed on a physical system (the Dallara AV-21 platform) and potential improvements to address these shortcomings. Finally, we convey lessons learned and discuss limitations and future directions for improvement.", "sections": [{"title": "Introduction", "content": "Historically, motorsports have been a venue for advancing automotive technology in the name of competition and brand recognition. Teams develop increasingly sophisticated technologies to shave off seconds from lap times. Over time, technology and lessons learned from car racing have been commercialized and adopted in standard passenger vehicles. With the advent of Autonomous Vehicle (AV) technology, motorsports are poised to play a similar role in its development. Autonomous Racing Vehicle (ARV) leagues, such as the Indy Autonomous Challenge (IAC) and Roborace, are challenging software, not drivers, to operate a vehicle at the performance limit."}, {"title": "1.1 Related work", "content": "The call for advancing autonomous vehicle technology has been present since the early twenty-first century when the Defense Advanced Research Projects Agency (DARPA) launched the 2004 and 2005 DARPA Grand Challenges [3], [4]. These challenges demonstrated some of the capabilities of AVs. The teams in the Grand Challenge autonomously navigated across southern Nevada on a 132-mile course of rugged desert terrain. Succeeding the Grand Challenges was the 2007 DARPA Urban Challenge [5], which introduced a time-based competition focused on city driving. This competition maintained the competitive nature of completing a course, but focused on navigating an urban environment. Each team needed to stop at stop signs, yield for oncoming traffic, complete U-turns, and obey all other traffic laws. These challenges were the first full-scale autonomous racing competitions and laid the groundwork for future AV research and development.\nSince then, there have been several autonomous racing competitions, such as Formula Student [8], [9], Rob-"}, {"title": "1.2 Overview & Key Takeaways", "content": "Our approach follows two main themes: modularity and speed. We have developed every portion of our software stack (hereinafter referred to as \"the stack\") to be stand-alone, allowing for replacing modules as requirements change. Since very little prior work existed with racing at the speeds the competition demands, it is challenging to develop a one-size-fits-all approach. There is a high level of uncertainty because of the many unknowns regarding how sensors or the vehicle will behave at higher speeds. Additionally, with a fast-paced competition and prototype hardware, requirements change day-to-day, necessitating frequent modifications to core functionality.\nSecondly, we define our approach by its speed. When racing at high speeds, algorithms must finish execution quickly and deterministically, i.e., sudden high execution times can be disastrous if they lead to instability. For example, our motion controller uses a dynamics motion model to generate an optimal feedback policy that is cheap and fast to compute, allowing for a high rate of execution with little deviation, which is vital for navigating at very high speeds and accelerations. However, our approach does not sacrifice quality to achieve its speed and efficiency; instead, the key challenge has been to choose algorithms intelligently and design efficient architectures around them.\nIn this work, we present a detailed description of our approach and system design for a full ARV software stack for the Indy Autonomous Challenge (IAC). We will also elaborate on successes, failures, and lessons learned during extensive field testing on oval race tracks over two competition seasons. Finally, we will provide insights in our design process across the whole stack and results from this approach. Overall, our stack demonstrates the following capabilities:\n\u2022 Stable trajectory tracking at speeds over 150mph while maintaining reasonable lateral deviations from the desired trajectory\n\u2022 Reliably detecting and tracking an opponent ARV at over 100m away, even at high speeds (i.e. > 125mph)\n\u2022 Safely passing and trailing an opponent ARV vehicle at high speeds (i.e. \u2265 125mph)"}, {"title": "2 The Competition", "content": "The MIT-Pitt-RW autonomous racing team is a team of students forming one of the nine teams that have successfully qualified to participate in the Indy Autonomous Challenge (IAC). The IAC is a global competition in which university teams compete to develop software for a standardized ARV platform named the Dallara AV-21. To date, there have been two seasons with two physical installments each, as seen in Table 1. The first installment was a single-agent, fastest-lap competition, and the following installments have been two-agent passing competitions. These installments have all been on oval super-speedways which has dictated the strategies for developing the stack. Before the in-person installments, there were multiple simulation practice events and a simulation competition, where the teams verified their software in single and multi-agent scenarios. All instalments of the competition have been supervised and executed by \"race control\", managed by the IAC. During competition, the race flags and team roles are remotely controlled by race control, allowing for minimal human intervention during the race.\nThe multi-agent passing competition [32] assigns one competitor the \"attacker\" role and the other the \"defender\" role. During each lap, the defender is remotely assigned a speed at which the attacker must pass the defender within two laps. If the pass is successful, the roles are exchanged and the defender's speed is incrementally increased (125mph, 135mph, etc.). A pass is complete once the attacker gains its position in front of the defender with a longitudinal gap of at least 30m. If an attacker fails to pass at a certain speed, the roles are exchanged. The winner of the round is determined once one of the attackers cannot complete a pass. If both teams cannot complete the pass, the round ends in a draw. Figure 4 shows a breakdown of the track and the possible paths to take into consideration.\nThere are two factors the attacker must consider while deciding to make a pass: safety and dynamic limitations. The attacker must maintain safe lateral and longitudinal separation from the defender at all times. Additionally, when considering a pass, the attacker must ensure its trajectory will keep the car within the"}, {"title": "2.1 AV-21 platform", "content": "The Dallara AV-21 is the official vehicle of the Indy Autonomous Challenge (IAC). Every competitor must use the same hardware, including vehicle setup, autonomy sensors, and compute. The vehicle is a modified version of the Indy Lights IL-15 chassis, retrofitted with a package of automated vehicle sensors, drive by wire, and compute. The engine is a 4 Piston Racing-built Honda K20C. Sensors onboard the AV-21 include 3 Luminar Hydra LiDARs, 3 Aptiv Medium Range Radars, 2 NovAtel PwrPak7D-E1 GNSS, and 6 Mako G-319 Cameras. In total, these sensors provide redundant 360\u00b0 coverage and over 200m of sensing range. Figure 5 shows the AV-21 platform and sensor locations.\nBetween Seasons One and Two of the IAC, the AV-21 underwent a hardware refresh that included the ad- dition of a VN-310 Vectornav GNSS system and an update to the main compute platform. In Season One, the main compute was an ADLINK AVA-3501 with an 8 core, 16 thread Intel Xeon CPU and an NVIDIA Quadro RTX 8000 GPU. In Season Two, a dSPACE AUTERA AutoBox with a 12 core, 24 thread Intel Xeon CPU and an RTX A5000 NVIDIA GPU served as the main compute platform. The AutoBox provided many"}, {"title": "3 Approach", "content": ""}, {"title": "3.1 Stack Overview", "content": "Our software architecture follows a typical, standard autonomy software design, with localization, perception, tracking, prediction and motion planning, and controls. The Robot Operating System (ROS), specifically ROS 2 Galactic, is used for communication between each process, or node. Various libraries and frameworks are utilized from ROS for visualization, math utilities, communication, and more. Figure 7 shows the data flow of the whole stack. All modules run asynchronously with one another, usually on a preset frequency, except for perception and portions of localization, which are driven by sensor data arrival."}, {"title": "3.2 Perception", "content": ""}, {"title": "3.2.1 Challenges and Requirements", "content": "The AV-21 is capable of very high accelerations (greater than 20m/s\u00b2) and speeds (greater than 180mph), meaning LiDAR or camera frame-to-frame movement can be significant. Additionally, there are no existing data sets for detecting AV-21s and little data showing the performance of sensors, such as LiDARS and cameras, at our target speeds. As a result, initially, data-driven approaches were not feasible, and the performance at higher speeds could not be immediately evaluated."}, {"title": "3.2.2 Overview of Approach", "content": "Final perception stack: Figure 9 shows our final perception stack's decoupled and multi-modal approach to accurately detecting and localizing all other agents on the track, which notably, no longer makes use of clustering. On the AV-21 Platform (Figure 5), there is an assortment of LiDARs, cameras, and radars, each with advantages and disadvantages. For example, cameras alone do not give an accurate depth estimation but can operate at a much higher frame rate (up to 75Hz) and resolution (2064 \u00d7 960) than LiDARs. A camera-based detection pipeline can provide a higher frequency update on our belief of the world and has the potential to see other agents from further away. Figure 10 shows some additional challenges faced with perception. To best exploit the sensors' strengths and for robustness and redundancy, our perception stack uses each sensor independently and in parallel and feeds all localized detections to our tracking stack."}, {"title": "3.2.3 Camera", "content": "For full coverage and maximum range, the two front-facing cameras utilize a narrow lens to improve far-field resolution. The four remaining cameras use a wider field of view (FOV) to provide 360\u00b0 coverage around the vehicle. Due to the lower effort required to label 2D bounding boxes, YOLO v5 [37] was chosen for our initial approach. The model was trained on a custom, hand-labeled data set of other AV-21 vehicles, with images taken from onboard our vehicle. Because the model outputs 2D bounding boxes, other assumptions and processing is required to provide a 3D pose of the other agents. By exploiting the fact that the size and shape of the vehicles are known, we can estimate a depth from the 2D bounding boxes from the model by using a standard pinhole optics model [38]:\n$Depth = \\frac{(Height_{known} * f)}{Height_{pixels}}$        (1)\nwhere $f$ is the calibrated focal length of the camera, $Height_{known}$ is the known height of the vehicle in meters, and $Height_{pixels}$ is the detected height of the detected vehicle in pixels. This monocular algorithm yields accurate results for mid/far-field detections; however, the error increases proportionally with the real- world distance between the camera and the other agent. While far-field detections (> 100m) tend to be less accurate, the additional sensor modalities, including LiDAR, cannot see as far as the camera with nearly the exact resolution and fidelity, so some measurement is better than none. As the other agent gets into the LiDAR operating range, we refine the estimates using these detections, and our confidence in the agent's position increases. The unique long-range capability of the camera perception pipeline can provide motion planning more time to respond to agents in our path. Figure 11 shows the result of the camera detection pipeline."}, {"title": "3.2.4 Point Pillars", "content": "The AV-21 platform has three Luminar Hydra LiDARs[39] positioned in a triangular fashion. Each LiDAR has a field of view (FOV) of 120\u00b0, together allowing for 360\u00b0 coverage around the vehicle. Each LiDAR is capable of excellent coverage at over 100m, thereby providing an over 200m radius circle of coverage around the track. Since the track is only so wide, this cloud is cropped further to being 200m \u00d7 40m.\nNumerous Deep Learning methods of object detection using LiDARs have shown promising results, such as VoxelNet [40], PointRCNN [41], SECOND [42], and others. Low-latency inference and accurate detections are of the utmost importance for our use case of high-speed autonomous racing. For this reason, PointPillars"}, {"title": "3.2.5 Data Collection, Labeling, & Training", "content": "No dataset exists that contains AV-21s racing head-to-head. Adequately training PointPillars required developing a large and robust dataset. Initially, data was collected in simulation, which helped developed an initial model. The first model dataset is broken down in Table 2. The simulation environment did not match the vehicle setup perfectly. In particular, while the range and coverage were similar, the point cloud was less dense than in real life. Interestingly, we found that the initial model trained off of this data transferred to detecting AV-21s on real data, especially at longer ranges, where the cloud is less dense. Figure 14 shows a comparison of PointPillars detections against the measured trajectory of an opponent ARV.\nWith an initial model, it was now possible to do \"auto-labeling\", where the model is used to generate new labels that are then hand-verified by a human annotator. Because the model often provides a detection that is close to ground truth, the workload on the human annotator is reduced. Additionally, by using the existing model to label more data, labels can be focused on the areas where the model performed most poorly."}, {"title": "3.2.6 Discussion: Strengths, Limitations, and Future Work", "content": "Given the prototypical nature of the AV-21 platform, the sensor plate must be disassembled every time the autonomy components need servicing. As a result, the extrinsic calibration between the cameras and the LiDARs changes frequently. This is less of an issue with the LiDARs, as they are all firmly fastened to the same aluminum plate. Additionally, the extreme operating conditions of the AV-21 platform (i.e. high speeds and accelerations) also necessitate re-calibrating the sensors regularly, even if the sensor plate has not been removed. Small extrinsic calibration errors can lead to very large projection errors, especially for distant objects. This problem is not exclusive to ARVs and is an active area of research [45][46]. Future work will center on streamlining the calibration process and developing systems that are less brittle to small errors.\nFinally, due to a severe crash less than 72 hours before the competition in Season Two at Las Vegas, the camera detection pipeline was disabled for the competition events. An image from the footage of the crash can be seen in Figure 15. With the focus being repairing the AV- 21 vehicle, no time was available to properly calibrate the sensors and the potential for projection errors outweighed the benefits. Because of the modular design of the perception stack, it was trivial to make such a drastic change. In a fast-paced competition environment, this modularity and flexibility proved paramount in allowing the vehicle to operate during the competition. While the redundancy and peak performance of"}, {"title": "3.3 Tracking", "content": ""}, {"title": "3.3.1 Challenges and Requirements", "content": "Tracking within an ARV software stack serves to provide downstream tasks with a single belief of the states of other agents within the world. Different perception modalities capture different portions of a given agent's state space. For example, the monocular camera perception provides a noisy estimate of an agent's position, but cannot accurately predict its orientation. Our LiDAR perception produces full pose estimates of other agents, but currently does not infer the agent's velocity. While using only one of these detection methods will yield a belief that is severely limited by the outlined weaknesses, the effective fusion of both can result in each modality compensating for the drawbacks of the other.\nOur implementation allows for the fusion of multiple sensing modalities in a straightforward manner, and serves to provide downstream planning tasks with the state of all perceived agents. Our decoupled approach to perception requires our tracking stack to meet the following requirements:\n1. Incorporate all modalities from perception, including LiDAR and monocular camera detections\n2. Estimate positions, velocities, and orientations in the world of all tracked agents\n3. Provide a precise and accurate state estimate of the opponent agents\n4. Provide a consistent measure of the uncertainty of the agents' state estimates\n5. Be robust to false positives, missed detections, and drop-outs from one or more sensor modalities\nFinally, tracking must perform all of the above while ensuring as little additional latency as possible, handling measurements from perception asynchronously and out of order, and compensating for any delay between sensor measurement and tracking."}, {"title": "3.3.2 Overview of Approach", "content": "Our approach consists of three main components: Filtering, Association, and Fusion. Filtering removes outliers. Association determines whether or not a detection is of a previously seen agent. Fusion is incorporating new measurements of agents' states. In order to minimize processing latency within the tracking stack, well-researched, efficient algorithms are leveraged for each module. Figure 16 presents the Tracking pipeline architecture.\nFiltering: Detections filtered by a confidence threshold, a hyper-parameter within our tracking stack, tuned empirically by analyzing the false positives and associated confidence produced by perception. Additionally, any detection that falls outside of the track bounds is ignored. The combination of these two filtering steps helps to ensure that only valid detections are processed and used to generate tracked agents.\nAssociation: AB3DMOT [47] provides the data association module utilized by tracking stack. By employing two computationally efficient algorithms, the Hungarian algorithm for data matching [48] and the Kalman filter [49] for fusion and prediction, the authors demonstrate strong results on multiple open-source data sets while also providing high-frequency predictions. In practice, we observed that the Hungarian algorithm with Euclidean distance often resulted in poor data association, especially during temporary sensor drop- out. Therefore, our implementation uses simple greedy matching with the Mahalanobis distance [50], which performed better in testing."}, {"title": "3.3.3 Discussion, Limitations, and Future Work", "content": "The tracking pipeline meets all requirements and is sufficiently accurately and performant to handle the IAC Passing Competition. Our modular design was especially important when the camera perception was disabled on race day, outlined in detail in Section 3.2.6. Given these successes, however, our system has not been robustly tested against multiple agents, specifically agents that are close together (i.e. \u2264 5m). In traditional motorsports, humans drive aggressively in close proximity to one another. While the competition format is far from this style of racing, future works will need to handle such operating domains in order to challenge professional drivers. Multiple agents in close proximity are more difficult to track, due to higher association ambiguity and occlusions.\nFinally, a Kalman filter will be replaced by an Extended Kalman filter (EKF) to enable a non-linear motion model. With an EKF and a better motion model (i.e. constant curvature), predictions of agent tracks will be more accurate, which is especially important during periods of infrequent detections (i.e. the other agent is in the blind-spot produced by the rear wing of the vehicle)."}, {"title": "3.4 Planning", "content": ""}, {"title": "3.4.1 Overview", "content": "We developed a fast, modular motion planning stack capable of predicting agent behavior and safely trailing and passing other agents. Figure 17 shows a high-level overview of the motion planner. The path planner identifies the agent's position at points in the track where it is most optimal to pass and extrapolates its position in the following lap. We have based our path planning approach on a set of primitive behaviors from which higher-level strategies can select. These primitive behaviors include opponent trailing, raceline following, and lane switching. This modularity allows for a clean separation between high-level decision- making and trajectory selection and generation.\nWe also take advantage of knowing the track geometry and develop a set of strong priors in the form of a trajectory bank. Offline, trajectories are generated for various lanes along the track, providing different levels of clearance from the inner and outer track boundaries. Online, the planner selects the best trajectory from the bank based on the selected action primitive."}, {"title": "3.4.2 Offline Trajectory Generation", "content": "The raceline generation is a two-step process that begins with defining a set of waypoints on the track. Afterward, splines are interpolated on these waypoints to ensure continuity. This process relies heavily on the manual selection of waypoints at apexes to properly leverage spline properties.\nFor the overtaking competition, waypoints were selected manually from two lanes. We consider a lane to be defined as a path with both an inner and outer boundary and derive a center-line equidistant from the two boundaries. We obtain these two lanes by dividing the width of the track in half along the full length. Within these lanes, the process of manual sampling begins. A series of N waypoints, (q1,..., qv), are used to interpolate closed Spiro splines [51]."}, {"title": "3.4.3 Online Action Selection", "content": "The multi-agent passing competition sets two roles for the competitors: \"defender\" and \"attacker\". As defined in Section 2, the attacker passes the defender that is maintaining a raceline within the inside lane of the track (see Figure 4). The attacker is also responsible for maintaining a safe distance from the defender. We define a set of Action Primitives to encode these maneuvers:\n\u2022 Maintain:\nMaintain the current raceline at a given speed.\n\u2022 Trail:\nMaintain a fixed distance behind the opponent vehicle on the current raceline.\n\u2022 Safe Merge:\nMerge between arbitrary lanes safely (i.e., avoid collisions with other agents).\nSelection of the primitives is dependent on the current role (defender or attacker) and the current Track Condition (i.e., Green, Yellow, Red, Waving Green), which are both defined by flags sent to the vehicle from Race Control.\nAttacker:\n\u2022 Under a Green Flag, the Attacker must close the gap with the defender (Trail)\n\u2022 Under a Waving Green Flag, the Attacker may initiate the pass (Safe Merge)\n\u2022 Under a Waving Green Flag and after the Attacker has passed the opponent by at least 30m, the Attacker must \"Close the Door\" by merging back to the inside lane (Safe Merge)\n\u2022 Under a Green Flag, the Attacker has the freedom to take any lane, but may not begin passing until explicitly allowed (Trail or Safe Merge)\nDefender:"}, {"title": "3.4.4 Online Raceline Merging", "content": "Merging between inner and outer racelines (i.e. \u201cSafe merging\") is accomplished by first calculating the closest pose on the inner raceline $q_{in}^{k}$ corresponding to every pose on the outer raceline $q_{out}^{k}$. Next, given a start and a final interpolation pose, $q_{in}^{k}$ and $q_{out}^{k}$, as well as the corresponding twist, the spline optimization formulation in Section 3.2 of [54] is used to generate a minimum jerk merging trajectory. Time intervals $h_k$ between consecutive time stamps can be decreased as desired to generate a sufficiently smooth raceline between $q_{in}^{k}$ and $q_{out}^{k}$."}, {"title": "3.4.5 Results and Discussion", "content": "Figure 17 displays a comprehensive flowchart of the mentioned approach and highlights the offline and online computations. Tasks performed by individual components of the planner are agnostic to the actions performed by other components. This includes adding or removing additional raceline primitives as well as action primitives, making this architecture highly modular. Future work on the planner includes automating the process of waypoint selection based on given inside and outside lines, which would consider varying track curvature. Finally, this approach is very fast, efficient, and able to easily execute at 20Hz (and capable of a much higher execution rate). Action selection, safe-merge trajectory interpolation, and velocity profiling are all very inexpensive to compute online."}, {"title": "3.5 Controls", "content": ""}, {"title": "3.5.1 Overview of Approach", "content": "Our controls architecture was designed for speed and modularity. To achieve this, we decompose our control task of path tracking into three tasks lateral control, longitudinal control, and gear selection. Lateral tracking generates a steering angle such that the vehicle converges to the path. The longitudinal controller maintains a particular longitudinal velocity. The gear-shifting controller is responsible for selecting the optimal gear for maintaining the current vehicle speed. The majority of the content in this section will focus on the lateral tracking element of the vehicle's controller. The architecture of the stack is described in Figure 20. Finally, the work presented here is a continuation of our previous work in [55]."}, {"title": "Longitudinal Control and Gear Shifting", "content": "The throttle and brake control utilizes a simple P control scheme paired with a feed-forward term to account for drag. This is expressed within Algorithm 1 line 6, which returns a command value. This command is then interpreted as either a throttle or brake command depending on whether the command is positive or negative, using a scaling factor on the brake signal to account for differences in magnitude between the throttle and brake. Smoothing on the throttle and brake signals prevents instantaneous acceleration or deceleration to help prevent vehicle instability. Algorithm 1 summarizes this algorithm.\nThe gear-shifting strategy is based on a simple lookup table where the optimal gear is computed given the current velocity of the vehicle. This gearing table was generated based on a model of the engine and track parameters to maximize torque, the computation of which is outside the scope of this paper."}, {"title": "Lateral LQR Control", "content": "The lateral controller is built around a Linear-Quadratic Regulator (LQR) that generates an optimal feedback policy based on a nominal vehicle dynamics model. LQR was chosen because (1) it is inexpensive and fast to compute the desired controls online and (2) it is optimal given the vehicle dynamics model. However, one downside is that LQR does not reason about actuation constraints and only considers the error concerning the reference at the current time step. However, the vehicle primarily drives on an oval race track where only a few degrees of steering are required even for the most aggressive maneuvers. Hence, LQR can stably control the vehicle and meet the reference tracking requirements to safely avoid the other agent, track barriers, and maintain the desired trajectory.\nGiven a continuous-time linear system of the form in Equation 2 a quadratic cost function such as Equation 3 can be defined which is constrained by Equation 2. Equation 3 also has the constraints that $Q = Q^T > 0$ and $R = R^T > 0$.\nAn optimal feedback policy can be derived such that the cost function in Equation 3 is minimized over an"}, {"title": "Dynamic Bicycle Model", "content": "We utilize a four-state dynamic bicycle model from [57], shown in Equations 5 and 7. As noted in Section 3.5.1, we decompose the problem into separate lateral and longitudinal controllers. The lateral controller reasons about the vehicle's lateral position y and yaw angle \u03c8 in the Local Tangent Plane (LTP) frame provided by localization. The input for this system is the steering angle \u03b4. It also assumes some constant velocity Vx, which is the forward component of the speed ignoring lateral slip. The Caf and Car parameters are the cornering stiffness of the front and rear tires respectively, which reflect the ability of the tires to resist deformation while the vehicle corners. In addition, lf and lr represent the length of the vehicle from the center of gravity to the front and rear axles respectively. The parameter Iz is the scalar moment of inertia about the z-axis, and m is the mass of the vehicle."}, {"title": "Pure Pursuit, LQR Tracking Algorithm", "content": "Given the model formulation and the LQR formulation, we can make a lateral tracking algorithm by combining a pure-pursuit style look-ahead point and a feedback"}, {"title": "3.5.2 Results and Discussion", "content": "The controller stack shown in Figure 20 was evaluated over several performance laps at the LVMS with velocities ranging between 25m/s and 60.5m/s. We experienced the worst performance at the highest speed targeting 60m/s. We plot velocity and cross-track error (CTE) for the portion of a run in which speeds of 138mph were achieved in Figure 22. The maximum CTE experienced was 1.3m, which occurred around bends, while the mean absolute CTE was 0.323m. Additionally, the average CTE for several high-speed runs (138mph, 141mph, and 137mph max speeds) at various speed brackets are presented in Table 5. Notably, for the run on January 7th, the controller was tuned to be less aggressive, which explains the degraded tracking.\nWe also evaluate the controller on lane-change tasks on the same track."}, {"title": "3.6 Localization and State Estimation", "content": ""}, {"title": "3.6.1 Challenges", "content": "To localize itself on the track, the AV-21 is equipped with two dual-antenna NovAtel PwrPak7D-E1 GNSS units with RTK, as well as individual wheel speed sensors (Table 6). However, the high speed and high acceleration nature of the competition necessitated a custom solution that is robust to partial and total failures. In particular, a solution was needed that could meet the following requirements, all while traveling at high speeds:\n\u2022 Detect partial and full failures of either unit finding an adequate GNSS solution\n\u2022 Handled degraded position and/or heading estimates from one or both units\n\u2022 Accurately estimate the vehicle pose, yaw, and velocity at 100Hz, despite only receiving GNSS measurements at 20Hz"}, {"title": "3.6.2 Overview of Approach", "content": "The Localization module is split into two main components: the Localization Executive and the Robot Localization [58] EKF Filter node. Figure 25 shows an overview of the localization stack. On the AV-21 are dual NovAtel PowerPak Dual-Antenna RTK GNSS systems with built-in IMU and Gyro. The NovAtels are configured to provide the best position (20Hz, velocity, and heading solutions). These measurements, from the varying sources, undergo the following:\n\u2022 Transformed into a Local Tangent Plane (LTP) coordinate frame, if applicable\n\u2022 Solution status flags, variance measurements, and other health indicators are tested against a set of heuristics to verify that the measurement is good and worth fusing\n\u2022 Finally, if the measurement meets quality checks, the measurement is converted to a standard ROS message type and sent to the filter for fusion\nThe node responsible for all of the above is called the Localization Executive. When building the Localization Executive, it was important to have a flexible, generic, and modular framework for defining sources, safety thresholds, and defining safety checks and heuristics. As testing progressed and more track time was put onto the vehicle, we quickly learned and adapted our fusion strategies. For example, the quality of the NovAtel with its dual antennas on the top and front nose of the vehicle often produces an overall better heading and position result than the other unit, likely due to a wider baseline. The Localization Executive's modularity allows us to prioritize high quality and filter out bad measurements on a per-source (i.e. pose, heading, velocity) basis."}, {"title": "3.7 System Monitoring (Base Station)", "content": "Developing an ARV stack requires monitoring software to ensure safe operations during high-speed testing and live operation during races. In this section, we describe the base station, our multi-level user interface for displaying the safety statuses of the vehicle and cumulatively all parts of the stack. The role of the interface is to be able to see the entire overall health of the vehicle at a glance. If any sensor or service goes offline, a quick look at the base station will clearly show what and where the problem is. There are primary and secondary user interfaces, each serving a specific role and level of surveillance over the stack and physical vehicle. Each interface is modular, meaning the operator can set what telemetry, track, and sensor data they want to see. There are no competition requirements for either interface, but it is a useful tool for the operators in the pit lane to be able to monitor the full health of the system at a glance.\nLike all parts of the stack, the base station architecture, shown in Figure 27, was developed with performance and reliability in mind. Existing solutions to rebroadcast ROS 2 DDS topics over the track's mesh network"}, {"title": "3.7.1 Primary Interface", "content": "The primary interface displays track states, vehicle states and speeds, watchdog states, sensor frequencies, GPS health, and the computer's compute utilization. Figure 28 shows the display for the primary interface, made up of a GUI using PyQT and an odometry display using RViz. Using PyQT, we can easily add or modify panel modules on the GUI to fit our area of concentration for a test run or competition run. With RViz, we can visualize the path the vehicle is following, the current vehicle odometry, and any agents we have detected and are tracking. RViz is easily configurable for what topics and data we want to see. This interface is meant primarily to be an information display and not to give base station operators control over vehicle functions.\nAvailable telemetry information includes the state of the power train, vehicle position and speed, and target speed. This information informs the operator if the vehicle is in a healthy state overall and what is it trying to do. The CPU and memory usage is also displayed to ensure the computer onboard is running smoothly. The watchdogs serve as surveillance agents over the stack, indicating if any component of the stack fails or crashes. When a watchdog fails, the stack will intelligently bring the car to a safe stop. All sensor frequencies are displayed, indicating if"}]}