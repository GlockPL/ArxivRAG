{"title": "LLM Agents Making Agent Tools", "authors": ["Georg W\u00f6lflein", "Dyke Ferber", "Daniel Truhn", "Ognjen Arandjelovi\u0107", "Jakob N. Kather"], "abstract": "Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains which demand large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose TOOLMAKER, a novel agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a short task description and a repository URL, TOOLMAKER autonomously installs required dependencies and generates code to perform the task, using a closed-loop self-correction mechanism to iteratively diagnose and rectify errors. To evaluate our approach, we introduce a benchmark comprising 15 diverse and complex computational tasks spanning both medical and non-medical domains with over 100 unit tests to objectively assess tool correctness and robustness. TOOLMAKER correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. TOOLMAKER therefore is a step towards fully autonomous agent-based scientific workflows. Our code and benchmark is available at https://github.com/KatherLab/ToolMaker.", "sections": [{"title": "Introduction", "content": "Scientific discovery is the foundation for innovation and progress Traditionally, the underlying research processes that guarantee progress have been entirely reliant on human expertise, involving the formulation of research ideas and hypotheses, the collection of information and analysis of data, the planning and execution of experiments, and iterative refinement to arrive at a solution. With the recent development of autonomous agent systems that employ LLMs to perform tasks through multi-step reasoning and planning, and by utilising tools (external pieces of software that the model can execute), we are at the cusp of a paradigm shift where artificial intelligence (AI) can assist throughout entire research projects as a virtual scientist (Figure 1), rather than being limited to addressing narrowly and a priori defined problems.\nAlthough LLM agents have shown success for specific tasks in domains such as software engineering (Wang et al., 2024; Yang et al., 2024), healthcare (Ferber et al., 2024; Kim et al., 2024), law (Li et al., 2024), and scientific research (Swanson et al., 2024; Gao et al., 2024; Schmidgall et al., 2025), they struggle to generalise to broader classes of tasks. This limitation arises from their reliance on tools that must be explicitly designed, implemented, and integrated by human developers \u2013 often requiring extensive technical expertise \u2013 before deployment (Ferber et al., 2024; Jimenez et al., 2024). While AI assistants can support this process, current systems still depend heavily on manual intervention to ensure compatibility and functionality.\nTo address this, some agentic frameworks have been designed that autonomously craft their own tools (Cai et al., 2024; Yuan et al., 2024; Qian et al., 2023). However, because these methods build"}, {"title": "Related work", "content": "In addition to demonstrating impressive capabilities in generating human-like text, LLMs such as ChatGPT (Ouyang et al., 2022), Claude (Anthropic, 2024), Gemini (Gemini Team, 2024) and Llama (Llama Team, 2024), on their own, have shown strong potential in question answering and reasoning on problems in natural science related fields, like math (Shao et al., 2024), chemistry (Bran et al., 2024) and healthcare (Singhal et al., 2023). However, LLMs often struggle solving more complex problems directly, especially in situations that require intermediate results from multiple steps (Valmeekam et al., 2023). To address this, LLM agents have been developed which enhance an LLM's capabilities by integrating external tools (Schick et al., 2023).\nIn the medical domain, LLM agents have been developed for tasks like clinical decision-making and diagnostics, e.g. AgentMD (Jin et al., 2024) creates risk calculators from medical publications, and Ferber et al. (2024) propose an autonomous oncology agent that consults guidelines, databases, and imaging tools. Multi-agent systems extend this idea to collaborative scenarios involving clinicians, patients, and entire hospitals (Kim et al., 2024; Li et al., 2025). Beyond clinical applications, bioinformatics agents have been proposed with special-"}, {"title": "TOOLMAKER", "content": "We design TOOLMAKER to autonomously convert stand-alone code repositories from scientific publications into LLM-compatible tools. Each tool should complete a specific, user-defined task. To do so, we require a minimal tool definition (see Figure 2, top), consisting of:\n1) a concise textual description of the task,\n2) GitHub URL of the associated repository, and\n3) a list of required input arguments, including an example value for each argument.\nThis tool definition could in principle be represented as the signature of a Python function with a docstring, like in existing code generation tasks (Zhuo et al., 2024; Jain et al., 2024). However, unlike previous work, we require the LLM to not only implement the function, but also to set up the environment wherein the function will be executed. The latter is necessary due to the complexity of our tasks which require e.g. installing external dependencies, downloading models, and setting up configurations while considering system and hardware specifications.\nWe structure TOOLMAKER as an agentic workflow (see Figure 3) that consists of two stages: environment setup and tool implementation. During environment setup, TOOLMAKER produces a reproducible \"snapshot\u201d of the system (a Docker image) in which the final tool will run. In the second stage, TOOLMAKER generates a Python function that implements the desired task."}, {"title": "Workflow components", "content": "We define the state of the workflow at any point in time to be a pair\n$s = (h, e) \\in H \\times E$.\nHere, $h \\in H$ is the conversation history (the ordered sequence of messages from the user, tools, and the LLM), and $e \\in E$ is the environment state (represented by a checkpointed Docker container).\nTOOLMAKER is built out of fundamental com-"}, {"title": "LLM calls", "content": "An LLM can be viewed as a function\nLLM: H\u2192 M,\nwhich, given a conversation history, produces a single new message. As a TOOLMAKER workflow component, an LLM call l: H\u2192 H \u00d7 M takes the workflow state's conversation history h, appends LLM(h), and returns the new message:\nh\u2192 (hLLM(h), LLM(h)).\nLLMs calls thus only update the conversation and do not modify the environment. We use OpenAI's gpt-40-2024-08-06 model for the LLM calls."}, {"title": "Environment interactions", "content": "An environment interaction is any action a \u2208 A that can read from or write to the environment state e. We may thus model it by\ne\u2192 (e', o),\nwhere e' is the updated environment state, and o \u2208 O is the observation produced by the action."}, {"title": "Agents", "content": "An agent \u03c0, illustrated in Figure 4, chains multiple LLM calls and environment interactions to accomplish a specific sub-task which is specified by a high-level instruction, M\u03c0 \u2208 M, e.g. \"install this repository and its dependencies\".\nFormally, an agent \u03c0 maps the current workflow state s = (h, e) to a new state s\u03c0 = (h\u03c0, e\u03c0) and return value r \u2208 R:\n\u2192 (h\u03c0, e\u03c0, r).\nThe agent follows a sequence of state transitions\n\u2192 s1 \u2192 ... \u2192 sT,\nwhere each state st = (ht, et) \u2208 S. At step t = 0, the agent receives the initial state\nSO\n= (h \u2295 M\u03c0, e).\nAt each step t, the agent employs a special tool-augmented LLM, denoted\nLLM: H A \u2192 T \u222a R,\nwhich, given the current conversation ht, either"}, {"title": "TOOLMAKER workflow", "content": "In this section, we describe our workflow in detail, which at a high level is illustrated in Figure 3, and in pseudocode in Algorithm 1, using the three types of components (LLM calls, environment interactions, and agents) introduced above.\nTOOLMAKER's initial conversation history h0 is a system prompt that contains the tool definition Mtool. We provide the full prompts in Appendix D.\nEnvironment setup To obtain the state of the execution environment necessary for the tool to execute, we employ the INSTALL_REPOSITORY agent (line 2) that is instructed to install and set up the repository. This agent clones and explores the repository, reads documentation, and downloads any dependencies it deems necessary such as mod-"}, {"title": "Execution environment", "content": "An important implementation detail is the execution environment, which is the environment in which (i) actions (A) are performed throughout the TOOLMAKER workflow, and (ii) wherein the final tool created by TOOLMAKER will be executed.\nThe execution environment itself is stateful. Specifically, write actions Aw = {,}\nmay mutate environment state. However, we require the ability to roll back to previous states, e.g. on line 10 of Algorithm 1, the execution environment is restored to the \"freshly installed\" state e. Furthermore, the execution environment should be sandboxed from the host system (for security reasons), and it should be reproducible (so the generated tool can be executed on any machine).\nWe satisfy these requirements by implementing the execution environment as a Docker container that TOOLMAKER controls via an HTTP server running inside the container, which can run the predefined actions A. State restoration is achieved via Docker's checkpointing functionality."}, {"title": "Benchmark", "content": "To evaluate our approach, we curate a dataset of 15 diverse tasks spanning multiple scientific dis-"}, {"title": "Task definitions", "content": "Each task definition consists of: (i) a concise one-sentence task description, (ii) a URL to the associated code repository, (iii) a list of input arguments required to execute the task, alongside an example invocation (see below), and (iv) a description of the expected output. Figure 2 (top) shows an example task definition and an overview of task names and associated papers can be found in Table 2. We provide a full list of all task definitions with their example invocations in Appendix C."}, {"title": "Invocations", "content": "A task invocation specifies a concrete value for each input argument, as well as external files and directories that should be made accessible from within the execution environment during the invocation. Indeed, most tasks in TM-BENCH require external files, e.g. stamp_train_classification_model takes an input dataset of whole slide images (WSIs) and a clinical data table, on which to train a classification model using the STAMP (El Nahhas et al., 2024) pipeline. Analysing and utilising datasets is a fundamental aspect of many real-world scientific tasks, which is why TM-BENCH explicitly supports this functionality, unlike many existing code generation benchmarks (Zhuo et al., 2024; Jain et al., 2024)."}, {"title": "Assessing correctness", "content": "TM-BENCH specifies 2-3 additional test invocations per task, which are different to the example invocation (using different argument values and external datasets) and are held-out from the tool creation process. For each invocation, TM-BENCH includes unit tests to assess whether the tool produces the expected output by checking various properties of the return value"}, {"title": "Results", "content": "TM-BENCH can evaluate any \"tool maker\" that produces an environment definition and a tool implementation . However, to the best of our knowledge, no existing approaches are specifically designed to address the \"paper repository \u2192 LLM tool\" problem. In order to nonetheless facilitate comparison with prior work, we adapt the Open-Hands (Wang et al., 2024) to this setting. Open-Hands is a software engineering agent that achieves SOTA performance on SWE-bench (Jimenez et al., 2024). We instruct OpenHands to generate the same artifacts as TOOLMAKER: an environment definition (expressed as a bash script to be run in a fresh python:3.12 Docker image to create the environment state required for the tool to execute) and a tool implementation (a Python function). To ensure a fair comparison, we reuse large parts of the TOOLMAKER prompts in the prompts we supply to the OpenHands, and add additional in-structions to encourage OpenHands to test the arti-facts it creates. We use gpt-40 for the OpenHands"}, {"title": "Ablations", "content": "Paper summaries Since each task is based on one or more research papers, we perform an ablation study to determine whether we can inject useful information from the papers into the tool creation process. Instead of directly including the full paper text in the prompts which would require too many tokens, we first provide the full text to gpt-40 and instruct it to summarise it with respect to the task at hand. Then, we provide these task-specific and paper-specific summaries in the prompts for TOOLMAKER and OpenHands.\nThe results in Table 3 indicate that including"}, {"title": "Choice of LLM", "content": "We also evaluate TOOLMAKER and OpenHands using OpenAI's 03-mini model instead of gpt-40, and find that while this reduces cost, it also degrades performance in both cases. Finally, since OpenHands achieved SOTA performance on SWE-bench (Jimenez et al., 2024) using Claude 3.5 Sonnet (Anthropic, 2024), we re-run the OpenHands baseline using this model, but find that it performs worse than using gpt-40 (see Table 3)."}, {"title": "Conclusion", "content": "In this work, we showed that autonomous tool creation can go beyond simple Python functions and produce tools for real-world scientific tasks. We introduced TOOLMAKER, a framework that autonomously transforms scientific code repositories into LLM-compatible tools, potentially drastically reducing the technical overhead in future for developing agents with specialised tool-sets. In evaluations across multiple scientific domains, TOOLMAKER surpassed the state-of-the-art software engineering agent, OpenHands, achieving 80% accuracy. Additionally, we release TM-BENCH as a comprehensive benchmark to spur further advancements in agentic tool creation.\nWe acknowledge that automated tool creation in life sciences carries significant risks that require careful consideration. The ability to autonomously implement complex biological and chemical tools could potentially be misused for creating harmful agents or bioweapons. Additionally, fully automated research systems might inadvertently gen-"}, {"title": "Limitations", "content": "While TOOLMAKER addresses the critical challenge of tool creation, we acknowledge that fully autonomous scientific discovery remains constrained by physical experimentation requirements. This is an aspect which our work does not address. However, with an increasing proportion of life science research being conducted in silico, TOOLMAKER provides a crucial building block for autonomous scientific workflows. Future work will focus on integrating TOOLMAKER into broader autonomous research systems, potentially enabling end-to-end scientific discovery pipelines that operate with minimal human intervention.\nOur framework assumes that the referenced code repositories are reasonably well-structured, up-to-date, and documented. In practice, however, open-source repositories may have poor documentation or incomplete implementation details, making them challenging to install or integrate automatically. In fact, there is no guarantee that any given repository will be installable and usable as a tool. For TM-BENCH, we manually curated the tasks such that we were able to successfully install and use the repository ourselves. This way, we could ensure that the tasks were possible in the first place.\nWhile TM-BENCH contains over 100 unit tests to evaluate the correctness of the tools, passing these tests does not guarantee perfect correctness in all real-world scenarios. Scientific workflows often involve edge cases, large-scale data, or unexpected computational patterns that are not captured by a small set of tests. Moreover, high-stakes ap-plications such as clinical research would naturally demand additional layers of rigorous validation and oversight by domain experts.\nFinally, we have endeavored to make TM-BENCH fully reproducible by specifying exact commits and branches for the referenced repositories to ensure tasks remain stable over time. Nonetheless, external factors such as repository deletion, force-pushing changes, or renaming branches, could render our pinned references"}, {"title": "A Detailed workflow description", "content": "We provide a detailed description of every step in the TOOLMAKER workflow to supplement Algorithm 1 and our discussion thereof in Section 3.2."}, {"title": "Setting up the environment", "content": "The environment definition is a state of the world (e.g. the operating system) that is required for the tool created by TOOLMAKER to execute. We can represent this state as a sequence of actions (e.g. bash commands or instructions in a Dockerfile, as shown in Figure 2, left) that mutate a known initial state (e.g. a freshly installed operating system) to the state required for the tool to execute.\nTo obtain the state of the execution environment necessary for the tool to execute, we employ an agent that is instructed to install and set up the repository (we provide the full prompt in Appendix D). This agent will clone and explore the repository, read documentation, and download any dependencies it deems necessary such as models, datasets, and libraries. Each of these steps involve planning and learning from previous mistakes such as error logs arising during execution. The agent begins with a clean state (a python:3.12 Docker image). Importantly, we record all actions that the agent performs. Since each of the write actions can be expressed as a bash command, we can simply concatenate the bash representations of all recorded write actions to obtain the environment definition in the form of a bash script or Dockerfile."}, {"title": "Initial tool implementation", "content": "Equipped with the environment definition, which allows TOOLMAKER to reset the state of the execution environment to the state in which the tool should be executed, it can now implement the tool itself. Note that we do not carry over the conversation history from the previous stage, in order to not pollute the context window with a large number of messages that are irrelevant for this stage."}, {"title": "Gather information", "content": "We first instruct an agent to explore the installed repository and gather all information necessary to implement the tool. We include the tool definition (see Figure 3, top left) as a Python function signature with a docstring in the initial prompt, so that it can use the information it has already gathered to create the plan."}, {"title": "Create a plan", "content": "Then, we perform an LLM call to create a step-by-step plan for the tool implementation. Here, we keep all of the agent's messages (including actions and observations) in the conversation history, so that it can use the information it has already gathered to create the plan."}, {"title": "Implement the tool function", "content": "Next, we instruct the LLM to implement the tool based on the plan. Again, we keep the entire conversation history in the context window of the LLM call, so that it can refer to previous messages. We now have our first candidate implementation of the tool function."}, {"title": "Closed-loop self-improvement", "content": "Run the tool Before executing the candidate implementation, we reset the execution environment to the environment definition because the agent may have performed write actions in the past (either in the process of exploring the repository, or in a previous iteration of the loop). Then, we run the candidate Python function in the execution environment, using the example invocation provided in the tool definition."}, {"title": "Assess tool execution", "content": "We instruct the LLM to assess whether the execution was successful, based on the result returned by the function, as well as the standard output and standard error streams produced during function execution. Specifically, we ask the LLM to check whether the result returned by the tool is in line with the task description (i.e. if the result is plausible), and whether the standard output and standard error streams contain any indications of errors. If the LLM determines that the tool execution was successful, we have arrived at our final tool implementation, and exit the loop. Otherwise, we continue the self-improvement loop."}, {"title": "Diagnose error", "content": "We instruct an agent to gather information about the error in order to diagnose its root cause, and to formulate a plan to fix the error. Importantly, we do not reset the execution environment - the agent is able to check intermediate files and outputs created during tool execution."}, {"title": "Re-implement the tool function", "content": "We perform an LLM call to re-implement the tool based on the current implementation, the error diagnosis, and the plan to fix the error."}, {"title": "Summarise the attempt", "content": "Given the conversation history of the current attempt, we instruct the LLM to summarise the attempt (i.e. the diagnosed error and steps taken to fix the error)."}, {"title": "Extended results", "content": ""}, {"title": "Per-task ablation results", "content": "In Tables 4 to 6, we provide detailed extended results for the ablations in a format similar to Table 2 in the main paper."}, {"title": "Raw unit test results", "content": "We provide the raw unit test results for all tasks in Tables 7 and 8 for the main experiments and Tables 9 to 13 for the ablations."}, {"title": "Transitions between tool calls", "content": "In Figure 5, we show the transitions between tool calls by TOOLMAKER."}]}