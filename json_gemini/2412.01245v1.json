{"title": "REVISITING GENERATIVE POLICIES: A SIMPLER REINFORCEMENT LEARNING ALGORITHMIC PERSPECTIVE", "authors": ["Jinouwen Zhang", "Rongkun Xue", "Yazhe Niu", "Yun Chen", "Jing Yang", "Hongsheng Li", "Yu Liu"], "abstract": "Generative models, particularly diffusion models, have achieved remarkable success in density estimation for multimodal data, drawing significant interest from the reinforcement learning (RL) community, especially in policy modeling in continuous action spaces. However, existing works exhibit significant variations in training schemes and RL optimization objectives, and some methods are only applicable to diffusion models. In this study, we compare and analyze various generative policy training and deployment techniques, identifying and validating effective designs for generative policy algorithms. Specifically, we revisit existing training objectives and classify them into two categories, each linked to a simpler approach. The first approach, Generative Model Policy Optimization (GMPO), employs a native advantage-weighted regression formulation as the training objective, which is significantly simpler than previous methods. The second approach, Generative Model Policy Gradient (GMPG), offers a numerically stable implementation of the native policy gradient method. We introduce a standardized experimental framework named GenerativeRL. Our experiments demonstrate that the proposed methods achieve state-of-the-art performance on various offline-RL datasets, offering a unified and practical guideline for training and deploying generative policies.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative models, such as flow models and diffusion models, have demonstrated remarkable capabilities in modeling multi-modal data across diverse applications, including image, video, and audio generation (Rombach et al., 2022; Ho et al., 2022; Mittal et al., 2021), and protein structure prediction (Abramson et al., 2024). Their expressive power stems from constructing continuous and invertible mappings between probability distributions, enabling the transformation of simple distributions like standard Gaussians into complex target distributions. Generative policies, which are RL policy models based on generative models, have become a focus of study within the RL community (Janner et al., 2022; Chi et al., 2023; Ren et al., 2024). They offer a principled approach to modeling expressive and nuanced action distributions, particularly important in robotics tasks with continuous and high-dimensional action spaces.\n\nDespite the success of recent studies in offline-RL (Chen et al., 2023; Wang et al., 2023; Lu et al., 2023; Hansen-Estruch et al., 2023), these studies often employ complex training schemes and lack systematic investigation. This hinders understanding of key factors in training generative models for policy modeling, leading to unnecessary dependencies, training inefficiencies, and higher inference costs. Some work focuses primarily on diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020), which has limited applicability to other generative models, such as flow models (Lipman et al., 2023; Liu et al., 2023; Pooladian et al., 2023; Albergo & Vanden-Eijnden, 2023; Tong et al., 2024)."}, {"title": "2 RELATED WORKS", "content": "Generative Policy Algorithms. Haarnoja et al. (2017) introduces Soft Q-Learning and uses an energy-based generative policy for learning continuous actions from continuous states. Haarnoja et al. (2018a) integrates hierarchical policies with discrete layers into the Soft Actor-Critic (Haarnoja et al., 2018b) framework, which is called SACLSP and utilizes normalizing flows. Janner et al. (2022) first incorporate diffusion models into RL named as diffuser, acting as optimal trajectory planners by linking generative models with value function guidance being applied rudimentary. Chi et al. (2023) explores diffusion models for policy modeling in robotics, which is the concept of diffusion policy is first introduced. Chen et al. (2023) demonstrates effective policy learning in offline Q-learning using a diffusion model as support called SfBC. Wang et al. (2023) introduces Diffusion-QL and achieves policy regularization by alternating training between a Q-function-guided diffusion model and a diffusion-supported Q-function. Lu et al. (2023) introduces Q-guided Policy Optimization (QGPO) and derives the exact formulation of energy guidance for energy-conditioned diffusion models, enabling precise Q-guidance for optimal policy. Hansen-Estruch et al. (2023) introduces Implicit Diffusion Q-learning (IDQL) which uncovers the implicit policy form after Implicit Q-learning, emphasizing the importance of sampling from both the behavior policy and the diffusion model. Chen et al. (2024) uses the score function of a pre-trained diffusion model as a regularizer, optimizing a Gaussian policy to maximize the Q-function by combining the multimodal properties of diffusion models with the fast inference of Gaussian policies, which is called Score Regularized Policy Optimization (SRPO). Similar methods can also be applied to models trained using the flow matching (Zheng et al., 2023b; Kim et al., 2024). More details about the generative policy analyzed in this paper are provided in Appendix C."}, {"title": "3 BACKGROUND", "content": "This section provides a brief introduction to reinforcement learning and generative models. For more fundamentals of diffusion and flow models, please refer to Appendix B."}, {"title": "3.1 REINFORCEMENT LEARNING", "content": "Reinforcement learning (RL) addresses sequential decision-making tasks typically modeled as a Markov decision process (MDP), defined by the tuple $(S, A, p, r, \\gamma)$. Here, $S$ is the state space, $A$ is the action space, $p(s_{t+1}|s_t, a_t)$ represents the transition dynamics, $r(s_t, a_t)$ is the reward function, and $\\gamma$ is the discount factor. At each time step $t$, the agent, in state $s_t$, takes an action $a_t \\in A$ according to the policy $\\pi(a_t|s_t)$. The agent then receives a reward $r_t$ and transitions to a new state $s_{t+1}$ based on the transition dynamics $p(s_{t+1}|s_t, a_t)$. The goal of the agent is to learn a policy $\\pi : S \\rightarrow A$ that maximizes the expected cumulative reward over time, using only previously collected data. This objective can be expressed as: $R_{\\theta, \\pi} = E_{s_0, a_0, s_1, a_1,...} [\\sum_{t=0}^{\\infty} \\gamma^t r_t]$.\n\nIn offline reinforcement learning (offline-RL), the agent has access to a fixed dataset $D_u$ of historical interaction trajectories $\\{s_t, a_t, r_t, s_{t+1}\\}$, collected by a behavior policy $\\mu(a_t|s_t)$, which is often suboptimal. Offline-RL is challenging because the agent cannot collect new data to correct its mistakes, unlike in online-RL, where exploration is possible.\n\nSuppose the value of action $a$ at state $s$ is modeled by a Q-function $Q(s, a) \\approx R_{s, a}^{\\pi}$, which estimates the expected return of taking action $a$ at state $s$ and following policy $\\pi$ thereafter. The value of state $s$ is modeled by a V-function $V(s) = E_{a \\sim \\pi(\\cdot | s)}[Q(s, a)]$"}, {"title": "3.2 GENERATIVE MODELS", "content": "Diffusion Models. Given a fixed data or target distribution, a diffusion model is determined by the diffusion process path as an stochastic differential equation (SDE): $dx = f(t)x dt + g(t)dw_t$. The transition distribution of a point $x \\in \\mathbb{R}^d$ from time 0 to t is: $p(x_t|x_0) \\sim \\mathcal{N}(x_t|a_tx_0, \\sigma_t^2\\mathbb{I})$. The reverse process path can be described by an ordinary differential equation (ODE):\n\n$\\frac{dx_t}{dt} = v(x_t) = f(t)x_t - \\frac{1}{2}g^2(t) \\nabla_{x_t} \\log p(x_t),$  (1)\n\nwhere $v(x_t)$ is the velocity function and $\\nabla_{x_t} \\log p(x_t)$ is the score function, typically modeled by a neural network with parameters $\\theta$, denoted as $v_{\\theta}(x_t)$ and $s_{\\theta}(x_t)$, respectively.\n\nFlow Models. Consider a flow model with time-varying velocity $v(x_t)$, whose flow path can be described by an ODE: $\\frac{dx_t}{dt} = v(x_t)$. The velocity field transforms the source distribution $p(x_0)$ at time $t = 0$ into the target distribution $p(x_1)$ at time $t = 1$ along the flow path and conforms to the continuity equation: $\\frac{\\partial p}{\\partial t} + \\nabla_x \\cdot (pv) = 0$."}, {"title": "4 REVISITING GENERATIVE POLICIES", "content": "We establish the formulation of the optimal policy in offline-RL in Section 4.1. Next, we review prior work on generative policies in Section 4.2, with additional details in Appendix C. Subsequently, Section 4.3 introduces two straightforward and effective training schemes."}, {"title": "4.1 OPTIMAL POLICY IN OFFLINE-RL", "content": "Previous works in offline-RL (Peters & Schaal, 2007; Peters et al., 2010; Abdolmaleki et al., 2018; Wu et al., 2020) formulate policy optimization as a constrained optimization problem. The policy is learned by maximizing the expected return, subject to the KL divergence constraint to the behavior policy: $\\pi^* = \\arg \\max_{\\pi} E_{s \\sim D, a \\sim \\pi(\\cdot|s)}[Q(s, a) - \\beta D_{KL}(\\pi(\\cdot|s) || \\mu(\\cdot|s))]$. This approach ensures the learned policy rarely acts outside the support of the behavior policy, thus avoiding extrapolation errors that could degrade performance, as emphasized by Kumar et al. (2019) and Fujimoto et al. (2019). The optimal policy has an analytical form, as shown by Peng et al. (2021): $\\pi^*(a|s) = \\frac{e^{\\beta (Q(s, a) - V(s))}}{Z(s)} \\mu(a|s)$, where $Z(s)$ is a normalizing factor, and $\\beta$ is a temperature parameter. In practice, we can build a parameterized neural-net policy $\\pi_{\\theta}$ to approximate the optimal policy $\\pi^*$. The policy model can be trained by minimizing the KL divergence:\n\n$L(\\theta) = E_{s \\sim D} [D_{KL}(\\pi^*(\\cdot|s) || \\pi_{\\theta}(\\cdot|s))] = E_{s \\sim D, a \\sim \\mu(\\cdot|s)}[-\\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)} \\log \\pi_{\\theta}(a|s)] + C,$  (4)\n\nwhere $C$ is a constant that does not depend on $\\theta$, or by using reverse KL divergence:\n\n$L(\\theta) = E_{s \\sim D} [D_{KL}(\\pi_{\\theta}(\\cdot|s) || \\pi^*(\\cdot|s))] = E_{s \\sim D, a \\sim \\pi_{\\theta}(\\cdot|s)}[-\\beta Q(s, a) + D_{KL}(\\pi_{\\theta}(\\cdot|s) || \\mu(\\cdot|s))] + C.$  (5)\n\nComparison and derivation details of Eq. 4 and Eq. 5 are provided in Appendix A."}, {"title": "4.2 PREVIOUS WORKS ON GENERATIVE POLICY", "content": "Table 1 summarizes existing approaches for obtaining optimal generative policies. Table 2 provides an overview of the training and inference schemes of these algorithms.\n\nGenerative Model Type Table 1 highlights that QGPO and SRPO are limited to diffusion models, while other algorithms accommodate various generative models. QGPO's restriction arises from its reliance on the Contrastive Energy Prediction method (Eq. 30), which distills the Q function into an energy guidance model specifically designed for diffusion models (Appendix C.2). Similarly, SRPO's constraint stems from its score-regularized loss, which naturally aligns with diffusion models due to their inherent modeling of the score function. This compatibility, however, does not extend to non-diffusion models, such as flow models (see Appendix C.4 for further explanation)."}, {"title": "4.3 GENERATIVE MODEL POLICY TRAINING", "content": "To improve upon previous methods, we propose two straightforward yet effective training schemes for generative model policy optimization, derived from Eq. 4 and Eq. 5, as shown in Table 2. These schemes satisfy three key requirements: (1) Generality: Ensure compatibility with any generative model through a simple and effective training process. (2) Decoupled Training: Train the Q-function directly using Implicit Q-Learning (IQL), minimizing reliance on generative sampling. (3) Explicit Policy Extraction: Consistently infer only one action at a time for model inference.\n\nGenerative Model Policy Optimization. Inspired by Song et al. (2021a), who demonstrated that training with a maximum likelihood objective is equivalent to score matching, we replace the log-likelihood term with the matching loss (Eq. 2) of the generative model. By keeping the exponential form of the advantage function as the importance weight, as in Eq. 4, we derive the following advantage-weighted regression training objective suitable for both diffusion and flow models:\n\n$\\mathcal{L}_{GMPO}(\\theta) = E_{s \\sim D, a \\sim \\pi^*(\\cdot | s)} [\\mathcal{L}_{Matching}(\\theta)] = E_{s \\sim D, a \\sim \\mu(\\cdot | s)}[\\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)} \\mathcal{L}_{Matching}(\\theta)]$  (6)\n\nAlthough Eq. 6 looks similar to Eq. 4, with the log-likelihood term replaced by the matching loss, it can be derived independently. See Appendix C.6 for more details.\n\nUnlike previous works, our GMPO approach does not have to use data augmentation from the behavior policy. This removes the necessity of behavior policy and uses only data from the offline dataset:\n\n$\\mathcal{L}_{GMPO}(\\theta) = E_{(s, a) \\sim D} [\\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)} \\mathcal{L}_{Matching}(\\theta)]$.  (7)\n\nMore details about GMPO are provided in Appendix C.6.\n\nGenerative Model Policy Gradient. This approach is directly derived from Eq. 5:\n\n$\\mathcal{L}_{GMPG}(\\theta) = E_{s \\sim D} [D_{KL}(\\pi_{\\theta}(\\cdot | s) || \\pi^*(\\cdot | s))] = E_{s \\sim D, a \\sim \\pi_{\\theta}(\\cdot | s)}[-\\beta Q(s, a) + D_{KL}(\\pi_{\\theta}(\\cdot | s) || \\mu(\\cdot | s))] = E_{s \\sim D, a \\sim \\pi_{\\theta}(\\cdot | s)}[-\\beta Q(s, a) + \\log \\pi_{\\theta}(a | s) - \\log \\mu(a | s)] .$  (8)\n\nAs an RL-native policy gradient method, GMPG directly calculates the log-likelihood term. However, efficiently computing gradients for diffusion and flow models is challenging due to their forward sampling process, which involves solving an initial value problem within an ODE solver. To address this, we employ advanced techniques such as the adjoint method or Neural ODEs (as proposed by Chen et al. (2018)), ensuring computational feasibility. Additionally, we utilize the Hutchinson trace estimator (Hutchinson, 1990; Grathwohl et al., 2019) to compute the log-likelihood of the policy for continuous-time generative models. See Appendix C.7.1 for more details.\n\nIn contrast to GMPG's direct log-likelihood calculation, Wang et al. (2023) propose an alternative approach that replaces the log-likelihood term with a score matching loss (Eq. 37), and further mitigates computational costs by employing a low time step (T = 5). However, this method has limitations, as it may not scale effectively to high-dimensional spaces and could potentially compromise generation quality with such a low time step. More details about GMPG are provided in Appendix C.7.\n\nWe illustrate the intuitive sampling trajectories of GMPO and GMPG with a 2D toy example in Appendix C.8 to clarify how it works."}, {"title": "5 FRAMEWORK", "content": "Based on the research needs analyzed in previous sections, we create GenerativeRL for verifying and comparing various generative models and reinforcement learning algorithms. The key distinction of GenerativeRL is its standardized implementation and unified API for generative models, allowing researchers to access these models at the configuration level without dealing with complex details. When compared to existing frameworks like CleanDiffuser (Dong et al., 2024), GenerativeRL differs in its design principles:\n\nUnified API: It offers a simple API that maximizes compatibility with different kinds of generative models, avoiding immature algorithms.\nFlexible Data Formats: It ensures consistent data formats for long-term use, supporting inputs and outputs as PyTorch tensors, tensordicts (Bou et al., 2023), and treetensors (Contributors, 2021), given the prevalence of dict-type data in RL.\nAuto-Grad Support: Designed to support Neural ODEs, it facilitates gradient-based inference via ODE or SDE, which is useful for many RL policies.\nDiverse Model Integration: It integrates various generative models, including flow and bridge models, treating diffusion models as a special case.\nThus, GenerativeRL seamlessly incorporates both diffusion and flow models for various RL algorithms, while decoupling the generative model from RL components. This allows for consistent comparisons and analyses of different generative models within the same RL context. See usage examples in Appendix E.1 and framework structure in Appendix E.2."}, {"title": "6 EXPERIMENTS", "content": ""}, {"title": "6.1 EXPERIMENTAL SETUP", "content": "Experiments are conducted on the classical offline-RL environments, D4RL dataset (Fu et al., 2021) and RL Unplugged DeepMind Control Suite datasets (Gulcehre et al., 2020).\n\nFollowing QGPO (Lu et al., 2023), we adopt the same U-Net architecture with three hidden layers for all generative policy algorithms. The sampling process is performed using the Euler-Maruyama method in an ODE solver with uniform time steps, T = 1000 in training for GMPG, and T = 32 in evaluation for both GMPO and GMPG. All evaluations are conducted and averaged over five random seeds. Performance scores on D4RL datasets are normalized as suggested by Fu et al. (2021). We re-implemented QGPO, IDQL and SRPO under the same experimental settings for fair comparisons, reporting both our implementation scores and the original scores from the respective papers.\n\nMore details about computation resources, hyperparameters, and training specifics can be found in Appendix D.1."}, {"title": "6.2 EXPERIMENTS AND ANALYSIS", "content": "We address two key questions: (1) Can simpler RL-native training schemes like GMPO and GMPG extract the optimal policy and achieve performance comparable to state-of-the-art algorithms on the classical offline-RL dataset? (2) What are the experimental differences between GMPO and GMPG, given that both forward KL and reverse KL theoretically point to the same optimal policy?\n\nTable 3 shows the performance of various generative policy algorithms on D4RL environments, including SfBC (Chen et al., 2023), Diffusion-QL (Wang et al., 2023), QGPO (Lu et al., 2023), IDQL (Hansen-Estruch et al., 2023), and SRPO (Chen et al., 2024). Table 4 presents the performance of generative policies on the RL Unplugged DeepMind Control Suite datasets (Gulcehre et al., 2020). For reference, we include the performance of two classical offline RL algorithms: RABM (Siegel et al., 2020) and D4PG (Barth-Maron et al., 2018), the latter being the algorithm for which most of this dataset was collected. The average performance across these tasks demonstrates that GMPO and GMPG effectively solve challenging continuous control tasks, achieving competitive results compared with other state-of-the-art algorithms.\n\nHansen-Estruch et al. (2023) claims that using highly expressive models with importance weighted objectives can be problematic as such models can increase the likelihood of all training points regardless of their weight. And they find using advantage-weighted regression in the DDPM objective to not help performance, so they recommend sampling from behavior policy and filter out the high Q-value actions with the softmax importance sampling. However, our practice overturns the above claims and found that generative policy trained with advantage-weighted regression even from a scratch initialization can gain comparably equivalent performance to IDQL using resampling tricks, as shown in Table 3. Since utilizing the same Implicit Q-learning method, GMPO and GMPG both successfully extract optimal policies from the Q function. GMPO, a simpler variant of QGPO, achieves comparable performance on these datasets. This indicates that the advantage-weighted regression loss, a common component of both methods, is crucial for successful training.\n\nUnlike Diffusion-QL, which separates the KL divergence into a simulation-free score matching loss but remains Q guidance through simulation, GMPG computes both Q guidance and the KL divergence directly through simulation, achieving similar performance. Despite using a significantly larger number of sampling steps (T = 1000) compared to Diffusion-QL (T = 5), GMPG does not exhibit computational difficulties in our implementation. This effectively addresses the limitations highlighted by Wang et al. (2023), where the maximum T they could afford was 20, and they opted for T = 5 to balance performance and computational cost.\n\nIn addition, our experiments challenged the intuition that a complete 1000-step inference inherently leads to high computational costs, even with Neural ODEs. Our results demonstrate that with direct gradient guidance, no extra computational cost is required for optimization. For example, in halfcheetah-medium-v2-GMPG-GVP, optimal performance is achieved in 50-100 steps, taking 5-10 hours on an A100 GPU. Similarly, for halfcheetah-medium-v2-GMPO-GVP, optimal performance occurs at 240K-480K steps, also requiring 5-10 hours. Despite slower calculations for a 1000-step inference for one gradient step, it uses fewer training batches, resulting in similar overall time costs, ensuring no computation is wasted during training.\n\nFigure 1 illustrates the log-likelihood of D4RL datasets evaluated by GMPO/GMPG policies across different training iterations. A higher log-likelihood indicates that the generative model output is closer to the original data distribution. For hopper-medium-v2, the generative model trained with GMPO maintains a certain distance from the original data distribution throughout training. In contrast, the GMPG-trained model closely aligns with the original data distribution during pretraining but diverges more during finetuning compared to GMPO. This divergence allows GMPG to achieve better performance. In the case of halfcheetah-medium-expert-v2, both GMPO and GMPG benefit from high-quality data, as the optimal policy is already near the original distribution. Here, GMPO excels in filtering out high Q-value actions, resulting in a slightly better performance compared to GMPG.\n\nMore experiment details for GMPO and GMPG on D4RL AntMaze dataset can be found in Table 11 (Appendix D).\n\nIn general, GMPG with reverse KL loss outperforms GMPO and other generative policies with Forward KL loss in most medium and medium-replay locomotion tasks. This suggests that the policy gradient method more aggressively leverages Q guidance and is less constrained by the behavior policy, allowing optimization into regions with less data support a beneficial exploration strategy in medium and medium-replay data, though it results in slightly poorer performance with expert data as shown in Table 3. Additionally, GMPO shows more stable training convergence with monotonic improvement, while GMPG exhibits fluctuating performance during training with small batch sizes, requiring larger batch sizes for stability."}, {"title": "6.3 ABLATION EXPERIMENTS", "content": "Generative Model Type. Table 5 presents a performance comparison of three generative models: GVP, VPSDE, and I-CFM (a more detailed comparison is available in Table 10). Overall, all three models demonstrate comparable performance. However, I-CFM exhibits slightly weaker performance in certain cases. This discrepancy may stem from its simpler flow path, as defined in Eq. 28, which could potentially limit its ability to capture the environment's complex dynamics effectively.\n\nSampling Scheme for GMPG. As shown in Table 6, our ablation study reveals that increasing the number of sampling time steps T within the GMPG algorithm can lead to improved performance.\n\nFurther ablation experiments about temperature coefficient $\\beta$ and solver schemes for sampling are detailed in Appendix D.3."}, {"title": "7 CONCLUSION", "content": "In this paper, we provide a comprehensive study of generative policies and propose two unified and RL-native training schemes, GMPO and GMPG, that are effective and straightforward for both diffusion and flow models. GMPO benefits from a stable training process and does not require pretraining the generative model before optimal policy extraction, making it more efficient and easier to train. GMPG is a native policy gradient-based method for continuous-time generative models, and we provide a numerically stable implementation for the RL community. Our experiment results"}, {"title": "A POLICY OPTIMIZATION", "content": "We provide a detailed derivation of the forward KL and reverse KL divergence training objectives in Appendix A.1, and discuss the theoretical connections and differences between these objectives in Appendix A.2."}, {"title": "A.1 DERIVATION DETAILS", "content": "The full derivation of the forward KL divergence training objective in Eq. 4 is as follows:\n\n$\\mathcal{L}(\\theta) = E_{s \\sim D} [D_{KL} [\\pi^*(\\cdot|s) || \\pi_{\\theta}(\\cdot|s)]] = E_{s \\sim D} [\\int \\pi^*(a|s) (\\log \\pi^*(a|s) - \\log \\pi_{\\theta}(a|s))da] = E_{s \\sim D} [\\int \\pi^*(a|s) \\log \\pi^*(a|s)da - \\int \\pi^*(a|s) \\log \\pi_{\\theta}(a|s)da] = E_{s \\sim D} [ -H_{\\pi^*(a|s)} - \\int \\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)} \\mu(a|s) \\log \\pi_{\\theta}(a|s)da]  (9) = E_{s \\sim D} [ -H_{\\pi^*(a|s)}] - E_{s \\sim D, a \\sim \\mu(s)}[ \\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)} \\log \\pi_{\\theta}(a|s)] = E_{s \\sim D, a \\sim \\mu(s)}[\\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)} \\log \\pi_{\\theta}(a|s)] + C$.\n\nThe full derivation of the reverse KL divergence training objective in Eq. 5 is as follows:\n\n$\\mathcal{L}(\\theta) = E_{s \\sim D} [D_{KL} [\\pi_{\\theta}(\\cdot|s) || \\pi^*(\\cdot|s)]] = E_{s \\sim D} [\\int \\pi_{\\theta}(a|s)(\\log \\pi_{\\theta}(a|s) - \\log \\pi^*(a|s))da] = E_{s \\sim D} [\\int \\pi_{\\theta}(a|s)(log \\pi_{\\theta}(a|s) - \\log( \\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)} \\mu(a|s)))da] = E_{s \\sim D} [\\int \\pi_{\\theta}(a|s)(log \\pi_{\\theta}(a|s) - log(\\mu(a|s)))da + \\int \\pi_{\\theta}(a|s) log(\\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)}) da] = E_{s \\sim D} [D_{KL} [\\pi_{\\theta}(\\cdot|s) || \\mu(\\cdot|s)] + \\int -\\pi_{\\theta}(a|s) log(\\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)}))da] = E_{s \\sim D} [D_{KL} [\\pi_{\\theta}(\\cdot|s) || \\mu(\\cdot|s)] + \\int -\\pi_{\\theta}(a|s)(\\beta Q(s, a) - \\beta V(s) - log Z(s))da] = E_{s \\sim D} [D_{KL} [\\pi_{\\theta}(\\cdot|s) || \\mu(\\cdot|s)] + \\int -\\pi_{\\theta}(a|s)(\\beta Q(s, a) da + (\\beta V(s) + log Z(s)) \\int \\pi_{\\theta}(a|s)da] = E_{s \\sim D} [\\int \\pi_{\\theta}(a|s)(-\\beta Q(s, a) + D_{KL} [\\pi_{\\theta}(\\cdot|s) || \\mu(\\cdot|s)])da] + (\\beta V(s) + log Z(s)) \\int \\pi_{\\theta}(a|s)da = E_{s \\sim D} [\\int \\pi_{\\theta}(a|s)(-\\beta Q(s, a) + D_{KL} [\\pi_{\\theta}(\\cdot|s) || \\mu(\\cdot|s)])da] + E_{s \\sim D} [\\beta V(s) + log Z(s)] = E_{s \\sim D, a \\sim \\pi_{\\theta}(\\cdot|s)} [-\\beta Q(s, a) + D_{KL} [\\pi_{\\theta}(\\cdot|s) || \\mu(\\cdot|s)]] + C.$  (10)"}, {"title": "A.2 COMPARISON BETWEEN FORWARD AND REVERSE KL DIVERGENCE TRAINING OBJECTIVES", "content": "To obtain a neural network approximation $\\pi_{\\theta}(a|s)$ of the optimal policy\n\n$\\pi^*(a|s) = \\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)} \\mu(a|s),$ (11)\n\nwhere the policy is a state-conditioned probability distribution, we can use either the forward KL divergence or reverse KL divergence as the training objective. Minimizing the KL divergence between $\\pi_{\\theta}(a|s)$ and the optimal policy $\\pi^*(a|s)$ reduces the discrepancy between the two distributions:\n\nMinimize $D_{KL} [\\pi^*(\\cdot|s) || \\pi_{\\theta}(\\cdot|s)]$ or $D_{KL} [\\pi_{\\theta}(\\cdot|s) || \\pi^*(\\cdot|s)]$. (12)\n\nHowever, as is well-known in the literature (Murphy, 2023), the forward and reverse KL divergence objectives have different properties and implications. Training with the forward KL divergence tends to be mode-covering, preferring to cover all modes of the target distribution, including those with low probability mass. In contrast, training with the reverse KL divergence tends to be mode-seeking, concentrating on modes with high probability mass while potentially ignoring others.\n\nAlthough we employ highly expressive probabilistic models, such as diffusion and flow models, which may mitigate these tendencies, the two training objectives still have different implications and may lead to different performance in practice. We provide a detailed term-by-term illustration of these two methods and their variants in Appendix A.2.1 and A.2.2, respectively."}, {"title": "A.2.1 FORWARD KL DIVERGENCE TRAINING OBJECTIVE", "content": "As shown in Eq. 13", "components": "n\nThe behavior policy $\\mu(\\cdot|s)$, from which actions are sampled.\nAn exponential function of the advantage, $e^{\\beta(Q(s, a) - V(s))}$, acting as an importance weight.\nThe log-likelihood term $\\log \\pi_{\\theta}(a|s)$ for maximum likelihood estimation (MLE).\n\n$\\mathcal{L}(\\theta) = E_{s \\sim D, a \\sim \\mu(\\cdot|s)}[\\underbrace{\\frac{e^{\\beta(Q(s, a) - V(s))}}{Z(s)}}_{Advantage Weights} \\underbrace{\\log \\pi_{\\theta}(a|s)}_{MLE}"}]}