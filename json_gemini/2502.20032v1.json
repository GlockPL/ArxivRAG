{"title": "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity Grouping", "authors": ["Guannan Lai", "Yujie Li", "Xiangkun Wang", "Junbo Zhang", "Tianrui Li", "Xin Yang"], "abstract": "Class Incremental Learning (CIL) requires a model to continuously learn new classes without forgetting previously learned ones. While recent studies have significantly alleviated the problem of catastrophic forgetting (CF), more and more research reveals that the order in which classes appear have significant influences on CIL models. Specifically, prioritizing the learning of classes with lower similarity will enhance the model's generalization performance and its ability to mitigate forgetting. Hence, it is imperative to develop an order-robust class incremental learning model that maintains stable performance even when faced with varying levels of class similarity in different orders. In response, we first provide additional theoretical analysis, which reveals that when the similarity among a group of classes is lower, the model demonstrates increased robustness to the class order. Then, we introduce a novel Graph-Driven Dynamic Similarity Grouping (GDDSG) method, which leverages a graph coloring algorithm for class-based similarity grouping. The proposed approach trains independent CIL models for each group of classes, ultimately combining these models to facilitate joint prediction. Experimental results demonstrate that our method effectively addresses the issue of class order sensitivity while achieving optimal performance in both model accuracy and anti-forgetting capability. Our code is available at https://github.com/AIGNLAI/GDDSG.", "sections": [{"title": "1. Introduction", "content": "Class Incremental Learning (CIL) necessitates that the model dynamically acquires knowledge of new classes while preserving the knowledge of previously learned classes within an infinite sequence of tasks [12, 15, 41]. CIL is realistic but a great challenge for deep neural networks [32], where existing works devoted to overcoming catastrophic forgetting (CF) and encouraging knowledge transfer across different tasks [26, 46, 54, 57]. With the rapid advancement of CIL, a growing number of methods [19, 34, 55] have been introduced to address the problem of CF from the perspective of the order in which classes appear (or task order). In practice, the arrival order of each class and the tasks to which they belong are random and the order in which tasks arrive is uncontrollable [3], further resulting in Class order sensitivity and Intra-task class conflicts [23]. Therefore, designing an order-robust CIL method is essential for the community.\nClass order sensitivity refers to the model exhibiting significant performance variations depending on the sequence in which classes are introduced [34]. This phenomenon is prevalent in real-world applications (see Figure 1(a)). For instance, in online recommendation systems, the order in which user data classes are received at different time points is difficult to control. If the system initially receives data from relatively few classes, the introduction of subsequent classes may impair the system's adaptability, resulting in unstable model performance on new tasks. Furthermore, the model's parameters may be overfitted to the classes of early tasks, diminishing its ability to generalize to subsequent task with new classes. Although existing research, such as APD [55] and HALRP [19], have attempted to mitigate the class order sensitivity problem by modifying network structures, their effectiveness remains limited and has not fundamentally addressed this challenge. Thus, designing a model capable of maintaining stable performance across varying class orders remains a critical unsolved issue in CIL.\nIntra-task class conflicts refers to the discrepancies in model performance caused by similarity between classes that are trained simultaneously in a specific task (see Figure 1(b)). In real-world applications, where the arrival of classes in the data stream is uncontrollable, significant similarities among classes can severely impact the model's resilience. For example, in a specific task from a sequence of tasks, a model may be trained to recognize different breeds within the same species. Within this task, due to the high similarity of features across categories, the model needs to develop resilience in distinguishing between closely related classes. However, existing CIL methods struggle to address this challenge, primarily due to the inherent limitations of the task setting. As CIL incrementally processes different classes, it cannot globally account for all class information, causing class conflicts to accumulate during training and negatively impact model performance. Thus, alleviating class conflicts and improving the model's generalization ability remains a significant challenge in CIL.\nHence, to tackle the challenges of class order sensitivity and Intra-task class similarity sensitivity, we first conduct an in-depth analysis beyond existing theories. Our theoretical findings suggest that as class similarity decreases in CIL, the model's robustness to class order increases, which, in turn, mitigates knowledge conflicts both across different tasks and within individual tasks. Then, we propose a similarity graph-based dynamic grouping method, called Graph-Driven Dynamic Similarity Grouping (GDDSG), to maintain the centroids of existing classes and dynamically groups tasks based on class similarity, assigning classes with lower similarity to the same group. This approach innovatively organizes class groups in CIL by utilizing a graph-based technique to minimize inter-group similarity. It dynamically assigns classes based on adaptive similarity thresholds and optimal graph coloring, thereby enhancing model robustness and computational efficiency across tasks. In the incremental learning process, GDDSG continuously updates existing groups or creates new ones, training a separate model for each group. Consequently, during the prediction phase, decisions are made by aggregating the outputs of multiple models.\nHence, our contributions can be summarised as follows:\n\u2022 In this paper, we elaborate on existing theories and derive an important Corollary: when the similarity between classes is low, the model's sensitivity to class order is significantly reduced, leading to a decrease in class conflicts.\n\u2022 Then, we provide a detailed introduction to the proposed GDDSG method, including its foundational algorithms and basic processes.\n\u2022 Additionally, we conduct extensive comparative experiments to validate the effectiveness of GDDSG, highlighting its advantages and potential in incremental learning tasks."}, {"title": "2. Related Work", "content": "Class-Incremental Learning (CIL) necessitates a model that can continuously learn new classes while retaining knowledge of previously learned ones [5, 10, 59, 61], which can be roughly divided into several categories. Regularization-based methods incorporate explicit regularization terms into the loss function to balance the weights assigned to new and old tasks [2, 17, 21, 48]. Replay-based methods address the problem of catastrophic forgetting by replaying data from previous classes during the training of new ones. This can be achieved by either directly using complete data from old classes [6, 25, 33, 40] or by generating samples [35, 63], such as employing GANs to synthesize samples from previous classes [8, 24]. Dynamic network methods adapt to new classes by adjusting the network structure, such as adding neurons or layers, to maintain sensitivity to previously learned knowledge while acquiring new tasks. This approach allows the model's capacity to expand based on task requirements, improving its ability to manage knowledge accumulation in CIL [1, 30, 42, 44]. Recently, CIL methods based on pre-trained models (PTMs) [5, 7, 61] have demonstrated promising results. Prompt-based methods utilize prompt tuning [14] to facilitate lightweight updates to PTMs. By keeping the pre-trained weights frozen, these methods preserve the generalizability of PTMs, thereby mitigating the forgetting in CIL [20, 36, 36, 43, 49, 49, 50]. Model mixture-based methods mitigate forgetting by saving models during training and integrating them through model ensemble or model merge techniques [11, 45, 47, 58, 60, 62]. Prototype-based methods draw from the concept of representation learning [53], leveraging the robust representation capabilities of PTMs for classification with NCM classifiers [27, 31, 59].\nThe Order in CIL remains a significant and unresolved challenge [46]. APD [55] effectively addresses the problem of CF by decomposing model parameters into task-shared and sparse task-specific components, thereby enhancing the model's robustness to changes in class order. HALRP [19], on the other hand, simulates parameter conversion in continuous tasks by applying low-rank approximation to task-adaptive parameters at each layer of the neural network, thereby improving the model's order robustness. However, the optimization strategies employed by these methods are confined to the network architecture itself and do not fundamentally resolve the underlying issues. Recent theoretical analyses of CIL [3, 23, 34, 52] indicate that prioritizing the learning of tasks (or classes) with lower similarity enhances the model's generalization and resistance to forgetting. Building on these theories, we conducted further research and developed corresponding methods in the following sections."}, {"title": "3. Problem Formulation and Theory Analysis", "content": "Definition 1. (Class Incremental Learning (CIL)) Given a sequence of tasks denoted as 1, ..., t, ..., each task i is associated with a training set (i.e., ground-truth data) $D^i = \\{X^i, Y^i\\}$, where $X^i$ represents the set of training samples and $Y^i$ is the set of labels. For task i, the set of classes is denoted as $CLS^i$ with the size of $|CLS^i|$, representing the number of classes in task i. With new tasks incrementally appearing, the goal of CIL is to learn a unified model $\\Phi : D^i \\rightarrow \\mathbb{R}^d$ mapping input data to an embedding space equipped with a classifier $f(\\cdot)$ that can perform well on all the tasks it has been learned.\nNote that for any pair of tasks i and j with 1 \u2264 i, j \u2264 n and $i \\ne j$, the sets of classes $CLS^i$ and $CLS^j$ are disjoint and data from other tasks is unavailable at the current task, ensuring distinctiveness and non-overlapping nature between classes across each task.\nIn [23], the authors theoretically derived the expected forgetting value and expected generalization error for CIL under a linear model, where $w_t$ denotes the optimal parameters of the model for the t-th task:\nTheorem 1. When $p > n + 2$, we must have:\n$E[F_T] = \\sum_{i=1}^{T-1} \\frac{1}{p} ||w_i^*||^2 [r^{T-i} + \\sum_{j>i}^T C_{i,j} ||w_i^* - w_j^*||^2] + \\frac{\\rho\\sigma^2}{p-n-1} (r-r^2),$ (1)\n$E[G_T] = ||w_T^*||^2 + \\sum_{i=1}^{T-1} ||w_i^*||^2 + \\sum_{j>i} ||w_j^* - w_i^*||^2 + \\frac{\\rho\\sigma^2}{p-n-1}(1-r^T).$ (2)\nwhere the overparameterization ratio $r = \\frac{n}{p} < 1$ in this context quantifies the degree of overparameterization in a model, where n represents the sample size, and p denotes the number of model parameters [13, 29]. The coefficients $C_{i,j} = (1 - r)(r^{T-i} - r^{j-i} + r^{T-j})$, with $1 < i < j < T$, correspond to the indices of tasks, and $\\sigma$ denotes a coefficient representing the model's noise level.\nTheorem 1 made a significant contribution to the study of class order in CIL, particularly in the two key expressions: $\\sum_{j>i}^T C_{i,j} ||w_i^* - w_j^*||^2$ in Equation 1 and $\\sum_{j>i} ||w_i^* - w_j^*||^2$ in Equation 2. These formulas highlight the crucial role that class order plays in CIL. Building on this theory, further in this work, we derive sufficient conditions to ensure order robustness.\nCorollary 1. A sufficient condition for the reduction of $Var(E[G_T])$ and $Var(E[F_T])$ is that the sum of the squared distances between the optimal parameters of tasks increases, i.e., $\\sum_{i,j=1}^T ||w_i^* - w_j^*||^2$ becomes larger.\nCorollary 1 integrates the similarity between tasks with the model's robustness to class order. Through Equation 1 and Equation 2, we observe that both forgetting and generalization errors are influenced by the optimal model gap between any two tasks, represented by $||w_i^* - w_j^*||^2$ for tasks i and j. This gap serves as a measure of task similarity: the smaller the gap, the greater the similarity. Corollary 1 demonstrates that a smaller similarity between tasks enhances the model's robustness in terms of generalization and resistance to forgetting across different class orders. This finding offers valuable insights for the design of new methods. The proof of Corollary 1 can be found in the supplementary material."}, {"title": "4. The Proposed Method: GDDSG", "content": "Overview. Figure 2 provides an overview of our proposed method. Using task t as an example, we begin by projecting all training samples into an embedding space utilizing a pre-trained backbone. In this space, we compute the centroids for each class. Next, we evaluate whether a new centroid $c_i$ should be integrated into an existing class group $G_j$. If $c_i$ is dissimilar to all classes within $G_j$, it is added to the group. If it is similar to any class in an existing group, it remains unassigned. For unassigned centroids, we construct new similarity graphs (SimGraphs) based on their pairwise similarities. We then apply graph coloring theory to these SimGraphs, forming new class groups by clustering dissimilar categories together. Finally, we update the NCM-based classifier with all class groups, facilitating efficient model updates with minimal computational overhead.\nCorollary 1 provides guidance for constructing a sequence of dissimilar tasks. A key idea is to dynamically assign each new class to a group during CIL, ensuring that the similarity between the new class and other classes within the group is minimized. This approach helps maintain the robustness of each group's incremental learning process to the order of tasks. For each group, a separate adapter can be trained, and the results from different adapters can be merged during prediction to enhance the model's overall performance.\nIn a given CIL task sequence, we organize the classes into several groups. The group list is denoted as $\\mathbb{G} = [G_1, ..., G_k]$, where each $G_i$ represents a distinct group of classes. For a specified task t and each class $C \\in CLS^t$, our objective is to assign class $C$ to an optimal group $G^*$, ensuring that the new class is dissimilar to all existing classes in that group.\nTo achieve this objective, we first define the similarity between classes. The similarity between any two classes, $CLS_i$ and $CLS_j$, is determined using an adaptive similarity threshold $\\eta_{i,j}$. This threshold is computed based on the mean distance between the training samples of each class and their respective centroids in a learned embedding space, as shown below:\n$\\eta_{i,j} = \\max[\\frac{\\sum_{x_k^t \\in X^t} \\mathbb{I}(y_k = i) d(x_k, c_i)}{\\sum_{x_k^t \\in X^t} \\mathbb{I}(y_k = i)}, \\frac{\\sum_{x_k^t \\in X^t} \\mathbb{I}(y_k = j) d(x_k, c_j)}{\\sum_{x_k^t \\in X^t} \\mathbb{I}(y_k = j)}],$ (3)\nwhere $d(\\cdot,\\cdot)$ denotes a distance metric function, $\\mathbb{I}(\\cdot)$ is an indicator function, and $c_i = \\frac{1}{\\vert X \\vert} \\sum_{j=1}^{\\vert X \\vert} x_j$ represents the centroid of class $C_i$.\nBuilding upon this framework, we define the condition under which two classes, $CLS_i$ and $CLS_j$, are considered dissimilar. Specifically, they are deemed dissimilar if the following condition holds:\n$d(C_i, C_j) > \\eta_{i,j}.$ (4)\nThus, class $C$ is assigned to group $G^*$ only if it is dissimilar to all classes within $G^*$, and $G^*$ is the choice with the lowest average similarity:\n$G^* = \\arg \\min_G \\frac{1}{\\vert G \\vert} \\sum_{C' \\in G} d(C, C').$ (5)\nThis approach is consistent with the principles outlined in Corollary 1 and ensures the robustness of the model across the entire task sequence.\nGraph algorithms provide an efficient method for dynamically grouping classes while minimizing intra-group similarity. In a graph-theoretic framework, classes are represented as nodes, with edge weights quantifying the similarity between them. The flexibility and analytical power of graph structures allow for dynamic adjustment of class assignments in CIL, facilitating optimal grouping in polynomial time. This approach significantly enhances the model's robustness and adaptability in incremental learning tasks. Therefore, we can leverage the similarity between classes to construct a SimGraph, defined as follows:\nDefinition 2. (SimGraph.) A SimGraph can be defined as an undirect graph $SimG = (V, E)$, where V is the set of nodes that represent each class's centroid and E is the set of edges connecting pair of nodes that represent classes that are determined as similar by Equation 4.\nThen, we aim to partition the vertex set of this graph into subsets, with each subset forming a maximal subgraph with no edges between vertices. This problem can be abstracted as the classic NP-hard combinatorial optimization problem of finding a minimum coloring of the graphs. Let $G^{-1}(\\cdot)$ be an assignment of class group identities to each vertex of a graph such that no edge connects two identically labeled vertices (i.e. $G^{-1}(i) \\ne G^{-1}(j)$ for all $(i, j) \\in E$). We can formulate the minimum coloring for graph $SimG$ as follows:\n$\\chi(SimG) = \\min |\\{G^{-1}(k)|k \\in V\\}|,$ (6)\nwhere $\\chi(SimG)$ is called the chromatic number of $SimG$ and $\\vert\\cdot\\vert$ denotes the size of the set.\nBrooks' theorem [4] offers an upper bound for the graph coloring problem. To apply this in our context, we must demonstrate that the similarity graphs constructed in CIL meet the conditions required by Brooks' theorem. By doing so, we can establish that the problem is solvable and that the solution converges, ensuring the effectiveness of our grouping and class coloring process in class incremental learning. Without loss of generality, we can make the following assumptions:\nAssumption 1. In the CIL task, class $C_i$ is randomly sampled without replacement from the set $\\mathbb{U} = \\bigcup_{i=1}^{i-1} C_i$, ensuring that $C_i \\ne C_j$ for all $i \\ne j$. The probability that any two classes $C_i$ and $C_j$ within the set $\\mathbb{U}$ meet the similarity condition (as described in Equation 4) is denoted by p.\nIn the CIL scenario with N classes, the probability of forming an odd cycle is given by $(p^2 (1 - p)^{(N-2)}) = p^{2N} (1 - p)^{N^2-2N}$. Similarly, the probability of forming a complete graph is $p^{\\binom{N}{2}} = p^{\\frac{N(N-1)}{2}}$. Thus, the probability that the CIL scenario satisfies Brooks' theorem can be expressed as:\n$P_{Satisfy \\ Brooks} = 1 - p^{2N} (1 - p)^{N^2-2N} - p^{\\frac{N(N-1)}{2}}.$ (7)\nIn the previous section, we introduced the motivation and core concepts behind the proposed algorithm. In this section, we will describe the entire training process in detail. Recent years have seen CIL methods based on pre-trained models achieve remarkable results [27, 31, 59, 59], largely due to their robust representation capabilities. Since our proposed class grouping method also relies heavily on the model's representation ability, we utilize a widely-adopted pre-trained model as a feature extractor. For each class group, we train independent classification heads, which enhances the model's adaptability and generalization to different class groups.\nAs outlined above, we utilize a frozen random projection matrix $W \\in \\mathbb{R}^{L \\times M}$ to enhance features across all class groups, where L is the output dimension of the pre-trained model and $M \\gg L$ is the expanded dimensionality. Given a task t and a sample x belonging to a class group s, the feature vector of the sample is denoted as h(x), and its one-hot encoded label as y(x). Specifically,\n$h(x) = g(\\phi(x)^T W),$ (8)\nwhere $\\phi(\\cdot)$ represents the feature extractor, and $g(\\cdot)$ is a nonlinear activation function. We define $H \\in \\mathbb{R}^{N \\times M}$ as the matrix containing feature vectors of N samples from group s. The corresponding Gram matrix is defined as:\n$Gram_s = H^T H \\in \\mathbb{R}^{M \\times M}.$ (9)\nAdditionally, the matrix $C_s^t$ consists of the concatenated column vectors of all classes within group s, with dimensions M \u00d7 $L^2$, where L represents the number of classes in group s for task t. When a new task arrives, the model applies the GDDSG algorithm to assign new classes to their respective groups. The Gram matrix $Gram_s$ and matrix $C_s$ for each group are updated according to the following formulas:\n$Gram_s^t = Gram_s^{t-1} + \\sum_{n=1}^{N_s^t} h(x_n^t) h(x_n^t)^T,$ (10)\n$C_s^t = \\begin{bmatrix} C_s^{t-1} & 0_M & 0_M & ... & 0_M \\\\ & & & (L - L^{t-1}) \\textit{times} & \\\\ \\frac{1}{\\sqrt{\\lambda \\mathbb{I}}} \\sum_{n=1}^{N_s^t} h(x_n^t) y(x_n^t) \\end{bmatrix},$ (11)\nwhere $0_M$ denotes a zero vector with M dimensions.\nDuring the test phase, we combine the classification heads of all groups $\\mathbb{G} = [G_1, G_2, ..., G_k]$ to make a joint prediction for a given sample x. For each class $c'$ in a group, the score is computed as follows:\n$s_{c'} = g(\\phi(x)^T W) (Gram_i + \\lambda \\mathbb{I})^{-1} C_i^t,$ (12)\nwhere $i = 1,..., k$ denotes the indices of each groups, and $\\lambda$ is the regularization parameter used to ensure that the Gram matrix remains invertible. The final classification result is then obtained by applying the following formula:\n$\\hat{c} = \\arg \\max_{c' \\in \\bigcup_{i=1}^K CLS^{G_i}} s_{c'}.$ (13)"}, {"title": "5. Experiment", "content": "Since most pre-trained models are currently trained on ImageNet-21K [9], we aim to assess the model's performance on entirely new data. To demonstrate the robustness of our model to task similarity, we conduct experiments using several datasets, including CIFAR100 [18], CUB200 [39], Stanford Dogs [16], and OmniBenchmark (OB) [56]. These datasets are divided into multiple, equally sized tasks, and various class orders are tested to evaluate the model's performance across different orders.\nBaseline. For fairness, we only compare against CL methods that have utilized pre-trained models in recent years. We compare GDDSG with the following six latest and effective CL methods with the PILOT toolbox [37]: L2P [50], Dualprompt [49], CODA-Prompt [36], SimpleCIL [59], ADAM [60], EASE [62], RanPAC [27].\nImplementations. Our code, implemented in PyTorch, has been open-sourced for accessibility. All experiments were conducted on a single Nvidia RTX 3090 GPU, using two random seeds, 2024 and 4202, to compute the average for a more robust model evaluation. We use a ViT-B/16 model, which is self-supervised and pre-trained on ImageNet-21K. Detailed dataset descriptions and experimental implementations are provided in the Supplementary Material.\nMetrics. We employ average final accuracy $A_N$ and average forgetting rate $F_N$ as metrics [50]. $A_N$ is the average final accuracy concerning all past classes over N tasks. $F_N$ measures the performance drop across N tasks, offering valuable information about plasticity and stability during CL. Following the protocol in [22], we use the Order-normalized Performance Disparity (OPD) metric to assess the robustness of the class order. OPD is calculated as the performance difference of task t across R random class orders, defined as:\n$OPD_t = \\max\\{\\overline{A}_t^1, ..., \\overline{A}_t^R\\} - \\min\\{\\overline{A}_t^1, ..., \\overline{A}_t^R\\}.$ (14)\nThe Maximum OPD (MOPD) and Average OPD (AOPD) are further defined as:\n$MOPD = \\max\\{OPD_1, ..., OPD_T\\},$ (15)\n$AOPD = \\frac{1}{T} \\sum_{t=1}^T OPD_t.$ (16)\nMain Results. Table 1 highlights the strong performance of our proposed GDDSG method in terms of accuracy and resistance to forgetting. The results demonstrate that GDDSG consistently outperforms other techniques, achieving state-of-the-art (SOTA) performance. Notably, GDDSG shows marked improvements in both accuracy and forgetting rate. Compared to the previous SOTA method, RanPAC, our approach achieves significantly higher accuracy while maintaining a low forgetting rate of around 1%, underscoring GDDSG's superior effectiveness in the CIL environment.\nAblation analysis. Our method's two components, SimGraphs and Class Groups, operate as a unified whole. Only after generating the SimGraphs can construct the Class Groups. Therefore, we can only conduct ablation experiments on either individual Class Groups or the SimGraphs and Class Groups combination as a whole, with results shown in Table 2. The results demonstrate a significant decrease in model performance after conducting the ablation, validating the effectiveness of SimGraphs and Class Groups.\nRobustness to Class Order. We conducted comparative experiments on existing order-robust CIL methods, including APD [22], APDfix [22], and HALRP [19], using 10 different class orders across four datasets and calculating their MOPD and AOPD metrics. The experimental results, presented in Figure 4, show that our proposed GDDSG method demonstrates excellent robustness to category order. MOPD decreased across all datasets, with AOPD showing a significant reduction, underscoring the practical effectiveness of our approach.\nAnalysis of Class Group Counts. Figure 5 illustrates how the number of class groups changes as tasks arrive and task lengths vary during the execution of the GDDSG algorithm. We observe that for relatively homogeneous datasets, such as CIFAR-100 and CUB-200, the optimal number of class groups generated remains relatively low and tends to stabilize midway through the task sequence. In contrast, datasets with broader domains and more categories, such as OB, result in a higher number of optimal class groups. Despite this increase, GDDSG maintains accurate matching, demonstrating its strong generalization capability.\nAdditionally, we simulate varying frequencies of intra-task class conflicts by altering the number of categories within a single task, which leads to differences in both intra-task and inter-task similarities. The results indicate that the optimal number of class groups determined by the GDDSG algorithm consistently converges to a stable value. This demonstrates that, for a specific dataset and pre-trained model, the optimal number of class groups is determined solely by the dataset itself, independent of factors such as task length and order in the CIL environment. This stability arises because the GDDSG algorithm primarily relies on data similarity, while disregarding task-specific information in real-world scenarios. The robustness of this approach is theoretically supported by Brooks' theorem [4] and the Welsh-Powell algorithm [51]. In conclusion, the GDDSG algorithm exhibits strong robustness to variations in task similarity, length, and order, making it highly valuable for a wide range of applications.\nDetailed Analysis of Backbone. Table 3 presents the optimal number of class groups that GDDSG generates under various pre-trained backbone networks, along with the corresponding $A_N$ and $F_N$ values. The results indicate that smaller backbones, such as ResNet18 and ResNet50, yield reduced accuracy and higher forgetting rates compared to using ViT as the backbone. Nonetheless, performance with ResNet50 remains highly competitive, achieving accuracy comparable to L2P while maintaining a relatively low forgetting rate. This further highlights the robustness of GDDSG, as it can reach performance levels similar to those of richer, more powerful backbones, even when using networks with fewer parameters and lower representational capacity.\nAnother noteworthy observation is that, for the same dataset, the number of class groups varies depending on the backbone network. Specifically, when using ResNet18 or ResNet50, the number of class groups for CIFAR-100 and CUB-200 increases significantly, whereas it decreases for Stanford Dogs and OB. This suggests that, for certain datasets, backbones with lower representational power may erroneously classify some originally dissimilar classes as similar, leading to an increase in class groups. In contrast, other datasets remain unaffected by the choice of backbone. This highlights that the impact of the backbone on the GDDSG algorithm is highly dataset-dependent.\nDetailed Analysis of Class Group Matching. To better understand the mechanism of class group matching, we visualized all the distance features using t-SNE [38] and UMAP [28] on the Split CIFAR100 dataset. The observations reveal that the distance features essentially conform to piecewise functions in high dimensions, exhibiting strong linear separability and powerful representation capabilities. Consequently, a class group identification matching model can be effectively fitted using some classical machine learning models, enabling fairly accurate predictions. However, it is crucial to emphasize the importance of this step for GDDSG. Under the partition of the GDDSG algorithm, the accuracy of a single class group can reach nearly 100%. Therefore, the precision of class group matching directly determines the overall model's accuracy."}, {"title": "6. Conclusion & Limitation", "content": "In this study, we aim to design an order-robust CIL model capable of addressing two critical challenges: class order sensitivity and intra-task conflicts. Building on existing theories, we find that as class similarity decreases, the model's sensitivity to class order also lessens, which effectively mitigates knowledge conflicts both across tasks and within individual tasks. To enhance the model's robustness across varying class orders, we propose a dynamic grouping method based on similarity graphs, termed GDDSG. The proposed approach maintains the centroids of learned classes and group classes based on dynamic similarity. In GDDSG, we introduce a novel approach to structuring class groups within class-incremental learning. Our GDDSG can continually update existing groups or form new ones, training distinct models for each group. During inference, predictions are derived through an ensemble of outputs from multiple models, thereby enhancing overall accuracy and robustness in CIL.\nInevitably, our method has certain limitations. First, GDDSG currently relies on NCM classifiers. In future work, we aim to explore order-robust CIL approaches with Softmax strategies. Also, while the memory overhead remains small, it could be further streamlined for efficiency, and we intend to address this limitation with future studies."}, {"title": "Appendix", "content": "In Table A.1, we introduce the notations throughout this paper."}]}