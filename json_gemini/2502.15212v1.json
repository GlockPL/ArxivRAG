{"title": "MEASURING AI AGENT AUTONOMY: TOWARDS A\nSCALABLE APPROACH WITH CODE INSPECTION", "authors": ["Peter Cihon", "Merlin Stein", "Gagan Bansal", "Sam Manning", "Kevin Xu"], "abstract": "AI agents are AI systems that can achieve complex goals autonomously. Assess-\ning the level of agent autonomy is crucial for understanding both their potential\nbenefits and risks. Current assessments of autonomy often focus on specific risks\nand rely on run-time evaluations \u2013 observations of agent actions during operation.\nWe introduce a code-based assessment of autonomy that eliminates the need to\nrun an Al agent to perform specific tasks, thereby reducing the costs and risks\nassociated with run-time evaluations. Using this code-based framework, the or-\nchestration code used to run an AI agent can be scored according to a taxonomy\nthat assesses attributes of autonomy: impact and oversight. We demonstrate this\napproach with the AutoGen framework and select applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Language model research and product attention focuses on creating Artificial Intelligence systems\ncapable of flexibly planning and acting to influence environments over time ('AI agents') (Wang\net al., 2024; Kapoor et al., 2024). In many cases, these systems orchestrate language models and\ntheir outputs to perform complex chains of thought and action for tasks ranging from software de-\nvelopment to vacation planning. The responsible development and deployment of AI agents present\nmany open questions today (Shavit et al., 2023; Chan et al., 2024; Gabriel et al., 2024; Wu et al.,\n2023).\nAI agents are designed to function autonomously. Autonomous systems can present risks of harm\nthat have drawn policy scrutiny globally (Cihon, 2024; Chan et al., 2023). Early evaluations of AI"}, {"title": "2 APPROACHES FOR ASSESSING AUTONOMY OF SYSTEMS", "content": "Autonomy has been defined and operationalized differently across contexts. Autonomy can include\nconceptions of self-actualization (constitutive) and capacity for interactions (behavioral) (Froese\net al., 2007). We focus on a form of the latter: decision autonomy, which refers to the ability\nof an AI system to implement decisions without human oversight (Walsh et al., 2021) \u00b9 It is this\nconcept that centers the impact of actions, environments and human oversight - which are central\nfor understanding and addressing risks from deployments.\nRegulatory initiatives worldwide have started to define autonomy with a focus on decision autonomy.\nThe EU AI Act considers AI systems with varying degrees of autonomy, meaning that \u201cthey have\nsome degree of independence of actions from human involvement and of capabilities to operate\nwithout human intervention\" (EU, 2024). Similarly, NIST (2005) guidance defines a system as fully\nautonomous if it is \"expected to accomplish its mission, within a defined scope, without human\nintervention\" and autonomy as levels characterized \"by factors including mission complexity and\nenvironmental difficulty.\" This guidance may be applicable across domains including manufacturing\nand national security (Huang, 2007).\nIn other industries, autonomy of systems is assessed along levels. For instance, autonomous driv-\ning levels are distinguished by the boundedness of the environment (operational design domain),\nthe responsibility for advanced actions (dynamic driving tasks) and the degree of human oversight\n(fallback) (SAE, 2021). Similarly, degrees of autonomy of medical robots or aviation depend on the\nenvironment, difficulty of tasks and human oversight (Yang et al., 2017; Anderson et al., 2018).\nFor AI systems, researchers have operationalized levels of autonomy with empirical evaluations on\ntasks requiring increasing autonomy (benchmarked to humans taking minutes, hours, days or months\nto complete it) (Kinniment et al., 2023; Morris et al., 2023), according to levels of human oversight\n(in-, on-, and off-the-loop) (Simmler & Frischknecht, 2021), autonomy of AI agents to determine\nsuitable outputs and actions to take in the environment (LangChain, 2023; Li et al., 2024). Proto-\ncols have also been developed to evaluate an AI system's capability to pose autonomy-related risks\n(METR, 2024). Appendix A.1 provides an overview of levels of autonomy of different approaches.\nIn summary, across regulatory definitions, standards in other industries, and initial AI-specific char-\nacterisations, degrees of autonomy are differentiated based on (1) possible impact as a result of\npossible actions and environments, and (2) oversight in relation to orchestrating interactions and\nfallbacks within the system or between the system and the human. In the next section, we will\noperationalize an assessment of these attributes with AutoGen, an AI agent framework."}, {"title": "3 ASSESSING AUTONOMY OF AUTOGEN", "content": "AutoGen is a popular\u00b2 open source software framework for building AI agent systems using lan-\nguage models (Wu et al., 2023). AutoGen supports multi-agent conversations and tool-use to achieve\narbitrary goals with varying levels of autonomy. It includes features that involve human users in the"}, {"title": "4 ASSESSING AUTONOMY OF AUTOGEN APPLICATIONS", "content": "We operationalize the autonomy taxonomy by inspecting the source code for AutoGen applications.\nAppendix A.2 describes code flags used for scoring. Ten AutoGen applications were scored by\nthree researchers. Consensus results are shown in Table 3 and further detail in Appendix A.2.\nThe analysis shows that a differentiated categorization and scoring of agent autonomy with code\ninspections is tractable. The three raters demonstrated substantial inter-rater agreement (Fleiss'\nk = 0.64) across all evaluation categories, indicating consistent application of the rating criteria;\nadditional agreement measures reported in Appendix A.2.\nThe variations between AutoGen applications are notable. Flexible platform applications like Auto-\nGen Studio or Composio allow users to build specific applications with multiple degrees of auton-\nomy. For example, this includes code executions, inside or outside pre-specified environments like\nDocker containers. Applications range from pre-configured machine learning agent with multiple\nsub-agents who build and critique code, to financial research agent and editing agent teams.\nSpecific applications like Dream Team for development with quality assurance or GraphRAG for\ndata-based reasoning are more constrained for most autonomy attributes. Potentially for their relia-\nbility for a specific purpose, they operate in bounded Docker environments. However, both applica-\ntions still spin up one subagent that requires a human-in-the-loop, as a fallback or main input, and\nthe remaining \u2013 executing \u2013 subagents human-off-the-loop.\nThis pilot has limitations, partly due to our small sample and partly due to the nature of code in-\nspections. Inter-rater agreement varied by attribute: orchestration (k = 0.67), human-in-the-loop\n(k = 0.65), environment (k = 0.60), observability (k = 0.47), and action (k = 0.30), with the latter\nthree being below the substantial agreement threshold. Actions may be enabled not only by the\nAutoGen framework but by downstream developer choices to make custom tools or use additional\nframeworks, complicating review. Observability proved challenging to consistently differentiate be-\ntween maintaining logs that may be useful to developers and meaningful awareness for users that\nmay limit autonomy in practice. Consistent assessment of the constrained environment also proved\nchallenging, where in some cases wider access may be possible through API calls to other systems.\nInter-rater agreements aide, additional limitations apply to the human-in-the-loop attribute. Is an\nagent that requires human approval only after trying multiple different information retrieval tools\nmore or less autonomous than an agent that requires human approval after each mouse click? It\nmight depend on more granular assessments of the kind of actions before which human approval is\nrequired.\nNo agent system received lower scores on actions and observability. The first may be explained by\nthe common interest in developing agent systems to take actions, i.e., to do more than simple chat\ninteractions that are also possible with AutoGen. The open-source nature and focus on developers of"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "This paper piloted a code-inspection approach to assessing agent systems for their level of autonomy.\nWith further development as identified below, this approach holds promise for agent assessments at\nscale. Code inspection rates AI agent applications without running them, which is resource-efficient,\nreduces risks associated with run-time evaluations, and can enable uniform, scaled assessments that\ncan inform stakeholder decisions \u2013 from model developers to policymakers. It also supports risk\nassessment to inform what level of system security might be important before running them. This\nmethod could be used to monitor the development of autonomous agent systems in the open source\necosystem and provide a framework to assess proprietary applications given code access.\nAlthough final results will require additional scale, this preliminary exercise identifies the relevance\nof defaults in shaping responsible behavior in the AI development value chain. The AutoGen frame-\nwork constructed default set-ups for human oversight that were readily used by downstream devel-\nopers. Most default set-ups were used consistently. There are exceptions - the Composio repository\nconsistently overrode defaults to use agent code execution outside a protected docker environment.\nScaling assessment can inform governance proposals to set responsible and effective defaults.\nFuture work can refine the code-inspection approach and subsequently scale it for all open source\nAutoGen applications using AI-assisted grading methods (Eloundou et al., 2024). Autonomy levels\ncan be developed further to reflect internal dependencies among the impact and oversight attributes\nand to better reflect literature from other fields that uses holistic and more numerous levels of au-\ntonomy. Future work could rate autonomy levels on additional categories, such as goal setting,\ninteractions between agent systems and fallback specifications (see Figure 1), though multiple mea-\nsures complementing code inspection may be needed for such attributes arising at the model layer\nor at inference time. Code inspection results for select systems can be compared to inference-time\nassessments using evaluation harnesses (UK AI Safety Institute, 2024). Subsequent work can differ-\nentiate between agent applications and platforms, and generalize this approach to assess additional\nframeworks."}]}