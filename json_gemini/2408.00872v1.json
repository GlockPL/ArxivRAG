{"title": "Online Detection of Anomalies in Temporal Knowledge Graphs with Interpretability", "authors": ["Jiasheng Zhang", "Jie Shao", "Rex Ying"], "abstract": "Temporal knowledge graphs (TKGs) are valuable resources for cap-\nturing evolving relationships among entities, yet they are often\nplagued by noise, necessitating robust anomaly detection mech-\nanisms. Existing dynamic graph anomaly detection approaches\nstruggle to capture the rich semantics introduced by node and edge\ncategories within TKGs, while TKG embedding methods lack in-\nterpretability, undermining the credibility of anomaly detection.\nMoreover, these methods falter in adapting to pattern changes and\nsemantic drifts resulting from knowledge updates. To tackle these\nchallenges, we introduce ANOT, an efficient TKG summarization\nmethod tailored for interpretable online anomaly detection in TKGs.\nANOT begins by summarizing a TKG into a novel rule graph, en-\nabling flexible inference of complex patterns in TKGs. When new\nknowledge emerges, ANOT maps it onto a node in the rule graph\nand traverses the rule graph recursively to derive the anomaly score\nof the knowledge. The traversal yields reachable nodes that furnish\ninterpretable evidence for the validity or the anomalous of the new\nknowledge. Overall, ANOT embodies a detector-updater-monitor\narchitecture, encompassing a detector for offline TKG summariza-\ntion and online scoring, an updater for real-time rule graph updates\nbased on emerging knowledge, and a monitor for estimating the\napproximation error of the rule graph. Experimental results on\nfour real-world datasets demonstrate that ANOT surpasses existing\nmethods significantly in terms of accuracy and interoperability. All\nof the raw datasets and the implementation of ANOT are provided\nin https://github.com/zjs123/ANOT.", "sections": [{"title": "1 INTRODUCTION", "content": "Many human activities, such as political interactions [14] and e-\ncommerce [30], can be effectively represented as time-evolving\ngraphs with semantics, which are referred to as temporal knowledge\ngraphs (TKGs). These TKGs are dynamic directed graphs character-\nized by node and edge categories. In this context, nodes represent\nentities in the real world (e.g., United States), while labeled edges\nsignify the relations between these entities (e.g., Born In). Each\nedge in conjunction with its connected nodes can constitute a tuple\n(s, r, o, t) that encapsulates a piece of real-world knowledge, where\ns and o denote the subject and object entities, r denotes the relation,\nand t represents the occurrence timestamp of the knowledge.\nWhile TKGs have demonstrated their value across various appli-\ncations [22, 63], they often contain numerous anomalies that can\nsignificantly impede their reliability. As illustrated in Figure 1, TKG\nconstruction relies on automatic extraction from unstructured text\n[64]. However, existing extraction techniques often encounter con-\nceptual confusion [36] and inaccurate relation matching [46], poten-\ntially introducing noisy tuples with erroneous entities or relations,\ntermed as conceptual errors. Furthermore, ongoing interactions\nin the real world lead to the formation of new knowledge or render\nexisting knowledge obsolete. However, knowledge-updating pro-\ncesses are often insufficient and delayed [48], resulting in either the\nomission of new knowledge or the retention of invalid knowledge,\ntermed missing errors and time errors. Specifically, these errors\nare indicated by their conflicts with preserved knowledge. Concep-\ntual errors conflict with the interaction preference of entities, while\ntime errors conflict with other timely updated knowledge in their\ntime order. The missing errors are valid knowledge not included in\nTKGs and should have few conflicts. TKG anomaly detection refers\nto detecting conceptual, time, and missing errors by measuring\nconflicts. Unfortunately, this field receives little attention.\nOne closely related research field is dynamic graph anomaly\ndetection [44], aimed at identifying abnormal connections in time-\nevolving graphs. However, existing methods primarily rely on sim-\nple structural properties, such as connectivity [1] or clusters [34],\nthus failing to capture the intricate patterns present in TKGs such as\nrelational closures [61] and temporal paths [14]. Furthermore, these\nmethods do not consider the semantic attributes of nodes and are\nthus ineffective in handling the rich semantics introduced by node\nand edge categories. Another related field is TKG embedding [50],\nwhich aims to represent entities and relations as low-dimensional"}, {"title": "2 RELATED WORK", "content": "Dynamic graph anomaly detection. Existing methods fall\ninto two categories. One is statistical methods, which leverage the\nshallow mechanisms to extract the structural information [12, 13,\n34, 39]. For example, CAD [47] detects abnormal edges by tracking\nchanges in structure and weight. DynAnom [21] uses the dynamic\nforward push algorithm to calculate the personalized PageRank\nvector for each node. AnoGraph [5] extends the count-min sketch\ndata structure to detect anomalous edges through dense subgraph\nsearches. F-FADE [8] models the time-evolving distributions of node\ninteractions using frequency factorization. However, they cannot\ncapture complex patterns brought by entity and relation semantics\nin TKGs. The other is deep learning-based methods, which detect\nanomalies by learning vector representations for nodes [2, 15, 32,\n62]. AEGIS [11] proposes a graph differentiation network to learn\nnode representations. Netwalk [59] combines random walk and a\ndynamic clustering-based model to score anomalies. AER [16] uses\nan anonymous representation strategy to identify edges by their\nlocal structures. TADDY [32] uses a dynamic graph transformer\nmodel to aggregate spatial and temporal information. However, they\nlack enough high-quality labels [18] and the learned representations\nare not interpretable, making their detection less convincing.\nTKG embedding. Factorization-based methods [50, 57, 60] re-\ngard TKGs as 4-order tensors and use tensor factorization for em-\nbeddings. TNT [28] builds on the complex vector model ComplEx\n[49] with temporal regularization. Timeplex [25] extends TNT by\ncapturing the recurrent nature of relations. TELM [54] learns multi-\nvector representations with canonical decomposition. However,\nthey learn tensors with fixed shapes, limiting their ability to handle\nnew entities and timestamps. Diachronic embedding-based meth-\nods [10, 56] model entity representations as time-related functions.\nDE-simple [20] uses nonlinear operations to model various evolu-\ntion trends of entity semantics. ATISE [55] uses multi-dimensional\nGaussian distributions to model the uncertainty of entity semantics.\nTA-DistMult [19] uses a sequence model for time-specific relation\nrepresentations. However, they over-simplify the evolution of TKG\nand ignore the graph structure. GNN-based methods [31, 37, 40]\nemploy the message-passing mechanism to simulate the entity in-\nteractions. TeMP [52] uses self-attention to model the spatial and\ntemporal locality. RE-GCN [29] auto-regressively models historical\nsequence and imposes attribute constraints on entity representa-\ntions. However, they lack interpretability and cannot handle online\nchanges."}, {"title": "3 PRELIMINARIES", "content": "3.1 Temporal Knowledge Graph\nA temporal knowledge graph is denoted as G = (E,R,T,F). E\nand R are entity set and relation set, respectively. T is the set\nof observed timestamps and F is the set of facts. In real-world\nscenarios, &, T, and F will be continuously enriched. Each tuple\n(s, r, o, t) \u2208 F connects the subject and object entities s, o \u2208 & via a\nrelation r\u2208 R in timestamp t \u2208 T, which means a unit knowledge\n(i.e., a fact). We represent the connectivity of G in each timestamp t\nwith a |8| x |8|\u00d7 |R| adjacency tensor At, where 1 represents that\nthe entities are connected by the relation in timestamp t. There are\ntwo common occurring relationships exist in TKGs. One is chain\noccurring defined as {(s, ri, o, t\u2081) \u2192 (s, rj, 0, tj)|tj \u2265 ti}, e.g.,\n(Obama, WintheSelection, UnitedStates, 2008/11/04) \u2192 (Obama,\nPresidentof, UnitedStates, 2009/01/20). The other is triadic oc-\ncurring defined as {((s, ri, o, t\u012f), (s, rj, p, tj)) \u2192 (o, rk, p, tk)|tk \u2265\nmax(ti, tj)}, e.g., ((China, HostVisit, SaudiArabia, 2023/03/06),\n(China, HostVisit, Iran, 2023/03/06) \u2192 (Saudi Arabia, SignAgree\nment, Iran, 2023/03/10). The facts on the left of the arrow are called\nhead facts, and those on the right are called tail facts.\n3.2 Anomalies in TKGS\nHere, we formally define three kinds of typical anomalies in TKGs.\nGiven a TKG G, we first define its corresponding ideal TKG as\n\u011c = (E,R,T,F) which removes all the incorrect facts from F\nand complete all the missing facts into F (i.e., (s, r, o, t) \u2208 F if\nand only if it holds in reality). We further define the ideal triple\nset \u00a3 = {(s,r,o)|(s, r,o, t) \u2208 F}. Note that, G and \u00ce are only\nconceptual aids that do not exist. We then use it to define anomalies.\n3.2.1 Conceptual Errors. Extraction methods may introduce noised\nfacts with error entities or relations in TKGs. Formally, we de-\nfine the conceptual errors as Fc = {(sc, rc, oc, tc)|(sc, rc, oc, tc) \u2208\nF, (sc, rc, 0c) \u2209 L}, e.g., (JoeBiden, BornIn, Ireland, 1942/11/20).\n3.2.2 Time Errors. Knowledge updating may make existing facts\ninvalid, but update delays will let these invalid facts not be re-\nmoved from TKGs. Formally, we define the time errors as Ft =\n{(st, rt, Ot, tt)|(St, rt, 0t, tt) \u2208 F, (st, rt, ot) \u2208 L, (st, rt, Ot, tt) \u2209 F}.\nFor example, (Obama, Presidentof, UnitedStates, 2023/10/21).\n3.2.3 Missing Errors. Insufficient updates also prevent some cor-\nrect facts not being added to TKGs. Formally, we define the missing"}, {"title": "XXX and XXX, et al.", "content": "errors as Fm = {($m, rm, 0m, tm) | (Sm, rm, Om, tm) & F, (sm, rm, Om,\ntm) \u2208 F}. For instance, a TKG might include the knowledge Barack\nObama left office but lack his inauguration. Unlike TKG completion\nthat predicts missing entities or relations for given tuples, we aim\nto find which tuple is missing in TKG. Note that these anomalies\nwill persist as TKGs keep growing in real-world situations.\n3.3 Minimum Description Length Principle\nIn the two-part minimum description length (MDL) principle [41],\ngiven a set of models M, the best model M\u2208 Mon data D min-\nimizes L(M) + L(D|M), where L(M) is the length (in bits) of the\ndescription of M, and L(D|M) is the length of the description of the\ndata when encoded using M. In this work, we leverage MDL to find\nthe optimal summarization model of a given TKG. Each MDL-based\napproach must devise its definitions for the description lengths,\nand here we follow the most commonly used primitives [17].\n3.4\nSummarization of A TKG\nThe summarization of a graph is a more refined and compact repre-\nsentation of the graph [6], including super-graphs [9], sparsified\ngraphs [33], and independent rules [45]. However, they fail to han-\ndle rich semantics and temporal relevance in TKGs, inspiring us to\npropose a novel rule graph as the summarization of a TKG.\n3.4.1 Atomic Rules. Given a TKG G, we first construct a function\nC(), which takes each entity as input and outputs its category.\nBased on this, each knowledge (s, r, o, t) \u2208 G can be mapped as an\natomic rule (C(s), r, C(o)), which summarizes the interaction pat-\ntern of the knowledge (e.g., (Obama, Win, NobelPeacePrize, 2019/\n10/09) can be mapped as (PERSON, Win, PRIZE)).\n3.4.2 Rule Graph. A rule graph is a directed graph G = {V, E},\nwhere each v \u2208 V is a node indicating an atomic rule, and each\ne \u2208 E is a rule edge preserving the sequential relevance between\natomic rules. There are two kinds of rule edges in E. One is derived\nfrom the chain occurring (e.g., (PERSON, Nominated, PRIZE) \u2192\n(PERSON, Win, PRIZE)), termed as (uh ut) where uh is the\nhead atomic rule and vt is the tail atomic rule. The other is derived\nfrom the triadic occurring (e.g., (PERSON, Write, BOOK), (BOOK,\nNominated, PRIZE) \u2192 (PERSON, Win, PRIZE)), termed as ((vh\nUm) \u2192 vt), where um is the middle atomic rule. By associating\natomic rules with rule edges, paths between atomic rules can de-\nscribe the occurrence relevance between two kinds of interactions.\n3.5 Problem Definition\nDetecting anomalies for TKGs that have been offline preserved in\nthe database is meaningful. However, it is a more valuable but diffi-\ncult problem to detect anomalies for TKGs that are online updating,\nrequiring the model to be efficient, adaptive to online changes, and\neasy to rebuild. We term it inductive anomaly detection in TKGs.\nDefinition 3.1 (Inductive anomaly detection in TKGs). Given an on-\nline updating TKG G where the most recently updated timestamp is\nte, inductive anomaly detection aims to construct a model M based\non G to find anomalies in the future timestamps t > te. It contains\nclassifying whether each newly arrived knowledge (s, r, o, t) is an\nanomaly (i.e., conceptual or time errors), and determining whether\nsome knowledge is missing in t (i.e., missing error)."}, {"title": "4 METHOD", "content": "4.1\nOverview\nMotivation. Reflecting on the challenges in TKG anomaly de-\ntection, we recognize that a rule-based summarization approach\ncould effectively tackle these issues. First, rules encapsulate the\nmost common patterns within a graph in a human-readable form.\nIf we can map new knowledge as a set of rules, then they can pro-\nvide interpretable evidence for its validity. Second, the complex\npatterns observed in TKGs stem from the composition of simpler,\nindependent patterns. If we can appropriately link these simple\nrules, then the complex patterns can be flexibly deduced based on\nthe individual rules. Last, rules describe the properties of a TKG in\na more compact and refined way. Thus ideally, any semantic and\npattern shifts can be described as modifications of the rules.\nSolution. In this paper, we propose ANOT, a novel summariza-\ntion method for TKG anomaly detection. As depicted in Figure 2(a),\nANOT takes an online updating TKG as input, identifies anomalies,\nand then filters valid knowledge. The process initiates with the\ndetector module, which constructs a rule graph based on the offline\npreserved part of TKG. Upon the arrival of new knowledge, this\nmodule evaluates it against the rule graph to compute an anomaly\nscore. Subsequently, the updater module receives valid knowledge\nidentified by the detector module, and then reforms them as edit\noperations on the rule graph to handle online semantic and pattern\nchanges. The monitor module estimates the approximate error of\nthe rule graph in representing the TKG. When the approximate\nerror exceeds the threshold, the monitor will inform the detector\nto refresh the rule graph based on the current TKG. In this way,\nthe reachable nodes during walking will give readable evidence for\ndetection, while the complex patterns can be flexibly described by\nthe walking paths, and the online changes are uniformly handled.\nIn the following, we first define the description length of G used\nto find the optimal rule graph, and then detail each part of A\u039d\u039f\u03a4.\n4.2 Description Length of The Rule Graph\nWe employ the minimum description length principle to guide the\nconstruction of the optimal rule graph. In other words, we consider\nit as a classic information-theoretic transmitter/receiver setting\n[51], where the goal is to describe the graph to the receiver using as\nfew bits as possible. As a result, we should first define the number\nof bits required to describe the TKG (i.e., L(M) and L(G|M))."}, {"title": "XXX and XXX, et al.", "content": "4.2.1 L(M). Based on the primitives of MDL principle [17] and\nthe definition of the rule graph, the encoding cost of a rule graph\nG = {V, E} consists of the number of atomic rules V, the number\nof rule edges E (both upper bounded by the number of possible\ncandidates), and the encoding cost of V and E, which is defined as\nL(M) =log(2 * |C8|2 * |R|) + log( |E||E|  ) +\n\u03a3L(v) + \u03a3L(e),\nv\u2208V e\u2208E\nwhere the first term is the upper bound of the number of candi-\ndate atomic rules. |C8| is the total number of entity categories\nderived from function C(.) (see Section 4.3.1). R is the number of\nrelations. Each atomic rule has the form of (CATEGORY, relation,\nCATEGORY), and thus results in |C8|2 * |R|. Twice because each\nrelation has two directions. The second term is the upper bound\nof the number of candidate rule edges, where log() means the de-\nscription length of uniformly choosing B elements from A elements.\nEach rule edge associates two or three atomic rules (i.e., chain or\ntriadic occurring), and thus results in B=3 as the upper bound. L(v)\nand L(e) are respectively the encoding costs of each atomic rule\nand each rule edge, defined as\nncs\nnco\nn\nL(v) = log|C\u025b] + (-log+1) + (-logg) + (-log+1),\nwhere the first term is the number of the categories of entities. The\nsecond to fourth terms are the number of bits used to encode subject\ncategories, object categories, and relations respectively. Note that\nwe use optimal prefix code [24] to encode actual categories, so\nn\u00bas is the number of times category cs \u2208 C\u025b occurs in G (no is\nsimilar), while n' is the number of times relation r\u2208 R occurs in\nG and 1 represents the direction of the relation. Since the number\nof relations is constant across different models, we ignore it during\nmodel selection. We define the encoding cost of each rule edge as\nnoh\nnum\nnut\nL(e) = log|E| + (-log-1) + (-log) + (-log+1),\nwhere E is the number of the rule edges and nuh is the number\nof head atomic rule on occurs in rule graph G. 1 represents the\ndirection of the edge. Note that for brevity, we only give L(e) of the\ntriadic occurring and the chain occurring can be easily extended\nby removing the auxiliary atomic rule part (i.e., the third term).\n4.2.2 L(GM). Each atomic rule can describe a set of facts in\nTKG (e.g., atomic rule (PERSON, Wins, PRIZE) can describe fact\n(Obama, Win, NobelPeacePrize, 2019/10/09)), and each rule edge\ncan describe a set of occurring relationships among facts. For\nexample, rule edge (PERSON, WintheSelection, COUNTRY) \u2192\n(PERSON, Presidentof, COUNTRY) can describe the relationship\nthat fact (Obama, Presidentof, UnitedStates, 2009/01/20) occurs\nsubsequently after the fact (Obama, WintheSelection, UnitedStates,\n2008/11/04). These described facts are called correct assertions.\nThey can be encoded by the given rule graph. Moreover, TKGs\ninevitably contain noise and uncommon facts and thus there may\nbe facts that cannot be encoded by the rule graph, called negative\nerrors. Therefore, the encoding cost of G by the rule graph M is\nL(G|M) = L(AG) + L(NG). L(AG) is the encoding cost of the"}, {"title": "4.3 Detector", "content": "The detector module has two functions: 1) Construct the optimal\nrule graph. As shown in Figure 2(b), a category function is first\nconstructed based on existing knowledge (Section 4.3.1), and then\nit will be used to map knowledge as candidate atomic rules and\ncandidate rule edges (Section 4.3.2). Finally, the most expressive\ncandidates will be iteratively selected to construct the rule graph\n(Section 4.3.3). More details can be found in Algorithm 1. 2) Gener-\nate anomaly scores. As shown in the upper part of Figure 2(c), new\nknowledge will be first mapped as a set of atomic rules. Atomic rules\nthat exist in the rule graph can give evidence of their conceptual\nvalidity, which will derive the static scores (Section 4.3.4). Then, the\nevidence of time validity is gathered by recursively walking on the\nrule graph and instantiating the processor nodes, which will derive\nthe temporal scores. More details can be found in Algorithm 2.\n4.3.1 Construct Category Function. Entity categories are often\nmissing in real-world TKGs [58]. Fortunately, we find that the cate-\ngory of an entity is largely related to the relations it interacts with.\nFor example, an entity that interacts with relations BornIn and\nPlays For should have a category of ATHLETE, and an entity that"}, {"title": "XXX and XXX, et al.", "content": "interacts with relations BornIn and Create can be an ARTIST. In-\ntuitively, the more relations are considered, the more fine-grained\ninformation a category can imply. This inspires us to generate\ncategories for entities by extracting the frequent relation combina-\ntions. A relation combination occurs more frequently across entities\nmeans it can describe a more general property shared by entities,\nwhich is more likely to imply a category.\nFormally, we define the interaction relation set of an entity e \u2208 &\nas R(e) = {r|(e, r, o, t) \u2208 F }. With each entity providing a relation\nset, we then use the PrefixSpan algorithm [38] to find the most\nfrequent subset within R(e) across all entities. Each identified sub-\nset is a frequent relation combination that suggests a potential\ncategory. Subsets encompassing more relations imply more granu-\nlar categories. However, finding frequent subsets with large sizes\nis very time-consuming. To counterbalance efficiency with cate-\ngories' quality, we propose to only find small relation combinations\n(i.e., up to 3 relations) and then iteratively aggregate selected sub-\nsets. Let the output of PrefixSpan be R = {(R1, E1), (R2, E2), ...},\nwhere Ri is the ith most frequent relation combination (r, rr)\nand Ei = {ele \u2208 &, R\u00a1 \u2286 R(e)} is the entities encompassed by Ri.\nWe commence with entity-based aggregation: if a significant\noverlap exists between entities in E\u00a1 and Ej (exceeding 90%), it\nindicates shared properties that necessitate simultaneous descrip-\ntion via both relation combinations. Consequently, we introduce\n(R = R\u00a1URj, E = E\u00a1\u2229Ej) into R as a more fine-grained category. We\nthen perform relation-based aggregation: if a significant overlap\nexists between relations in Rm and Rn (exceeding 90%), it means\nthe categories implied by Rm and Rn are very similar. Therefore,\nwe add (R = RmRn, E = Em U En) into R as a more generalizable\ncategory. These aggregation steps are circularly executed until no\nfurther combinations can be aggregated. Following this phase, each\nrelation combination R\u012f is conceptualized as an implicit category ci,\nwith the respective entities in E\u00a1 being categorized accordingly. To\nreduce category redundancy, we sort the relation combinations in\ndescending order of the number of their covered entities and select\none by one until each entity has at least k categories.\n4.3.2 Candidate Generation. To construct the rule graph, we should\nfirst generate all possible rules and rule edges as candidates based\non the input TKG. For each fact (s, r, o, t), we generate the corre-\nsponding candidate atomic rules as {(ci, r, cj)|ci \u2208 C(s), cj \u2208 C(0)},\nwhere C(s) is the category set of s. We gather the rules derived\nfrom all facts (s, r, o, t) \u2208 F as the candidate set of atomic rules.\nTo generate all possible chain-occurring-based rule edges, we\nfirst construct the interaction sequence S(s, 0) = {1, 2, ...} for\neach entity pair appeared in G. The interaction sequence preserves\nall the relations that occurred between s and o and is sorted by\nthe ascending order of their occurrence timestamps. Thus, any\nadjacent relations in the sequence represent two interactions be-\ntween s and o that occur successively, which may imply a chain-\noccurring pattern. Formally, given each entity pair (s, o) and its\ncorresponding interaction sequence, we generate the candidate rule\nedges as {(cs, rm, Co) \u2192 (cs, rn, Co) Cs \u2208 C(s), co \u2208 C(o), rm,rn \u2208\nS(s, o), m < n}. We gather the rule edges derived from all entity\npairs that appear in G as the candidate set of chain-occurring-based\nrule edges. Since the occurrence timespan between different rela-\ntions may vary, e.g., MakeStatement may occur a few days after"}, {"title": "XXX and XXX, et al.", "content": "WintheElection, but Retirement may occur several years later, con-\nsidering the timespan of occurrence between two relations assists\nin determining the occurrence of a fact at a particular timestamp.\nTherefore, we also preserve the occurrence timespans of facts for\neach rule edge e (e.g., tn - tm for chain-occurring facts (s, rm, 0, tm)\nand (s, rn, 0, tn)) and results in a timespan set T(e).\nFor triadic-occurring-based rule edges, in each timestamp te\nT, we find facts that occur at t and share one same entity (e.g.,\n(s, rm, o, t) and (h, rn, o, t)). Then, we find the most closely occurred\nfact in tp \u2265 t that contains s and h (e.g., (s, rp, h, t + 3)). These three\nfacts describe the formation process of a triadic closure, which may\nimply a triadic-occurring pattern. Since there is local randomness\nin the occurrence time of facts [35], we relax the same time re-\nstriction of the former two facts as co-occurring within a short\nperiod. Formally, for each timestamp t, the triadic-occurring-based\ncandidate rule edges are generated as {((cs, rm, Co), (Ch, rn, Co)) \u2192\n(Cs, rp, Ch) Cs \u2208 C(s), co \u2208 C(o),ch \u2208 C(h),|T(es, rm, eo) - t| \u2264\nL, |T(eh, rn, eo) \u2013 t| \u2264 L, T (es, rp, eh) \u2265 t + L}, where T(\u00b7) is the\noccurrence timestamp of fact and L is a hyperparameter. We also\npreserve the occurrence timespans for these facts.\n4.3.3 Ranking and Selection. We propose a greedy approach to\nselect the most representative candidate into the rule graph iter-\natively. Our objective is to select the candidate that leads to the\nlargest encoding cost reduction in each iteration. Recognizing that\nvarying orders of selection may yield inconsistent models, thus\naffecting reproducibility, we implement a structured ranking mech-\nanism that ensures a consistent selection order of candidates. Since\nthe more a candidate can reduce negative errors, the more valuable\nit might be, we first rank candidates based on the descending order\nof their error reduction AL(G|MU{x}) = L(G|M)-L(G|MU {x}),\nwhere x represents candidate atomic rule v or rule edge e. The\nties in the ranking are broken by selecting candidates with more\ncorrect assertions. The final tie-breaker is the ID of each candidate.\nWe separately rank atomic rules and rule edges since they have\ndifferent magnitudes in the cost reduction. The ranked candidate\nrules and rule edges are respectively termed as P(v) and P(e).\nAfter ranking the candidates, M is initialized as \u00d8 and each v \u2208\nP(v) is first selected in ranked order. For each v, we compute the\ndescription length when u is added into M (i.e., L(G, M\u222a {v})). If\nit is less than L(G, M), u can enhance the expressive capability of\nM. Thus, we add v into M. We perform the selection passes over\nP(v) until no new atomic rules can be added. We then perform\nthe same selection process on P(e) to add rule edges into M. Note\nthat some selected rule edges may contain atomic rules that are not\nselected in the former process. We restrict the usage of these atomic\nrules only to verify the time errors. The obtained approximately\noptimized rule graph is termed as M* = {V*, E*}.\n4.3.4 Deriving Anomaly Scores. Intuitively, nodes and edges in the\nrule graph explain the common patterns of knowledge occurring\nin TKG. Thus, new knowledge that cannot be mapped as nodes or\ncannot be associated with other knowledge via edges is unexplained\nand likely to be anomalous. We make this intuition more principled\nby defining static scores and temporal scores for tuples.\nStatic scores. Nodes in M* represent valid interaction patterns\nfound in the TKG. When new knowledge is mapped to a node in\nM*, it means that the knowledge can be explained by an observed"}, {"title": "Algorithm 2 Derive the anomaly scores", "content": "1: Input: New knowledge (s, r, o, t) and rule graph M*\n2: Output: Static score $(s, r, o, t) and temporal score T(s, r, o, t)\n3: Generate mapped atomic rule set V(s, r, o, t)\n4: for e \u2208 (s, o) do\n5: for v \u2208 V(s, r, o, t) do\n\u25ba Eq. 9\n6:\ntmps - tmps + |A|\n7: $(s, r, o,t) = tmps\n1\n8: if tmps < 1 then\n\u25ba A is a threshold\n9:\nreturn $(s, r, o, t)\n10: V'V(s, r, o, t), tmpt 0, MAX_STEP \u2190 0\n11: tmpList 0\n12: for v \u2208 V' do\n\u25ba Eq. 10\n13: for vi \u2208 Nin (v) do\n14: if vi is instantiable then\n15:\ntmpt tmpt +0+1\n\u25ba K is a hyper-parameter"}, {"title": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY", "content": "XXX and XXX, et al.\nAlgorithm 3 Update the rule graph\n1: Input: New valid knowledge (s, r, o, t) and rule graph M*\n2: Output: The updated rule graph M*\n3: GGU {(s, r, o, t)}\n\u25ba handle graph structure changes\n4: temps \u2190 0\n5: if r & R(e) then\n\u25ba handle entity semantic changes\n6:\n\u25ba handle new"}, {"title": "Online Detection of Anomalies in Temporal Knowledge Graphs with Interpretability", "content": "\u25ba add more e to M*"}, {"title": "Conference acronym 'XX, June 03-05, 2018, Woodstock, NY", "content": "xxx+1"}]}