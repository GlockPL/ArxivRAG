{"title": "LalaEval: A Holistic Human Evaluation Framework for Domain-Specific Large Language Models", "authors": ["Chongyan Sun", "Ken Lin", "Shiwei Wang", "Hulong Wu", "Chengfei Fu", "Zhen Wang"], "abstract": "This paper introduces LalaEval, a holistic framework designed for the hu-man evaluation of domain-specific large language models (LLMs). LalaEvalproposes a comprehensive suite of end-to-end protocols that cover five maincomponents including domain specification, criteria establishment, bench-mark dataset creation, construction of evaluation rubrics, and thoroughanalysis and interpretation of evaluation outcomes. This initiative aimsto fill a crucial research gap by providing a systematic methodology forconducting standardized human evaluations within specific domains, apractice that, despite its widespread application, lacks substantial coveragein the literature and human evaluation are often criticized to be less reliabledue to subjective factors, so standardized procedures adapted to the nu-anced requirements of specific domains or even individual organizationsare in great need. Furthermore, the paper demonstrates the framework'sapplication within the logistics industry, presenting domain-specific eval-uation benchmarks, datasets, and a comparative analysis of LLMs for thelogistics domain use, highlighting the framework's capacity to elucidateperformance differences and guide model selection and development fordomain-specific LLMs. Through real-world deployment, the paper un-derscores the framework's effectiveness in advancing the field of domain-specific LLM evaluation, thereby contributing significantly to the ongoingdiscussion on LLMs' practical utility and performance in domain-specificapplications.", "sections": [{"title": "1 Introduction", "content": "The rise of large language models (LLMs) represents a significant step towards artificialgeneral intelligence (Bubeck et al., 2023), showcasing their powerful ability to understandand produce natural language. Although these models have generally been designed forwide-ranging use, one of their most promising applications lies within specific domainssuch as medicine (Lee et al., 2023; Wang et al., 2023a; Thirunavukarasu et al., 2023), law (Cuiet al., 2023) and finance (Li et al., 2023; Wu et al., 2023). For businesses looking to integrateLLMs into their operations, the focus naturally shifts towards models' capabilities in certainindustries, as conversations within a particular industry are more prevalent and relevantthan broad conversations when in real-world business setting (Guo & Yu, 2022; Zhao et al.,2023).The process of evaluating these domain-specific models is crucial. Typically, evaluationsrange from automatic techniques, e.g. Zheng et al. (2024), to human evaluation, with thelatter widely regarded as the most comprehensive method. Human evaluation's importance lies in its unparalleled ability to grasp the intricacies of language and context, aspects thatautomatic methods might miss (Novikova et al., 2017), and more importantly in commercialuse, human evaluation also reflects true human preference of businesses' stakeholders,so when developing and comparing LLMs, particularly those meant for domain-specificreal-world business use, human evaluation becomes essentially gold standard. It plays a"}, {"title": "2 Related Work", "content": "Chang et al. (2023) presents an extensive survey on the evaluation of LLMs, providinga detailed taxonomy of evaluation aspects, including what, where, and how to evaluate.General-purpose models, such as OpenAI's GPT and Google's Gemini, typically do notserve downstream tasks directly. Thus, their evaluation often involves a multifacetedapproach (Bang et al., 2023), incorporating a variety of tasks, as demonstrated by Bai et al.(2024); Bian et al. (2023); Liang et al. (2022).In contrast, the most economically potent applications for commercial, domain-specificuse are arguably found in customer operations, i.e. service chatbots (Chui et al., 2023).Accordingly, this paper concentrates on developing an evaluation framework for domain-specific LLMs, particularly for chatbot conversations. Although initially designed for single-round conversations, this framework can be readily extended to multi-round interactions.There is a significant body of work addressing where to evaluate. Numerous benchmarksand datasets have been established to test the capabilities of LLMs (Hendrycks et al., 2020;Xu et al., 2023; Gu et al., 2023). These efforts provide essential benchmarking datasets andresults for mainstream LLMs, facilitating a unified comparison. However, these benchmarksare often not directly applicable to specific company uses due to differences in domains,knowledge, applications, etc. This paper distinguishes itself by offering frameworks andprotocols to guide the construction of benchmarks and datasets within commercial organi-zations for specific domains.Another stream of literature focuses on how to evaluate. Many benchmarks adopt automaticevaluation methods to obtain results, ranging from traditional metrics (Chin-Yew, 2004) tomore LLM-based judgments (Wang et al., 2023b). The advantages of automatic evaluationover human evaluation are clear: it is more standardized and objective, easier to scale, andless costly. However, for non-standard tasks like open generation, human evaluation provesto be more reliable and better aligned with general human preferences (Novikova et al., 2017).Moreover, evaluating domain-specific LLMs often requires profound domain knowledgeand even internal organizational knowledge. The human evaluation is typically criticizedby its cost and susceptibility to subjective bias, but within the context of commercial useof domain-specific LLMs, a company can often afford the expense of human evaluatorsand organize effective training. Moreover, domain-specific LLMs typically have large real-world influence (e.g. chatbot serving millions of consumers) and have significant impact onfirm operation. In these widespread high-stake circumstances, the resources required arereasonable compared to the potential benefits of ensuring best practice. Nevertheless, there isvery little literature offering a holistic framework to systematically guide the implementation"}, {"title": "3 Methodology", "content": "LalaEval encompasses five main components to establish comprehensive protocols guidingthe evaluation process of LLMs: (1) Domain Specification, (2) Criteria Establishment, (3) Bench-mark Dataset Creation, (4) Construction of Evaluation Rubrics, and (5) Analysis and Interpretationof Evaluation Results.\n1. Domain Specification: This component involves defining the scope of specific fieldsof interest, largely influenced by an organization's goals or objectives with LLMs. Itestablishes the evaluation process's boundaries.\n2. Criteria Establishment: This component defines the LLMs' capability dimensions forevaluating performance, effectiveness, or suitability. This ensures that evaluationsare based on relevant, objective, and consistently applied measures.\n3. Benchmark Dataset Creation: This component entails developing standardized testsand compiling carefully curated data collections from scrutinized informationsources. It allows for evaluation under consistent conditions, facilitating compara-tive measurement and analysis.\n4. Construction of Evaluation Rubrics: This component describes the careful design ofgrading schemes that detail specific guidelines for measuring various performanceaspects. It provides a structured framework to train human evaluators.\n5. Analysis and Interpretation of Evaluation Results: This component involves system-atically examining data collected from the evaluation process to minimize intrap-ersonal, interpersonal, intramodel, and intermodel variability. It aims to derivemeaningful insights and guide decision-making, ensuring the constructive applica-tion of outcomes."}, {"title": "3.2 Domain specification", "content": "In the conceptualization of LalaEval, the first step entails defining the domain that is usuallyinherently derived from the industries within which an organization operates. The breadthof such an industry definition, however, remains an area for exploration. For instance,should the evaluation scope for a medical LLM include only radiology or extend to thebroader field of medicine? Should the focus for a financial LLM be confined to bonds, orshould it cover the wider realm of capital markets?We propose a hierarchical structuring of subdomains for LalaEval, which organizes thesesubdomains by their relative significance, ranging from narrow to broad scopes. Employingbackward induction, we begin with a highly specific subdomain pertinent to the organi-zation, progressing towards a more generalized industry definition. By adhering to theprinciple of mutual exclusivity and collective exhaustiveness, we enumerate the most gran-ular subdomains and their parallel subdomains, progressively ascending to encompassbroader subdomains. This process is iterated multiple times until a sufficiently broaddomain scope is achieved.Subsequently, we employ qualitative prioritization to define both the scope and the hi-erarchical significance of each subdomain. This prioritization may adopt either a linearprogression from the most specific to the most broad domain or a tree-like structure thatincludes parallel subdomains at various levels, contingent upon business imperatives.Figure 1 presents our case by hierarchical domain specification in the logistics industry,with the final chosen subdomains highlighted. We establish both the scope and prioritiesfor subdomains, ranking them from high (P0) to low (P3) priority: P0 (Intracity Freight"}, {"title": "3.3 Criteria Establishment", "content": "The objective of establishing criteria for general capabilities is to evaluate the performanceof LLMs a wide array of natural language tasks not confined to any specific domain. Thisincludes their capability in comprehending and generating natural language, recogniz-ing contextual cues, sustaining coherence throughout conversations, and processing andconveying information with accuracy.\nThe rationale for evaluating such general capabilities includes (1) Foundation for domaincapability: A solid foundation in general capability is crucial for the performance of any LLM,providing the groundwork upon which domain capability is constructed. A lack in generallanguage capability can impede LLMs' capability to understand or generate responsesaccurately within specialized domains. (2) Flexibility and Adaptability: LLMs that exhibitstrong general language capabilities demonstrate enhanced flexibility and adaptability. Thisis vital for applications requiring comprehension of inputs from diverse domains or theintegration of new knowledge without the need for extensive retraining. (3) Understand-ing and Reasoning: Evaluating general capabilities aids in identifying LLMs' capacity forunderstanding complex queries, reasoning, and generating coherent responses that arecontextually appropriate. These attributes are indispensable for practical applications.To systematically evaluate these general capabilities, we have adapted from (Xu et al., 2023)and outlined dimensions of general capabilities in the following, and we also show examplequestions and defined difficulty levels in Appendix A."}, {"title": "3.3.1 General Capability", "content": "The objective of establishing criteria for general capabilities is to evaluate the performanceof LLMs a wide array of natural language tasks not confined to any specific domain. Thisincludes their capability in comprehending and generating natural language, recogniz-ing contextual cues, sustaining coherence throughout conversations, and processing andconveying information with accuracy.\nThe rationale for evaluating such general capabilities includes (1) Foundation for domaincapability: A solid foundation in general capability is crucial for the performance of any LLM,providing the groundwork upon which domain capability is constructed. A lack in generallanguage capability can impede LLMs' capability to understand or generate responsesaccurately within specialized domains. (2) Flexibility and Adaptability: LLMs that exhibitstrong general language capabilities demonstrate enhanced flexibility and adaptability. Thisis vital for applications requiring comprehension of inputs from diverse domains or theintegration of new knowledge without the need for extensive retraining. (3) Understand-ing and Reasoning: Evaluating general capabilities aids in identifying LLMs' capacity forunderstanding complex queries, reasoning, and generating coherent responses that arecontextually appropriate. These attributes are indispensable for practical applications.To systematically evaluate these general capabilities, we have adapted from (Xu et al., 2023)and outlined dimensions of general capabilities in the following, and we also show examplequestions and defined difficulty levels in Appendix A.\n\u2022 Semantic Understanding: Crucial for LLMs is understanding the basic meaningof language, and special terms such as idioms and cultural references, to ensuremeaningful user interaction. Evaluation involves questions that test understandingacross linguistic occasions.\n\u2022 Contextual Conversation: LLMs need to remember past interactions to maintaincoherent conversations, essential for sustained engagement in customer service orconversational applications. Multi-round questions can evaluate this skill.\n\u2022 Answer Completeness and Coherence: Responses must be clear, concise, and addressqueries directly, ensuring outputs are practical and user-centric, especially in infor-mative or decision-support settings.\n\u2022 Factuality: Accuracy in responses, especially for questions expecting definitiveanswers, is critical. This is vital in industries like logistics where errors can havesignificant consequences. Verification against trusted sources evaluates this aspect.\n\u2022 Creativity: The ability to generate creative content, such as marketing material orinnovative responses, highlights LLMs' capacity for engaging content creation. Thisis evaluated through creativity requests with certain constraints.\n\u2022 Logical Reasoning: Tasks requiring numerical or logical deduction evaluate thiscapability, relevant for problem-solving in industries like logistics. Evaluationincludes mathematical or logic puzzles."}, {"title": "3.3.2 Domain Capability", "content": "The domain capability focuses on LLMs' expertise within certain industries, evaluating itsunderstanding of domain-specific terminologies, concepts, regulations, and operationalnuances. It also evaluates LLMs' capability to provide insightful, accurate responses todomain-specific queries.\nThe rationale for evaluating domain-specific capability is (1) Domain-Specific Performance:The principal motive is to ascertain the efficacy of LLMs in logistics-specific applications.It is imperative for LLMs to not merely comprehend general language but also to exhibita profound understanding of specialized domain knowledge and intricacies. (2) PracticalUsability: LLMs endowed with strong logistics domain capabilities hold greater practicalvalue for industry professionals and businesses. Such LLMs are capable of providing moreaccurate and relevant insights, thereby facilitating enhanced decision-making processesand operational efficiency. (3) Customized Solutions: Evaluating this capability allows for thedevelopment of more customized, domain-specific solutions that can address specific chal-lenges and needs within the logistics domain, providing a competitive edge to businessesleveraging these LLMs.\nFocusing on the dimensions of factuality and creativity, we have outlined and illustratedsubdimensions of capabilities in logistics domain in the following, and we also showexample questions and defined difficulty levels in Appendix B.\n\u2022 Conceptual and Terminological Understanding: Knowledge of specific terms and con-cepts is fundamental in accurately interpreting and responding to industry-relatedqueries. This necessitates an evaluation that includes questions derived from thelogistics domain's lexicon and operational nuances.\n\u2022 Company Information: LLMs should be conversant with key players in the industryand relevant corporate data, reflecting the model's utility as an informative resource.Questions about major companies, their operations, and market positions canevaluate this aspect.\n\u2022 Legal and Policy Knowledge: Given the regulatory environment surrounding logisticsoperations, LLMs must be adept at navigating legal and policy-related queries.This can be evaluated through questions that require the LLM to reference specificregulations or guidelines applicable to logistics.\n\u2022 Industry Insights: The ability to provide informed opinions or data about the logisticsmarket, trends, and future outlooks showcases LLMs' depth of knowledge andanalytical capabilities. Crafting scenarios that ask for analysis or predictions basedon current data evaluates this competency.\n\u2022 Company-specific Knowledge: For LLMs deployed by specific companies, such asHuolala, understanding the company's services, history, and strategic vision is"}, {"title": "3.4 Benchmark Dataset Creation", "content": "The primary goal of establishing a high-quality benchmark dataset is to develop an evolvingbank of question-answer (QA) pairs for human evaluation. Existing public benchmarksand datasets are not utilized due to (1) the absence of benchmarks tailored to the specificdomain; (2) the proprietary benchmark's closer alignment with company-specific needs;and (3) the lack of continuous updates in most public datasets, which fails to meet the timelyknowledge requirements for the business application of LLMs. Concurrently, this datasetaims to include a broad spectrum of capabilities outlined in Section 3.3, thereby ensuring auniform evaluation of LLMs' performance across diverse metrics.\nThe benchmark dataset are developed through the process outlined in following steps:\n1. Accumulation of the Raw Corpus: Guided by the previous components, we system-atically collect and compile original texts and corpora from verified sources. Thisrepository of raw corpus forms the groundwork for generating insightful andpertinent QA pairs.\n2. Production of QA Pairs: (1) Development of the Question Plan: Formulate a struc-tured plan outlining the desired number of QA pairs, categorized by various levelsof difficulty and capabilities. This plan should be iterative to allow for the con-tinuous enhancement of the dataset. (2) Selection of Question Designers: Identifyand appoint individuals responsible for question design, ensuring they possess anin-depth understanding of the evaluation framework and access to the raw corpus.(3) Creation of QA Pairs: The question designers will distill relevant informationfrom the raw corpus or other authoritative sources to craft QA pairs. Each pair willinclude a question and its corresponding standard answer, for instance, \"Q: Howmany hours are there in a day? A: 24 hours.\u201d Importantly, the source of informationfor each QA pair must be documented to maintain traceability and credibility.\n3. Quality Inspection and Database Entry: Following the generation of QA pairs, athorough quality inspection is conducted to verify that the pairs adhere to prede-fined criteria. QA pairs that pass this inspection are incorporated into the dataset,whereas those failing to meet the standards are redirected back to the designers forrefinement.\nThe process of proposing QA pairs for the benchmark dataset is a critical component inthe evaluation of LLMs, particularly within specific domains such as the logistics industry.Through a systematic approach to compiling raw corpora, creating relevant QA pairs, andexecuting rigorous quality inspections, this initiative seeks to establish a comprehensivedataset. This dataset not only lays the foundation for a thorough evaluation of LLMs butalso facilitates the continuous improvement and benchmarking of LLMs, thereby ensuringtheir relevance and applicability in real-world scenarios."}, {"title": "3.5 Construction of Evaluation Rubrics", "content": "The objective of the construction of evaluation rubrics is to systematically evaluate theperformance of various LLMs using the benchmark dataset from Section 3.4. This process isdesigned to guide the training of human evaluators and ensure more consistent outcomes,both intraperson and interperson.\nThe general evaluation scale is 0-3 points. If a response contains any incorrect information,it will get 0 points. 1-3 points measure the degree of correctness, completeness, creativity etc.Meanwhile, special consideration is given to the timeliness of responses, recognizing the"}, {"title": "3.6 Analysis and Interpretation of Evaluation Outcomes", "content": "After trained human evaluators are ready (see Appendix G for details of human evaluatortraining process), we randomly draw a subset from the dataset based on agreed difficultylevel and quantities in the grading process. To keep integrity, we employ a single-blindprocedure, wherein model responses to the QA pairs are anonymized and presented ina randomized order to a panel of at least three human evaluators. This procedure miti-gates bias, ensuring that human evaluators cannot infer the origin of the responses, thusmaintaining objectivity. Table 1 shows the demo of human evaluators' interface.\nThe evaluation results are systematically compiled into a four-dimensional table that encapsulates the evaluated capabilities, question numbers, human evaluator IDs, and gradesallocated to each LLM for every question like Table 2. This structured data allows foranalysis across different dimensions of model performance. Grades for each model withinspecific capabilities are aggregated and normalized to 100 points, offering a detailed viewof model capabilities. Moreover, more comprehensive grading can be achieved throughthe weighted aggregation of detailed grades, reflecting the LLMs' overall performancespectrum.\nThe grade calculation for model q involves single dimension j grade calculation using theformula Grade(qj) = \\frac{\\sum_{k=1}^{K_j} \\sum_{i=1}^{L} ASqki}{Kj \\cdot L} , then aggregating across all dimensions to computethe total grade as Grade(q) = \\sum_{j=1}^J w_jGrade(qj) . This process evaluates a LLM's capabilityacross various dimensions and overall, by weighing each dimension's grade, equally orotherwise. Reporting includes both the grades for different dimensions and the total grade,facilitating a comprehensive assessment of each LLM's performance. Repeating these stepsfor all models under evaluation provides a comparative analysis of their strengths andweaknesses.\nOur methodology also aims to reduce the subjective factors of human evaluators as muchas possible, incorporating automated dispute analysis and checks for grade stability andreliability. This phase is crucial for identifying low-quality grades and questions and"}, {"title": "3.7 Overall Deployment Structure", "content": "After describing the five main conceptual components of LalaEval, we now turn to thepracticalities of deployment, presenting an approach that brings our framework into opera-tional reality, ensuring a comprehensive evaluation process for domain-specific LLMs. Thisintegration strategy, designed to foster standardization, as well as efficiency and adaptability,is succinctly illustrated in Figure 2. Through this streamlined depiction, the operationalblueprint that guides the application of LalaEval is presented, facilitating a clear under-standing of its modular architecture and the dynamic interactions essential for evaluation.The results presented in Section 4 are also generated using this deployment strategy."}, {"title": "4 Results", "content": "To demonstrate the effectiveness and the practical application of LalaEval, we present thezero-shot evaluation results of various LLMs including OpenAI's GPT-4 (without webaccess), Baidu's Ernie Bot (with web access), and our proprietary LLM v1, v2, and v3(PLLM1/2/3). PLLM1/2/3 are iterations fine-tuned from the ChatGLM2-6B foundation,incorporating web access, retrieval-augmented generation, or a combination of both, respec-tively.\nOur evaluation is demonstrated in two distinct result sets: (1) The accuracy of the LLMs(Table 3), where we present the percentage of responses achieving non-zero grades acrossdifferent capability dimensions. This metric serves as a direct indicator of each LLM'scapability to generate relevant and accurate response. (2) The normalized average grades(Table 4) for each LLM across capability dimensions, providing more quantitative viewof LLM performance. The complete results of grades and disagreement by all capabilitydimensions are included in Appendix H. The results reflect various levels of coverage ofcapability dimension defined in Section 3.3 where Domain-Factuality is defined as the average"}, {"title": "5 Future Work", "content": "LalaEval may have some potential limitations, including evaluator subjectivity, data selec-tion bias, dynamic changes in domain knowledge, and scalability concerns. To addressthese challenges and further enhance LalaEval's effectiveness, several avenues for futurework can be proposed.\n\u2022 Standardized Training and Assessment: Beyond procedures described in Appendix G,exploring more structured training protocols across domains can further improvethe reliability and generalizability of LalaEval.\n\u2022 Enhanced Dataset Representation: Ensuring the benchmark dataset is comprehensiveand representative in the domain can minimize biases arising from data selection.This step involves continuous refinement and expansion of dataset sources to coverdiverse scenarios.\n\u2022 Adaptability to Dynamic Domains: Given the evolving nature of domain-specificknowledge, continuous updates to evaluation protocols and benchmarks are essen-tial. This adaptive approach helps LalaEval stay relevant and effective in terms ofdynamic industry changes.\n\u2022 Automation Integration: Supplementing human evaluation with automated methodsand support systems can streamline the evaluation process. Automation can assist inhandling large volumes of data and tasks, enhancing scalability while maintainingevaluation reliability."}, {"title": "6 Conclusion", "content": "This study introduces LalaEval, a novel framework for evaluating domain-specific LLMs,with a focus on standardizing human evaluations. By detailing a comprehensive methodol-ogy spanning domain specification to results analysis, LalaEval addresses the critical gapin the standardized human evaluations of domain-specific LLMs, exemplified through itsapplication in the logistics domain.\nLalaEval's deployment showcases its capability to illuminate performance differencesamong LLMs, guiding model selection and development for domain-specific applications.LalaEval not only advances the field of LLM evaluation by establishing end-to-end humanevaluation protocols but also emphasizes the importance of aligning LLM capabilities withpractical needs.\nLalaEval is also a general human evaluation framework which are disentangled with anyspecific domain. The results of only logistics domain are reported because LalaEval wasdeveloped and finalized when exploring standardized human evaluation framework in thisdomain. LalaEval has been applied to other domains such as HR/IT maintanance/text-to-SQL/telemarketing and brought business value. It is believed those who are interested inemploying LalaEval to evaluate other domain-specific LLMs can easily implement it.\nIn summary, LalaEval represents a significant step forward in the evaluation of domain-specific LLMs, offering a structured and human-centric approach to understanding LLMperformance. Its contributions lay the groundwork for future research and application ofLLMs across various domains, highlighting the evolving need for evaluation methodologiesthat are as dynamic and specialized as the LLMs they seek to evaluate."}]}