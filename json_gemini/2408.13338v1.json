{"title": "LalaEval: A Holistic Human Evaluation Framework for Domain-Specific Large Language Models", "authors": ["Chongyan Sun", "Ken Lin", "Shiwei Wang", "Hulong Wu", "Chengfei Fu", "Zhen Wang"], "abstract": "This paper introduces LalaEval, a holistic framework designed for the human evaluation of domain-specific large language models (LLMs). LalaEval proposes a comprehensive suite of end-to-end protocols that cover five main components including domain specification, criteria establishment, benchmark dataset creation, construction of evaluation rubrics, and thorough analysis and interpretation of evaluation outcomes. This initiative aims to fill a crucial research gap by providing a systematic methodology for conducting standardized human evaluations within specific domains, a practice that, despite its widespread application, lacks substantial coverage in the literature and human evaluation are often criticized to be less reliable due to subjective factors, so standardized procedures adapted to the nuanced requirements of specific domains or even individual organizations are in great need. Furthermore, the paper demonstrates the framework's application within the logistics industry, presenting domain-specific evaluation benchmarks, datasets, and a comparative analysis of LLMs for the logistics domain use, highlighting the framework's capacity to elucidate performance differences and guide model selection and development for domain-specific LLMs. Through real-world deployment, the paper underscores the framework's effectiveness in advancing the field of domain-specific LLM evaluation, thereby contributing significantly to the ongoing discussion on LLMs' practical utility and performance in domain-specific applications.", "sections": [{"title": "1 Introduction", "content": "The rise of large language models (LLMs) represents a significant step towards artificial general intelligence (Bubeck et al., 2023), showcasing their powerful ability to understand and produce natural language. Although these models have generally been designed for wide-ranging use, one of their most promising applications lies within specific domains such as medicine (Lee et al., 2023; Wang et al., 2023a; Thirunavukarasu et al., 2023), law (Cui et al., 2023) and finance (Li et al., 2023; Wu et al., 2023). For businesses looking to integrate LLMs into their operations, the focus naturally shifts towards models' capabilities in certain industries, as conversations within a particular industry are more prevalent and relevant than broad conversations when in real-world business setting (Guo & Yu, 2022; Zhao et al., 2023).\nThe process of evaluating these domain-specific models is crucial. Typically, evaluations range from automatic techniques, e.g. Zheng et al. (2024), to human evaluation, with the latter widely regarded as the most comprehensive method. Human evaluation's importance lies in its unparalleled ability to grasp the intricacies of language and context, aspects that automatic methods might miss (Novikova et al., 2017), and more importantly in commercial use, human evaluation also reflects true human preference of businesses' stakeholders, so when developing and comparing LLMs, particularly those meant for domain-specific real-world business use, human evaluation becomes essentially gold standard. It plays a"}, {"title": "2 Related Work", "content": "Chang et al. (2023) presents an extensive survey on the evaluation of LLMs, providing a detailed taxonomy of evaluation aspects, including what, where, and how to evaluate. General-purpose models, such as OpenAI's GPT and Google's Gemini, typically do not serve downstream tasks directly. Thus, their evaluation often involves a multifaceted approach (Bang et al., 2023), incorporating a variety of tasks, as demonstrated by Bai et al. (2024); Bian et al. (2023); Liang et al. (2022).\nIn contrast, the most economically potent applications for commercial, domain-specific use are arguably found in customer operations, i.e. service chatbots (Chui et al., 2023). Accordingly, this paper concentrates on developing an evaluation framework for domain-specific LLMs, particularly for chatbot conversations. Although initially designed for single-round conversations, this framework can be readily extended to multi-round interactions.\nThere is a significant body of work addressing where to evaluate. Numerous benchmarks and datasets have been established to test the capabilities of LLMs (Hendrycks et al., 2020; Xu et al., 2023; Gu et al., 2023). These efforts provide essential benchmarking datasets and results for mainstream LLMs, facilitating a unified comparison. However, these benchmarks are often not directly applicable to specific company uses due to differences in domains, knowledge, applications, etc. This paper distinguishes itself by offering frameworks and protocols to guide the construction of benchmarks and datasets within commercial organizations for specific domains.\nAnother stream of literature focuses on how to evaluate. Many benchmarks adopt automatic evaluation methods to obtain results, ranging from traditional metrics (Chin-Yew, 2004) to more LLM-based judgments (Wang et al., 2023b). The advantages of automatic evaluation over human evaluation are clear: it is more standardized and objective, easier to scale, and less costly. However, for non-standard tasks like open generation, human evaluation proves to be more reliable and better aligned with general human preferences (Novikova et al., 2017). Moreover, evaluating domain-specific LLMs often requires profound domain knowledge and even internal organizational knowledge. The human evaluation is typically criticized by its cost and susceptibility to subjective bias, but within the context of commercial use of domain-specific LLMs, a company can often afford the expense of human evaluators and organize effective training. Moreover, domain-specific LLMs typically have large real-world influence (e.g. chatbot serving millions of consumers) and have significant impact on firm operation. In these widespread high-stake circumstances, the resources required are reasonable compared to the potential benefits of ensuring best practice. Nevertheless, there is very little literature offering a holistic framework to systematically guide the implementation"}, {"title": "3 Methodology", "content": "LalaEval encompasses five main components to establish comprehensive protocols guiding the evaluation process of LLMs: (1) Domain Specification, (2) Criteria Establishment, (3) Benchmark Dataset Creation, (4) Construction of Evaluation Rubrics, and (5) Analysis and Interpretation of Evaluation Results.\n1. Domain Specification: This component involves defining the scope of specific fields of interest, largely influenced by an organization's goals or objectives with LLMs. It establishes the evaluation process's boundaries.\n2. Criteria Establishment: This component defines the LLMs' capability dimensions for evaluating performance, effectiveness, or suitability. This ensures that evaluations are based on relevant, objective, and consistently applied measures.\n3. Benchmark Dataset Creation: This component entails developing standardized tests and compiling carefully curated data collections from scrutinized information sources. It allows for evaluation under consistent conditions, facilitating comparative measurement and analysis.\n4. Construction of Evaluation Rubrics: This component describes the careful design of grading schemes that detail specific guidelines for measuring various performance aspects. It provides a structured framework to train human evaluators.\n5. Analysis and Interpretation of Evaluation Results: This component involves systematically examining data collected from the evaluation process to minimize intrapersonal, interpersonal, intramodel, and intermodel variability. It aims to derive meaningful insights and guide decision-making, ensuring the constructive application of outcomes."}, {"title": "3.2 Domain specification", "content": "In the conceptualization of LalaEval, the first step entails defining the domain that is usually inherently derived from the industries within which an organization operates. The breadth of such an industry definition, however, remains an area for exploration. For instance, should the evaluation scope for a medical LLM include only radiology or extend to the broader field of medicine? Should the focus for a financial LLM be confined to bonds, or should it cover the wider realm of capital markets?\nWe propose a hierarchical structuring of subdomains for LalaEval, which organizes these subdomains by their relative significance, ranging from narrow to broad scopes. Employing backward induction, we begin with a highly specific subdomain pertinent to the organization, progressing towards a more generalized industry definition. By adhering to the principle of mutual exclusivity and collective exhaustiveness, we enumerate the most granular subdomains and their parallel subdomains, progressively ascending to encompass broader subdomains. This process is iterated multiple times until a sufficiently broad domain scope is achieved.\nSubsequently, we employ qualitative prioritization to define both the scope and the hierarchical significance of each subdomain. This prioritization may adopt either a linear progression from the most specific to the most broad domain or a tree-like structure that includes parallel subdomains at various levels, contingent upon business imperatives."}, {"title": "3.3 Criteria Establishment", "content": "The objective of establishing criteria for general capabilities is to evaluate the performance of LLMs a wide array of natural language tasks not confined to any specific domain. This includes their capability in comprehending and generating natural language, recognizing contextual cues, sustaining coherence throughout conversations, and processing and conveying information with accuracy.\nThe rationale for evaluating such general capabilities includes (1) Foundation for domain capability: A solid foundation in general capability is crucial for the performance of any LLM, providing the groundwork upon which domain capability is constructed. A lack in general language capability can impede LLMs' capability to understand or generate responses accurately within specialized domains. (2) Flexibility and Adaptability: LLMs that exhibit strong general language capabilities demonstrate enhanced flexibility and adaptability. This is vital for applications requiring comprehension of inputs from diverse domains or the integration of new knowledge without the need for extensive retraining. (3) Understanding and Reasoning: Evaluating general capabilities aids in identifying LLMs' capacity for understanding complex queries, reasoning, and generating coherent responses that are contextually appropriate. These attributes are indispensable for practical applications.\nTo systematically evaluate these general capabilities, we have adapted from (Xu et al., 2023) and outlined dimensions of general capabilities in the following, and we also show example questions and defined difficulty levels in Appendix A.\n\u2022 Semantic Understanding: Crucial for LLMs is understanding the basic meaning of language, and special terms such as idioms and cultural references, to ensure meaningful user interaction. Evaluation involves questions that test understanding across linguistic occasions."}, {"title": "3.3.2 Domain Capability", "content": "The domain capability focuses on LLMs' expertise within certain industries, evaluating its understanding of domain-specific terminologies, concepts, regulations, and operational nuances. It also evaluates LLMs' capability to provide insightful, accurate responses to domain-specific queries.\nThe rationale for evaluating domain-specific capability is (1) Domain-Specific Performance: The principal motive is to ascertain the efficacy of LLMs in logistics-specific applications. It is imperative for LLMs to not merely comprehend general language but also to exhibit a profound understanding of specialized domain knowledge and intricacies. (2) Practical Usability: LLMs endowed with strong logistics domain capabilities hold greater practical value for industry professionals and businesses. Such LLMs are capable of providing more accurate and relevant insights, thereby facilitating enhanced decision-making processes and operational efficiency. (3) Customized Solutions: Evaluating this capability allows for the development of more customized, domain-specific solutions that can address specific challenges and needs within the logistics domain, providing a competitive edge to businesses leveraging these LLMs.\nFocusing on the dimensions of factuality and creativity, we have outlined and illustrated subdimensions of capabilities in logistics domain in the following, and we also show example questions and defined difficulty levels in Appendix B.\n\u2022 Conceptual and Terminological Understanding: Knowledge of specific terms and concepts is fundamental in accurately interpreting and responding to industry-related queries. This necessitates an evaluation that includes questions derived from the logistics domain's lexicon and operational nuances.\n\u2022 Company Information: LLMs should be conversant with key players in the industry and relevant corporate data, reflecting the model's utility as an informative resource. Questions about major companies, their operations, and market positions can evaluate this aspect.\n\u2022 Legal and Policy Knowledge: Given the regulatory environment surrounding logistics operations, LLMs must be adept at navigating legal and policy-related queries. This can be evaluated through questions that require the LLM to reference specific regulations or guidelines applicable to logistics.\n\u2022 Industry Insights: The ability to provide informed opinions or data about the logistics market, trends, and future outlooks showcases LLMs' depth of knowledge and analytical capabilities. Crafting scenarios that ask for analysis or predictions based on current data evaluates this competency.\n\u2022 Company-specific Knowledge: For LLMs deployed by specific companies, such as Huolala, understanding the company's services, history, and strategic vision is"}, {"title": "3.4 Benchmark Dataset Creation", "content": "The primary goal of establishing a high-quality benchmark dataset is to develop an evolving bank of question-answer (QA) pairs for human evaluation. Existing public benchmarks and datasets are not utilized due to (1) the absence of benchmarks tailored to the specific domain; (2) the proprietary benchmark's closer alignment with company-specific needs; and (3) the lack of continuous updates in most public datasets, which fails to meet the timely knowledge requirements for the business application of LLMs. Concurrently, this dataset aims to include a broad spectrum of capabilities outlined in Section 3.3, thereby ensuring a uniform evaluation of LLMs' performance across diverse metrics.\nThe benchmark dataset are developed through the process outlined in following steps:\n1. Accumulation of the Raw Corpus: Guided by the previous components, we systematically collect and compile original texts and corpora from verified sources. This repository of raw corpus forms the groundwork for generating insightful and pertinent QA pairs.\n2. Production of QA Pairs: (1) Development of the Question Plan: Formulate a structured plan outlining the desired number of QA pairs, categorized by various levels of difficulty and capabilities. This plan should be iterative to allow for the continuous enhancement of the dataset. (2) Selection of Question Designers: Identify and appoint individuals responsible for question design, ensuring they possess an in-depth understanding of the evaluation framework and access to the raw corpus. (3) Creation of QA Pairs: The question designers will distill relevant information from the raw corpus or other authoritative sources to craft QA pairs. Each pair will include a question and its corresponding standard answer, for instance, \"Q: How many hours are there in a day? A: 24 hours.\u201d Importantly, the source of information for each QA pair must be documented to maintain traceability and credibility.\n3. Quality Inspection and Database Entry: Following the generation of QA pairs, a thorough quality inspection is conducted to verify that the pairs adhere to predefined criteria. QA pairs that pass this inspection are incorporated into the dataset, whereas those failing to meet the standards are redirected back to the designers for refinement.\nThe process of proposing QA pairs for the benchmark dataset is a critical component in the evaluation of LLMs, particularly within specific domains such as the logistics industry. Through a systematic approach to compiling raw corpora, creating relevant QA pairs, and executing rigorous quality inspections, this initiative seeks to establish a comprehensive dataset. This dataset not only lays the foundation for a thorough evaluation of LLMs but also facilitates the continuous improvement and benchmarking of LLMs, thereby ensuring their relevance and applicability in real-world scenarios."}, {"title": "3.5 Construction of Evaluation Rubrics", "content": "The objective of the construction of evaluation rubrics is to systematically evaluate the performance of various LLMs using the benchmark dataset from Section 3.4. This process is designed to guide the training of human evaluators and ensure more consistent outcomes, both intraperson and interperson.\nThe general evaluation scale is 0-3 points. If a response contains any incorrect information, it will get 0 points. 1-3 points measure the degree of correctness, completeness, creativity etc. Meanwhile, special consideration is given to the timeliness of responses, recognizing the"}, {"title": "3.6 Analysis and Interpretation of Evaluation Outcomes", "content": "After trained human evaluators are ready (see Appendix G for details of human evaluator training process), we randomly draw a subset from the dataset based on agreed difficulty level and quantities in the grading process. To keep integrity, we employ a single-blind procedure, wherein model responses to the QA pairs are anonymized and presented in a randomized order to a panel of at least three human evaluators. This procedure mitigates bias, ensuring that human evaluators cannot infer the origin of the responses, thus maintaining objectivity.\nThe evaluation results are systematically compiled into a four-dimensional table that encapsulates the evaluated capabilities, question numbers, human evaluator IDs, and grades allocated to each LLM for every question like Table 2. This structured data allows for analysis across different dimensions of model performance. Grades for each model within specific capabilities are aggregated and normalized to 100 points, offering a detailed view of model capabilities. Moreover, more comprehensive grading can be achieved through the weighted aggregation of detailed grades, reflecting the LLMs' overall performance spectrum.\nThe grade calculation for model $q$ involves single dimension $j$ grade calculation using the formula $Grade(qj) = \\frac{\\sum_{k=1}^{K_j} \\sum_{i=1}^{I} ASqki}{\\sum_{k=1}^{K_j} \\sum_{i=1}^{I} TSk}$, then aggregating across all dimensions to compute the total grade as $Grade(q) = \\sum_{j=1}^{J}w_jGrade(qj)$. This process evaluates a LLM's capability across various dimensions and overall, by weighing each dimension's grade, equally or otherwise. Reporting includes both the grades for different dimensions and the total grade, facilitating a comprehensive assessment of each LLM's performance. Repeating these steps for all models under evaluation provides a comparative analysis of their strengths and weaknesses.\nOur methodology also aims to reduce the subjective factors of human evaluators as much as possible, incorporating automated dispute analysis and checks for grade stability and reliability. This phase is crucial for identifying low-quality grades and questions and"}, {"title": "3.7 Overall Deployment Structure", "content": "After describing the five main conceptual components of LalaEval, we now turn to the practicalities of deployment, presenting an approach that brings our framework into operational reality, ensuring a comprehensive evaluation process for domain-specific LLMs. This integration strategy, designed to foster standardization, as well as efficiency and adaptability, is succinctly illustrated in Figure 2. Through this streamlined depiction, the operational blueprint that guides the application of LalaEval is presented, facilitating a clear understanding of its modular architecture and the dynamic interactions essential for evaluation. The results presented in Section 4 are also generated using this deployment strategy."}, {"title": "4 Results", "content": "To demonstrate the effectiveness and the practical application of LalaEval, we present the zero-shot evaluation results of various LLMs including OpenAI's GPT-4 (without web access), Baidu's Ernie Bot (with web access), and our proprietary LLM v1, v2, and v3 (PLLM1/2/3). PLLM1/2/3 are iterations fine-tuned from the ChatGLM2-6B foundation, incorporating web access, retrieval-augmented generation, or a combination of both, respectively.\nOur evaluation is demonstrated in two distinct result sets: (1) The accuracy of the LLMs (Table 3), where we present the percentage of responses achieving non-zero grades across different capability dimensions. This metric serves as a direct indicator of each LLM's capability to generate relevant and accurate response. (2) The normalized average grades (Table 4) for each LLM across capability dimensions, providing more quantitative view of LLM performance. The complete results of grades and disagreement by all capability dimensions are included in Appendix H. The results reflect various levels of coverage of capability dimension defined in Section 3.3 where Domain-Factuality is defined as the average"}, {"title": "5 Future Work", "content": "LalaEval may have some potential limitations, including evaluator subjectivity, data selection bias, dynamic changes in domain knowledge, and scalability concerns. To address these challenges and further enhance LalaEval's effectiveness, several avenues for future work can be proposed.\n\u2022 Standardized Training and Assessment: Beyond procedures described in Appendix G, exploring more structured training protocols across domains can further improve the reliability and generalizability of LalaEval.\n\u2022 Enhanced Dataset Representation: Ensuring the benchmark dataset is comprehensive and representative in the domain can minimize biases arising from data selection. This step involves continuous refinement and expansion of dataset sources to cover diverse scenarios.\n\u2022 Adaptability to Dynamic Domains: Given the evolving nature of domain-specific knowledge, continuous updates to evaluation protocols and benchmarks are essential. This adaptive approach helps LalaEval stay relevant and effective in terms of dynamic industry changes.\n\u2022 Automation Integration: Supplementing human evaluation with automated methods and support systems can streamline the evaluation process. Automation can assist in handling large volumes of data and tasks, enhancing scalability while maintaining evaluation reliability."}, {"title": "6 Conclusion", "content": "This study introduces LalaEval, a novel framework for evaluating domain-specific LLMs, with a focus on standardizing human evaluations. By detailing a comprehensive methodology spanning domain specification to results analysis, LalaEval addresses the critical gap in the standardized human evaluations of domain-specific LLMs, exemplified through its application in the logistics domain.\nLalaEval's deployment showcases its capability to illuminate performance differences among LLMs, guiding model selection and development for domain-specific applications. LalaEval not only advances the field of LLM evaluation by establishing end-to-end human evaluation protocols but also emphasizes the importance of aligning LLM capabilities with practical needs.\nLalaEval is also a general human evaluation framework which are disentangled with any specific domain. The results of only logistics domain are reported because LalaEval was developed and finalized when exploring standardized human evaluation framework in this domain. LalaEval has been applied to other domains such as HR/IT maintanance/text-to-SQL/telemarketing and brought business value. It is believed those who are interested in employing LalaEval to evaluate other domain-specific LLMs can easily implement it.\nIn summary, LalaEval represents a significant step forward in the evaluation of domain-specific LLMs, offering a structured and human-centric approach to understanding LLM performance. Its contributions lay the groundwork for future research and application of LLMs across various domains, highlighting the evolving need for evaluation methodologies that are as dynamic and specialized as the LLMs they seek to evaluate."}, {"title": "A Details of General Capability", "content": null}, {"title": "B Details of Logistics Domain Capability", "content": null}, {"title": "CGrading Principle", "content": "C.1 General Grading Principle\nFactual Questions:\n\u2022 If the response contains incorrect information, grade 0.\n\u2022 If the response is correct but less complete than the standard answer, grade 1.\n\u2022 If the response is fully consistent with the standard answer's points, grade 2.\n\u2022 If the response is fully consistent with the standard answer's points and provides additional correct information, grade 3.\nOpen-ended Questions:\n\u2022 If the answer contains incorrect information, grade 0.\n\u2022 In the absence of errors, the answer is judged by evaluators based on depth and breadth compared to the standard answer to grade 1 or 2.\nC.2 Special Consideration\nTimeliness of Responses:\n\u2022 If the response does not specify a clear time but matches the standard answer, it is considered correct.\n\u2022 If the response does not specify a clear time and does not match the standard answer, it is considered a vague answer and judged incorrect.\n\u2022 If the response includes explicit time that is different from the question's time, but that response would be correct from the given time point, it is considered correct."}, {"title": "D Grading Rubrics of General Capability", "content": null}, {"title": "E Grading Rubrics of Logistics Domain Capability", "content": null}, {"title": "F Dispute Analysis and Grade Fluctuation Analysis", "content": "F.1 Dispute Analysis\n1. Evaluator Dispute: automatically identifies potential low-quality grades and evaluators.\n(a) Define the grading dispute decision function $Fikq$: This function takes the value 0 or 1. When the ith grader has a dispute over the grade for the qth model on the kth question (QA pair kq), $Fikq = 1$, otherwise $Fikq = 0$. The criterion for determining a dispute can be a manual rule. For example: if evaluator i's grade > 0(= 0), but all other graders' grades = 0(> 0), it is defined that grader i has a dispute over the QA pair, assigning $Fikq$ as 1, otherwise, it is considered as no dispute, assigned as 0.\n(b) Calculate the dispute level of grade i on dimension j: $C(ij) = \\frac{\\sum_{k=1}^{K_j} \\sum_{q=1}^{Q} Fika}{KQ}$.\nrepresents the number of LLMs evaluated. This signifies the proportion of QA pairs that grade i has disputed grades within that dimension.\n(c) Calculate the overall dispute level of grader i: $C(i) = \\sum_{j=1}^{J}w_jC(ij)$. Here J represents the number of dimensions, wj the weight of dimension j, and $\\sum_{j=1}^{J} W_j = 1$. If no particular dimension is of special interest, all dimensions can be equally weighted.\n(d) Repeat the previous steps to calculate the dispute level for all graders.\n(e) List the total dispute level of all graders, dispute levels in different dimensions, and identify graders with high dispute levels. Review the disputed grades of high-dispute graders, and if a manual review confirms significant issues with their grades, conduct retraining for the disputed graders and consider invalidating their grading results.\n2. Question Dispute: automatically identifies potential low-quality QA pairs.\n(a) Define the question dispute decision function Gkq: This function takes the value 0 or 1. When there is a divergence among graders' grades for the qth model on the kth question (QA pair kq), Gkq = 1, otherwise Gkq = 0. The criterion for determining dispute can be a manual rule. For example: when half of the graders' grades > 0(= 0), and the other graders' grades = 0(> 0), the QA pair is considered to have divergence, assigning Gkq as 1, otherwise it is considered non-dispute, assigned as 0."}]}