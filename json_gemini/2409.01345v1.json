{"title": "Language Models Benefit from Preparation with Elicited Knowledge", "authors": ["Jiacan Yu", "Hannah Y. An", "Lenhart K. Schubert"], "abstract": "The zero-shot chain of thought (CoT) approach[6] is often used in question answering (QA) by language models (LMs) for tasks that require multiple reasoning steps, typically enhanced by the prompt \"Let's think step by step.\" However, some QA tasks hinge more on accessing relevant knowledge than on chaining reasoning steps. We introduce a simple general prompting technique, called PREP, that involves using two instances of LMs: the first (LM1) generates relevant information, and the second (LM2) answers the question based on this information. PREP is designed to be general and independent of the user's domain knowledge, making it applicable across various QA tasks without the need for specialized prompt engineering. To evaluate the effectiveness of our prompting method, we create a dataset of 100 binary-choice questions, derived from an extensive schematic dataset on artifact parts and material composition. These questions ask which of two artifacts is less likely to share materials with another artifact. Such questions probe the LM's knowledge of shared materials in the part structure of different artifacts. We test our method on our dataset and three published commonsense reasoning datasets. The average accuracy of our method is consistently higher than that of all the other tested methods across all the tested datasets.", "sections": [{"title": "Introduction", "content": "Zero-shot Chain of Thought (CoT) [6] has emerged as a popular technique in question answering (QA) by language models (LMs), particularly for tasks that require multiple reasoning steps. This approach leverages the prompt \"Let's think step by step\" to enhance the model's capacity for handling complex reasoning processes. However, not all QA tasks benefit from this method. Kojima et al. [6] show that the zero-shot CoT approach does not offer any performance improvements over the zero-shot baseline on commonsense reasoning datasets, such as CommonsenseQA [9] and StrategyQA [5]. This suggests that many tasks rely on the model's ability to effectively access and utilize relevant knowledge than on step-by-step reasoning."}, {"title": "Related Work", "content": ""}, {"title": "Related Work on Generic User-independent LM Prompting", "content": "The zero-shot Chain of Thought method, introduced by Kojima et al. [6], is a variant of the CoT prompting concept, originally designed for few-shot learning. Zero-shot CoT involves a two-stage process: First, it elicits step-by-step reasoning from the model using a general trigger phrase such as \"Let's think step by step,\" and second, it extracts the final answer from the generated reasoning."}, {"title": "Prompting Methods that Require Task-Specific Design", "content": "Perhaps the method most similar to our own is generated-knowledge prompting [7]. This prompts for knowledge relevant to a multi-choice question, after providing 5 hand-engineered examples of question-knowledge pairs for each of the 4 commonsense domains the question is taken from: NumerSense (focused on numeric attributes, such as numbers of certain parts of an entity, or the freezing temperature of a certain liquid); CSQA (crowd-sourced questions and answers inspired by ConceptNet pairs of triples such as stream AtLocation river, river AtLocation bridge), CSQA2 (also based on ConceptNet-inspired questions, obtained via a QA game); and QASC (grade school science questions, e.g., about using wind to generate electricity). Our method differs in that it does not require engineering of prompts aimed at classes of questions. In a somewhat similar vein, Wang, Deng, & Sun [10] developed a Transformer-based iterative prompter, aimed at multi-hop reasoning, trained on multi-hop datasets obtained by crowdsourcing. Again, our approach is less demanding, not requiring training (or downloading) of a separate prompting LM.\nSome other works in the field make use of more sophisticated reasoning processes to improve their performance, for example, tree of thoughts (ToT) [12] and graph of thoughts (GoT) [3]. These two methods require significant human intervention to design task-specific mechanisms to shape the reasoning process into a tree or graph structure. The ToT framework enhances problem-solving abilities of LMs by structuring the reasoning process as a tree. Each node in this tree represents a partial solution. To apply this method to a task, a user needs to determine a general strategy for decomposing intermediate thought steps based on the task properties. The method also requires a thought generator, generating potential next partial solutions, and a state evaluator, evaluating the effectiveness of the partial solutions. Then search algorithms like breadth-first search"}, {"title": "Our PREP Approach", "content": "Our dual-instance prompting method, called PREP, is a simple technique that uses knowledge elicited from one instance of a LM to PREPare another instance of the LM for a QA task:\nStep 1. Knowledge Elicitation: We present the question to the first instance of the LM (LM1) and ask it to provide information relevant to answering the question.\nStep 2. Knowledge Transfer: The response from LM1 is copied and sent to the second instance of the LM (LM2) along with the question. LM2 now answers the question.\nThis method is designed to be both general and user-knowledge independent:\nGenerality: Our method is versatile and can be applied to a wide range of QA domains and tasks.\nUser-Knowledge Independence: Our method does not require further prompt engineering, training, or other customizations that require the user's knowledge of the task domain."}, {"title": "Experimental Settings", "content": ""}, {"title": "Dataset Creation", "content": "To evaluate the effectiveness of our prompting method, we designed a nontrivial QA task, asking LMs binary-choice questions about shared materials between objects. This task challenges LMs to consider detailed information about the possible materials that objects can be made of. To construct our dataset, we began with an extensive schematic dataset detailing the parts and material composition of primarily man-made objects. From this dataset, we identified triples of objects (\u041e\u0410, \u041e\u0432, \u041e\u0441) where objects Os and Ob share a common material, while object Oc does not share any material with OB. We then manually selected 100 triples from this set and corrected any inaccuracies to generate a curated set of test questions for our new dataset. The questions (Q) are formatted as follows:\nNormally, which of the following is less likely to be at least partially\nmade of a material that is a constituent of OB?\na) OA b) Oc\nThis phrasing is preferable to asking which artifact is more likely to share materials with the target object, as it avoids the connotation that the amount of shared material is quantitatively significant for the correct answer. To keep the test fair and avoid any bias, we ensured there were an equal number of questions with the correct answers being 'a)' and 'b)'."}, {"title": "Experiments", "content": "We evaluate our PREP method by comparing it to various prompting techniques. The comparison includes methods with different levels of reliance on user knowledge,"}, {"title": "Prompt Details", "content": "The exact prompts tested are listed here:\nDual-Instance\nUser's Message to LM1:\nConsider the following binary-choice problem:\n(Insert the question Q)\nPlease list specific facts that seem most relevant to answering the\nquestion. Do not answer the question, and do not include anything\nother than the list in your response.\nUser's Message to LM2:\nHere are some facts that are relevant to the question I will ask you:\n(Insert the response of LM1)\nHere is the question:\n(Insert the question Q)"}, {"title": "Results and Discussion", "content": "The average accuarcy of our dual-instance method is consistently higher than or equal to that of all other tested methods across all the tested datasets. Particularly, the average accuracy of our method surpasses zero-shot CoT by approximately 5% on CSQA, StrategyQA, and OpenBookQA. When comparing our method with the experiment 1 inst. info. copied, we observe that the repetition of information in the context negatively impacts accuracy. This demonstrates the necessity of creating another instance of LM. Furthermore, when comparing our method with the last two single-instance methods listed in the tables, we find that although all these methods aim to provide similar contextual information to assist the LMs in answering the questions, the accuracy improves when the relevant information is provided by the user rather than the model,"}, {"title": "Conclusions", "content": "In this study, we introduced a dual-instance prompting method, PREP, aimed at improving the performance of LMs on QA tasks that require accessing relevant knowledge rather than following step-by-step reasoning. By employing two instances of LMs, one for information elicitation and the other for answering the question based on the elicited information, we demonstrate a notable improvement in accuracy compared to direct questioning and zero-shot CoT approaches across different models. Our method is general and user-independent, applicable across various QA tasks without the need for specialized prompt engineering. Experimental results show that our dual-instance methods outperform single-instance methods. This underscores the value of (ostensibly) user-provided context in enhancing LM performance."}]}