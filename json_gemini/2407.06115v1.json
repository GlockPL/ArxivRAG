{"title": "Infer Induced Sentiment of Comment Response to Video: A New Task, Dataset and Baseline", "authors": ["Qi Jia", "Baoyu Fan", "Cong Xu", "Lu Liu", "Liang Jin", "Guoguang Du", "Zhenhua Guo", "Yaqian Zhao", "Xuanjing Huang", "Rengang Li"], "abstract": "Existing video multi-modal sentiment analysis mainly focuses on the sentiment expression of people within the video, yet often neglects the induced sentiment of viewers while watching the videos. Induced sentiment of viewers is essential for inferring the public response to videos, has broad application in analyzing public societal sentiment, effectiveness of advertising and other areas. The micro videos and the related comments provide a rich application scenario for viewers' induced sentiment analysis. In light of this, we introduces a novel research task, Multi-modal Sentiment Analysis for Comment Response of Video Induced(MSA-CRVI), aims to inferring opinions and emotions according to the comments response to micro video. Meanwhile, we manually annotate a dataset named Comment Sentiment toward to Micro Video (CSMV) to support this research. It is the largest video multi-modal sentiment dataset in terms of scale and video duration to our knowledge, containing 107, 267 comments and 8, 210 micro videos with a video duration of 68.83 hours. To infer the induced sentiment of comment should leverage the video content, so we propose the Video Content-aware Comment Sentiment Analysis (VC-CSA) method as baseline to address the challenges inherent in this new task. Extensive experiments demonstrate that our method is showing significant improvements over other established baselines. We make the dataset and source code publicly available at https://github.com/AnonymousUserabc/ MVI-MSA_DateAndSource.git.", "sections": [{"title": "1 Introduction", "content": "Video multi-modal sentiment analysis, a captivating and challenging research field, has exhibited rapid advancements with a variety of benchmarks proposed in recent years [28, 4, 19, 45, 8, 31]. These benchmarks aim to understand the opinions or emotions of speakers in monologues or dialogues, as depicted in Fig. 1a. They consider the combined input from visual, audio, and subtitle text at the utterance level, which maintain the same semantic(\"It is absolutely wonderful, I fell in love with it from the very first time I saw it and used it.\"). Current methods infer the speaker's opinions or emotions by examining elements such as sequential images (e.g., facial expressions, smiles, gazes), audio cues (e.g., tones, pauses, pitch), and transcribed text from spoken words [13].\nNevertheless, current research has primarily centered on the sentiments of the people in the video, paying less attention to viewers' induced sentiment while watching the video. People create and upload micro videos, and viewers contribute comments as responses to the micro video [5]. These\nConsidering this, we introduce a new task termed Multi-modal Sentiment Analysis for Comment Response of Video Induced(MSA-CRVI). This task focuses on understanding the induced sentiment of the video, as conveyed through viewers' comments. MSA-CRVI incorporates both the textual comment and the associated video as inputs. Unlike existing video multi-modal sentiment analysis, MSA-CRVI task presents unique challenges within this innovative paradigm. Firstly, it is challenging to ground the associated video content with each comment. A single video yields a multitude comments that emphasize diverse aspects, necessitating grounding the relevant video contents for each comment's sentiment analysis. Since comments are responses to the video rather than mere textual descriptions, it becomes challenging to directly grounding the video content with the comment. Secondly, it is challenging to model the correlation between comments and their corresponding micro videos due to its temporal complexity. Comments could focus on different temporal-granularity content within the video. Meanwhile, comment may be grounding multiple segment across various video timeline. This implies the necessity for carefully encoding video temporal features and precisely processing the grounding video information.\nWe have developed a dataset to support the MSA-CRVI task, called Comment Sentiment toward Micro Video (CSMV), collected from TikTok, a popular micro video social media platform. CSMV comprises micro videos and associated comments, each of which is annotated for opinions and emotions. Furthermore, we propose a strong baseline method, named Video Content-aware Comment Sentiment Analysis (VC-CSA) to address these challenges by designing three key modules: Multi- scale Temporal Representation, Consensus Semantic Learning and Golden Feature Grounding. Comprehensive experiments have validated that our method significantly outperforms established baselines. The data and source code are released at https://github.com/AnonymousUserabc/ MVI-MSA_DateAndSource.git.\nOur main contributions including (1) We introduce the MSA-CRVI task with a novel setting in multi-modal sentiment analysis. This task involves inferring the induced sentiment according to the comments toward micro-video. (2) To support this task, we have created a dataset named CSMV, comprising manually annotated opinions/emotions on comments and related videos. To our knowledge, CSMV is the largest dataset of its kind in terms of scale and video duration. (3) As an initial exploration of the task, we present the VC-CSA method, which focus on understanding the correlation between comments and micro-videos to infer the opinions and emotions induced by the videos. (4) The extensive experiments demonstrated that VC-CSA outperforms other state-of-the-art multi-modal sentiment analysis methods on the CSMV dataset. We also highlight the critical role of video in the MSA-CRVI task."}, {"title": "2 Related work", "content": "Multi-modal sentiment analysis has gained substantial attention and driven the rapid expansion of multi-modal applications. Over the years, a variety of multi-modal datasets have emerged, typically categorized based on the presentation of videos. One category comprises datasets featur- ing monologue-style videos, such as MOUD [28], OMG-Emotion [4], CH-SIMS [43, 19], CMU- MOSEI [45] and so on. Another category encompasses dialogue-style video datasets like IEMO- CAP [8], MELD [31], often derived from movies and TV shows. Existing multi-modal sentiment analysis datasets impose stringent presentation constraints, but micro-videos is much more diverse in content and format than monologue and dialogue.\nMany approaches have been proposed for the multi-modal sentiment analysis. Md Shad Akhtar et al. [1] introduced a context-level inter-modal attention framework aiming to infer the sentiment expressed by the speaker utterance. Delbrouck et al. [11] proposed a transformer-based joint-encoding method employing cross-modal attention mechanisms to capture inter-modality interactions. Their experimental verification revealed the limited role of the visual modality in reasoning within existing multi-modal sentiment analysis. Subsequent studies, including MMIM [15], Self-MM [44], and MISA [16], further support this perspective. Obviously, researchers focus on fusing signals from varied modalities to extract complementary information sharing the same semantic meaning. However, these relevant approaches are confined by the current setting which is same as the constraints of the benchmark. Different from the prior research, VI-MSA task in this paper provides a more complex analysis situation between the video and the textual comment. Additionally, the video themes exhibit greater diversity and a freer style.\nInduced emotion analysis, distinct from perceiving emotion conveyed by content creators, pertains to analyzing emotional reactions induced from content consumers [17, 38]. Presently, there is a growing interest in comprehending the patterns of emotion induced by video [6], since it has a wide range of applications in various perspectives [37, 35, 27, 3]. Currently, researchers mainly investigate induced emotion of viewer responses to movies (e.g., DEAP [18], COGNIMUSE [46], LIRIS-ACCEDE [6]). They capture viewers' physiological features, like EEG and facial videos, to facilitate the induced emotion analysis [18, 33]. Typically, these datasets offer continuous numerical labels for arousal and valence. Due to these characteristics, theses dataset is expensive in construction cost and severely restricted for application. In comparison, micro videos are largely created in a freestyle, and the related comments are often easy to obtain and directly reflect induced sentiment.\nSeveral attempts have been made to infer the induced emotions of videos. Benini et al. [7] argued that similar connotations in movie scenes could evoke identical emotional responses. They proposed a method to develop a construct for affective description for movies based on their connotative properties. Tian et al. [38] emphasized the difference between perceived and induced emotions in the viewer. They employed an LSTM-based model to recognize induced emotions from the viewers' physiological features, showcasing the effect of integrating multiple modalities, including external information like affective cues in movies. Muszy\u0144ski et al. [25] further investigated the correlation between dialogue and aesthetic features in inducing emotions in movies. They introduced an innovative multi-modal model for predicting induced emotions. Liu et al. [21] employed EEG signals to real-time infer induced emotions in audiences while watching movies. Their study centered on the widely used LIRIS-ACCEDE database [6] in recent research on induced emotions in movies. These studies aimed to advance the movie art research and aid filmmakers in creating emotionally engaging content, where features beyond video are employed to infer induced emotions. These"}, {"title": "3 Dataset", "content": "TikTok is one of the most popular micro video social media platforms. Users spontaneously create micro videos and contribute related comments as responses on TikTok. These videos encompass diverse topics (e.g., sports, politics, technology), reflecting human experiences, thereby provide a substantial amount of valuable data for our proposed task MSA-CRVI. The metadata of micro videos on TikTok includes hashtags denoting video topics and the number of likes on comments, facilitating raw data processing [5].\nWe employ hashtags to collect raw data from Tiktok. Hashtags are formed spontaneously by users creating micro videos, reflect current trends on social media platforms. To enhance data diversity, we set many hashtags with different topics and no restrictions on micro video representation format. A set of hashtags encompassing diverse topics like policy, business, sports, and technology is manually selected. For ensuring the quality of micro videos and comments, micro videos with less than 1,000 comments are excluded. Then, we sort comments for each micro video based on the number of likes and select the top 20 English comments for annotation. Furthermore, a series of pre-processing steps is undertaken to prevent personal information leakage. Initially, we delete the metadata about the creators of micro videos and comments. Subsequently, any personal information within textual comments (e.g., usernames, emails, phone numbers) is removed. Lastly, instead of the raw video data, micro video features generated via the pre-trained visual model I3D [9] are published. The same features will serve to evaluate our proposed method.\nWe employed 30 human annotators to manually label comments, defining two distinct types of labels: opinion and emotion. The opinion label indicates the user's attitude towards the micro video in comment. This can encompass agreement with or expression of feelings towards the video, ranging from positive, negative, to neutral. Specifically, the neutral label signifies an absence of clear opinion or views unrelated to the video. The emotion label illustrates the emotional reaction in a comment evoked by the micro video. We employ the Plutchik wheel [29] to define eight categories: joy, disgust, surprise, sadness, trust, fear, anger, and anticipation. These categories encompass a wide range of emotional directions, each illuminated into three levels from mild to intense, effectively capturing human emotional expressions."}, {"title": "4 Method", "content": "To infer the comment's induced sentiment toward to related micro video, we propose a novel method called Video Content-aware Comment Sentiment Analysis (VC-CSA). It takes a comment and the related micro video as input to infer the opinion and emotion toward to the video which expressed through the comment. Fig. 2 is the architecture of the framework. The visual encoder is a video pre-trained model I3D [9], which encodes the micro video into a set of vector representations as original temporal visual features input. The comment text is encoded by a RoBERTa [20] language pre-trained model to extract text features from the comment. Our proposed method consists of three principal modules: Multi-scale Temporal Representation, Consensus Semantic Learning, and Golden Feature Grounding. We integrate the multi-scale video golden feature with the textual comment with a fusion module and utilize a Softmax classifier to infer opinions and emotions. For training, we apply cross-entropy loss to each classification head and aggregate these losses for optimization.\nEach video has the potential to provoke a multitude of comments from viewers. These comments may address specific segments or the entirety of the video story. For instance, in a video where a dog chases a cat and ultimately collides with a door, one comment, 'That hurts,' refers to the end of video. In contrast, another comment, 'Dogs do not like cats, reflect the general theme of the video. Consequently, it is crucial to encode the semantic features of the video across various temporal scales to facilitate the correlation between the video and comments spanning different different time ranges.\nThe Multi-scale Temporal Representation module is design to capture the visual features from the video in various temporal scales. This is accomplished by stacking multiple 1D Convolutional Neural Networks (CNNs) and employing the ReLU activation function between layers. Each 1D-CNN operates with a kernel size of three and a stride of one, traversing the temporal dimension of the video's visual features input. As the number of layers increases, the network progressively expand broader temporal contexts within the video's visual features. Consequently, we obtain hierarchy of multi-scale temporal representations, denoted as ${f_i \\in \\mathbb{R}^{v_i \\times d_v}}$ of the video visual information, capturing a spectrum from finer to coarser granularities, where $v_i$ represents the length of the video, $d_v$ denotes the dimension of the visual representations, and i indexes the layers.. This approach is instrumental in analyzing the video's contextual representation difference over various time spans.\nThe comments, being responses to a video, do not describe the video content directly, creating a semantic gap between the video and the comments. This distinction hinder directly grounding\nrelevant video content based on the comment text. To address this challenge, we introduce Consensus Semantic Learning, a module designed to deeply model semantic correlation between the video and the comments, facilitating more effective video content grounding.\nThe foundation for effectively grounding video content lies in construct a well-defined query to bridge the semantic gap between the comments and videos. Therefore, we introduce a video-comment consensus transformer to capture the shared semantic occurrences between comment and video. As illustrated in Fig. 2, this transformer is a special structure derived from transformer encoder block. It processes video feature $f_i$, text feature $f_t$, and several trainable Consensus Tokens as input.\nConsensus token representations $f_{con} \\in \\mathbb{R}^{u \\times d'}$, are randomly initialized and become trainable in training phase, $d'$ denotes the dimension of the consensus transformer. With in the construction, the attention connection between the video and text features are masked, allowing for information exchange solely through the Consensus Token. Consequently, the consensus tokens serve as mediums, sharing the semantic between the video and the comment. We take the consensus tokens representation output $F_{con}$ in Eq. 1 as a consensus feature to reduce the semantic gap like a bridge.\nIn Eq. 1, $f_t \\in \\mathbb{R}^{l_t \\times d_t}$ denotes the comment text feature, where $l_t$ is the length of text, $d_t$ is the dimension. Importantly, the consensus transformer block is parameter-independent for each temporal scale of {$f_i$}. This methodology is applied across all multi-scale temporal features.\nTo accurately interpret the sentiment of comment related to a video, it is important to ground the video content referenced by the comment. Given the temporal nature of video, continuous frames often exhibit high similarity, leading to redundant information during the grounding phase. To control this, We design Golden Feature Grounding module, which comprises a two-steps approach to compute grounding weight. In the first-order grounding, we employ a multi-head attention mechanism. This mechanism utilize the consensus token representation $F_{con}$ as the query, the video visual feature $f_i$ as the key and the value. The process of calculation is illustrated as Eq. 2:\nHere, the attention score $S_{con Att} \\in \\mathbb{R}^{head \\times v_i}$ reflects the comment attention across the video temporal, with head representing the number of attention heads.\nViewers' attention to video content varies over time, often focusing on multiple segments simul- taneously. This attention score $S_{con Att}$ may exhibit smoothness due to the similarity for adjacent temporal segments, resulting from the the temporal nature of video. Consequently, shorter segments could be overshadowed by longer ones if the attention score $S_{con Att}$ is use directly to obtain video information relevant to the comment. Address this, we design a second-order grounding to filter out the redundancies and obtain the golden feature that represents the essence of video relevant to the comment. We take the multi-head attention score $S_{con Att}$ as an indicator of temporal attention trends across distinct vector space. This score is input to a memory module to analyze the trend of attention scores in the temporal direction. The memory module is same as the the cell state of LSTM. Then, the representation is processed through a ReLU function to obtain the global temporal grounding weight $W^g \\in \\mathbb{R}$.\nThis grounding weight is then multiply to the video features $f_i$ to produce the features $f^g \\in \\mathbb{R}^{d_v}$ along the temporal axis, which we regard as the golden features related to the comment. The calculation process is shown as Eq. 4:\nAfter the steps outlined above, we introduce a fusion module designed to integrate video features across various temporal scales into the comment feature, thereby enriching the interaction between comment text and video data. To facilitate this, we employ a multi-view attention mechanism, wherein the comment text token feature denoted as $f_t^i$ as the query, and the video golden features in"}, {"title": "5 Experiments", "content": "We select representative sentiment analysis methods for comparison. Notably, our selection included methods that primarily utilize textual input, such as BERT [12] and RoBERTa [20]. We exclusively trained these models on the comment text from the CSMV dataset, facilitating an evaluation of the micro videos' impact on the MSA-CRVI task. Furthermore, we select several typical traditional multi-modal sentiment analysis methods: TBJE [11], SELF-MM [44], MISA [16], MMIM [15] and CubeMLP [36]. Our method and comparative methods are implemented on the PyTorch platform [26] and trained on 4 Nvidia Tesla V100 GPUs. Video visual features were integrated using the I3D model [9]. For the implementation of our proposed model, we set the hidden dimensions $d_v$, $d_T$ and $d_t$ to 768. To ensure equitable comparisons, we align the training settings (e.g., loss function, batch size, learning rate strategy, etc) with all methods. To evaluate the performance of the models, we randomly split our dataset into training, development (dev), and testing sets using a ratio of 7:1:2. The dev set serves as the basis for selecting the most effective model for each method based on performance outcomes. We follow prevailing evaluation protocols to use F1-score as the primary metrics to measure the performance. Additionally, we calculate mean values from 5 random seeds for each performance metric.\nComparison Analysis. The performance metrics of each method is presented in Tab. 3 individually. It is evident that the VC-CSA achieve the highest scores with a micro-F1 value of 73.52 for opinion recognition and 62.99 for emotion recognition. It exhibits significant advantages over existing multi- modal methods in our proposed task, indicating the limitation of current approaches in addressing the distinctive challenges presented by our research. Meanwhile, she results clearly demonstrate that multi-modal approaches outperform those depending solely on text, underscoring the importance of video content in interpreting sentiments of comments.\nAblation Study. We execute ablation studies on the three principal modules to validate the effective- ness. We adopted standard strategy instead of our custom design to assess performance difference. For the Multi-scale Temporal Representation, we use the only single layer and the only last layer"}, {"title": "6 Conclusion", "content": "In conclusion, this study introduces the task of multi-modal sentiment analysis in comment of video- induced (MSA-CRVI), focusing on understanding sentiment from comments related to micro-video content. To support in this task, we have developed CSMV dataset, consisting of micro videos and their annotated comments. The proposed VC-CSA method effectively infers sentiments from comments within the context of corresponding video, making a significant contribution for the novel multi-modal sentiment analysis setting. Looking forward, we aim to enlarge the dataset and release more feature representations including audio features to further refine sentiment analysis capabilities."}]}