{"title": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for Document-level Machine Translation", "authors": ["Jiaxin GUO", "Yuanchang Luo", "Daimeng Wei", "Ling Zhang", "Zongyao Li", "Hengchao Shang", "Zhiqiang Rao", "Shaojun Li", "Jinlong Yang", "Zhanglin Wu", "Hao Yang"], "abstract": "The field of artificial intelligence has witnessed significant advancements in natural language processing, largely attributed to the capabilities of Large Language Models (LLMs). These models form the backbone of Agents designed to address long-context dependencies, particularly in Document-level Machine Translation (DocMT). DocMT presents unique challenges, with quality, consistency, and fluency being the key metrics for evaluation. Existing approaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise fluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an incremental sentence-level forced decoding strategy to ensure every sentence is translated while enhancing the fluency of adjacent sentences. Our Agent leverages a Doc-Guided Memory, focusing solely on the summary and its translation, which we find to be an efficient approach to maintaining consistency. Through extensive testing across multiple languages and domains, we demonstrate that Sent2Sent++ outperforms other methods in terms of quality, consistency, and fluency. The results indicate that, our approach has achieved significant improvements in metrics such as s-COMET, d-COMET, LTCR-1f, and document-level perplexity (d-ppl). The contributions of this paper include a detailed analysis of current DocMT research, the introduction of the Sent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of its effectiveness across languages and domains.", "sections": [{"title": "Introduction", "content": "In the domain of artificial intelligence, Large Language Models (LLMs) like GPT, LLaMa and Qwen play a crucial role in advancements in natural language processing (OpenAI, 2023; Touvron et al., 2023a,b; Bai et al., 2023; Yang et al., 2024). LLMs are typically used as a foundation to construct Agents that can unleash their capabilities. In an Agent, Memory (Lyu et al., 2024) is a key component for addressing long-context dependencies.\nDocument-level Machine Translation (DocMT) (Kim et al., 2019; Maruf et al., 2022; Fernandes et al., 2021) is a very challenging task in the traditional field of machine translation. Thanks to the development of LLMs, DocMT has been receiving growing focus in recent years. Quality, consistency, and fluency are the three core metrics for evaluating DocMT. Recently, some LLMs-based DocMT (Cui et al., 2024; Wu and Hu, 2023; Wu et al., 2024a; Wang et al., 2017; Tan et al., 2021; Lyu et al., 2021) research mainly focuses on constructing DocMT Agents. Typically, these studies use a Memory component to maintain the consistency of text translation across the entire document.\nThe decoding strategies of these DocMT Agent research typically fall into two categories: Doc2Doc and Doc2Sent. TransAgent (Wu and Hu, 2023) addresses the translation of ultra-long texts through Multi-Agent cooperation, often employing a Doc2Doc decoding approach with the aim of achieving smoother translations.However, this method may result in the omission of certain sentences, which can compromise the completeness of the overall translation (Karpinska and Iyyer, 2023). While the omission of individual sentences might be tolerable for novel translations, it is unacceptable for serious literature or news reports, where omissions can be detrimental. To tackle the issue of sentence omissions, Delta (Wang et al., 2017) and IncreD (Lyu et al., 2021) have turned to a Doc2Sent decoding framework. These frameworks translate the document by breaking it down into individual sentences, ensuring that each sentence is translated with context information in Memory. Nonetheless, this approach may compromise fluency. The question is: Is there a strategy that can both avoid sentence omissions and maintain the fluency of the document?\nIn this paper, we propose an Agent named Doc-"}, {"title": "Motivation", "content": null}, {"title": "Current LLM-based DocMT", "content": "Doc2Doc Agent In the field of DocMT, the Doc2Doc method is a direct approach that translates entire documents from the source language to the target language. As shown in Figure 1(a), this method maintains the integrity and consistency of the document by incorporating additional information or steps, while also addressing the complex linguistic and cultural nuances within the document.\nSuch as TransAgent (Wu et al., 2024b) introduces a multi-agent framework based on LLMs for literary translation, implemented through a virtual translation company named TransAgents. TransAgents emulates the traditional translation and publishing process, harnessing the collective expertise of multiple agents to meet the complex demands of translating literary works. The essence of this framework is the collaboration within the multi-agent system, which includes senior editors, junior editors, translators, localization experts, and proofreaders, all working together to ensure that the translation maintains high quality and consistency throughout the document."}, {"title": "Which method is more suitable for DocMT?", "content": "As shown in Table 1, the Doc2Doc Agent capitalizes on the LLM's advanced long-context understanding and generation capabilities to translate entire documents directly from the source to the target language. While this method preserves document consistency and fluency, it occasionally omits paragraphs and sentences. Such omissions are acceptable for entertainment texts, like novels, but can be problematic for serious literary works or news reports. Furthermore, the decoding time is relatively high due to the inclusion of extensive long memory information and lengthy document content. In contrast, the Doc2Sent Agent decomposes document-level translation into sentence-level tasks, using large language models to extract comprehensive information from the document, thus maintaining translation consistency and accuracy. Moreover, this approach ensures no sentence-level omissions. However, aggregating individual sentence translations into a discourse-level translation can diminish sentence fluency. Frequent updates to document memory information and the concatenation of sentence translations result in a considerable increase in the number of calls to large models.\nTherefore, we propose a novel method for LLM-based DocMT a Doc-Guided Sent2Sent++ Agent. This agent not only ensures consistency in document translation but also enhances fluency and guarantees no sentence omissions, offering superior translation efficiency compared to both Doc2Doc and Doc2Sent Agents."}, {"title": "Approach", "content": "In this paper, we present an Agent titled Doc-Guided Sent2Sent++, or Sent2Sent++ for short, crafted to tackle the complexities of Document-level Machine Translation, see Figure 2. Sent2Sent++ utilizes an incremental sentence-level forced decoding strategy, wherein we decode two adjacent sentences in tandem, yet incrementally process only the latter; the prior sentence's translation, generated in the previous round, acts as a prefix in the current decoding phase. This approach not only guarantees the translation of every sentence but also enhances the fluency between adjacent sentences, thereby improving the overall document translation fluency. Moreover, aligning with other studies, our Agent incorporates a Memory component. Notably, we term this Memory as"}, {"title": "Sent2Sent++", "content": "We split the input document into sentences:\nSdoc = {Ssent1, Ssent2, Ssent3, \u2026\u2026, Ssentn }, Transform document translations to sentence-level translations. Sent2Sent++ forced decoding strategy modifies the input and output processes of sentence-level translation. Specifically, the input is expanded to include two sentences Ssenti\u22121 and Ssenti, with the output of the preceding sentence Tsenti\u22121 acting as the mandatory decoding segment for the subsequent process. satisfying the following formula:\nTsenti = LLM(Tsenti-1|Ssenti-1, Ssenti) (1)\nAs a result, the final translation, formed by concatenating individual sentence translations: Tdoc =\n(Tsent1, Tsent2, Tsent3,\u2026\u2026,Tsentn), inherently exhibits greater fluency. The distinction between the forced decoding strategy in Sent2Sent++ and the storage of context information in History Memory in Doc2Sent is illustrated in Figure 3.\nFurthermore, the integrated Doc-Guided Memory component maintains output consistency, allowing forced decoding to rely on only one sentence to enhance fluency and markedly improve efficiency."}, {"title": "Doc-Guided Memory", "content": "Our Doc-Guided Memory is primarily divided into two components. Initially, it leverages the abstractive capabilities of large models to generate a summary of the document: Ssummary =\nLLM(Sdoc), referred to as the summary component. This component encapsulates the general information, terminological details, and stylistic elements of the document. Subsequently, this summary is translated into the target language Tsummary = LLM(Ssummary), forming what we call the doc2doc component: M =\n(Ssummary, Tsummary). This bilingual summary continuously guides the sentence-level translation throughout the Sent2Sent++ process.\nOnce the Doc-Guided Memory is integrated, the generation process for the sentence ith in the translated document can be articulated as follows:\nTsenti = LLM(Tsenti\u22121|Ssenti\u22121, Ssenti, M) (2)"}, {"title": "Metrics", "content": "In the domain of document translation, quality, consistency, and fluency are the three pivotal criteria for assessing the performance of translation systems. Consequently, this paper introduces automatic evaluation metrics focused on these three core criteria.\nQuality Metric Quality primarily indicates the accuracy of the translated text, hence we assess it using established machine translation evaluation metrics such as s-COMET, d-Bleu and d-COMET (Vernikos et al., 2022). We utilize the model Unbabel/wmt22-comet-das to obtain the s-COMET scores and Unbabel/wmt21-comet-mqm6 obtain the d-COMET scores.\nConsistency Metric Consistency primarily indicates the terminological coherence and completeness in document translation, thus we evaluate it using LTCR-1 f (Wang et al., 2024b).\nFluency Metric Fluency, which denotes the naturalness and smoothness of the generated translation text, is assessed by calculating the perplexity (ppl) of the translation with the Llama-3.1-8B7."}, {"title": "Methods", "content": "We include the following approaches as our Methods. It is important to note that all methods utilize the same LLMs and the same parameter settings.\n\u2022 Sentence: We utilize LLMs for a sentence-level translation process to establish baseline results.\n\u2022 Doc2Doc: Employing a Large Language Model (LLM), text translation results are derived directly from the original document. Since the test data consists of paragraphs rather than complete texts, using the LLM for paragraph translation already yields high consistency, hence, we have not specifically added a Memory component here.\n\u2022 Doc2Sent: The document is segmented into sentence-level units, information is appended to each sentence, and a Large Language Model (LLM) is used to translate these units,"}, {"title": "Results", "content": "All results indicate that compared to the Doc2Doc method, our approach has fewer omissions and a higher d-BLEU score; compared to the Doc2Sent method, our approach offers better fluency and a lower d-ppl value."}, {"title": "The results for the Multi-domain test sets", "content": "Table 2 presents the results of Chinese-to-English and English-to-Chinese document translation on the IWSLT17 and GUOFENG test sets. To assess the quality of the document translation, we utilized five automatic evaluation metrics.\nIn terms of consistency, the Sentence method exhibits a notable deficit compared to the other three methods, with a difference of 4-5 in LTCR-1f values. This discrepancy is expected, as independent sentence translation in discourse naturally results in contextual and terminological inconsistencies. The LTCR-1 f values for the Doc2Sent method and the Doc-Guided Sent2Sent++ method closely align with those of the Doc2Doc method, When memory information is removed, the LTCR-1f values decrease significantly. it shows that incorporating discourse information during sentence-level translation with LLMs can effectively maintain discourse consistency.\nWhile ensuring discourse consistency, the Doc2Sent method, fundamentally a sentence-level translation method, exhibits a considerably d-ppl value compared to the Doc2Doc method, aligning with the level of the Sentence method. This represents a significant limitation of the"}, {"title": "Ablation Study", "content": null}, {"title": "Why our Doc-Guided Memory only use summary information?", "content": "In extended experiments, we compared the method that incorporates both summaries and term information in Memory following Wu et al. (2024b) with our approach the Doc-Guided Sent2Sent++ Agent. Our method, by capturing the full-text summary, has already captured most of the terminology and stylistic nuances."}, {"title": "Do we need more context in the forced decoding of Sent2Sent++?", "content": "In the decoding process of our Sent2Sent++, we only used the translation of the previous sentence for forced decoding. What if we use the translations of the first two sentences or more?\nAs shown in Table 5, we found no significant differences in various metrics as the number of preceding sentences increased. Therefore, considering efficiency, we only used the previous sentence."}, {"title": "Related Work", "content": "Many works have been published on the topic of document-level NMT. The widely used baseline approach consists of simply concatenating a few adjacent sentences and feeding this as an input to the MT system, without modifying the system architecture in any way (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Agrawal et al., 2018; Nguyen et al., 2021). Also, several modifications to this baseline concatenation approach have been proposed. (Ma et al., 2020) introduce segment embeddings and also partially constrain the attention to the tokens of the current sentence. (Zhang et al., 2020) propose to calculate the self-attention both on the sentence- and on the document-level and then combine the two representations. (Fernandes et al., 2021) and (Yang et al., 2023) both mask out tokens in the current sentence to increase context utilization while (Lei et al., 2022) remove tokens from the context if they are not attended.\nBeyond simple concatenation methods, there exist alternative document-level Neural Machine Translation (NMT) approaches. Sequential De-"}, {"title": "Conclusion", "content": "In this paper, we have presented Doc-Guided Sent2Sent++, an innovative approach to Document-level Machine Translation (DocMT) that addresses the challenges of sentence omissions and document fluency. Our Agent, Sent2Sent++, employs an incremental sentence-level forced decoding strategy, which ensures the translation of every sentence while enhancing the fluency of adjacent sentences. Furthermore, we introduced the Doc-Guided Memory mechanism, which leverages abstract bilingual information as key contextual cues. This approach stands out from Doc2Doc and Doc2Sent methods by maintaining both the completeness and fluency of document translations. Our experiments across multiple languages and domains have demonstrated that Sent2Sent++ significantly outperforms existing methods in terms of s-COMET, d-COMET, LTCR-1f, and perplexity (ppl), which are critical metrics for evaluating the quality of machine translation. We believe that our approach will pave the way for future research and development in this area, leading to even more sophisticated and accurate translation models."}, {"title": "Limitation", "content": "The Doc-Guided Sent2Sent++ Agent presented in this paper has demonstrated satisfactory results with respect to quality, consistency, and fluency in discourse translation. Although our experiments compared this method with approaches such as Doc2Doc and Doc2Sent, a direct comparison with commercial models, including GPT-4o, was not feasible. Owing to the decoding strategy of Sent2Sent++, the issue of multiple calls to large models persists. Future work will investigate the potential to harness the advanced capabilities of large models to perform forced decoding of multiple sentences simultaneously, with the goal of further enhancing translation efficiency without compromising quality."}]}