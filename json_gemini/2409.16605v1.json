{"title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications", "authors": ["Ethan Lin", "Zhiyuan Peng", "Yi Fang"], "abstract": "Recent studies have evaluated the creativity/novelty of large language models (LLMs) primarily from a semantic perspective, using benchmarks from cognitive science. However, accessing the novelty in scholarly publications is a largely unexplored area in evaluating LLMs. In this paper, we introduce a scholarly novelty benchmark (SchNovel 1) to evaluate LLMs' ability to assess novelty in scholarly papers. SchNovel consists of 15000 pairs of papers across six fields sampled from the arXiv dataset with publication dates spanning 2 to 10 years apart. In each pair, the more recently published paper is assumed to be more novel. Additionally, we propose RAG-Novelty, which simulates the review process taken by human reviewers by leveraging the retrieval of similar papers to assess novelty. Extensive experiments provide insights into the capabilities of different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms recent baseline models.", "sections": [{"title": "Introduction", "content": "Creativity, traditionally defined as the capability of producing innovative ideas, forming unique connections, and developing solutions that are both novel and effective (Runco and Jaeger, 2012), was thought to be exclusive to humans. With the advent of large language models (LLMs) (OpenAI, 2023; Dubey et al., 2024b), however, this assumption has been challenged as LLMs have demonstrated remarkable proficiency in following human instructions to address a wide range of tasks, such as open-domain question answering, coding, solving math problems, and even utilizing external tools like search engines when necessary (Asai et al., 2024). In a recent study, Si et al. (2024) showed that LLMs can generate research ideas that are more novel than those produced by human experts. While their generative capabilities are impressive, they also raise concerns. For instance, some artists are uncomfortable with LLMs producing artwork that closely mimics established styles, prompting debates about whether LLMs can genuinely be considered creative.\nRecent studies evaluating the generative creativity of LLMs have yielded inconsistent conclusions. Orwig et al. (2024) concluded that GPT-4 (OpenAI, 2023) generates stories that are comparable to those written by humans in terms of creativity. Similarly, P\u00e9pin et al. (2024) found that LLMs can even surpass humans in specific creative tasks, such as divergent association and creative writing. However, Anderson et al. (2024) argued that AI-based creativity support tools (CSTs) like ChatGPT are not yet well-suited to fostering truly original ideas, as they can lead to homogenization of human creativity. Chakrabarty et al. (2024) observed that LLM-generated stories pass the Torrance Test for Creative Writing (TTCW) tests 3 to 10 times less frequently than those written by professionals. Additionally, Chakrabarty et al. (2023) pointed out that LLMs often rely on clich\u00e9s, produce text lacking nuance, and frequently resort to overly moralistic and predictable endings in stories. These discrepancies can be attributed to using different evaluation benchmarks and metrics, highlighting the lack of widely accepted standards for accessing LLM creativity.\nThe evaluation benchmarks used in current studies are primarily derived from cognitive science, such as the Torrance Tests of Creative Thinking (TTCT) (Lissitz and Willhoft, 1985), Alternative Use Task (AUT) (Guilford, 1964), and the Runco Creativity Assessment Battery (rCAB) (Runco, 2011). These benchmarks focus on assessing semantic creativity by tasks like generating responses to pictures or listing as many uses as possible for a common object. Corresponding metrics include flu-"}, {"title": "Related Work", "content": "Traditional general novelty assessment methods use pre-defined metrics like the similarity to existing methods (Just et al., 2024) and the diversity of references (Shibayama et al., 2021) to score the novelty of a method or scholarly paper. To assess LLMs' capability of generating or assessing creativity and novelty, current studies employ different prompt strategies to interact with LLMs and collect responses for evaluation. Guzik et al. (2023) utilized a basic prompt to evaluate GPT-4 on the TTCT benchmark. Mehrotra et al. (2024) applied associative thinking (Mednick, 1962) in prompts designed for specific tasks like product design and marketing. Zhao et al. (2024) analyzed LLMs' responses to an expanded TTCT benchmark, applying diverse prompts, including basic prompts, instructive prompts, post-instructive prompts, and Chain of Thought (CoT) prompts. Stevenson et al. (2022) demonstrates that defining"}, {"title": "LLM Performance Evaluation", "content": "Most existing studies (Summers-Stay et al., 2023; Stevenson et al., 2022; Guzik et al., 2023; Mednick, 1962) evaluate LLM performance on benchmarks (Section 2.1) using human assessments. For example, Guzik et al. (2023) evaluated LLM responses to the TTCT, which were scored by Scholastic Testing Services (STS). Other studies rely on LLMs to score responses from another LLM. Zhao et al. (2024) used a more powerful GPT-4 to evaluate the performance of smaller LLMs, while Lu et al. (2024) utilized ChatGPT to assess responses generated by GPT-4. Additionally, Lu et al. (2024) compared LLM-generated scores with human evaluations, finding that LLM evaluations correlated more closely with the average human score. Both Luchini et al. (2023) and (Organisciak et al., 2023) fine-tuned models on human-scored data to evaluate LLM responses. Since our benchmark provides ground-truth binary labels, evaluation is straightforward."}, {"title": "Scholarly Novelty Benchmark", "content": "Unlike the semantic novelty evaluated by the benchmarks from cognitive science (Section 2.1), novelty in scholarly publications refers to introducing new ideas, methods, or discoveries that have previously not been explored or established in the literature. Evaluating novelty is fundamentally an exercise in understanding the relationship between ideas across time rather than simply assessing new ideas or techniques. This understanding is crucial in determining the contribution of a research paper. The assumption can be made that later works are more novel than prior works, as they typically introduce new ideas and methodologies in the current research climate. In this paper, we apply this assumption to establish ground truth values for our created benchmark SchNovel."}, {"title": "Dataset Collection and Structure", "content": "The arXiv dataset comprises approximately 2.5 million articles, with the earliest dating back to 1986. All articles are categorized into eight distinct fields each of which has some sub-fields. We picked up six out of eight fields: Computer Science (cs), Mathematics (math), Physics (physics), Quantitative Biology (q-bio), Quantitative Finance (q-fin), and Statistics (stat), as we did not collect enough papers in other fields. To assess the ability of LLMs to assess the novelty of research papers, we sampled a subset of articles from each field, denoted as dataset D = {$\\left(f, g, s, x, y, \\text {label}\\right)_{i}\\right\\}_{i=1}^{N}$ where N = 15000, following the procedure outlined in Algorithm 1 in Appendix A.4, where f represent the field, x and y represent the paper ids, s represents the year in which paper x was published, g represents the number of years paper y was published before paper x and label equals to paper x as we assume in the same field, later published paper is more novel."}, {"title": "Tasks and Evaluation Metrics", "content": "We define the task as assessing which paper is more novel when given a pair of papers. Specifically, for each tuple $(f, g, s, x, y, \\text{label})_i$, the selected LLM is provided with the title, abstract, and optional metadata for each paper\u2014information typically available to a reviewer. However, unlike a full review, the model does not have access to the full text, making the task more challenging. While the abstract offers a strong indication of a paper's content and key findings, important details may be missed. By limiting the context to the abstract and metadata, we also improve efficiency in terms of token consumption and cost. We will discuss the potential limitations of this approach in Section 8. Various comparison methods, such as point-wise and pair-wise, can be employed, and we evaluate performance based on accuracy."}, {"title": "RAG-Novelty", "content": "Assessing the novelty in scholarly papers requires the model to have a good understanding of past and present works to accurately judge whether a paper is novel in the current research climate. However, once trained, LLMs are frozen in time, meaning that they are no longer updated with the latest information, so they lack this understanding of the field's current state. Inspired by RAG, we propose a novel method, RAG-Novelty, to further improve LLMs' capability to assess novelty in our benchmark. As shown in Figure 1, apart from the information, like abstract, that can be utilized for a paper, we apply the paper abstract as a query to retrieve top-K papers from the already built index, and then we create a prompt based on the query paper and the retrieved papers to ask LLM score the novelty of query paper from 0 to 10."}, {"title": "Indexing and Retriever", "content": "To assess the novelty of a paper with the information provided by our SchNovel, such as title, abstract, and other metadata excluding the whole paper, an expert human reviewer in the same field may accurately score the novelty, a junior human reviewer, however, is likely not confident of scoring the novelty directly and instead, he/she will first review some similar papers and then assess the novelty. To mimic the review process taken by a human reviewer, we randomly sampled 500 papers from all years from 2000 to 2023, yielding 12000 papers for each field. Then, the abstracts of these papers are encoded into embeddings using OpenAI's text-embedding-3-small model. The retrieve is the exact search method based on cosine similarity, as the number of candidates is very small. Our method can also handle huge candidate corpus by building an approximate nearest neighbor searching index using faiss (Douze et al., 2024; Johnson et al., 2019).\nWhen a human reviewer conducts a literature search, it is naturally impossible to retrieve papers published after the query paper's publication date. To simulate this realistic constraint in our evaluation, we filtered out any papers published after the query paper and retrieved the top-k relevant papers from those published prior to or on the same date. However, in the context of pairwise comparisons, where we are assessing the novelty between two"}, {"title": "Prompt", "content": "We first compared the zero-shot, two-shot, and self-reflection prompts and found that the self-reflection prompt performed the best (Section xx). So, for RAG-Novelty, we built the prompt, shown in Appendix A.6, based on the self-reflection prompt, shown in Appendix A.3, by incorporating the information of the retrieved papers. Specifically, we added a \"Contextual Data Analysis\" instruction that assumes that the more latest papers are re-trieved, the more novel this query paper is:"}, {"title": "Experimental Setup", "content": "Zero-Shot as shown in Appendix A.2, involves providing the model with two research papers' titles, abstracts, and four-step instructions, guiding the LLM to leverage its internal knowledge to make an informed decision. We also conducted the point-"}, {"title": "RAG-Novelty vs. Baseline Models (R7)", "content": "In this experiment, we evaluate the performance of RAG-Novelty against baseline methods. All methods use GPT-4o-mini, and the accuracy is averaged across different start years s and year gaps g. Pairwise comparison is applied to all methods, and we account for position bias by swapping the order of the two papers in the comparisons.\nTwo-Shot does not improve upon Zero-Shot as it typically does in other tasks. We attribute this to the complexity of the novelty assessment task,"}, {"title": "Pointwise vs Pairwise (R1)", "content": "As mentioned in Section 5.1, we revised the pairwise Zero-Shot prompt (Appendix A.2) to a pointwise one. We compared the two methods by evaluating them in the cs field with the start year 2023, crossing different year gaps. As shown in Figure 2, pairwise is consistently much better than pointwise across different year gaps. This significant difference highlights the importance of context. As with human evaluations, providing relevant context or reference points is crucial for accurate assessments (Yan et al., 2022), allowing reviewers to consider the broad implications of a paper within the current research landscape. Pairwise comparisons align with this process, simplifying the task of considering the relative merits of two papers side-by-side rather than evaluating each one in isolation. Thus, pairwise comparisons are used in the rest of the following experiments."}, {"title": "The Impact of Different Fields (R3)", "content": "Different research domains require varying expertise to evaluate novelty. This section examines how different fields influence LLM performance in novelty evaluation.\nIn Figure 3, the cs category shows the highest accuracy across most year gaps (starting in 2023), likely due to the availability of data and well-defined evaluation metrics. In contrast, math and physics show lower accuracy, likely due to domain-specific challenges such as complex notation in mathematics and theoretical frameworks in physics.\nOne explanation is the lack of domain knowledge in ChatGPT's training data, which, being sourced from the internet, may not adequately cover specialized fields. Research has shown that LLMs exhibit biases in various prompts and tasks (Cheng et al., 2023; Stranisci et al., 2023), suggesting potential categorical biases in lesser-known or slower-growing domains. This has significant implications for using AI tools in academia and industry, particularly in automated scoring or ranking systems, where such biases could perpetuate inequalities."}, {"title": "The Impact of Different Start Years and Year Gaps (R4 & R5)", "content": "To better understand how different start years affect the performance of LLMs in evaluating novelty, we investigated the model's results for five distinct start years. As shown in Figure 4, the model's results for all five start years were relatively consistent across different year gaps. This suggests that the model's ability to evaluate novelty between two papers is more dependent on the year gap between them than the specific publication years.\nFor example, evaluating two papers with a 10-year gap from 2009 to 2019 should be equivalent in difficulty to evaluating two papers with a 10-year gap from 2013 to 2023. Regardless of the boundary years within those ranges (i.e., considering papers published at specific points like 2009 and 2019, versus 2013 and 2023), it's the decade-long gap between the papers' publication times that makes it easier for the model to make such a binary evaluation."}, {"title": "The Impact of Different LLMs (R2)", "content": "All LLMs can vary significantly depending on their training data and model architecture. With var-"}, {"title": "The Impact of Metadata (R6)", "content": "Previously, our experiments evaluated novelty based solely on a paper's title and abstract. However, human evaluations often take into account various metadata that can subtly influence reviewers' decisions. This metadata-induced bias has significant implications for research evaluations and highlights the need for more anonymous reviewal processes leading to solutions such as double-blind reviewal processes. In this section, we explore how metadata affects LLMs' ability to evaluate novelty. A pairwise comparison was applied for all the experiments in this section, and we accounted for position bias by swapping the order of the two papers in the comparisons."}, {"title": "Adding a TLDR Summary", "content": "We utilized the SciTLDR model (Cachola et al., 2020) from the Semantic Scholar API (Kinney et al., 2023) to generate TLDRs for our dataset, expecting this additional information to enhance accuracy through the addition of context to help the model generalize and better understand the paper. As shown in Table 3, adding TLDRs decreases the accuracy across all year gaps. Nevertheless, incorporating such data did mitigate position bias, as evidenced by the almost negligible difference between ascending and descending year accuracies across nearly all year gaps."}, {"title": "Adding Author", "content": "We then added the author to the prompt, expecting that this additional information would not affect the model performance as the authors should not influence the novelty assessment. To our surprise, adding such information did help mitigate some of the position bias, as seen in the bold results in Table 3, but overall decreased the performance slightly."}, {"title": "Adding Affiliation", "content": "We selected two universities, one of which is a top research university and the other a teaching university, to study whether affiliation bias exists in LLMs' assessment of novelty. Specifically, we first assigned the top research university as the affiliation of the more recently published paper and the teaching university to the earlier published paper, with the results shown in blue. Then, we swapped the affiliations, and the results are shown in red. As illustrated in Figure 5, the top research university starts with similar accuracy to the teaching university at a year gap of g = 2, but as the year gap increases, the top research university consistently outperforms the teaching university. This suggests that affiliation bias exists in LLMs' novelty assess-"}, {"title": "Conclusion and Future Work", "content": "To evaluate LLMs' ability to assess novelty in scholarly publications, we introduce SchNovel, a benchmark consisting of 15,000 pairs of papers across six fields. We conducted extensive experiments to understand how various factors influence LLM performance on SchNovel. To enhance LLMs' capability in assessing novelty, we propose RAG-Novelty, which significantly outperforms strong baseline models in comprehensive experiments. For future work, we plan to expand SchNovel by including more papers and covering additional fields to evaluate LLMs on a larger scale. Another promising direction is investigating which part of a paper best represents the whole for novelty assessment by LLMs. Additionally, studying how LLMs process affiliation and addressing biases in novelty evaluation, such as position and affiliation bias, is an important area for further research."}, {"title": "Limitations", "content": "Our study evaluates an LLM's ability to assess novelty using a research paper's title, abstract, and metadata. While the abstract provides a strong indication of a paper's content and key findings, it may not fully capture the novelty of the research compared to the complete text. Abstracts often summarize the main ideas but may omit important technical details. Although this approach streamlines the evaluation process, it could occasionally limit the depth of the novelty assessment due to the absence of a more comprehensive context."}, {"title": "SchNovel", "content": "Fields \u2190 [cs, math, physics, qbio, qfin, stat]\nstartYear \u2190 [2019, 2020, 2021, 2022, 2023]\nyearGap \u2190 [2, 4, 6, 8, 10]\nsampleNum \u2190 100\nN \u2190 0\nDataset \u2190 []\nfor f in Fields do\nfor s in startYear do\nfor g in yearGap do\nwhile N \u2260 sampleNum do\nx \u2190 paper published in s from f\ny \u2190 paper published in s-g from f\nlabel \u2190 x\nDataset \u2190 (f, g, s, x, y, label)\nN \u2190 N+1\nend while\nN \u2190 0\nend for\nend for\nend for"}, {"title": "LLM Discussion", "content": "You are a [Role] with expertise across all areas of [Category]. You will be provided with the titles and abstracts of two research papers. Your task is to determine which of the two articles is more novel by evaluating their originality, contribution to the field, and potential impact. Focus on aspects such as new methodologies, unexplored problems, innovative solutions, and how the work advances the state of the art. Follow these steps for evaluation.\nStep 1: Identify the problem and solution that the research paper attempts to solve.\nStep 2: Determine how unique the solution is given the current research landscape in 2024. Does the paper introduce a new idea, theory, or concept that has not been previously discussed in the literature?\nStep 3: Determine how creative the solution is given the current research landscape in 2024. Does it apply a known idea in a completely new context or in a way that has not been done before?\nStep 4: Using the findings from Steps 1-3, determine which paper is more novel.\nPlease limit your response to 150 tokens max. In your response please conclude with: \"The more novel and impactful paper is [Paper X or Paper Y]\""}, {"title": "RAG-Novelty", "content": "You are an advanced language model tasked with determining the novelty of research papers in 2024. Your goal is to evaluate and compare the novelty of two research papers based on their titles and abstracts.\nThe order in which the papers are presented is random and should not influence your evaluation.\nStep 1: Independent Evaluation\nAnalyze each research paper's title and abstract independently. Treat each paper as if it is the only one under review at that moment.\nRetrieve similar abstracts from a vector database based on the provided abstracts.\nContextual Date Analysis: Average the published dates of the retrieved documents. Use this average date as additional context for your evaluation. Consider that papers with an average date that is later or more recent in time are generally more novel.\nConsider the following aspects for each paper:\nNovelty of Methodology: Are the methods used new and innovative?\nSurprisingness of Findings: Are the findings unexpected or counterintuitive?\nImpact on Existing Knowledge: How does the research challenge or expand current scientific understanding?\nPotential for Future Research: Does the paper open up new directions for research?\nRelevance to 2024 Scientific Understanding: How well does the paper align with or push the boundaries of current trends?\nStep 2: Quantitative Assessment\nAssign a score from 1-10 to each research paper for its novelty, with 10 being the most novel. This score should be based on the content of the title and abstract, as well as the contextual information from the average published dates.\nProvide a brief justification for the score, using specific quotes and context.\nStep 3: Final Comparison\nAfter independently scoring each paper, compare the scores.\nDetermine which paper exhibits greater novelty based on the higher score, and conclude with: \"The more novel and impactful paper is [Paper X or Paper Y].\""}]}