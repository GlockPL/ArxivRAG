{"title": "Tailored-LLaMA: Optimizing Few-Shot Learning in Pruned LLaMA Models with Task-Specific Prompts", "authors": ["Danyal Aftaba", "Steven Davy"], "abstract": "Large language models demonstrate impressive proficiency in language understanding and generation. Nonetheless, training these models from scratch, even the least complex billion-parameter variant demands significant computational resources rendering it economically impractical for many organizations. With large language models functioning as general-purpose task solvers, this paper investigates their task-specific fine-tuning. We employ task-specific datasets and prompts to fine-tune two pruned LLAMA models having 5 billion and 4 billion parameters. This process utilizes the pre-trained weights and focuses on a subset of weights using the LORA method. One challenge in fine-tuning the LLaMA model is crafting a precise prompt tailored to the specific task. To address this, we propose a novel approach to fine-tune the LLaMA model under two primary constraints: task specificity and prompt effectiveness. Our approach, Tailored LLaMA initially employs structural pruning to reduce the model sizes from 7B to 5B and 4B parameters. Subsequently, it applies a carefully designed prompt specific to the task and utilizes the LoRA method to accelerate the fine-tuning process. Moreover, fine-tuning a model pruned by 50% for less than one hour restores the mean accuracy of classification tasks to 95.68% at a 20% compression ratio and to 86.54% at a 50% compression ratio through few-shot learning with 50 shots. Our validation of Tailored LLaMA on these two pruned variants demonstrates that even when compressed to 50%, the models maintain over 65% of the baseline model accuracy in few-shot classification and generation tasks. These findings highlight the efficacy of our tailored approach in maintaining high performance with significantly reduced model sizes.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) [31, 36, 40, 41] trained on massive textual data have demonstrated remarkable proficiency in interpreting complex language-based tasks [4, 6, 45] and generating text. Consequently, there is a growing interest in developing large-scale language models such as LLaMA [41], MPT [39], and Falcon [1] that allow for efficient inference and fine-tuning. These LLMs are available in various sizes each suitable for specific tasks. However, training the LLMs from scratch even for the smallest billion-parameter model requires substantial computational resources which is economically unfeasible for most organizations.\nIn this paper, we introduce a novel approach to produce a compressed, task-specific, and efficient LLaMA model [41] by leveraging the pre-trained weights, while having less training cost compared to the one training from scratch. Moreover, we use the structure pruning method to accomplish this objective. Pruning is a widely used method for compressing the task-specific models [17, 21, 22, 24, 47] eliminating redundant parameters to speed up inference while maintaining performance. However, pruning the general purpose LLMs often results in significant performance degradation compared to original models [15, 27, 38], especially in scenarios where minimal computational resources are allocated after pruning. In this work, to expedite the fine-tuning process and increase the efficiency of the pruned model under limited data we employ the Low-Rank Adaptation (LORA) [19] method.\nIn efficiently fine-tuning the pruned LLaMA model, we identify two primary technical challenges. Firstly, how can we optimize the adaptive weights of a pruned LLaMA model for a specialized task like classification, question-answering, and sentiment analysis? Traditional fine-tuning methods for the sparse LLMs [27, 47] depend on datasets designed for multi-tasking approaches. These approaches often result in sub-optimal performance for the specific tasks. Secondly, the selection of appropriate prompts is crucial for attaining optimal performance. Figure 1 shows that employing varied prompts across distinct domains results in inconsistent accuracy levels, whereas training with task-specific prompts consistently yields higher accuracy. This demonstrates that even after reducing LLMs with extensive parameters can efficiently adapt to a particular task when fine-tuned with relevant prompts. Our main contributions are:\n\u2022 We propose a novel fine-tuning algorithm for a pruned LLAMA model dubbed targeted task fine-tuning which finetunes a pruned model to a specified target task\n\u2022 We devise a prompt evaluation strategy that selects prompts based on their impact on the task, which enhances the pruned model accuracy and adaptability. This focused approach along with the LORA method accelerates performance improvement.\n\u2022 We demonstrate the effectiveness of our approach by fine-tuning the LLaMA model across two pruned variants with parameters decreased from 7 billion to 5 billion and 4 billion.\nAlthough our experimental focus was on the 7 billion parameter LLaMA model, the Tailored-LLaMA approach exhibits significant potential for generalizability and adaptability to LLMs of varying sizes having fewer parameters than the baseline models.\nThis paper is organized as follows; Section 2 provides a comprehensive overview of related work in structure pruning and fine-tuning"}, {"title": "2 Related Work", "content": "Network Pruning: Extensive research has focused on structured pruning as a technique for compressing models in Computer Vision and Natural Language Processing (NLP). This approach is particularly useful for over-parameterized task-specific models such as those used for classification that can sustain significant pruning with minimal loss on performance as evidenced by numerous studies [5, 11, 17, 18, 21, 22, 26, 34, 44, 46, 47]. In contrast, unstructured pruning [11, 14, 25, 35] which targets individual neurons rather than entire blocks achieves higher levels of compression but fails to enhance model efficiency making it impractical for accelerating model performance.\nIn the era of LLMs, the prevailing NLP pipeline has transitioned from specialized models to general-purpose LLMs resulting in limited redundancy. Various approaches such as unstructured pruning, semi-structured pruning [15, 38], and structured pruning [27] have shown a notable performance degradation in LLMs even with moderate sparsity. It is important to note that the aforementioned studies either maintain the original model parameters or tune them minimally. In our work, we view pruning as an initial step and emphasize the need to allocate significant computational resources toward post-structural pruning to regain performance levels.\nTransformer language models: The Transformer model [42] is a type of architecture that heavily relies on self-attention for sequence-to-sequence tasks. Subsequently, Transformer-based language models have emerged as the leading approach in NLP achieving top performance across various tasks. The introduction of BERT [12] and GPT-2 [33] further advanced this field as both are large-scale Transformer language models trained on massive textual data. These new approaches involve fine-tuning the models on specific tasks after pre-training them on general text data leading to significant performance improvements compared to training directly on task-specific data. The ongoing research in this area suggests that training larger Transformer models generally yields better results as evidenced by the continuous development in this direction. GPT-3 [4] currently holds the record as the largest single Transformer language model having 175 billion parameters.\nPrompt Engineering: While LLaMA 70B [41] can adjust its behavior with minimal additional training instances, the effectiveness"}, {"title": "3 Method", "content": "In this section, we provide a comprehensive description of Tailored-LLaMA. Following the traditional fine-tuning process of pruned LLM models [19], Tailored-LLaMA consists of three stages:\n1. Structure Pruning. This stage focuses on finding the groups of in-"}, {"title": "3.1 Structure pruning", "content": "In the context of limited data availability for the post-training process of LLMs, it is imperative to remove the structure inside the model that has minimal impact on model performance when compressing it. This highlights the significance of structure pruning which ensures that interconnected parameters are pruned collectively based on their importance scores. Similar to DepGraph [13], the dependency graph is built by computing the inter-dependency between layers present in Multi-Head Attention (MHA) and Feed-Forward Network (FFN) modules. Let \\(P_i\\) and \\(P_j\\) denote two parameters within the model. The terms \\(In(P_i)\\) and \\(Out(P_i)\\) refer to all parameters that respectively point toward or point from \\(P_i\\). The inter-dependency among parameters is defined as shown in Equation (1):\n\\(P_j \\in Out(P_i) \\quad Deg^-(P_j) = 1 \\rightarrow P_j \\text{ is dependent on } P_i\\) (1)\nWhere \\(Deg^-(P_j)\\) denotes the parameter \\(P_j\\) in-degree, it is important to note that this dependency exhibits direction. Hence, we can correspondingly obtain additional dependency as shown in Equation (2):\n\\(P_i \\in In(P_j) \\quad Deg^+(P_i) = 1 \\Rightarrow P_i \\text{ is dependent on } P_j\\) (2)\nThe out-degree of a parameter \\(P_i\\) denoted as \\(Deg^+(P_i)\\) signifies the number of connections leaving \\(P_i\\). The concept of dependency in this context suggests that if a particular parameter such as \\(P_i\\) relies entirely on another parameter \\(P_j\\) and \\(P_i\\) is pruned then \\(P_j\\) will also need to undergo pruning.\nAccording to the dependency definition, the linked structures in the LLM are evaluated automatically. Any parameter located within the LLM can be regarded as the central initiator possessing the ability to trigger parameters that depend on it. Consequently, these newly activated parameters then act as the subsequent initiators to identify their corresponding parameters that are dependent and activate them. This repetitive process persists until no additional parameters are identified. These identified parameters then form a group for further pruning. Taking LLaMA as an example, this approach analyzes each parameter as the central trigger allowing us to identify all interconnected parameters as illustrated in Figure 2.\nTo preserve the accuracy of the model, it is important to simultaneously prune the collection of weights in a group. A group denoted by \\(G = \\{P_i\\}_{i=1}^N\\) is defined as a set of interconnected parameters, where N is the number of coupled structures in one group and \\(P_i\\) is the weight for each structure. During the pruning process, the objective is to eliminate the group that has minimal effect on the model predictive performance. This impact can be quantified by analyzing the deviation in the loss function. To assess the specific importance of \\(P_i\\), the change in the loss function can be formulated as Equation (3):\n\\(I_{P_i} = |\\Delta L| = |L_P - L_{P_i=0}| = |\\frac{\\partial L}{\\partial P_i}P_i + \\frac{1}{2}P_i^T H P_i + O(||P||^3)|\\) (3)\nWhere L represents the prediction loss of the next token and H is the hessian matrix. In prior studies [15, 23, 43] the initial term denoted as \\(\\frac{\\partial L}{\\partial P_i}P_i\\) is often disregarded due to the model convergence on training dataset where the gradient of L concerning \\(P_i\\) is approximately zero. However, as the dataset is not derived from the original training data in this case, the \\(\\frac{\\partial L}{\\partial P_i}P_i\\) is not close to zero. Since the second term hessian matrix cannot be computed with \\(O(N^2)\\) complexity on the LLM, this offers a desired property for determining the significance of \\(P_i\\) by the gradient term under LLMs.\nThe importance of group G is estimated by aggregating the importance scores of each parameter denoted by \\(I_g = \\sum_{i=1}^N I_{P_i}\\). After calculating the importance of each group we proceed to assign a rank to each group according to their importance and then prune those with lower importance by a predetermined pruning ratio."}, {"title": "3.2 Prompt Engineering", "content": "The approach known as reinforcement learning from human feedback (RLHF) [7, 37] uses human preferences as a reward signal to fine-tune the LLaMA model and it was used to follow a wide class of textual instructions just like GPT-4. When LLaMA is given a prompt, it initially converts the input text into tokens that the model can understand. These tokens are then processed by transformer layers, which analyze their relationships and context. Within these layers, attention mechanisms assign distinct weights to tokens based on their importance and context. Following the attention process, the model generates its own interpretations of the input data, referred to as intermediate representations. These representations are later transformed back into readable text.\nAn essential component of this process is the randomness function, which is affected by two key parameters: temperature and top-k sampling. Temperature helps to balance the randomness and predictability of the output. A higher temperature leads to more varied outputs, while a lower temperature results in more predictable outputs. On the other hand, top-k sampling restricts the model choices to the most probable tokens at each stage of output generation. In our approach, we employ the optimal decoding strategy for superior results by using temperature 1 and top-k sampling 50.\nThis work shows the effect of prompt on the accuracy of the LLaMA model. Here, we explore a heuristic strategy observed in human reading behavior when they are giving instruction also known as re-reading [48]. When prompted with instructions that lack specificity for the task, the model produces inferior results compared to those generated with task-specific direction as shown in Table 1. Therefore, providing a specific description is crucial for generating precise and relevant outputs.\nEffective prompting strategies are crucial for guiding LLMs towards generating desired outputs. This involves formulating clear and specific prompts that minimize ambiguity. LLM architectures are typically trained on large amounts of textual data encapsulating the combined information from numerous authors. When confronted with a broad or uninformative prompt, the LLM output tends to be generic, applicable in various contexts but potentially sub-optimal for a specific task. Conversely, a detailed and precise prompt reduces the model uncertainty and aligns it towards the appropriate response, enabling the generation of content that aligns more closely with the unique requirements of the given scenario."}, {"title": "3.3 Recovery Phase", "content": "To recover the accuracy of the model under limited data and expedite the fine-tuning process, it is imperative to select the minimum num-"}, {"title": "4 Experiments and results", "content": "4.1 Dataset and Evaluation\nTo demonstrate the effectiveness of Tailored-LLaMA, we test it over two variants of the pruned LLaMA model having 5 billion and 4 billion parameters. We perform few-shot task classification to evaluate the fine-tuned models using Im-evaluation-harness [16] strategy on common sense reasoning datasets: PIQA [3], HellaSwag [49], BoolQ [8], WinoGrande [34], ARC-easy [9], ARC-challenge [9] and OpenbookQA [30]. Furthermore, we supplement our evaluation with a few-shot perplexity (PPL) analysis on two language modeling datasets WikiText2 [29] and PTB [28].\n4.2 Implementation Details\nDuring the model pruning process 20 samples were arbitrarily chosen from Bookcorpus [51] and reduced to a sequence length of 128 to compute the gradient. For the recovery stage, we employ few-shot learning using task-specific datasets. For instance, the Hellaswag"}, {"title": "4.3 Few-shot performance", "content": "We evaluate the few-shot performance of a fine-tuned LLaMA model on two pruned variants: 6 billion and 5 billion parameters as shown in Table 1. Our analysis demonstrates that fine-tuning the pruned LLaMA model on a task-specific dataset consistently yields better performance compared to training on a composite dataset. For instance, the LLaMA model pruned to a sparsity ratio of 20% achieved an accuracy of 76.33 when fine-tuned on the BoolQ dataset. This performance surpassed its accuracy on other fine-tuning datasets, including PIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, and OBQA. Similarly, fine-tuning the LLaMA model pruned by 50% yields an accuracy of 72.01 on the PIQA task which surpasses its performance on all other datasets used in the fine-tuning process. These patterns are consistently observed across 7 tasks, as indicated by the bold values, which suggest that employing a dataset aligned with the specific task remarkably benefits the pruned model performance. Additionally, in the case of the 50% pruned LLaMA model, the BoolQ prompt achieved an accuracy of 76.17, which is 99.57% of the baseline accuracy of 76.5, thereby surpassing other prompts in restoring performance. Furthermore, the PIQA, HellaSwag, Wino-Grande, ARC-e, ARC-c, and OBQA prompts preserved 90.24%, 81.08%, 95.59%, 81.38%, 77.62% and 74.12% of their original performance. Despite fine-tuning the pruned LLaMA model for less than 1 hour, our Tailored-LLaMA outperforms other prominent fine-tuning methods by achieving a mean recovery rate of 95.68% for compression ratio 20% and 86.54% for 50% post-structural pruning of comparable scales as shown in Table 2. This signifies the feasibility of using the Tailored-LLaMA to effectively fine-tune the LLAMA model within a short period."}, {"title": "4.4 Ablation Study", "content": "We conduct tests on all proposed prompts mentioned in Figure 1. The results can be found in Table 1. To learn the specific representation of each task we conduct an ablation study for various K shots as shown in Table 3. Our findings from Table 3 suggest an upward trend in performance with an increase in sample size indicating that the larger datasets generally enhance model capabilities. However, this"}, {"title": "5 Conclusion", "content": "This paper proposes a novel method for constructing a compressed, task-specific, and efficient LLaMA model by leveraging domain-specific prompts. This approach offers a more cost-effective solution compared to training a LLaMA on a composite dataset. Firstly, we accomplish structure pruning by iteratively analyzing each parameter within the model as a central trigger to construct dependency groups, thereby constructing the LLaMA dependency graph. Subsequently, we evaluate the significance of these groups using parameter-wise estimation. Secondly, we fine-tune the LLaMA model using task-specific datasets and prompts. Lastly, to reduce the recovery time of LLaMA we use the LORA method. We evaluate the efficacy of Tailored-LLaMA on two pruned LLaMA models with capacities of 5 billion and 4 billion parameters, using multiple few-shot datasets. Our experimental results indicate that Tailored-LLaMA outperforms other prominent fine-tuning methods."}]}