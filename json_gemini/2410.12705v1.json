{"title": "WORLDCUISINES: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines", "authors": ["Genta Indra Winata", "Frederikus Hudi", "Patrick Amadeus Irawan", "David Anugraha", "Rifki Afina Putri", "Yutong Wang", "Adam Nohejl", "Ubaidillah Ariq Prathama", "Nedjma Ousidhoum", "Afifa Amriani", "Anar Rzayev", "Anirban Das", "Ashmari Pramodya", "Aulia Adila", "Bryan Wilie", "Candy Olivia Mawalim", "Ching Lam Cheng", "Daud Abolade", "Emmanuele Chersoni", "E. Santus", "Fariz Ikhwantri", "Garry Kuwanto", "Hanyang Zhao", "Haryo Akbarianto Wibowo", "Holy Lovenia", "Jan Christian Blaise Cruz", "Jan Wira Gotama Putra", "Junho Myung", "Lucky Susanto", "Maria Angelica Riera Machin", "Marina Zhukova", "Michael Anugraha", "Muhammad Farid Adilazuarda", "Natasha Santosa", "Peerat Limkonchotiwat", "Raj Dabre", "Rio Alexander Audino", "Samuel Cahyawijaya", "Shi-Xiong Zhang", "Stephanie Yulia Salim", "Yi Zhou", "Yinxuan Gui", "David Ifeoluwa Adelani", "En-Shiun Annie Lee", "Shogo Okada", "Ayu Purwarianti", "Alham Fikri Aji", "Taro Watanabe", "Derry Tanti Wijaya", "Alice Oh", "Chong-Wah Ngo"], "abstract": "Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WORLDCUISINES, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.", "sections": [{"title": "1 Introduction", "content": "Food is an essential medium for the exchange of regional cultures, serving to connect diverse peoples and traditions (Wahlqvist, 2007). Analyzing various culinary practices provides valuable insights into the cultural values, historical narratives, and social customs of the communities that produce and consume these foods (Holtzman, 2006). Furthermore, food plays a significant role in shaping lan-"}, {"title": "2 WORLDCUISINES", "content": "We propose WORLDCUISINES, an open-source benchmark designed to evaluate the cultural relevance and understanding of VLMs. Figure 2 displays VQA examples in English, alongside selected parallel translations in Japanese and French."}, {"title": "2.1 Overview", "content": "We develop both a VQA dataset (WC-VQA) and a curated KB for world cuisines (WC-KB). The WC-VQA dataset is constructed using WC-KB, which serves as the primary data source. We design two tasks as follows:\n\u2022 Task 1: Dish Name Prediction. This task involves predicting the name of a dish based on its image, a question, and contextual information. It comprises three subtasks, each with distinct query types: (a) no-context question, (b) contextualized question, and (c) adversarial contextualized question.\n\u2022 Task 2: Location prediction. The task is to predict location where the food is commonly consumed and originated given the dish image, question, and a context.\nWC-KB. A KB encompassing 2,414 dishes worldwide includes 6,045 images and metadata,"}, {"title": "2.2 WC-KB Construction", "content": "Our data sources are gathered from Wikipedia4 and Wikimedia Commons to ensure they can be easily redistributed under an accepted open-source license. The data construction process involves four key steps: (1) dish selection, (2) metadata annotation, (3) quality assurance, and (4) data compilation."}, {"title": "2.2.1 Dish Selection", "content": "We compile a comprehensive list of dish names sourced from Wikipedia. We manually review pages that feature lists of dishes to determine whether each dish is a specialty unique to a specific culture, as we aim to focus on dishes that have distinct cultural significance. We exclude generic categories, such as ice cream, which lacks a specific cultural association. We ensure that each dish on our list has its own dedicated Wikipedia page. If a dish does not have a Wikipedia page, it is also excluded from our compilation. This meticulous approach ensures that our dataset is both culturally relevant and well-documented."}, {"title": "2.2.2 Metadata Annotation", "content": "Given a dish name and its corresponding Wikipedia page link, we then ask annotators to manually compile metadata based on the provided information."}, {"title": "2.2.3 Quality Assurance", "content": "Before beginning the quality assurance process, we first identify common issues that arise during the annotation and develop automated rules to detect easily identifiable annotation errors, such as incorrect string formatting. Annotators are then asked to correct these errors. To further ensure data quality and validity, we conduct several rounds of quality assurance. Initially, we focus on image quality by removing instances where images are blurry, dark, or contain distracting elements such as people or other dishes. We also verify image licenses by cross-referencing them with information on Wikimedia Commons. Next, we refine dish categorization and descriptions, ensuring consistency in category assignments and maintaining descriptions free from \"information breaches\u201d (e.g., excluding regional details from the description). We standardize cuisine names and eliminate any redundancies. Finally, we meticulously review all country and area information to ensure its accuracy. This comprehensive approach guarantees the integrity and reliability of our dataset."}, {"title": "2.2.4 Data Compilation", "content": "In this phase, we verify the overall quality check done by annotators, and identify any potential inconsistencies that are missed during the quality assurance. Then, we compile the dataset by collecting the metadata into a single file."}, {"title": "2.3 VQA Generation", "content": "In this phase, we generate VQA data by sampling from WC-KB. An entry of VQA data comprises visual image, question text, and answer text. This process involves four stages: (1) conducting a similarity search for dish names, (2) constructing questions and contexts, (3) translating these elements into multiple languages, and (4) generating the VQA triplets."}, {"title": "2.3.1 Dish Names Similarity Search", "content": "To identify similar dishes in our dataset, we follow the approach from Winata et al. (2024) to employ a multilingual model E5LARGE Instruct (Wang et al., 2024) for computing text embedding. Formally, given a dish $x$ with name $x_{name}$ and text description $x_{desc}$, we use a multilingual model $\\Theta$ to compute the embedding vector $v_x = \\Theta({x_{name}; x_{desc}})$, then apply cosine similarity to compute a score $s = similarity(v_i, v_j)$ between dish $i$ and dish $j$. For each dish, we consider the top-k most similar dishes to generate distractors in the multiple choice question."}, {"title": "2.3.2 Question and Context Construction", "content": "Dish name prediction (Task 1) is divided into three question variations depending on the context: (1a) no-context question, where we simply ask for the name of the dish without any provided context; (1b) contextualized question where we provide additional information related to cuisine or location;"}, {"title": "2.3.3 Multiple Language Translation", "content": "Question and Context. All questions and contexts are initially collected in English, which are then carefully translated by native human speakers into 30 language varieties: 23 different languages with 7 languages having two different varieties each. We instructed the translators to prioritise the naturalness, and then followed by the diversity of translations when the duplication occurs.\nFood Name Alias. Using Wikipedia pages as our primary source, we can verify if the English page has translations available in other languages. This enables us to extract dish names in multiple languages and compile them as translations for each dish. We utilize both the Wikipedia page titles in various languages and the alias text found on the English page. These translations are especially valuable for multilingual prompt translation, as they allow us to use the dish's native name instead of its English equivalent, enhancing cultural relevance and accuracy. We use the English name as default when the translation is unavailable.\nLocations and Cuisines. As there are more than 400 unique locations, including countries, cities, and areas, we first translate the English locations into other languages by using GPT-40, followed by proofreading each translation by the native speakers. The string values for the regional cuisines, i.e., the adjective form of the location in English, are translated in the same manner as location.\nMorphological Inflections. Indo-European languages, such as Czech or Spanish, are rich in inflectional morphology which involves word modification to express different grammatical categories, such as number, gender, or case. For example, the equivalents of the phrases \u201cin Japan\u201d and \u201cfrom Japan\u201d in Czech are \u201cv Japonsku\u201d and \u201cz Japonska\u201d, respectively. We provide a framework for the human translators to use the inflections in the prompt template to prioritize the naturalness while keeping the inflections as few as possible."}, {"title": "2.3.4 Generating VQA Triplets", "content": "To ensure no overlap in train and test subsets, we split the dishes and the multilingual-questions into two subsets each, to ensure no dish or multilingual questions leakage between train and test. For every"}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Experimental Setup", "content": "Metrics. We use accuracy as the primary metric to evaluate predictions. For Task 2 (open-ended), we employ BERTScore (Zhang et al., 2019) with XLM-R Large (Conneau and Lample, 2019) as a secondary metric to determine if the model-generated content includes food names similar to those in the gold labels. For open-ended questions, we compute the accuracy of each test sample against multiple references, including translations of the dish in different languages. This approach allows us to accommodate predictions that may not be in the expected language."}, {"title": "Models.", "content": "We evaluate our benchmark on various available VLMs, including 14 open-source models and 3 proprietary models. During the inference of the open-source model, we use 16-bit floating point and employ greedy decoding. We access the proprietary models through API. The complete list of the models is available in Table 3."}, {"title": "4 Results and Discussion", "content": null}, {"title": "4.1 Overall Results", "content": "The results for WC-VQA are summarized in Table 3. The multiple-choice question (MCQ) results without any context exhibit significant variability, ranging from 30% to 80%, indicating considerable differences in model performance. This variability indicates that predicting MCQs remains a challenging task for many models. Notably, proprietary models, particularly GPT-40, demonstrate exceptional performance, outperforming all other models. In the open-ended question (OEQ) setting, the task proves even more difficult than the MCQ, with models achieving a maximum accuracy of less than 20% for dish name predictions and slightly higher for location predictions when no context is provided. However, incorporating context en-"}, {"title": "4.2 The Role of Context", "content": "For dish name prediction (Task 1), incorporating more relevant context significantly enhances performance across all language families. However, when adversarial context is introduced, performance drops significantly. The adversarial context included in the prompt significantly affects the prediction. Instead of relying solely on the image input, the model often sways and makes predictions based on incorrect location or cuisine information, even when the context is unrelated to the query. This observation is particularly intriguing, as it signifies that such prompts can shift the model's attention and influence its generation process."}, {"title": "4.3 Results by Language", "content": "In Task 1 with OEQ setting (Figure 5b), some languages with non-Latin scripts, such as Arabic, Korean, Japanese, and Marathi, tend to perform poorly, with the exception of Chinese. For Task 2 with OEQ setting, most models struggle with Sino-Tibetan languages (i.e., Chinese, Cantonese, and Hokkien) and Niger-Congo languages (i.e., Yoruba). In contrast, the models demonstrate relatively strong performance with Japonic, Koreanic, Kra-Dai (i.e., Thai), and Turkic (i.e., Azerbaijani) languages. We also observe that answering OEQs in underrepresented languages remains particularly challenging for the models, as shown by the relatively lower results for the \u201cleft behind\u201d, \u201cscraping by\u201d, and \u201chopeful\u201d languages. Interestingly, lower performance in the OEQ does not necessarily translate to the lower performance in the MCQ setting where the performance gap between language categories is less pronounced. The gap between OEQ and MCQ, especially for underrepresented languages, suggests that the bottleneck might lie in the factors beyond cultural understanding, such as text generation capabilities."}, {"title": "4.4 Scaling Law", "content": "It is evident that models with large sizes perform better than smaller ones, showing the scaling law still exists in this experiment, as shown in Figure 6. It is very interesting to see the same trend across different model families (e.g., Llava, Qwen, and even GPT-40 series). However, it is pretty clear for open-source models, Llama 3.2 Instruct has the lead for overall performance, which may be due the coverage of multilingual data used in its training, although it is still unclear since there is no evidence or supporting information that can back up the finding. Regardless, NVLM-D model does not perform as good as their base model Qwen2 VL Instruct in our benchmark. One reason could be the NVLM model is highly tuned for English, but not in languages other than English."}, {"title": "5 Related Work", "content": "Cultural VQA. Several prior studies have focused on developing culturally relevant VQA benchmarks, including FM-IQA (Gao et al., 2015), MCVQA (Gupta et al., 2020), xGQA (Pfeiffer et al., 2022), MaXM (Changpinyo et al., 2023),"}, {"title": "6 Conclusion", "content": "We introduce WORLDCUISINES, an open-source, large-scale benchmark designed for multilingual and multicultural, visually grounded language understanding. It comprises over 1 million data points across 30 languages and dialects. Our findings reveal that this benchmark remains challenging for models, particularly with dishes from specific regions and in low-resource languages. This provides insight into how well models understand regional cuisines. To enhance usability, we offer a dedicated evaluation split with two datasets of varying sizes. Our evaluation shows that while VLMs perform better with the correct context, they struggle with adversarial contexts intended to mislead them. Additionally, we are releasing a comprehensive knowledge base, VQA dataset, code, and leaderboard as open-source resources to support future research."}, {"title": "Limitations", "content": "In this paper, we limit our investigation to avoid exhaustively evaluating all possible models due to resource constraints. Our primary focus is on developing a benchmark that facilitates exploration for future research. We also provide a training data split for reference, allowing other researchers to utilize it to enhance their VLMs and evaluate their models against our test sets. Currently, we include 30 different languages and dialects, establishing one of the largest and most diverse benchmarks for comprehensive multilingual VQA. We aim to extend this benchmark to encompass additional languages in the future, making it more inclusive and representative of a broader range of linguistic diversity.\nRegarding the food entries coverage, it is important to note that our food entries are currently sourced from English Wikipedia. Although we aim to include as many diverse dishes as possible, we acknowledge that this approach limits the coverage of some regions. Nevertheless, our dataset serves as a valuable starting point. In future work, we plan to incorporate entries from non-English Wikipedia pages to improve regional representation and cultural diversity. For evaluation purposes, we include accuracy metrics for overall model performance and BERTScore for more detailed analysis. However, we recognize that evaluating VQA model performance on multicultural data remains an open challenge. Appropriate evaluation metrics are needed to effectively model the diversity of cultural contexts and linguistic variations. Addressing this issue will be a key focus of our future research efforts."}, {"title": "Ethical Considerations", "content": "Our research focuses on evaluating VLMs within the context of multilingual and multicultural VQA, a field that holds significant implications for diverse multilingual communities. We are committed to conducting our data collection and evaluations with the highest standards of transparency and fairness. To achieve this, we have adopted a crowd-sourcing approach for"}, {"title": "A Data Statement", "content": null}, {"title": "A.1 Executive Summary", "content": "WORLDCUISINES is a vision-language benchmark comprised of two resources: (1) WC-VQA, a multilingual parallel question answering dataset covering 30 languages and dialects where each dish image is accompanied by questions and context constructed through human translation; and (2) WC-KB, a knowledge base containing images and metadata associated with the dishes."}, {"title": "A.2 Curation Rationale", "content": "The goal of WORLDCUISINES is to evaluate the cultural understanding of vision-language models (VLMs) within the food domain. To achieve this, we develop WC-VQA and WC-KB. Dish names and their information are collected from English Wikipedia, and the images are selected from Wikimedia Commons to ensure a permissive license, with an emphasis on representing a wide range of food categories and geographic origins (or where the dish is popular). This selection strategy aims to provide insights into the VLMs' ability to generalize across diverse culinary and cultural contexts."}, {"title": "A.3 Language Variety", "content": "WORLDCUISINES covers 30 languages and dialects spoken across diverse countries and regions. The complete list of languages and dialects is shown in Table 5. An example of the multilingual prompt is shown in Table 6."}, {"title": "A.4 Annotator Demographic", "content": "Over 30 annotators are involved in building WORLDCUISINES, specifically in translating the query and context for the WC-VQA dataset. Most annotators are native speakers of the target languages or dialects included in our data; some are L2 speakers with more than 10 years of study in their respective languages. The detailed demographics for each language are elaborated below."}, {"title": "A.4.1 Austronesian", "content": "Indonesian Two native Indonesian speakers are involved as translators. One is in the 26\u201335 age range, and the other is in the 16-25 age range.\nTagalog One native Tagalog speaker in the 16-25 age range is involved as a translator.\nSundanese Two L2 Sundanese speakers contribute to the translation. One, in the 16-25 age range with 15 years of experience with the Sundanese language, assists with translation. The other, in the 26-35 age range with 25 years of experience with the language, primarily serves as the proof-reader.\nJavanese One native Javanese speaker with Central Java dialect in the 16\u201325 age range translates for both registers of the language (Krama and Ngoko)."}, {"title": "A.4.2 Japonic", "content": "Japanese Three L2 Japanese speakers with over 10 years of language study contribute to the Japanese translation. Two are in the 26\u201335 age range, and one is in the 36\u201345 age range. A native Japanese speaker then proofreads the translated sentences. Additionally, one native Japanese speaker from Western Japan in the 16\u201325 age range gives input for the casual form."}, {"title": "A.4.3 Sino-Tibetan", "content": "Chinese One native Chinese speaker in the 16-25 age range is involved as a translator.\nCantonese Two native Cantonese speakers are involved as translators. One is in 36\u201345 age range, and the other is in the 16\u201325 age range.\nHokkien Two native Hokkien speakers in the Medan dialect translate for both written and spoken versions of the language. Both are in the 26\u201335 age range."}, {"title": "A.4.4 Koreanic", "content": "Korean One native Korean speaker in the 16-25 age range translates the formal and casual versions of the language."}, {"title": "A.4.5 Kra-Dai", "content": "Thai One native Thai speaker in the 26\u201335 age range is involved as a translator."}, {"title": "A.4.6 Indo-European", "content": "English Query and context in English are constructed. All are L2 English speakers with over"}, {"title": "A.4.7 Afro-Asiatic", "content": "Arabic (MSA) One native Arabic speaker in the 26-35 age range is involved in the Modern Standard Arabic (MSA) translation."}, {"title": "A.4.8 Niger-Congo", "content": "Yoruba One native Yoruba speaker in the 16\u201325 age range is involved as a translator."}, {"title": "A.4.9 Turkic", "content": "Azerbaijani One native Azerbaijani speaker in the 16-25 age range is involved as a translator."}, {"title": "B Open-Source Collaborative Effort", "content": "The WORLDCUISINES data collection and benchmark construction is a fully open-source project. We invite contributions from researchers, practitioners, and grassroots communities, such as local NLP communities, who are interested in participating. Contributions can include data collection, annotation, quality checks, and evaluation. To ensure high-quality data, we engage native speakers of local languages in the annotation process with strict quality control (QC). The contributors who provide substantial contribution are invited to have co-authorship on"}, {"title": "C More Results", "content": null}, {"title": "C.1 Primary Metric: Accuracy (%)", "content": "Table 7 presents the comprehensive results of WC-VQA for both Test Small and Test Large. Additionally, we examine the performance gap between different references used in the evaluation, with the results displayed in Figure 8."}, {"title": "C.2 Secondary Metric: BERTScore", "content": "As a secondary metric, we employ BERTScore using XLM-R Large as the base model. Table 8 presents the comprehensive results of WC-VQA for both Test Small and Test Large. Figure 7 illustrates the model's performance categorized by language, language vitality, and language family."}, {"title": "Robustness and Error Analysis.", "content": "Figure 9 illustrates the correlation between BERTScore and accuracy in the open-ended setting through regression analysis. The R-squared value is 0.41, indicating a low correlation between BERTScore and accuracy."}]}