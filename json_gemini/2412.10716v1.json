{"title": "Control of Overfitting with Physics", "authors": ["Sergei V. Kozyrev", "Ilya A. Lopatin", "Alexander N. Pechen"], "abstract": "While there are many works on the applications of machine learning, not so many of them are trying to understand the theoretical justifications to explain their efficiency. In this work, overfitting control (or generalization property) in machine learning is explained using analogies from physics and biology. For stochastic gradient Langevin dynamics, we show that the Eyring formula of kinetic theory allows to control overfitting in the algorithmic stability approach when wide minima of the risk function with low free energy correspond to low overfitting. For the generative adversarial network (GAN) model, we establish an analogy between GAN and the predator-prey model in biology. An application of this analogy allows us to explain the selection of wide likelihood maxima and overfitting reduction for GANs.", "sections": [{"title": "Introduction", "content": "Analogies from physics and other fields, particularly population genetics, are of interest when studying problems in machine learning theory. Analogies between machine learning theory and Darwinian evolution theory were discussed already by Alan Turing [1]. Biological analogies in computing were discussed by John von Neumann [2]. Physical models in relation to computing were discussed by Yuri Manin [3]. Such analogies allow physical intuition to be used in learning theory. Among the well-known examples are genetic [4] and evolutionary algorithms [5], models of neural networks and physical systems with emergent collective computational abilities and content-addressable memory [6], a parallel search learning method based on statistical mechanics and Boltzmann machines that mimic Ising spin chains [7]. A phenomenological model of population genetics, the Lotka-Volterra model with mutations, related to generative adversarial network (GAN) was introduced in [8]. Analogies between evolution operator in physics and transformers (an artificial intelligence model) were discussed in [9]. Ideas of thermodynamics in application to learning were considered in [10,11] and in relation to the evolution theory in [12,13]. GANs have"}, {"title": "Stochastic Gradient Descent and the Eyring Formula", "content": "Within the framework of such analogies, it is natural to discuss biological analogs for various models of the machine learning theory. In this work, two analogies are considered, between the stochastic gradient Langevin dynamics (SGLD) in machine learning and the Eyring formula in the kinetic theory [24], and between the GAN model [25] and the mathematical predator-prey model in biology, where we suggest to consider the discriminator and generator in GAN playing the role of prey and predator, respectively. The proposed analogies allow us to explain the efficiency of controlling overfitting, which is the lack of generalization abilities for a machine learning approach.\nIt is known that for stochastic gradient descent (SGD), overfitting is reduced; for the GAN, this effect of reducing overfitting is even more significant. We propose to explain overfitting control in these processes within the framework of the algorithmic stability approach by suppressing narrow minima of the empirical risk function.\nIn Section 2, we consider the stochastic gradient Langevin dynamics (SGLD) using a stochastic differential equation (SDE). We show that the reduction of overfitting in such a model follows from ideas used in chemical kinetics such as the Eyring formula, which states that the reaction rate (the rate of the transition between two potential wells due to diffusion) is determined by free energy of the transition state (the saddle between two potential wells) and the free energy of the initial state of the reaction (optimization of quantities involving entropy-dependent Helmholtz free energy also appears in quantum optimization, e.g., [26]).\nIn Section 3, we describe the minimax problem for the GAN model by a system of two stochastic differential equations (one is for the discriminator and another is for the generator). In this sense, the GAN model is a two-body generalization of the SGLD model considered in Section 2. However, this generalization significantly changes the behavior of the learning system, as demonstrated by the simulations below. We show that this model implements a selection of wide maxima of the likelihood function, leading to a reduction of overfitting. A biological interpretation of the GAN model is provided in terms of the interaction between a predator and a prey (the predator is the generator, the prey is the discriminator). Learning for GANs by solving a system of ordinary differential equations was considered in [27-29].\nIn Section 4, we introduce a generalization of the GAN for a type of population genetics model. We consider a branching random process with diffusion for two types of particles (discriminators and generators), in which discriminators and generators can replicate. In this case, the rates of replication and death of particles depend on the contributions to the functional, minimax, which is defined by the GAN model.\nIn Section 5, we provide the numerical simulations illustrating the behavior of the stochastic gradient descent and the predator-prey model of the minimax problem for the GAN for a simple"}, {"title": "Stochastic Gradient Descent", "content": "Let {zl}, l = 1, . . ., L (with the elements belonging to Rn), a loss function fl(x) = L(zl, x) for the l-th sample object, and hypothesis x (we assume that the hypothesis space is Rm). Minimization of the empirical risk is the following problem:\n$$f(x) = \\frac{1}{L} \\sum_{l=1}^{L} f_l(x) \\rightarrow \\min_x.$$", "latex": ["f(x) = \\frac{1}{L} \\sum_{l=1}^{L} f_l(x) \\rightarrow \\min_x."]}, {"title": null, "content": "Gradient descent algorithm for this problem, when the loss function belongs to the space of continuously differentiable functions C1(X), is defined by the solution of the differential equation $$\\frac{df}{dt} = - \\frac{d}{dx} f(x)$$, which is numerically described by the iterative process starting from some initial guess xo, such that\n$$x_{k+1} = x_k - \\alpha_k f'(x_k) = x_k - \\alpha_k \\frac{1}{L} \\sum_{l=1}^{L} f'_l(x_k),$$", "latex": ["\\frac{df}{dt} = - \\frac{d}{dx} f(x)", "x_{k+1} = x_k - \\alpha_k f'(x_k) = x_k - \\alpha_k \\frac{1}{L} \\sum_{l=1}^{L} f'_l(x_k),"]}, {"title": null, "content": "where \\(\\alpha_k\\) is a gradient step [which is also often called as learning rate (lr)] at the kth iteration. There are different methods for introducing stochasticity in training for the problem (1), such as mini-batch learning or dropout. In this paper, we consider a method based on adding Gaussian noise to (2), which is related to stochastic gradient Langevin dynamics.\nStochastic gradient Langevin dynamics encompasses the modification of the above procedure where small independent random perturbations are added at each step of gradient descent, such that\n$$x_{k+1} = x_k + w_k - \\alpha_k f'(x_k),$$", "latex": ["x_{k+1} = x_k + w_k - \\alpha_k f'(x_k),"]}, {"title": null, "content": "where the set of all \\(w_k\\) is a set of independent Gaussian random vectors.\nThis procedure can be considered as a discrete-time version of the stochastic equation\n$$d\\xi^i(t) = \\sqrt{2\\theta}dw^i(t) - \\frac{\\partial f(\\xi(t))}{\\partial x^i}dt,$$", "latex": ["d\\xi^i(t) = \\sqrt{2\\theta}dw^i(t) - \\frac{\\partial f(\\xi(t))}{\\partial x^i}dt,"]}, {"title": null, "content": "where \\(dw^i(t)\\) is the stochastic differential of the Wiener process (factor \\(\\sqrt{2}\\) is used for convenience to have unit instead of 1/2 in front of \\(\\Delta\\) in Equation (5) below).\nThe stochastic gradient Langevin dynamics procedure with an equation of the form (4) was discussed in [30-33].\nDiffusion equation in the potential is the partial differential equation\n$$\\frac{\\partial u}{\\partial t} = \\theta\\Delta u + u \\cdot \\nabla f + u \\Delta f,$$", "latex": ["\\frac{\\partial u}{\\partial t} = \\theta\\Delta u + u \\cdot \\nabla f + u \\Delta f,"]}, {"title": null, "content": "where x \\(\\in\\) Rd, u = u(x, t) is the distribution function, f = f(x) is the potential, f \\(\\in\\) C2(Rd), and \\(\\theta\\) > 0 is the temperature.\nEquivalently, this diffusion equation can be rewritten as (where \\(\\beta\\) = 1/\\(\\theta\\) is the inverse temperature)\n$$\\frac{\\partial u}{\\partial t} = \\div [e^{-\\beta f} \\grad [ue^{\\beta f}]].$$", "latex": ["\\frac{\\partial u}{\\partial t} = \\div [e^{-\\beta f} \\grad [ue^{\\beta f}]]."]}, {"title": null, "content": "Gibbs distribution \\(e^{-\\beta f}\\) is a stationary solution of this equation. The solution converges to the Gibbs distribution under certain conditions on f, as discussed in [34].\nDiffusion Equation (5) is the Fokker-Planck equation (see (21) in Appendix 7) for stochastic gradient Langevin dynamics (4).\nWe would like to highlight the following: The well-known stable diffusion neural network and, in general latent, diffusion models [35, 36] do use diffusion described by a stochastic differential equation (as in SGLD). However, the general formulation of the problem is different there, where a special type of diffusion is used for generating objects (particularly images), while we are considering the diffusion (SGLD) for learning in a potential", "latex": []}, {"title": "Overfitting Control for Stochastic Gradient Descent", "content": "Overfitting is the lack of ability to generalize for the solution of the learning problem (i.e., high likelihood on the training sample and low likelihood for the validation sample). One approach to overfitting control is based on the algorithmic stability, i.e., on the stability of the solution obtained by a learning algorithm to perturbations of the training sample [37-39]. In this case, narrow (sharp) minima of the empirical risk functional (in the hypothesis space) are associated with overfitting, and wide (flat) minima correspond to solutions of the learning algorithm without overfitting [40].\nIntroducing noise into the gradient descent procedure, i.e., considering SDE (4) and diffusion (5), is related to the problem of overfitting as follows: The Eyring formula, which is a generalization of the Arrhenius formula, describes the reaction rate (the rate of transition between two potential wells due to diffusion of the form (5)) in the kinetic theory: the reaction rate is proportional to\n$$e^{-\\beta(F_1 - F_0)},$$", "latex": ["e^{-\\beta(F_1 - F_0)},"]}, {"title": null, "content": "where F1 is the free energy of the transition state (the saddle between two potential wells), and F0 is the free energy of the initial state of the reaction (the potential well from which the transition occurs). The free energy of a state is F = E \u2212 \u03b2\u22121S, where E is the energy and S is the entropy of the state. In general, free energy of a set U (say a potential well) is defined as\n$$e^{-\\beta F(U)} = \\int_U e^{-\\beta E(x)} dx.$$", "latex": ["e^{-\\beta F(U)} = \\int_U e^{-\\beta E(x)} dx."]}, {"title": null, "content": "The connection between the Arrhenius and Eyring formulas and the spectral asymptotics for the Schr\u00f6dinger operator corresponding to the subbarrier (tunnel) transition between two potential wells was discussed in [34].\nLet us emphasize that we do not assume any minimization procedure\u2014stochastic gradient Langevin dynamics (4) generate the Gibbs distribution according to diffusion Equation (5), and this Gibbs distribution will be concentrated in potential wells with low free energy. In the view"}, {"title": null, "content": "of the above discussion, learning with stochastic gradient Langevin dynamics is a search for a potential well that gives the global minimum of free energy. The influence of the temperature is important when comparing the entropy and the energy parts of free energy. For the effect of particle capture by a well (i.e., learning) to take place, it is important that the temperature should be significantly (e.g., by several times) less than the difference in the free energies of the well and the saddle; hence, the temperature is important for the SGLD. Thus, for successful learning, the temperature should be low enough to make the capture of the particle by a potential well possible.\nThe Eyring formula (6) implies that, for equal free energies of the transition state and equal energies of the initial state, the transition rate will be lower (the free energy of the initial state will be lower) for potential wells with higher entropy (wider ones). Thus, stochastic gradient Langevin dynamics (4) will correspond to a regime in which wider wells (with higher entropy) more effectively capture the learning system, i.e., algorithmically stable solutions to the learning problem will be selected during stochastic gradient optimization (here, we consider the SGLD procedure but we believe the same effect will hold for other forms of SGD, such as mini-batch procedure).\nThe validity of the proposed approach is limited by the assumptions for the Eyring formula. The Eyring formula describes transitions for a diffusion equation in the potential and might be applied to various complicated high-dimensional landscapes having clear minima and saddles between them. Its validity under quite general conditions has been justified in [41]. For landscapes which might not exhibit clear minima and saddles between them, the proposed approach based on the Eyring formula may not work.\nPhysical analogies in machine learning, particularly the application of free energy, were dis- cussed in [10]. In [11], the following procedure was considered: the empirical risk functional (depending on the hypothesis x) was replaced by the so called \u201clocal entropy\u201d functional, which looks like minus free energy of some vicinity of x (where energy is the empirical risk). In this way, the wider empirical risk minima will correspond to deeper minima of the new\u201clocal entropy\u201d functional. Relation to the generalization property in the approach of [40] (flat minima) was discussed (although the authors of this paper do not discuss the Eyring formula)."}, {"title": "The GAN Model and Overfitting", "content": null}, {"title": "Stochastic Gradient Langevin Dynamic for GAN", "content": "The generative adversarial network (GAN) model is a minimax problem, such that [25]\n$$\\min_y \\max_x V(x,y);$$", "latex": ["\\min_y \\max_x V(x,y);"]}, {"title": null, "content": "$$V(x, y) = \\frac{1}{L} \\sum_{l=1}^{L} \\log D(z_l, x) + \\int_Z p_{gen}(z, y) \\log(1 - D(z,x)) dz;$$", "latex": ["V(x, y) = \\frac{1}{L} \\sum_{l=1}^{L} \\log D(z_l, x) + \\int_Z p_{gen}(z, y) \\log(1 - D(z,x)) dz;"]}, {"title": null, "content": "{zl} is a sample, where zl \\(\\in\\) Z, D(z,x) and pgen(z,y) are parametric families of probability distributions on the space Z, called the discriminator and the generator, with parameters x and y from statistical manifolds X and Y (which we will assume to be real vector spaces).\nIn [25], the generator was considered as a parametric family of mappings from some auxiliary space to Z. These mappings transferred the probability distribution on the auxiliary space to Z. The discriminator was described by a distribution on the same space as the data, and the"}, {"title": null, "content": "interpretation was as follows: the discriminator outputs binary variable \u201cone\u201d or \u201czero\u201d given the data and given the generator according to this distribution (i.e., outputs \u201cone\u201d if the discriminator considers these inputs as correct).\nThe first contribution to V (x, y) in (8) is the log-likelihood function. The second contribution behaves qualitatively as the minus inverse of the Kullback-Leibler distance (Kullback-Leibler divergence, KL distance, or KLD) between the distributions of the discriminator and the generator (that is, this contribution is negative, large in magnitude for small KL-distances, and grows to zero for large KL-distances), as was mentioned in [25].\n$$V(x,y) = V_1(x) + V_2(\\rho(x, y)).$$", "latex": ["V(x,y) = V_1(x) + V_2(\\rho(x, y))."]}, {"title": null, "content": "Here\n$$\\rho(x, y) = KL(D(x)|p_{gen}(y)),$$", "latex": ["\\rho(x, y) = KL(D(x)|p_{gen}(y)),"]}, {"title": null, "content": "where the Kullback-Leibler distance between probability distributions p and q is defined as\n$$KL(p\\Vert q) = \\int_Z p(z) \\log \\frac{p(z)}{q(z)} dz.$$", "latex": ["KL(p\\Vert q) = \\int_Z p(z) \\log \\frac{p(z)}{q(z)} dz."]}, {"title": null, "content": "The minimax for V(x, y) over x, y is obtained from the local maximum of V1(x) over x. Transitions between the local maxima of V1(x) generate transitions between local minimaxes of V(x, y) (the generator follows the discriminator) as shown below.\nThe stochastic gradient Langevin dynamics optimization for the problem (7), (8) can be described by a system of SDEs defining random walks \\(\\xi = (\\xi^i(t)), i = 1,...,m, \\eta = (\\eta^j(t)), j = 1, . . ., n\\) on statistical manifolds X and Y of the discriminator and generator,\n$$d\\xi^i(t) = \\sqrt{2\\theta}dw^i(t) + \\frac{\\partial}{\\partial x^i}V(\\xi(t), \\eta(t)),$$", "latex": ["d\\xi^i(t) = \\sqrt{2\\theta}dw^i(t) + \\frac{\\partial}{\\partial x^i}V(\\xi(t), \\eta(t)),"]}, {"title": null, "content": "$$d\\eta^j(t) = \\sqrt{2\\theta}dv^j(t) - \\frac{\\partial}{\\partial y^j}V(\\xi(t), \\eta(t)).$$", "latex": ["d\\eta^j(t) = \\sqrt{2\\theta}dv^j(t) - \\frac{\\partial}{\\partial y^j}V(\\xi(t), \\eta(t))."]}, {"title": null, "content": "Here, \\(\\frac{\\partial}{\\partial x}V\\) and \\(\\frac{\\partial}{\\partial y}V\\) are derivatives of V with respect to the first and second arguments, respectively; \\(w^i(t)\\) and \\(v^j(t)\\) are Wiener processes on the parameter spaces of the discriminator and generator. This system of stochastic equations follows directly from the minimax condition and is independent from the KL distance formulation. However, the KLD formulation is used later for analyzing the behavior of the solution. In this system of SDEs, the discriminator seeks to maximize the function V(x, y) (8) with respect to x, and the generator seeks to minimize this function with respect to y."}, {"title": null, "content": "Example. Let us consider one-dimensional parameters x and y for the discriminator and for the generator, respectively, and functional V = wxy with minimax located at the origin. The noiseless GAN equation system is\n$$\\frac{dx}{dt} = \\frac{\\partial}{\\partial x} V(x, y) = \\omega y,$$", "latex": ["\\frac{dx}{dt} = \\frac{\\partial}{\\partial x} V(x, y) = \\omega y,"]}, {"title": null, "content": "$$\\frac{dy}{dt} = - \\frac{\\partial}{\\partial y}V(x,y) = -\\omega x.$$", "latex": ["\\frac{dy}{dt} = - \\frac{\\partial}{\\partial y}V(x,y) = -\\omega x."]}, {"title": null, "content": "Its solution is\nx = A sin \\omega (t \u2212 to), y = A cos \\omega (t \u2212 to)\nwith oscillations around the minimax."}, {"title": "Overfitting Control for GAN", "content": "In [27-29], the convergence of the optimization of the GAN model by the gradient descent method with respect to the parameters of the discriminator and generator in the neighborhood of the functional's local minimax was studied, and oscillations of the parameters were discussed.\nIf we ignore the presence of the generator, then the dynamics of the discriminator (9) for opti- mization with noise will correspond to the diffusion in the potential generated by the data. Thus, the arguments of Section 2 will be applicable. Therefore, overfitting can be reduced according to the Eyring formula.\nThe presence of the generator will further suppress overfitting. The minimax problem for the GAN (7) can be described as follows. The discriminator (9) tries to reach regions of the parameter x with high values of V(x, y). The generator (10) tries to reach regions of the parameter y with low values of V(x, y). In this case, the contribution to V(x, y) (8) from the likelihood function depends only on the parameters of the discriminator, i.e., the discriminator tries to increase both contributions to (8), and the generator tries to decrease only the second contribution. The second contribution to (8) decreases at small Kullback-Leibler distances between the discriminator and the generator.\nTherefore, the compromise between the optimization problems for the discriminator and the generator will be achieved when they are located at maxima of the contribution from the likeli- hood function to (8) which are sufficiently wide in the space of the parameters (x, y) (where the average KL-distance between the discriminator and the generator is not too small). Selecting wide maxima, in accordance with the algorithmic stability approach, will reduce the effect of overfitting.\nHere, we propose a biological prey-predator interpretation for the GAN model, which is com- pletely different from the interpretation used in [25]. In our interpretation, the discriminator is herbivore (prey), the generator is predator, and the data are grass. Then, the minimax prob- lem (7) describes the situation when the discriminator searches for grass (the maximum of the likelihood function; this corresponds to an increase in the first contribution in (8)) and also runs away from the predator (this corresponds to an increase in the second contribution to (8)), while the predator chases the prey (and hence decreases this contribution). In our interpretation, the discriminator (herbivore)\u2014as a distribution tries to get closer to the data distribution (grass) and farther from the generator (predator) as a distribution (in the KL distance sense), and the generator tries to be closer to the discriminator. As a result of this interaction, the generator also moves toward the data (grass) because herbivores (discriminator) are likely to be found there; however, this does not mean that the predator tries to imitate grass (this is a mixture of the two interpretations of the GAN). Minimization in (7) for the generator forces the predator to move to fields (or meadows, likelihood maxima) where the discriminator is present. The interaction of the two contributions to (8) forces the discriminator to search for sufficiently wide meadows (likeli- hood maxima) where the average KL-distance from the predator is not too small. In general, the predator pushes out the prey from narrow fields of grass, and both the prey and predator move to wide grass fields. Thus, the GAN model implements the selection of wide likelihood maxima, which reduces overfitting.\nSimulations illustrating the discussed above behavior are considered in Section 5 below."}, {"title": "Branching Random Process for GAN", "content": "In this section, a branching random process with diffusion and particle interactions describing the populations of discriminators and generators in a generalization of the GAN model is introduced.\nThe theory of branching random processes and its connection with population genetics have been actively discussed in the literature, for example, in [42,43]. Previously, in [8], a generalization of the GAN model related to population genetics (a Lotka-Volterra-type model with mutations), was discussed. In this model, discriminators and generators could reproduce and form populations. The phenomenological equations of population dynamics were considered, and the suppression of overfitting was discussed.\nConsider a generalization of the GAN to the case of several discriminators (particles in the hypothesis space of the discriminator with parameter x, particles are indexed by a) and generators (particles in the hypothesis space of the generator with parameter y, particles are indexed by b). The analog of the SDE system (9), (10) will take the form\n$$d\\xi^{(a) i}(t) = \\sqrt{2\\theta}dw^{(a) i}(t) + dt \\frac{\\partial}{\\partial x^i}V(\\xi^{(a)}(t), \\eta(t));$$", "latex": ["d\\xi^{(a) i}(t) = \\sqrt{2\\theta}dw^{(a) i}(t) + dt \\frac{\\partial}{\\partial x^i}V(\\xi^{(a)}(t), \\eta(t));"]}, {"title": null, "content": "$$d\\eta^{(b) j}(t) = \\sqrt{2\\theta}dv^{(b) j}(t) - dt \\frac{\\partial}{\\partial y^j}W(\\xi(t), \\eta^{(b)}(t));$$", "latex": ["d\\eta^{(b) j}(t) = \\sqrt{2\\theta}dv^{(b) j}(t) - dt \\frac{\\partial}{\\partial y^j}W(\\xi(t), \\eta^{(b)}(t));"]}, {"title": null, "content": "where each particle is associated with its own independent Wiener process \\(w^{(a) i}(t), v^{(b) j}(t)\\) on the right-hand side of the equation in the discriminator and generator spaces, respectively, and the terms with interaction on the right-hand sides of the equations have the form\n$$V(x, y) = \\frac{1}{L} \\sum_{l=1}^{L} \\log D(z_l, x) + \\sum_b \\int_Z p_{gen}(z, y_b) \\log(1 - D(z, x)) dz = V_1(x, {z}) + V_2(x, y);$$", "latex": ["V(x, y) = \\frac{1}{L} \\sum_{l=1}^{L} \\log D(z_l, x) + \\sum_b \\int_Z p_{gen}(z, y_b) \\log(1 - D(z, x)) dz = V_1(x, {z}) + V_2(x, y);"]}, {"title": null, "content": "$$W(x, y) = \\sum_a \\int_Z p_{gen}(z, y) \\log(1 - D(z, x_a)) dz.$$", "latex": ["W(x, y) = \\sum_a \\int_Z p_{gen}(z, y) \\log(1 - D(z, x_a)) dz."]}, {"title": null, "content": "Here, V1(x, {z}) is the likelihood function for the discriminator x.\nThis corresponds to a GAN-type model with functional V(x, y) for discriminator x and func- tional W(x, y) for generator y. Equations (11) and (12) describe optimization by the stochastic gradient Langevin dynamics. Each discriminator interacts with a set of generators and similarly, each generator interacts with a set of discriminators. Here, \\(\\overrightarrow{x} = {x_1,...,x_M}\\) and \\(\\overrightarrow{y} = {y_1, \u2026\u2026,y_N}\\) are sets of discriminators and generators, respectively. The second contribution V2(x, y) and the function W (x, y) contain sums from contributions that behave qualitatively as -KL(x, y)\u22121, where KL(x, y) is the Kullback-Leibler distance between discriminators and generators with parameters x and y.\nLet us define a model which mimics the population genetics, defined by a branching random process with diffusion and interaction with particles of two types \\(\\xi^{(a)}(t), \\eta^{(b)}(t)\\) (discriminators and generators), which can perform random walks in accordance with Equations (11) and (12), and have the ability to replicate and die, with the probabilities of such processes depending on the functionals (13), (14). The replication of a particle consists of replacing it with two particles of the same type with the same coordinates (which can then perform random walks in accordance with (11), (12))."}, {"title": null, "content": "We propose to use the following branching rates (as related to Lotka-Volterra-type model discussed in [8]): the death rate of generators is considered as fixed, while the replication rates of the generators \\(\\eta^{(b)}(t)\\) are proportional to (recall that both W and V2 are negative)\n$$-W(\\xi(t), \\eta^{(b)}(t));$$", "latex": ["-W(\\xi(t), \\eta^{(b)}(t));"]}, {"title": null, "content": "the replication rates of discriminators \\(\\xi^{(a)}(t)\\) are proportional to\n$$\\exp (V_1(\\xi^{(a)}(t), {z}));$$", "latex": ["\\exp (V_1(\\xi^{(a)}(t), {z});"]}, {"title": null, "content": "the rate of death of the discriminator \\(\\xi^{(a)}(t)\\) is proportional to\n$$-V_2(\\xi^{(a)}(t), y).$$", "latex": ["-V_2(\\xi^{(a)}(t), y)."]}, {"title": null, "content": "Thus", "follows": "if the discriminator has replicated on the likelihood maximum (on its statistical manifold X), the generator will tend to go there and replicate there (the generator will tend to the corresponding regions of its statistical manifold Y, such that the KL-distance between D(\u00b7, x) and pgen(\u00b7, y) is small). In this case, for a narrow likelihood maximum, the average KL-distance will be small, i.e., the predator will eat the prey more effectively (and then suffer from hunger) than for a wide maximum. This is how the effect of selective suppression of narrow population maxima in X and Y corresponding to narrow like- li"}]}