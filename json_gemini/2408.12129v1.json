{"title": "Deep Analysis of Time Series Data for Smart Grid Startup\nStrategies: A Transformer-LSTM-PSO Model Approach", "authors": ["Zecheng Zhang"], "abstract": "Grid startup, an integral component of the power system, holds strategic importance for ensuring\nthe reliability and efficiency of the electrical grid. However, current methodologies for in-depth\nanalysis and precise prediction of grid startup scenarios are inadequate. To address these challenges,\nwe propose a novel method based on the Transformer-LSTM-PSO model. This model uniquely\ncombines the Transformer's self-attention mechanism, LSTM's temporal modeling capabilities, and\nthe parameter tuning features of the particle swarm optimization algorithm. It is designed to more\neffectively capture the complex temporal relationships in grid startup schemes. Our experiments\ndemonstrate significant improvements, with our model achieving lower RMSE and MAE values\nacross multiple datasets compared to existing benchmarks, particularly in the NYISO Electric Market\ndataset where the RMSE was reduced by approximately 15% and the MAE by 20% compared to\nconventional models. Our main contribution is the development of a Transformer-LSTM-PSO model\nthat significantly enhances the accuracy and efficiency of smart grid startup predictions. The\napplication of the Transformer-LSTM-PSO model represents a significant advancement in smart grid\npredictive analytics, concurrently fostering the development of more reliable and intelligent grid\nmanagement systems.", "sections": [{"title": "1. Introduction", "content": "The power system is one of the indispensable infrastructure components in modern society, and\nsmart grids represent a significant development direction in modern power systems. They achieve\nefficient, reliable, and environmentally friendly grid operation by integrating advanced information\ncommunication technology, automation technology, and new energy technology[1, 2]. In a smart grid,\nthe concept of startup strategies refers to a series of strategies and technologies for safely and\neffectively initiating and operating the grid under different conditions, such as daily operation,\nemergency response, disaster recovery, etc[1, 3]. The main challenges faced by current startup\nstrategies in smart grids include efficient integration and management of the increasing amount of\nrenewable energy sources[4, 5, 48-50], ensuring grid stability and resilience in the face of extreme\nweather conditions and system failures[6], and handling and analyzing the growing volume of grid\ndata to optimize operational strategies[7]. With the rapid development of artificial intelligence\ntechnologies, especially deep learning, more and more research efforts are focusing on using these\ntechnologies to address the issues related to startup strategies in smart grids. The advantages of deep\nlearning in data processing and pattern recognition make it a powerful tool for analyzing complex\ngrid systems. For instance, leveraging deep learning models enables researchers to enhance the\nprecision of grid load and energy demand predictions, as well as the assessment of fault probabilities.\nConsequently, this facilitates the formulation of more efficacious grid startup and operational\nstrategies.[8]. Additionally, time series forecasting plays a pivotal role in the research of smart grid\nstartup strategies. Grid operation data, such as loads, supply, and weather conditions, often exhibit\nsignificant temporal correlations[9, 10]. Effective time series analysis and forecasting are crucial\nfor developing precise grid startup strategies. Through time series forecasting, it becomes possible to\nmore accurately anticipate the future state and demands of the grid, especially in predicting renewable\nenergy output and grid load variations. This is indispensable for designing startup strategies capable\nof adapting to future changes and mitigating potential risks.\nThe research field of grid startup scenarios has made significant progress in recent years,\nespecially in deep learning and time series forecasting. One study adopted a long short-term memory\nnetwork (LSTM) model and focused on grid load forecasting[11]. This research uses the powerful\nability of LSTM to process time series data to analyze and predict the grid load pattern to optimize\ngrid startup and load deployment strategies. Nonetheless, the accuracy of this study in handling highly\nnonlinear and anomalous data still needs to be improved. Another study used a convolutional neural\nnetwork (CNN) model[56-62] and focused on the performance of power grids under extreme weather\nconditions[12]. Through CNN, the study identified and analyzed power grid data patterns related to\nextreme climate events, guiding power grid operation under extreme conditions[13]. However, the\nstudy is not accurate enough in predicting a few extreme events, which may lead to prediction errors\nin practical applications. Another work adopted a graph neural network (GNN) model to analyze the\ninteractions between power grid nodes[14]. This research reveals the complex dependencies between\nnodes in the power grid through GNN, providing new insights into the overall stability of the power\ngrid. However, the scalability and efficiency of this model in large-scale power grid systems have not\nbeen fully verified. The final study combines neural networks and reinforcement learning to optimize\ngrid emergency start-up plans[15]. This model, which combines real-time learning and adaptation to\ngrid dynamics, improves the efficiency and accuracy of emergency response. These studies have\nmade important contributions to the optimization of power grid startup schemes, while also revealing\ntheir respective limitations. Current methods often struggle with the nonlinear and anomalous nature\nof grid data, face challenges in accurately predicting extreme events, and lack scalability and\nefficiency in large-scale systems. Therefore, it is crucial to develop specialized methods to ensure the\nsafety and reliability of power grid startup strategies.\nBuilding upon the limitations identified in previous research, the focus of this study is to"}, {"title": "2. Related Work", "content": "introduce the Transformer-LSTM-PSO network[51-55], an innovative hybrid model engineered to\nenhance the accuracy and efficiency of grid startup schemes. This model combines three key\ntechnologies: Transformer, LSTM network, and PSO algorithm. The Transformer manages intricate\nrelationships and long-term dependencies in grid data, enhancing prediction accuracy. LSTM captures\nshort-term dependencies and temporal correlations, ideal for time series data like load and supply-\ndemand predictions. PSO optimizes the parameters of the Transformer and LSTM, enhancing training\nefficiency. The primary focus of this study is to address the limitations of previous models by\nimproving prediction accuracy and training efficiency for complex grid data. Our transformer-LSTM-\nPSO network offers a comprehensive solution, improving prediction accuracy and adaptability in\ndynamic grid conditions. The PSO algorithm ensures optimal model performance in various scenarios,\nvital for efficient smart grid startup solutions. Our work pioneers innovative deep learning techniques,\nadvancing predictive accuracy in smart grids and opening new research directions, addressing current\nlimitations in the field.\nThe main contributions of this study are as follows:\n\u2022 We propose an innovative Transformer-LSTM-PSO network model, which effectively integrates\nthe long-term dependency processing capabilities of the Transformer, the short-term data analysis\ncapabilities of LSTM, and the optimization mechanism of the PSO algorithm. Our model shows\nexcellent performance in handling large-scale and intricate datasets from smart grids, especially\nin accurately predicting grid load and supply demand.\n\u2022 Our research makes significant progress in time series forecasting of smart grid data. By in-depth\nanalysis and utilization of the time attributes of power grid data, our model can more accurately\npredict short-term and long-term operating trends of the power grid, providing strong data\nsupport for effective management and emergency response of the power grid.\n\u2022 Our research also provides a new methodological framework and new perspectives and ideas for\nfuture research on smart grid startup solutions. Our model can not only be applied to the current\npower grid system but also has good scalability and is suitable for the development of future\npower grid technology and the integration of new energy sources.\nOverall, our work not only contributes at the technical level but also provides a new theoretical\nand practical basis for the future development of smart grids.\nThe remainder of this paper is structured as follows: Section 2 describes the methodology,\nSection 3 presents the experimental setup and results, Section 4 discusses these findings, and Section\n5 concludes with a summary and future directions."}, {"title": "2.1Utilization of Deep Learning in Power Systems", "content": "Deep learning has made significant progress in the field of power systems, and its applications\nhave impacted many aspects of the power industry. By utilizing deep learning models, such as\nrecurrent neural networks (RNN)[63-69], long short-term memory networks (LSTM)[70-73], etc.., \npower systems can achieve more accurate power load forecasting, thereby better planning power"}, {"title": "2.2 Application of Parameter Optimization Method in the Power Grid", "content": "In the domain of power grids, parameter optimization methods are deployed to enhance the\nsystem's performance, efficiency, and stability. These methods optimize grid system parameters to\nbetter adapt to diverse operational conditions, ensuring more reliable and efficient functionality[21,\n22]. In power grid load flow calculations, parameter optimization methods play a crucial role in\nrefining parameters associated with grid components, such as generator output power and\ntransmission line impedance[23]. By doing so, they enable a more rational distribution of power flow\nthroughout the grid, thereby enhancing transmission efficiency and stability. Similarly, in the realm\nof power grid planning, these methods are instrumental in determining parameters for newly\nconstructed grid facilities, including the capacity of substations and the routing of transmission\nlines[24]. This optimization process ensures the optimal allocation of grid resources, resulting in\nimproved power supply capacity and adaptability. Within the context of smart grids, parameter\noptimization methods are utilized to fine-tune the parameters of various smart devices[25, 26]. This\noptimization enables intelligent control of the grid system, leading to enhanced response speed and\nimproved management efficiency. In the integration of renewable energy sources, parameter\noptimization methods focus on optimizing parameters of renewable energy devices, such as the blade\nangles of wind turbines and the tilt angles of photovoltaic arrays[27]. This optimization maximizes\nthe utilization of renewable energy, thereby increasing the proportion of clean energy and improving\nenvironmental performance. Finally, in the domain of power grid operation control, parameter\noptimization methods are employed to optimize the parameters of various control devices. This\noptimization facilitates automation and intelligent operation of the grid system, resulting in improved\noperational efficiency and stability. In summary, parameter optimization methods offer promising\napplications in the power grid domain, driving advancements towards grid intelligence, cleanliness,\nand sustainability."}, {"title": "2.3 Application of Attention Mechanism in the Power Grid", "content": "In the field of power grids, the application of attention mechanisms is an emerging method aimed\nat improving the intelligence and performance of power grid systems. This mechanism mimics the\nway the human visual system works, by giving different weights to different parts of the input data,\nallowing the model to focus more on important information relevant to the task[28, 29]. In areas such\nas load forecasting, anomaly detection, equipment status monitoring, operation optimization, and\nplanning, attention mechanisms help models automatically identify and focus on crucial features,\nthereby improving prediction accuracy, anomaly detection precision, equipment status monitoring\naccuracy, operation efficiency, and planning decision accuracy[30, 31]. For example, in load\nforecasting, the attention mechanism can automatically capture the time series features that have the\ngreatest impact on load changes, thus enhancing the accuracy of the forecast model for future\nloads[32].The attention mechanism allows the model to prioritize data patterns associated with\nanomalies, thus enhancing both the sensitivity and accuracy of anomaly detection.[33, 34]. Therefore,\nthe application of the attention mechanism provides new ideas and methods for the intelligent\ndevelopment of power grid systems, and helps promote the modernization and sustainable\ndevelopment of power grids. In power grid operation and management, the application of the attention\nmechanism is also a potential method, which can help the power grid system cope with complex\noperating environments and task requirements more effectively, and achieve smarter, more efficient,\nand more reliable power grid operation."}, {"title": "3. Method", "content": ""}, {"title": "3.1 Overview of Our Network", "content": "In this study, we develop a deep learning model for smart grid startup, to enhance the operational\nefficiency and reliability of the grid. The model combines the Transformer network, the long short-\nterm memory network (LSTM)[74-78] and the particle swarm optimization (PSO) algorithm, with\neach part specifically optimized for different aspects of power grid data processing and prediction.\nThe main function of the Transformer network is to handle long-term dependencies in power grid\ndata. Its attention mechanism enables it to effectively identify and focus on key information points in\nthe data, which is particularly important for predicting complex operating modes of the power grid.\nThe LSTM network[79-82] focuses on capturing short-term dependencies and time correlations in\npower grid data. It is particularly suitable for processing data that changes rapidly in the short term\nsuch as power grid load and supply demand, thus playing a key role in understanding and processing\npower grid time series data. The PSO algorithm is used to optimize the parameter configuration of\nTransformer and LSTM during the model-building process. By simulating the collective behavior of\na flock of birds, it finds the optimal solution in the parameter space and improves the overall\nperformance and training efficiency of the model. During the network construction process, we first\nthoroughly preprocessed the power grid data, including data cleaning, standardization, and\nsegmentation, to ensure the quality and consistency of the data. Subsequently, we configured and\ninitialized the parameters of the Transformer and LSTM network according to the specific needs and\ncharacteristics of the power grid, and trained and adjusted the model on the experimental data, in\nwhich the PSO algorithm was used to dynamically optimize the model parameters. This model is\ncrucial for the implementation of smart grids, significantly enhancing operational efficiency and\nstability while predicting and identifying potential abnormalities. This capability provides essential\nsupport for emergency response and disaster recovery. In addition, the model is also critical for the\nintegration of renewable energy management into the grid, helping to achieve more efficient and\nenvironmentally friendly grid operations. With this deep learning technology, we provide grid\noperators with a powerful tool to optimize start-up and operation strategies to effectively respond to\nchanging grid conditions and challenges. As shown in Figure 1, which illustrates the overall network\nflow."}, {"title": "3.2 Transformer Model", "content": "The Transformer model[83-86] has been a breakthrough in the field of deep learning in recent\nyears, especially in processing sequence data. The Transformer's fundamental principle[87-92] relies\non the 'attention mechanism,' enabling the model to process sequence data by considering the\ninterrelations among all elements simultaneously[35]. In contrast to conventional Recurrent Neural\nNetworks (RNN) and Long Short-Term Memory Networks (LSTM) that operate sequentially on\nsequence data, Transformers possess the unique ability to process the entire sequence in parallel. This\nparallel processing capability not only significantly enhances computational efficiency but also\ndiminishes model training duration. Moreover, the attention mechanism embedded within\nTransformers enables the capture of long-range dependencies within the data. This feature holds\nparticular significance when dealing with sequence data characterized by intricate dependencies[36].\nIn our Transformer-LSTM-PSO model, the introduction of the Transformer module has a significant\nimpact on model performance. First, it significantly enhances the efficiency of power grid data\nprocessing, especially when dealing with large-scale data sets. Moreover, the Transformer utilizes an\nattention mechanism to effectively capture complex relationships within grid data, enabling accurate\npredictions of grid load and energy demand over extended periods. Additionally, the Transformer's\nparallel processing capabilities substantially improve the model's overall performance and reduce\ntraining time. In deep learning research applied to smart grid startup solutions, these characteristics\nof Transformer make it the key to improving prediction accuracy and computing efficiency. The\nTransformer not only processes and analyzes vast amounts of grid operation data but also identifies\nand understands intricate patterns in grid behavior, thus providing grid operators with more accurate\nand comprehensive data analysis and forecasts. Therefore, Transformer significantly improves the\ntechnical performance of the model in our research, while also offering vital support for promoting\nthe efficient and reliable operation of smart grids. The core of the Transformer model lies in its\nmathematical formulations. As shown in Figure 2, it illustrates the transformer network.\nHere are the key mathematical equations that define the Transformer architecture:\nThis equation represents the Self-Attention mechanism, which allows the model to focus on\ndifferent positions of the input sequence and compute a weighted sum to generate the output."}, {"title": "", "content": "Attention(Q, K,V) = softmax(\\frac{QKT}{\\sqrt{d_k}})V  (1)\nWhere: Q: Query matrix K: Key matrix V: Value matrix $d_k$: Dimension of the key vectors.\nThe Multi-Head Self-Attention mechanism allows the model to learn different attention patterns\nin parallel and combine the outputs through concatenation and linear transformation.\nMultiHead(Q, K, V) = Concat(head\u2081, \u2026\u2026, head\u1e25)W\u00b0  (2)\nWhere: head\u2081 = Attention(QW, KWK, VW)W, WK, W are the weight matrices for the i-\nth attention head W\u00ba is the output concatenation weight matrix.\nThe Position-wise Feed-Forward Network applies a non-linear transformation to each position\u0393\ns input to enhance the modell s representational capacity.\nFFN(x) = max(0, xW\u2081 + b\u2081)W2 + b2  (3)\nWhere: x is the input vector W\u2081, b\u2081 are the weights and bias of the first linear layer W2, b2\nare the weights and bias of the second linear layer.\nThe Residual Connection adds the output of the sublayer to the input and normalizes the result.\nOutput = LayerNorm(x + Sublayer(x))  (4)\nWhere: Sublayer(x) is the output of the sublayer (either self-attention or position-wise feed-\nforward network) LayerNorm is the layer normalization operation.\nPositional Encoding is used to provide additional information about the position of each element\nin the input sequence.\nPE(pos,2i) = sin(\\frac{pos}{10000^{2i/dmodel}}) (5)\nPE(pos,2i+1) = cos(\\frac{pos}{10000^{2i/dmodel}}) (6)\nWhere: pos is the position in the input sequence, i is the index of the positional encoding\ndimension, dmodel is the model's dimension."}, {"title": "3.3 Long Short-Term Memory model", "content": "Long Short-Term Memory Networks (LSTMs) represent a distinct variant of Recurrent Neural\nNetworks (RNNs), engineered to effectively address and anticipate prolonged dependencies within\nsequential data. A defining characteristic of LSTMs lies in their internal architecture, which\nincorporates a set of gating mechanisms comprising an input gate, a forget gate, and an output gate.\nThese gates collectively empower LSTMs to selectively retain or discard information over extended\nperiods enabling the model to effectively capture and utilize the inherent long-term dependencies\nwithin the data. These gates control the flow of information between cells, allowing the network to\nremember or forget information when necessary. This structure makes LSTM more effective in\nprocessing long sequence data than ordinary RNN, and can avoid the vanishing gradient problem\nfaced by traditional RNN, avoiding the vanishing gradient problem faced by traditional RNNs. In our\nmodel, the addition of LSTM greatly improves the model's performance in processing time series\ndata, especially short-term data dependencies. In the scenario of smart grid prediction, this means that\nLSTM can effectively capture short-term changes in grid load, energy demand, etc. In addition, the\nability of LSTM lies in processing and memorizing important events in time series, which is crucial\nfor making accurate predictions in dynamic and changing power grid environments. These\ncharacteristics of LSTM are crucial and play an important role in enhancing the prediction accuracy\nand efficiency of the power grid startup strategy. It not only supports real-time monitoring of the grid,\nbut also provides in-depth insights into grid behavior, helping grid operators make more precise\ndecisions. By integrating LSTM, our model not only improves the understanding of power grid\ndynamics, but also enhances the ability of the power grid system to cope with various challenges and\npromotes the development of smart grid technology. The operation process of the LSTM model is\nshown in Figure 3."}, {"title": "", "content": "it = I (Wxixt + Whiht\u22121 + WciCt-1 + bi)  (7)\nwhere: it: Output of the input gate, x\u2081: Current input, ht\u22121: Hidden state from the previous\ntime step, Ct-1: Cell state from the previous time step,Wxi, Whi, Wci: Weight matrices, b\u2081: Bias.\nReset Gate :\nft = I (Wxfxt + Whfht-1 + WcfCt-1 + bf)  (8)\nwhere: f\u2081: Output of the forget gate Other variables are similar to the input gate.\nCandidate Cell State:\n\u010ct = tanh(Wxcxt + Whcht\u22121 + bc) (9)\nwhere: \u010ct: New cell state, tanh: Hyperbolic tangent function.\nUpdated Cell State:\nCt = ft. Ct-1 + it. \u010ct (10)\nwhere: ct: Updated cell state, ft: Output of the forget gate, it: Output of the input gate, \u010ct:\nNew cell state.\nOutput Gate:\nOt = I (Wxoxt + Whoht\u22121 + Wcoct + bo) (11)\nwhere: ot: Output of the output gate, W\u040c, Wh\u045c, Wc\u045c: Weight matrices for input, hidden\nstate, and cell state, x\u2081:Current input h\u2081\u20131: Hidden state from the previous time step, c\u2081: Cell state\nat the current time step, bo:Bias."}, {"title": "3.4 PSO: Particle Swarm Optimization", "content": "The particle swarm optimization (PSO) algorithm is an optimization algorithm based on swarm\nintelligence, which is inspired by the collective behavior of a flock of birds or a school of fish[37]. In\nthe PSO algorithm, each \"particle\" represents a potential solution in the problem space and optimizes\nits position by tracking and imitating the best-performing particles in the population. Each particle\nhas its position and velocity, with the position representing a potential solution and the velocity\ndetermining the direction and speed of the search. The particles fly in the solution space, constantly\nadjusting their direction based on their own and the group's experience to find the optimal or near-\noptimal solution. The PSO algorithm helps the model better adapt to and predict the complex data of\nsmart grids by accurately adjusting the parameters of the Transformer and LSTM models. This\noptimization ensures that the model is not only improved in training efficiency but also achieves\nhigher accuracy when handling grid prediction tasks. In the context of smart grids, the complexity\nand dynamic changes of grid data require models to be highly adaptable and accurate. The PSO\nalgorithm enables our model to effectively cope with these challenges by optimizing model\nparameters, thereby playing a key role in the efficient and reliable operation of smart grids. The flow\nchart of PSO is illustrated in Figure 4."}, {"title": "", "content": "x\u2081(t + 1) = x(t) + vi(t + 1)  (12)\nwhere: x\u2081(t) is the position of particle i at time t,and vi(t + 1) is the velocity of particle i\nat time t +1.\nvi(t + 1) = w \u2022 v\u2081(t) + c\u2081 \u00b7 r\u2081 \u00b7 (pbest\u2081 \u2212 x\u2081(t)) + c2 \u00b7 2 \u00b7 (gbest \u2013 x\u2081(t)) (13)\nwhere: vi(t) is the velocity of particle i at time t, w is the inertia weight, C\u2081 and C2 are\nlearning factors, r\u2081 and r2 are random numbers in the range [0,1], pbesti is the personal best\nposition of particle i, and gbest is the global best position.\nPersonal Best Position Update:\npbest;(t + 1) = x(t+1), iff(x(t+1)) <f(pbest;(t))\n(pbest(t), otherwise (14)\nwhere: f is the objective function of the optimization problem, pbest\u2081(t) and pbe.st\u2081(t + 1)\nare the personal best positions of particle i at times t and t + 1, respectively.\nGlobal Best Position Update:\nXi gbest(t + 1) = {(xi(t+1), iff(x(t+1)) < f(gbest(t))\n(gbest(t), otherwise (15)\nwhere: gbest(t) and gbest(t+1) are the global best positions at times t and t+\n1,respectively.\nInertia Weight Adjustment:\nW Wmax - (\\frac{Wmax-wmin}{tmax})t (16)\nwhere: Wmax and Wmin are the maximum and minimum values of the inertia weight, t is the\ncurrent iteration number, and tmax is the maximum number of iterations."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1 Datasets", "content": "Our research used four datasets: The NYISO (New York Independent System Operator) Electric\nMarket dataset, EIA Electric Power Dataset, ENTSO-E European Power System Dataset, IEA\nelectricity dataset.\nNYISO Electric Market dataset[38]: This dataset comprises a comprehensive collection of data\nabout the operation of the electric power market within the state of New York, USA. This dataset\nencompasses information on market prices, load data, power generation details, transmission\ninfrastructure, market transactions, renewable energy generation, weather conditions, demand\nresponse programs, regulatory updates, and historical market data. It serves as a vital resource for\nmarket participants, researchers, analysts, and policymakers, enabling market analysis, price\nprediction, grid management, and policy formulation to ensure the efficient and reliable functioning\nof the New York electricity market.\nEIA Electric Power Dataset[39]: The EIA Electric Power Dataset, provided by the U.S. Energy\nInformation Administration (EIA), offers comprehensive information about the U.S. electric power\nsystem. This dataset encompasses data on electricity production, consumption, and distribution\nthroughout the United States. It includes details on power sources, generation methods, electricity\nprices, regional loads, electricity markets, and more. The EIA Electric Power Dataset serves as a\ncrucial resource for government agencies, energy companies, research institutions, and analysts,\nproviding in-depth insights into the operation and trends of the U.S. electric power system. It plays a\nvital role in energy policy formulation, market analysis, and power system planning, contributing\nsignificantly to the understanding of U.S. electricity supply, renewable energy integration, and energy\nconsumption."}, {"title": "4.2 Experimental Environment", "content": "ENTSO-E Power System Dataset[40]: This dataset comprises a comprehensive collection of\ndata about the operation, management, and performance of electrical power systems across European\ncountries. This dataset encompasses critical information, including grid operation status, load patterns,\npower generation sources, market transactions, transmission infrastructure, renewable energy\ngeneration, weather conditions, grid stability, and historical data. It serves as a vital resource for\nenergy operators, policymakers, researchers, and analysts, enabling grid management, market\nanalysis, renewable energy integration, cross-border trading, and informed policy decisions to ensure\nthe efficient and sustainable functioning of the European electricity grid.\nIEA electricity dataset[41]: The International Energy Agency (IEA) provides a comprehensive\nelectricity dataset that is updated monthly and includes a wide range of information related to the\nelectricity sector. This dataset encompasses electricity production and trade statistics for OECD\nmember countries and a selection of other economies. It features an interactive data explorer for\ndynamic data analysis. Additionally, the dataset covers various aspects such as electricity and heat\nsupply and consumption, electricity and heat generation, net electricity, and heat production by\nautoproducers, and net electrical capacity for OECD countries and selected other nations. This rich\ndataset is instrumental in understanding the role of electricity in modern economies, its contribution\nto final energy consumption, and its significance in the context of transitioning towards net-zero\nemissions by 2050. The IEA's Energy Statistics Data Browser further enhances this dataset by\noffering extensive statistics, charts, and tables on multiple energy topics, catering to over 170\ncountries and regions, thus providing a comprehensive view of global electricity and energy trends.\nOur experiments were conducted on a server equipped with an Intel Xeon E5-2690 v4 CPU and\n128 GB of DDR4 RAM, ensuring robust processing capabilities for complex computations. The\nserver also featured four NVIDIA Tesla V100 GPUs, each with 32 GB of memory, facilitating\nefficient data processing. The system ran on Ubuntu 20.04 LTS, using Python 3.8 with TensorFlow\n2.4 and PyTorch 1.7 libraries to support the computational demands of the study."}, {"title": "4.3 Experimental Details", "content": ""}, {"title": "4.3.1. Data preprocessing", "content": "Data Cleaning: In this step, we will identify and handle missing values, outliers, and duplicates\nin the dataset. For missing values, if the proportion of missing values is small (less than 3\\%) and will\nnot have a significant impact on the analysis results, we consider directly deleting the samples or time\npoints where the missing values are located. At the same time, we use interpolation methods (linear\ninterpolation, spline interpolation, etc.) to fill in missing values to maintain the continuity of the time\nseries. Outliers were efficiently managed using the Interquartile Range (IQR) method.\nData Standardization: To enhance the model's performance and stability, data standardization\nwill be implemented. This involves transforming the data into a form with similar scales and\ndistributions. We use a normalization method (Z-score normalization) to scale the data to a range with\na mean of 0 and a standard deviation of 1."}, {"title": "4.3.2. Model training", "content": "Data Splitting: The data set is divided into three parts: training set, validation set, and test set.\nApproximately 70% of the data will be used to train the model, 20% will be used to verify the model's\nperformance and hyperparameter tuning, and the remaining 10% will be reserved for evaluating the\nmodel's performance and generalization ability. This data partitioning process ensures efficient\ntraining and evaluation of our models. These steps are essential to prepare the data adequately for\nbuilding and evaluating deep learning models.\nNetwork Parameter Settings: At this stage, we carefully tune the model's hyperparameters to\noptimize performance. We chose an Adam optimizer with a learning rate of 0.001 to ensure fast and\nstable convergence. The batch size is set to 64, which is a good balance between efficiency and\nmemory usage. To prevent overfitting, we add a dropout ratio of 0.5 at the appropriate layers of the\nnetwork. In addition, to accurately tune the model performance, we set 500 training epochs and use\nan early stopping strategy when the performance on the validation set does not improve.\nModel Architecture Design: Our model adopts a multi-layer architecture integrating Transformer\nand LSTM layers. Specifically, the model includes three Transformer encoding layers, each with 12\nattention heads, to capture long-term dependencies in power grid data. Next are two stacked LSTM\nlayers with 128 hidden units each, specifically designed to handle short-term dynamics in time series\ndata. Finally, the model outputs predictions through a fully connected layer with 256 neurons and\nuses the ReLU activation function.\nModel Training Process: During the training process, we first thoroughly shuffled the entire data\nset to ensure the randomness and representativeness of the data. Next, the model performs forward\npropagation and backpropagation on the training set to learn the mapping relationship from input data\nto predicted output. After each training cycle, we evaluate model performance on the validation set\nand adjust hyperparameters as needed. To ensure the effectiveness of training, we monitor key metrics\nsuch as loss function values and accuracy and make necessary adjustments when signs of overfitting\nare found. Through this iterative approach, the model gradually reaches higher accuracy and\ngeneralization capabilities.\nAlgorithm 1 outlines the training flow presented in this paper.\nAlgorithm 1: Training of Transformer-LSTM-PSO Network\nInitialize model parameters: WT, WL, WP ;\nInitialize PSO parameters: C1, C2, w;\nInitialize personal best positions: Pbest;\nInitialize global best position: Gbest;\nInitialize velocities: VT, VL, VP;\nWhile not converged do\nSample a batch of data: X, Y;\nCalculate loss using Transformer: LT ;\nCalculate loss using LSTM: LL;\nCalculate loss using PSO: Lp ;\nCalculate total loss: Ltotal = LT + LL + LP ;\nUpdate model parameters: WT, WL, WP ;\nUpdate PSO swarm positions and velocities: Pi, Vi;\nUpdate personal best positions for PSO particles:Pbesti;\nIf improved personal best then\n| Update global best\nposition: end\nend\nTransfer Learning: Pretrain model on IEA Electricity Dataset: Wpretrain;\nFine-tune on NYISO Electric Market Dataset: Wfine-tuned;\nEvaluation: MAE, RMSE, and other metrics;\nEnd"}, {"title": "4.3.1. Model Evaluation", "content": "Model Performance Metrics: To measure the effectiveness of the Transformer-LSTM-PSO\nmodel in smart grid startup prediction, we adopt specific evaluation metrics. These metrics include,\nbut are not limited to, root mean square error (RMSE), mean absolute error (MAE), and coefficient\nof determination (R-squared). RMSE and MAE are used to measure the prediction accuracy of the\nmodel, while R-squared is used to evaluate how well the model fits the observed data. We evaluate\nthe performance of the model by comparing the values of these metrics to determine its predictive\ncapabilities in smart grid launch scenarios.\nCross-Validation: We divided the dataset into multiple subsets and then trained and evaluated\nthe model multiple times, each time using a different subset as the validation set. The most commonly\nused is K-fold cross-validation, where K represents the number of subsets. Through cross-validation,\nwe can more fully evaluate the performance of the model, reduce the randomness introduced by\nsplitting the data set, and discover differences in the performance of the model on different subsets of\nthe data.\nHere, we introduce the primary evaluation metrics used in this paper:\nMAE:\nMAE = \\frac{1}{n}\\Sigma_{i=1}^{n}|y_i-\\hat{y}_i| (17)\nWhere: n is the number of samples, yi is the true value of the ith sample, \u0177\u00a1 is the predicted value\nof the ith sample.\nRMSE:\nRMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}(y_i - \\hat{y}_i)^2} (18)\nWhere: n is the number of samples, yi is the true value of the ith sample, \u0177\u00a1 is the predicted value\nof the ith sample.\nSMAPE:"}, {"title": "", "content": "SMAPE = \\frac{100\\%"}, {"19)\nWhere": "n is the number of samples", "sample.\nR2": "nR2 = 1 - \\frac{\\Sigma_{i=1}^{n}(y_i-\\hat{y}_i)^2}{\\Sigma_{i=1}^{"}]}