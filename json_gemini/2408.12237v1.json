{"title": "Weight Scope Alignment: A Frustratingly Easy Method for Model Merging", "authors": ["Yichu Xu", "Xin-Chun Li", "Le Gan", "De-Chuan Zhan"], "abstract": "Merging models becomes a fundamental procedure in some applications that consider model efficiency and robustness. The training randomness or Non-I.I.D. data poses a huge challenge for averaging-based model fusion. Previous research efforts focus on element-wise regularization or neural permutations to enhance model averaging while overlooking weight scope variations among models, which can significantly affect merging effectiveness. In this paper, we reveal variations in weight scope under different training conditions, shedding light on its influence on model merging. Fortunately, the parameters in each layer basically follow the Gaussian distribution, which inspires a novel and simple regularization approach named Weight Scope Alignment (WSA). It contains two key components: 1) leveraging a target weight scope to guide the model training process for ensuring weight scope matching in the subsequent model merging. 2) fusing the weight scope of two or more models into a unified one for multi-stage model fusion. We extend the WSA regularization to two different scenarios, including Mode Connectivity and Federated Learning. Abundant experimental studies validate the effectiveness of our approach.", "sections": [{"title": "1 Introduction", "content": "As a technique for combining multiple deep models into a single model, model fusion [36, 26] [36] has gained widespread applications across various domains, including Mode Connectivity [4, 9, 10] and Federated Learning [1, 40, 55]. First, the model interpolation could shed light on the properties of the mode connectivity in neural networks [16, 11, 14]. Then, due to data privacy protection, transmitting intermediate models across edge nodes and fusing them on the server has been the common procedure in federated learning [50, 35, 39]. To be brief, model fusion matters a lot in these applications and has attracted a wide range of research interest. The primary goal of model fusion is to retain the capabilities of the original models while achieving improved generalization, efficiency, and robustness.\n\nThe coordinate-based parameter averaging is the most common approach for model fusion in deep neural networks [40, 36, 52]. The research on mode connectivity involves a linear or piece-wise interpolation between models [16, 48], while federated learning takes the averaging of local models from edge nodes for aggregation [40, 7]. Although parameter averaging exhibits favorable properties, it may not perform optimally in more complex training scenarios, especially when faced with various training conditions or Non-Independent and Identically Distributed (Non-I.I.D.) data. For example, the Non-I.I.D. data in federated learning means that the data of local nodes are naturally heterogeneous, making the model aggregation suffer from diverged update directions [21, 24]. Additionally, the property of permutation invariance that neural networks own exacerbates the challenge of model fusion because of the neuron misalignment phenomenon [10, 57, 5, 15]. Hence, solutions have been proposed from the aspect of element-wise regularization [35, 1, 7] or mitigating the permutation invariance [37, 3, 45, 43]. Few of these methods, however, have considered the impact of weight ranges across models on model fusion.\n\nIn this paper, we investigate the influence of different training conditions on model weight distributions (defined as Weight Scope) and further study model merging under various weight scopes. We first conduct several experiments under various training hyper-parameters or data quality conditions and find that the weight scopes of the converged models differ a lot, a phenomenon we define as \"weight scope mismatch\". Figure 1 illustrates the model weight distributions under different training conditions, revealing noticeable differences despite all distributions being approximated by Gaussian distributions. Specifically, the top five sub-figures are parameters from models that use the same optimizer, while the bottom ones take different optimizers. In the rightmost of Figure 1, the linear interpolation results that reflect the mode connectivity property are also provided. Clearly, the mismatched weight scope leads to a worse linear interpolation, highlighting the impact of weight range inconsistency on model fusion. To intuitively explain, parameters with similar distributions can be aggregated more easily, whereas those with dissimilar distributions often present challenges in model merging."}, {"title": "2 Related Works", "content": "Model fusion is a fundamental technique in several applications. The related scenarios and solutions are introduced as follows.\n\nModel Merging in Mode Connectivity. Visualizing loss landscape is an intuitive way to understand the mode connectivity [33, 17, 38], where the landscape is shown in a 2-dim or 3-dim space via model interpolations. The model interpolations between the initialization and the converged model could reflect the monotonic linear interpolation phenomenon [16, 11, 51, 46]. [13] bridges the model connectivity and lottery ticket hypothesis [12], proposing a method for model pruning. Mode connectivity is also related to the model optimization and generalization [29, 8].\n\nThe work [16] also points out that two independent minima suffer a barrier in their linear interpolation, which attracts several solutions to decrease the barrier. [9, 14] make a notable discovery that the independent minima are possible to be connected via a simple piece-wise or quadratic curve. [42, 52] find that the minima finetuned from the same pre-trained model could mitigate the barrier in linear interpolation. [10] guesses that the independent minima are located in the same basin with the consideration of permutation invariance, conjecturing that the minima matched via the simulated annealing algorithm encounter no barrier. [45] propose the weight-based and activation-based matching method via the optimal transport. [47] decreases the barrier via both the permutation alignment and the quadratic curve. [3] employs three distinct neuron-matching methods to corroborate the low-barrier hypothesis. Although the permutation invariance is considered in these works, the mismatch of weight scope in neural networks is also fundamental to model fusion. Our proposed method could further decrease the barrier on the basis of these works.\n\nModel Merging in Federated Learning. Federated learning (FL) aims to break the limitation of data privacy, utilizing a server that collaborates with client devices to train a model [55]. As the most standard algorithm in FL, FedAvg [40] takes a simple coordinate-based parameter averaging on the server to accomplish the model fusion process. A huge challenge that FL faces is the Non-I.I.D. data [21, 28], where the inherent data heterogeneity leads to weight divergence during local training [24]. Applying coordinate-wise regularization on local models is a popular solution to solve the Non-I.I.D. challenge in FL. [34] claims that weight decay [32] can lead to divergent optimization objectives among Non-I.I.D. clients in FL. [60, 35] introduce a proximal term in local optimization. This term helps align local optimization more closely with given model parameters, facilitating model aggregation. Additionally, [28] implements constraints on the local gradient directions of each client, nudging them closer to a global direction. However, the coordinate-wise regu-"}, {"title": "3 Proposed Methods", "content": "3.1 Preliminaries\n\nWe introduce a collection of models, denoted as K, with the size of the collection |K| \u2265 1. Each model within this collection is constructed based on the same underlying model architecture. The distinctiveness among the models in K arises from the variations in their weight layers, which are derived from training under various hyper-parameters or different data.\n\nTo encapsulate the statistical characteristics of the weight layers across different models, we denote the weight matrix of the lth layer in the kth model as $w_l^k$. Our assumption is that the elements within this matrix follow a Gaussian distribution which is characterized by its mean $\u00b5_l^k$ and standard deviation $\u03c3_l^k$. The assumption is rational and we will empirically present the weight distributions of the converged model in Section 5.1. Formally, the distribution of the weight matrix $w_l^k$ is as follows:\n\n$p(w_l^k) = N(\u00b5_l^k, (\u03c3_l^k)^2),$ (1)\n\n$\u00b5_l^k = \\frac{1}{|w_l^k|} \\sum_{w \u2208 w_l^k} w, \u03c3_l^k = \\sqrt{\\frac{1}{|w_l^k|} \\sum_{w \u2208 w_l^k}(w - \u00b5_l^k)^2}.$ (2)\n\nIn Equation 2, we apply Maximum Likelihood Estimation to derive the standard deviation $\u03c3_l^k$ and mean $\u00b5_l^k$ of the weight $w$.\n\n3.2 Weight Scope Alignment\n\nFirstly, we introduce Weight Scope Regularization. Then, for more complex multi-stage fusion, we propose Weight Scope Fusion for better target alignment.\n\nWeight Scope Regularization. Given a weight distribution $N(\u00b5, \u03c3^2)$ and a target weight distribution $N(\\tilde{\u00b5}, \\tilde{\u03c3}^2)$, our method endeavors to ensure consistency between them. This consistency is crucial for guaranteeing scope-matched distribution among the new models in the subsequent model fusion. To achieve this, we employ a divergence measure known as the Kullback-Leibler (KL) divergence, specifically focusing on calculating the divergence between the two univariate Gaussian distributions, which is given by:\n\n$D_{KL} = \\frac{1}{2} [log(\\frac{\\tilde{\u03c3}^2}{\u03c3^2}) + \\frac{\u03c3^2 + (\u03bc - \\tilde{\u03bc})^2}{2\\tilde{\u03c3}^2} - \\frac{1}{2}],$ (3)\n\nwhere $\u00b5\u0303, \u03c3\u0303$ are hyperparameters. By minimizing the KL divergence between the training models' weight distribution and their goal, it ensures that the weight distributions are closely aligned, facilitating more effective and harmonious model fusion.\n\nWeight Scope Fusion. In some complex scenarios with large amounts of models and multiply stages of model merging, a given pre-defined weight distribution is not adaptable. Therefore, we propose a method named Weight Scope Fusion to enhance the applicability of Weight Scope Regularization. Focusing on the weights of the lth layer, we assume that each weight $w_l^k, k \u2208 K$, follows its own Gaussian distribution, and they are independent of each other. We can get the fused Gaussian distribution $N(\\tilde{\u00b5}^l, (\\tilde{\u03c3}^l)^2)$ by:\n\n$\\tilde{\u00b5}^l = \\frac{1}{|K|} \\sum_{k \u2208 K} \u00b5_l^k, \\tilde{\u03c3}^l = \\sqrt{\\frac{1}{|K|} \\sum_{k \u2208 K} (\u03c3_l^k)^2}.$ (4)"}, {"title": "3.3 Analysis and Comparisons with Other Methods", "content": "We next provide some analysis of the proposed simple method, especially the comparisons with existing works.\n\nComparison with Weight Decay. Weight decay [32] stands as a cornerstone in model regularization, advocating for weight constraints that push the weights towards zero and promote uniformity. Considering a weight vector, denoted as $w \u2208 R^n$, with elements $[w_1, w_2,..., w_n]$. For ease of analysis, we define the mean, standard deviation, and $L2$ norm of this vector as follows: $\u00b5 = \\frac{1}{n} \\sum_{i=1}^n w_i, \u03c3 = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (w_i - \u00b5)^2}, ||w||_2 = \\sum w_i^2$.\n\nThe weight decay term is formulated as $\u03bb||w||_2^2$. We can express weight decay in terms of \u00b5 and \u03c3 as follows:\n\n$\\frac{\u03bb}{2} ||w||_2^2 = \\frac{\u03bbn}{2} (\u03c3^2 + \u00b5^2).$ (5)\n\nThe weight decay term brings the mean and variance of model weights close to zero, and several research [1, 28] has designed FL algorithms with adjustment of weight decay term in local training. However, the impact of weight decay on models can not determine the shape of weight distributions and could not align parameter distributions under different conditions. Hence, it does not necessarily lead to a harmonization of the model scopes before fusion.\n\nComparison with Proximal Term. The proximal term is used to keep the model weight w and another one $w' \u2208 R^n$ as close as possible during the update of w, which is represented as $||w - w'||_2^2$. In FedProx [35], the proximal term constrains the local models to stay close to the global model, thereby ensuring stability during model fusion. For clarity, we also define $\u00b5\u0303$ and $\u03c3\u0303$ as the mean and standard deviation of vectors $w'$. The proximal term can be decomposed as follows:\n\n$||w - w'||_2^2 = n\u03c3^2 + n\u00b5^2 + n\\tilde{\u03c3}^2 + n\\tilde{\u03bc}^2 - 2 \\sum_{i=1}^n w_i\\tilde{w_i}.$ (6)\n\nBecause the part of $n\\tilde{\u03c3}^2 + n\\tilde{\u03bc}^2$ can be seen as a constant term, the proximal term differs from weight decay by only an additional term of $-2 \\sum_{i=1}^n w_i\\tilde{w_i}$, which encourage the weights in w to align with the direction of $w'$ as closely as possible. However, the restriction is too strict because it requires the direction alignment for each specific element. This may limit the normal training process and perturb the effects of other loss functions.\n\nIn the context of model fusion, weight decay represents a more flexible approach, while the proximal term seems to introduce directional constraints additionally. Both of them have overlooked the influence of weight scope on model fusion and are unable to align weight scopes, as demonstrated in Figure 11. Our proposed method aims to achieve consistency in model weight ranges for improved model fusion results."}, {"title": "4 Applications of WSA", "content": "This section presents the applications of Weight Scope Alignment to mode connectivity and federated learning. They can be seen as one-stage and multi-stage model fusion scenarios, respectively. While the specific processes or the model set to be fused may differ across them, they all share the common thread of leveraging the proposed WSA as a constraint during training.\n\n4.1 Mode Connectivity\n\nGiven two well-trained models, $w_1$ and $w_2$, the loss barrier along their linear interpolation path is:\n\n$\\mathop{max}\\limits_{\u03b1\u2208 [0,1]} L ((1-\u03b1)w_1 + \u03b1w_2) \u2013 [(1 \u2013 \u03b1)L (w_1) + \u03b1L (w_2)],$ (7)\n\nwhere \u03b1 is the interpolation coefficient, and the loss barrier represents the point of maximum loss increase along the linear interpolation path. A higher loss barrier suggests that the two models may not be in the same basin within the loss landscape, while a lower loss barrier indicates linear mode connectivity.\n\nBoth OTFusion [45] and Git Re-basin [3] have improved the way they perform model interpolation by considering neuron matching. They calculate matching relationships \u03a0 between the weights of each layer in the two models, using a permutation matrix or optimal transport matrix, leading to the fusion formula:\n\n$(1 \u2013 \u03b1)w_1 + \u03b1\u03a0w_2.$ (8)\n\nOur approach can easily be integrated into the aforementioned model interpolation methods to achieve improved mode connectivity. For separately trained models, they are typically randomly initialized from the same distribution, and their weight scopes are similar. The mean \u00b5\u0303 and standard deviation \u03c3\u0303 of the target weight distribution are the hyperparameters and keep invariant during each model's training. We incorporate a Weight Scope Regularization term (i.e., Equation 3) to ensure that the weight range remains close to the target weight scope. With our method, the weight scope of the converged models are matched, which is beneficial for searching the matching matrix.\n\n4.2 Federated Learning\n\nModel fusion is a fundamental procedure in Federated Learning (FL), i.e., improving the performance of joint training by merging models trained on different data sources. Its challenge lies in the fact"}, {"title": "Algorithm 1 FedAvg with WSA", "content": "Input: model w, number of rounds T, local iteration steps \u03c4, parameters \u03b7, \u03bb, Client k \u2208 K in parallel do: Set wk \u2190 w for s = 0, ..., \u03c4 \u2013 1 local iterations do Update wk \u2190 wk \u2212 \u03b7\u2207L(wk; Dk, p) using loss in Equation 10 Send {wk, {\u00b5k, \u03c3k}\u2113\u2208[L]} to the server Global server: Update global model w \u2190 P k\u2208K wk Update \u00b5\u2113, \u03c3\u2113 using Equation 4\n\nAlgorithm 1 FedAvg with WSA Input: model w, number of rounds T, local iteration steps \u03c4, parameters \u03b7, \u03bb"}, {"title": "5 Experiments", "content": "In Section 5.1, we verify that different training conditions lead to variations in weight scope, and the differences between weight scope can affect the performance of model merging. In Section 5.2, we explore the effectiveness of the WSA method in the mode connectivity scenario. In Section 5.3, we demonstrate the performance improvements achieved by WSA in various federated learning scenarios and analyze the impact of the method.\n\n5.1 Basic Experiments\n\nObservations. Our basic assumption is that the weight scope could be formulated as the Gaussian distribution. Hence, we first study the weight distributions of the models. The first observation is that the converged weight scope is irrelevant to the way of weight initialization, e.g., the Kaiming uniform or the Kaiming normal initialization method [18]. Figure 3 shows the weight distributions of several layers in VGG16 [44] with BatchNorm [23], where we train the network on CIFAR-100 [31] for 200 epochs. In the Appendix, we will present\n\nThe Influence of Weight Scope on Model Fusion. We then investigate the difference of weight scope under different conditions, which include: 1) hyperparameters, e.g., optimizer, batch-size, learning rate, and weight decay; 2) data quality, e.g., label imbalance, label noise, feature noise, and data size. For each condition, we select two specific settings, and then train models under the same setting or not. For example, the optimizer could be SGD or Adam [30], and we train three models with each using SGD, SGD, and Adam as the optimizer, respectively. Then, we plot the weight scopes of these models and investigate the linear interpolation accuracy. Figure 1 shows that models trained using different optimizers own varying weight scopes and the interpolation meets an obvious barrier. The details of training conditions and the corresponding illustration results can be found in the Appendix.\n\nFrom Figure 1, we can observe that the weight scopes under the same training condition are near the same. In fact, we calculate their KL divergence, and the results are near zero. Hence, we calculate the average KL divergence of weight scopes under different conditions, e.g., the average divergence of Gaussian distributions in the bottom five sub-figures (the \"KL D.\" column in Table 1). Additionally, we calculate the barriers of the two interpolated curves, and their difference, which are listed in columns of \"Ba(=)\", \"Ba(\u2260)\", and \"Diff\" in Table 1. In the table, the \"Ba(=)\" column is commonly smaller than \"Ba(\u2260)\", which verifies that models under the same condition are indeed similar in weight scopes. Empirically, for models under different conditions, a larger KL divergence corresponds to a larger"}, {"title": "Model Fusion under Manual Scaling", "content": "The above experiments could not support that the weight scope is the cause of the barrier in model fusion. Thanks to the scale invariance of neural networks, we could manually scale the networks and create the weight scope mismatch phenomenon. Specifically, we train two networks under the same condition with different random initialization. To avoid overuse of symbols, we denote them as $w_1$ and $w_2$, respectively. To create the scope mismatch, we select one layer in $w_2$ and multiply its weight by \u03b1, and divide its following layer's weight by \u03b1. The obtained model is denoted as $w_{2,\u03b1}$, which performs the same as $w_2$. However, when taking interpolations between $w_1$ and $w_{2,\u03b1}$, the interpolation curves differ a lot. The results are shown in Figure 4, where the \"Scale\" in the legend denotes \u03b1. Using \u03b1 = 1.0 means no scaling. Clearly, scaling the layers could make the barrier more obvious, especially in VGG8. Experimental details and more analysis can be found in the Appendix."}, {"title": "5.2 Performance in Mode Connectivity", "content": "To investigate the effectiveness of our approach on mode connectivity, we apply the proposed WSA to OTFusion [45] and Git Re-Basin [3]. Specifically, we first initialize and train two models, and plot the vanilla interpolation curve. Then, we use the activation-based OTFusion method to search for an alignment and plot the interpolated curve after matching. Finally, we replace the models with another two models trained using the WSA as introduced in Section 4.1 and also use the OTFusion to search the alignment matrix. The results are shown in Figure 5. OTFusion could decrease the barrier of vanilla interpolation, and our proposed WSA could improve its performance further.\n\nThen, we combine our WSA with Git Re-Basin, which searches the permutation matrix instead of an optimal transport matrix compared with OTFusion. Similar to OTFusion, we also train models"}, {"title": "5.3 Performance in Federated Learning", "content": "To simulate real-world federated learning (FL) scenarios, we initially consider image classification tasks across three datasets: CIFAR-10 [31], CIFAR-100 [31], and CINIC-10 [6]. We explore two different client scenarios: \"cross-device\" with 100 small-data clients where only 10 clients participate in each training round, and \"cross-silo\" with 10 clients, each having larger datasets and engaging each training round. We use the same CNN architecture used in [28, 25]. To ensure Non-I.I.D. data distribution in data partitioning, we employ the Dirichlet distribution for creating heterogeneous data on each client [56, 37]. We compare with several state-of-the-art (SOTA) methods, including FedAvg [40], FedProx [35], SCAFFOLD [28], and FedExp [25]. Throughout all experiments, unless otherwise specified, we use a default batch size of 50, 20 local update steps, and 1500 communication rounds. Additional details and results are provided in the appendix.\n\nTable 2 presents the performance comparison of CNN model trained in various FL settings with 10 clients where FedAvg+WSA outperforms FedAvg in performance. Moreover, as a plug-in, we can easily adapt it to existing FL methods, and the experiments indicate that incorporating WSA leads to significant improvements in each method. This underscores the effectiveness of our approach in model fusion of FL. Table 3 shows that similar results are observed with 100 clients. Furthermore, Figure 9 illustrates the adaptability of WSA to deeper models on CIFAR-10.\n\nTo further understand the impact of WSA on model merging, we conduct experiments on the CIFAR-10 dataset over various durations. We consider three different intervention times during training: from the start of training until iteration [0, 1], from iteration [t1, T] until convergence, and during the iterations within the interval [t1, t2]. Figure 10 displays the convergence curves for these three scenarios. In the first scenario, we observe that the longer the intervention time, the better our method enhances the convergence performance. In the second one, we find that employing our method earlier results in greater performance improvements and faster convergence. Lastly, even if our method only intervenes during the initial [0, 125] rounds, it still significantly boosts performance. Drawing inspiration from the concept of a \"critical learning period\" [54, 2], we conclude"}, {"title": "6 Conclusion", "content": "In this study, we investigate the impact of different weight ranges of model parameters on model merging. We observe that models trained under various conditions exhibit inconsistencies in their weight ranges, leading to a decrease in fusion performance. To address this issue, we propose Weight Scope Alignment method, which utilize Weight Scope Regularization to constrain the alignment of weight scopes during training, ensuring that the model's weights closely match the specified scope. Moverover, for multi-stage fusion scenarios, we design Weight Scope Fusion to fuse the weight scopes"}, {"title": "7 Dataset and Network Details", "content": "7.1 Datasets\n\nIn this paper, we use SVHN [41], CIFAR-10 [31], CIFAR-100 [31], CINIC-10 [6] and CUB [49] datasets. We provide a general introduction to these datasets as follows:\n\n1. SVHN [41]: The Street View House Numbers (SVHN) dataset, a real-world image collection used in Figure 5, features small, cropped digits captured from street view images, often exhibiting variations in rotation. It comprises 73,257 digit images for training and 26,032 for testing, spanning across 10 classes.\n\n2. CIFAR-10 [31]: The CIFAR-10 dataset comprises 60,000 32x32 color images, evenly distributed across 10 classes with each class containing 6,000 images.\n\n3. CIFAR-100 [31]: The CIFAR-100 dataset consists of 100 classes, each with 600 images. These classes are further divided into 20 superclasses, grouping similar categories together.\n\n4. CINIC-10 [6]: The CINIC-10 dataset, containing a total of 270,000 images, is derived from two sources: ImageNet and CIFAR-10. It is divided into three subsets: training, validation, and testing, with each subset comprising 90,000 images. In our study, the validation subset is not utilized.\n\n5. CUB [49]: The Caltech-UCSD Birds-200-2011 (CUB) dataset is designed for fine-grained visual categorization tasks, featuring 11,788 images across 200 bird subcategories. It is divided into 5,944 training images and 5,794 testing images.\n\n7.2 Networks\n\nIn the work, our used networks include VGG [44], ResNet [19], Wide ResNet [58], ResNeXt [53], MobileNet-v2 [20]. Here is a general introduction to these networks as follows:\n\n1. CNN: The model architecture consists of two convolutional layers, the first with 32 5\u00d75 filters and the second with 64 5\u00d75 filters, followed by two linear layers containing 384 and 194 neurons, respectively. A softmax layer concludes the architecture. This configuration is commonly utilized in studies [40, 25, 28].\n\n2. VGG [44]: VGG models consist of multiple convolution layers and fully-connected layers for classification, typically with 11, 13, 16, or 19 layers. We do not employ batch normalization in VGG11, VGG13, VGG16, or VGG19 (referred to as VGG11-BN, VGG13-BN, VGG16-BN, and VGG19-BN when batch normalization is used). Additionally, we utilize 8-layer and 9-layer VGG networks without batch normalization (VGG8 and VGG9), as mentioned in [56, 37].\n\n3. ResNet [19]: ResNet, leveraging residual connections and batch normalization, aims for enhanced performance and robustness. We use ResNet-18 and ResNet-32 in this work. The implementation code we use is provided by [25].\n\n4. Wide ResNet [58]: Wide ResNet (WRN), an extension of ResNet, varies in width to offer different capacities. Due to increased training time with wider networks, we explore variations like WRN16x2, 22x2, 28x2, 34x2, 40x2, and 22x3, 22x4, 22x5, 22x6.\n\n5. ResNeXt [53]: ResNeXt substitutes group convolutions for partial convolutions in ResNet. We utilize the pre-trained ResNeXt models available in PyTorch for analyzing weight scope distributions, as they offer aggregated residual transformations.\n\n6. MobileNet-v2 [20]: MobileNet-v2, known for its compact structure suitable for portable devices, is employed in our study us-"}, {"title": "8 Experimental Details", "content": "8.1 Weight Scope Mismatch\n\nWe first explain the experimental setup for the phenomenon of weight scope mismatch in Figure 1, which the influenced factor is the type of optimizer. Specifically, Figure 1 trains VGG8 network on CIFAR-10 dataset with 200 epochs. We first use SGD optimizer to individually train two models, which are named M0 and M1, respectively. Then we use Adam optimizer to train model M2. We select the parameters including \"conv1.weight\", \"conv3.weight\", \"conv3.bias\u201d, \u201cconv5.weight\", and \"fc.weight\" and show their weight distributions. Other hyperparameters are listed as follows: the weight decay is 10-5, the batch size is 128, the learning rate is 0.03 for SGD optimizer, while the learning rate is 0.0003 for Adam. The momentum for SGD is 0.9 by default. The weight distributions are visualized by seaborn 1.\n\nFurthermore, we explore the following key factors that may influence the weight scope of parameters:\n\n1. Learning Rate: we use 0.03 for M0 and M1, while we use 0.001 for M2.\n\n2. Weight Decay: we use 10-5 for M0 and M1, while we use 10-3 for M2.\n\n3. Batch Size: we use 32 for M0 and M1, while we use 1024 for M2.\n\n4. Optimizer: we use SGD for M0 and M1, while we use Adam for M2.\n\n5. Feature Noise: we add gaussian noise to the input features, sampled from the gaussian distribution N (1.0, \u00b2). We set \u00b2 = 0.0 for M0 and M1, while we set \u00b2 = 0.1 for M2.\n\n6. Label Noise: the label of each training sample is flipped to a random class with a probability of p. We set p = 0.0 for M0 and M1, indicating no label flipped. We set p = 0.1 for M2.\n\n7. Dataset Size: a fraction q of data is randomly selected to train the model. We set q = 1.0 for M0 and M1, while we set q = 0.1 for M2.\n\n8. Label Imbalance: we split the training data into two parts. The first part contains 90% samples of the first five classes and 10% samples of the last five classes, while the second part contains other samples. We train M0, M1 on the first part, while we train M2 on the second part.\n\nAside from the investigated factor, the other hyperparameters are set as same as Figure 1, ensuring the training data remains original without any noise or sampling. Then, we could plot figures for each factor which are similar as Figure 1. Figure 13 and Figure 14 illustrates different dataset size or learning rate similarly lead to lead to weight scope mismatch and impact model interpolation.\n\nThen we provide explanations for each of the columns in Table 1."}, {"title": "8.4 Mode Connectivity", "content": "In applying mode connectivity, our method is compared with model averaging (Vanilla), OTFusion [45] and Git-rebasin [3] across different datasets and models. It is important to note that OTFusion and Git-rebasin are designed for different scenarios, necessitating separate comparative analyses. To make a comparison with OTFusion, we conduct experiments with VGG8 on SVHN and VGGBN8 on CIFAR10, as illustrated in Figure 5. Additionally, we present results for MLP on MNIST and ResNet-32 on CIFAR100 in Figure 16. These results collectively demonstrate the benefits of weight scope alignment in enhancing mode connectivity within permutation-based model fusion methods.\n\nFurthermore, we conduct comparisons with Git-rebasin [3] on the CIFAR-10 dataset using ResNet-18 and WRN. In this experiment, we train two models with different random seeds for 100 epochs and then compute their loss barriers on the test set using Equation 7, with a learning rate of 0.01 and a weight decay of 10\u22124. The experimental results are displayed in Figure 6."}, {"title": "8.5 Federated Learning", "content": "To simulate various clients, we employ a Dirichlet distribution to allocate data across clients for the CIFAR-10, CIFAR-100, and CINIC-10 datasets [22], utilizing the parameter \u03b1 to control data heterogeneity. Figure 17 and Figure 18 depict the data distribution among clients for \u03b1 values of 0.5 and 1.0, respectively, with lower \u03b1 values indicating increased data heterogeneity. The training convergence curves in 10 clients and 100 clients setting are illustrated in Figure 19.\n\nOur experiments are conducted using the FedExP framework 2. The experimental setup includes a learning rate of 0.01, weight decay of 10-4, a decay of the learning rate by 0.998 in each round, a maximum gradient norm of 10, a fusion alpha of 1.0, and 20 local steps for all models and datasets. Specifically for FedProx, \u00b5 is set to 0.1, 1, and 0.001 for CIFAR-10, CIFAR-100, and CINIC-10, respectively. In the case of FedExp, \u03b5 is maintained at 0.001 which is identified as the optimal value for all three datasets in [25]."}, {"title": "8.5.1 Additional Experiments", "content": "Hyperparameter sensitivity In Equation 10, \u03bb is the hyperparameter that controls the strength of weight scope regularization. We explore the sensitivity of the hyperparameter \u03bb by testing values in [1,"}]}