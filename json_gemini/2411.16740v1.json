{"title": "Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents", "authors": ["Jun Chen", "Dannong Xu", "Junjie Fei", "Chun-Mei Feng", "Mohamed Elhoseiny"], "abstract": "Large multimodal models (LMMs) have achieved impressive progress in vision-language understanding, yet they face limitations in real-world applications requiring complex reasoning over a large number of images. Existing benchmarks for multi-image question-answering are limited in scope, each question is paired with only up to 30 images, which does not fully capture the demands of large-scale retrieval tasks encountered in the real-world usages. To reduce these gaps, we introduce two document haystack benchmarks, dubbed DocHaystack and InfoHaystack, designed to evaluate LMM performance on large-scale visual document retrieval and understanding. Additionally, we propose V-RAG, a novel, vision-centric retrieval-augmented generation (RAG) framework that leverages a suite of multimodal vision encoders, each optimized for specific strengths, and a dedicated question-document relevance module. V-RAG sets a new standard, with a 9% and 11% improvement in Recall@1 on the challenging DocHaystack-1000 and InfoHaystack-1000 benchmarks, respectively, compared to the previous best baseline models. Additionally, integrating V-RAG with LMMs enables them to efficiently operate across thousands of images, yielding significant improvements on our DocHaystack and", "sections": [{"title": "1. Introduction", "content": "Large Multimodal Models (LMMs) [1, 20, 30, 41] have made remarkable progress in the vision-language understanding. However, these models still face challenges when tasked with reasoning over extensive collections of images or documents [43], limiting their effectiveness in real-world applications, such as visual search or querying over large sets of images or documents, like those stored on personal devices or in photo albums. However, there lacks such proper benchmarks to evaluate these capabilities. To address this gap, we introduce the DocHaystack and InfoHaystack benchmarks, designed to evaluate LMMs on large-scale image retrieval and understanding capabilities, pushing the boundaries of LMM performance in complex, real-world scenarios.\nThe existing multi-image retrieving and reasoning benchmarks are primarily constructed on a small scale, as highlighted in works such as [32, 37]. Each question in these benchmarks is paired with only up to 30 images as illustrated in Figure 1 (a). However, this limited scope does not align well with real-world scenarios, which often require retrieval and reasoning across hundreds or thousands of images or documents. In contrast, our established benchmarks, depicted in Figure 1 (b), allow for querying questions from a large-scale collection of up to 1,000 documents, necessitating that models retrieve and reason over an extensive set of documents for each question. This scale better simulates practical applications and their demands.\nThe main challenge in constructing such benchmarks is collecting specific questions while ensuring there are no ambiguous answers across a large set of images. Existing datasets, such as those in DocVQA and InfographicVQA [27, 28], contain numerous \u201cgeneral\u201d questions, like \"What is the table number?\", where answers could be derived from multiple images, leading to non-unique responses. To address this, we implemented a rigorous data filtering pipeline. First, we employed both a large language model (LLM) and human annotators to systematically filter out \"general\" questions based on carefully defined criteria. Additionally, we used the LLM to exclude questions relying on generic knowledge, such as \"What is the capital of Missouri?\", which can be answered without image context. This approach ensures that the questions in the benchmark can only be answered through specific visual cues from the provided images, maintaining the benchmark's integrity for evaluating image-based understanding.\nTo enable the current LMMs effectively reason over a large number of images, we propose a vision-centric"}, {"title": "2. Related Works", "content": "VQA benchmarks. VQA play a critical role in assessing a model's ability to understand and reason across visual contexts [6, 11, 12]. Traditional VQA datasets typically measure a model's comprehension of object attributes [15, 19], spatial relationships [15], as well as its understanding of documents [27, 28], charts [26], mathematics [23, 40, 46], and open knowledges [25, 35]. Additionally, these benchmarks explore models' knowledge across varied fields, including science and the arts [22, 44]. This broad array of benchmarks has greatly advanced vision-language models by cultivating diverse visual comprehension skills, particularly for modern foundation models in vision-language understanding [1, 3, 8, 20, 22, 30, 31, 39, 47]. Notably, these benchmarks have primarily focused on question answering within single image or document. In contrast, our benchmark shifts the focus towards retrieval and comprehensive understanding across a large collection of visual documents, presenting new challenges and expanding the scope of visual question answering.\nSeveral previous efforts have tackled the challenge of vi-"}, {"title": "3. DocHaystack and InfoHaystack Benchmarks", "content": "To support effective retrieval and reasoning across extensive document collections, we present two new benchmarks\u2014DocHaystack and InfoHaystack\u2014designed to ensure each question yields a unique, document-specific answer. Derived from DocVQA [28] and InfographicVQA [27], these benchmarks address the challenge of answer ambiguity by selectively curating questions that can only be answered by a single document within a large dataset.\nBenchmark construction pipeline. There exists many general questions in the existing benchmarks and lead to multiple answers for different document context. For example, general questions like \u201cWhat is the table number?\u201d may apply to various documents and yield multiple valid answers, while a targeted question like \"Who is the reviewer for the article titled\n'An antithyroid factor in milk'?\" is likely to produce a unique answer, as only a single document or a limited set of documents would contain that information. Therefore, our benchmark construction follows a structured three-step filtering pipeline, illustrated in Figure"}, {"title": "4. Method", "content": "Current large multimodal models (LMMs) face substantial challenges when reasoning across hundreds or thousands of images, due not only to context length limitations but also to the inherent complexity of the task. This issue is particularly pronounced in our benchmarks, which contain 1k document files requiring high-resolution images to capture and interpret small-font text effectively. To enable LMMs to perform reasoning over a substantial number of documents, we introduce a vision-centric retrieval-augmented generation (V-RAG) framework. V-RAG efficiently retrieves a re-"}, {"title": "Task definition.", "content": "Given a question q and a collection of\nN documents D = {D1, ..., DN }, the V-RAG framework\naims to retrieve the top-k most relevant documents to sup-\nport LMMs understanding and answering the question q. V-\nRAG accomplishes this through a two-step retrieval process\ndesigned to effectively identify and rank relevant documents\nfor each question."}, {"title": "Vision encoder ensemble.", "content": "Document files often contain\na mix of text, symbols, and visual elements across various\nscales, requiring vision encoders to capture a comprehen-\nsive understanding of these complex structures. To effi-\nciently handle this diversity, we represent each document\nas an image and utilize an ensemble of vision encoders,\nincluding CLIP [33], SigLIP [45], and OpenCLIP [16],\neach bringing distinct strengths to the image understand-\ning, as depicted in Figure 3. For example, the ConvNext\nencoder [21] from OpenCLIP [16] is particularly effective\nfor high-resolution image encoding. We compute the sim-\nilarity score between each question q and all documents in\nthe document set D according to Equation 1, with similarity\nscores from each encoder represented as $S_{ime}, Sim_{s}, and\nSim_s$ respectively."}, {"title": null, "content": "S(q, D) = cos($t(q), \u03c6v(Dj)) | Dj \u2208 D, (1)"}, {"title": null, "content": "where S denotes the computing the similarity between the\nquery q and a collection of documents D. cos denotes the\ncosine similarity. t denotes the text encoder, and v denotes the vision encoder."}, {"title": null, "content": "To derive a final relevance score, we calculate the aver-\nage similarity Simavg for each question-image pair by com-\nbining Sime, Sim., and Sims. We then rank the images\nbased on Simavg in descending order, selecting the top-m\nmost relevant images according to their similarity scores."}, {"title": "LMM-filter module.", "content": "To refine the selection of top-m rel-\nevant images further, we introduce a LMM-based question-\nimage relevance assessment module. This module evaluates\nthe relevance between each question and the top-m images\nidentified in the first filtering step. Specifically, we pair each\nimage with the question text and input them into an open-\nsource vision-language model, prompting, \u201cCan this image\nprovide answers to this question? Respond only with yes\nor no\u201d. We only retain the question-image pairs that are\nidentified as \"yes\" from LMM, and remove other irrelevant\nimages."}, {"title": "LMM-VQA module.", "content": "Achieving high top-1 ranking accu-\nracy in image retrieval is challenging, so we retain the top-\nk images from the LMM-filtered ranking list and present\nthem to the LMM-VQA to improve the likelihood of includ-\ning relevant images. We input these top-k images along-"}, {"title": "5. Experiments", "content": "In the experiments section, we will primarily describe\nour training setup, covering evaluation metrics, baseline\nmodels, and the fine-tuning procedure for the LMM-VQA\nmodel. We also present the main experimental results along\nwith an ablation study to provide further insights."}, {"title": "5.1. Training setup", "content": "Metric. In our evaluation of the DocHaystack and Info-\nHaystack benchmarks, we employ a model-based assess-\nment by leveraging GPT-40-mini [30] to accurately deter-\nmine whether the model predictions match target answers.\nThis method uses a carefully structured prompt to facili-\ntate GPT-40-mini's evaluation of answer correctness. We\nempirically found that the model-based evaluation achieves\nhigher consistency and alignment with human judgment.\nAdditional details on the prompt design are provided in the\nAppendix.\nFor the document retrieval evaluation, we report the\nbaseline results using Recall@1, Recall@3, and Recall@5\nmetrics. These metrics enable a thorough assessment of re-\ntrieval accuracy across varying levels of precision.\nBaselines. In our experiment, we have evaluated sev-\neral open and closed-sourced vision-language models on\nthe retrieval and VQA performance. For the large multi-\nmodal model, we used the gpt-4o-2024-08-06 version of\nGPT-40 [30], the LLaVA-OneVision-Qwen2-7b-OV-HF ver-\nsion of LLaVA-OneVision [20], and the Qwen2-VL-7B-\nInstruct version of Qwen2-VL [3]. For computing the\ntext-to-image similarities, we employed the Jina-CLIP-\nv1 [18] variant, Nomic-Embed-Vision-v1.5 [29] variant,\nCLIP [33] ViT-L/14@336 variant, for SigLIP [45], the\nViT-SO400M/14@384 variant, and for OpenCLIP [16],\nthe ConvNeXt-XXL@1024 variant as well as text-based\nmethod, BM25.\nIn our V-RAG setting, we apply LLaVA-OneVision-\nQwen2-7b-OV-HF for the LMM-filter module and Qwen2-\nVL-7B-Instruct for the LMM-VQA module. We select m as\n60 and k as 5 in our experiment.\nOptimizing the LMM-VQA module. To improve the ro-\nbustness of the LMM-VQA model in handling visual ques-\ntion answering with multiple distractor images, we further\nfine-tune the model using our curated training data.\nDuring this fine-tuning process, we introduce 1-10 ran-\ndomly sampled distractor images for each question, creating\na challenging setting that encourages the model to focus on\nrelevant content amid a mix of positive and negative images.\nThe fine-tuning is conducted with a batch size of 32 and a"}, {"title": "5.2. Main Experimental Results", "content": "We evaluated a range of open-source and closed-source\nvision-language models for VQA tasks. We also evaluate\nseveral text-to-image and text-to-text (with OCR) retrieval\nmodels to evaluate their retrieval capabilities on our bench-\nmarks. More detailed performance analysis are described in\nthe following sections.\nRetrieving results. The retrieval results in Table 2 demon-\nstrate the superiority of our proposed V-RAG framework\nover several baseline methods across both DocHaystack and\nInfoHaystack benchmarks. V-RAG consistently achieves"}, {"title": "5.3. Ablation Studies", "content": "Ablation study on Top-k Selection. This figure presents\nthe top-k selection ablation analysis for LMM-VQA\nacross four models: LLaVA-OV, Qwen2-VL, GPT-40,\nand the fine-tuned Qwen2-VL (Qwen2-VL-f.t.), evaluated\non the DocHaystack-100/1000 and InfoHaystack-100/1000\nbenchmarks. The analysis reports VQA accuracy as a func-\ntion of top-k selection (Top 1, Top 3, and Top 5). Overall,\naccuracy tends to improve with larger k-values, suggest-\ning that offering more retrieval options positively impacts\nmodel performance. However, for LLaVA-OV, there is a\nmarked decrease in performance at top-5, indicating that\nthis model struggles to process multiple images at this scale.\nAblation study on the V-RAG framework components.\nThe ablation study in Table 4 highlights the contribu-\ntions of each component in the V-RAG framework on the\nDocHaystack-1000 and InfoHaystack-1000 benchmarks.\nUsing CLIP alone yields low performance (e.g., Re-\ncall@1 of 23.85% on DocHaystack-1000 and 45.81% on\nInfoHaystack-1000), indicating its limited retrieval capabil-\nity on its own. Adding SigLIP and OpenCLIP incrementally\nimproves results.\nThe highest performance is achieved when all three en-\ncoders are combined with the VLM-filter module, leading\nto Recall@1 scores of 66.06% on DocHaystack-1000 and\n64.52% on InfoHaystack-1000. This setup also achieves\nthe top Recall@1, Recall@3 and Recall@5 values, demon-\nstrating that the VLM-filter is essential for refining the en-\nsemble outputs and significantly improving retrieval accu-\nracy. These results confirm that each module contributes to\nV-RAG's overall effectiveness."}, {"title": "6. Conclusion", "content": "In this work, we introduced the DocHaystack and Info-\nHaystack benchmarks to evaluate LMMs for retrieving and\nreasoning across large-scale documents. Our benchmarks\nproviding a more rigorous and realistic assessment of large\nmultimodal models in real-world, large-scale retrieval sce-\nnarios. To tackle these challenges, we proposed V-RAG,\na vision-centric retrieval-augmented generation framework\nthat significantly enhances retrieval precision and overall"}, {"title": "9. Question Filtering Pipeline", "content": "General questions can typically be answered from multiple documents, with several possible correct answers. For example, the question \u201cWho wrote the letter\u201d is a general question that can be answered by any document containing a letter. Generic Knowledge refers to information or facts that are widely accessible and can be answered using general world knowledge, often independent of specific visual or contextual cues from accompanying content, such as images. For example, the question \"How many events were featured in the 2014 Winter Olympics?\u201d is a generic knowledge that can be answered without accessing any image. In visual question answering tasks such as DocVQA and InfographicVQA, generic knowledge introduces a language bias when large language models (LLMs) rely on pre-existing knowledge rather than visual content, thereby undermining the focus on image-based reasoning. Therefore, it is important to exclude such questions to evaluate the true image-based reasoning capability of models. In this section, we show how we filter the data to extract the specific question."}, {"title": "9.1. General Question LLM Filtering", "content": "First, we leverage an LLM to filter out the general questions. This approach allows for the automatic filtering of numerous general questions. We give some filtered general"}, {"title": "9.2. General-question manual review", "content": "However, the LLM-based filtering is not entirely accurate. Therefore, in the second stage, we involve manual filtering, where annotators are tasked with filtering out any general questions that were missed by the LLM. The filtered general question by human are as follow."}, {"title": "9.3. Generic knowledge filtering", "content": "After filtering out the general questions, we leverage GPT to filter out the remaining questions that can be answered by generic knowledge. The following is a list of some filtered questions."}, {"title": "9.5. InfoHaystack Random 20 Final Questions", "content": "We randomly sample 20 questions in our final list of Info-Haystack as below."}]}