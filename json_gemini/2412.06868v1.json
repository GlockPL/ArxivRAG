{"title": "Compression for Better: A General and Stable Lossless Compression Framework", "authors": ["Boyang Zhang", "Daning Cheng", "Yunquan Zhang", "Fangmin Liu", "Wenguang Chen"], "abstract": "This work focus on how to stabilize and lossless model compression, aiming to reduce model complexity and enhance efficiency without sacrificing performance due to compression errors. A key challenge is effectively leveraging compression errors and defining the boundaries for lossless compression to minimize model loss. i.e., compression for better. Currently, there is no systematic approach to determining this error boundary or understanding its specific impact on model performance. We propose a general LossLess Compression theoretical framework (LLC), which further delineates the compression neighborhood and higher-order analysis boundaries through the total differential, thereby specifying the error range within which a model can be compressed without loss. To verify the effectiveness of LLC, we apply various compression techniques, including quantization and decomposition. Specifically, for quantization, we reformulate the classic quantization search problem as a grouped knapsack problem within the lossless neighborhood, achieving lossless quantization while improving computational efficiency. For decomposition, LLC addresses the approximation problem under low-rank constraints, automatically determining the rank for each layer and producing lossless low-rank models. We conduct extensive experiments on multiple neural network architectures on different datasets. The results show that without fancy tricks, LLC can effectively achieve lossless model compression. Our code will be made publicly.", "sections": [{"title": "1. Introduction", "content": "The scale and complexity of Deep Neural Networks (DNNs) have rapidly increased, driving up memory and FLOPS demands. To tackle these challenges, model compression has become a crucial method for improving efficiency, reducing energy consumption, and speeding up inference. However, achieving effective compression without sacrificing performance remains a key challenge. As such, model compression must balance two critical objectives: maximizing the compression ratio while preserving model performance. In this work, we focus on post-training compression, where the model is compressed after training without requiring modifications to the training process. Its advantage is that the computing resource consumption is low and there is no need to adjust the model training process.\nIn such scenarios, most existing compression schemes focus on maximizing the compression rate while trying to optimize the performance of the compressed model. Taking model quantization and matrix decomposition as examples, quantization significantly speeds up inference and reduces model size by using lower bit widths to represent tensors. For example, Banner et al. [1] minimized tensor-level errors to partially mitigate the accuracy loss incurred during quantization, achieving model performance close to the unquantized original model in the low-bit case. HAWQ [3] employed layer-wise sensitivity metrics to determine the precision of different layers, striking a good balance between error and compression ratio.\nOn the other hand, matrix decomposition decomposes the weight matrix into two or more smaller matrices, using these smaller matrices during actual storage and computation. Compared to other compression methods, matrix decomposition has a solid mathematical foundation and can retain the latent information in the data. For example, Yu et al. [21] combined feature map reconstruction and low-rank factorization to effectively reduce the number of parameters in fully connected layers. Hsu et al. [7] incorporated weighted Fisher information into singular value decomposition to reduce the model degradation after decomposition. Although these methods strive to reduce the performance degradation caused by compression at different compression"}, {"title": "2. Related Works", "content": "2.1. Quantization\nQuantization uses low-bit-width representations for tensors while maintaining their dense format, aiming to reduce model storage and computational overhead. In typical setups, mixed-precision quantization strategies are employed, where different layers are assigned varying bit-widths based on their sensitivity to quantization. This approach minimizes performance loss after compression. For example, HAQ [16] used reinforcement learning to determine the quantization strategy for each layer, incorporating feedback from hardware accelerators to improve computational efficiency. AutoQ [11] introduced a layered deep reinforcement learning (DRL) method that sequentially determines kernel bit-widths. HAWQ [3] employed the top Hessian eigenvalues to measure each layer's sensitivity to quantization, providing a relative sensitivity score, although bit-width allocation still relies on manual selection. HAWQ-V2 [4] replaced this with the trace of the Hessian matrix. BRECQ [10] further introduced block-wise optimization, which used different granularities of quantization to significantly reduce the model degradation induced by quantization. While these methods narrow the performance gap between the compressed and original models in practice, model degradation is still difficult to fully avoid, even under 8-bit quantization. Moreover, although these methods are effective empirically, they lack a principled explanation of optimality. Furthermore, bit-width assignment for each layer leads to an exponentially growing search space, decreasing efficiency.\n2.2. Decomposition\nTraditional decomposition methods, such as Singular Value Decomposition(SVD), CANDECOMP/PARAFAC(CP), and Tucker decomposition, involve decomposing model weight matrices and directly assigning the decomposed weights back to the original model. However, this approach often leads to significant increases in model loss, typically rising 5-10 times compared to the original model. To mitigate this issue, existing methods incorporate fine-tuning after decomposition, which entails retraining to reduce the loss. Yu et al. [21] leveraged weight structure information by combining low-rank weight matrices and feature map reconstruction to reduce fully-connected layer parameters. Xu et al. [19] integrated low-rank approximation with regularization into the training process, achieving a notable reduction in performance degradation. Yang et al. [20] introduced an SVD-based decomposition training method that first decomposes each layer into full-rank forms and then retrains the decomposed weights. Zhang et al. [23] used multiple low-rank matrices to approximate gated recurrent unit (GRU) weight matrices and subsequently retrained the model. While these methods can mitigate loss through fine-tuning, they still often yield some level of model degradation and entail significant time costs in the retraining phase.\nThe above methods aim to reduce the gap between the compressed and original models. In contrast to the view that compression inevitably leads to degradation, we aim to offer a method where model loss consistently decreases after compression, without requiring fine-tuning or other additional steps-in other words, compression yields gains."}, {"title": "3. Lossless Theoretical Framework", "content": "Basic Analysis. The theoretical framework of LLC is primarily based on the mathematical properties of extreme points, oriented to the loss function, and aims to reduce the loss value and improve the model performance. In general, for an n-layer neural network model, the loss of the model is optimized according to the following equation\n$\\min _{W} f(W)=\\mathbb{E}_{S a m p l e} l(W, Sample)=\\frac{1}{m} \\sum_{(x i, y i) \\in \\mathbb{D}} l(W, x i, y i)=\\mathcal{L}(\\operatorname{model}_{n}(x i, W), y i)$,\n$\\operatorname{model}_{n}=h_{1}(h_{2}(h_{3}(h_{4}(\\ldots(h_{n+1}, W_{n}) \\ldots, w_{4}), w_{3}), w_{2}), w_{1})$\nwhere $f(.)$ represents the loss of the model on a dataset, $\\mathbb{E}$ stands for expectation, $m$ is the size of the dataset, $l(.)$ is the loss function for a sample, and $(x i, y i)$ denotes a sample in the dataset along with its corresponding label, $\\mathcal{L}(.)$ represents the loss function, such as the cross-entropy function; $h_{i}$, with $i \\in[1, \\ldots, n]$, represents the $(n-i+1)$ th layer in the neural network; $W=\\left(w_{n}, w_{n-1}, \\ldots, w_{1}\\right)^{T}$, where $w_{i}$ is the parameter in $h_{i}(.)$; and for the reason of a unified format, $h_{n+1}$ denotes the sample $x$. This form ensures that LLC is independent of the specific network architecture, making it applicable to different models.\nCompression techniques such as quantization and decomposition are mathematically considered to be the process of adding noise to the original weights and activations of the model. After compression, for a sample, the model loss $l$ during the inference process is restated as\n$l(w, x i, y i)=\\mathcal{L}(h_{1}(h_{2}(h_{n}(h_{n+1}+\\epsilon_{n}, W_{n}+\\delta_{n})+\\epsilon_{n-1}, W_{2}+\\delta_{2})+\\epsilon_{1}, W_{1}+\\delta_{1}), y i)$\nwhere $\\delta i, i \\in 1, \\ldots, n$, and $\\epsilon i, i \\in[1, \\ldots, n]$ are errors caused by compression, such as data type conversion in quantization and low-rank error in decomposition.\nLLC directly associates the compression noise error and the change of the loss function through total differentials. According to total differentials, the following equation can be obtained\n$l(\\tilde{w}, x i, y i)-l(w, x i, y i)=\\sum_{i=1}^{n} \\frac{\\partial l}{\\partial h_{i+1}} \\epsilon_{i}+\\frac{\\partial l}{\\partial w_{i}} \\delta_{i}+\\frac{1}{2}(\\epsilon i, \\delta i)^{T} \\mathcal{H}(\\epsilon i, \\delta i)+O(\\|(\\epsilon i, \\delta i)\\|*)$\nwhere $\\mathcal{H}$ represents the Hessian matrix and $O(\\|(\\epsilon i, \\delta i)\\|n)$ represents the high-order term, $\\cdot$ is inner product and $*$ is the scalar product. For the loss on whole dataset, we can gain\n$\\min f(\\tilde{w})-f(w)=\\frac{1}{m} \\sum_{(x j, y j) \\in \\mathbb{D}} \\sum_{i=1}^{n} \\frac{\\partial l}{\\partial h_{i+1}} \\epsilon_{i}+\\frac{\\partial l}{\\partial w_{i}} \\delta_{i}+(\\epsilon i, \\delta i) \\mathcal{H}(\\epsilon i, \\delta i)+O(\\|(\\epsilon i, \\delta i)\\|*)$\nwhere $f(\\tilde{w})=\\sum l(.)$. This equation directly links compression and model performance (Loss). Thus, we can optimize the above expression to make $f(\\tilde{w})-f(w)<0$, meaning that the loss after compression is smaller than the original model's loss.\nTheorem 3.1. The total differential describes the increment of a smooth, differentiable function under arbitrarily small parameter changes.\nTheorem 3.1 sets limitations on the use of the total differential: first, the function must be smooth and differentiable, and second, parameter changes must be sufficiently small. According to the chain rule, multilayer neural networks are continuously differentiable with respect to all parameters, meaning they are inherently smooth and differentiable. Therefore, Eq. 4 generally satisfies $C^{k}$ continuity. As the scale of compression governs the parameter changes, we primarily focus on the magnitude of noise.\nLemma 3.2. The total differential relies on a linear approximation assumption, valid only when the changes in the function's variables are sufficiently small.\nWhen the variations $\\epsilon, \\delta$ are small enough, the actual change in the loss function can be accurately described by the total differential $d f$. The lemma above outlines the theoretical range for noise. Thus, it is essential to identify this \"sufficiently small\" threshold within the practical model.\nNoise neighborhood mapping. Since each layer can accommodate different noise sizes, we set the noise function to $c_{i}(.)$. We then calculate the gap $U(x)$ between theory and practice in Eq. 4\n$U_{s k}(x i) : \\mid l(w \\pm \\delta, x i, y i)-\\left(l(w, x i, y i)+\\sum_{i=1}^{n} \\frac{\\partial l_{k}}{\\partial w_{i}} . \\delta+\\frac{1}{2}(\\delta) \\mathcal{H}(\\delta)+O(\\|(\\delta)\\|*)\\right) \\mid$\n$U_{\\epsilon k}(x i) : \\mid l(w, x i, y i)-\\left(l(w, x i, y i)+\\sum_{i=1}^{n} \\frac{\\partial l_{k}}{\\partial h_{i+1}} . \\epsilon+\\frac{1}{2}(\\epsilon) \\mathcal{H}(\\epsilon)+O(\\|(\\epsilon)\\|*)\\right) \\mid$\nthe left side represents the loss due to actual noise disturbances, while the right side represents the theoretical loss induced by noise. The k controls the compression level, such as 4/8-bit or rank. Eq. 5 and Eq. 6 calculate the noise bounds for weights and activations, respectively. LLC measures the change in loss through perturbation of the first- and second-order terms. As shown in Fig. 1, the perturbation noise boundaries under LLC and their dominant influencing orders are illustrated. First, for activations, when the noise range is below $10^{-3}$, the first-order term is the dominant factor, and the gradient can be treated as the primary optimization target, with the influence of higher-order terms negligible. This is because higher-order errors decay exponentially compared to lower-order ones. When the noise range is between $\\[10^{-3}, 8 \\times 10^{-2}\\]$, both the first- and second-order terms significantly affect the loss, thus the second-order Hessian information should be included in the optimization target. For noise levels above $8 \\times 10^{-2}$, the negative impact increases, and higher-order terms need to be considered.\nFor weights, ideally, the first-order term in a trained-well model should be zero. However, in practice, gradients are rarely zero, so they must be included in the optimization. When the noise is less than $8 \\times 10^{-3}$, the first-order term should be the main optimization target. When the noise range is between $\\[8 \\times 10^{-3}, 2 \\times 10^{-1}\\]$, the second-order term's influence should be considered. When the noise exceeds $2 \\times 10^{-1}$, the influence of weight noise on the loss becomes significant, and higher-order terms should not be omitted. This range defines the noise boundaries and the dominant terms in the compression process.\nMultiple experiments show that weights have higher noise tolerance than activations, meaning weights can be deeply compressed, whereas activations cannot. This phenomenon aligns with the consensus that weights are more easily compressed. Since our focus is on stable and efficient lossless compression, experimental results show that the first-order terms dominate across all noise ranges, while the contribution of higher-order terms exponentially decays and has a minimal effect on loss changes. In the quantization process, we evaluated the impact of the second-order terms, and the error loss was found to be below 0.00001. In decomposition, due to the lack of consideration for the full covariance structure of the original data, errors introduced by the decomposition can severely distort the Hessian matrix, leading to incorrect estimates. Furthermore, the computation of second-order terms is computationally expensive and time-consuming. Therefore, in our analysis, given the dominant\neffect of the first-order terms and the consideration of time efficiency, LLC omits higher-order terms, as their impact on performance is negligible. Thus, LLC mainly focuses on gradient-driven lossless compression.\nLLC Framework. The LLC framework is a highly efficient, lossless compression method based on the first-order analysis range. Within the first-order range, the Eq. 4 is updated to $\\min f(\\tilde{w})-f(w)=\\frac{1}{m} \\sum_{\\left(x_{j}, y_{j}\\right) \\in \\mathbb{D}} \\sum_{i=1}^{n} \\frac{\\partial l}{\\partial h_{i+1}} \\epsilon_{i}+\\frac{\\partial l}{\\partial w_{i}} \\delta$. We need to find appropriate noise vectors $\\epsilon$ and $\\delta$ to obtain a model with minimal loss. When the inner product is negative, the compressed model's loss is lower than the full-precision model, meaning we find noise vectors opposite to the model's gradient direction. Thus, in theory, the goal of lossless compression is achieved.\nTo select the appropriate compression noise, we must ensure that it opposes the model gradient direction. Taking activation compression as an example, we will first explain the rationale behind this choice. Using the language of probability theory, we describe $\\frac{\\partial l}{\\partial h_{i+1}} \\epsilon$ for $\\epsilon$ is a stochastic vector naturally. Let $\\epsilon=\\left\\[\\epsilon_{1}, \\epsilon_{2}, . ., \\epsilon_{k}\\right\\]$ and $\\epsilon z$ is i.i.d. random variable. We also set that $p=\\left\\[p_{1}, p_{2}, \\ldots, p_{k}\\right\\]$ and $p i$ represent i.i.d. random variable. $\\epsilon$ and $p$ are independence to each other, $k$ is the length of the vector. Alternatively, $p i$ can be treated as the random variable with different distributions or directly use $\\mathbb{E} p$ vector in analyses. The conclusions are the same or close with current analysis. We have $\\frac{\\partial l}{\\partial h} . \\epsilon=\\sum_{i=1}^{k} \\epsilon i * p i$ and Eq. 7\n$\\mathbb{E} \\frac{\\partial l}{\\partial h_{i+1}} . \\epsilon=\\mathbb{E} \\sum_{i=1}^{k} \\epsilon_{i} * p_{i}=k \\mathbb{E} \\epsilon \\mathbb{E} p$\nFor a well-trained model, the $\\mathbb{E} p$ can be computed as $\\mathbb{E} p=$ 1. Then to gain a negative $\\mathbb{E} \\frac{\\partial l}{\\partial h_{i+1}} . \\epsilon$, the $\\mathbb{E} \\epsilon$\nshould be different signs with $\\mathbb{E} p$. For specific compression methods, such as quantization, we use different rounding functions to ensure the sign of $\\mathbb{E} \\epsilon$. In decomposition, we calculate the noise direction at different ranks. This type of method is not the only way to obtain a negative inner product, but it is easy to calculate and effective.\nAfter having a compression method, we also need to analyze the performance improvement brought by the compressed model and the probability of obtaining a lower loss model. Based on the above analysis and the Chebyshev's inequality, we can infer\n$P\\left(\\frac{\\partial l}{\\partial h_{i+1}} . \\epsilon \\geq 0\\right)<P\\left(\\left|\\frac{\\partial l}{\\partial h_{i+1}} . \\epsilon-\\mathbb{E} \\epsilon \\mathbb{E} p\\right| \\geq|\\mathbb{E} \\epsilon \\mathbb{E} p|\\right)<\\frac{V a r(\\epsilon p)}{(\\mathbb{E} \\epsilon \\mathbb{E} p)^{2}}=\\frac{V a r(\\epsilon) V a r(p)}{(\\mathbb{E} \\epsilon)^{2}(\\mathbb{E} p)^{2}}+\\frac{V a r(\\epsilon) V a r(p)}{|E e|^{2}|E p|^{2}}$\nHence, when $\\mathbb{E} p$ is larger, i.e., $\\frac{\\partial l}{\\partial h_{i+1}} . \\epsilon$ is larger, $\\operatorname{Var}(p)$ is smaller, making it more likely to obtain good results."}, {"title": "4. LLC Quantization and Decomposition", "content": "Quantization. Lossless mixed-precision quantization addresses two key challenges: first, how to achieve stable lossless compression under mixed-precision quantization; and second, how to efficiently select the optimal quantization bit-width for each layer, which is an NP-hard problem. For the first challenge, LLC quantization is applied for first-order analysis, ensuring lossless quantization within the first-order bounds. The second challenge is reformulated as a group knapsack problem, which is solved efficiently using dynamic programming. In the LLC framework, the loss function is treated as the \"value P\", each layer i is considered a \"group\" with one bit-width j choice per group, and the model size is treated as the \"knapsack capacity W\". This transforms the original problem into a low-computation group knapsack problem, where the goal is to select the optimal bit-width for each layer to minimize loss while keeping the quantized model size within the specified capacity C.\n$\\min \\sum_{i=1}^{n} P\\[i\\]\\[j\\] \\quad \\text { s.t. } \\sum_{i=1}^{n} W\\[i\\]\\[j\\]<C, \\quad j \\in\\[1, k\\], j \\in \\mathbb{Z}$\nwhere n is the number of model layers. The problem scale of the grouped backpack is very small, usually less than n * k, and has a significant efficiency advantage. The overall process of our proposed method is shown in Algorithm 1. In the algorithm, $\\epsilon$ and $\\delta$ are the quantization noise errors of activation and weight. Positive and negative are the choices of different quantization directions. We set the quantization level of Algorithm 1 to k = 4 categories, namely 2/4/8/16 bit. The total time complexity is O(n * k * feature).\nDecomposition. The main challenge of post-training decomposition is how to choose a low rank, so as to reduce the model loss stably while compressing. Under the LLC framework, we view the decomposition problem as a numerical rank-deficiency issue and study how the rank of weight matrices at different layers affects the final model loss. In our decomposition approach, we opt for the simplest low-rank decomposition scheme due to its minimal parameter introduction and highest efficiency.\nWe treat the LLC error calculation boundary as a differential neighborhood and combine it with the low-rank assumption as an inequality constraint in the optimization objective, as shown below\n$\\min _{s k} f(\\tilde{w})-f(w)=\\frac{1}{m} \\sum_{i=1}^{n} \\sum_{\\left(x_{j}, y_{j}\\right) \\in \\mathbb{D}} \\frac{\\partial l}{\\partial w_{i}} \\delta \\quad \\delta \\epsilon \\Delta$\ns.t. $U_{s k} : {\\|\\| W_{i j}-l_{i j} r_{i j}\\|\\|_{F}\\right\\}_{i, j} \\leq \\gamma, \\forall i, j$\n$0<k<\\frac{N M}{N+M}$"}, {"title": "5. Experiment", "content": "5.1. Datasets and Details.\nDatasets. The ImageNet-1K dataset [9] consists of 1.28 million training and 50K validation images. ImageNet-1K is usually used as the benchmark for model compression. SWAG dataset [22] consists of 113k multiple-choice questions about grounded situations. The Stanford Question Answering Dataset (SQUAD) [13] is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers to questions can be any sequence of tokens in the given text. MNLI [18] is a dataset for natural language reasoning tasks. Its corpus is a collection of textual implication annotations of sentences through crowdsourcing. The task is to predict whether the premise sentence and the hypothesis sentence are logically compatible (entailment, contradiction, neutral).\nDetails. The LLC scheme does not involve fine-tuning or retraining. We utilize the ResNet series (including ResNet-18, 34, and 50) to determine the error bounds depicted in Figure 1. In the implementation, error bounds can be flexibly computed using Eq. 5 and Eq. 6 across various models on multiple datasets. Experiments show that, although the error bounds vary, the majority of models fall within this defined range. The parameters $errormax$ and $\\gamma$ are set to approximately $10^{-4}$ in the algorithm. Quantization parameters are calculated using the ACIQ method. The validation set of ImageNet is used as the calibration set, where we check gradients without updating the weights. To ensure fairness, all experiments are conducted under identical optimization settings and executed on two NVIDIA A800 GPUs. The models are implemented based on pre-trained full-precision configurations in PyTorch. The code is implemented in PyTorch.\n5.2. Ablation\nCompressed Noise Bounds. The calculation of error bounds depends on the sensitivity of different models to noise, resulting in varying error bounds for each model. When a model is sensitive to noise, the extent of lossless compression is limited. The error neighborhood extends beyond the analytically manageable range of total differentials, making stable lossless compression unachievable. Firstly, as shown in Table 1, we present the first-order analysis error bounds for different models on ImageNet. The data in the table are the actual calculation results of different models in Eq. 6. When the noise is large, $U_{c k}$ will also increase, indicating that there is a gap between the theoretical calculation results and the actual. According to Equation 3, when the error is less than 1, the second-order term is the square of the error, which further diminishes the influence of the second-order term, establishing that the first-order analysis error is dominant. Experimental results indicate that when noise is low, the actual results for LLC align closely with theoretical predictions. The data in the table suggest that the loss impact from the second-order term is negligible. For instance, if we want the impact of the loss function to be less than 6 * $10^{-5}$, which is the minimum positive number for FP16 ($\\epsilon < \\sqrt{0.00006}$), resulting in a small second-order impact, the first-order derivative estimation performs effectively.\nWeight Gradient and Compression Level. In theory, the weight gradients of a well-trained model should be close to"}, {"title": "5.3. Comparison", "content": "In the comparison experiments, we conduct LLC-based lossless quantization tests alongside standard benchmarks. The lossless experiments are compared against uncompressed models, while the comparison benchmarks are tested against existing methods to highlight the effectiveness of LLC. Specifically, we apply LLC for quantization and decomposition on a range of models across different tasks.\nLossless in Quantization. As shown in Table 2, we quantize activations and weights and validated on ImageNet, CIFAR-100, SQUAD and MNIST datasets. The results indicate that LLC achieves stable, lossless quantization across various models while maintaining high compression rates. On VGG series models, we even employed 2-bit quantization, as some layers were less sensitive to noise, and the INT2 noise boundary still fell within LLC's differential neighborhood on Cifar. In NLP tasks, such as question-answering with BERT, LLC compression continued to show strong performance. Importantly, our focus is on stable, lossless compression rather than striving for lower-bit compression. Additionally, within the bounds of differential analysis, when compression noise opposes the gradient direction and has a larger magnitude (i.e., lower compression), the model loss decreases more substantially.\nComparisons in Quantization. Table 3 compares LLC with various quantization methods at the same compression ratio. Existing methods generally lead to increased loss during quantization, whereas LLC achieves stable, lossless quantization through differential analysis. Although HAWQ slightly improves accuracy on MobileNet, it still incurs higher loss and fails to show consistent accuracy gains on other models. In contrast, LLC demonstrates stable performance across different models, effectively reducing model loss while maintaining broad applicability.\nHAWQ series methods require multiple GPUs for quantization bit-width search, yet still take 30-50 minutes. In contrast, thanks to our efficient grouped knapsack search algorithm, our approach completes bit-width search in under 10 minutes on a single GPU. Additionally, since LLC's lossless quantization process requires no fine-tuning or retraining, the quantization speed is extremely fast, taking less than 5 minutes in total. This demonstrates a significant efficiency advantage.\nLossless in Decomposition. In lossless decomposition, network depth significantly impacts model performance and matrix rank. Based on this, we divide models into shallow and deep categories for experiments, decomposing the linear layers on the ImageNet dataset. Table 4 presents the results of applying LLC to shallow and deep models. The results indicate that LLC enables lossless decomposition across different model architectures. Unlike quantization, decomposition alters the structure of the original parameter matrix, making compression more challenging. Nevertheless, LLC achieves reduced model loss while maintaining compression rates, demonstrating the effectiveness of its first-order differential analysis. Additionally, LLC shows lower loss than the original model and achieves comparable or even higher accuracy in some cases. It is noteworthy that LLC achieves a significant improvement in model loss reduction. This improvement is calculated using $\\frac{OriginalLoss - ModelLoss}{OriginalLoss}$. Since both the gradient and compression noise values are less than 1, the extent of loss reduction cannot theoretically exceed 1.\nComparisons in Decomposition. Table 5 and 6 shows the performance of our proposed LLC method compared to other existing approaches on NLP datasets. Unlike current methods, LLC reliably achieves lossless model decomposition while significantly reducing model loss after compression. All methods in Table 5 use the same compression rate. We compressed the BERT model by 20% while maintaining leading accuracy. LLC consistently achieved loss reduction on both the validation and test sets, further demonstrating the"}, {"title": "6. Conclusion", "content": "This paper introduces a general lossless compression framework designed to achieve stable and lossless model compression. LLC defines the compression neighborhood and higher-order analysis boundaries through total differentiation, specifying the permissible error range for lossless model compression. Ultimately, LLC has been effectively applied to both quantization and decomposition, achieving efficient compression outcomes."}]}