{"title": "From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities", "authors": ["Wanpeng Zhang", "Zilong Xie", "Yicheng Feng", "Yijiang Li", "Xingrun Xing", "Sipeng Zheng", "Zongqing Lu"], "abstract": "Multimodal Large Language Models have made significant strides in integrating visual and textual information, yet they often struggle with effectively aligning these modalities. We introduce a novel image tokenizer that bridges this gap by applying the principle of Byte-Pair Encoding (BPE) to visual data. Unlike conventional approaches that rely on separate visual encoders, our method directly incorporates structural prior information into image tokens, mirroring the successful tokenization strategies used in text-only Large Language Models. This innovative approach enables Transformer models to more effectively learn and reason across modalities. Through theoretical analysis and extensive experiments, we demonstrate that our BPE Image Tokenizer significantly enhances MLLMs' multimodal understanding capabilities, even with limited training data. Our method not only improves performance across various benchmarks but also shows promising scalability, potentially paving the way for more efficient and capable multimodal foundation models.", "sections": [{"title": "1 Introduction", "content": "The development of Multimodal Large Language Models (MLLMs) has made significant progress (Yin et al., 2023; Team et al., 2023; Liu et al., 2024b). However, these multimodal foundation models often model different modalities separately, incorporating many modality-specific designs such as specialized encoders and decoders (Liu et al., 2024b; Zhang et al., 2024; Jin et al., 2023). While this approach allows training data to align well with these modality-specific designs, it often struggles to achieve a unified understanding of multimodal information (Team, 2024). The primary reason for this limitation could be that while encoders of other modalities can learn rich information, without the assistance of the corresponding decoders, LLMs cannot fully comprehend the complex patterns contained within the embeddings provided by the encoder. In other words, LLMs need to learn to interpret the token embeddings again, which is the job of the decoders of other modalities, leading to difficulties in aligning with these modalities (Baltru\u0161aitis et al., 2018)."}, {"title": "2 Notations and Formulation", "content": "Before delving into the theoretical analysis of our proposed paradigm, we introduce key notations and concepts used throughout this paper.\nImage Representation and Quantization. We represent an image as a set of patches X = {Xij}1<i,j\u2264m, where m is the number of patches per row/column, and Xij denotes the patch at position (i, j). We employ Vector Quantization (VQ) to discretize these patches, using a codebook C with size C = |C|, and a quantization function VQ : Rd \u2192 C.\nBPE Image Tokenizer. Our proposed BPE Image Tokenizer converts a quantized image into a sequence of token IDs, defined as enc: X \u2192 T*, where T is the set of tokens and T* is the set of all finite sequences of tokens. A special case of encoding is flattening, flat : X \u2192 Cm\u00b2, which arranges image patches into a sequence row by row.\nUnigram Model. For a unigram model Q \u2208 Q1-gram, given a token sequence t = (t1, ..., t|t|), the probability is defined as:\nQ(t) = Q#(|t|) Qtok(tr). (1)"}, {"title": "3 Theoretical Analysis", "content": "Previous studies have demonstrated that Transformer models struggle to effectively learn certain one-dimensional sequence data (Rajaraman et al., 2024; Makkuva et al., 2024). In this section, we extend this concept to two-dimensional image data, considering a simplified model where the image data-generating distribution follows a 2D kth-order Markov process. We define this model as:\nDefinition 1 (2D kth-order Markov process). For each variable Xi,j, with probability \u00bd, it depends only on Xi\u2212k,j, i.e., Pr(Xi,j = 1|Xi\u2212k,j = 0) = p and Pr(Xi,j = 1|Xi\u2212k,j = 1) = 1-q; With prob- ability, it depends only on Xi,j\u2212k, i.e., Pr(Xi,j = 1|Xi,j\u2212k = 0) = p and Pr(Xi,j = 1|Xi,j\u2212k = 1) = 1 - q.\nThis simplification is intuitive, as pixels in an image often exhibit conditional dependencies with other pixels at specific horizontal and vertical distances. Consequently, real-world image data can be viewed as a composite of multiple such simplified processes. When attempting to learn such sequence data directly using a Transformer model, an interesting phenomenon occurs: the Transformer fails to surpass the performance of the stationary unigram model, regardless of various hyperparameter choices.\nTo formally analyze this phenomenon in a more general setting, we first let m be the number of patches per row in an square image, and let Xij denote the patch at position (i, j) for 1 \u2264 i, j \u2264 m. An image can be represented by the set of m\u00b2 patches {Xij}1<i,j\u2264m, which we abbreviate as X for simplicity. Each patch can be encoded into an index with a VQ-GAN model's codebook, denoted by C, with C = |C| representing the size of the codebook."}, {"title": "5 Experimental Results and Analysis", "content": "Our experiments aimed to evaluate the effectiveness of the proposed BPE Image Tokenizer and its impact on MLLM performance. We analyze the results from three main perspectives: the impact of the BPE Image Tokenizer, the effect of additional data scaling, and the influence of extended vocabulary.\n5.1 Experiments Setup\nBenchmarks: We evaluated our model using multiple benchmarks:\nData Scaling Experiments: To further investigate the impact of dataset size on performance, we gradually augment our training data with RefCOCO (50.6K) (Kazemzadeh et al., 2014), AOKVQA (66.2K) (Schwenk et al., 2022), ShareGPT40 (57.3K) (Chen et al., 2023), and ALLaVA Inst (70K) (Chen et al., 2024).\n5.2 Impact of the BPE Image Tokenizer\nImportance of Two-Stage Training: Our results clearly demonstrate the necessity of the two-stage training process. Models trained only with SFT show notably lower performance compared to those that underwent both PT and SFT. This finding underscores the importance of the pretraining stage in guiding the expanded token embedding layers to learn meaningful visual representations.\nFreezing vs. Full Pretraining: We observe a consistent performance advantage when freezing the text token embeddings during pretraining, i.e., PT(freeze), compared to training all embed- dings, i.e., PT(full). For instance, in the \u2018LLM+VQ+BPE' setting, PT(freeze)+SFT outperforms PT(full)+SFT across all benchmarks, with notable improvements in MM\u0395\u03a1 (1223.5 vs. 1144.6)\n5.3 Impact of Additional Data Scaling\nTo investigate the scalability of our approach, we incrementally add more datasets to both the pretraining and SFT phases. The results, shown in the lower part of Table 5.1, reveal a clear trend of performance improvement with increased data volume.\nPretraining Data Scaling: Adding RefCOCO (50.6K) and AOKVQA (66.2K) to the pretraining phase leads to consistent improvements across all benchmarks. For instance, VQAv2 scores increase from 57.1 to 59.6, and MMBench scores rise from 40.9 to 43.1. This suggests that our model benefits from exposure to a wider range of visual concepts and question-answering patterns during pretraining.\nSFT Data Scaling: Further improvements are observed when adding ShareGPT40 (57.3K) and ALLAVA Inst (70K) to the SFT phase. The consistent improvement across different benchmarks indicates that our approach effectively leverages additional data to enhance multimodal understanding.\nScalability Potential: The continuous performance improvements with data scaling suggest that our training paradigm has not yet reached its upper limit. This finding is encouraging, as it implies that further performance gains could be achieved with larger datasets or more diverse data sources."}, {"title": "5.4 Impact of BPE Vocabulary and Token Usage Patterns", "content": "We conduct a detailed analysis of how the vocabulary of the BPE Image Tokenizer affects model performance and token usage patterns. Figure 3(a) illustrates the relationship between vocabulary size and normalized performance scores across different benchmarks. We observed that when the vocabulary size is lower than 8K (which equals the codebook size of the VQ-GAN model), the performance improves with the increase in vocabulary size. However, when the vocabulary size reaches 16K, all models on datasets except POPE show performance decline. This trend suggests an optimal vocabulary size should balances efficiency and model complexity.\nTo further verify this finding and gain deeper insights into how models utilize different vocabularies, we visualize the token embedding weights of models trained with different vocabulary sizes. We observe three distinct patterns: 1) With a smaller extended vocabulary (4K), the model tends to utilize more extended tokens from the BPE image tokenizer; 2) While with a larger extended vocabulary (16K), the model uses more of the original token IDs covered by the VQ-GAN's codebook; 3) Only when using a balanced extended vocabulary size (8k) does the model's token selection become more uniform, achieving the best performance.\nWe hypothesize that this phenomenon occurs because the original token IDs covered by the VQ- GAN codebook contain fine-grained visual information, while the combined token IDs provided by the BPE Image Tokenizer capture higher-level semantic concepts. A balanced utilization of these two types of information appears to be most conducive to the model's overall learning and performance. These findings may provide valuable insights for future MLLM training design."}, {"title": "6 Related Work", "content": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated re- markable capabilities in various tasks, including visual question answering, image captioning, and cross-modal retrieval (Yin et al., 2023). The evolution of MLLMs can be broadly categorized into two main approaches: late-fusion models with specialized encoders and early-fusion token-based models.\nTraditional late-fusion MLLMs typically employ distinct specialized designs for different modalities (Team, 2024). In this approach, modality-specific modules are trained separately using specialized data, followed by additional alignment steps to achieve late-fusion of different modalities.\nTo address these challenges, recent research has explored early-fusion approaches through unified representations, proposing token-based methods for multimodal learning (Team, 2024; Lu et al., 2024; Zheng et al., 2024). These methods typically utilize Vector Quantization (VQ) models (Van Den Oord et al., 2017; Razavi et al., 2019; Esser et al., 2021) to convert images into discrete tokens. The concept of token-based multimodal learning was initially explored in studies such as BEIT (Bao et al., 2021), which introduced a self-supervised method for acquiring visual representations based on tokenized image patches. This idea was further developed in works like Cm3 (Aghajanyan et al., 2022) and CM3Leon (Yu et al., 2023), which enabled joint reasoning across modalities and scaled up to autoregressive text-to-image generation. More recent models like Gemini (Team et al., 2023), Unicode (Zheng et al., 2024), and Chameleon (Team, 2024) have adopted end-to-end token-based approaches for multimodal learning.\nWhile these token-based MLLMs demonstrate enhanced reasoning and generation capabilities across various modalities without requiring modality-specific components, they still face challenges in rep- resentation learning and alignment (Baltru\u0161aitis et al., 2018). Our proposed BPE Image Tokenizer paradigm addresses these challenges by adhering more closely to the learning method of text-only LLMs. Unlike current token-based approaches, our method directly incorporates crucial structural prior information into the tokens through explicit merging. This approach enables Transformer models to learn input data more effectively, as supported by both our theoretical analysis and experimental results.\nThe key distinction of our work lies in its focus on optimizing the tokenization process itself, rather than relying solely on pre-trained visual encoders or simple quantization. While traditional visual encoders can achieve high compression rates, their encoded embeddings often depend on specialized decoders for interpretation. In contrast, our BPE Image Tokenizer creates tokens that are directly meaningful to the Transformer model, facilitating more effective learning of visual information without the need for specialized decoders. This approach bridges the gap between visual and textual modalities more seamlessly, potentially leading to more robust and versatile MLLMs."}, {"title": "7 Conclusions, Limitations and Future Work", "content": "In this paper, we proposed a novel paradigm for MLLM training, demonstrating improvements in image understanding capabilities across various benchmarks, even with limited training data. Our approach centers on the novel BPE Image Tokenizer, which effectively combines image token IDs to facilitate better integration of visual information into MLLMs. We provided theoretical insights into the importance of tokenization for learning 2D sequence data with Transformer models, supporting our empirical findings and offering a new perspective on processing visual information in language models. Our experiments not only showcased the effectiveness of the BPE Image Tokenizer but also demonstrated the scalability of our approach.\nWhile our results are promising, we acknowledge several limitations of our current work. The experiments were conducted with limited computational resources and training data compared to state-of-the-art MLLMs, potentially understating the full potential of our approach. Additionally, the simplified 2D Markov process used in our theoretical analysis, while instructive, may not fully capture the complexities of real-world image data.\nBased on the findings and limitations, we propose several directions for future work. Scaling up the training data and model size would allow us to fully explore the potential of our BPE Image Tokenizer approach in large-scale MLLMs. Investigating the applicability of our method to other visual tasks, including video understanding, could significantly broaden its impact. Exploring more sophisticated tokenization strategies that can better capture the complex dependencies in real-world multimodal data is another promising avenue."}, {"title": "A Full Proofs", "content": "Proof of Proposition 1. We begin by proving equation 3. For any unigram model Q\u2208 Q1-gram on the tokens, there exist distributions Q# and Qtok, supported on N and Dict (dictionary of the tokenizer) respectively, such that for all token sequence t = (t1, ..., t|t|),\nQ(t) = Q#(|t|) Qtok (tr).\nConsequently, for every m\u2208 N we have\nLm(Q) = E [log Q# (| flat(X)|)] - E \u03a3log Qtok(t)\n= E [log (m\u00b2)] - \u03a3\u03a3\u0395 [log Qtok(X;;)]\n= \u03a3\u03a3\u0395 [log tok (Xij)]\n\u2265 \u03a3\u03c0(\u03b1)log(Qtok(a))\n\u2265 \u0397(\u03c0),\nwhere in (i) we apply the definition of flat(\u00b7), and in (ii) we use the fact that is the stationary distribution for each column/row as assumed in Scenario 1/Scenario 2. Thus, equation 3 is proved.\nNext, we proceed to prove equation 4. For an arbitrary model Q, we have\nm\u00b2Lm(Q) = -E[log(Q(flat(X)))]\n= E[log(P* (flat(X))/Q(flat(X)))] \u2013 E[log(Pm (flat(X))] =DKL(P||Q) + H(Pm), \u2265 H(Pm).\nwhere Pm denotes the actual joint distribution of flat(X) and DKL(PM||Q) denotes the KL diver- gence between P* and Q. Under Scenario 1, leveraging the independence between columns we derive\nH(Pm)= \u03a3E[log \u03c0(X1j)] + \u03a3E[log P(Xij | Xi-1,j)]\n- \u03a3\u03c0(a) log(x(a)) + (m - 1)\u03a3\u03a3\u03c0(\u03b1)P(a' | a) log (P(a' | a))\n= mH(\u03c0) + m(m - 1)H\u221e.\nThis result is also applicable under Scenario 2 by switching the roles of i and j. As a result, in both scenarios we have\nlim inf min Lm(Q) \u2265 lim H(Pm) = H\u221e\nConversely, it is evident that\nmin Lm(Q) \u2264 Lm(Pm) = H(Pm).\nHence,\nlim sup min Lm(Q) \u2264 lim \u00a3m(Pm) = Hx. This finishes the proof of equation 4.\nProof of Proposition 2. In the one-dimensional case, where the data is a string generated from an ergodic Markov process, authors in (Rajaraman et al., 2024) have proved the following result:\nLemma A.1 (Theorem 4.1 in (Rajaraman et al., 2024)). Suppose a string s of length m is generated from an ergodic data source supported on a finite set A, with a transition kernel P(. ) and a stationary distribution \u03c0. Assume the transition kernel satisfies mina,a'\u2208A P(a' | a) = 8 > 0. Then, there exists a tokenizer with a dictionary containing at most D tokens, along with an encoding function enc(\u00b7) applied to s, such that\nlim sup min E[log(1/Q(enc(s)))])1- \u03b5 \u03a3\u03a3\u03c0(\u03b1)P(a' | a) log (1/P(a' | a)),\nwhere \u025b = log(1/8)/(0.99 log(D)) and D\u2208 N is an arbitrary constant that is sufficiently large.\nBuilding on the results of Lemma A.1, we extend these findings to images generated under Scenario 1 or 2.\nIn both scenarios, the generated image is composed of m parallel Markov sequences, each of length m, sharing the same transition kernel P(. | \u00b7) and stationary distribution \u03c0.\nConsider Scenario 1. According to Lemma A.1 and equation 6, there exists a tokenizer equipped with a dictionary, denoted as Dict where | Dict | < D, and an encoding function, denoted as enccol(\u00b7), and there also exists a unigram model Qm \u2208 Q1-gram for each m\u2208 N, such that\nlim sup-E \u03a3 log(1/Qtok (t)) \u2264 1- \u03b5 H\u221e\nFor x \\in \\"}, {"title": "B Implementation Details", "content": "B.1 Additional Pseudo Codes The additional functions used in Algorithm 4.1 are shown in Algorithm B.1 and B.2."}]}