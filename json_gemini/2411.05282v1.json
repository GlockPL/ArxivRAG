{"title": "MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization", "authors": ["Akshat Ramachandran", "Souvik Kundu", "Tushar Krishna"], "abstract": "Quantization of foundational models (FMs) is significantly more challenging than traditional DNNs due to the emergence of large magnitude features called outliers. Existing outlier-aware algorithm/architecture co-design techniques either use mixed-precision, retaining outliers at high precision but compromise hardware efficiency, or quantize inliers and outliers at the same precision, improving hardware efficiency at the cost of accuracy. To address this mutual exclusivity, in this paper, we propose MicroScopiQ, a novel co-design technique that leverages pruning to complement outlier-aware quantization. MicroScopiQ retains outliers at higher precision while pruning a certain fraction of least important weights to distribute the additional outlier bits; ensuring high accuracy, aligned memory and hardware efficiency. We design a high-throughput, low overhead accelerator architecture composed of simple multi-precision INT processing elements and a novel network-on-chip called ReCoN that efficiently abstracts the complexity of supporting high-precision outliers. Additionally, unlike existing alternatives, MicroScopiQ does not assume any locality of outlier weights, enabling applicability to a broad range of FMs. Extensive experiments across various quantization settings show that MicroScopiQ achieves SoTA quantization performance while simultaneously improving inference performance by 3\u00d7 and reducing energy by 2\u00d7 over existing alternatives.", "sections": [{"title": "1 Introduction", "content": "In recent years, technological advancements in Generative Artificial Intelligence [2, 25, 43, 77] has been driven by a class of transformer-based [80] models called foundational models (FMs) [6, 8]. FMs, such as large language models (LLMs, e.g. Meta's LLaMA3 [53]) and vision language models (VLMs, e.g. Microsoft's LLaVA [47]) \u00b9 are billion-scale parameter models, trained on a large corpora of data. FM scaling laws [7, 9, 90] correlate better learning with model size [84]. Thus, inference with FMs has high demand for memory, data movement, energy, and computation.\nRecent research has focused on various model compression techniques such as pruning [3, 22] and quantization [27, 37, 45, 67] to reduce memory footprint and computational costs, to make FM inference service more accessible to resource-constrained devices.\nModel pruning [22] reduces memory footprint by removing in-effectual model parameters, such as individual weights (unstructured) or blocks of weights (structured), and storing sparse tensors in a compressed format [34]. However, pruning of FMs may be infeasible due to, significant accuracy drops even at low pruning ratios [40, 85], with potential demand for compute and memory-intensive fine-tuning to regain accuracy. Model quantization, on the other hand, reduces the size of a target model by representing weights and/or activations at low precision [37, 62, 63]. Quantization not only reduces the memory footprint but also enables efficient low-bit-width arithmetic operations [79, 87]. Recent works on quantization [17, 27] have identified that quantizing LLMs is considerably more challenging than quantizing traditional DNNS [44, 75] due to the emergence of large magnitude features known as outliers [88]. These outliers significantly impact model accuracy and require specialized handling [27] compared to inliers (small magnitude features).\nTo address the issue of outliers in FMs, recent algorithm/ architecture co-design techniques [27, 45, 67] have proposed different types of outlier-aware quantization. These techniques can be broadly categorized based on their outlier handling approach: Maintaining outliers at higher precision compared to the inliers, or 6 Quantizing outliers at the same precision as inliers while using different data formats or scaling factors for outliers.\nTechniques in group, such as OWQ [42], SpQR [18] (algorithm) and GOBO [88], OLAccel [56] (architecture co-design) exhibit low accuracy degradation. This is because they typically store outliers at high precision separated from lower precision inliers. However, these techniques result in, (a) low compression factor with high effective bit-width (EBW\u00b2) and (b) inefficient hardware and unaligned memory access.\nOn the other hand, techniques in group 6, such as AWQ [45] (algorithm) and Olive [27] (architecture co-design) quantize outliers at the same precision as inliers following different strategies. AWQ tries to identify a separate outlier-specific scale factor via channel-wise scaling. OliVe uses the \"flint\" data format [28] for inliers and \"abfloat\" [27] for outliers, both at 4-bit precision. These techniques mitigate the unaligned memory access while providing high compression. However, they suffer from significant accuracy degradation, particularly at ultra-low bit widths. This may be attributed to the reduced representational range available to outliers"}, {"title": "2 Background", "content": "A typical quantization [44, 62] process involves two steps: establishing the quantization parameters, given the quantization data format"}, {"title": "2.1 Model Quantization", "content": "A typical quantization [44, 62] process involves two steps: establishing the quantization parameters, given the quantization data format (\u03c4) and bit-width (b), and mapping the high-precision tensor to the quantized representation. For a typical symmetric quantization [92] (zero-point is 0) of a tensor X, the scale factor (s) is given by,\n$S = \\frac{max(X)}{max_{INT}}$ (1)\nwhere, $max_{INT}$ is the maximum representable value of a data format [62]. For b-bit INT quantization, $max_{max INT} = 2^{b-1}-1$. After determining the quantization parameters, the quantized tensor can be calculated as [62],\n$Q(X, s, b) = clip(\\frac{X}{s}, min, max)$  (2)\nIn model quantization, the quantization parameters can be shared at different granularities for different accuracy-overhead trade-offs. In increasing order of overheads, we have per-tensor quantization, wherein the scale factor is shared among all tensor elements. In per-channel quantization, the scale factor is shared per row/column of a tensor. Finally, in group quantization, the parameters are shared at a finer granularity between groups of k (64, 128 etc.) elements in a row or column. These groups are formed by dividing channels into multiple non-overlapping contiguous blocks. In this paper, we adopt MX-FP and MX-INT quantization for inliers and outliers, respectively."}, {"title": "2.2 Microscaling Data Format", "content": "The MX data format proposed by prior works [16, 64], is standardized by the Open Compute Project [59] with support from Microsoft, Intel, NVIDIA and others. As shown in Figure 1, MX is a variant of block data representation (BDR) [16] that defines a format to represent a group of values collectively using shared scale factors. It leverages multi-level, power-of-two scaling, at fine- (level-1, k\u2081) and ultra-fine (level-2, k2) granularity [15, 20]. The MX data format is characterized by four components: i) scale factors (level-1, 2), ii) data type (t), iii) bit-width (b) and iv) group sizes (k1, k2). In this paper we denote an MX-FP format as MX-FP-bk1,k2. In this work, we adopt the version of the MX-FP data format proposed in [64], employing multi-level scaling, which differs from the single-level scaling used in the standardized format [59]. The level-1 scale factor for MX-FP is computed following Equation 1. Conversely, for level-2 scale factor, we identify that the MX-FP leverages the sharing of exponent field of FP values [81] (referred as \u00b5X in Figure 1). We show later in \u00a74.2 that by taking advantage of this insight i.e., the concept of shared \u00b5X, we are able to represent FP-outliers in INT format, thereby, enabling the design of simple, homogeneous INT-based PEs. For inliers, we employ MX-INT-bk\u2081 with a single level of scale factor following [59]. This is because, INT format does not possess an exponent field, thereby, a level-2 scale factor similar to MX-FP is not applicable. For simplified understanding, MX-INT-bk\u2081 inlier quantization can be viewed as analogous to INT group quantization utilizing an E8M0 scale factor. We refer the interested reader to [16] for an in-depth evaluation of the MX format and comparison against prior BDR formats."}, {"title": "3 Motivation", "content": ""}, {"title": "3.1 Limitations of existing techniques", "content": "In Table 1, we compare candidate proposals from group: GOBO [88] and group 6: Olive [27] across various metrics. GOBO is able to achieve high accuracy by retaining outliers at full-precision. It stores outliers separately from low-precision inliers by using sparse representations with the associated outlier indices (see Figure 3(b)). By retaining outliers at full-precision, GOBO results in high EBW. Moreover, the compressed sparse storage and multiple precisions results in unaligned and random memory accesses [27], significantly impacting inference latency. Furthermore, GOBO's outlier handling is hardware inefficient, requiring complex PE and control, with significant hardware overheads.\nOlive [27] proposes a scheme to ensure aligned memory accesses by quantizing inliers and outliers at the same precision (low EBW), but using different data formats. To enable differentiation between the inlier and outlier formats, it prunes the value adjacent to the outlier for use as an identifier (see Figure 3(c)). However, Olive results in significant accuracy degradation, especially at low precision (see Figure 2(b)), due to: 1) sacrificing a number encoding from inliers for exclusive use as an identifier, reducing the number of representable values in the quantized range, and 2) the flawed assumption of outlier locality-particularly for modern FMs-that outliers are never adjacent to each other and only inliers are almost always adjacent to outliers (see \u00a73.2), leading to unintended outlier pruning. Furthermore, it still requires a fairly complex PE design incurring significant encoding/decoding overheads to convert the different formats into a unified processing format (exponent-integer pair). In this paper, we show that despite quantizing outliers at higher-precision and in a different format, we ensure aligned memory access, simple PE design and minimal hardware overhead."}, {"title": "3.2 Adjacent Outliers Matter", "content": "Similar to prior works [27, 56], we leverage the 30 rule [60] to categorize weights as outliers. We visually demonstrate the distribution of outliers and adjacent outliers 4 as a percentage of the total number of weights in a layer across different FMs in Figure 2(a). As the orange box-plot shows, outliers depict a maximum percentage of ~5.1%. Outliers are prevalent in FMs, and preserving their values is crucial for maintaining quantized model accuracy. Importantly, from the green box-plots, we observe that modern FMs on average possess > 0.5% adjacent outliers per layer, with some FM layers showing peaks of > 2%. This is in stark contrast to the models evaluated by Olive, such as BERT [19] and OPT [91] which have < 0.04% adjacent outliers (two orders of magnitude lower than FMs like LLaMA3 and LlaVa). This indicates that while pruning values adjacent to outliers could have been ideal for models like BERT [19], it is sub-optimal for most modern FMs as it removes crucial outlier values, leading to higher accuracy degradation. This is evident from Figure 2(b) where Olive has significant accuracy degradation at 4-bit quantization due to its assumption of outlier locality. Unlike Olive, MicroScopiQ does not naively prune adjacent values; instead it leverages the Hessian information [23] to identify the least important values to prune ensuring outliers are preserved. This directly translates to high quantized model accuracy and MicroScopiQ at 2-bit weight quantization consistently outperforms Olive across different FMs."}, {"title": "3.3 Outlier Precision and Data Format", "content": "The ability of group techniques like GOBO [88] to achieve high quantized model accuracy even at extreme quantization levels of inliers (<4-bits) is due to retaining outliers at higher precision. This is particularly crucial at ultra-low bit width quantization because, if inliers and outliers are to be quantized to the same precision, there will be higher outlier quantization error due to the reduced representational range. We demonstrate this effect on the MicroScopiQ quantized FM accuracy in Table 6 wherein the quantized FM has poor performance when inliers and outliers are at 2-bits compared to outliers at 4-bits. Furthermore, evidence from recent work [82] demonstrates that FP-based formats for LLMs results in superior quantization performance compared to INTs. To validate this, we compare INT v/s MX-FP inlier and outlier quantization in Table 6. Evidently using MX-FP instead of INT for outliers results in better performance. The performance boost from employing MX-FP is due to higher dynamic range of FPs, which is particularly beneficial at extreme quantization levels. In this work, we quantize outliers at a higher precision (2x) compared to inliers, using MX-FP for outliers and MX-INT for inliers."}, {"title": "4 MicroScopiQ Quantization Methodology", "content": "We present an overview of the MicroScopiQ quantization framework in Figure 3(a) and detail its steps in Algorithm 1. MicroScopiQ supports various group size granularities and any inlier and outlier (2x inliers) data precision. For simplicity, we explain with inlier precision of 2- and 4-bit and group sizes of 128 for inliers and 8 (level-1 and 2) for outliers. Experimental results in \u00a76 demonstrate the effects of different precisions and group sizes."}, {"title": "4.1 Preliminaries", "content": "The MicroScopiQ quantization framework models the layer-wise post-training quantization of FMs by partitioning each layer into pair of contiguous outliers"}, {"title": "4.2 Inlier and Outlier Weight Quantization", "content": "Each row to be quantized is first divided into multiple non overlapping contiguous macro-blocks (MaBs) of size BM = 128. All inliers are quantized within a MaB and share the scale factor. Each MaB is then subdivided into multiple non-overlapping contiguous micro-blocks (\u00b5Bs) of size B\u00b5 = 8 with sixteen \u00b5Bs forming a MaB. The outliers present in each \u00b5B shares same scale(s). As depicted in Figure 3(a), tab 1, the quantization process begins by first identifying inliers and outliers in each MaB by using the 3\u03c3 rule. A shared 8-bit power-of-two scale factor (2lsf), following Equation 1 is calculated for all inliers in a MaB and the inliers are quantized to 2-bit or 4-bit integers resulting in MX-INT-(2/4)128 quantization. Interestingly, we observe that the inlier scale factor in each Ma\u00df is always a negative power of two for all FMs under consideration. We leverage this observation to reduce outlier magnitude, by multiplying all outlier values in a MaB with the inlier scale factor (2lsf) (can also be perceived as division by 2-lsf, for conformity with Equation 2). This pre-processing helps make outlier quantization easier, by pre-reducing its maximum dynamic range.\nUnlike inliers, outliers are quantized per \u00b5B, to reduce quantization error due to shared scaling over a larger group size (see \u00a76). After identifying outliers present in a \u00b5B, we compute a shared 8-bit MXScale that is calculated by concatenating the level-1 power-of-two scale factor (2Osf) and level-2 microExponent (\u00b5X). The level-1 scale factor is calculated by following Equation 1 to obtain 7 or 5-bit MSBs of MXScale depending on size of \u00b5X being 1 or 3-bit of the LSBs-corresponds to exponent size of the FP format (depicted in tab 2 in Figure 3(a)). The outliers in a \u00b5B are scaled by (2Osf), following Equation 2 and then quantized to either e1m2/e3m4 FP-format for b = 4 or 8-bit, respectively. Post quantization of outliers, the level-two scale factor or the \u00b5X is obtained by extracting the common exponent among all outliers in a \u00b5B. This process results in a MX-FP-68,8 outlier quantization. The final outlier scale factor is 2Osf where, Osf is expressed as Osf = O + \u00b5X \u2013 lsf. The term lsf in the final outlier scale factor accounts for multiplication by 2lsf (or division by the inverse) during outlier pre-processing."}, {"title": "4.3 Outlier Value Encoding through N:M Structured Pruning", "content": "Let us assume n outliers are present in a \u00b5B quantized to MX-FP-48,8. After sharing 1-bit \u00b5X across the n outliers, each outlier has 1 sign (s) and 2 mantissa bits (m\u2081mo). The inliers are 2-bit 2's complement MX-INT and has 1-bit sign and 1-bit magnitude. Since the outlier bits have two mantissa bits with one sign, to ensure symmetric outlier distribution, we duplicate their sign bit and assign each sign to a mantissa creating and partitioning into two halves Upper, Lower of size 2-bit each mimicking inlier MX-INT structure i.e., {sm1, smo} (see Figure 3(a), tab 2). While we demonstrate this for a specific example, the process can be generalized to other inlier and outlier formats bit-widths, wherein the remaining outlier bits need to be reorganized into Upper and Lower distributable halves each of size bb (per element bit-budget), with bb representing the operational bit-width of each PE.\nTo ensure a fixed bb and data-type of a layer for aligned memory access and simple PE design, we distribute the outlier LSBs (Lower half) to the least important inlier locations that are pruned within a \u00b5B. For n outliers in a \u00b5B we identify n least important inlier locations to prune via the Hessian information (see Algorithm"}, {"title": "4.4 Effective Bit Width Calculation", "content": "Following [23, 92] we report the EBW as the average number of bits required for storing each element including the metadata. The EBW of MicroScopiQ quantized FM varies dynamically across models and is dependent on the outlier percentage and choice of \u00b5B size. For INT-2BM inlier and MX-FP-4B1,B\u2081 outlier quantization, if there are no outliers in a \u00b5B the EBW of that \u00b5B is EBW\u2081 = 2, i.e., bb, whereas if there are outliers present in a mB the EBW is EBWO = ($perm_{bits}$ + 2B + O$ _{bits}$)/B\u00b5, which translates to 6-bits for B\u00b5=8, MXScale of 8-bits and permutation list size of 24-bits (see \u00a74.3). Since the inlier scale factor is shared across a larger group size BM, its contribution to the EBW is negligible and hence ignored. As a rule of thumb, if there are no outlier present in a \u00b5B the EBW of that \u00b5B is equal bb else the EBW includes the contribution of all outlier-specific metadata. The presence or absence of outlier metadata is delineated by a 1-bit identifier per \u00b5B, wherein a 0 indicates no outlier metadata and vice versa. This identifier contributes negligible overhead to the EBW (0.05-0.09 bits) similar to the inlier scale factor and is hence ignored in the EBW calculation. For a FM with I layers and each layer having m \u00b5Bs and x% of these \u00b5Bs consist of outliers, the EBW of the FM is,\n$EBW_{FM} = \\frac{ \u03a3_{l=1} (x\u00b7m\u00b7 EBW_{O} + (100 \u2013 x) \u00b7 m \u00b7 EBW_{I})/m }$ (4)"}, {"title": "5 MicroScopiQ Architecture", "content": ""}, {"title": "5.1 Memory Organization", "content": "In Figure 4(b), we show how a typical weight layer is organized in memory (DRAM/HBM) for acceleration by MicroScopiQ inline with other multi-precision accelerators [15, 27, 38, 63] as shown in Figure 4(a) for Olive. It is comprised of two sections, quantized weights and metadata. For MicroScopiQ-W4, all weight elements occupy 4-bits and assuming 1-byte as the smallest addressable memory [27], two adjacent weight elements occupy a memory word and aligns 8-bit memory accesses with high efficiency. The metadata section consists of 8-bit inlier scale factors which are shared per MaB and for a \u00b5B of size 8, there are 16 \u00b5Bs. A 1-bit identifier per \u00b5B is used to notify the controller whether a particular \u00b5B has outliers and if the corresponding metadata (8-bit MXScale and 24-bit permutation list) needs to be fetched. The controller calculates how many \u00b5B's metadata are stored in memory and burst accesses them accordingly and stores it in the instruction buffer. All the metadata is also memory word aligned, in line with the definition proposed in [27]. Following the tiling, banking scheme of popular accelerators, [12, 15, 38, 54] we employ dedicated interleaved SRAM banks for each row for weights and activations (circularly rotated across banks) [58] and a separate bank for the metadata, enabling it to be loaded in parallel. In WS systolic arrays, the weight matrix W is spatially laid out onto the systolic arrays; thus, W must be partitioned into tiles of R \u00d7 C to match the array dimensions, where R and C denote the number of array rows and columns, respectively. Because the first dimension of W must match the second dimension of the activation matrix X in a matrix multiplication, X's second dimension is also required to be partitioned with a size of R."}, {"title": "5.2 Computation Flow", "content": "In Figure 5, we depict the MicroScopiQ accelerator architecture, integrated into a standard weight stationary systolic array. The accelerator is optimized for throughput and all compute units (PE array, ReCoN etc.) are internally pipelined with interleaved pipeline stages. Through the numbered steps in Figure 5, we explain the computational flow of the MicroScopiQ accelerator. Assume a quantized FM with potential outliers conditioned and distributed within a \u00b5B and corresponding metadata calculated offline. \u25cf One \u00b5B or multiple \u00b5Bs of weights are mapped to each PE row as typically B\u00b5 \u2264 # of columns in PE array [36] 5 Each PE in a row either receives a weight at high-precision (e.g. 4-bit) or multiple weights with low-precision (e.g. two 2-bits). PE row 0 receives quantized input activation (iAct) from left and input accumulation (iAcc) from top. The PEs in row 0 perform INT multiplication of the stationary weights with the iActs, irrespective of outlier/inlier weights. During accumulation, assuming the uBs mapped to PE row 0 do not contain outliers, the PEs accumulate the computed multiplication result with iAcc and direct the partial sums to PE row 1. PE row 1 receives partial sums from top and similarly performs INT multiplication between weights and iAct. In parallel, the output scale factors can be calculated as described in \u00a75.6.6 Assuming the mapped uBs to PE row 1 have atleast one outlier, the controller directs all PE outputs to ReCoN. The PE with outliers offloads the accumulation to ReCoN by sending a concatenation of the multiplication result (Res) and iAcc at its output. This is needed because the outlier will have its two halves distributed in different locations and possibly mapped to different PEs. While the PEs in row 1 with inlier weights can accumulate the partial sum output, the PEs with the Upper and Lower outlier halves cannot compute the outlier partial sum output. After receiving outputs from row 1, ReCoN reorders and calculates the FP-outlier partial sum, leaving the inlier partial sums untouched. The controller signals PE row 2 to expect input partial sums from ReCoN and not from the previous row. Similarly computations proceed and finally producing output activations (oActs). oActs are post-processed (scaled and quantized). The post-processing of oActs does not require all the oActs to be computed. The post processing operations can be conducted as and when the oActs are generated and overlapped with the rest of the computation so as to hide computation and memory access latency overhead. In situations involving back-to-back GEMMs, we follow the standard practice adopted by SoTA accelerators [12, 38, 63, 68], wherein the oActs are either directly routed to the iAct buffer or staged in L2 memory (based on the buffer capacity and oAct size) so as to eventually load into the iAct buffer for the next computation. (shown by the green dotted line in Figure 5).\nIn Figure 6, through a small-scale example on a 2 \u00d7 2 Micro-ScopiQ array, we demonstrate how iActs and weights are tiled and scheduled for processing across space and time. In Figure 6, row 0 of mapped weights has only inliers and row 1 of mapped weights has a single outlier with its upper and lower halves occupying two spatial locations."}, {"title": "5.3 On-chip Storage and Control", "content": "Weight/Activation Organization. The iActs are stored in the iAct buffer as 8-bit INTs. Lower-precision iActs (<8-bit) are supported through sign-extension. The same process is followed for oActs.\nThe weight buffer stores weights at 4-bit granularity. At lower precision (2-bits), each buffer location simultaneously stores two 2-bit weights. The MODE signal from the controller identifies the interpretation of bit format of weights in the weight buffer i.e, one 4-bit weight or two 2-bit weights.\nInstruction Buffer (IB). It stores all the metadata required for inference of MicroScopiQ quantized FMs. Particularly, the IB stores the outlier distribution permutation list i.e., configurations for Re-CoN and scale factors.\nMicroScopiQ Controller. The controller generates appropriate control and signals to all units. It is functionally very similar to standard systolic array controllers [71], however, with added functionality to support specific features of ReCoN, multi-precision PEs, post-processing unit. Furthermore, unlike traditional controllers, the MicroScopiQ controller exerts fine-grained control using hand-shaking signals on the streaming of iActs and iAccs into the PE array to account for the increased pipeline depth through ReCoN for PE rows with outliers."}, {"title": "5.4 Multi-Precision INT-PE Array", "content": "Multi-Precision. Existing multi-precision accelerators [27, 28, 71] typically follow a bottom-up approach, employing low-precision PEs and grouping neighboring PEs to support higher precision. This sacrifices throughput for multi-precision support, as multiple PE columns are required to perform a single MAC operation. This reduces parallelism and increases latency. We adopt a different strategy via the MODE signal for multi-precision support by mapping multiple weights that share the same rightbound iAct to the same PE at lower precision (2-bits) or a single weight at higher precision (4-bits). The two 2-bit weights mapped to the same PE in Micro-ScopiQ are the weights that would have been mapped to different columns within the same row in existing multi-precision accelerators. At high-precision operation we utilize available parallelism from the PE array while at lower-precision operation we increase computation throughput by facilitating the parallel evaluation of multiple partial sums along each column.\nMultiplication Stage. Inspired by [48, 78] we present a partial product multiplier-tree architecture for multi-precision INT multiplication. The weights and iAct are partitioned across the four 4-bit \u00d7 2-bit multipliers to calculate partial sums (P00, P01, P10, P11) as described in Figure 5. Based on the MODE signal the partial sums above are combined using a combination of adders and shifters to provide the result (Res) of different bit-precision weight with iAct as follows,\nRes = $ \\begin{cases}  {(P_{11} << 2 + P_{10}), (P_{01} << 2 + P_{00})}, MODE_{2b} \\\\ (P_{11} <<4+P_{00}) + (P_{01} <<2+ P_{10} << 2), MODE_{4b}  \\end{cases} $ (5)\nAccumulation Stage. It receives the MUL-stage output (Res) and also iAcc from either ReCoN or the previous PE row. The controller signals the ADD stage via Outlier_Present to modify accumulation behavior based on whether inlier/outlier weights are present in the PE. If the Res corresponds to a multiplication of inlier weight with iAct, then the corresponding Res is added with iAcc using two 2-bit + 4-bit adders with a multiplexer that enables multi-precision by propagating the carry from one adder to other. In low precision MODE, the two adders work independently to calculate partial sums in parallel. For high precision MODE, both adders work together to produce a single accumulation result with carry propagation through the multiplexer. On the other hand, if Res corresponds to multiplication of either of the outlier halves with iAct, the actual outlier accumulation is offloaded to ReCoN by concatenating Res and iAcc\u00f3. This is done to prevent incorrect outlier partial sum calculation. The controller finally directs the partial sum output to ReCoN or the next PE row, via the OAcc_NoC/PE signal."}, {"title": "5.5 Redistribution and Coordination NoC", "content": "The Redistribution and Coordination NoC (ReCoN) is a multistage butterfly NoC in the MicroScopiQ accelerator time-multiplexed and shared across all PE rows (see \u00a76.5). This is a more cost effective way of handling outliers compared to Olive and GOBO [27, 88] that handle outliers within the PE. Since PEs are large in number Olive and GOBO [27, 88] will incur larger costs compared to MicroScopiQ. Note: Not all rows require access to ReCoN, only rows that have one or more outlier weights in the PE require access. Since outliers are still minimal (though important) compared to inliers, time-multiplexing access to ReCoN is a cost-effective solution with minimal access conflicts to ReCoN (see \u00a76.5).\nReCoN topology. ReCoN is composed of n (log2 (n) +1) {2-input, 2-output} ReCoN switches, in a multistage butterfly NoC topology [39]. The input and output stages of ReCoN also employ {2-input, 2-output} switches, with a dedicated switch for each column receiving (transmitting) partial sums into (from) one of the two input (output)"}, {"title": "5.6 Post Processing and Output Scale Factor Computation", "content": "The shared output scale factor for BM=128 is calculated as oActs f = Osf + iActsf (See expansion of Osf above.). The calculation of oActsf (with simple adders and subtractors) is independent of the processing done in the PE array. Therefore, we overlap the computation of oActsf with the processing of oActs to efficiently hide the computation latency similar to prior works [15, 63]. Since we maintain different scales for inliers and outliers (see \u00a74.2), and the oAct scale factor is calculated based on the outlier scale factor (Osf) (see above) (we found this to result in least quantization error of oActs), the oActs which are generated through computation with only inlier weights (identified through the in/out control signal) are shifted by the shift value depicted in the \u201cPost Processing and Scale Compute Unit\" in Figure 5 to ensure conformity with the final scale factor. Finally, the oActs are scaled with the computed output scale factor through a simple right shift (since it is a power-of-two scale factor, division can be implemented through right shift operation) and quantized to MX-INT-(4/8)128 before being sent to external memory or routed back to the iAct buffer for computation with the next layer's weights. The cyclic behavior diagram of the post processing unit in Figure 5 depicts the single clock cycle latency incurred to scale, quantize and store the oActs. This is inline with SOTA accelerators' [15, 38, 68] post processing latency. Similar to prior works [63], [38] the post processing unit is also responsible for handling all non-linear operations."}, {"title": "6 Experimental Evaluations", "content": ""}, {"title": "6.1 Experimental Setup7", "content": "Models and Datasets. We benchmark on various FMs including LLMs and VLMs, for a comprehensive evaluation of our method. For LLMs, we evaluate on OPT [91], LLaMA2 [77] LLaMA3 [53], Phi-3 (SLM) [1] and Mixtral (MoE) [35] model families and the VLMs evaluated are OpenFlamingo [5], VILA [46] and LlaVa [47]. We use a calibration dataset consisting of 128 random samples from the PILE dataset [24], so as to not overfit to any particular downstream domain [45]. We compare different LLMs based on perplexity (PPL \u2193) on the WikiText2 [52] dataset and benchmark accuracy on 6 different tasks BoolQ [13], HellaSwag [89], PIQA [10], ARC-c [14], MMLU [31] and Winogrande [65]. Similarly, we evaluate VLMs on 5 vision-language benchmarks: COCO captioning [11], VQAv2 [26], VizWiz [29], TextVQA [72], and GQA [33].\nAlgorithm Implementation. We implement the MicroScopiQ Quantization Framework in PyTorch [57]. All FMs are quantized using a single NVIDIA H100 GPU. The complete quantization process' runtime ranges between 30 mins. to 9 hours depending on model size (3.8B to 175B), which is at par with recent SoTA quantization frameworks [23, 67].\nAlgorithm Baselines. We compare the performance of our MicroScopiQ Quantization Framework against existing SoTA quantization algorithms: OmniQuant [67], AWQ [45], GPTQ [23] and co-design techniques: Olive [27] and GOBO [88].\nAccelerator Implementation. We implement the MicroScopiQ accelerator in Verilog and synthesize, place and route using Synopsys Design Compiler, Innovus with a TSMC 7nm technology library"}, {"title": "6.2 LLM Quantization Results", "content": "Weight-Only Quantization. In Table 2, we compare the WikiText2 PPL (lower the better) of different LLMs for W4A16 and W2A16 settings. Apparent from the table, MicroScopiQ consistently outperforms all the baselines across different models and quantization settings. At W4A16, MicroScopiQ achieves near-lossless quantization performance. MicroScopiQ has minimal increase in PPL due to its ability to quantize outliers at higher-precision and in FP-format to reduce outlier"}]}