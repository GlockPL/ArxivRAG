{"title": "UNLOCKING THE POWER OF FUNCTION VECTORS FOR CHARACTERIZING AND MITIGATING CATASTROPHIC FORGETTING IN CONTINUAL INSTRUCTION TUNING", "authors": ["Gangwei Jiang", "Caigao Jiang", "Zhaoyi Li", "Siqiao Xue", "Jun Zhou", "Linqi Song", "Defu Lian", "Yin Wei"], "abstract": "Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior. Our study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions. Leveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics. We plan to make our code publicly accessible in the near future.", "sections": [{"title": "1 INTRODUCTION", "content": "Continual instruction tuning (Peng et al., 2023; Chung et al., 2024) has emerged as an indispens- able ingredient in the development of Large Language Models (LLMs) (Brown et al., 2020; Radford et al., 2019; Touvron et al., 2023b), enabling them to meet the demands of specific domains (Roziere et al., 2023; Thirunavukarasu et al., 2023; Xue et al., 2024) and human preferences (Ouyang et al., 2022). However, a notable concern with such continual tuning is \"catastrophic forgetting\" (Mc- Closkey & Cohen, 1989; Kirkpatrick et al., 2017), where models may lose essential skills (Dou et al., 2023; Chen et al., 2023) such as mathematical reasoning while adjusting to user instructions. While instruction tuning effectively evolves LLMs, it's critical to characterize and mitigate forget- ting within these models.\nResearch on LLM forgetting (Luo et al., 2024; Wang et al., 2023c; Wu et al., 2024a) generally ex- amines alterations in specific abilities like reading comprehension, factual knowledge, mathematical reasoning skills, and so on, underscoring the universal existence of catastrophic forgetting. As they have primarily studied from a single training sequence, they fail to establish the connection between model forgetting and the characteristics of training data. Concurrently, there is a notable gap in understanding the internal mechanisms that underlie model forgetting. To date, only a limited body of research has ventured into this area; notably, the work of Kotha et al. (2024), proposing the task"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 CATASTROPHIC FORGETTING", "content": "Continual learning (Serra et al., 2018; Wu et al., 2024c;b) seeks to tackle the core challenge of incrementally learning from a sequence of real-world tasks over time, specifically addressing how to adapt to new tasks without forgetting previously learned knowledge a phenomenon widely known as catastrophic forgetting (McCloskey & Cohen, 1989; Kirkpatrick et al., 2017).\nIn this paper, we focus on continual learning of a language model Mo, which has already been pre-trained on a vast data corpus DPT using language modeling tasks (Brown et al., 2020; Radford et al., 2019) followed by preference optimization via human feedback (Ouyang et al., 2022). Specif-"}, {"title": "2.2 FUNCTION VECTOR", "content": "Following the mechanistic interpretability work in LLMs (Todd et al., 2023; Hendel et al., 2023), we investigate the internal workings of a task on LLMs through function vector compact vector representation of input-output task identified within the hidden states of transformers during in- context learning (ICL (Brown et al., 2020)). An activation patching (Meng et al., 2022; 2023; Wang et al., 2023a) procedure is performed on the ICL hidden states to determine the casual set of attention heads that mediate tasks. These heads collaborate to convey a function vector (FV) which represents the LLM's function specific to input-output task. Function vector is regraded as an efficient way to characterize function execution within LLMs (Todd et al., 2023).\nFormally, for a given dataset DT of task T, the function vector \\(\\theta_{\\tau}\\) is derived through two steps:\nFirst, the activation patching is performed to determine the attention heads set (denoted as S) with significant cause-and-effect relationship between ICL inputs and correct outputs. Specifically, the model will run on a counterfactual ICL input [p, x] incorporating a label-shuffled prompt p = [(x1, y1), ..., (xn, \u0177n)], which typically leading to incorrect outcomes. Then the representation at specific head for [p, x] is substitute by the averaged task activation \\(h_T^c\\) and calculate its causal effect (CE) on the model's output."}, {"title": "", "content": "CElk([p, x]) = PM \u2192 (y | [p, x]) \u2013 P\u043c(\u0443 | [p, x]).\nHere, \\(h_i^k \\in \\mathbb{R}^d\\) symbolizes the mean activations of in-context learning input for task T at the last token position across layer l and head k, with d being the dimension of the layer output as \\(h_l^k = head_{l,k} W_o^k\\). \\(M_{h_i^k \\rightarrow h_l^k}\\) denotes the model with a replacement operation on attention head (l, k) at last token. A higher CE implies that the specific head's state is critical for accurate predictions which encoding of more task-relevant information. In this paper, S is the attention head with top-10 CE.\nSecond, function vector \\(\\theta_T\\) is assembled by summing up the averaged ICL inputs activation from the attention heads within S, formulated as \\(\\theta_{\\tau} = \\sum_{(l,k) \\in S} h_l^k \\in \\mathbb{R}^d\\). The comprehensive methodology for this extraction process can be found in the Appendix F.\nIn this paper, we revisit the definition of FV and study its dynamics before and after learning a new task, providing a surrogate lens to uncover the inherent mechanisms of forgetting in LLMs."}, {"title": "3 CATASTROPHIC FORGETTING OF LLMS", "content": "This section empirically examines the research question of when forgetting occurs during continual instruction tuning, specifically in relation to task types, training stages and language models.\nDatasets. We build the task sequences for continual instruction tuning using SuperNI (Wang et al., 2022), a collection of NLP tasks with expert-written instructions that are unseen to the pre-trained model. SuperNI is commonly used for assessing cross-task generation and conflict after fine-tuning language models. To investigate the relationship between forgetting and task types, i.e., generation and classification, we design six task sequences. These sequences consist of pure generation tasks, pure classification tasks and mixed sequences containing both generation and classification tasks. Their main information are listed in Table 5, with additional details available in Appendix D."}, {"title": "", "content": "Evaluation metrics. We adopt the following five metrics to quantify various aspects of forgetting:\n(1) \\(GP = \\frac{1}{N^g} \\sum_{j=1}^{N^g} a_j^N\\), which is the average zero-shot performance across \\(N^g\\) general evalua- tion tasks after instruction tuning on the final N-th task. Here, \\(a_q^m\\) denotes the zero-shot perfor- mance on task q after sequentially tuning the m-th task, and T refers to the j-th general evaluation"}, {"title": "4 CORRELATIONS BETWEEN FUNCTION VECTORS AND FORGETTING", "content": "The previous section prompts a more effective measure for characterizing catastrophic forgetting, surpassing those traditionally used in continual learning research with small models, such as feature similarity (Ramasesh et al., 2020; Lee et al., 2021) and readout similarity (Lee et al., 2021) between tasks. We have proven them loosely correlated with forgetting under LLMs. Other model-dependent measures, such as the l2-distance of model parameters after tuning on new tasks (Lin et al., 2023; Evron et al., 2024), necessitate expensive training for their computation. In this section, we first establish that the similarity between function vectors (FV, see Sec. 2) is strongly correlated with diverse forgetting patterns across task types, training stages, and language models.\nForgetting coincides with changes in FV similarity between model updates. We now explore the relationship between forgetting and variations in function vectors across evaluation tasks. As shown in Figure 2, we evaluate the performance on the four general evaluation tasks throughout the sequence for both generation and classification settings, alongside shifts in function vectors \u03b8\u0442\u0435. \u201cFv sim\u201d in the diagram refers to Cosine(\\(\\theta_0^{Te}\\), \\(\\theta_j^{Te}\\)), where re is the FV of evaluation task Te after fine-tuning the j-th task. We observe a clear consistency between the performance decline and variations in function vectors. Specifically, as performance drops, the similarity between FV \\(\\theta_0^{Te}\\) and FV \\(\\theta_j^{Te}\\) generally declines, whereas this similarity tends to increase as performance recovers. For example, in the Hellaswag task within NI-Seq-G1 (top right subplot in Figure 2), the correlation coefficient (R2 value) between zero-shot performance and our proposed similarity measure reaches 0.873. This finding underscores that fluctuations in the FV often coincide with model forgetting, and justifies the rationale of characterizing forgetting through function vectors."}, {"title": "5 CAUSAL PATHWAY TO FORGETTING THROUGH FUNCTION VECTORS", "content": "Before delving into how function vectors causally influence forgetting, we revisit the function vector \\(\\theta_T\\) from the perspective of latent variable models (Baum & Petrie, 1966; Gruber et al., 2007). The"}, {"title": "", "content": "specific FV \u04e9\u0442 in Sec. 2 is derived from the sum of activations of certain attention heads in the LLM via causal analysis, provided with ICL input of task T. \u03b8\u0442 represents a latent variable descriptive of task T; conditioning predictions on \u03b8\u03c4 produces a model function specific to task T, i.e.,\nPM(y/x, \u03a3\u03b9\u03ba = \u04e9\u0442) \u2192 fr(y|x).\nHere, fr characterizes the function of task T within the model, where different function vectors activate distinct functions, enabling the model to exhibit diverse abilities.\nPrior research (Xie et al., 2021; Wang et al., 2024a) has established that under a latent variable assumption, in-context learning in LLMs can be rewritten as:\nPM(yx, y,...,x, yn, x) = Jo PM (y 0, \u0445)\u0420\u043c (0x1, y1,...,xn,yn,x) do,\nwhere PM denotes the predictive probability of the LLM M, and @ is a potentially high dimensional latent variable within the space \u0398. For example, in the task of predicting the antonym (y) of a word (x), the latent variable could correspond to \"writing the antonym of the word\u201d (\u03b8). This framework indicates that ICL boosts performance by deducing the correct @ from observed input-output pairs."}, {"title": "6 FUNCTION VECTOR GUIDED TRAINING DESIGN", "content": "In this section, we present the function vector guided training design that serves as an effective mechanism for mitigating the forgetting of general abilities applicable to a wide range of language models and continual learning baselines. We introduce the overall architecture, present experimental results, and analyze how function vector guided training works.\nFunction vector guided training. The correlation between forgetting and the function vector im- plies a principle for training method design. That is, training should be capable of maintaining the correct mapping of inputs to function vectors and thus mitigate forgetting. Based on this principle, we propose function vector-guided training as a simple yet efficient design to mitigate forgetting. Our method introduces two novel regularization terms:\nFirst, to mitigate the introduction of a biased function vector during training, we restrict the alter- ations in FVs tied to the training tasks, effectively maintaining the model's PM(\u04e9\u0442|x) unchanged. To this end, restrictions are imposed on the activation values of specific heads signifying the function vector with a FV consistency loss. When training task Tj, the loss is specified as follows:"}, {"title": "", "content": "lFV = \u03a3d((x), h(x)),\nwhere \\(h_{lj}^{M_T}(x)\\) denotes the activations on last input token of head j in layer l on input x from model M and Mj-1 is the model before training task Tj. d(\u00b7, \u00b7) is the distance metrics between variables, and we adopt L2 distance in this paper.\nFurthermore, we introduce a FV-guided KL-divergence loss to align the task function raised by zero-shot inputs and the FV-intervened one. The detailed function for training task Tj is as bellow:\nlKL = KL[PM(\u00b7 | x)||Pm1+09; (\u00b7 | x)].\nPM( x) is the predicted probability distribution for each token in the vocabulary Y given input  Mh+r; denotes the model train after j 1 tasks with intervention by the function vector 0, that is, is added to the activation h\u012b of layer l during forward. I is selected as 9 in this paper."}, {"title": "7 RELATED WORK", "content": "Catastrophic forgetting in fine-tuned language models. Fine-tuning foundational LLMs (Tou- vron et al., 2023a;b) has become a generic technique for enhancing their capacity of following instructions (Wei et al., 2022; Zhang et al., 2024a;b) and mastering domain-specific content (Yue et al., 2023; Christophe et al., 2024). However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting (Kirk- patrick et al., 2017; Luo et al., 2024; Kotha et al., 2024; Wu et al., 2024b). In context of LLMs, existing approaches towards mitigating this issue can mostly be categorized into three types: regu- larizing the update of model parameters (Huang et al., 2021; Cha et al., 2021), replaying previous or self-synthesized data (Scialom et al., 2022; Huang et al., 2024a) and resisting interference via parameter-efficient fine-tuning (Razdaibiedina et al., 2023; Wang et al., 2023b). In this work, we aims to characterize CF in LLMs through the function vector, concluding that such forgetting primar- ily stems from biases in function activation rather than the overwriting of task processing functions. To this end, We propose function vector guided training, a regularization-based method to protect task activation from being improperly destroyed during fine-tuning to cure the forgetting issue.\nMechanistic analysis to fine-tuning. Existing works on analyzing the internal mecha- nism (R\u00e4uker et al., 2023; Ferrando et al., 2024) of fine-tuning mainly focus on the question that how LLMs acquire new capacity in the learning process, arguing that models learn a minimal trans- formation on top of the original capability (Jain et al., 2024) (wrappers), subtractable and reusable parameter shift vectors (Huang et al., 2024b; Gao et al., 2024) (task vectors) and to align input queries with their internal knowledge that are already acquired in the pre-training stage (Ren et al., 2024). Nevertheless the inherent reason for the forgetting issue brought by fine-tuning currently remains unclear, and hence our work instead targets on this important point. We have successfully identified the compact task representation, known as the function vector, can tracks task forgetting in LLMs. Our empirical data indicate a strong correlation between shifts in the function vector and the phenomenon of task forgetting."}, {"title": "8 CONCLUSION", "content": "In this study, we tackle the issue of catastrophic forgetting in Large Language Models (LLMs) via a detailed investigation using the Function Vector (FV) approach, highlighting its pivotal role in characterizing and mitigating forgetting phenomena. Our analysis across a vast array of benchmarks reveals that model forgetting is intricately linked to shifts in latent concept variables (characterized by function vector), facilitated by our novel function vector-guided training strategy. This method, integrating a regularization term with a function vector-guided Kullback-Leibler divergence loss, significantly curtails forgetting, thereby enhancing both general and in-context learning capabilities of LLMs in continual learning settings."}, {"title": "A FUNCTION VECTOR EXTRACTION", "content": "We next consider how to extract de for a given dataset De, drawing on the concept of function vectors proposed by Todd et al. (2023). This extraction is carried out using in-context learning (ICL) samples, where the model incorporates task-relevant information into its hidden states as it engages with examples with the ICL prompt. This process is associated with the emergence of dc (Todd et al., 2023; Hendel et al., 2023). Subsequently, a causal mediation analysis (Pearl, 2013; Vig et al., 2020; Li et al., 2024) is conducted on the ICL inputs to identify attention heads with significant causal impacts on the output, and aggregating their representations results in \u03b8. Interestingly, this vector remains effective even under zero-shot input scenarios. The detailed procedure is outlined below:\nFirst, we start by gathering the task-conditioned activation for each model head by averaging the ICL input representation of the given task D\u00ba, i.e.,"}, {"title": "", "content": "hij = De hej ([p, x]).\nWhere p = [(x1, Y1), ..., (XN, YN)] represents the N-shot ICL prompt text made up of held-out samples of task c, hij is the model activation at the last token, layer l and position j, and hij represents the task-conditioned activations.\nThen to assess the existence of a cause-and-effect relationship between h\u2081 and correct output, we employ causal mediation analysis. The model will run on a counterfactual ICL input [p, x] incorporating a label-shuffled prompt \u00ee\u00f4 = [(x1, \u01771), ..., (XN, \u0177N)], typically leading to incorrect outcomes. We then substitute the value of the specific head with the task-specific conditioned activation hij and calculate its causal effect (CE) on the model's output.\nCElj ([p, x]) = Phishi; (Yi | [P, x]) - PM (Yi | [p, x]).\nHere, Mhij\u2192hij denotes the model with a replacement operation on attention head (l,j) at last token of the input sentence. A higher CE suggests that the specific head's state is crucial in enabling accurate predictions, denoting the encoding of more task-relevant information. For each head at layer l and position j,we adopt the approach proposed by Todd et al. (2023) to calculate the average CE across a variety of tasks. Subsequently, we identify the top 10 heads with the highest average CE (recorded as set S) as the most critical in conveying task-relevant information. The task vector \u03b8e is is then obtained by aggregating the task-conditioned activation from the attention heads in the set S, i.e., \u03b8c = \u2211(1,3)es hij\nWe then evaluates the effectiveness of the function vector (0) through intervention experiments on the initial model across multiple datasets. Results show that the FV significantly influences the"}, {"title": "B FUNCTION VECTOR GUIDED TRAINING ALGORITHM", "content": "Algorithm 1 outlines the procedure for function vector guided continual learning. It begins with a sequence of tasks, each paired with its corresponding dataset, as well as a pre-trained language model (referred to as Mo). Based on the approach from Todd et al. (2023), a collection of held-out datasets {D1, D2, . . ., DK } is utilized to determine the set of function vector heads. Furthermore, it is proposed that the function vector head set S is applicable across different datasets, allowing us to collect this set S only once."}, {"title": "C ILLUSTRATION OF CAUSAL PATHWAY TO FORGETTING.", "content": "To help the understanding of \"the causal pathway to forgetting through function vector\", we provide the illustrations in Figure 5 and the detailed discussions in Section 5. Refer to the caption of Figure 5 for more information."}, {"title": "D DATASETS", "content": "Three continual instruction tuning benchmarks and severel general evaluation datasets are adopts in this paper. The detailed information is as follows:\nTRACE benchmark. TRACE benchmark is released by Wang et al. (2023c) for the study of forgetting in LLMs, which consists of 8 different complex generation tasks including multi-choice QA, code generation, mathematical reasoning and summary. Without loss of generaliztion, we select 6 out of 8 raw tasks to construct the training sequence as our experiments setup. The statistical information is listed in Table 3.\nThe training epoch for this benchmark is 5 for C-STANCE, Py150, NumGLUE-cm, 3 for FOMC and ScienceQA, and 7 for MeetingBank. We evaluate them with a self-construct evaluation code based on OpenCompass code framework.\nSuperNI benchmark. SuperNI benchmark is widely utilized in existing instruction-following works. We select 26 tasks from the original dataset and set the training size to 1000 and training epoch set to 10. The statistical information is listed in Table 4.\nGeneral evaluation sets. For the general evaluation datasets, we utilize Hellaswag (Zellers et al., 2019), CommonsenseQA (Talmor et al., 2018), OpenbookQA (Mihaylov et al., 2018), Natural Question Kwiatkowski et al. (2019), Lambada Paperno et al. (2016), Alpaca Taori et al. (2023)"}]}