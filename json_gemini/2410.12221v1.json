{"title": "EdgeRL: Reinforcement Learning-driven Deep Learning Model Inference Optimization at Edge", "authors": ["Motahare Mounesan", "Xiaojie Zhang", "Saptarshi Debroy"], "abstract": "Balancing mutually diverging performance metrics, such as, processing latency, outcome accuracy, and end device energy consumption is a challenging undertaking for deep learning model inference in ad-hoc edge environments. In this paper, we propose EdgeRL framework that seeks to strike such balance by using an Advantage Actor-Critic (A2C) Reinforcement Learning (RL) approach that can choose optimal run-time DNN inference parameters and aligns the performance metrics based on the application requirements. Using real world deep learning model and a hardware testbed, we evaluate the benefits of EdgeRL framework in terms of end device energy savings, inference accuracy improvement, and end-to-end inference latency reduction.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning models, particularly deep neural networks (DNN), are becoming increasingly important for mission-critical applications, such as public safety, tactical scenarios, search and rescue, and emergency triage, most of which are often edge-native. Unlike traditional edge that are typically part of the network infrastructure, a new paradigm of ad-hoc deployments of edge computing environments are currently being adopted by public safety agencies and armed forces [1]-[3] to support mission-critical use cases. Here, heterogeneous components in the form of energy-constrained end devices (e.g., drones, robots, IoT devices) and edge servers with varied degrees of computational and energy capacities are loosely coupled to primarily run pre-trained DNN model inference with strict latency and accuracy requirements.\nIn such implementations, running the entire DNN inference on end devices (e.g., drones, robots) is impractical due to their resource constraints, which makes them incapable of satisfying the inference latency and accuracy requirements. It is also not prudent to run those entire DNN models on the edge servers as they lack sufficient resource capacity (i.e., in comparison to cloud servers) that can support heterogeneous inference workloads generated from multiple end devices, simultaneously [4]. Thus, in recent times, an alternative approach of partial offloading/DNN partitioning/DNN splitting/collaborative inference [1], [5] has gained traction that embraces segmenting the DNN models and processing the segments on end devices and edge servers collaboratively.\nHowever, any attempt to make such partial offloading strategy effective and practical, needs to consider the fundamental three-way trade-off between end-to-end inference latency, model inference accuracy, and end device energy consumption metrics (henceforth referred to as 'latency-accuracy-energy'). This is because, each of the metrics in 'latency-accuracy-energy' trade-off problem is a function of the convolutional layer of the DNN where such partition/split is carried out, as well as the unique characteristics of the involved DNNs, such as, the number of convolutional layers, the computational complexity of each layer, and the output data size at each convolutional layer of the DNN, among other things [6]\u2013[9]. For example, if a DNN is split at a layer whose output data size is larger, then the overall inference latency may improve due to lightweight computation at the device. However, this may result in an increase in the device energy consumption for transmitting the larger amount of output data to the edge server [1], [10], [11]. While, a lightweight/compressed version of a particular DNN model, when chosen to run collaboratively to lower inference latency, on the flip-side, can compromise inference accuracy due to the lower number of convolutional layers and hyperparameters in the compressed DNN.\nIn this paper, we address this non-trivial and grossly under-explored three-way trade-off problem by designing and developing a novel EdgeRL framework. The framework allows the ad-hoc edge environments select a DNN execution profile, which involves choosing an optimized version of a given DNN model from multiple pre-cached versions, whether lightweight or heavyweight, and selecting a partition cut point layer for the chosen version to perform collaborative inference with the edge server. This execution profile selection is framed as a Markov Decision Process (MDP) and solved using an Advantage Actor-Critic (A2C) based reinforcement learning approach. The model integrates inputs such as the end device's battery status, activity profile, available bandwidth, and kinetic activity to continuously adapt and learn system dynamics through iterative actions and rewards. The reward function aims to maximize a customizable performance metric that balances 'latency-accuracy-energy', addressing"}, {"title": "II. SYSTEM MODEL AND SOLUTION APPROACH", "content": "In this section, we describe the ad-hoc edge deployment system model, along with its energy and latency considerations, the details of the EdgeRL framework, and the RL-driven solution approach."}, {"title": "A. System Model", "content": "As shown in Fig. 1, we assume an exemplary ad-hoc edge environment where end devices (UAVs in this case), in collaboration with one limited capacity edge server, perform DNN model inference for real-time missions. The components of the system are as follows:\nDNN Model \u2013 We define a set of m DNN models, denoted as M = {M1, M2, ..., Mm}, each tailored for specific objectives and tasks towards the missions. We consider model Mi to have Vi different versions {Mi,1, Mi,2, ..., Mi,v; }, generated as a result of model optimization, with each employing either a compressed or extended architecture with diverse layers. These versions exhibit unique characteristics in accuracy and computational complexity. The accuracy and number of layers of the i-th model in its j-th version are expressed as $M_{i,j}^{acc}$ and $M_{i,j}^{layers}$, respectively. In this context, $M_{i,j}^l$ denotes the 'head' of the model up to and including layer l when referring to computations on the end device or local computation, and the 'tail' of the model from layer l + 1 onwards when referring to computations on the edge server. Though our framework solutions are adaptable to all classes of DNNs, in this work, we focus on video processing DNNs commonly used in mission-critical applications. Consequently, we set stringent performance requirements for DNN model accuracy and latency. Specifically, each model Mi must have an end-to-end inference latency not exceeding $T_{latency}$ and must achieve an accuracy of $a_{acc}$ for the application to be successful.\nFor the end devices, we assume realistic scenarios that are popular for mission critical use cases adopting ad-hoc edge environments. Specifically, we model end devices that perform other (i.e., mostly kinetic) activities on top of capturing video/image of a scene and partially performing computation. The objective is to create a realistic yet challenging device energy consumption scenario for the EdgeRL framework to address.\nEnd Device - We define a set of n heterogeneous Unmanned Aerial Vehicles (UAVs) or drones (very common for mission critical use cases), denoted as U = {U1, U2, ..., Un}, each equipped with computational capabilities. The heterogeneity comes from the UAV model that defines UAV weights and architecture, battery level, and UAV kinetic activity profile, explained later. Each UAV collaboratively executes a DNN inference task in collaboration with an edge server, facilitated through wireless connectivity between the two. Each UAV Uk is defined by a quadruple (ID, build, battery level, trajectory), where \u2018ID\u2019"}, {"title": "B. Latency model", "content": "The latency $T_{i,j}$ for executing $M_{i,j}$ collaboratively between $U_k$ and the edge server $E$, partitioned at cut point $l$, consists of three main components. First, the local processing time $T_{local,i,j}$ represents the latency involved in processing the 'head' of $M_{i,j}$ at $U_k$, constrained by the limited processing capabilities available at $U_k$. Second, the transmission time $T_{trans,i,j}(U_k, \\lambda)$ refers to the latency associated with data transfer between $U_k$ and the edge server, determined by the transmission rate $ \\lambda $. This bandwidth can be very limited depending on the mission-critical use case, often making it a limiting factor for full offloading of $M_{i,j}$ to the edge server, thereby necessitating partial offloading as discussed in [1]. Finally, the server or remote processing time $T_{remote,i,j}$ includes both the computation time $T_{comp,i,j}$ for the tail of the model on the server $E$ and the server queue time $T_{queue}$. The total server processing time is given by:\n$T_{remote,i,j}(E) = T_{queue}(E) + T_{comp,i,j}(\u03b5)$ \n The variability in $T_{queue}$ at server $E$, influenced by concurrent tasks managed for other jobs by the edge server, is crucial for accurately modeling the operational dynamics of limited resource ad-hoc edge servers. Thus, the total end-to-end latency is:\n$T_{i,j}(U_k, \\lambda, E) = T_{local,i,j} (U_k) + T_{trans,i,j} (U_k, \\lambda) + T_{remote,i,j} (E)$"}, {"title": "C. Deep Reinforcement Learning (DRL) Agent", "content": "The controller trains a DRL agent to handle the dynamic nature of the system in ad-hoc edge environments. Specifically, we model the DNN optimization problem as a Markov Decision Process (MDP) and use a time-slot-based decision-making approach based on the Advantage Actor-Critic (A2C) algorithm [13], [14]. The choice of A2C is driven by its efficiency and effectiveness. In A2C, an agent serves both as the actor and the critic, combining policy-based and gradient-based methods. The actor makes decisions, while the critic evaluates these decisions and provides feedback to refine strategies. This collaborative approach accelerates training and enhances learning with each experience. Moreover, A2C is a stable algorithm capable of handling large observation spaces, such as environments with potentially multiple UAVs and corresponding DNN models and versions. The A2C agent operates in an environment characterized by a finite set of states denoted as S and a finite set of actions denoted as A, under a time-slot based system, with intervals of 8 time units.\nS represents the state space of the environment. At time t, the state s(t) \u2208 S includes the battery level of the kth UAV (bk(t)), task availability (ak(t)), available transmission power (P), the DNN model (mk), and the percentages of forward"}, {"title": "D. EdgeRL Framework Design", "content": "The proposed EdgeRL framework has a centralized controller, serving as the system manager for decision-making processes. The end devices, i.e., UAVs in our case play a crucial role by transmitting essential information such as task details, battery levels, and available transmission speeds to the controller. This aggregated data forms the system's state, which is then processed by an actor network within the controller. The actor network utilizes this information to generate actions, taking into account various factors like system performance and resource availability. Once actions are generated, they are relayed back to the respective UAV devices. Simultaneously, the system records rewards based on performance metrics such as accuracy, latency, and energy consumption within the edge environment. Following this, a critic network estimates the advantage values and trains both the actor and critic networks based on the actions taken and the resulting rewards. Continuing through this iterative learning process, the system refines and adapts until it reaches convergence, ensuring optimal performance and responsiveness to environmental variables. Each episode concludes when all UAV devices' batteries are depleted."}, {"title": "III. EVALUATION", "content": "Next, we evaluate the performance of our proposed framework through hardware testbed experimental evaluation."}, {"title": "A. Testbed Setup and Experiment Design", "content": "For our ad-hoc edge deployment testbed, we utilize three NVIDIA Jetson TX2 devices as computational units of the end devices/UAVs. Additionally, a Dell PowerEdge desktop with 16 cores 3.2 GHz CPU serves as the edge server. The network connectivity between the TX2 devices and the edge server is established through an Ettus USRP B210 acting as the access point, and can operate on both WiFi and LTE bands. Due to the lack of UAV hardware availability, we simulate UAV kinetic activity based on an average size drone UAV Systems Aurelia X4 Standard and compute energy consumption of each movement based on the model proposed in [12]. To account for device heterogeneity, we consider three distinct activity profiles for UAVs, each representing varying levels of kinetic activity. For our experiment, we specifically use the High activity profile, which features a dominant forward flight rate of 80%, with minimal vertical and rotational movements (10% each). This profile represents the most challenging scenario, as it emphasizes extensive forward motion, which generally requires greater coverage.\nFor the experiments, we mostly focus on object classification tasks as exemplar video processing applications. The UAV devices, execute three popular classification DNNs, viz., VGG, ResNet, and DenseNet. As for different versions, we assume that each DNN has two variants: a lightweight, less accurate model (e.g., VGG11, ResNet18, and DenseNet121), and a heavyweight, more accurate model (e.g., VGG19, ResNet50, and DenseNet161). Furthermore, drawing from insights our analysis, we identify four potential cut points for each such DNN version (Table I), to enable collaborative DNN inference. Apart from the object classification jobs arriving from the UAVs, we simulate the edge server to support other mission related jobs with exponential arrival rate which impact the size of the queue to follow a Poisson point process. We use a time slot duration of 8 = 30s to meet reconnaissance demands in ad-hoc edge environments, with the controller making decisions at each interval."}, {"title": "B. Reward Sensitivity Analysis", "content": "We explore the sensitivity of the reward function for each performance metric across different DNN models."}, {"title": "1) Sensitivity of accuracy weight::", "content": "Fig. 2 showcases the system performance for varying weight of accuracy reward in Eqn. (8). A notable observation (as seen in Fig. 2(a)) is that the higher accuracy versions demonstrate better latency and energy efficiency. This is evident by the sustained high accuracy even when the accuracy reward weight is set to zero. There is minimal improvement in accuracy with the increase in accuracy weight in the reward function. Moreover, as we increase the weight, there is a noticeable decline in latency and energy performance. This trend can be attributed to the selection of cut points."}, {"title": "2) Sensitivity of latency weight::", "content": "Fig. 3 presents the findings from similar experiments with latency reward weight manipulation. As we increase the emphasis on latency, there's a noticeable decrease in the average latency of the models. However, this reduction comes at the cost of increased inference energy consumption. Consequently, such a pattern inevitably leads to a diminished battery life for the devices. This observation is further corroborated when we compare Figs. 3(b) and 4(b) where a clear tradeoff between latency and energy consumption emerges. The selection of versions and cut points for the $w_2 = 0$ and $w_2 = 1$ models is detailed in Tab. II. Notably, with a latency weight of 0, the energy score predominance results in a greater proportion of layers being processed remotely. However, due to latency in transmission for $w_2 = 1$, offloading is postponed until later layers."}, {"title": "3) Sensitivity of energy weight::", "content": "Next, Fig. 4 illustrates the findings from similar experiments with energy consumption reward weight manipulation. For obvious reasons with increasing the weight, the inference energy consumption drops as energy is given the higher priority. However, end device/UAV energy consumption (from Fig. 4(e)) shows a different trend, specifically, the device running DenseNet - the most energy-consuming among the models. For accuracy performance, it can be observed"}, {"title": "IV. CONCLUSIONS", "content": "In this paper, we analyzed the end-to-end latency vs. inference accuracy vs. device energy consumption trade-off for ad-hoc edge deployments and proposed EdgeRL framework that employs a novel A2C based RL model. The EdgeRL framework performs DNN version selection and cut point selection based on resource availability and system performance requirements. We demonstrated how the underlying A2C based RL agent learnt about the environment through actions and rewards which eventually converged at an optimal trade-off point for involved performance metrics. Using real world DNNs and a hardware testbed, we evaluated the benefits of EdgeRL in terms of device energy saving, accuracy improvement, and end-to-end latency reduction."}]}