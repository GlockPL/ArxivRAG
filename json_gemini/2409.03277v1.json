{"title": "ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding", "authors": ["Zhengzhuo Xu", "Bowen Qu", "Yiyan Qi", "Sinan Du", "Chengjin Xu", "Chun Yuan", "Jian Guo"], "abstract": "Automatic chart understanding is crucial for content comprehension and document parsing. Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in chart understanding through domain-specific alignment and fine-tuning. However, the application of alignment training within the chart domain is still underexplored. To address it, we propose ChartMoE, which employs the mixture of expert (MoE) architecture to replace the traditional linear projector to bridge the modality gap. Specifically, we train multiple linear connectors through distinct alignment tasks, which are utilized as the foundational initialization parameters for different experts. Additionally, we introduce ChartMoE-Align, a dataset with over 900K chart-table-JSON-code quadruples to conduct three alignment tasks (chart-table/JSON/code). Combined with the vanilla connector, we initialize different experts in four distinct ways and adopt high-quality knowledge learning to further refine the MoE connector and LLM parameters. Extensive experiments demonstrate the effectiveness of the MoE connector and our initialization strategy, e.g., ChartMoE improves the accuracy of the previous state-of-the-art from 80.48% to 84.64% on the ChartQA benchmark.", "sections": [{"title": "Introduction", "content": "Charts serve as a fundamental tool for data visualization, with automated chart interpretation gaining prominence in domains such as text analysis Hoque et al. (2017), scientific research Hsu et al. (2021), and policy-making Wu et al. (2024). Chart understanding is a complex task that demands the identification of visual cues, the comprehension of intricate interactions, and the precise inference of values informed by prior knowledge. Previous work Lee et al. (2023); Liu et al. (2023b,a) typically pretrained on structured datasets or domain-specific charts to improve the model performance and efficiency. However, they are constrained by limited resources and narrow task focus, thereby restricting the application in diverse real-world scenarios. In contrast, MLLMs Li et al. (2023); Liu et al. (2023d); Bai et al. (2023a); Ye et al. (2023b); Chen et al. (2023a); OpenAI (2023) exhibit substantial potential in image comprehension and instruction following. With the exponential growth of chart data, automated chart interpretation via MLLMs is emerging as a promising avenue.\nThe community has achieved advanced progress by creating chart understanding datasets Liu et al. (2023c); Han et al. (2023); Masry et al. (2024b); Xu et al. (2023) and applying supervised fine-tuning on charts Meng et al. (2024); Yan et al. (2024); Carbune et al. (2024). Recent studies advocate for alignment pre-training as a foundational step in this paradigm, which only updates the connector's parameters to bridge the modality gap. ChartAst Meng et al. (2024) proposes to align chart with table, significantly improving the performance across several chart benchmarks. ChartReformer Yan et al. (2024) proposes to switch the alignment intermediary from table to"}, {"title": "Related Work", "content": "Multimodal large language models leverages a connector to bridge the gap between large language models Touvron et al. (2023); Radford et al. (2018); Brown et al. (2020); Zhang et al. (2022); Zheng et al. (2023) and vision encoders Radford et al. (2021); Oquab et al. (2023) to enable enriched capabilities of comprehension and instruction following. Approaches such as BLIP2 Li et al. (2023), Flamingo Alayrac et al. (2022), mPLUG-Owl Ye et al. (2023b), and Qwen-VL Bai et al. (2023b) utilize QFormers or Resamplers to align modalities on extensive datasets of image-text pairs. LLaVA Liu et al. (2023d, 2024b) is the pioneering work to extend the instruction tuning paradigm to visual tasks with text-only GPT-4 OpenAI (2023), achieving tremendous performance using a simple MLP without compromising visual information to refine the multimodal alignment.\nSome works Lin et al. (2023); Tong et al. (2024b,a) explore the combination of various vision encoders, complementarily enhancing visual representations to bolster the fine-grained visual perception of MLLMs. Despite efforts in structural design, training strategies and data quality remain crucial in the advancement of MLLMs.\nChart Reasoning refers to chart analysis, summarization, and etc. Existing methods can be categorized as 1) Two-stage methods use specialized extraction modules to generate intermediary representations of chart information, like tables, which are provided as textual prompts for LLMs. Pix2Struct Lee et al. (2023) aligns markdown data with charts. MatCha Liu et al. (2023b) aligns various data formats (e.g., tables and code) with charts on several downstream tasks. DePlot Liu et al. (2023a) fine-tunes Pix2Struct for table extraction and uses LLMs to process queries based on the extracted data. ChartVLM Xia et al. (2024) employs a discriminator to ascertain the necessity of intervention by LLMs for a given query. 2) End-to-end methods strive to tackle chart reasoning challenges with a unified model. ChartLlama Han et al. (2023) incorporates diverse charts and downstream tasks based on LLaVA Liu et al. (2023d). ChartPaLI Carbune et al. (2024), ChartAst Meng et al. (2024), and MMC Liu et al. (2023c) conduct alignment on table-chart pairs. UReader Ye et al. (2023a) aligns all data with markdown, while mPLUG-Owl2 Ye et al. (2023c) achieves superior performance with high-resolution inputs. ChartThinker Liu et al. (2024c) and DOMINO Wang et al. (2023) propose the CoT Wei et al. (2022) for chart reasoning. LaMenDa Zhuowan et al. (2024) trains MLLMs via step-by-step reasoning QA. ChartReformer Yan et al. (2024) introduces chart-JSON alignment, while OneChart Chen et al. (2024) aligns charts with Python dictionaries. MiniGPT-v2 Chen et al. (2023a), Doc-Owl Hu et al. (2024), and TinyChart Zhang et al. (2024) tackle the reasoning efficiency for high-resolution charts by merging tokens."}, {"title": "ChartMoE", "content": "The ChartMoE is based on InternlmXC-v2 Dong et al. (2024) due to the concise LLaVA-like architecture Liu et al. (2023d) and performance on par with GPT-4 on text-image comprehension. The base model includes a vision encoder and a LLM connected by a two-layer MLP. In our ChartMoE we replace the MLP with an MoE architecture as the connector to leverage diverse prior knowledge.\nWe utilize CLIP ViT-Large Radford et al. (2021) as the vision encoder, leveraging its rich prior knowledge gained from training on millions of image-text pairs. Considering the impact of chart resolution on performance, we set the input resolution to 490 \u00d7 490 to strike a balance between efficiency and performance. Formally, the visual encoder MV(\u00b7) will project the chart I into N tokens V := {v1, v2, ..., vN }, where N = 1225 in the ChartMoE.\nAs illustrated in Fig. 2c, the MoE architecture employs a parallel multi-expert collaboration approach. This architecture comprises L experts ME(\u00b7), each designed with the same linear layer as the baseline. For a visual token vi given by MV, the gating network MG(\u00b7) will calculate the routing weight gj(vi) of each expert MF(\u00b7) and select top-K to activate. Finally, the tokens processed by each expert M will be averaged according to the weight gj(Vi) given by MG to get the token v\u2081 for the LLM branch ML.\nFollowing the baseline, we employ the InternLM2-7B-ChatSFT variant as the LLM M\u00b9, implemented as a transformer decoder with a causal attention mask. We concate the visual tokens V := {\u00db1, \u00db2, . . ., \u00dbN} given by MoE connector with the M input text T tokens T := {t1, t2, ..., t\u043c } to form the input token sequence for the LLM M\u00b9. Formally, give the chart"}, {"title": "Architecture", "content": "We introduce ChartMoE, an instruction-tuned MLLM specifically designed for chart comprehension and reasoning. As illustrated in Fig. 1&2, we replace the linear connector with the Mixture of Experts Zoph et al. (2022) (MoE) architecture, each expert utilizes a two-layer MLP with GeLU activation Lee (2023). Specifically, We train three distinct connectors with different alignment tasks independently, i.e., chart to table, chart to JSON, and chart to code, as shown in Fig. 1. For better alignment, we construct ChartMoE-Align via existing chart datasets, which consist of over 900K quadruplets {chart, table, JSON, code}, as illustrated in Fig. 3. Additionally, we retain the original vanilla connector to preserve the base model's foundational knowledge and conversational abilities. Consequently, we offer four distinct pathways for initializing the expert parameters. Compared to random initialization, ChartMoE achieves faster and more stable convergence. Unlike Co-Upcycling initialization Komatsuzaki et al. (2023), ChartMoE is grounded in a more principled design to prevent experts' homogenization effectively.\nWe conduct high-quality knowledge learning using the MMC instruction Liu et al. (2023c) to update the routing network, expert connectors, and LLM LORA parameters while further employing annealing training with datasets like ChartQA Masry et al. (2022) and ChartGemma Masry et al. (2024b). Following the previous work Zhang et al. (2024); Masry et al. (2024b), we integrate the Program of Thought (PoT) prompting Chen et al. (2023b) to enhance the mathematical computation capabilities. ChartMoE can be deployed for inference on a single A100-40G GPU, achieving state-of-the-art results in areas such as question-answering, number extraction, and chart editing&highlighting. In summary, our contributions are:\na) We present ChartMoE for faithful and reasonable chart understanding, with an MoE architecture connector to bridge the visual and LLM branches, enabling differential expert initialization based on various alignment training tasks.\nb) We introduce ChartMoE-Align, a large-scale dataset with over 900K chart-table-JSON-code quadruplets for chart alignment pretraining.\nc) We propose to train ChartMoE with a three-stage training paradigm, including alignment pre-training, high-quality knowledge learning, and annealing chart tuning.\nd) Extensive quantitative and qualitative studies demonstrate that ChartMoE significantly outperforms previous state-of-the-art across several benchmarks by a large margin."}, {"title": "Architecture", "content": "I and instruction T, the output O of proposed ChartMoE can be formulated as:\n{\u00db1, \u00db2, ..., \u00dbN} = MV (I), (1)\n\nvi =  \u2211L j=1 gj(Vi)M(Vi), gj(vi) = Top(\u03c3(MG(vi)); K), (2)\nO = ML({\u00db1, \u00db2, ..., \u00dbN ; t1, t2, ..., t\u043c}), (3)\nwhere \u03c3 indicates softmax and the Top(\u00b7; K) will reset the non-Top K routing weight to 0."}, {"title": "Alignment Pretraining.", "content": "Unlike previous work, we obtain the initialization parameters for the various experts during the different stages of alignment pre-training. Specifically, as illustrated in Fig. 2b, we align expert connectors using three distinct alignment tasks, where only the connector parameters will be updated.\nChartAst Meng et al. (2024) first proposes performing chart alignment before supervised fine-tuning, effectively enhancing the model's ability in numerical extraction and chart understanding. We align charts and tables (in CSV format, shown in Fig. 2a) to improve the model's capability to extract tabular data.\nChartReformer Yan et al. (2024) proposes using JSON instead of table data for alignment pre-training. JSON contains not only the values of the table but also drawing properties of the chart, such as font format, line type, color style, etc. This task is more difficult than chart-to-table, contributing to attribute-based chart editing and highlighting.\nChart editing is primarily achieved by reverse engineering the plotting code and re-rendering the chart Han et al. (2023); Xia et al. (2024). Therefore, we propose the chart-to-code task. Specifically, the code contains precise data and plotting attributes, such as numerical values represented by Python lists and colors represented by hexadecimal codes, as shown in Fig. 2a. This task is more information-rich compared to JSON and Table alignment."}, {"title": "Supervised Finetuning.", "content": "We initialize ChartMoE using the structure shown in Fig. 2c after aligning the connectors across 3 distinct tasks separately. We also retain the vanilla connector to maintain the baseline's excellent dialogue capabilities, which aligns with the principle of residual optimization He et al. (2016). We train the MoE connector and LLM during this stage with LoRA Hu et al. (2022), as shown in Fig. 2c. Considering the training principles proposed in LLaVA-NeXT Liu et al. (2024a), this stage is divided into high-quality knowledge learning and chart-specific annealing training."}, {"title": "High-Quality Knowledge Learning", "content": "We adopt MMC Liu et al. (2023c) to enhance the ChartMoE's knowledge. MMC includes a variety of chart types and tasks such as chart-related question answering, translation, extraction, reasoning, and analysis. Considering data quality, we only utilize the MMC-Instruction subset, which has been manually verified. Notice that the quality of instruction data is more important than quantity in this stage."}, {"title": "Chart Specific Annealing Tuning", "content": "Following Llama-v3.1 Team et al. (2024b), we perform annealing tuning before evaluating mainstream benchmarks. We increase the learning rate and conduct instruction tuning using the training sets of ChartQA Masry et al. (2022) and Chart-Gemma Masry et al. (2024b) to adjust the query styles and answer formats of these benchmarks.\nPoT Chen et al. (2023b) requires the MLLM to generate the variables and operation code rather than producing direct answers. This inference pipeline addresses the mathematical capabilities by employing Python to handle the logical computations, which is the shortcoming of all open-sourced models. With better numerical extraction abilities, PoT can significantly enhance our ChartMoE's question-answering performance."}, {"title": "Experiment", "content": "The ChartMoE builds on InternlmXC-v2 Dong et al. (2024), integrating ViT-Large Radford et al. (2021) as the visual encoder and InternLM-v2 Cai et al. (2024) as the language model. We replace the MLP with MoE architecture as the connector. During the alignment stage, we train the connector parameters and keep the visual encoder and LLM parameters fixed for 1 epoch. The learning rate is set to 5e-5 with a warmup phase covering the first 1% of training steps. In the supervised fine-tuning stage, we continue training the connector while employing LoRA to update the LLM parameters with the rank of 64. The learning rate is adjusted to le-5 for the high-quality knowledge learning period and 5e-5 for the chart-specific annealing tuning period. The weight decay is 0.1 for all stages. We use the cosine annealing learning rate schedule. The global batch size is set to 64 for all stages. The training process is conducted on A100-40G GPU, with the alignment stage taking"}, {"title": "Evaluation Metrics", "content": "ChartQA Masry et al. (2022) test split consists of 1,250 questions in both human and augmented parts. The charts are three common chart types and are sourced from the real world. It features a variety of human-crafted questions and answers to evaluate models' understanding, reasoning, and data extraction skills. ChartQA adopts relaxed accuracy, which is highlighted shortcomings by recent studies Chen et al. (2024); Xu et al. (2023), such as simplistic string matching and direct float conversion. Therefore, we improve it by 1) using regular expression matching to extract number values, 2) optimizing string matching for short answers, and 3) demonstrating model performance under various relaxed margins. We adopt it for all experiment results in this paper.\nChartBench Xu et al. (2023) focuses on charts without data point annotations. It includes a broader range of chart types, with 9 main categories and 42 subcategories, each containing 50 charts. Chart-Bench focuses on extracting numerical values, posing a greater challenge as models cannot depend on OCR for precise answers. It adopts Acc+ for judgments and relaxed accuracy for NQA tasks. The benchmark proposes to extract number values by LLMs first, which is omitted for the stratifying instruction-following ability of ChartMoE.\nThey use accuracy to verify whether the claim aligns with the input chart, marking a significant advancement in chart recognition and reasoning abilities. This also identifies the potential hallucinations in chart-related contexts. The ChartFC test set has 1,591 questions, and the ChartCheck test set has two splits, containing 937 questions and 981 questions."}, {"title": "Comparative Models", "content": "We compare PaliGemma Beyer et al. (2024), LLaVA-v1.5 Liu et al. (2023d) with an MLP connector, Qwen-VL Bai et al. (2023b) with a Qformer Li et al. (2023) connector, DocOwl-v1.5 Hu et al. (2024) that employs multi-level image resolution and token convolution techniques, and the current open-source SOTA, InternlmXC-v2 Dong et al. (2024).\nWe compare Pix2Struct Lee et al. (2023), Matcha Liu et al. (2023b), UniChart Masry et al. (2023), and Deplot Liu et al. (2023a). Notably, Deplot fails to handle questions in arbitrary formats, so we extract table information with Deplot and use LLaVA-v1.6 to answer the questions.\nChartLLaMA Han et al. (2023) proposes to generate high-quality instruction data to"}, {"title": "Main Results", "content": "Tab.2 presents detailed comparisons of ChartMoE on ChartQA. ChartMoE significantly improves the baseline (InternlmXC-v2) performance (72.00% vs. 84.64%, +12.64%\u2191 in Acc.@0.05). Compared to previous SOTA (TinyChart+PoT @768 pixel), ChartMoE consistently surpasses it across all metrics. The PoT effectively enhances the model's mathematical reasoning capabilities, which is a common shortfall in current MLLMs. ChartMoE integrates better with PoT, indicating that it accurately extracts fundamental elements from charts. The questions in Human part tend to be more computationally complex and challenging, and ChartMoE shows more significant improvement in this area, especially after incorporating PoT. Notably, our error analysis in the Augmented part reveals that many errors stem from limitations of the evaluation criteria, i.e., string matching.\nTab. 3 presents detailed comparisons of ChartMoE on ChartBench. None of the models, including our ChartMoE, undergo supervised finetuning on the ChartBench trainset to ensure fair experimental comparison. Chart-specific models typically underperform due to their limited generalization ability, which prevents them from effectively managing charts with sparse data point annotations (< 10%). The baseline (InternlmXC-v2) demonstrates strong generalization on ChartBench (48.41%), which may benefit from pre-training instructions designed for unannotated charts. Without additional design, ChartMoE improves the baseline performance comprehensively (48.41% vs. 51.67%), especially on extra chart types (39.72% vs. 55.58%, +15.86%\u2191)."}, {"title": "Model Architecture Ablation", "content": "We investigate the impact of three factors on our MoE connector: the number of experts, the number of activated experts, and the expert initialization manner. All the experiments are conducted with ChartQA training data and evaluated on ChartQA test split with relax accuracy metric."}, {"title": "Effect of the Expert Initialization Manner", "content": "The initialization strategy plays a crucial role in determining the performance of the MoE connector. Effective initialization is essential to ensure that each expert performs its designated function optimally. Following CuMo Li et al. (2024), we employ the Co-Upcycle strategy by replicating the table-JSON-code aligned connector for all experts. Given the same starting point, this approach lacks expert diversity, which limits its effectiveness, resulting in an accuracy of 77.48% at Acc.@0.05. In contrast, our initialization assigns distinct parameters to each expert. This tailored approach enables each expert to capitalize on its specific strengths, resulting in the highest performance, achieving 78.76% in Acc.@0.05."}, {"title": "Effect of Number of Experts and Activated Experts", "content": "We compare ChartMoE configurations with 4 and 8 experts, keeping 2 experts activated. The 8 experts are initialized in pairs using the 4 methods illustrated in Fig. 2c. ChartMoE achieves 78.76% in Acc.@0.05 with 4 experts, which is slightly higher than the 78.60% achieved with 8 experts, showing a marginal increase of +0.16%. In rows 4-5, we compare the performance of configurations with 2 and 4 activated experts, finding similar results: 78.60% vs. 78.64% in Acc.@0.05. This analysis indicates that simply increasing the number of experts or activated experts does not necessarily lead to better performance. The configuration with 4 experts and 2 activated experts effectively balances complexity and performance, making it a suitable choice for ChartMoE."}, {"title": "Training Strategy Ablation", "content": "We analyze the impact of the training strategy across alignment and supervised fine-tuning stages. We use InternlmXC-v2 with ChartQA fine-tuning as our baseline, maintaining the same hyperparameters as the chart-specific annealing tuning stage."}, {"title": "Effect of Alignment Strategy", "content": "translating the chart image into structural text formats such as table, JSON, and code during the alignment stage significantly enhances performance in downstream chart understanding tasks. After applying table-JSON-code alignment, the model achieves 77.20% in Acc.@0.05, representing a notable improvement of"}, {"title": "Additional Capabilities on Chart Editing and Highlighting", "content": "Leveraging fine-grained alignment, particularly in translating input charts into corresponding Python code, ChartMoE demonstrates advanced capabilities in editing and highlighting key elements. As illustrated in Fig. 11, ChartMoE effectively modifies input charts based on user instructions, such as converting a vertical bar chart into a horizontal one, transforming a pie chart into a bar chart, and adjusting the values of specific elements.\nChartMoE also excels at adding auxiliary lines and highlighting elements within charts, facilitating the rapid extraction of valuable insights."}]}