{"title": "Leveraging Information Consistency in Frequency and Spatial Domain for Adversarial Attacks", "authors": ["Zhibo Jin", "Jiayu Zhang", "Zhiyu Zhu", "Xinyi Wang", "Yiyun Huang", "Huaming Chen"], "abstract": "Adversarial examples are a key method to exploit deep neural networks. Using gradient information, such examples can be generated in an efficient way without altering the victim model. Recent frequency domain transformation has further enhanced the transferability of such adversarial examples, such as spectrum simulation attack. In this work, we investigate the effectiveness of frequency domain-based attacks, aligning with similar findings in the spatial domain. Furthermore, such consistency between the frequency and spatial domains provides insights into how gradient-based adversarial attacks induce perturbations across different domains, which is yet to be explored. Hence, we propose a simple, effective, and scalable gradient-based adversarial attack algorithm leveraging the information consistency in both frequency and spatial domains. We evaluate the algorithm for its effectiveness against different models. Extensive experiments demonstrate that our algorithm achieves state-of-the-art results compared to other gradient-based algorithms. Our code is available at: https://github.com/LMBTough/FSA.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) are susceptible to subtle perturbations, which can lead to erroneous predictions [26]. The attacks with adversarial examples are generally classified into white-box and black-box attacks based on the level of information accessible to the attacker. In white-box attacks [8], the attacker has access to model information, such as model parameters, network structure, training dataset, and defense mechanisms. This allows for the deliberate construction of adversarial examples. In contrast, black-box attacks limit the access to model information for the attackers [3,15,13]. Some black-box defense models can restrict or disable external access upon detecting an adversarial attack attempt, significantly reducing the attack success rate.\n* These authors contributed equally to this work."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Gradient-based adversarial attacks", "content": "Gradient-based adversarial attacks are divided into white-box and black-box categories, with white-box attacks having access to the model's internals. The Fast Gradient Sign Method (FGSM) [8] is a typical white-box approach that enhances adversarial examples with a single gradient ascent step. Iterative FGSM (I-FGSM) [16] refines these examples through multiple iterations. Momentum Iterative FGSM (MI-FGSM)[5] adds momentum to gradient updates, avoiding local maxima and stabilizing the attack. TI-FGSM[6] uses transformation kernels on gradients, while DI-FGSM [31] employs random resizing and padding. Projected Gradient Descent (PGD) [19] and C&W[4] further refine attacks by constraining perturbation sizes and optimizing effectiveness."}, {"title": "2.2 Frequency-based adversarial attacks", "content": "Frequency domain analysis presents significant relevance in adversarial attacks. Wang et al. [29] found that DNNs have unique advantages in the frequency"}, {"title": "3 Approach", "content": ""}, {"title": "3.1 Preliminaries of adversarial attack", "content": "Formally, consider a deep learning network $N : \\mathbb{R}^n \\rightarrow \\mathbb{R}^c$, where $n$ is the input dimension and $c$ is the number of classes and original image sample $x \\in \\mathbb{R}$, if an imperceptible perturbation $\\sum_{k=0}^{t-1} \\Delta x^k$ is applied to the original sample $x^0$, this may mislead the network $N$ into classify the manipulated input $x^t = x^0 + \\sum_{k=0}^{t-1} \\Delta x^k$ as having the label $m$. This manipulated input can also be denoted as $x^{adv}$. Assuming the output of the sample $x$ is denoted by $N(x)$, the optimization goal is:\n$||x - x^0|| < \\epsilon \\text{ subject to } N(x) \\neq N(x^0)$"}, {"title": "3.2 Frequency transfromation", "content": "In this subsection, we introduce the DCT (Discrete Cosine Transform) and IDCT (Inverse Discrete Cosine Transform) transformations [7]. The DCT aims to transform an image from the spatial domain to the frequency domain, while the IDCT transforms the image back to the spatial domain.\n$D(x) [u,v] = \\frac{1}{\\sqrt{2N}}C(u)C(v) \\sum_{x=0}^{N-1} \\sum_{y=0}^{N-1}x[k, m]\\cos{\\left[\\frac{(2k+1)u\\pi}{2N}\\right]}\\cos{\\left[\\frac{(2m+1)v\\pi}{2N}\\right]}$"}, {"title": "3.3 Frequency and Spatial Consistency Based Adversarial Attack", "content": "Leveraging DCT and IDCT operations, we propose FSA, which comprises two distinct adversarial attack steps: the spatial attack and the frequency attack.\n$\\Delta grad (x_t) = \\alpha \\cdot sign\\left(\\frac{\\partial L(x_t; y)}{\\partial x_t}\\right)$"}, {"content": "Equation 3 defines the formula for spatial attack feature changes. $\\Delta grad (x_t)$ denotes the original adversarial attack changes, which comprise learning rate $\\alpha$ and the gradients $\\left(\\frac{\\partial L(x_t; y)}{\\partial x_t}\\right)$ under the t-th sample $x$. Other gradient-based adversarial attacks like PGD [19] can also obtain $\\Delta grad (x_t)$, and can use a projection function to constrain the feature changes. Specifically, the sign function in Equation 3 has the following two different properties:\nDecoupling the attack from model parameters. Consider a simple neural network $Y = W \\cdot X$, where Y is a one-dimensional value, as an example. It can be calculated that the gradients of X can be represented as $W^T$, which are the model's parameters. The greater value in parameters, the more significant the changes, which is not a desired outcome in the attack progress. This phenomenon can be mitigated by applying the sign function.\nBalance spatial attack and frequency attack. With the sign function and attack value ranges between $\\alpha$, -$\\alpha$, and 0, the consistency between spatial and frequency attacks can be inferred from the similarity of their attack value.\n$\\Delta frequency(x_t) = \\alpha \\cdot sign \\left[IDCT \\left(\\frac{\\partial L(IDCT(DCT(x_t)), y)}{\\partial DCT(x_t)}\\right)\\right]$"}, {"content": "Eq. 4 is the formula for frequency attack feature changes. It calculates the gradient in the frequency domain $DCT(x_t)$ and uses the IDCT transformation to convert the attack changes back to the spatial domain. Subsequently, the sign function is used to align the attack with the spatial domain.\n$m_t = (\\Delta grad (x_t) == \\Delta frequency(x_t))$\n$\\Chi_t = x_t + m_t \\cdot \\Delta grad (x_t)$\n$x_{t+1} = \\Chi_t + m_t \\cdot \\Delta grad (x_t)$"}, {"content": "Finally, we obtain the consistency mask $m_t$ by aligning the values from two domain attacks and apply it in FSA attack step. It is important to note that $m_t$ is a binary mask, taking values 0 or 1. Eq. 5 is used to compute this mask. Here,"}, {"title": "4 Experiments", "content": "In the experiment, we have conducted a comparative analysis of nine models, including DenseNet_121 [12], Inception_v3 [25], VGG16 [23], MobileNet_v2 [21], GoogLeNet [24], EfficientNet_B0 [27], VGG19 [23], MobileNet_v3_large [11], and ResNet_50 [10], utilizing five distinct adversarial methods: MI-FGSM [5], DI-FGSM [31], TI-FGSM [6], I-FGSM [16], and PGD [19]. The objective of these comparison experiments is to investigate how FSA method could enhance the attack effectiveness of the aforementioned methods. By integrating the FSA method with the aforementioned adversarial techniques, we provide comprehensive results to demonstrate the improvements in their adversarial capabilities."}, {"title": "4.1 Dataset", "content": "The dataset consists of 1000 images, selected in accordance with the settings used in these methods [33,37,36,34,39,41,40]. These images were randomly chosen from diverse categories within the ILSVRC 2012 validation set [20], which is widely recognized and extensively utilized for adversarial attacks."}, {"title": "4.2 Evaluation Metrics", "content": "This experiment aimed to evaluate various attack methods using the attack success rate as the metric. A higher success rate indicates a more effective adversarial attack. We calculated the difference in success rates before and after applying the FSA method to demonstrate its effectiveness in enhancing attack methods."}, {"title": "4.3 Parameter Setting", "content": "The parameters considered in this experiment are the maximum perturbation (Epsilon) and the number of iterations (Steps). The maximum perturbation value was set to 1.0, respectively, while the number of iterations was set to 5."}, {"title": "4.4 Experimental Results", "content": "The analysis depicted in Figure 3 clearly shows variations in the success rate improvements of different attack methods under varying Epsilon and Steps counts, across different models. Nevertheless, the utilization of the FSA method yielded significant enhancements in attack success rates for a considerable number of attack methods across these models. Table 1 shows the attack success rate of our algorithm in various environments. Specifically, employing the FSA method results in a maximum increase in attack success rates of 28.98% across the range of attack methods, with an average increase of 5.23%. Additionally, while a slight decrease in attack success rates was observed for PGD method, the magnitude of the decrease was minimal, only 0.23% and 0.12%. These findings provide substantial evidence supporting the effectiveness of the FSA approach. Moreover, as illustrated in Figure 3 and Table 1, it is evident that different attack methods experience a substantial enhancement in success rates as the attack perturbation levels increase. However, the effectiveness of the FSA method in improving attack success rates diminishes across various attack methods. This observation suggests that the FSA method exhibits more pronounced improvements when the initial attack success rates are relatively low. Conversely, when the original success rates are already high, the impact of the FSA method in enhancing the success rates becomes limited."}, {"title": "4.5 Ablation Experiments", "content": ""}, {"title": "Effect of different the Epsilon", "content": "We conduct a comparative analysis of the effect of different Epsilon values on the performance of FSA method. Specifically, we investigate the impact of Epsilon values from 1.0 to 2.0 on the FSA method under the Steps settings of 5, 10, and 16, respectively. Figure 4 and Table 2 illustrate the results of a decreasing trend in the attack success rate of the FSA method as the Epsilon value increases. Notably, the TI-FGSM method demonstrates the largest decline in success rate improvement, with a decrease of 10.19% and an average decrease of 3.17% in attack success rate improvement. Moreover, the effectiveness of FSA declines with higher Epsilon values, suggesting that FSA performs better when the Epsilon is set to 1.0."}, {"title": "Effect of different Steps", "content": "We compare the effects of different step parameters on the performance of FSA. We first examine the influence of different step values, namely 5, 10, and 16, on the gain achieved by FSA, with EPS values set to 1.0 and 2.0, respectively. As shown in Figure 5 and Table 3, it can be observed that with an increase in steps, there is no significant fluctuation in the effectiveness of FSA method across various attack methods, regardless of"}, {"title": "5 Conclusion", "content": "In this paper, we introduce the Frequency and Spatial Consistency-Based Adversarial Attack (FSA) method to enhance the success rate of most white-box algorithms by leveraging consistency in both frequency and spatial domains. Noticing the limitations of using singular domain information for attacks, we extend the frequency transformation using DCT and IDCT operations during training, significantly boosting FSA's performance. Experiments with MI-FGSM, DI-FGSM, TI-FGSM, and I-FGSM demonstrate that our method improves attack success rates and achieves state-of-the-art results."}]}