{"title": "Leveraging Information Consistency in Frequency and Spatial Domain for Adversarial Attacks", "authors": ["Zhibo Jin", "Jiayu Zhang", "Zhiyu Zhu", "Xinyi Wang", "Yiyun Huang", "Huaming Chen"], "abstract": "Adversarial examples are a key method to exploit deep neu-\nral networks. Using gradient information, such examples can be gener-\nated in an efficient way without altering the victim model. Recent fre-\nquency domain transformation has further enhanced the transferability\nof such adversarial examples, such as spectrum simulation attack. In this\nwork, we investigate the effectiveness of frequency domain-based attacks,\naligning with similar findings in the spatial domain. Furthermore, such\nconsistency between the frequency and spatial domains provides insights\ninto how gradient-based adversarial attacks induce perturbations across\ndifferent domains, which is yet to be explored. Hence, we propose a sim-\nple, effective, and scalable gradient-based adversarial attack algorithm\nleveraging the information consistency in both frequency and spatial do-\nmains. We evaluate the algorithm for its effectiveness against different\nmodels. Extensive experiments demonstrate that our algorithm achieves\nstate-of-the-art results compared to other gradient-based algorithms. Our\ncode is available at: https://github.com/LMBTough/FSA.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) are susceptible to subtle perturbations, which\ncan lead to erroneous predictions [26]. The attacks with adversarial examples\nare generally classified into white-box and black-box attacks based on the level\nof information accessible to the attacker. In white-box attacks [8], the attacker\nhas access to model information, such as model parameters, network structure,\ntraining dataset, and defense mechanisms. This allows for the deliberate con-\nstruction of adversarial examples. In contrast, black-box attacks limit the access\nto model information for the attackers [3,15,13]. Some black-box defense mod-\nels can restrict or disable external access upon detecting an adversarial attack\nattempt, significantly reducing the attack success rate."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Gradient-based adversarial attacks", "content": "Gradient-based adversarial attacks are divided into white-box and black-box\ncategories, with white-box attacks having access to the model's internals. The\nFast Gradient Sign Method (FGSM) [8] is a typical white-box approach that en-\nhances adversarial examples with a single gradient ascent step. Iterative FGSM\n(I-FGSM) [16] refines these examples through multiple iterations. Momentum\nIterative FGSM (MI-FGSM)[5] adds momentum to gradient updates, avoiding\nlocal maxima and stabilizing the attack. TI-FGSM[6] uses transformation ker-\nnels on gradients, while DI-FGSM [31] employs random resizing and padding.\nProjected Gradient Descent (PGD) [19] and C&W[4] further refine attacks by\nconstraining perturbation sizes and optimizing effectiveness."}, {"title": "2.2 Frequency-based adversarial attacks", "content": "Frequency domain analysis presents significant relevance in adversarial attacks.\nWang et al. [29] found that DNNs have unique advantages in the frequency"}, {"title": "3 Approach", "content": ""}, {"title": "3.1 Preliminaries of adversarial attack", "content": "Formally, consider a deep learning network $N : R^n \\rightarrow R^c$, where n is the input\ndimension and c is the number of classes and original image sample $x^0 \\in R^n$,\nif an imperceptible perturbation $\\sum_{k=0}^{t-1} \\Delta x^k$ is applied to the original sample\n$x^0$, this may mislead the network N into classify the manipulated input $x^t =$\n$x^0 + \\sum_{k=0}^{t-1} \\Delta x^k$ as having the label m. This manipulated input can also be\ndenoted as $x^{adv}$. Assuming the output of the sample x is denoted by N(x), the\noptimization goal is:\n$||x^t - x^0|| < \\epsilon$ subject to $N(x^t) \\neq N(x^0)$ \t\t(1)\nHere, $||.||$ represents the n-norm distance."}, {"title": "3.2 Frequency transfromation", "content": "In this subsection, we introduce the DCT (Discrete Cosine Transform) and IDCT\n(Inverse Discrete Cosine Transform) transformations [7]. The DCT aims to trans-\nform an image from the spatial domain to the frequency domain, while the IDCT\ntransforms the image back to the spatial domain.\n$D(x) [u,v] = \\frac{1}{\\sqrt{2N}}C(u)C(v) \\sum_{x=0}^{N-1} \\sum_{y=0}^{N-1}x[k, m]$\n$\\cos [\\frac{(2k+1)u\\pi}{2N}]\\cos [\\frac{(2m+1)v\\pi}{2N}]$ \t(2)"}, {"title": "3.3 Frequency and Spatial Consistency Based Adversarial Attack", "content": "Leveraging DCT and IDCT operations, we propose FSA, which comprises two\ndistinct adversarial attack steps: the spatial attack and the frequency attack.\n$\\Delta grad (x_t) = \\alpha . sign (\\frac{\\partial L(x_t; y)}{\\partial x_t})$ \t\t\t\t (3)\nEquation 3 defines the formula for spatial attack feature changes. $\\Delta grad (x_t)$\ndenotes the original adversarial attack changes, which comprise learning rate $\\alpha$\nand the gradients $(\\frac{\\partial L(x_t; y)}{\\partial x_t})$ under the t-th sample $x_t$. Other gradient-based adver-\nsarial attacks like PGD [19] can also obtain $\\Delta grad (x_t)$, and can use a projection\nfunction to constrain the feature changes. Specifically, the sign function in Equa-\ntion 3 has the following two different properties:\nDecoupling the attack from model parameters. Consider a simple neural network\n$Y = W. X$, where Y is a one-dimensional value, as an example. It can be\ncalculated that the gradients of X can be represented as $W^T$, which are the\nmodel's parameters. The greater value in parameters, the more significant the\nchanges, which is not a desired outcome in the attack progress. This phenomenon\ncan be mitigated by applying the sign function.\nBalance spatial attack and frequency attack. With the sign function and attack\nvalue ranges between $\\alpha$, -$\\alpha$, and 0, the consistency between spatial and frequency\nattacks can be inferred from the similarity of their attack value.\n$\\Delta frequency(x_t) = \\alpha . sign IDCT [\\frac{\\partial L(IDCT(DCT(x_t)), y)}{\\partial DCT(x_t)}]$ \t\t(4)\nEq. 4 is the formula for frequency attack feature changes. It calculates the gra-\ndient in the frequency domain $DCT(x_t)$ and uses the IDCT transformation to\nconvert the attack changes back to the spatial domain. Subsequently, the sign\nfunction is used to align the attack with the spatial domain.\n$m_t = ((\\Delta grad (x_t) == \\Delta frequency(x_t)))$ \t\t\t\t (5)\n$\\tilde{x_t} = x_t + m_t. \\Delta grad (x_t)$ \t\t\t\t (6)\n$x_t^{'} = \\tilde{x_t} + m_t. \\Delta grad (x_t)$ \t\t\t\t (7)\nFinally, we obtain the consistency mask $m_t$ by aligning the values from two\ndomain attacks and apply it in FSA attack step. It is important to note that $m_t$\nis a binary mask, taking values 0 or 1. Eq. 5 is used to compute this mask. Here,"}, {"title": "4 Experiments", "content": "In the experiment, we have conducted a comparative analysis of nine models, in-\ncluding DenseNet_121 [12], Inception_v3 [25], VGG16 [23], MobileNet_v2 [21],\nGoogLeNet [24], EfficientNet_B0 [27], VGG19 [23], MobileNet_v3_large [11],\nand ResNet_50 [10], utilizing five distinct adversarial methods: MI-FGSM [5],\nDI-FGSM [31], TI-FGSM [6], I-FGSM [16], and PGD [19]. The objective of\nthese comparison experiments is to investigate how FSA method could enhance\nthe attack effectiveness of the aforementioned methods. By integrating the FSA\nmethod with the aforementioned adversarial techniques, we provide comprehen-\nsive results to demonstrate the improvements in their adversarial capabilities."}, {"title": "4.1 Dataset", "content": "The dataset consists of 1000 images, selected in accordance with the settings used\nin these methods [33,37,36,34,39,41,40]. These images were randomly chosen\nfrom diverse categories within the ILSVRC 2012 validation set [20], which is\nwidely recognized and extensively utilized for adversarial attacks."}, {"title": "4.2 Evaluation Metrics", "content": "This experiment aimed to evaluate various attack methods using the attack\nsuccess rate as the metric. A higher success rate indicates a more effective ad-\nversarial attack. We calculated the difference in success rates before and after\napplying the FSA method to demonstrate its effectiveness in enhancing attack\nmethods."}, {"title": "4.3 Parameter Setting", "content": "The parameters considered in this experiment are the maximum perturbation\n(Epsilon) and the number of iterations (Steps). The maximum perturbation value\nwas set to 1.0, respectively, while the number of iterations was set to 5."}, {"title": "4.4 Experimental Results", "content": "The analysis depicted in Figure 3 clearly shows variations in the success rate im-\nprovements of different attack methods under varying Epsilon and Steps counts,\nacross different models. Nevertheless, the utilization of the FSA method yielded\nsignificant enhancements in attack success rates for a considerable number of\nattack methods across these models. Table 1 shows the attack success rate of\nour algorithm in various environments. Specifically, employing the FSA method\nresults in a maximum increase in attack success rates of 28.98% across the range\nof attack methods, with an average increase of 5.23%. Additionally, while a slight\ndecrease in attack success rates was observed for PGD method, the magnitude\nof the decrease was minimal, only 0.23% and 0.12%. These findings provide sub-\nstantial evidence supporting the effectiveness of the FSA approach. Moreover,\nas illustrated in Figure 3 and Table 1, it is evident that different attack methods\nexperience a substantial enhancement in success rates as the attack perturba-\ntion levels increase. However, the effectiveness of the FSA method in improving\nattack success rates diminishes across various attack methods. This observation\nsuggests that the FSA method exhibits more pronounced improvements when\nthe initial attack success rates are relatively low. Conversely, when the original\nsuccess rates are already high, the impact of the FSA method in enhancing the\nsuccess rates becomes limited."}, {"title": "4.5 Ablation Experiments", "content": "Effect of different the Epsilon We conduct a comparative analysis of the ef-\nfect of different Epsilon values on the performance of FSA method. Specifically,\nwe investigate the impact of Epsilon values from 1.0 to 2.0 on the FSA method\nunder the Steps settings of 5, 10, and 16, respectively. Figure 4 and Table 2\nillustrate the results of a decreasing trend in the attack success rate of the FSA\nmethod as the Epsilon value increases. Notably, the TI-FGSM method demon-\nstrates the largest decline in success rate improvement, with a decrease of 10.19%\nand an average decrease of 3.17% in attack success rate improvement. Moreover,\nthe effectiveness of FSA declines with higher Epsilon values, suggesting that FSA\nperforms better when the Epsilon is set to 1.0.\nEffect of different Steps We compare the effects of different step parameters\non the performance of FSA. We first examine the influence of different step\nvalues, namely 5, 10, and 16, on the gain achieved by FSA, with EPS values\nset to 1.0 and 2.0, respectively. As shown in Figure 5 and Table 3, it can be\nobserved that with an increase in steps, there is no significant fluctuation in\nthe effectiveness of FSA method across various attack methods, regardless of"}, {"title": "5 Conclusion", "content": "In this paper, we introduce the Frequency and Spatial Consistency-Based Adver-\nsarial Attack (FSA) method to enhance the success rate of most white-box algo-\nrithms by leveraging consistency in both frequency and spatial domains. Noticing\nthe limitations of using singular domain information for attacks, we extend the\nfrequency transformation using DCT and IDCT operations during training, sig-\nnificantly boosting FSA's performance. Experiments with MI-FGSM, DI-FGSM,\nTI-FGSM, and I-FGSM demonstrate that our method improves attack success\nrates and achieves state-of-the-art results."}]}