{"title": "QuaCK-TSF: Quantum-Classical Kernelized Time Series Forecasting", "authors": ["Abdallah Aaraba", "Soumaya Cherkaoui", "Ola Ahmad", "Jean-Fr\u00e9d\u00e9ric Laprade", "Olivier Nahman-L\u00e9vesque", "Alexis Vieloszynski", "Shengrui Wang"], "abstract": "Forecasting in probabilistic time series is a complex endeavor that extends beyond predicting future values to also quantifying the uncertainty inherent in these predictions. Gaussian process regression stands out as a Bayesian machine learning technique adept at addressing this multifaceted challenge. This paper introduces a novel approach that blends the robustness of this Bayesian technique with the nuanced insights provided by the kernel perspective on quantum models, aimed at advancing quantum kernelized probabilistic forecasting. We incorporate a quantum feature map inspired by Ising interactions and demonstrate its effectiveness in capturing the temporal dependencies critical for precise forecasting. The optimization of our model's hyperparameters circumvents the need for computationally intensive gradient descent by employing gradient-free Bayesian optimization. Comparative benchmarks against established classical kernel models are provided, affirming that our quantum-enhanced approach achieves competitive performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum computing is poised to revolutionize both scientific research and industrial applications [1]\u2013[4], driven by rapid hardware advancements [5]\u2013[7] and significant algorithmic developments aimed at harnessing quantum advantage beyond basic demonstrations [8]\u2013[11]. In particular, quantum machine learning (QML), which integrates quantum computing with machine learning, is anticipated to be among the early beneficiaries of these advancements [12]. Nevertheless, the size and coherence of the quantum circuits needed to attain a regime where quantum computers can tackle real world problems exceeds the capabilities of the present noisy intermediate-scale quantum (NISQ) technology [13].\nRecently, quantum kernel methods have emerged as a promising approach to leveraging the capabilities of current NISQ devices when viewing QML models [14]. The value of this perspective is rooted in the compatibility of quantum models with the extensive range of classical kernel theory tools, which can significantly streamline the optimization process for quantum models [15], [16]. These methods involve mapping data into the Hilbert space associated to a system of qubits using a quantum feature map that facilitates the calculation of a kernel matrix through the pairwise inner product of data points. This matrix can subsequently be utilized in conventional methods such as support vector machines or kernel ridge regression [17], [18]. The underlying premise is that quantum Hilbert space data encoding can enhance the feature map with quantum-exclusive resources, offering benefits over classical maps. Such advantages have been demonstrated for specific datasets [12], [19].\nTime series forecasting (TSF) represents a domain ripe for the transformative impacts of quantum computing. This field encompasses critical activities such as financial forecasting [20], where the potential speed-up offered by quantum computing could lead to markedly faster predictions, conferring a significant competitive edge [21]. The implications of such advancements are profound, particularly in the high-stakes realm of finance where rapid and accurate predictions are paramount. Additionally, the role of forecasting is becoming increasingly pivotal in security contexts. This importance is underscored by the recent development of methods that connect QML advancements with time series anomaly detection challenges [22], [23], showcasing the potential of QML in addressing complex forecasting tasks.\nQuantifying uncertainty in time series forecasts is a critical component of the prediction process [24]. Consequently, probabilistic TSF techniques have garnered considerable attention, especially in areas where managing uncertainty is vital, such as anomaly detection. Gaussian process regression (GPR) stands out as a simple yet effective probabilistic TSF method [25]. Based on Bayesian inference, this approach adeptly captures uncertainty by presenting the forecast of the next value in a time series as a posterior probability distribution. This distribution not only provides an expected value for the next point in the series (i.e., the mean) but also quantifies the"}, {"title": "II. BACKGROUND", "content": "Consider a set of observations $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{1\\leq i \\leq c}$, with each $\\mathbf{x}_i \\in \\mathbb{R}^w$ representing a $w$-dimensional input vector of covariates and $y_i \\in \\mathbb{R}$ signifying the scalar output or target. We organize these observations into a $w \\times c$ design matrix $\\mathbf{X}$, and stack the corresponding target values into the vector $\\mathbf{y}$, thereby representing the dataset as $\\mathcal{D} = (\\mathbf{X}, \\mathbf{y})$. In regression analysis, the goal is to model the input-target relationship through the function $y = f(\\mathbf{x}) + \\epsilon$, where $f: \\mathbb{R}^m \\rightarrow \\mathbb{R}$ is designed to approximate the unknown relationship, and $\\epsilon$ is a normally distributed error term with zero mean and variance $\\sigma^2$, denoted as $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\nParametric models, such as linear regression, posit a specific functional form for $f$, typically involving a set of parameters $\\mu$, e.g., $f(\\mathbf{x}) = \\mathbf{x}^T \\mu$, $\\mu \\in \\mathbb{R}^m$, for linear models. Conversely, GPR adopts a non-parametric approach, wherein $f$ is not constrained by a predefined structure but is instead described by a distribution over possible functions. This probabilistic treatment of the model function $f$ allows GPR to flexibly capture complex patterns in the data without committing to a fixed form, a significant advantage when our prior knowledge is limited or when the true underlying relationships are intricate and not well-understood.\nTo manage the intractable set of all possible functions from $\\mathbb{R}^m \\rightarrow \\mathbb{R}$, we impose a structure on the prior distribution of $f$, encapsulating it within the framework of a Gaussian process (GP). This GP, represented by the collection of random variables $(f(\\mathbf{x}_1), f(\\mathbf{x}_2), ..., f(\\mathbf{x}_c))$, is fully characterized by its mean function $m(\\mathbf{x})$ and covariance function $\\kappa(\\mathbf{x}, \\mathbf{x}')$. The mean function captures the expected value of the process, while the covariance function quantifies the extent to which the outputs at different inputs co-vary. Formally, we express these functions as $m(\\mathbf{x}) = \\mathbb{E}[f(\\mathbf{x})]$ and $\\kappa(\\mathbf{x},\\mathbf{x}') = \\text{Cov}(f(\\mathbf{x}), f(\\mathbf{x}'))$, denoting the process as $f(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), \\kappa(\\mathbf{x}, \\mathbf{x}'))$.\nThe selection of the kernel function $\\kappa$ profoundly influences the characteristics of the function class deemed likely to model the data. For instance, the radial basis function (RBF) kernel, expressed in (15), predisposes the model to smooth functions, while a periodic kernel, detailed in (18), biases it towards periodic functions. This critical choice of kernel is thus instrumental in steering the regression outcomes and warrants careful consideration to align with the nature of the data and the problem at hand."}, {"title": "B. Quantum machine learning", "content": "QML is a burgeoning paradigm in artificial intelligence, leveraging the unique capabilities of quantum computers to handle computations within exponentially large vector spaces through quantum algorithms [29]\u2013[31]. In typical QML applications, classical datasets residing in some space $\\mathcal{X}$ are used for tasks like classification or data generation. These datasets are transferred to the quantum domain using a feature map $\\phi: \\mathcal{X} \\rightarrow \\mathcal{H}$, where $\\mathcal{H}$ denotes the Hilbert space of an $n$-qubit quantum computer, with dimensions $2^n$. For each data point $\\mathbf{x} \\in \\mathcal{X}$, embedding unitaries $U(\\mathbf{x})$ are applied such that $U(\\mathbf{x}) |0\\rangle^{\\otimes n} = |\\phi(\\mathbf{x})\\rangle$ produces a state dependent on the data. The model typically evolves by training a measurement operator $M_{\\theta}$ to minimize a loss function $\\mathcal{L}$ over the dataset, with the model output defined as $h_{\\theta}(\\mathbf{x}) = \\langle \\phi(\\mathbf{x})| M_{\\theta} |\\phi(\\mathbf{x}) \\rangle$ [30]."}, {"title": "III. METHODOLOGY", "content": "Consider a time series $s$, observed over the time interval $\\mathcal{T} = \\{t_l : l \\in [1..T]\\}$, where each $t_l$ is a positive real number and $T$ is a positive integer. This series comprises a sequence of real-valued observations $x_{t_l}$, systematically recorded in chronological order as expressed by:\n$s = (x_t : t \\in \\mathcal{T}), x_t \\in \\mathbb{R}$.\nProblem. Our focus is on the task of probabilistic forecasting, aiming to predict the forthcoming value $x_{t_{l+1}}$ in the series $s$, based on a historical window of observations of length $w: (x_t : t_{l-w+1} \\leq t \\leq t_l)$. The challenge lies in accurately determining the conditional probability distribution of the next observation as follows:\n$\\mathbb{P}(x_{t_{l+1}} | (x_t : t_{l-w+1} \\leq t \\leq t_l))$,\nensuring the continuity of the sequence where the lookback period starts after $t_l$ and the prediction point $t_{l+1}$ falls within the observation period ending at $t_l$.\nWe conceptualize our forecasting challenge within the regression framework as follows:\n$y = f(\\mathbf{x}) + \\epsilon$,\nwhere $y$, the target value, is defined as $y = x_{t_{l+1}}$ and represents the next value in the time series. The input vector $\\mathbf{x} \\in \\mathbb{R}^w$ comprises a sequence of $w$ preceding observations $(x_t: t_{l-w+1} \\leq t \\leq t_l)^T$, and $\\epsilon$ denotes an independent and identically distributed normal additive noise, specifically $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. Drawing from the GPR framework [25], we postulate that any set of input vectors $\\mathcal{X} = \\{\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_c\\}$, extracted from the time series $s$ and numbering $c$ in total, induces a stochastic process $(f(\\mathbf{x}_1), f(\\mathbf{x}_2), ..., f(\\mathbf{x}_c))$ that conforms to a Gaussian distribution. Here, each $f(\\mathbf{x}_i)$ is a random variable, with randomness originating from the selection of the function $f$ from a space of possible functions $\\mathbb{R}^w \\rightarrow \\mathbb{R}$.\nThis Gaussian process is delineated by its prior distribution, defined by a mean function $m(\\mathbf{x})$\u2014typically assumed constant (i.e., $m(\\mathbf{x}) = m$ with $m$ as a hyperparameter, usually set to zero)\u2014and a covariance kernel function $\\kappa_{\\alpha}(\\mathbf{x}_j, \\mathbf{x}_{j'})$ reliant on some hyperparameters $\\alpha \\in \\mathbb{R}^{d_{\\alpha}}$, where $d_{\\alpha} \\in \\mathbb{Z}^+$ is the number of such hyperparameters. Thus, the prior distribution for a given set of input windows $\\mathcal{X}$ is expressed as\n$f(\\mathcal{X}) \\sim \\mathcal{GP}(m, \\kappa_{\\alpha}(\\mathbf{x}_j, \\mathbf{x}_{j'}))$.\nThroughout this discussion, we employ $f(\\mathbf{x})$ and $f$ interchangeably to refer to the model's output, thereby treating it as a random variable influenced by the underlying probabilistic nature of the GPR model."}, {"title": "C. Predictive distribution", "content": "In the context of a new observation window $\\mathbf{x}$, and considering a set of training windows $\\mathcal{X} = \\{\\mathbf{x}_1,\\mathbf{x}_2, ..., \\mathbf{x}_c\\}$ with their corresponding targets $\\mathcal{Y} = \\{y_1, y_2, ..., y_c\\}$, the posterior distribution of the forecasted values $f_*$, conditioned on both the training windows and the new input sequence $\\mathbf{x}$, is captured by the predictive distribution\n$f_* | \\mathbf{X},\\mathbf{y},\\mathbf{x} \\sim \\mathcal{N} (\\bar{f}, \\sigma_*^2)$,\nwhere $\\mathbf{X}$ denotes the matrix compiled from the vectors $\\mathbf{x}_j$, arranged as columns such that $\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_c)$ and occupies a space in $\\mathbb{R}^{w \\times c}$. The vector $\\mathbf{y}$ consolidates the target values into $(y_1, y_2, ..., y_c)$ within $\\mathbb{R}^c$. Here, $\\bar{f}$ represents the mean prediction, and $\\sigma_*^2$ denotes the forecast's variance. Such distribution parameters are derived in [25] as:\n$\\bar{f} = \\mathbf{k}_*^T (\\mathbf{K} + \\sigma^2\\mathbf{I})^{-1} \\mathbf{y} + m, $\n$\\sigma_*^2 = \\kappa_{\\alpha}(\\mathbf{x},\\mathbf{x}) - \\mathbf{k}_*^T (\\mathbf{K} + \\sigma^2\\mathbf{I})^{-1} \\mathbf{k}_*$,\nwhere $\\mathbf{k}_*$ is a vector in $\\mathbb{R}^c$, formed by the covariance values between the new observation window $\\mathbf{x}$ and each of the training windows $\\mathbf{x}_j$, specifically $\\mathbf{k}_* = (\\kappa_{\\alpha}(\\mathbf{x},\\mathbf{x}_j) : j \\in [1..c])^T$. The covariance matrix $\\mathbf{K}$, a square matrix of dimensions $c \\times c$, is described as $\\mathbf{K} = \\mathbf{K}(\\mathbf{X},\\mathbf{X}) = (\\kappa_{\\alpha}(\\mathbf{x}_i, \\mathbf{x}_j))_{i,j=1}^c$, encapsulating the pairwise covariances between all training window combinations. This formulation elegantly encapsulates the uncertainty and expectations of model predictions, providing a comprehensive probabilistic outlook on forecasting."}, {"title": "D. Kernel function", "content": "Our method employs a strategy of constructing the kernel function in a bottom-up manner using sub-windows. This approach enables the generation of embeddings that capture the intricate temporal patterns within each subsequence\u2014a nuance that methods focusing on individual time point embeddings might miss [32]."}, {"title": "IV. EXPERIMENTS", "content": "To test our quantum kernel-based GPR model, we have opted for a synthetic time series to build intuition. Our goal is to create a synthetic TSF problem where (i) data is not linearly forecasted (thus kernelized approaches are required), (ii) the dynamics of the time series are not static and evolve with time in order to mimic real world time series. To this end, we have generated a synthetic time series, illustrated in Fig. 3, that combines linear trend components, non-linear dynamics through sine waves, and Gaussian noise to create complex patterns for testing and analysis. Specifically, for a given number of time steps (240 our case), it first divides the series into segments according to the specified number of trend changes (4), alternating between upward and downward linear trends with slight slopes. In parallel, it overlays two sine waves: one with a longer period (10 time steps) and larger amplitude (scale of 1) to introduce long-term cyclic patterns, and another with a period determined by the frequency of trend changes and a smaller amplitude (scale of 0.5) to capture quicker, more nuanced oscillations. The resultant series is further perturbed by adding Gaussian noise with a specified level (0.5), to mimic real-world unpredictability. This process results in a synthetic dataset that exhibits realistic characteristics such as trends, seasonality, and noise, making it useful for testing TSF models.\nAdhering to the preprocessing steps outlined in [39], we commence our experimental procedure by standardizing the generated synthetic time series. This normalization process adjusts the series values to be centered around zero with a standard deviation of one. Subsequently, the series is segmented into training and testing sets, depicted by the blue and orange curves, respectively, in Fig. 3. The training dataset is denoted as $\\mathcal{D} = (\\mathbf{X}, \\mathbf{Y})$, where $\\mathbf{X} = \\{\\mathbf{x}_1,..., \\mathbf{x}_c\\}$ encompasses the input windows, and $\\mathbf{Y} = \\{y_1, ..., y_c\\}$ contains the corresponding target values. In our experiments, we selected a window length of 5, necessitating the use of 5 qubits per window, aiming for a balanced qubit count that strikes a compromise between being overly extensive and unduly limited. To minimize redundancy in the training data, consecutive training windows $\\mathbf{x}_i$ and $\\mathbf{x}_{i+1}$ are allowed an overlap of merely 2 time steps. The test dataset, $\\mathcal{D}' = (\\mathbf{X}', \\mathbf{Y}')$, is structured to commence prediction at the immediate subsequent time point following the last prediction in the training phase. The prediction process then advances by a single time step, sequentially addressing the remaining time points in the series until its conclusion."}, {"title": "A. Results of the quantum kernel", "content": "In our exploration, the quantum-based GPR model, trained on a classical simulator, was fine-tuned to fit the synthetically generated time series data employing the BO technique as detailed previously. The hyperparameters optimization was structured into two key steps: an initialization phase and a query point generation phase, each with 25 sample points, i.e., $n_0 = N = 25$, to enhance the model's predictive accuracy. The hyperparameter selection was meticulously designed to maximize the marginal log likelihood, ensuring the model's optimal performance during test set predictions."}, {"title": "V. CONCLUSION", "content": "In this work, we introduced a hybrid quantum-classical kernelized approach for probabilistic time series forecasting that goes beyond mere value predictions to effectively quantify uncertainties. Leveraging the sophisticated kernel perspective of quantum machine learning models and the robustness of Gaussian process regression, our approach establishes a nexus between quantum models and kernelized probabilistic forecasting. This methodology, incorporating a quantum feature map inspired by Ising interactions, adeptly captures various complex temporal dependencies essential for accurate forecasting. Additionally, we optimize the model's hyperparameters using gradient-free Bayesian optimization, thus sidestepping the computational intensity of gradient descent. Comparative benchmarks against classical kernel models have not only validated our innovative approach but also confirmed its competitive performance within the field.\nLooking ahead, this advancement in quantum-enhanced probabilistic forecasting sets the stage for significant future developments. It is crucial to focus on increasing the efficiency of the model and extending its application across diverse datasets to fully exploit its potential. Furthermore, the exploration of quantum feature maps specifically designed to capture complex time series patterns is of great interest for better understanding of the possible enhancements. The focus should be put towards maps that offer a potential quantum advantage that could significantly propel the capabilities of quantum-enhanced time series forecasting."}, {"title": "APPENDIX A BAYESIAN OPTIMIZATION", "content": "In the realm of complex machine learning challenges, parameter tuning emerges as a critical task often reliant on time-intensive evaluations. These evaluations spring from the execution of a black-box objective function $g$, with parameters $\\theta$ drawn from a designated parameter space $\\Theta$. Due to the substantial resources required for each evaluation, $g$ is characterized as an expensive-to-evaluate function. The objective, therefore, narrows down to identifying an optimal or near-optimal parameter configuration $\\theta^* \\in \\Theta$ that optimizes $g$ through a minimal number of evaluations:\n$\\theta^* = \\arg \\max_{\\Theta \\in \\Theta} g(\\theta)$.\nIn such contexts, resorting to gradient-based optimization proves impractical as it typically necessitates a prohibitive number of evaluations to approximate the optimum of $g$ effectively.\nBayesian optimization (BO) presents a strategic framework aimed at minimizing the evaluation count of $g$, all while incorporating historical evaluation data [44], [45]. This method elegantly balances exploration of highly uncertain regions against exploitation in areas showing promising performance. Central to BO are two components: a surrogate model, which provides a probabilistic representation of the objective function, and an acquisition function that strategizes the subsequent sampling point. Initially, BO embarks on a space exploration using a predefined strategy, typically involving $n_0$ points, either through random selection or employing space-filling designs like Sobol sequences [46]. This phase yields an initial dataset $\\mathcal{D}_1 = \\{(\\theta_i, z_i)\\}_{1 \\leq i \\leq n_0}$, capturing the outcomes $z_i = g(\\theta_i)$ at each $\\theta_i$.\nIn the second step, we iteratively refine our understanding of $g$, starting with the initial dataset $\\mathcal{D}_1$ to inform a prior over $g$. This process iteratively selects $N$ query points to evaluate, each chosen to maximize the acquisition function, thereby leveraging the posterior distribution of $g$. A Gaussian process is frequently the model of choice for the surrogate, updating the posterior distribution with each new observation and thus enhancing our predictive capability over $g$'s outcomes at untested points. The acquisition function, often chosen to be the expected improvement [44], is defined as:\n$EI_{z^*}(\\theta) = \\mathbb{E}_{g(\\theta)} [\\max (0, g(\\theta) - z^*)]$,\nwhere $z^* = \\max_i z_i$ denotes the best observed outcome thus far, or the incumbent. This function quantifies the utility of sampling $g$ at new candidate points. An illustration of the BO cycle in Fig. 9 showcases how the expected improvement achieves a compromise between areas of high uncertainty and those with favorable observed results."}, {"title": "APPENDIX B FURTHER EXPERIMENTAL DETAILS", "content": "To perform the quantum-classical comparison, we employed a comprehensive suite of 8 metrics calculated on the test set $\\mathcal{D}'$. For each input window in the test set $\\mathbf{x}'$, denoted"}]}