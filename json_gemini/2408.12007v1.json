{"title": "QuaCK-TSF: Quantum-Classical Kernelized Time Series Forecasting", "authors": ["Abdallah Aaraba", "Soumaya Cherkaoui", "Ola Ahmad", "Jean-Fr\u00e9d\u00e9ric Laprade", "Olivier Nahman-L\u00e9vesque", "Alexis Vieloszynski", "Shengrui Wang"], "abstract": "Forecasting in probabilistic time series is a complex endeavor that extends beyond predicting future values to also quantifying the uncertainty inherent in these predictions. Gaussian process regression stands out as a Bayesian machine learning technique adept at addressing this multifaceted challenge. This paper introduces a novel approach that blends the robustness of this Bayesian technique with the nuanced insights provided by the kernel perspective on quantum models, aimed at advancing quantum kernelized probabilistic forecasting. We incorporate a quantum feature map inspired by Ising interactions and demonstrate its effectiveness in capturing the temporal dependencies critical for precise forecasting. The optimization of our model's hyperparameters circumvents the need for computationally intensive gradient descent by employing gradient-free Bayesian optimization. Comparative benchmarks against established classical kernel models are provided, affirming that our quantum-enhanced approach achieves competitive performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Quantum computing is poised to revolutionize both scientific research and industrial applications [1]-[4], driven by rapid hardware advancements [5]\u2013[7] and significant algorithmic developments aimed at harnessing quantum advantage beyond basic demonstrations [8]\u2013[11]. In particular, quantum machine learning (QML), which integrates quantum computing with machine learning, is anticipated to be among the early beneficiaries of these advancements [12]. Nevertheless, the size and coherence of the quantum circuits needed to attain a regime where quantum computers can tackle real world problems exceeds the capabilities of the present noisy intermediate-scale quantum (NISQ) technology [13].\nRecently, quantum kernel methods have emerged as a promising approach to leveraging the capabilities of current NISQ devices when viewing QML models [14]. The value of this perspective is rooted in the compatibility of quantum models with the extensive range of classical kernel theory tools, which can significantly streamline the optimization process for quantum models [15], [16]. These methods involve mapping data into the Hilbert space associated to a system of qubits using a quantum feature map that facilitates the calculation of a kernel matrix through the pairwise inner product of data points. This matrix can subsequently be utilized in conventional methods such as support vector machines or kernel ridge regression [17], [18]. The underlying premise is that quantum Hilbert space data encoding can enhance the feature map with quantum-exclusive resources, offering benefits over classical maps. Such advantages have been demonstrated for specific datasets [12], [19].\nTime series forecasting (TSF) represents a domain ripe for the transformative impacts of quantum computing. This field encompasses critical activities such as financial forecasting [20], where the potential speed-up offered by quantum computing could lead to markedly faster predictions, conferring a significant competitive edge [21]. The implications of such advancements are profound, particularly in the high-stakes realm of finance where rapid and accurate predictions are paramount. Additionally, the role of forecasting is becoming increasingly pivotal in security contexts. This importance is underscored by the recent development of methods that connect QML advancements with time series anomaly detection challenges [22], [23], showcasing the potential of QML in addressing complex forecasting tasks.\nQuantifying uncertainty in time series forecasts is a critical component of the prediction process [24]. Consequently, probabilistic TSF techniques have garnered considerable attention, especially in areas where managing uncertainty is vital, such as anomaly detection. Gaussian process regression (GPR) stands out as a simple yet effective probabilistic TSF method [25]. Based on Bayesian inference, this approach adeptly captures uncertainty by presenting the forecast of the next value in a time series as a posterior probability distribution. This distribution not only provides an expected value for the next point in the series (i.e., the mean) but also quantifies the confidence in this prediction through the variance, offering a comprehensive view of future expectations and their reliability.\nIn recent research, the focus has been predominantly on exploring quantum versions of traditional kernel machines, notably the support vector machine [14], [26], showcasing their potential in leveraging quantum computing capabilities. Despite this interest, the development and exploration of quantum probabilistic kernel methods within the context of noisy intermediate-scale quantum (NISQ) technologies remain relatively underexplored [27]. This gap is particularly evident in the field of probabilistic TSF, where the application of quantum-enhanced probabilistic kernel methods has barely been addressed in scholarly works.\nIn pursuit of advancing the field of quantum kernelized probabilistic TSF, we introduce QuaCK-TSF: Quantum-Classical Kernelized Time Series Forecasting. Our model endeavors to predict the subsequent value of a time series S = (Xt\u2081, Xt2, ..., Xtr) at time step tr+1, with each Xt\u2081 \u2208 R and time t\u2081 \u2208 R+. Leveraging the GPR framework, we assert that for a given kernel function and a training dataset D derived from the series s, the posterior distribution p(xtr+1D) can be modeled as a Gaussian distribution N (PtT+1, \u03c3^2(+1,+1)), where ItT+1 represents the mean prediction, and \u03c3^2(+1,+1) signifies the prediction's variance.\nWe construct a portal to quantum-enhanced forecasting by employing a quantum kernel that is conceptualized as the state fidelity overlap between quantum states [15]. Our model employs a variant of the instantaneous quantum polynomial-time (IQP) feature map [14], similar to the approach in [28], as detailed in subsection III-D2. The choice of the IQP feature map is motivated by its conjectured resistance to classical simulation [14] and its ability to capture temporal relationships through qubit interactions, akin to Ising models. By encoding temporal data points as qubit states, the feature map effectively distills the complex non-linear temporal dynamics of the time series, as demonstrated by our empirical analysis. Additionally, we refine our model's hyperparameters using Bayesian optimization (BO) to circumvent the tedious gradient descent optimization, generally realized following the parameter-shift rule. To the best of our knowledge, this work represents the first application of GPR based on a quantum kernel to address the challenge of probabilistic TSF.\nPaper Outline. The remainder of this paper is organized as follows. Section II introduces relevant background concepts. Section III presents our modeling approach. Section IV outlines the empirical evaluation setup and our results. Finally, Section V concludes the paper."}, {"title": "II. BACKGROUND", "content": "A. Gaussian process regression\nConsider a set of observations D = {(xi, Yi)}1\u2264ic, with each x \u2208 R representing a w-dimensional input vector of covariates and y \u2208 R signifying the scalar output or target. We organize these observations into a w \u00d7 c design matrix X, and stack the corresponding target values into the vector y, thereby representing the dataset as D = (X, y). In regression analysis, the goal is to model the input-target relationship through the function y = f(x) + \u20ac, where f : R\u2122 \u2192 R is designed to approximate the unknown relationship, and e is a normally distributed error term with zero mean and variance 02, denoted as \u03b5 ~ \u039d(0, 02).\nParametric models, such as linear regression, posit a specific functional form for f, typically involving a set of parameters \u03bc, e.g., f(x) = x\u012b\u03bc, \u03bc\u2208R\u2122, for linear models. Conversely, GPR adopts a non-parametric approach, wherein f is not constrained by a predefined structure but is instead described by a distribution over possible functions. This probabilistic treatment of the model function f allows GPR to flexibly capture complex patterns in the data without committing to a fixed form, a significant advantage when our prior knowledge is limited or when the true underlying relationships are intricate and not well-understood.\nTo manage the intractable set of all possible functions from R\u2122 \u2192 R, we impose a structure on the prior distribution of f, encapsulating it within the framework of a Gaussian process (GP). This GP, represented by the collection of random variables (f(x1), f (x2), ..., f (xc)), is fully characterized by its mean function m(x) and covariance function (x, x'). The mean function captures the expected value of the process, while the covariance function quantifies the extent to which the outputs at different inputs co-vary. Formally, we express these functions as m(x) = E[f(x)] and (x,x') = Cov(f(x), f(x')), denoting the process as f(x) ~ GP(m(x), \u03ba(x, x')).\nThe selection of the kernel function \u03ba profoundly influences the characteristics of the function class deemed likely to model the data. For instance, the radial basis function (RBF) kernel, expressed in (15), predisposes the model to smooth functions, while a periodic kernel, detailed in (18), biases it towards periodic functions. This critical choice of kernel is thus instrumental in steering the regression outcomes and warrants careful consideration to align with the nature of the data and the problem at hand.\nB. Quantum machine learning\nQML is a burgeoning paradigm in artificial intelligence, leveraging the unique capabilities of quantum computers to handle computations within exponentially large vector spaces through quantum algorithms [29]\u2013[31]. In typical QML applications, classical datasets residing in some space X are used for tasks like classification or data generation. These datasets are transferred to the quantum domain using a feature map \u03c6: X \u2192H, where H denotes the Hilbert space of an n- qubit quantum computer, with dimensions 2n. For each data point x \u2208 X, embedding unitaries U(x) are applied such that U(x) |0)&n = |$(x)) produces a state dependent on the data. The model typically evolves by training a measurement operator Me to minimize a loss function Lover the dataset, with the model output defined as ho(x) = ($(x)| M\u0473 |$(x)) [30]."}, {"title": "III. METHODOLOGY", "content": "A. Problem statement\nConsider a time series s, observed over the time interval T = {t\u2081 : l \u2208 [1..T]}, where each ti is a positive real number and T is a positive integer. This series comprises a sequence of real-valued observations xt, systematically recorded in chronological order as expressed by:\ns = (xt : t \u2208 T), xt\u2208 R.\n(2)\nProblem. Our focus is on the task of probabilistic forecasting, aiming to predict the forthcoming value Xt1+1 in the series s, based on a historical window of observations of length w: (xt : tl-w+1 \u2264 t \u2264 t\u2081). The challenge lies in accurately determining the conditional probability distribution of the next observation as follows:\nP(Xt1+1 | (xt : ti-w+1 \u2264 t \u2264 t\u2081));\n(3)\nensuring the continuity of the sequence where the lookback period starts after t\u2081 and the prediction point t1+1 falls within the observation period ending at tr.\nB. Forecasting as a regression problem\nWe conceptualize our forecasting challenge within the regression framework as follows:\ny = f(x) + \u20ac,\n(4)\nwhere y, the target value, is defined as y = Xt1+1 and represents the next value in the time series. The input vector x \u2208 R comprises a sequence of w preceding observations (xt: tl-w+1 \u2264 t \u2264 t\u2081)T, and e denotes an independent and identically distributed normal additive noise, specifically \u03b5 ~ \u039d(0, \u03c3\u03c4). Drawing from the GPR framework [25], we postulate that any set of input vectors X = {X1, X2, ..., Xc}, extracted from the time series s and numbering c in total, induces a stochastic process (f(x1), f (x2), ..., f (xc)) that conforms to a Gaussian distribution. Here, each f(x) is a random variable, with randomness originating from the selection of the function f from a space of possible functions RR.\nThis Gaussian process is delineated by its prior distribution, defined by a mean function m(x)\u2014typically assumed constant (i.\u0435., m(x) = m with m as a hyperparameter, usually set to zero)\u2014and a covariance kernel function \u03ba\u03b1(xj, xj') reliant on some hyperparameters a \u2208 Rda, where da \u2208 Z+ is the number of such hyperparameters. Thus, the prior distribution for a given set of input windows X is expressed as\nf(x) ~ GP(\u0442, \u043a\u0430(Xj, Xj')).\n(5)\nThroughout this discussion, we employ f(x) and f interchangeably to refer to the model's output, thereby treating it as a random variable influenced by the underlying probabilistic nature of the GPR model.\nC. Predictive distribution\nIn the context of a new observation window x, and considering a set of training windows X = {X1,X2, ..., Xc} with their corresponding targets Y = {Y1, Y2, ..., Yc}, the posterior distribution of the forecasted values f, conditioned on both the training windows and the new input sequence x, is captured by the predictive distribution\nf | X,y,x ~ N (f,o}),\n(6)\nwhere X denotes the matrix compiled from the vectors xj, arranged as columns such that X = (X1, X2, ..., Xxc) and occupies a space in Rw\u00d7c. The vector y consolidates the target values into (y1, Y2, ..., Yc) within R. Here, f represents the mean prediction, and of denotes the forecast's variance. Such distribution parameters are derived in [25] as:\nf = k* (K + \u03c3I)\u00af\u00b9 y + m,\n\u03c3\u03be = \u03ba\u03b1(x,x) \u2013 k (K + \u03c3\u0399)\u00af\u00b9k,\n(7)\nwhere k is a vector in R\u00ba, formed by the covariance values between the new observation window x and each of the training windows xj, specifically k = (ka(x,xj) : j\u2208 [1..c])T. The covariance matrix K, a square matrix of dimensions c \u00d7 c, is described as K = K(X,X) = (\u043a\u0430(Xi, Xj))i,j=1, encapsulating the pairwise covariances between all training window combinations. This formulation elegantly encapsulates the uncertainty and expectations of model predictions, providing a comprehensive probabilistic outlook on forecasting.\nD. Kernel function\nOur method employs a strategy of constructing the kernel function in a bottom-up manner using sub-windows. This approach enables the generation of embeddings that capture the intricate temporal patterns within each subsequence\u2014a nuance that methods focusing on individual time point embeddings might miss [32]."}, {"title": "1) Quantum encoding feature map:", "content": "Consider a time series window x = (xt\u2081, Xt2, ..., X tw ), ti \u2208 T. We map this sequence into a quantum state via the data encoding feature map \u0444, producing a quantum state |\u03c6(x, \u03b1)) as follows:\n\u03c6:RH\nx + (x, \u03b1)),\n(8)\nwhere a \u2208 R, is introduced as a tunable hyperparameter. The state |\u03c6(x, \u03b1)) is prepared by applying the embedding unitary U(x, a) to the initial state |0)\u2297n, as shown by the equation\nU(x, \u03b1) |0)&n = |\u03c6(x, \u03b1)) .\n(9)"}, {"title": "2) IQP feature map:", "content": "For our feature map to take full advantage of the capacity of quantum computers, it must leverage quantum properties challenging for classical computation to simulate. Accordingly, we have designed our embedding strategy to incorporate an Instantaneous Quantum Polynomial-time (IQP)-style circuit, echoing the proposals for achieving quantum advantage introduced in [14].\nThe architecture of our feature map U(x, a), depicted in Fig. 1, is described by the equation:\n|$(x, \u03b1)) = Uz(x, a)H\u00aenUz(x, a)H\u00aen |0)\u00aen,\n(10)\nwhere Hon denotes the Hadamard gates applied simultaneously to all qubits. The unitary Uz(x, a) is defined as:\nUz(x, \u03b1) = exp (\u03b1\u2211XtZj + \u03b1\u00b2 \u2211XtXtZjZj );\n(11)\nwith Zj being the Pauli-Z gate affecting the j-th qubit, and \u03b1\u2208 [0,1] representing the bandwidth coefficient. This coefficient is introduced to improve the model's generalization capability by confining the embeddings within a limited region of the quantum state space, thereby mitigating the risk of generalization errors in expansive Hilbert spaces [33], [34]. It is assumed that the input sequence x will be normalized during data preprocessing to center around zero with a standard deviation of one, aligning with the initial proposal's framework [14].\nThis quantum feature map aims to nonlinearly unveil the intricate temporal dynamics inherent in the sequence x. By translating classical data into quantum information, the feature map captures not only individual temporal contributions but also their interactive dynamics. These interactions, resembling Ising-type interactions, encode temporal correlations across different time points, enabling a sophisticated understanding of the temporal structure in x.\nThe ability of this embedding to entangle subsystems contributes significantly to its classical intractability. Entanglement arises through the application of the ZjZj, unitaries, inducing correlations between qubit pairs and thus intertwining different subsystems. Additionally, the circuit's dual-layer architecture plays a crucial role in its complexity, as demonstrated in [14], since a single-layer circuit may still permit classical estimation, underscoring the nuanced design of our feature map for harnessing quantum advantage."}, {"title": "3) The kernel function:", "content": "Employing the embedding feature map x (x,a)), we quantify the similarity between sequences x and x' by calculating the fidelity between their quantum embeddings, |(x, \u03b1)) and |(x', \u03b1)). This measure, rooted in established literature [14], [15], is defined as:\n\u03ba\u03b1(x, x') = | ((x, \u03b1)|\u03c6(x', \u03b1)) |2.\n(12)\nCrucially, this kernel function fulfills the symmetry requirement, a core attribute for kernel validity, guaranteeing that \u039a\u03b1(\u03a7, \u03a7') = \u03ba\u03b1(x', x) holds for any pair of sequences x and x'."}, {"title": "E. Hyperparameters fine-tuning", "content": "Optimizing hyperparameters \u03b8\u2208 R\u00b3, which encompass the Gaussian Process (GP)'s mean constant m, noise variance 02, and bandwidth a, is paramount in our model. The objective function g(0), designated for optimization, is the marginal log likelihood (MLL). This function quantifies the model's fit by marginalizing over the set of all possible functions f \u2208 RC:\nlog po (y X) = log po (y|f, X)po(f|X).\n(13)\nWithin the Gaussian process framework, with the prior on f being Gaussian, f|X ~ N(m, K), and the likelihood as a factorized Gaussian, y|f, X = y|f ~ N(f, I), this integral assumes a tractable form [25].\nWhile gradient descent could be a natural choice for its precision, the computational demands, particularly the increased number of quantum circuit runs required by the parameter shift rule [35], make it less appealing. Additionally, a comprehensive exploration of the parameter space using gradient descent demands a large number of evaluations. Thus, we adopt Bayesian optimization for its efficiency with expensive objective functions and its derivative-free nature. Further details about the Bayesian optimization framework can be found in appendix A.\nFor this optimization process, we employ another GP model-distinct from the one used for TSF-to serve as our BO's surrogate model, selecting the Mat\u00e9rn 5/2 kernel, expressed in (16), for its flexibility and capability to model less smooth surfaces compared to the RBF kernel [25]. The log expected improvement is chosen as the acquisition function for its numerical optimization ease [36].\nThe initial phase of our BO utilizes a Sobol sequence for space-filling design, generating quasi-random sequences that more uniformly cover the hyperparameter space than pure random sampling. In the subsequent phase, we enter a loop where, at each step 1 \u2264 j \u2264 N, a new query point 0; is selected to optimize the acquisition function, i.e.,\n0; = arg max Elz (0),\n\u03b8\u0395\u0398\n(14)\nwith z representing the highest observed value so far, and Dj the dataset of observations made so far.\nThe optimization bounds are specifically set for each parameter: 0 < a < 1 for the bandwidth, mo < m < m\u2081 for the mean constant, and \u03c3\u03c4\u03bf \u2264\u03c3\u03b7 \u2264 \u03c3\u03b7\u2081 for the noise level, where the bounds are user-defined. The problem defined in (14) is tackled through an inner optimization using the Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Bounds (L- BFGS-B) algorithm, preferred for bound-constrained optimization [37], [38]. Upon determining the next query point \u03b8j, the function g is evaluated, added to the set of observations, and the surrogate GP model is updated, paving the way for precise and efficient hyperparameter tuning."}, {"title": "IV. EXPERIMENTS", "content": "To test our quantum kernel-based GPR model, we have opted for a synthetic time series to build intuition. Our goal is to create a synthetic TSF problem where (i) data is not linearly forecasted (thus kernelized approaches are required), (ii) the dynamics of the time series are not static and evolve with time in order to mimic real world time series. To this end, we have generated a synthetic time series, illustrated in Fig. 3, that combines linear trend components, non-linear dynamics through sine waves, and Gaussian noise to create complex patterns for testing and analysis. Specifically, for a given number of time steps (240 our case), it first divides the series into segments according to the specified number of trend changes (4), alternating between upward and downward linear trends with slight slopes. In parallel, it overlays two sine waves: one with a longer period (10 time steps) and larger amplitude (scale of 1) to introduce long-term cyclic patterns, and another with a period determined by the frequency of trend changes and a smaller amplitude (scale of 0.5) to capture quicker, more nuanced oscillations. The resultant series is further perturbed by adding Gaussian noise with a specified level (0.5), to mimic real-world unpredictability. This process results in a synthetic dataset that exhibits realistic characteristics such as trends, seasonality, and noise, making it useful for testing TSF models.\nAdhering to the preprocessing steps outlined in [39], we commence our experimental procedure by standardizing the generated synthetic time series. This normalization process adjusts the series values to be centered around zero with a standard deviation of one. Subsequently, the series is segmented into training and testing sets, depicted by the blue and orange curves, respectively, in Fig. 3. The training dataset is denoted as D = (X, Y), where X = {x1,..., xc} encompasses the input windows, and Y = {y1, ..., yc} contains the corresponding target values. In our experiments, we selected a window length of 5, necessitating the use of 5 qubits per window, aiming for a balanced qubit count that strikes a compromise between being overly extensive and unduly limited. To minimize redundancy in the training data, consecutive training windows xi and xi+1 are allowed an overlap of merely 2 time steps. The test dataset, D' = (X', Y'), is structured to commence prediction at the immediate subsequent time point following the last prediction in the training phase. The prediction process then advances by a single time step, sequentially addressing the remaining time points in the series until its conclusion."}, {"title": "A. Results of the quantum kernel", "content": "In our exploration, the quantum-based GPR model, trained on a classical simulator, was fine-tuned to fit the synthetically generated time series data employing the BO technique as detailed previously. The hyperparameters optimization was structured into two key steps: an initialization phase and a query point generation phase, each with 25 sample points, i.e., \u043f\u043e = N = 25, to enhance the model's predictive accuracy. The hyperparameter selection was meticulously designed to maximize the marginal log likelihood, ensuring the model's optimal performance during test set predictions.\nFig. 4 showcases the predictive prowess of our model on the test set, where the training data is illustrated by a light blue curve, the test data by an orange curve, and the model's predictions by a dark blue curve, surrounded by a lightly shaded area indicating the 95% confidence interval. Observably, the model exhibits a good fit to the test series, with the confidence interval accommodating the series' variability. This illustrates the quantum kernel's aptitude in capturing the nonlinear dynamics of the series while maintaining flexibility, highlighting its effectiveness in forecasting intricate time series data.\nThroughout the optimization journey, the objective function, known for its computational expense, was evaluated 50 times to fine-tune the hyperparameters. The optimization bounds for the GPR model's mean constant m and the noise level \u03c3 were set to \u22121 < m < 1 and 0 < 2 < 1, respectively. This decision was influenced by the noise level in the synthetic dataset, set at 0.5, and the mean constant m = 0, derived from the data normalization process, hence selecting intervals that encompass these values.\nUltimately, the optimal configuration for our quantum-based GPR model was identified as 0 = (\u03b1, \u03c3\u03b7, m) = (0.243, 0.350, 0.503)T. The optimal bandwidth parameter a = 0.243 suggests a strategic limitation of the latent Hilbert space, signifying that our kernel could effectively model the time series data with restrained expressibility. This constraint acts as a form of regularization, aiding in the learning process and preventing model overfitting.\nFig. 5 visualizes the various configurations, evaluated during the optimization process, viewed in two ways, with a color gradient from white to dark green denoting the marginal log likelihood values-darker shades represent higher values, and square markers indicate the evaluation of the objective function. The sub-figures are two views of the same process plotting the MLL against different sub-configurations of hyperparameters (a,m) and (\u03b1, \u03c32). Post-initialization, which employed a Sobol sequence for diverse hyperparameter sampling, the model progressively focused on configurations within the higher-value (dark green) domain, reflecting a sophisticated convergence towards the most promising hyperparameter region."}, {"title": "B. Quantum kernel vs baseline classical kernels", "content": "1) Baseline kernels: To evaluate the effectiveness of our proposed quantum kernel, we conducted comparisons against a selection of widely recognized classical kernels in the literature. To this end, we have selected four classical baseline kernels for comparison, providing a brief overview of each below.\nOur first comparison involves the Radial Basis Function (RBF) kernel, which is notably prevalent in classical machine learning literature [40]. It quantifies the similarity between two samples, x, x' \u2208 Rw, through the equation\nKRBF1, (X, X') = exp (- ||x - x'|| /21^2), (15)\nwhere ly denotes the lengthscale hyperparameter, constrained between 0.1 and 30 in our experiments, and ||x - x'|| represents the Euclidean distance between the two feature vectors.\nNext, we consider the Mat\u00e9rn kernel [41], which models the covariance between x, x' \u2208 Rw as\nKMAT, (x, x') = 2^(1-\u03bd) / \u0393(\u03bd) * (\u221a2\u03bd d / lm)^\u03bd * Kv(\u221a2\u03bd d / lm), (16)\nwhere d is the scaled Euclidean distance between x and x', lm is the lengthscale hyperparameter (with a constraint of 0.1 \u2264 Im \u2264 30), vindicates the smoothness parameter with values in 1/2,3/2,5/2-affecting the kernel's smoothness, and K is the modified Bessel function.\nFor the third kernel, we examine is the Rational Quadratic (RQ) kernel [42], which measures similarity as\nKRQB,la (x, x) = (1+ (||x - x'||2 / 2\u03b2\u03b9\u03b1))^-\u03b2, (17)\nwhere B serves as the rational quadratic relative weighting parameter (ranging from 0.1 to 10), and lq is the lengthscale hyperparameter, constrained between 0.1 and 30.\nFinally, our analysis includes the Periodic kernel [43], represented by\nKPERp,lp (X, X') = exp (-2 / lp^2 * \u2211 sin\u00b2((Xi-xi) / p)), (18)\nwhere p indicates the period length parameter (set between 5 and 35), and lp is the lengthscale hyperparameter, with a constraint range of 0.1 to 30.\nIt is worth noting that the constraints on the classical kernels' hyperparameters were carefully chosen to accommodate the diverse periodicities, characteristics, and fluctuations observed in the synthetic time series generated for our study.\n2) Quantum-classic kernels comparison results: The predictive capabilities of both classical and quantum kernels, each fine-tuned using the BO process for hyperparameter tuning, are illustrated in Fig. 6. Notwithstanding minor variations in the shape of the confidence intervals, the quantum kernel's predictions broadly align with those generated by classical counterparts. This alignment suggests that our quantum model performs competitively with classical alternatives. Furthermore, our model produces a smoother and slightly different curve for mean predictions compared to classical counterparts, notably demonstrated by the reduced sharpness of the peak around time step 210. This suggests that our quantum feature map provides a different perspective on the data, potentially enriching the representation tools at our disposal.\nTo expand our initial findings, we employed a suite of performance metrics to assess the efficacy of each kernel based on predictions made on the test set D'. The chosen metrics include the Mean Squared Error (MSE), the Root Mean Squared Error (RMSE), the Mean Absolute Percentage Error (MAPE), the symmetric MAPE (sMAPE), and the Weighted Average Percentage Error (WAPE). These metrics serve to quantify the discrepancy between the mean predictions and the actual target values, thereby evaluating the forecast quality. Furthermore, to gauge the model's proficiency in accurately representing the observed data, we incorporated two additional metrics: the mean Continuous Ranked Probability Score (mCRPS) and the Log Likelihood (LL). Detailed explanations of these metrics and their computational methodologies are available in appendix B-\u0410."}, {"title": "C. Ablation study", "content": "1) Fidelity state overlap analysis: To evaluate the effectiveness of our learned feature map, we delve into the fidelity state overlap among various quantum states, utilizing the kernel values as a proxy for this overlap. The kernel value represents the inner product between two quantum states (x, \u03b1)) and |$(x', \u03b1)), effectively capturing the state overlap.\nFig. 7 illustrates the variation in fidelity overlap for different positions within the input space. This analysis was conducted by selecting a reference point x0 = )0,...,0(\u05d6 \u220b Rw, where each dimension is set to zero, aligning with the mean of the standardized time series s. The fidelity overlaps are calculated using Ka(x0,x) = |{$(x0, \u03b1), \u03c6(x, \u03b1))|2, for various points x from the set XF. This set comprises points x with the last w - 2 dimensions fixed at zero, and only the first two dimensions varied within the range of the standardized time series' minimum (about -2.7) and maximum values (approximately 2.4).\nThe figure reveals that points x closer to the reference point xo exhibit higher fidelity values, indicating strong similarity. Conversely, as points deviate further from x0, their similarity diminishes. This pattern confirms that our quantum kernel effectively functions as expected of a robust kernel function, demonstrating its ability to discern varying degrees of similarity based on proximity in the quantum state space.\n2) Varying the number of qubits: To assess how the number of qubits, equating to window length, affects our model's performance, we conducted an ablation study focusing on the relationship between the quantity of qubits and various performance metrics. Fig. 8 illustrates the impact of changing the number of qubits on the LL and MAE during the testing phase. We explored a range of qubit quantities, specifically {5, 6, 7, 8, 9, 10}, optimizing the model afresh for each configuration before proceeding to measure the respective metrics. Additionally, two modifications were made from the previously mentioned experiment in subsection IV-A: firstly, the overlap between consecutive windows was increased to 4 time steps; secondly, the total count of synthetically generated time steps was set to 480. These adjustments were made to accommodate a larger dataset required by the increased number of qubits.\nOur analysis demonstrated that selecting 8 qubits delivered the most favorable outcomes, marking it as the optimum number of qubits for our model configuration. This finding indicates that an increased number of qubits significantly improved the testing metrics compared to the outcomes observed with 5 qubits. However, enhancing the qubit count beyond 8 led to diminishing returns in performance metrics. Such a decline is largely due to the necessity for a more extensive dataset to support precise GPR model inferences with a higher qubit count. Expanding the dataset substantially, though, introduces computational constraints, given that the quantum circuit runs exhibit a quadratic increase in time complexity relative to the size of the dataset. Therefore, the selection of an optimal qubit number should be carefully balanced against the constraints of dataset size and computational capacity."}, {"title": "V. CONCLUSION", "content": "In this work, we introduced a hybrid quantum-classical kernelized approach for probabilistic time series forecasting that goes beyond mere value predictions to effectively quantify uncertainties. Leveraging the sophisticated kernel perspective of quantum machine learning models and the robustness of Gaussian process regression, our approach establishes a nexus between quantum models and kernelized probabilistic forecasting. This methodology, incorporating a quantum feature map inspired by Ising interactions, adeptly captures various complex temporal dependencies essential for accurate forecasting. Additionally, we optimize the model's hyperparameters using gradient-free Bayesian optimization, thus sidestepping the computational intensity of gradient descent. Comparative benchmarks against classical kernel models have not only validated our innovative approach but also confirmed its competitive performance within the field.\nLooking ahead, this advancement in quantum-enhanced probabilistic forecasting sets the stage for significant future developments. It is crucial to focus on increasing the efficiency of the model and extending its application across diverse datasets to fully exploit its potential. Furthermore, the exploration of quantum feature maps specifically designed to capture complex time series patterns is of great interest for better understanding of the possible enhancements. The focus should be put towards maps that offer a potential quantum advantage that could significantly propel the capabilities of quantum-enhanced time series forecasting."}, {"title": "APPENDIX A\nBAYESIAN OPTIMIZATION", "content": "In the realm of complex machine learning challenges, parameter tuning emerges as a critical task often reliant on time-intensive evaluations. These evaluations spring from the execution of a black-box objective function g, with parameters @ drawn from a designated parameter space. Due to the substantial resources required for each evaluation, g is characterized as an expensive-to-evaluate function. The objective, therefore, narrows down to identifying an optimal or near-optimal parameter configuration 0* \u2208 that optimizes g through a minimal number of evaluations:\n0* = arg max; g(0).\n\u0398\u0395\u0398\n(19)\nIn such contexts, resorting to gradient-based optimization proves impractical as it typically necessitates a prohibitive number of evaluations to approximate the optimum of g effectively.\nBayesian optimization (BO) presents a strategic framework aimed at minimizing the evaluation count of g, all while incorporating historical evaluation data [44], [45]. This method elegantly balances exploration of highly uncertain regions against exploitation in areas showing promising performance. Central to BO are two components: a surrogate model, which provides a probabilistic representation of the objective function, and an acquisition function that strategizes the subsequent sampling point. Initially, BO embarks on a space exploration using a predefined strategy, typically involving no points, either through random selection or employing space-filling designs like Sobol sequences [46]. This phase yields an initial dataset D\u2081 = {(0i, zi)}1<i<no, capturing the outcomes zi = g(0i) at each \u03b8.\nIn the second step, we iteratively refine our understanding of g, starting with the initial dataset D\u2081 to inform a prior over g. This process iteratively selects N query points to evaluate, each chosen to maximize the acquisition function, thereby leveraging the posterior distribution of g. A Gaussian process is frequently the model of choice for the surrogate, updating the posterior distribution with each new observation and thus enhancing our predictive capability over g's outcomes at untested points. The acquisition function, often chosen to be the expected improvement [44], is defined as:\nEI2* (0) = Eg(0) [max (0, g(0) \u2013 z*)],\n(20)\nwhere z* = maxi zi denotes the best observed outcome thus far, or the incumbent. This function quantifies the utility of sampling gat new candidate points. An illustration of the BO cycle in Fig. 9 showcases how the expected improvement achieves a compromise between areas of high uncertainty and those with favorable observed results."}, {"title": "APPENDIX B\nFURTHER EXPERIMENTAL DETAILS", "content": "A. Performance metrics\nTo perform the quantum-classical comparison, we employed a comprehensive suite of 8 metrics calculated on the test set D'. For each input window in the test set X', denoted as x = (x1,...x"}]}