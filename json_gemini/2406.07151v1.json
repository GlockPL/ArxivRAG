{"title": "EEG-ImageNet: An Electroencephalogram Dataset and Benchmarks with Image Visual Stimuli of Multi-Granularity Labels", "authors": ["Shuqi Zhu", "Ziyi Ye", "Qingyao Ai", "Yiqun Liu"], "abstract": "Identifying and reconstructing what we see from brain activity gives us a special insight into investigating how the biological visual system represents the world. While recent efforts have achieved high-performance image classification and high-quality image reconstruction from brain signals collected by Functional Magnetic Resonance Imaging (fMRI) or magnetoencephalogram (MEG), the expensiveness and bulkiness of these devices make relevant applications difficult to generalize to practical applications. On the other hand, Electroencephalography (EEG), despite its advantages of ease of use, cost-efficiency, high temporal resolution, and non-invasive nature, has not been fully explored in relevant studies due to the lack of comprehensive datasets. To address this gap, we introduce EEG-ImageNet, a novel EEG dataset comprising recordings from 16 subjects exposed to 4000 images selected from the ImageNet dataset. EEG-ImageNet consists of 5 times EEG-image pairs larger than existing similar EEG benchmarks. EEG-ImageNet is collected with image stimuli of multi-granularity labels, i.e., 40 images with coarse-grained labels and 40 with fine-grained labels. Based on it, we establish benchmarks for object classification and image reconstruction. Experiments with several commonly used models show that the best models can achieve object classification with accuracy around 60% and image reconstruction with two-way identification around 64%. These results demonstrate the dataset's potential to advance EEG-based visual brain-computer interfaces, understand the visual perception of biological systems, and provide potential applications in improving machine visual models.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in reconstructing visual experiences from the human brain have seen significant progress, largely driven by the extensive use of functional magnetic resonance imaging (fMRI) ([8, 22, 23]) and magnetoencephalogram (MEG) [3] datasets. fMRI and MEG are widely used to investigate various cognitive functions, neurological disorders, and brain connectivity patterns ([2, 40, 37, 35]). Driven by the use of deep neural networks, particularly diffusion-based and transformer-based models( [34, 30, 25, 6]), it is even possible to reconstruct human's visual perceptions from fMRI or MEG recordings. The application of large-scale deep neural networks in neuroscience research has further underscored the importance of large-scale and high-quality datasets [27, 20]. For example, the Natural Scenes Dataset ([1]), contains up to hundreds of thousands of high-quality natural image-fMRI pairs of 8 subjects, providing a solid data foundation for recent work in visual neuroscience. These models and large-scale datasets, in turn, have opened new avenues for understanding the brain's intricate functions and for developing advanced applications in brain-computer interfaces, neuroimaging, and beyond [33].\nOn the other hand, electroencephalography (EEG) is another vital tool in neuroscience research. In comparison with fMRI and MEG, EEG is easy to use, cost-efficient, and has superior temporal resolution, making it a valuable tool for capturing rapid and real-time brain dynamics on the order of milliseconds [36]. EEG signals can be obtained non-invasively by placing electrodes on the scalp, making it a less intrusive method for monitoring brain activity. These attributes position EEG as another promising modality for visual neuroscience research.\nAlthough visual reconstruction has been achieved using fMRI and MEG, the high cost and inconvenience of these devices limit their widespread application in practical settings. In contrast, EEG presents advantages over both fMRI and MEG with its cost-efficient and portable features. However, studies on visual perception with EEG signals are limited because of two challenges: (1) the lack of large-scale, high-quality EEG datasets and (2) existing EEG datasets typically featured coarse-grained image categories, lacking fine-grained categories. To the best of our knowledge, the most frequently used dataset is the data set provided by Spampinato et al. [32], which involves 6 participants each watching 2000 image stimuli. However, this dataset's scale is smaller than existing fMRI datasets, limiting the possibilities for investigating neural aspects related to visual perception and developing deep learning models for relevant visual classification and reconstruction tasks.\nOn the other hand, the labels in existing EEG datasets are frequently coarse and lack the granularity needed for detailed analysis. Multi-granularity labels are essential because they allow for a more nuanced analysis at different levels of detail. For instance, labels can range from broad categories like \"panda\" or \"golf ball\" to more specific attributes like \u201cRottweiler\u201d or \u201cSamoyed\". These challenges underscore the necessity for new, large-scale EEG datasets with high-quality, multi-granularity labels. Such datasets would enable researchers to explore the intricacies of visual processing with greater accuracy and depth, facilitating advancements in both basic neuroscience and applied fields like brain-computer interfaces and clinical diagnostics.\nTo address these challenges, we present EEG-ImageNet, a novel EEG dataset specifically designed to promote research related to visual neuroscience, biomedical engineering, etc. EEG-ImageNet is a comprehensive dataset that includes EEG recordings from 16 subjects, each exposed to 4,000 images"}, {"title": "2 Related Work", "content": "In this section, we review some datasets related to visual recognition and neuroscience and compare them with EEG-ImageNet, as shown in Table 1.\n2.1 Visual Recognition Dataset\nVisual recognition is a cornerstone of computer vision, driven by datasets like ImageNet [28], CIFAR [16], and MS COCO [20]. ImageNet contains over 14 million annotated images, CIFAR-10/100 consist of thousands of 32x32 images in 10 and 100 classes, respectively, and MS COCO is known for its rich annotations supporting tasks such as object detection and segmentation. Later works have matched image datasets with additional modalities.\nEfforts to combine visual recognition with neuroscience have led to datasets like the Microsoft COCO Captions [5], which adds textual descriptions to the MS COCO images, enhancing the dataset with a multimodal aspect for evaluating image captioning models. The SALICON dataset [10] extends MS COCO with eye-tracking data, enabling the study of visual attention and saliency through large-scale annotations. Neuroscience datasets utilizing fMRI have further enriched this field. The BOLD5000 dataset [4] includes fMRI data from subjects viewing 5000 images, aiding the exploration of visual perception. The Generic Object Decoding dataset [9] captures brain activity while subjects view and imagine objects, facilitating the decoding of mental images. The VIM-1 dataset [14] from the study \"Identifying Natural Images from Human Brain Activity\" demonstrates the feasibility of decoding viewed images from brain activity. Additionally, the NSD [1] is a large-scale fMRI dataset in visual neuroscience, recording high-resolution (1.8-mm) whole-brain 7T fMRI data from eight subjects exposed to 9,000-10,000 color natural scenes from the MS COCO dataset over the course of a year. Integrating neuroimaging and eye-tracking datasets with visual recognition tasks has opened new avenues for understanding how the brain interprets visual information, leading to insights and applications in brain-computer interfaces and neural decoding.\n2.2 EEG dataset\nfMRI is renowned for its high spatial resolution, allowing researchers to obtain detailed images of brain activity by measuring changes in blood flow [23]. In contrast, EEG offers several distinct advantages over fMRI. EEG is relatively easy to use and cost-efficient, with a straightforward setup that involves placing electrodes on the scalp to measure electrical activity. One of the most significant benefits of EEG is its exceptional temporal resolution, which captures neural dynamics on the order of milliseconds [36]. This high temporal resolution makes EEG ideal for studying fast-occurring brain processes and real-time neural responses, providing insights into the timing and sequence of neural events [21]. EEG's non-invasive nature also makes it suitable for a wider range of participants. However, EEG signals collected using portable devices often have a low signal-to-noise ratio, which can complicate data analysis and reduce the accuracy of the results [13].\nExisting EEG datasets span a variety of research areas. The SEED [43] focuses on emotion recognition with detailed EEG recordings from subjects exposed to various emotional stimuli. The BCI"}, {"title": "3 Dataset Construction", "content": "During the data collection process of our user study, participants are presented with a visual stimuli dataset containing 4000 natural images from ImageNet. Throughout this process, we continuously record their EEG signals. The whole experimental process is carried out in the laboratory environment. This section describes the entire process of EEG-ImageNet dataset construction. The EEG-ImageNet dataset can be accessed openly through the url https://github.com/\nPromise-Z5Q2SQ/EEG-ImageNet-Dataset.\n3.1 Ethical and Privacy\nTo protect participants' privacy and physical health, our user study adheres to strict ethical guidelines for human research, with approval from the ethics committee of the School of Psychology at Tsinghua University. The study has undergone a comprehensive ethical review to safeguard participants' rights. In accordance with ethical standards, we have taken several steps to protect participants' privacy, including data anonymization and obtaining informed consent from all participants. Additionally, participants are thoroughly informed about the study's objectives, procedures, and potential outcomes. The EEG data collection method employed in this research is non-invasive and poses no risk to participants. This approach ensures compliance with ethical standards while maintaining the integrity of the research findings.\n3.2 Participants\nWe enlist a total of 16 participants via social media, including 10 males and 6 females. These participants are all college students aged between 21 and 27, with an average age of 24.06 and a standard deviation of 1.69. Their majors encompass computer science, mechanical engineering, chemistry, and environmental engineering, and they range from undergraduate to postgraduate levels. All participants are right-handed and assert their proficiency in utilizing image search engines in their daily routines. Each participant dedicates approximately 2 hours to complete the experiment, which includes 30 minutes for equipment setup and task instructions. Before the experiment, participants are informed of a compensation of US$11.8 per hour upon completion, to ensure the quality of the data collected for the study.\n3.3 Stimuli Dataset\nThe dataset used for visual stimuli was a subset of ImageNet21k, containing 80 categories of objects. Each category comprises 50 manually curated images, ensuring that each image has a width and height greater than 300 pixels and prominently features an object corresponding to its class label in ImageNet. Additionally, every image is free of watermarks. In this manner, we have selected a total of 4000 high-quality natural images as our visual stimulus dataset.\nAmong all categories, the first half is consistent with the EEG-Classification dataset ([32]), comprising 40 significantly distinct categories from ImageNet1k. We treat these as coarse-grained tasks. The latter 40 categories are designed as a fine-grained task, divided into 5 groups with 8 categories each. The categories within the same group share the same parent node in WordNet, and each category label is either a leaf node or a sub-leaf node in WordNet. This selection ensures that the chosen categories represent similar granularity while avoiding overly obscure categories, thereby minimizing potential biases in the experimental results. For instance, coarse-grained categories include items such as African elephants, pandas, mobile phones, golf balls, bananas, and pizzas. Under the parent node \"musical instruments,\" the fine-grained categories include accordions, cellos, flutes, oboes, snare drums, and trombones. Detailed information about all the visual stimuli categories and their respective WordNet IDs can be found in our GitHub repository.\n3.4 Procedure\nBefore engaging in the user study, participants are required to fill out an entry questionnaire and sign a consent about the protection of privacy security. They will receive an orientation regarding the primary tasks and operational procedures. Additionally, they will be notified of their right to withdraw from the study at any point. Before the main trials, participants will undergo a series of training trials designed to acquaint participants with the procedures of the formal experiments.\nEvery participant is required to select a random seed before the experiment to randomize the order of the categories. This randomization guarantees a fair distribution of categories and images among participants. The experimental platform follows a sequential and repetitive process as illustrated in Figure 1. (S1) The experimental platform presents the current category label. Participants can proceed to the next stage by pressing the space key. (S2) A fixation cross is shown at the center of the screen, ensuring attention is drawn when images are displayed. This fixation period lasts for 500 ms. (S3) The 50 images of this category are sequentially presented using the Rapid Serial Visual Presentation (RSVP) paradigm, which is commonly employed in psychological experiments. Each image is presented for a duration of 500 ms [12]. (S4) Random tests are conducted to verify the participant's engagement in the experiment after the presentation. Data from categories for which participants fail the test will not be included in final analyses. The EEG signals of the participant will be captured and recorded continuously during the entire process. The program will cycle back to step S1 and display the next category, repeating this process until all the images have been presented.\n3.5 Dataset Description\nThe EEG-ImageNet dataset contains a total of 63,850 EEG-image pairs from 16 participants. Each EEG data sample has a size of (n_channels, fs.T), where n_channels is the number of EEG electrodes, which is 62 in our dataset; fs is the sampling frequency of the device, which is 1000 Hz in our dataset; and T is the time window size, which in our dataset is the duration of the image stimulus"}, {"title": "4 Benchmarks", "content": "In this section, we detail the benchmarks of our study shown in Figure 1 by outlining the preprocessing steps, feature extraction methods, task definitions, and models used.\n4.1 Preprocessing\nWe perform a series of preprocessing steps for the raw EEG data we collect to eliminate noise and artifacts and improve signal quality. The preprocessing pipeline includes the following stages: First, re-referencing: Re-referencing is done using the offline linked mastoids method, which uses the average of the M1 and M2 mastoid electrodes as the new reference point [39]. This minimizes potential biases and improves the signal-to-noise ratio. Then, filtering: Filtering is performed using a 0.5 Hz to 80 Hz band-pass filter to remove low-frequency drifts (<0.5 Hz) and high-frequency noise (>80 Hz). Additionally, 50 Hz environmental noise is eliminated. Finally, artifact removal: Artifact removal eliminates abnormal amplitude signals and artifacts caused by blinks or head movements.\n4.2 Feature Extraction\nIn our benchmarks, for models requiring time-domain signals as direct input, we extract the 40ms-440ms segment of each EEG signal as the feature input. This approach helps to minimize the influence of preceding and subsequent image stimuli on the current stimulus. For models requiring frequency-domain features as input, we extract the differential entropy (DE) of the extracted time-domain signals as features, as this characteristic effectively captures the complexity and variability of brain activity in the frequency domain [7]. According to the general division in neuroscience, the frequency bands are categorized as delta (0.5-4 Hz), theta (4-8 Hz), alpha (8-13 Hz), beta (13-30 Hz), and gamma (30-80 Hz). We use the Welch method with a sliding window to estimate the power spectral density P(f) in each frequency band. Then, we normalize the data and calculate the differential entropy (DE) using the formula, $DE = \\int P(f)log(P(f))df$. Consequently, for each segment of EEG signals, we obtain the differential entropy (DE) for each electrode and each frequency band.\n4.3 Task Definition\nIn our benchmarks, we test our dataset on two tasks: object classification and image reconstruction. The object classification task aims to classify the category of the corresponding image stimulus the participant is exposed to with their EEG signals. We evaluate the models using classification accuracy. In the image reconstruction task, given a specific EEG segment, the goal is to reconstruct the image stimulus the participant is exposed to. We evaluate the generated results using two-way identification [30] under different visual neural networks (refer to Appendix A.3 for details.).\nOn our multi-granularity labeled image dataset, we test various tasks with different levels of granularity. Additionally, due to the inherent significant inter-individual differences in EEG signals, all models in our benchmarks are trained exclusively in an intra-subject experimental setup. Furthermore, to mitigate the significant temporal effects [19] observed in the EEG-Classification dataset, all our experimental setups strictly adhere to a dataset split where the first 30 images of each category are used as the training set, and the last 20 images are used as the test set. We strongly recommend that all researchers using the EEG-ImageNet dataset adopt a similar dataset split methodology. We also explain the impact of this split in Appendix A.5.\n4.4 Models\nIn the object classification task, we employ simple machine-learning classification models such as ridge regression, KNN, random forest, and SVM. Additionally, we implement deep learning models including MLP, EEGNet [17], and RGNN [44]. MLP consists of two hidden layers, while EEGNet and RGNN are implemented using their original architectures. We use cross-entropy as the loss function. We train the models for each participant for 2000-3000 epochs on an NVIDIA 4090 GPU."}, {"title": "5 Experiments", "content": "In this chapter, we present the experimental results of our study, focusing on two benchmark tasks: object classification and image reconstruction.\n5.1 Object Classification\nIn the object classification task, we conduct experiments at three levels of granularity: all, coarse, and fine. The \u201call\u201d category represents the 80-class classification accuracy, the \u201ccoarse\u201d category represents the 40-class classification accuracy, and the \u201cfine\u201d category represents the average accuracy of five 8-class classification tasks. The performance of each model is detailed in Table 2, which shows the average results of all participants.\nThe table shows that RGNN achieves the highest accuracy in the \"all\" task among all models, with an 80-class classification accuracy reaching 40.50%. MLP slightly lags behind RGNN in the \"all\" task accuracy, but it significantly outperforms in the \u201ccoarse\" and \"fine\" tasks, with a 40-class accuracy of 53.39% and an average 8-class accuracy of 81.63%. Among the classical machine learning models, SVM exhibits the highest classification performance, only slightly trailing RGNN and MLP across the three different granularity scenarios. The lower accuracy of RGNN in the \"fine\" task might be due to the complex model structure, which may require more fine-tuning to better adapt to easier tasks. Meanwhile, the lower performance of time-domain features compared to frequency-domain features suggests that frequency-domain features might be more informative for the classification tasks in this study. Specific performance details for each participant in each task are provided in Appendix A.4. We find that the ranking of participants' accuracy is relatively consistent across different models.\nWe compile the accuracy of each participant for the \"all\" task under the SVM, MLP, and RGNN models, as shown in Figure 2a. We observe significant differences between participants, indicating high variability in individual responses. The best-performing participant achieves an accuracy of 60.88%. To better compare the differences between \u201cfine\u201d and \u201ccoarse\u201d tasks in EEG-ImageNet, we modify the coarse-grained task. We randomly select 8 coarse-grained categories and use the RGNN model for training and testing. This process is repeated 5 times, and the average accuracy for each participant is calculated and plotted alongside their average \"fine\u201d task accuracy in Figure 2b. We then perform linear regression on the data points, and the resulting function has a slope greater than 1, indicating that participants generally achieve better results on the \u201ccoarse\u201d classification tasks. This finding is also consistent with intuition.\n5.2 Image Reconstruction\nFigure 3 shows some of the results from our image reconstruction pipeline of a single participant (S8). From the selected images, it can be seen that the reconstruction pipeline can effectively restore the category information of the image stimuli. However, restoring low-level details such as color, position, and shape is inaccurate. This may be because we align the EEG signals to the image captions obtained from BLIP. Due to the limited descriptive precision of BLIP, we cannot fully leverage the diffusion model's generative capabilities.\nTable 3 shows the average two-way identification (chance=50%) of the images generated by our reconstruction pipeline using different visual neural networks. Two-way identification refers to comparing the generated image with the original and one distractor to evaluate accuracy. The highest accuracy achieved by CLIP suggests that integrating vision transformers with extensive pre-training on diverse image-text data can significantly enhance model performance for complex tasks like image reconstruction from EEG signals. Additional results can be found in Appendix A.4."}, {"title": "6 Discussion and Conclusion", "content": "In this section, we outline the limitations of our dataset and explore how it could guide future research efforts to advance machine learning and brain-computer interface design.\nLimitation. Firstly, while our dataset is more comprehensive than similar works, each participant's data is still relatively limited. This necessitates the development of inter-subject models to overcome this limitation and enhance generalizability. Additionally, it is limited in representation, as participants"}, {"title": "Insight for ML", "content": "The EEG-ImageNet dataset provides a comprehensive resource for developing models in visual recognition tasks, enabling the development of sophisticated deep-learning models capable of capturing intricate patterns within EEG data. Future research could leverage the dataset to enhance domain adaptation and transfer learning techniques, facilitating effective inter-subject task completion. Researchers might develop state-of-the-art models for the benchmarks we defined, or even create new tasks. By offering a diverse set of visual stimuli and supporting multi-level classification tasks, EEG-ImageNet could foster the creation of hierarchical models that mirror human cognitive processes and improve the generalization capabilities of machine learning algorithms."}, {"title": "Insight for BCI", "content": "As hardware technology progresses, portable EEG devices are becoming increasingly feasible, offering new opportunities for real-time BCI applications. Researchers could use the dataset to develop robust BCI systems that accurately interpret user intent from EEG signals. The comprehensive size and diverse visual stimuli in EEG-ImageNet allow for the creation of adaptive BCI systems that learn and respond to individual user patterns. This paves the way for personalized neurotechnology solutions, particularly enhancing human-computer interaction for individuals with disabilities. Furthermore, addressing privacy protection and ethical concerns will be crucial as BCI technology advances, ensuring user data is securely handled and individual rights are respected."}, {"title": "A Appendix", "content": "A.1 Additional Information about Dataset\nThe specific statistics of the dataset are shown in Table 4.\nAs shown in Listing 1, the EEG-ImageNet dataset storage format is provided. The dataset can be accessed through the cloud storage link available in our GitHub repository https://github. com/Promise-Z5Q2SQ/EEG-ImageNet-Dataset. Due to file size limitations on the cloud storage platform, we split the dataset into two parts: \u201cEEG-ImageNet_1.pth\u201d and \u201cEEG-ImageNet_2.pth\", each containing data from 8 participants. Users can choose to use only one of the parts based on their specific needs or device limitations.\nA.2 Apparatus\nAll the image stimuli are presented on a desktop computer that has a 27-inch monitor with a resolution of 2,560x1,440 pixels and a refresh rate of 60 Hz. Participants are required to use the keyboard to interact with the platform. EEG signals are captured and amplified using a Scan NuAmps Express system (Compumedics Ltd., VIC, Australia) and a 64-channel Quik-Cap (Compumedical NeuroScan). A laptop computer functions as a server to record EEG signals and triggers using Curry8 software. Throughout the experiment, electrode-scalp impedance is maintained under 50\u03a9, and the sampling rate is set at 1,000Hz.\nA.3 Experimental Setup Details\nIn the object classification task, we conduct experiments under three different granularity settings: the \"all\" task includes all 80 categories; the \"coarse\" task includes 40 coarse-grained categories; and the \"fine\" task includes 8 fine-grained categories that belong to the same parent node, with the average accuracy calculated across 5 groups. Each classification model maintain parameter consistency across the three tasks. We train one model per participant, ensuring that the parameters are consistent between models for different participants as well.\nThe model structures and hyperparameters are as follows. For SVM, we try linear, polynomial, and radial basis function (RBF) kernels. The regularization parameter is tested from values {10-3,10-2,10-1, 1, 101, 102, 103}. For RandomForest, we try to set the number of trees in the"}, {"title": "A.5 Temporal Effect", "content": "In our experimental paradigm, to reduce the cognitive load on participants, we group images of the same category together and use the RSVP (Rapid Serial Visual Presentation) paradigm for continuous rapid stimulation. This approach may cause the model to learn temporal continuity features rather than the intrinsic characteristics of the category stimuli. In this study, we use narrower cropped time segments as input features and divide the training and test sets based on temporal order to reduce the impact of temporal effects. In Figure 6, we plotted the average classification accuracy for images at different index positions in the test set under various training and test set splits.\nWe observed that the first few images in the test set have significantly higher accuracy, indicating a strong temporal effect, while the accuracy tends to stabilize for the subsequent images. Next, we also used several statistical methods for analysis. For the 30-20 split, we first calculated the sliding window standard deviation to measure local volatility. With a sliding window size of 5, the standard deviation at the 14th data point was less than 0.01, indicating a convergence trend at this point. We"}]}