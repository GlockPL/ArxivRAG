{"title": "Unlearning Clients, Features and Samples in Vertical Federated Learning", "authors": ["Ayush K. Varshney", "Konstantinos Vandikas", "Vicen\u00e7 Torra"], "abstract": "Federated Learning (FL) has emerged as a prominent distributed learning paradigm that allows multiple users to collaboratively train a model without sharing their data thus preserving privacy. Within the scope of privacy preservation, information privacy regulations such as GDPR entitle users to request the removal (or unlearning) of their contribution from a service that is hosting the model. For this purpose, a server hosting an ML model must be able to unlearn certain information in cases such as copyright infringement or security issues that can make the model vulnerable or impact the performance of a service based on that model. While most unlearning approaches in FL focus on Horizontal Federated Learning (HFL), where clients share the feature space and the global model, Vertical Federated Learning (VFL) has received less attention from the research community. VFL involves clients (passive parties) sharing the sample space among them while not having access to the labels. In this paper, we explore unlearning in VFL from three perspectives: unlearning passive parties, unlearning features, and unlearning samples. To unlearn passive parties and features we introduce VFU-KD which is based on knowledge distillation (KD) while to unlearn samples, VFU-GA is introduced which is based on gradient ascent (GA). To provide evidence of approximate unlearning, we utilize Membership Inference Attack (MIA) to audit the effectiveness of our unlearning approach. Our experiments across six tabular datasets and two image datasets demonstrate that VFU-KD and VFU-GA achieve performance comparable to or better than both retraining from scratch and the benchmark R2S method in many cases, with improvements of (0 \u2013 2%). In the remaining cases, utility scores remain comparable, with a modest utility loss ranging from 1 \u2013 5%. Unlike existing methods, VFU-KD and VFU-GA require no communication between active and passive parties during unlearning. However, they do require the active party to store the previously communicated embeddings.", "sections": [{"title": "1 INTRODUCTION", "content": "Machine Learning (ML) models have established themselves as the prominent artificial intelligence (AI) approach due to their ability to learn complex patterns from large-scale user data. The huge amount of data used to train these models is often sensitive in nature. To safe guard the information and privacy of the users, information privacy regulations such as GDPR, and CCPA have been proposed. These regulations allow users the right to be forgotten, i.e., to remove their data and its influence from the ML model.\nRemoving the influence of the data or Unlearning the data is a challenging task. A naive approach is to retrain the model in the ab- sence of the data to remove. This approach can be time-consuming and assumes that the original data is available. A more attractive approach should remove the data and its influence without the need of retraining from scratch. The objective is to modify the model pa- rameters of a ML model in such a way that the modified parameters are the same as those parameters of a model that would have been retrained from scratch with an original dataset deprived of the data to be forgotten or unlearned. However, achieving such an objective can be computationally expensive. An approach proposed by Bour- toule et al. [3] divides the dataset into shards, retraining only the shard containing the data to be removed upon receiving an unlearn- ing request. While this method minimizes the scope of retraining, it may struggle to capture complex relationships across shards and be- comes computationally expensive for frequent unlearning requests. To address such challenges, Ginart et al. [9] introduced the concept of approximate unlearning, where model parameters are adjusted to closely approximate those of a retrained model, reducing the need for full retraining. This concept has since been extended in several studies. For instance, Halimi et al. [12] propose updating the model using gradient ascent on the data to be forgotten. Wu et al. [35] leverage knowledge distillation for unlearning specific samples, while Tarun et al. [29] refine model parameters through modified fine-tuning for efficient unlearning.\nThe majority of existing approaches in machine unlearning op- erate under the assumption that the original dataset, or the specific data points to be removed, are readily available, refer [39] for more information. These methods typically rely on access to the dataset for performing the unlearning process, which involves directly manipulating or retraining the model on the modified data. This assumption simplifies the unlearning task but may not be prac- tical or feasible in many real-world scenarios where data access is restricted due to privacy concerns, regulatory requirements, or logistical constraints.\nIn a distributed paradigm like Federated Learning (FL) [25], clients collaboratively train a ML model without sharing their data. They only communicate the model weights to a server, which in"}, {"title": "2 BACKGROUND", "content": "2.1 Vertical federated learning\nThe demand for VFL has grown rapidly in recent years [22]. VFL is a distributed learning paradigm that allows organizations with distinct feature spaces to collaboratively train ML model while safeguarding the privacy of their raw data. This approach is partic- ularly valuable in scenarios where data privacy is critical, such as in healthcare [38], finance [40], and cross-enterprise collaborations [23]. In these domains, institutions often possess complementary datasets but are constrained from sharing them due to regulatory requirements or competitive concerns. In a typical VFL setup, there are N passive parties (i.e., clients) that own data but lack access to the labels, and a single active party, typically the server or a trusted third party, that holds the labels as shown in Fig. 1. This en- sures that the learning process can leverage distributed data sources while maintaining strict privacy guarantees. Organizations and in- stitutions with limited and fragmented datasets constantly seek data partners to collaboratively train ML models to maximize data utilization [19]. VFL requires all participants to share sample space but allows for different feature spaces. This essentially suggests"}, {"title": "2.2 Knowledge distillation", "content": "KD approaches has been used in the literature for their ability to transfer knowledge from a bigger teacher model to a smaller student model [13]. The simple idea of KD is that student model tries to mimic the output probabilities of the teacher model. Usually, student model learns from its own loss and its divergence from the teacher's logits. For a labeled dataset with \u0177 as true labels, the loss function for the student model can be given as follows:\n$L = (1 - \\alpha)L_{pred}(\\hat{y}, \\hat{y}_{student}) + \\alpha KL\\_DIV(\\hat{y}_{student}, \\hat{y}_{teacher})$ (1)\nhere \u03b1 is used to manage the trade-off between distillation loss and prediction loss; \u0177student, \u0177teacher are the logits of the student and teacher models respectively; and KL_DIV() is the KL diver- gence between them. Please refer [10] for a discussion of recent advancements in the area of KD."}, {"title": "2.3 Gradient ascent", "content": "Gradient ascent is the counterpart of gradient descent which is the typical optimization approach used to train a machine learning model. In gradient descent, the objective is to minimize the loss function for a given set of samples, whereas in gradient ascent (GA), the goal is to maximize the loss function for those samples. This approach is particularly useful for unlearning, as it allows for the approximate removal of specific samples by adjusting the model weights in the direction that maximizes the loss on the target samples [29], [30]. Let \u03b8 represent the model weight, \u03b7 be the learning rate and L be the loss function, then Gradient Ascent (GA) iteratively updates the model weights in the following manner.\n$\\theta_{\\tau+1} = \\theta_{\\tau} + \\eta \\frac{\\delta L}{\\delta \\theta_{\\tau}}$ (2)"}, {"title": "2.4 Membership inference attack", "content": "MIA enables adversaries to determine whether a particular record was part of the training set in a ML model. MIA leverages the mem- orization of ML models i.e, ML models behave differently on the data seen during training. Consequently by analyzing the model's"}, {"title": "3 PROPOSED WORK", "content": "In this section, we present the details of the proposed unlearning framework for passive party unlearning, sample unlearning and fea- ture unlearning in VFL. We also provide the details of MIA used to audit unlearning in our framework. Unlearning is a crucial capabil- ity for addressing a variety of issues, including privacy compliance, security, and adaptability. Existing approaches either violate VFL constraints or are expensive in terms of communication for pas- sive party unlearning. None of the approaches (to the best of our knowledge) focuses on feature unlearning in VFL.\nAlgorithm 1 shows the generic VFL framework with K passive parties, and Kth party being the active party as well. Let G() be a function which takes model parameters \u03b8, and minibatch x as input and returns the embeddings from the model.\nPassive parties have their local models \u03b81, \u03b82, ..., \u03b8\u03ba and \u0398K is the active model. For each batch, passive parties do a forward pass and communicate their embeddings to the server. Server in turn trains its local model and forward the gradients with respect to\neach embedding ($L = \\frac{\\delta L}{\\delta \\Theta_K}$, $\\frac{\\delta L}{\\delta H_K}$,$\\frac{\\delta L}{\\delta \\theta_K}$), So, L being the loss func- tion) back to their respective passive party. Algorithm 1 requires parameters \u03b71, \u03b72 which needs to be calibrated to have a successful learning. However this can be eliminated if we assume the loss L to be twice differential and strictly convex, then the parameter update can be written as:\n$\\Theta_{K}^{t+1} = \\Theta_{K}^{t} - \\eta_{1}H^{-1}\\frac{\\delta L}{\\delta \\Theta_{K}}$ (3)\nAnd for passive parties:\n$\\theta_{K}^{t+1} = \\theta_{K}^{t} - \\eta_{2}H^{-1}\\frac{\\delta L}{\\delta \\theta_{K}}$ (4)\nwhere H is the hessian matrix for each model. [33] suggests com- puting and storing comes at an additional computational cost of O(np\u00b2 + p\u00b3) and O(p2) respectively. We have considered both the approaches i.e., using \u03b71, \u03b72 and H\u00af\u00b9 for the respective models.\nTo unlearn a passive party, the active party must remove the contribution of the target passive party from all of its historical embeddings, i.e., the active party updates H = H \\ Hu in all the training rounds, Hu being the target passive party. In our work, we use the KD approach as the unlearning mechanism as it can deal with model compression, and since the active party already has the embeddings from the previous rounds, the active party does not need to have any communication between active and passive parties.\nIn our approach, we first randomly initialize a new student model to eliminate any previous information, and assign the old active model as the teacher model. The active party updates its student model based on the historical embeddings from the rest of the clients. The loss for the student model is the combination of predic- tion loss and the distillation loss.\n$L = \\alpha * L_{distil} + (1 - \\alpha) * L_{pred}$ (5)"}, {"title": "4 EXPERIMENTAL ANALYSIS", "content": "In this section, we present the experimental analysis of our pro- posed unlearning framework. As discussed in Section 2.1, the most common VFL setting typically involves two parties, with a max- imum of four parties. For this paper, we consider a three-party VFL setup consisting of clientA, clientB, and clientC, collaborating to train a joint VFL model. The training process spans 50 epochs, with clientA having the flexibility to request unlearning at any point during the training process. Since, communication in VFL is communication-intensive, using a larger batch size is preferred to optimize efficiency [8]. Accordingly, we set the batch size to 512 in our experiments. The learning rate is set to 10-2 for tabu- lar data and 10-\u00b3 for image data for both active and passive par- ties. Additionally, the distillation parameter which controls the trade-off between actual loss and distillation loss is set to 0.3. To demonstrate the effectiveness of our approach in unlearning at any stage during training, we conduct experiments at various epochs: [5th, 15th, 25th, 35th, 45th]. After confirming the effectiveness and feasibility of our unlearning method, we fix the unlearning epoch at the 25th epoch for further evaluation. Each experiment is then repeated three times to account for variability and capture uncer- tainty in the results.\nIn our experiments, we have considered 6 tabular datasets, namely Adult, ai4i, hepmass, susy, and wine dataset from UCI repository [16], poqemon dataset [1] and 2 image dataset CIFAR10 [17] and STL10 dataset from [5]. For tabular datasets, passive models have a single hidden layer models with 8 hidden neurons, and active model is also a single hidden layer model with 32 neurons followed by an output layer. The features are distributed equally among pas- sive parties. For example, wine dataset has 12 features, clientA has first 4, clientB has next 4 and clientC has last 4 features. For image datasets, clients have resnet-18 model as the passive model, and the active model is a single hidden layer model with 512 neurons followed by an output layer. The number of neurons in output layer for active model depends on the output classes of the datasets, e.g., 2 for adult, 5 for poqemon, 10 for CIFAR10.\nIt is important to emphasize that these experimental setups are not optimized for peak performance. Our primary objective is to showcase the effects of unlearning on both tabular and im- age datasets. The parameters were selected arbitrarily and are not finetuned for optimal results on each dataset. Fine-tuning the ex- perimental configurations for optimal results on each dataset is out-of-scope for this work."}, {"title": "4.1 Passive party unlearning", "content": "We compare our approach with the gold standard i.e., a retrained model from scratch and a benchmark R2S fast retraining unlearn- ing approach [32]. For passive party unlearning, the R2S method is"}, {"title": "4.2 Feature unlearning", "content": "Now that we have shown the feasibility and effectiveness of our approach specially at the later epochs. We fix the unlearning epoch at 25th for feature and sample unlearning. For feature unlearning, we have removed the most important and least important features from tabular datasets. The importance of the feature is computed with feature ablation\u00b2 (feature importance for all the tabular dataset is available in supplementary material). Fig. 6 shows the training"}, {"title": "4.3 Sample unlearning", "content": "For sample unlearning, we removed 5 batches from the active model for each dataset. The number of gradient ascent steps was set to 5, chosen arbitrarily. This parameter can be adjusted and increased based on the unlearning requirements of the application. Fig. 9 shows the comparison of training and test loss curves between our approach, retrained from scratch method and R2S method and Table 7 shows the utility scores (F1 and AUC score) when unlearn- ing happened at 25th epoch. The results show that VFU-GA has better utility score than retrained model. Notably, in the case of the Poqemon dataset, VFU-GA demonstrates significantly superior"}, {"title": "4.4 Auditing VFU-KD", "content": "In this section, we evaluate the effectiveness of the unlearning process using a MIA. As discussed in Section 3.1, we train an MIA model with the output logits for 10 epochs both in the presence and absence of clientA. The MIA model consists of a single hidden layer comprising 32 neurons. The output layer of the MIA model is a binary classifier which predicts whether clientA was present during training or not.\nFig. 8 shows the accuracy of the MIA on the tabular dataset starting from epoch 10 onwards. The results clearly demonstrate a significant drop in MIA accuracy at the 25th epoch, thereby in- dicating the effect of the unlearning process. Similar results were observed in Fig. 10 on all the datasets for MIA attack accuracy. However, the drop in accuracy can vary with the impact of samples unlearned."}, {"title": "4.5 Limitations", "content": "Based on our analysis and the results obtained, we highlight the following limitations of our approach.\n(1) Attack Vulnerability: Dishonest or honest-but-curious pas- sive parties could potentially exploit the spikes caused by distillation to perform membership inference attacks or gra- dient based attacks."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduced a framework for unlearning in vertical federated learning (VFL), focusing on passive party unlearning and feature unlearning using knowledge distillation, termed VFU-KD, and sample unlearning using gradient ascent, termed VFU-GA. VFL is inherently communication intensive. Thus, an effective unlearn- ing approach should aim to minimize the communication between the active and passive parties. In VFU-KD, the active party is re- sponsible for passive party unlearning, while the respective passive party handles feature unlearning. VFU-KD leverages knowledge distillation for effective model compression and unlearning. On the other hand, since sample unlearning does not require model com- pression, gradient ascent provides a more computationally efficient option in VFU-GA.\nOur approach does not require any communication between active party and passive party for unlearning. However, it requires that the active party stores the communicated embeddings. This is essential in order to not have any communication. We have also proposed a MIA which can be used to audit unlearning in VFL. We have compared VFU-KD, VFU-GA with the gold standard unlearn- ing model i.e., model retrained from scratch and R2S optimization based faster retraining, on 6 tabular datasets, and 2 image dataset. The results demonstrate that, with our approach both active and passive parties can perform unlearning without any significant utility loss.\nIn our experiments, we employed a simple binary classifier for the membership inference attack (MIA). However, leveraging a more advanced and robust MIA model, such as the one proposed in [4], could potentially yield more insightful and accurate results. Exploring this avenue remains a priority for future work. Addition- ally, investigating the relationship between the distillation-induced spikes and their susceptibility to membership inference attacks presents an intriguing research direction. Another interesting direc- tion for future work is to develop strategies to reduce the storage overhead for the active party, further enhancing the efficiency and scalability of the approach."}]}