{"title": "Recurrent Stochastic Configuration Networks with Incremental Blocks", "authors": ["Gang Dang", "Dianhui Wang"], "abstract": "Recurrent stochastic configuration networks (RSCNs) have shown promise in modelling nonlinear dynamic systems with order uncertainty due to their advantages of easy implementation, less human intervention, and strong approximation capability. This paper develops the original RSCNs with block increments, termed block RSCNs (BRSCNs), to further enhance the learning capacity and efficiency of the network. BRSCNs can simultaneously add multiple reservoir nodes (subreservoirs) during the construction. Each subreservoir is configured with a unique structure in the light of a supervisory mechanism, ensuring the universal approximation property. The reservoir feedback matrix is appropriately scaled to guarantee the echo state property of the network. Furthermore, the output weights are updated online using a projection algorithm, and the persistent excitation conditions that facilitate parameter convergence are also established. Numerical results over a time series prediction, a nonlinear system identification task, and two industrial data predictive analyses demonstrate that the proposed BRSCN performs favourably in terms of modelling efficiency, learning, and generalization performance, highlighting their significant potential for coping with complex dynamics.", "sections": [{"title": "I. INTRODUCTION", "content": "NOWDAYS, using neural networks (NNs) to analyze nonlinear dynamic systems has received considerable attention [1]-[3]. As a class of data-driven techniques, the per- formance of NNs is significantly influenced by input variables. However, due to the uncertainties or changes in the controlled plant and external disturbances, the input order often varies over time, leading to systems with unknown dynamic orders. Recurrent neural networks (RNNs) have feedback connections between neurons, which can store the historical information and use them to handle the uncertainty caused by the selected input variables. Unfortunately, RNNs employ the error back- propagation (BP) algorithm to train the network, which suffers from the sensitivity of learning rate, slow convergence, and local minima [4]-[6]. Reservoir computing (RC) provides an alternative scheme for training RNNs, which utilizes a large-scale sparsely connected reservoir to capture the state information and obtain an adaptable readout [7], [8]. As a class of randomized learning algorithms, RC effectively overcomes the limitations of gradient-based methods and encompasses various versions of RNNs such as echo state networks (ESNs) [9] and liquid state machines (LSMs) [10].\nIn recent years, ESNs have been widely applied for tackling complex dynamics due to their computational simplicity, fast learning speed, and strong nonlinear processing capability [11]-[13]. Distinguished from other RC approaches, ESNS have the echo state property (ESP), that is, the reservoir state $x(n)$ is the echo of the input and $x(n)$ should asymp- totically depend only on the driving input signal [9]. This unique characteristic makes them well-suited for temporal data analysis. However, ESNs face challenges in random parame- ter selection and structure setting, significantly affecting the model performance. To address the issues of structural design, researchers have introduced several approaches, including the simple circular reservoir (SCR) to minimize computational complexity while maintaining effectiveness [14], the leaky integrator ESN (LIESN) to enhance the model flexibility [15], deep ESNs to enrich the feature representation by construct- ing stacked structures [16], [17], and pruning and growing strategies to adaptively adjust the reservoir topology [18], [19]. Nevertheless, properly setting the learning parameters for these methods is quite challenging in practical applications. While some optimization algorithms can aid in obtaining improved network parameters [20]-[22], they are constrained by complex iterations and their sensitivity to the initial state and learning rate. Furthermore, the aforementioned approaches cannot guarantee the model's universal approximation prop- erty, which is essential for data modelling theory and practice. According to [23], [24], a randomized learner model exhibits excellent learning and generalization performance when it is incrementally built using a data-dependent random parame- ter scope, with the structural construction guided by certain theoretical principles.\nIn 2017, Wang and Li pioneered an innovative random- ized learner model, termed stochastic configuration networks (SCNs) [25], which randomly assign the weights and biases in the light of a supervisory mechanism. Built on the SCN concept, in [26], we introduced a recurrent version of SCNS"}, {"title": "II. RELATED WORK", "content": "In this section, two related works are introduced, including the well-known echo state networks and recurrent stochastic configuration networks."}, {"title": "A. Echo state networks", "content": "The ESN can be regarded as a simplified version of RNN, which utilizes a large-scale sparsely connected reservoir to transform input signals into a high-dimensional state space [9]. The random parameters are generated from a fixed uniform distribution and remain constant during training. Only the output weights need to be calculated by the least square method.\nGiven an ESN model,\n$x(n) = g(W_{in}u(n) + W_rx(n - 1) + b)$,\n$y(n) = W_{out} (x(n), u(n)),$\nwhere $u(n) \\in \\mathbb{R}^K$ is the input signal; $x(n) \\in \\mathbb{R}^N$ is the internal state of the reservoir; $W_{in} \\in \\mathbb{R}^{N\\times K}$, $W_r\\in \\mathbb{R}^{N\\times N}$ represent the input and reservoir weights, respectively; $b$ is the bias; $W_{out} \\in \\mathbb{R}^{L\\times (N+K)}$ is the output weight; $K$ and $L$ are the dimensions of input and output; and $g$ is the acti- vation function. $W_{in}, W_r, b$ are generated from the uniform distribution $[-\\lambda, \\lambda]$. The value of $\\lambda$ has a significant impact on the model performance. The original ESNs use a fixed $\\lambda$, which may lead to poor performance. Scholars have focused on optimizing the weight scope, and some promising results have been reported in [21], [22]. However, the optimization process inevitably increases the complexity of the algorithm. Therefore, selecting a data-dependent and adjustable $\\lambda$ is crucial to improve the effectiveness and efficiency of the resulting model.\nDefine $X= [(x(1), u(1)), ..., (x(n_{max}), u(n_{max}))]$, where $n_{max}$ is the number of training samples and the output is\n$Y = [y (1), y (2), ..., y (n_{max})] = W_{out} X.$\nThe output weight $W_{out}$ can be calculated by the least square method, that is,\n$W_{out} = (XX^T)^{-1}XT$,\nwhere $T = [t (1), t (2), ...t (N_{max})]$ is the desired output. $x(0)$ usually starts with a zero matrix, and a few warm-up samples are used to minimize the influence of the initial states."}, {"title": "B. Recurrent stochastic configuration networks", "content": "This section reviews our proposed RSCNs [26], in which the random weights and biases are assigned in the light of a supervisory mechanism. This innovative learning scheme effectively addresses the issue of randomized neural network parameter selection and structure design, while theoretically ensuring the universal approximation performance of the net- work. With benefits such as high learning efficiency, less human intervention, and strong approximation ability, RSCNS have demonstrated promising potential for modelling complex dynamics."}, {"title": "III. BLOCK INCREMENTAL RECURRENT STOCHASTIC CONFIGURATION NETWORKS", "content": "This section details the proposed BRSCNs, including the algorithm description and proofs of the echo state property and the universal approximation property."}, {"title": "A. Algorithm description", "content": "As shown in Fig. 2, the BRSCN adds reservoir nodes with block increments, and each block can be viewed as a subreservoir. Given the input $U= [u(1), . . ., u(n_{max})]$ and desired output $T = [t (1), ...t (N_{max})]$, assume that the first subreservoir with $N$ nodes has been built, that is,\n$x^{(1)} (n) = g(W_{in, N}u(n) + W_{r,N}x^{(1)} (n - 1) + b_N)$,\n$Y = W_{out}X^{(1)}$,\nwhere $W_{in, N}, W_{r,N}, b$, and $x^{(1)}(n)$ are the input weight, reservoir weight, bias, and reservoir state of the first sub- reservoir, respectively. $W_{out}$ is the output weight, and $X^{(1)}= [x^{(1)} (1), x^{(1)} (2),...,x^{(1)} (n_{max})].$\nLet $j = 1$ and calculate the current error $e_0 := e_j = Y - T$. If $||e_0||_F > \\varepsilon$, we need to add sub- reservoirs under the supervisory mechanism until satisfy- ing termination conditions. Assign $W_{in, N}^{(j+1)}, W_{r, N}^{(j+1)}$ and $b_N^{(j+1)}$ stochastically in $G_{max}$ times from an adjustable uni- form distribution $[-\\lambda, \\lambda]$ to obtain the candidates of the subreservoir state $X_1^{(j+1)}, X_2^{(j+1)},..., X_{G_{max}}^{(j+1)}$. Sub- stitute $X_1^{(j+1)}, X_2^{(j+1)}, ..., X_{G_{max}}^{(j+1)}$ into the following inequality constraint:\n$(1-r - \\mu_{j+1}) ||e_{j,q}||^2 - \\frac{(e_{j,q}, g_{j,i})^2}{g_{j,i}^Tg_{j,i}} + (\\mu_{j+1} + \\Omega)  \\leq 0,$\n$q = 1,2,..., L, i = 1,2,..., G_{max},$\nwhere {$\\mu_{j+1}$} is a non-negative real sequence satisfies $\\lim_{j\\rightarrow \\infty} \\mu_{j+1} = 0$ and $\\mu_{j+1} \\leq (1 - r)$.\nRemark 2: To ensure the echo state property of the model, the reservoir weight $W_{r, N}^{(j+1)}$ needs to be scaled by\n$W_{r, N}^{(j+1)} = \\frac{\\alpha_{j+1}}{ \\rho_{max}} W_{r, N}^{(j+1)}$,\nwhere $0 < \\alpha < 1$ is the scaling factor, and $\\rho_{max}$ is the maximum eigenvalue value of $W_{r, N}^{(j+1)}$.\nSeek the subreservoirs that satisfy Eq. (12) and define a set of variables $$\\Xi_{j+1}= [\\xi_{j+1,1}, \\xi_{j+1,2}, ..., \\xi_{j+1,L}],$\n$\\xi_{j+1,q} = \\frac{(e_{j,q}, X_i^{(j+1)})^2}{(X_i^{(j+1)}, X_i^{(j+1)})} - (1 - r - \\mu_{j+1}) e_{j,q}^Te_{j,q}.$\nA larger positive value of $\\sum_{q=1}^L \\xi_{j+1,q}$ implies a better configuration of the adding subreservoir.\nCalculate the current training and validation residual error $e_{j+1}$ and $eval,_{j+1}$, and a step size $J_{step}$ ($J_{step} < j$) is used in the early stopping criterion, that is,\n$||eval,_{j-j_{step}} ||_F \\leq ||eval,_{j-j_{step}+1} ||_F \\leq... \\leq ||eval,_{j}||_F.$\nRenew $e_0 := e_{j+1}, j = j + 1$, and continue to add subreservoirs until $||e_0||_F \\leq \\varepsilon$ or $j > J_{max}$ or Eq. (15) is met. The general construction process can be summarized as follows:\n$x^{(1)} (n)$\n$x^{(2)} (n)$\n$W_{in, N}^{(1)}$\n$W_{in, N}^{(2)}$\n= g\\left[ \\begin{array}{c}\n0\n...\\\\\nW_{r, N}^{(1)} \\\\\n\\end{array} \\right]$\n... W_{in, N}^{(2)} \\\\\nu(n)+\\left[\\begin{array}{c} x^{(1)} (n - 1) \\\nx^{(2)} (n - 1) \\\n...\\\\\nx^{(j)} (n - 1)\n\\end{array}\\right] + \\left[ \\begin{array}{c}b_N^{(1)}\nb_N^{(2)}\n... \\\\b_N^{(j)}\n\\end{array}\\right]$\n$x^{(j)} (n)$\n0\n0\n0\n... W_{in, N}^{(j)}\n0\n0\nW_{r, N}^{(1)}\n...\nx(1) (n - 1)\nb1) \\\\\n+ \\left[ \\begin{array}{c} x(2) (n - 1) \\\nb(2) \\\\\\n...\\\\\n\\end{array} \\right]$ Y = $W_{out} \\left[ X^{(1)} X^{(2)} ... X^{(j)}\\right] $\nwhere $W_{out}^{(k)}$ is the output weight corresponding to $j$-th subreservoir.\nFinally, we have $\\lim_{j\\rightarrow \\infty} ||T - F_j || = 0$, where $F_j$ is the final output with $j$ subreservoirs. A complete algorithm description of the proposed BRSCNs is summarized in Algorithm 1."}, {"title": "B. The echo state property for BRSCN", "content": "One key distinguishing feature of ESN compared to other RC approaches is its echo state property. This unique property ensures that as the input sequence length approaches infinity, the discrepancy between two reservoir states driven by the same input sequence but with different initial conditions becomes negligible. The reliance on the initial state $x (0) diminished over time. Considering an ESN without output feedback, if the maximum singular value of the reservoir weight matrix is less than 1, the resulting learner model exhibits the echo state property [9].\nTheorem 1. Given a BRSCN with J subreservoirs, the maximum singular values of each subreservoir weight matrix are $\\rho_{max, 1},..., \\rho_{max, J}$. If the scaling factor in Eq. (13) is selected as\n$0 < \\alpha < \\frac{\\rho_{min}}{ \\rho_{max}^{(W_r)}}$,\nthe built model holds the echo state property.\nProof. According to Eq. (16), the reservoir weight matrix can be written as:\nW_r = \\left[ \\begin{array}{cccc} W_r = \\left[ \\begin{array}{cccc} W_{r,N}^{(1)} & 0 & 0 & 0 \\\n0 & W_{r,N}^{(2)} & 0 & 0 \\\n: & : & : & : \\\n0 & 0 & 0 & W_{r,N}^{(J)} \\\\\n\\end{array} \\right] = diag \\left{ W_{r,N}^{(1)}, W_{r,N}^{(2)},..., W_{r,N}^{(J)} \\right\\}\n= diag \\{P_1S_1Q_1, ..., P_JS_JQ_J\\} = P'diag \\{S_1, ..., S_J \\} Q'\n= P'diag \\{\\sigma_1^1, \\sigma_2^1, ..., \\sigma_N^1,..., \\sigma_1^J, \\sigma_2^J,...,\\sigma_N^J\\} Q',\nwhere $P$ and $Q$ are orthogonal matrices generated by SVD decomposition, $P' = diag \\{P_1,..., P_J\\}, Q' = diag \\{Q_1,..., Q_J\\}, and $S_j = diag{\\{\\sigma_1^j,\\sigma_2^j,....,\\sigma_N^j\\}$ is composed of the singular values of the $j$-th subreservoir with $N$ nodes. Thus, it can be inferred that $diag \\{S_1,..., S_J\\}$ has the same singular values with $W_r$. Observe that\n$\\sigma_{max} \\leq \\alpha \\frac{\\rho_{max} \\left(W_{r,N}^{(2)}\\right)}{ \\rho_{max} \\left(W_{r,N}^{(1)}\\right)} \\cdot \\cdot \\cdot \\frac{\\rho_{max} \\left(W_{r,N}^{(j)}\\right)}{ \\rho_{max} \\left(W_{r,N}^{(1)}\\right)} \\frac{ \\rho_{max}}{ \\rho_{max} \\left(W_{r,N}^{(1)}\\right)}= 1.$\nThen, we can easily obtain the maximum singular value of $W_r$ is less than 1, which completes the proof."}, {"title": "C. The universal approximation property for BRSCN", "content": "BRSCNS generate subreservoirs in the light of the su- pervisory mechanism, enabling the network to effectively approximate any nonlinear mappings. This subsection presents the theoretical result of its universal approximation property, which is an extension proposed in [28].\nGiven a cost function\n$J_{W_{out}} = ||T - F_{j+1}||^2$\n$= ||T - F_j - W_{out}^{(j+1)}X^{(j+1)}||$\n$= \\sum_{q=1}^L ||e_{j,q} - W_{out,q}^{(j+1)}X^{(j+1)}||^2$\n$= \\sum_{q=1}^L (e_{j,q} - W_{out,q}^{(j+1)}X^{(j+1)})^2$\n$= || e_j ||^2 - 2\\sum_{q=1}^L  (e_{j,q}, W_{out,q}^{(j+1)}X^{(j+1)})  + \\sum_{q=1}^L (W_{out,q}^{(j+1)}X^{(j+1)}) (W_{out,q}^{(j+1)}X^{(j+1)})^T\\$\n$= || e_j ||^2 - 2\\sum_{q=1}^L (e_{j,q}, W_{out,q}^{(j+1)}X^{(j+1)})+ \\sum_{q=1}^L (W_{out,q}^{(j+1)}X^{(j+1)}) (W_{out,q}^{(j+1)}X^{(j+1)})$\n$ - (W_{out,q}^{(j+1)}X^{(j+1)}),\\$\ntaking the derivative of Eq. (20) with respect to $W_{out,q}^{(j+1)}$, yields\n$\\frac{\\partial J}{\\partial W_{out,q}^{(j+1)}} = -2e_{j,q}X^{(j+1)}+ 2W_{out,q}^{(j+1)}X^{(j+1)}X^{(j+1)^T}.$\nThen, we have\n$W_{out,q}^{(j+1)} = e_{j,q}^TX^{(j+1)^T}(X^{(j+1)}X^{(j+1)^T})^{-1}.$\nTheorem 2. Assume span(\u0413) is dense on $L_2$ space. Given $0 < r < 1$ and a non-negative real sequence {$\\mu_{j+1}$} satisfies $\\lim_{j\\rightarrow \\infty} \\mu_{j+1} = 0$ and $\\mu_{j+1} \\leq (1 - r)$. For $j = 1,2..., and q = 1, 2, ..., L, define\n$\\delta_{j+1,q} = (1 - r - \\mu_{j+1}) ||e_{j,q}||^2,$\n$\\sum_{j+1} = \\sum_{q=1}^L \\delta_{j+1,q}.$\nIf the subreservoir state $X^{(j+1)}$ is generated by the following inequality constraint:\n$(e_{j,q}, X_{out}^{(j+1)}X_{out}^{(j+1)^T}) \\geq \\delta_{j+1,q}^*,$\nand the output weights are evaluated by Eq. (16), we have\n$\\lim_{j\\rightarrow \\infty} ||T - F_{j+1}|| = 0.$\nProof. With simple computation, we have\n$||e_{j+1}||^2 = \\sum_{q=1}^L  \\left[e_{j,q} - W_{out,q}^{(j+1)}X^{(j+1)} \\right]^2 = \\sum_{q=1}^L ||e_{j,q}||^2 = (W_{out,q}^{(j+1)}X^{(j+1)}W_{out,q}^{(j+1)}X^{(j+1)}),$\n$ = \\sum_{q=1}^L \\left[(1 - r - \\mu_{j+1}) (e_{j,q}, e_{j,q}) - 2(W_{out,q}^{(j+1)}X^{(j+1)}, e_{j,q}) +(W_{out,q}^{(j+1)}X^{(j+1)}W_{out,q}^{(j+1)}X^{(j+1)}\\right],$\n$= \\sum_{q=1}^L (1 - r - \\mu_{j+1}) \\left[ \\sum_{q=1}^L (\\delta_{j+1,q} -2(W_{out,q}^{(j+1)}X^{(j+1)}W_{out,q}^{(j+1)}X^{(j+1)}) \\leq 0.$\nTherefore, $||e_{j+1}||^2 \\leq (r + \\mu_{j+1}) ||e_j||^2$ and we can easily establish\n$||e_{j+1}||^2 \\leq r ||e_j||^2 + \\mu_{j+1} || e_j||^2.$\nObviously, $\\lim_{j\\rightarrow \\infty} \\mu_{j+1} ||e_j||^2 = 0$. Combining Eq. (26) and 0 < r < 1, we can further obtain $\\lim_{j\\rightarrow \\infty} ||e_{j+1}||^2 = 0$, indicating $\\lim_{j\\rightarrow \\infty} ||e_j|| = 0$, which completes the proof."}, {"title": "IV. PARAMETERS LEARNING", "content": "In this section, an online parameter learning strategy based on the projection algorithm is provided. Moreover, we inves- tigate the persistent excitation conditions that facilitate pa- rameter convergence and present the corresponding theoretical results."}, {"title": "A. Online learning of parameters", "content": "Due to the dynamic changes in the actual industrial process, it is essential to update the model parameters timely to achieve a superior modelling performance using the updated model. Given a BRSCN model constructed by Eq. (16) and denoted by $g(n) = [x^{(1)} (n), ..., x^{(i)} (n)]$, a projection algorithm [27] is applied to update the output weights $W_{out}$.\nLet $H = \\{W_{out}: y(n) = W_{out}g(n)\\}$ and define $W_{out} (n - 1)$ as the output weight at the $(n - 1)$-th step. Choose the closest weight to $W_{out} (n - 1)$, and find $W_{out} (n)$ through minimizing the following cost function:\n$J = ||W_{out} (n) \u2013 W_{out} (n - 1) ||^2$\ns.t. $y (n) = W_{out} (n) g(n)$\nBy introducing the Lagrange operator $\\Lambda_p$, we have\n$J_e = || W_{out} (n) \u2013 W_{out} (n - 1)||^2 + \\Lambda_p (y (n) \u2013 W_{out} (n) g(n)).$\nThe necessary conditions for $J_e$ to be minimal are\n$\\frac{J_e}{W_{out} (n)} = 0$ and $\\frac{J_e}{\\partial \\Lambda_p} = 0$.\nThus, we can obtain\n$\\begin{cases}\nW_{out} (n) - W_{out} (n - 1) - \\Lambda_p g(n) = 0\\\\\ny (n) - W_{out} (n) g(n) = 0\n\\end{cases}$\n$\\Lambda_p = \\frac{y (n) - W_{out} (n - 1) g(n)}{g(n)g(n)}$.\nSubstituting Eq. (31) into Eq. (30), the online update rule for the output weights can be expressed as\nW_{out} (n) = W_{out} (n - 1) + \\frac{g(n)}{g(n)g(n)} (y (n) - W_{out} (n - 1) g(n)).\nTo prevent division by zero, a small constant c is added to the denominator. Furthermore, a coefficient $\\gamma > 0$ can be multiplied by the numerator to obtain the improved projection algorithm, that is,\nW_{out} (n) = W_{out} (n - 1) + \\frac{\\gamma g(n)}{c + g(n)g(n)} (y (n) - W_{out} (n - 1) g(n)).$\nRemark 3: The detailed stability and convergence analysis of the proposed approach based on the projection algorithm has been presented in our previous work [26], and an enhanced condition is introduced to further improve the model's stability. These theoretical results are crucial for evaluating whether the algorithm can achieve a stable solution over time, which directly influences the prediction performance and reliability of the model during the identification process."}, {"title": "B. Persistent excitation condition for parameter convergence", "content": "This subsection offers a persistent excitation condition for online learning based on the projection algorithm. It necessitates that the input sequence exhibits sufficient richness or diversity over an extended time window, facilitating a comprehensive understanding of each state of the system. This theoretical result is essential to ensure that the parameters converge during the update process, ultimately resulting in accurate estimates.\nTheorem 3. The convergence of parameters can be guar- anteed if the input signal satisfies the following persistent excitation conditions:\n$\\eta_1 \\geq \\int_{n_0}^{n_0+n_w} g(n)g(n)dn \\geq \\eta_2,$\n$\\Delta P^{-1}(n) \\leq 2 \\gamma \\eta_2 \u2013 \\gamma \\eta_1,$\nwhere $\\eta$ is a positive constant, $n_w$ is the length of the time window, $\\Delta P^{-1} (n) = P^{-1} (n) - P^{-1} (n - 1)$, and $P^{-1} (n) = c + g(n)g(n)^{-1}$.\nProof. The parameter estimation error can be calculated by\n$E (n - 1) = W_o - W_{out} (n - 1),$\nwhere $W_o$ is an ideal output weight satisfying $y (n) \\approx W_og(n)$. Combining Eq. (33) and Eq. (36), we have\n$E (n) = W_o - W_{out} (n)$\n$ = W_o - W_{out}(n - 1)$\n$+ \\frac{\\gamma g(n)}{c + g(n)g(n)} (y (n) - W_{out} (n - 1) g(n))$\n$ = W_o - W_{out}(n - 1)$\\n$-P(n)g(n) (W_og(n) \u2013 W_{out} (n - 1) g(n))$\n$= (1 - P(n)g(n) g(n)) E (n - 1).$\nDefine a Lyapunov function candidate, that is,\n$V (n) = E(n)P^{-1} (n) E (n).$\nThe change of $V (n)$ is denoted by $\\Delta V (n) = V (n) - V (n - 1)$. Combining Eq. (37) and Eq. (38), we can obtain\n$\\Delta V (n)$\n$= E(n) P^{-1} (n) E (n) \u2013 V (n \u2212 1)$\n$= \\left[ (I-P(n) g(n) g(n)) E (n-1)\\right]^T P^{-1} (n) \\left[ (I-P(n)g(n) g(n)) E (n - 1)\\right]$\n$E(n \u2212 1) P^{-1} (n \u2212 1) E (n \u2212 1)$\n$= E(n - 1)  P^{-1} (n) - P^{-1} (n - 1) - 2\\gamma g(n) g(n)$+$\\gamma^2P(n)g(n) g(n)P^{-1} (n) g(n) g(n) E (n - 1).$\nFrom Eq. (34), it can be shown that $\\eta_1 \\geq g(n) g(n) \\geq \\eta_2$ and we have\n$\\Delta V (n) \\leq E(n \u2212 1)  (P^{-1} (n) \u2013 P^{-1} (n \u2212 1)\n\u2013 2\\gamma \\eta_2 +\\gamma^2\\eta E (n \u2212 1).$\nAs Eq. (35) holds, we can easily obtain $\\Delta V (n) \\leq 0$. This demonstrates that updating the output weights based on the projection algorithm can guarantee the asymptotic convergence of the parameters, which completes the proof."}, {"title": "V. EXPERIMENT RESULTS", "content": "In this section, the effectiveness of the BRSCN is tested on the Mackey-Glass time series prediction, a nonlinear system identification task, and two industrial data predictive analyses. The performance of the proposed BRSCN is compared with the original ESN and RSCN, and two block incremental ESNs, that is, growing ESN (GESN) [19] and decoupled ESN (DESN) [29]. The output weights are updated online through the projection algorithm [27], in response to the dynamic variations within the system. The normalized root means square error (NRMSE) is used to evaluate the model performance, that is,\n$NRMSE = \\sqrt{\\frac{\\sum_{n=1}^{N_{max}} (y (n) - t (n))^2}{N_{max} var (t)}},$\nwhere var (t) denotes the variance of the desired output.\nThe key parameters are taken as follows: the sparsity of the reservoir weight is set to [0.01, 0.03], and the scaling factor of spectral radius $\\alpha$ ranges from 0.5 to 1. For the ESN, DESN, and GESN, the scope setting of input and reservoir weights are set with $\\lambda \\in [0.1, 1]$. Specifically, for the DESN and GESN, the subreservoir size is set to 10. For RSC-based frameworks, the following parameters are taken as weight scale sequence {0.5, 1, 5, 10, 30, 50, 100}, contractive sequence $r [0.9, 0.99, 0.999, 0.9999, 0.99999]$, the max- imum number of stochastic configurations $G_{max} = 100$, and training tolerance $\\varepsilon = 10^{-6}$. The initial reservoir size of RSCN is set to 5. The grid search method is used to identify the hyperparameters, including the reservoir size $N$ and the block size. Each experiment consists of 50 independent trials conducted under the same conditions, and the mean and standard deviation of NRMSE and training time are exploited to evaluate the model performance."}, {"title": "A. Mackey-Glass system (MGS)", "content": "MGS is a classical chaotic system, which has been widely used for time series prediction and nonlinear system identifi- cation. A standard MGS can be described by the differential equations with time delays, that is,\n$\\frac{du}{dn} = -vu (n) + \\frac{\\alpha u (n \u2013 \\tau)}{1 + u(n \u2013 \\tau)^{10}},$\nWhen $\\tau > 16.8$, the system transitions into a chaotic, aperiodic, non-convergent, and divergent state. According to [9], the parameters in Eq. (42) are set to The initial values ${y (0),...,y (7)}$ are selected from [0.1,1.3]. The inputs consist of ${y (n), y (n \u2212 6), y (n \u2212 12), y (n \u2212 18)}$, which are used to predict $y (n + 6)$. The second-order Runge-Kutta method is employed to generate 1177 sequence points. In our simulation, samples from time steps 1 to 500 are utilized for training the network, samples from 501 to 800 are used for validation, and the remaining samples serve for testing. The first 20 samples from each set are washed out. Moreover, con- sidering the order uncertainty, we assume that certain orders are unknown and design two experimental setups. In the MG1 task, we select $u(n) = [y (n \u2212 6), y (n \u2212 12), y (n \u2212 18)]$ to predict $y (n + 6)$. In the MG2 task, the input is set as $u (n) = [y (n \u2212 12), y (n \u2212 18)]^T$. These settings are inten- tionally designed to assess the performance of the RSCN under conditions of incomplete input variables.\nThe size of the reservoir significantly affects the perfor- mance of the model. In our experiments, we analyze the validation NRMSE curves of different models to identify the optimal reservoir size. Furthermore, the performance of BRSCNs with varying subreservoir sizes is also considered. Fig. 3 displays the validation performance of various models on the MG task. It can be seen that the validation NRMSE decreases gradually as the reservoir sizes increase, suggesting underfitting. However, with an increase in the number of nodes, the validation NRMSE surpasses the minimum point, indicating overfitting. The validation performance of BRSCN varies with different subreservoir sizes, and the best result can be obtained when the subreservoir size is 10. Evidently, the optimal reservoir sizes for the ESN, DESN, GESN, RSCN, and BRSCN are determined to be 96, 80, 90, 68, and 60, respectively. Moreover, the validation performance of the BRSCN always outperforms other models, highlighting that the proposed method can contribute to sound performance if certain tricks are adopted to prevent overfitting."}, {"title": "B. Nonlinear system identification"}]}