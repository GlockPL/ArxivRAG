{"title": "HARD-CONSTRAINED NEURAL NETWORKS\nWITH UNIVERSAL APPROXIMATION GUARANTEES", "authors": ["Youngjae Min", "Anoopkumar Sonar", "Navid Azizan"], "abstract": "Incorporating prior knowledge or specifications of input-output relationships into\nmachine learning models has gained significant attention, as it enhances general-\nization from limited data and leads to conforming outputs. However, most exist-\ning approaches use soft constraints by penalizing violations through regulariza-\ntion, which offers no guarantee of constraint satisfaction-an essential require-\nment in safety-critical applications. On the other hand, imposing hard constraints\non neural networks may hinder their representational power, adversely affecting\nperformance. To address this, we propose HardNet, a practical framework for\nconstructing neural networks that inherently satisfy hard constraints without sacri-\nficing model capacity. Specifically, we encode affine and convex hard constraints,\ndependent on both inputs and outputs, by appending a differentiable projection\nlayer to the network's output. This architecture allows unconstrained optimization\nof the network parameters using standard algorithms while ensuring constraint\nsatisfaction by construction. Furthermore, we show that HardNet retains the uni-\nversal approximation capabilities of neural networks. We demonstrate the versa-\ntility and effectiveness of HardNet across various applications: fitting functions\nunder constraints, learning optimization solvers, optimizing control policies in\nsafety-critical systems, and learning safe decision logic for aircraft systems.", "sections": [{"title": "INTRODUCTION", "content": "Neural networks are widely adopted for their generalization capabilities and their ability to model\nhighly non-linear functions in high-dimensional spaces. With their increasing proliferation, it has\nbecome more important to be able to impose constraints on neural networks in many applications.\nBy incorporating domain knowledge about input-output relationships into neural networks through\nconstraints, we can enhance their generalization abilities, particularly when the available data is\nlimited (Pathak et al., 2015; Oktay et al., 2017; Raissi et al., 2019). These constraints introduce\ninductive biases that can guide the model's learning process toward plausible solutions that adhere to\nknown properties of the problem domain, potentially reducing overfitting to limited data. As a result,\nneural networks can more effectively capture underlying patterns and make accurate predictions on\nunseen data, despite the scarcity of training examples.\nMoreover, adherence to specific requirements is critical in many practical applications. For instance,\nin robotics, this could translate to imposing collision avoidance or pose manifold constraints (Ding\n& Fan, 2014; Wang & Yan, 2023; Ryu et al., 2022; Huang et al., 2022). In geometric learning, this\ncould mean imposing a manifold constraint (Lin & Zha, 2008; Simeonov et al., 2022). In financial\nrisk-management scenarios, violating constraints on the solvency of the portfolio can lead to large\nfines (McNeil et al., 2015). By enforcing the neural network outputs to satisfy these non-negotiable\nrules (i.e., hard constraints), we can make the models more reliable, interpretable, and aligned with\nthe underlying problem structure.\nHowever, introducing hard constraints can potentially limit a neural network's expressive power. To\nillustrate this point, consider a constraint that requires the neural network's output to be less than or\nequal to 1. One could simply restrict the model to always output a constant value less than 1, which\nensures the satisfaction of the hard constraint but obviously limits the model capacity drastically.\nThis raises the question:\nCan we enforce hard constraints on neural networks without losing their expressive power?\nThe model capacity of neural networks is often explained through the universal approximation the-\norem, which shows that a neural network can approximate any continuous function given a suffi-\nciently wide/deep architecture. Demonstrating that this theorem still holds under hard constraints is\nessential to understanding the trade-off between constraint satisfaction and model capacity.\nContributions We tackle the problem of enforcing hard constraints on neural networks by\n\u2022 Presenting a practical framework called HardNet (short for hard-constrained neural net) for\nconstructing neural networks that satisfy input-dependent affine/convex constraints by construc-\ntion. HardNet allows for unconstrained optimization of the networks' parameters with standard\nalgorithms.\n\u2022 Proving a universal approximation theorem for our method, showing that despite enforcing\nthe hard constraints, our construction retains the expressive power of neural networks.\n\u2022 Demonstrating the utility of our method on a variety of scenarios where it's critical to satisfy\nhard constraints \u2013 learning optimization solvers, optimizing control policies in safety-critical\nsystems, and learning safe decision logic for aircraft systems.\n\u2022 Outlining a survey of the literature on constructing neural networks that satisfy hard constraints."}, {"title": "RELATED WORK", "content": "Neural Networks with Soft Constraints Early directions focused on implementing data augmen-\ntation or domain randomization methods to structure the dataset to satisfy the necessary constraints\nbefore training the neural network. However, this does not guarantee constraint satisfaction for\narbitrary inputs (especially those far from the training distribution), and the output often violates\nthe constraints marginally on in-distribution inputs as well. Other initial directions focused on in-\ntroducing the constraints as soft penalties (M\u00e1rquez-Neila et al., 2017; Dener et al., 2020) to the\ncost function of the neural network along with Lagrange multipliers as hyperparameters. Raissi\net al. (2019); Li et al. (2024) leveraged this idea in their work on physics-informed neural networks\n(PINNS) to enforce that the output satisfies a given differential equation.\nNeural Networks with Hard Constraints Some of the conventional neural network components\ncan already enforce specific types of hard constraints. For instance, sigmoids can be used for en-\nforcing lower and upper bounds, softmax layers are helpful for imposing simplex constraints, and\nReLU layers are essentially projections onto the positive orthant. The convolution layer in ConvNets\nencodes a translational equivariance constraint which led to significant improvements in empirical\nperformance. Learning new equivariances and inductive biases that accelerate learning for specific\ntasks is an active area of research.\nRecent work has focused on developing new architectures to impose various hard constraints as\ndescribed in Table 1. Frerix et al. (2020) considered homogeneous linear inequality constraints by\nembedding a parameterization of the feasible set in a neural network layer. LinSATNet (Wang et al.,\n2023) enforces positive linear constraints by extending the Sinkhorn algorithm that iteratively finds\nfeasible solutions. Beyond the affine constraints, RAYEN (Tordesillas et al., 2023) imposes a set of\nconvex constraints by parameterizing the feasible set with the neural network output representing a\ntranslation from an interior point of the convex feasible set. However, these methods are limited to\nconstraints that depend only on the output (and not the input).\nAnother line of work considers hard constraints that depend on both input and output. Balestriero\n& LeCun (2023) proposed the POLICE framework for enforcing the output to be an affine function\nof the input in certain regions of the input space by reformulating the neural networks as continuous\npiecewise affine mappings. KKT-hPINN (Chen et al., 2024) enforces more general affine equality\nconstraints by projecting the output to the feasible set where the projection is computed using the\nKKT conditions of the constraints. However, these affine equality constraints are too restrictive.\nDC3 (Donti et al., 2021b) is a framework for more general nonlinear constraints which reduces\nthe violations of inequality constraints through gradient-based methods over the manifold where"}, {"title": "PRELIMINARIES", "content": "3.1 NOTATION\nFor $p \\in [1, \\infty)$, $\\|v\\|_p$ denotes the $\\ell^p$-norm for a vector $v \\in \\mathbb{R}^m$, and $\\|A\\|_p$ denotes the operator norm\nfor a matrix $A \\in \\mathbb{R}^{k \\times m}$ induced by the $\\ell^p$-norm, i.e., $\\|A\\|_p = \\sup_{w\\neq o} \\|Aw\\|_p/\\|W\\|_p$. $V(i) \\in \\mathbb{R}$,\n$V(:,i) \\in \\mathbb{R}^i$, and $v(i:) \\in \\mathbb{R}^{m-i}$ denote the $i$-th component, the first $i$ and the last $m - i$ components of\n$v$, respectively. Similarly, $A(:,i) \\in \\mathbb{R}^{k\\times i}$ and $A(i:) \\in \\mathbb{R}^{k\\times(m-i)}$ denote the first $i$ and the last $m - i$\ncolumns of $A$, respectively."}, {"title": "UNIVERSAL APPROXIMATION THEOREM", "content": "The universal approximation property is a foundational concept in understanding the capabilities of\nneural networks in various applications. Classical results reveal that shallow neural networks with\narbitrary width can approximate any continuous function defined on a compact set as formalized in\nthe following theorem (Cybenko, 1989; Hornik et al., 1989; Pinkus, 1999):\nTheorem 3.1 (Universal Approximation Theorem for Shallow Networks). Let $\\rho: \\mathbb{R} \\to \\mathbb{R}$ be any\ncontinuous function and $K \\in \\mathbb{R}$ be a compact set. Then, depth-two neural networks with $\\rho$ activation\nfunction universally approximate $C(K, \\mathbb{R})$ if and only if $\\rho$ is nonpolynomial.\nTo further understand the success of deep learning, the universal approximation property for deep\nand narrow neural networks has also been studied in the literature (Lu et al., 2017; Hanin & Sellke,\n2017; Kidger & Lyons, 2020; Park et al., 2021). Interesting results show that a critical threshold\nexists on the width of deep networks that attain the universal approximation property. For instance,\ndeep networks with ReLU activation function with a certain minimum width can approximate any\n$L^p$ function as described in the following theorem (Park et al., 2021, Thm. 1):\nTheorem 3.2 (Universal Approximation Theorem for Deep Networks). For any $p\\in [1,\\infty)$, $w$-\nwidth neural networks with ReLU activation function universally approximate $L^p(\\mathbb{R}^{\\text{nin}}, \\mathbb{R}^{\\text{nout}})$ if\nand only if $w > \\text{max}\\{\\text{nin} + 1, \\text{nout}\\}$.\nDespite these powerful approximation guarantees, they fall short in scenarios where neural networks\nare required to satisfy hard constraints, such as physical laws or safety requirements. These theo-\nrems ensure that a neural network can approximate a target function arbitrarily closely but do not\nguarantee that the approximation will adhere to necessary constraints. Consequently, even if the\ntarget function inherently satisfies specific hard constraints, the neural network approximator might\nviolate them-especially in regions where the target function barely meets the constraints. This\nshortcoming is particularly problematic for applications that demand strict compliance with non-\nnegotiable domain-specific rules. Therefore, ensuring that neural networks can both approximate\ntarget functions accurately and rigorously satisfy hard constraints remains a critical challenge for\ntheir deployment in practical applications."}, {"title": "HardNet: HARD-CONSTRAINED NEURAL NETWORKS", "content": "In this section, we present a practical framework HardNet for enforcing hard constraints on neural\nnetworks while retaining their universal approximation properties. In a nutshell, for a parameter-\nized (neural network) function $f_\\theta : \\mathcal{X} \\subset \\mathbb{R}^{\\text{nin}} \\to \\mathbb{R}^{\\text{nout}}$, we ensure the satisfaction of given con-\nstraints by appending a differentiable projection layer $P$ to $f_\\theta$. This results in the projected function\n$P(f_\\theta) : \\mathcal{X} \\to \\mathbb{R}^{\\text{nout}}$ meeting the required constraints while allowing its output to be backpropagated\nthrough to train the model via gradient-based algorithms. Importantly, we show that the proposed\narchitecture has universal approximation guarantees, i.e., it universally approximates the class of\nfunctions that satisfy the constraints.\nWe begin with the simple intuitive case of a single affine constraint and then generalize the ap-\nproach in two directions. First, we propose HardNet-Aff that ensures compliance with multiple\ninput-dependent affine constraints through a differentiable closed-form projection. Then, we present\nHardNet-Cvx as a framework to satisfy general input-dependent convex constraints exploiting dif-\nferentiable convex optimization solvers."}, {"title": "HardNet-Aff: IMPOSING INPUT-DEPENDENT AFFINE CONSTRAINTS", "content": "First, consider the following single input-dependent affine constraint for a function $f : \\mathcal{X} \\to \\mathbb{R}^{\\text{nout}}$:\n$a(x)^T f(x) \\leq b(x) \\quad \\forall x \\in \\mathcal{X},$ (1)\nwhere $a(x) \\in \\mathbb{R}^{\\text{nout}}$ and $b(x) \\in \\mathbb{R}$. We assume $a(x) \\neq 0$ as the constraint solely depending on\nthe input is irrelevant to the function. We can ensure this constraint on $P(f_\\theta)$ by constructing the\nprojection for all $x \\in \\mathcal{X}$ as:\n$\\begin{aligned}\nP(f_\\theta)(x) & = \\underset{z \\in \\mathbb{R}^{\\text{nout}}}{\\text{arg min}} \\|z - f_\\theta(x)\\|^2 \\quad \\text{s.t.} \\quad a(x)^T z \\leq b(x) \\\\\n& = f_\\theta(x) - \\frac{a(x)}{\\|a(x)\\|^2} \\text{ReLU} \\left(a(x)^T f_\\theta(x) - b(x)\\right).\n\\end{aligned}$\n(2)\n(3)\nThis closed-form solution is differentiable almost everywhere so that the projected function can be\ntrained using conventional gradient-based algorithms such as SGD. This type of closed-form pro-\njection has been recently utilized in the control literature as in Kolter & Manek (2019) for learning\nstable dynamics and in Donti et al. (2021a); Min et al. (2023) for learning stabilizing controllers.\nNonetheless, they are limited to enforcing only a single inequality constraint. Moreover, their em-\npirical success in learning the desired functions has not been theoretically understood. To that end,\nwe generalize the method to satisfy more general constraints and provide a rigorous explanation for\nits expressivity through universal approximation guarantees.\nSuppose we have multiple input-dependent affine constraints in an aggregated form:\n$A(x)f(x) \\leq b(x), \\quad C(x)f(x) = d(x) \\quad \\forall x \\in \\mathcal{X},$ (4)\nwhere $A(x) \\in \\mathbb{R}^{\\text{nineq} \\times \\text{nout}}$, $b(x) \\in \\mathbb{R}^{\\text{nineq}}$, $C(x) \\in \\mathbb{R}^{\\text{neq} \\times \\text{nout}}$, $d(x) \\in \\mathbb{R}^{\\text{neq}}$ for $\\text{nineq}$ ineqaulity and $\\text{neq}$\nequality constraints. For partitions $A(x) = [A(:,\\text{neq}) \\; A(\\text{neq}:)]$ and $C(x) = [C(:,\\text{neq}) \\; C(\\text{neq}:)]$, we make\nthe following assumptions about the constraints:\nAssumption 4.1. i) The constraints in (4) are feasible, ii) $C(:,\\text{neq})$ is invertible for all $x \\in \\mathcal{X}$,\niii) $\\bar{A}(x) := A(\\text{neq}:) - A(:,\\text{neq})C(:,\\text{neq})^{-1} C(\\text{neq}:)$ has full row rank for all $x \\in \\mathcal{X}$.\nGiven $x \\in \\mathcal{X}$, when $C(x)$ has full row rank (i.e., no redundant constraints), there exists an invert-\nible submatrix of $C(x)$ with its $\\text{neq}$ columns. Without loss of generality, we can assume $C(:,\\text{neq})$\nis such submatrix by considering a proper permutation of the components of $f$. Then, the second\nassumption holds when the same permutation lets $C'(:,\\text{neq})$ invertible for all $x \\in \\mathcal{X}$. The last assump-\ntion requires the total number of the constraints $\\text{nineq} + \\text{neq}$ to be less than or equal to the output\ndimension $\\text{nout}$ and $\\bar{A}(x)$ to have full row rank."}, {"title": "HardNet-Cvx: IMPOSING GENERAL INPUT-DEPENDENT CONVEX CONSTRAINTS", "content": "Going beyond the affine constraints, we present HardNet-Cvx as a framework that enforces general\ninput-dependent convex constraints:\n$f(x) \\in C(x) \\quad \\forall x \\in \\mathcal{X}$ (8)\nwhere $C(x) \\subset \\mathbb{R}^{\\text{nout}}$ is a convex set. Unlike the affine constraints case, we cannot extend the closed-\nform projection of the single constraint case in (3) for general convex constraints. Thus, we present\nHardNet-Cvx by generalizing the optimization-based projection in (2) as below:\n$P(f_\\theta)(x) = \\underset{z \\in \\mathbb{R}^{\\text{nout}}}{\\text{arg min}} \\|z - f_\\theta(x)\\|^2 \\quad \\text{s.t.} \\quad z \\in C(x) \\quad \\forall x \\in \\mathcal{X}.$ (9)\nWhile no general closed-form solution for this optimization problem exists, we can employ differen-\ntiable convex optimization solvers such as Amos & Kolter (2017) for affine constraints (when their\nnumber exceeds the output dimension $\\text{nout}$) and Agrawal et al. (2019) for more general convex con-\nstraints. Originally, these solvers were introduced as new layer architectures within neural networks\nto enable learning optimization processes in latent spaces, rather than solving given optimization\nproblems directly. In our approach, we freeze these layers and exploit their differentiability to back-\npropagate through the solutions. This allows us to project the output $f_\\theta (x)$ onto the feasible set $C(x)$\nand train the projected function via conventional gradient-based algorithms. It is worth noting that\nthis idea was mentioned in passing by Donti et al. (2021b) but was not pursued due to computational\ncomplexity concerns.\nAs in Section 4.1, we demonstrate that HardNet-Cvx preserves the expressive power of neural\nnetworks by proving the following universal approximation theorem; see Appendix A.3 for a proof.\nTheorem 4.4 (Universal Approximation Theorem with Convex Constraints). Consider input-\ndependent constrained sets $C(x) \\subset \\mathbb{R}^{\\text{nout}}$ that are convex for all $x \\in \\mathcal{X} \\subset \\mathbb{R}^{\\text{nin}}$. For any $p \\in [1,\\infty)$,\nlet $\\mathcal{F} = \\{f \\in L^p(\\mathcal{X},\\mathbb{R}^{\\text{nout}})| f(x) \\in C(x) \\; \\forall x \\in \\mathcal{X}\\}$. Then, HardNet-Cvx with $w$-width ReLU\nneural networks defined in (9) universally approximates $\\mathcal{F}$ if $w > \\text{max}\\{\\text{nin} + 1, \\text{nout}\\}$."}, {"title": "EXPERIMENTS", "content": "In this section, we demonstrate the versatility and effectiveness of HardNet over four scenarios\nwith required hard constraints: fitting functions under constraints, learning optimization solvers,\noptimizing control policies in safety-critical systems, and learning safe decision logic for aircraft\nsystems.\nAs evaluation metrics, we measure the violation of constraints in addition to the application-specific\nperformance metrics. For a test sample $x \\in \\mathcal{X}$ and $\\text{nineq}$ inequality constraints $C_< (f(x)) \\leq$\n$0 \\in \\mathbb{R}^{\\text{nineq}}$, their violation is measured with the maximum ($\\leq$ max) and mean ($\\leq$ mean) of\n$\\text{ReLU}(C_<(f(x)))$ and the number of violated constraints ($< \\#$). Similar quantities of $\\|C_=(f(x))\\|$\nare measured for $\\text{neq}$ equality constraints $C_=(f(x)) = 0 \\in \\mathbb{R}^{\\text{neq}}$. Then, they are averaged over all\ntest samples. The inference time for the test set is also compared.\nWe compare HardNet with the following baselines: (i) NN: Plain neural networks, (ii) Soft: Soft-\nconstrained neural networks. To penalize constraint violation, for a sample point $x \\in \\mathcal{X}$, ad-\nditional regularization terms $\\lambda_< \\|\\text{ReLU}(C_<(f(x_s)))\\|^2 + \\lambda_=\\|C_=(f(x)) \\|^2$ are added to the loss\nfunction, (iii) NN+Proj/Soft+Proj: The projection of HardNet is applied at test time to the outputs\nof NN/Soft, (iv) DC3 (Donti et al., 2021b): Similarly to HardNet-Aff, DC3 takes a neural network\nthat approximates the part of the target function. It first augments the neural network output to\nsatisfy the equality constraints. Then, it corrects the augmented output to minimize the violation\nof the inequality constraints via the gradient descent algorithm. DC3 backpropagates through this\niterative correction procedure to train the model. For all methods, we use 3-layer fully connected\nneural networks with 200 neurons in each hidden layer and ReLU activation function."}, {"title": "FUNCTION FITTING UNDER CONSTRAINTS", "content": "In this experiment, we demonstrate the efficacy of HardNet-Aff on a problem involving fitting a\ncontinuous function $f: [-2,2] \\to \\mathbb{R}$ shown in Fig. 2. The function outputs are required to avoid"}, {"title": "LEARNING OPTIMIZATION SOLVER", "content": "We consider the problem of learning optimization solvers as in Donti et al. (2021b) with the follow-\ning nonconvex optimization problem:\n$f(x) = \\underset{y}{\\text{arg min}} \\frac{1}{2} y^T Q y + p^T \\sin y \\quad \\text{s.t.} \\quad Ay \\leq b, Cy = x,$\n(10)\nwhere $Q \\in \\mathbb{R}^{\\text{nout} \\times \\text{nout}} \\succ 0$, $p \\in \\mathbb{R}^{\\text{nout}}$, $A \\in \\mathbb{R}^{\\text{nineq} \\times \\text{nout}}$, $b \\in \\mathbb{R}^{\\text{nineq}}$, $C \\in \\mathbb{R}^{\\text{neq} \\times \\text{nout}}$ are constants and\n$\\sin$ is the element-wise sine function. The target function $f$ outputs the solution of each optimiza-\ntion problem instance determined by the input $x \\in [-1,1]^{\\text{neq}}$. The main benefit of learning this\nnonconvex optimization solver with neural networks is their faster inference time than optimizers\nbased on iterative methods. To ensure that the learned neural networks provide feasible solutions,\nthe constraints of the optimization problems are set as hard constraints.\nIn this experiment, we guarantee that the given constraints are feasible for all $x \\in [-1,1]^{\\text{neq}}$ by\ncomputing a proper $b$ for randomly generated $A, C$ as described in Donti et al. (2021b). Then the\nmodels are trained on 10000 unlabeled data points uniformly sampled from $[-1,1]^{\\text{neq}}$. To perform\nthis unsupervised learning task, the loss function for each sample $x$, is set as $f_\\theta(x_s)^T Q f_\\theta(x_s) +$\n$p^T \\sin f_\\theta(x_s)$. As shown in Table 3, HardNet-Aff consistently finds feasible solutions with small\nsuboptimality gap from the optimizer (IPOPT) with much shorter inference time."}, {"title": "OPTIMIZING SAFE CONTROL POLICY", "content": "In this experiment, we apply HardNet-Aff to enforce\nsafety constraints in control systems. Consider a control-\naffine system with its dynamics:\n$\\dot{x}(t) = f(x(t)) + g(x(t))u,$ (11)\nwhere $x(t) \\in \\mathbb{R}^{\\text{nin}}$ is the system state at time $t$ and $u \\in$\n$\\mathbb{R}^{\\text{hout}}$ is the control input. For safety reasons (e.g., avoid-\ning obstables), the system requires $x(t) \\in \\mathcal{X}_{\\text{safe}} \\subset \\mathbb{R}^{\\text{nin}}$\nfor all $t$. We translate this safety condition into a state-\ndependent affine constraint on the control input using a\ncontrol barrier function (CBF) $h : \\mathbb{R}^{\\text{nin}} \\to \\mathbb{R}$ (Ames et al.,\n2019). Suppose its super-level set $\\{x \\in \\mathbb{R}^{\\text{nin}}|h(x) \\geq$\n$0\\} \\subset \\mathcal{X}_{\\text{safe}}$ and $h(x(0)) \\geq 0$. Then, we can ensure\n$h(x(t)) \\geq 0 \\; \\forall t \\geq 0$ by guaranteeing\n$\\dot{h}(x) = \\nabla h(x)^T (f(x) + g(x)\\pi(x)) \\geq -\\alpha h(x)$ (12)\nat each $x(t)$ for a state-feedback control policy $\\pi:$\n$\\mathbb{R}^{\\text{nin}} \\to \\mathbb{R}^{\\text{hout}}$ with some $\\alpha > 0$. Enforcing (12) for multi-\nple CBFs ensures the trajectory remains within the inter-\nsection of the corresponding safe sets.\nWe consider controlling a unicycle system to minimize\nthe cost over trajectories while avoiding collisions with\ntwo elliptical obstacles, each presented with a CBF (see\nAppendix A.5 for details). Given a nominal controller\n$\\pi_{nom}: \\mathbb{R}^{\\text{nin}} \\to \\mathbb{R}^{\\text{hout}}$ designed without obstacle consider-\nations, a conventional approach to find a safe controller is\nto solve a quadratic program:\nCBF-QP: $\\pi^{\\text{CBF-QP}}(x) = \\underset{u}{\\text{arg min}} \\|u - \\pi_{nom}(x)\\|^2 \\; \\text{s.t.} \\; (12) \\text{ holds for all CBFs}$ (13)"}, {"title": "AIRBORNE COLLISION AVOIDANCE SYSTEM (ACAS)", "content": "In this example, we consider learning an airborne collision avoidance system called the Horizontal\nCollision Avoidance System (HCAS), which is a variant of the popular system ACAS Xu used by\nKatz et al. (2017), for which the labeled training dataset is publicly available (Julian & Kochenderfer,\n2019). The goal of the HCAS system is to recommend horizontal maneuvers-clear of conflict,\nweak left, strong left, weak right, strong right-to aircrafts in order to stay safe and avoid collisions.\nThe system takes the state of the ownship and the relative state of the intruder airplane as inputs, and\noutputs a score for each of the 5 possible horizontal maneuvers listed above. Traditionally, this was\naccomplished using lookup tables but using neural networks has recently become customary.\nIn their original work, Katz et al. (2017) developed a method called Reluplex for formally verifying\nthat the deep neural networks used for generating the scores of the advisories always satisfy certain\n\"properties\" or, as we would call them in our framework, hard constraints. This requires training\na model and then verifying that it satisfies certain properties by solving a satisfiability problem. In\naddition, the original work trains 45 different models for various values of $\\tau$ (the time to collision or\nloss of vertical separation) and $p_{ra}$ (the previously recommended advisory) in order to satisfy strict\ncomputational limits imposed by the inference hardware. In our work, we train a single model that\ngeneralizes across various values of $\\tau$ and $p_{ra}$.\nIn our implementation, we demonstrate a method for learning neural networks that output con-\nstrained airplane advisories by construction, rather than engaging in an iterative cycle of training a\nneural network and separately verifying it until convergence. While some of the properties in the\noriginal problem (Katz et al., 2017) are non-convex in nature, we pick all five of the properties that\ncan be encoded in a convex form. The results for this example are presented in Table 5. Additional\ndetails for the experiment can be found in Appendix A.6."}, {"title": "CONCLUSION", "content": "In this paper, we presented HardNet, a practical framework for constructing neural networks that\ninherently satisfy input-dependent affine/convex constraints. We proved that imposing these hard\nconstraints does not limit the expressive power of these neural networks by providing universal\napproximation guarantees. We also demonstrated the utility and versatility of our method across\nseveral application scenarios, such as learning solvers for optimization problems, control policies\nfor safety-critical systems, and advisories for aircraft navigation systems. Using HardNet in other\napplication domains that benefit from incorporating domain-specific knowledge is a promising di-\nrection for future work. Additionally, we aim to explore developing methods for performing fast\nprojections for problems with more general constraints. Lastly, extending our approach to support\nother forms of inductive biases, such as equivariances and invariances, would potentially be of great\ninterest."}, {"title": "APPENDIX", "content": "A.1 PROOF OF PROPOSITION 4.2\nProof. We simplify the notation of the partition by $(\\cdot)_1 := (\\cdot)(:,\\text{neq})$ and $(\\cdot)_2 := (\\cdot)(\\text{neq}:)$. Then,\n$A(x)P(f_\\theta)(x) = A_1 P(f_\\theta)_1 + A_2 P(f_\\theta)_2$ (14)\n$= A_1 C_1^{-1} (d - C_2 f_\\theta) + A_2 f_\\theta$ (15)\n$= (A_2 - A_1 C_1^{-1} C_2) f_\\theta + A_1 C_1^{-1} d$ (16)\n$= \\bar{A} f_\\theta - \\bar{b} + \\bar{b}$ (17)\n$= \\bar{A} f_\\theta - \\text{ReLU}(\\bar{A} f_\\theta - \\bar{b}) - \\bar{b} + \\bar{b} \\leq \\bar{b} - \\bar{b} + \\bar{b} = b(x)$. (18)\nThis shows (i). For (ii),\n$C(x)P(f_\\theta)(x) = C_1 P(f_\\theta)_1 + C_2 P(f_\\theta)_2 = (d - C_2 f_\\theta) + C_2 f_\\theta = d(x)$. (19)\nFor (iii), we first observe that\n$a_i^T f_\\theta(x) \\leq b^{(i)} (x) \\iff a_1^T f_\\theta + a_2^T C_1^{-1} (d - C_2 f_\\theta) \\leq b^{(i)} \\iff a_i^T f_\\theta \\leq b^{(i)}$. (20)\nThen, if $a_i^T f_\\theta(x) \\leq b^{(i)} (x)$,\n$\\begin{aligned}\na_i^T P(f_\\theta)(x) & = a_1^T P(f_\\theta)_1 + a_2^T P(f_\\theta)_2 \\\\\n& = (a_2^T - a_1^T C_1^{-1} C_2) f_\\theta + a_1^T C_1^{-1} d \\\\\n& = (a_2^T - a_1^T C_1^{-1} C_2) f_\\theta + a_1^T C_1^{-1} d = a_i^T f_\\theta(x),\n\\end{aligned}$\n(21)\n(22)\n(23)\nwhere the second last equality is from $\\bar{A} f_\\theta = \\bar{A} f_\\theta - \\text{ReLU}(\\bar{A} f_\\theta - \\bar{b})$ and $a_i^T f_\\theta \\leq b^{(i)}$. Similarly,\nif $a_i^T f_\\theta(x) > b^{(i)} (X)$,\n$a_i^T P(f_\\theta)(x) = (a_2^T - a_1^T C_1^{-1} C_2) f_\\theta + a_1^T C_1^{-1} d = \\bar{b}^{(i)} + a_1^T C_1^{-1} d = b^{(i)} (x)$ (24)"}, {"title": "PROOF OF THEOREM 4.3", "content": "Proof. We first show that $\\| f(x) -P(f_\\theta)(x)\\|^2$ can be bounded by some constant times $\\| f(\\text{neq"}, "X) -$\n$f_\\theta(x)\\|^2$. From (7),\n$\\|f(x) - P(f_\\theta)(x)\\|^2 = \\| f(:,\\text{neq})(x) - P(f_\\theta)(:,\\text{neq})(x)\\|^2 + \\|f(\\text{neq}:) (x) - P(f_\\theta) (\\text{neq}:)(x)\\|^2$ (25)\n$= \\|C(:,\\text{neq})C(\\text{neq}:)^{-1} (f(\\text{neq}:) - f_\\theta)\\|^2 + \\|f(\\text{neq}:) - f_\\theta \\|^2$ ("]}