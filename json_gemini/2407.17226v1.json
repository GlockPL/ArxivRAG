{"title": "Sublinear Regret for An Actor-Critic Algorithm in Continuous-Time Linear-Quadratic Reinforcement Learning", "authors": ["Yilie Huang", "Yanwei Jia", "Xun Yu Zhou"], "abstract": "We study reinforcement learning (RL) for a class of continuous-time linear-quadratic (LQ) control problems for diffusions where volatility of the state processes depends on both state and control variables. We apply a model-free approach that relies neither on knowledge of model parameters nor on their estimations, and devise an actor-critic algorithm to learn the optimal policy parameter directly. Our main contributions include the introduction of a novel exploration schedule and a regret analysis of the proposed algorithm. We provide the convergence rate of the policy parameter to the optimal one, and prove that the algorithm achieves a regret bound of O(N) up to a logarithmic factor. We conduct a simulation study to validate the theoretical results and demonstrate the effectiveness and reliability of the proposed algorithm. We also perform numerical comparisons between our method and those of the recent model-based stochastic LQ RL studies adapted to the state- and control-dependent volatility setting, demonstrating a better performance of the former in terms of regret bounds.", "sections": [{"title": "1 Introduction", "content": "Linear-quadratic (LQ) control, in which the system dynamics are linear in the state and control variables while the reward is quadratic in them, takes up a center stage in classical model-based control theory when the model parameters are assumed to be given and known. The reason is twofold: LQ control can be solved explicitly and elegantly, and it can be used to approximate more complicated nonlinear control problems. Detailed theoretical accounts in the continuous-time setting can be found in Anderson and Moore (2007) for deterministic control (i.e., dynamics are described by ordinary differential equations) and in Yong and Zhou (1999) for stochastic control (i.e., dynamics are governed by stochastic differential equations).\nMany real-world applications often present themselves with partially known or entirely unknown environments. Specifically in the LQ context, one may know that a problem is structurally LQ, namely the system responds linearly to state and control whereas the reward is quadratic (e.g. a variance is involved) in these variables, yet without knowing some or any of the model parameters. The so-called plug-in method has been traditionally used to solve such a problem, namely, one first estimates the model parameters based on observed data and then plugs in the estimated parameters and applies the classical optimal control theory to derive the solutions. Such an approach is essentially still model-based because it takes learning the model as its core mission. It is well known, however, that the plug-in method has significant drawbacks, especially in that optimal controls are typically very sensitive to the model parameters, yet estimating some of the parameters accurately when data are limited is a daunting, sometimes impossible, task (e.g., the return rate of a stock (Merton, 1980; Luenberger, 1998)).\nReinforcement learning (RL) has been developed to tackle complex control problems in largely unknown environments. Its successful applications range from strategic board games such as chess and Go (Silver et al., 2016, 2017) to robotic systems (Gu et al., 2017; Khan et al., 2020). However, RL has been predominantly studied for discrete-time, Markov decision processes (MDPs) with discrete state and control spaces, even though most real-life applications are inherently continuous-time with continuous state space and possibly continuous control space (e.g., autonomous driving, stock trading, and video game playing). More importantly, while one can turn a continuous-time problem into a discrete-time MDP upfront by time discretization, such an approach is very sensitive to time step size and performs poorly with small time steps Munos (2006); Tallec et al. (2019); Park et al. (2021).\nWhile there are studies directly on continuous-time RL, these are rare and far between (Baird, 1994; Doya, 2000; Vamvoudakis and Lewis, 2010; Lee and Sutton, 2021; Kim et al., 2021), over-all lacking a systematic and unified theory. Starting with Wang et al. (2020) that introduces an entropy-regularized relaxed control framework for continuous-time RL, a series of subsequent papers (Jia and Zhou, 2022a,b, 2023) develop theories on policy evaluation, policy gradient, and q-learning respectively within this framework. This strand of research is characterized by focusing on learning the optimal control policies directly without attempting to estimate or learn the model parameters, underlining a model-free (up to the unknown dynamics being governed by diffusion processes) and data-driven approach. The mathematical foundation of the entire theory is the martingale prop-erty of certain stochastic processes, the enforcement of which naturally leads to various temporal difference and actor-critic algorithms to train and learn q-functions, value functions, and optimal (stochastic) policies. Subsequently, there has been active follow-up research with various extensions and applications; see, e.g. Huang et al. (2022); Dai et al. (2023); Wang et al. (2023); Frikha et al. (2023); Wei and Yu (2023).\nA crucial question in RL is the convergence and regret bounds of RL algorithms that pro-vide theoretical guidance and guarantee their effectiveness and reliability. For LQ problems, such theoretical results exist, for example, for deterministic systems (Bradtke, 1992; Fazel et al., 2018; Malik et al., 2019) as well as systems with identically and independently distributed noises (Abbasi-Yadkori and Szepesv\u00e1ri, 2011; Abeille and Lazaric, 2018; Cohen et al., 2018, 2019; Hambly et al., 2021; Wang et al., 2021; Cassel and Koren, 2021; Yang et al., 2019; Zhou and Lu, 2023; Chen et al., 2023), covering finite-horizon, infinite-horizon, and ergodic cases. These studies are nevertheless all for discrete-time models, with control not affecting the variability of the state dynamics. Some of them, e.g. Abbasi-Yadkori and Szepesv\u00e1ri (2011); Abeille and Lazaric (2018); Hambly et al. (2021); Wang et al. (2021); Zhou and Lu (2023), design their algorithms based on policy gradient. However, the gradient representations therein rely on estimations of the drift parameters; hence, the methods are essentially model-based. In addition, the semidefinite programming formulation in (Cohen et al., 2018, 2019) does not seem applicable to continuous-time systems.\nThe algorithm proposed and analyzed in the present paper belongs to the class of actor-critic algorithms originally put forward by Konda and Tsitsiklis (1999). Such algorithms for discrete-time systems have been studied in Wu et al. (2020); Xu et al. (2021); Cen et al. (2022), and in particular, for ergodic LQ problems in Yang et al. (2019); Chen et al. (2023) and for episodic linear MDPs in Cai et al. (2020); Zhong and Zhang (2023). The \"optimal\" regret of these algorithms is mostly of the order $O(\\sqrt{N})$, where N is the number of episodes or timesteps. However, it is unclear whether they still work for the diffusion case where the volatility also depends on state and control. For general continuous-time diffusion environments, however, the aforementioned series of papers (Wang et al. (2020); Jia and Zhou (2022a,b, 2023)) and their follow-up study have not addressed the problems of convergence and regret bounds. These remain highly significant yet challenging open questions due to the model-free nature of the underlying approach and the stochastic approximation type of algorithms involved.\nRecently, there has been some progress on regret analysis for continuous-time stochastic LQ RL in (Basei et al., 2022; Szpruch et al., 2024) that achieve sublinear regrets of their respective algorithms. Both papers assume that the diffusion coefficients are constant independent of state and control, which is vital for their approaches to work. Again, in essence, these works are model-based because they apply either least-square or Bayesian methods to estimate model parameters and use the corresponding estimation errors to deduce the regret bounds. There is an intrinsic drawback in these model-based methods specific to LQ problems: optimal control policies are linear feedbacks of the state; hence, when one obtains an estimate of the drift coefficient through observed data in a closed-loop system, it is hard to disentangle the original state and control coefficients from the feedback gain. Indeed, when we tried to carry out numerical experiments based on the results of (Basei et al., 2022; Szpruch et al., 2024) we often ended up with infeasible solutions. Moreover, (Basei et al., 2022) requires both the batch size and the number of timesteps to increase exponentially over episodes, adding substantial computational and memory costs.\nThis paper endeavors to design an RL algorithm with a provable sublinear regret for a class of stochastic LQ RL problems in the model-free framework of Wang et al. (2020); Jia and Zhou (2022a,b, 2023). We allow the diffusion coefficients to depend on both state and control, the latter being of particular practical significance (e.g. the wealth equation in continuous-time finance (Zhou and Li, 2000)). Indeed, this type of stochastic LQ problems have led to a very active research area called \"indefinite stochastic LQ control\" in the classical, model-based literature, starting from (Chen et al., 1998; Rami and Zhou, 2000).\nMain Contributions. In this paper, we propose a policy gradient based actor-critic algorithm to solve a class of continuous-time, finite-horizon stochastic LQ problems under the model-free, episodic RL setting. Our main contributions are\n(1) We provide a convergence and regret analysis when the volatility of the state process is affected by both state and control. The regret is upper bounded by the order of O(N) (up to a logarithmic factor), where N is the number of episodes. While it may not yet be the best regret bound, to our best knowledge, it is the first sublinear regret result obtained in the entropy-regularized exploratory framework of Wang et al. (2020), with state- and action-dependent volatility.\n(2) We take a model-free approach to develop our algorithm, i.e., a policy gradient based (soft) actor-critic algorithm, and base our analysis on the stochastic approximation scheme. In particular, the policy gradient in this paper is a \"model-free gradient\" instead of a \"model-based gradient\" commonly taken in discrete-time RL. As a result, we do not need to estimate model primitives in the entire analysis, circumventing the issues discussed earlier arising from estimating/learning those model parameters.\n(3) We propose a novel exploration schedule. Note that stochastic policies are considered in this paper for both conceptual and technical reasons. Conceptually, stochastic policies reach more action areas otherwise not necessarily explored by deterministic policies. Technically, we apply the policy gradient method developed in Jia and Zhou (2022b) that works only for stochastic policies. Gaussian exploration policies are shown to be optimal in achieving the ideal balance between exploration and exploitation, whose variance represents the level of exploration. We propose a decreasing schedule of variances for the Gaussian exploration over iterations, guided by the desired regret bound.\nThe remainder of the paper is structured as follows. Section 2 formulates the problem and provides some preliminary results necessary for the subsequent development. Section 3 describes and explains the steps leading to our RL algorithm. Section 4 presents the main theoretical results on convergence and a regret bound of the proposed algorithm. Section 5 reports the results of numerical experiments. Finally, Section 6 concludes. The appendices contain proofs of the main results and a detailed description of the numerical experiments."}, {"title": "2 Problem Formulation and Preliminaries", "content": ""}, {"title": "2.1 Classical Stochastic LQ Control", "content": "We begin by recalling the classical stochastic LQ control formulation and main results. Denote by $x^u = \\{x^u(t) \\in \\mathbb{R} : 0 \\le t \\le T\\}$ the state process under a control process $u = \\{u(t) \\in \\mathbb{R} : 0 \\le t \\le T\\}$, whose dynamic is described by the following stochastic differential equation (SDE):\n$dx^u(t) = (Ax^u(t) + Bu(t))dt + (Cx^u(t) + Du(t))dW(t),$\nwhere $x^u(0) = x_0 \\neq 0$ and $W = \\{W(t) \\in \\mathbb{R} : 0 \\le t \\le T\\}$ is a standard Brownian motion. The goal of the control problem is to choose a control process u to maximize the expected value of a quadratic objective functional:\n$\\max E[\\int_0^T \\frac{1}{2}(-Qx^u(t)^2)dt-\\frac{1}{2}Hx^u(T)^2]$\nwhere $Q \\ge 0$ and $H \\ge 0$ are given weighting parameters. One can define the optimal value function\n$V_{CL}(t,x) = \\max_u E[\\int_t^T \\frac{1}{2}(-Qx^u(s)^2)ds - \\frac{1}{2}Hx^u(T)^2 | x(t) = x].$\nWhile the existing works on stochastic LQ RL assume the diffusion coefficient to be a constant, control- and state-dependent diffusion terms appear in many applications. On the other hand, there is no running control reward in our problem which is a crucial assumption for our approach to work. This class of problems cover important applications such as the mean-variance portfolio selection (Zhou and Li, 2000; Dai et al., 2023; Huang et al., 2022).\nIf the model parameters A, B, C, D, Q, and H are all known, this problem can be solved explicitly as detailed in (Yong and Zhou, 1999, Chapter 6). Specifically, the optimal value function and optimal feedback control policy are respectively\n$V_{CL}(t, x) = \\frac{Q}{2\\Lambda}[\\frac{H-\\Lambda+e^{2\\Lambda(t-T)}}{H-\\Lambda}]e^{\\Lambda(t-T)}x^2,$\n$U_{CL}(t, x) = -\\frac{B+CD}{D^2}x,$\nwhere $\\Lambda = \\frac{1}{D^2}(B^2 + 2BCD - 2AD^2)$.\nThroughout this paper (including the appendices) we use c or its variants for generic constants (depending only on the model parameters A, B, C, D, Q, H, $x_0$ and T) whose values may change from line to line."}, {"title": "2.2 RL Theory for LQ", "content": "In most real-life problems, it is often unrealistic to assume precise knowledge of the parameters such as A, B, C, and D. These problems call for RL which differs fundamentally from the traditional estimate-then-optimize methods. The essence of RL is to strike an exploration-exploitation balance by strategically exploring the unknown environment (Sutton and Barto, 2018). To achieve this, RL employs randomized controls to capture exploration where control processes u are sampled from a process $\\pi = \\{\\pi(\\cdot,t) \\in \\mathcal{P}(\\mathbb{R}) : 0 \\le t \\le T\\}$ of probability distributions with $\\mathcal{P}(\\mathbb{R})$ being the space of all probability density functions over R, and adds an entropy term in the objective function to encourage exploration. Such an entropy regularization is linked to soft-max approximation and Boltzmann exploration (Haarnoja et al., 2018; Ziebart et al., 2008). Wang et al. (2020) is the first to present a rigorous mathematical formulation of entropy regularized RL for (continuous-time) controlled diffusion processes.\nFollowing Wang et al. (2020), under a given randomized control the dynamic of the LQ RL satisfies SDE:\n$dx^{\\pi} (t) = b(x^{\\pi} (t), \\pi(\\cdot, t))dt + \\tilde{\\sigma}(x(t), \\pi(\\cdot, t))dW(t),$\nwhere\n$b(x, \\psi) := Ax + B\\int_{\\mathbb{R}} u\\psi(u)du,$\n$\\tilde{\\sigma}(x,\\psi) := \\sqrt{\\int_{\\mathbb{R}} (Cx + Du)^2\\psi(u)du}, (x,\\psi) \\in \\mathbb{R} \\times \\mathcal{P}(\\mathbb{R}).$\nThe entropy-regularized value function of $\\pi$ is\n$J(t, x; \\pi) = E\\Big[\\int_t^T \\frac{1}{2}(-Qx^{\\pi}(s)^2 + \\gamma\\rho^{\\pi} (s)) ds - \\frac{1}{2}Hx^{\\pi} (T)^2 | x(t) = x\\Big],$\nwhere $\\rho^{\\pi} (t) = - \\int_{\\mathbb{R}} \\pi(t, u) \\log \\pi(t, u)du$ is the entropy term for $\\pi$ and $\\gamma \\ge 0$, known as the temperature parameter, is the weight on exploration. The optimal value function is then\n$V(t, x) = \\max_{\\pi} J(t, x; \\pi).$\nIt is shown by Wang et al. (2020) that the optimal value function and optimal randomized/s-tochastic (feedback) policy are determined by\n$V(t, x) = -\\frac{1}{2}k_1(t)x^2 + k_3(t),$\n$\\pi(u | t,x) = \\mathcal{N}\\Big(u \\Big| -\\frac{(B+CD)}{D^2}x, \\frac{\\gamma}{D^2k_1(t)}\\Big),$\nwhere $k_1 > 0$ and $k_3$ are certain functions of t that can be determined completely by the model primitives, and $\\mathcal{N}(\\cdot|\\mu, \\sigma^2)$ is the Gaussian density with mean $\\mu$ and variance $\\sigma^2$. These theoretical results cannot be used to compute the solution of the exploratory problem because the model parameters are unknown, yet they reveal the structure of the solution inherent to LQ RL (i.e. the optimal value function is quadratic in x and optimal stochastic policy is Gaussian) that can be utilized to significantly reduce the complexity of function parameterization/approximation in learning."}, {"title": "3 A Continuous-Time RL Algorithm", "content": "This section presents the steps of designing a continuous-time RL algorithm for solving our LQ problem, including function parameterization, policy evaluation and policy gradient.1 We will introduce various techniques such as exploration scheduling and projection for deriving the convergence rate of the policy parameter and the regret bound. We will also describe time discretization for final implementation."}, {"title": "3.1 Function Parameterization", "content": "Inspired by (11) and (12), we parameterize the value function with parameters $\\theta \\in \\mathbb{R}^d$:\n$J(t, x; \\theta) = -\\frac{1}{2}k_1(t; \\theta)x^2 + k_3(t; \\theta),$\nand parameterize the policy with parameters $\\phi = (\\phi_1, \\phi_2 > 0)^T$:\n$\\pi(u | x; \\phi) = \\mathcal{N}(u | \\phi_1x, \\phi_2).$\nNote that (12) suggests that the optimal feedback policy is time-dependent, whose variance depends explicitly on t. In our parameterization, the time-dependent variance of the Gaussian policies is replaced by a decaying schedule, called an exploration schedule, of $\\phi_2$ as a function of the number of iterations, to be presented shortly.\nHenceforth we assume that there are positive constants $c_1, c_2, c_3$ such that $1/c_2 \\le k_1(t; \\theta) \\le c_2$, $|k_1(t;\\theta)| \\le c_1$ and $|k_3(t;\\theta)| \\le c_3$, for any $0 \\le t \\le T$. These assumptions are consistent with the fact that the corresponding functions satisfy the same conditions when the model parameters are known."}, {"title": "3.2 Policy Evaluation", "content": "Policy evaluation (PE) is generally a key step in RL to learn the value function of a given control policy.\nThe general continuous-time PE method developed in (Jia and Zhou, 2022a) dictates that one first parameterizes the value function $J(\\cdot,\\cdot; \\pi)$ and the policy $\\pi$ by (13) and (14) respectively (with a slight abuse of notation), with the corresponding $\\rho^{\\pi} (t) = \\rho(t; \\phi)$, and then updates $\\theta$ in an offline learning setting:\n$\\theta \\leftarrow \\theta + \\alpha \\int_0^T \\frac{\\partial}{\\partial \\theta} J(t, x(t); \\theta) \\Big[dJ(t, x(t); \\theta) - \\frac{1}{2}Q x^{\\pi} (t)^2dt - \\gamma\\rho(t; \\phi)dt\\Big],$\nwhere $\\alpha$ is the learning rate.\nIntriguingly, however, our subsequent theoretical proofs indicate that the convergence and regret results depend only on the bounds (i.e. the constants $c_1, c_2$ and $c_3$) of the functions $k_1$ and $k_2$, not on the specific forms of these functions. This feature is due to the special class of LQ control problems we are tackling. As a result, in our numerical experiments we actually fix a value function (or equivalently $\\theta$) throughout without updating it."}, {"title": "3.3 Policy Iteration", "content": "Having learned the value function associated with a Gaussian policy, the next step is to improve the policy by updating $\\phi = (\\phi_1, \\phi_2)^T$. For $\\phi_1$, we employ the continuous-time policy gradient (PG) method established in (Jia and Zhou, 2022b) to get the following updating rule:\n$\\Phi_1 \\leftarrow \\Phi_1 + \\alpha Z_1(T),$\nwhere $\\alpha$ is the learning rate, and\n$Z_1(s) = \\int_0^s \\Big\\{\\frac{\\partial \\log \\pi}{\\partial \\phi_1}(u(t) | x(t); \\phi) \\Big[dJ (t, x(t); \\theta) - \\frac{1}{2}Qx(t)^2dt +\\gamma\\rho^{\\pi} (t, \\phi) dt\\Big] + \\frac{\\partial \\rho}{\\partial \\phi_1} (t, \\phi) dt\\Big\\}, 0 \\le s \\le T.$\nAs discussed earlier, the other parameter, $\\phi_2$, controls the level of exploration. In our algorithm, we set a deterministic schedule of this parameter which decreases to 0 as the number of iterations grows. Specifically, we set $\\phi_{2,n} = \\frac{1}{b_n}$, where $b_n \\uparrow \\infty$ is specified in Theorem 1 below. The order"}, {"title": "3.4 Projections", "content": "Our updating rules for the parameters $\\theta$ and $\\phi$ are types of stochastic approximation (SA), a technique pioneered by (Robbins and Monro, 1951). To tailor the general SA algorithms to our specific requirements\u2014primarily to circumvent issues like extreme state values and unbounded estimation errors\u2014we include projection, a technique originally proposed by (Andrad\u00f3ttir, 1995). The projection maps do not depend on prior environmental knowledge, allowing our method to remain model-free while ensuring that the learning regions expand to cover the entire parameter space over time.\nDefine $\\Pi_K(x) := \\arg \\min_{y \\in K} |y - x|^2$ to be a general projection mapping a point x onto a given set K. Let\n$K_{\\theta,n} = {\\theta_n \\in \\mathbb{R}^d | 1/c_2 \\le k_1(t; \\theta_n) < c_2, |k_1(t; \\theta_n)| \\le c_1, |k_3(t; \\theta_n)| \\le c_3},\\newline K_{1,n} = {\\phi_{1,n} \\in \\mathbb{R} ||\\phi_{1,n}| \\le c_{1,n}},$\nwhere $c_1, c_2, c_3$ are hyperparameters, and $\\{c_{1,n}\\}$ is an increasing sequence to be specified in Theorem 1 below. Note here the choice of $K_{\\theta,n}$ is specific to the special class of LQ problems under consideration - it is independent of n as the regret analysis does not rely on the convergence of $\\theta_n$. For a general problem, $K_{\\theta,n}$ needs to be an expanding sequence of sets.\nWith projection the updating rules for $\\theta$ and $\\phi_1$ in (15) and (16) are modified to\n$\\theta_{n+1} \\leftarrow \\Pi_{K_{\\theta,n+1}}\\Big(\\theta_n + \\alpha_n \\frac{\\partial}{\\partial \\theta}(t, x_n(t); \\theta_n) \\Big[dJ (t, x_n(t); \\theta_n) - \\frac{1}{2}Q x_n(t)^2dt + \\gamma\\rho^{\\pi} (t, \\phi_n) dt\\Big]\\Big),$\n$\\phi_{1,n+1} \\leftarrow \\Pi_{K_{1,n+1}}(\\phi_{1,n} + \\alpha_n Z_{1,n}(T));$"}, {"title": "3.5 Discretization", "content": "Our approach for continuous-time RL is characterized by carrying out the entire analysis in the continuous-time setting and discretizing time only at the final implementation stage. The iterations in (19) and (20) involve integrals that can be computed only by approximated discretized summations as well as the dJ term that can be approximated by the temporal difference between two consecutive time steps. We therefore discretize the interval [0,T] into uniform time intervals of length $\\Delta t$, leading to the following schemes:\n$\\theta_{n+1} \\leftarrow \\Pi_{K_{\\theta,n+1}} \\Big(\\theta_n + \\alpha_n \\sum_{k=0}^{\\lfloor\\frac{T}{\\Delta t}\\rfloor} \\frac{\\partial}{\\partial \\theta} J(t_k, x_n (t_k); \\theta_n) \\Big[ J (t_{k+1}, x_n (t_{k+1}); \\theta_n) - J (t_k, x_n (t_k); \\theta_n) - \\frac{1}{2}Q x_n(t_k)^2 \\Delta t + \\gamma\\rho (t_k, \\Phi_n) \\Delta t \\Big] \\Big),$\n$\\Phi_{1,n+1} \\leftarrow \\Pi_{K_{1,n+1}} \\Big( \\Phi_{1,n} + \\alpha_n \\sum_{k=0}^{\\lfloor\\frac{T}{\\Delta t}\\rfloor-1} \\Big\\{\\frac{\\partial \\log \\pi}{\\partial \\phi_1}(u_n(t_k) | t_k,x_n(t_k); \\Phi_n) \\Big[J (t_{k+1}, x_n (t_{k+1}); \\theta_n) - J (t_k, x_n (t_k); \\theta_n) - \\frac{1}{2}Q x_n (t_k)^2 \\Delta t \\Big] + \\frac{\\partial \\rho}{\\partial \\phi_1}(t_k, \\Phi_n) \\Delta t \\Big\\}\\Big).$"}, {"title": "3.6 RL-LQ Algorithm", "content": "The analysis above leads to the following RL algorithm for the LQ problem:"}, {"title": "4 Regret Analysis", "content": "This section presents the main result of the paper - a sublinear regret bound of the RL-LQ algo-rithm, Algorithm 1. For that, we need to first examine the convergence property and convergence rate of the parameter $\\phi_{1,n}$, whose analysis forms the theoretical underpinning of the algorithm. Proofs of the results in this section are provided in Appendices A and B."}, {"title": "4.1 Convergence of $\u03c6_{1,n}$", "content": "The following theorem shows the convergence and convergence rate of the parameter $\\phi_{1,n}$.\nTheorem 1. In Algorithm 1, let the hyperparameters $c_1, c_2, c_3$ and $\\gamma$ be fixed positive constants. Set\n$\\alpha_n = \\frac{\\alpha}{n^{\\frac{3}{4}}},\\; b_n = 1 \\vee \\frac{1}{(n+\\beta)^{\\frac{1}{\\alpha}}}, \\qquad c_{1,n} = 1 \\vee (\\log \\log n)^{\\frac{1}{4}},$\nwhere $\\alpha > 0$ and $\\beta > 0$ are constants. Then,\n(a) as n \u2192 \u221e, $\\phi_{1,n}$ converges almost surely to $\\phi_1^* = -\\frac{B+CD}{D^2}$, and\n(b) for any n, $E[|\\phi_{1,n} - \\phi_1^*|^2] \\leq c\\frac{(\\log n)^p(\\log \\log n)}{n^2}$ for some positive constants c and p.\nThese results ensure the convergence of the learned policy. Moreover, it is a prerequisite for deriving the regret bound of Algorithm 1."}, {"title": "4.2 Regret Bound", "content": "A regret bound measures the cumulative derivation (over episodes) of the value functions of the learned policies from the oracle optimal value function. A sublinear regret bound guarantees an almost optimal performance of the RL policy in the long run.\nDenote\n$J(\\phi_1, \\phi_2) = E \\Big[\\int_0^T (-\\frac{1}{2}Qx^{\\pi}(s)^2) ds - \\frac{1}{2}Hx^{\\pi} (T)^2|x^{\\pi}(0) = x_0\\Big]; \\text{where } \\pi = \\mathcal{N}(\\cdot|\\phi_1x, \\phi_2).$\nSo $J(\\phi_1, \\phi_2)$ is the value of the Gaussian policy $\\mathcal{N}(\\cdot|\\phi_1x, \\phi_2)$ assessed using the original objective function (i.e. one without the entropy regularization term). Clearly, $J(\\phi_1^*, 0)$ is the oracle value of the original problem.\nTheorem 2. Under the assumptions of Theorem 1, applying Algorithm 1 results in a cumulative regret bound over N episodes given by:\n$\\sum_{n=1}^N E[J(\\phi_1^*, 0) - J(\\phi_{1,n}, \\Phi_{2,n})] \\leq c + cN^{\\frac{3}{4}} (\\log N)^{\\frac{p+1}{2}}(\\log \\log N)^{\\frac{1}{2}},$\nwhere c > 0 is a constant independent of N, and p is the same constant appearing in Theorem 1."}, {"title": "5 Numerical Experiments", "content": "This section reports the results of numerically evaluating the convergence rate of $\\phi_{1,n}$ and the sublinear regret bound of our RL-LQ algorithm, compared with a benchmark algorithm. The benchmark is based on the model-based methods in (Basei et al., 2022; Szpruch et al., 2024) adapted to our setting of state- and control-dependent volatility.\nA Modified Model-Based Algorithm. The algorithms proposed in (Basei et al., 2022; Szpruch et al., 2024) are designed to estimate parameters A and B in the drift term under the constant volatility setting. To compare with our algorithm tailored for state- and control-dependent volatil-ities, we extend their methods to include estimating the parameters C and D. The details of this modified algorithm are described in Appendix C.1.\nSimulation Setup. In our simulation we set the model parameters (A, B, C, D, Q, H, $x_0$, T) to be all 1, and set the exploration schedule $b_n = 0.2(n + 1)^{1/4}$. Other sequences such as that of the learning rate $\\{a_n\\}$ are configured according to the assumptions stated in Theorem 1 and Subsection 3.1; for details see Appendix C.2. In each experiment we execute both Algorithm 1 and Algorithm 2 over N = 400,000 iterations, while we replicate the experiment independently for 120 times to draw statistical conclusions.\nFigures 1 and 2 compare the mean-squared convergence rates of $\\phi_{1,n}$ for our model-free Al-gorithm 1 and the model-based Algorithm 2, using a log-log plot of Mean Squared Error (MSE) versus iterations. The fitted linear regression shows our model's slope of -0.5, confirming Theorem 1 and outperforming the model-based benchmark slope of -0.08.\nA comparison of regrets between Algorithms 1 and 2 is presented in Figures 3 and 4. The former yields a regret slope of around 0.73, which is close to the theoretical bound stipulated in Theorem 2 and superior to the slope of 0.88 achieved by the latter.\nThese experimental results support the theoretical claims and demonstrate the outperformance of our RL-LQ algorithm compared with its model-based counterpart in terms of both the conver-gence rates of the policy parameters and the regret bounds."}, {"title": "6 Conclusions", "content": "This paper is the first to derive a convergence rate and a regret bound within the model-free framework of continuous-time entropy-regularized RL for controlled diffusion processes initiated by Wang et al. (2020). Here, by model-free, we mean that neither theory nor algorithm involves estimating model parameters. While it deals with the LQ case, it treats the case in which the diffusion term depends both on state and control, one that has not been studied in the RL literature to our best knowledge.\nNote that the paper assumes scalar-valued states and controls for notational simplicity, though extending the results and algorithm"}]}