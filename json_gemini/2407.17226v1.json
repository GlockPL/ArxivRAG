{"title": "Sublinear Regret for An Actor-Critic Algorithm in Continuous-Time Linear-Quadratic Reinforcement Learning", "authors": ["Yilie Huang", "Yanwei Jia", "Xun Yu Zhou"], "abstract": "We study reinforcement learning (RL) for a class of continuous-time linear-quadratic (LQ) control problems for diffusions where volatility of the state processes depends on both state and control variables. We apply a model-free approach that relies neither on knowledge of model parameters nor on their estimations, and devise an actor-critic algorithm to learn the optimal policy parameter directly. Our main contributions include the introduction of a novel exploration schedule and a regret analysis of the proposed algorithm. We provide the convergence rate of the policy parameter to the optimal one, and prove that the algorithm achieves a regret bound of O(N) up to a logarithmic factor. We conduct a simulation study to validate the theoretical results and demonstrate the effectiveness and reliability of the proposed algorithm. We also perform numerical comparisons between our method and those of the recent model-based stochastic LQ RL studies adapted to the state- and control-dependent volatility setting, demonstrating a better performance of the former in terms of regret bounds.", "sections": [{"title": "1 Introduction", "content": "Linear-quadratic (LQ) control, in which the system dynamics are linear in the state and control variables while the reward is quadratic in them, takes up a center stage in classical model-based control theory when the model parameters are assumed to be given and known. The reason is twofold: LQ control can be solved explicitly and elegantly, and it can be used to approximate more complicated nonlinear control problems. Detailed theoretical accounts in the continuous-time setting can be found in Anderson and Moore (2007) for deterministic control (i.e., dynamics are"}, {"title": "2 Problem Formulation and Preliminaries", "content": "We begin by recalling the classical stochastic LQ control formulation and main results. Denote by xu = {xu(t) \u2208 R : 0 \u2264 t \u2264 T} the state process under a control process u = {u(t) \u2208 R : 0 \u2264 t \u2264 T}, whose dynamic is described by the following stochastic differential equation (SDE):\n dxu(t) = (Axu(t) + Bu(t))dt + (Cxu(t) + Du(t))dW(t), (1)\nwhere x\u00b2(0) = x0 \u2260 0 and W = {W(t) \u2208 R : 0 \u2264 t \u2264 T} is a standard Brownian motion. The goal of the control problem is to choose a control process u to maximize the expected value of a"}, {"title": "2.1 Classical Stochastic LQ Control", "content": "quadratic objective functional:\n max E[\u222bT012Qx\u2122(t)2dt\u221212Hx\u2122(T)2] (2)\nwhere Q \u2265 0 and H \u2265 0 are given weighting parameters. One can define the optimal value function\n VCL(t, x) = max E[\u222bTts12Qxu(s)2ds\u221212Hxu(T)2|x(t) = x]. (3)\nWhile the existing works on stochastic LQ RL assume the diffusion coefficient to be a constant, control- and state-dependent diffusion terms appear in many applications. On the other hand, there is no running control reward in our problem which is a crucial assumption for our approach to work. This class of problems cover important applications such as the mean-variance portfolio selection (Zhou and Li, 2000; Dai et al., 2023; Huang et al., 2022).\nIf the model parameters A, B, C, D, Q, and H are all known, this problem can be solved explicitly as detailed in (Yong and Zhou, 1999, Chapter 6). Specifically, the optimal value function and optimal feedback control policy are respectively\n VCL(t, x) = \u2212x[Q2\u039b(H\u2212C\u039b)e\u039b(t\u2212T)]\u22121x2, (4)\n UCL(t, x) = B+CDD2x, (5)\nwhere \u039b = 12(B2 + 2BCD \u2013 2AD2).\nThroughout this paper (including the appendices) we use c or its variants for generic constants (depending only on the model parameters A, B, C, D, Q, H, 10 and T) whose values may change from line to line."}, {"title": "2.2 RL Theory for LQ", "content": "In most real-life problems, it is often unrealistic to assume precise knowledge of the parameters such as A, B, C, and D. These problems call for RL which differs fundamentally from the traditional estimate-then-optimize methods. The essence of RL is to strike an exploration-exploitation balance by strategically exploring the unknown environment (Sutton and Barto, 2018). To achieve this, RL employs randomized controls to capture exploration where control processes u are sampled from a process \u03c0 = {\u03c0(\u00b7,t) \u2208 P(R) : 0 \u2264 t \u2264 T} of probability distributions with P(R) being the space"}, {"title": "3 A Continuous-Time RL Algorithm", "content": "This section presents the steps of designing a continuous-time RL algorithm for solving our LQ problem, including function parameterization, policy evaluation and policy gradient.1 We will introduce various techniques such as exploration scheduling and projection for deriving the convergence rate of the policy parameter and the regret bound. We will also describe time discretization for final implementation."}, {"title": "3.1 Function Parameterization", "content": "Inspired by (11) and (12), we parameterize the value function with parameters \u03b8\u2208Rd:\n J(t, x; \u03b8) = \u221212k1(t; \u03b8)x2 + k3(t; \u03b8), (13)\nand parameterize the policy with parameters $ = (\u00a21, \u00a22 > 0)T:\n \u03c0(u | x; \u03c6) = N(u | \u03a61x, \u03a62). (14)\nNote that (12) suggests that the optimal feedback policy is time-dependent, whose variance depends explicitly on t. In our parameterization, the time-dependent variance of the Gaussian policies is replaced by a decaying schedule, called an exploration schedule, of $2 as a function of the number of iterations, to be presented shortly.\nHenceforth we assume that there are positive constants C1, C2, C3 such that 1/c2 \u2264 k\u2081(t; 8) \u2264 C2, |k\u2081(t;0)| \u2264 c\u2081 and |k3(t;0)| \u2264 C3, for any 0 \u2264 t \u2264 T. These assumptions are consistent with the fact that the corresponding functions satisfy the same conditions when the model parameters are known."}, {"title": "3.2 Policy Evaluation", "content": "Policy evaluation (PE) is generally a key step in RL to learn the value function of a given control policy.\nThe general continuous-time PE method developed in (Jia and Zhou, 2022a) dictates that one first parameterizes the value function J(\u00b7,\u00b7; \u03c0) and the policy \u3160 by (13) and (14) respectively (with a slight abuse of notation), with the corresponding p\u2122 (t) = p(t; $), and then updates e in an offline learning setting:\n 0 - 0 + \u03b1 \u222bT0(\u2202\u2202\u03b8J(t, x(t); \u03b8)dJ(t, x(t); \u03b8) \u2013 Qx ( 2dt \u2013 \u03b3p(t; \u03c6)dt), (15)\nwhere a is the learning rate.\nIntriguingly, however, our subsequent theoretical proofs indicate that the convergence and regret results depend only on the bounds (i.e. the constants C1, C2 and c3) of the functions k\u2081 and k2, not on the specific forms of these functions. This feature is due to the special class of LQ control problems we are tackling. As a result, in our numerical experiments we actually fix a value function (or equivalently ) throughout without updating it."}, {"title": "3.3 Policy Iteration", "content": "Having learned the value function associated with a Gaussian policy, the next step is to improve the policy by updating $ = ($1, $2)T. For $1, we employ the continuous-time policy gradient (PG) method established in (Jia and Zhou, 2022b) to get the following updating rule:\n \u03a61 \u2190 \u03a61 + \u03b1\u03961(T), (16)\nwhere a is the learning rate, and\n Z1(s) = \u222bs0{\u2202\u2202\u03c6log \u03c0(u(t) | x(t); \u03c6)[dJ (t, x(t); \u03b8) \u2212 12Qx(t)2dt+yp(t, \u03c6) dt] + \u2202\u2202\u03c6p(t, \u03c6) dt}, 0 \u2264 s \u2264 T. (17)\nAs discussed earlier, the other parameter, $2, controls the level of exploration. In our algorithm, we set a deterministic schedule of this parameter which decreases to 0 as the number of iterations grows. Specifically, we set $2,n = bn1 where bn \u2191 \u221e is specified in Theorem 1 below. The order"}, {"title": "3.4 Projections", "content": "Our updating rules for the parameters 0 and $ are types of stochastic approximation (SA), a technique pioneered by (Robbins and Monro, 1951). To tailor the general SA algorithms to our specific requirements\u2014primarily to circumvent issues like extreme state values and unbounded estimation errors\u2014we include projection, a technique originally proposed by (Andrad\u00f3ttir, 1995). The projection maps do not depend on prior environmental knowledge, allowing our method to remain model-free while ensuring that the learning regions expand to cover the entire parameter space over time.\nDefine \u03a0\u03ba(x) := arg miny\u2208K |y - x|2 to be a general projection mapping a point x onto a given set K. Let\n Ko,n = {On \u2208 Rd|1/C2 < k\u2081(t; On) < C2, k\u2081(t; On)| \u2264 C1, k3(t; 0n)| \u2264 c3}, (18)\nK1,n = {$1,n \u20ac R||$1,n] < C1,n},\nwhere C1, C2, C3 are hyperparameters, and {C1,n} is an increasing sequence to be specified in Theorem 1 below. Note here the choice of Kon is specific to the special class of LQ problems under consideration - it is independent of n as the regret analysis does not rely on the convergence of On. For a general problem, Kon needs to be an expanding sequence of sets.\nWith projection the updating rules for 0 and $1 in (15) and (16) are modified to\n \u03b8n+1 \u2190 \u03a0\u039a\u03b8,n+1(\u03b8n + an \u2202\u2202\u03b8J(t, xn(t); \u03b8n)[dJ (t, xn(t); \u03b8n) \u2212 12Qx2ndt + \u03b3p(t, \u03c6n) dt]), (19)\n $1,n+1 \u2190 \u03a0K1,n+1($1,n + anZ1,n(T)); (20)"}, {"title": "3.5 Discretization", "content": "Our approach for continuous-time RL is characterized by carrying out the entire analysis in the continuous-time setting and discretizing time only at the final implementation stage. The iterations in (19) and (20) involve integrals that can be computed only by approximated discretized summations as well as the dJ term that can be approximated by the temporal difference between two consecutive time steps. We therefore discretize the interval [0,T] into uniform time intervals of length At, leading to the following schemes:\n \u03b8n+1 \u2190 \u03a0\u039a\u03b8,n+1(\u03b8n + \u03b1\u03c0 \u2211kJ=0\u2202\u2202\u03b8J(tk, xn (tk); \u03b8n)[ J (tk+1, xn (tk+1); \u03b8n) - J (tk, In (tk); On)\u2212 12Qxn(tk)2\u2206t + \u03b3p (tk, \u03a6n) \u2206t]), (22)\n \u03a61,n+1 \u2190 \u03a0K1,n+1(\u03a61,n + an [\u22121]\u2211k=0\u2202\u2202\u03c6log \u03c0(un(tk) | tk,Xn(tk); \u03a6n)[ J (tk+1, Xn (tk+1); \u03b8\u03b7)\u2212J (tk, xn (tk); On)\u2212 12Qxn (tk)2\u2206t + \u03b3p (tk, \u03a6\u03b7) \u2206t]). (23)"}, {"title": "3.6 RL-LQ Algorithm", "content": "The analysis above leads to the following RL algorithm for the LQ problem:"}, {"title": "4 Regret Analysis", "content": "This section presents the main result of the paper a sublinear regret bound of the RL-LQ algorithm, Algorithm 1. For that, we need to first examine the convergence property and convergence rate of the parameter $1,n, whose analysis forms the theoretical underpinning of the algorithm. Proofs of the results in this section are provided in Appendices A and B."}, {"title": "4.1 Convergence of $1,n", "content": "The following theorem shows the convergence and convergence rate of the parameter $1,\u03b7\u00b7\nTheorem 1. In Algorithm 1, let the hyperparameters C1, C2, C3 and y be fixed positive constants. Set\n\u03b1n = \u03b11(n+\u03b2)34, bn = 1 \u2228 (n+\u03b2)14, C1,n = 1 \u2228 (log log n)14, where a > 0 and \u1e9e > 0 are constants. Then,\n(a) as n \u2192 \u221e, $1,n converges almost surely to 1\u2217=B+CDD2, and\n(b) for any n, E[|41,n - 1\u2217|2] <c(logn)p(log logn)rn2for some positive constants c and p.\nThese results ensure the convergence of the learned policy. Moreover, it is a prerequisite for deriving the regret bound of Algorithm 1."}, {"title": "4.2 Regret Bound", "content": "A regret bound measures the cumulative derivation (over episodes) of the value functions of the learned policies from the oracle optimal value function. A sublinear regret bound guarantees an almost optimal performance of the RL policy in the long run.\nDenote\n J(\u03c61, \u03c62) = E [\u222bT0(\u221212Qx\u03c0(s)2) ds \u2212 12Hx\u03c0(T)2|x\u03c0(0) = x0], where \u03c0 = N(\u00b7|\u03c61x, \u03c62). (24)\nSo J(1, 2) is the value of the Gaussian policy N(\u00b7|\u04441x, \u04442) assessed using the original objective function (i.e. one without the entropy regularization term). Clearly, J(1,0) is the oracle value of the original problem.\nTheorem 2. Under the assumptions of Theorem 1, applying Algorithm 1 results in a cumulative regret bound over N episodes given by:\n\u2211Nn=1E[J(1,0) \u2013 J(\u03c61,n, \u03a62,n)] \u2264 c + cN34(log N)p+12(log log N)2, where c > 0 is a constant independent of N, and p is the same constant appearing in Theorem 1."}, {"title": "5 Numerical Experiments", "content": "This section reports the results of numerically evaluating the convergence rate of $1,n and the sublinear regret bound of our RL-LQ algorithm, compared with a benchmark algorithm. The"}, {"title": "6 Conclusions", "content": "This paper is the first to derive a convergence rate and a regret bound within the model-free framework of continuous-time entropy-regularized RL for controlled diffusion processes initiated by Wang et al. (2020). Here, by model-free, we mean that neither theory nor algorithm involves estimating model parameters. While it deals with the LQ case, it treats the case in which the diffusion term depends both on state and control, one that has not been studied in the RL literature to our best knowledge.\nNote that the paper assumes scalar-valued states and controls for notational simplicity, though extending the results and algorithm to the multi-dimensional case presents no essential technical difficulty.\nThere are several limitations in the setting or results of the paper. First, the quadratic objective functional (2) has no running reward from controls, a key assumption needed to simplify our analysis so that it suffices to consider only (time-invariant) stationary policies. Second, we are unable to achieve a better sublinear regret, e.g., a square-root one, which is typical in episodic RL algorithms"}, {"title": "A A Proof of Theorem 1", "content": "Let xn = {xn(t) : 0 \u2264 t \u2264 T} be the sample state trajectory in the n-th iteration that follows the dynamics:\n dxn(t) = (Axn(t) + Bun(t))dt + (Cxn(t) + Dun(t))dWn(t), 0 \u2264 t \u2264T, (25)\nwhere Wn is a standard Brownian motion in the n-th iteration, and the policy un(t) | Xn(t) ~ N(\u00b7|\u03a61,nxn(t), $2,n) independent of Wn(t).\nRecall Z\u2081(\u00b7) defined by (17), and Z1,n(T) defined by (21) as the value of Z1(T) at the n-th iteration. Moreover, let \u017d1,n(T) denote an empirical realization of Z1,n(T) observed in the algorithm iterations, which includes inherent random errors. The expectation of Z1,n(T) conditioned on the parameters is denoted by\n h1(\u03a61,n, \u03a62,n; \u03b8\u03b7) = E[Z1,n(T) | \u03b8\u03b7, \u03a6\u03b7],\nand the noise contained in 21,n(T) is defined as\n \u00a71,n = 21,n(T) \u2013 h1(\u03a61,\u03b7, \u03a62,\u03b7; \u03b8\u03b7).\nHence, the updating rule for $1 is given by:\n \u03a61,n+1 = \u03a0\u039a1,n+1(\u00d81,n + an[h1(\u00a21,n, \u03a62,n; \u03b8n) + \u00a71,n]). (26)\nTo prove Theorem 1, we need a series of lemmas to adapt the general stochastic approximation techniques and results (e.g. (Andrad\u00f3ttir, 1995) and (Broadie et al., 2011)) to our specific setting."}, {"title": "A.1 Moment Estimates", "content": "Let {x(t) : 0 \u2264 t \u2264 T} be the state process under the policy (14) following the dynamic (6). The following lemma gives some moment estimates of x(t) in terms of \u00fe.\nLemma 1. There exists a constant c > 0 (that only depends on A,B,C,D) such that\nE[x(t)] = x0e(A+B\u00a21)t,"}, {"title": "A.2 Mean Increment", "content": "We now analyze the mean increment h\u2081($1,n, $2,n; 0n) in the updating rule (26). First, note that \u222bs0Zn(t)dWn(t) is a martingale by virtue of Lemma 1 and (32). Taking integration and expectation in (31), we get\n E[Z1,n(s)] = E{12\u03c62,n\u222bs0xn(t)[\u22122Bk1(t; 0n)xn(t)(un(t) \u2212 $1,nxn(t))2-2CDk1(t; 0n)xn(t)(Un(t) \u2013 $1,nxn(t))- 2D2k1(t; 0n)xn(t)(un(t) \u2013 $1,nxn(t))\u00b2]dt\u2212 \u222bs0k1(t; 0n)(B + CD + D241,n)E[xn(t)\u00b2]dt}, 0 \u2264 s < T. (33)\nHence\n h1(\u00a21,n, \u03a62,n; \u03b8n) = \u2212(B+CD+D2\u03c61,n)\u222bT0k1(t;0n)E[xn(t)2]dt (34)\n= \u2212($1,n \u2212 1)l($1,n, \u03a62,n; \u03b8\u03b7),\nwhere\n 1(\u00a21,\u03b7, \u03a62,\u03b7; \u03b8\u03b7) = D2\u222bT0k1(t; On)E[xn(t)\u00b2]dt. (35)\nNext we study h1($1,n, \u03a62,n; On). It follows from (27) that a(61) is a quadratic function of $1 and \u03b1(\u03c61) \u2265 \u2013B2+2BCD-2AD2D2. Hence, by (29), we have\n EXn(t)2=D22,ne\u222bt0ea(\u03c61,n)(t\u2212s)ds+x0ea(\u03c61,n)t \u2265 x20ea(\u03c61,n)t \u2265 c (36)\nwhere c > 0 is a constant independent of n. Thus,\n 1(\u00a21,\u03b7, \u03a62,\u03b7; \u03b8\u03b7) = D2\u222bT0k1(t; On)E[xn(t)2]dt (37)\n> D2\u222bT0(1/c2)cdt = D2(1/c2)cT \u2265 c\nwhere 0 <c< 1 is a constant independent of n."}, {"title": "A.3 Almost Sure Convergence of $1,n", "content": "We first prove the almost sure convergence of $1,n. Part (a) of Theorem 1 is a special case of the following result when \u1e9e1,n = 0.\nTheorem 3. Assume the noise term \u00a71,n satisfies E [E1,n|Gn] = B1,n and\n E[|\u03be1,n \u2212 \u03b21,n|2| Gn] <cbn(1 + \u03c681,n|8 + (log bn)8) exp {c|\u03c61,n|6}, (39)\nwhere c > 0 is a constant independent of n, and {Gn} is the filtration generated by {0m, \u04241,m, \u04242,m, m ="}, {"title": "A.4 Mean-Squared Error of $1,n \u2013", "content": "In this section, we establish the convergence rate of $1,n to \u2020 stipulated in part (b) of Theorem 1.\nThe following lemma shows a general recursive relation satisfied by some sequences of learning rates.\nLemma 3. For any w > 0, there exists positive numbers a > 11 and \u1e9e \u2265 max(wa-1, w\u00b2a\u00b3) such that the sequence an = a(n+\u03b2)34 satisfies an \u2264 an+1(1 + wan+1) for all n \u2265 0."}, {"title": "B A Proof of Theorem 2", "content": "This section is dedicated to proving Theorem 2, which pivots around analyzing the value function Jin terms of $1 and $2."}, {"title": "B.1 Analyzing J($1,$2)", "content": "Recall that J($1, \u03a62) = J(0, xo; \u03c0) with \u03b3 = 0, where \u03c0 = N(\u00b7|\u03a61x, \u03a62).\nLemma 4. The value function can be expressed as J(\u00a21,\u00a22) = f(a(\u00a21)) + $29(a(\u03c61)), where a(\u03c61) is defined by (27) and the functions f and g are defined as follows:\n f(a) = {x3(\u2212H-QT)2 if a = 0,(Q\u2212eaTQ \u2212 HeaTa)4ax20 if a \u2260 0, (49)\n g(a) = {D2T(\u2212H\u2212QT)4 if a = 0,2a2(QTa + Q + Ha \u2212 eaTQ \u2212 HeaTa)x20 if a \u2260 0. (50)\nProof. The value function of the policy N(\u00b7|\u04441x, \u03c62) with y = 0 is (with a slight abuse of notation)\n J(t, x; 1, 2) = E[\u222bTt\u221212Qx$(s)2ds\u221212Hx$(T)2|x(t) = x]\nwhere $ = ($1, $2) and {x$(s) : t \u2264 t \u2264 T} is the corresponding exploratory state process. By the Feynman-Kac formula, J(\u00b7,\u00b7; 1, \u03a62) satisfies\n {vt + 12\u222bR(Cx + Du)2N(u|\u00a21x, \u03c62)duvxx + (Ax + B\u222bR uN (u|1x, $2)du)vx \u2013 Qx2 = 0,v(\u03a4,x) = \u2212Hx2. (51)\nThe solution to the above PDE is\n J(t, x; 1, 2) = [Q2\u03b1(\u03c61)\u2212ea(\u03c61)(T\u2212t)(QQ\u03b1(\u03c61)+H)](\u221212)Q2\u03b1(\u03c61)Q+H\u03b1(\u03c61)\u2212QT\u03b1(\u03c61)2+(QQ\u03b1(\u03c61)+H)He2. (52)"}, {"title": "B.2 Regret Analysis", "content": "We now proceed to prove Theorem 2."}, {"title": "C Specifics of Numerical Experiments", "content": "This section provides implementation specifics of the numerical experiments described in Section 5. To ensure full reproducibility, necessary codes along with detailed instructions for conducting all the 120 independent experiments will be provided as a supplementary material in a zip file.\nThe section is organized into three subsections: the first one presents and explains the modified algorithm that adapts the model-based methods (Basei et al., 2022; Szpruch et al., 2024) to our setting involving state and control dependent volatility. The second one details the experimental conditions, parameter settings, and the overall framework used to validate our claims and assess the performance of our algorithmic enhancements. The third one describes the computational resources used for these experiments."}, {"title": "C.1 A Modified Model-Based Algorithm", "content": "The key component in the algorithms developed by (Basei et al., 2022; Szpruch et al., 2024) is to estimate the parameters A and B in the drift term whereas the diffusion term is assumed to be constant. Clearly, these algorithms cannot be used directly to our setting where the diffusion term is state- and control-dependent. Here we extend them to also including estimates of the parameters Cand D.\nSpecifically, in the n-th iteration, the policy is defined as\n un(t, x) = $1,nx, (60)\nwhere 61,n is distributed according to N(.- Bn+CnDn, Un), with un being a deterministic sequence defined by un = m+1, and Bn, Cn, Dn being the current estimation of hte parameter B, C, D. Applying this feedback policy to the classical dynamic (1) yields\n dxn(t) =(A + B1,n)xn(t)dt + (C + D41,n)xn(t)dWn(t)\n :=Pnxn(t)dt + Rnxn(t)dWn(t), (61)\nwhere Wn(t) is the Browian motion for the n-th iteration.\nGiven an observed state trajectory {xn(t) : 0 \u2264 t \u2264 T} following (61), we discretize it uniformly into m intervals resulting in the \"snapshots\u201d of the state, {xn(to), Xn (t1),...,xn(tm)}, and then employ a statistical approach to estimate P and R:\n p = logxn(tm)\u2212logxn(t0)T,R2n=\u2211mk=1(logxn(tk)\u2212logxn(tk\u22121))2T (62)\nParameters An and Bn are subsequently estimated via linear regression, using 01,n as the independent variable and \u00cen as the dependent variable. Similarly, parameters Cn and Dn are determined using quadratic regression, with din and 61,n as the independent variables and Ras the dependent variable. We enhance parameter estimation accuracy by incorporating an experience replay mechanism (Lillicrap et al., 2015; Mnih et al., 2015), which utilizes all historical data for ongoing updates.\nThe pseudocode for implementing this modified model-based algorithm is presented below:\""}, {"title": "C.3 Compute Resources", "content": "All experiments were performed on a MacBook Pro (16-inch, 2019) equipped with a 2.4 GHz 8-Core Intel Core i9 processor, 32 GB of 2667 MHz DDR4 memory, and dual graphics processors, comprising an AMD Radeon Pro 5500M with 8 GB and an Intel UHD Graphics 630 with 1536 MB.\nNot having a high-powered server, this consumer-grade laptop was sufficient to handle the com-putational task of conducting 120 independent experiments sequentially, each running for 400,000 iterations. The model-free actor-critic algorithm required approximately 26 hours for a complete sequential run, whereas the model-based plugin algorithm took about 83 hours. This significant difference in running times also demonstrates the efficiency of our model-free approach compared with the model-based one."}]}