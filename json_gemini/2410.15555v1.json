{"title": "Bayesian Concept Bottleneck Models with LLM Priors", "authors": ["Jean Feng", "Avni Kothari", "Luke Zier", "Chandan Singh", "Yan Shuo Tan"], "abstract": "Concept Bottleneck Models (CBMs) have been proposed as a compromise between white-box and black-box models, aiming to achieve interpretability without sacrificing accuracy. The standard training procedure for CBMs is to predefine a candidate set of human-interpretable concepts, extract their values from the training data, and identify a sparse subset as inputs to a transparent prediction model. However, such approaches are often hampered by the tradeoff between enumerating a sufficiently large set of concepts to include those that are truly relevant versus controlling the cost of obtaining concept extractions. This work investigates a novel approach that sidesteps these challenges: BC-LLM iteratively searches over a potentially infinite set of concepts within a Bayesian framework, in which Large Language Models (LLMs) serve as both a concept extraction mechanism and prior. BC-LLM is broadly applicable and multi-modal. Despite imperfections in LLMs, we prove that BC-LLM can provide rigorous statistical inference and uncertainty quantification. In experiments, it outperforms comparator methods including black-box models, converges more rapidly towards relevant concepts and away from spuriously correlated ones, and is more robust to out-of-distribution samples.", "sections": [{"title": "1 Introduction", "content": "Although machine learning (ML) algorithms have demonstrated remarkable predictive performance, many lack the interpretability and transparency necessary for human experts to audit their accuracy, fairness, and safety (Dwork et al., 2012; Rudin, 2018). This has limited their adoption in high-stakes applications such as medicine (Thirunavukarasu et al., 2023) and settings where regulatory agencies require algorithms to be explainable (Mesk\u00f3 and Topol, 2023).\nRecent works have explored Concept Bottleneck Models (CBMs) (Koh et al., 2020; Kim et al., 2023) as a potential solution: these methods leverage black-box algorithms to extract a small number of interpretable concepts, which are subsequently processed by a fully transparent tabular model to make predictions. This allows users to understand how exactly each concept contributes to the final prediction and, if necessary, modify the extracted concepts to fix model predictions.\nHowever, there are major limitations in the way CBMs are currently trained. They require human experts to identify a large pool of concepts a priori and annotate the concepts present in each observation. This heavy dependence on human experts is expensive and often impractical. More importantly, it is difficult to ensure this pool of concepts includes the truly relevant concepts. Consequently, resulting CBMs are often outperformed by their black-box counterparts (Yuksekgonul et al., 2022) and may even mislead users as to which concepts are truly relevant (Ramaswamy et al., 2023).\nConsider the task of predicting hospital readmission from patient notes. It is impossible to prespecify all possible related concepts. Not only are there many different concepts, but any single concept can also be refined. For instance, for the concept regarding the \"patient's smoking status,\" there is the more general concept regarding \"substance use,\" the correlated concept of \"whether a patient has quit smoking,\" and many more. While iterative refinement of candidate concepts may help, it would make learning a CBM even more burdensome.\nLLMs have the potential to address these challenges. They are cheaper than human experts, can provide (relatively) high-quality concept annotations, and have sufficient world knowledge to hypothesize useful concepts. However, applying LLMs as a simple stand-in for a human expert does not fully address the challenges of concept discovery. First, LLM queries, though cheaper than human experts, still have a non-zero cost, which practically limits the number of concepts one can explore. Second, searching over concepts using LLMs introduces a \"garden of forking paths\" and may lead to the selection of spuriously correlated concepts (Gelman and Loken, 2013). Finally, LLMs are imperfect query engines that can hallucinate, suggest bad candidate concepts, provide incorrect concept annotations, and may not even be self-consistent in their prior beliefs (Wang et al., 2023). As such, current methods that rely on LLMs to learn CBMs have struggled to achieve good performance, particularly in areas where the LLM's prior knowledge is not well-aligned with the truth (Ludan et al., 2023; Benara et al., 2024).\nTo address the aforementioned challenges, we show how wrapping LLMs within a Bayesian framework allows one to iteratively explore concepts in a way that is statistically principled and provides uncertainty quantification. An LLM can serve multiple purposes in this framework: it defines a prior over a potentially infinite set of candidate concepts, proposes concepts to explore during posterior sampling, and annotates observations with the candidate concepts. The procedure, which we refer to as BC-LLM, is designed to be computationally efficient; at each iteration, the LLM proposes a batch of candidate concepts so their respective annotations for each observation can be batched into a single LLM query. We prove that asymptotically, BC-LLM converges to the correct concepts even when the LLM prior is inconsistent or incompatible. Moreover, BC-LLM is highly generalizable and works across multiple modalities (text, images, and tabular data). Experiments across multiple datasets show that BC-LLM outperforms comparator methods and often even black-box models, converges more rapidly towards the true concepts and away from spuriously correlated ones, and is more robust to out-of-distribution (OOD) samples. Finally, when BC-LLM assisted a real-world hospital's data science team to revise an existing tabular ML model, clinicians found the resulting CBM to be substantially more interpretable and actionable than the prior model."}, {"title": "2 Related work", "content": "Prior CBM methods train on observations with hand-annotated (Koh et al., 2020; Kim et al., 2023; Yuksekgonul et al., 2023) and, more recently, LLM-annotated concepts (Patel et al., 2023; McInerney et al., 2023; Benara et al., 2024) for a set of prespecified concepts by human experts. These methods are known to be highly sensitive to the selected set of concepts (Ramaswamy et al., 2023). More recent works have suggested using LLMs to prespecify the list of concepts instead (Oikarinen et al., 2023; Yang et al., 2023), but this requires the LLM to have accurate prior knowledge about the supervised learning task.\nConsequently, recent works have hypothesized that taking an iterative approach can more effectively find a set of relevant concepts (Ludan et al., 2023; Liu et al., 2024a). These methods employ a boosting-like approach, where the LLM is provided a set of misclassified examples and asked to add/revise concepts to improve performance. However, the LLM can typically only review a few observations due to limits on context length, so the resulting concept proposals are far from optimal.\nAn alternative strategy is to conduct post-hoc distillation of a black-box model into interpretable concepts (Kim et al., 2018; Yeh et al., 2020; Zhao et al., 2024; Jiang et al., 2023), which can be used to learn an interpretable prediction model. Nevertheless, prior work has shown that the generated explanations are often not faithful to the black-box model, leading to poorer performance from the interpretable model (Doshi-Velez and Kim, 2017).\nAnother related line of work uses LLMs to describe conceptual differences between pairs of datasets (Zhong et al., 2023, 2022; Dunlap et al., 2024). However, existing methods are primarily suited for simple classification tasks, as they essentially learn a CBM with a single concept rather than a set of concepts.\nFinally, recent works have considered combining LLMs with Bayesian techniques (Yang et al., 2024; Liu et al., 2024b,c). Some have suggested that In-Context Learning (ICL) can be viewed as LLMs conducting Bayesian inference (Xie et al., 2021; Ye et al., 2024), though recent work has proven that ICL is not fully consistent with Bayesian inference (Falck et al., 2024). To our knowledge, this work is the first to leverage LLMs in a Bayesian framework for learning CBMs and show that rigorous statistical inference is feasible despite imperfections in LLM reasoning."}, {"title": "3 Method", "content": "Consider a dataset $D = \\{(x_1,y_1), ..., (x_n, y_n)\\}$, where $x_i$ are model inputs and $y_i$ are labels. Let $X := (x_1,x_2,...,x_n)$ and $y := (y_1,..., y_n)$. A concept $c$ defines a binary- or real-valued function $\\phi_c$ of a model input, e.g. \"Does the doctor note describe an elderly patient?\" where 1=yes and 0=no. A CBM selects a fixed number of $K$ concepts $c = (c_1,...,c_K)$ and fits a parametric model for $y$ using the extractions $(\\phi_{c_1}(X), ..., \\phi_{c_K}(X))$. For simplicity of presentation, our discussion will focus on logistic regression (LR)-based CBMs, although other choices are possible. In this case, the data is modeled by\n$p(Y = 1/X = x, \\theta, c) = \\sigma(\\sum_{k=1}^K \\theta_k \\phi_{c_k}(x) + \\theta_0)$,\nwhere $\\sigma$ is the sigmoid function and $\\theta = (\\theta_0,...,\\theta_K)$ are the parameters. We refer to $c$ as the \"support\" of the model. Throughout, we use capital and lowercase letters to differentiate between random variables and realized values, respectively. For instance, $C$ refers to a random concept whereas $c$ refers to a particular one."}, {"title": "3.1 Review: Bayesian variable selection", "content": "Learning CBMs shares many similarities with the more standard problem of learning sparse models in high-dimensional settings, where one must select among a finite number of features that have already been tabulated (Ishwaran et al., 2008; Castillo et al., 2015; Biswas et al., 2022). Here, we review the Bayesian solution for this more standard setting. In later sections, we describe extensions to the CBM setting, where there are a potentially infinite number of concepts and a cost associated with extracting each.\nGiven priors on $c$ and $\\theta$, the goal of Bayesian inference for sparse models in high-dimensional settings is to sample from the posterior $p(\\theta, c | y, X)$, which allows for uncertainty quantification of the true support and coefficients. Posterior samples also describe the uncertainty at a new point $x_0$ via the posterior predictive distribution $p(y_0 | x_0, y, X, c) = \\int\\int p(y_0|x_0, \\theta, c)p(\\theta, c | y, X) d\\theta dc$, which can be viewed as combining the posterior samples into an ensemble model. Factorizing the posterior as $p(\\theta, c | y, X) = p(\\theta | c, y, X)p(c | y, X)$, the first term describes the posterior for the model parameters and the second describes variable selection. As the former can be readily obtained by classical Bayesian inference for low-dimensional settings (e.g., posterior inference for LR coefficients), our discussion will focus on the latter.\nInference for $p(c | y, X)$ is typically achieved through Gibbs sampling. For each epoch, Gibbs rotates through indices $k = 1, . . ., K$, during which it replaces the $k$th concept in the current iterate $\\hat{c}$ by drawing from the posterior distribution for $C_k$ conditional on the other concepts $C_{-k}$. When one is unable to sample from the conditional posterior, Metropolis-within-Gibbs sampling can be used instead (Gilks et al., 1995). Rather than replacing the $k$th concept, it proposes a candidate concept $\\check{c}_k$ given the other concepts per some distribution $Q(C_k; C_{-k} = \\hat{c}_{-k})$ and accepts the candidate concept with probability $\\alpha$. This acceptance probability $\\alpha$, delineated in Algorithm 1, depends on the relative posterior probabilities of the candidate and existing concepts and is carefully designed so that the posterior distribution is stationary with respect to the sampling procedure."}, {"title": "3.2 Bayesian variable selection with LLMs", "content": "Given the Gibbs procedures described above, there are various ways to integrate LLMs for learning CBMs. To begin, we describe two na\u00efve approaches:\nApproach 1: Propose with LLM \"prior.\" Let the LLM define a conditional prior over concepts, i.e. $p(C_k | C_{-k})$, and propose concepts from this conditional prior, i.e. $Q(C_k; C_{-k}) = p(C_k|c_{-k})$. As concepts are generated in a data-agnostic fashion, the LLM may produce many irrelevant concepts that have low acceptance probabilities. So while Approach 1 may provide valid inference, it is inefficient and costly.\nApproach 2: Propose with LLM \"posterior.\" Let the LLM conduct posterior inference for the $k$th concept given all the data and all but the $k$th concept through ICL and let the LLM propose from this conditional posterior, i.e. $Q(C_k; C_{-k}) = p(C_k | C_{-k}, y, X)$. This simplifies Metropolis-within-Gibbs to Gibbs, which al-"}, {"title": "3.3 BC-LLM", "content": "We now describe the main steps in BC-LLM (Fig 1): the procedure is initialized in Step 0, and each iteration (Steps 1 to 4) samples a CBM. The samples then describe the posterior distribution over CBMs and form an ensemble prediction model (after removing those from the burn-in period). Depending on one's use case, the search over concepts can be guided by the LLM's prior knowledge about the target Y by injecting this information into the prompts (e.g. \"we are trying to predict readmission...\") or in an \"unbiased\" fashion by masking the meaning of Y (e.g. \"we are trying to predict some label Y...\"). Example prompts and other implementation details are provided in the Appendix.\nStep 0: Initialization and keyphrase extraction\nWe first use an LLM to extract \"keyphrases\" describing each observation x, which will later be used by the LLM to brainstorm candidate concepts. Here, we use \"keyphrases\" to refer to short phrases that describe the observation: keyphrases for images describe what's in the image (e.g. blue eyes), while keyphrases for text data can be phrases that appear in the text or summarize the main ideas (e.g. diabetes) (Papagiannopoulou and Tsoumakas, 2020). Unlike concepts, keyphrases do not define a function that outputs a value, though"}, {"title": "3.4 Computational cost", "content": "The key computational cost of BC-LLM is from querying the LLM. Running BC-LLM requires $O(nTK)$ queries, where T is the number of epochs. K is typically small by design, because large K leads to less interpretable CBMs. One may be concerned by T, as standard Bayesian inference procedures typically draw thousands of posterior samples. Nevertheless, combined with a greedy warm-start procedure, we found that a small T to be quite effective; our experiments go as low as T = 5. Generally speaking, if the goal is solely prediction accuracy, a small T may suffice. If the goal is uncertainty quantification for relevant concepts, a larger T (e.g., 10) is helpful for more complete exploration of concepts."}, {"title": "4 Experimental results", "content": "4.1 Experimental setup\nWe compare BC-LLM against One-pass summarization, where an LLM performs post-hoc distillation of the keyphrase model into a CBM (Pham et al., 2023), and Boosting, where an LLM iteratively adds concepts to a CBM by analyzing observations with large residuals (Ludan et al., 2023; Liu et al., 2024a). We also compare it to black-box and semi-interpretable models to assess whether BC-LLM can close the performance gap. When the true concepts are known, we also fit an Oracle CBM.\nMethods were evaluated with respect to predictive performance as measured by AUC and uncertainty quantification as measured by log likelihood (higher is better). When true concepts are known, we also quantify concept selection rates using \"concept precision,\" as defined by $\\frac{1}{K} \\sum_{k=1}^K P(C_k \\in c^* | y, X)$, and \"concept recall,\" as defined by $\\frac{1}{K} \\sum_{c \\in c^*} P(c \\in \\hat{C} | y, X)$, where $C$ is the posterior distribution over concept sets from BC-LLM or a single set of concepts learned by a non-Bayesian procedure. Quantifying a \"correct\u201d concept match requires going beyond exact string matching, as there are many possible refinements for a single concept. For instance, for the concept \"Does the note mention the patient smoking in the present or the past?\", there are rephrasings (\u201cDoes this note mention"}, {"title": "4.4 Augmenting an existing tabular model with real-world clinical notes", "content": "Finally, we demonstrate how BC-LLM can help ML developers integrate different data modalities to improve model performance. In particular, the data science team at the Zuckerberg San Francisco General Hospital (ZSFG) has both tabular data and unstructured clinical notes from the electronic health record (EHR). The team has so far fit a model using the tabular data to predict 30-day unplanned readmission risk for patients with congestive heart failure (CHF). Using CBMs, the team wants to assess if clinical notes contain valuable information to extract as additional features. Moreover, the learned model should be interpretable and easy to audit.\nTo address this, we extend BC-LLM by having the CBM take as inputs (i) the risk prediction from the original tabular model and (ii) K concepts extracted from the clinical notes. This can be viewed as revising the existing model by including additional features. We set to K = 4. The CBMs were trained on 1000 patients and evaluated on 500 held-out patients. Data in this experiment contained PHI, for which IRB ap-"}, {"title": "5 Discussion", "content": "BC-LLM is a new method for learning CBMs that iteratively proposes concepts using an LLM within a Bayesian framework, which allows for rigorous uncertainty quantification despite LLMs being prone to error and hallucinations. The method is highly general: it is compatible with various data modalities (text, images, and tabular data) and can be extended beyond the settings of binary and multiclass classification. The empirical results show that BC-LLM outperforms existing methods, learns to recover relevant concepts and downweight spurious ones, and can suggest useful features for ML developers to engineer. Code for running BC-LLM and reproducing results in the paper are available at https://github.com/jjfeng/bc-llm."}, {"title": "Appendix", "content": "A Multiple-Try Metropolis-Hastings\nIn this section, we provide intuition for the Multiple-Try Metropolis-Hastings partial posterior method (Algorithm 3). To draw connections with the original Multiple-Try Metropolis-Hastings method (Liu et al., 2000), we first work in a completely abstract setting.\nGiven a state space $\\Omega$, stationary distribution $\\pi$, and proposal transition kernel $T(y; x)$, define weights $w(x,y) = \\pi(x)T(y; x)$. Note that under regular Metropolis-Hastings, the acceptance ratio for proposing the state $y$ given a state $x$ is $\\min\\{w(y, x)/w(x, y), 1\\}$.\nClassical version. We briefly describe Liu et al. (2000)'s original method. Suppose the current state is x.\n1. Draw $z_1, z_2, ..., z_M \\sim T(Z; x)$.\n2. Sample y from a distribution with probabilities\n$P\\{Y = z_j\\} = \\frac{w(z_j,x)}{\\sum_{i=1}^M w(z_i, x)}$,\nLet m denote the sampled index.\n3. Sample $z_1^*,..., z_{M-1}^* \\sim T(Z;y)$, and set $z_m^* = x$.\n4. Accept y with probability\n$\\beta(x, y; z, z^*) = \\min \\{\\frac{\\sum_{i=1}^M w(z_i^*, y)}{\\sum_{i=1}^M w(z_i, x)}, 1\\}$ .\nOne can show that this Markov chain satisfies the detailed balance equations when marginalizing out the intermediate states $z_1, z_2,..., z_M, z_1^*,..., z_m^*$ (see Theorem 1 in Liu et al. (2000)). We would like, however, to minimize the number of proposals necessary, so it is natural to ask whether the algorithm can be modified such that the proposals needed for the backward transitions, $z_1^*,..., z_{M-1}^*$, can be omitted. This is indeed possible.\nModified version.\n1. Draw $z_1, z_2,..., z_M \\sim T(Z; x)$.\n2. Sample y from a distribution with probabilities\n$P\\{Y = z_j\\} = \\frac{w(z_j, x)}{\\sum_{i=1}^M w(z_i, x)}$\nLet m denote the sampled index, $z = (z_1, z_2, ..., z_M)$, $z^* = (z_1^*,..., z_{m-1}^*, x, z_{m+1}^*,..., z_M^*)$, and set $q(y; z) = P\\{Y = y|z_1,..., z_M \\}$.\n3. Accept y with probability\n$\\alpha(x, y; Z_{-m}) = \\min \\{\\frac{p(y \\rightarrow z^*)}{p(x \\rightarrow z)}, 1\\} = \\min \\{\\frac{\\pi(y) \\prod_{i=1}^M T(z_i^*; y) q(x; z^*)}{\\pi(x) \\prod_{i=1}^M T(z_i; x) q(y; z)}, 1\\}$ ."}, {"title": "A.1 Proposition", "content": "Proposition A.1. The modified Multiple-Try MH sampler is reversible.\nProof. Let $\\Phi(y; x)$ denote the actual transition matrix. We want to show\n$\\pi(x)\\Phi(x,y) = \\pi(y)\\Phi(x; y)$.\nTo see this, we compute\n$\\pi(x)\\Phi(x,y) = k\\pi(x)T(y; x) \\int \\prod_{i=2}^M T (z_i; x)q(y; z)\\alpha(x, y; Z_{-1})dz_{-1}$\n$= k \\int \\min \\{\\frac{\\pi(y)}{\\pi(x)}, \\frac{\\prod_{i=2}^M T (z_i^*; y)q(x; z^*)}{\\prod_{i=2}^M T (z_i; z)q(y; z)}\\} \\prod_{i=2}^M T(z_i; z) dz_{-1}$\n$= \\pi(y)\\Phi(y, x)$.\nConnections between modified and classical versions. The acceptance probability in the modified version of Multiple-Try MH can be expanded as follows:\n$\\min \\{\\frac{\\pi(y) \\prod_{i=1}^M T(z_i^*; y)q(x; z^*)}{\\pi(x) \\prod_{i=1}^M T(z_i; x)q(y; z)}, 1\\} = \\min \\{\\frac{\\prod_{i=2}^M \\frac{T(z_i; y)}{T(z_i; z)}}{\\frac{\\pi(x)}{\\pi(y)}}, \\frac{w(y,x)}{\\sum_{i \\neq m} w(z_i, y) + w(x, y)} \\frac{\\sum_{i \\neq m} w(z_i, x) + w(y,x)}{\\prod_{i=2}^M \\frac{T(z_i; x)}{T(z_i; y)}} , 1\\}$ \nIf $T(-, -)$ is invariant in the first argument (as in our application to BC-LLM), then this last formula is exactly equal to $\\beta(x, y; z, z^*)$, the acceptance probability for the classical version, but if we were to use the same points for both the non-realized forward and backward proposals.\nEquivalence with Multi-SS-MH-Update. Finally, we show that the modified version of Multiple-Try MH described in the previous sections is equivalent to what is implemented in MULTI-SS-MH-UPDATE, for a fixed S. To see this, we make the replacements:\n* $x \\leftarrow c$\n* $z_i \\leftarrow (\\check{c_i}, c_{-k})$ for $i = 1, ..., M$\n* $y \\leftarrow (c^{(m)}, c_{-k})$\n* $T(\\cdot; -) \\leftarrow Q(\\cdot; -, y_s, X)$\n* $\\pi(-) \\leftarrow p(-|y, X)$\nPlugging these into (9), the acceptance ratio becomes\n$\\min \\{\\frac{Q(c_k; c_{-k}, y_s, X) \\sum_{m=1}^M p((c^{(m)}, c_{-k})|y, X)}{Q(\\check{c_k}^{(m)}; c_{-k}, y_s, X) \\sum_{m=1}^M p((c^{(m)}, c_{-k})|y, X)}, 1\\}$ \n$\\min \\{\\frac{Q(c_k; c_{-k}, y_s, X) \\sum_{0 \\le m \\le M} p(y_{sc}|y_s, (c^{(m)}, \\check{c}_{-k}), X)Q(\\check{c_k}^{(m)}; c_{-k}, y_s, X)}{Q(\\check{c_k}^{(m)}; c_{-k}, y_s, X) \\sum_{0 \\le m \\le M, m \\ne m} p(y_{sc}|y_s, (c^{(m)}, \\check{c}_{-k}), X)Q(\\check{c_k}^{(m)}; c_{-k}, y_s, X)}, 1\\}$ \nwhere the equality comes from (47) and we set $\\check{c}^{(0)} = c_k$ for convenience of notation. Observe that this is exactly the formula in Line 6 of Algorithm 3.\nMeanwhile, plugging into (6) and using (47), we see that the sampling weights used in the modified version of Multiple-Try MH are equivalent to those in Line 3 of Algorithm 3. This completes the proof of the equivalence."}, {"title": "B Implementation details for BC-LLM", "content": "Here we discuss how the hyperparameters for BC-LLM should be selected:\n* Fraction w of data used for partial posterior: Choosing a small w may lead to the LLM proposing less relevant concepts, but tends to lead to more diverse proposals. In contrast, a large w tends to lead to less diverse proposals because the LLM is encouraged to propose concepts that are relevant to the dataset D, which may not necessarily generalize. In experiments, we found that w = 0.5 provided good results.\n* Number of candidate concepts M: More candidate proposals per iteration can allow for more efficient exploration of concepts. The number of candidates is limited by the number of concepts that the LLM can reliably extract in a single batch. There tends to also be diminishing returns, as the first few candidates generated by the LLM based on the top keyphrases tend to be highest quality and most relevant. We found that setting M = 10 provided good performance.\n* Warm-start and Burn-in: Since Gibbs sampling can be slow to converge, we precede it with a warm-start, which we obtain by updating concepts greedily. That is, we select the concept that maximizes argmax $p(c_k, ys, \\Theta)$, instead of sampling from the distribution. In experiments, we run warm-start for one epoch and stored the last 20 iterates as posterior samples; the rest of the samples were treated as burn-in.\n* Number of iterations T: Although Monte Carlo procedures for Bayesian inference typically have thousands of samples, this is cost-prohibitive when an LLM must query each observation for a new concept per iteration. In experiments, we found that even setting T as low as 4 to be quite effective. Generally speaking, if the goal is solely prediction accuracy, a small T may suffice. On the other hand, if the goal is uncertainty quantification for the relevant concepts, one may prefer T on the higher end (e.g. 10) for more complete exploration of concepts."}, {"title": "C Example prompt for keyphrase extraction (Step 0)", "content": "Here is a clinical note: (note)\nOutput a list of descriptors that summarizes the patient case (such as aspects on demographics, diagnoses, social determinants of health, etc). For each descriptor, also list as many descriptors that mean the same thing or generalizations of the descriptor. All descriptors, synonyms, and generalizations cannot be more than two words. Output as a JSON in the following format. Do not output more than 30 entries in the JSON:"}, {"title": "D Example prompt for proposing concepts (Step 2)", "content": "The goal is to come up with a concept bottleneck model (CBM) that only extracts 5 meta-concepts from patient notes to predict some outcome Y with maximum accuracy. A meta-concept is a binary feature extractor defined by a yes/no question. We have 4 meta-concepts so far:\n1. Does the note mention the patient having social support?\n2. Does the note mention the patient having stable blood pressure?\n3. Does the note mention the patient not experiencing respiratory distress?\n4. Does the note mention the patient experiencing substance abuse or dependence?\nTo come up with the 5th meta-concept, I have done the following: I first fit a CBM on the 4 existing meta-concepts. Then to figure out how to improve this 4-concept CBM, I first asked an LLM to extract a list of concepts that are present in each note, and then fit a linear regression model on the extracted concepts to predict the residuals of the 4-concept CBM. These are the top extracted concepts in the resulting residual model, in descending order of importance:\n(top words from residual model)\nTo interpret this residual model, a general rule of thumb is that an extracted concept with a large positive coefficient means that the outcome is positively associated with the concept being mentioned"}, {"title": "E Example prompt for annotations for candidate concepts (Step 3)", "content": "A simple concept extraction procedure is to obtain binary or categorical concept extractions. In line with findings in Kim et al. (2023), we found that BC-LLM performed better when the LLM quantified its uncertainty when extracting concepts. So if the LLM was unsure about the concept's value, it was allowed to return a probability. As such, we use prompts like the following:\nYou will be given a clinical note. I will give you a series of questions. Your task is answer each question with 1 for yes or 0 for no. If the answer to the question is not clearly yes or no, you may answer with the probability that the answer is a yes. Respond with a JSON that includes your answer to all of the questions. Questions:\n1. Does the note mention the patient having social support?\n2. Does the note mention the patient having stable blood pressure?\n3. Does the note mention the patient not experiencing respiratory distress?\n4. Does the note mention the patient experiencing substance abuse or dependence?\n5. Does the note mention the patient being uninsured?\nclinical note: (note)"}, {"title": "F Experiment details", "content": "Note on computation time: When running BC-LLM with GPT-40-mini-2024-07-18 and GPT-40, each iteration completed within a few minutes.\nF.1 MIMIC\nThe entire dataset consists of 7043 observations of which we trained on 100, 200, 400, and 800 randomly selected observations and evaluated 500 held-out observations. We used the \"Chief Complaint\" and \"Social History\""}, {"title": "F.2 CUB-Birds dataset", "content": "To train a bird classifier for R subtypes, One-pass summarization and BC-LLM were run to fit CBMs with K = min(10, max(4, R)) concepts. BC-LLM was run for a maximum of T = 5 epochs or 25 iterations, whichever was reached first. Boosting ran for 10 iterations, in which at most 1 new candidate was added each iteration."}, {"title": "F.3 Clinical notes and tabular data from ZSFG", "content": "To predict readmission risk, the models were trained to analyze the sections \"Brief history leading to hospitalization\" and the \u201cSummary of hospitalization\" in the discharge summary for each patient. One-pass summarization and BC-LLM were run to fit CBMs with K = 4 concepts. BC-LLM was run for T = 5 epochs. Boosting ran for 10 iterations, in which at most 1 new candidate was added each iteration. For the black-box comparator, we used a ResNet50 model pre-trained on ImageNet V2, which ensures ResNet's training set does not overlap with the CUB-Birds dataset.\nFor the OOD experiment, the Bunting CBM was applied to 30 images of finches."}, {"title": "G Laplace Approximation of Split-Sample Posterior", "content": "In this section, we describe how to perform Laplace approximation of the split-sample posterior $p(y_{s_c} | y_s, c, X)$ when the likelihood model is logistic and the prior on the coefficient vectore $\\theta$ is a standard normal $N(0, \\gamma^2I)$. For simplicity, we omit discussing the constant term $\\theta_0$. Fixing the list of concepts $c$, we let $\\Phi$ denote the $n \\times K$ matrix whose $(i, j)$-th entry is given by $\\Phi_{ij} = \\phi_{c_j}(x_i)$. Given any subset of example indices $T \\subset [n"}]}