{"title": "SEER: Self-Explainability Enhancement of Large Language Models' Representations", "authors": ["Guanxu Chen", "Dongrui Liu", "Tao Luo", "Jing Shao"], "abstract": "Explaining the hidden representations of Large Language Models (LLMs) is a perspective to understand LLMs' underlying inference logic and improve their reliability in application scenarios. However, previous methods introduce external \"black-box\" modules to explain \u201cblack-box\u201d LLMs, increasing the potential uncertainty and failing to provide faithful explanations. In this paper, we propose a self-explaining method SEER, enhancing LLMs' explainability by aggregating the same concept and disentangling the different concepts in the representation space. In this way, SEER provides faithful explanations carried by representations synchronously with the LLMs' output. Additionally, we showcase the applications of SEER on trustworthiness-related tasks (e.g., the safety risks classification and detoxification tasks), where self-explained LLMs achieve consistent improvement in explainability and performance. More crucially, we theoretically analyze the improvement of SEER on LLMs' generalization ability through optimal transport theory. The code is available at https://github.com/AI45Lab/SEER.", "sections": [{"title": "1. Introduction", "content": "The wide use of LLMs provides convenience for people's work and life (Achiam et al., 2023; Cai et al., 2024). However, the internal mechanisms of LLMs remain unclear, making them hard to be reliably applied in tasks like finance and healthcare (Li et al., 2023; Nazi & Peng, 2024; Dang et al., 2024). Thus, it is increasingly critical to explain and understand the inference logic of LLMs. LLMs transform the input into intermediate representations layer by layer, contributing to the final prediction. The hidden representations contain rich semantic information (Shen et al., 2021; Zhang et al., 2024c; Wu et al., 2024), reflecting the logic of model prediction. Therefore, explaining hidden representations can help us understand the inference logic, improving the trustworthiness and reliability of LLMs' applications.\nTo this end, prior works on explaining LLMs' hidden representations can be roughly divided into three classes, as shown in Figure 1. Firstly, Liu et al. (2024) and Li et al. (2024b) develop external detectors (e.g., Probes) to identify dishonesty and toxic concepts in LLMs' representations. Secondly, sparse autoencoders (SAEs) decompose LLMs'"}, {"title": "2. Related Work", "content": "Interpretability of LLMs. Global interpretability methods provide insight into the internal mechanisms of LLM (Liu et al., 2023; Zhou et al., 2024; Ren et al., 2024a;c; Dang et al., 2024). Previously, external modules are often trained to identify semantic information from intermediate representations (Liu et al., 2025; 2024). Some methods also project representations into the vocabulary space (nostalgebraist, 2020) or other interpretable space (Gao et al., 2024; Lieberum et al., 2024). Due to the powerful capabilities of LLMs, they are utilized to explain representations with natural language by directly decoding representations (Chen et al., 2024; Ghandeharioun et al., 2024) or summarizing patterns of representations (Bills et al., 2023). When an LLM serves as the explained model and the explaining tool simultaneously in these above approaches, they can be called self-explaining methods. Self-explaining methods use models to explain themselves. Instead of explaining representations, chain-of-thought prompting (CoT, Nye et al. (2021); Wei et al. (2022)) enables LLMs to tell how they make predictions and improves reasoning ability. However, Huang et al. (2023) and Turpin et al. (2024) show that CoT may provide unfaithful explanations and bring potential dangers to the utilization of LLMs.\nRepresentations of LLMs. Several works focus on representations of LLMs instead of their output on tasks of LLMs' alignment (Li et al., 2024e; Yin et al., 2024; Qian et al., 2024b; Zhang et al., 2024a;b), evaluation (Wei et al., 2024; Azaria & Mitchell, 2023; Xu et al., 2024; Azaria & Mitchell, 2023; Orgad et al., 2024) and copyright protection (Zhang et al., 2024c; Sevastjanova et al., 2022; Yang & Wu, 2024). Li et al. (2024b) design steering vectors and insert them into model representations to control model generations without training. Rosati et al. (2024), Zou et al. (2024) and Li et al. (2024d) perform machine unlearning by rotating the representation of harmful samples or pushing them towards a random distribution. What's more, Wu et al. (2024) performs intervention functions precisely on the model's target layer and the target position of the input tokens. Qian et al. (2024a) disentangles LLMs' awareness of fairness and privacy by deactivating the entangled neurons in representations.\nContrastive Learning. Contrastive self-supervised learning on computer vision utilizes positive and negative pairs constructed by data augmentation to learn general and high-quality representations (He et al., 2020; Chen et al., 2020; Zbontar et al., 2021). Radford et al. (2021) connects natural language and visual modality through contrastive learning of text-image pairs. Recent work extracts human value representations of LLMs by applying multi-view contrastive Learning (Cahyawijaya et al., 2024)."}, {"title": "3. SEER", "content": "In this section, we propose SEER and introduce how SEER improve the self-explainability of LLMs in Section 3.1. Then we theoretically analyze the effect of SEER to LLMs' generalization capability in Section 3.2 and conduct experimental verification in Section 4. In Section 3.3, we verify the effectiveness of SEER across three scenarios, such as math, knowledge, and safety."}, {"title": "3.1. Framework Description", "content": "We aim to faithfully explain LLMs' inference logic and provide self-explanations. Suppose that representations of different concepts are disentangled from each other, then we can easily know which concept the LLM is considering (e.g., \"honesty\" and \"violence\u201d as shown in Figure 1) and take appropriate intervention during the inference process. Therefore, we can the inference logic of LLMs through the disentanglement between different concepts.\nNotations. Given an LLM fo with L layers, we use $f_{\\theta \\le l}(\u00b7)$ to denote intermediate outputs in the l-th layer. With an input $x = (x_1, x_2,...,x_T)$, LLMs can be described as\n$h^{(l)} = (h_1^{(l)},...,h_t^{(l)},..., h_T^{(l)}) = f_{\\theta \\le l}(x)$, (1)\n$\\pi_{\\theta}(x) = (f_{\\theta}(x_2 | x_{\\le 1}),\u2026\u2026\u2026, f_{\\theta}(x_T | x_{\\le T-1}))$, (2)\nwhere $h_t^{(l)}$ denotes intermediate representations of token position t in l-th layer and $h^{(l)} \\in R^{T\\times d}$ is the matrix of the representations for all tokens in l-th layer; $f_{\\theta}(x_t | x_{\\le t-1})$ denotes the probability of token $x_t$ given the previous tokens $x_{<t-1}$ and $\\pi_{\\theta}(x)$ is the output sequence of probabilities.\nDisentanglement of representations between concepts. In this part, we aim to maximize the similarities of examples from the same concept (e.g., two QA examples from \"bias\" related data, called positive pair) and minimize the similarities of examples from the different concepts (e.g., one QA example from \"bias\u201d and the other from \"honesty\u201d related data, called negative pair) in representation space of LLMs. We utilize a Disentangle Set $\\{D_j\\}_{j=1}^C$ consisting of subsets $D_j = \\{x_i^j\\}_{i=1}^{n_j}$ from C different concepts, where $n_j$ is the number of samples from concept j, and total number of data n can be calculated by $n = \\sum_{j=1}^C N_j$.\nConcretely, SEER samples B concepts $\\{c_k\\}_{k=1}^B$ and then construct positive pairs $\\{x,x^+\\}_{k=1}^B$ by sampling two examples from each concept. We use the disentangle loss $\\mathcal{L}_d$, a classical InfoNCE loss, to disentangle the representations from different concepts\n$\\mathcal{L}_d = -E_{\\{c_k\\}_{k=1}^B} \\sum_{k=1}^B log \\frac{exp(z_t^{(l)} \\cdot z_t^{'(l)} / \\sigma)}{\\sum_{j=1}^B exp(z_t^{(l)} \\cdot z_t^{'(l)} / \\sigma)}$, (3)\nwhere $z_t^{(l)}$ denotes the normalized representations of input $x$ from l-th layer and token position t, calculated by $h_t^{(l)} / ||h_t^{(l)}||$ and $\\sigma$ adjusts the degree of disentanglement.\nMaintenance of LLMs' general performance. We aim to obtain a faithfully self-explained LLM with outstanding general capabilities, rather than an encoder of concepts without normal ability of conversations. Therefore, the LLM should maintain general capabilities and provide normal output on the disentangled concepts. To obtain the representations associated with the general performance of LLMs, we introduce the Retain Set $\\mathcal{D}_{retain}$, which includes data related to general capabilities. Meanwhile, we utilize the first example of each positive pair constructed in the previous paragraph to get output probabilities on disentangled concepts.\nThe goal of our retain loss $\\mathcal{L}_r$ is to maintain general capabilities and keep stable output on edited concepts. Specifically, we denote the original model as $f_{\\theta_{ref}}$ and calculate the first term of $\\mathcal{L}_r$ by imposing an $l_2$ norm constraint on representations before and after disentanglement following (Zou et al., 2024). Additionally, the second term of $\\mathcal{L}_r$ is calculated with the KL penalty on output probabilities before and after disentanglement, as suggested in (Ouyang et al., 2022).\n$\\mathcal{L}_r = E_{\\{x_k\\}_{k=1}^B} || f_{\\theta \\le l}(x_k) - f_{\\theta_{ref} \\le l}(x_k) ||_2  + \\alpha E_{\\{x_k\\}_{k=1}^B} D_{KL} [\\pi_{\\theta}(x) || \\pi_{\\theta_{ref}}(x)]$, (4)\nwhere l is the target layer of disentanglement and $\\{x_k^*\\}_{k=1}^B$ denote the data sampled from $\\mathcal{D}_{retain}$; $\\alpha$ is to adjust the"}, {"title": "3.2. Theoretical Analysis of SEER", "content": "In this subsection, we theoretically prove that disentanglement of LLMs' representations improves the generalization ability of LLMS, following prior works (Chuang et al., 2021; Solomon et al., 2022) through optimal transport theory.\nDefinition of distance from optimal transport. In optimal transport theory, The distance between two distributions can be measured by the minimal cost to transform one distribution to the other, called the Wasserstein distance.\nDefinition 1 (s-Wasserstein distance (Villani & Villani, 2009)). Given two probability measures $p$ and $q \\in Prob(R^m)$, their s-Wasserstein distance with cost function c(.) is calculated as\n$D_s(p,q) = inf_{\\gamma \\in \\Gamma(p,q)} [E_{(U,V)\\sim\\gamma}c(U, V)]$, (6)\nwhere the set $\\Gamma(p, q) \\in Prob(R^m \\times R^m)$ consisting of all the couplings whose marginals are p and q, respectively.\nTo measure the property of a distribution, we introduce k-variance, a generalization of variance built on the machinery of random bipartite matching (Solomon et al., 2022; Chuang et al., 2021). In this paper. we consider the unnormalized version of k-variance with 1-Wasserstein distance following (Solomon et al., 2022; Chuang et al., 2021).\nDefinition 2 (k-variance). Letting $p \\in Prob(R^m)$ be a probability measure and $k \\in N$ denote the number of data sampled following p, the k-variance is defined as\n$Var_k(p) = E_{x_1,...,x_k\\sim p^k}  [\\frac{k}{k-1}  \\sum_{i=1}^k D(x_i,  \\frac{1}{k}  \\sum_{i=1}^k x_i)]$, (7)\nwhere $\\sum_{i=1}^k \\delta_{x_i}$ denotes the empirical measures of p for $x_i \\overset{i.i.d}{\\sim} p^k$ and euclidean cost function is applied here.\nFormulation of LLMs' generalization ability. To analyze LLMs' generalization ability, we simplify LLMs from a next-token predictor to a classifier between concepts following (Abburi et al., 2023; Chen et al., 2023; Lang et al., 2024). For example, the safety-related tasks can be transformed into a prompt classification task between safe concepts and harmful concepts (Inan et al., 2023; Li et al., 2024b).\nSpecifically, given an input $x \\in \\mathcal{X}$ and the concept space $\\mathcal{C} = \\{1,...,C'\\}$, we formulate the LLM $f_\\theta$ as a compositional hypothesis class $G \\circ \\Phi$. We consider the output of LLMs as a prediction of concept $j \\in \\mathcal{C}$, where the LLM $f_\\theta$ can be decomposed as a hidden representation encoder $\\phi := f_{\\theta<l} \\in \\Phi$ and a score-based classifier $g := \\psi \\circ f_{\\theta>l} \\in G$. $\\psi$ is a hypothesis component to transform LLMs' output into the concept-level prediction.\nIn this way, we can measure the generalization ability of LLMs following (Chuang et al., 2021). Given the classifier $g = (g_1,..., g_C)$, $g_j \\in G_j$, the prediction for input $x \\in \\mathcal{X}$ is calculated by $arg \\max_{j \\in \\mathcal{C}} g_j(\\phi(x))$. The margin of g for a data x from concept j is defined by\n$\\rho_g(\\phi(x_j)) := g_j(\\phi(x_j)) - \\max_{j'\\neq j} g_{j'}(\\phi(x))$, (8)\nwhere g misclassifies if $\\rho_g(\\phi(x_j)) \\le 0$. In our task, the Disentangle Set $\\{D_j\\}_{j=1}^C$ can be considered as obtained i.i.d from distribution p over $\\mathcal{X} \\times \\mathcal{C}$. We use $p_j$ to denote the marginal over a class $j \\in \\mathcal{C}$. The pushforward measure of p with respect to $\\phi$ is represented as $\\phi_{\\#} p. We consider expected zero-one loss of a hypothesis $g \\circ \\phi$ with the distribution $\\mu(j)$ over the concept space:\n$R_\\rho(g \\circ \\phi) = E_{x_j \\overset{i.i.d}{\\sim} p_j}[1_{\\rho_g(\\phi(x_j)) \\le 0}]$, (9)"}, {"title": "3.3. Verification of Disentanglement", "content": "In this subsection, we utilize metrics related to the quality of disentanglement to validate the disentanglement effective-"}, {"title": "4. Case Studies on Safety-related Tasks", "content": "In this section, we showcase the application of SEER in safety-related scenarios (Ren et al., 2024b; Hu et al., 2024), such as the safety risks classification task in Section 4.1 and the detoxification task in Section 4.2. SEER achieves a consistent improvement of explainability and task performance on both of these tasks, verifying our theoretical analysis in Section 3.2 and demonstrating the ability of SEER to mitigate the potential safety risks of LLMs."}, {"title": "4.1. Safety Risks Classification", "content": "The safety risks classification task is practical and important in safety-related scenarios. In this subsection, we showcase the application of SEER on this task by disentangling representations of different safety risks.\nDatasets. (1) Binary classification task utilizes the two broad concepts of safety and harm. Based on BeaverTails (Ji et al., 2024), we screen data related to only one type of safety risk, selecting five risks to form the binary classification train set. (2) Multi-risks classification task considers the classification across safety concepts and the previous five"}, {"title": "4.2. Detoxification Tasks", "content": "The detoxification of LLMs is an important task for improving their safety performance. In this subsection, we compare the safety performance of LLMs before and after applying SEER, using the experimental settings of data introduced in Section 4.1. What's more, we show the improvement of LLMs' safety performance through applying SEER both before and after SFT (Huang et al., 2024; Li et al., 2024a).\nEvaluation benchmarks. We evaluate the safety performance with the test set of BeaverTails (i.e., BT in the table) and the base set of SaladBench (i.e., SB in the table, Li et al. (2024c)) through the safety rate (\u2191). We utilize the XSTest (XST, R\u00f6ttger et al. (2023)) with refusal rate (\u2193) to measure the over-refusal of LLMs. we use the GSM8k, MMLU and AGIEval to evaluate their general capabilities and show the average scores (\u2191). Please see more experimental details in Appendix A.4."}, {"title": "5. Conclusion", "content": "In this paper, we propose SEER to provide faithful explanations of LLMs' inference logic. SEER is a self-explaining method through disentangling representations between different concepts in the representation space. More crucially, SEER not only enhances the LLM's explainability but also improves its performance in trustworthiness-related tasks. Furthermore, we theoretically explain the improvement of SEER on LLMs' generalization ability in optimal transport theory. In this way, SEER provides a new perspective on the explainability of LLMs and contributes to the reliable utilization of advanced artificial intelligence."}, {"title": "Impact Statement", "content": "This work aims to advance the field of Large Language Models' Explainability by proposing a self-explaining method named SEER, which faithfully explains the inference logic of large language models. SEER is not just explaining the hidden representations of large language models, but further enhancing their self-explainability. We hope that SEER facilitates progress in this area with such a novel perspective that has the potential to achieve consistent improvements between explainability and capabilities of large language models. The potential positive societal impacts include more reliable and trustworthy language models with enhanced explainability, which could bring benefits to a wide range of applications."}, {"title": "A. Additional Experiment Results", "content": "A.1. SEER with Different Contrastive Loss Functions\nWe select five classical contrastive loss functions to compare the quality of disentanglement with five metrics introduced in Section 3.3 and the performance on downstream tasks: (1) Contrastive loss (Hadsell et al., 2006); (2) Triplet loss (Schroff et al., 2015); (3) Barlow Twins loss (Zbontar et al., 2021); (4) NT-Xent Loss (Chen et al., 2020) and (5) InfoNCE Loss (Oord et al., 2018). We conduct the experiments following the experimental settings of the multi-risks classification task in Section 4.2 and compare the classification accuracy with two baselines, Self-Sim, and Linear Probe. We finally show the value of metrics and average rankings of these loss functions in Table 5.\nA.2. More Experimental Details to Verify the Effectiveness of SEER on the Disentanglement Quality\nDatasets and models. In Section 3.3, we sample 740 examples as the train set and 400 examples as the test set, respectively, from each branch of the dataset MATH for the mathematic scenario, where 740 is the least amount of train data of mathematical branches and 400 is the least amount of test data. We select 200 examples as the train set and 100 examples as the test set from each of the seven subsets of the dataset MMLU for the knowledge scenario. The data setting for the safety scenario is the same as the settings introduced in Section 4.1. We choose the layer located at 80% of the hidden layer count as the target layer (e.g., the 25th layer in Llama-3.1-8B and Mistral-7B-v0.3, the 21st layer in Qwen2.5-7B, and the 33rd layer in Gemma2-9B, starting from the Oth layer of LLMs). To evaluate the general capabilities, we utilize the LLMs Evaluation Platform, OpenCompass (Contributors, 2023).\nSettings of SEER. We perform SEER on the last token of QA pairs, which is usually the eot token. We utilize hooks to obtain the intermediate representations and calculate the disentangle loss La where the temperature parameter o is 0.1. All of the hyperparameter settings are shown in Table 6."}, {"title": "A.3. More Experimental Details of Safety Risks Classification Task", "content": "Datasets and models. Based on BeaverTails (Ji et al., 2024), we screen data related to only one type of safety risk, selecting five risks with more than 1600 entries each and 8000 entries from safe QA pairs to form the binary classification train set. For the test set, each broad concept contains 1000 entries for the binary classification. 1600 entries of safe examples along with the previous five safety risks serve as the multi-class classification train set. Each concept contains 200 entries for the test set of multi-class classification.\nSettings of SEER. Compared with the representation-based baseline methods, we first fine-tune the LLMs through SEER on the last token of QA pairs and then evaluate the classification performance of baseline methods on self-explained LLMs. For SFT, we apply SEER before SFT without KL penalty (i.e., a = 0) in Section 4.1. We also perform SEER after SFT, which also achieves improvement in classification. Such experimental results verify our theoretical analysis again, as shown in Table 7 named Post-SEER."}, {"title": "A.4. More Experimental Details for The Detoxification Task", "content": "In Section 4.2, we perform SFT as a baseline, where we collect the same questions following the binary classification settings in Section 4.1 and generate safety responses with the LLMs themselves. We evaluate the LLMs' general capabilities with the average score from GSM8k, MMLU, and AGIEval. In this task, we apply SEER on both the last token of question and answer without KL penalty (i.e., a = 0). To compare with the SFT, we perform SEER before and after SFT, which both improve the safety performance of SFT. The experimental results of the latter have been presented in Table 4 of Section 4.2, and the results of the former can be seen in Table 8, named Pre-SEER."}, {"title": "A.5. Seer Can Improve the Safety Performance of LLMs with Larger Size.", "content": "We apply SEER on Qwen2.5-14B-Instruct with the same experimental setting introduced in Section 4.2. The experimental results shown in Table 9 indicates that SEER improves the safe performance of the original LLM by 6.6% and 29.7% compared with the supervised fine-tuned LLM, which verify the effectiveness of SEER on LLMs with Larger Size."}, {"title": "A.6. Ablation Study on the Components of Retain Loss Cr", "content": "We conducted ablation studies on the components that maintain the general performance of LLMs. Specifically, as described in Section 3.1, the framework of SEER consists of two hyperparameters related to retaining LLMs' general capabilities: \u5165 and a. In setting (a), if A and a are non-zero, SEER employs both the 12 norm constraint and the KL penalty. In setting (b), when A is non-zero but a is set to 0, SEER only applies the norm constraint and discards the KL penalty. In setting (c), when A is set to 0, the SEER does not utilize the retain loss Lr. Following the experimental settings in Section 3.3, we perform the ablation study on Llama-3.1-8B-Instruct."}, {"title": "B. Additional Details of Theoretical Analysis", "content": "B.1. Additional Details of the Formulation of LLMs\nIn Section 3.2, we introduce a hypothesis component & to decompose the LLM $f_\\theta$ as a hidden representation encoder $\\phi := f_{\\theta<l} \\in \\Phi$ and a score-based classifier $g := \\psi \\circ f_{\\theta>l} \\in G$. Here, with the vocabulary space V and the maximum output length tmax, $\\psi \\in R^{|V|\\times tmax \\times RC}$ is a mapping from the output logits space $R^{|V|\\times tmax}$ to the score-based concept prediction space RC. For example, in the safety-related scenario, & can be described as a judger LLM (Li et al., 2024c; Inan et al., 2023), whose logits of the tokens safe and unsafe can be seen as the scores of the classifer g.\nTo describe the data distribution, we introduce p and p; to represent the distribution followed by the entire Disentangle Set $\\{D_j\\}_{j=1}^C$ and the distribution followed by a subset Dj of the concept j, respectively. Moreover, we use $\\mu(j) \\in C \\times R$ to describe the probability distribution j ~ \u03bc over the concept space C."}, {"title": "B.2. Proof of Theorem 1", "content": "Definition 3. (The ramp loss from (Bartlett et al., 2017; Chuang et al., 2021))\nGiven the margin 7, the ramp loss is calculated as\n$L_{\\tau}(u) = 1_{u\\le0} + (1 - \\frac{u}{\\tau})1_{0<u{\\tau}}$\nProposition 4. (Proven in Lemma A.4 in (Bartlett et al., 2017))\nFor any g: Rm \u2192 RC and every \u0442 > 0,\n$R_\\rho(g \\circ \\phi) = Pr(arg max g_{j'}(x_j) \\neq j) \\le E_{(xj,j)}L_{\\tau}(\\rho_g(\\phi(x_j)))$"}]}