{"title": "LLM-Supported Natural Language to Bash Translation", "authors": ["Finnian Westenfelder", "Erik Hemberg", "Miguel Tulla", "Stephen Moskal", "Una-May O'Reilly", "Silviu Chiricescu"], "abstract": "The Bourne-Again Shell (Bash) command-line interface for Linux systems has complex syntax and requires extensive specialized knowledge. Using the natural language to Bash command (NL2SH) translation capabilities of large language models (LLMs) for command composition circumvents these issues. However, the NL2SH performance of LLMs is difficult to assess due to inaccurate test data and unreliable heuristics for determining the functional equivalence of Bash commands. We present a manually verified test dataset of 600 instruction-command pairs and a training dataset of 40,939 pairs, increasing the size of previous datasets by 441% and 135%, respectively. Further, we present a novel functional equivalence heuristic that combines command execution with LLM evaluation of command outputs. Our heuristic can determine the functional equivalence of two Bash commands with 95% confidence, a 16% increase over previous heuristics. Evaluation of popular LLMs using our test dataset and heuristic demonstrates that parsing, in-context learning, in-weight learning and constrained decoding can improve NL2SH accuracy up to 32%. Our findings emphasize the importance of dataset quality, execution-based evaluation and translation method for advancing NL2SH translation.", "sections": [{"title": "1 Introduction", "content": "The default command-line interface (CLI) for interacting with Linux systems is the Bourne-Again Shell (Bash) (Shotts, 2019). Bash commands allow computer users to control processes, interact with the file system and manage the network. However, using Bash requires knowledge of numerous utilities, each with unique parameters and complex syntax (Ramey and Fox, 2024). Moreover, the reference documentation for these utilities, called manual pages, can be cumbersome and confusing (Kerrisk, 2024). This makes the CLI a barrier for inexperienced users and increases the chance of errors for experienced users (Agarwal et al., 2021). Language models that convert natural language to command-line instructions, a task referred to as NL2SH, NL2CMD or NL2Bash Translation, offer a promising solution to this problem (Agarwal et al., 2021). We use the term NL2SH model to refer to models trained specifically for the task of NL2SH, as well as the NL2SH capabilities of general-purpose large language models (LLMs). NL2SH models are well suited for CLIs because they are designed for text-based interactions. NL2SH models can simplify human-computer interactions by allowing users to interact with Linux systems through natural language on the command line. This advancement enhances usability by reducing the need for syntax memorization (Sammet, 1966)."}, {"title": "2 Background", "content": "NL2SH translation falls under the broader domain of machine translation, where models automatically translate text or speech from one language to another. LLMs are well suited for this task, enabling translation that was impossible with previous methods (Zhu et al., 2024b). Evaluating NL2SH models requires determining the functional correctness of generated commands. Functional correctness is defined as whether the code produces the correct output for each input, as specified, or as compared to ground truth (Chen et al., 2024). Functional correctness does not consider the diversity of code generated, or other factors such as run time and memory consumption (Chon et al., 2024).\nEnsuring the functional correctness of code is difficult because validation methods are error-prone and take an impractical amount of time for large volumes of code (Chen et al., 2024). There are two main validation techniques: static and dynamic analysis. Static analysis checks code without execution, using parsers, lexical analysis or control flow checking (Shaikhelislamov et al., 2024). Dynamic analysis evaluates code outputs and runtime behavior using an execution environment (Yang et al., 2023). Some frameworks combine static and dynamic analysis (Aggarwal et al., 2024).\nDetermining the functional correctness of Bash commands translated from natural language is a sub-problem of validating code correctness. We assess the functional correctness of Bash commands by comparing the generated (model) command with a ground truth Bash command. We define the term \"functional equivalence heuristic\" (FEH) to describe a heuristic that performs this comparison and determines the functional correctness of a Bash command. Due to varying definitions in this field, we provide notation defining the terms used in this paper."}, {"title": "3 Related Work", "content": "NL2SH translation is a well-studied natural language processing (NLP) task. The table is sparsely populated because the majority of previous work focused on improving a NL2SH dataset, FEH, or model in isolation."}, {"title": "4 Methodology", "content": "4.1 Dataset Creation\nBash is considered a low-resource programming language due to the limited availability of NL2SH data (Joel et al., 2024). We aim to augment NL2SH datasets and begin with an evaluation of Inter-Code. All 224 commands in the InterCode dataset were manually curated from the NL2Bash dataset presented by Lin et al. (2018), containing 9,305 instruction-command pairs. The InterCode dataset is significantly smaller than the NL2Bash dataset because a Docker environment is configured for each command, enabling execution. We manually verify all 224 instruction-command pairs and find that over half of the InterCode dataset is erroneous.\nWe define three types of errors: invalid prompt, invalid command, and invalid environment. An invalid prompt error refers to a natural language instruction that describes an impossible task or does not give enough information to accomplish the task. An invalid command error refers to a Bash command that does not accomplish the task described in the prompt or does not execute. An invalid environment error refers to an incorrect Docker configuration such as a missing file, environment variable, or utility that prevents a valid command from accomplishing the task.\nWe fix 82 of these errors by correcting natural language prompts, Bash commands, and Docker configuration files. We remove 11 duplicate and 20 irreparable pairs from the dataset, resulting in 193 verified pairs. We create 117 more verified pairs by referencing Bash tutorials and books, such as The Linux Command Line by Shotts (2019) and the Linux Command Line and Shell Scripting Bible by Blum and Bresnahan (2021).\nAdditionally, for our 300 verified pairs, we create a second Bash command that accomplishes the task described in the prompt. Our final test dataset contains two functionally equivalent, ground truth Bash commands for each natural language instruction, for a total of 600 instruction-command pairs.\nWe collect training data by combining the NL2Bash dataset with three publicly available NL2SH datasets (Ramesh, 2022; Romit, 2024; Mali, 2023). Further, we scrape the tldr-pages, a collection of example Bash commands, as a new data source (Krishna et al., 2024). We combine these data sources and deduplicate with exact matching. Then, we use the bashlex parser to remove unparsable commands (Kamara, 2016).\nWe de-conflict our training and test dataset using exact matching and semantic similarity, removing 917 rows from the training data. First, we remove rows from the training data that exactly match instructions or commands in the test data. Next, we remove pairs from the training data with a natural language prompt that is syntactically similar to a prompt in the test data using the mxbai-embed-large-v1 embedding model and a cosine similarity threshold of 0.9 (Lee et al., 2024). Our final training dataset contains 40,939 instruction-command pairs, an increase of 135% over the previous largest dataset."}, {"title": "4.2 Functional Equivalence Heuristic (FEH)", "content": "Our evaluation of related work in Section 3 reveals the InterCode benchmark is more accurate than previous NL2SH benchmarks because its FEH uses execution-based evaluation. However, its TFIDF method for comparing command outputs may fail to determine functional equivalence because syntactically different outputs may convey the same information to the end user.\nFor example, consider the prompt \"Print the disk usage of the current directory\", a ground truth command of \"du -s.\" and a model command of \"du -d0-h\". The first command outputs the number of bytes and the second command outputs the number of bytes in human-readable format. The two commands are functionally equivalent, conditioned on the prompt. However, their outputs contain different characters, resulting in a low similarity score using TfidfVectorizer. Similar issues arise when comparing commands that print hardware information or system time in different formats, display text with line numbers or other delimiters, and use non-deterministic utilities, such as those that interact with the network. The difficulty of determining functional equivalence is exacerbated by ambiguity in natural language prompts, which is an inherent problem with human inputs.\nTo address this problem, we replace the Tfid-fVectorizer method for comparing command outputs with an LLM. Our intuition is that an LLM can determine more complex cases of functional equivalence by evaluating the semantics of command outputs with relation to the prompt. Replacing TfidfVectorizer with an LLM increases the computational cost of the FEH. Additionally, since LLMs are stochastic, our FEH has inherent variability. We compare our FEH with previous heuristics in Section 5.1, finding it achieves superior performance.\nWe present our FEH and test dataset as a new version of the InterCode benchmark, InterCode-ALFA. Our benchmark and datasets are released under MIT licenses. In addition to the dataset and FEH modifications, we add error handling and update the Docker configuration files to use stable Linux releases. We also identify and fix an error in the benchmark's Docker reset script that causes the filesystem structure of the two execution environments to diverge. We publish the benchmark source code on GitHub and provide a Python package on PyPI for ease of use. Our benchmark and dataset can be configured with 10 lines of code, simplifying the process for evaluating new models."}, {"title": "4.3 Translation Methods", "content": "Using our benchmark, we evaluate the NL2SH performance of the Llama, Qwen and GPT model families (Llama, 2024; Hui et al., 2024; OpenAI, 2024). We find the models have poor baseline performance and identify three translation failure modes: incorrect output format, incorrect utility and syntactically incorrect Bash command.\nIncorrect output format refers to a translation with extraneous information, such as an explanation of the translation, or additional text formatting, such as markdown code blocks. Incorrect utility refers to a translation with a utility that cannot accomplish the task described in the prompt. Syntactically incorrect Bash command refers to a translation that is not valid Bash syntax.\nTo address these failure modes, we evaluate four methods for improving model performance: markdown parsing 4.3.1, constrained decoding (CD) 4.3.2, in-context learning (ICL) 4.3.3, and in-weight learning (IWL) 4.3.4.\n4.3.1 Markdown Parser\nDespite prompting models with \"You will not output markdown or other formatting\", translations often include markdown formatting, likely due to instruct tuning. We implement a markdown parser to extract the Bash command from the first code block in model outputs, discarding additional text.\n4.3.2 Constrained Decoding\nWe inspect the token probabilities for each ground truth Bash command in our test dataset using the Llama3.1-8b-Instruct model. We find the average relative probability of the first token is four orders of magnitude smaller than the following tokens. In our case, the first token of each command is a Bash utility. This indicates the model is unlikely to select the correct utility as the first token. However, if it does select the correct utility, the following flags and arguments are correct with high probability. We address this by constraining the first tokens of the model output to a list of Bash utilities using grammar-constrained decoding Geng et al. (2023).\n4.3.3 In-Context Learning\nIn-context learning can improve model performance for a variety of tasks (Brown et al., 2020; Alves et al., 2023). We select 50 representative instruction-command pairs from our training dataset as ICL examples. We create embeddings for the commands using the mxbai-embed-large-v1 model and cluster the embeddings using k-means clustering. The closest instruction-command pair to each centroid is selected as an ICL example. We append these pairs to our translation prompt as show in Figure 9. We evaluate the performance of Llama3.1-8b-Instruct with the number of appended pairs ranging from 1-50 and find the optimal number to be 25, with performance saturating as more pairs are added. We use 25 example instruction-command pairs for all ICL evaluations.\n4.3.4 In-Weight Learning\nWe use our training dataset to perform a LoRA fine-tune of the Llama and Qwen models (Hu et al., 2021). We experiment with common hyperparameters within our hardware constraint of a single Nvidia RTX A6000. We find that training each model for 10 epochs with an adapter rank of 64, adapter alpha of 32, adapter dropout of 0.1, batch size of 32 and learning rate of 1e-5 results in the best performance. We do not fine-tune the GPT models due to financial constraints and the inability to control training hyper-parameters."}, {"title": "5 Experiments", "content": "5.1 Evaluation of FEHS\nWe compare our FEH with the heuristics presented by Sparck Jones (1988), Papineni et al. (2002), Agarwal et al. (2021), Yang et al. (2023) and Song et al. (2024) in previous work using our test dataset. A FEH should return true given two functionally equivalent Bash commands, and false given two non-equivalent Bash commands. We record the precision, recall, F1 score and accuracy of each FEH and report our results in Table 4.\nOur test dataset, described in Section 4.1, is structured {nl, bash, bash2}, providing 300 pairs of functionally equivalent commands. To create a set of non-equivalent commands, we arbitrarily rotate the third column of the dataset by ten positions. The result is 600 pairs of Bash commands. Explicitly, each FEH is tested using 300 functionally equivalent pairs $m(t, g(b, e), g(b, e)) = 1$ where $g(b,e) \\approx g(b',e)$ and 300 non-equivalent pairs $m(t, g(b, e), g(b, e)) = 0$ where $g(b, e) \\neq g(b', e)$.\nFor the bleu and nl2cmd FEHs we use a threshold of 0.75 for functional equivalence. For the tfidf and mxbai-embed FEHs we calculate the cosine similarity of the resulting embeddings and use a threshold of 0.75 for functional equivalence. For the llama-3.1-8b-inst, gpt-3.5-t-0125 and gpt-4-0613 FEHs we pass the tasks and commands to each model using the prompt in Figure 5. For the exec + tfidf and exec + mxbai-embed FEHs, we pass the stdout of command execution to the models, calculate the cosine similarity of the resulting embeddings and use a threshold of 0.75 for functional equivalence. Finally, for the exec + llama-3.1-8b-inst, exec + gpt-3.5-t-0125 and exec + gpt-4-0613 FEHs, we pass the tasks, commands and stdouts of command execution to each model using the prompt in Figure 6. We use a temperature of zero and a static seed value of 123 for all LLMs.\n5.2 Evaluation of Translation Methods\nWe evaluate the impact of parsing, constrained decoding, in-context learning and in-weight learning on the NL2SH performance of the Llama, Qwen and GPT model families. All models are evaluated using version 0.3.6 of the InterCode-ALFA benchmark with the execution + mxbai-embed FEH. Our results are summarized in Table 5.\nWe use the prompt in Figure 7 for the baseline evaluation. For the constrained decoding evaluation, we use the prompt in Figure 8 and constrain the first tokens of the model output to a list of Bash utilities, as described in Section 4.3.2. For the parser evaluation, we use the prompt in Figure 8 and pass model outputs to a markdown parser, as described in Section 4.3.1. For the in-context learning evaluation, we use the prompt in Figure 9. Finally, for the in-weight learning evaluation, we use the prompt in Figure 8 and the fine-tuned models described in Section 4.3.4. We use a temperature of zero and a static seed value of 123 for all model evaluations."}, {"title": "6 Discussion", "content": "6.1 Dataset\nOur first research question aims to validate NL2SH datasets to ensure models are evaluated using valid translations. Manual verification of the Inter-Code dataset identified over half of the instruction-command pairs as erroneous. We find human verification of data is important for reliable evaluations. Manual creation and verification of our test dataset took over 100 hours, highlighting the need for more efficient means to verify larger datasets. Further, since the InterCode dataset is sampled from the NL2Bash dataset, there is a risk the NL2Bash dataset contains a significant percentage of erroneous data. This is concerning because the NL2Bash dataset is commonly used to train NL2SH models (Fu et al., 2021; Lin, 2017; Shi et al., 2023; Bharadwaj et al., 2022; Joshi, 2024).\nWe are confident our training dataset contains valid data due to our filtering process to remove invalid Bash commands. Moreover, fine-tuning the Llama and Qwen models using our dataset results in an average performance increase of 11%.\n6.2 Functional Equivalence Heuristic\nOur second research question aims to design an FEH that accurately measures the quality of model translations. We find that command execution paired with LLM evaluation of command outputs can determine the functional equivalence of Bash commands with 95% accuracy. The ability of LLMs to condition command outputs on natural language inputs is an advancement that was not possible with previous heuristics. Further, command execution improves performance across methods and the use of an LLM significantly increases recall. Broadly, LLM evaluation of execution outputs is a promising advancement for measuring the functional correctness of generated code and more investigation is warranted.\nIn accordance with Maveli et al. (2024), we find that without execution, current LLMs are poor arbiters of functional equivalence, achieving similar performance when compared to conventional evaluation methods. Non-execution methods likely fail because two commands can be syntactically similar and yield different results when executed. For example, changing a single flag can result in vastly different command outputs. Further, two commands with no syntactic similarity can yield identical results when executed. For example, the awk and sed utilities can accomplish identical text processing tasks but use different domain-specific languages, requiring different syntax. Notably, the low recall of bleu and nl2cmd FEHs indicates these methods cannot identify cases where syntactically different commands are functionally equivalent.\n6.3 Translation Methods\nOur third research question aims to improve the accuracy of NL2SH models as measured by a reliable benchmark. We find that constrained decoding, parsing, in-context learning and in-weight learning can improve model performance by up to 32%. Our baseline evaluation shows that model performance is correlated with number of parameters.\nWe find that constrained decoding is model dependent, with performance increases for Llama models and significant performance decreases for Qwen models. Parsing and ICL provide performance increases across Llama and Qwen models, with average increases of 21% and 19%, respectively. However, these methods have a decreasing impact as model size increases. This is evidence that incorrect output format is the dominant failure mode for small (less than 7b parameter) models. With IWL, llama-3.2-3b-instruct and qwen2.5-coder-0.5b-instruct achieve the baseline performance of llama-3.1-8b-instruct and qwen2.5-coder-3b-instruct, respectively. Despite performance gains for small models, fine-tuning decreases the performance of llama-3.1-8b-instruct and qwen2.5-coder-7b-instruct. This is likely due to computational constraints on the size of our LoRA adapters, which we are unable to scale with model size.\nWe find that gpt-40-2024-08-06 achieves SOTA performance on our benchmark, correctly translating 74% of test cases. From the total of our experiences, we find that NL2SH translation is a difficult task for current models, necessitating improvements before models can be used in practice."}, {"title": "7 Conclusion", "content": "In this paper, we explore applications for LLMs in NL2SH translation and benchmarking. We identify issues with current benchmarks, including inaccurate datasets and unreliable functional equivalence heuristics. To address these problems, we correct and expand NL2SH datasets and create a new heuristic to determine the functional equivalence of Bash commands. We assess our heuristic and find that Bash command execution paired with language model evaluation of command outputs can determine the functional equivalence of commands more accurately than previous heuristics. Using our dataset and heuristic, we evaluate how constrained decoding, parsing, in-context learning and in-weight learning impact the performance of Llama, Qwen and GPT models. We find that parsing and in-context learning reliably improve the performance of open and closed-source LLMs for the task of NL2SH translation. Ultimately, we find that NL2SH translation is a difficult task for LLMs, necessitating further research in this field. In future work, we plan to investigate efficient means to verify our training dataset and conduct more fine-tuning experiments."}, {"title": "8 Limitations", "content": "This work presents a verified and expanded NL2SH test dataset. However, due to the time and effort required to manually configure an execution environment for each command, the dataset remains small, with only 600 test cases. In contrast, our training dataset is too large for manual verification, and we are unable to guarantee its correctness. Our datasets are limited to English prompts and one-line Bash commands. We do not consider other natural languages or scripting languages.\nAlthough improved over previous methods, our functional equivalence heuristic has inherent variability due to the use of an LLM, requiring multiple runs to assess model performance. The use of an LLM also increases the computational cost of running our heuristic compared to conventional methods. Finally, despite improved model performance with constrained decoding, parsing, ICL and IWL, the accuracy of SOTA LLMs for NL2SH translation remains low, motivating further research."}, {"title": "9 Ethical Considerations", "content": "Due to the low performance of current NL2SH models, using these models in practice could result in invalid commands that have unintended effects on a system. We recommend that model-generated commands are never used without human verification. Further, we recommend that users test commands using a sand-boxed environment, such as try (Zhu et al., 2024a), before running them on personal systems. Natural language to Bash translation aims to increase computer accessability by simplifying interactions with the command-line interface. Unfortunately, good and bad actors can benefit from increased accessability. Models could be used to generate malicious Bash commands. This risk is difficult to mitigate because malicious use depends on the intent of the user. For example, a command to delete files could be used for a legitimate or harmful purpose. We do not believe current NL2SH models pose any risks beyond those of other readily available Bash resources."}, {"title": "Appendix A. Prompts", "content": "Functional Equivalence Heuristic Prompt: LLM\nYou will be given a task and two Bash commands. The first command is the ground truth. If the second command accomplishes the task, return true. Otherwise, return false. Only output 'true' or 'false'. Task: natural_language_prompt, Ground Truth Command: ground_truth_command, Model Command: model_command.\nFunctional Equivalence Heuristic Prompt: Execution + LLM\nYou will be given a task, two Bash commands, and the output of the two Bash commands. The first command is the ground truth. If the second command accomplishes the task, return true. Otherwise, return false. Only output 'true' or 'false'. Task: natural_language_prompt, Ground Truth Command: ground_truth_command, Model Command: model_command, Ground Truth Command Output: ground_truth_command_output, Model Command Output: model_command_output.\nTranslation Prompt: Baseline\nYour task is to translate a natural language instruction to a Bash command. You will receive an instruction in English and output a Bash command that can be run in a Linux terminal. You will not output markdown or other formatting. You will not include additional information. natural_language_prompt\nTranslation Prompt: Parser, Constrained Decoding and In-Weight Learning\nYour task is to translate a natural language instruction to a Bash command. You will receive an instruction in English and output a Bash command that can be run in a Linux terminal. natural_language_prompt\nTranslation Prompt: In-Context Learning\nYour task is to translate a natural language instruction to a Bash command. You will receive an instruction in English and output a Bash command that can be\nrun in a Linux terminal.\nShow logged-in users info\nW\nPrint the contents of \"xx.sh\"\ncat xx.sh\nChange owner to \"root\" and group to \"www-data\" of \"/foobar/test_file\"\nchown root:www-data/foobar/test_file\ndelete all the text files in the current folder\nfind. -type f-name \"*.txt\" -delete\nfind all the files in the /path folder and delete them\nfind /path -type f-delete\nPrint the exit status of the last executed command\necho $?\nDisplay a tree of processes\npstree\nDisplay information about all CPUs\nlscpu\nMake an HTTPS GET request to example.com and dump the contents in 'stdout'\ncurl https://example.com\nDisplay system memory\nfree\nList all files, including hidden files\nls -a\nPrint a sequence from 1 to 10\nseq 10\nGet the properties of all the user limits\nulimit -a\nList the name and status of all services\nservice-status-all\nDisplay a calendar for the current month\ncal\nShow the environment\nenv\ncreate directory TestProject\nmkdir TestProject\nQuery the default name server for the IP address of example.com\nnslookup example.com\nPrint Hello World\necho \"Hello World\"\nList all bound commands and their hotkeys\nbind-p\nDisplay the openssl version\nopenssl version\nPrint current time, uptime, number of logged-in users\nuptime\nPrint file system disk space usage\ndf\nList all configuration values available\ngetconf -a\nDelete empty folder 'nonsense_dir'.\nrmdir nonsense_dir\nnatural_language_prompt"}]}