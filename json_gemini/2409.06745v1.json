{"title": "Personalized Knowledge Tracing through Student Representation Reconstruction and Class Imbalance Mitigation", "authors": ["Zhiyu Chen", "Wei Ji", "Jing Xiao", "Zitao Liu"], "abstract": "Knowledge tracing is a technique that predicts students' future performance by analyzing their learning process through historical interactions with intelligent educational platforms, enabling a precise evaluation of their knowledge mastery. Recent studies have achieved significant progress by leveraging powerful deep neural networks. These models construct complex input representations using questions, skills, and other auxiliary information but overlook individual student characteristics, which limits the capability for personalized assessment. Additionally, the available datasets in the field exhibit class imbalance issues. The models that simply predict all responses as correct without substantial effort can yield impressive accuracy. In this paper, we propose PKT, a novel approach for personalized knowledge tracing. PKT reconstructs representations from sequences of interactions with a tutoring platform to capture latent information about the students. Moreover, PKT incorporates focal loss to improve prioritize minority classes, thereby achieving more balanced predictions. Extensive experimental results on four publicly available educational datasets demonstrate the advanced predictive performance of PKT in comparison with 16 state-of-the-art models. To ensure the reproducibility of our research, the code is publicly available at https://anonymous.4open.science/r/PKT.", "sections": [{"title": "Introduction", "content": "Knowledge tracing (KT) is a continuous predictive task aimed at simulating students' performance on questions by establishing models to forecast their level of knowledge mastery during interactions with learning platforms. This process leverages students' historical learning interaction data to construct models for estimating their proficiency levels and utilizes these models to predict their performance over a future period of time. KT holds significant potential to support educators in identifying students who require additional attention, recommending tailored learning materials, and delivering valuable feedback to enhance student learning outcomes. Additionally, KT can be utilized to customize learning plans, provide early warnings, and offer targeted guidance in instructional practices, thereby enhancing learning outcomes and student performance."}, {"title": "Related Work", "content": "In this section, we first delineate relevant KT works. Then, we present a brief overview of class balance issue and focal loss."}, {"title": "Knowledge Tracing", "content": "As a result of the expansion of deep learning methods over the past decade, researchers have been trying to incorporate deep learning strategies into KT research, contributing significantly to the development of online education.\nThe pioneering work of Deep Knowledge Tracing (DKT), which applies deep learning to the field of KT, has sparked a series of methods primarily focused on sequence modeling. DKT+ enhances the basic DKT loss function by introducing two additional regularization terms to address the limitations in reconstructing response inputs and reducing inconsistency in predicting exercises with similar concepts.\nInspired by memory-augmented neural networks, researchers have enhanced DKT by introducing an external memory structure to better track students' learning of complex concepts with stronger representational capacity. Specifically, the knowledge state is represented using a key-value memory structure, where the key matrix stores concept representations and the value matrix stores the degree of mastery for each concept by the student. SKVMN addresses irrelevant knowledge concepts in DKT and DKVMN by using an enhanced Hop-LSTM, while retaining DKVMN's key-value memory structure and loss function.\nIn addressing the lack of interpretability in DKT, researchers have attempted to embed interpretability directly into the structure of individual models. The SAKT model, for the first time, models the interactive embedding sequences using a self-attention mechanism to learn the importance of each practice. The SAINT model tackles the shallow attention layer issue of SAKT by enhancing performance through the addition of attention layers and increasing the number of layers in the encoder and decoder. AKT incorporates a context-aware attention mechanism, considering students' overall interaction history, the influence of past questions and responses, and quantifying the time difference between previously answered questions."}, {"title": "Class Balance and Focal Loss", "content": "Deep neural networks, leveraging the powerful representational capabilities learned from high-quality data, have been successfully applied to various tasks. However, class imbalance often arises when sample distribution is uneven, particularly in educational contexts where varying knowledge component difficulties lead to disproportionate practice samples. This imbalance can cause models to overemphasize data-rich classes, impairing performance and generalization in KT tasks. Focal loss, which is widely used in tasks such as object detection and text classification, addresses this issue by down-weighting easily classified samples, enabling the model to focus on more challenging samples. In KT tasks, focal loss improves learning efficiency and mitigates bias towards majority classes, thereby enhancing the model's ability to handle class imbalance."}, {"title": "Problem Statement", "content": "KT endeavors to monitor the evolution of students' cognitive states throughout the learning process, while leveraging their prior interactions to forecast their future performance on subsequent tasks. It can be succinctly articulated as follows: Given the historical sequence of student interactions, denoted as $X = \\{X_1,X_2,...,X_t\\}$, where each interaction $X = \\langle q, \\{s|s \\in N_q\\},a,t \\rangle$ signifies the student's response $a$ to question $q$ involving a set of skills $s$ at time $t$, where $a \\in \\{0,1\\}$ is a binary indicator denoting correctness (1 for correct, 0 for wrong), the aim is to predict the probability of the student answering the question correctly at time $t + 1$."}, {"title": "The PKT Model", "content": "The architecture of our method, shown in Figure 3, includes five components: (1) the student representation module uses GRU to encode skill and response information from historical practice records; (2) the capsule blocks module creates capsule representations via an attention mechanism; (3) the knowledge tracing module predicts the probability $p$ of correctly answering the next question using a sigmoid function; (4) the reconstruction module calculates the reconstruction representation by multiplying $p$ with the student's representation; (5) the class imbalance module uses focal loss to reduce the influence of simple examples, focusing on challenging cases."}, {"title": "Student Representation Module", "content": "Inspired by the simple but effective DKT, each interaction is initially characterized as $e$ through the encoding of skills and the information provided in responses:\n$e_t = s_t + a_t \\times E$     (1)\nwhere $E$ is the total number of skills.\nStandard RNNs struggle with long-term dependencies. The Gated Recurrent Unit (GRU) addresses this issue with its simplicity, fewer parameters, faster training, and superior performance across tasks. A GRU cell controls the flow of information through two gating mechanisms to compute the hidden state $h_t$. Given the input $e_t$ at the current time step $t$ and the hidden state $h_{t-1}$ at the previous time step $t - 1$, the calculations for the update gate $z_t$, reset gate $r_t$, candidate hidden state $h_t$ and the final hidden state $h_t$ are as follows:\n$z_t = \\sigma(W_z[h_{t-1}, e_t] + b_z)$,\n$r_t = \\sigma(W_r[h_{t-1}, e_t] + b_r)$,\n$\\tilde{h_t} = tanh(W_h[r_t \\odot h_{t-1}, e_t] + b_h)$,\n$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h_t}$      (2)\nwhere $W_z$, $W_r$, and $W_h$ are weight matrices, $b_z$, $b_r$, and $b_h$ are bias vectors, $\\sigma$ is a Sigmoid function. $\\odot$ denotes element-by-element multiplication, and $[h_{t-1}, x_t]$ indicates stitching the hidden state $h_{t-1}$ of the previous time step with the input $x_t$ of the current time step.\nFormally, the student representation $u_s$ is the average of the hidden vectors obtained from GRU:\n$u_s = \\frac{1}{N_s} \\sum_{j=1}^{N_s} h_i$     (3)\nwhere $N_s$ denotes the actual number of practice questions in the student's sequence, and each item is represented by a dense vector."}, {"title": "Capsule Blocks Module", "content": "Capsule blocks are used to construct the capsule structure, one for the 'correct' activation state and the others for the 'wrong' activation states. Given the output vectors encoded by GRU (i.e., $h_t$), the attention mechanism is used to construct capsule representations:\n$a_{t,j} = h_tW_{a,j}$,\n$\\alpha_{t,j} = \\frac{exp(a_{t,j})}{\\sum_{i=1}^{N_s}exp(a_{i,j})}$,\n$u_{c,j} = \\sum_{i=1}^{N_s}\\alpha_{t,j}h_t$,     (4)\nwhere $W_{a,j}$ represents the parameter of capsule block $j$ of the attention layer. The attention score for each position, $a_{t,j}$, is derived by multiplying the representation $h_t$ by the weight matrix $W_{a,j}$ and subsequently normalizing it to form a probability distribution across the items $A_{t,j} = [A_{1,j},A_{2,j}, ..., A_{N_s,j}]$. Finally, the capsule representation vector, $u_{c,j}$, is obtained as a weighted summation of all positions using the attention scores as weights."}, {"title": "Knowledge Tracing Module", "content": "We use the capsule representation vector obtained from the capsule blocks module to calculate the probability $p$ that a student can correctly answer the next question:\n$p_j = \\sigma(W_{p,j}u_{c,j} + b_{p,j})$,\n$p = mean(stack(p_j))$      (5)\nwhere $W_{p,j}$ and $b_{p,j}$ are the probability parameters for the current capsule block $j$, $N_c$ represents the number of capsule blocks."}, {"title": "Reconstruction Representation Module", "content": "Note that the capsule representation vector $u_{c,j}$ acquired through the attention mechanism constitutes a sophisticated encoding of the complete input student sequence information. This vector will be employed for the student representation reconstruction. Such reconstruction involves multiplying $u_{c,j}$ by the probability $p_j$:\n$r'_{s,j} = p_ju_{c,j}$,\n$r = max(stack(r'_{s,j}))$     (6)\nTo assess the effectiveness of the reconstructed representation, we evaluate the similarity between the reconstructed representation and the original student representation, primarily using the inner product between the reconstructed representation and the original student representation:\n$sim = \\sigma(ur)$     (7)"}, {"title": "Class Imbalance Module", "content": "The cross-entropy loss function is commonly utilized in the training of DLKT models. However, the phenomenon of class imbalance often results in easily classifiable instances dominating the loss and gradient computations. Drawing inspiration from the superior performance of focal loss in addressing long-tail issues in the visual domain, we apply focal loss to KT tasks. Focal loss not only balances the importance of positive and negative examples but also distinguishes between easy and hard examples. Specifically, it reduces the weight of easily classifiable examples, thereby emphasizing the training on hard-to-classify negative instances:\n$L_{CI} = -\\alpha c_t (1 - p)^\\gamma log(p)$     (8)\nwhere $\\alpha c_t$ is the weighting factor, empirically set to the class imbalance ratio, and $\\gamma$ is the tunable focusing parameter, $\\gamma \\ge 0$."}, {"title": "Training Objective", "content": "The training goal of the PKT model involves minimizing performance prediction loss, reconstruction loss of student representations, and addressing class imbalance. To achieve these goals, the model incorporates two additional losses: reconstruction loss and class balance loss, which complement the primary KT task.\nThe basic KT task still employs gradient descent to update the parameters of the model, aiming to minimize the cross-entropy loss between the model's final predictions and the ground truth labels:\n$L_{KT} = \\sum_{t=1}^T (\\alpha_{KT} log(p) + (1 - \\alpha_{KT}) log(1-p))$      (9)\nwhere T is the maximum length of the student sequence.\nTo ensure the similarity between the reconstructed representation obtained through the capsule blocks module and the original student representation, similarly, we utilize the previously obtained similarity $sim$ with the ground truth labels for cross-entropy loss computation:\n$L_{RR} = \\sum_{t=1}^T (\\alpha_{RR} log(sim) + (1 - \\alpha_{RR}) log(1-sim))$ (10)\nFinally, the overall training objective will be achieved through the following formula:\n$L = L_{KT} + \\lambda_{RR} L_{RR} + \\lambda_{CI} L_{CI}$     (11)\nwhere $\\lambda_{RR}$ and $\\lambda_{CI}$ are hyperparameters."}, {"title": "EXPERIMENT", "content": "In this section, we first experiment with four publicly educational datasets to comprehensively evaluate the effectiveness and advantage of the PKT."}, {"title": "Datasets", "content": "There are four publicly available datasets that support research on KT tasks. \nFollowing the preprocessing method from PYKT , we perform these steps: (1) remove attributes with null values; (2) exclude students with fewer than three records; (3) split questions involving multiple skills into separate interactions while retaining the 'user_id'; (4) adjust sequence lengths to the average student sequence length. Sequences longer than this average are truncated, those between three and the average are padded with -1, and sequences with fewer than three records are discarded."}, {"title": "Baselines", "content": "To validate the superiority and effectiveness of our model, we evaluate a total of 16 representative benchmark works. In addition to the aforementioned related works, we further compare the PKT model with the following state-of-the-art DLKT models including KQN , Deep-IRT , HawkesKT , DIMKT , AT-DKT , SimpleKT , and SparseKT ."}, {"title": "Experimental Setup", "content": "Following PYKT, we use 5-fold cross-validation for all DLKT methods and datasets. Interaction sequences are split with 80% for training and validation and 20% for testing. The maximum sequence length is set to the average for each dataset. We train the model with the ADAM optimizer for 200 epochs, applying early stopping if AUC does not improve in 10 epochs. All experiments are run on an NVIDIA GeForce RTX 3090. AUC serves as the primary metric, while accuracy and AUCPRC (Area Under the Precision-Recall Curve) are additionally employed to evaluate performance and ensure class balance ."}, {"title": "Results", "content": "Overall Performance summarizes the predictive performance of PKT and all baseline works on four publicly available datasets in terms of both AUC and ACC metrics. We can observe the following results: (1) PKT significantly outperforms the 16 baselines on all four datasets (except for AKT on ASSIST12 dataset which has a 0.25% AUC loss). More importantly, as a representative of DLKT models, our proposed model improves the AUC by 1.92%, 3.46%, and 0.25% on the ASSIST09, ASSIST15, and NIPS34 datasets,"}, {"title": "Ablation Study", "content": "To validate the effectiveness of the key components in the PKT model, we conduct three sets of ablation experiments to compare different variants of PKT."}, {"title": "Analysis of sequence length", "content": "The key distinction from the preprocessed data in PYKT is that, for each dataset, the sequence length is set to the average sequence length of the student rather than a fixed length of 200. As shown in Figure 4, setting the maximum sequence length to the student's average length improves PKT's performance by 1.80%, 1.67%, and 0.40% on the ASSIST09, ASSIST12, and ASSIST15 datasets, respectively. However, it results in a 1.21% performance decline on the NIPS34 dataset. This decline on the ASSIST datasets occurs because the average sequence length is less than 200, which means that using a fixed length introduces many -1 values as padding, adding invalid information. Overall, this validates the effectiveness of setting PKT's input sequence length to the average student sequence length for each dataset."}, {"title": "Visualization of Similarity", "content": "As shown in Figure 5, we randomly select a sequence record of a student's practice skill in the ASSIST09 dataset and calculate the similarity between the reconstructed representation and the original student representation using the inner product, denoted as $sim$ in Eq. 7. We then employ the t-SNE algorithm to project the representations into a two-dimensional space, with the x-axis and y-axis representing two new feature dimensions. These dimensions do not directly correspond to the original data's features but aim to preserve the similarity between data points. The use of t-SNE for dimensionality reduction enables us to visualize how the representations cluster and interact in a two-dimensional plane. Similar representations tend to cluster together, while dissimilar ones remain farther apart. In Figure 5, most data points cluster together, except for a few outliers, which confirms that the reconstructed representations are similar to the original student representations."}, {"title": "Visualization of Attention", "content": "In this section, we qualitatively present the visualization of the prediction results generated by PKT, as shown in Figure 6. To better understand the predictive behavior of the model, we randomly select the weights of the associations between the predicted future performance of the user and their historical interactions recorded in the ASSIST2009 dataset. The model utilizes four attention heads, each focusing on different aspects of the historical interaction sequence, thereby enhancing the overall prediction accuracy of the model."}, {"title": "Conclusion", "content": "In this paper, we present a personalized knowledge tracing (PKT) approach. In contrast to recent DLKT models, PKT reconstructs student representations from historical interaction sequences rather than constructing complex inputs from questions, skills, and auxiliary information. In addition, PKT employs focal loss to direct student attention to minorities, allowing the model to assess students' abilities in more balanced scenarios. Through qualitative and quantitative analyses, PKT outperforms 16 state-of-the-art DLKT methods across four publicly available datasets, achieving superior AUC and AUCPRC, with AUCPRC being a common metric for class imbalance issues."}]}