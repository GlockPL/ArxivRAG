{"title": "FLAG-TRADER: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading", "authors": ["Guojun Xiong", "Zhiyang Deng", "Keyi Wang", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Xueqing Peng", "Mingquan Lin", "Kaleb E Smith", "Xiao-Yang Liu Yanglet", "Jimin Huang", "Sophia Ananiadou", "Qianqian Xie"], "abstract": "Large language models (LLMs) fine-tuned on multimodal financial data have demonstrated impressive reasoning capabilities in various financial tasks. However, they often struggle with multi-step, goal-oriented scenarios in interactive financial markets, such as trading, where complex agentic approaches are required to improve decision-making. To address this, we propose FLAG-TRADER, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven reinforcement learning (RL) policy optimization, in which a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. Through policy gradient optimization driven by trading rewards, our framework not only enhances LLM performance in trading but also improves results on other financial-domain tasks. We present extensive empirical evidence to validate these enhancements.", "sections": [{"title": "1 Introduction", "content": "Algorithmic financial trading represents a critically complex decision-making domain that perpetually grapples with the intertwined challenges of synthesizing heterogeneous market signals and dynamically refining strategies (Hambly et al., 2023; Yu et al., 2024b; Li et al., 2023). Traditional reinforcement learning (RL) approaches, despite their theoretical grounding in Markov Decision Processes (MDPs), confront three fundamental limitations when deployed in financial markets. Firstly, their inability to coherently model multimodal market states-spanning sequential price movements, quantitative technical indicators, and unstructured textual sentiments\u2014compromises data integration (Zhang et al., 2019; Nassirtoussi et al., 2014). Secondly, non-stationary data distributions inherent to financial systems systematically erode strategy generalizability across market regimes (Zhang et al., 2019). Thirdly, the heavy reliance on manually crafted technical indicators (e.g., MACD, RSI) and complex feature engineering (Liang et al., 2018) introduces subjective biases, leads to information loss, and reduces the robustness of real-time decision-making, especially in volatile market conditions.\n\nThe emergence of Large Language Models (LLMs) offer significant potential for financial decision-making by addressing key limitations of RL-based trading strategies. Leveraging their transformer architecture, they serve as multimodal feature extractors, integrating time-series and textual data, capturing long-range dependencies, and generalizing across market regimes, while also extracting nuanced sentiment signals without relying on manually crafted features (Chen et al., 2021; Yang et al., 2023a; Jin et al., 2023; Wood et al., 2021; Yu et al., 2024a; Deng et al., 2023). Nonetheless, adapting LLMs for trading presents key challenges. First, their deployment often relies on agentic frameworks (Li et al., 2024b, 2023; Yu et al., 2025), which incur high implementation and operational costs due to their complex architecture. Second, LLMs are primarily trained for static text generation, making them ill-suited for sequential decision-making in trading. This prompts us to the following question:\n\nCan we design a framework that seamlessly integrates LLMs' reasoning with RL's reward-driven optimization to tackle the challenges of financial sequential decision-making?\n\nTo resolve these interconnected challenges, we FLAG-TRADER, a unified architecture integrating linguistic processing (via LLMs) with gradient-driven RL policy optimization, as shown in Figure 1. This framework advances two synergistic innovations: a parameter-efficient fine-tuning module that jointly encodes temporal market data and textual streams into unified state representations and a hybrid RL component that explicitly incorporates external environment reward gradients into policy updates, ensuring alignment with trading performance metrics. Our contributions are summarized as follows.\n\nFirst, we propose the FLAG-TRADER framework, where a partially fine-tuned LLM acts as the policy network, leveraging pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning. The model processes market data using a textual state representation, enabling it to interpret and respond to market conditions effectively. Rather than fine-tuning the entire LLM, only a subset of its parameters is updated, balancing domain adaptation and knowledge retention. This design allows FLAG-TRADER to make informed trading decisions while remaining computationally efficient and preserving the LLM's general reasoning capabilities.\n\nSecond, we conduct extensive experiments to evaluate FLAG-TRADER across multiple financial trading tasks. Our results demonstrate that FLAG-TRADER consistently outperforms both the buy-and-hold strategy and LLM-agentic baselines, particularly in terms of cumulative return and Sharpe ratio, which we prioritize for financial performance assessment. Notably, our approach enables a small-scale (135M parameter) open-source LLM to surpass much larger proprietary models, highlighting the effectiveness of RL fine-tuning in optimizing LLM-driven trading strategies. These findings underscore the potential of integrating LLMs with RL to enhance financial decision-making while maintaining computational efficiency."}, {"title": "2 Related Work", "content": "RL in Finance. RL has shown promise for financial decision-making, spanning Q-learning approaches for Sharpe ratio maximization (Gao and Chan, 2000), dynamic asset allocation (Jangmin et al., 2006), deep Q-learning (Jeong and Kim, 2019), tabular SARSA (de Oliveira et al., 2020), policy-based portfolio optimization (Shi et al., 2019), and actor-critic methods (Ye et al., 2020) enhanced by adversarial training (Liang et al., 2018) and transformer-based architectures (Huang et al., 2024). Recent research efforts in RL for financial applications have been greatly aided by open-source frameworks like FinRL (Liu et al., 2022), which standardize implementations and provide reproducible benchmarks. Comprehensive surveys (Hambly et al., 2023; Sun et al., 2023) further showcase advances in both methodological rigor and real-world deployment. Despite these advances, RL-based trading still requires large training data, struggles with non-stationary markets, and faces challenges incorporating multimodal information in real time.\n\nLLMs in Finance. A growing trend is the integration of LLMs into financial decision-making. Hybrid systems like FinCon (Yu et al., 2025) and TradingGPT (Li et al., 2023) leverage language understanding to enhance trading agents, while domain-specific models such as FINBERT (Araci, 2019; Yang et al., 2020), FLANG (Shah et al.,"}, {"title": "3 Problem Statement", "content": "We define the financial decision-making process as a finite horizon partially observable Markov decision process (MDP) with time index {0,..., T}, represented by the tuple: M = (S, A, T, R, \u03b3), where each component is described in detail below.\n\nState. The state space $S = X \\times Y$ consists of two components: market observations and trading account balance, i.e., $s_t = (m_t, b_t) \\in S$. Specifically, $m_t = (P_t, N_t) \\in X$ represents the market observation process, includingstock price $P_t$ at time t, and financial news sentiment or macroeconomic indicators $N_t$; $b_t = (C_t, H_t) \\in Y$ represents the trading account balance, including available cash $C_t$ at time t, and number of stock shares $H_t$.\n\nAction. The agent chooses from a discrete set of trading actions $A = {Sell :-1,Hold : 0, Buy : 1}$, where $a_t = -1$ denotes selling all holdings (liquidate the portfolio), $a_t = 0$ denotes holding (no trading action), and $a_t = 1$ represents buying with all available cash (convert all cash into stocks).\n\nState Transition. The state transition dynamics are governed by a stochastic process $s_{t+1} \\sim T(\\cdot|s_t, a_t)$. The trading account evolves according to the following equations:\n\n*   If Sell: $C_{t+1} = C_t + H_tP_{t+1}, H_{t+1} = 0$.\n*   If Hold: $C_{t+1} = C_t, H_{t+1} = H_t$.\n*   If Buy: $C_{t+1} = 0, H_{t+1} = H_t + \\frac{C_t}{P_{t+1}}$\n\nReward. The agent receives a reward based on the daily trading profit & loss (PnLs):\n\n$R(s_t, a_t) = SR_t - SR_{t-1}$,\n\nwhere $SR_t$ denotes the Sharpe ratio at day t, computed by using the historical PnL from time 0 to time t. Moreover, PnL at time t is calculated as\n\n$pnlt := (C_t - C_{t-1}) + (H_tP_t - H_{t-1}P_{t-1})$.\n\nThen, the Sharpe ratio $SR_t$ at time t can be calculated as:\n\n$SR_t := \\frac{E[pnl_1,\\dots, pnl_t] - r_f}{\\sigma[pnl_1,\\dots, pnl_t]}$,\n\nwhere $E[pnl_1,\\dots, pnl_t]$ is the sample average of daily PnL up to time t, $r_f$ is the risk-free rate, and $\\sigma[pnl_1,\\dots, pnl_t]$ is the sample standard deviation of daily PnL up to time t.\n\nThe goal is to find an admissible policy \u03c0 to maximize the expected value of cumulative discounted reward, i.e.,\n\n$\\underset{\\pi}{max} V^{\\pi}(s) = E_{\\substack{s_0=s, a_t\\sim\\pi(S_t) \\\\ S_{t+1}\\sim T(S_t,a_t)}}[\\sum_{t=0}^T \\gamma^t R_t]$,\n\nwhere Rt is a shortened version R(st, at) and \u03b3\u2208 (0, 1] is the discount factor controlling the importance of future rewards.\n\nOur goal is to train an LLM agent parameterized by \u03b8 to find the optimized policy $\u03c0_\u03b8$ for (2), i.e.,\n\n$a_t \\sim \\pi_\\theta(\\cdot|S_t) = LLM(lang(S_t); \\theta)$,\n\nwhere lang(st) are the prompts generated by converting state st into structured text. The proposed pipeline is illustrated in Figure 1."}, {"title": "4 FLAG-TRADER", "content": "To tackle the challenge of directly fine-tuning an LLM for both alignment and decision-making, we introduce FLAG-TRADER, a fused LLM-agent and RL framework for financial stock trading. In FLAG-TRADER, a partially fine-tuned LLM serves as the policy network, leveraging its pre-trained knowledge while adapting to the financial domain through parameter-efficient fine-tuning, as shown in Figure 2. The model processes financial information using a textual state representation, allowing it to interpret and respond to market conditions effectively. Instead of fine-tuning the entire network, only a subset of the LLM's parameters is trained, striking a balance between adaptation and knowledge retention. In the following, we will present the prompt input design and the detailed architecture of FLAG-TRADER."}, {"title": "4.1 Prompt Input Design", "content": "The first stage of the pipeline involves designing a robust and informative prompt, denoted as lang(st), which is constructed based on the current state st to guide the LLM in making effective trading decisions. The prompt is carefully structured to encapsulate essential elements that provide context and ensure coherent, actionable outputs. It consists of four key components: a task description, which defines the financial trading objective, outlining the problem domain and expected actions; a legible action space, specifying the available trading decisions (Sell,\" Hold,\" \"Buy\"); a current state representation, incorporating market indicators, historical price data, and portfolio status to contextualize the decision-making process; and an output action, which generates an executable trading decision. This structured prompt ensures that the LLM receives comprehensive input, enabling it to produce well-informed and actionable trading strategies, as illustrated in Figure 3."}, {"title": "4.2 FLAG-TRADER Architecture", "content": "To incorporate parameter-efficient fine-tuning into the policy gradient framework, we partition the intrinsic parameters of the LLM into two distinct components: the frozen parameters inherited from pretraining, denoted as $\u03b8_{froze}$ , and the trainable parameters, denoted as $\u03b8_{train}$. This separation allows the model to retain general language understanding while adapting to financial decision-making with minimal computational overhead. Building upon this LLM structure, we introduce a policy network and a value network, both of which leverage the trainable top layers of the LLM for domain adaptation while sharing the frozen layers for knowledge retention. The overall architecture is illustrated in Figure 2."}, {"title": "4.2.1 Policy Network Design", "content": "The policy network is responsible for generating an optimal action distribution over the trading decision space A, conditioned on the observed market state. It consists of three main components:\n\nState Encoding. To effectively process financial data using the LLM, the numerical market state s is first converted into structured text using a predefined template\u00b9\n\n$lang(s) = \"Price: \\$p, Vol: v, RSI: r,...\".$\n\nThis transformation enables the model to leverage the LLM's textual reasoning capabilities, allowing it to understand and infer trading decisions in a structured, language-based manner.\n\nLLM Processing. The tokenized text representation of the state is then passed through the LLM backbone, which consists of: 1) Frozen layers (preserve general knowledge): Token embeddings E = Embed(lang(s)) pass through LLM frozen layers, i.e.,\n\n$h^{(1)} = LLM_{1:N}(E; \\theta_{frozen})$.\n\nThese layers preserve general knowledge acquired from pretraining, ensuring that the model maintains a strong foundational understanding of language and reasoning. 2) Trainable layers (domain-specific adaptation): The output from the frozen layers is then passed through the trainable layers, which are fine-tuned specifically for financial decision-making, i.e.,\n\n$h^{(2)} = LLM_{N+1:N+M}(h^{(1)}; \\theta_{train})$.\n\nThis structure enables efficient adaptation to the financial domain without modifying the entire LLM, significantly reducing training cost while maintaining performance.\n\nPolicy Head. Finally, the processed representation is fed into the policy head, which outputs a probability distribution over the available trading actions according to\n\n$logits = POLICY\\_NET(h^{(2)}, \\theta_p) \\in R^{|A|}$,\n\nwhere $\u03b8_p$ is the parameter of POLICY_NET, with action masking for invalid trades:\n\n$\\pi(a|s) = \\begin{cases}0 & a \\notin A, \\\\ \\frac{exp(logits(a))}{\\sum_{a'}exp(logits(a'))} & otherwise.\\end{cases}$\n\nThis ensures that actions outside the valid set A (e.g., selling when no stocks are held) have zero probability, preventing invalid execution."}, {"title": "4.2.2 Value Network Design", "content": "The value network serves as the critic in the RL framework, estimating the expected return of a given state to guide the policy network's optimization. To efficiently leverage the shared LLM representation, the value network shares the same backbone as the policy network, processing the textual state representation through the frozen and trainable layers (4)-(6). This design ensures efficient parameter utilization while maintaining a structured and informative state encoding. After passing through the LLM processing layers, the output $h^{(2)}$ is fed into a separate value prediction head, which maps the extracted features to a scalar value estimation:\n\n$V(s) = VALUE\\_NET(h^{(2)}, \\theta_v) \\in R^1$,\n\nwhere $\u03b8_v$ is the parameter of VALUE_NET."}, {"title": "4.3 Online Policy Gradient Learning", "content": "The policy and value networks in FLAG-TRADER are trained using an online policy gradient approach, ensuring that the model continuously refines its decision-making ability. The learning process follows an iterative cycle of state observation, action generation, reward evaluation, and policy optimization. The parameters of the model are updated using stochastic gradient descent (SGD), leveraging the computed policy and value losses to drive optimization.\n\nAt each training step, we define two key loss functions, i.e., policy loss Lp: measures how well the policy network aligns with the expected advantage-weighted log probability of actions; value loss $L_V$: ensures that the value network accurately estimates the expected return.\n\nBased on these loss functions, the model updates the respective network parameters using backpropagation as follows.\n\nUpdate Policy Head. The policy network parameters $\u03b8_p$ are updated via SGD to minimize the policy loss LP\n\n$\\theta_p \\leftarrow \\theta_p - \\eta \\nabla_{\\theta_p} L_P$,\n\nUpdate Value Head. The value network parameters $\u03b8_v$ are optimized via SGD to minimize the temporal difference (TD) error over policy loss $L_V$\n\n$\\theta_v \\leftarrow \\theta_v - \\eta \\nabla_{\\theta_v} L_V$.\n\nUpdate Trainable LLM Layers. The trainable LLM parameters $\u03b8_{train}$ are updated via SGD jointly based on both the policy and value losses, i.e., $L_P$ and $L_V$, allowing the shared LLM representation to align with optimal decision-making:\n\n$\\theta_{train} \\leftarrow \\theta_{train} - \\beta \\nabla_{\\theta_{train}}(L_P + L_V)$,\n\nThe updates in (10)\u2013(12) are performed iteratively until the stopping criteria are met, as outlined in Algorithm 1. This iterative learning process effectively balances exploration and exploitation, enhancing policy performance while maintaining stability."}, {"title": "5 Experiments", "content": "This section describes the overall experimental design and environmental setup for comparing the performance of different trading agents under consistent conditions."}, {"title": "5.1 Experiment Setup", "content": "For our single-asset trading tasks, we adopt two baselines: the buy-and-hold strategy and the LLM-based trading agent from INVESTORBENCH (Li et al., 2024a), which integrates 13 proprietary or open-source large language models. Our proposed model, FLAG-TRADER (built on a 135M-parameter LLM), is then evaluated against these baselines for a comprehensive performance comparison.\n\nWe focus on five stocks and one crypto: Microsoft Corporation (MSFT), Johnson & Johnson"}, {"title": "5.2 Evaluation Metrics", "content": "We use four widely recognized financial metrics (Hull, 2007) to evaluate and compare the investment performance of various LLM backbones across different tasks: Cumulative Return (CR), Sharpe Ratio (SR), Annualized Volatility (AV), and Maximum Drawdown (MDD). As CR and SR focus more on long-term gains and risk-adjusted returns, they are typically considered more important than AV and MDD for assessing asset trading performance. Accordingly, we treat CR and SR as our primary metrics for the final evaluation.\n\nCumulative Return (CR) % measures the total value change of an investment over time by summing logarithmic return calculated from daily PnL:\n\n$CR = \\sum_{t=1}^T log (1 + \\frac{pnl_t}{C_{t-1}+H_{t-1}P_{t\u22121}})$,\\\\\n\nwhere pnlt is the PnL at time t, and $C_{t-1} + H_{t-1}P_{t\u22121}$ is the account balance at time t 1. Notice that higher values indicate better strategy effectiveness.\n\nSharpe Ratio (SR) assesses risk-adjusted returns by dividing the average excess return (Rp) over the risk-free rate (rf) by its volatility (\u03c3\u03c1):\n\n$SR = \\frac{R_p - r_f}{\\sigma_p}$\n\nNotice that higher ratios signify better performance.\n\nAnnualized Volatility (AV) % and Daily Volatility (DV) % quantify return fluctuations; AV is derived by scaling DV (standard deviation of daily logarithmic returns) by the square root of the annual trading days (252):\n\n$AV = DV \\times \\sqrt{252}$.\n\nThis metric highlights potential return deviations across the year.\n\nMax Drawdown (MDD) % calculates the largest drop from peak to trough of the value of balance account:\n\n$MDD = max( \\frac{V_{trough}}{V_{peak}})$,\\\\\n\nNotice that lower values indicate lesser risk and higher strategy robustness."}, {"title": "5.3 Experimental Results", "content": "FLAG-Trader achieves superior stock trading performance. Unlike the baseline agent, which relies on an LLM-agentic framework, FLAG-TRADER undergoes an RL-based post-training phase. FLAG-TRADER consistently outperforms the baseline across multiple metrics, demonstrating its stronger adaptability and optimization in diverse market environments.\n\nConvergence to a (stable) optimal policy. When using an LLM as the policy network in deep RL, the policy is jointly parameterized by the model's intrinsic parameters and the prompt. Although the initial prompts strongly influence policy generation during the first few training epochs, this effect diminishes over time. Consequently, the system converges to a relatively stable policy that becomes less sensitive to the initial prompts, indicating that RL training allows the LLM-based agent to refine its strategy and achieve a robust trading policy.\n\nFLAG-TRADER enables small-scale models to surpass large-scale counterparts. While increasing model size generally enhances financial decision-making and robustness as seen with large proprietary models"}, {"title": "6 Conclusion", "content": "In this paper, we introduced FLAG-TRADER, a novel framework that integrates LLMs with RL for financial trading. In particular, FLAG-TRADER leverages LLMs as policy networks, allowing for natural language-driven decision-making while benefiting from reward-driven optimization through RL fine-tuning. Our framework enables small-scale LLMs to surpass larger proprietary models by efficiently adapting to market conditions via a structured reinforcement learning approach. Through extensive experiments across multiple stock trading scenarios, we demonstrated that FLAG-TRADER consistently outperforms baseline methods, including LLM-agentic frameworks and conventional RL-based trading agents. These results highlight the potential of integrating LLMs with RL to achieve adaptability in financial decision-making."}, {"title": "Limitations and Potential Risk", "content": "Despite its promising results, FLAG-TRADER has several limitations. First, while our approach significantly enhances the decision-making ability of LLMs, it remains computationally expensive, particularly when fine-tuning on large-scale market datasets. Reducing computational overhead while maintaining performance is an important direction for future research. Second, financial markets exhibit high volatility and non-stationarity, posing challenges for long-term generalization. Future work should explore techniques such as continual learning or meta-learning to enhance model adaptability in evolving market conditions. Third, while FLAG-TRADER effectively integrates textual and numerical data, its reliance on structured prompts could introduce biases in decision-making. Improving prompt design or exploring retrieval-augmented methods may further enhance robustness. Lastly, real-world trading requires stringent risk management, and FLAG-TRADER currently optimizes for financial returns without explicitly incorporating risk-sensitive constraints. Extending the framework to integrate risk-aware objectives and dynamic portfolio optimization could provide more robust and practical financial trading solutions."}, {"title": "A Additional Algorithmic Details: FLAG-TRADER with PPO", "content": "In this section, we outline a detailed procedure for training the FLAG-TRADER architecture via PPO, where the POLICY_NET (actor) and the VALUE_NET (critic) share a subset of trainable parameters from a LLM, with $\u03b8 = (\u03b8_{train}, \u03b8_p, \u03b8_v)$. We define $\u03b8_{policy} = (\u03b8_{train}, \u03b8_p)$ and $\u03b8_{value} = (\u03b8_{train}, \u03b8_v)$ for simplicity.\n\nAdvantage Estimation. We use the Generalized Advantage Estimation (GAE) to compute the advantage function $A_t$:\n\n$A_t = \\sum_{k=0}^{T-1} (\u03b3\u03bb)^k [r_{t+k} + \u03b3V_{\u03b8_{value}}(s_{t+k+1}) \u2212 V_{\u03b8_{value}}(s_{t+k})]$,\n\nwhere \u03b3 is the discount factor, and \u03bb is the GAE parameter.\n\nProbability Ratio. Let $\u03b8_{policy,old}$ denote the parameters before the current update. The PPO probability ratio is\n\n$r_t(\u03b8_{policy}) = \\frac{\u03c0_{\u03b8_{policy}} (a_t | s_t)}{\u03c0_{\u03b8_{policy,old}} (a_t | s_t)}$,\n\nPPO Clipped Objective. PPO clips this ratio to prevent overly large updates. The surrogate objective is\n\n$L_P(\u03b8_{policy}) = E_t[min(r_t(\u03b8_{policy}) A_t, clip (r_t(\u03b8_{policy}), 1 \u2212 \u03b5, 1 + \u03b5) A_t)]$,\n\nwhere \u03b5 is a hyperparameter.\n\nValue Function Loss. The critic (value network) is updated by minimizing the difference between the predicted value $V_{\u03b8_{value}}(s_t)$ and the target return $R_t$. A common choice is:\n\n$L_V(\u03b8_{value}) = E_t [(V_{\u03b8_{value}}(s_t) - R_t)^2]$.\n\nCombined Loss. We often add an entropy term to encourage exploration, yielding the overall objective:\n\n$L_{total}(\u03b8) = \u2212L_P(\u03b8_{policy}) + c_1 L_V(\u03b8_{value}) - c_2H(\u03c0_{\u03b8_{policy}})$,\n\nwhere c\u2081 and c\u2082 are weighting coefficients, and $H(\u03c0_{\u03b8_{policy}})$ represents the policy entropy.\n\nParameter Updates. At each iteration, we apply gradient descent on the total loss:\n\n$\u03b8_p \u2190 \u03b8_p - \u03b7 \\nabla_{\u03b8_p} L_P$,\n\n$\u03b8_v \u2190 \u03b8_v - \u03b7 \\nabla_{\u03b8_v} L_V$,\n\n$\u03b8_{train} \u2190 \u03b8_{train} - \u03b2 \\nabla_{\u03b8_{train}} L_{total}$,\n\nwhere \u03b7 and \u03b2 are learning rates for the policy head, value head, and trainable LLM layers respectively. The algorithm is summerized in Algorithm 2."}, {"title": "B Additional Experimental Details", "content": "Hyperparameters for Finetuening FLAG-TRADER with PPO in Algorithm 2"}]}