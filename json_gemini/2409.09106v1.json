{"title": "RECENT TRENDS IN MODELLING THE CONTINUOUS TIME\nSERIES USING DEEP LEARNING: A SURVEY.", "authors": ["Mansura Habiba", "Barak A. Pearlmutter", "Mehrdad Maleki"], "abstract": "Continuous-time series is an essential component for different modern days application areas, e.g.\nhealthcare, automobile, energy, finance, Internet of things (IoT) and other related areas. Different\napplication needs to process as well as analyse a massive amount of data in time series structure in\norder to determine the data-driven result, for example, financial trend prediction, potential probability\nof the occurrence of a particular event occurrence identification, patient health record processing and\nso many more. However, modeling real-time data using a continuous-time series is a very challenging\ntask since the dynamical systems behind the data could be a differential equation. Several research\nworks have tried to solve the challenges of modeling the continuous-time series using different neural\nnetwork models and approaches for data processing and learning. The existing deep learning models\nare not free from challenges and limitations due to diversity among different attributes, behaviour,\nduration of steps, energy, and data sampling rate. In this paper, we have described the general problem\ndomain of time series and reviewed the challenges in terms of modelling the continuous time series.\nWe have presented a comparative analysis of recent developments in deep learning models and their\ncontribution in order to solve different challenges of modeling the continuous time series. We have\nalso identified the limitations of the existing neural network model and open issues. The main goal of\nthis review is to understand the recent trend of neural network models used in a different real-world\napplication with continuous-time data.", "sections": [{"title": "1 Introduction", "content": "Deep learning algorithms have demonstrated impressive performance in terms of the continuous-time series modelling.\nAt the same time, the sequence-based data model, for example, time series has become trendy in different areas of\nindustry and science. Recent deep learning model architectures are focused on learning time sequence modelling"}, {"title": "2 Time Series Analysis", "content": "Due to the dynamic nature of the time series, learning the continuous-time data with deep learning algorithms is a\nvery complex task. Moreover, efficient and precise computation is even more challenging. One of the fundamental\nbehavior is that continuous time series vary in length, and the complexity of learning such a long time series increases\nwith the increase of the length. Besides, there is an implicit dependency between different time states in a series. The\nprevious input and past computation have a significant role in calculating the current or any future state in a time series.\nTherefore, most of the time, there is a requirement for memorizing the previous state in the current state. As a result, a\ntraditional feed-forward neural network, where input at each state is independent of each other is not capable of learning\ntime series. Therefore, it is essential to understand the different dynamic and specific nature of the continuous-time\nseries to design a neural network model. In this section, we discuss those unique characteristics of a time series."}, {"title": "2.1 What is the time series?", "content": "Time series is as a sequence of numeric observations of a variable at a continuous time. For example, Fig. 1 shows the\nobservation of certain variable, i.e. stock price of Dell over a definite time period. More precisely, a time series is a\nfunction $x : T \\rightarrow R^N$, where $T \\subseteq R$, with a probability distribution P, i.e., $(x_t)_{t\\in T} \\sim P$. If $t \\in T$ then $x(t)$ or $x_t$\nrefers to the observed value at time t. If T is a finite set we have definition of discrete time series. If $T = {t_1,...,t_n}$\nwith $t_{i+1} - t_i = h$ then $(x_t)_{t\\in T}$ generates a discrete time time-series. If T = [a, b] then a continuous time time-series\n[11] is produced.\nFor example, any variable x can have value as $X_1,X_2, ..., I_n$ at a different time step, such as $t_1, t_2, ..., t_n$. Eq (1)\nshows time series as a vector [12], where each value $x_t \\in R$. Each entry in the X vector is the real-time value\nof a time-dependent variable measured at a corresponding time t of a continuous or discrete-time set T, i.e., $x_1 =$\n$x(t_1), x_2 = x(t_2), ..., where t_1, t_2,\u2026\u2026 \\in T$.\n$X_n = (x_1,x_2,...,x_n)$"}, {"title": "2.2 What are the properties of a continuous-time series?", "content": "Continuous-time data has it's unique properties. Some of them are listed as follows\n\\bullet Length: Different research application such as automobile, health, they need to observe the value for any\nvariable for a very long period to get correct data. Therefore, most continuous Real-time data is usually\nvery long. For example, weather data analysis for ten years, financial trend analysis for the last decade,\nunemployment statistics analysis, event data from sensors for a self-driving car for one hour. For accurate\nanalysis in any type of time series problem including classification, anomaly detection, synthetic generation, it\nis important to train data for a long duration.\n\\bullet Higher Sampling Frequency: For a continuous time series, the intermediate time step is short, and the\nsampling frequency is much higher. For example, existing recurrent neural network (RNN) models use a\nfixed time step, and instead of modeling a continuous time series, these models first convert the time data to\ndiscrete-time time series data and then process it with a higher sampling rate. As a result, for a long time series,\nthe model chooses a comparatively small fixed time step but a high data sampling rate, for better accuracy.\n\\bullet Pathological dependencies: One of the main goals for time series modeling using deep learning algorithms is\nto identify underline temporal relation of consecutive data points in order to detect the pattern. In the case\nof most time-series, there are dependencies between consecutive data points. This dependency can be either\nimplicit or explicit. Another related property of continuous-time series is that the length of mutually dependent\nsequence length is unknown. For example, $x_t$ can have a dependency on previous observations at a very distant\npast($x_{t-n}$) or a very short distance past($x_{-1}$).\n\\bullet Higher Dimension: Every data point $x_t$ in Eq. (1) consists of thousands of attributes. Multiple parameters\nare a common feature for continuous real-time data. For example, $x_t$ in Eq. (1) can be rich in dimensionality\nwith a large number of parameters in order to describe an individual observation. Multi-variate time series\nshows very high dimensionality. The value for x at time t does not only depend on time t but also depend on\nother variables(multivariate time series), i.e., if $X(t) = [x_1(t),...,x_n(t)]$ then $X(t) = A_1X(t - 1) +$\n$A_pX(t-p) + b + \\varepsilon(t)$ in its simplest form, where $A_1$ are $N \\times N$ autoregressive matrices and error term e\nand $(x_i(t))_{t\\in T}$ for i = 1, . . ., N are different time series [13].\n\\bullet Additional Noise: Along with the real-time value of any observations $x_t$ at a time t, continuous-time also\ncontains excessive noise components for the entire series.\n\\bullet Higher Energy Consumption: The dimension of a continuous-time series is often very long, which requires\nextensive computation and thereby energy.\n\\bullet Uncertainity: Continuous-time series is often very dynamic and uncertain with the missing pattern as well\nas irregularities. Therefore, it is often impossible to model the complete time sequence. There are different\nwell-known data imputation techniques are used to replace the missing values in a time series.\n\\bullet Irregular Sampling Rate : Besides having a higher sampling rate, continuous-time series also exhibits\nan irregular sampling rate. The irregular sampling rate, high complexity, and massive length of real-world\ncontinuous time series often cause missing data point that ultimately affects the result of neural network"}, {"title": "3 Problem Domain Analysis", "content": "Time series related problems are not new, and the categories of these problems are wide. Primarily, these problems\ncan be classified into four categories [14],as shown in figure [2]. Again Modeling can be either sequence-to-sequence\nmapping or augmentation. Time series classification has a wide range of applications, as shown in Table 4. Classification\nproblem can be either related to the classification of a pattern embedded in the data or classification of continuous-time\nseries data such as video, text, wavelet and others. Prediction probably the most popular problem type in the domain of\ncontinuous time series. Besides these, sometimes detection of embedding pattern in the continuous-time data are also\nessential problem category. Another kind of problem in time series is anomaly detection. There is no abundant number\nof work in this area of time series problem domain. [15] proposed an anomaly detection mechanism using low-level\ntracking algorithms and [16] highlights the following questions that need to be solved for deep learning to be ready to\nprovide accurate results.\n\\bullet How does understanding (explicitly extracting the geometrical structure of a low-dimensional system) relate to\nlearning (adaptively building models that emulate a complex system)?\n\\bullet When a neural network correctly forecasts a low-dimensional system, it has to have formed a representation of\nthe system.\n\\bullet What is this representation?\n\\bullet Can the representation be separated from the network's implementation?\n\\bullet Can a connection be found between the entrails of the internal structure in a possibly recurrent network,\nthe accessible structure in the state-space reconstruction, the structure in the time series, and ultimately the\nstructure of the underlying system?\nIt is fascinating to say that most of the questions are now known. As an answer to the third question, the time series is\nrepresented using a graph or N \u00d7 M vector as the input of a neural network. The neural network proposed in [8, 9]\ndescribes a neural network model where the structure of space, as well as the structure of the time series, are considered\nfor neural network model architecture. These works are practical examples to answer the fifth question mentioned\nabove. Spatio-temporal-graph [8] has become a popular representation of space and time sequence.\nThe unique behaviour of continuous-time series attracts the attention of deep learning researchers in the early '90s.\nTable 4 shows some related works in every type of continuous-time series-based problem. In this paper, we have\nevaluated recent works in mainly the last decade.\nThere is no single solution for each of the above-mentioned problem category. Different types of neural network\nmodels are suitable for different problems types. For example, Artificial wavelet neural network (ANN) [12] is suitable\nfor modelling and prediction of continuous time series. On the other hand, RNN pioneer in time series modelling,\nclassification and prediction problem domains due to it's a suitable set of properties. For time series classification,\nCNN is mostly used, model. CNN can learn classes from a continuous-time series through unsupervised learning\nwith minimum human interaction. In most cases, CNN, along with for time series modelling, is not a practical choice.\nCNN is often used in a hybrid model along with other kinds of the neural network such as AWNN or RNN, which can\nimprove the performance significantly. A recent trend in using hybrid neural network models becoming very popular in"}, {"title": "Classification:", "content": "Time series classification task is a complex task for deep learning fraemwork. Time series can be\neither univariate as described by Eq (1) or M-Dimentional as described in Eq. (2), where $X_i$ itself is a univariate time\nseries. Sometimes time series can be even more complex where $X_i$ in Eq. (2) can be a multi-variate time series instead\nof being uni-variate.\n$X = X_1, X_\u2081 ... X_n$\nFor Time series classification task, time series is described as collection of tuple in [17] as shown in Eq.(3). Here time\nseries is a data set, D, consists of a collection of pair $(X_i, Y_i)$, where $X_i$ can be either uni-variate or multi-variate time\nseries and $Y_i$ is a label.\n$D = {(X_1,Y_1), (X_2,Y_2), . . ., (X_N, Y_N)}$\nTime series classification task becomes more challenging as the number of dimention increases."}, {"title": "Prediction :", "content": "Prediction tasks uses previous oversavation for any variable such as stock price, rain fall, house price,\nenergy consumption and others; and forecast a future oversavation value for corresponding variable. [18, 12] describes\nthe time series prediction using neural network identifies the relation between previous value and current value of a\nvariable. For example, in Eq. (4a), f is a neura network that can identify the relation between past values of x from\ntime t \u2013 n to time t \u2013 1 with it's current value x(t) at time t. This is an exmaple of simplest time series prediction with\none step. Similarly Eq. (4b) shows multi-step ahead prediction, where the next h-th value for variable x is predicted\nusing neural network f.\n$x(t) = f{x(t - 1), x(k - 2), . . ., x(t - n 1)}$\n$x(t + h) = f{x(k), x(k - 1), x(k-2),...,x(k n 1)}$\nIn Fig 3 the predicted number of confirmed Covid-19 cases in Ireland are platted."}, {"title": "Detection:", "content": "Detection task is often used for classification and prediction. For example, to undertsnad the difference or\nanomaly in Actual value and Predicted value shown in Fig3, time series anomaly detection is essential."}, {"title": "Augmentation:", "content": "A recent survey paper, [19] describes different techniques for successful time series augmentation\nused for generating synthetic time series data. 4 shows the taxonomy of time series augmentation for deep learning\nalgorithms proposed by [19]. This review finds the most available time-series augmentation technique. However,\ndue to the complex nature of different fields, the time series data has particular characteristics that need additional\ntreatment. For example, finance multi-variate time data often shows intrinsic probabilistic patterns as a relationship"}, {"title": "3.1 Different dataset", "content": "This section discuss some well-known datasets used for different research. Different dataset focuses on different\ncharacteristics of continuous-time data as discussed in 3. Table 2 shows the feature for some well known datasets.\nDifferent libraries and packages are developed to generate time targeting different algorithms for time series problems.\n[40] is a python package which provides the implementation along with example dataset for different time series\nclassification algorithms such as Dynamic Time Warping, Shapelet Transform, Markov Transition Field, Time Series\nForest and others.\nFor healthcare, the challenges of availability of dynamic dataset is sever than other research fields. Due to data privacy\nissue, a limited number of the dataset is publicly available, which focuses on a limited number of aspects of health care."}, {"title": "4 Different Applications of Continuous Time Series", "content": "There are several applications of sequential data modelling, for example, speech recognition, bioinformatics and human\nactivity recognition. In speech recognition, the input is a continuous or discrete audio clip P, which needs to be\nmapped to corresponding text transcript Q. In this example, the input and the output are both sequential data containing\ntemporal information. Input, P, is an audio clip and so that plays out over time and output, Q, is a sequence of\nwords. Therefore, deep learning models are suitable for sequential data such as recurrent neural networks and its other\nvariations, have become promising for speech recognition\u2014another example of the continuous-time series modelling in\nmusic generation. In the case of music generation, only the output Q is a sequence of music notes, where the input can\nbe an empty set, or a single integer, just representing the genre of music. The input can also be a set of the first few\nnotes. The output Q is a sequence of data. This sequence also has implicit temporal information. A third example is"}, {"title": "5 Different challenges in modelling time sequence", "content": "Continuous-time dataset possesses several unique behaviours, which makes modelling data as well as learning the\nhidden dynamics or pattern extensively challenging. This section elaborates different challenges of continuous-time\ndata processing using neural networks, and highlight the recent trend for solving those challenges."}, {"title": "5.1 Modelling hidden dynamics of dynamic temporal system", "content": "Modelling continuous time series is the fundamental task for each of the continuous-time series problem categories\nmentioned in section 3. Different deep learning models demonstrate a wide range of techniques to overcome challenges"}, {"title": "5.1.1 Irregular data sampling rate and fixed time step", "content": "Existing deep learning models for solving continuous-time data usually use fixed data sampling rate, but in real-time\ntime rate is mostly irregular. For example, event stream data in the automobile industry, patient record in healthcare,\nweather data in climate applications, all these real-time data demonstrate irregular sampling rate. So far, RNN based\nmodel pioneer among all state-of-art neural network models for modelling irregularly sampled data by considering the\ncontinuous-time data as a sequence of discrete fixed-step data which often suffer from loss inaccuracy."}, {"title": "5.1.2 Informative missingness", "content": "One of the serious outcomes of the irregular sampling rate is informative missingness [28]. Informative missingness is a\nmajor challenge in terms of processing time series. Problems in the prediction category, suffer missingness challenge the\nmost. Missing values in the time series and their missing patterns are often correlated. Understanding this correlation\ncan lead to better prediction result. Although due to some useful attributes in the design, RNN is well equipped to\ncapture long-term temporal dependencies and variable-length observations. There are some relevant models based on\nRNN, as shown in table 7. Among these works, [28] impressively improve RNN structures to incorporate the patterns\nof missingness for time series classification problems. Besides, the sparse and asynchronous nature of the data sampling\nrate causes missing observation. For each missing observation, the modelling of time series gets interrupted and can\nnever recover again. The primary way to battle this irregular data sampling rate is to impute the missing data to provide\nvalue for missing observations similar as proposed in GRU-D[28] model. Another efficient mechanism, usually used by\nRNN based models, is to let the model know when there is data available and take action accordingly, as demonstrated\nin Phased-LSTM [4] model. Over the last decades, several research works motivate to fix the informative missingness\ndue to missing observation in the time series. Here are some conventional approaches to solve this problem as follows:\n1. To ignore the missing data point and to perform the analysis only on the observed data. The limitation of\nthis approach is that if the missing rate is high, and the sampling rate is too few, the performance decreases\nsignificantly.\n2. To substitute in the missing values using data imputation. Data imputation mechanism [91, 92, 28] are widely\npopular for solving missing data. However, data imputation does not always capture variable correlations,\ncomplex pattern and other important attributes. GRU-D [28] is based on the idea that. This model combines\ntwo different representation of missing patterns, such as masking and time interval in the deep learning\narchitecture to detect the long-term pathological time dependencies in the time series and also uses the missing\npattern to achieve better prediction result.\n3. To Leverage ordinary as well as neural network based on partial differential equation helps to learn the change\nin data over time and use the instant derivative of the state of any dynamical system over time to replace the\nmissing observation state value."}, {"title": "5.1.3 Temporal and Spatial Coherence", "content": "Real-world continuous-time data dynamically evolve continuously with noise as well as statistical properties, both\ntemporal and spatial coherence influence the efficiency of the neural network used for time series modelling. Temporal\ncoherence refers to the correlation between $z(t_1)$ and $z(t_2)$, z is a vector representing any dynamical system, and\n$z(t_1)$ and $z(t_2)$ are the state value of z in two different data points $t_1$ and $t_2$. Different real-world applications [95]\nare tightly dependent on temporal coherence. The recent trend of data-driven neural network [90, 96] supports the\nimportance of temporal coherence in most time series problems. For some domain-specific continuous-time data,\nsuch as prediction of wind, energy consumption, climate phenomenon [97, 98], exhibits spatial coherent pattern in\ncorresponding multivariate time series. Wavelets analysis [99, 100, 36] is one of the most popular mechanisms to\nlearn transient coherent patterns in time series. Dynamic spatial correlation for multivariate time series [57] combines\nwavelet analysis along with non-stationary Multifractal surrogate-data generation algorithm to detect short-term spatial\ncoherence in multivariate time series. The surrogated data are generated by the stochastic process, with the amplitude\nand time-frequency distributions of original data being preserved. An LSTM based neural network [9] demonstrate the\nusage of spatial coherence in neural network modelling for time series with Spatial Coherence."}, {"title": "5.1.4 High Dimensionality", "content": "Real-world time series contains noise components which may add additional complexity to the modelling and processing\nof time series [77, 57]. Besides, multivariate time-series data samples exhibit high dimensionality. For time-series data\nprocessing, it is essential to optimise the noise as well as Dimension. There are several mechanisms to overcome high\ndimensionality problem for multivariate time series, such as restricted Boltzmann Machine [26], feature extraction,\nwavelet analysis, filtering feature and others. One of the solutions is to use Wavelet analysis which removes some\nportion of noise and reduces the dimensionality. Different kind of optimisation functions can be used to optimise\nthe weight matrix. Peng et al. [56] have transformed the weight matrix optimisation problem into a Lagrangian dual\nproblem to overcome the high dimensionality. Another popular solution is feature extraction. In feature extraction,\nneural network models only focus on a set of features from the long time series for computation. However, this can\nimpose a negative effect on the time series process as an essential or relevant feature can be excluded, and the result can\nbe misleading. On the other hand, if the number of selected feature is large, the associated computation, time, memory\nand energy would be extensive as well, and that would influence the performance negatively."}, {"title": "5.1.5 Over-fitting", "content": "For any deep learning model, it is crucial to provide a mechanism for avoiding over-fitting. In the case of modelling a\ncontinuous-time data, it is mandatory and challenging. Some common approach for avoiding over-fitting are listed as\nfollows\n1. Bach normalization [101]\n2. Dropout [29]\n3. Optimizing weight matrix [56]\n4. Using un-tuned weight matrix\n5. Reducing the Dimension of the representation of the input vector by shifting a row or column\n6. Using Gaussian or similar process for feature extraction\n7. Using comparatively less memory size so the model cannot memorise the input sequence\n8. ANN-based model uses early stopping procedure to avoid the over-fitting problem. In the case of early stopping\na predetermined number of step controls an automated stopping procedure.\n9. Using a broad set of training and test datasets can reduce the chances of over-fitting."}, {"title": "5.1.6 Length of Sequence", "content": "Recently, several works, as shown in Table 8, argue that the long time series is more efficient for learning the continuous-\ntime data. Usually, a long time series has shorter time steps which improve the accuracy of the result, but it still needs\nto overcome the higher sampling rate complexity. At the same time, the longer the sequence is, the more complex the\nneural network needs to be.\nA minimal number of works demonstrate efficient processing of lengthy continuous-time series. The usual approach is\nto normalise the long continuous-time data and train it in multiple batches with a subset of data for each batch. The\nWavenet [103] is one of the most popular works on long sequence time series classification. Dilated convolutional\nneural networks are often used to overcome the sequence length problem. Another new addition to this solution is\nphased-LSTM, a number of variations [6] of Phased-LSTM [4] have been proposed since 2016 to combat the larger\nsequence length problem."}, {"title": "5.1.7 Memory size", "content": "Memory size is another problem that may arise due to the sequence length of the time series. For memory related\nproblem, RNN is more popular than ANN and CNN due to its ability to recover memory from long past events. However,\nrecently Temporal Convolution Networks (TCNs) has achieved better performance in terms of maintaining a more\nextended history than RNN."}, {"title": "5.2 Pathological long-term dependencies", "content": "Another sequence-related limitation of deep learning architecture for time series problem is pathological long-term\ndependencies [104]. Some of the data points in a long time series dataset, as shown in 1, can be related while some\nothers are entirely independent. An excellent example described in [5], where N consecutive visits of a single patient\ncan be related to the same physical problem where the range between two consecutive visits is comparatively short,\nwhile N consecutive visits of a single patient can be for entirely independent of each other, but in this case, the range\nbetween two consecutive visits is relatively broad. Usually, problems in long-term time series prediction problems have\na long sequence of inputs. The future prediction for such input series can be only dependent on a few time-stamps at the\nend of the input series, and there is a long irrelevant part of random input in the sequence. Healthcare shows this kind of\nproblems more often than in other sectors. This kind of pathological long -term dependency makes the problem even\nharder to solve. The challenges increases relative with the length of the sequence (L) as longer sequences exhibit a\nbroader range of dependencies."}, {"title": "5.2.1 Influential feature selection", "content": "Time series usually have a massive number of parameters. In order to model the time series, some of the parameters are\nmore significant than others. Also, domain-specific feature strongly influences the accuracy of the result generated\nby different neural network model. As a result, different models demonstrate different performance for similar tasks\nwith the same dataset and domain. Selection of the correct feature for each task is critical for time series modelling,\nespecially in case of the time series classification problem. However, it is not feasible to design a domain-specific neural\nnetwork for different tasks. Therefore, it is essential to distinguish the essential parameters and removes irrelevant\nparameters to reduce noise. If the length of the time series is L and M is the number of features to describe any problem,\nthe scope of M features is M\u00d7L. For this reason, influential feature selection is crucial. Many methodologies are\navailable for feature extraction from different static domain-specific data, for example, static images and computer\nvision. However, extracting features from continuous-time data still an open problem. One of the solution to improve\nthe efficiency of a neural network model in terms of feature selection is that the design of the models needs to be\ndata-driven. Several data-driven neural networks, as shown in Table 9 demonstrate promising result for feature selection\nand optimising the performance of neural network models."}, {"title": "5.3 Challenges faced by different Applications", "content": "There are several applications of sequential data modelling, for example, speech recognition, bioinformatics and human\nactivity recognition. Different real-world application data suffers from different challenges mentioned in this section.\nTable 11 shows how different characteristics of continuous-time data affect different applications.\nAs shown in the table 11, healthcare data significantly suffers from informative missingness, sampling irregularity and\nPathological dependencies. Table [12] depicts the current trends of using continuous time series as the data model for\nhealth care problems. As a result, most research in the Healthcare industry uses RNN based neural network. Due to\nprivacy, Healthcare industry suffers from a lack of real clinical data. Some researches use GAN based neural network to\ngenerate sample clinical data for further research.\nContinous-time data is the primary block for the Internet of Things(IoT). The work presented [4] has significantly\ncontributed to opening new areas of investigation for processing asynchronous sensory events that carry timing\ninformation. This work has been extended and used in several applications [109, 110, 76, 10]. Some core characteristics\nof sensor-based data are explained by[4], such as Irregular sampling rate, Higher sampling frequency and High\nDimension. High Dimension is an unavoidable outcome of multivariate time series. Most sensor-generated data for\nweather, climate, automobile and other applications are generally multivariate. For example, sensor data from wearable\nconsists of multiple channels of data, such as, step-count in smart mobile devices use the accelerometer and optical\nheart rate sensor. This sensor collects data of heart rate and step count. There are several use cases where data is\ncoming from multiple channels. For a more specific example, the heart rate monitor in smart mobile devices. In recent\ndays, most of the research fields deal with a temporal sequence where the input channel is more than one. In addition,"}, {"title": "6 Different Neural Networks Models For Time Series Processing", "content": "In this section, we present different neural network architectures, which are generally used to learn a continuous time\nseries. The different neural network models have their strengths and weaknesses in terms of modelling continuous time\nseries. Fig. 7 shows the primary types of neural network for modelling continuous-time series over decades."}, {"title": "6.1 Artificial Neural Network (ANN)", "content": "Wind speed prediction, energy prediction, financial time series forecasting, and so many other continuous series\nprediction problems have been solved using the Artificial neural network (ANN). In early 2000, along with the CNN\nand the RNN based solution, ANN is a widely popular neural network for modelling non-linear time series. ANN does\nnot need any previous assumption or linearity. The ANN successfully provides a mechanism to determine the number\nof neurons to be fired in the hidden layer. Some other characteristic features of ANN are listed as below"}, {"title": "6.1.1", "content": "\\bullet ANN can derive its computing power through massively parallel distributed structure and learn the correspond-\ning model.\n\\bullet The architecture of the model is straightforward. It consists of units. These units are connected using the\nsymmetric weighted connection. This connection can be either one-directional or bi-directional. The weight of\nthe connection can be inhibitory or excitatory.\n\\bullet Usually a sigmoid function is used as the activation function, which is also known as the squashing function.\nBesides other activation function such as Linear, Atanh, Logistic, Exponential and Sinus are used as the\nactivation function in the hidden as well as the output layer.\n\\bullet As the ANN is simple in terms of design and system structure, it is possible to use different artificial\nintelligence algorithms, such as particle swarm optimisation algorithm (PSO) for learning as well as adjusting\nthe parameters. Due to the simplicity of the underlying design of ANN, most ANN-based models are hybrid,\nwhere other algorithms are integrated to improve the performance of ANN. Some of the most popular methods\nused with ANN for time series prediction, classification and detection problems are as follows:\nHidden Markov model (HMM)\nGenetic Algorithms (GA)\nGeneralised regression neural networks model (GRNN)\nFuzzy regression models\nSimulated Annealing algorithm (SA)\nEcho State Network (ESN)\nParticles Swarm Optimisation algorithm (PSO). [112]\nElman Recurrent Neural Networks (ERNN) [61]\nHydrodynamic Neural Network\nBack Propagation Neural Network (BPNN)\nRecurrent Multiplicative Neuron Model (RMNM)[113]\nWavelet Neural network (WNN) [79]\nHilbert-Huang transform (HHT) [21]\nMultilayer Perceptron Networks (MLP)\nSupport Vector Machine (SVM) [81, 80]\n\\bullet Mean square error (MSE) and mean absolute percentage error (MAPE) is the loss functions used in ANN-based\nmodels.\n\\bullet Artificial neural networks (ANNs) is very suitable for time series prediction, univariate time series forecasting,\nfinancial trend detection, wind speed and water fluctuation detection and other similar continuous-time\nproblems. This neural network is also beneficial for pattern classification and pattern recognition [37].\n\\bullet ANN models are mostly data-driven without any initial assumption.\n\\bullet ANN models usually perform better for time series prediction because of its ability to model any continuous\nfunctional relationship between input and output.\n\\bullet ANN models can capture the underlying non-linearity of the system with highly non-linear dynamics by using\na non-linear activation function.\n\\bullet ANN model can adapt conditional training quickly. Therefore, the conditional time series forecasting is one of\nthe most used areas for ANN."}, {"title": "6.1.2 Recent ANN models to overcome different challenges", "content": "Recurrent Multiplicative Neuron Model(RMNM) proposed by [113] is a recent work deal the lagged variable of error\nas input along with its recurrent structure in the case of learning time series. It overcomes the traditional challenge of\nANN of deciding the number of neuron in the hidden layer. Fig 8 shows the RMNM model architecture."}, {"title": "6.2 Recurrent Neural Network(RNN)", "content": "In the field of processing continuous sequence, the recurrent neural network is the pioneer. As repetitive cell connected\nwith a lateral connection creates a more extensive neural network in an RNN which process data sequentially, which is\nprecisely what a temporal data sequence requires. Therefore, it is very efficient to model sequence structure. Even\nthough RNN has demonstrated significant satisfactory result for modelling variable-length time sequence, as mentioned\nby [8"}]}