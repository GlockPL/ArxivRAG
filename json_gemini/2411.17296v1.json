{"title": "GrokFormer: Graph Fourier Kolmogorov-Arnold Transformers", "authors": ["Guoguo Ai", "Guansong Pang", "Hezhe Qiao", "Yuan Gao", "Hui Yan"], "abstract": "Graph Transformers (GTs) have demonstrated remarkable performance in incorporating various graph structure information, e.g., long-range structural dependency, into graph representation learning. However, self-attention \u2013 the core module of GTs - preserves only low-frequency signals on graph features, retaining only homophilic patterns that capture similar features among the connected nodes. Consequently, it has insufficient capacity in modeling complex node label patterns, such as the opposite of homophilic patterns \u2013 heterophilic patterns. Some improved GTs deal with the problem by learning polynomial filters or performing self-attention over the first-order graph spectrum. However, these GTs either ignore rich information contained in the whole spectrum or neglect higher-order spectrum information, resulting in limited flexibility and frequency response in their spectral filters. To tackle these challenges, we propose a novel GT network, namely Graph Fourier Kolmogorov-Arnold Transformers (GrokFormer), to go beyond the self-attention in GTs. GrokFormer leverages learnable activation functions in order-K graph spectrum through Fourier series modeling to i) learn eigenvalue-targeted filter functions producing learnable base that can capture a broad range of frequency signals flexibly, and ii) extract first- and higher-order graph spectral information adaptively. In doing so, GrokFormer can effectively capture intricate patterns hidden across different orders and levels of frequency signals, learning expressive, order-and-frequency-adaptive graph representations. Comprehensive experiments conducted on 10 node classification datasets across various domains, scales, and levels of graph heterophily, as well as 5 graph classification datasets, demonstrate that GrokFormer outperforms state-of-the-art GTs and other advanced graph neural networks. Our code is available at https://github.com/GGA23/GrokFormer.", "sections": [{"title": "I. INTRODUCTION", "content": "Transformers [1] have proved to be successful in numerous domains, such as natural language understanding [1] and computer vision [2]. One key advantage of transformers is that a single self-attention layer in the Transformer networks can capture interactions between any pair of input instances, e.g., graph nodes, enabling the direct modeling of long-range relationships. This capability has led researchers to explore ways to adapt the Transformer architecture for graph representation learning that captures long-range dependencies."}, {"title": "II. RELATED WORK", "content": "Graph Neural Networks. Existing GNNs are mainly divided into two main streams: spatial-based and spectral-based methods. Spatial-based GNNs, like GCN [25] and GAT [26], update node representations by aggregating information from first-order neighbors. By stacking multiple layers, they may learn long-range dependencies but suffer from over-smoothing [27] and over-squashing [28]. Besides, the smooth nature of their aggregation mechanism prevents them from being successfully applied to heterophilic scenarios. To this end, some spatial-based methods, such as H2GCN [29] and HopGNN [30], propose to combine first-order and higher-order neighborhood representations. Other studies [24], [31] point out from a spectral perspective that GCN only considers the first-order Chebyshev polynomial, which acts as a low-pass filter. As a result, various spectral-based GNNs have been proposed to leverage different types of polynomials to approximate arbitrary filters. For instance, GPR-GNN [18] employs the Monomial basis for graph filter approximation. ChebyNet [20] proposes a fast localized convolutional filter based on Chebyshev polynomial. APPNP [32] utilizes Personalized PageRank (PPR) to achieve a low-pass filter. BernNet [17] learns arbitrary graph spectral filters by an order-K Bernstein polynomial approximation. JacobiConv [21] approximates filter functions with Jacobi basis. HiGCN [33] incorporates Flower-Petals Laplacians into simplicial complexes, capable of learning graph filters across varying topological scales. However, these pre-defined linear polynomial filters usually have limited learning ability.\nGraph Transformers. Compared to GNNs, the attention weights learned by Transformers can be seen as a weighted adjacency matrix of a fully connected graph, which is advantageous to learn long-range dependency of the graph nodes [34]. However, the primary transformers lack structural topology information. Therefore, graph transformers (GTs) are proposed by incorporating graph structural information into the Transformer architecture via various ways. For example, GT [4] leverages eigenvectors of graph Laplacian matrix as part of the input features to encode the local structure information. SAN [5] leverages the full spectrum of the Laplacian matrix to learn the positional encodings. GraphTrans [7], GPS [35], SAT [10] and SGFormer [12] combine GNN and Transformers with GNN model used as the structure extractor to capture local structural information. Graphormer [9], NodeFormer [36], and EGT [37] transform various graph structure features into attention biases, allowing the Transformer to capture graph structural information. However, all these GTs rely on the attention matrix to learn node representations, which has been proven as a smoothing operation on all nodes, capturing only the low-frequency information of graph features [14]. This is insufficient to extract complex label distributions resulting from a mixture of low- and high-frequency information [38].\nRecently, advanced GTs have increasingly focused on capturing various frequency signals to enhance the performance [14], [22], [39]. SignGT [39] designs a new signed self-attention mechanism to capture low-frequency and high-frequency information. FeTA [14] learns polynomial filters using the self-attention weights. PolyFormer [22] performs self-attention on polynomial tokens to protect the rich frequency information. Specformer [13] parameterizes the spectral filters by performing self-attention on N eigenvalue tokens of the first-order graph Laplacian. Although these improved GTs capture different frequency signals on graph features, they still struggle to derive the desired frequency response hidden at varying order graph spectrum."}, {"title": "III. PRELIMINARIES", "content": "In this section, we first briefly introduce notations and the problem definition, then introduce the graph signal filter in the spectral domain. Finally, we introduce the key module self-attention of Transformer architecture and Kolmogorov-Arnold Network."}, {"title": "A. Notations and the Problem", "content": "An undirected, unweighted attributed graph is represented as G = (V,E,X), where V denotes the node set with $v_i \\in V$ and $|V| = N$, E denotes the edge set, and $X = [x_1, x_2,..., x_N] \\in \\mathbb{R}^{N \\times F}$ is a set of node attributes (features). Each node $v_i$ has a F-dimensional feature representation $x_i$. The topological structure of G is represented by an adjacency matrix A = $[a_{ij}] \\in \\mathbb{R}^{N \\times N}$, $A_{ij} = a_{ji} = 1$ if $(v_i, v_j) \\in E$, or 0 otherwise. D$\\in \\mathbb{R}^{N \\times N}$ denotes a degree matrix which is a diagonal matrix with $d_{ii} = \\sum_j a_{ij}$. Normalized Laplacian matrix L is defined by $L = I_N \u2013 D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$, where $I_N \\in \\mathbb{R}^{N \\times N}$ denotes an identity matrix.\nThe homophily ratio H as a measure of the graph ho-mophily level is used to define graphs with strong ho-mophily/heterophily. The mathematical expression for the homophily ratio is $H = \\frac{|\\{(v_i, v_j) : (v_i, v_j) \\in E \\wedge y_i = y_j\\}|}{|E|}$ [29], which is the fraction of edges in a graph which connect nodes that have the same class label. Homophily ratio $H \\rightarrow 1$ represents the graph exhibit strong homophily, while the graph with strong heterophily (or low/weak homophily) have small homophily ratio $H \\rightarrow 0$.\nEach node $v_i$ contains a unique class label $y_i$. The goal of semi-supervised node classification is to learn a mapping $M_n : V \\rightarrow Y$, where Y is the set of labels, given a set of labeled node $V_L = \\{(v_1, y_1), (v_2, y_2), \u00b7\u00b7\u00b7, (v_t, y_t)\\}$ as training data. Similarly, the goal of the graph classification is to find a mapping $M_g : G \\rightarrow Y$, given a set of graphs $G = \\{G_1,\u2026\u2026,G_t\\}$ and a set of labels $Y = \\{y_1,\u2026, Y_t\\}$ as training data."}, {"title": "B. Graph Signal Filter", "content": "Suppose that Laplacian matrix L is the graph shift op-erator that respects the graph topology. The eigendecompo-sition of Laplacian matrix is $L = U \\Lambda U^T$, where $U = (u_1, u_2, ..., u_N)$ is a complete set of orthonormal eigenvectors known as graph Fourier modes and $\\Lambda = diag (\\{\\lambda_i\\}_{i=1}^N)$ is a diagonal matrix of the eigenvalues of L. The $i^{th}$ column of U satisfies $Lu_i = \\lambda_i u_i$."}, {"title": "C. Self-Attention", "content": "The multi-head self-attention is the critical module of Trans-formers and has garnered substantial attention due to their capacity for point-to-point feature interactions within item sequences. Let $X \\in \\mathbb{R}^{N \\times F}$ denote the input of the self-attention, for simplicity of illustration, we consider the single-head self-attention for description. It first projects X into three subspaces Q, K, V through three projection matrices $W_Q W_K, W_V$:\n$Q = XW_Q, K = XW_K, V = XW_V$.\nThe self-attention is then calculated as:\n$Attetion(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$.\n(2)\nThe attention weights capture the pair-wise similarity of input items in the sequences. This mechanism effectively addresses the challenge of global dependencies and enables the incorporation of longer sequences enriched with a wealth of information."}, {"title": "D. Kolmogorov-Arnold Network", "content": "The KAN offers a promising alternative to Multi-Layer Per-ceptron (MLP). While MLP is based on the universal approx-imation theorem [42], KAN is grounded in the Kolmogorov-Arnold representation theorem [43]. The fundamental idea of KAN is to create an arbitrary function at each hidden neuron by overlaying multiple nonlinear functions onto the input features. It can be expressed in the following formula:\n$KAN = f(x) = \\sum_{q=1}^{2n+1} \\Phi_q (\\sum_{p=1}^{n} \\phi_{q,p}(x_p))$,\n(4)\nwhere $\\phi_{q,p}$ is trainable activation function, and $\\Phi_q: [0,1] \\rightarrow \\mathbb{R}$ and $\\phi_{q,p}: \\mathbb{R} \\rightarrow \\mathbb{R}$ are univariate functions that map each input variable $x_p$. In the Kolmogov-Arnold theorem, the inner functions form a KAN layer with $n_{in} = n$ and $n_{out} = 2n+1$, and the outer functions form a KAN layer with $n_{in} = 2n +1$ and $n_{out} = n$. Thus, the Kolmogorov-Arnold representations in this formula are simply compositions of two KAN layers. A useful trick is that it includes a basis function b(x) such that the activation function $\\phi(x)$ is the sum of the basis function b(x) and the spline function:\n$\\phi(x) = w_1b(x) + w_s spline(x), b(x) = silu(x) = \\frac{x}{1 + e^{-x}}$,\n(5)\nwhere spline(x) is parametrized as a linear combination of B-splines in most cases. It can be expressed as follows:\nspline(x) = \\sum_i c_i B_i(x),\n(6)\nwhere $c_i$ is trainable."}, {"title": "IV. METHODOLOGY", "content": "A. Overview of the Proposed GrokFormer\nOur new approach falls into the framework of Transformer with a Graph Fourier Kolmogorov-Arnold Network being added as a new spectral graph convolutional module, as shown in Fig. 2, so we named it as Graph Fourier Kolmogorov-Arnold Transformers (GrokFormer). The central piece of the GrokFormer is a order-and-frequency-adaptive convolution module designed to extract diverse frequency information, going beyond the self-attention mechanism, to model varying levels of graph homophily. Specifically, we first leverage learnable activation functions modeled as Fourier series to learn eigenvalue-targeted filter function from K order graph spectrum. After that, K spectral filters from the first order to the K-th order are adaptively fused to produces the learnable filter base for conducting graph convolution. Finally, the information from the self-attention mechanism and spectral graph convolution is merged to output the final representation for prediction."}, {"title": "B. Order-and-frequency-adaptive Graph Filter", "content": "To capture various frequency details in a flexible and fine-grained manner from the K order graph spectrum, we design a novel spectral graph convolution module. Motivated by the Kolmogorov-Arnold representation theorem [43] and Kolmogorov-Arnold Network (KAN) that involves a network design where traditional weight parameters are substituted with learnable activation functions, we develop Graph Fourier KAN, which utilizes learnable activation functions modeled as the Fourier series to learn eigenvalue-target frequency adaptive filter function. This approach allows for adaptively capturing global and a broad range of frequency components. Specifically, we leverage Fourier series representation [44] to parameterize each learnable function since the spline functions in KAN are piecewise and difficult to train. The specific filter learning is can be defined as follows:\n$\\Phi_n(\\lambda) = \\sum_{k=1}^{K} \\sum_{m=0}^{M} (cos (m k) \\cdot a_{km} + sin (m k) \\cdot b_{km})$,\n(7)\nwhere k is the order. $a_{km}$ and $b_{km}$ are trainable Fourier coefficients. The hyperparameter m represents the number of frequency components (or grid size) and can be fine-tuned. The spectral properties of the Fourier series, which represent different frequency components, enable the model to control and achieve a broad range of frequency responses. Furthermore, trainable Fourier coefficients empower the model to adaptively capture the various frequency information.\nThe benefits of $\\phi_n(\\lambda)$ are five-fold: (i) efficiency. Compared to spline functions or performing self-attention over N eigenvalues, which has a complexity of $O(N^2)$, Fourier series representation has a lower training complexity, with a linear complexity of O(N). (ii) Effectiveness. Sine and cosine in the Fourier series are orthogonality. Orthogonal basis have been shown to enable more effective learning of graph filters [21]. (iii) Informative. Many sine and cosine terms with different frequency components can capture rich frequency responses in different graph spectral spaces. (iv) Convergence guarantee. In approximation theory, it is well established that the Fourier series attains the best convergence rate for function approximation. (v) Globality. The filter function pay attention to the whole spectrum (all eigenvalues), capturing the global information [13].\nPrevious work focuses on learning graph filters solely from the eigenvalues derived from the first-order graph Laplacian, which inevitably overlooks higher-order spectral information. To address this limitation, we consider the K-order graph spectrum in this work. Furthermore, to adaptively capture diverse patterns across different orders, we first learn filtering functions for each order separately. Thus, we can rewrite Eq. (7) as follows:\n$h_k(\\lambda) = \\sum_{m=0}^{M} (cos (m \\lambda) \\cdot a_{m} + sin (m \\lambda) \\cdot b_{m})$,\n(8)\nwhere $k = [1,2,..., K]$. Furthermore, to achieve order-adaptive filter function, we learn a free parameter a to adaptively fusing varying order's filter as follows,\n$h(\\lambda) = \\sum_{k} \\alpha_k h_k(\\lambda)$.\n(9)\nTherefore, the corresponding spectral graph convolution in GrokFormer is as follows,\n$X_F = U diag(h(\\lambda)) U^T X^{(l-1)}$,\n(10)\nwhere diag(\u00b7) is a function that creates a diagonal matrix. $X^{(0)} = f_o(X)$, and $f_l$ is a two-layer MLP (embedding layer). $U diag(h(\\lambda)) U^T$ is the filter base, similar to the purpose of those polynomial bases in the literature. In doing so, GrokFormer achieves eigenvalue-target order-and-frequency-adaptive graph filter capturing desired frequency response tailored to varying network properties."}, {"title": "C. Network Architecture of GrokFormer", "content": "GrokFormer is built upon the original implementation of a classic Transformer encoder. Specifically, we first apply layer normalization (LN) on the representations before feeding them into other sub-layers, i.e., the multi-head self-attention (MHA) and the feed-forward blocks (FFN). The LN trick has been used widely to improve the optimization of Transformer [9]. Then, we combine the information from self-attention block and the Graph Fourier KAN through summation to generate informative node representations. We formally characterize the GrokFormer layer as follows:\n$X'^{(l)} = MHA(LN(X^{(l-1)})) + X^{(l-1)} + X_F$,\n(11)\n$X^{(l)} = FFN(LN(X'^{(l)})) + X'^{(l)}$.\nIn the final layer of GrokFormer, we calculate the prediction scores of the nodes from class c. This score is given by:\n$\\hat{Y} = softmax(X^{(L)})$,(12)\nwhere $X^{(L)}$ is the output of the final layer, and $\\hat{Y}$ is the predicted class label.\nThen, GrokFormer can be trained by minimizing the cross entropy between the predicted and the (partial) ground-truth labels:\n$L_{ce} = - \\sum_{i\\in V_L} \\sum_{c=1}^{C} y_{ic} \\log y_{ic}$,\n(13)\nwhere C is the number of classes, $y$ is the real labels, and $V_L$ is the training set."}, {"title": "D. Complexity and Scalability Analysis", "content": "Complexity. Firstly, like some previous methods, such as NAGphormer [45], Specformer [13] and Sp\u00b2GCL [46], GrokFormer also needs spectral decomposition, which is preprocessing and has the complexity of O(N\u00b3). Secondly, GrokFormer has another part of the computation, i.e., the forward process. The forward complexity of GrokFormer includes efficient self-attention, filter base learning, and graph convolution. Their corresponding complexities are O(Nd\u00b2), O(KNM), and $O(N^2d)$, respectively, where d denotes the hidden dimension, N represents the number of nodes, M is the number of terms used in the Fourier series expansion, and K is the order. The overall forward complexity is O(Nd(N + d) + KNM).\nScalability. In the context of large graphs, the complex-ity of decomposition is unacceptable. Therefore, we employ Sparse Generalized Eigenvalue (SGE) algorithms, as outlined in earlier studies [13], [47], to compute q eigenvalues and corresponding eigenvectors, which reduces the complexity to $O(N^2q)$ [46]. Accordingly, the complexity of graph convolution reduce to $O(q^2d)$, thus the forward complexity reduce to $O(Nd^2+KNM+q^2d)$, $q \\ll N$, which is linear in the number of nodes. Note that the decomposition is only computed once, thus the overhead of the preprocessing should be amortized by each training epoch (see Sec. V-A for empirical results)."}, {"title": "E. Remarks on the Advantages of GrokFormer", "content": "Comparison to Spectral GNNs. Most spectral GNNs design linear polynomial spectral filters, as shown in Table I. In contrast, our GrokFormer has the following advantages: (1) glob-ality. On the one hand, the transformer architecture enables our model to capture long-range dependencies on graphs. On the other hand, the GrokFormer fits all N eigenvectors capturing the global information. However, fixed-order polynomial filters are local. (2) Flexibility. The graph spectral convolution module in our GrokFormer learns spectrum frequency and order adaptive filter functions, whereas spectral GNNs leverage truncated and linear polynomial bases to approximate filters, limiting their expressiveness and flexibility. (3) Universality. The polynomial spectral GNNs can be used as a filter learning module in the GrokFormer.\nComparison to Graph Transformers. Most GTs can only capture low-frequency information due to the low-pass prop-erty of self-attention [39]. Therefore, they do not show com-petitiveness in node-level tasks when faced with complex node label distributions. Although some improved GTs have been proposed to capture different frequency information, they still have certain weaknesses. For instance, SignGT [39] designed a signed attention mechanism but can only capture low-frequency and high-frequency information. FeTA [14] and PolyFormer [22] aim to capture rich frequency information through learning polynomial spectral filtering, but they share the same drawbacks as polynomial spectral GNNs. Specformer effectively encodes all eigenvalues through self-attention but is restricted to the first-order graph spectrum. Additionally, its self-attention-based filter learning introduces a computational complexity of $O(N^2)$ and numerous free parameters, making it both inefficient and susceptible to overfitting. In contrast, our GrokFormer learns eigenvalue-target functions by Fourier series expansion, which offering higher computational efficiency than self-attention, and flexibly capture Korder rich frequency signals, thus performing well on a range of graph tasks. Besides, Specformer and PolyFormer focus only on node representation learning in the spectral domain, while our method captures global graph information from both the spatial and spectral domains, making our approach more general."}, {"title": "V. EXPERIMENTS", "content": "In this section, we conduct comprehensive experiments on a wide range of real-world graph datasets in both node classification and graph classification tasks to verify the effectiveness of our GrokFormer. Then, we evaluate the ability of GrokFilter on synthetic and real-world datasets. Finally, we conduct the ablation study, parameter analysis, and complexity comparison experiments.\nFor the implementation, we utilize NetworkX, Pytorch, and Pytorch Geometric for model construction. All experiments are conducted on NVIDIA GeForce RTX 3090 GPUs with 24 GB memory, TITAN Xp GPU machines equipped with 12 GB memory. Following the previous work [25], [26], classification accuracy is used as a metric to evaluate the performance of all models in node and graph classification tasks, which denotes the percentage of nodes/graphs whose labels are correctly classified."}, {"title": "A. Node Classification", "content": "Dataset Description. We conduct node classification experiments on 10 widely used datasets, including five homophilic and five heterophilic datasets. For homophilic datasets, we adopt three citation graphs Cora, Citeseer, Pubmed, an Amazon co-purchase graph Photo [17], and a co-authorship network Physics [48]. For heterophilic graphs, we adopt the Wikipedia graphs Chameleon and Squirrel, the Actor co-occurrence graph [49], webpage graphs Texas from WebKB\u00b9, and Penn94, a large-scale friendship network from the Facebook 100 networks [50]. Following the previous works [13], [17], the node set was split into train/validation/test set with ratio 60%/20%/20%. We summarize the statistics of these datasets in Table II.\nBaselines and Settings. We compare GrokFormer with fifteen competitive baselines, including (i) four spatial-based GNNs: GCN [25], GAT [26], H2GCN [29] and HopGNN [30], (ii) five spectral-based GNNs: ChebyNet [20], GPR-GNN [18], BernNet [17], JacobiConv [21] and HiGCN [33], and (iii) six GTs: vanilla Transformer [1], NodeFormer [36], SGFormer [12], NAGphormer [45], Specformer [13] and Poly-Former(Cheb) [22]. For fairness, we generate 10 random splits and evaluate all models on the same splits, and report the average metric for each model. For polynomial GNNs, we set the order of polynomials K = 10, consistent with their original setting. In addition to the above settings, for the baseline models, we adopt the best hyperparameters provided by the authors if available, and we meticulously fine-tune the parameters to achieve good performance otherwise. If the baseline methods are consistent with our dataset partition and the experimental results we run are close to their paper reported, we directly use the results provided by the authors. We train the proposed GrokFormer model with the learning rate lr \u2208 {0.005, 0.01}, the weight decay wd \u2208 {0.001, 0.005, 0.0001, 0.0005, 0.00005}, dropout \u2208 {0.0,0.1,..., 0.8} with step 0.1, and M\u2208 {16,32}. We set GrokFormer to adaptively cover the spectrum from the 1st up to 6-th order, i.e., $K \\in \\{1,2,...,6\\}$."}, {"title": "B. Graph Classification", "content": "Dataset. We conduct graph classification experiments on five datasets from diverse domains to evaluate the performance of the proposed GrokFormer. The datasets are divided into two main categories: bioinformatics datasets and social network datasets. The bioinformatics datasets include PROTEINS [51], PTC-MR [52], and MUTAG [53], where the graphs denote chemical compounds. Two social network datasets IMDB-BINARY and IMDB-MULTI [54], where the graph originates from a specific movie genre, nodes represent actors/actresses, and edges connect them if they appear in the same movie. The statistics of these datasets are summarized in Table IV.\nBaselines and Settings. We compare our GrokFormer with three prevalent graph classification models, including three kernel-based methods: GK [55], WL kernel [56] and DGK [54], five popular GNN-based models: DGCNN [57], GCN, GIN [58], GDN [59], and HiGCN, as well as five graph transformer models: Transformer, Graphormer [9], SGFormer, NAGphormer, Specformer. We follow the same evaluation pro-tocol of InfoGraph [60] to conduct a 10-fold cross-validation scheme for a fair comparison and report the maximum average validation accuracy across folds. We set the hyper-parameters of the baseline methods suggested by their authors. For the proposed GrokFormer, the hyperparameter space used for experiments is enumerated follows: learning rate lr \u2208 {0.001, 0.005, 0.0005, 0.0001}, the weight decay wd \u2208 {0.0, 0.0005, 0.00005}, the hidden dimension d \u2208 {32, 64}, the batch size is searched in {32, 64, 128}. We implement readout operations by conducting max pooling to obtain a global embedding for each graph.\nResults. The performance of graph classification is pre-sented in Table V. We find that the proposed models out-perform state-of-the-art baselines on 4 out of 5 datasets and achieve 11.9% relative improvement on the PTC-MR, which validates the superiority of our GrokFormer. In addition, compared to the kernel-based model, our approaches achieve a greater improvement, with a maximum improvement of 34.9% on the PTC-MR dataset. In contrast, GNN methods and graph Transformers usually perform better than traditional kernel methods. GrokFormer shows consistent superiority over the strongest baseline, Specformer, achieving an average improve-ment of 5.6% across all datasets, although it records a slightly lower score in the PROTEINS dataset. GrokFormer exhibits remarkable capabilities in graph representation learning by effectively capturing complex and desired frequency signals from the first- to higher-order graph spectrum, surpassing the performance of these baselines."}, {"title": "C. Effectiveness of Graph Fourier KAN Filters", "content": "1) Fitting Signals on Synthetic Datasets. : In this subsec-tion, we assess the effectiveness of the adaptive filter learning module of GrokFormer, i.e., namely Graph Fourier KAN Filter (GrokFilter), using synthetic datasets. This evaluation highlights the improved abilities of GrokFilter in learning filter patterns, without requiring any prior knowledge of predefined filters.\nSynthetic Datasets. Following prior works [13], [17], we conduct experiment using 50 real images with a resolution of 100 \u00d7 100 from the Image Processing in Matlab library\u00b2. Each image can be represented as a 2d regular grid graph with 4-neighborhood connectivity. The pixel values serve as node signals ranging from 0 to 1. The m-th image has an adjacency matrix $A_m \\in \\mathbb{R}^{10000\\times10000}$ and a node signal $x_m \\in \\mathbb{R}^{10000}$. For each image, we apply different predefined filters to the spectral domain of its signal, with each filter detailed in Table VI. Models are expected to learn these predefined filtering patterns.\nSetup. We compare GrokFilter with seven baseline meth-ods, including GCN, GAT, ChebyNet, GPR-GNN, BernNet, Poly Attn(Cheb) [22] and Specformer. To ensure a fair compar-ison, we adjust the number of hidden units to maintain nearly 2K parameters for all models and set the polynomial order K to 10 for ChebyNet, GPR-GNN, and BernNet. In addition, we uniformly set the learning rate to 0.01, and the training epochs to 2,000 with the patience of early stop 100. We assess each method based on two criteria: the sum of squared errors, where lower values are desirable, and the R\u00b2 score, which should be maximized.\nResults. Table VI reports the average of the sum of squared error and the R\u00b2 scores. We can observe that (1) GrokFilter consistently achieved superior performance compared to these baseline models in two metrics. For complex graph filters, such as Comb and Band, our method performs well, proving the effectiveness of GrokFilter in learning complex graph filters. (2) GCN and GAT can only learn low-pass filter well, which is not enough in heterophilic graph learning. (3) Polynomial-based spectral GNNs, ChebyNet, GPR-GNN, and BernNet perform better than GCN by approximating graph filtering using polynomials. However, the expressiveness of their linear polynomials is still limited in learning complex filters. (4) PolyAttn(Cheb) executes non-linear on polynomials obtains better results than linear polynomials, but it fails in capturing the complex filter patterns in Comb. (5) Specformer performs self-attention on eigenvalues possessing a stronger expression ability than polynomial filters, but it is still weaker than our GrokFilter."}, {"title": "2) Performance on Real-world Datasets.", "content": ": We conduct this experiment on nine real-world node classification datasets to evaluate the efficacy of our GrokFilter.\nSetup. The experimental setting is the same as the node classification task, as stated in settings of Section V-A.\nResults. As shown in Fig. 4, we plot the filtering curve h(x) learned by the GrokFilter module on nine different datasets. From Fig. 4, we can observe that (1) on homophilic graphs, GrokFilter learns low-pass filters with different amplitude and frequency responses for them, which is consistent with the homophily property, i.e., the low-frequency information is important in the homophilic scenario. (2) On Chameleon and Squirrel, GrokFilter learns comb-alike filters with complex frequency components. Meanwhile, we find that our GrokFilter outperforms baselines on these two datasets in Table III, this indicates that our method learns the complex frequency responses that are difficult for other methods to capture but are essential for Chameleon and Squirrel. (3) GrokFilter learns an all-pass filter on the Actor dataset protecting its raw features, which is consistent with the fact that its raw features are associated with labels [17]. (4) In heterophilic graph Texas, GrokFilter learns a high-pass filter, which matches the strong heterophily, i.e., the high-frequency information that captures the difference between nodes is important. Compared to the filter learned on Texas, the filter learned on Penn94 retain relatively more low-frequency components due to its higher homophily than Texas. In summary, our GrokFilter can adaptively learn various required filter functions for networks with different properties under the Transformer architecture to help capture the desired frequency response beyond the low-frequency retained by the self-attention mechanism."}, {"title": "D. Ablation Study", "content": "This section conducts an ablation study on ten node classification and five graph classification datasets. As ablation study models, we present two distinct models: i) the first ablation model, designated as *-w/o-GrokFilter, incorporates only the efficient self-attention module of GrokFormer, without the spectral graph convolution module; ii) the second model is the spectral graph convolution module GrokFilter, excluding the self-attention. The mean classification results of the ablation study are reported in Table VII. The following observations can be derived:\n(1) GrokFormer consistently outperforms its variants across all datasets on different graph tasks, demonstrating that the designed GrokFilter in the GrokFormer contributes to cap-turing various frequency information beyond low frequency, and cooperates with self-attention to learn the precise and informative node representations and improving performance. (2) The performance of GrokFilter is generally better than *-w/o-GrokFilter on all datasets, which shows that the frequency adaptive for node/graph representation learning plays an important role. (3) On the node-level task, *-w/o-GrokFilter is difficult to perform well on homophilic datasets due to the loss of important graph structure information, but it performs well on heterophilic networks (Actor and Texas) with a high correlation between feature information and node labels. This finding shows that the self-attention in spatial domain and GrokFilter in spectral domain are both useful. (4) Comparing the node classification results in Tables VII and III, as well as the graph classification results in Tables VII and V, GrokFilter still maintains competitive performance against the baselines offering powerful expression capabilities, and demonstrating that GrokFormer is effective and competitive across a range of graph tasks."}, {"title": "E. Hyperparameter Analysis", "content": "To study the impact of the graph spectral order, we conduct analytical experiments regarding the order K on six node classification datasets with different homophily. We vary the order K from one to six to observe the change of the node classification accuracy. As shown in Fig. 5, the optimal value of K for GrokFormer varies across different datasets because the graph properties of each dataset are different, which shows that it is necessary to learn graph filters from K-order graph information. We can find that GrokFormer achieves the best accuracy on Squirrel and Chameleon when K is greater than three, but on other datasets, optimal performance usually occurs when K is less"}]}