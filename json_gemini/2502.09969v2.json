{"title": "Data Valuation using Neural Networks for Efficient Instruction Fine-Tuning", "authors": ["Ishika Agarwal", "Dilek Hakkani-T\u00fcr"], "abstract": "Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks \u2013 which we refer to as the InfluenceNetwork \u2013 to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for efficient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT.", "sections": [{"title": "Introduction", "content": "The strong instruction-following abilities of large language models (LLMs) can be attributed to instruction fine-tuning (IFT) (Zhang et al., 2024). IFT builds on top of current language modeling capabilities and strengthens the instruction following abilities of models. Recent works have taken data efficient approaches for IFT. The goal is to select a small subset of samples on which to fine-tune a model (Agarwal et al., 2025; Mirzasoleiman et al., 2020; Das and Khetan, 2024; Xia et al., 2024; Renduchintala et al., 2024; Liu et al., 2024c) that emulates the full dataset.\nData efficient pipelines typically consist of two stages: (1) data valuation: designing functions to estimate the influence of data points, and (2) data selection: using influence estimates to choose a balanced set of influential data. Usually, data selection is cheaper than valuation \u2013 for instance, DELIFT (SE)\u00b9 (Agarwal et al., 2025) computes the similarity of sentence embeddings between pairs of data (expensive) for valuation and selects representative data using a submodular function (cheap).\nFormally, influence functions estimate the value of data. For instance, brute force influence functions use leave-one-out (LOO) training to measure impact by omitting each data point and evaluating performance (Scanlon, 1982). More recent influence functions use LLMs to estimate influence."}, {"title": "Related Works", "content": "Wei et al. (2023) hint that different models extract different information from the same data. Hence, effective fine-tuning requires datasets to be specific to each model. Not all data points affect the model equally - models learn more from certain data points than others. Therefore, data valuation methods prune out such low-influence data for efficient fine-tuning (Xia et al., 2024; Agarwal et al., 2025). Current research is divided into model-independent and model-dependent valuation metrics.\nModel-independent methods, such as distance or clustering-based methods (Das and Khetan, 2024; Liu et al., 2024c; Renduchintala et al., 2024) are faster and less computationally expensive. Distance-based methods assign more \"influence\" to data points that are further from each other, optimizing for a diverse subset. Clustering-based methods assign more \"influence\" to data points that are representative (i.e., the centroids of clusters).\nOn the other hand, model-dependent methods \u2013 such as inference-based and gradient-based \u2013 are more resource intensive. Inference-based methods (Liu et al., 2024a; Agarwal et al., 2025) use model inference signals (e.g., token distributions) to evaluate the performance or confidence of models, and valuate data based on how performative/confident they are. Gradient based methods (Xia et al., 2024; Mirzasoleiman et al., 2020; Killamsetty et al., 2021; Koh and Liang, 2020), on the other hand, can assign higher influence to data points with (1) higher magnitudes of gradients, or (2) gradients that match domain-specific data (for domain-specific fine-tuning, for example).\nWhile they are expensive to calculate, when paired with data selection algorithms, model-dependent data valuation metrics can be used to select subsets of data that are specific to a model's capabilities. Model-dependent data valuation metrics help to select data that will maximize a certain objective for each model, rendering fine-tuning more effective."}, {"title": "Data Selection", "content": "Data selection aims to prune redundant and noisy data samples from large datasets to produce a small, information-rich subset (Agarwal et al., 2025; Xia et al., 2024). This subset should be representative of the larger dataset while performing comparably, if not better, than using the full dataset. Data selection methods usually have objectives for selecting data: (1) instruction tuning (Liu et al., 2024a), (2) task-specific fine-tuning (Liu et al., 2024c), (3) continual learning (Agarwal et al., 2025), (4) preference alignment (Liu et al., 2024b), etc. While certain objectives are subsets of others (e.g. (2) is subset of (1)), the data selected for each purpose may not necessarily overlap. For instance, (1) requires data that is representative of a particular dataset, whereas (2) focuses on samples that reflect specific tasks like math reasoning, question answering, or summarization. Similarly, (3)'s samples are specifically chosen to introduce new information to a model without overriding or repeating previously learned information."}, {"title": "Problem Formulation", "content": "Given a model M and fine-tuning data $D_F$, the goal is to select a small subset $S_F \\subset D_F$ that maximizes the performance of M after fine-tuning M on $S_F$. $S_F$ is the optimal subset if it can be used to train a model that is comparable to a model trained on $D_F$. However, more recent works jointly optimize other objectives during subset selection. Examples of objectives include not only representation, but also task-specific refinement and continual learning. For such joint optimization, the subset $S_F$ is aligned with another target domain dataset $D_T$. The choice of $D_T$ can guide the subset selection towards various objectives. For example, if the objective is representation or task-specific refinement, $S_F$ will contain points from $D_F$ that are similar to $D_T$ (Liu et al., 2024c; Xia et al., 2024; Das and Khetan, 2024). Alternatively, if the objective is continual learning, $S_F$ will contain points from $D_F$ that would allow the model M to learn new information that is present in $D_T$ (Agarwal et al., 2025; Tiwari et al., 2022).\nAs mentioned before, computing influence functions can be a very expensive process. There are"}, {"title": "Our motivation", "content": "Overall, our aim is to reduce the total number of forward or back propagations through models with millions and billions of parameters by replacing a large portion with forward propagations through small neural networks with (merely) hundreds of thousands of parameters. Pairwise influence functions calculate the similarity between two data points (denoted as sim(i, j)). Because influence values are usually not learned, they need to be re-computed for any data beyond the training data. In other words, as data is constantly being collected, influence values for new data must be recomputed. However, NN-CIFT is learned. Hence, our method does not require any extra computation to estimate influence values, unlike previous work."}, {"title": "Learning Influence Estimation", "content": "This section describes in detail Steps 1 and 2 in Figure 1. It outlines the structure and initial experimentation of the InfluenceNetwork."}, {"title": "Defining the InfluenceNetwork.", "content": "For estimating the influence values of data samples, we call our neural network the InfluenceNetwork. It is a 2-layer neural network with a hidden size of 100 neurons, and an output size of 1 neuron. For activation, we use ReLU in between the layers. The function $I_{\\Theta}$ represents the neural network with parameters $\\Theta$. As input, $I_{\\Theta}$ takes two data points i and j and outputs the estimated influence of i on j. Specifically, embeddings for i and j are computed (denoted as emb() below) using the BAAI General Embedding model (bge-large-en-v1.5, in particular) (Xiao et al., 2023) and are concatenated:\n$0 \\leq I_{\\Theta}(i, j) \\leq 1$,\n$I_{\\Theta}(concat(emb(i), emb(j))) \\leq 1$,\n$\\forall (i, j) \\in D_F \\times D_T$\nThe bge-large-en-v1.5 model generates embeddings of size 1,024, which means the input has a total length of 2,048. Hence, the InfluenceNetwork has exactly 204,900 parameters. For training, we use 20 epochs and a learning rate $\\eta = 0.0001$."}, {"title": "Training the InfluenceNetwork.", "content": "Below is an illustration of the quadratic similarity matrix that is computed during the data valuation stages. Previous influence compute the entire matrix for data valuation \u2013 we only use Q1."}, {"title": "Evaluating the InfluenceNetwork.", "content": "To ensure our InfluenceNetwork is able to output influence values correctly, we compute the average mean squared error (MSE) between the ground truth influence values (from Appendix B) and the predicted influence values:\n$\\frac{1}{|D_F \\times D_T|} \\sum_{(i,j) \\in D_F \\times D_T} (IF(i, j) \u2013 sim(i, j))^2$\nWe separate the evaluation between the four quadrants of data to study the performance with ID and OOD data.\nTo train the InfluenceNetwork, we use DELIFT's influence values on the MixInstruct dataset (Jiang et al., 2023) to train our InfluenceNetwork (more dataset details in Section 5). We report the results from InfluenceNetwork and two other baselines: (1) Randomly generating a number between 0 and 1, and (2) only Predicting 0 influence."}, {"title": "Hyperparameter Study #1: InfluenceNetwork sizes", "content": "We vary the number of layers and dimensions of each layer. For simplicity, we plot the number of parameters in the InfluenceNetwork versus the MSE. The results can be found in Figure 3. This figure shows that small InfluenceNetwork's perform comparatively well as larger InfluenceNetwork's."}, {"title": "Subset Selection Evaluation", "content": "Motivated by the results in Figure 2, we apply the InfluenceNetwork to the downstream task of subset selection: can we achieve the same performance when using the InfluenceNetwork instead of the original influence function? Thus, this section corresponds to Step 3 in Figure 1.\nDatasets and models. We use MixInstruct as well as Alpaca (Taori et al., 2023) to evaluate NN-CIFT. These are both instruction tuning datasets where we use 15k for training, 5k for validation, and 5k for testing. We evaluate using two models: microsoft/Phi-3-small-8k-instruct (Abdin et al., 2024) and meta-llama/Llama-3.1-8B"}, {"title": "Analysis", "content": "Table 4 reports the costs for each method, in seconds. It shows that data valuation can be performed at 77-99% faster than the original influence functions. This is because the number of parameters in NN-CIFT is 0.0026-0.0029% the size of the language model in the original influence function. Also, when using the DistilGPT2 model, which is near 1% the size of the language model, the costs are reduced by 54-91%. While these results are promising, the results on the downstream task of subset selection clearly differentiate NN-CIFT and the DistilGPT2 baseline. To elaborate, despite the significant speedups, NN-CIFT"}, {"title": "Hyperparameter study #2: Trade-off between u and v", "content": "We perform a hyperparameter study between u and v on MixInstruct using DELIFT's influence function (Equation 1). We perform a grid search where u = v = {0,0.01, 0.05, 0.1, 0.15, 0.20, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}, amounting to 169 experiments. Figure 4 shows the results using the BGE metric from each of these experiments. As shown, the two figures in each row follow the same general trend, showcasing that NN-CIFT can effectively replace the expensive influence function estimation.\nAs expected, we notice a few trends. (1) QLoRA generally has better performance than ICL. This is because fine-tuning has more impact on the model than simply adding examples to the prompt (i.e., prompt engineering). (2) The bottom right tends to be darker as fewer IFT data lead to insufficient training. (3) Larger IFT subsets, especially in the ICL setting, lead to poorer performance. During ICL, the top-5 semantically similar samples are chosen from the subset to add as in-context examples. However, semantic similarity does not always translate to performance enhancement as these samples can be harmful to the model's performance. Finally, a follow-up to (3), the highest performance regions tend to be around v = 0.2 - 0.4. Appendix A contains results on smaller subsets of IFT data (v = 0.1 and 0.2)."}, {"title": "Conclusion", "content": "In this paper, we introduce NN-CIFT: Neural Networks for efficient Instruction Fine-Tuning to distill highly parameterized models used in modern influence functions into small neural networks. We empirically show the effectiveness of our InfluenceNetwork design through low prediction error rates, and competitive performance on the downstream task of subset selection for IFT. We use four different influence functions to test with NN-CIFT; our experimentation shows that NN-CIFT can lower costs for expensive data valuation, is adaptive to all kinds of influence functions (model-dependent or -independent; pairwise or pointwise), and does not require retraining for new data."}, {"title": "Limitations", "content": "While NN-CIFT is effective, it is heavily dependent on the influence function. The influence func-"}, {"title": "Pairwise Influence Functions", "content": "DELIFT (Agarwal et al., 2025) is a model-dependent, inference-based metric. Samples (ix, iy) \u2208 DF are used as in-context examples for evaluating (jx, jy) \u2208 DT, and those with improved model performance are chosen to represent DT. This can be calculated by comparing the performance with and without (ix, iy) as an in-context example (where D(\u00b7,\u00b7) \u2208 [0,1] is a function to measure distance between two probability distributions, and f(q|0) is a language model with parameters @ and input query q):\nsim(i, j) = D(jy, f (ix, iy, jx|0))-D(jy, f(jx|0))\nAfter data valuation, the data selection stage consists of using submodular functions (Bilmes, 2022). In particular, we use the Facility Location submodular function. It takes as input a similarity kernel that will optimize the maximum similarity between the chosen subset and the overall dataset while also minimizing the size of the chosen subset. To minimize the subset size, the Facility Location \u2013 and submodular functions, in general \u2013 employ a diminishing gains property. This property states that samples added to a smaller subset have more value than samples added to a larger subset. Hence, we rely on our influence function to capture the informativeness of samples, and submodular functions to choose a set of representative samples, resulting in a small, information-rich subset on which to fine-tune a model."}, {"title": "DELIFT (SE)", "content": "DELIFT (SE) (Agarwal et al., 2025) is a model-independent metric, and chooses samples from DF which are semantically closest to the samples from DT. Semantic distance is calculated by the cosine distance between embeddings of samples:"}, {"title": "LESS", "content": "LESS (Xia et al., 2024) is model-dependent, gradient-based metric. Here, gradients between samples in DF and DT are matched by cosine similarity, and those that match the highest are chosen to represent DT (where \u2207 (q; 0) is the gradient of data point q from a model with parameters 0):\n$\\\\sim(i,j)=\\frac{\\left<\\nabla\\left((i_{x}, i_{y});\\theta\\right),\\nabla\\left((j_{x}, j_{y});\\theta\\right)\\right>}{\\left\\|\\nabla\\left((i_{x}, i_{y});\\theta\\right)\\right\\|\\cdot\\left\\|\\nabla\\left((j_{x}, j_{y});\\theta\\right)\\right\\|}$"}, {"title": "Pointwise Influence Functions", "content": "Finally, SelectIT (Liu et al., 2024a) is another model-dependent metric that uses performance signals for data valuation, but incurs linear cost as it uses a model's uncertainty to rank data samples. Still, as mentioned in Table 1 from the main text, the linear time operations are forward propagations through LLMs.\nSelectIT ranks data points based on their token-"}]}