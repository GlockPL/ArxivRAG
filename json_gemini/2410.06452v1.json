{"title": "MODELING CHAOTIC LORENZ ODE SYSTEM USING SCIENTIFIC MACHINE LEARNING", "authors": ["Sameera S Kashyap", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "abstract": "In climate science, models for global warming and weather prediction face significant challenges due to the limited availability of high-quality data and the difficulty in obtaining it, making data efficiency crucial. In the past few years, Scientific Machine Learning (SciML) models have gained tremendous traction as they can be trained in a data-efficient manner, making them highly suitable for real-world climate applications. Despite this, very little attention has been paid to chaotic climate system modeling utilizing SciML methods. In this paper, we have integrated SciML methods into foundational weather models, where we have enhanced large-scale climate predictions with a physics-informed approach that achieves high accuracy with reduced data. We successfully demonstrate that by combining the interpretability of physical climate models with the computational power of neural networks, SciML models can prove to be a reliable tool for modeling climate. This indicates a shift from the traditional black box-based machine learning modeling of climate systems to physics-informed decision-making, leading to effective climate policy implementation.", "sections": [{"title": "1 Introduction", "content": "The Lorenz system of equations is a set of ordinary differential equations to represent a simplified model of atmospheric convection Sparrow [1982]. These set of equations have a wide range of applications in fields ranging from fluid mechanics to laser physics to weather prediction.\nOne of the most interesting properties of the Lorenz ODE System is that it is chaotic in nature Fowler et al. [1982]. Small changes in the initial conditions can lead to vastly different outcomes in the end result Liao S. [2014]. When simulated over a given period, the Lorenz ODEs show oscillations in time. Usually, numerical methods implemented in computational software modeling tools like Python, Julia, or Matlab are used to simulate the Lorenz System of ODEs. These methods are inefficient as Lorentz equations are sensitive to initial conditions and minute changes to the conditions and tiny rounding errors can lead to the accumulation of numerical errors over time. Very few studies have been aimed at integrating machine learning-aided methods in simulating the chaotic Lorenz system. In this study, we provide a robust investigation of the effect of two physics-aided machine learning models in simulating the Lorenz system of ODEs: Neural Ordinary Differential Equations (Neural ODEs) Chen et al. [2018] and Universal Differential Equations (UDEs) Rackauckas et al. [2020a]."}, {"title": "1.1 Impact on climate modeling", "content": "In our study, we demonstrate how these Scientific ML models can be integrated into one of the fundamental models of weather prediction: The Lorenz ODEs. This opens the door to integrating these methods into large-scale weather prediction models, which would traditionally require a large amount of data Mihir Bhawsar1 [2021], A H M Jakaria [2020]. One of the key advantages of Scientific ML methods, such as Neural ODEs and UDEs, is their ability to encode underlying physical laws into the model. This physics-informed approach allows these models to achieve high accuracy with less data compared to traditional machine-learning models. In the context of climate science models such as global warming or weather prediction models, where high-quality data can be sparse or difficult to obtain, this data efficiency is crucial. By requiring less data, SciML models can be trained more quickly and with fewer resources, making them highly suitable for real-world climate applications where rapid and accurate predictions are essential Lai et al. [2021a].\nAs governments and organizations seek to implement effective climate strategies, we need to look at integrating the interpretability of physical climate models with the power of neural networks. As we demonstrate in our study, the ability of Neural ODEs and UDEs to model and forecast chaotic systems with high accuracy ensures that policymakers have access to reliable information, empowering them to make informed decisions that address the climate crisis."}, {"title": "1.2 Neural ordinary differential equations and universal differential equations", "content": "Neural Ordinary Differential Equations and Universal Differential Equations come under the pillar of the rapidly growing field of Scientific Machine Learning Chen et al. [2018], Rackauckas et al. [2020a]. Scientific Machine Learning (SciML) merges physical models with neural networks. In doing so, this field of SciML leverages the interpretability of scientific structures with the expressivity of neural networks. This combination makes the SciML approach very powerful in discovering previously unknown physics and solving complex differential equations with even less data. As of this writing, there have been multiple studies investigating the applications of Scientific Machine Learning in the fields of epidemiology, optics, gene modeling, civil engineering, fluid mechanics, battery modeling, material discovery, drug discovery, quantum circuits, astronomy and a huge range of related fields Baker et al. [2019], Dandekar et al. [2020a,b], Abhijit Dandekar [2022], Ji et al. [2022], Bills et al. [2020], Lai et al. [2021b], Nieves et al. [2024], Wang et al. [2023], Ramadhan [2024], Rackauckas et al., Sharma et al. [2023a,b], Aboelyazeed et al. [2023]. SciML uses methodologies that combine traditional ODE and PDE solvers with neural networks, where the network partially or entirely replaces the equation, the neural network is then trained using gradient descent to obtain a model that captures the underlying physics. Broadly, the rise of Scientific Machine Learning can be attributed to three popular methodologies:\n\u2022 Neural Ordinary Differential Equations (Neural ODEs): The entire forward pass of an ODE/PDE is replaced with neural networks. We perform backpropagation through the neural network augmented ODE/PDE. In doing so, we find the optimal values of the neural network parameters. Chen et al. [2018], Dupont et al. [2019], Massaroli et al. [2020], Yan et al. [2019]\n\u2022 Universal Differential Equations (UDEs): In contrast to Neural ODEs, only certain terms of the ODE/PDES are replaced with neural networks. We then discover these terms by optimizing the neural network parameters. Universal Differential Equations can be used to correct existing underlying ODEs/PDEs as well as to discover new, missing physics. Rackauckas et al. [2020a], Bolibar et al. [2023], Teshima et al. [2020], Bournez and Pouly [2020]\n\u2022 Physics Informed Neural Networks (PINNs): PINNs are predominantly used as an alternative to traditional ODE/PDE solvers to solve an entire ODE/PDE. We replace the output variable with a neural network and the loss function is determined by the ODE/PDE solution and the boundary conditions. When we minimize the loss function, we automatically find the optimal solution to the ODE/PDE. Raissi et al. [2019], Cai et al. [2021a], Karniadakis et al. [2021], Cai et al. [2021b]\nDespite the rise in Scientific Machine Learning frameworks, very little attention has been paid to the systematic applications of the above SciML pillars on chaotic ODE system modeling, like the Lorenz system. Although some studies have investigated Neural ODEs applications on the Lorenz system Linot and Graham [2022], Mehta et al. [2020], Chattopadhyay et al. [2020], Zhi et al. [2022], there is no study that has looked at integrating Universal Differential Equations (UDEs) with the Lorenz system. In particular, the following questions are still unanswered:\n\u2022 Can Neural ODEs and UDEs be used as alternatives for traditional system modeling tasks involving numerical differentiation?\n\u2022 In the spirit of UDEs, can we replace certain terms of the Lorenz system with neural networks and recover them?"}, {"title": "2 Methodology", "content": "This section describes the methodological approach to exploring chaotic dynamics in climate modeling through Neural ODEs and UDEs on the Lorenz system.\nThe equations are defined as:\n$\\frac{dx}{dt} = \\sigma(y - x)$ (1)\n$\\frac{dy}{dt} = x(\\rho - z) - y$ (2)\n$\\frac{dz}{dt} = xy - \\beta z$ (3)\nThe constants \u03c3, \u03c1, and \u03b2 are specifically used in the Lorenz system because they are derived from the physical parameters of atmospheric convection and help to capture the essence of the convective process in a simplified mathematical model.\n\u03c3 - The Prandtl Number: Verzicco and Camussi [1999] represents the ratio of the rate of momentum diffusion to thermal diffusion. In the context of the Lorenz system, \u03c3 = 10 is a common value chosen to reflect the properties of air, which has a Prandtl number around this value. This value helps to ensure that the model accurately captures the relative effects of viscosity and thermal diffusion in atmospheric convection\n\u03c1 - The Rayleigh Number: Howard [1966] It quantifies the driving force for convection due to the temperature difference across the fluid layer. The value \u03c1 = 28 is chosen to place the system in a chaotic regime. In the Lorenz system, this particular value is known to lead to chaotic behavior, which is of interest for studying complex, unpredictable dynamics similar to those observed in the atmosphere.\n\u03b2 - The Geometric Factor: It accounts for the aspect ratio of the convective cells. The specific value \u03b2 = 8/3 is chosen based on empirical and theoretical studies of convective systems. This value ensures that the simplified model retains key characteristics of the full convection equations, allowing it to accurately represent the interplay between the fluid's horizontal and vertical motions.\nAs shown in Figure 1, the chaotic solutions of the Lorenz system often settle into a pattern known as the Lorenz attractor. The Lorenz attractor is a fractal structure in phase space that illustrates the complex, deterministic yet unpredictable behavior of the system. It consists of two lobes, where the system's trajectory oscillates back and forth in an aperiodic manner, never repeating exactly but confined to a bounded region. This is exactly the outcome that is produced on solving the Lorenz system of equations using ordinary differential equations, it is especially useful for the inherent unpredictability in weather systems due to sensitivity to initial conditions, a concept often summarized by the \"butterfly effect.\"\nFor all experiments conducted forward, the constants \u03c3, \u03c1, and \u03b2, are set to 10.0, 28.0 and 8/3 respectively. These constants control different aspects of the convective motion in the Lorenz system, which was originally developed to model atmospheric convection. these values specify a particular regime of chaotic behavior in the Lorenz attractor,"}, {"title": "2.1 Neural ODE", "content": "Neural Ordinary Differential Equations (Neural ODEs) are a class of models that represent continuous-depth neural networks. Introduced by Chen et al. [2018], Neural ODEs have opened up new possibilities in modeling continuous processes by using differential equations to define the evolution of hidden states in neural networks Lee and Parish [2021], Sorourifar et al. [2023], Kim et al. [2021], Kiani Shahvandi et al. [2022], Finlay et al. [2020], Portwood et al. [2019]. Neural ODEs represent a novel approach in machine learning and computational modeling that combines neural networks with ordinary differential equations (ODEs). Neural ODEs are a subset of the broader spectrum of scientific machine learning and physics-informed machine learning. The key idea behind Neural ODEs is to use a neural network to approximate the solution of an ODE, thereby allowing for flexible modeling of continuous-time dynamics.\nIn a traditional neural network, hidden states are updated using discrete layers. In contrast, Neural ODEs use a continuous transformation defined by an ordinary differential equation:\n$\\frac{dh}{dt} = f(h(t), t, \\theta)$ (4)\nwhere,\n\u2022 h(t) is the hidden state at time t.\n\u2022 f is a neural network parameterized by \u03b8.\n\u2022 The hidden state evolves according to the function f.\nNeural ODEs can be implemented by setting up a Neural Network function with a set number of Hidden layers, weights for each layer, and the activation function that would be used to train the network. Once the neural network is set up, training a Neural ODE involves solving the differential equation using numerical ODE solvers (e.g., Runge-Kutta methods). The output at the final time t\u2081 is compared to the target, and gradients are back-propagated through the ODE solver to update the parameters \u03b8. Neural ODEs are more efficient than traditional machine learning as they are most suitable to model complex distributions as they use continuous transformations defined by differential equations Lu et al. [2021], hence they retain and integrate physical laws into the learning process by combining data-driven and physics-based models making them more memory efficient to model complex systems or system of equations.\nTo solve the Lorenz system of equations using Neural ODEs, several crucial steps must be followed. The process begins with setting up an ODE solver capable of integrating the differential equations over the desired time span. This solver must be selected based on its ability to handle stiff equations, ensuring numerical stability and accuracy."}, {"title": "2.2 Universal Differential Equations", "content": "Universal Differential Equations (UDEs) introduced Rackauckas et al. [2020a], combine traditional differential equations with machine learning models, such as neural networks, to create a more flexible and powerful tool for modeling complex systems. This approach integrates the robustness of classical differential equations with the adaptability of neural networks, allowing for more accurate and efficient modeling of systems with unknown or partially known dynamics. UDEs offer improved predictive power by combining data-driven approaches with physical laws. This is particularly useful in scenarios where purely data-driven models might overfit or fail to generalize Lu et al. [2019], Innes et al. [2019], Shen et al. [2023], Rackauckas et al. [2020b]. The physical laws embedded in UDEs constrain the learning process, ensuring that the model adheres to known scientific principles. Compared to purely data-driven models, UDEs often require fewer data points to achieve high accuracy. The known differential equations provide a strong prior that guides the learning process, reducing the amount of data needed for training. This efficiency makes UDEs suitable for applications with limited data availability.\nTo implement UDEs in our experiment, we will consider the Lorenz Attractor equations and augment them replacing some terms in the equation with its neural network. Some terms are multiplied by a factor to enhance the training accuracy and improve loss optimization.\nWe modify equations (1, 2, 3) with, the augmented equations are defined as:\n$\\frac{dx}{dt} = \\sigma(y - NN_1(t,x,y,z;\\theta))$ (5)\n$\\frac{dy}{dt} = -y + 0.1NN_2(t,x,y,z;\\theta)$ (6)\n$\\frac{dz}{dt} = -\\beta + z + 10NN_3(t,x,y,z;\\theta)$ (7)\nTo experiment with Universal Differential Equations (UDEs) in a manner similar to Neural ODEs, it is essential to establish an ODE solver that can yield the true solution. This true solution serves as a benchmark for performing backpropagation on the augmented UDE equations. By analyzing the outcomes from training Neural ODEs, we can eliminate the less effective activation functions while maintaining the initial values. This refined approach allows us to implement the neural network."}, {"title": "3 Results", "content": null}, {"title": "3.1 Neural ODE", "content": "The training process involves iteratively updating the network parameters \u03b8 to minimize the loss function, thus ensuring that the Neural ODE accurately captures the dynamics of the Lorenz system. Table 1 showcases the different hyperparameters considered for the experiment. In this experiment, we explored various hyperparameters to optimize the network's performance. Our focus was on three key areas: activation function, architecture, and optimizer."}, {"title": "3.2 Universal Differential Equations", "content": "Table 2 displays the range of hyperparameters used in the UDE experiment. From previous findings of the Neural ODE experiment, we utilize the proven hyperparameters, activation function sigmoid, hidden units (25), and epochs (50000), to obtain optimal training results.\nSimilar to neural ode, UDEs do not perform well when activation functions apart from sigmoid are employed, although they fare better results when compared to Neural ODEs, their overfitting and stagnation are still relatively high when compared to sigmoid activation function. Similarly, using a simple architecture of hidden layers less than 25 for example, produces better results with fewer resources when compared to a neural network with more than 50 or 100 hidden layers which only produces negligible improvements while consuming more time and compute resources.\nFigure 4b depicts the loss over time, from these findings it is clear that with the optimized parameter set, UDEs excel in identifying and recovering the missing terms, thereby reducing the loss to a minimal value. However, there was potential for further optimization. To achieve even better accuracy and further minimize the loss, we ran the UDE problem with a different optimizer. Specifically, we can refine the solution obtained from the initial prediction using the BFGS optimizer. This approach aims to enhance accuracy and reduce the loss even more effectively.\nBy leveraging the BFGS optimizer, known for its efficiency in handling non-linear optimization problems, we can fine-tune the parameters to achieve a more precise model. The process involves re-evaluating the solution with the new optimizer, which iteratively adjusts the parameters to minimize the loss function further. This refinement step played a crucial in ensuring the UDE model reaches its optimal performance reducing the loss to \u2248 0.0257, thereby accurately representing the underlying system dynamics.\nComparing the line graphs of deep and shallow neural networks in Figure 4a, we observe the impact of increasing the number of hidden layers from 2 to 3 and expanding the dimensions to 100 in the third layer. The plot reveals a trend of decreasing points with a stagnation period between epochs 2000 onwards. Toward the end of 48000 epochs, the loss decreases by a factor of 10 (\u2248 0.00107) compared to the shallow neural network results. While this performance improvement is noteworthy, it significantly increases the training time. Given that the initial loss value is already quite small, the advantages of employing a deeper neural network are debatable. Furthermore, the diminishing returns in performance raise questions about the necessity and efficiency of deeper architectures for certain tasks, suggesting that the benefits might not always justify the increased complexity and computational resources required.\nFigure 5a depicts the plot of a Universal Differential Equation (UDE) training plot, the y-axis represents time, extending from 0 to 8 units. The graph demonstrates an exceptional alignment between true data and predicted points, where they meet precisely at every time point. This fit is a result of optimized parameter findings, which significantly reduced the loss to an exceptionally low value of 0.0257. The optimized parameters ensure that the model not only aligns perfectly with the true data within the observed interval but also exemplifies the potential of UDEs to achieve superior predictive performance.\nFigure 5b depicts the forecast plot for the trained UDE model, for an extended period from 0 to 15 units, the predictions closely align with the true data for the majority of the interval, showcasing the model's strong predictive capability. However, beyond the timespan, the prediction begins to diverge from the true data, highlighting a limitation of the"}, {"title": "3.3 Effect of noise", "content": "The phenomenon observed in figures 7a and 7b, where the prediction markers for u1, u2, and u3 do not perfectly coincide with the ground truth data, can be attributed to the sensitivity of neural ODEs and UDEs to noise in the training data. During training, the neural network learns to approximate the underlying dynamics of the system based on the provided data. However, when noise is introduced into the training data (in this case, by adding Gaussian noise with a noise level of 0.1 to the original ODE data), the network must learn not only the true dynamics but also accommodate the noise. This can lead to overfitting, where the model captures the noise characteristics rather than the true underlying dynamics, resulting in poor generalization to unseen data or even to the same data when noise is present. The breakdown of predictions indicates that the model is not robust to the noise introduced during training, which is a common challenge in machine learning models dealing with differential equations."}, {"title": "3.4 Forecasting breakdown points for the SciML models", "content": "Despite the accurate training and prediction of both models, they come with their limitations. As shown by Figure 8a the Neural ODE model beyond a certain point stops capturing the underlying physics and starts stagnating into a straight line indicating a need for continuous model validation and potential enhancements for long-term predictions. On the other hand in the case of UDE as shown by Figure 8b, prediction for u\u2081 completely breaks down towards a high negative value suggesting a lack of reliability for extended prediction. Future work will focus on improving the forecasting performance of these models by integrating symbolic regression to discover the symbolic formulations of recovered terms."}, {"title": "4 Discussion and Conclusion", "content": "We were able to approximate the underlying data very well through a trained Neural ODE. The combination of the sigmoid activation function, Adam optimizer with an appropriate learning rate, and a streamlined neural network architecture has synergistically contributed to the significant reduction in loss observed. The Neural ODE model forecasts well from 0 to 15 units but breaks down after that. This divergence is indicative of the model's ability to capture the underlying dynamics up to a certain extent, beyond which its predictive accuracy diminishes.\nUDEs excel in identifying and recovering the missing terms, thereby reducing the loss to a minimal value of 0.0257. The efficiency of UDEs in learning from data and refining model parameters suggests their broad applicability across various scientific and engineering domains. Similar to the Neural ODE, the UDE model forecasts well till a timespan of 15 units. Beyond the timespan of 15 units, the prediction begins to diverge from the true data, highlighting a limitation of the model. This deviation suggests that while the UDE model excels within the observed time range, its ability to generalize and maintain accuracy diminishes over extended periods."}]}