{"title": "Any Target Can be Offense: Adversarial Example Generation via Generalized Latent Infection", "authors": ["Youheng Sun", "Shengming Yuan", "Xuanhan Wang", "Lianli Gao", "Jingkuan Song"], "abstract": "Targeted adversarial attack, which aims to mislead a model to recognize any image as a target object by imperceptible perturbations, has become a mainstream tool for vulnerability assessment of deep neural networks (DNNs). Since existing targeted attackers only learn to attack known target classes, they cannot generalize well to unknown classes. To tackle this issue, we propose Generalized Adversarial attacKER (GAKer), which is able to construct adversarial examples to any target class. The core idea behind GAKer is to craft a latently infected representation during adversarial example generation. To this end, the extracted latent representations of the target object are first injected into intermediate features of an input image in an adversarial generator. Then, the generator is optimized to ensure visual consistency with the input image while being close to the target object in the feature space. Since the GAKer is class-agnostic yet model-agnostic, it can be regarded as a general tool that not only reveals the vulnerability of more DNNs but also identifies deficiencies of DNNs in a wider range of classes. Extensive experiments have demonstrated the effectiveness of our proposed method in generating adversarial examples for both known and unknown classes. Notably, compared with other generative methods, our method achieves an approximately 14.13% higher attack success rate for unknown classes and an approximately 4.23% higher success rate for known classes.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) have significantly advanced the field of artificial intelligence, achieving remarkable success in various domains, including image"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Iterative Methods", "content": "Since the discovery of adversarial examples, most iterative methods are pro- posed, which utilize model gradients to iteratively add adversarial perturbations to specified images. These methods are mainly categorized as gradient-based op- timization and input transformation. The gradient-based optimization aims to circumvent poor local optima by employing optimization techniques. MI-FGSM [3] and NI-FGSM [17] introduce momentum and Nesterov accelerated gradient into the iterative attack process to enhance black-box transferability, respec- tively. PI-FGSM [6] introduces patch-wise perturbations to better cover the dis- criminative region. VMI-FGSM [37] tunes the current gradient with the gradient variance from the neighborhood. RAP [26] advocates injecting worst-case per- turbations at each step of the optimization procedure rather than minimizing the loss of individual adversarial points. The input transformation methods also increase adversarial transferability by preventing overfitting to the surrogate"}, {"title": "2.2 Generative Methods", "content": "Another branch of targeted attacks utilizes generators to craft adversarial ex- amples. Compared with iterative-based attacks, generator-based attacks have several characteristics [8]: high efficiency with just a single model-forward pass at test time and superior generalizability through learning the target distribu- tion rather than class-boundary information [21]. Thus, many generator-based targeted attack methods are proposed, which can divided into single-target and multi-target generator attacks. Notably, this work focuses on scenarios where black-box models are entirely inaccessible, so query-based generator attacks which requiring extensive querying are not within the scope of discussion.\nSingle-target Generative Methods: Early generative targeted attacks employed a single generator to attack a specific target, primarily aiming to enhance transferability across various models. Pourseed [25] proposes a gen-"}, {"title": "2.3 Adversarial Defenses", "content": "A primary class of defense methods processes adversarial images to break the perturbations. For instance, Guo et al. [9] introduces several techniques for input transformation, such as JPEG compression [9], to mitigate adversarial pertur- bations. R&P [41] employs random resizing and padding to reduce adversarial effects. HGD [16] develops a high-level representation guided denoiser to dimin- ish the impact of adversarial disturbances. ComDefend [14] proposes an end-to- end image compression model to defend against adversarial examples. NRP [20] trains a neural representation purifier model that removes adversarial pertur- bations using automatically derived supervision. Another approach enhances re- silience to attacks by incorporating adversarial examples into the training phase. For instance, Tram\u00e8r et al. [35] bolster black-box robustness by utilizing ad- versarial examples generated from unrelated models. Similarly, Xie et al. [42] integrate feature denoising modules trained on adversarial examples to develop robustness in the white-box model."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Problem Formulation", "content": "Formally, let \\(x_s\\) denote a clean image, and \\(y\\) is the corresponding label. \\(F(\\cdot)\\) is the classifier with parameters \\(\\psi\\). The targeted attack aims to mislead the classifier"}, {"title": "3.2 Generalized Adversarial Attacker", "content": "In this section, we introduce the details of the proposed Generalized Adver- sarial Attacker (GAKer). The overall pipeline is depicted in Fig. 2. Given a"}, {"title": "3.3 Latent Infection", "content": "This section describes how to inject target features into the source image in the latent feature space. There are two major modules in GAKer: the Feature Ex- tractor \\(F_\\psi\\) and the Generator \\(G_\theta\\), as shown in the Fig. 2. The Feature Extractor is adapted from a pretrained model by removing the classification head. It is specialized for extracting feature vectors from input images. During the train- ing, the weights of this module are frozen. We employ a UNet [5] as the basic architecture of the generator. It is designed to generate adversarial examples by using clean images with features of the target object obtained from the Feature Extractor."}, {"title": "4 Experiment", "content": null}, {"title": "4.1 Experimental Settings", "content": "Datasets. We train our models on the ImageNet training set [2]. Correspond- ingly, we evaluate the performance on the ImageNet val set.\nNetworks. We consider several models, including DenseNet-121 (Dense-121) [13], ResNet-50 (Res-50) [11] and VGG-19 as surrogate models. We select various black-box models, i.e., ResNet-152 (Res-152) [11], VGG-19 [28], Inception-v3 (Inc-v3) [30], ViT [32], DeiT [33] and CLIP [27], for testing the transferability of attacks. Additionally, we evaluate the proposed method on defense models, including Inc-v3adv, Inc-v3ens3, Inc-v3ens4, IncRes-v2ens [35], and Large Vision- Language Models (LVLM) such as LLaVA [18,19] and Qwen-VL [1].\nBaselines. For iterative attacks, we compare our method with MI [3] and the advanced method SU [40], which is competitive in target settings. For single- target generative attacks, we choose TTP [21] as the method for comparison with our approach. Specifically, we train multiple TTP models to accomplish multi-target attacks. For multi-target generative attacks, we choose HGN [44] and ESMA [5]. Both of these methods, along with ours, only require training a single generator to perform attacks on multiple targets.\nImplementation details. In all experiments, the perturbation constraint \\(\\epsilon\\) is set to 16, the number of known classes \\(N\\) is set to 200, the \\(a\\) is set to 0.5 and the number of samples in each known class \\(M\\) is 325. We train the generator with an AdamW optimizer for 20 epochs. For the MI method, we set the decay factor \\(\\mu\\) to 1. For the SU method, we perform the combinational attack of DTMI [3,43]"}, {"title": "4.2 Main Results", "content": "Results on Unknown Classes. Compared with existing generator-based at- tacks, the best innovation of our method is the ability to attack unknown classes. We select 200 classes as known classes and the remaining 800 classes as unknown classes from ImageNet. Then we train the generator with the known classes and evaluate the targeted attack success rate on unknown classes. Notably, only HGN and our method can be evaluated on unknown classes.\nAs shown in Tab. 1, our method significantly outperforms HGN, highlight- ing the superior transferability of our approach. For instance, with a substitute model of ResNet-50 and a black-box model of VGG-19, our method achieves a success rate of 41.69% on unknown classes, while HGN only achieves 0.05%. This result underscores the limitation of existing methods in generating tar- geted adversarial examples for unknown classes, while our method demonstrates effective generalization to such classes. Separate average results on all unknown classes can be found in Appendix D.\nResults on Known Classes. For evaluation on known classes, we compare our method with state-of-the-art iterative-based method (SU), single-target generator- based attacks (TTP), and multi-target generator-based attacks (HGN, ESMA). All multi-target generator-based attacks (HGN, ESMA, and our method) are trained on the same 200 classes. Due to the TTP method requiring training a model for each target class, the cost of training 200 models is prohibitively high. Therefore, we randomly select 10 classes from the 200 classes and train 10 TTP models separately for each substitute model (TTP-10). We then test our method on the same 10 classes (GAKer-10).\nTable 2 shows the targeted attack success rates on known classes for each method. Compared with the iterative-based method SU, our method achieves"}, {"title": "4.3 Results on Other Models", "content": "To further evaluate the generalization ability of our method, we also test our method on defense models and Large Vision-Language Models (LVLM).\nResults on Defense Models. In addition to attacking normally trained mod- els, we evaluate the attack performance of various methods on adversarially trained models when using ResNet50 as the white-box model. As shown in Tab. 3, our method surpasses the HGN method in attacking targets in unknown and known classes. For example, with the defense model as Inc-v3adv, we out- perform HGN by 6.46% in terms of target success rate on the known classes. On the unknown classes, HGN cannot achieve an attack at all, similar to the performance of the clean samples, whereas our GAKer still exhibits an attack success rate of 5.95%. This result indicates that our generator has discovered more common model vulnerabilities, regardless of whether the model has been specifically trained for defense."}, {"title": "4.4 Ablation Study", "content": "This section discusses how different parameter selections in training dataset con- struction impact the generator's attack capability.\nNumbers of Known Classes. Figure 4 demonstrates the impact of the number \\(N\\) of known classes on the generator's performance. Specifically, to mitigate the impact of adding new classes as the number of known classes in- creases, we evaluate targeted attack success rate on a common set of 10 known classes and 500 common un- known classes. When \\(N\\) is 10 or 50, the attack success rate on unknown classes is low due to limited train- ing data. With \\(N\\) increased to 500, the white-box success rate reaches 49.45% on 500 unknown classes. De- spite higher training costs, perfor- mance does not significantly improve with more known classes. Therefore, \\(N\\) is set to 200 to balance performance and training cost.\nWhen \\(N\\) is 10 or 50, the attack suc- cess rate on unknown classes is low due to limited training data. With \\(N\\) increased to 500, the white-box suc- cess rate reaches 49.45% on 500 un-\nStrategies for Choosing Known Classes. Previous work [44] on multi-target generators has highlighted the importance of not only determining the number of target classes but also selecting which target classes to attack. When there"}, {"title": "5 Conclusion", "content": "Generator-based targeted attacks are able to mislead DNNs to any target they have been trained on, showing their dangers. For the first time, we find that the attack also extends to untrained unknown classes, and the extent of its potential harm is revealed. We propose the Generalized Adversarial attacker, which injects target feature into adversarial examples to attack unknown classes. Through comprehensive experiments across standard, defense, and large vision-language models, we demonstrate that our method can effectively attack unknown and known classes across models. We hope our work will draw attention to the po- tential dangers of generator-based targeted attacks and inspire future research in this area."}, {"title": "6 Societal Impacts & Limitation.", "content": "Societal Impacts. Previous research on generator-based target attacks re- quires the attacker to know the target class. Our proposed algorithm allows suc- cessful targeted attacks without this information, highlighting the risk of relying solely on dataset and model confidentiality for security. Moreover, our method's success on unknown classes reveals inherent model vulnerabilities, offering new insights for advancing security.\nLimitation. While our method validates the possibility of attacking un- known classes using generator-based methods, the gap in target attack success rates between known and unknown classes still exists. In the future, we will focus on analyzing the reasons for this difference."}, {"title": "A Implement Details", "content": "During training, we consider samples with small classification loss to be excellent target samples with more prominent features. We continue to use this estimation during testing.\nTest on Known Classes. When evaluating the target attack success rate on known classes, since the attacker entirely constructs the training set of known classes, we select the target sample with the smallest classification loss in the train set as the target for that known class.\nTest on Unknown Classes. When testing on unknown classes, the attacker obviously cannot access the complete target dataset to select the best sample but can still choose some target samples that are relatively clear and unobstructed. To simulate this process, we select 10 images with relatively small classification losses for each target class from the validation set of ImageNet as targets. During testing, the target samples of unknown classes are randomly selected from these 10 images."}, {"title": "B Efficiency Analysis", "content": "In this section, we analyze the efficiency of different methods. To simulate the scenario of unknown classes, we divide the ImageNet-1k dataset into 5 parts, with each part appearing sequentially. Unlike other generator-based methods that require retraining for each new part, our GAKer can attack any target class with just one training process. As shown in Tab. 6, our method only requires 1% of the training time compared to TTP, highlighting the remarkable efficiency of our GAKer."}, {"title": "C Parameter Sensitivity Analysis", "content": "To explore the impact of the parameter \\(\u03b1\\), we conduct several experiments. We train the model with \\(\u03b1\\) set to 0, 0.25, 0.5, 0.75, and 1, respectively. Then, we validate the effect of different \\(\u03b1\\) settings on the attack success rate. In this experiment, we use ResNet-50 (Res-50) [11] as the substitute model and test the target attack success rates (TASR) on known and unknown classes on ResNet-50 (Res-50) [11], ResNet-152 (Res-152) [11], VGG-19 [28], DenseNet-121 (Dense- 121) [13]. The experimental results are shown in Fig. 7. Finally, we select \\(\u03b1\\) as 0.5, which yields the best experimental results."}, {"title": "D Further Analysis on Unknown Classes", "content": "To further analyze the performance of our GAKer on unknown classes, we visualize the targeted attack success rate (TASR) for all 800 unknown classes in Fig. 8. Meanwhile, we present the histogram of TASR in Fig. 9. The statistics show that there is a large variation in TASR among different unknown classes. The TASR for most of the unknown classes exceeds 50%, but some classes are hardly attacked, with a TASR lower than 50%. This phenomenon indicates that different classes have varying levels of difficulty."}, {"title": "E Results on GPT-4V", "content": "We conduct a series of tests on the GPT-4V [45] to showcase the effectiveness of our method when dealing with Large Vision-Language Models. All experiments on GPT-4V are conducted on 13 March 2024."}, {"title": "F Visualization of Adversarial Examples", "content": "In this section, we show some adversarial examples in Fig. 18."}]}