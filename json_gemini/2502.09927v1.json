{"title": "GRANITE VISION: A LIGHTWEIGHT, OPEN-SOURCE MULTIMODAL MODEL FOR ENTERPRISE INTELLIGENCE", "authors": ["Granite Vision Team, IBM Research"], "abstract": "We introduce Granite Vision, a lightweight large language model with vision capabilities, specifically designed to excel in enterprise use cases, particularly in visual document understanding. Our model is trained on a comprehensive instruction-following dataset, including document-related tasks, such as content extraction from tables, charts, diagrams, sketches, and infographics, as well as general image tasks. The architecture of Granite Vision is centered around visual modality alignment with a decoder-only, 2 billion-parameter Granite large language model. Additionally, we introduce a dedicated safety classification approach in test-time that leverages a sparse set of attention vectors to identify potential harmful inputs. Despite its lightweight architecture, Granite Vision achieves strong results in standard benchmarks related to visual document understanding, as well as on the LiveXiv benchmark, which is designed to avoid test set contamination by using a constantly updated corpus of recently published Arxiv papers. We are releasing the model under the Apache-2 license, allowing for both research and commercial use, while offering complete visibility into the training data and other relevant details. See https://huggingface.co/ibm-granite/ for model weights.", "sections": [{"title": "1 INTRODUCTION", "content": "The confluence of computer vision and natural language processing has led to significant advances in multimodal learning, enabling large language models to effectively integrate and reason about visual content and linguistic data.\nWhile this progress is exciting, a notable disparity remains: smaller models, typically with 1-4 billion parameters, have consistently fallen short of achieving performance comparable to their larger counterparts containing tens of billions of parameters. In addition, existing models are predominantly trained on natural images, which can limit their performance in other domains, such as visual document understanding, where the unique visual characteristics, such as layouts, fonts, and graphics, significantly differ from natural images and require a more fine-grained comprehension of the visual content.\nIn this work, we introduce Granite Vision, a compact vision-language model with approximately 3 billion parameters\u00b9, tailored to excel in enterprise use cases. Although our model can process general images, the first version of Granite Vision is particularly focused on visual document understanding, enabling automated content extraction from tables, charts, infographics, plots, diagrams, sketches, and more. Figure 1 shows qualitative examples of our model's output for visual document understanding tasks and general image description. Granite Vision extends the Granite family of large language models (Granite Team (2024)), which have been trained on more than 12 trillion tokens, achieving state-of-the-art performance for their size, while being designed for enterprise usage, with full visibility into the training data."}, {"title": "2 RELATED WORK", "content": "Recent advances in multimodal large language models (MLLMs) have demonstrated significant progress in understanding and generating content across different data modalities.\nTo mitigate the high costs associated with training large models in an end-to-end manner, a common approach involves employing modular architectures. These architectures typically combine a pre-trained modality encoder and a pre-trained large language model (LLM) with a learnable modality connector, with the latter comprising only a small portion of the total parameters (Bai et al., 2023; Tong et al., 2024; Chen et al., 2024b). More recently, Zhang et al. (2024a) demonstrated that even relatively small models can achieve strong performance with careful data curation and optimized training strategies. Building on these insights, our model uniquely achieves state-of-the-art results on standard document and other benchmarks, all while operating at a significantly reduced scale (around 3 billion parameters).\nVisual document understanding, particularly the ability to comprehend charts, diagrams, tables, and document images, represents a crucial application area for MLLMs. Two primary technical challenges emerge in enabling MLLMs to process documents and associated images effectively: adequately encoding highresolution images, and accurately interpreting visually-situated text within the documents. Recent approaches to addressing these challenges are often broadly categorized into two groups based on their text recognition methodology. The first category, including models like DocFormer (Appalaraju et al., 2021), LayoutLMv3 (Huang et al., 2022), and, more recently, DocLLM (Wang et al., 2023), relies on external optical charac-"}, {"title": "3 DATA", "content": "Granite Vision has been trained on a comprehensive instruction-following dataset, which covers a wide variety of visual categories ranging from document images to general images. The training data consists of a combination of pre-curated public vision datasets, common-crawl PDFs, and data that is synthesized in-house. A detailed view of our data is presented in Figure 2, which depicts document understanding datasets, and Figure 3, which represents general image datasets. Our document understanding data (Figure 2) covers a variety of document classes such as general document images, charts, flowcharts, diagrams and several more encompassing a diverse set of visual Q&A tasks. See Table 5 (in Appendix) for a comprehensive overview of the datasets. Such diverse data is chosen to ensure the model's ability to generalize across a wide array of document-centric applications. In addition to document datasets, we incorporated training data from multiple publicly available generic image datasets (Figure 3). In the following sections, we dive deep into different data categories, data collection processes, synthetic data generation techniques, and data pre-processing pipelines."}, {"title": "3.1 IBM CURATED DATASETS", "content": "In this section, we will describe several large synthetic document Q&A datasets created at IBM and focused on a diverse set of document and Q&A tasks. In order to seed this effort, we leveraged DocFM, a large-scale comprehensive dataset effort at IBM consisting of 85 million document pages extracted from unique PDF documents sourced from Common Crawl, Wikipedia, and ESG (Environmental, Social, and Governance) reports. DocFM served as a foundation for generating our synthetic datasets, which capitalized on its enriched nature for granular filtering and balancing.\nDocFM was created using Docling (Livathinos et al., 2025), an open-source document conversion toolkit developed by IBM. Docling facilitated the extraction and enrichment of each page with a rich set of attributes, resulting in a highly detailed representation of document pages. Specifically, this enabled:\n\u2022 Text and Positional Metadata Extraction: Parsing each PDF page to retrieve textual content along with its positional metadata.\n\u2022 Layout Understanding: Identification of structural components such as headers, footers, paragraphs, lists, captions, code, formulas, form elements, pictures, footnotes, and document indices.\n\u2022 Reading Order Detection: Establishing the reading sequence of extracted content based on layout and parsed output.\n\u2022 Language Detection: Identifying the primary language present on each page.\n\u2022 Figure Classification: Labeling visual elements, such as charts, images, and diagrams.\nTo ensure high-quality textual representation, we focused exclusively on PDFs with extractable content, ensuring reliable and efficient text processing while minimizing noise."}, {"title": "3.1.1 DoCFM - DATA COLLECTION & FILTERING", "content": "A large portion of the DocFM dataset was obtained from the Common Crawl corpus which is an open repository of web data. It is freely accessible and distributed as monthly archive files in various formats (\"segments\"). Among these segments, the URL index contains a list of crawled URLs accompanied by metadata, such as the media type specified by the HTTP header and the media type detected by Common Crawl from the actual URL content. By adhering to the Robots Exclusion Protocol (Koster et al., 2022) (commonly known as"}, {"title": "3.1.2 IBM SYNTHETIC VISUAL Q&A DATASETS", "content": "DocFM-VQA (Synthetic Document VQA): There's a lack of large-scale instruction-following datasets focused on visually-rich document understanding. The most commonly used dataset, DocVQA (Mathew et al., 2021), is limited in size as it contains only 10K images and 40K QA pairs. Docmatix (HuggingFace, 2025) has been recently introduced as a potential large-scale alternative to DocVQA, but it uses only the textual parts of documents for QA pairs generation.\nIn our work, we leverage a large language model (Mixtral 8x22B), to synthetically generate a large-scale visual question answering (VQA) dataset using a subset of DocFM as seed. We call this dataset DocFM verbalization-based VQA or DocFM-VQA in short. We employed a \u201cverbalized\u201d representation of the documents as context for the LLM's (Mixtral 8x22B) QA generation. This involved extracting text from programmatic PDFs and augmenting it with verbal descriptions of visual elements such as charts, images, and tables. This approach encouraged the generation of questions that specifically target these elements. Our verbalization process included: 1) Converting charts to tables using DePlot (Liu et al., 2023a), 2) Replacing images with captions generated by BLIP2 (Li et al., 2023). 3) Converting tables to markdown format using TableFormer (Nassar et al., 2022). The DocFM-VQA contains a total of 20 million QA pairs from 2.4 million randomly selected pages from the DocFM dataset. This dataset facilitates the training of models capable of robust performance on visually rich documents. While we explored the use of leading Visual Language Models (VLMs) to filter out potential hallucinations in the generated QAs, the observed performance benefits did not outweigh the computational cost. Therefore, filtering was not incorporated into the current model version.\nDocFM-VQA-charts (Synthetic Chart VQA): A general framework for synthetic QA generation on charts is to use tabular data in textual form, using an LLM to generate QAs on it, and using code to generate the appropriate visual chart. However, we observed two main issues with this approach. First, the generated QAs are unable to refer to visual properties like the used markers or plot colors. Second, LLMs tend to generate simple questions, such as a single data point extraction. As a result, large-scale synthetic chart QA datasets tend to dramatically improve the performance on synthetically generated benchmarks but have a smaller effect on human questions or human-annotated benchmarks (e.g. ChartQA's augmented vs. human splits).\nIn our work, we addressed the two issues mentioned above by augmenting the tabular form of the data, used as input to the LLM for generating QAs, with additional information. To solve the first issue of the LLM lacking knowledge of the visual appearance of the chart, we first choose a random marker and color for each data sequence. This information is added to the table consumed by the LLM and provided as input to the code used for rendering the chart. To solve the second issue (of very simplistic QAs), we added to each table additional rows and columns. These rows and columns contain additional information calculated based on the original table using code. It includes max, min, sum, average, and a random operation (addition or subtraction) between two randomly chosen columns/rows, e.g. \u201cColumn 1 + Column 3\u201d. This pre-calculated information encourages the model to ask questions requiring higher-order reasoning. It also allows the model to produce more accurate ground-truth answers for cases that require arithmetic capabilities that LLMs struggle with (Schwartz et al., 2024).\nWe used the augmented table approach as input to the DocFM-VQA pipeline to generate the DocFM-VQACharts dataset. It contains 5.5M QAs on about 500K synthetic charts rendered based on random numeric tables extracted from the DocFM dataset. We additionally augmented the ChartQA training set with the precalculated columns and rows information (without markers and colors) and generated additional QAs for the dataset's existing images.\nDocFM-VQA-flowcharts (Synthetic Flowchart VQA): In order for the model to better understand flowcharts we generated synthetic flowcharts with QA pairs. We used an LLM (Mixtral 8x7B) for generating the flowchart nodes and edges. We then use those nodes and edges to query an LLM for QA pairs and to render a visual representation of the flowchart. We used the pipeline described below to generate 17K JSON representation and rendered flowcharts. The JSONs are fed to the DocFM-VQA pipeline to generate 76K QA pairs.\nThe flowchart synthesis pipeline:\n\u2022 Identify Business Domains and Industries: Utilize the LLM to extract various business domains and identify industries associated with typical business processes.\n\u2022 List Business Process Types: For each business domain and industry, use the LLM to generate a comprehensive list of business process types.\n\u2022 Generate Business Processes: For each business process type, we employ the LLM to create multiple business processes constructed in a JSON format. Each process should have nodes and edges, with the number of nodes ranging from 5 to 20.\n\u2022 Create Flowchart Images: For each generated business process, use the 'graphviz' package to create a flowchart image. Incorporate random shapes, arrows, and colors to enhance visual diversity."}, {"title": "DocFM-ChartExtraction:", "content": "We created a training dataset of 550K chart images to ensure that the model can perform structured data extraction from chart visualizations. The dataset was generated by converting tables extracted from DocFM and public stock market data into diverse chart types using matplotlib, including line, scatter, bar, pie, candlestick, and OHLC charts. For each visualization, training examples included extraction instruction with randomized output format requirements (Markdown, HTML, or JSON) and varied natural language phrasings. To enhance document-level understanding capabilities, 100K DocFM-derived charts were embedded into their source document contexts by covering the original table with the chart image. The financial subset comprises 40K candlestick and OHLC charts generated from NASDAQ, S&P500, and NYSE stock data. This synthetic dataset enables the training of vision-language models to extract structured information from diverse visualization types while handling multiple output formats and document contexts."}, {"title": "DocFM Visual Cue and Captioning:", "content": "In order to create this dataset, we masked captions on images and then used the captions to generate questions regarding images. We implemented a multi-step process as follows: 1) Using Docling (Livathinos et al., 2025) we filtered to get figures associated with their corresponding captions; 2) We utilized Granite 3.1 8B model to analyze the captions for the presence of visual cues such as \"black arrow,\" \"on the left,\u201d \u201cred circle,\" and similar indicators of figure elements or regions of interest; 3) If the caption contained an explicit identifier (e.g., \u201cFigure 5\u201d), we extracted it using the parsed text from the PDF. The caption body text was then masked on the image, leaving only the identifier visible. If no explicit identifier was present, we used the figure's bounding box as a fallback identifier. This identifier can then be used for the question, \"What is the black arrow pointing at in Figure 5.?\". 4) Leveraging the LLM, we generated context-specific questions about the figure, using the caption to inform the question formulation and answer while hiding the textual context in the image.\""}, {"title": "DocFM Rule Based Grounding:", "content": "We further complemented our data by implementing rule-based approaches to generate synthetic tasks focused on specific document elements like text localization, figures, and tables. Leveraging Docling's layout detector (Pfitzmann et al., 2022), we generated questions that query the location of text passages and other page elements. To encourage a deeper understanding of visual components, we masked captions associated with figures and tables, keeping only their identifiers. Subsequently, we employed templates to generate questions prompting the model to provide captions for these figures and tables. Furthermore, using defined templates we generated more operations such as selecting row, columns by index, or by value or comparison using only images. For the latter, we leverage the table understanding methods from Docling (Nassar et al., 2022; Lysak et al., 2023)."}, {"title": "3.2 PUBLIC DATASETS", "content": "Our Granite Vision model is primarily focused on visual document understanding tasks. Therefore, we further enriched our data using a number of pre-curated high-quality, publicly available document-related datasets, which encompass several diverse tasks such as Document QA, Table QA, Chart QA, Diagram QA, OCR and scene-text related tasks, reasoning and grounding, UI screen/code and structure understanding. A detailed view of public document centric datasets is shown in Figure 2. For details readers are referred to Table 5 (in Appendix).\nBeyond document-centric content, we also included high quality general image Q&As sourced from public datasets to ensure robust performance in general visual tasks as well. Specifically, we acquired data from three high-quality collections: Cauldron4, Cambrian-7M5 and LLaVa-OneVision-Data\". These datasets cover a wide range of domains and tasks, including image captioning, visual question answering (VQA), and object recognition, providing the foundational knowledge for general-purpose vision-language models. More details and statistics about general images data are shown in Figure 3."}, {"title": "3.3 DATA PREPROCESSING", "content": "Data pre-processing consisted of multiple annotation and filtering steps to conform to regulatory and safety requirements and improve overall data quality as described below.\n\u2022 Restricted Data removal: All public datasets underwent a rigorous legal vetting process to identify data that could violate any legal or PII related requirements. All such data was removed, including datasets originating outside of the US and those with unclear license agreements.\n\u2022 Sexual Abuse Material (CSAM) removal: All CSAM was removed from the datasets using state-ofthe-art NSFW (not safe for work) detection methods.\n\u2022 Face blurring: Face blurring was performed to obfuscate all visual PII from the data.\n\u2022 Deduplication: Since we combined multiple dataset collections, there could be duplication of text as well as visual data. Similar images were detected through two methods: exact pixel-wise match, or perceptual hash matching (Zauner, 2010), for robustness against minor transformations. Records with identical texts and similar images were removed."}, {"title": "4 GRANITE VISION MODEL", "content": "In this section, we describe our model, including the architecture (Section 4.1), the training procedure (Section 4.2), and the use of Sparse Attention Vectors (SAVs) for safety classification (Section 4.3). An overview of our architecture is shown in Figure 4."}, {"title": "4.1 ARCHITECTURE", "content": "Vision-and-language models are designed to process two data modalities simultaneously, such as images and corresponding text instructions. Each modality is encoded into a shared embedding space, which is then utilized for reasoning by a language model $f_{\\theta}$ parameterized by $\\theta$. Specifically, an image $I$ is encoded using a pre-trained visual encoder, denoted as $v_{\\phi}$ parameterized by $\\phi$, and a projector $M_{\\lambda}$ parameterized by $\\lambda$. A corresponding textual prompt $P$ is tokenized and encoded using a fixed language encoder $l_{\\gamma}$ parameterized by $\\gamma$. Given an input image $I$ and a text instruction $P$, the language model generates a text response $R$ as follows: $R = f_{\\theta}(M_{\\lambda}(v_{\\phi}(I)), l_{\\gamma}(P))$.$ This section provides a detailed description of each of these components.\nVision Encoder $v_{\\phi}$. For an input image $I$, we use a vision encoder $v_{\\phi}$ to provide visual features $X = v_{\\phi}(I)$. In our implementation, $X$ is a concatenation of outputs from multiple layers, allowing us to combine these different levels of representation, providing rich representations that are beneficial for visual document understanding. We use SigLIP (Zhai et al., 2023) as our vision encoder with 384 \u00d7 384 resolution. Additionally, we use the \"AnyRes\" technique in our training as in Li et al. (2024c), which is designed to accommodate images of various high resolutions. The ability to process high-resolution images is of particular importance for document understanding, especially when dealing with documents containing small text fonts, as it enables accurate text recognition.\nSpecifically, we use a global image, and a grid configuration that divides the image into multiple patches (or views), resulting in an additional multipatch setting. We select the closest resolution from a predefined set that allows the original image size to be tiled into patches of size 384 \u00d7 384. We tile the image using up to ten patches, leading to a wide variety of aspect ratios ranging from 1 : 10 through 1 : 1 to 10 : 1 at different scales resulting in a total of 27 different tiling options. Next, the visual features $X$ are forwarded to the projector $M_{\\lambda}$."}, {"title": "4.2 TRAINING PROCEDURE", "content": "The training procedure of our Granite Vision model consists of three stages: (i) Pre-training for projector $M_{\\lambda}$, (ii) Pre-training with language model $f_{\\theta}$ and the Projector $M_{\\lambda}$, and (iii) Instruction tuning. Each image $I$ is accompanied by a text instruction $P$, and the predictions consist of the answer $A$ produced by the model. Specifically, for a response $R$, we compute the probability of the target words by the following equation:\n$p(A | I, P) = \\prod_{i=1}^{R} P_{\\psi}(x_i | I, P)$\nwhere $\\psi$ represents the trainable parameters. We note that $\\psi$ contains the following trainable parameters mentioned above ($\\theta, \\phi, \\lambda$), with different subsets are trained at various training stages. In addition, $x_i$ is the current prediction token. To calculate loss, we use the standard cross-entropy function with these probabilities. Next, we describe our three-step training process.\nStage 1: Pre-training for Projector $M_{\\lambda}$. In this step, the entire network (other than the projector $M_{\\lambda}$) is frozen to correctly align visual and language tokens. For this stage, we use 558k image-text pairs with captions taken from LLaVA-Pretrain7. Additionally, we use only the following scales in this stage: (384, 768), (768, 384), (768, 768), (1152, 384), and (384, 1152).\nFinally, we perform a hyperparameter search randomly on 5% of the data to select a batch size of 512, warm up ratio of 0.03, and learning rate of 1e - 4 with a cosine scheduler. We also used the following multi-turn conversation template:\n<|system|>\n\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n<|user|>P1 <| assistant |>A\u00b9<|end_of_text|>\n<|user|>P2 <| assistant |>A\u00b2<|end_of_text|>\nwhere (Pi, A\u00b9) is the i-th conversation pair iteration.\nStage 2: Pre-training with Language Model $f_{\\theta}$ for Projector $M_{\\lambda}$. In order to align the LLM with the updated visual features that have been processed by the projector, we use a second pre-training stage, in which we keep the vision encoder frozen and fine-tune the LLM $f_{\\theta}$ and the Projector $M_{\\lambda}$. We use the same image-text pairs data and same multi-conversation template as in stage 1.\nIt is important to note that at the end of this stage, we retain only Projector $M_{\\lambda}$ weights and discard the fine-tuned LLM, reverting the language model to its original weights for the next stage. We found that this procedure leads to a better starting point for the following instruction tuning stage.\nIn stage 2, for the LLM and Projector $M_{\\lambda}$, we use different learning rates of 8e-05 and 32e-05, respectively, and kept the same other stage 1 hyperparameters."}, {"title": "4.3 SPARSE ATTENTION VECTORS FOR SAFETY", "content": "Here we outline how we use sparse attention vectors (as proposed in Mitra et al. (2024) and Huang et al. (2024)), derived from the activation space of a Granite Vision model, as features for a safety classification setup. Our key insight is that within Granite Vision's many attention heads and transformer layers, there is a sparse subset of features that are useful for identifying safety if we formalize a range of safety tasks as classification problems. We present the following three-step method to identify and utilize these safety features.\nStep 1: Extracting General Attention Vectors. Given our model and few-shot samples of label pairs {(x\u2081,y\u2081), (x\u2082,y\u2082), ..., (xN,yN)}, we first extract the attention vectors for each sequence xi. Here, the labels in the few-shot demonstrations indicate specific safety characteristics of the query, like 'harmful' and 'non-harmful' classes. Next, for every xi, we compute the attention vector $h_{l,m}^{i}$ for head m from layer l for the final token x. This yields a set of attention vectors {$h_{l,m}^{i}(x_{i}) | i = 1, ..., N$} for each head m and layer l.\nStep 2: Finding Safety Vectors. In order to identify which attention vectors are naturally suited to a safety task, we evaluate each vector's discriminative ability by computing its performance under a nearest class centroid classifier.\nSpecifically, for each class c \u2208 C, we compute its mean attention vector across few shot examples:\n$\\mu_{l,m}^c = \\frac{1}{|N_c|} \\sum_{i \\in N_c} h_{l,m}^{i}(x_{i})$\nwhere $N_c$ is the set of indices of examples with label c. For each input xi, we compute its cosine similarity to each class centroid head:\n$s_{l,m}(x_i, c) = \\frac{h_{l,m}^{i}(x_{i}) \\cdot \\mu_{l,m}^c}{||h_{l,m}^{i}(x_{i})|| ||\\mu_{l,m}^c||}, \\forall c \\in C$\nNext, we measure the discriminative ability of each head by its performance as follows:\n$score(l, m) = \\sum_{i=1}^N \\mathbb{1}[\\hat{y} = y_i]$\nwhere the nearest class centroid label is given as \u0177, and $\\mathbb{1}[\\cdot]$ is the indicator function that evaluates to 1 when the condition is true. We denote the set of k top-scoring heads as $H_{SAV}$:\n$H_{SAV} = \\{(l, m) | score(l, m) \\text{ is among top } k\\}$\nStep 3: Classification with Safety Vectors. Given a sample sequence input Q to classify, we leverage our sparse set of heads $H_{SAV}$ for safety prediction. For each head (l, m) \u2208 $H_{SAV}$, we compute the class centroid $\\hat{y}_{l,m}$ closest to the sequence input as follows:\n$\\hat{y}_{l,m} = arg \\underset{c \\in C}{max} s_{l,m}(Q^T, c)$\nwhere si,m is defined as in Step 2. The final prediction counts the majority across all heads in $H_{SAV}$:\n$arg \\underset{y \\in C}{max} \\sum_{(l,m) \\in H_{SAV}} \\mathbb{1}[\\hat{y}_{l,m} = y]$\nUsing this approach, we are able to determine whether a new input sequence is safe. We note that this approach can be used more generally, as it has been used in Mitra et al. (2024) to extract multimodal features from large multimodal models for general discriminative vision-language tasks."}, {"title": "5 EVALUATION", "content": "We evaluated our models on a set of popular public benchmarks. Since the current Granite Vision release is mainly geared towards document understanding, our focus is on document-related benchmarks. However, we also report results for key natural-image benchmarks. We used the standardized lmms-eval package (Zhang et al., 2024b; Li et al., 2024a) to run the evaluations. We do not use any test time optimizations such as prompt tuning or chain-of-thought. The results for most other models were also produced by running the standard lmms-eval benchmarks when possible. For the remaining models, we used the results reported by the original authors or from the publicly reported benchmarks."}, {"title": "Document related benchmarks", "content": "\u2022 DocVQA: A benchmark designed to evaluate the models' ability to understand and extract textual information from documents (Mathew et al., 2021).\n\u2022 ChartQA: A benchmark which focuses on question answering about charts, requiring both visual and logical reasoning to interpret data presented in various chart formats (Masry et al., 2022).\n\u2022 TextVQA: Challenges models to read and reason about text present within images to answer questions accurately (Yang et al., 2021).\n\u2022 AI2D: Contains grade school science diagrams, aimed at evaluating diagram understanding and question answering capabilities (Kembhavi et al., 2016).\n\u2022 InfoVQA: Contains infographics - documents that combine textual, graphical, and visual elements to communicate information effectively. The questions require models to perform joint reasoning across document layout, text, and data visualizations (Mathew et al., 2022).\n\u2022 OCRBench: A benchmark designed to evaluate Optical Character Recognition (OCR) capabilities within various contexts, assessing the accuracy and robustness of OCR in documents and in-the-wild (Liu et al., 2024b).\n\u2022 WebSRC: A Web-based Structural Reading Comprehension benchmark. It consists of screenshots and question-answer pairs created based on the corresponding HTML source code and metadata. Each question in WebSRC requires a certain structural understanding of a web page to answer, and the answer is either a text span on the web page or yes/no.\n\u2022 LiveXiv: A benchmark which specifically focuses on testing multi-modal models' ability to understand domain-specific visual content like graphs, charts, and tables from scientific manuscripts. It is a live benchmark, containing recently published papers on Arxiv, helping prevent test set contamination that can occur with static benchmarks when models are trained on web-scraped data. It contains questions dealing with figures (VQA) and with tables (TQA) (Shabtay et al., 2024)."}, {"title": "Natural image benchmarks", "content": "\u2022 MMMU: A comprehensive benchmark designed to evaluate Multimodal Large Language Models on both perception and cognition abilities across various subtasks, ensuring robust and diverse testing of these models (Yue et al., 2024).\n\u2022 VQAv2: A benchmark for Visual Question Answering containing open-ended questions about images, designed to test a model's ability to understand and reason about visual content (Goyal et al., 2017).\n\u2022 RealWorldQA: Focuses on question answering in real-world scenarios, requiring models to interpret and reason over complex visual and textual information present in everyday images.\n\u2022 VizWiz: Contains over 31,000 visual questions originating from blind individuals, aiming to help answer visual questions posed by this community, with each question accompanied by crowdsourced answers (Gurari et al., 2018).\n\u2022 OK VQA: A benchmark designed for visual question answering tasks that require external knowledge beyond what is visible in the image, featuring over 14,000 questions to evaluate the reasoning abilities of AI models (Marino et al., 2019)."}, {"title": "5.2 ADDITIONAL EVALUATIONS", "content": "Converting tables and charts to a structured format are important tasks for enterprise use cases. We added to lmms-eval the benchmarks described below and compared our model with other open models.\nTable extraction To evaluate the ability of models to turn a table image into a structured format, we used the test sets of PubTables (Smock et al., 2022) and FinTabNet (Chen et al., 2021). We instruct the model to extract the data in HTML table format and evaluate performance according to Tree-Edit-Distance-based Similarity (TEDS) from Zhong et al. (2020). It measures both the table structure similarity and each cell string edit distance."}, {"title": "Chart extraction", "content": "Similar to table extraction evaluation, we also evaluate the ability of models to turn a chart image into a structured format. We used the test set of ChartQA (Masry et al., 2022) for this evaluation. We instruct the model to extract the data in Markdown or HTML format, the prompt includes an example of the expected format. Model performance is evaluated according to a Modified TEDS (mTEDS) metric. The original TEDS measures string edit distance for each cell, which is not optimal for numerical values where the distance should take into account the scale of chart values. We define mTEDS as follows, let T be a table with numeric values V = {vij} excluding headers. Define the normalized function:\n$N(v_{ij}) = round(\\Big( 20 \\cdot \\frac{v_{ij}}{max_{i,j}(abs(V_{gt}))} \\Big))$\nwhere Vgt represents the ground truth table values. The metric applies standard TEDS computation on the normalized values, comparing structural and content similarity between predicted and ground truth tables. This normalization accounts for scale-dependent accuracy in chart data extraction while maintaining TEDS's ability to evaluate structural correctness."}, {"title": "5.3 SAFETY BENCHMARKS", "content": "To evaluate the safety capabilities of our Granite Vision model, we employ two different setups. First, we utilize the standard VLM-as-a-Judge setup as described in (Li et al., 2024f). Second, we also introduce a new safety classification setup, where we formalize a range of safety tasks as classification problems. This new setup is aimed at building safer and more reliable AI models by leveraging strong classification capabilities. While existing generative MLLMs typically do not excel at classification tasks (Mitra et al., 2024; Huang et al., 2024), we believe that enhancing their discriminative capabilities is essential for enabling diverse applications and more sophisticated reasoning.\nFor the new safety classification setup, we also apply our Safety Vectors (SVs) approach (See Section 4.3) to our Granite Vision model. Our method is compared with other strong baselines on three public benchmarks: VLGuard (Zong et al., 2024), RTVLM (Li et al., 2024f), and LMM-Halucination (Chen et al., 2024a).\nBenchmarks. (i) VLGuard (Zong et al., 2024) focuses on vision-language safety and identifies four main categories of harmful content: Privacy, Risky Behavior, Deception and Hateful Speech. The dataset consists of images from diverse sources and the instructions are generated by GPT-4V (OpenAI, 2023a) with each safe image having both safe and unsafe instructions, and each unsafe image having a single instruction. (ii) RTVLM (Li et al., 2024f) is the first red teaming dataset to benchmark current MLLMs in terms of several different aspects: faithfulness, privacy, safety, and fairness. In this dataset, 5,200 samples have been annotated"}, {"title": "VLM-as-a-Judge Setup.", "content": "Here, we perform a standard evaluation using the approach of VLM-as-a-Judge in the same manner as described in the RTVLM evaluation procedure (see Table 8 in Li et al. (2024f)). Particularly, we utilize GPT-4V, and the score has a range between [0,10"}]}