{"title": "POLICY DECORATOR: MODEL-AGNOSTIC ONLINE REFINEMENT FOR LARGE POLICY MODEL", "authors": ["Xiu Yuan", "Tongzhou Mu", "Stone Tao", "Yunhao Fang", "Mengke Zhang", "Hao Su"], "abstract": "Recent advancements in robot learning have used imitation learning with large models and extensive demonstrations to develop effective policies. However, these models are often limited by the quantity, quality, and diversity of demonstrations. This paper explores improving offline-trained imitation learning models through online interactions with the environment. We introduce Policy Decorator, which uses a model-agnostic residual policy to refine large imitation learning models during online interactions. By implementing controlled exploration strategies, Policy Decorator enables stable, sample-efficient online learning. Our evaluation spans eight tasks across two benchmarks-ManiSkill and Adroit\u2014and involves two state-of-the-art imitation learning models (Behavior Transformer and Diffusion Policy). The results show Policy Decorator effectively improves the offline-trained policies and preserves the smooth motion of imitation learning models, avoiding the erratic behaviors of pure RL policies. See our project page for videos.", "sections": [{"title": "1 INTRODUCTION", "content": "Encouraged by the recent success of large language and vision foundation models (Brown et al., 2020; Kirillov et al., 2023), the field of robot learning has seen significant advances through imitation learning (particularly behavior cloning), where large models leverage extensive robotic demonstrations to develop effective policies (Bousmalis et al., 2023; Brohan et al., 2022; 2023; Ahn et al., 2022). Despite these advancements, the performance of learned models is limited by the quantity, quality, and diversity of pre-collected demonstration data. This limitation often prevents models from handling all potential corner cases, as demonstrations cannot cover every possible scenario (e.g., test-time objects can be entirely different from training ones). Unlike NLP and CV, scaling up demonstration collection in robotics, such as RT-1 (Brohan et al., 2022) and Open X-Embodiment (Collaboration, 2023), requires extensive time and resources, involving years of data collection by numerous human teleoperators, making it costly and time-consuming. In contrast, cognitive research indicates that infants acquire skills through active interaction with their environment rather than merely observing others (Saylor & Ganea, 2018; Sheya & Smith, 2010; Adolph et al., 1997; Corbetta, 2021). This raises a natural question: Can we further improve an offline-trained large policy through online interactions with the environment?\nThe most straightforward approach to improving an offline-trained imitation learning policy is to fine-tune it using reinforcement learning (RL) with a sparse reward (Kumar et al., 2022; Yang et al., 2023a). However, several challenges hinder this strategy. Firstly, many state-of-the-art imitation"}, {"title": "2 RELATED WORKS", "content": "Learning from Demo Learning control policies through trial and error can be inefficient and unstable, prompting research into leveraging demonstrations to enhance online learning. Demonstrations can be utilized through pure offline imitation learning, including behavior cloning (Pomerleau, 1988) and inverse reinforcement learning (Ng et al., 2000). Alternatively, demonstrations can be incorporated during online learning, serving as off-policy experience (Mnih et al., 2015; Hessel et al., 2018; Ball et al., 2023; Nair et al., 2018) or for on-policy regularization (Kang et al., 2018; Rajeswaran et al., 2017). Furthermore, demonstrations can be used to estimate reward functions for RL problems (Xie et al., 2018; Aytar et al., 2018; Vecerik et al., 2019; Zolna et al., 2020; Singh et al., 2019). When the offline dataset includes both demonstrations and negative trajectories, offline-to-online RL approaches first apply offline RL to learn effective policy and value initializations from offline data, followed by online fine-tuning (Nair et al., 2020; Kostrikov et al., 2021; Lyu et al., 2022; Nakamoto et al., 2024). In this work, we adopt a more direct approach to utilize demonstrations: distilling demonstrations into a large policy model and subsequently improving it through online interactions.\nResidual Learning The concept of learning residual components has been widely applied across various domains, including addressing the vanishing gradient problem in deep neural networks (He et al., 2016; Vaswani, 2017) and parameter-efficient fine-tuning (Hu et al., 2021). In robotic control, researchers have employed online RL to learn corrective residual components for various base policies, such as hand-crafted controllers (Johannink et al., 2019), non-parametric models (Haldar et al., 2023b), and pre-trained neural networks (Alakuijala et al., 2021; Ankile et al., 2024). Residual learning can also be achieved through supervised learning (Jiang et al., 2024). Our work focuses on the online improvement of large policy models, identifying residual policy learning as an ideal solution due to its model-agnostic nature. We highlight the uncontrolled exploration issue in vanilla residual RL, propose a set of strategies to address it, and further enhance its efficiency through careful examination of design choices.\nAdvanced Imitation Learning Imitation learning provides an effective approach to teaching robots complex skills. However, naive imitation learning often struggles to model multi-modal distributions within demonstration datasets (Chi et al., 2023). Several advanced methods have been proposed to address this limitation: Shafiullah et al. (2022); Lee et al. (2024); Sridhar et al. (2023); Pari et al. (2021) represent actions as discrete values with offsets, Florence et al. (2022) employs energy-based models, and Chi et al. (2023) leverages diffusion models. While these techniques effectively learn from multi-modal data, they often create models that are non-trivial to fine-tune using RL. Even if they were compatible with RL, the fine-tuning process can be computationally prohibitive due to the large number of parameters in modern policy models. These limitations motivate our approach of using online residual policy learning to improve imitation learning models."}, {"title": "3 PROBLEM SETUP", "content": "In this paper, we focus on improving an offline-trained large policy (referred to as \"base policy\") through online interactions. We make the following assumptions:"}, {"title": "4 POLICY DECORATOR: MODEL-AGNOSTIC ONLINE REFINEMENT", "content": "In this work, our goal is to online improve a large policy model, which is usually offline-trained by imitation learning and usually has some specific designs in model architecture. To this end, we propose Policy Decorator, a model-agnostic framework for refining large policy models via online interactions with environments. Fig. 2 provides an overview of our framework.\nPolicy Decorator is grounded on learning a residual policy via reinforcement learning with sparse rewards, which is described in Sec. 4.1. On top of it, we devise a set of strategies to ensure the RL agent (in combination with the base policy and the residual policy) explores the environment in a controlled manner. Such a controlled exploration mechanism is detailed in Sec. 4.2. Finally, we discuss several important design choices that further enhance learning efficiency in Sec. 4.3."}, {"title": "4.1 LEARNING RESIDUAL POLICY VIA RL", "content": "Given the base policy $\\pi_{base}$, we then train a residual policy $\\pi_{res}$ on top of it using reinforcement learning. The base policy $\\pi_{base}$ can be either deterministic (e.g., Behavior Transformer (Shafiullah et al., 2022)) or stochastic (e.g., Diffusion Policy (Chi et al., 2023)), and it remains frozen during the RL process. The residual policy $\\pi_{res}$ is updated through RL gradients, so it should be a differentiable function compatible with RL gradients. In this work, we model the residual policy $\\pi_{res}$ as a Gaussian policy parameterized by a small neural network (either an MLP or a CNN, depending on the observation modality). To interact with the environment, the actions from both policies are combined by summing their output actions, i.e., the action executed in the environment is $\\pi_{base}(s) + \\pi_{res}(s)$. For stochastic policies, actions are sampled individually from both policies and then summed.\nThe residual policy is trained to maximize the expected discounted return derived from the sparse reward (i.e., the task's success signal). We employ the Soft Actor-Critic (SAC) algorithm (Haarnoja et al., 2018) due to its superior sample efficiency and stability. Several important design choices arise when implementing SAC for learning the residual policy, which we discuss in Sec. 4.3. Our method is also compatible with PPO (Schulman et al., 2017), as illustrated in Appendix D.3."}, {"title": "4.2 CONTROLLED EXPLORATION", "content": "While learning a residual policy by RL can in principle refine a base policy, practical implementation can be challenging. As demonstrated in our experiments (Sec. 5), without constraints, random exploration during RL training often leads to failure in tasks requiring precise control, resulting in no learning signals in sparse reward settings (see this video for an example). To overcome these challenges and enable stable, sample-efficient learning, we propose a set of strategies ensuring the RL agent (in combination with the base policy and the residual policy) explores the environment in a controlled manner. The goal is to make sure the agent continuously receives sufficient success signals while adequately exploring the environment.\nBounded Residual Action When using the residual policy to correct the base policy, we do not want the resulting trajectory to deviate too much from the original trajectory because it usually leads to failure. Instead, we expect the residual policy to only make a bit \"refinement\" at the finer parts of the"}, {"title": "4.3 DESIGN CHOICES & IMPLEMENTATION DETAILS", "content": "We investigated a few important design choices in the implementation of Policy Decorator, with supporting experiments provided in Appendix E.\nInput of Residual Policy The residual policy can receive input in the form of either observation alone or both observation and action from the base policy. Our experiments indicate that using only observation typically produces better results, as illustrated in Fig. 19.\nInput of Critic In SAC, the critic Q(s, a) takes an action as input, and there are several design choices regarding this action: we can use 1) the sum of the base action and residual action; 2) the concatenation of both; or 3) the residual action alone. Based on our experiments shown in Fig. 20, using the sum of both actions yields the best performance."}, {"title": "5 EXPERIMENTS", "content": "The goal of our experimental evaluation is to study the following questions:\n1. Can Policy Decorator effectively refine offline-trained imitation policies using online RL with sparse rewards under different setups (different tasks, base policy architectures, demonstration sources, and observation modalities)? (Sec. 5.3)\n2. What are the effects of the components introduced by the Policy Decorator? (Sec. 5.4)\n3. Does Policy Decorator generate better task-solving behaviors compared to other types of learning paradigms (e.g., pure IL and pure RL)? (Sec. 5.5)"}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "To validate Policy Decorator's versatility, our experimental setup incorporates variations across the following dimensions:\n\u2022 Task Types: Stationary robot arm manipulation, mobile manipulation, dual-arm coordination, dexterous hand manipulation, articulated object manipulation, and high-precision tasks. Fig. 5 illustrates sample tasks from each benchmark.\n\u2022 Base Policies: Behavior Transformer and Diffusion Policy.\n\u2022 Demo Sources: Teleoperation, Task and Motion Planning, RL, and Model Predictive Control.\n\u2022 Observation Modalities: State observation (low-dim) and visual observation (high-dim).\nWe summarize the key details of our setups as follows (further details on the task descriptions, demonstrations, and base policy implementation can be found in Appendix A and B.1)."}, {"title": "5.1.1 TASK DESCRIPTION", "content": "Our experiments are conducted on 8 tasks across 2 benchmarks: ManiSkill (robotic manipulation; 4 tasks), and Adroit (dexterous manipulation; 4 tasks). See Fig. 5 for illustrations.\nManiSkill We consider four challenging tasks from ManiSkill. StackCube and PegInsertionSide demand high-precision control, with PegInsertion featuring a mere 3mm clearance. TurnFaucet and PushChair introduce object variations, where the base policy is trained on source environment objects, but target environments for online interactions contain different objects. These complexities make it challenging for pure offline imitation learning to achieve near-perfect success rates, necessitating online learning approaches. For all ManiSkill tasks, we use 1000 demonstrations provided by the benchmark (Mu et al., 2021; Gu et al., 2023) across all methods. These demonstrations are generated through task and motion planning, model predictive control, and reinforcement learning.\nAdroit We consider all four dexterous manipulation tasks from Adroit: Door, Hammer, Pen, and Relocate. The tasks should be solved using a complex, 24-DoF manipulator, simulating a real hand. For all Adroit tasks, we use 25 demonstrations provided by the original paper (Rajeswaran et al., 2017) for all methods. These demonstrations are collected by human teleoperation."}, {"title": "5.1.2 BASE POLICY MODEL", "content": "We selected two popular imitation learning models as our base policy models for improvement.\nBehavior Transformer (Shafiullah et al., 2022) is a GPT-based policy architecture for behavior cloning. It handles multi-modal action distribution by representing an action as a combination of a cluster center (predicted by a classification head) and an offset (predicted by a regression head). The action cluster centers are determined by k means, which is non-differentiable, thus only the offset can be fine-tuned using RL gradients.\nDiffusion Policy (Chi et al., 2023) is a state-of-the-art imitation learning method that leverages recent advancements in denoising diffusion probabilistic models. It generates robot action sequences through a conditional denoising diffusion process and employs action sequences with receding horizon control. The training of Diffusion Policy requires ground truth action labels to supervise its denoising process; however, these action labels are unavailable in RL setups, making the original training recipe incompatible with RL. Nevertheless, recent approaches have been developed to fine-tune diffusion models using RL in certain scenarios. See Appendix H for a more detailed discussion.\nThe implementation details of these two base policies can be found in Appendix B.1. To further demonstrate the versatility of our method, we also present the results on other types of base policies (including MLP, RNN, and CNN) in Appendix D.1."}, {"title": "5.2 BASELINES", "content": "We compare our approach against a set of strong baselines for online policy improvement, including both fine-tuning-based methods and methods that do not involve fine-tuning. A brief description of each baseline is provided below, with further implementation details available in Appendix B.5."}, {"title": "5.2.1 FINE-TUNING METHODS", "content": "As discussed in Sec. 1, making our base policies compatible with online RL is non-trivial. We implemented several specific modifications to the base policies to enable fine-tuning, as detailed in Appendix B.4. Since we consider the problem of improving large policy models where full-parameter fine-tuning can be costly, we employ LoRA (Hu et al., 2021) for parameter-efficient fine-tuning.\nOur fine-tuning baseline selection follows this rationale: we first choose a basic RL algorithm for each base policy based on their specific properties, which serves as a basic baseline. Additionally, assuming access to the demonstrations used to train the base policies, we consider various learning-from-demonstration methods as potential baselines. Table 1 lists the most relevant learning-from-demo baselines. From these, we select the strongest and most representative methods in each category and implement them on top of the basic RL algorithm we initially selected.\nBasic RL We use SAC (Haarnoja et al., 2018) as our basic fine-tuning method for Behavior Transformer, and use DIPO (Yang et al., 2023b) for Diffusion Policy (see Appendix H.2 for a discussion on other RL methods for Diffusion Policy). For both methods, we initialize the actor with the pre-trained base policy and use a randomly initialized MLP for the critic (refer to Appendix F.5.1 for an ablation study on this design choice). We also noticed a concurrent work (DPPO (Ren et al., 2024)) designed for fine-tuning Diffusion Policy using RL, and we compare our approach with it on their tasks. See Appendix C.1 for more details.\nRLPD (Ball et al., 2023) is a state-of-the-art online learning-from-demonstration method that utilizes demonstrations as off-policy experience. It enhances vanilla SACfd with critic layer normalization, symmetric sampling, and sample-efficient RL techniques.\nROT (Haldar et al., 2023a) is a representative online learning-from-demonstration algorithm that utilizes demonstrations to derive dense rewards and for policy regularization. It adaptively combines offline behavior cloning with online trajectory-matching based rewards.\nCal-QL (Nakamoto et al., 2024) is a state-of-the-art offline-to-online RL method that \"calibrates\" the Q function in CQL (Kumar et al., 2020) for efficient online fine-tuning. In our setting, we use the same demonstration set used in other baselines as the offline data for Cal-QL. Unlike other fine-tuning baselines that initialize the critic randomly, Cal-QL can potentially benefit from the pre-trained critic."}, {"title": "5.2.2 \u039d\u039fN-FINE-TUNING METHODS", "content": "JSRL (Uchendu et al., 2023) is a curriculum learning method that employs a guiding policy to bring the agent closer to the goal. In our setting, the pre-trained base policy serves as the guiding policy.\nResidual RL (Johannink et al., 2019) learns a residual control signal on top of a hand-crafted conventional controller. Unlike our approach, it explores the environment in an entirely uncontrolled manner. For a fair comparison, we replace its hand-crafted controller with our base policies.\nFISH (Haldar et al., 2023b) builds upon Residual RL by incorporating a non-parametric VINN (Pari et al., 2021) policy and learning an online offset actor with optimal transport rewards."}, {"title": "5.3 MAIN RESULTS & ANALYSIS", "content": "Our Approach We evaluate Policy Decorator with Behavior Transformer and Diffusion Policy as base policies, and the results are summarized in Fig. 6 and 7, respectively (see Fig. 1 for a barplot)."}, {"title": "5.4 ABLATION STUDY", "content": "We conducted various ablations on Stack Cube and Push Chair tasks to provide further insights."}, {"title": "5.4.1 RELATIVE IMPORTANCE OF EACH COMPONENT", "content": "We examined the relative importance of Policy Decorator's main components: 1) residual policy learning; 2) progressive exploration schedule; and 3) bounded residual action. We thoroughly evaluated all possible combinations of these components, with results shown in Fig. 9. Each component greatly contributes to the overall performance, both individually and collectively. While residual policy learning establishes the foundation of our framework, using it alone does not sufficiently improve the base policy. Bounded residual action is essential for effective residual policy learning, and the progressive exploration schedule further enhances sample efficiency."}, {"title": "5.4.2 INFLUENCE OF KEY HYPERPARAMETERS", "content": "Bound $\\alpha$ of Residual Actions The hyperparameter $\\alpha$ determines the maximum adjustment the residual policy can make. Fig. 10 illustrates how $\\alpha$ affects the learning process. If $\\alpha$ is too small, the final performance may be adversely affected. Conversely, if $\\alpha$ is too large, it may lead to poor sample efficiency during training. Although certain values achieve optimal sample efficiency, $\\alpha$ values within a broad range (e.g., 0.1 to 0.5 for PushChair and 0.03 to 0.1 for StackCube) eventually converge to similar success rates, albeit with varying sample efficiencies. This indicates that while"}, {"title": "5.4.3 ADDITIONAL ABLATION STUDIES", "content": "Additional ablation studies are provided in Appendix D, with key conclusions summarized as follows:\n\u2022 Policy Decorator also works with other types of base policies (e.g., MLP, RNN, and CNN). D.1\n\u2022 Policy Decorator remains effective when applied to low-performing checkpoints. D.2\n\u2022 Policy Decorator is also effective when using PPO as the backbone RL algorithm. D.3"}, {"title": "5.5 PROPERTIES OF THE REFINED POLICY", "content": "An intriguing aspect of Policy Decorator is its ability to combine the strengths of both Imitation Learning and Reinforcement Learning policies. Previous observations have highlighted that robotic policies trained solely by RL often exhibit jerky actions, rendering them unsuitable for real-world application (Qin et al., 2022). Conversely, policies derived from demonstrations, whether from human teleoperation or motion planning, tend to produce more natural and smooth motions. However, the performance of such policies is constrained by the diversity and quantity of the demonstrations.\nOur refined policy, learned through Policy Decorator, achieves remarkably high success rates while retaining the favorable attributes of the base policy. This is intuitive \u2013 by constraining residual actions, the resulting trajectory maintains proximity to the original trajectory, minimizing deviation.\nComparison with RL policies reveals that our refined approach exhibits significantly smoother behavior (see videos here). Furthermore, when compared with offline-trained base policies, our refined policy shows superior performance, effortlessly navigating through the finest part of the task (shown in this video), while preserving its multi-modal property (see Appendix J for details)."}, {"title": "6 CONCLUSIONS, DISCUSSIONS, & LIMITATIONS", "content": "We propose the Policy Decorator framework, a flexible method for improving large behavior models using online interactions. We introduce controlled exploration strategies that boost the base policy's performance efficiently. Our method achieves near-perfect success rates on most tasks while preserving the smooth motions typically seen in imitation learning models, unlike the jerky movements often found in reinforcement learning policies.\nLimitations Enhancing large models with online interactions requires significant training time and resources. While learning a small residual policy reduces computational costs compared to fully fine-tuning the large model, the process remains resource-intensive, especially for slow-inference models like diffusion policies. We found that only a few critical states need adjustment. Future research could focus on identifying and correcting these points more precisely to improve efficiency."}]}