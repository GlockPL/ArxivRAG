{"title": "QUALITY SCALABLE QUANTIZATION\nMETHODOLOGY FOR DEEP LEARNING ON\nEDGE", "authors": ["Salman Abdul Khaliq", "Dr. Rehan Hafiz"], "abstract": "Abstract-Deep Learning Architectures employ heavy\ncomputations and bulk of the computational energy is taken up by\nthe convolution operations in the Convolutional Neural Networks.\nThe objective of our proposed work is to reduce the energy\nconsumption and size of CNN for using machine learning\ntechniques in edge computing on ubiquitous computing devices.\nWe propose Systematic Quality Scalable Design Methodology\nconsisting of Quality Scalable Quantization on a higher\nabstraction level and Quality Scalable Multipliers at lower\nabstraction level. The first component consists of parameter\ncompression where we approximate representation of values in\nfilters of deep learning models by encoding in 3 bits. A shift and\nscale based on-chip decoding hardware is proposed which can\ndecode these 3-bit representations to recover approximate filter\nvalues. The size of the DNN model is reduced this way and can be\nsent over a communication channel to be decoded on the edge\ncomputing devices. This way power is reduced by limiting data bits\nby approximation. In the second component we propose a quality\nscalable multiplier which reduces the number of partial products\nby converting numbers in canonic sign digit representations and\nfurther approximating the number by reducing least significant\nbits. These quantized CNNs provide almost same ac-curacy as\nnetwork with original weights with little or no fine-tuning. The\nhardware for the adaptive multipliers utilize gate clocking for\nreducing energy consumption during multiplications. The\nproposed methodology greatly reduces the memory and power\nrequirements of DNN models making it a feasible approach to\ndeploy Deep Learning on edge computing. The experiments done\non LeNet and ConvNets show an increase upto 6% of zeros and\nmemory savings upto 82.4919% while keeping the accuracy near\nthe state of the art.", "sections": [{"title": "I. INTRODUCTION", "content": "HIS Machine Learning techniques such as \"Deep\nLearning\" [1] have enabled the most remarkable\ninnovations in artificial intelligence. Deep Neural\nNetworks find their usage in novel areas such as human activity\nrecognition, image to text translation and autonomous driving\ncars [1]. However, these impressive methods are much costly in\nterms of computations. Training a modern DNN architecture is\nessentially an exascale computational task where a high\nperformance compute system is required. Training of these\nnetworks implemented using the cloud or a cluster of high\nperformance GPUs that can consume power varying from\nhundreds to thousands of megawatts [2]. On the other hand\ntrained DNNs are deployed in power-constrained embedded\nplatforms for using in different applications and feasibility of\nrunning deep learning networks on edge is also under\nconsideration.\nThe energy efficiency is equally important for training using\nhigh performance computing systems as much as it is for\ndeploying deep learning on mobile platforms or embedded\ndevices. The architectural strategies for increasing resource\nutilization and reducing off-chip memory accesses include\nusage of systolic array [10], parallel computing architecture\n[11], tiling etc. and algorithmic improvements include pruning\n[12], quantization [13], efficient gradient descent [14] etc. to\nreduce the overall size of the deep learning models and speeding\ncomputations. Large deep learning models require large\nmemory for model storage and high memory bandwidth for\ncomputations of the convolution operations.\nMemory utilization is therefore an important factor for\ncomputing devices. The off-chip memory accesses effect the\noverall power requirements along with the computational\nenergy consumed by resources."}, {"title": "II. LITERATURE SURVEY", "content": "In this section we have described some of state of the art\nalgorithm level optimization strategies which we have\nsurveyed. Table 1 below shows the various state of the art\nalgorithm level optimization techniques. The two major\ntechniques aim to reduce the computational cost and compress\nthe DNN model size. The classification of these techniques is\nmade on the basis of how the bit-width is reduced which\nrepresents the classification filter values.\nThe DNN models are able to classify the input data into various\nclasses after training the values in filters of convolution layers.\nThe filters in convolution layers may have many number of\nweight matrices where each matrix may have many channels\nknown as depth depending upon the output of the previous\nlayer. Weighted quantization techniques are a set of methods\nwhich aim to minimize following optimization program\n$J(B,a) = min||W \u2013 \u03b1\u0392||^2$ (1)\nThis is a generic optimization for achieving quantization. Here\na represents a scaling number which can be any real number.\nIn case of XNOR-Net [22] B represents a vector such that $B\u2208\n{1, -1}^n$, n here is the length equal to the number of values in\nW. W represents a single dimensional vector containing values\nof convolution filters. This optimization program solution for"}, {"title": "III. PROPOSED METHODOLOGY", "content": "In the previous chapter we discussed how state of the art\ntechniques provided energy efficient deep learning methods.\nThe algorithm level techniques introduced pruning, binary and\nternary quantization and dynamic bit widths which reduce\nmodel parameters and computations. These techniques\nprovided energy optimization but quality scalable techniques\nwere needed to configure energy requirements, so that various\narchitectures with different computational resources could be\nused for deep learning. This quality scalability is also useful for\nedge computing environments where user end devices vary in\ncompute capability. In this chapter we provide framework for\nquality scalable methodology for deep learning which\nincorporate scalable quantization."}, {"title": "A. Framework for Quality Scalable Methodology for Deep\nLearning Models", "content": "In this framework we first of all choose a deep learning model\nwhich we need to quantize to achieve parameter reduction. The\ntraining of the model is carried out using Keras library for deep\nlearning, after the training is complete we extract the trained\nfilter weights to quantize. The trained weights are divided into\nvectors of length N, for each vector we assume a Gaussian\ndistribution for which we compute mean, and a relative scalar\na. We also compute standard deviation for positive and negative\nvector values represented by op and on respectively. Then we\nquantize the weights for different quantization levels, for this\nexperiment we considered three quantization levels. After\nquantization we fine-tuned the model if required. Then the\nquality scalable multipliers are employed for convolution\noperations of scaled quantized weights with input data in\nforward path. The blocks with blue background is our\ncontribution and white blocks represent tools which are\navailable. Cylindrical blocks represent a container for values,\nrhombus represents decision and oval blocks represent\noperations.\nAt each convolution layer. The options available for\nquantization of values in the filter are analyzed. The available\npoints in design space which gives lowest error in accuracy and\nminimum memory footprint are determined. The shape of the\ndistribution of trained weight values is used to determine the\nvariance using Maximum Likelihood Estimation.\nThe thresholds and configuration of minimum Quantization\nLevels are then determined by exhaustive search. After\ndetermination of hyper parameters trained weights are\nconverted into quantized values. The accuracy of the DNN\narchitecture is computed."}, {"title": "B. Determining Weights for different Quantization Levels", "content": "Quality Scalable Quantization introduces different quantization\nlevels for better representation of filter values of pre-trained\nnetworks. We assume a filter W with dimensions $b\u00d7h\u00d7c$,\nwhere b, h and c represent breadth, height and number of\nchannels respectively. Let i be the number of filters with depth\nof c channels. From here Filter matrix of each channel has b\nbreadth and h height, for each value located at w \u00d7 h across all\nchannels are formed in a vector we call Wi. We need to find the\nclosest 12 displacement between quantized and original\nweights, which can be mathematically represented by equation\n3.1. Here the optimization program for minimizing error\nbetween quantized and original trained weights is given\n$J(\u03b2, \u03b1) = min||W\u2081 \u2013 ai\u03b2i||^2$ (5)\nHere a\u2081 represents the scalar value associated with each vector\nand B\u2081 represents a vector containing quantization values\nexponentially increasing with powers of 2 such that\n$\u03b2\u2081 \u2208 {+4, +2, +1,0, -1, -2, -4}\u00ba$ (6)\nwhere c are the number of channels in a convolution layer. The\nvariance of the data in the filter channels vary from one filter to\nanother and we estimate the variance and mean of the vector\nusing the \"Maximum Likelihood Estimation\u201d (MLE). The\nshape of PDF of the vector are assumed to be Guassian. The"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We tested our methodology on LeNet and ConvNet\narchitectures on MNIST and Cifar-10 datasets respectively\nusing three different configuration modes. We used Keras\nlibraries for training and inference for quantized and normal\nDNN models. We have performed various experiments to\ndetermine the feasibility of quantization and demonstrated\nquality scalability for CNN models."}, {"title": "A. Effect of Quantization on Classification Accuracy", "content": "The classification accuracy of LeNet Architecture on MNIST\ncame out to be 98.86% when trained from scratch. The\nQuantization was performed on the LeNet Architecture, and\nwithout any fine-tuning we achieved an accuracy of 97.59%\nwhich is a good enough approximation. The results on LeNet\nArchitecture on MNIST dataset are summarized in Table 3\nbelow. The fine-tuning of FC layer only increases the accuracy\nupto 98.35%.\nThe parameters of LeNet model were reduced upto 82.4919%\nand quantization also resulted in increased the number of zeros\nupto 6% as compared to the non-quantized trained model of\nLeNet."}, {"title": "B. Evaluation of Quality Scalable Methodology", "content": "We have evaluated quality scalability by testing three different\nquantization levels. The weights are quantized by changing the\nparameter \u00d8. We have tested for \u00d8 values of 1,2 and 4 and\ncorresponding values of a,\u1e9e and \u03c3. We evaluated two deep\nlearning architectures, LeNet and 4 layer ConvNet on MNIST\nhand written number recognition and Cifar-10 datasets\nrespectively. The accuracy varies as shown in figure 4.1 after\nquality scalable quantization on LeNet. The quantization for \u00d8\nvalues of 1, 2 and 4 correspond to data points {+1,-1}, {+2,-2}"}, {"title": "C. Design Space based on Energy Efficiency and CNN\nAccuracy", "content": "The energy required for moving 32 bits from DRAM to\ncomputing chip which can be CPU, ASIC or FPGA is given as\n6400pJ [8]. We have computed how much energy is required\nfor moving encoded filter values which are quantized and for\nmoving original weights. For calculating number of bits in non-\nquantized model equation 11 is used. The equation for\ncalculating total number of bits in ternary weights or 2 bit\nencoded and higher quantization levels or 3 bit encoded are\ngiven by equation 12.\n$NBits\u2081 = (FPB \u00d7 H\u00a1 \u00d7 W\u00a1 \u00d7 C\u00a1 \u00d7 Num\u2081)$ (11)\n$NBitsi(imp) = (BE \u00d7 H\u00a1 \u00d7 W \u00d7 C\u00a1 \u00d7 Num\u012f)\n+ (H \u00d7 W \u00d7 C\u00a1 \u00d7 FPB)$ (12)\nHere H and W represent the height and width of ith convolution\nlayer weights, Num represents number of filters, C represents\nnumber of channels and FPB means Full Precision Bits which\nin our case is assumed to be 32. For various vector lengths (N),\nwe demonstrated that energy efficiency for 2 bit coded although\nis higher than 3 bit coded models, still there remains a\nsignificant difference of accuracies between these two types of\nencoding schemes.\nThe number of bits required to save the ternary weights is 2, so\n2 Bit-Encoding (BE) is sufficient for representing ternary\nweights and 3 Bit-Encoding (BE) is required to represent the\nfilter weights for \u00d8 up to 4. In Figure 9 we have plotted various\ndesign points with respect to accuracy and energy savings\ncomputed from the method we mentioned earlier. These design\npoints are computed for varying vector lengths N of 2, 4, 8, 16,\n32 and 64 values. The energy efficiency for ternary weights in\nterms of energy savings is slightly more than the higher\nquantization models, however this slight advantage has a much\nhigher cost in terms of quality."}, {"title": "V. DISCUSSION", "content": "In this section we discuss the usability of quantization\nmethodology in terms of hardware optimization and discuss\nhow quality scalability can be introduced at microarchitecture\nlevel using approximate multipliers."}, {"title": "A. Compression of CNN Model Parameters", "content": "There can be many advantages of quantized deep learning\nmodels. For example in quantized CNN models, the weights\nwhich are quantized can be related to shifted or inverted\nversions of a single scalar. Therefore only a single scalar is\nrequired to be fetched from main memory along with the\ninformation about how much left or right shifting is required. The reduction in DNN model size depends on the number of\nchannels or depth of each convolution layer, the more depth\nresults in more values in a single filter vector. Which in turn\nreduces the number of bits to represent filter weights. Following\nfigure shows reduction in the model size after quantization. Here N represents the depth of the filter and W represents a\nsingle value in filter."}, {"title": "B. Quality Scalable Multipliers", "content": "The quality scalable methodology can also be complemented\nby using approximate multiplier technique on microarchitecture\nlevel. Since quality scalable design methodology is targeted\ntowards edge computing devices, low power multipliers can\nachieve high energy savings also with quality scalable\nproperties. We can convert the binary data in Canonic Sign\nDigit (CSD) format and further increase the benefit in quality\nscalable design. The Canonic Sign Digit (CSD) representation\nreduces the number of non-zeros in any binary number and thus\nif the multiplier is replaced with its CSD it results in less\nnumber of partial products. A quality scalable multiplier can\nreduce the number of partial products in the CSD representation\nto provide with an approximate answer with a trade off with\nenergy consumption.\nThe Deep Learning model contain convolution layers which\nrequire immense multiply and accumulate operations. However\nbecause of resilience of these models to slight error or noise\n[24] reducing the least significant bits from the multiplier would\nnot result in significant reduction in classification accuracy. This claim is supported by Fig. 10 below which shows that few\nnumber of non-zeros are required to accurately represent most\nof the values in AlexNet model filters.\nThese statistics were obtained using MATLAB's fi library for\nopen source trained architecture of AlexNet CNN. Furthermore\nif we use CSD representation of weights we can further reduce\nthe number of non-zero bits for representation of values of filter\nweights. If the number of non-zeros are restricted for quality\nscalable architecture design, we can reduce number of partial\nproducts generated in multiplications and without much quality\ndegradation and we can achieve inference at very low-power."}, {"title": "VI. CONCLUSION", "content": "In conclusion we proposed a Systematic Quality Scalable\nDesign Methodology for mobile devices to perform machine\nlearning operations in edge computing. The design\nmethodology consists of Quality Scalable Quantization Scheme\nat a higher abstraction level. We reduced model size using\nquantization across filters and channels with which we achieved\nup to 82.49% reduction in model size for LeNet with almost\nsame accuracy as the original DNN. For 4 layer ConvNet with\n2 bit encoding we achieved 91.95% energy efficiency compared\nwith the original model in terms of DRAM accesses with\n68.47% classification accuracy. 3 bit encoding was able to\nachieve 73.28% accuracy with 88.82% energy efficiency with\nrespect to original CNN. We demonstrated that increased\nquantization methodology despite requiring 3 bit encoding as\nopposed to 2 bit encoding for ternary quantization, gave far\nbetter accuracies which provided a good energy saving to\naccuracy ratio overall. We have also shown a general trend how\nquantization levels can scale quality of deep learning models."}]}