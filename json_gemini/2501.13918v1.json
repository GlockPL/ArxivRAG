{"title": "Improving Video Generation with Human Feedback", "authors": ["Jie Liu", "Gongye Liu", "Jiajun Liang", "Ziyang Yuan", "Xiaokun Liu", "Mingwu Zheng", "Xiele Wu", "Qiulin Wang", "Wenyu Qin", "Menghan Xia", "Xintao Wang", "Xiaohong Liu", "Fei Yang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Yujiu Yang", "Wanli Ouyang"], "abstract": "Video generation has achieved significant advances through rectified flow techniques, but issues like unsmooth motion and misalignment between videos and prompts persist. In this work, we develop a systematic pipeline that harnesses human feedback to mitigate these problems and refine the video generation model. Specifically, we begin by constructing a large-scale human preference dataset focused on modern video generation models, incorporating pairwise annotations across multi-dimensions. We then introduce VideoReward, a multi-dimensional video reward model, and examine how annotations and various design choices impact its rewarding efficacy. From a unified reinforcement learning perspective aimed at maximizing reward with KL regularization, we introduce three alignment algorithms for flow-based models by extending those from diffusion models. These include two training-time strategies: direct preference optimization for flow (Flow-DPO) and reward weighted regression for flow (Flow-RWR), and an inference-time technique, Flow-NRG, which applies reward guidance directly to noisy videos. Experimental results indicate that VideoReward significantly outperforms existing reward models, and Flow-DPO demonstrates superior performance compared to both Flow-RWR and standard supervised fine-tuning methods. Additionally, Flow-NRG lets users assign custom weights to multiple objectives during inference, meeting personalized video quality needs.", "sections": [{"title": "1. Introduction", "content": "Recent advances in video generation have yielded powerful models (Kong et al., 2024; Polyak et al., 2024; Kuaishou, 2024; Brooks et al., 2024), capable of producing videos with convincing details and relatively coherent motion. Despite these notable achievements, current video generation systems still face persistent challenges, including unstable motion, imperfect alignment with user prompts, unsatisfactory visual quality, and insufficient adherence to human preferences (Zeng et al., 2024). In language modeling and image generation, learning from human preference (Ouyang et al., 2022; Zhou et al., 2023; Wallace et al., 2024) has proved highly effective in improving the response quality and aligning generative models with user expectations.\nHowever, applying such preference-driven alignment strategies to video generation remains in its infancy. One key obstacle is the lack of large-scale, high-quality preference data. A pioneering work (He et al., 2024) introduces a human-rated video preference dataset, and concurrent studies (Wang et al., 2024b; Liu et al., 2024b; Xu et al., 2024a) have contributed additional annotations. Yet, these datasets primarily focus on videos generated by earlier open-source models, which are often of relatively low quality, contain artifacts, or remain limited in duration. With video generation techniques rapidly advancing, the gap between such datasets and the capabilities of state-of-the-art video diffusion models (VDMs) has become increasingly pronounced.\nA second challenge arises from the internal mechanisms of cutting-edge video generation models. Many modern systems employ rectified flow (Liu et al., 2022; Lipman et al., 2022), predicting velocity rather than noise. Recent studies (Wang et al., 2024b; Zhang et al., 2024a; Liu et al., 2024b; Xu et al., 2024a) have tested DPO (Rafailov et al., 2024; Wallace et al., 2024) and RWR (Peng et al., 2019; Lee et al., 2023; Furuta et al., 2024) on diffusion-based video generation approaches. However, adapting existing alignment methods to these flow-based models introduces new questions. While concurrent work (Domingo-Enrich et al., 2024) extended Diffusion-DPO to flow-based image generation, the aligned flow-based models in these explorations underperformed their unaligned counterparts."}, {"title": "2. Related Works", "content": "Evaluation and Reward Models. Evaluation models and reward models play a pivotal role in aligning generative models with human preferences. Earlier approaches and benchmarks (Huang et al., 2023; Liu et al., 2024c; Huang et al., 2024) relied on metrics like FID (Heusel et al., 2017) and CLIP scores (Radford et al., 2021) to assess visual quality and semantic consistency. Recent works (Wu et al., 2023a; Xu et al., 2024b; Kirstain et al., 2023; Zhang et al., 2024b; Liang et al., 2024a) have shifted towards utilizing human preference datasets to train CLIP-based models, enabling them to predict preference scores with improved accuracy. With the advent of large vision-language models (VLMs) (Achiam et al., 2023; Wang et al., 2024a), their powerful capabilities in visual understanding and text-visual alignment make them a natural proxy for reward modeling. A common approach involves replacing the token classification head of VLMs with a regression head that predicts multi-dimensional scores for diverse evaluation tasks.\nTwo main learning paradigms have emerged based on the type of human annotation. The first paradigm relies on point-wise regression, where the model learns to fit annotated scores (He et al., 2024) or labels (Xu et al., 2024a) directly. Another paradigm focuses on pair-wise comparisons, leveraging Bradley-Terry (BT) (Bradley & Terry, 1952) loss to model relative preferences, which is largely unexplored for video reward model. Beyond these methods, some works (Wang et al., 2024b) also leverage the intrinsic reasoning capabilities of VLMs through VLM-as-a-judge (Li et al., 2023; Lin et al., 2024b), where VLMs are adopted to generate preference judgments or scores in textual format through instruction tuning. Despite these promising advances, most existing video reward models primarily focus on legacy video generation models, typically from the pre-Sora (OpenAI, 2024) era, which are constrained by short video durations and relatively low quality. Furthermore, the design and technical choices underlying the vision reward models remain underexplored. Our work seeks to address these limitations by focusing more on modern video generation models and investigating a broader range of reward modeling strategies.\nAlignment for Image & Video Generation. In large language models, Reinforcement Learning from Human Feedback (RLHF) improves alignment with human preferences, enhancing response quality (Ouyang et al., 2022; Jaech et al., 2024). Similar methods have been applied to image generation, using reward models or human preference data to align pretrained models. Key approaches include: (1) direct"}, {"title": "3. VideoReward", "content": "3.1. Human Preference Data Collection\nExisting human preference datasets for video generation (Liu et al., 2024c; Huang et al., 2024; He et al., 2024; Wang et al., 2024b; Xu et al., 2024a) primarily consist of videos generated by earlier open-source models, which are often characterized by low quality, noticeable artifacts, and short video durations. As advancements in video diffusion models (VDMs) continue to elevate the state-of-the-art, the gap between current preference datasets and the capabilities of modern VDMs has become increasingly pronounced. To bridge this gap, we focus on developing a reward model tailored for the latest VDMs. Inspired by Zhang et al. (2024b), we collect a diverse set of prompts from the internet, organiz-"}, {"title": "3.2. Reward Model Learning", "content": "Following prior works that leverage Vision-Laguage Models (VLMs) for related tasks (He et al., 2024; Wang et al., 2024b; Xu et al., 2024a), we adopt Qwen2-VL-2B (Wang et al., 2024a) as the base model for our video reward framework. While existing studies have demonstrated the effective application of reward models in both evaluation (Wu et al., 2023b;a; He et al., 2024) and optimization (Lee et al., 2023; Wallace et al., 2024; Xu et al., 2024b; Prabhudesai et al., 2024), the underlying design choices remain insufficiently explored. We start with a Bradley-Terry style reward model as our baseline, exploring how technical decisions affect the video reward model's performance.\nScore Regression v.s. Bradley-Terry. We first explore the merits of adopting a Bradley-Terry pairwise comparison approach compared to a pointwise score regression approach for training reward models. The BT model (Bradley & Terry, 1952) uses a pairwise log-likelihood loss to capture the probability that one video is preferred over another, enabling the prediction of rewards based on the relative preferences. Conversely, the pointwise regression model directly estimates the absolute quality scores by minimizing the MSE loss. The loss functions for these methodologies are formally defined as follows:\n$L_{BT} = -E_{(y, x_0^w, x_0^l) \\sim D} [log (\\sigma (r(x_0^w, y) \u2013 r(x_0^l, y)))]$ (1)\n$L_{reg} = E_{(y, x_0, z) \\sim D} [||r(x_0, y) \u2013 z||^2]$ (2)\nIn these formulations, $(y, x_0^w, x_0^l)$ denotes pairwise annotations, where y is the input prompt and $x_0^w$ and $x_0^l$ are the two generated videos with corresponding preference annotations. Meanwhile, $(y, x_0, z)$ represents pointwise"}, {"title": "4. Video Alignment", "content": "Let $x_0 \\sim q(x_0)$ denote a real data sample drawn from the true data distribution, and let $x_1 \\sim p(x_1)$ denote a noise sample, where $x_0, x_1 \\in R^d$. Recent advanced image-generation models (e.g., (Esser et al., 2024)) and video-generation models (e.g., (Kong et al., 2024)) adopt the Rectified Flow framework (Liu et al., 2022), which defines the"}, {"title": "4.1. Flow-DPO", "content": "Consider a fixed dataset $D = {y,x_0^w,x_0^l}$, where each sample consists of a prompt y and two videos $x_0^w$ and $x_0^l$ generated by a reference model $p_{\\theta_{\\text{ref}}}$, with human annotations indicating that $x_0^w$ is preferred over $x_0^l$ (i.e., $x_0^w > x_0^l$). The goal of RLHF is to learn a conditional distribution $p_{\\theta} (x_0 | y)$ that maximizes a reward model $r(x_0, y)$, while controlling the regularization term (KL-divergence) from the reference model $p_{\\theta_{\\text{ref}}}$ via a coefficient \u03b2:\n$\\max_{p_{\\theta}} E_{y \\sim D_c, x_0 \\sim p_{\\theta}(x_0|y)} [r(x_0, Y)] - \u03b2D_{KL} [p_{\\theta} (x_0 | Y) || p_{\\theta_{\\text{ref}}}(x_0 | Y)]$ (7)\nIn this context, $p_{\\theta_{\\text{ref}}}$ corresponds to $p_{\\theta_{\\text{ref}},0}$ and $p_{\\theta}$ corresponds to $p_{\\theta,0}$. For the sake of simplicity, the timestep subscripts are omitted. However, RLHF is usually unstable and resource intensive (Rafailov et al., 2024). To address this issue, Diffusion-DPO aligns diffusion models with human preferences by directly solving the above RLHF objective (Eq. 7) analytically. It interprets alignment as a classification problem, and optimizes a policy to satisfy human preferences by supervised training. For simplicity, we omit the conditioning prompt y in the following equations. The Diffusion-DPO objective $L_{DD} (\\theta)$ is given by:\n$-E_{(x_0^w, x_0^l) \\sim D} [log \\sigma(\\frac{\\||\\epsilon^l - \\epsilon_{\\theta}(x, t)\\|^2 - \\|\\epsilon^w - \\epsilon_{\\theta}(x, t)\\|^2}{2}-\\frac{(\\||\\epsilon^l - \\epsilon_{\\text{ref}}(x, t)\\|^2 - \\|\\epsilon^w - \\epsilon_{\\text{ref}}(x, t)\\|^2)}{2}))]$ (8)\nwhere $x = (1-t) x_0 + t \\epsilon, \\epsilon \\sim N(0, I)$. The superscript \"*\" indicates either \"w\" (for the preferred data) or \"l\" (for the less preferred data). The expectation is taken over samples {$x_0^w, x_0^l$} ~ D and the noise schedule t. In Rectified Flow, we relate the noise vector $ \\epsilon^*$ to a velocity field $v^*$. Specifically, Lemma A.1 in Appendix A shows that\n$\\||\\epsilon^* - \\epsilon_{\\text{pred}}(x, t) \\|^2 = (1 - t)^2\\|\\|v^* \u2013 v_{\\text{pred}}(x,t)\\|^2,$ (9)"}, {"title": "4.2. Flow-RWR", "content": "Drawing inspiration from the application of Reward-weighted Regression (RWR) (Peters & Schaal, 2007) in diffusion models (Lee et al., 2023; Furuta et al., 2024), we propose a counterpart for flow-based models based on expectation-maximization. Starting from the general KL-regularized reward-maximization problem in Eq. 7, prior work (Rafailov et al., 2024; Peters & Schaal, 2007) shows that its optimal closed-form solution can be written as\n$p_{\\theta} (X_o | y) = \\frac{1}{Z(y)} p_{\\theta_{\\text{ref}}}(x_o | y) \\exp (\\frac{r(x,y)}{\u03b2}),$ (11)\nwhere $Z(y) = \\Sigma_{xo} p_{\\theta_{\\text{ref}}} (x_o | y) \\exp(r(x_0,y))$ is the partition function. Following (Furuta et al., 2024), we can"}, {"title": "4.3. Noisy Reward Guidance", "content": "Recall that Eq. 7 has a closed-form solution as presented in Eq. 11. This expression indicates that the goal of RLHF is to transform the original distribution $p_{\\theta_{\\text{ref}}} (x_0 | y)$ into the new target distribution $p_{\\theta} (x_0 | y)$. Since the constants \u03b2 and w can be absorbed into r(x, y), Eq. 11 becomes\n$p_{\\theta} (x_oy) \\propto p_{\\theta_{\\text{ref}}} (x_o | y) [\\exp(r(x_o, y))]^w,$ (14)\nwhere w \u2208 R controls the strength of the reward guidance. Notably, this formulation aligns with the concept of classifier guidance (Dhariwal & Nichol, 2021; Song et al., 2020), so we refer to it as reward guidance.\nAs we proved in Appendix A.2, for Rectified Flow, if we set the marginal velocity field to\n$v_t (x_t | y) = v_t (x_t | Y) - \\frac{w}{1-t} v_r(x, y),$ (15)\nthe marginal distribution is modified from $p_t (x_t | y)$ to\n$p_t(x_t | y) \\propto p_t(x_t | y) [\\exp(r(x_t, Y))]^w$.\nWhen t \u2192 0, the resulting samples are drawn from Eq. 14. We provide a pseudo-code in Appendix E. To producing meaningful reward values for noised videos $x_t$, we train a time-dependent reward function. We use the same dataset D = {y,xw,xl} described in Section 3.2, where $x^w$ and $x^l$ represent two videos with different preference levels. In order to train our reward function, we assume that applying"}, {"title": "5. Experiments", "content": "In this section, we empirically evaluate our reward model and reinforcement learning alignment algorithms on Text-to-Video (T2V) tasks.\n5.1. Reward Learning\nTraining Setting. We utilize Qwen2-VL-2B (Wang et al., 2024a) as the backbone for our reward model and train it with BTT loss. To finetune the model, LoRA (Hu et al., 2021) is applied to update all linear layers in the language model, while the vision encoder's parameters are fully optimized. The training process is conducted with a batch of 32 and a learning rate of 2 \u00d7 10-6, with the model trained over two epochs. This setup requires approximately 72 A800 GPU hours. Several observations were made during training. First, higher video resolution and more frames generally improved the reward model's performance. Second, using a stable sampling interval instead of a fixed frame number"}, {"title": "5.2. Video Alignment", "content": "Training Setting. Our pretrained model $p_{\\theta_{\\text{ref}}}$ is a transformer-based latent flow model trained using rectified flow. In all alignment experiments, we applied LORA to fine-tune the transformer models' linear layers, as our findings indicate that full parameter fine-tuning can degrade the model's performance or potentially lead to model collapse. For supervised fine-tuning (SFT), we utilize only the \"chosen data.\" In the RWR experiments, we first calculate the mean and variance of the rewards across all training samples and normalize the rewards to have a mean of 0 and a variance of 1. For the VDM reward model, we extract and concatenate the features from the first 20 transformer blocks of our VDM. These concatenated features are then fed into three fully connected heads, which output a three-dimensional reward. All parameters of the first 20 transformer blocks are trained. For the training dataset, following the approach of Rafailov et al. (Rafailov et al., 2024), we employ VideoReward as the ground-truth reward model to simulate human feedback and relabel our training dataset. Consequently, T2V models trained on this synthetically relabeled dataset can be reliably evaluated using VideoReward, ensuring a fair assessment of their performance. More experimental details, including LoRA architectures, VDM reward model and final hyperparameter settings, are provided in the Appendix F.\nEvaluation. We evaluate performance using both automatic and human assessments. The automatic evaluation comprises the win rate, as determined by VideoReward, and the Vbench score. The win rate is calculated by having VideoReward assign scores to outputs from both the pretrained and aligned models, then determining the proportion of instances where the aligned model's reward exceeds that of the pretrained model. Since VideoReward generates continuous floating-point scores, ties do not occur. Vbench serves as a fine-grained benchmark for evaluating T2V models from two main perspectives: Quality and Semantic. The Quality aspect focuses on the perceptual quality of the synthesized video without considering the input condition, corresponding to our VQ and MQ metrics. The Semantic aspect assesses whether the synthesized video aligns with the"}, {"title": "6. Conclusion", "content": "We have constructed a large-scale preference dataset with 200k human preference annotations, encompassing visual quality, motion quality, and text alignment for modern video generation models. Building on this dataset, we develop VideoReward, a sophisticated multi-dimensional video reward model, and establish the VideoGen-RewardBench"}, {"title": "Limitations & Future Work", "content": "In our experiments, excessive training with Flow-DPO led to a significant deterioration in model quality, despite improvements in alignment across specific dimensions. To prevent this decline, we employed LoRA training. Future work can explore the simultaneous use of high-quality data for supervised learning during DPO training, aiming to preserve video quality while enhancing alignment. Additionally, our algorithms have been validated on text-to-video tasks; future work can extend the validation to other conditional generation tasks, such as image-to-video generation. Moreover, our VideoReward model is vulnerable to reward hacking, wherein human assessments indicate a marked decrease in video quality, yet the reward model continues to assign high scores. This issue arises because the reward function is differentiable, making it susceptible to manipulation. Future research should focus on developing more robust reward models, potentially by incorporating uncertainty estimates and increasing data augmentation. Additionally, there is potential to apply more RLHF algorithms, such as PPO, to flow-based video generation tasks."}, {"title": "A.1. Relation beween Velocity and Noise", "content": "Lemma A.1. Let $X_0 \\sim q$ be a real data sample drawn from the true data distribution and $X_1 \\sim p$ be a noise sample, where $X_0, X_1 \\in R^d$. Define $v_t(x_t | X_0, X_1)$ to be the conditional velocity field specified by a Rectified Flow (Liu et al., 2022), and let $v_{\\text{red}} (x_t)$ be the predicted marginal velocity field. Then the L2 error of the noise prediction is related to the L2 error of the velocity field prediction by\n$\\|X_1-X_{\\text{pred}}(x_t, t) \\|^2 = (1 - t)^2 \\|v_t(x_t | X_0, X_1) \u2013 v_{\\text{pred}} (x_t) \\|^2.$\nProof. The Rectified Flow is a time-dependent flow $ \\psi : [0, 1] \\times R^d \\rightarrow R^d$ for $t \\in [0, 1]$, defined by\n$ \\psi(X_0, X_1) = (1-t) X_0 + t X_1.$\nBy definition, the marginal velocity field $v_t (x_t)$ is\n$v_t(x_t) = E[v_t (X_t | X_0, X_1) | X_t = x_t]$\n$= E[\\dot{\\psi}(X_t | X_0, X_1) | X_t = x_t]$\n$= E[X_1 - X_0 | X_t = x_t]$\n$= E[\\frac{X_1 - ((1-t) X_0 + t X_1)}{1-t} | X_t = x_t]$\n$= \\frac{E[X_1 | X_t = x_t ] - X_t}{1-t}.$\nMeanwhile, the conditional velocity field $v_t(x_t | X_0, X_1)$ is given by\n$v_t(x_t | X_0, X_1) = \\frac{X_1 - X_t}{1-t}.$\nSubstituting equation 20 into equation 19, we obtain\n$\\|v_t (x_t | X_0, X_1) \u2013 v_t(x_t) \\|^2 = \\frac{\\|X_1 \u2013 E[ X_1 | X_t = x_t ] \\|^2}{(1-t)^2}.$\nAssuming that\n$X_{\\text{pred}}(x,t) = E[X_1 | X_t = x_t]$ and $v_{\\text{pred}} (x_t) = v_t(x_t)$.\nConsequently,\n$\\|X_1 - X_{\\text{pred}}(x,t)\\|^2 = (1-t)^2 \\|v_t (x_t | X_0, X_1) \u2013 v_{\\text{pred}} (x_t) \\|^2.$"}, {"title": "A.2. Reward as Classifier Guidance", "content": "We begin by citing a lemma from the Guided Flows paper (Zheng et al., 2023)\nLemma A.2. Let $p_t(x|y)$ be a Gaussian Path defined by a scheduler $(\\alpha_t, \\sigma_t)$, i.e., $p_t(x|x_o) = N(x|\\alpha_tx_o, \\sigma_t I)$ where $y \\in R^k$ is a conditioning variable, then its generating velocity field $v_t(x|y)$ is related to the score function $ \\nabla log p_t(x|y)$ by\n$v_t(x|y) = a_t x + b_t\\nabla log p_t(x|y),$\nwhere\n$a_t = \\frac{\\dot{\\alpha_t}}{\\sigma_t^2}, b_t = \\frac{\\dot{\\sigma_t}}{\\sigma_t}, \\sigma_t = (\\sigma_t^2 + \\alpha_t^2)$.\nSeed Appendix A.61 of the Guided Flows paper (Zheng et al., 2023) for detailed derivations.\nIf we define\n$ \\tilde{v}_t(x_t|Y) = v_t(x_t|y) + w[a_tx_t + b_t\\nabla r(x_t, y) \u2013 v_t(x_t|Y)]$\n$v_t(x_t|Y) + w[a_tx_t + b_t\\nabla log \\exp(r(x_t, y)) \u2013 v_t(x_t|Y)]$\n$= (1 - w)v_t (x_t|y) + w[a_tx_t + b_t\\nabla log \\exp(r(x_t, y))]$\n$= a_tx_t + b_t\\nabla [(1 \u2013 w) \\log p_t(x_t|y) + w \\log\\exp(r(x_t,y))]$\n$= a_tx_t + b_t\\nabla log \\tilde{p}_t (x_t y)$\nwhere $\\tilde{p}_t(x_t|y) \\propto p_t(x_t|y)^{1-w}[\\exp(r(x_t, y))]^w$. We change our goal from sampling from the distribution $p_t(x_t|y)$ to sampling from the distribution $\\tilde{p}_t(x_t|y)$.\nWe note, however, that this analysis shows that Reward Guided Flows are guaranteed to sample from $\\tilde{q}(\u00b7|y)$ at time t = 1 if the probability path $p_t(\u00b7|y)$ is close to the marginal probability path $ \\int p_t(\u00b7|x_1)q(x_1|y)dx_1$, but it is not clear to what extent this assumption holds in practice. This also mens that $\\tilde{p}_t (x_t|y)$ is also a marginal gaussian path defined by $\\tilde{p}_t(x|x_1) = N(x|a_tx_1,\\sigma I)$.\nSimlirly, if we define\n$ \\tilde{v}_t(x_t|Y) = v_t(x_t|y) + wb_t\\nabla r(x_t, y)$\n$= v_t(x_t|y) + wb_t\\nabla log \\exp(r(x_t, y))$\n$a_tx_t + b_t\\nabla [log p_t (x_t|y) + w log \\exp(r(x_t, y))]$\n$= a_tx_t + b_t\\nabla log \\hat{p}_t (x_t y)$\nwhere $\\hat{p}_t(x_t|y) \\propto p_t(x_t|y)[\\exp(r(x_t, y))]^w$. We change our goal from sampling from the distribution $p_t(x_t|y)$ to sampling from the distribution $\\hat{p}_t(x_t|y)$.\nReward Guidance for Rectified Flow. Rectified Flow (Liu et al., 2022) is also a Gaussian path defined by\n$x_t = (1-t)x_o + tx_1$\nwhere $x_1$ is from normal Gaussian distribution. Then\n$p_t(x | x_o) = N(x|(1 \u2013 t)x_o, t^2I)$\nwhere $a_t = 1 - t, \\sigma_t = t$. Then we get\n$a_t = \\frac{1}{t-1}, b_t = \\frac{t}{t-1}.$\nEq. 24 becomes\n$ \\tilde{v}_t(x_t|Y) = v_t(x_t|Y) + w[\\frac{1}{1-t} x_t + \\frac{t}{1-t} \\nabla r(x_t, y) + v_t(x_t|Y)].$\nEq. 29 becomes\n$ \\tilde{v}_t(x_t|Y) = v_t(x_t|Y) - w\\frac{t}{1-t} \\nabla r(x_t, y).$\nWe use Eq. 24 or Eq. 29 to guide inference through reward model."}, {"title": "B. Input Template for Reward Model", "content": "Full Input Template\n[VIDEO] You are tasked with evaluating a generated video based on three distinct criteria: Visual Quality, Motion Quality, and Text Alignment. Please provide a rating from 0 to 10 for each of the three categories, with 0 being the worst and 10 being the best. Each evaluation should be independent of the others.\n**Visual Quality:**\nEvaluate the overall visual quality of the video, with a focus on static factors. The following sub-dimensions should be considered:\n**Reasonableness:** The video should not contain any significant biological or logical errors, such as abnormal body structures or nonsensical environmental setups.\n**Clarity:** Evaluate the sharpness and visibility of the video. The image should be clear and easy to interpret, with no blurring or indistinct areas.\n**Detail Richness:** Consider the level of detail in textures, materials, lighting, and other visual elements (e.g., hair, clothing, shadows).\n**Aesthetic and Creativity:** Assess the artistic aspects of the video, including the color scheme, composition, atmosphere, depth of field, and the overall creative appeal. The scene should convey a sense of harmony and balance.\n**Safety:** The video should not contain harmful or inappropriate content, such as political, violent, or adult material. If such content is present, the image quality and satisfaction score should be the lowest possible.\nPlease provide the ratings of Visual Quality: <|VQ_reward|>\nEND\n**Motion Quality:**\nAssess the dynamic aspects of the video, with a focus on dynamic factors. Consider the following sub-dimensions:\n**Stability:** Evaluate the continuity and stability between frames. There should be no sudden, unnatural jumps, and the video should maintain stable attributes (e.g., no fluctuating colors, textures, or missing body parts).\n**Naturalness:** The movement should align with physical laws and be realistic. For example, clothing should flow naturally with motion, and facial expressions should change appropriately (e.g., blinking, mouth movements).\n**Aesthetic Quality:** The movement should be smooth and fluid. The transitions between different motions or camera angles should be seamless, and the overall dynamic feel should be visually pleasing.\n**Fusion:** Ensure that elements in motion (e.g., edges of the subject, hair, clothing) blend naturally with the background, without obvious artifacts or the feeling of cut-and-paste effects.\n**Clarity of Motion:** The video should be clear and smooth in motion. Pay attention to any areas where the video might have blurry or unsteady sections that hinder visual continuity.\n**Amplitude:** If the video is largely static or has little movement, assign a low score for motion quality.\nPlease provide the ratings of Motion Quality: <|MQ_reward|>\nEND\n**Text Alignment:**\nAssess how well the video matches the textual prompt across the following sub-dimensions:\n**Subject Relevance** Evaluate how accurately the subject(s) in the video (e.g., person, animal, object) align with the textual description. The subject should match the description in terms of number, appearance, and behavior.\n**Motion Relevance:** Evaluate if the dynamic actions (e.g., gestures, posture, facial expressions like talking or blinking) align with the described prompt. The motion should match the prompt in terms of type, scale, and direction.\n**Environment Relevance:** Assess whether the background and scene fit the prompt. This includes checking if real-world locations or scenes are accurately represented, though some stylistic adaptation is acceptable.\n**Style Relevance:** If the prompt specifies a particular artistic or stylistic style, evaluate how well the video adheres to this style.\n**Camera Movement Relevance:** Check if the camera movements (e.g., following the subject, focus shifts) are consistent with the expected behavior from the prompt.\nTextual prompt - [PROMPT]\nPlease provide the ratings of Text Alignment: <|TA_reward|>\nEND"}, {"title": "C. Details of Human Annotation", "content": "We provide additional details regarding the annotation process. First, annotators are provided with detailed scoring guidelines and undergo training sessions to ensure they fully understand the criteria; Tab 8 summarizes the key points for each dimension as outlined in the guidelines. Reference examples are provided to help annotators better grasp the evaluation standards. Each sample is evaluated by three independent annotators. For training and validation sets, annotators provide pairwise preference annotations and pointwise scores for Visual Quality (VQ), Motion Quality (MQ), and Tempotal Alignment (TA). For VideoGen-RewardBench, annotators evaluate the same three aspects along with an additional overall quality, using only pairwise preferences. In cases where the annotators disagree on a sample, an additional reviewer is tasked with resolving the discrepancy. The final label is determined on the basis of the reviewer's evaluation, ensuring consistency across the dataset. Furthermore, during the annotation process, all annotators are instructed to flag any content deemed unsafe. Videos identified as unsafe are excluded from the dataset, ensuring the safety of the data used for training and evaluation."}, {"title": "D. Details of Reward Model Evaluation", "content": "D.1. Evaluation Benchmarks\nWe use two benchmarks to evaluate the performance of our reward model: GenAI-Bench (Jiang et al., 2024b) and VideoGen-RewardBench. GenAI"}]}