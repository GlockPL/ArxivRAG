{"title": "So You Think You Can Scale Up Autonomous Robot Data Collection?", "authors": ["Suvir Mirchandani", "Suneel Belkhale", "Joey Hejna", "Evelyn Choi", "Md Sazzad Islam", "Dorsa Sadigh"], "abstract": "A long-standing goal in robot learning is to develop methods for robots to acquire new skills autonomously. While reinforcement learning (RL) comes with the promise of enabling autonomous data collection, it remains challenging to scale in the real-world partly due to the significant effort required for environment design and instrumentation, including the need for designing reset functions or accurate success detectors. On the other hand, imitation learning (IL) methods require little to no environment design effort, but instead require significant human supervision in the form of collected demonstrations. To address these shortcomings, recent works in autonomous IL start with an initial seed dataset of human demonstrations that an autonomous policy can bootstrap from. While autonomous IL approaches come with the promise of addressing the challenges of autonomous RL-environment design challenges\u2014as well as the challenges of pure IL strategies-extensive human supervision\u2014in this work, we posit that such techniques do not deliver on this promise and are still unable to scale up autonomous data collection in the real world. Through a series of targeted real-world experiments, we demonstrate that these approaches, when scaled up to realistic settings, face much of the same scaling challenges as prior attempts in RL in terms of environment design. Further, we perform a rigorous study of various autonomous IL methods across different data scales and 7 simulation and real-world tasks, and demonstrate that while autonomous data collection can modestly improve performance (on the order of 10%), simply collecting more human data often provides significantly more improvement. Our work suggests a negative result: that scaling up autonomous data collection for learning robot policies for real-world tasks is more challenging and impractical than what is suggested in prior work. We hope these insights about the core challenges of scaling up data collection help inform future efforts in autonomous learning.", "sections": [{"title": "1 Introduction", "content": "Enabling robots to acquire skills in the wild from autonomous, self-supervised interaction has been a long-standing goal in robot learning. To this end, a variety of efforts have focused on developing methods for reinforcement learning (RL) in the real-world [1-3]. Despite substantial progress, RL for real-world robotics requires a significant amount of human effort on environment design, such as developing reset mechanisms, safe guards, success detectors, and reward functions. These challenges exacerbated by sample efficiency issues have constrained the complexity of tasks that are possible with today's methods for real-world RL. As a consequence, many have shifted their attention to imitation learning (IL) methods, which scale much better with task complexity [4, 5]. However, IL methods rely on increasingly large amounts of high-quality human demonstrations as tasks become more diverse and complex, thus shifting the human effort required to human supervision-i.e., demands on the time of expert operators. In fact, from \"pure autonomous\" RL methods to \"pure human\" IL methods, there exists a spectrum that trades off between environment design effort and human supervision effort. We visualize this spectrum in Fig. 1, where one might expect that by moving from either side towards the middle of this spectrum, the human effort in both environment design and supervision can go down.\n8th Conference on Robot Learning (CoRL 2024), Munich, Germany."}, {"title": "2 From RL to IL: Preliminaries and Related Work", "content": "Here, we introduce the spectrum of robot learning methods from RL to IL, their assumptions, and prior work."}, {"title": "2.1 Reinforcement Learning", "content": "Reinforcement Learning (RL) methods adopt the model of a standard Markov decision process (MDP) M= (S,A,T,R,\u03c1\u2080,\u03b3) consisting of a state space S, continuous action space A, transition function T: S\u00d7A\u00d7S\u2192[0,1], reward function R:S\u00d7A\u2192R, initial state distribution \u03c1\u2080, and discount factor \u03b3\u2208 [0,1].\nThese methods aim to learn a policy \u03c0 to maximize the expected discounted sum of rewards J(\u03c0) = \u0395[\u03a3\u1d57\u03b3\u1d57R(s\u1d57,a\u1d57)]. Implementing RL algorithms in practice is challenging due to a number of factors:\nSuccess Detector (S, R). As an evaluation mechanism or for early termination of episodes, it is typical to utilize a success detector f: S\u2192 {0,1} to detect if a state s falls within a set of terminal states S\u208a \u2286 S. The success detector f may be learned, scripted, or labeled by a human.\nReset Mechanism (\u03c1\u2080). Generating full episodes assumes the ability to sample from \u03c1\u2080. This requires access to a reset policy (commonly referred to as a backward policy ) such that following \u03c0\u1d47 leads to a new initial state s\u2080 ~\u03c1\u2080. The existence of \u03c0\u1d47 necessitates that all s\u2080 \u2208\u03c1\u2080 are reachable from any s\u2208 S at which the episodes terminate. However, this assumption does not hold in the case of irreversible actions. In"}, {"title": "2.2 Imitation Learning", "content": "In contrast, Imitation Learning (IL) methods aim to learn a policy \u03c0 that imitates the behaviors from a dataset D where each demonstration \u03be \u2208D consists of state-action transitions {(s\u2080,a\u2080),..., (s\u209c,a\u209c)} [22]. Rather than assuming access to reward R, IL generally assumes that each demonstration \u03be is given by an expert, and thus attains maximum reward. While avoiding the requirements of a reward function, an automated reset function, and success detector, IL imposes additional key assumptions:\nData Collection Time. The creation of D requires access to expert operator(s) to collect demonstrations. In practice, it is common for the operator to additionally perform resets between episodes.\nOptimal Demonstrations. The majority of works assume D is composed of optimal demonstrations. Producing high-quality demonstrations imposes additional burdens on operators, such as practicing before collecting demonstrations, as well as filtering out low-quality demonstrations during data collection [23, 24]."}, {"title": "2.3 The Middle Ground: Mixed Autonomy Methods", "content": "In an attempt to strike a balance between the environment design challenges of pure RL and human supervision challenges of pure IL, several works have proposed methods that mix human demonstrations and learning from autonomous execution. Closer to the left side of Fig. 1, hybrid RL+IL methods use a prior dataset of human demonstrations to guide RL (providing some amount of exploration guidance and sample efficiency gains) [6, 8-10, 25]. Closer to the right side of Fig. 1 are interactive imitation learning (IIL) methods, which allow a human to intervene on a robot's autonomous execution, and use these interventions as a learning signal [11, 13, 17, 26]. In practice, these works still require significant environment design effort (e.g., to prevent unsafe behaviors or instrument success detectors) and human supervision effort (e.g., for resetting environments, or supervising autonomous execution until an intervention is needed).\nMore recently, interest has grown in autonomous IL methods which self-bootstrap starting from a policy trained on human demonstrations [17-19]. The hope is that these methods can bypass the need for high envi- ronment design effort by removing reward and safety constraints. They could also reduce human supervision by only requiring a fraction of the initial data that pure IL would require for the same tasks. In this work, our aim is to stress-test these ideas with various autonomous IL methods as we scale up task complexity.\nWe provide a general recipe for autonomous IL in Alg. 1. Given a dataset of human demonstrations D\u2095, a policy \u03c0\u2080 is trained using algorithm A (Line 2). For M rounds, a new dataset D\u1d62 is generated by collecting N\u2090 rollouts (Line 4) with a filter function F. A new policy \u03c0\u1d62 is trained using algorithm B on a mixture of the prior datasets specified with a function mix (Line 5). The simplest, na\u00efve instantiation is filtered behavior cloning (BC): where both algorithms A and B are BC, the number of rounds M = 1, and the filter function F only accepts successful episodes, so as not to pollute the training data with failures (Line 4). Past works have alluded to this idea that na\u00efvely adding successful autonomous rollouts added to the training data in this way should be helpful to performance [17-19]. We systematically test whether this is the case in the single task setting. We first analyze the challenges of satisfying the environment assumptions of Alg. 1 as we scale task complexity"}, {"title": "3 Challenges of Scaling Up: Analyzing Environment Design", "content": "In this section, we provide a case study on the challenges of running autonomous IL in practice on useful real-world tasks. These challenges correspond to satisfying pre-conditions of most autonomous IL algorithms as in Alg. 1, but for concreteness, we limit the discussion to filtered BC as a simple instantiation of Alg. 1: (1) \u03c0\u2080 must achieve nontrivial success given N\u2095 demonstrations; (2) the success detector f used to define the filter F must be accurate; (3) the environment dynamics T must be stationary; (4) the reset mechanism to sample from \u03c1\u2080 must be robust. This case study illustrates that environment design effort, while often underemphasized, is difficult to reduce as we approach more useful and realistic tasks.\nUseful but Feasible Tasks: from organizing laundry to folding socks. To test if autonomous IL delivers on its promise of addressing challenges of IL and RL, we need to select useful real-world tasks, where IL and RL techniques struggle. Consider the task of folding laundry: this task requires manipulation of deformable clothing of many shapes and sizes with a broad distribution of initial states (e.g., object configurations). For autonomous IL, this causes a few issues: (1) The initial autonomous policy must achieve nontrivial success rates in order for us to collect any autonomous data: thus a large amount of initial human supervision effort (demonstrations) is already necessary for these realistic tasks. Recent works have shown one can achieve challenging tasks similar to laundry folding\u2014albeit with limited generalization, i.e., limited variations in scenes or initial configurations-via imitation learning on thousands of demonstrations [27]. (2) We need to be able to reset the environment to initial configurations that autonomous IL can bootstrap from, which is often infeasible for realistic tasks like laundry folding which have a broad set of initial configurations. (3) Performing controlled evaluations on different models is challenging when the set of initial conditions is so broad. Thus, to even begin to study the problem of autonomous IL, we scope the task down significantly to a more controlled setting: sock folding from an arbitrary configuration (see Fig. 2). This task nevertheless represents a step up in difficulty from toy tasks (e.g., folding a square cloth that always begins unfolded). A diffusion-based imitation policy [5] trained on our sock folding task attains only ~30% success trained on 250 human demonstrations.\nReliable Success Detection: from folding socks to hanging mitts. To train on only successful autonomous data without human supervision, we need a precise and reliable success detector. Without a good success detector, datasets can be polluted with false positives, and controlled evaluation becomes very challenging. Even when limiting ourselves to the sock folding task, simple object shape and area heuristics (which have been used in prior work on toy cloth folding tasks) prove to be insufficient for crisply detecting whether a sock has been folded in half; indeed, autonomous execution inevitably encounters \"edge cases\" which are challenging even to annotate by hand. We also attempt to train a success classifier on terminal states from a combination of 200 human demonstrations and 700 hand-annotated rollouts of the autonomous policy with an approach similar to [18], still obtaining a validation error of 10-20%, representing the challenge of training success detectors for realistic tasks, even under generous data assumptions, without the use of more specialized or domain-specific techniques. Please see Appx. A for more details.\nDespite our attempts to reduce environment design effort, we find it necessary to \"shape\" the task to fit our requirements-e.g., changing the choice of sock (to be shorter and stiffer) as well as the material of the table to be a higher friction surface (often used in prior work with cloth manipulation [28]). Even with this task"}, {"title": "4 Challenges of Scaling Up: Analyzing Human Supervision", "content": "In \u00a73, we describe environment design challenges that hinder scaling up IL on useful, realistic tasks. Now let us assume that environment challenges are surmountable (i.e., by limiting tasks to simple rigid-body manipulation like pick-place or insertion). In such settings, can autonomous IL methods meaningfully reduce the amount of human supervision needed to learn an effective policy? Once again, our attempts to reduce this source of effort lead us to several key challenges in scaling autonomous data collection."}, {"title": "4.1 Experiment Overview", "content": "We study a variety of instantiations of Alg. 1, from straightforward techniques such as filtered BC (simply rolling out a policy trained on human data and adding successful rollouts back into the training set), to more complex ones such as active learning and offline RL. For the majority of experiments, we use Diffusion Pol- icy [5] as the choice for algorithms A and B given its expressivity and ability to capture multimodal action dis- tributions [5]. We also set M = 1 unless otherwise specified, and train models from scratch at each iteration.\n\u2022 In \u00a74.2, we study the impact of data scales (of N\u2095 and N\u2090); and number of rounds (setting M>1).\n\u2022 In \u00a74.3, we investigate novelty-based reweighting strategies (more sophisticated mix functions).\n\u2022 In \u00a74.4, we study active learning guided by failures (where F excludes rollouts whose initial states are near previous successes).\n\u2022 In Appx. B, we provide ablations on data weights (upweighting human or autonomous data with mix), training methods (modifying B to train from scratch versus fine-tune \u03c0\u1d62\u208b\u2081), policy class (replacing A and B with ACT [4]), offline RL algorithms (where F allows both successes and failures, and B is an offline RL algorithm), and training with out-of-distribution autonomous successes (modifying Line 4).\nTask Selection. Due to the environment challenges described in \u00a73, we limit our analysis to two rigid-body real-world tasks shown in Fig. 2: HangTape (on a hook) and NutInsertion (on a peg) as well as four simu- lation tasks from LIBERO [29]: SoupInBasket, BookInCaddy, StackBowls, and RedMugOnPlate; and one task from Robomimic [24]: Square. These simulation tasks (shown in Fig. 8) enable a thorough evaluation of design choices for scaling up autonomous data collection while avoiding environment design challenges. Appx. A contains more details on task implementation, success detection, and reset procedures. For each policy evaluation in this work, we perform 100 trials for real-world tasks and 200 trials for simulation tasks.\nData Scale Definitions. Throughout this section, we abbreviate data quantities as follows: \u2193=low, \u25c7=medium, \u2191=high amounts of data for human demonstrations (H) compared to autonomous data (A). For example, \u2193H means N\u2095 is a low amount of demonstrations, and \u2193H + \u2191A means low amount of demonstra- tions combined with high amount of autonomous data, generated by rolling out a policy trained on H until the requisite number of autonomous successes is reached. Generally, the exact data amounts are not equal between A and H, and we chose the H data scales so that BC performance was roughly in the 20-50% range for \u2193 and 50-70% for \u2191. Exact data quantities for each environment are provided in plots and in Appx. A."}, {"title": "4.2 Diminishing Returns of Filtered BC", "content": "In this section, we instantiate filtered BC (na\u00efve autonomous IL), where autonomous rollouts from a policy trained on human data are simply added to the training data. We study the impact of data weights, data scales, and number of collection rounds. In all experiments, we train a Diffusion Policy from scratch on the human-autonomous data mixture. Guided by our results on data weights (Appx. B.2), we train from scratch with 50-50 human-autonomous mixtures for the experiments in this section. Please see Appx. B for additional training details and hyperparameters, and for ablations on training methods (e.g., fine-tuning).\nHuman and Autonomous Data Scales. We now study how the scale of initial human data and the ratio of human data to autonomous data affect performance. In Fig. 4, we compare the low and medium human data regimes across all simulation and real environments, for various ratios of human to autonomous data. Overall, adding autonomous data can modestly improve policy performance on both low and high data regimes (on average 10-20%); but occasionally, it has a minor negative effect (Square, BookInCaddy (\u25c7H), StackBowls (\u2191H)). In most cases, any positive effect saturates as autonomous data scales. In line with prior work in data quality, we suspect that some amount of autonomous data is good for capturing more state diversity, but sometimes that data can also cause action consistency to decrease [23, 30]. Moreover, adding more human data (purple vs. orange) tends to have a stronger effect than adding autonomous data.\nMultiple Collection Rounds. Observing the somewhat positive effect for incorporating autonomous data, one might expect this trend to continue for multiple rounds of autonomous data collection followed by training on the newest round of autonomous data (i.e., when M > 1). This procedure is similar to running Reward-Weighted Regression seeded by demonstrations [31]. In Fig. 5, we run autonomous collection for 2-4 rounds in several simulation and real environments. Interestingly, multiple rounds can in fact improve performance (most LIBERO tasks), but just like a single round of collection, it can also saturate performance (BookInCaddy, SoupInBasket (\u2191H), and real tasks) or even hurt performance (Square). We suspect that"}, {"title": "4.3 Inconsistent Response to Novelty-Based Reweighting", "content": "Adding successful autonomous data to human data can yield modest improvements in some cases. Instead of simply adding all successful autonomous data to the training dataset, one might ask the question, is all the autonomous data equally useful? Specifically, what states and actions are valuable to learn from? Building on prior work [13, 23, 30], we consider using state novelty as a metric for the utility of a new state-action pair. Our intuition is that more common states are either redundant or potentially have conflicting actions; this data is likely less useful than more novel states. Building on prior work, we inform the sampling weights of the autonomous dataset in the mixture function mix based on different notions of novelty. We use two measures of novelty in Fig. 6 on the Square task; see Appx. B.5 for formal definitions.\n1. Action Novelty: Measure novelty as proportional to the variance in action predicted by an ensemble of policies trained on the same data.\n2. Image Embedding Novelty: Measure novelty as proportional to the variance in image embeddings from an ensemble of vision encoders of policies trained on the same data.\nBoth novelty metrics measure policy uncertainty, but in action novelty the uncertainty is more action-driven, whereas embedding novelty is more state-driven. For both action- and embedding-based novelty metrics, reweighting has a slight positive effect on performance compared to the naive strategies for \u2193H, a more no- table effect for \u25c7H, and a slight decrease for \u2191H. Thus, novelty-reweighting provides inconclusive results and still underperforms just adding a bit more human data. Yet, these results confirm our intuition that in many cases, much of the autonomous data is redundant and can be filtered out without decreasing performance."}, {"title": "4.4 Inability to Learn from Failure Data", "content": "Reweighting the autonomous dataset (e.g., using novelty) still depends on the distribution of states visited by the autonomous policy. And in the novelty-weighting case study, by setting F such that only successful autonomous rollouts are included in the autonomous dataset, we are critically biasing this distribution toward states where the policy is already proficient. Does learning from failure data yield any improvement? We study two approaches to learn from failures: active learning and offline RL (Appx. B)."}, {"title": "Active Learning Guided by Failures", "content": "Are there more intelligent ways to target the new data we collect? One such approach, inspired by active learning methods, is to use autonomous failure data to generate a distribution of failure states to intelligently query a user. Querying a user for a single expert action at arbitrary failure states is challenging in practice, and it is often easier for a user to provide a complete demonstration. Therefore, we implement a more practical approach: sampling from the distribution of initial states from failed autonomous rollouts to query for complete demonstrations. In Fig. 7, we compare (1) using the autonomous policy to collect new demonstrations near the failure initial states (Near-Failure A), and (2) querying a human for demonstrations initialized at these states (Near-Failure H).\nIncorporating near-failure autonomous data has no positive effect beyond the na\u00efve method of random selection. However, collecting new human demonstrations near autonomous failures (Near-Failure H) does yield improvement over an equal number of randomly selected human demonstrations (Random). Similar to the filtered BC case study, collecting more human demonstra- tions tends to yield larger improvement than even high amounts of autonomous data, especially if those demonstrations are collected from autonomous failures' initial states.\nSummary. We find that the most salient factors leading to policy improvement for autonomous IL are, in order: (1) the amount and utility of new human demonstration data and (2) the availability of some amount of autonomous data. Data weights, amount and rounds of autonomous collection, and novelty-based reweighting strategies all have little effect in comparison. Repeatedly, we find that additional human data is significantly better than access to even ten times the amount of autonomous data. This suggests that even under the assumption of no environment challenges, many autonomous IL methods struggle to match the performance of simply incorporating a small amount of additional human data."}, {"title": "5 Discussion", "content": "In this work, we take a practical look at the challenges involved in scaling autonomous IL to complex tasks. While autonomous IL does not require all of the assumptions of fully autonomous methods like RL, we affirm that they still require immense environment design effort which scales with task complexity. Designing robust resets and precise success detectors remains an open challenge for such complex tasks, as does dealing with environment non-stationarity. These factors are critical to conducting meaningful evaluations and deploying algorithms with any level of autonomy in the real world, and so scaling robot learning requires advances in how we can mitigate or amortize environment design effort. For example, a direction for improvement is in learning general purpose success detectors using foundation models. Even assuming we can reduce environment design effort, in this work our best attempts at learning from the autonomous data could not match the improvement of marginally increasing human supervision effort with traditional IL. We hope future work will build on the insights in our work to extract as much as possible from autonomous data collection--for example, by guiding both human and autonomous data collection based on insights (e.g., novelty, failure states) gleaned from the autonomous trials.\nLimitations. While our study of autonomous IL considers a wide variety of methods, we primarily consider single-task imitation learning, and the performance of autonomous IL methods may differ in multi-task settings. Multi-task environments could also enable learning the reset and the task at the same time, alleviating some environment design effort. Finally, we focus on settings where models are trained from scratch; future work should study effects of large-scale pre-training in the autonomous IL setting."}, {"title": "A Task Details", "content": "In this section, we give additional information on the tasks studied in this work. We give verbal descriptions in Appx. A.1, definitions of data scales in Appx. A.2, and details on the evaluation procedures in Appx. A.3."}, {"title": "A.1 Task Descriptions", "content": "\u2022 FoldSock. Fold a sock (with random configuration) neatly in half.\n\u2022 HangOvenMitt. Hang an oven mitt (with random position and orientation) on a hook (fixed position).\n\u2022 HangTape. Hang a roll of masking tape (with random initial position) on a hook (fixed position).\n\u2022 NutInsertion. Insert a plastic nut (with random initial position) on a peg (fixed position).\n\u2022 Square: Insert a square nut on a square peg (from [24]).\n\u2022 SoupInBasket: Place a small soup can into a basket (from [29]).\n\u2022 BookInCaddy: Place a book into a narrow book caddy (from [29]).\n\u2022 StackBowls: Stack two bowls together and place both on a plate (from [29]).\n\u2022 RedMugOnPlate: Put a red mug on a specific plate (from [29]).\nWe illustrate initial and final states of our real-world and simulation tasks in Fig. 8. We include an illustration of initial state distributions, sample initial and successful states, and sample camera observations for the NutInsertion and HangTape tasks in Fig. 9."}, {"title": "A.2 Data Scale Definitions", "content": "For concision, and to focus on trends, we abbreviate data scales (i.e., number of demonstrations) as low (\u2193), medium (\u25c7), and high (\u2191) for each of human demonstrations (H) and autonomous rollouts (A). Due to the fact that tasks vary widely in difficulty, the absolute value of demonstrations for each data scale varies per task. We include these values in Table 1.\nExample. To generate the training set for the \u2193H + \u2193A setting on the NutInsertion task, we do the following:\n\u2022 Collect 50 human demonstrations from randomly sampled initial states.\n\u2022 Train an initial policy on the human demonstrations to convergence (approximately 47% success rate)."}, {"title": "A.3 Evaluation Procedure", "content": "Unless otherwise specified, all success rates in this work are calculated by uniformly sampling an initial state s\u2080~ \u03c1\u2080 and rolling out the learned policy under consideration until either a success state is achieved or a maximum time horizon is reached. For all simulation results, we perform 200 trials. For all real results, we perform 100 trials."}, {"title": "A.4 Success Detection and Resets", "content": "In this section, we provide additional details and rationales for the success detection and reset pipelines that we used in our real-world tasks. For tasks in simulation, success detection and resets were provided by the environment."}, {"title": "A.4.1 Success Detection", "content": "\u2022 FoldSock. As we found that scripting a sock-foldedness detector based on heuristics like object shape and area produced false positive and false negative rates on the order of 20%, we attempted to train a success classifier using a similar procedure to [18]. We assemble a training dataset of 200 human demonstrations (which are curated to be always successful) and roughly 700 rollouts from the autonomous collection policy (which we hand-label as success or failure). The training set includes 301 successful trajectories and 438 failure trajectories, and we sample from the end from each rollout (last 5 images) to yield images to associate with the success/failure label. We train a ResNet-18-based architecture with a binary classification head. The validation error of the trained classifier is approximately 15%.\n\u2022 HangOvenMitt, HangTape, NutInsertion. These tasks include bottlenecks which must be reached in order to succeed at the task: hanging an object on a hook or placing an object on a peg. Therefore, successes and failures are easy to separate. For simplicity, we use scripted rules similar to prior work (e.g., [10]). Specifically, we use color thresholds at pixels located at these bottlenecks, coupled with the condition that the gripper must be open for five steps prior to success. This ensures that the agent has placed the relevant object at the bottleneck in question. We manually verify that the error rate of this detection scheme is near-zero. While we could in principle train classifiers to learn the boundary between success and failure, our higher-level message is that environment challenges like success detection can be a bottleneck for realistic tasks like FoldSock, and can influence task design to make tasks more constrained such that success and failure are easy to detect. In \u00a74, we set aside environment challenges (i.e., assume"}, {"title": "A.4.2 Resets", "content": "In our study, we use object-centric primitives of various complexity to perform resets. Instrumenting environments with hand-crafted primitives, physical reset mechanisms, or requiring humans to perform resets is a common technique in real-world reinforcement learning [20, 21]. As we illustrate in \u00a73, the human effort of environment design (e.g., by instrumenting the environment to make reset primitives possible) remains when we utilize autonomous IL methods, and these can get more involved as we move towards more useful and complex tasks.\n\u2022 FoldSock. We reset the scene by flinging the sock: locating the sock using a segmentation pipeline (GroundingDINO [32] + FastSAM [33]), picking it up using a top-down grasp, bringing it to the center of the workspace, and executing a fling primitive to randomize its configuration for the next episode.\n\u2022 HangOvenMitt. The final state of the mitt has two cases in the case of success, the mitt is hanging and the mitt can be pulled off the hook by replaying a pre-recorded trajectory; in the case of failure, the mitt is pulled back to a reachable location via a string attached to the robot\u2014and in both cases, the mitt's location is then randomized using a parameterized pick-and-place primitive.\n\u2022 HangTape. We follow a similar procedure as in HangOvenMitt: if the tape is on the hook (i.e., the previous episode was successful), we replay a pre-recorded trajectory to pull it off of the hook. Otherwise, we detect the location of the tape using a simple color mask and execute a pick-and-place primitive to randomize its initial location for the next episode.\n\u2022 NutInsertion. We once again utilize the fact that the final state of the previous episode is either a success, for which the nut can be removed from the peg using a pre-recorded trajectory, or a failure, for which the nut's location can be randomized using a pick-and-place primitive."}, {"title": "B Analyzing Human Supervision: Additional Results", "content": "In this section, we provide further details on the results in \u00a74 of the main text. In Appx. B.1, we ablate the choice of training from scratch on human-autonomous mixtures (the recipe used in all experiments in the main text). We also provide additional details regarding training with different data weights (Appx. B.2), data scales (Appx. B.3), policy class (Appx. B.3.1), number of rounds (Appx. B.4), and novelty-based reweighting (Appx. B.5), active learning from failures (Appx. B.6), and offline RL (Appx. B.7). While experiments in the main text focus on autonomous data collected in-distribution, we provide additional experiments in Appx. B.8 on training with autonomous data collected from out-of-distribution (OOD) scenarios. Finally, we provide qualitative examples of human and autonomous trajectory distributions in Appx. B.9."}, {"title": "B.1 Training from Scratch vs. Fine-tuning", "content": "All of the models trained on human-autonomous data mixtures in \u00a74 are trained from scratch until convergence. In this subsection, we justify this choice by comparing training from scratch to methods involving fine-tuning.\nSpecifically, we focus on a single round of autonomous collection for the Square task in simulation. Unless otherwise specified, each model is trained on a mixture of 50% autonomous, 50% human data. We compare the following training recipes:\n\u2022 Scratch: Train a new model from scratch on the human-autonomous mixture.\n\u2022 Fine-tune: Fine-tune the autonomous policy checkpoint that generated the autonomous data on the human-autonomous mixture.\n\u2022 Pre-train Autonomous + Fine-tune: Pre-train a policy from scratch on the autonomous data only, and then fine-tune on the human-autonomous mixture.\n\u2022 Scratch Add: Directly aggregate human and auto data in one dataset (no explicit 50-50 sampling), and train from scratch on this dataset.\nIn Table 2, we find that training from scratch, fine-tuning from the base policy, and training on combined human and auto datasets all perform comparably. In fact, training methods seem to matter much less than"}, {"title": "B.2 Human and Autonomous Data Weights", "content": "Our experiments on Data Weights study the impact of relative sampling weights of human-to-autonomous data in the training mixture (i.e., changing mix). These experiments keep the amount of autonomous data fixed (\u2193A) and investigate if success rate changes for two scales of human data (\u2193H and \u25c7H) at different sampling ratios (75-25, 50-50, 25-75). We include these results in table form in Table 3 and Table 4. We find that changing the training data weights has almost no impact for a given data scale. This is line with expectations from prior work when using importance weighted objectives with highly expressive models [34]. Guided by these results, we use the simple training from scratch setting with 50-50 human-autonomous mixtures for the remaining experiments in \u00a74."}, {"title": "B.3 Human and Autonomous Data Scales", "content": "Our experiments on Data Scales (Fig. 4) use a 50-50 mixture and examine how success rate is impacted by the scale of initial human data and the ratio of human to autonomous data. We include the results in table form in Table 5. Including some amount of autonomous data tends to have mild positive effects in most cases, though these effects generally saturate as autonomous data scales. Increasing the scale of human data generally has a stronger effect than adding autonomous data."}, {"title": "B.3.1 Human and Autonomous Data Scales under Different Policy Classes", "content": "In this section, we provide additional results on Data Scales using a 50-50 mixture, keeping the task the same but testing two different policy classes: Diffusion Policy (DP) [5] and Action Chunking with Transformers (ACT) [4]. Both methods are capable of modeling diverse action distribution modes. While ACT underperforms DP in this task, the effects on success rate when re-training with different scales of autonomous data are largely similar: there is mild improvement which appears to plateau. The compatible results on ACT and Diffusion Policy suggest that our observations are not unique to the policy class."}, {"title": "B.4 Multiple Collection Rounds", "content": "Our experiments on Multiple Collection Rounds (Fig. 5) measure if any positive effects of autonomous data continue over multiple iterations. Specifically, we replace the autonomous data in the training mixture with the latest round of autonomous data collection, and re-train the model from scratch. The amount of autonomous data is kept constant at each round (\u25c7A; \u2191A for LIBERO tasks). We investigate the effects of multiple collection rounds at multiple scales of human data (\u2193H and \u25c7H) in simulation and at the \u2191H scale in real. We present the results in table form in Table 7 and Table 9, generally observing plateaus in performance after an initial improvement in the first iteration. Interestingly, in the Square task, we observe a slight decrease in performance. Unlike the LIBERO tasks, Square contains a more challenging bottleneck state, and we hypothesize that subtle variations in the action distributions over multiple rounds of autonomous data collection and training may amplify this challenge. As evidence, in Table 8, we examine the \"staged\" success rate in Square over multiple iterations: note that the subtask for \u201cmoving the square\u201d increases in success rate while the full task (which includes the insertion bottleneck) decreases in success rate."}, {"title": "B.5 Novelty-Based Reweighting Strategies", "content": "In \u00a74.3", "30": ".", "Novelty": "Measure state novelty"}]}