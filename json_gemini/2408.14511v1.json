{"title": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods", "authors": ["Xinyang Hu", "Fengzhuo Zhang", "Siyu Chen", "Zhuoran Yang"], "abstract": "Chain-of-Thought (CoT) prompting and its variants have gained popularity as effec-\ntive methods for solving multi-step reasoning problems using pretrained large language\nmodels (LLMs). In this work, we analyze CoT prompting from a statistical estimation\nperspective, providing a comprehensive characterization of its sample complexity. To this\nend, we introduce a multi-step latent variable model that encapsulates the reasoning pro-\ncess, where the latent variable encodes the task information. Under this framework, we\ndemonstrate that when the pretraining dataset is sufficiently large, the estimator formed\nby CoT prompting is equivalent to a Bayesian estimator. This estimator effectively solves\nthe multi-step reasoning problem by aggregating a posterior distribution inferred from\nthe demonstration examples in the prompt.\nMoreover, we prove that the statistical error of the CoT estimator can be decomposed\ninto two main components: (i) a prompting error, which arises from inferring the true\ntask using CoT prompts, and (ii) the statistical error of the pretrained LLM. We establish\nthat, under appropriate assumptions, the prompting error decays exponentially to zero\nas the number of demonstrations increases. Additionally, we explicitly characterize the\napproximation and generalization errors of the pretrained LLM. Notably, we construct a\ntransformer model that approximates the target distribution of the multi-step reasoning\nproblem with an error that decreases exponentially in the number of transformer blocks.\nOur analysis extends to other variants of CoT, including Self-Consistent CoT, Tree-of-\nThought, and Selection-Inference, offering a broad perspective on the efficacy of these\nmethods. We also provide numerical experiments to validate the theoretical findings.", "sections": [{"title": "1 Introduction", "content": "Autoregressive Large Language Models (LLMs) have tremendously revolutionized the field\nof Natural Language Processing (NLP) and related areas due to their striking ability to\nunderstand languages and follow instructions. These models, based on the transformer\narchitecture (Vaswani et al., 2017), are probabilistic models that predict the next token\nbased on preceding tokens, also known as a prompt. The training of LLMs typically involves\ntwo phases: pretraining and post-training. During the pretraining phase, the LLMs are\ntrained on vast text corpora via unsupervised learning (Ahmad et al., 2021; Zoph et al.,\n2020; Erhan et al., 2010; Hendrycks et al., 2019). This process allows them to acquire a\nbroad understanding of language and general knowledge. Subsequently, additional post-\ntraining approaches, including supervised fine-tuning (Wei et al., 2021) and reinforcement\nlearning with human feedback (RLHF) (Ouyang et al., 2022), are adopted to enhance the\nchat capabilities of LLMs. Finally, the trained LLMs are deployed to interact with human\nusers, with their neural network parameters remaining fixed.\nHuman users interact with LLMs through prompting, which refers to text generation\nconditioned on the prompts provided by the users. Designing effective prompts to induce\nspecific desired behaviors in LLMs is known as prompt engineering (Sahoo et al., 2024),\nwhich is largely a heuristic enterprise. Prompt engineering represents a paradigm shift from\nstandard statistical learning. Specifically, when using LLMs to solve a task via prompting,\nthe LLMs essentially \u201clearn\u201d from the prompts by passing them through the neural network\nwith fixed parameters, which have been trained without data from the desired task.\nOne of the most widely used prompting heuristics is In-Context Learning (ICL) (Brown\net al., 2020; Dong et al., 2022), a technique that enables LLMs to comprehend concepts by\nincluding a few examples in the prompt. This involves feeding the LLM with a few input-\noutput examples and then asking for the output corresponding to a new input. On many\ntasks, the LLM can successfully extract the relationship between inputs and outputs and\ngeneralize it to the new input to get the desired output. This simple and intuitive prompting\nmethod has recently drawn considerable research interest and has become the foundation\nfor many sophisticated prompting methods designed for more complicated tasks (Wei et al.,\n2022; Zhou et al., 2022; Kim et al., 2022; Zhang et al., 2022; Rubin et al., 2021; Sorensen\net al., 2022; Creswell et al., 2022; Yao et al., 2023; Wang et al., 2022).\nA prominent example of ICL is Chain-of-Thought (CoT) prompting (Wei et al., 2022),\nwhich generalizes ICL for multi-step reasoning tasks. Specifically, the vanilla version of few-\nshot CoT proposes including intermediate reasoning steps in addition to input and output\nin the demonstration examples, helping LLMs understand the reasoning path from input to\noutput. Building upon vanilla CoT, other sophisticated variants of CoT have been proposed\nto efficiently select reasoning paths via majority votes or tree search (Creswell et al., 2022;\nYao et al., 2023; Wang et al., 2022).\nWhile CoT prompting methods have found great empirical success in multi-step reasoning\nproblems such as arithmetic, commonsense, and symbolic reasoning, there is still a lack of\ntheoretical understanding of why CoT works and how it compares with vanilla ICL. In this\nwork, we aim to rigorously understand why the practice of \u201cpretrained LLM + CoT\nprompting\u201d is capable of solving multi-step reasoning problems. Additionally, we aim to\ndemystify how the transformer architecture of the LLMs and the intermediate reasoning\nsteps in the prompts contribute to this success. Specifically, we aim to answer the following\nfour questions:\n(a) What are the statistical estimators constructed by CoT and its variants?\n(b) What are the statistical properties of these estimators?\n(c) How does the transformer architecture enable the LLMs to learn these estimators?\n(d) Does CoT prompting always outperform vanilla ICL?"}, {"title": "2 Related Works", "content": "Our work adds to the literature on theoretically understanding prompting methods. In\nparticular, our work is closely related to CoT prompting and its variants. In addition, our\nwork is related to the body of works that aim to understand the ability of ICL and CoT\nfrom both empirical and theoretical perspectives.\nCoT Prompting and its Variants. The vanilla CoT prompting method is proposed in\nWei et al. (2022) for solving multi-step reasoning problems using LLMs. Based on this work,\nmany variants of CoT have been proposed to enhance the efficiency and reliability of LLMs\nin solving multi-step reasoning problems. See, e.g., Yao et al. (2023); Wang et al. (2022);\nCreswell et al. (2022); Zhou et al. (2022); Chen et al. (2022); Zhang et al. (2023c); Besta\net al. (2024) and also see Chu et al. (2023); Zhang et al. (2023b) for recent surveys of CoT\nmethods. In particular, our work offers a theoretical understanding for vanilla CoT and\nvariants including Self-Consistency (SC) CoT (Wang et al., 2022), Selection-Inference (SI)\nCreswell et al. (2022), and Tree-of-thoughts (ToT) (Yao et al., 2023).\nExisting Research on Understanding ICL. Our work is closely related to the body of\nworks that aim to understand the ability of ICL from both empirical and theoretical perspec-\ntives. From an empirical point of view, Garg et al. (2022); Min et al. (2022); Krishnamurthy\net al. (2024); Zhang et al. (2022); Dziri et al. (2024); Olsson et al. (2022) explore the under-\nstanding of the behavior and capability of ICL. In particular, Garg et al. (2022) show that\ntransformers can learn unseen linear functions via ICL. Min et al. (2022) demonstrate that\nshuffled input-output pairs in few-shot ICL induce little degradation in the performance on a\nrange of classification and multi-choice tasks. Dziri et al. (2024) study how transformer-based\nLLMs solve compositional tasks and their limitations in reasoning.\nFrom a theoretical perspective, Aky\u00fcrek et al. (2022); Von Oswald et al. (2023); Bai\net al. (2023); Dai et al. (2023); Wang et al. (2023a) establish theoretical understandings of\nICL. The theories proposed in these works mainly offer two explanations of ICL: (i) LLMs\nperform ICL by running iteration optimization algorithms such as gradient descent, and\n(ii) LLMs perform ICL by implementing Bayesian inference through the architecture. The\nworks Aky\u00fcrek et al. (2022); Von Oswald et al. (2023); Bai et al. (2023); Dai et al. (2023)\nindicate that ICL implicitly implements the gradient descent or least-square algorithms from\nthe function approximation perspective. Hou et al. (2023) hypothesize that LLMs implicitly\nperform multi-step reasoning within their architecture by going through a reasoning tree. Li\net al. (2023a) derive the generalization bound for ICL from the view of multi-task learning.\nHahn and Goyal (2023) adopt a linguistic point of view and bounds the ICL error using\ndescription length. The works Ahn et al. (2023); Huang et al. (2023b); Fu et al. (2023);\nMahankali et al. (2023); Wu et al. (2023a) consider linear attention models to study the per-\nformance of ICL, which restricts the function class that can be represented by transformers\nto linear functions.\nAnother line of work lies in the Bayesian interpretation of the ICL paradigm (Jiang, 2023;\nWang et al., 2023b; Xie et al., 2021; Wies et al., 2023; Zhang et al., 2023a; He et al., 2024).\nUnder the Bayesian framework, Xie et al. (2021) use Hidden Markov Model (HMM)(Rabiner\nand Juang, 1986) to model the token generation process and assume access to the true lan-\nguage distribution. However, the HMM assumption is restrictive, and the perfect pretraining\nassumption does not incorporate the pretraining phase into the story. To this end, Wies et al.\n(2023) relax these two assumptions by adopting a general i.i.d. data model and analyzing\na pretrained model that well approximates the true distribution given any token sequence,\nwhich is also unrealistic. These works do not mention the relationship between transformer\narchitecture, pretraining process, and the Bayesian interpretation of ICL.\nAmong these works, our work is most related to Zhang et al. (2023a) and He et al. (2024)."}, {"title": "3 Background", "content": "In this section, we introduce the background knowledge about transformer-based large lan-\nguage models and CoT prompting.\nAutoregressive LLMs. Most commercial LLMs such as GPT-4 (OpenAI, 2023), Claude\n(Anthropic, 2023), Llama (Touvron et al., 2023), and Gemini (Team et al., 2023), are autore-\ngressive in the sense that they generate in a token-by-token fashion. An autoregressive LLM,\ndenoted by $\\mathbb{P}_{\\text{LLM}}$, is a conditional probability model that continuously predicts future tokens\nbased on a sequence of past tokens, known as the prompt. Here we denote the space of all\nthe tokens as $\\mathcal{X}$. Given an input prompt $S_t = (x_1, ..., x_t) \\in \\mathcal{X}^t$, to generate the response to\nit, the LLM first generate the next token as $x_{t+1} \\in \\mathcal{X} \\sim \\mathbb{P}_{\\text{LLM}}(\\cdot|S_t)$. Then it appends the\ngenerated token $x_{t+1}$ to the end of $S_t$ to form $S_{t+1} = (S_t, x_{t+1})$. The LLM will generate $x_{t+2}$\nbased on $S_{t+1}$, and it repeats this generation process till the generation of the end of the\nsentense.\nTransformers and Attention Mechanism. The transformer model is based on the Multi-\nHead Attention (MHA) mechanism (Bahdanau et al., 2014; Phuong and Hutter, 2022),\ntogether with other modules such as the tokenizer and the positional embeddings (Wang\nand Chen, 2020; Su et al., 2023), residual connections, feed-forward networks, and layer\nnormalization (Ba et al., 2016). The tokenizer maps the input sequence to a sequence of\nvectors in Euclidean space, and the positional embeddings add the position information of\ntokens to these vectors.\nThe attention mechanism captures the relationship between different tokens, which is the\nbackbone of transformer-based LLM (Devlin et al., 2018). The attention mechanism takes\nin queries, keys, and values as inputs, and outputs the response of each query as a weighted\nsum of values, where the weights are the similarity scores between the query and the keys.\nSpecifically, let $K\\in \\mathbb{R}^{L\\times d_k}$ and $V\\in \\mathbb{R}^{L\\times d_v}$ denote the $L$ key and value vectors, respectively.\nThe attention output of a single query $q \\in \\mathbb{R}^{d_k}$ is computed as:\n$$\\text{attn}(q, K, V) = V^T \\text{softmax}(Kq),$$\nwhere $\\text{softmax}(Kq)$ is a probability distribution over $[L]$. Here $\\text{softmax}(Kq)$ quantifies the\nsimilarity between the query $q$ and each row of $K$, which is used to aggregate the value vectors\nof $V$. The attention $\\text{attn}(Q, K, V)$ that takes in multiple queries outputs the responses as\n$V^T \\text{softmax}(KQ^T)$, where $Q \\in \\mathbb{R}^{L\\times d_k}$ contains $L$ query vectors. The predefined attention\nmechanism captures the relationship between the keys and queries via a single softmax\nmodule, and thus is called single-head attention. MHA refers to passing the inputs through\nmultiple attention functions in parallel, and outputs the aggregation of these sub-modules.\nTaking $X \\in \\mathbb{R}^{L\\times r}$ as the input, a MHA layer with $\\eta$ heads outputs\n$$\\text{mha}(X, W_{\\text{mha}}) = \\sum_{i=1}^{\\eta} \\text{attn}(XW_i^Q, XW_i^K, XW_i^V),$$\nThe parameter set $W_{\\text{mha}} = \\{ W_i^Q, W_i^K, W_i^V \\}_{i=1}^{\\eta}$ are the weight matrices for queries, keys, and\nvalues, where $W_i^Q \\in \\mathbb{R}^{r\\times d_k}, W_i^K \\in \\mathbb{R}^{r\\times d_k}$, and $W_i^V \\in \\mathbb{R}^{r\\times d_v}$. Intuitively, different heads can\nattend to different parts of the data, and thus MHA offers a more expressive model class.\nCompared to the MHA defined in Vaswani et al. (2017), we absorb the matrix $W^O$ into $W^V$\nfor each head.\nEach MHA layer is followed by a Feed-Forward (FF) layer. Given an input $X \\in \\mathbb{R}^{L\\times r}$, a\nFF layer with $d_F$ neurons maps the input $X$ to\n$$\\text{ff}(X, W_{\\text{ff}}) = \\text{ReLU}(XW_{ff,1})W_{ff,2}, \\quad \\text{where } W_{\\text{ff}} = \\{ W_{ff,1} \\in \\mathbb{R}^{r\\times d_F}, W_{ff,2} \\in \\mathbb{R}^{d_F\\times r} \\}$$\nare weight matrices. There are also normalization layers between the MHA and FF layers.\nWe defer their details to Appendix G.1 for brevity.\nLLM Training. The training of an LLM involves two stages: (i) pretraining (Zoph et al.,\n2020) and (ii) post-training (Ouyang et al., 2022; Wei et al., 2021). In the pre-training"}, {"title": "4 A Latent Variable View of Multi-Step Reasoning", "content": "In this section, we show that CoT prompting can be understood as a Bayesian estimator\non a multi-step latent variable dynamical model. In particular, we propose a multi-step\nlatent variable model in Section 4.1 to capture the multi-step reasoning process, which is\nfurther generalized in Appendix A to the non-i.i.d. setting. Then in Section 4.2, we study the\npractice of CoT prompting of pretrained LLMs from a statistical perspective. In Section 4.3,\nwe show that such a practice is equivalent to a BMA estimator for the multi-step latent\nvariable model, which answers Question (a) raised in Section 1. Moreover, we show that\nthe softmax attention mechanism in the transformer architecture parameterizes the BMA\nalgorithm, which partially answers Question (c).\n4.1 A Multi-Step Latent Variable Model\nWe introduce a multi-step latent variable model to capture the multi-step reasoning process\nof CoT, which serves as the data-generating model for studying CoT.\nCoT Prompting Paradigm. Recall that we define the CoT prompt $\\text{prompt}_{\\text{CoT}}(n) =$\n$(\\{s^i\\}_{i=1}^n, z_{\\text{test}}) = (\\mathcal{Y}_n, z_{\\text{test}})$ in Section 3, which contains $n$ demonstration examples $\\mathcal{Y}_n =$\n$\\{s^i\\}_{i=1} = \\{z_{0:H}^i\\}_{i=1}^n$ and a testing query $z_{\\text{test}}$. To generate such a prompt, we first specify a\nlatent concept vector, which is denoted as $\\theta^* \\in \\Theta$. Here $\\Theta$ denotes the set of all the latent\nconcepts. Semantically, $\\theta^*$ determines the task we would like to achieve via CoT, e.g., the\ncolor description of objects, the calculation of math equations. Thus, we will use the terms\ntask and latent concept interchangeably in the following. Statistically, the latent concept $\\theta^*$\nspecifies the task-specific joint distribution $\\mathbb{P}(\\cdot|\\theta^*)$ of demonstration examples and testing\nquery in the prompt, which will be specified later in (4.2). Given the generated prompt\n$\\text{prompt}_{\\text{CoT}}(n)$, we feed it to the LLM, and the LLM recursively generates the intermediate\nsteps $(z_{\\text{test}}^1,...,z_{\\text{test}}^{H-1})$ and the final answer $z_{\\text{test}}^H \\overset{\\wedge}{=} y_{\\text{test}}$ via\n$$z_{h+1}^{\\text{test}} \\sim \\mathbb{P}_{\\text{LLM}}(\\cdot|\\text{prompt}_{\\text{CoT}}(n), z_{\\text{test}}^1, ..., z_{\\text{test}}^h), \\quad \\forall h \\in [H-1].$$\nTo evaluate the performance of CoT, we compare the distribution of $z_{\\text{test}}^H$ in (4.1) with\nthe ground truth distribution $\\mathbb{P}(z_{\\text{test}}^H| \\text{prompt}_{\\text{CoT}}(n), \\theta^*)$, which is the target task-specific\ndistribution of the final answer given the prompt. We illustrate the CoT paradigm with a\nconcrete example as follows.\nAs a concrete example, consider the task $\\theta^* =$ \u201ccalculate twice the area code\nof the given country.\u201d The prompt in Figure 1 is a CoT prompt with $n = 2$ and\n$H = 2$, where the input of the first example is $z_0^1 =$\u201cThe US = ?", "The US has area code 1\", and the second step of the solution is\n$z_2^1 =$\u201cso the answer is 2": "The query is $z_{\\text{test}}^0 =$ \"Japan = ?", "126": "When tested on ChatGPT (Achiam et al., 2023), it indeed outputs\nthe correct answer with an intermediate reasoning step:\u201cJapan has area code 81,\nso the answer is 162.\" In comparison, the vanilla ICL prompt has $x^1 =$\u201cThe US =\n?", "The answer is 2": "x^2 =$\u201cFrance = ?", "The answer is 66": "and\n$x_{\\text{test}} =$ \"Japan = ?\". In this case, however, ChatGPT is unable to provide the correct\nanswer because it fails to find the relationship between the area code and the country. See\nFigure 1 for a visual illustration of CoT and vanilla ICL prompts. Thus, seen from this\nexample, by providing additional reasoning steps, CoT prompts can significantly boost the\naccuracy of the LLM compared with vanilla ICL prompts.\n4.2 Pretrained LLM + CoT Prompting\nThe previous section proposes a latent variable model that captures the multi-step reasoning\nprocess of CoT. Based on this model, we will formulate the estimator constructed by CoT\nprompting on a pretrained autoregressive LLM from a statistical perspective.\nPretraining LLM. We assume that the LLM is pretrained with data generated according"}, {"title": "5 Statistical Errors of CoT Prompting", "content": "In this section", "n-1": "and $h\\in [H", "z_{0": "h"}, {"n": ""}, "for simplicity, i.e., the query error is zero. We will allow a distributional shift in the\nnext section.\nWith no distributional shift in $z_{\\text{test}}$, we can essentially regard the test instance as the\n$(n+1)$-th example, since all the examples are conditionally i.i.d. when they are conditioned\non $\\theta^*$. Thus, in this section, we will only study how the $n$ prompt examples help a perfectly\npretrained LLM infer $\\theta^*$, namely, the in-context error.\nEquivalence Class Induced by Multi-Step Reasoning. In the following, to simplify\nthe notation, we use $X = Z_0, Z_1, ..., Z_H = Y$ to denote a random trajectory sampled from\nthe model in (4.2). Note that the prompting error in (5.1) only concerns the distribution of\nthe output $Y$ and neglects the intermediate reasoning steps $Z_1,...,Z_{H-1}$. As a result, it is\npossible that there exists another $\\theta\\in \\Theta$ with the same distribution of $Y$. Such a relationship\ninduces a set of equivalence classes over $\\Theta$.\nDefinition 5.3 (Equivalence Classes over $\\Theta$). Let $\\mathbb{P}(Z_0, Z_1,\\cdots, Z_{H-1}, Y |\\theta)$ denote the joint\ndistribution of $Z_{0:H}$ conditioning on the latent variable $\\theta^* = \\theta$. We define an equivalence\nrelation $\\sim$ based on conditional density of $Y$ given $Z_0$ as follows.\n$$\\theta \\sim \\theta' \\text{ if and only if } \\mathbb{P}(Y = y | Z_0 = z_0, \\theta) = \\mathbb{P}(Y = y | Z_0 = z_0, \\theta'), \\quad \\forall (z_0, y).$$\nThis relation $\\sim$ induces a set of equivalence classes over $\\Theta$. In particular, for any $\\theta$, define\n$\\Theta_{\\text{eq}}(\\theta) = \\{\\theta' \\in \\Theta : \\mathbb{P}(y | z_0, \\theta) = \\mathbb{P}(y | z_0, \\theta'), \\forall (z_0, y)\\}$ as the set of parameters equivalent to $\\theta$,\ni.e., the equivalence class represented by $\\theta$. Let $\\Theta_{\\text{eq}}$ denote the complete set of representatives\nof all disjoint equivalent classes. Then $\\Theta \\cap \\Theta_{\\text{eq}} (\\theta) \\cap \\Theta_{\\text{eq}} (\\theta') = \\emptyset$ for all $\\theta,\\theta' \\in \\Theta$ and we can\nfurther write $\\Theta$ as $\\cup_{\\theta \\in \\Theta} \\Theta_{\\text{eq}}(\\theta)$.\nThe intuition of the equivalence relation $\\sim$ is that there might be multiple reasoning paths\nthat all lead to the correct answer. For example, Newtonian, Lagrangian, and Hamiltonian\nmechanics are three different approaches to classical mechanics. Their intermediate steps are\ndifferent but will lead to the same answer. Based on this intuition, any parameter in $\\Theta_{\\text{eq}}(\\theta^*)$\nis equally good for predicting $Y$, and we only need to infer $\\Theta_{\\text{eq}}(\\theta^*)$ from CoT prompts.\nWe state a regularity condition for CoT prompting in terms of $\\Theta_{\\text{eq}}(\\theta^*)$.\nAssumption 5.4. Given a task $\\theta^*$ during CoT prompting, let $\\Theta_\\setminus = \\Theta\\backslash\\Theta_{\\text{eq}}(\\theta^*)$ denote the\ncomplement of the equivalence class of $\\theta^*$. We assume that there exists a strict separation\nbetween the ground truth task $\\theta^*$ and any other tasks $\\theta \\in \\Theta_\\setminus$. Specifically, there exists $\\lambda > 0$\nthat lower bounds the Hellinger distance:\n$$\\inf_{\\theta \\in \\Theta_\\setminus} H^2 (\\mathbb{P}(Z_{0:H} = \\cdot|\\theta^*), \\mathbb{P}(Z_{0:H} = \\cdot|\\theta)) \\geq \\lambda,$$\nwhere $H^2(\\cdot,\\cdot)$ denotes the squared Hellinger distance. Moreover, we assume tasks in $\\Theta_{\\text{eq}}(\\theta^*)$\nare well covered by the pretraining distribution in the sense that $\\pi(\\Theta_{\\text{eq}}(\\theta^*)) > 0$.</nAssumption 5.5. Let $\\Theta_{\\text{eq}}$ be a representative set of the equivalence classes introduced in\nDefinition 5.3. We assume that there exist positive numbers $\\alpha$ and $\\alpha_0$ such that for all $\\theta \\in \\Theta$\nand $\\theta' \\in \\Theta_{\\text{eq}}(\\theta)$, we have\n$$\\sup_{Z_{0:H}} \\log \\frac{\\mathbb{P}(Z_{0:H} = z_{0:H}|\\theta)}{\\mathbb{P}(Z_{0:H} = z_{0:H}|\\theta')} \\leq \\alpha,$$\n$$\\sup_{Z_{0}} \\log \\frac{\\mathbb{P}(Z_{0} = z_{0}|\\theta)}{\\mathbb{P}(Z_{0} = z_{0}|\\theta')} \\leq \\alpha_0.$$\nMoreover, we assume that $\\alpha \\in (0, \\lambda)$, where $\\lambda$ appears in Assumption 5.4.\nAssumptions 5.4 and 5.6 imply that distributions are similar within each equivalence\nclass but disparate between equivalence classes. We establish a new upper bound as follows.\nTheorem 5.7. Under Assumptions 5.1, 5.4, and 5.6, with probability $1 - \\delta$ over the ran-\ndomness of the CoT prompt, we have\n$$\\text{err}_{\\text{CoT}} \\leq O(Hb^* \\pi(\\theta^*)^{"]}