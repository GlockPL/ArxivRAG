{"title": "Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods", "authors": ["Xinyang Hu", "Fengzhuo Zhang", "Siyu Chen", "Zhuoran Yang"], "abstract": "Chain-of-Thought (CoT) prompting and its variants have gained popularity as effective methods for solving multi-step reasoning problems using pretrained large language models (LLMs). In this work, we analyze CoT prompting from a statistical estimation perspective, providing a comprehensive characterization of its sample complexity. To this end, we introduce a multi-step latent variable model that encapsulates the reasoning process, where the latent variable encodes the task information. Under this framework, we demonstrate that when the pretraining dataset is sufficiently large, the estimator formed by CoT prompting is equivalent to a Bayesian estimator. This estimator effectively solves the multi-step reasoning problem by aggregating a posterior distribution inferred from the demonstration examples in the prompt.\nMoreover, we prove that the statistical error of the CoT estimator can be decomposed into two main components: (i) a prompting error, which arises from inferring the true task using CoT prompts, and (ii) the statistical error of the pretrained LLM. We establish that, under appropriate assumptions, the prompting error decays exponentially to zero as the number of demonstrations increases. Additionally, we explicitly characterize the approximation and generalization errors of the pretrained LLM. Notably, we construct a transformer model that approximates the target distribution of the multi-step reasoning problem with an error that decreases exponentially in the number of transformer blocks. Our analysis extends to other variants of CoT, including Self-Consistent CoT, Tree-of-Thought, and Selection-Inference, offering a broad perspective on the efficacy of these methods. We also provide numerical experiments to validate the theoretical findings.", "sections": [{"title": "1 Introduction", "content": "Autoregressive Large Language Models (LLMs) have tremendously revolutionized the field of Natural Language Processing (NLP) and related areas due to their striking ability to understand languages and follow instructions. These models, based on the transformer architecture (Vaswani et al., 2017), are probabilistic models that predict the next token based on preceding tokens, also known as a prompt. The training of LLMs typically involves two phases: pretraining and post-training. During the pretraining phase, the LLMs are trained on vast text corpora via unsupervised learning (Ahmad et al., 2021; Zoph et al., 2020; Erhan et al., 2010; Hendrycks et al., 2019). This process allows them to acquire a broad understanding of language and general knowledge. Subsequently, additional post-training approaches, including supervised fine-tuning (Wei et al., 2021) and reinforcement"}, {"title": "2 Related Works", "content": "Our work adds to the literature on theoretically understanding prompting methods. In particular, our work is closely related to CoT prompting and its variants. In addition, our work is related to the body of works that aim to understand the ability of ICL and CoT from both empirical and theoretical perspectives.\nCoT Prompting and its Variants. The vanilla CoT prompting method is proposed in Wei et al. (2022) for solving multi-step reasoning problems using LLMs. Based on this work,"}, {"title": "3 Background", "content": "In this section, we introduce the background knowledge about transformer-based large language models and CoT prompting.\nAutoregressive LLMs. Most commercial LLMs such as GPT-4 (OpenAI, 2023), Claude (Anthropic, 2023), Llama (Touvron et al., 2023), and Gemini (Team et al., 2023), are autoregressive in the sense that they generate in a token-by-token fashion. An autoregressive LLM, denoted by $P_{LLM}$, is a conditional probability model that continuously predicts future tokens based on a sequence of past tokens, known as the prompt. Here we denote the space of all the tokens as $\\mathcal{X}$. Given an input prompt $S_t = (x_1, ..., x_t) \\in \\mathcal{X}^t$, to generate the response to it, the LLM first generate the next token as $x_{t+1} \\in \\mathcal{X} \\sim P_{LLM}(\\cdot|S_t)$. Then it appends the generated token $x_{t+1}$ to the end of $S_t$ to form $S_{t+1} = (S_t, x_{t+1})$. The LLM will generate $x_{t+2}$"}, {"title": "Transformers and Attention Mechanism", "content": "The transformer model is based on the Multi-Head Attention (MHA) mechanism (Bahdanau et al., 2014; Phuong and Hutter, 2022), together with other modules such as the tokenizer and the positional embeddings (Wang and Chen, 2020; Su et al., 2023), residual connections, feed-forward networks, and layer normalization (Ba et al., 2016). The tokenizer maps the input sequence to a sequence of vectors in Euclidean space, and the positional embeddings add the position information of tokens to these vectors.\nThe attention mechanism captures the relationship between different tokens, which is the backbone of transformer-based LLM (Devlin et al., 2018). The attention mechanism takes in queries, keys, and values as inputs, and outputs the response of each query as a weighted sum of values, where the weights are the similarity scores between the query and the keys. Specifically, let $K \\in \\mathbb{R}^{L \\times d_k}$ and $V \\in \\mathbb{R}^{L \\times d_v}$ denote the $L$ key and value vectors, respectively. The attention output of a single query $q \\in \\mathbb{R}^{d_k}$ is computed as:\n$$\n\\text{attn}(q, K, V) = V^T \\text{softmax}(Kq),\n\\qquad(3.1)\n$$\nwhere $\\text{softmax}(Kq)$ is a probability distribution over $[L]$. Here $\\text{softmax}(Kq)$ quantifies the similarity between the query $q$ and each row of $K$, which is used to aggregate the value vectors of $V$. The attention $\\text{attn}(Q, K, V)$ that takes in multiple queries outputs the responses as $V^T \\text{softmax}(KQ^T)$, where $Q \\in \\mathbb{R}^{L \\times d_k}$ contains $L$ query vectors. The predefined attention mechanism captures the relationship between the keys and queries via a single $\\text{softmax}$ module, and thus is called single-head attention. MHA refers to passing the inputs through multiple attention functions in parallel, and outputs the aggregation of these sub-modules. Taking $X \\in \\mathbb{R}^{L\\times r}$ as the input, a MHA layer with $\\eta$ heads outputs\n$$\n\\text{mha}(X, W_{\\text{mha}}) = \\sum_{i=1}^{\\eta} \\text{attn}(XW_Q^i, XW_K^i, XW_V^i),\n\\qquad(3.2)\n$$\nThe parameter set $W_{\\text{mha}} = \\{W_Q^i, W_K^i, W_V^i\\}_{i=1}^{\\eta}$ are the weight matrices for queries, keys, and values, where $W_Q^i \\in \\mathbb{R}^{r \\times d_k}, W_K^i \\in \\mathbb{R}^{r \\times d_k}$, and $W_V^i \\in \\mathbb{R}^{r \\times d_v}$. Intuitively, different heads can attend to different parts of the data, and thus MHA offers a more expressive model class. Compared to the MHA defined in Vaswani et al. (2017), we absorb the matrix $W^O$ into $W_V$ for each head.\nEach MHA layer is followed by a Feed-Forward (FF) layer. Given an input $X \\in \\mathbb{R}^{L \\times r}$, a FF layer with $d_F$ neurons maps the input $X$ to\n$$\n\\text{ff}(X, W_{ff}) = \\text{ReLU}(XW_{ff,1})W_{ff,2}, \\text{where } W_{ff} = \\{W_{ff,1} \\in \\mathbb{R}^{r \\times d_F}, W_{ff,2} \\in \\mathbb{R}^{d_F \\times r}\\} \\qquad(3.3)\n$$\nare weight matrices. There are also normalization layers between the MHA and FF layers. We defer their details to Appendix G.1 for brevity.\nLLM Training. The training of an LLM involves two stages: (i) pretraining (Zoph et al., 2020) and (ii) post-training (Ouyang et al., 2022; Wei et al., 2021). In the pre-training"}, {"title": "4 A Latent Variable View of Multi-Step Reasoning", "content": "In this section, we show that CoT prompting can be understood as a Bayesian estimator on a multi-step latent variable dynamical model. In particular, we propose a multi-step latent variable model in Section 4.1 to capture the multi-step reasoning process, which is further generalized in Appendix A to the non-i.i.d. setting. Then in Section 4.2, we study the practice of CoT prompting of pretrained LLMs from a statistical perspective. In Section 4.3, we show that such a practice is equivalent to a BMA estimator for the multi-step latent variable model, which answers Question (a) raised in Section 1. Moreover, we show that the softmax attention mechanism in the transformer architecture parameterizes the BMA algorithm, which partially answers Question (c).\n4.1 A Multi-Step Latent Variable Model\nWe introduce a multi-step latent variable model to capture the multi-step reasoning process of CoT, which serves as the data-generating model for studying CoT."}, {"title": "CoT Prompting Paradigm", "content": "Recall that we define the CoT prompt $\\text{prompt}_{CoT}(n) = \\{(\\mathbf{s}^i)_{i=1}^n, \\mathbf{z}_{test}\\} = (\\mathcal{Y}_n, \\mathbf{z}_{test})$ in Section 3, which contains $n$ demonstration examples $\\mathcal{Y}_n = \\{\\mathbf{s}^i\\}_{i=1}^n = \\{(\\mathbf{z}_{0:H}^i)\\}_{i=1}^n$ and a testing query $\\mathbf{z}_{test}$. To generate such a prompt, we first specify a latent concept vector, which is denoted as $\\theta^* \\in \\Theta$. Here $\\Theta$ denotes the set of all the latent concepts. Semantically, $\\theta^*$ determines the task we would like to achieve via CoT, e.g., the color description of objects, the calculation of math equations. Thus, we will use the terms task and latent concept interchangeably in the following. Statistically, the latent concept $\\theta^*$ specifies the task-specific joint distribution $P(\\cdot|\\theta^*)$ of demonstration examples and testing query in the prompt, which will be specified later in (4.2). Given the generated prompt $\\text{prompt}_{CoT}(n)$, we feed it to the LLM, and the LLM recursively generates the intermediate steps $(\\mathbf{z}_{test}^1,..., \\mathbf{z}_{test}^{H-1})$ and the final answer $\\mathbf{z}_{test}^H = y_{test}$ via\n$$\n\\mathbf{z}_{h+1}^{test} \\sim P_{LLM}(\\cdot|\\text{prompt}_{CoT}(n), \\mathbf{z}_{test}^1, ..., \\mathbf{z}_{test}^h), \\quad \\forall h \\in [H - 1]. \\qquad(4.1)\n$$\nTo evaluate the performance of CoT, we compare the distribution of $\\mathbf{z}_{H}^{test}$ in (4.1) with the ground truth distribution $P(\\mathbf{z}_{H}^{test} | \\text{prompt}_{CoT}(n), \\theta^*)$, which is the target task-specific distribution of the final answer given the prompt. We illustrate the CoT paradigm with a concrete example as follows."}, {"title": "The Multi-Step Latent Variable Model", "content": "To analyze CoT from a statistical perspective, we need to specify the pre-mentioned task-specific distribution $P(\\cdot|\\theta^*)$, which serves as the data-generating distribution for the CoT prompt. We assume that the concept $\\theta^*$ is a random variable sampled from a prior $\\pi \\in \\mathcal{P}(\\Theta)$ and the examples $\\{\\mathbf{s}^i\\}_{i=1}^n$ are i.i.d. sequences conditioning on $\\theta^* \\in \\Theta$. For any $\\theta \\in \\Theta$, when $\\theta^* = \\theta$, within the reasoning chain $i \\in [n]$, we sample $\\mathbf{z}_{0:H}^i$ according to the following stochastic dynamical system with joint distribution $P(\\mathbf{s}^i | \\theta^* = \\theta)$ given by\n$$\nP(\\mathbf{s}^i | \\theta^* = \\theta) : \\quad\n\\mathbf{z}_{0}^i = f_{\\theta}(\\zeta_0^i), \\quad \\mathbf{z}_{h}^i = F_{\\theta} (\\mathbf{z}_{0}^i,\\mathbf{z}_{h-1}^i, \\zeta_h^i), \\quad \\forall 1 \\leq h \\leq H. \\qquad(4.2)\n$$\nHere $\\{\\zeta_0^i, \\{\\zeta_h^i\\}_{h\\in[H]}\\}_{i\\in[n]}$ are i.i.d. noise variables, and $f_{\\theta}$ and $F_{\\theta}$ are two functions parameterized by $\\theta \\in \\Theta$. The same is true for the test sample $\\mathbf{z}_{0}^{test}$ and this distribution will serve as the target distribution for LLM to learn in context during the prompting stage. Specifically, $f_{\\theta}$ generates the first query $\\mathbf{z}_{0}^i$ based on the task $\\theta^* = \\theta$, and $F_{\\theta}$ models the evolution of the \u201creasoning process\u201d $\\{\\mathbf{z}_{h}^i\\}_{h\\in[H]}$. Specifically, each $\\mathbf{z}_{h}^i$ depends on all of the previous reasoning steps as well as the latent variable $\\theta^*$. The rationale behind this model is that the generation of these reasoning steps is autoregressive and the distribution of the whole sequence is specific to the task $\\theta^*$. The random variables $\\{\\zeta_h^i\\}_{h\\in[H]}$ allow the reasoning process to be stochastic. See Figure 2 for an illustration of this model."}, {"title": "4.2 Pretrained LLM + CoT Prompting", "content": "The previous section proposes a latent variable model that captures the multi-step reasoning process of CoT. Based on this model, we will formulate the estimator constructed by CoT prompting on a pretrained autoregressive LLM from a statistical perspective.\nPretraining LLM. We assume that the LLM is pretrained with data generated according"}, {"title": "CoT Prompting as an Estimator", "content": "After pretraining, we fix the parameter of the LLM as $\\hat{\\rho}$ and prompt the LLM with a CoT prompt $\\text{prompt}_{CoT}(n) = (\\mathcal{Y}_n, \\mathbf{z}_{test})$. To connect the pretraining and prompting stages, we note that prompting a pretrained LLM with prompt induces a conditional distribution $P_{LLM}(\\cdot | \\text{prompt})$. When using a CoT prompt, we aim to induce the LLM to eventually generate a desired final answer defined by (4.2). The distribution of the final answer $y_{test} = \\mathbf{z}_{H}^{test}$ induced by the LLM via CoT reasoning is $P_{LLM}(y_{test} = \\cdot|\\text{prompt}_{CoT}(n))$, which is given by marginalizing out the intermediate steps"}, {"title": "4.3 BMA Interpretation of CoT", "content": "In the following, we show that the CoT estimator $P_{LLM}(\\cdot|\\text{prompt}_{CoT}(n))$ can be understood as a Bayesian model averaging (BMA) estimator for the latent variable model in (4.2).\nPretrained LLM + CoT \u2248 BMA. Recall that the pretraining process of LLM is given in (4.3), where the data is generated from the latent variable model in (4.2). When $N$ and"}, {"title": "4.4 Attention Approximately Parameterizes BMA", "content": "We now show that the attention mechanism in the transformer architecture is able to encode the BMA algorithm for a special case of the latent variable model in (4.2).\nA Simplified Model. In this special case, we Let $f_{\\theta^*}$ in (4.2) be a function independent of $\\theta^*$, i.e., the inputs do not depend on $\\theta^*$. Moreover, we assume that $F_{\\theta^*}$ in (4.2) encodes a linear model in the latent space. Specifically, for any $h \\in [H]$, let $d_k$ and $d_v$ be two integers and let $k: \\mathcal{L} \\rightarrow \\mathbb{R}^{d_k}$ and be $v: \\mathcal{L} \\rightarrow \\mathbb{R}^{d_v}$ be two feature mappings that maps each reasoning"}, {"title": "The BMA Estimator", "content": "To study the BMA estimator under this model, we further impose a Gaussian prior over $\\theta^*$. Specifically, we assume that the entries of $\\theta^*$ are i.i.d. with prior distribution $\\mathcal{N}(0, \\lambda)$ for some fixed $\\lambda > 0$. Based on the $n$ examples in the CoT prompt $\\text{prompt}_{CoT}(n)$, we define $V_n = (v(\\mathbf{z}_h^i))_{h\\in[H], i=1}^n \\in \\mathbb{R}^{d_v \\times Hn}$ and $K_n = (k(\\mathbf{z}_h^i))_{h\\in[H], i=1}^n \\in \\mathbb{R}^{d_k \\times Hn}$ and let $\\phi(K_n)$ denote the $\\mathbb{R}^{d_\\phi \\times Hn}$ feature matrix induced by $K_n$. Under the simplified model, the inputs $\\{\\mathbf{z}_{0}^i\\}_{i\\in[n]}$ and $\\mathbf{z}_{test}$ do not contain information about $\\theta^*$. Thus, conditioning on $\\text{prompt}_{CoT}(n)$, the posterior distribution of $\\theta^*$ is a Gaussian distribution, centered at the ridge estimator\n$$\n\\hat{\\theta} = V_n^T \\phi(K_n) \\left(\\phi(K_n) \\phi(K_n)^T + \\sigma^2/\\lambda \\cdot I\\right)^{-1},\n$$\nwhere $I$ is the identity matrix of size $\\mathbb{R}^{d_\\phi \\times d_\\phi}$.\nGiven any $\\theta \\in \\Theta$ as an estimate of $\\theta^*$ and $\\mathbf{z}_{test}$, to predict $y_{test} = \\mathbf{z}_{H}^{test}$ according to the linear model in (4.6), it suffices to generate $\\{v_{test} = v(\\mathbf{z}_{h}^{test})\\}_{h\\in[H]}$ autoregressively. Specifically, for any $h \\geq 0$, conditioning on $\\{\\mathbf{z}_{test}^0,..., \\mathbf{z}_{test}^{h}\\} = \\{\\mathbf{z}_{test}^0,..., \\mathbf{z}_{test}^{h}\\}$, the distribution of $v_{test}^h$ is $\\mathcal{N}(\\theta(\\mathbf{k}_{test}^h), \\sigma^2)$ where we define $\\mathbf{k}_{test}^h$ as\n$$\n\\mathbf{k}_{test}^h = (k(\\mathbf{z}_{test}^0), k(\\mathbf{z}_{test}^1)..., k(\\mathbf{z}_{test}^h), \\overbrace{0, ..., 0}).\n\\qquad(4.7)\n$$\nTherefore, to get the BMA estimator, we aggregate the distribution of $v_{test}^h$ according to the posterior distribution of $\\theta$, and return the mean value as the predictor, which is given by\n$$\n\\mathbf{v}^{test} = V_n^T \\phi(K_n) \\left(\\phi(K_n) \\phi(K_n)^T + \\sigma^2/\\lambda \\cdot I\\right)^{-1} \\phi(\\mathbf{k}_{test}).\n\\qquad(4.8)\n$$\nThe final BMA estimator is given by $\\{v^{-1}(\\mathbf{v}_{test}^h)\\}_{h\\in[H]}$."}, {"title": "Estimator Produced by Transformer", "content": "In the following, we introduce another autoregressive estimator based on a transformer with softmax attention. Transformer is a mapping that maps a sequence of vectors to another sequence of vectors and the mapping involves"}, {"title": "5 Statistical Errors of CoT Prompting", "content": "In this section, we study the error incurred during the prompting stage. We first state an error decomposition result and then study the vanilla CoT prompting in Section 5.1. Then we extend the theory to three variants of CoT in Section 5.2 and compare CoT with vanilla ICL in Section 5.3. Regarding the four questions raised in the introduction, this section answers Question (b) partly and Question (d).\n5.1 Statistical Errors of Vanilla CoT\nRecall that we define the statistical error induced by the CoT prompting $\\text{err}_{CoT}$ in equation (4.4) and the error comes from both pertaining and prompting stages, as listed in table 1. We explicitly decompose these two error sources as follows. To this end, we first state a regularity condition for the pretrained LLM. Before we proceed, let us define the following partial prompt $\\text{prompt}_{CoT}^{/i}$. For any integers $i \\in [0, n - 1]$ and $h\\in [H]$, we let $\\text{prompt}_{CoT}^{/i} = \\{\\mathbf{s}^j\\}_{j\\leq i} \\cup \\{\\mathbf{z}_{0:h+1}^{i+1}\\}$. That is, $\\text{prompt}_{CoT}^{/i}$ contains the first $i$ demonstration examples and the first $h$ steps of the $(i+1)$-th example. Let $\\theta^*$ denote the target task. In Section 5, we assume that the prompt is generated from the ground truth distribution, meaning that $\\text{prompt}_{CoT}(n) \\sim P(\\cdot | \\theta^*)$.\nAssumption 5.1. We assume there exists a positive number $b^*$ such that for any $0 < h \\leq H$, $0 \\leq i \\leq n - 1$, and $\\text{prompt}_{CoT}^{/i} \\in \\mathcal{L}^*$, we have for the data distribution $P$ and the pretrained model $P_{LLM}$ that\n$$\n\\sup_{\\mathbf{z} \\in \\mathcal{L}} \\Big| \\log P(\\mathbf{z}_{i+1}^h = z | \\text{prompt}_{CoT}^{/i}) - \\log P_{LLM}(\\mathbf{z}_{i+1}^h = z | \\text{prompt}_{CoT}^{/i}) \\Big| \\leq b^*.\n$$\nThis assumption postulates that the true distribution $P$ of the model in (4.2) and that learned by the LLM are close. The proximity is measured in terms of the log likelihood. We will justify the existence of $b^*$ in Section 6 under explicit assumptions on pretraining.\nLemma 5.2 (CoT Error Decomposition). Under Assumption 5.1, the statistical error $\\text{err}_{CoT}$ in (4.4) can be upper bounded by the sum of a pretraining error and a prompting error, i.e.,\n$$\n\\text{err}_{CoT} \\leq \\text{err}_{pre} (P, P_{\\hat{\\rho}}; \\text{prompt}_{CoT}(n)) + \\text{err}_{prompt}(P, \\theta^*, \\text{prompt}_{CoT}(n)).\n$$\nwhere we define the prompting error as\n$$\n\\begin{aligned}\n\\text{err}_{prompt} (P, \\theta^*, \\text{prompt}_{CoT}(n)) = &\\\\\n&KL\\left(P(y_{test} = \\cdot | \\mathbf{z}_{test}, \\theta^*), P(y_{test} = \\cdot | \\text{prompt}_{CoT}(n))\\right)\\\\\n&+ 2\\sqrt{2}Hb^* \\cdot KL^{1/2}\\left(P(y_{test} = \\cdot | \\mathbf{z}_{test}, \\theta^*), P(y_{test} = \\cdot | \\text{prompt}_{CoT}(n))\\right),\n\\end{aligned} \\qquad(5.1)\n$$\nand the pretraining error as\n$$\n\\text{err}_{pre} (P, P_{\\hat{\\rho}}; \\text{prompt}_{CoT}(n)) = KL\\left(P(y_{test} = \\cdot | \\text{prompt}_{CoT}(n)), P_{\\hat{\\rho}}(y_{test} = \\cdot | \\text{prompt}_{CoT}(n))\\right). \\qquad(5.2)\n$$"}, {"title": "5.2 Statistical Errors of Variants of CoT", "content": "The predictions of LLMs are inherently stochastic, which is a main source of LLM hallucination (Huang et al., 2023a; Tonmoy et al., 2024). To increase the prediction accuracy, various selection techniques such as majority vote (Wang et al., 2022) and tree search (Yao et al., 2023) are combined with CoT. In the following, we modify Theorem 5.5 for a few variants of CoT, including Self-Consistency CoT (Wang et al., 2022), Tree-of-Thought (Yao et al., 2023), and Selection-Inference (Creswell et al., 2022). For simplicity, we also assume zero pretraining error and input query does not have a distributional shift, i.e., $P_{LLM} = P$ and $\\mathbf{z}_{test} \\sim P(\\cdot|\\theta^*)$.\nSelf-Consistency CoT (SC-COT)\nGiven the same prompt as in vanilla CoT, i.e., $\\text{prompt}_{CoT}(n)$, SC-CoT first generate $K$ i.i.d. reasoning paths and then output the final answer by a majority vote. That is, we first sample $K$ i.i.d. reasoning paths $\\{\\mathbf{z}_{0:H}^{test, k}\\}_{k=1}^K \\sim P(\\cdot | \\text{prompt}_{CoT}(n))$ and then report the mode of the empirical distribution of $\\{y_{test,i}\\}_{i=1}^K$, denoted by $\\hat{y}_K$. The empirical distribution of $\\{y_{test,i}\\}_{i=1}^K$ is denoted by $p_K(y) = K^{-1} \\sum_{i=1}^K \\mathbb{1}\\{y_{test,i} = y\\}, \\forall y \\in \\mathcal{L}$. The sample mode $\\hat{y}_K$ is defined as $\\hat{y}_K = \\arg \\max_{y\\in \\mathcal{L}} p_K(y)$, where we pick any element if there are multiple maximizers. See Figure 5 for an illustration."}, {"title": "Tree-of-Thought (ToT)", "content": "Recall that SC-CoT samples multiple parallel reasoning paths and performs a selection in the last step. Tree-of-Thought (Yao et al., 2023) instead proposes to include selection in"}, {"title": "Population Problem", "content": "The goal of ToT is to select the optimal reasoning path that solves a desired task. Mathematically, for each step h, let $t_h = (\\mathbf{z}_0,..., \\mathbf{z}_h)$ denote the partial history up to step $h$. Let $V_{\\theta^*}$ be a function that maps each partial history to a value in $[0, 1]$. Intuitively, $V_{\\theta^*}$ can be viewed as the success probability of the partial history for solving task $\\theta^*$. Starting from $\\mathbf{z}_{test}^0$, the optimal reasoning path is obtained by solving\n$$\n\\mathbf{t}_{h}^{test,*} = (\\mathbf{t}_{h-1}^{test,*}, \\mathbf{z}_h^{test,*}), \\text{ where } \\mathbf{z}_h^{test,*} = \\arg \\max_{\\mathbf{z}_{h}^{test}} V_{\\theta^*} (\\mathbf{t}_{h-1}^{test,*}, \\mathbf{z}_{h}^{test}), \\quad \\mathbf{t}_0^{test,*} = \\mathbf{z}_{test}^0, \\mathbf{z}_{test}^0. \\qquad(5.3)\n$$\nMoreover, let $P(\\mathbf{z}_{0:H} = \\cdot|\\theta^*)$ be the task-specific distribution of the multi-step latent variable model defined in (4.2). At the population level, the goal is to draw samples from such a distribution, and select the optimal reasoning path according to the value function $V_{\\theta^*}$. In the following, we condition on $\\text{prompt}_{CoT}(n)$, and thus the optimal reasoning path $\\mathbf{z}_{0:H}^{test,*}$ can be regarded fixed.\nTree-of-Thought with Breadth-First-Search. As we do not have access to the distribution $P(\\mathbf{z}_{test} = \\cdot|\\theta^*)$, ToT proposes to sample from the LLM and then approximately solve (5.3) via selection. To simplify the notation, for each $h \\in [H]$, we denote $\\mathbf{t}_{h}^{test} = (\\mathbf{z}_{test}^0,..., \\mathbf{z}_{test}^{h})$, which is the partial history of the test example up to step $h - 1$. In step $h$, instead of passing the complete prompt $\\text{prompt}_{CoT}(n)$, we truncate each demonstration in $\\text{prompt}_{CoT}(n)$ up to step $h$ and denote the truncated prompt by $\\text{prompt}_h(n) = \\{\\mathbf{z}_{0:h}^i| \\mathbf{z}_{0:H}^i \\in \\text{prompt}_{CoT}(n)\\}$. Then the LLM samples $\\mathbf{z}_{test}^h \\sim P(\\cdot | \\text{prompt}_h(n), \\mathbf{t}_{h-1}^{test})$ and obtain $\\mathbf{t}_{h}^{test}$, and so on."}, {"title": "Selection-Inference (SI)", "content": "Selection-Inference (SI) (Creswell et al., 2022) is a structured LLM reasoning method that decomposes each step of reasoning into two components \u2013 a selection module that retrieves relevant facts from the context and an inference module that predicts the next step solely based on the selected facts. To this end, SI uses an LLM as both a selection module and an inference module through prompting. The selection module extracts information from the reasoning path and the inference module predicts the next reasoning step based on the information extracted from the selection module.\nA Hierarchical Latent Variable Model. In the context of SI, we assume a special case of the model in (4.2) with a hierarchical structure. Specifically, we assume the latent variable $\\theta^*$ has two component $\\theta^* = (\\theta_{se}^* , \\theta_{in}^*)$ and the examples of reasoning paths are i.i.d. given $\\theta^*$, which has a prior distribution $\\pi$. Let $\\{\\mathbf{z}_0,...,\\mathbf{z}_H\\}$ be a reasoning path. We let $\\mathbf{t}_h = \\{\\mathbf{z}_0,...,\\mathbf{z}_h\\}$ be the partial history up to step $h$. We assume that $\\mathbf{z}_{h+1}$ depends on $\\mathbf{t}_h$ only through a subset of $\\mathbf{t}_h$, denoted by $\\mathcal{T}_{h+1}$, and $\\mathcal{T}_{h+1} \\subseteq \\mathbf{t}_h$ is selected from $\\mathbf{t}_h$. Specifically, the joint distribution of $P(\\mathbf{z}_{0:H} |\\theta^*)$ is given by\n$$\n\\mathbf{z}_0 \\sim P(\\mathbf{z}_0 = \\cdot|\\theta^*), \\qquad \\mathcal{T}_{h+1} \\sim P(\\mathcal{T}_{h+1} = \\cdot|\\mathbf{t}_{h}, \\theta_{se}^*), \\qquad \\mathbf{z}_{h+1} \\sim P(\\mathbf{z}_{h+1} = \\cdot| \\mathcal{T}_{h+1},\\theta_{in}^*), \\qquad(5.5)\n$$\nwhere $\\mathbf{t}_0 = \\{\\mathbf{z}_0\\}$ and $\\mathbf{t}_h = \\mathbf{t}_{h-1} \\cup \\{\\mathbf{z}_h\\}$. Intuitively, this model captures the fact that reasoning often involves summarizing existing information and making predictions. The selection"}, {"title": "SI Prompting", "content": "The SI prompting method solves a multi-step reasoning problem following the hierarchical structure specified in (5.5), with the unknown task $\\theta^*$ inferred implicitly via in-context learning. Specifically, given a desired task $\\theta^*$ and a query input $\\mathbf{z}_{test}^0$, we sample $n$ i.i.d. samples from the distribution in (5.5), denoted by $\\{\\mathbf{z}_{0:H}, \\mathcal{T}_H\\}_{h\\in[H],i\\in[n]}$. We define $\\mathcal{S}_{se}(n)$ and $\\mathcal{S}_{in}(n)$ as\n$$\n\\mathcal{S}_{se}(n) = \\{\\mathbf{t}_{h-1}^i, \\mathcal{T}_h^i\\}_{h\\in[H],i\\in[n]}, \\qquad \\mathcal{S}_{in}(n) = \\{\\mathcal{T}_h^i, \\mathbf{z}_h^i\\}_{h\\in[H],i\\in[n]}, \\qquad(5.6)\n$$\nwhere $\\mathbf{t}_h^i$ is the partial history of the $i$-th example. That is, $\\mathcal{S}_{se}(n)$ and $\\mathcal{S}_{in}(n)$ contain the demonstration examples for selection and inference, respectively."}]}