{"title": "Optimizing Variational Quantum Circuits Using Metaheuristic Strategies in Reinforcement Learning", "authors": ["Michael K\u00f6lle", "Daniel Seidl", "Maximilian Zorn", "Philipp Altmann", "Jonas Stein", "Thomas Gabor"], "abstract": "Quantum Reinforcement Learning (QRL) offers potential advantages over classical Reinforcement Learning, such as compact state space representation and faster convergence in certain scenarios. However, practical benefits require further validation. QRL faces challenges like flat solution landscapes, where traditional gradient-based methods are inefficient, necessitating the use of gradient-free algorithms. This work explores the integration of metaheuristic algorithms - Particle Swarm Optimization, Ant Colony Optimization, Tabu Search, Genetic Algorithm, Simulated Annealing, and Harmony Search - into QRL. These algorithms provide flexibility and efficiency in parameter optimization. Evaluations in 5x5 MiniGrid Reinforcement Learning environments show that, all algorithms yield near-optimal results, with Simulated Annealing and Particle Swarm Optimization performing best. In the Cart Pole environment, Simulated Annealing, Genetic Algorithms, and Particle Swarm Optimization achieve optimal results, while the others perform slightly better than random action selection. These findings demonstrate the potential of Particle Swarm Optimization and Simulated Annealing for efficient QRL learning, emphasizing the need for careful algorithm selection and adaptation.", "sections": [{"title": "I. INTRODUCTION", "content": "Computer science is advancing significantly, particularly in Quantum Computing and Reinforcement Learning (RL). These developments present new research and application opportunities in industries such as automotive, chemistry, and finance [1]. This work addresses the intersection of these two fields, known as QRL, which combines quantum mechanical principles with machine learning strategies, presenting new challenges and opportunities [2].\nOne major issue in Quantum Computing is the flat solution landscape and vanishing gradients characteristic of variational quantum circuits [3], posing significant challenges for gradient-based methods used in conventional RL applications [4]. In quantum environments, gradient-based methods can be less effective due to inadequate or misleading gradient information [3], impacting traditional optimization techniques in QRL. Therefore, alternative optimization methods need exploration. Studies like Chen et al.'s [5] demonstrate the successful use of genetic algorithms for parameter optimization in QRL. This work aims to systematically compare various gradient-free, metaheuristic optimization approaches in QRL, focusing on evaluating their effectiveness in different QRL scenarios and deriving recommendations for using metaheuristics in QRL.\nWe evaluate the metaheuristics Simulated Annealing (SA), Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO), Tabu Search (TS), and Harmony Search (HS) for optimizing Variational Quantum Circuits (VQC) parameters and compare them to existing benchmarks of Genetic Algorithms (GAs) for QRL. We test these methods in the Cart Pole environment and a Gridworld navigation task, and their performance will be evaluated based on criteria such as performance, convergence speed, and stability to provide a comprehensive comparison. For full reproducibility of our results, the experiment code is available here\u00b9.\nThe structure of this work is as follows: Section II presents the selected metaheuristic algorithms. The experimental setup is described in Section III. Experimental results are presented and discussed in Section IV. The work concludes with a summary, answers to the research questions, and an outlook in Section V."}, {"title": "II. \u041c\u0415\u0422\u0410HEURISTIC OPTIMIZATION ALGORITHMS", "content": "Metaheuristic optimization algorithms have proven effective for solving complex optimization problems in classical RL and QRL. For instance, genetic algorithms have already been used to optimize hyperparameters of Deep Q-Networks, significantly improving agent performance [6]. In QRL, Lockwood and Si and Chen et al. successfully applied genetic algorithms to optimize parameters of VQCs, leading to more efficient exploration and enhanced learning performance [5], [7].\nThis Section focuses on metaheuristic optimization algorithms in QRL. It introduces selected metaheuristics, elaborates the algorithm and discusses their application in QRL."}, {"title": "A. Simulated Annealing", "content": "SA is a probabilistic optimization algorithm inspired by metallurgical processes. Introduced in the 1980s by Kirkpatrick, Gelatt, and Vecchi [8], and independently by \u010cern\u00fd [9], SA simulates the process of gradually cooling a material to achieve thermodynamic equilibrium. The algorithm starts with a random solution and a high temperature, which gradually decreases, reducing the probability of accepting suboptimal solutions and allowing a finer search near a local optimum. SA's effectiveness lies in overcoming local optima, making it suitable for complex, rugged landscapes where traditional descent algorithms often fail [8]. In our implementation SA explores the parameter space by sampling different parameter configurations and then uses gradient descent to approach optimal solutions. The temperature mechanism enables it to escape local optima."}, {"title": "B. Particle Swarm Optimization", "content": "PSO is a stochastic, population-based optimization algorithm inspired by the social behavior of bird flocks and fish schools. Introduced in 1995 by Kennedy and Eberhart, PSO simulates social interaction and communication within a group of individuals (particles) to find optimal solutions in a multidimensional search space [10]. Each particle is associated with a potential solution, adjusting its position and velocity based on its own and neighbors' experiences. PSO is effective for various optimization problems, including function optimization and neural network training, due to its simplicity and ability to solve non-linear problems without gradient information [11]. We implement PSO by updating the swarmagents' positions and velocity depending on the individuals personal best found rewards and the best found rewards of the swarm. This way they orientate towards the global best solution."}, {"title": "C. Ant Colony Optimization", "content": "ACO is a probabilistic approach based on ant foraging behavior, developed in the early 1990s by Dorigo [12]. Ants explore the solution space, leaving pheromone trails that influence the probability of others choosing the same path. This behavior is modeled to solve optimization problems, particularly discrete ones like route planning. ACO is particularly effective for combinatorial optimization problems like the traveling salesman problem and network design, owing to its ability to find good solutions for complex problems where traditional methods struggle [12]. ACO is implemented by creating a pheromone-matrix the size of the number of agents times itself. Based on the found rewards of the agents/ants, the matrix gets updated and therefore the probabilities for selecting a specific agent change. The selected agent then gets replicated and slightly mutated, to represent a new agent in the next iteration. This simulates the ant moving into the direction of a strong pheromon source, e.g. a successful other ant in regards to obtaining a higher reward."}, {"title": "D. Tabu Search", "content": "TS, developed by Fred Glover in the late 1980s, uses memory structures that record recently explored solutions to avoid cycles and encourage exploration of new areas [13]. The Tabu list temporarily forbids certain moves, increasing the likelihood of escaping local optima. TS is effective for scheduling, route planning, and network design due to its ability to quickly find good solutions while deeply exploring the search space [14]. We implemented a tabu list, which stores a list of solution candidates, that are not accepted. This is based off of their performance, e.g. worse rewards than other candidates. A neighborlist is compared with the tabu list and non-tabu solution candidates will be taken into account. The tabu list then gets updated with the newly selected candidate, so new solutions can be considered more."}, {"title": "E. Harmony Search", "content": "Harmony Search (HS), developed in 2001 by Zong Woo Geem, Joong Hoon Kim, and G. V. Loganathan, is inspired by the musical process of searching for a perfect harmony [15]. It uses a harmony memory database to store the best solutions found and generates new solutions by combining random selection, memory consideration, and pitch adjustment. HS is applied in engineering, transport, and energy distribution for its simplicity, flexibility, and robustness against initial conditions and parameter settings [16]. In our work, HS is implemented by creating a harmony memory, that stores a set amount of best solutions. Based off of those, new solutions are created via a small manipulation, depending on variables like bandwidth. The harmony memory then gets updated, with newly and possibly better found solutions, replacing the current worst solution in the harmony memory in the case of a higher reward being found."}, {"title": "F. Genetic Algorithms", "content": "GAs are evolutionary algorithms simulating natural selection, introduced by John Holland [17] in the 1970s. They start with a population of potential solutions that evolve toward optimality through selection, mutation, and crossover. Fitter solutions, as determined by a fitness function, are more likely to be chosen for breeding, guiding the population toward better solutions. GAs are effective for complex optimization problems due to their ability to avoid local optima [18]. Chen et al. [19] integrate GAs with quantum computing to optimize quantum circuits for reinforcement learning tasks, showing GAs' utility in high-dimensional problem-solving. In this work, top candidates are selected for mutation to create new generations."}, {"title": "III. EXPERIMENTAL SETUP", "content": "This Section provides a overview of the components and methodologies utilized in the study. It includes the specific environments we used for testing, namely the 5 \u00d7 5 MiniGrid and Cart Pole environment and the variational quantum circuit architecture. The Section also covers the evaluation metrics and hyperparameter selection."}, {"title": "A. Environments", "content": "We evaluate our selected metaheuristics in two popular RL environments for their effectiveness and adaptability in QRL. We use the 5 x 5 MiniGrid and Cart Pole environment from the OpenAI Gym library [20].\n1) 5 \u00d7 5 MiniGrid Environment: The 5\u00d75 MiniGrid environment [21] is a simplified, grid-based simulation scenario. We use the Empty variant, where an agent navigates a 5x5 grid aiming to reach a target position. The observation space has a dimensionality of 75, derived from 5 \u00d7 5 \u00d7 3. In this environment, the Markov Decision Process (MDP) is defined as follows: The state set S includes all possible agent positions in the 5x5 grid. The action set A comprises six actions: turn left, turn right, move forward, pick up object, drop object, and use object [21]. Transition probabilities Pare deterministic, moving the agent to the neighboring state unless blocked by a wall or obstacle. The reward function R assigns a positive reward for reaching the target, decreasing with the number of steps taken, while failure to reach the target results in zero reward [5]. The discount factor y is typically high, promoting long-term planning [22]. The objective is to find an optimal policy \u03c0* that maximizes the expected cumulative reward."}, {"title": "2) Cart Pole Environment:", "content": "The Cart Pole environment, also known as the Inverted Pendulum, challenges an agent to balance a pole on a cart by moving the cart left or right. This environment is ideal for demonstrating control of continuous systems and balance tasks [20], [23]. The MDP is defined as follows: The state set S includes the cart's position and velocity and the pole's angle and angular velocity. The action set A consists of moving the cart left or right. Transition probabilities Pare determined by differential equations modeling the system's dynamics. The reward function R gives +1 for each timestep the pole remains upright, ending the episode if the pole deviates too far or the cart moves out of bounds, with a maximum reward of 500. The discount factor y is usually high, encouraging long-term stability. The agent's objective is to keep the pole upright for as long as possible by controlling the cart. The dynamics of Cart Pole are modeled by differential equations, representing the continuous nature of states and actions. The reward function R : S \u00d7 A \u2192 R is designed to encourage balancing the pole near vertical and penalizing deviations."}, {"title": "B. Variational Quantum Circuits", "content": "The architecture of a VQC typically consists of three parts: the encoding, the variational part, and the measurement (as seen in Figs. 1 and 2). The encoding is responsible to handle classical input and encode it into the Hilbert space. The variational part contains trainable parameters and is optimized by a classical optimizer to fit the target function. In our RL case, the VQC architecture depends on the dimensions of the state and action space of each environment. In current NISQ hardware, environment states may be too big to be encoded in the limited number of qubits available. Therefore, the state may be compressed. Low-dimensional environments like Cart Pole do not require dimensionality reduction, and observations can be encoded directly into qubits using, e.g., Amplitude Encoding [5]. For higher-dimensional environments (like MiniGrid), Matrix Product States (MPS) can reduce complexity without significant information loss [5].\n1) Matrix Product States: MPS efficiently represent and manipulate quantum states [24]. For the 5 \u00d7 5 MiniGrid environment, the state vector dimension is 75 (5 \u00d7 5 \u00d7 3) [21]. Applying a feature map (\u03c5) with features \u2208 {1, ..., N} reduces this complexity:\n$\\upsilon \\rightarrow |\\varphi(\\upsilon)) = |\\varphi(v_1)) \\otimes |\\varphi(v_2)) \u00b7\u00b7\u00b7 |\\varphi(v_n))$, (1)\nWith each (vj) as a d-dimensional vector. Following Chen et al. [5], we use d = 2:\n$\\varphi(v_j) = [1 - v_j, v_j] $. (2)\nMPS transforms the chain of matrices into a state vector, reducing input dimension from 75 to 8 for efficient processing. [5]\n2) Variational Encoding: Variational Encoding, combined with MPS, prepares states in quantum circuits. The compressed state vector from MPS serves as input. Hadamard gates create an initial state:\n$|\\Psi_0) = H^{\\otimes n} |0\\rangle^{\\otimes n} = \\frac{1}{2^{n/2}} \\sum_{i=0}^{2^n-1} |i\\rangle$, (3)\nRotational gates Ry and Rz encode the state of the environment:\n$R_y(\\arctan(x_i)) R_z(\\arctan(x_i))$, (4)\nwhere xi is from the compressed state vector. Overall:\n$|\\Psi_{enc}) = \\prod_{i=1}^{n/2}(R_y(\\arctan(x_i)) R_z(\\arctan(x_i))) |\\Psi_0\\rangle$. (5)\nThis encoded state is then used by the VQC for decision-making.\n3) Amplitude Encoding: The Cart Pole environment provides a 4-dimensional observation vector representing the cart's position and velocity, and the pole's angle and rotational velocity. Amplitude Encoding translates real vectors into the amplitudes of a quantum state [25]\u2013[27]. An n-dimensional real vector x = (x1,...,xn) is mapped to a quantum state $|\\Psi_x\\rangle$ represented by $[\\log_2 n]$ qubits, where the amplitudes are the normalized components of the (possibly zero-padded) input vector:\n$|\\Psi_x\\rangle = \\frac{1}{\\sqrt{\\sum_{i=1}^{n}x_i^2}} \\sum_{i=1}^{n}x_i|i\\rangle$, (6)\nFor the Cart Pole environment, the 4-dimensional observation requires 2 qubits:\n$|\\Psi_x\\rangle = \\frac{x_1 |00\\rangle + x_2 |01\\rangle + x_3 |10\\rangle + x_4 |11\\rangle}{\\sqrt{\\sum_{i=1}^{4} |x_i|^2}}$. (7)"}, {"title": "4) Variational Quantum Circuit:", "content": "VQC is essential in Quantum Machine Learning, especially in QRL. For Cart Pole, Amplitude Encoding is used. For 5\u00d75 MiniGrid, MPS reduces state complexity before encoding. VQC uses these states to make decisions, optimizing actions based on encoded states. The action selection process in this work is similar to Quantum Deep Q-Learning. [5]"}, {"title": "C. Evaluation Metrics", "content": "1) Learning Speed: The learning speed $v_i$ indicates how quickly an algorithm can develop effective strategies. It is assessed by the number of episodes (E) needed to reach a specific performance level (P):\n$v_i = \\frac{P}{E}$. (8)\nFaster learning speed (i.e. smaller $v_i$) is desirable in resource-limited or real-time conditions.\n2) Stability: The stability S refers to the consistency of an algorithm's performance across multiple runs, quantified as the standard deviation (i.e. \u03c3) of performance (Pi) over N runs:\n$\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N}(P_i - \\overline{P})^2}$. (9)\nLower standard deviation (smaller \u03c3) indicates higher stability.\n3) Maximum Performance: The maximum performance $P_{max}$ indicates the highest achieved performance (average reward), reflecting the effectiveness of the learning process."}, {"title": "D. Hyperparameter Selection", "content": "We determined the optimal hyperparameters for each meta-heuristic algorithm through a grid search, systematically testing different combinations to identify the best configuration for each environment. Each algorithm is allocated the same computation time for fairness. Agents are initialized with parameter values drawn from a standard normal distribution N(0,1), scaled by 0.01 for suitable initial values. Each hyperparameter configuration was run with 3 different seeds \u2208 [0, 2]. A maximum runtime per run of tmax = 20000 seconds is set to ensure sufficient iterations and convergence without excessive computation time. The optimal hyperparameters for each metaheuristic can be found in Table I."}, {"title": "IV. RESULTS", "content": "This section presents the experimental results from applying selected metaheuristic algorithms to QRL problems in the 5\u00d75 MiniGrid and Cart Pole environments. The performance of each algorithm is compared to highlight their strengths and weaknesses."}, {"title": "A. Results in the 5 x 5 MiniGrid Environment", "content": "In the 5\u00d75 MiniGrid environment, the maximum achievable score is 0.955. Fig. 3a shows the results of each algorithm over a runtime of Tmax = 20000 seconds (approximately 5.5 hours) on a Intel(R) Core(TM) i5-4570 CPU @ 3.20GHz. All algorithms quickly reach average rewards above 0.7. Except for ACO, all algorithms exceed rewards of 0.8 within 30 minutes. ACO converges to 0.83 after 1.75 hours. HS, TS, and SA behave similarly, reaching between 0.83 and 0.85 after a little over an hour and converging to 0.88 after 2.75 hours. SA finds better solutions earlier, reaching 0.88 after 1.5 hours and converging to 0.9-0.91 after nearly 4 hours. PSO performs best, reaching 0.85 early and 0.9 after less than 15 minutes, converging to 0.93-0.94 in less than 2 hours. At the same time, the GA does not reach near-optimal rewards. As ACO it stays below rewards of 0.8, after roughly 3.5 hours it then finds better solutions, but with a high difference in top rewards over the 5 runs (low stability), ranging from 0.8-0.94.\nThese results indicate that PSO excels with quick convergence and high rewards, while ACO is less suitable for this environment, converging slower and achieving lower maximum rewards. All algorithms outperform a random action selection algorithm, as shown in Fig. 3a."}, {"title": "B. Results in the Cart Pole Environment", "content": "In the Cart Pole environment, the maximum score is 500. Fig. 3b shows the results of each metaheuristic. ACO performs the worst, converging to a reward of 72 after 2 hours. HS and TS have similar performance, reaching rewards of 70 after 30 minutes and converging to 85 and 90, respectively, after around 3 hours. SA performs well, achieving rewards of 300 within 15 minutes and converging to the maximum reward of 500 in 3 out of 5 runs after 1 hour. PSO performs the best, achieving rewards of 400 within 10 minutes and converging to the maximum reward of 500 after about 5 hours. Other than for the 5 x 5 MiniGrid Environment, the GA starts slow like the bottom 4 algorithms but then quickly finds better solutions over time, getting rewards between 420 and 500 after 5 hours of runtime.\nThese results show that SA and PSO perform excellently, finding optimal solutions quickly. GA also finds optimal solutions, even if not in all of the runs. HS and TS perform worse, needing longer times and achieving lower rewards. ACO is unsuitable, converging slowly and achieving low rewards."}, {"title": "C. Comparison of Metaheuristic Algorithms", "content": "In the 5\u00d75 MiniGrid environment, PSO achieves the highest learning speed, reaching a maximum performance of 0.931 in 6380 seconds (1.77 hours), and shows the highest average performance of 0.931. In the Cart Pole environment, PSO again demonstrates the highest learning speed, achieving a score of 498 in 9760 seconds (2.71 hours), and exhibits the highest average performance of 498. PSO also shows the highest stability in the Cart Pole environment with a standard deviation of 4.47, while ACO has the highest stability in the 5 \u00d7 5 MiniGrid environment with a standard deviation of 0.004. The GA exhibits low stability in both environments.\nPSO's high adaptability is evident as it archives the highest maximum performance and maintains good stability across both environments. SA also displays good adaptability but with higher performance variability in the Cart Pole environment. HS, TS, and ACO perform near-optimal in the 5\u00d75 MiniGrid environment but poorly in the Cart Pole environment, indicating lower adaptability.The GA performs less optimal in the 5\u00d75 MiniGrid environment but shows high performance in the Cart Pole environment. Overall, PSO demonstrates the highest adaptability and performance in both environments, with the fastest learning speed, highest maximum performance, and good stability, making it particularly suitable for optimizing QRL models across various problems. SA also performs well, especially when high maximum performance is prioritized over consistency.The GA performs well in the Cart Pole environment but shows weaker performance in the 5 \u00d7 5 MiniGrid environment. HS, TS, and ACO are less robust, showing acceptable results in specific environments but less adaptability to different problems. It is important to note that optimization with the GA reaches near-optimal solutions in both environments after a prolonged period, i.e. significantly longer than the runtimes shown in this work."}, {"title": "V. CONCLUSION", "content": "This study evaluated various metaheuristic optimization algorithms for QRL in the 5 \u00d7 5 MiniGrid and Cart Pole environments, using criteria such as learning speed, stability, maximum performance, and adaptability. The PSO algorithm demonstrated the highest learning speed, maximum performance, and adaptability in both environments, showing good stability across runs. The SA algorithm also performed well, particularly in terms of maximum performance, though it showed less stability in the Cart Pole environment.The GA demonstrates high performance in the Cart Pole environment but requires longer to reach near-optimal solutions in both environments. Other algorithms, including HS, TS, and ACO, performed acceptably in the 5 \u00d7 5 MiniGrid environment but poorly in the Cart Pole environment, indicating lower adaptability.\nFuture research could explore the performance of meta-heuristic algorithms in more complex environments with higher-dimensional state spaces or continuous action spaces. Real-world applications, such as robotics, could also be investigated. Additionally, optimizing algorithm-specific hyperparameters through automated methods could enhance efficiency. Developing adaptive hyperparameter strategies and hybrid approaches that combine different metaheuristic algorithms could further improve performance. Finally, testing these algorithms on actual quantum hardware rather than simulators could uncover new challenges and opportunities in QRL."}]}