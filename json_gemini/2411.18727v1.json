{"title": "Generative Visual Communication in the Era of Vision-Language Models", "authors": ["Yael Vinker"], "abstract": "Visual communication entails the use of visual elements to convey ideas and information. It is considered a design discipline, often synonymous with graphic design. This encompasses various visual mediums, including drawings, signs, icons, posters, typography, illustrations, advertising, animation, and more. Effective visual communication designs should be concise and simple so that they convey messages clearly to a large audience, transcending geographical and cultural barriers. In addition, the demand for compelling and memorable designs in a world saturated with visual stimuli is becoming more and more difficult to meet.\nIn this dissertation, we pose the question: Can computers automatically produce effective visual communication designs that are clear and concise across various mediums? Recent advancements in large vision-language models have brought this question to the forefront. These models have demonstrated unprecedented capabilities in generating high-quality, realistic images from textual descriptions, and have rapidly gained popularity among professional and novice designers. Our aim is to demonstrate how pretrained large vision-language models can be utilized to address challenging design-related tasks within visual communication, which require rich visual knowledge and deep understanding of complex concepts. This has the potential to support designers in the challenging process of creating compelling graphic designs that deliver messages effectively. We examine this objective from multiple perspectives.\nIn a first research direction we focus on sketches and visual abstractions - fundamental elements of visual expression and creativity. We introduce two optimization-based generative tools, CLIPasso and CLIPascene, to generate sketches at different levels of abstraction from images. CLIPasso focuses on object sketching, adjusting the number of vector strokes to control abstraction, while CLIPascene extends this approach to scene sketching, and broadens the notion of sketch abstraction. Both tools utilize the prior of a pretrained vision-language model to guide the generation process, reducing the reliance on human-drawn sketch datasets.\nThe second research direction examines typography, a central tool in visual communication. We showcase the application of automatically generating word-as-image illustrations by utilizing the prior of a text-to-image diffusion model. These illustrations aim to visually represent the meaning of a given word by manipulating the appearance of its letters. Expanding upon this work, we explore the use of pretrained text-to-video models to animate still input sketches based on a provided text prompt. By incorporating domain-specific regularizations, we achieve success in these challenging tasks. The highly editable nature of vector representations makes these results valuable as initial solutions for complex design problems, offering designers the flexibility to refine and adapt the generated outputs.\nIn another research direction, we explore how the prior of pretrained large models can be leveraged to provide visual inspiration within the design process. In design, visual inspiration often involves decomposing a source of inspiration into its key aspects and reassembling them to meet design goals. For example, the Lotus Temple in India was inspired by the structure of the lotus flower, with the architect focusing on its form while disregarding its color. We utilize text-to-image personalization to decompose a visual concept into distinct aspects, organized in a hierarchical tree structure. These individual aspects can then be combined with elements from other visual references to foster the generation and exploration of novel visual concepts and designs.", "sections": [{"title": "1 Introduction", "content": "Visual communication is the conveyance of information and ideas through visual means. Visual communication is a vital aspect of human interaction that has evolved alongside our capacity to create and interpret visual stimuli. Historically, visual communication traces back to prehistoric cave paintings, ancient Egyptian hieroglyphics, and medieval manuscripts, where visuals were used to communicate stories, convey messages, and record important events.\nWhile the term 'visual communication' encompasses a broad spectrum of visual stimuli, 'visual communication design' typically refer to a specific design discipline, often centered on graphic design. It includes a wide range of domains such as images, symbols, typography, infographics, illustrations, presentations, animation, logo design, commercials, and more.\nSome visual communication designs are so ingrained in our daily lives that we use them without consciously recognizing them as designed objects. For example, a stop sign is a red octagonal sign with the word \"STOP\" written in white. It's a simple yet highly effective visual cue used in traffic control. The color red signals danger or caution, the octagonal shape is highly distinguishable, and the word \"STOP\" clearly convey the instruction for drivers. Another example of an effective visual communication design is the Apple logo, designed by Rob Janoff. The logo is a simple yet iconic image of an apple with a bite taken out of it. This minimalist design has become one of the most recognizable symbols in the world, conveying a sense of innovation and human-centric design philosophy that has come to be associated with Apple products. It has transcended its original purpose as a corporate logo to become a symbol of modernity and technological expertise.\nMotivated by the fundamental role of visual communication in our society, in this dissertation we aim to explore the ability of computers to automatically produce effective visual communication designs across different mediums, especially in light of recent advancements in large vision-language models and generative models (also called \u201cGenerative Artificial Intelligence\"). Our primary objectives are twofold: (1) to shed light on how the hidden representations of modern \u201cGenerative AI\" systems can be leveraged to tackle challenging cognitive-visual tasks from a design standpoint; and (2) to develop practical generative tools that can serve and empower designers in their creative endeavors, taking into account the methodologies and mediums commonly employed by graphic designers.\nWe note that our goal is not to build systems or applications that would replace graphic designers. Instead, we aim to leverage recent advancements in \u201cGenerative AI\u201d to develop tools that more synergistically support the creative process of designers.\nCreating effective visual-communication designs presents a number of challenges. First, there's the delicate balance between clarity and complexity, requiring careful consideration in simplifying complex ideas while retaining essential details. For instance, the iconic Apple logo exemplifies simplicity yet effectively conveys a strong message of technology and prestige. Second, communicating effectively across diverse cultures presents challenges due to varying interpretations a certain visual might have. Third, designers must possess a rich interdisciplinary knowledge of concepts in fields such as history, politics, and art to ensure designs are sophisticated, up-to-date, and capable of conveying complex messages.\nUntil recent years, generative models were quite limited in their ability to assist designers with complex visual communication tasks. These models primarily relied on task-specific datasets, which were often limited in scope, failing to incorporate necessary social knowledge and visual understanding. Additionally, the quality of the generated content was relatively low, and the user interfaces were not intuitive, making these models more suitable for developers than for designers.\nIn recent years, significant advancements have been made in several parallel fields, including diffusion models, contrastive learning, and transformers. Coupled with increasing computational power that facilitates the handling of extensive datasets, these developments have led to the creation of strong vision-language models (VLMs) that have significantly transformed the generative landscape.\nVLMs are multimodal deep neural networks that learn rich vision-language correlations from images and text. Having been trained on millions (and recently, billions of image-text pairs, these models possess extensive knowledge about visual and semantic concepts and have demonstrated remarkable generative abilities. High-quality visual content can be created automatically, by anyone, simply by typing a descriptive sentence. These models can generate very realistic images depicting surprising and creative combinations of endless concepts and in numerous styles. These capabilities, combined with the ease of use, facilitated their transition from the academic domain to different applications and widespread adoption, gaining popularity among professionals and amateurs alike across various disciplines.\nA significant research effort in the computer vision and graphics communities aims at enhancing VLMs in terms of generation quality, speed, and better user control. In addition, it has been discussed that such models hold immense potential in assisting designers in their creative processes. We believe that these models also posses substantial potential in enhancing visual communication, reshaping how messages are conveyed. This aspect has not yet been a large part of computational research, a gap we wish to narrow in this thesis."}, {"title": "1.1 Visual Abstractions Through Sketches", "content": "Visual abstraction refers to the process of reducing visual information to its essential features or underlying structure. Abstraction is not merely a stylistic choice in art but a fundamental aspect of human cognition, shaping how we understand and interact with the world around us. Artists and viewers alike engage in abstraction when perceiving and creating visual representations. For example, in the famous \"Le Taureau\u201d series, Picasso depicts the progressive abstraction of a bull. In this series of lithographs, the artist transforms a bull from a concrete, anatomical drawing, into a sketch composition of a few lines that still manages to capture the essence of a bull.\nThe process of visual abstraction is central to design and to visual communication in particular. By distilling information to its essential elements, abstraction directs attention to key points and encourages viewer interpretation, making visuals more memorable, clear, and versatile across different mediums. Achieving a plausible visual abstraction that is both accurate and comprehensible is a highly challenging task, as it requires cognitive and artistic decisions about what to include and omit while ensuring coherence, clarity, and avoidance of ambiguity in the final representation.\nIn a first research direction, we explore the ability of large pretrained VLMs to understand and generate visual abstractions. Since visual abstractions encompass a very wide range of representations, we narrow our investigation into a specific and fundamental domain - sketches.\nSketches are one of the most accessible and enduring technique employed by individuals across various cultures to visually express and explore ideas. The term 'sketch' refers to the result of a rough, preliminary mark-making activity. A sketch is an excellent example of an expressive visual representation that is highly simplified. The act of sketching has been described as \u201ca conversation with ourselves in which we communicate with sketches\". Sketches consist of only strokes, and often only a limited number of strokes. Hence, the process of abstraction is central to sketching.\nDesigners use sketches to think, plan, and develop ideas. Figure 1.5 provides examples of freehand sketches used by artists, designers, and inventors, leading to some well-known designs. For instance, Van Gogh's famous artwork, the \u201cChair\u201d began as a simple sketch, allowing the artist to plan and contemplate the chair's form, pose, and composition before creating the detailed colored oil painting.\nWhile computational sketch generation has been an active research field in computer graphics and vision, the notion of abstraction which is core to sketches is often ignored in previous works. In Chapters 3 and 4, we propose a new approach for automatic sketch generation while focusing on the importance of abstractions in the sketching process. In addition, our work was the first in the sketch domain to utilize the semantic-visual prior embedded within pre-trained VLMs instead of reliance on human-drawn sketch datasets.\nOur first work in this domain is CLIPasso, which converts an image of an object into a sketch. We emphasize the importance of abstraction in sketches and focus on generating sketches with different levels of abstraction. Our method is built upon three key components: (1) Representation: We represent a sketch as a set of strokes, using vector representation, allowing us to control the level of abstraction by altering the number of strokes. (2) We utilize a differentiable rasterizer to compute pixel-based losses between the rasterized sketch and the input image. This enables us to backpropagate gradients and modify the parameters of strokes accordingly. (3) We incorporate CLIP, a pre-trained language-vision model, to bridge the semantic gap and guide the optimization of strokes parameters with respect to the input image. We demonstrate the robustness of our method across various domains and levels of abstraction. Through both human evaluation and quantitative metrics, we validate that our sketches accurately depict the input object in both semantic and geometric aspects."}, {"title": "1.2 Communicative Illustrations in Typography", "content": "In a second research direction, we investigate how recent advancements in vision-language models can be leveraged to support visual communication from the perspective of typography.\nTypography encompasses the design and arrangement of type, including letters, numbers, and symbols, to create visually appealing and communicative text. Typography plays a significant role in visual communication by facilitating clarity, readability, and emotional resonance in design. Moreover, typography contributes to establishing and reinforcing brand identity, as consistent font usage across various platforms aids in brand recognition. In Figure 1.6 we show examples of how typography is used to enhance visual communication by pioneering graphic designers in various ways.\nIn our research, we investigate how typographical elements can be automatically created with generative models to enhance visual expression. Specifically, in Chapter 5 we focus on the automatic generation of word-as-image illustrations, also known as typographic illustrations or text art. Word-as-image illustrations are visual representations where the textual elements of a word or phrase are manipulated to create a visual image that embodies the meaning of the word itself. In Figure 1.7 we show examples of such illustration made by the artist Ji Lee. Word-as-image illustrations blur the lines between text and image, engaging viewers with their inventive and playful use of typography. They can be found in various contexts, including advertising, graphic design, digital art, and typographic posters. While traditional typography primarily focuses on legibility and readability, word-as-image typography goes beyond mere text to convey meaning through visual imagery. Therefore, word-as-image can be seen as a specialized and expressive subset of typography, where the emphasis is on the visual impact and artistic interpretation of text."}, {"title": "1.3 Text Guided Generation of Short Animations", "content": "Short animations play a significant role in visual communication by conveying messages, stories, and emotions through visual dynamic sequences. They are highly effective for capturing viewers' attention, conveying complex ideas in a concise format, and evoking emotional responses through movement, sound, and visual effects. Short animations are utilized across various platforms, including social media, advertising, education, and entertainment, to engage audiences and communicate messages effectively.\nAn iconic example of a simple yet expressive short animation is \"La Linea\", a classic"}, {"title": "1.4 Generative Models for Visual Inspiration", "content": "Following our goal to empower designers and artists, in a parallel research direction, we examine how VLMs can be leveraged to support effective visual communication by providing visual inspiration. Such inspiration can stimulate creativity by exposing designers to new ideas and techniques, which is essential for solving communication challenges. Furthermore, drawing inspiration from diverse sources helps in creating designs that stand out in a crowded visual landscape.\nA creative idea is often born from transforming, combining, and modifying ideas from existing visual examples capturing various concepts. Large vision-language models posses a huge prior knowledge about visual and semantic concepts, this knowledge"}, {"title": "1.5 Thesis Overview", "content": "We begin with an overview of fundamental concepts in generative models (Chapter 2), covering the necessary background information and notations essential for understanding the ideas presented in this dissertation. The relevant publications comprising this dissertation are then organized into chapters divided into three parts:\n\u2022 Part I - Sketch Generation and Abstraction\nChapter 3 - We present CLIPasso, a method to convert an image of an object into a sketch, allowing for varying levels of abstraction, while preserving the key visual features of the input object.\nChapter 4 - In a follow-up work, CLIPascene, we extend CLIPasso to scene sketching. We also propose an extended definition to sketch abstraction by disentangling abstraction into two axes of control: fidelity and simplicity.\n\u2022 Part II - Communicative Illustrations in Typography\nChapter 5 - We suggest an algorithm for the automatic generation of word-as-image illustrations, where a the appearance of a given word is manipulated to present a visualization of its meaning.\n\u2022 Part III - Text Guided Generation of Short Animations\nChapter 6 - We present a method that automatically adds motion to a single-subject sketch (hence, \u201cbreathing life into it\u201d), merely by providing a text prompt indicating the desired motion. The output is a short animation provided in vector representation, which can be easily edited.\n\u2022 Part IV - Generative Models for Visual Inspiration\nChapter 7 - We leverage the prior knowledge of pretrained text-to-image model to build a tree-structured visual exploration space based on a given visual concept. The nodes of the tree encode different aspects of the subject. Through examining combinations within and across trees, the different aspects can inspire the creation of new designs and concepts."}, {"title": "2 Background", "content": "The works presented in this dissertation are all based on leveraging the priors of pretrained large vision-language models to facilitate design-related generative tasks. Some of the works also employ a vector representation for images. This chapter provides a review of fundamental concepts in vector graphics, generative models, and vision-language models. The notations used in subsequent chapters are introduced below."}, {"title": "2.1 Vector Representation for Images", "content": "Vector graphics allow us to create visual images directly from geometric shapes such as points, lines, curves, and polygons. The vector image can therefore be represented by a set of parameters defining the geometric shapes. Unlike raster images (represented with pixels), vector representation is resolution-free, more compact, and easier to modify. This quality makes vector images the preferred choice for various design applications, such as logo design, prints, animation, CAD, typography, web design, infographics, and illustrations.\nScalable Vector Graphics (SVG) stands out as a popular vector image format due to its excellent support for interactivity and animation. SVG is a XML-based format for describing two-dimensional vector graphics. An SVG is a plain text file that describes geometric shapes and paths using XML markup. The structure of an SVG"}, {"title": "2.2 Generative Models", "content": "Generative models are a class of computational data-driven models aiming at automatically generating new content, based on a given dataset. In the field of computer vision and graphics, we focus on generative models that learn how to generate visual content, such as images, 3D data, videos, and more.\nGenerally, the idea behind generative models is to learn a function that estimates the underlying distribution of a given dataset, and by that it could generate a new sample from this distribution with high probability. For example, let's assume we have a large dataset of images of cats. This data defines a distribution over the space of pixels, where sampling an image of a cat means that the pixel's value in that image will be structured in a specific way so that the image they compose would look like a cat"}, {"title": "2.3 CLIP (Contrastive Language-Image Pretraining)", "content": "CLIP [210] is a large vision-language model which is based on a neural network trained on image-text pairs with the objective of creating a joint latent space of text and images. CLIP was trained on a large data of 400 million image-text pairs collected from the internet [227], and was one of the first large-scale open-source models, therefore significantly advancing scientific research.\nIn CLIP, two encoders are being trained - a text encoder ET and an image encoder EI. During training, at each step, a large batch of N paired images and text are sampled from the training dataset, where the paired images and text form the positive pair and the unpaired ones are considered as negative samples. The training loss is based on a contrastive approach, and can be formulated with:\n$L_{clip}=-\\sum_{i=1}^N \\log \\frac{exp(sim(E_I(x_i^I), E_T(x_i^T))/\\tau)}{\\sum_{j=1}^N exp(sim(E_I(x_i^I), E_T(x_j^T))/\\tau)}$\nwhere  are the i'th image-text pair in a batch, and sim(;) is the cosine similarity (a dot product between the input vectors). The temperature \\tau is a learned parameter. This loss simply means that in this shared latent space, the representation of positive text-image pairs should be close to each other (under the cosine distance metric), while negative pairs should be far away from each other. This procedure is illustrated in Figure 2.4 (the illustration was taken from the CLIP paper [210]).\nBeing trained on a wide variety of image domains along with lingual concepts, CLIP models are found to be very useful for a wide range of zero-shot tasks. It was demonstrated by the authors of CLIP that the pretrained model can be used for image classification, and they show that the model is robust to distribution shift. As an example, CLIP achieved high classification rates for sketches and cartoons despite not having been trained with this objective.\nDue to these impressive capabilities, CLIP was later used in many zero-shot tasks. In our context, CLIP was also used for more creative generative tasks as described next.\nThe first work to leverage CLIP's prior with vector data was CLIPDraw [72]. CLIP-Draw optimizes a set of random B\u00e9zier curves to create a drawing that maximizes the CLIP similarity for a given text prompt. Later, Tian and Ha [247] employ evolutionary algorithms combined with CLIP, to produce creative abstract concepts represented by colored triangles guided by text or shape. Their results are limited to either fully semantic (using CLIP's text encoder) or entirely geometric (using the L2 loss)."}, {"title": "2.4 Diffusion Models", "content": "Diffusion models are generative models trained to learn a data distribution p(x) by gradually denoising a variable sampled from a Gaussian distribution. Denoising diffusion models consist of two processes: Forward diffusion process that gradually adds noise to the input, and reverse denoising process that learns to generate data by denoising. A visualization of such a forward and backward operation in the context of image generation is shown in Figure 2.5.\nThe denoising process corresponds to learning the reverse process of a fixed Markov"}, {"title": "2.5 Score Distillation Sampling", "content": "One of our goals in this work was to utilize the strong prior of pretrained large text-image models for the generation of modalities beyond rasterized images. In Stable Diffusion, text conditioning is performed via the cross-attention layers defined at different resolutions in the UNet network. Thus, it is not trivial to guide an optimization process using the conditioned diffusion model.\nThe score-distillation sampling (SDS) loss, first proposed in Poole et al. [201], serves as a means for extracting a signal from a pretrained text-to-image diffusion model. In their seminal work, Poole et al. proposed a way to use the diffusion loss to optimize the parameters of a NeRF [182] for text-to-3D generation. At each iteration, the radiance field is rendered from a random angle, forming the image x, which is then noised to some intermediate diffusion time step t: $x_t = \\alpha_t x + \\sigma_t \\epsilon$ (as described in eq. (2.6)).\nThe noised image is then passed through the diffusion model, conditioned on a text-prompt c describing some desired scene. The diffusion model's output, $e_\\theta(x_t, t, c)$, the prediction of the noise added to the image. The deviation of this prediction from the true noise, , can serve as a measure of the difference between the input image and one that better matches the prompt. This measure can then be used to approximate the gradients to the initial image synthesis model's parameters, , that would better align its outputs with the prompt. Specifically,\n$\\nabla_\\phi L_{SDS} = w(t)(\\epsilon_\\theta(x_t, t, y) - \\epsilon) \\frac{\\partial x_t}{\\partial x} \\nabla_\\phi x$ where w(t) is a constant that depends on \\alpha_t. This optimization process is repeated, with the parametric model converging toward outputs that match the conditioning prompt.\nNote that the gradients of the UNet are skipped, and the gradients to modify the Nerf's parameters are derived directly from the LDM loss."}, {"title": "2.6 Textual Inversion", "content": "Textual inversion [74] is a method for performing personalization of text to image generation results. In such method, a pretrained text-to-image model is adapted such that it can generate images of novel, user-provided concepts. These concepts are typically unseen during training, and may represent specific personal objects (such as the user's pet) or more abstract categories (new artistic style). There are multiple approaches for performing text-to-image personalization. Existing methods employ either token optimization techniques [74, 311, 264], fine-tuning the model's weights [217, 111, 141], or a combination of both [3, 8, 13]. We focus in this part on the details of Textual Inversion [74] as we rely on its settings in Chapter 7.\nGiven a text prompt y, for example \u201cA photo of a cat\u201d, the sentence is first converted into tokens, which are indexed into a pre-defined dictionary of vector embeddings. The dictionary is a lookup table that connects each token to a unique embedding vector. After retrieving the vectors for a given sentence from the table, they are passed to a text transformer, which processes the connections between the individual words in the sentence and outputs c(y). The output encoding c(y) is then used as a condition to the UNet in the denoising process: $e_\\theta(z_t,t,c(y))$. Let us denote words with S, and the vector embeddings from the lookup table with V.\nIn [74] the embedding space of V is chosen as the target for inversion. They formulate the task of inversion as fitting a new word s* to represent a personal concept, depicted by a small set of input images provided by the user. They extend the predefined lookup table with a new embedding vector v\u2217 that is linked to s*. The vector v\u2217 is often initialized"}, {"title": "3 CLIPasso: Semantically-Aware Object Sketching", "content": "Figure 3.1: Our work converts an image of an object to a sketch, allowing for varying levels of abstraction, while preserving its key visual features. Even with a very minimal representation (the rightmost flamingo and horse are drawn with only a few strokes), one can recognize both the semantics and the structure of the subject depicted.\nFree-hand sketching is a valuable visual tool for expressing ideas, concepts, and actions [67, 100, 293, 88, 256]. As sketches consist of only strokes, and often only a limited number of strokes, the process of abstraction is central to sketching. An artist must make representational decisions to choose key visual features of the subject drawn to capture the relevant information she wishes to express, while omitting (many) others [37, 68, 295].\nFor example, in the famous \"Le Taureau\" series, Picasso depicts the progressive abstraction of a bull. In this series of lithographs, the artist transforms a bull from a concrete, fully rendered, anatomical drawing, into a sketch composition of a few lines that still manages to capture the essence of the bull.\nIn this paper, we pose the question: Can computers imitate such a process of sketching abstraction, converting an image from a concrete depiction to an abstract one?\nToday, machines can render realistic sketches simply by applying mathematical and geometric operations to an input photograph [281, 32]. However, creating abstractions is"}, {"title": "3.1 Related Work", "content": "Unlike edge-map extraction methods [281, 32] which are purely based on geometry, free-hand sketch generation aims to produce sketches that are abstract in terms of structure and semantic interpretation so as to mimic a human-like style. This high-level goal varies among different works, as there are many styles and levels of abstraction that can be produced. Consequently, existing works tend to choose the desired output style based on a given dataset: from highly abstract - guided only by a category-based text prompt [90], to more concrete [9], which is guided by contour detection. Figure 3.3 illustrates this spectrum. While methods that rely on sketch datasets are limited to the abstraction levels present, our method is optimization based. Hence, it is capable of producing multiple levels of abstraction without relying on the existence of suitable sketch datasets or requiring a lengthy new training phase."}, {"title": "3.2 Method", "content": "We define a sketch as a set of n black strokes {$s_1,..s_n$} placed on a white background. We use a two-dimensional B\u00e9zier curve with four control points $s_i = {p_j^i}_{j=1}^4 = {(x_i, y_i)}_{j=1}^4$ to represent each stroke. For simplicity, we only optimize the position of control points and choose to keep the degree, width, and opacity of the strokes fixed. How- ever, these parameters can later be used to achieve variations in style (see Figure 3.9a)."}, {"title": "3.3 Results", "content": "Section 3.3.1 provide qualitative evaluations. In section 3.3.2 we compare our method with existing image-to-sketch methods, which were all trained on sketch-specific datasets. In Section 3.3.3, we supply a quantitative evaluation of our method's ability to produce recognizable sketches testing both category and instance recognition. For images with background, we use an automatic method (U2-Net [206]) to mask out their background. We provide further analysis of our method, extra results, and extended comparison with other methods in the supplemental file."}, {"title": "3.3.1 Qualitative Evaluation", "content": "Our approach is different from conventional sketching methods in that it does not utilize a sketch dataset for training, rather it is optimized under the guidance of CLIP. Thus, our method is not limited to specific categories observed during training, as no category definition was introduced at any stage. This makes our method robust to various inputs, as shown in Figures 3.1 and 3.8.\nIn Figures 3.1 and 3.2 we demonstrate the ability of our method to produce sketches at different levels of abstraction. As the number of strokes decreases, the task of minimizing the loss becomes more challenging, forcing the strokes to capture the essence of the object. For example, in the abstraction process of the flamingo in Figure 3.1, the transition from"}, {"title": "3.3.2 Comparison with Existing Methods", "content": "Sketches with different levels of abstraction. Only a few works have attempted to sketch objects at different levels of abstraction. In Figure 3.10 we compare with Muhammad et al. [187] and Berger et al. [20]. The results by Muhammad et al. demonstrate four levels of abstraction on two simple inputs - a shoe and a chair (in the absence of their code, the results were taken directly from the paper). We produce sketches at four levels of abstraction using 32, 16, 8, and 4 strokes. The sketches by Muhammad et al. are coherent with the geometry of the image; but to achieve higher levels of abstraction, they only remove strokes from the generated sketch without changing the remaining ones. This can result in losing class-level recognizability at higher levels of abstraction (rightmost sketches). Such an approach is sub-optimal, since a better arrangement may be possible for fewer strokes. Our method successfully produces a recognizable rendition of the subject while preserving its geometry, even in the challenging 4-stroke case (rightmost sketch).\nIn the right bottom part of Figure 3.10 we compare with the method of Berger et al. [20]. Their results were provided by the authors and demonstrate two levels of abstraction generated based on the style of a particular artist. We use 64 and 8 strokes, respectively, to achieve two comparable levels of abstraction and place a pencil style on top of the generated sketch to better fit the artist's style. As can be seen, our approach is more geometrically coherent while still allowing abstraction. Their results fit better to a specific style, but can only work with faces and are limited to the dataset gathered."}, {"title": "4 CLIPascene: Scene Sketching with Different Types and Levels of Abstraction", "content": "Figure 4.1: Our method converts a scene image into a sketch with different types and levels of abstraction by disentangling abstraction into two axes of control: fidelity and simplicity. The sketches on the left were selected from a complete matrix generated by our method (an example is shown on the right), encompassing a broad range of possible sketch abstractions for a given image. Our sketches are generated in vector form, which can be easily used by designers for further editing.\nSeveral studies have demonstrated that abstract, minimal representations are not only visually pleasing but also helpful in conveying an idea more effectively by emphasizing the essence of the subject [101, 24]. In this paper, we concentrate on converting photographs of natural scenes to sketches as a prominent minimal representation.\nConverting a photograph to a sketch involves abstraction, which requires the ability to understand, analyze, and interpret the complexity of the visual scene. A scene consists of multiple objects of varying complexity, as well as relationships between the foreground and background (see Figure 4.2). Therefore, when sketching a scene, the artist has many options regarding how to express the various components and the relations between them (see Figure 4.3)."}, {"title": "4.1 Related Work", "content": "Free-hand sketch generation differs from edge-map extraction [32, 281] in that it attempts to produce sketches that are representative of the style of human drawings. Yet, there are significant differences in drawing styles among individuals depending on their goals, skill levels, and more (see Figure 4.3). As such, computational sketching methods must consider a wide range of sketch representations.\nThis ranges from methods aiming to produce sketches that are grounded in the edge map of the input image [149, 286, 250, 152], to those that aim to produce sketches that are more abstract [22, 72, 262, 91, 205, 78, 209, 179, 316]. Several works have attempted to develop a unified algorithm that can output sketches with a variety of styles [38, 301, 160]. There are, however, only a few works that attempt to provide various levels of abstraction [20, 187, 262]. In the following, we focus on scene-sketching approaches, and we refer the reader to [292] for a comprehensive survey on computational sketching techniques."}, {"title": "4.2 Method", "content": "Given an input image I of a scene, our goal is to produce a set of corresponding sketches at n levels of fidelity and m levels of simplicity, forming a sketch abstraction matrix of size m \u00d7 n. We begin by producing a set of sketches along the fidelity axis (Sections 4.2.1 and 4.2.2) with no simplification, thus forming the top row in the abstraction matrix. Next, for each sketch at a given level of fidelity, we perform an iterative visual simplification by learning how to best remove select strokes and adjust the locations of the remaining strokes (Section 4.2.3). For clarity, in the following we describe"}, {"title": "4.2.1 Training Scheme", "content": "Following the sketch representation introduced in Chapter 3, we define a sketch as a set of n strokes placed over a white background, where each stroke is a two-dimensional B\u00e9zier curve with four control points. We mark the i-th stroke by its set of control points  and denote the set of the n strokes by Z = {z_i}^{n}_{i=1}. Our goal is to find the set of stroke parameters that produces a sketch adequately depicting the input scene image.\nAn overview of our training scheme used to produce a single sketch image is presented in the gray area of Figure 4.7. We train an MLP network, denoted by MLPloc, that receives an initial set of control points $Z_{init} \\in R^{n\u00d74\u00d72}$ (marked in blue) and returns a vector of offsets  with respect to the initial stroke locations. The final set of control points are then given by Z = Zinit + \\Delta Z, which are then passed to a differentiable rasterizer R [150", "41": ".", "94": "CLIP model for the sketching process (and struggles with depicting a scene image), we find that the ViT-based [59", "211": "that ViT models better capture more global information at lower layers compared to ResNet-based models. We further analyze this design choice in the supplementary material.\nThe loss function is then defined as the L2 distance between the activations of CLIP on the image I and sketch S at a layer lk"}]}