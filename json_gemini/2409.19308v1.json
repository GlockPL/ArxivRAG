{"title": "Designing Domain-Specific Large Language Models: The Critical Role of Fine-Tuning in Public Opinion Simulation", "authors": ["Haocheng Lin"], "abstract": "Large language models (LLMs) have transformed natural language processing across diverse fields, yet their general-purpose design limits their effectiveness in specialized domains, such as simulating opinions on environmental policies. This paper presents an approach for fine-tuning LLMs using data from the UK Household Longitudinal Study, improving the accuracy of opinion generation by conditioning models on socio-demographic factors like age, income, education, and region. By emulating diverse synthetic profiles, fine-tuned models capture the subtle differences across demographic groups more effectively than pre-trained versions. Metrics such as Chi-Squared, Cosine Similarity, Jaccard Index, and KL-divergence, demonstrate a strong alignment between synthetic and real-world opinion data. This approach highlights the potential of fine-tuning LLMs to provide more informed, representative, and ethical insights into public sentiments on environmental issues. The findings underscore the importance of tailoring LLMs to specific societal contexts for more accurate and ethical policy simulations.", "sections": [{"title": "Introduction", "content": null}, {"title": "Overview of Large Language Models and Their Growing Application Across Industries", "content": "Large Language Models (LLMs), such as GPT-4 and BERT, have revolutionized natural language processing (NLP) by performing complex tasks like generating human-like text, translating languages, summarizing lengthy documents, understanding context to engage in diverse conversations. These models are trained on vast amount of data and are supported by billions of parameters to understand and execute required tasks, making them applicable across a wide range of industries, including healthcare, finance, legal services, and education. For example, GPT-4, a transformer-based multimodal model, has been applied in areas such as image processing, dialogue systems, and machine translation. An earlier model, GPT-3, with its 175 billion parameters, will be compared with GPT-4 to determine how GPT-4 evolved its capability to handle generalized tasks with minimal supervision. Despite their versatility, these general-purpose LLMs often struggle in specialized domains such as public policy or environmental governance, where a deeper background and contextual understanding are required. The LLMs are limited at predicting fringe socio-political opinions and require context to integrate socio-demographic and policy factors."}, {"title": "Limitations of General-Purpose LLMs in Domain-Specific Tasks", "content": "Training on extensive public-domain datasets ensures that the LLMs could successfully perform multiple tasks. However, this prevents the LLMs from performing specialised tasks, like understanding a diverse set of opinions on environmental policies. General purpose models fail to understand how socio-demographic factors influence an individual's answer to questions about environmental issues. For example, previous research by S. Lee and L. P. Argyle identified that the LLMs overestimated the proportion of respondents concerned about climate change. GPT-3 identified differences between the predicted and actual vote shares in the most recent four presidential elections: -0.081 R (2012), - 0.029 R (2016), +0.06 R (2020 - ANES), and +0.004 R (2020 \u2013 Actual Results). These results confirm that LLMs produce generic results that lack the depth for simulating real-world scenarios. Overestimating the level of support for environmental policies can led to the government introducing policies disconnected with the people's concerns, which could diminish public trust in institutions and possibly amplify polarising views on environmental issues. Also, ignoring minority views unfairly impact potential vulnerable populations and leaving them to face some of the worst impacts of climate change as evident in a study confirming LLMs reflect environmental issues mostly in Europe and the US regions. Also, pre-trained models may amplify biases present in their training data, which causes skewed or inaccurate predictions when simulating public opinions. This happens because some groups are underrepresented in the training data. An example comparison can be made with a decision-making algorithm, such as the COMPAS system, determining the chances of a reoffending defendant. This system was criticized for reinforcing historical biases by overgeneralizing specific cases and disproportionately labelling individuals from marginalized groups as high-risk. Similarly, LLMs can propagate biases, if the training data overrepresents certain narratives or groups. While COMPAS makes categorical decisions, LLMs generate text that reflects the biases from the training data, unless specialised fine-tuning and prompt engineering are applied. These limitations highlight the importance of using specialized approaches, like adversarial debiasing and fairness constraints, to mitigate bias in LLMs and ensure more accurate, representative outputs in real-life."}, {"title": "Fine-Tuning as a Solution for Domain-Specific Adaptation", "content": "Fine-tuning provides a method to overcome the models' limitation by adapting pre-trained models in domain-specific contexts. A smaller and domain-specific dataset will guide the LLMs to align with a target task, allowing the model to adjust its parameters based on new data by optimising the models' after several iterative implementations. An iterative cycle of trial and improvements of variable selection, weight adjustment, and learning rates improves the models' performance on specific tasks, such as simulating public opinions in environmental policymaking."}, {"title": "Objectives", "content": "This paper explores how fine-tuning enhances the LLMs' performance in domain-specific tasks, with a focus on how they simulate public opinions on environmental policies. The UKHLS dataset serves as a conditioning and fine-tuning dataset to improve the LLMs' ability at predicting public opinions on environmental issues and set a benchmark for comparing the synthetic with expected distributions.\nThis comparison enables this study to demonstrate how fine-tuning allows LLMs to understand the subtle nuances between synthetic responses, which provides better insights to inform policymakers in public policy and environmental governance.\nThis study uses the following objectives to define its goals:\n1. Enhance the LLMs' prediction accuracy by fine-tuning with the UK Household Longitudinal Study Datasets as the training dataset.\n2. Define a set of benchmarks to evaluate the fine-tuned model's response distribution against real-world public opinion data using evaluation metrics, like Chi-Square test scores, Cosine Similarity, and Jaccard Index, to assess the alignment between the distributions.\n3. Demonstrate that fine-tuning enables an understanding of different public opinions on environmental policies, representing profiles with complex socio-demographic variables.\n4. Identify the limitations of fine-tuning, such as overfitting, bias, and computational costs, and use them to help design informed policies and improve engagement in environmental issues."}, {"title": "Literature Review", "content": null}, {"title": "LLM Architecture: Development of Transformer-Based Models", "content": "Large Language Models (LLMs) use a transformer architecture, a model introduced by Vaswani et al. (2017), supported by deep learning methods and trained on vast amounts of data. Unlike Recurrent Neural Networks (RNNs) or Long Short-Term Memory (LSTM) networks, which process the input sequentially, transformers use a self-attention mechanism that allows parallel processing. This enables the transformers to scale efficiently with large datasets and handle long-range text dependencies.\nIn transformer-based models, the architecture consists of two main components: self-attention layers and feed-forward networks. Self-attention allows the model to weigh the importance of each token relative to all other tokens simultaneously, capturing relationships and context effectively. The query, key, and value vectors are used to calculate attention weights, which determine how much focus the model should place on each part of the input sequence.\nThe attention-weighted are passed through a feed-forward network, followed by multiple layers of attention and transformation. This layered structure enables the model to capture different levels of abstraction, with the lower layers capturing syntactic patterns and the higher layers identifying semantic relationships. This makes transformers efficient in NLP tasks, like language modelling, translation, text generation, question answering, and summarization.\nUnlike existing recurrent neural networks (RNNs) and long short-term memory (LSTM) models, large language models could process long-range dependencies more efficiently. LLMs enable parallel processing of text in a sequence when training on the fine-tuning dataset, which reduces the training time and allows a more efficient usage of computational resources. One example of a LLM architecture, which contains a multi-head attention for capturing different types of dependencies between conditioning profiling variables, different layers to provide a deeper understanding of how the input transitions through the different layers of the LLMs. Having a such architecture allowed models like GPT-4, BERT, and T5 to scale up their processing abilities over more parameters and larger datasets."}, {"title": "Fine-Tuning Techniques: Adapting LLMs for Domain-Specific Applications", "content": "Fine-tuning techniques aim to ensure that the LLMs perform well in specialised fields by retraining them on a smaller domain-specific dataset, which ensures that the models' outputs align with the expected distribution. In addition to contextual knowledge, a successful fine-tuning strategy requires adjusting the models' weights, learning rate, batch size, and the number of epochs. \nTraditional fine-tuning methods train LLMs on entire domain-specific dataset, which could be computational expensive and costly. For example, when using 100 synthetic profiles to fine-tune a GPT-3.5-turbo model, the cost is approximately $1. Recent advancements introduced efficient fine-tuning methods, such as Adapters and"}, {"title": "Ethical and Domain-Specific Considerations", "content": "There are some ethical challenges when fine-tuning LLMs, such as emphasizing the risks of bias. Pre-trained LLMs, such as GPT-3 and BERT, inherit biases from their large-scale training datasets. The biases could be from gender, racial, or socio-economic disparities, skewing the LLMs' response distributions. If the fine-tuning data isn't representative enough, it could reinforce existing biases. This amplification can have real-world consequences, especially in domains like public policy, where AI-driven simulations can influence decision-making.\nIn public opinion simulations, it is crucial to ensure that fine-tuning captures a full spectrum of public sentiment across different demographic groups. In studies, scholars observe an underrepresentation of the lower socio-economic groups , due to their participation being restricted by digital divides, personal reluctance to participate, and sampling biases. These gaps in data represents a serious risk of designed policies favouring wealthier demographics, reinforcing societal inequalities. Moreover, views are also influenced by an echo chamber effect from their social media usage, where individuals are more likely to engage in contents that resonate with their preferences. In an Irish climate change network, the forums used for studying policy attitudes fail to sample balanced responses due to the echo chamber effect."}, {"title": null, "content": "Another case study about news media consumption in the UK shows three stakeholder groups: 22% are 'news lovers' who consume from many sources, 55% are 'daily briefers' who use fewer sources, and 23% are 'casual users' who access no daily news. These patterns suggest that over half of the users are exposed to diverse viewpoints. However, when conducting a survey asking about political preferences, only 2% and 5% of the online users claim to be a member of left- or right-wing echo chambers, the responses might be from the users desensitized by their consumed contents. This echo chamber effect explain why people with environmental-friendly attitudes are more likely to participate in studies about environmental issues.\nTo address potential biases, a set of careful data preprocessing is required to correct potential biases during future fine-tuning stages. A pipeline of preprocessing steps aims to ensure that the data is cleaned and formatted for the LLMs to learn from fine-tuning:\n1. Data cleaning removes irrelevant content, such as corrupt or incomplete values, linguistic errors, and random characters that don't contribute to the study.\n2. Normalization standards the text format to ensure an easier comparison between each synthetic profiles to minimize the risks of the LLMs misunderstanding the training data.\n3. Checking and removing duplicate profiles.\n4. Balancing the datasets by ensuring that each profiling variable characteristics are represented.\n5. If there are some underrepresented samples, search for booster datasets for increasing their proportion in the training dataset.\n6. Shuffle the training data during each training cycle to prevent the LLMs from learning specific patterns from its ordering."}, {"title": "Methodology: Fine-Tuning LLMs for Public Opinion Simulation", "content": null}, {"title": "Pre-Trained Model Overview", "content": "This study uses GPT-4 variants (GPT-40, GPT-40 mini, and GPT-401-preview) to examine how fine-tuned models are specialised for simulating opinions on environmental policies. Prior to fine-tuning, these models can perform different natural language processing tasks due to their pre-training on large datasets. For example, GPT-3.5-turbo uses 175 billion parameters while supported by diverse open-source datasets. However, these pre-trained models often struggle with specialised tasks that requires a lot of domain-specific knowledge to generate responses reflecting subtle differences between synthetic profiles."}, {"title": "Datasets: UK Household Longitudinal Study (UKHLS)", "content": "The UK Household Longitudinal Study Dataset fine-tunes selected LLMs, which contains an extensive panel survey tracking 40,000 households from January 2009 to May 2023. These profiling variables, such as age, gender, income, education, region, and family, offers insights into how the demographic structure is shifting and influencing attitudes to climate change and environmental policies over time. Some of the key environmental variables used for this study include:\n\u2022 Current lifestyle (scenv_crlf): determines if an individual's lifestyle is environmentally friendly and if they need to adopt pro-environmental behaviours.\n\u2022 Support for climate policies (scenv_pmep, orga3): Quantifies the public support for environmental policies from gathering personal habit variables, such as willingness to pay for green tariff and whether an individual is happy to devote time to environmental groups.\n\u2022 Perceptions about climate change (scenv_meds): If an individual has a positive perception about the world's future at tackling environmental issues. This determines their mindset about whether they think it is worth the effort to implement green policies.\nThese environmental variable add depth to the study with a comprehensive review of public opinions from different angles, making these variables ideal for fine-tuning an LLM into producing representative responses. In addition to the environmental issue relating variables, Table 1 contains 3 sample profiles of synthetic respondents used for defining the LLMs' role through a system prompt.\nTo prepare UKHLS data for fine-tuning, several preprocessing steps aim to ensure that the data is consistent and relevant. Firstly, imputation corrects the invalid profiling values to rebalance the demographic structure. Secondly, the variables' format is standardized to optimise the number of prompting tokens and to ensure the outputs distributions are comparable with each other. Lastly, sampling techniques, like the Synthetic Minority Over-sampling Technique (SMOTE) increase the proportion of underrepresented groups to ensure balance across socio-demographic groups."}, {"title": "Fine-Tuning Process", "content": "Fine-tuning enables the selected LLMs to improve their simulation of public opinions on environmental issues using the UKHLS datasets. To effectively prepare the models for fine-tuning, the parameters are weighted according to the importance of the selected features. This step allows the LLMs to become sensitive to socio-demographic features that significantly influence public opinions. For example, understanding the pattern that higher-income individuals are likely to support carbon taxes illustrates how incorporating this information into fine-tuned LLMs helps predict how different income groups answer climate relating questions. \nTo further illustrate this process, an example fine-tuning method by Howard and Ruder uses universal language model fine-tuning (ULMFiT). This approach addresses overfitting on smaller training datasets and establishes a robust inductive learning structure for NLP tasks. Specifically, this architecture uses three layers of Long Short-Term Memory (LSTM) networks, emphasizing the retention of conditioning knowledge, which has demonstrated superior performance compared to existing text classification models . While ULMFiT demonstrated superior performance in NLP tasks than other pre-training methods, it is more suitable for the LSTM-based models. In contrast, transformer-based models, such as GPT or BERT, use different fine-tuning techniques, like Adapters or LoRA (Low-Rank Adaptation)."}, {"title": "Prompt Engineering", "content": "Prompt engineering defines the context for fine-tuning LLMs into generating realistic and context-sensitive responses. There are two types of prompts for simulating public responses:\n1. The system prompt defines the role of the system using a set of specific profiling variables from the UKHLS datasets. An example of a system prompt is illustrated in Figure 3, which specifies the profile of an individual, including their demographic information and attitudes toward environmental issues."}, {"title": null, "content": "2. A user prompt defines the instruction for the system to execute, which guides the LLMs into generating more contextually-appropriate predictions of opinions. Figure 4 presents an example of a user prompt that instructs LLMs to determine if an individual is willing to pay for environmental-friendly products.\nIdeologically, I describe myself as a Liberal Democrat supporter. Racially, I am British. I am male.\nMy marital status is Single. In terms of my qualifications, My highest qualification is Secondary\neducation. I have 5 children. I live in the Southeast. I live in a rural area. In terms of my age, my age\ngroup is 60-69 years old. My profession is Semi-Routine Occupations. When I asked to write my\nresponse to the question, \"And which of these would you say best describes your current\nlifestyle?\", I respond with I do quite a few things that are environmentally friendly."}, {"title": "Evaluation Metrics", "content": "Several evaluation metrics assess how effective fine-tuning is at guiding the models into producing accurate public opinions:\n1. Chi-Square Tests\n2. Cosine Similarity\n3. Jaccard Index\n4. Kullback-Leibler Divergence (KL Divergence)"}, {"title": null, "content": "Chi-Square tests assess whether synthetic distributions match with the expected distributions from the UKHLS dataset. Equation 1 highlights how these tests help to evaluate the LLMs' ability at learning the relationship between demographic variables and the public opinions on environmental policies.\n$\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}$ (Equation 1)\nCosine similarity measures similarities by modelling synthetic and expected distributions as vectors between -1 (dissimilar) and +1 (very similar) (Equation 2). Unlike other metrics, cosine similarity is a robust metric that aren't affected by the scale of the responses when comparing distributions, which is helpful for accounting minority or controversial opinions. As normalized values, cosine similarity is better at measuring the direction of differences in opinions instead of being distorted by the magnitude in differences.\n$S_c(A, B) = cos(\\theta) = \\frac{A \\cdot B}{||A|| \\cdot ||B||} = \\frac{\\sum_{i=1}^{N} A_i B_i}{\\sqrt{\\sum_{i=1}^{N} A_i^2} \\sqrt{\\sum_{i=1}^{N} B_i^2}}$ (Equation 2)\nWhen evaluating questions with categorical response options, particularly the questions with a binary set of response options, the Jaccard Index is great for quantifying the similarities and determine if a characteristic is present or absent from the synthetic responses. By converting the responses into a binary format, Jaccard index divides the size of intersection between the synthetic and expected distributions by the size of their union to produce a value between 0 (no overlap) and 1 (perfect overlap) (Equation 3).\n$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$ (Equation 3)\nKL-divergence, commonly known as the relative entropy between two distributions, measures how much the synthetic distribution diverges from the expected distribution for selected questions about environmental issues (Equation 4). If KL-divergence is 0, the synthetic and expected distribution match perfectly, as the KL-divergence score increase, the discrepancies also increase.\n$D_{KL}(P||Q) = \\sum_{i} P(i) \\cdot log(\\frac{P(i)}{Q(i)})$ (Equation 4)"}, {"title": "Results", "content": null}, {"title": "Improvement through Fine-Tuning", "content": "Fine-tuning improved the large language models' (LLM) performance in ten questions about attitudes to environmental issues. Fine-tuned models achieved more similarities between the synthetic and expected distributions as observed from the smaller Chi-Square (average 0.9288 (fine-tuned) vs 1.1827 (pre-trained)) and KL-divergence (average 0.1137 (fine-tuned) vs 0.1630 (pre-trained)). Tables 3 and 4 show the pre-trained and fine-tuned LLMs' response distributions across each selected question. After the visualisations in Figures 5 to 14 determined that fine-tuning improved the LLMs' ability at generating responses representative of the expected distributions (see Fig. A1 \u2013 A10 from Page 26 to Page 35). For example, in Figure 7, fine-tuned results show a reduced proportion of pro-environmental responses, which balances the data distribution into a more consistent shape with the expected data distribution."}, {"title": "Comparison of Pre-Trained vs. Fine-Tuned Models", "content": "Although the pre-trained models were powerful for performing general natural language processing (NLP) tasks, they struggle to interpret and generate domain-specific public opinions. Before fine-tuning, the models' output often fail to capture how different socio-demographic factors influence scepticism in environmental policy-making. An example simulating public opinions on carbon taxes, a pre-trained model generates a uniform response distribution, neglecting how different income or support for political parties affect the people's opinions . After fine-tuning, the model produced more diverse responses by demographic groups. For example, when prompting with a profile of a 45-year-old and university-educated individual living in an urban area, the fine-tuned model predicted support for developing renewable energy infrastructure and increasing carbon taxes on businesses, an output mirroring responses from the UKHLS datasets. On the other hand, LLMs without fine-tuning produce oversimplified responses that ignored regional factors, inaccurately assuming that people living in rural and urban areas share same views of support for renewable energy. Table 2 summarises a comparison between the pre-trained and fine-tuned models by policy support and showed more polarising levels of support for carbon taxes by income groups."}, {"title": "Quantitative and Qualitative Analysis", "content": "The statistical results from the fine-tuned models in Table 5 provide critical insights into their performance in simulating public opinions on environmental issues by summarising the statistics from Tables Al and A2 on Pages 36 and 37. The Chi-Squared test, with an average score of 0.9288 for the fine-tuned models compared to 1.1827 for the pre-trained models, indicates closer alignment with expected distributions sampled from the UKHLS dataset. These results show that fine-tuned models are successful at integrating socio-demographic factors in understanding how they shape public attitudes, demonstrating their enhanced capabilities at reflecting complex real-world opinions.\nAn average p-value of 0.3358 for fine-tuned models, compared to 0.2768 for pre-trained models, suggests that while differences between observed and expected distributions aren't significant, fine-tuned LLMs are better equipped for mimicking public sentiments. These outcomes imply that policymakers can use these models to accurately predict public support for environmental initiatives, enabling more informed decision-making.\nCosine similarity scores quantify semantic similarities between both synthetic and expected distributions. The LLMs achieved an average cosine similarity score of 0.72 (fine-tuned) and 0.6792 (pre-trained), which shows that fine-tuned models generated data distributions that matches more closely with the expected distributions than the pre-trained models (Table 5). This reinforces the idea that fine-tuning optimises the models' ability at capturing variations in opinions, essential for understanding the impact of conditioning demographic factors.\nJaccard Index measures the overlap between binary categorical outputs. The fine-tuned models scored an average of 0.72 (fine-tuned) and 0.6792 (pre-trained), which highlights their ability at accurately predicting categorical results. When examining the Jaccard indexes of the questions with binary response options, e.g., Pollution (fine-tuned: 0.7167 and pre-trained: 0.68) and Membership of an Environmental Organization (fine-tuned: 0.7167 and pre-trained: 0.69), the similar Jaccard indexes implies that fine-tuning had a more limited impact on improving the LLMs performance on classifying questions with binary outcomes.\nThis limitation suggest that more synthetic examples are needed to highlight the subtle variation in the questions containing binary response options, such as more precise geographical data could help determining the level of pollution alongside reports of local air quality index (AQI), Particulate Matter Concentration (PM2.5 and PM10), water quality index (WQI), and noise pollution levels (dB). These additional metrics create detailed profiling examples to understand how an individual answers binary-response questions. Both examples show a class imbalance by the pre-trained models with a higher proportion of people being members of an environmental organization and experiencing pollution (see Fig. A8 and A9 on Pages 33 and 34)."}, {"title": "Discussion", "content": null}, {"title": "Fine-Tuning as a Key Element in LLM Design", "content": "Fine-tuning turns general-purpose LLMs into specialized models that can effectively handle domain-specific tasks, in fields like environmental and climate sciences (Objective 3). In this study, fine-tuning significantly improved the LLMs' ability to generate representative responses closely aligning with the real-world data from UKHLS datasets (Objective 1). This aligns with similar findings from Betka (2023), who optimised fine-tuned models to reduce runtime by 50% while maintaining accuracy within a 5% margin. For example, in this study, fine-tuned models reduced the fraction of people who are highly aware of their environmental impact, a prominent area of bias in pre-trained models (see Fig. A3 on Page 28). This reduction in bias is comparable to the improvements seen from Xu's ChildTuning method, which updates one subset of parameters in the pre-trained network while masking non-child network gradients, improves vanilla pre-trained models by 1.5 to 8.6 points across four selected models. Unlike Xu's study, this study's results confirm that fine-tuned LLMs can accurately represent minority opinions, such as those who are opposing the green tariffs (see Fig. A1 \u2013 A10 from Page 26 to Page 35).\nBuilding on this foundation, including tailored profiling variables such as job insecurity and spending habits revealed additional factors influencing renewable energy spending decisions. These profiling variables enhanced the fine-tuned models' ability to identify stakeholders who are against spending more on green tariff electricity (see Fig. A4 on Page 29). However, this inclusion risks creating stereotypes against a particular income group (see Table 2 on Page 13), which calls for more effective metrics, such as Chi-Square tests, adversarial debiasing, and user feedback mechanisms, for addressing both technical and ethical dimensions of the LLMs' performance."}, {"title": "Challenges in Fine-Tuning", "content": "One significant challenge in fine-tuning LLMs is overfitting, especially when working with smaller or more domain-specific datasets such as the UK Household Longitudinal Study (UKHLS) datasets (Objective 4). Since the UKHLS dataset, collected between January 2009 and May 2023, has a temporal gap that might not capture more recent technological developments, evolving awareness of climate change, and shifts in the geo-political landscape. Therefore, using the UKHLS as a reference data might amplify existing biases between the dataset and contemporary opinions, which mistakenly guides the LLMs in a wrong direction for fine-tuning.\nWhen selecting the data, sampling bias remains a critical issue when providing accurate training datasets for conditioning and comparing the LLM-generated synthetic responses with (Objective 4). In the conditioning dataset, there are plenty of missing and invalid profile parameters, such as 97% of the \"qfhigh\" (highest qualification) variable are inapplicable (-8). After imputing invalid values for the highest qualification variable, it distribution might not be nationally representative. E.g., comparing the selected UKHLS variable with the"}, {"title": "Ethical Considerations", "content": "Fine-tuning LLMs have some profound ethical implications. For example, the LLMs inherit biases from their training data, which makes it difficult to select a perfect sample to train the models. Although the sampled data typically consists of surveys intended to be representative on a national or regional level, they suffer from non-responses in some socio-demographic groups. Mitigating these biases requires careful data processing, such as designing tailored questionnaires to track the interest of each stakeholder group continuously.\nAnother concern is about the transparency of the LLMs and how they generate the outputs based on the roles defined through the profiling variables. To ensure transparency, it is vital to record profiles used to condition the LLMs. Having a transparent model is vital for building trust with stakeholders, so that they know how their data is used. Counterfactuals are great for learning about different opinions as observed in the two examples from Table 6 that shows two opposite opinions about whether climate change is controllable.\nIdeologically, I describe myself as a Green Party supporter. Racially, I am Mixed-race. I am a female. My marital status is Single. In terms of my qualifications, my highest qualification is bachelor's degree. I have no children. I live in the Southwest. I live in a rural area. In terms of my age, my age group is 25-29 years old.\nMy profession is Creative Occupations. When asked to write my response to the question, \"And which of these would you say best describes your current lifestyle?\", I respond with I do a lot of things that are environmentally friendly. I strongly agree that climate change is controllable.\nIdeologically, I describe myself as a Reform UK supporter. Racially, I am White British. I am male. My marital status is Married. In terms of my qualifications, my highest qualification is Secondary education. I have 3 children. I live in the Northeast. I live in a suburban area. In terms of my age, my age group is 45-49 years old. My profession is Skilled Trades. When asked to write my response to the question, \"And which of these would you say best describes your current lifestyle?\", I respond with I do some things that are environmentally friendly. I tend to disagree that climate change is controllable."}, {"title": "Future Work", "content": "While this study demonstrates how effective fine-tuning is at enhancing LLMs for generating public opinions, there are areas for exploring how multi-tasking learning optimises fine-tuning by allowing the LLMs to process multiple tasks simultaneously while aiming to reduce the risks of overfitting. Transfer learning allows LLMs to multi-task by processing the training data subsets efficiently, especially when the data is sparse or expensive to obtain, such as in environmental policy simulations. A sample study by Pilault identified a method for multi-task learning to help select relevant profiling parameters for conditioning LLMs. This method balances the weighting of the parameters and achieved 2.2% better performance than fully fine-tuned BERT models when applied on the GLUE benchmark while compressing the data to 64.6% of its original volume.\nWhen optimising fine-tuning, additional methods adjust learning rate and depth depending on the complexity of the LLMs' tasks. Jin conducted a research evaluating learning rates and how they might not be suitable for the current LLM architectures; this feedback was used to propose an alternative LRBench framework, which identifies optimal learning rate parameters for fine-tuning LLMs. Focusing on the models' layers to maximise their relevance for performing in new domains, adaptive fine-tuning is a more efficient alternative without compromising the models' performance.\nFinally, integrating active learning into LLMs ensures that they could query informative samples for learning during fine-tuning. For example, Mahalingam identified that active learning helps to reduce the number of labelled data required while enhancing performance in areas like image classification and object recognition. Despite an occasional hallucinating output, the results of active learning confirm that supplementing the models with actual information, \u201cground truth\u201d, reduces the number of false positives and improves the performance above the baseline for all the selected models."}, {"title": "Conclusion", "content": null}, {"title": "Summary of Findings", "content": "This study has demonstrated that fine-tuning prepares large language models (LLMs) for domain-specific applications, such as simulating opinions for reshaping environmental policies. After fine-tuning, models such as GPT-40, GPT-40-mini, and GPT-401-preview, produced improved synthetic responses that closely align with real-world distributions, as evidenced by the Chi-Square test scores, Cosine Similarity, KL-divergence scores, and Jaccard Indexes (see Table 5 on Page 19). These metrics allow the LLMs to account for socio-demographic factors, like regional, income, and educational differences. As a result, pre-trained models tend to generalize, producing more opinions favouring environmental policies. Fine-tuning improved the response distribution and allowed the representation of minority opinions, such as determining if an individual is willing to pay for green products and whether an individual wants to join an environmental organisation (see Fig. Al - A10 on Pages 26 to 35). These results affirm that fine-tuning is essential to transform LLMs from general-purpose tools into specialized systems that can simulate complex public opinions accurately."}, {"title": "Broader Implications", "content": "Fine-tuning has emerged as a foundational tool to prepare LLMs to meet its domain-specific requirements. Recent applications include integrating fine-tuned models for detecting hallucinations in machine-translated text, which showed excellent performances across different languages, and simulating how the American population reacts to diverse stimuli, achieving high levels of correlation between synthetic and real-world test data. This study contributes to the growing evidence that fine-tuning improved the outputs' relevance. In high-stakes fields such as healthcare, education, law, and finance, where precision and accountability are vital, fine-tuning ensures reliable and domain-specific outputs.\nFurthermore, the success of fine-tuned models in simulating opinions on environmental policies underscores their potential for reshaping existing policies and boosting public engagement with scalable tools for learning diverse societal attitudes. As the demand for LLMs grow, fine-tuning will be vital for guiding the models into meeting the ethical and technical guidelines. Embedding data privacy into RoBERTa model helped them to achieve an 87.8% accuracy with a privacy budget of \u025b = 6.7, which confirms the maintenance of high accuracy . Looking forward, future developments will likely focus on reducing the computational costs of fine-tuning while enhancing LLMs' capabilities across multiple domains.\nIn conclusion, this research defines the indispensable role of fine-tuning in optimising LLMs. The results set the stage for performing further explorations into how to select suitable conditioning datasets for implementing more dynamic domain-specific multi-tasking missions."}]}