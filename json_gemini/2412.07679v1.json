{"title": "RADIO Amplified: Improved Baselines for Agglomerative Vision Foundation Models", "authors": ["Greg Heinrich*", "Mike Ranzinger*", "Hongxu (Danny) Yin", "Yao Lu", "Jan Kautz", "Andrew Tao", "Bryan Catanzaro", "Pavlo Molchanov"], "abstract": "Agglomerative models have recently emerged as a powerful approach to training vision foundation models, leveraging multi-teacher distillation from existing models such as CLIP, DINO, and SAM. This strategy enables the efficient creation of robust models, combining the strengths of individual teachers while significantly reducing computational and resource demands. In this paper, we thoroughly analyze state-of-the-art agglomerative models, identifying critical challenges including resolution mode shifts, teacher imbalance, idiosyncratic teacher artifacts, and an excessive number of output tokens. To address these issues, we propose several novel solutions: multi-resolution training, mosaic augmentation, and improved balancing of teacher loss functions. Specifically, in the context of Vision Language Models, we introduce a token compression technique to maintain high-resolution information within a fixed token count. We release our top-performing models, available in multiple scales (-B, -L, -H, and -g), alongside inference code and pretrained weights.", "sections": [{"title": "1. Introduction", "content": "The rise of specialized vision foundation models has created a need for methods to consolidate knowledge from multiple models into a single one. SAM-CLIP [39] addresses this challenge by combining SAM [19] and CLIP [27] to integrate capabilities from both. In another approach, AM-RADIO [30] introduces label-free knowledge distillation from multiple teacher models, enabling knowledge transfer without direct supervision. UNIC [32] adds intermediate teacher matching projectors and dynamic teacher selection. Theia [33] aims to facilitate robot learning by distilling insights from multiple vision teachers. Meanwhile, PHI-S [29] studies the importance of normalizing the distinct teacher distributions to simplify their balancing. Relatedly, Eagle [34] leverages a mixture of vision encoders to achieve inference-time knowledge aggregation within the context of vision-language models (VLMs).\nDespite these advancements, this growing body of work on knowledge agglomeration still leaves open several critical challenges:\n\u2022\n\u2022\n\u2022\nResolution balancing: Teacher models operate at varying resolutions due to different architectures and training goals, creating feature granularity inconsistencies. Effective techniques are needed to balance these resolutions in the student model to capture both fine details and broader abstractions.\nTeacher distribution balancing: Existing models have different distribution moments and the distillation process should account for this to prevent biased learning.\nGenerating multi-resolution features for diverse applications: Vision models support various applications requiring different feature resolutions, from image captioning to dense segmentation. A VFM that can flexibly produce features at any resolution could serve multiple tasks, reducing the need for separate models, and opening up new opportunities.\nIn our analysis of existing agglomerative models, we study and propose a fix for the notable \"mode switch\" phenomenon in AM-RADIO, where feature distributions shift significantly based on input resolution (Section 3.1). Specifically, low-resolution inputs yield DINO-like features, while high-resolution inputs produce SAM-like features. We trace this behavior to the student learning different teachers at different resolutions during training. In section 4.2, we introduce a solution to stabilize these mode switches, achieving strong resolution robustness and improved quality scaling.\nArmed now with a vision encoder that works best at high resolution, we next look toward integrating it into VLMs. In the case of downstream applications with vision-language models, a common pitfall is the number of output tokens/features. By processing images in high resolution, most methods will return more"}, {"title": "2. Background", "content": ""}, {"title": "2.1. Baseline Model", "content": "Following AM-RADIO[30], our model consists of a Vision Transformer (ViT) [10] backbone, including CPE for multi-resolution support. We use DFN CLIP [13], OpenAI CLIP [27], DINOv2-g-reg [9] and SAM-H [19] as teachers. For each teacher to distill from, we augment the backbone with an adaptor for the summarization token, and an adaptor for the patch tokens. Our adaptor is a 2-layer MLP with a LayerNorm[1] and a GeLU[16] in between. We train the student using teacher features generated by inferring images of the DataComp1B [14] dataset. We train the model with a batch size of 1024+128 for 600k iterations. Training is split into two concurrent partitions: one partition trains the student at a resolution of 4322 against CLIP and DINOv2 teachers with batch size 1024, while the other partition trains the student at a resolution of 10242 against SAM with a batch size of 128."}, {"title": "2.2. Evaluation Framework", "content": "Evaluating the quality of a foundation model is a daunting task due to the wide spectrum of downstream applications. We employ a rigorous evaluation framework to identify areas for improvement and guide design decisions:\n\u2022\n\u2022\nImage-level reasoning: We focus on ImageNet-1k [31] Top-1 classification accuracy using (i) zero-shot [27] classification using a pretrained and frozen language model; and (ii) k-NN [4] classification.\nPixel-level semantic segmentation: We linearly probe the frozen backbone features for se-"}, {"title": "3. Challenges", "content": ""}, {"title": "3.1. Achieving Multi-Resolution Robustness", "content": "Although AM-RADIO [30] supports a wide range of input resolutions, we noticed poor performance on high-resolution benchmarks. As illustrated in Figure 2, Zero-Shot Top-1 accuracy drops sharply after inputs are larger than roughly 512px. This showcases an undesirable property of this model, which is that it doesn't reliably scale to high-resolution images. AM-RADIO hints at this issue in its conclusion: the model exhibits a \"mode switching\" behavior as resolution increases. This prompted us to visualize the features and investigate the effect of a resolution increase. Figure 3 illustrates the issue well: at resolutions lower than or equal to 5122, the features most closely resemble those of DINOv2 and appear to be expressive of depth and semantic content. At higher resolutions, the model starts to behave more like SAM, with features that show sharp delineations around contours, with homogeneous contents within. This behavior is intuitive given the fact that in the high-resolution regime the student only sees SAM features. Table 2 shows a quantified measure of feature variance across scales (see implementation in appendix). DINOv2 exhibits much smaller scale variance."}, {"title": "3.2. Token Count", "content": "Dense downstream tasks, such as pixel-level semantic segmentation and depth estimation, benefit from receiving fine features from a vision encoder. However, for vision-language models like LLaVA [21], where visual tokens are integrated into a sequence of text embeddings, an excessive number of vision tokens can negatively impact performance or lead to sequence overflows. Pixel unshuffling, as proposed in InternVL1.5 [6], addresses this by reducing the number of vision tokens, grouping 2 \u00d7 2 tokens along the channel dimension. However, this approach cannot adapt to varying information densities within an image. For instance, a text document might have large areas of white background that should be more strongly compressed than areas containing text."}, {"title": "4. Method", "content": ""}, {"title": "4.1. Scale Equivariance", "content": ""}, {"title": "4.1.1. A Measure of Scale Equivariance", "content": "We define a measure of scale equivariance for a set of multi-scale features by normalizing and interpolating all features to the scale of the lowest resolution, then calculating the spatial average of the variance along the scale dimension. Our formula can be found in appendix A.5.\nAs can be seen in Table 2, our baseline student exhibits much worse scale equivariance than that of DINOv2 (the only teacher that is capable of operating at multiple resolutions, hence our only point of comparison)."}, {"title": "4.1.2. Tiling", "content": "Tiling is commonly employed in VLMS (LLaVa-NeXT[21], InternVL1.5[6]) as a way to enable high-resolution inference when the vision encoder only supports fixed-resolution inputs. Thus it could be argued that scale equivariance is of little importance, given that it is possible to resort to tiling instead of increasing input image resolution. However tiling incurs additional challenges:\n\u2022\n\u2022\nTiling makes resolution increases very coarse.\nAs the number of tiles increases, the vision encoder sees an increasingly small subset of the full image, limiting its ability to reason about the full context, or even know what it's looking at entirely."}, {"title": "4.2. Multi-Resolution Training", "content": "The previous observations motivate a change in the training schedule: with multi-resolution training we enable the student to learn from all teachers across multiple resolutions. This is easily achieved with DINOv2, since DINOv2 can infer images at any resolution. For CLIP teachers, we feed them with images at the teacher's native resolution, and we feed the student with images at multiple resolutions. We interpolate student features down to the resolution of the teacher's features before applying the loss function. SAM presents an extra challenge, as noted in AM-RADIO[30], since interpolating SAM features significantly deteriorates their quality. Therefore in order to train our student against SAM at lower resolutions than 10242, we pad smaller images to 10242, then crop SAM features to the effective size of the unpadded image.\nTraining the student for 600k iterations with a multi-resolution setup is costly. Thus we break down training into three stages:\n\u2022\n\u2022\n\u2022\nIn a first stage, we train the student at low resolution (2562) for 300k iterations.\nIn a second stage, we train the student at medium resolution (4322) for 300k iterations.\nIn the third stage, we train the student simultaneously at 4322 and 10242 resolutions for 300k"}, {"title": "4.3. Mosaic Augmentation", "content": "The training schedule described in Section 4.2 involves running SAM inference on padded images, using cropped features to train the student against SAM at low resolution. This approach incurs a substantial computational cost, as SAM is the most resource-intensive teacher.\nTo improve efficiency when training a student at a resolution < 5122 against SAM, we can instead create a mosaic of k \u00d7 k images, with k = [1024] and x being the student resolution, resulting in a single 10242 image. We then perform SAM inference on this mosaic and extract k\u00b2 individual feature maps to train the student. Mosaic augmentation includes padding as needed to maximize efficiency. For example, to train a student at 4322 resolution, we can create a 2\u00d72 mosaic with 80-pixel padding around each image. Figure 7 shows sample mosaic augmentations under 256 and 432 student resolutions. Qualitatively, we observe cleaner features after applying mosaic augmentation, which we believe is due to the increased diversity in image positions, helping to reduce positional encoding artifacts. We also show SAM'S PCA features for these mosaic images in figures A5 and A4 in the appendix."}, {"title": "4.4. Teacher Loss Balancing", "content": "PHI-S [29] highlights the significant variations in activation magnitudes among vision foundation models, observing that SAM's activations tend to overshadow those of CLIP and DINOv2 models. We adopt the PCA-Hadamard Isotropic Standardization (PHI-S) method, achieving improved balance among teacher losses. PHI-S rotates teacher activations to evenly distribute variance across all channels and then scales them to obtain unit variance. This process can be easily reversed by projecting the student activations back into each teacher's original feature space. PHI-S enhances training stability and overall benchmark performance. For a given teacher feature map X with embedding size C, PHI-S applies the following transformation:\nX'_i = \\Phi_i R_i X_i, R_i = H_c U_i, \\Phi_i = \\sqrt{\\frac{1}{ \\Lambda + \\epsilon }},(1)\nWhere Hc is a normalized Hadamard matrix of dimension C, Aj are the Eigen values of the covariance matrix \u03a3 [X], and U are the corresponding eigenvectors. di and Ri are specific to the ith teacher.\nAs a starting point, we define a measure of fidelity, similar to that in the classification-distillation literature ([37, 2]), however we do so without the use of labels or explicitly produced distributions over classes. Instead, since our loss objective is to directly match the features of the teachers, we have\nF[X] = \\frac{Var [t_i (X)]}{Var [f(X) - t_i(X)]} = \\frac{Var [t_i (X)]}{MSE(f(X),t_i(X))}(2)\nwith f(X) being the student feature distribution, and ti (X) being the ith teacher distribution. This function represents the ratio of the target distribution variance to the student's estimation error variance. A value of < 1 means random sampling from the teacher distribution would be better, and would be perfect matching. We show the results of this in table 3, where it is apparent that the baseline allocates too much energy to matching SAM due it its disproportionately large distribution variance, consistent with [29]. The errors relative to the variance are overall smaller when applying PHI-S."}, {"title": "4.5. Is SAM a good teacher?", "content": "SAM [19] has been a controversial choice in the recent agglomerative models literature. AM-RADIO [30] struggled to prove that its inclusion improved any"}, {"title": "4.6. Partitioning", "content": "In PHI-S [29], the authors opted to put teachers in their own partitions, which reduces the teacher overhead (as the per-teacher batch size is reduced). However, the paper does not ablate whether this choice came with model quality consequences. In Table 5 we study the number of partitions for the first two stages of training. We find that fewer partitions is strongly better for summarization tasks, and less clear for dense tasks."}, {"title": "4.7. Token Compression", "content": "Inspired by ToMeSD [3], we evaluate the use of bipartite soft matching to merge similar tokens. We apply strided partitioning to ensure that each image region retains some representation in the compressed features. For evaluation, we track merged token indices, enabling us to unmerge tokens and measure the reconstruction error and inform hyperparameter selection. The method is illustrated in Figure 8. While ToMe recommends using attention keys for matching, we found that using the features themselves improves reconstruction error. Table A9 of the appendix summarizes our findings.\nWe also qualitatively evaluate token merging by visualizing feature and token merging outcomes in Figure 5. Token merging is assessed within the context"}, {"title": "4.8. Feature Selection", "content": "For each image, our foundation model outputs a summary vector along with patch tokens at a granularity of one per 162 input pixel block. For image-level tasks such as classification, search, or curation, the summary vector provides a rich embedding. For dense tasks, such as segmentation or 3D understanding, the patch tokens are a natural choice. However, as demonstrated in previous work, incorporating additional intermediate activations further enhances performance. For example, [11] uses a Dense Prediction Transformer (DPT) head [28] for 3D reasoning, while [43] averages multiple ranges of intermediate activations to feed into an LLM for VLMs.\nIn this section, we investigate various feature selection methods and present the results in Table 7. We experiment with sparse feature selection (selecting activations from individual layers throughout the model) and dense feature selection (aggregating information across all layers by averaging groups of"}, {"title": "4.9. SigLIP Teacher", "content": "Building on previous related work ([38], [20]), we replace OpenAI-CLIP [27] with SigLIP [44] and observe significant improvements in VLM tasks, as shown in Table 1."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Ablation Studies", "content": "Ablation studies are summarized in Table 1. Our initial baseline is a reproduction of AM-RADIO, using a ViT-L backbone and the DataComp1B[14] dataset. Our hyperparameters are detailed in the appendix. We experiment with several variations of the training schedule, as described in Section 4."}, {"title": "5.2. Comparative Results", "content": ""}, {"title": "5.2.1. Dense Task Evaluation", "content": "We evaluate our models in comparison to other agglomerative models of similar size (SAM-CLIP [39], Theia [33], UNIC [32], UNIT [46]). Following MLORE [42] we report metrics on NYUDv2 and PASCAL Context for our model. We train with a learning rate of le 3, and use a weight of 1 for all tasks. We purposefully don't tune any hyperparameters. We keep the backbone model frozen, and use MLORE's \"conv\" head for each task. The conv head is defined as follows: Conv-3x3 \u2192 BatchNorm \u2192 GeLU \u2192 Conv-1x1.\nResults are reported in Table 8. RADIO-AMP models exhibit a consistent improvement over previous baselines."}, {"title": "5.2.2. VLM Evaluation", "content": "We compare our models against state-of-the-art vision encoders: OpenAI-CLIP [27], AM-RADIO-H [30] and SigLIP-SO400M [44]. We pair the vision encoders with MN-Minitron-8B [36] and train them in VILA [20] using the LLaVA1.6 [21] data mixture. For all vision encoders, we use Token Merging to compress the visual features to 196 tokens in order to ensure a fixed cost to the Language Model. We report results in Table 9. We observe that RADIO-AMP models consistently exceed the accuracy of other vision encoders."}, {"title": "6. Conclusion", "content": "In this paper, we presented RADIO Amplified, a robust framework for multi-teacher vision model distillation that accommodates variations in teacher resolutions, activation magnitudes, and noise patterns. The key differentiating factor in our work is our focus on preserving accuracy across a broad range of resolutions. Our multi-resolution training approach mitigates resolution-dependent performance degradation (mode switching). Our findings show that mosaic augmentation and PHI-S effectively balance computational load and loss contributions from each teacher.\nToken compression enables efficient integration with VLMs by preserving critical visual information, even at high compression ratios. Our feature selection study highlighted the advantages of using intermediate activations for dense tasks, particularly when non-linear transformations are used."}]}