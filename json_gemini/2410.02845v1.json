{"title": "TOWARDS LAYER-WISE PERSONALIZED FEDERATED LEARNING: ADAPTIVE LAYER DISENTANGLEMENT VIA CONFLICTING GRADIENTS", "authors": ["Minh-Duong Nguyen", "Nguyen H. Tran", "Khoi Do", "Khanh Le", "Duc Nguyen", "Chien Trinh", "Zhaohui Yang"], "abstract": "In personalized Federated Learning (pFL), high data heterogeneity can cause significant gradient divergence across devices, adversely affecting the learning process. This divergence, especially when gradients from different users form an obtuse angle during aggregation, can negate progress, leading to severe weight and gradient update degradation. To address this issue, we introduce a new approach to pFL design, namely Federated Learning with Layer-wise Aggregation via Gradient Analysis (FedLAG), utilizing the concept of gradient conflict at the layer level. Specifically, when layer-wise gradients of different clients form acute angles, those gradients align in the same direction, enabling updates across different clients toward identifying client-invariant features. Conversely, when layer-wise gradient pairs make create obtuse angles, the layers tend to focus on client-specific tasks. In hindsights, FedLAG assigns layers for personalization based on the extent of layer-wise gradient conflicts. Specifically, layers with gradient conflicts are excluded from the global aggregation process. The theoretical evaluation demonstrates that when integrated into other pFL baselines, FedLAG enhances pFL performance by a certain margin. Therefore, our proposed method achieves superior convergence behavior compared with other baselines. Extensive experiments show that our FedLAG outperforms several state-of-the-art methods and can be easily incorporated with many existing methods to further enhance performance.", "sections": [{"title": "INTRODUCTION", "content": "The challenge of non-independent and non-identically distributed (non-IID) data significantly impacts personalized Federated Learning (pFL). Addressing the aforementioned problem, numerous re-searchers have delved deeply into various directions, e.g., 1) consisting adding regularization (T. Dinh et al., 2020; Li et al., 2020), 2) pseudo representations generation (Zhu et al., 2021; Zhang et al., 2022), 3) on-client data pre-processing (Huang et al., 2022; 2024a), 4) gradient update modification (Reddi et al., 2021; Wang et al., 2020; Sun et al., 2023; Huang et al., 2024b), 5) cross-client feature alignments (Zhang et al., 2023a; Dinh et al., 2022), and 6) adaptive global update by leveraging on-server gradients (Jhunjhunwala et al., 2023; Panchal et al., 2023). These approaches are orthogonal in nature, enabling their combined integration to achieve further performance enhancement.\nLately, model layer disentanglement has emerged as a promising approach to further enhancing the performance of pFL (Oh et al., 2022; Collins et al., 2021; Chen & Chao, 2022; Xu et al., 2023). This approach involves the disentanglement of local model into two distinct components: global aggregation layers (GAL) and personalized layers (PL). GAL manages common tasks among all users, while PL handles the specific tasks according to each user. However, existing methods require extensive fine-tuning to determine which layers should be used for global aggregation and"}, {"title": "2 PROBLEM FORMULATION & PRELIMINARIES", "content": "We use $\\text{span}(g_1,...,g_s)$ as the subspaces that are contained by all vectors ${g_1,..., g_s}$. We consider an FL system comprising a set of users denoted by $\\mathcal{U} = {u | u = 1, 2, ..., U}$. Each user gains access to its local data, which remains inaccessible to others. Specifically, the data collected by the $u$-th user can be represented by $\\mathcal{D}_u \\in \\mathbb{R}^{N_u}$, where $N_u$ denotes the number of data instances for user $u$. The entire dataset available across all users can be denoted by $\\mathcal{D} = \\bigcup_{u \\in \\mathcal{U}} \\mathcal{D}_u$, and we have $N = \\sum_{u=1}^U N_u$. We use the term $E$ to represent the number of local epochs. We abuse the notations $\\mathbf{w}_u^{(r)}$ and $\\mathbf{w}^{(r)}$ to refer to the local model of user $u$ and the global model at round $r$, respectively."}, {"title": "2.1 NOTATIONS", "content": "."}, {"title": "2.2 PROBLEM SETUP", "content": "During FL training, users iteratively conduct local training and communicate with the server for model updating. To be specific, our FL concept works as follows:"}, {"title": "2.3 NEGATIVE TRANSFER AND GRADIENT CONFLICTS IN MULTI-TASK LEARNING", "content": "A major challenge for MTL is negative transfer, which refers to the performance drop on a task caused by the learning from other tasks, resulting in degradation in overall performance. A rationale of this phenomenon is the conflicting gradients (Yu et al., 2020a). Specifically, gradients from different tasks may point in different directions so that directly optimizing the average loss is detrimental to a specific task's performance. Denote $\\mathbf{g}_i = \\nabla \\mathcal{L}(\\mathbf{w}_i)$ as the gradient of task $i$, and $\\theta_{ij}$ as the angle between two task gradients $\\mathbf{g}_i$ and $\\mathbf{g}_j$, we have\nDefinition 2.1 (Conflicting gradients (Yu et al., 2020a)). Given two gradients $\\mathbf{g}_i, \\mathbf{g}_j$ ($i \\neq j$), and $\\cos \\theta_{ij} = \\frac{\\mathbf{g}_i^\\top \\mathbf{g}_j}{\\|\\mathbf{g}_i\\| \\|\\mathbf{g}_j\\|}$ is the cosine between two vectors. $\\mathbf{g}_i$ and $\\mathbf{g}_j$ ($i \\neq j$) are said to be conflicting with each other if $\\cos \\theta_{ij} < 0$.\nThe definition suggests that gradient conflicts within a model can be analyzed at the layer level, offering a foundation for exploring and determining optimal strategies for layer disentanglement."}, {"title": "3 VALIDATION OF LAYER-WISE GRADIENT CONFLICTS IN FL", "content": "We argue that the aggregation of layers with layer-wise gradient conflicts is detrimental in FL. To validate our arguments, we conduct two experiments to answer two questions:\nQuestion 3.1. Is gradient conflicts appear among users in federated learning?\nTo answer the question 3.1, we conduct the experiments on 2-D toy dataset (the details of the toy dataset is reported in Appendix G). The result is visualized Fig. 2a. In the non-IID setting, the divergences between two pairs of gradients (e.g., $g_{1u}^{(r)}$ vs. $g_{3u}^{(r)}$ and $g_{2u}^{(r)}$ vs. $g_{4u}^{(r)}$) result in the divergence of the FL process (Fig. 2a, top). Due to gradient conflicts (angles exceeding $\\pi/2$), the model diverges along two hyper-spaces, $\\text{span}(g_{1u}^{(r)}, g_{3u}^{(r)})$ and $\\text{span}(g_{2u}^{(r)}, g_{4u}^{(r)})$ (Fig. 2a, bottom). In contrast, under IID settings, the gradients form smaller angles and align toward consistent directions. Consequently, the model progresses more directly toward the optimal solution. As noted in Appendix G, the optimal solution is located at $y = 0$, implying that $w_1 = 0$, $w_2 \\neq 0$, and $b = 0."}, {"title": "4 \u041c\u0415\u0422\u041dODOLOGY", "content": "Building upon the validations presented in Section 3, we introduce FedLAG, a method that exploits layer-wise gradient conflicts to identify the optimal selection for disentangling personalized and generic layers. The process of FedLAG is briefly illustrated as in Figure 3. The core contribution of FedLAG is its ability to utilize users' gradients as a proxy for their behavior on the server, enabling an analysis of user relationships without direct access to local datasets. By taking user gradients as input, FedLAG applies the gradient divergence analysis (GDA) to examine gradients at the layer level (see Section 4.1). Upon performing GDA, the $GC_{\\varepsilon}(l)$ score is computed, which quantifies the degree to which a given layer should be personalized. Consequently, we can leverage $GC_{\\varepsilon}(l)$ to adaptively disentangle generic and personalized layers (see Section 4.2)."}, {"title": "4.1 GRADIENT DIVERGENCE ANALYSIS", "content": "To implement the GDA, we calculate the gradient of each user $u$ on round $r$:\n$h_u^{(r)} = \\sum_{e=0}^{E-1} \\eta_\\mathcal{L}'(\\mathbf{w}_u^{(r,e)}) =  \\mathbf{w}_u^{(r,E)} - \\mathbf{w}_u^{(r)},$  (1)\nwhere the layer-wise gradient $h_l^{(r)}$ of $h_u^{(r)}$ can be represented as\n$h_{l,u}^{(r)} = \\sum_{e=0}^{E-1} \\eta_\\mathcal{L}'(\\theta_{l,u}^{(r,e)}) = (\\theta_{l,u}^{(r,E)} - \\theta_{l,u}^{(r)})$  (2)\nHere, $h_u^{(r)}$ and $h_l^{(r)}$ denote the gradient of $\\mathbf{w}_u^{(r)}$ and the $l^{th}$ layer of $\\mathbf{w}_u^{(r)}$, respectively. We further define $\\mathbf{h}_u^{(r)} = [h_{1,u}^{(r)}, h_{2,u}^{(r)}, ..., h_{L,u}^{(r)}]$, where $h_{l,u}^{(r)}$ is a l-th layer-wise gradient of user u. The gradient is calculated by utilizing the models from the previous round in conjunction with the recently received model. As a result, FedLAG is both communication-efficient and capable of information aggregation.\nTo evaluate the layer-wise gradient divergence between two users, we introduce $\\phi^{(r)}(u, v)$ to signify the angle between $h_u^{(r)}$ and $h_v^{(r)}$. Based on Definition 2.1, we define gradient conflict of layer $l$ in the round $r$ with hyper-parameter $\\xi$ (w.r.t $-1 < \\xi \\leq 0$) as the layer-wise gradient conflict score, denoted by $GC_{\\varepsilon}(l)$.\nDefinition 4.1 ($GC_{\\varepsilon}(l)$ score). Given the threshold $\\xi$, the $GC_{\\varepsilon}(l)$ of the $l$-th layer is calculated as the number of distinct user pairs $(u, v)$ (where $u \\neq v$) that satisfy $\\cos \\phi^{(r)}(u, v) < \\xi$. For instance,\nGC_{\\varepsilon}(l) = \\frac{1}{\\binom{U}{2}} \\sum_{u=1}^U \\sum_{v=1,v\\neq u}^U  \\mathbb{I}_{u,v}, \\text{ s.t. } \\mathbb{I}_{u,v} = \\begin{cases} 1, \\text{ if } \\cos \\phi^{(r)}(u, v) < \\xi,\\\\ 0, \\text{ otherwise}, \\end{cases} (3)\nwhere $\\cos \\phi^{(r)}(u, v) = \\frac{\\langle h_{l,u}^{(r)}, h_{l,v}^{(r)} \\rangle}{\\|h_{l,u}^{(r)}\\| \\|h_{l,v}^{(r)}\\|}, \\forall v, u \\in \\mathcal{U}$; $\\xi$ denotes the extent of conflict severity. To elaborate, by setting a smaller value for $\\xi$, the angle between the two vectors becomes more obtuse.\nConsequently, the Definition 4.1 allows us to focus primarily on the count of more prominent conflicts. Subsequently, $GC_{\\varepsilon}(l)$ acts as an indicator for conflicting gradients across various severity levels within the layers. If $GC_{\\varepsilon}(l)$ takes on the value $\\binom{U}{2}$, it implies that for any two users, there is a conflict in their gradients w.r.t the l-th layer. By computing these layer-wise conflict scores, we can pinpoint the layers where conflicts occur most frequently."}, {"title": "4.2 LAYER-WISE PERSONALIZED MODEL AGGREGATION", "content": "To achieve adaptive layer disentanglement, we base our approach on two key principles: (1) maintain the global aggregation of FL on non-conflict layers, and (2) motivate the personalized learning on conflict layers. Specifically, when a layer is significantly affected by gradient conflict, we transform it into a personalized layer to prevent negative transfer resulting from the aggregation process at the global server. To this end, rather than broadcasting the entire model to the local users, we restrict the local model update to the global layer (i.e., layers assigned to GAL):\n$\\theta_{l,u}^{(r+1)} = \\frac{1}{U} \\sum_{u=1}^U \\theta_{l,u}^{(r,E)}, \\text{ if } l \\in \\mathcal{L}_g.$  (4)\nTo motivate the personalized learning, the users do not update the conflict layers:\n$\\theta_{l,u}^{(r+1)} = \\theta_{l,u}^{(r,E)}, \\text{ if } l \\in \\mathcal{L}_p.$   (5)\nHere, $\\mathcal{L}_p$ represents the layers that suffer from the gradient conflict and need to be converted into personalized layers. The detailed description of our method is demonstrated in Algorithm 1. Compared to the conventional pFL, FedLAG only alters the aggregation process on the global server, leaving the local training unaffected. Consequently, our technique is applicable to various existing FL or pFL approaches."}, {"title": "5 THEORETICAL ANALYSIS", "content": "In this section, we discuss the convergence of our algorithm. Our theoretical analysis aims to show the improvement in terms of upper bound reduction for the convergence upper boundary."}, {"title": "5.1 LAYER-WISE LOSS IMPROVEMENT", "content": "To show the robustness and prove the convergence of FedLAG, we first want to analyze the performance improvement when using our LAG algorithm in FL.\nLemma 5.1 (Personalization Improvement). Each user u achieve an improvement in loss when using FedLAG over the vanilla FL approach as follows:\n$\\mathcal{L}(W_{LAG,u}^{(r)}) - \\mathcal{L}(W_{VFL,u}^{(r)}) = -\\eta \\sum_{v=1}^{U} \\sum_{l \\in \\mathcal{L}_p} (\\|h_{l,v}^{(r-1)}\\| \\|h_{l,u}^{(r-1)}\\| - \\cos \\Phi_{l,u}^{(r-1)}\\))  < 0,$   \nLemma 5.2 (Generalization Improvement). For any sufficiently small learning rate $\\eta$, the following holds:\n$\\mathcal{L}(W_{LAG,g}^{(r)}) - \\mathcal{L}(W_{VFL,g}^{(r)}) = -\\eta \\sum_{u=1}^{U} \\sum_{v=1}^{U} \\sum_{l \\in \\mathcal{L}_p}  \\mathbb{I}_{vu} \\|h_{l,u}^{(r)}\\|^2 \\times (\\|h_{l,v}^{(r-1)}\\| \\cos \\Phi_{l,u}^{(r-1)}\\)) < 0,$   \nwhere $W_{LAG,g} = {\\theta_{s,g}, \\theta_{p,g}}$ and $W_{VFL,g} = {\\theta_{s,u}, \\theta_{p,u}}$ stand for the FL with and without the integration of FedLAG, respectively.\nAccording to Lemma 5.2, at each round $r$, the loss function of the layer-wise pFL model consistently surpasses that of the vanilla pFL model by a specific margin. This enhancement is directly proportional to both the layer-wise gradient norm from the previous round $r-1$ and the layer-wise angle between pairs of users in the FL system. Consequently, the proof establishes that the application of FedLAG is viable by relying on the previous gradients for estimating gradient conflicts, eliminating the need for using the upcoming local gradients, which is unavailable at the global server. This approach avoids communication overheads associated with exchanging information among local users. Furthermore, the improvement remains constant in each round upon integrating the LAG, in contrast to the FL algorithm lacking LAG integration. The proof of Lemma 5.2 is provided in Appendix H.4."}, {"title": "5.2 CONVERGENCE ANALYSIS", "content": "We use Assumptions H.1, H.2, H.4, H.3 and have the following theorem:\nTheorem 5.3. Assuming users compute full-batch gradient with full participation and $\\frac{1}{2\\sqrt{6}E^2L} < \\eta < \\frac{\\mu}{L^2}$, the series ${\\mathbf{w}^{(r)}}$ generated by FedLAG satisfy:\n$\\mathcal{L}(\\mathbf{w}^{(R)}) - \\mathcal{L}(\\mathbf{w}^*) \\leq O(\\frac{\\|\\mathbf{w}_g^{(0)} - \\mathbf{w}^*\\|^2}{RnE}) + O(\\eta \\sigma^2) + O(\\eta^2 E (E-1) L\\sigma^2) \n- O(A \\sum_{u=1}^U \\sum_{v=1}^{U} \\sum_{l \\in \\mathcal{L}_p} \\|h_{l,v}^{(r)}\\|(\\|h_{l,u}^{(r)}\\| - \\cos \\Phi_{l,u}^{(r)}\\|)),$  (6)\nwhere $\\mathbf{w}^* \\arg \\min_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w})$ is the optimal global model, and $A = (24E^2(E-1)\\eta^4 L^2 - \\eta^2 E) > 0, \\eta,\\sigma, E, L$.\nThe convergence rate in Theorem 5.3 consists of four terms:\n\u2022 The first term $O(\\frac{\\|\\mathbf{w}_g^{(0)} - \\mathbf{w}^*\\|^2}{RnE})$ is the initialization error term that depends on the total communication rounds $R$ and local learning epoch $E$. This term is fixed among all FL algorithms and independent of the FedLAG hyperparameters.\n\u2022 The second term $O(\\eta \\sigma^2)$ is the noise at optimum. This term reveals that the prediction at the optimum always make a certain variance.\n\u2022 The third term $O(\\eta^2 E (E-1) L\\sigma^2)$ refers to the user drift error, which is the error induced by the divergence when the user drift toward their specific domain characteristics. It affects by the data characteristics and FL hyperparameters such as the number of epoch $E$ and learning rate $\\eta$.\n\u2022 The last term $O(A \\sum_{u=1}^U \\sum_{v=1}^{U} \\sum_{l \\in \\mathcal{L}_p} \\|h_{l,v}^{(r)}\\|(\\|h_{l,u}^{(r)}\\| - \\cos \\Phi_{l,u}^{(r)}\\|))$ is the personalized loss improvement term. This loss shows the improvement in loss thanks to the disentanglement in FL models into GAP and PL. We can see that this term consistently reduces the bound and thus creates an absolute improvement to the FL system regardless of whether the FL algorithm is integrated into it.\nFrom the Theorem 5.3, we can have the following remarks:\nRemark 5.4. When the conflict does not occur (e.g., the data is IID), no layer being assigned to the PL subset, which makes $\\sum_{u=1}^U \\sum_{v=1}^{U} \\sum_{l \\in \\mathcal{L}_p} \\|h_{l,v}^{(r)}\\|(\\|h_{l,u}^{(r)}\\| - \\cos \\Phi_{l,u}^{(r)}\\|))$ becomes 0. Therefore, the convergence of the FedLAG reduces to $O(\\frac{\\|\\mathbf{w}_g^{(0)} - \\mathbf{w}^*\\|^2}{RnE}) + O(\\eta \\sigma^2) + O(\\eta^2 E (E-1) L\\sigma^2)$.\nRemark 5.5. When we set the top k layers to 0, the sum of $\\sum_{u=1}^U \\sum_{v=1}^{U} \\sum_{l \\in \\mathcal{L}_p} \\|h_{l,v}^{(r)}\\|(\\|h_{l,u}^{(r)}\\| - \\cos \\Phi_{l,u}^{(r)}\\|))$ becomes 0, therefore, the convergence of FedLAG reduces to $O(\\frac{\\|\\mathbf{w}_g^{(0)} - \\mathbf{w}^*\\|^2}{RnE}) + O(\\eta \\sigma^2) + O(\\eta^2 E (E-1) L\\sigma^2)$.\nRemark 5.6. From the equation, as the number of layers becomes large (i.e., over-parameterization), the right hand side may close to 0. However, in Appendix H.6, we prove that the value of the last term is agnostic to the number of parameters in the network model. The reason is because as the number of layers increase, the layer-wise gradient norm will decrease respectively.\nRemark 5.7. Increasing $E$ and $\\eta$ amplifies the value of A, boosting the improvement term. However, this enhancement affects local learning in distributed users. For instance, a high value of E intro-duces bias towards local characteristics, causing users to forget global knowledge learned through aggregation, which loses the generality of the pFL concept.\nRemark 5.8. The FedLAG's main contribution is on the server. Therefore, FedLAG can be integrated with any other FL algorithms to improve other algorithms' performance. The theoretical results of integration between FedLAG and other FL algorithms are demonstrated in Appendix D."}, {"title": "6 EXPERIMENT SETUP", "content": "To conduct a fair comparison among methods, we consider four different context data sets including MNIST (28 \u00d7 28, 10 modalities) (LeCun et al., 1998), CIFAR10 (32 \u00d7 32 \u00d7 3, 10 modalities) (Krizhevsky, 2012), EMNIST (28 \u00d7 28, 62 modalities) (Cohen et al., 2017), and CIFAR100 (32 \u00d7 32 x 3, 100 modalities) (Krizhevsky, 2012). Otherwise, each data set is divided into several parts corresponding to the number of users by randomly distinguished distribution. Each divided part is identical to the others to ensure that non-IID term of FL problem. Each user owns each part containing a train and test set, to which no data augmentation method is applied."}, {"title": "6.1 DATASETS", "content": "To assess the robustness of our proposed FedLAG, we run evaluations on 5 other baselines (i.e., FedAvg (McMahan et al., 2017), PerAvg (Fallah et al., 2020), FedBABU (Oh et al., 2022), FedPAC (Xu et al., 2023), and FedRoD (Chen & Chao, 2022), FedCAC (Wu et al., 2023), FedDBE (Zhang et al., 2023b), GBFL (Zhang et al., 2023c), which are all trained from scratch using ResNet18 (He et al., 2016). We apply the same settings on all baselines to achieve the fairest experimental evaluations. In detail, each method is conducted in with unbalanced data distribution and non-IID scenarios, along with different sampling rates \u03b1 = 0.1%, and 0.5%. Otherwise, various numbers of users are applied to perform a fair comparison among methods (i.e. 20, 40, 60, 80, 100), corresponding with the different number of global rounds: 100, 200, 400, 600, and 800 rounds, respectively."}, {"title": "6.2 BASELINES", "content": ""}, {"title": "7 EXPERIMENTAL EVALUATIONS", "content": "To evaluate the overall performance of the FL system, we compute average results across users. presents a detailed overview of comprehensive performance metrics, where \u03b1 represents the Dirichlet coefficient. The table explores the FL performance of two settings: 1) varying participation ratio and 2) different levels of heterogeneity."}, {"title": "7.1 OVERALL PERFORMANCE", "content": ""}, {"title": "7.1.1 DIFFERENT HETEROGENEITY LEVELS", "content": "We assess at two heterogeneity levels, i.e., \u03b1 = 0.1, 1. The table reveals that FedLAG outperforms other baselines, with improvements ranging from an average of 5 \u2013 7% to 15% when \u03b1 = 0.5. The resilience of FedLAG becomes more evident in a more challenging setting, namely \u03b1 = 0.1, where it demonstrates a substantial improvement over other baselines, averaging 10% to 40%. Among the competing baselines, FedRoD poses a notable challenge. This is because, in FedRoD, the authors"}, {"title": "7.1.2 DIFFERENT USER PARTICIPATION RATIO", "content": "We evaluated under five different participation ratio, i.e., {20%, 40%, 60%, 80%, 100%}, and illustrated as in As it can easily be seen from the table, our proposed FedLAG can achieve significantly higher performance, as opposed to other baselines (i.e., an average of 20% up to 40% in performance improvement). The improvement is more significant when we employ the algorithm in more challenging data sets, which showcases a more divergence in data characteristics among users. The detailed comparisons between FedLAG and other baseline models in terms of comprehensive training are presented in Appendix E."}, {"title": "7.2 ABLATION TEST", "content": ""}, {"title": "7.2.1 INTEGRATABILITY", "content": "In this section, we evaluavate FedLAG 's compatibility into other FL baselines on the Cifar10 dataset, and illustrate the overall performance as in Figure 5. The integration of FedLAG showcase a significantly better accuracy over baselines."}, {"title": "7.2.2 Is FEDLAG MORE EFFICIENT THAN FIXED LAYER DISENTANGLEMENT?", "content": "To prove the robustness of FedLAG over fixed layer disentanglement techniques, we conduct experiments on FedBABU, and fix K layers for personalized layers. Tab. 4 demonstrates that the fixing the last K layers generally yields the most efficient performance. Other settings show significant drop in performance, compared to that of the FedLAG. This observation aligns with the assumption that gradient conflicts tend to be concentrated in the final layers, though not all layers with high conflict scores are necessarily the last layers. In total, we can obviously see that the adaptive layer disentanglement of FedLAG shows a significant robustness over fixed layer disentanglement."}, {"title": "7.2.3 TOP k SCORE LAYER", "content": "We conducted experiments with varying values of k, and the results are presented in As shown in the table, selecting the top k layers leads to significantly improved performance compared to the case where k = 0. This enhancement is notable because when k = 0, the algorithm reduces to the FedAvg algorithm, lacking the robustness inherent in the GDA algorithm. Our analysis reveals that gradient conflicts predominantly affect only a small number of layers in the FL model. Consequently, setting k = 5, 10, 15 does not result in a significant performance improvement, as the most conflicted gradients are already captured in the initial layers with the highest conflicts. Detailed results are presented in Appendix F.2."}, {"title": "7.2.4 CONFLICT GRADIENT SCORE", "content": "demonstrates the performance of FedLAG under different conflict score $\\varepsilon$. As observed in the table, there is negligible performance disparity despite different gradient scores being applied. This is because the score primarily impacts the frequency of gradient conflicts across various model layers, yet it does not alter the distribution of conflicted gradients throughout the model layers. A high value of $\\varepsilon$ (e.g., 0) results in performance drops due to an excessive inclusion of gradient pairs with small conflicts. Some of these pairs are beneficial for learning common structures and should not be excluded. Conversely, a too-small value for $\\varepsilon$ (e.g., -0.3) also leads to performance degradation by ignoring many gradient pairs with significant conflicts, which can be detrimental to the learning process. Detailed results are presented in Appendix F.3."}, {"title": "7.2.5 EFFICIENCY WHEN TRAINING WITHOUT PRETRAINED MODELS", "content": "In this section, we evaluate FL without pretrained models on the CIFAR-10 and CIFAR-100 datasets to assess FL performance when training from scratch (see Figs. 6, 7). We only apply the FedLAG after first 30 rounds to sample the very first gradient trajectories for the initial process of GDA. Most baseline models encounter significant challenges when trained without pre-trained parameters. In contrast, our FedLAG consistently demonstrates superior performance, achieving the highest results alongside FedRod and FedPAC. The significant superiority of FedLAG is shown when training on the challenging CIFAR-100 dataset, which contains a large number of labels. While other federated learning baselines struggle to exceed random performance, FedLAG rapidly surpasses this threshold and achieves effective convergence in fewer than 100 communication rounds."}, {"title": "8 CONCLUSION", "content": "Current researches on layer disentanglement in pFL typically require extensive fine-tuning to achieve optimal separation between generic and personalized layers. In our study, we introduce an adaptive approach to disentangle these layers by leveraging a well-established principle in multi-task learning, namely, conflicting gradients. To address conflicting gradients among users without incurring communication overhead from user-to-user interactions, we propose a novel data-free gradient divergence analysis method performed on the server. This technique enhances federated learning performance by enabling the selective assignment of network layers to personalization when layer-wise gradients are in conflict, and to generic layers otherwise. Our proposed method, FedLAG, demonstrates significant improvements over current baselines in both accuracy and convergence time."}, {"title": "B RELATED WORKS", "content": "In MTL, the AI model consists of two distinguished groups: 1) shared encoder that is common to all tasks, and 2) task-specific decoders which are designed and learned independently for each task. Addressing the contemporary challenge of task conflicts in MTL, numerous approaches have been devised to mitigate the aforementioned issue, which can be categorized in two main directions, i.e., task loss balancing and gradient manipulation. (Yu et al., 2020a), projects each gradient onto the normal plane of another gradient and employs the average of these projected gradients for updates. (Chen et al., 2020), randomly drops some elements of gradients based on element-wise conflicts. (Liu et al., 2021), ensures convergence to a minimum of the average loss across tasks through gradient manipulation. (Javaloy & Valera, 2022), re-weights task gradients and rotates the shared feature space to mitigate conflicts. (Shi et al., 2023), leverages gradient information to modify network structure and address task conflicts at their core.\nPrior work has investigated how to improve the FL robustness against non-IID data. (Xu et al., 2023) training personalized models by exploiting a better feature extractor and user-specific classifier collaboration. (Li et al., 2020) adds a proximal term to the local training objective to keep updated parameters close to the original downloaded model. (Karimireddy et al., 2020) introduces control variates to correct the drift in local updates. (Li et al., 2021) adopts contrastive loss to improve representation learning.\n(Oh et al., 2022) decompose the entire local network into the body (extractor), which is related to universality, and the head (classifier), which is related to personalization. (Collins et al., 2021) proposes a method where the entire network is trained sequentially during local updates, but only the body is aggregated. During the local update phase, each client first trains the head using the aggregated representation. Then, within the same epoch, the client trains the body using its own head. (Chen & Chao, 2022) proposes to use the balanced softmax for learning generic models and vanilla softmax for personalized heads. Xu et al. (2023) design an objective function to constraint the body with the task of learning invariant features. However, most layer disentanglement approaches demand significant effort to identify the optimal layer selection, often leading to arbitrary choices for the body and head layers. Currently, there is no clear method for determining which layers should be personalized and which should remain generic.\nRecently, negative transfer has been discov-ered to be one of the most crucial issue in FL. (Bao et al., 2023) minimizes the pair-wise distribution distances between users to alleviate the negative transfer among clients. DisentAFL (Chen & Zhang, 2024) integrates mixtures of experts to selectively aggregate clients' representations. To this end, the FL system can aggregate the fine-grained inter-client relationships to achieve sufficient positive transfer while avoiding negative transfer. However, we believe that current methods lack a strong theoretical foundation that directly addresses negative transfer among clients."}, {"title": "Gradient-based Multi-task Learning", "content": "In MTL, the AI model consists of two distinguished groups: 1) shared encoder that is common to all tasks, and 2) task-specific decoders which are designed and learned independently for each task. Addressing the contemporary challenge of task conflicts in MTL, numerous approaches have been devised to mitigate the aforementioned issue, which can be categorized in two main directions, i.e., task loss balancing and gradient manipulation. PCGrad (Yu et al., 2020a), projects each gradient onto the normal plane of another gradient and employs the average of these projected gradients for updates. GradDrop (Chen et al., 2020), randomly drops some elements of gradients based on element-wise conflicts. CAGrad (Liu et al., 2021), ensures convergence to a minimum of the average loss across tasks through gradient manipulation. RotoGrad (Javaloy & Valera, 2022), re-weights task gradients and rotates the shared feature space to mitigate conflicts. RECON (Shi et al., 2023), leverages gradient information to modify network structure and address task conflicts at their core."}, {"title": "Mitigating non-IID in Federated Learning", "content": "Prior work has investigated how to improve the FL robustness against non-IID data. FedPAC (Xu et al., 2023) training personalized models by exploiting a better feature extractor and"}]}