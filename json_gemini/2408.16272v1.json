{"title": "Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal Grounding", "authors": ["Kaijing Ma", "Haojian Huang", "Jin Chen", "Haodong Chen", "Pengliang Ji", "Xianghao Zang", "Han Fang", "Chao Ban", "Hao Sun", "Mulin Chen", "Xuelong Li"], "abstract": "Existing Video Temporal Grounding (VTG) models excel in accuracy but often overlook open-world challenges posed by open-vocabulary queries and untrimmed videos. This leads to unreliable predictions for noisy, corrupted, and out-of-distribution data. Adapting VTG models to dynamically estimate uncertainties based on user input can address this issue. To this end, we introduce SRAM, a robust network module that benefits from a two-stage cross-modal alignment task. More importantly, it integrates Deep Evidential Regression (DER) to explicitly and thoroughly quantify uncertainty during training, thus allowing the model to say \"I do not know\" in scenarios beyond its handling capacity. However, the direct application of traditional DER theory and its regularizer reveals structural flaws, leading to unintended constraints in VTG tasks. In response, we develop a simple yet effective Geom-regularizer that enhances the uncertainty learning framework from the ground up. To the best of our knowledge, this marks the first successful attempt of DER in VTG. Our extensive quantitative and qualitative results affirm the effectiveness, robustness, and interpretability of our modules and the uncertainty learning paradigm in VTG tasks. The code will be made available.", "sections": [{"title": "Introduction", "content": "Video is emerging as the primary information carrier in the era of streaming media. With the influx of video data, the need for efficiently and precisely extracting desired content from videos according to users' queries is becoming increasingly essential. In response to these demands, Video Temporal Grounding (VTG) emerges as a core research area in the field of computer vision [9, 28, 7, 6, 21]. Given an untrimmed video with a textual query, VTG can be categorized into three main types: 1) accurately identifying specific segments, referred to as Moment Retrieval [12, 14, 27, 32]; 2) comprehending the meaningful gist of a video, known as Highlight Detection [27, 48, 54, 19]; and 3) grasping the overall content of the entire video or specific sections, termed Video Summarization [46, 3, 15, 47].\nExtensive research aimed at enhancing cross-modal reasoning to facilitate fine-grained and precise multi-modal alignment has led to significant advances in the field of VTG [30]. Nevertheless, few studies have focused on the epistemic and aleatoric uncertainties present in open-world human-computer interaction (HCI) [1]. To begin with, epistemic uncertainty can be attributed to Knowledge Gap and Semantic Ambiguity as shown in Figure 1 (a) and (b) respectively. Specifically, user queries and video inputs often come from out-of-distribution (OOD) sources, diverging\nTo this end, we attempt to integrate deep evidential regression (DER) [1] in VTG for the first time, leading to a novel network module, named Scrupulous Refinement with uncertainty-Awareness"}, {"title": "Related work", "content": "2.1 Video temporal grounding\nVideo Temporal Grounding (VTG) identifies correlated shots in videos based on natural language queries, which broadly supports various downstream video comprehension tasks, such as video moment retrieval [2, 8, 61, 38, 29], highlight detection [43, 48, 38], and video summarization [15, 23, 34, 39, 46, 51]. These tasks generally involve formulating the boundaries of significant semantic segments [23, 38, 30]. Numerous innovative and effective methods have been developed to address the challenges in VTG. For instance, CTRL [2] and MCN [12] initially generate proposals using sliding windows, which rank in terms of a cross-modal matching score. MomentDETR [27] applies a transformer to predict potential moments through learnable queries. Furthermore, QD-DETR [38] employs a cross-attention module and a negative pair training scheme to enhance multi-modal alignment. MomentDiff [29] initially sets random boundaries for temporal segments and iteratively refines them to better match the intended semantics. However, existing approaches typically yield deterministic predictions, operating under the assumption that semantic segments are demarcated by clear and precise boundaries. This presumption neglects the inherent ambiguity and uncertainty associated with determining the true extent of these segments. To address this gap, we explicitly model and quantify the semantic uncertainty of video segment boundaries.\n2.2 Uncertainty learning\nRecent studies have highlighted inherent ambiguities and biases in VTG datasets, which significantly impact the integrity and performance of models [63, 59]. These uncertainties are categorized into annotation and query uncertainties. Annotation uncertainty stems from varying temporal boundaries assigned by different annotators to the same query, while query uncertainty arises from the use of differing descriptions for the same video moment, underscoring the subjective nature of video interpretation [63, 59]. Furthermore, these datasets exhibit pronounced biases, with common events being overly represented and a small subset of queries accounting for most actions, leading to a skewed distribution that creates a long tail in ground-truth timestamps [59, 40]. These findings underscore the need for meticulous curation of datasets and the adoption of uncertainty-aware modeling approaches [4, 35, 62, 13]. Among the techniques for modeling uncertainty, Evidential Deep Learning (EDL) has shown promise. Originating from the principles of Dempster-Shafer Theory [45] and Subjective Logic [44, 24], EDL models uncertainty explicitly through the distribution of \"second-order probabilities\" over network outputs, finding applications across various classification tasks including action recognition [5], multi-view clustering [16, 17], zero-shot learning [21], and semantic segmentation [18] etc. Leveraging DER [1] for its extension, EDL has effectively been applied to regression tasks such as stereo matching [49] and emotion attributes estimation [52]. Nevertheless, DER faces challenges like evidence contraction due to the non-negativity of prior parameters in the Normal Inverse-Gamma (NIG) distribution. New regularizers have been developed"}, {"title": "Methodology", "content": "3.1 Problem definition\nGiven a video $V = \\{v_i\\}_{i=1}^{L_v}$ and a query $Q = \\{q_i\\}_{i=1}^{L_q}$, each represented as vectors in $R^D$ where $L_v$ and $L_q$ denote the counts of clips and tokens respectively, the task of VTG is to assign each clip a label $(m_i, s_i, f_i)$ signifying its relevance to Q.\n\u2022 Moment Retrieval: Identify clips in $V$ corresponding to moments in Q, with $m_i = [m_s, m_e]$ marking the time span of the nearest relevant moment.\n\u2022 Highlight Detection: Determine each clip's relevance to Q using a saliency score $s_i$, which reflects semantic alignment, scaled between [0, 1].\n\u2022 Video Summarization: Select clips for a concise video summary, where $f_i$ indicates whether a clip is included, taking values in {0, 1}.\n3.2 Overview\nFigure 3 illustrates our method's workflow, starting with encoding an untrimmed video and masked query using a frozen encoder. Our process involves two stages. Firstly, SRAM reconstructs masked query tokens with RFF blocks for cross-modal alignment. Next, it performs VTG based on the query. The evidential head assesses aleatoric and epistemic uncertainties, while the VTG and MLM heads manage corresponding task stages. Details on these components are in the subsequent sections.\n3.3 Scrupulous reflection with uncertainty awareness module\nReflective flipped fusion block. The Reflective Flipped Fusion (RFF) block processes inputs from the video and text branches, alternating the roles of video and text as queries and keys/values using shared parameters. Through the cross-attention module, initial features $V^{(1)}$ and $Q^{(1)}$ of video and"}, {"title": "Methodology", "content": "text branches are respectively updated, reflecting each other's information. Following cross-attention,\neach branch refines its features through self-attention to enhance internal feature representation.\nThe outputs of self-attention serve as inputs to the next iteration of the RFF block, progressively\nenhancing modal alignment. This process is applied sequentially from block 1 to block n in terms of\nEq. 1. After completing the n-th layer, the refined video and query features are output. The specific\nworkflow process is detailed in Appendix E.\n$V^{(i+1)} = SA^{(i)}_{vq}(CA^{(i)}_{vq}), Q^{(i+1)} = SA^{(i)}_{qv}(CA^{(i)}_{qv}), i = 1, 2, ..., n - 1$ (1)\nEvidential head. Temporal continuity in videos often causes adjacent frames to share similar\nsemantics, complicating precise boundary delineation and introducing subjective biases in annotations.\nTo mitigate this, we model semantic boundary moments using Gaussian distributions. Specifically,\nthe start and end moments of a video-query pair (V, Q) are each governed by distinct Gaussian\ndistributions. Observations of the same type (either all starts or all ends) are assumed to be i.i.d..\nWithout loss of generality, we formulate as follows:\n$b \\sim N(\\mu, \\sigma^2)$, (2)\nwhere $b \\in R^{1\\times H}$ represents the start or end of moments observed H times. The corresponding\nexpectation $\\mu$ and variance $\\sigma^2$ of the Gaussian distribution subject to NIG prior:\n$p(\\mu, \\sigma^2 | \\gamma, \\upsilon, \\alpha, \\beta) = N(\\mu | \\gamma, \\sigma^2\\upsilon^{-1})\\Gamma^{-1}(\\sigma^2 | \\alpha, \\beta)$ (3)\n$=\\frac{{\\beta}^{\\alpha}\\upsilon^{\\frac{1}{2}}}{\\Gamma(\\alpha)\\sqrt{2\\pi}\\sigma^2}\\sigma^{\\alpha+1}exp \\{-\\frac{1}{2\\sigma^2}[2\\beta + \\upsilon(\\gamma - \\mu)^2]$\\} (4)\nwhere $\\varphi = (\\gamma, \\upsilon, \\alpha, \\beta)$ are the prior NIG distribution parameters derived from the video content and\nuser queries, serve as conditionals for the Gaussian estimates of $b_i$, with $\\gamma\\in R, \\upsilon > 0, \\alpha > 1, \\beta > 0$.\nThe gamma function is denoted by $\\Gamma(\\cdot)$. We use a linear evidential predictor to estimate $\\varphi$, training it\nto maximize the likelihood. The maximum likelihood estimation for $b_i$ is given by:\n$p(b_i|\\varphi) = \\int_{0}^{\\infty}\\int_{-\\infty}^{\\infty} p(b_i | \\mu, \\sigma^2)p(\\mu, \\sigma^2 | \\varphi)d\\mu d\\sigma^2 = St(b_i; \\gamma, \\frac{\\beta(1 + \\upsilon)}{\\upsilon\\alpha}, 2\\alpha)$. (5)\nSince the likelihood function has a form of Student-t distribution (St), we minimize the negative\nlogarithmic likelihood (NLL) as follows. Detailed formulation can be found in Appendix A.1 and A.2.\n$C_{NLL}^{i} = - log p(b_i) = - log St (b_i; \\gamma, \\frac{\\beta(1 + \\upsilon)}{\\upsilon\\alpha}, 2\\alpha ))$. (6)\nUsing the NIG distribution, prediction, aleatoric, and epistemic uncertainties are calculated as follows:\n$E[\\mu] = \\gamma, E[\\sigma^2] = \\frac{\\beta}{\\alpha-1}, Var[\\mu] = \\frac{\\beta}{\\upsilon(\\alpha - 1)}$ (7)\nprediction aleatoric epistemic\nFor guiding evidential preditor to evaluate more reasonable and informative uncertainty, we attempt\nto design Geom-regularization to assist training. We introduce it in detail in Section 3.4.\nVideo temporal grounding head. The VTG head features three distinct modules for tasks outlined\nin section 3.1. For Video Summarization, the output from the frozen video encoder undergoes three\n1x3 Conv layers, each with a ReLU activation. The Moment Retrieval head is similar but outputs two\nchannels for offsets. Highlight Detection uses attentive pooling to form a sentence representation\nfrom query tokens, then computes the saliency score between video tokens and query as their cosine\nsimilarity. Details for each module and corresponding loss are available in the Appendix F.\nSemantic masking alignment. To ensure robust cross-modal alignment capabilities, during the\ninitial phase of alignment, entities within the query are masked at a specified ratio. This approach\ncompels the model to leverage contextual information available from the corresponding video and the\nremaining unmasked tokens in the query. Through the MLM head, the model infers and reconstructs"}, {"title": "Methodology", "content": "the masked tokens. The loss function associated with this process, aimed at optimizing the model's\ncross-modal inference capabilities, is outlined below:\n$L_{mlm} = -E[\\sum_{i=1}^l log P(w_i | U, V)]$ (8)\nwhere $l$ represents the number of masked tokens, $w_i$ the i-th masked token, U the unmasked\ntokens providing linguistic context, and V the video features that enhance cross-modal contextual\nunderstanding for accurate token prediction. After the warm-up in the first phase, in the next phase,\nthe MLM head is frozen and $L_{mlm}$ is not computed.\n3.4 Geom-regularization\nModels optimized only on observed samples with the NLL loss (i.e. Eq. 6) tend to overfit and exhibit\noverconfidence. To counter this, DER introduced a regularizer for the i-th prediction as follows:\n$\\pounds(\\varphi) = \\Delta \\cdot \\Phi$, (9)\nwhere $\\Delta = |b_i - y|$ represents the error, $\\Phi = 2\\upsilon + \\alpha$ denotes the evidence, and $\\theta$ are the model\nparameters, with $b_i$ as the ground truth. This heuristic regularization aims to mitigate overconfidence\nby suppressing evidence, particularly for samples with high error. However, excessive suppression\ncan lead to underconfidence due to non-adaptive suppression and sample imbalance. To be clear, we\nfirst consider the minus gradient of $L_R$ with respect to $\\Phi$ as follows:\n$-\\nabla_{\\Phi}\\pounds = -\\Delta$, (10)\nNon-adaptive suppression. We find that the gradient is solely related to error and unrelated to\nevidence, which means the model cannot determine when evidence has been sufficiently suppressed,\nas shown in Appendix C.3. This can lead to excessively harsh penalties on evidence.\nPenalties bias. As the model converges, the dominance of low error samples with small gradients\nskews the batch's average gradient. Consequently, this leads to over-suppression of their evidence,\nwhile high error samples see their evidence neglected or adversely adjusted, as shown in Appendix C.5.\nTo overcome these limitations, we introduce Geom-regularization, inspired by [1], promoting the\nprinciple that \"accurate predictions should have high evidence, while inaccurate ones should have\nlow evidence\". This approach provides more rational constraints rather than merely suppressing\nevidence. Initially, we normalize $\\Delta$ to $\\overline{\\Delta}$ and $\\Phi$ to $\\overline{\\Phi}$ (i.e. Appendix B.2), which ensures that the\nmodel assigns $\\overline{\\Phi} = 1$ to samples with $\\overline{\\Delta} = 0$, and $\\overline{\\Phi} = 0$ to samples with $\\overline{\\Delta} = 1$. We then ensure\nthat the points ($\\overline{\\Delta}, \\overline{\\Phi}$) closely follow the line $\\overline{\\Phi} + \\overline{\\Delta} = 1$ using a Type I-line regularizer as below:\n$\\pounds^{-L}(\\omega) = ||\\overline{\\Phi} + \\overline{\\Delta} - 1||_2$, (11)\nwe can follow the analysis for $\\pounds_R$. The minus gradient of $\\pounds^{-L}$ with respect to $\\overline{\\Phi}$ as below:\n$-\\nabla_{\\overline{\\Phi}}\\pounds^{-L} = -2(\\overline{\\Delta} + \\overline{\\Phi} - 1)$, (12)\nwhich indicates this simple regularizer offers a gradient that relates to both error and evidence,\nenabling adaptive evidence suppression. To reinforce this constraint for the minority of extreme\nsamples in a batch, we also introduce Type II-line regularizers:\n$\\pounds^{-L}(\\omega) = ||\\overline{\\Phi} + \\overline{\\Delta} - 1||_2 - ||\\overline{\\Phi} - \\overline{\\Delta}||_2^2$, (13)\n$\\pounds^{I-L}$ imposes constraints ensuring that the point ($\\overline{\\Delta}, \\overline{\\Phi}$) deviates from the line $\\overline{\\Delta} = \\overline{\\Phi}$, thereby\nexerting stricter control over extreme samples. In Appendix C.3, we demonstrate that our regularizers\neffectively guide the optimization direction for evidence.\nOur training objective for the evidential head is the combination of NLL and Geom-regularization:\n$\\pounds(\\omega) = \\{\\begin{array}{cc}\\lambda_{NLL}C_{NLL}^{i} + \\lambda_{geom}\\pounds^{L}(\\omega), & Type I\\\\\n\\lambda_{NLL}C_{NLL}^{i} + \\lambda_{geom}\\pounds^{I-L}(\\omega), & Type II\\end{array}$ (14)\nTo this end, our total loss can be formulated by a combination of grounding loss $L_G$ (discussed in\nAppendix F) and evidential loss:\n$\\pounds = L_G + \\frac{\\lambda_{der}}{N} \\sum_{i=1}^N \\pounds(\\omega)$, (15)\nwhere N symbolizes the number of clips in a training set."}, {"title": "Conclusion", "content": "As the development of Artificial General Intelligence (AGI) progresses, increasingly sophisticated VTG models are emerging. However, these models often falter when confronted with open-ended user inputs. Addressing this challenge, this paper introduces a robust VTG model, namely SRAM, that not only possesses VTG capabilities but also enables explicit and comprehensive quantification of potential uncertainties. This allows the model to provide credible responses to queries that exceed its operational scope, paving the way for future reliable, video-driven HCIs. Limited by data quality and scale, the model's modality alignment capabilities are not particularly notable. Nevertheless, it offers strategies for enhancing the trustworthiness of AI decisions. Future research will focus on further expanding the decision-making reliability and interpretability of multimodal large language models in video-related downstream tasks."}]}