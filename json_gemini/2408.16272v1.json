{"title": "Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal Grounding", "authors": ["Kaijing Ma", "Haojian Huang", "Jin Chen", "Haodong Chen", "Pengliang Ji", "Xianghao Zang", "Han Fang", "Chao Ban", "Hao Sun", "Mulin Chen", "Xuelong Li"], "abstract": "Existing Video Temporal Grounding (VTG) models excel in accuracy but often overlook open-world challenges posed by open-vocabulary queries and untrimmed videos. This leads to unreliable predictions for noisy, corrupted, and out-of-distribution data. Adapting VTG models to dynamically estimate uncertainties based on user input can address this issue. To this end, we introduce SRAM, a robust network module that benefits from a two-stage cross-modal alignment task. More importantly, it integrates Deep Evidential Regression (DER) to explicitly and thoroughly quantify uncertainty during training, thus allowing the model to say \"I do not know\" in scenarios beyond its handling capacity. However, the direct application of traditional DER theory and its regularizer reveals structural flaws, leading to unintended constraints in VTG tasks. In response, we develop a simple yet effective Geom-regularizer that enhances the uncertainty learning framework from the ground up. To the best of our knowledge, this marks the first successful attempt of DER in VTG. Our extensive quantitative and qualitative results affirm the effectiveness, robustness, and interpretability of our modules and the uncertainty learning paradigm in VTG tasks. The code will be made available.", "sections": [{"title": "1 Introduction", "content": "Video is emerging as the primary information carrier in the era of streaming media. With the influx of video data, the need for efficiently and precisely extracting desired content from videos according to users' queries is becoming increasingly essential. In response to these demands, Video Temporal Grounding (VTG) emerges as a core research area in the field of computer vision [9, 28, 7, 6, 21]. Given an untrimmed video with a textual query, VTG can be categorized into three main types: 1) accurately identifying specific segments, referred to as Moment Retrieval [12, 14, 27, 32]; 2) comprehending the meaningful gist of a video, known as Highlight Detection [27, 48, 54, 19]; and 3) grasping the overall content of the entire video or specific sections, termed Video Summarization [46, 3, 15, 47].\nExtensive research aimed at enhancing cross-modal reasoning to facilitate fine-grained and precise multi-modal alignment has led to significant advances in the field of VTG [30]. Nevertheless, few studies have focused on the epistemic and aleatoric uncertainties present in open-world human-computer interaction (HCI) [1]. To begin with, epistemic uncertainty can be attributed to Knowledge Gap and Semantic Ambiguity as shown in Figure 1 (a) and (b) respectively. Specifically, user queries and video inputs often come from out-of-distribution (OOD) sources, diverging significantly from the in-distribution (ID) data used in training. This natural discrepancy creates a Knowledge Gap, making it challenging for models to accurately understand and respond to user needs. Moreover, semantic ambiguities hinder accurate contextual understanding. Visual ambiguities often occur in images with extensive uniform backgrounds, such as planes in the sky, where the distinction between foreground and background blurs. On the textual side, ambiguities stem from subjective elements like emotions or speaking styles, making the intended meaning open to various interpretations. While epistemic uncertainty measures the uncertainty in model inference due to incomplete knowledge, aleatoric uncertainty arises from inherent variability in the dataset. In VTG, aleatoric uncertainty typically arises from subjective annotations and variations in low-level features. Subjective annotations occur when different annotators provide varying queries and labels for the same sample, influenced by their personal views and habits. Additionally, randomness stems from differences in video features such as texture, edges, resolution, lighting, camera jittering, and scene transitions, all of which disrupt the consistency of video analysis. Unfortunately, when confronted with anomalies, such as the abnormal queries shown in Figure 2, conventional VTG models often respond with nearly random and indiscreet answers. These models fail to handle potential uncertainties in an open world appropriately. This inadequacy is detrimental in scenarios that require cautious decision-making, such as security and confidential environments.\nTo this end, we attempt to integrate deep evidential regression (DER) [1] in VTG for the first time, leading to a novel network module, named Scrupulous Refinement with uncertainty-Awareness"}, {"title": "2 Related work", "content": "2.1 Video temporal grounding\nVideo Temporal Grounding (VTG) identifies correlated shots in videos based on natural language queries, which broadly supports various downstream video comprehension tasks, such as video moment retrieval [2, 8, 61, 38, 29], highlight detection [43, 48, 38], and video summarization [15, 23, 34, 39, 46, 51]. These tasks generally involve formulating the boundaries of significant semantic segments [23, 38, 30]. Numerous innovative and effective methods have been developed to address the challenges in VTG. For instance, CTRL [2] and MCN [12] initially generate proposals using sliding windows, which rank in terms of a cross-modal matching score. MomentDETR [27] applies a transformer to predict potential moments through learnable queries. Furthermore, QD-DETR [38] employs a cross-attention module and a negative pair training scheme to enhance multi-modal alignment. MomentDiff [29] initially sets random boundaries for temporal segments and iteratively refines them to better match the intended semantics. However, existing approaches typically yield deterministic predictions, operating under the assumption that semantic segments are demarcated by clear and precise boundaries. This presumption neglects the inherent ambiguity and uncertainty associated with determining the true extent of these segments. To address this gap, we explicitly model and quantify the semantic uncertainty of video segment boundaries.\n2.2 Uncertainty learning\nRecent studies have highlighted inherent ambiguities and biases in VTG datasets, which significantly impact the integrity and performance of models [63, 59]. These uncertainties are categorized into annotation and query uncertainties. Annotation uncertainty stems from varying temporal boundaries assigned by different annotators to the same query, while query uncertainty arises from the use of differing descriptions for the same video moment, underscoring the subjective nature of video interpretation [63, 59]. Furthermore, these datasets exhibit pronounced biases, with common events being overly represented and a small subset of queries accounting for most actions, leading to a skewed distribution that creates a long tail in ground-truth timestamps [59, 40]. These findings underscore the need for meticulous curation of datasets and the adoption of uncertainty-aware modeling approaches [4, 35, 62, 13]. Among the techniques for modeling uncertainty, Evidential Deep Learning (EDL) has shown promise. Originating from the principles of Dempster-Shafer Theory [45] and Subjective Logic [44, 24], EDL models uncertainty explicitly through the distribution of \"second-order probabilities\" over network outputs, finding applications across various classification tasks including action recognition [5], multi-view clustering [16, 17], zero-shot learning [21], and semantic segmentation [18] etc. Leveraging DER [1] for its extension, EDL has effectively been applied to regression tasks such as stereo matching [49] and emotion attributes estimation [52]. Nevertheless, DER faces challenges like evidence contraction due to the non-negativity of prior parameters in the Normal Inverse-Gamma (NIG) distribution. New regularizers have been developed"}, {"title": "3 Methodology", "content": "3.1 Problem definition\nGiven a video \\(V = \\{v_i\\}_{i=1}^{L_v}\\) and a query \\(Q = \\{q_i\\}_{i=1}^{L_q}\\), each represented as vectors in \\(R^D\\) where \\(L_v\\) and \\(L_q\\) denote the counts of clips and tokens respectively, the task of VTG is to assign each clip a label \\((m_i, s_i, f_i)\\) signifying its relevance to Q.\n*   Moment Retrieval: Identify clips in V corresponding to moments in Q, with \\(m_i = [m_s, m_e]\\) marking the time span of the nearest relevant moment.\n*   Highlight Detection: Determine each clip's relevance to Q using a saliency score \\(s_i\\), which reflects semantic alignment, scaled between [0, 1].\n*   Video Summarization: Select clips for a concise video summary, where \\(f_i\\) indicates whether a clip is included, taking values in {0, 1}.\n3.2 Overview\nFigure 3 illustrates our method's workflow, starting with encoding an untrimmed video and masked query using a frozen encoder. Our process involves two stages. Firstly, SRAM reconstructs masked query tokens with RFF blocks for cross-modal alignment. Next, it performs VTG based on the query. The evidential head assesses aleatoric and epistemic uncertainties, while the VTG and MLM heads manage corresponding task stages. Details on these components are in the subsequent sections.\n3.3 Scrupulous reflection with uncertainty awareness module\nReflective flipped fusion block. The Reflective Flipped Fusion (RFF) block processes inputs from the video and text branches, alternating the roles of video and text as queries and keys/values using shared parameters. Through the cross-attention module, initial features \\(V^{(1)}\\) and \\(Q^{(1)}\\) of video and"}, {"title": "4 Experiment", "content": "We focus on the following key considerations to conduct convincing experiments: 1) Despite focusing on robustness and interpretability, does our proposed SRAM model demonstrate competitive performance relative to current state-of-the-art VTG models? 2) Does the proposed Semantic Masking Alignment (SMA) and RFF blocks enhance performance in VTG tasks? 3) Does SRAM give low uncertainty when performing high localization accuracy statistically, and vice versa? 4) Is our proposed Geom-regularizer more robust than the vanilla regularizer (i.e., Eq. 9)? 5) Can the model output a high uncertainty score in various OOD scenarios to inform abnormality?\n4.1 Datasets and Implementation details\nDatasets. We conducted experiments on several widely used public datasets from diverse scenes: Charades-STA [12] (in-door activities), QVHighlights [27] (untrimmed daily vlogs & news), TACOS [42] (cooking scenes), Ego4d-NLQ [14] (egocentric videos) and TVSum (YouTube videos). The detailed information including specific task domains and sizes for different datasets is reported in Appendix B.1 Table 5 with their different hyperparameters."}, {"title": "4.2 Quantitative results", "content": "Comparison with the state-of-the-art. To demonstrate the effectiveness of SRAM, we compared it with 17 state-of-the-art methods across different datasets. As reported in Table 1, SRAM outperforms existing methods on various metrics. On the QVHighlights test set, for the Moment Retrieval task, SRAM-Large obtains 62.26% for R1@0.5 and 45.53% for R1@0.7. For the Highlight Detection task, SRAM-Large reaches 63.04% for HIT@1. In the Moment Retrieval task, SRAM-Large outperforms MomentDiff [29] by an average of 5.72%, and in the Highlight Detection task, it surpasses Uni-VTG [30] by an average of 1.65%. According to Table 2, we supplement the performance comparison on the SRAM-Base model for the TACOS, Charades-STA, and Ego4d-NLQ datasets. On the TACOS dataset, SRAM-Base outperforms the nearest competitor UniVTG by 2.3% and 2.0% respectively. For the Charades-STA dataset, SRAM-Base surpasses previous results by 0.8%, 2.2%, and 2.3% respectively. We observe that the Eg4d-NLQ dataset has distinct characteristics such as long video durations (8-20 minutes) and high textual ambiguity (mainly consisting of questions, i.e., \"When did I put my shirt into the closet?\"). These features pose significant challenges for accurate grounding moments. To validate SRAM's performance in video summarization, we present a comparison of the TVSum dataset in Table 3. For each domain in TVSum, SRAM has generally demonstrated strong performance. Specifically, in the VU domain, SRAM outperforms UniVTG by 8%. Additionally, on the overall average metrics, SRAM exceeds UniVTG by 3.6% and surpasses the UMT model, which utilizes audio modality, by 1.5%.\nAblation study. We first validated the effectiveness of SMA pretraining. Figure 19 (a) shows the model performance differences under different settings of epochs. Notably, when the number of SMA epochs increased from 0 to 50, there was a significant improvement in MAP (+1.5%). Furthermore, we tested the effectiveness of the proposed RFF blocks. As shown in Table 4, keeping the number of RFF blocks constant, using flipped cross attention significantly improved R1@0.5 by 1.55% compared to using cross attention separately in the visual and textual branches (split cross attention). A deeper ablation analysis of DER with our proposed Geom-regularizer is reported in Appendix D.1.\nParameter analysis. We analyze the impact of SMA epochs, SMA learning rate, weights of DER, and Geom-regularizer on the model's grounding performance. The results can be seen in Appendix D.2."}, {"title": "4.3 Qualitive resluts", "content": "Uncertainty Calibration. Since we propose geom-regularization to enhance the calibration of model predictions, we aim to assess the efficacy of our approach by contrasting the performance of aleatoric and epistemic uncertainty estimation with and without our regularization technique, as well as against using vanilla regularization in[1]. Ideally, optimal uncertainty measures should effectively identify deviations in predictions (i.e., take high uncertainty when the model is making errors). Figure 7 in Appendix C.2 illustrates our comparison of different regularization methods on the QVHighlights val set. The horizontal axis of each scatter plot represents \u2206 (i.e. normalized error), while the vertical axis represents one of the two types of uncertainty.\nBias sensitivity. As previously reported in studies, most Moment Retrieval datasets exhibit significant imbalances in the duration and position of moments. As shown in Figure 4 (a), using the QVHighlights dataset as an example, we visualize the joint distribution of the normalized start times and end times of all ground truth moments. The light-colored areas in the figure indicate regions with almost no moment distribution, leading to Temporal OOD. Higher epistemic uncertainty is demanded when samples belong to the Temporal OOD region. We analyzed the uncertainty predicted by the model in different time regions under various experimental settings in the QVHighlights dataset. Figure 4 (b) shows that without DER constraints, the evidential head's predicted aleatoric and epistemic uncertainty tends to simply fit the biased temporal distribution. Figure 4 (c) shows that using only the NLL constraint (i.e., Eq. 6), most regions exhibit extremely low epistemic uncertainty, indicating the model is overly confident in its predictions. Figure 4 (d) illustrates that with Vanilla regularization, although this approach does suppress the concentration of uncertainty in a specific region, it does not show sensitivity to OOD data. Figure 4 (e) demonstrates that with our proposed Geom-regularization term, the temporal OOD regions exhibit significantly higher epistemic uncertainty.\nAdversarial experiments To further validate the robustness of SRAM in OOD scenarios, we conducted adversarial experiments using the QVHighlights validation set. Specifically, we gradually add noise to (a) video embeddings (i.e., Figure 11), (b) text embeddings, as shown in Figure 12, and (c) both text and video embeddings (i.e., Figure 13). Using Gaussian kernel density estimation (KDE), we plotted the uncertainty distribution of the predictions for the entire validation set. It can be observed that as the level of noise increases, the model's epistemic uncertainty gradually shifts from smaller to larger values. Notably, when noise is added to both modalities simultaneously, the uncertainty exhibits a more pronounced increase.\nCase study. We constructed a typical adversarial example to demonstrate the effectiveness of SRAM. For example, if the video depicts \"A plane is flying in the sky,\" the model outputs very low epistemic uncertainty when given the correct textual query. However, when given an adversarial query (e.g. \"A bird is flying in the sky\"), the misalignment between the visual content and the semantics causes SRAM to exhibit higher aleatoric and epistemic uncertainty. More cases can be seen in Appendix 9."}, {"title": "5 Conclusion", "content": "As the development of Artificial General Intelligence (AGI) progresses, increasingly sophisticated VTG models are emerging. However, these models often falter when confronted with open-ended user inputs. Addressing this challenge, this paper introduces a robust VTG model, namely SRAM, that not only possesses VTG capabilities but also enables explicit and comprehensive quantification of potential uncertainties. This allows the model to provide credible responses to queries that exceed its operational scope, paving the way for future reliable, video-driven HCIs. Limited by data quality and scale, the model's modality alignment capabilities are not particularly notable. Nevertheless, it offers strategies for enhancing the trustworthiness of AI decisions. Future research will focus on further expanding the decision-making reliability and interpretability of multimodal large language models in video-related downstream tasks."}, {"title": "A Derivations", "content": "A.1 Normal Inverse-Gamma moments\nWe assume our data was drawn from a Gaussian with unknown mean and variance, (\\(\\mu, \\sigma^2\\)). We probabilistically model these parameters, \\(\\theta\\), according to:\n\\begin{equation}\n\\mu \\sim N(\\gamma, \\sigma^2v^{-1})\n\\end{equation}\n\\begin{equation}\n\\sigma^2 \\sim \\Gamma^{-1}(\\alpha, \\beta).\n\\end{equation}\nTherefore, the prior joint distribution can be written as:\n\\begin{equation}\np(\\mu, \\sigma^2 | \\gamma, v, \\alpha, \\beta) = p(\\mu) p(\\sigma^2)\n\\theta\\qquad\\qquad\\qquad\\quad\\  \\varphi\n\\end{equation}\n\\begin{equation}\n= N(\\gamma, \\sigma^2v^{-1}) \\Gamma^{-1}(\\alpha, \\beta)\n\\end{equation}\n\\begin{equation}\n= \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha) \\sqrt{2\\pi \\sigma^2} \\sigma^2^{\\alpha+1}} exp\\{-\\frac{2\\beta + v(\\gamma -\\mu)^2}{2\\sigma^2}}.\n\\end{equation}\nThe first-order moments of this distribution represent the maximum likelihood prediction as well as uncertainty (both aleatoric and epistemic).\n\\begin{equation}\nE[\\mu] = \\int_{-\\infty}^{\\infty} \\mu p(\\mu) d\\mu= \\gamma\n\\end{equation}\n\\begin{equation}\nE[\\sigma^2] = \\int_{\\sigma^2=0}^{\\infty} \\sigma^2 p(\\sigma^2) d\\sigma^2\n\\end{equation}\n\\begin{equation}\n= \\int_{\\sigma^2=0}^{\\infty} \\sigma^2 \\Gamma^{-1}(\\sigma^2)(2\\sigma) d\\sigma\n\\end{equation}\n\\begin{equation}\n= \\frac{\\beta}{\\alpha-1}, \\quad \\alpha > 1\n\\end{equation}\n\\begin{equation}\nVar[\\mu] = \\int_{-\\infty}^{\\infty} \\mu^2 p(\\mu) d\\mu - (E[\\mu])^2\n\\end{equation}\n\\begin{equation}\n= \\int_{-\\infty}^{\\infty} \\mu^2  N(\u03bc|\u03b3, \u03c3^2\u03c5^{\u22121})d\u03bc - \u03b3^2\n\\end{equation}\n\\begin{equation}\n= \\gamma^2 + \\frac{\\sigma^2}{v} - \\gamma^2\n\\end{equation}\n\\begin{equation}\n=\\frac{\\beta}{v(\\alpha-1)}, \\quad \\alpha > 1.\n\\end{equation}\nIn summary,\n\\begin{equation}\n\\text{E[}\\mu\\text{]} = \\gamma,\\qquad\\text{E[}\\sigma^2\\text{]} = \\frac{\\beta}{\\alpha-1},\\qquad Var[\u03bc] = \\frac{\\beta}{v(\\alpha \u2212 1)}\\nonumber\\\\\nprediction \\qquad \\qquad\\text{aleatoric } \\qquad \\text{ epistenic}\n\\end{equation}\nA.2 Model evidence & Type II Maximum Likelihood Loss\nIn this subsection, we derive the posterior predictive or model evidence (i.e. Eq. 30) of a NIG distribution. Marginalizing out \\(\\mu\\) and \\(\\sigma\\) gives our desired result:"}, {"title": "B.2 Implementations for normalizations", "content": "Normalization: We have tried two normalization operations, i.e.min-max normalization and using activation function to normalize.\n*   Min-Max normalization: Assume we have evaluated an increasing sequence of errors, that is:\n\\begin{equation}\n\\{\\Delta_1, \\Delta_2,..., \\Delta_n\\}\n\\end{equation}\nwhere n represents batch size. Min-Max Normalization maps \\(\\Delta_i\\) to \\(A_i\\) by:\n\\begin{equation}\n\\Delta_i = \\frac{\\Delta_i}{\\Delta_n - \\Delta_1}\n\\end{equation}\nWe recommend using this normalization method in training and batch testing.\n*   Normalization using activation functions: Use activation functions tanh(\\(\\cdot\\)) so that we can map A to \u2206\u00bf, which is between 0 and 1:\n\\begin{equation}\n\\Delta_i = tanh(Ai)\n\\end{equation}\nAnd I is normalized in the same way to \u03a6. We recommend this normalization for single-point or small-batch testing.\nHistogram equalization: Although we normalize the uncertainty, we still find that the distribution of uncertainty is extremely biased to 0. We consider that this is still due to the overconfidence effect that NLL brings to the model. In order to obtain a more expressive uncertainty estimation in the inference process, sometimes we use histogram equalization to post-process the normalized uncertainty."}, {"title": "D.1 Quantitative analysis of Geom-regularizer", "content": "To measure the degree to which the uncertainty predicted under different regularization settings obeys the prior of \"the larger the error, the greater the uncertainty\", we define measure: Error-Uncertainty Consistency Measure (EUCM). EUCM is calculated as:\n\\begin{equation}\nEUCM = ||\\Delta + U||_2,\n\\end{equation}\nwhere U represents uncertainty. Moreover, we also compute the information entropy of different uncertainty distributions, which is used to evaluate the expressive ability of evidential predictor. We tested EUCM and entropy on the validation set of QVHighlights [27] in Table 6 and 7 under different settings. We can notice that the entropy of predictions with the vanilla regularizer is greater than that with the Geom-regularizer. However, as demonstrated in section 4.3 predictions made with the vanilla regularizer are prone to be misleading. Consequently, even though it exhibits higher information entropy, this entropy may encompass substantial \"wrong information\". In contrast, the Geom-regularizer not only achieves higher information entropy but also results in a lower EUCM score, indicating its superior performance."}, {"title": "E Detailed Workflow of the RFF Block", "content": "The RFF block processes inputs from the video and text branches, with initial features denoted as \\(V^{(1)}\\) and \\(Q^{(1)}\\) respectively. The cross-attention module, which shares parameters, alternates the roles of the video and text branches as queries and keys/values:\n\\begin{equation}\nCA_{vq}^{(1)} = Softmax\\(\\frac{V^{(1)}Q^{(1)T}}{\\sqrt{d_k}}\\)Q^{(1)}\n\\end{equation}\n\\begin{equation}\nCA_{qv}^{(1)} = Softmax\\(\\frac{Q^{(1)}V^{(1)T}}{\\sqrt{d_k}}\\)V^{(1)}\n\\end{equation}"}, {"title": "F Details of the VTG head", "content": "F.1 Moment retrieval head\nThe design of this head is similar to the foreground head, except it features a last layer with two output channels for the left and right offsets. Given \\(\\hat{V}_k \\in \\mathbb{R}^{L_v\\times D}\\), this head generates a series of offsets \\(\\{m_i\\}_{i=1}\\) for each unit. We then define the predicted boundary \\(m\\) and the corresponding interval \\(d_i\\) (i.e., \\(d_i = m_r - m_l\\)). For training objectives, we use a combination of smooth L1 loss and generalized IoU loss to optimize the model's performance.\n\\begin{equation}\n\\mathcal{L}_b = \\frac{1}{L_v} \\sum_{i=1}^{L_v} [\\mathbb{1}_{f_i=1} (\\lambda_{L1} \\mathcal{L}_{SmoothL1}(d_i, \\hat{d_i}) + \\lambda_{iou} \\mathcal{L}_{IoU}(m_i, \\hat{m_i}))].\n\\end{equation}\nNotably, this regression objective is only devised for foreground clips i.e., \\(f_i = 1\\).\nF.2 Video summarization head\nFrom the frozen video encoder, the output \\(\\hat{V}_k \\in \\mathbb{R}^{L_v\\times D}\\) passes through a series of three 1 \u00d7 3 convolutional layers, each layer having D filters and equipped with ReLU activation functions. Following these layers, sigmoid activations are used to generate the prediction \\(f_i\\) for each unit. Focal loss serves as the training objective, with \\(\\gamma = 2.0\\) and \\(\\alpha = 0.9\\).\n\\begin{equation}\n\\mathcal{L}_{\u00a3} = -\\sum_{i=1} a (1 - f_i)^{\\gamma} \\log{(f_i)}\n\\end{equation}\nF.3 Highlight detection head\nGiven that saliency is defined as the relevance between visual context and a text query, it is appropriate to assess this relationship through a similarity measure between video and text modalities. Let the video tokens be denoted as \\(\\{v_i\\}_{i=1} \\in \\mathbb{R}^{L \\times D}\\) and the sentence representation as \\(S \\in \\mathbb{R}^{1\\times D}\\). We then calculate the predicted saliency score \\(\\hat{s_i}\\) for each video token \\(v_i\\) in relation to the text query Q, using their cosine similarities.\n\\begin{equation}\n\\hat{s_i} = cos(v_i, S) := \\frac{v_i^T S}{\\|v_i\\|_2 \\|S\\|_2}\n\\end{equation}\nwhere \\(|| \\cdot ||_2\\) represents the L2-norm of a vector.\nFor each video V, we randomly sample a foreground clip \\(v_p\\) with \\(f_p = 1\\) and \\(s_p > 0\\) as a positive sample; we treat other clips in the same video \\(v_j\\) with saliency \\(s_j\\) less than \\(s_p\\) as negative samples, i.e., \\(\\Omega = \\{ j|s_j < s_p, 1 \\leq j \\leq L_v \\}\\), and perform intra-video contrastive learning:\n\\begin{equation}\n\\mathcal{L}_{intra} = -log\\frac{exp{(\\hat{s_p}/\\tau)}}{\\exp{(\\hat{s_p}/\\tau)} + \\sum_{j \\in \\Omega} \\exp{(\\hat{s_j}/\\tau)}}\n\\end{equation}\nwhere \\(\\tau\\) is a temperature parameter and set as 0.07. And we further propose query-driven clip-by-clip contrastive learning where clips within the target moment are treated as positive samples and clips outside as negative samples. Specifically, samples are selected based on the salience scores, with positive samples ranked in descending order and negative samples in ascending order. The top K samples from each are chosen for similarity computation. Given two sets of samples, Pos (positive) and Neg (negative), each containing K elements, the similarity is computed using the dot product, resulting in a similarity matrix S. The similarity matrix S is derived from the dot product between vectors \\(v_i\\) from the positive set Pos and \\(v_j\\) from the negative set Neg. Each vector represents a moment in the video, with \\(v_i \\in Pos\\) and \\(v_j \\in Neg\\). The similarity \\(S_{ij}\\) between any two moments is computed as follows:\n\\begin{equation}\nS_{ij} = (v_{i})^T(v_{j}),\n\\end{equation}\nThe loss function is defined as the negative mean of the trace of S, formally given by:\n\\begin{equation}\n\\mathcal{L}_{intra}^{clip} = - \\frac{1}{N} \\sum_{i=1}^{N} S_{ii},\\nonumber\n\\end{equation}\nwhere N is the clip number of the training set. In datasets other than QVHighlight [27], where ground truth salience scores are not provided, the foreground flag f is used to dichotomize the samples into positive and negative sets. K samples are then randomly selected from each set for computing the similarity and loss in terms of Eq. 57 and 58.\nBesides, we regard sentences from other samples within batches \\(k \\in B\\) as negative samples, and develop the inter-video contrastive learning for cross-sample supervision:\n\\begin{equation}\n\\mathcal{L}_{inter} = - log \\frac{exp{(\\hat{s_p}/\\tau)}}{\\sum_{k \\in B} exp{(\\hat{s_k}/\\tau)}},\n\\end{equation}"}]}