{"title": "Continuous Control with Coarse-to-fine Reinforcement Learning", "authors": ["Younggyo Seo", "Jafar Uru\u00e7", "Stephen James"], "abstract": "Despite recent advances in improving the sample-efficiency of reinforcement learning (RL) algorithms, designing an RL algorithm that can be practically deployed in real-world environments remains a challenge. In this paper, we present Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL agents to zoom-into a continuous action space in a coarse-to-fine manner, enabling the use of stable, sample-efficient value-based RL algorithms for fine-grained continuous control tasks. Our key idea is to train agents that output actions by iterating the procedure of (i) discretizing the continuous action space into multiple intervals and (ii) selecting the interval with the highest Q-value to further discretize at the next level. We then introduce a concrete, value-based algorithm within the CRL frame-work called Coarse-to-fine Q-Network (CQN). Our experiments demonstrate that CQN significantly outperforms RL and behavior cloning baselines on 20 sparsely-rewarded RLBench manipulation tasks with a modest number of environment interactions and expert demonstrations. We also show that CQN robustly learns to solve real-world manipulation tasks within a few minutes of online training.", "sections": [{"title": "1 Introduction", "content": "Recent reinforcement learning (RL) algorithms have made significant advances in learning end-to-end continuous control policies from online experiences [4, 5, 6, 7, 8, 9]. However, these algorithms often require a large number of online samples for learning robotic skills [6, 9], making it impractical for real-world environments where practitioners need to deal with resetting procedures and hardware failures. Therefore, recent successful approaches in learning visuomotor policies for real-world tasks have mostly been methods that learn from static offline datasets, such as offline RL [10] or behavior"}, {"title": "2 Related Work", "content": "Actor-critic RL algorithms for continuous control Most prior applications of RL to continuous control have been based on actor-critic algorithms [2, 4, 5, 7, 15, 16, 29, 30, 31, 32, 33, 34] that introduce a separate, parameterized actor network as a policy [14]. This is because they allow for addressing one of the main challenges in applying Q-learning to continuous domains, i.e., finding continuous actions that maximize Q-values. However, in continuous control domains, actor-critic algorithms are known to be brittle and often suffer from instabilities due to the complex interactions between actor and critic networks [17, 18], despite recent efforts to stabilize them [7, 15, 16]. \u03a4\u03bf address this limitation, several approaches proposed to discretize the continuous action space and learn discrete policies for continuous control. For instance, Tang and Agrawal [35] learned a policy in a factorized action space and Seyde et al. [36] learned a bang-bang controller with actor-critic RL algorithms. This paper introduces a framework that enables the use of both actor-critic and value-based RL algorithms for learning discrete policies that can solve fine-grained control tasks.\nValue-based RL algorithms for continuous control Despite their simple critic-only architecture, value-based RL algorithms have achieved remarkable successes [19, 20, 21, 22]. However, because they require a discrete action space, there have been recent efforts to enable their use for continuous control by applying discretization to a continuous action space [10, 23, 26, 24, 25, 37] or by learning high-level discrete actions from offline data [38, 39]. For instance, some works have proposed training an autoregressive critic by treating each action dimension as a separate action to avoid the curse of dimensionality from action discretization [10, 37]. Our work is orthogonal to this, as our coarse-to-fine approach can be combined with this idea. On the other hand, several works have demonstrated that training factorized critics for each action dimension can achieve competitive performance to actor-critic algorithms [24, 25]. However, this single-level discretization may not be scalable to domains requiring high-precision actions, as such domains typically necessitate fine-grained discretization [10]. To address this limitation, Seyde et al. [26] proposed gradually enlarging action spaces throughout training, but this introduces a challenge of constrained optimization. In contrast, our CRL framework enables us to learn discrete policies for continuous control in a stable and simple manner.\nNotably, the closest work to ours is C2F-ARM [40] that trains value-based RL agents to zoom-into a voxelized 3D robot workspace by predicting the voxel to further discretize. C2F-ARM is a special case of our CRL framework, where the agent operates as a hierarchical, next-best pose agent [34]; it splits the robot manipulation problem into high-level next-best-pose control and low-level control (usually a motion planning) problems. CQN on the other hand, is more general and can be used for any action mode, including joint control. We provide additional discussion in Appendix F."}, {"title": "3 Method", "content": "We present Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL agents to zoom-into a continuous action space in a coarse-to-fine manner (see Section 3.1). Within this framework, we introduce Coarse-to-fine Q-Network (CQN), a value-based RL algorithm for continuous control (see Section 3.2) and describe various design choices for improving CQN in visual robotic manipulation tasks (see Section 3.3). We provide the overview and pseudocode in Figure 2 and Appendix B."}, {"title": "3.1 Framework: Coarse-to-fine Reinforcement Learning", "content": "To enable the use of value-based RL algorithms for learning discrete policies in fine-grained con-tinuous control domains, we propose to formulate the continuous control problem as a multi-level discrete control problem via coarse-to-fine action discretization. Specifically, given a number of levels $L$ and a number of bins $B$, we apply discretization to the continuous action space $L$ times (see Figure 3), in contrast to prior approaches that discretize action space into multiple intervals in a single-level [25, 41]. We then train RL agents to zoom-into the continuous action space by repeating the procedure of (i) discretizing the continuous action space at the current level into $B$ intervals and (ii) selecting the interval with the highest Q-value to further discretize at the next level (see Figure 2a)."}, {"title": "3.2 Algorithm: Coarse-to-fine Q-Network", "content": "Problem setup We formulate a vision-based continuous control problem as a partially observable Markov decision process [42, 43], where, at each time step $t$, an agent encounters an observation $o_t$, selects an action $a_t$, receives a reward $r_{t+1}$, and encounters a new observation $o_{t+1}$ from an environment. Our goal is to learn a policy that maximizes the expected sum of rewards through RL in a sample-efficient manner, i.e., by using as few online samples as possible.\nInputs and encoder We consider an observation of consisting of pixel observations $(o^1, ..., o^v)$ captured from viewpoints $(v_1, ..., v_v)$ and low-dimensional proprioceptive states $o^{prop}$. We then use a lightweight 4-layer convolutional neural network (CNN) encoder $f_{enc}$ to encode pixels $o^i$ into visual features $h^i$, i.e., $h^i = f_{enc}(o^i)$. To fuse information from view-wise features, we concatenate features from all viewpoints and project them into low-dimensional features. Then we concatenate fused features with proprioceptive states $o^{prop}$ to construct features $h_t$.\nCoarse-to-fine critic architecture Let $a_n^l$ be an action at level $l$ and action dimension $n$ (e.g., delta angle for $n$-th joint of a robotic arm) and $\\bar{a}^{l-1} = (a^{l-1}_1, ..., a^{l-1}_N)$ be an action at level $l$ where $a_n^l$ is defined as a zero action vector. By following the design of Seyde et al. [25] that introduce factorized Q-networks for different action dimensions, we define our coarse-to-fine critic to consist of individual Q-networks at level $l$ and action dimension $n$ as below (see Figure 2b for an illustration):\n$Q_\\theta^{l,n}(h_t, a_n^l, \\bar{a}^{l-1}) \\text{ for } n \\in \\{1, ..., N\\} \\text{ and } l \\in \\{1, ..., L\\}$  (1)\nWe note that our design mainly differs from prior work with a single-level critic [24, 25] in that our Q-network takes $\\bar{a}^{l-1}$, i.e., actions from all dimensions at previous level, to enable each Q-network to be aware of other networks' decisions at the previous level. We also design our critic to share most of parameters for all levels and dimensions by sharing linear layers except the last linear layer [41] and making Q-networks take one-hot level index as inputs."}, {"title": "Inference procedure", "content": "We describe our coarse-to-fine inference procedure for selecting actions at time step $t$ (see Figure 2a and Appendix B for the illustration and pseudocode of our inference procedure). We first introduce constants $a_{n, low}^l$ and $a_{n, high}^l$ that are initialized with \u20131 and 1 for each action dimension $n$. For all action dimensions $n$, we repeat the following steps for $l \\in \\{1, ..., L\\}$:\nStep 1 (Discretization): We discretize an interval $[a_{n, low}^l, a_{n, high}^l]$ into $B$ uniform intervals, each of which becomes the action space for Q-network $Q_\\theta^{l,n}$.\nStep 2 (Bin selection): We find $\\text{argmax}_{a'} Q_\\theta^{l,n}(h_t, a', \\bar{a}^{l-1})$ for each $n$, which corresponds to the interval with the largest Q-value. We then set $a_n^l$ to the centroid of the selected interval and concatenate actions from all dimensions into $a$.\nStep 3 (Zoom-in): We set $a_{n, low}^l$ and $a_{n, high}^l$ to the minimum and maximum value of the selected interval, zooming into the selected intervals within the action space.\nWe use the last level's action $a^L$ as the action at time step $t$. In practice, we parallelize the procedures across all the action dimensions $n$ for faster inference. We further describe a procedure for computing Q-values with input actions, along with its pseudocode, in Appendix B."}, {"title": "Q-learning objective", "content": "Q-learning objective for action dimension $n$ at level $l$ is defined as below:\n$\\mathcal{L}_{RL}^{l,n} = (Q_\\theta^{l,n}(h_t, a_n^l, \\bar{a}^{l-1}) - [r_{t+1} + \\gamma \\max_{a'} Q_{\\theta'}^{l,n}(h_{t+1}, a', \\pi^{l-1}(h_{t+1}))])^2$  (2)\nwhere $\\theta'$ are delayed critic parameters updated with Polyak averaging [44] and $\\pi^l$ is a policy that outputs the action $a^l$ at each level $l$ via the inference steps with our critic, i.e., $\\pi^l(h_t) = a$.\nImplementation and training details We use the 2-layer dueling network [45] and a distributional critic [46] with 51 atoms. By following Hafner et al. [47], we use layer normalization [48] with SiLU activation [49] for every linear and convolutional layers. We use AdamW optimizer [50] with weight decay of 0.1 by following Schwarzer et al. [51]. Following prior work that learn from offline data [52, 53], we sample minibatches of size 256 each from the online replay buffer and the demonstration replay buffer, resulting in a total batch size of 512. More details are available in Appendix C."}, {"title": "3.3 Optimizations for Visual Robotic Manipulation", "content": "We describe various design choices for improving CQN in visual robotic manipulation tasks.\nAuxiliary behavior cloning objective Following the idea of prior work [54, 55], we introduce an auxiliary behavior cloning (BC) objective that encourages agents to imitate expert actions. Specifically, given an expert action $\\tilde{a}_t$, we introduce an auxiliary margin loss [56] that encourages $Q(h_t, \\tilde{a})$ to be higher than Q-values of non-expert actions $Q(h_t, a)$ for all levels $l$ as below:\n$\\mathcal{L}_{BC}^{l,n} = \\max_{\\tilde{a}^\\prime} (Q_\\theta^{l,n}(h_t, a^\\prime, \\bar{a}^{l-1}) + f_{margin} (a^\\prime, a^\\prime_{exp})) - Q_\\theta^{l,n}(h_t, a^\\prime_{exp}, \\bar{a}^{l-1})$ (3)\nwhere $f_{margin}$ is a function that gives 0 when $a^\\prime = a^\\prime_{exp}$ and a margin value $m$ otherwise. This objective encourages Q-values for expert actions to be at least higher than other Q-values by $m$. We describe how we modify BC objective to align better with the distributional critic in Appendix A.\nRelabeling successful online trajectories as demonstrations Inspired by the idea of self-imitation learning [57] that encourages agents to reproduce their own good decisions, we label the successful trajectories from environment interaction as demonstrations. We find that this simple scheme can be helpful for RL training by widening the distribution of demonstrations throughout training.\nEnvironment interaction Similar to prior value-based RL algorithms [51, 58], we choose actions using the target Q-network to improve the stability throughout environment rollouts. Moreover, as we find that standard exploration techniques of injecting noises [4, 59, 60] make it difficult to solve fine-grained control tasks, we instead add a small Gaussian noise with standard deviation of 0.01."}, {"title": "4 Experiments", "content": "We design our experiments to investigate the following questions: (i) How does CQN compare to previous RL and BC baselines? (ii) Can CQN be sample-efficient enough to be practically used in real-world environments? (iii) How do various design factors of CQN affect the performance?"}, {"title": "4.1 RLBench Experiments", "content": "Setup For quantiative evaluation, we mainly consider a demo-driven RL setup where we aim to solve visual robotic manipulation tasks from RLBench [1] environment with access to a limited number of environment interactions and expert demonstrations. Unlike prior work that designed experiments to make RLBench tasks less challenging by using hand-designed rewards [55, 61] or heuristics that depend on motion planning, e.g., keypoint extraction [34, 40], we consider a sparse-reward setup without the use of motion planner. Specifically, we label the reward of the last timestep in successful episodes as 1.0 and train RL agents to output the difference of joint angles at each time step by using deltaJointPosition mode in RLBench. We use RGB observations with 84 \u00d7 84 resolution captured from front, wrist, left-shoulder, and right-shoulder cameras. Proprioceptive states consist of 7-dimensional joint positions and a binary gripper state. Similar to Mnih et al. [19], we use a history of 8 observations as inputs. For all tasks, we use the same set of hyperparameters, e.g., 3 levels and 5 bins, without tuning them for each task. See Appendix C for more details.\nRL baselines Because CQN is a generic value-based RL algorithm compatible with other techniques for improving value-based RL [51, 58] or demo-driven RL [52, 53, 62, 63], we mainly focus on comparing CQN against representative baselines to which comparison can highlight the benefit of our framework. To this end, we first consider DrQ-v2 [2], a widely-used actor-critic RL algorithm, as our RL baseline. Moreover, for a fair comparison, we design our strong RL baseline: DrQ-v2+, a highly optimized variant of DrQ-v2 that incorporates a distributional critic and our recipes for manipulation tasks (see Section 3.3). We also note that all RL methods have an auxiliary BC objective."}, {"title": "4.2 Real-world Experiments", "content": "Setup We further demonstrate the effectiveness of CQN in real-world tasks that use a UR5 robot arm with 20 to 50 human-collected demonstrations (see Figure 5 for examples of real-world tasks). Unlike RLBench experiments that take one update step per every environment step, we take 50 or 100 update steps between episodes to avoid jerky motions during the environment interaction. All RL methods have an auxiliary BC objective and we report the running mean across 5 recent episodes. For ACT, we report the average success rate over 20 episodes to evaluate it with the same randomization range used in RL experiments. We use stack of 4 observations as inputs and 4 levels with 3 bins. Unless otherwise specified, we use the same hyperparameters as in RLBench experiments for all methods, which shows the robustness of CQN to hyperparameters. See Appendix D for more details.\nResults In Figure 6, we observe intriguing results where CQN can learn to solve complex real-world tasks within 10 minutes of online training, while a baseline without RL objective often fails to do so. In particular, we find that this baseline without RL objective nearly succeeds in solving the task but makes a mistake in states that require high-precision actions, which demonstrates the benefit of RL similar to the results in simulated RLBench environment (see Table 1c). Moreover, we observe that the training of DrQ-v2+ is unstable especially when it encounters unseen observations during training. In contrast, CQN robustly learns to solve the tasks and consistently outperforms DrQ-v2+ in all tasks. We provide full videos of real-world RL training for all tasks in our project website."}, {"title": "4.3 Analysis and Ablation Studies", "content": "We investigate the effect of hyperparameters and various design choices by running experiments on 4 tasks from RLBench. We provide more analysis and ablation studies in Appendix A."}, {"title": "Effect of levels and bins", "content": "In Table 1a and Table 1b, we investigate the effect of levels and bins within CQN. As shown in Table 1a, we find that single-level baseline performance peaks at 65 bins and decreases after it, which shows the limitation of single-level action discretization that struggles to scale up to tasks that require high-precision actions. Moreover, we find that 3-level CQN also struggles with more bins, as learning Q-networks with more actions can be difficult. In Table 1b, we find that 3 or 4 levels are sufficient and performance keeps decreasing with more levels. We hypothesize this is because learning signals from levels with too fine-grained actions may confuse the network with limited capacity because of sharing parameters for all the levels."}, {"title": "Effect of objectives and distributional critic", "content": "In Table 1c, we investigate the effect of RL and BC objectives, along with the effect of using distributional critic (i.e., C51) [46]. To summarize, we find that (i) RL objective is crucial as in real-world experiments (see Section 4.2), (ii) auxiliary BC objective is crucial as RL agents struggle to keep close to demonstration distribution without the BC loss, and (iii) distributional critic is important; severe value overestimation makes RL training unstable in the initial phase of RL training without the distributional critic."}, {"title": "Effect of exploration", "content": "We further investigate the effect of how our agents do exploration, i.e., which network to use for selecting actions and how to add noise to actions, in Table 1d. We find that using target Q-network for selecting actions outperforms using online Q-network. We hypothesize this is because (i) Polyak averaging [44] can improve the generalization [64] and (ii) online network changes throughout episode. We also find that using a small Gaussian noise with $\\mathcal{N}(0, 0.01)$ outperforms a variant with a strong noise because manipulation tasks require high-precision actions."}, {"title": "5 Discussion", "content": "We present CRL, a framework that enables the use of value-based RL algorithms in fine-grained continuous control domains, and CQN, a concrete value-based RL within this framework. Our key idea is to train RL agents to zoom-into a continuous action space in a coarse-to-fine manner. Extensive experiments demonstrate that CQN efficiently learns to solve a range of continuous control tasks.\nLimitations and future directions Overall, we are excited about the potential of our framework and there are many exciting future directions: supporting high update-to-data ratio [51, 58, 65], 3D representations [55, 66, 67, 68, 69, 70, 71, 72], tree-based search [20, 73], and bootstrapping RL from BC [62, 74] or offline RL [75, 76, 77], to name but a few. One particular limitation we are keen to address is that we still need quite a number of demonstrations. Reducing the number of demonstrations by incorporating pre-trained models [78, 79, 80] or augmentation techniques [81, 82, 83] would be an interesting future direction. We discuss more limitations and future directions in Appendix G."}, {"title": "Data-driven action scaling", "content": "For all experiments, we follow James and Davison [34] that compute the minimum and maximum actions from the demonstrations and scale actions using these values as the action space bounds. We investigate the effect of this scaling scheme in Table 2b, where we find that this makes it easy to learn to solve manipulation tasks."}, {"title": "Using a history of observations", "content": "Similar to prior researches that show the effectiveness of using a history of observations when training IL agents for robotic manipulation [11, 86], we find that using stacked observations [19] is also crucial when training RL agents for manipulation in Table 2c."}, {"title": "B Pseudocode", "content": "In this section, we first provide an inference procedure for computing Q-values. We then provide the pseudocode of inference procedures and CQN training in Algorithm 1 and Algorithm 2."}, {"title": "Inference procedure for computing Q-values", "content": "We describe the procedure for computing Q-values when actions at are given as inputs, which is similar to action selection procedure in Section 3.2. We first introduce constants $a_{n, low}^l$ and $a_{n, high}^l$ that are initialized with \u20131 and 1 for each action dimension $n$. For all action dimensions $n$, we repeat the following steps for $l \\in \\{1, ..., L\\}$:\nStep 1 (Discretization): We discretize an interval $[a_{n, low}^l, a_{n, high}^l]$ into $B$ uniform intervals, each of which becomes the action space for Q-network $Q_\\theta^{l,n}$.\nStep 2 (Bin selection): We find the interval that contains given input actions at and compute Q-value $Q_\\theta^{l,n}(h_t, a', \\bar{a}^{l-1})$ for the selected interval.\nStep 3 (Zoom-in): We set $a_{n, low}^l$ and $a_{n, high}^l$ to the minimum and maximum value of the selected interval, zooming into the selected intervals within the action space.\nWe then obtain the set of Q-values $\\{Q_\\theta^{l,n}(h_t, a_n, \\bar{a}^{l-1})\\}$."}, {"title": "G Limitations and Future Directions", "content": "Data augmentation In this work, we applied very simple data augmentations: RandomShift [2] that shifts pixels by 4 pixels, brightness augmentation, and contrast augmentation. However, as shown in recent works that investigated the effectiveness of augmentations for learning visuomotor policies [115, 116], applying more strong augmentations can also be helpful for improving the generalization capability of RL agents. Moreover, applying augmentation to images with generative models [117] can further enhance the generalization capability of RL agents to unseen environments. Incorporating such strong augmentations potentially with techniques for stabilizing RL training [82, 83] can be an interesting future direction.\nAdvanced vision encoder and representation learning CQN uses a simple, light-weight visual encoder, i.e., 4-layer CNN encoder, and also a na\u00efve way of fusing view-wise features that concate-nates image features. While this has an advantage of having a simple architecture and thus a very fast inference speed, incorporating an advanced vision encoder architectures such as ResNet [118] or Vision Transformer [119] may improve the performance in tasks that require fine-grained control. Moreover, given the recent improvements in learning multi-view representations [55, 66, 120] or generating 3D models [121, 122, 123, 124, 125], incorporating such improvements and 3D prior into encoder design can be helpful for improving the sample-efficiency of CQN, especially in tasks that require multi-view information as already shown in recent several behavior cloning approaches [67, 68, 69, 70, 71, 72]. Learning such representations by pre-training the visual encoder on large multi-view datasets [126, 127, 128] would also be an interesting direction.\nHandling a history of observations For taking a history of observations as inputs, we follow a very simple scheme of Mnih et al. [19] that stacks observations. However, this might not be scalable to long-horizon tasks where such a stacking of 4 or 8 observations may not provide a sufficient information required for solving the target tasks. In that sense, designing a model-based RL algorithm within our CRL framework based on recent works [61, 47, 129] or incorporating architectures that can handle a sequence of observations, such as RNNs [130, 131], Transformers [132], and state-space models [133], can be a natural future direction to our work.\nTraining with high update-to-data ratio Recent work have demonstrated the effectiveness of using high update-to-data (UTD) ratio (i.e., number of update steps per every environment step) for improving the sample-efficiency of RL algorithms [51, 58, 65]. In this work, we used 1 UTD ratio in RLBench experiments for faster experimentation as using higher UTD ratio slows down training. This slow-down in training speed can be an issue in real-world experiments where practitioners often need to be physically around the robot and monitor the progress of training for labelling the episode or safety reason. Thus, investigating the performance of CQN with high UTD by utilizing a design or software that supports asynchronous training [33, 106] would be an interesting future direction we are keen to explore. Furthermore, we note that recent approaches typically depend on resetting technique for supporting high-UTD but such resetting can be problematic in that it may lead to dangerous behaviors with real robots. Investigating how to support high UTD without such a resetting technique can be also an interesting future direction especially in the context of real-world RL.\nSearch-based action selection CQN uses a simple inference scheme that greedily selects an interval with the highest Q-value from the first level. However, there is a room for improvement in action selection by incorporating search algorithms that exploit the discrete action space [73].\nBootstrapping from offline data with BC or offline RL While our experiments show that CQN can quickly match and outperform the performance of BC baseline such as ACT [3], there is a room for improvement by investigating how to bootstrap RL training from offline RL [75, 76, 77] or BC policies [62, 74]. For instance, pre-training CQN agents with offline RL techniques on robot learning dataset [134, 135] or utilizing a separate BC policy pre-trained on demonstrations would be interesting and straightforward future directions."}, {"title": "Human-in-the-loop learning", "content": "One critical limitation of applying RL to real-world applications is that practitioners need to be physically around the robot in most cases; otherwise it involves a huge engineering to automate resetting procedures and designing a success detection system. However, this can lead to another interesting and promising future direction of leveraging human guidance in the training pipeline in the form of human-in-the-loop learning. For instance, incorporating a DAgger-like system that provides human-guided trajectory for RL agents [136], investigating a way to utilize human-labelled reward but address the subjectivity of such human labels throughout training via preference learning [137, 138] can be interesting future directions."}, {"title": "H Things that did not work", "content": "We describe the methods and techniques that did not work in our RLBench experiments when we use default hyperparameters and setups from the original work.\nSmall batch RL and prioritized sampling We tried using small batch size [139] but find that large batch size performs better in RLBench experiments. This aligns with the original observation of Obando Ceron et al. [139] where large batch size performs better with fewer number of environment interactions. We also tried using prioritized experience replay [140] but we find that it slows down training without a significant performance gain.\nExploration with NoisyNet Instead of manually setting a small Gaussian noise $\\mathcal{N}(0, 0.01)$, we tried using NoisyNet [59] with varying magnitudes of initial noise scale. But we find that it perturbs action too much regardless of noise scales, making it not possible to solve the manipulation tasks.\nLearning critic with classification loss We tried the idea of Farebrother et al. [141] that proposed to train value functions with categorical cross-entropy loss. But we find that using a distributional critic [46] works better when value bounds are set to -1 and 1 for sparsely-rewarded tasks.\nDifferent distributional RL algorithms We tried distributional RL algorithms other than C51, i.e., QR-DQN [142] and IQN [143], but find no difference between them in our experiments.\nL2 feature normalization We tried normalizing every feature vectors to have a unit norm following Hussing et al. [144] but this significantly degraded the performance in our experiments.\nRL with action chunking Motivated by recent BC approaches that demonstrated the effectiveness of predicting a sequence of actions (i.e., action chunk) [3, 11], we also tried incorporating action chunking into RL. Specifically, we expand the action space by treating actions from multiple timesteps as a single action. But we find that this na\u00efve approach does not work well; investigating how to incorporate such an idea into RL would be an interesting future direction."}]}