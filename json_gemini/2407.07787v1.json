{"title": "Continuous Control with Coarse-to-fine Reinforcement Learning", "authors": ["Younggyo Seo", "Jafar Uru\u00e7", "Stephen James"], "abstract": "Despite recent advances in improving the sample-efficiency of reinforcement learning (RL) algorithms, designing an RL algorithm that can be practically deployed in real-world environments remains a challenge. In this paper, we present Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL agents to zoom-into a continuous action space in a coarse-to-fine manner, enabling the use of stable, sample-efficient value-based RL algorithms for fine-grained continuous control tasks. Our key idea is to train agents that output actions by iterating the procedure of (i) discretizing the continuous action space into multiple intervals and (ii) selecting the interval with the highest Q-value to further discretize at the next level. We then introduce a concrete, value-based algorithm within the CRL framework called Coarse-to-fine Q-Network (CQN). Our experiments demonstrate that CQN significantly outperforms RL and behavior cloning baselines on 20 sparsely-rewarded RLBench manipulation tasks with a modest number of environment interactions and expert demonstrations. We also show that CQN robustly learns to solve real-world manipulation tasks within a few minutes of online training.", "sections": [{"title": "1 Introduction", "content": "Recent reinforcement learning (RL) algorithms have made significant advances in learning end-to-end continuous control policies from online experiences [4, 5, 6, 7, 8, 9]. However, these algorithms often require a large number of online samples for learning robotic skills [6, 9], making it impractical for real-world environments where practitioners need to deal with resetting procedures and hardware failures. Therefore, recent successful approaches in learning visuomotor policies for real-world tasks have mostly been methods that learn from static offline datasets, such as offline RL [10] or behavior"}, {"title": "2 Related Work", "content": "Actor-critic RL algorithms for continuous control Most prior applications of RL to continuous control have been based on actor-critic algorithms [2, 4, 5, 7, 15, 16, 29, 30, 31, 32, 33, 34] that introduce a separate, parameterized actor network as a policy [14]. This is because they allow for addressing one of the main challenges in applying Q-learning to continuous domains, i.e., finding continuous actions that maximize Q-values. However, in continuous control domains, actor-critic algorithms are known to be brittle and often suffer from instabilities due to the complex interactions between actor and critic networks [17, 18], despite recent efforts to stabilize them [7, 15, 16]. \u03a4\u03bf address this limitation, several approaches proposed to discretize the continuous action space and learn discrete policies for continuous control. For instance, Tang and Agrawal [35] learned a policy in a factorized action space and Seyde et al. [36] learned a bang-bang controller with actor-critic RL algorithms. This paper introduces a framework that enables the use of both actor-critic and value-based RL algorithms for learning discrete policies that can solve fine-grained control tasks.\nValue-based RL algorithms for continuous control Despite their simple critic-only architecture, value-based RL algorithms have achieved remarkable successes [19, 20, 21, 22]. However, because they require a discrete action space, there have been recent efforts to enable their use for continuous control by applying discretization to a continuous action space [10, 23, 26, 24, 25, 37] or by learning high-level discrete actions from offline data [38, 39]. For instance, some works have proposed training an autoregressive critic by treating each action dimension as a separate action to avoid the curse of dimensionality from action discretization [10, 37]. Our work is orthogonal to this, as our coarse-to-fine approach can be combined with this idea. On the other hand, several works have demonstrated that training factorized critics for each action dimension can achieve competitive performance to actor-critic algorithms [24, 25]. However, this single-level discretization may not be scalable to domains requiring high-precision actions, as such domains typically necessitate fine-grained discretization [10]. To address this limitation, Seyde et al. [26] proposed gradually enlarging action spaces throughout training, but this introduces a challenge of constrained optimization. In contrast, our CRL framework enables us to learn discrete policies for continuous control in a stable and simple manner.\nNotably, the closest work to ours is C2F-ARM [40] that trains value-based RL agents to zoom-into a voxelized 3D robot workspace by predicting the voxel to further discretize. C2F-ARM is a special case of our CRL framework, where the agent operates as a hierarchical, next-best pose agent [34]; it splits the robot manipulation problem into high-level next-best-pose control and low-level control (usually a motion planning) problems. CQN on the other hand, is more general and can be used for any action mode, including joint control. We provide additional discussion in Appendix F."}, {"title": "3 Method", "content": "We present Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL agents to zoom-into a continuous action space in a coarse-to-fine manner (see Section 3.1). Within this framework, we introduce Coarse-to-fine Q-Network (CQN), a value-based RL algorithm for continuous control (see Section 3.2) and describe various design choices for improving CQN in visual robotic manipulation tasks (see Section 3.3). We provide the overview and pseudocode in Figure 2 and Appendix B."}, {"title": "3.1 Framework: Coarse-to-fine Reinforcement Learning", "content": "To enable the use of value-based RL algorithms for learning discrete policies in fine-grained continuous control domains, we propose to formulate the continuous control problem as a multi-level discrete control problem via coarse-to-fine action discretization. Specifically, given a number of levels L and a number of bins B, we apply discretization to the continuous action space L times (see Figure 3), in contrast to prior approaches that discretize action space into multiple intervals in a single-level [25, 41]. We then train RL agents to zoom-into the continuous action space by repeating the procedure of (i) discretizing the continuous action space at the current level into B intervals and (ii) selecting the interval with the highest Q-value to further discretize at the next level (see Figure 2a)."}, {"title": "3.2 Algorithm: Coarse-to-fine Q-Network", "content": "Problem setup We formulate a vision-based continuous control problem as a partially observable Markov decision process [42, 43], where, at each time step t, an agent encounters an observation $o_t$, selects an action $a_t$, receives a reward $r_{t+1}$, and encounters a new observation $o_{t+1}$ from an environment. Our goal is to learn a policy that maximizes the expected sum of rewards through RL in a sample-efficient manner, i.e., by using as few online samples as possible.\nInputs and encoder We consider an observation of consisting of pixel observations $(o^1, ..., o^v)$ captured from viewpoints $(v_1, ..., v_v)$ and low-dimensional proprioceptive states $o^{low}$. We then use a lightweight 4-layer convolutional neural network (CNN) encoder $f_{enc}$ to encode pixels $o^i$ into visual features $h^i$, i.e., $h^i = f_{enc}(o^i)$. To fuse information from view-wise features, we concatenate features from all viewpoints and project them into low-dimensional features. Then we concatenate fused features with proprioceptive states $o^{low}$ to construct features $h_t$.\nCoarse-to-fine critic architecture Let $a_n^l$ be an action at level l and action dimension n (e.g., delta angle for n-th joint of a robotic arm) and $\\bar{a}^{l-1} = (a_1^{l-1}, ..., a_N^{l-1})$ be an action at level l where $a_n^l$ is defined as a zero action vector. By following the design of Seyde et al. [25] that introduce factorized Q-networks for different action dimensions, we define our coarse-to-fine critic to consist of individual Q-networks at level l and action dimension n as below (see Figure 2b for an illustration):\n$Q_{\\theta}^l(h_t, a_n^l, \\bar{a}^{l-1})\\text{ for }n \\in \\{1, ..., N\\} \\text{ and }l \\in \\{1, ..., L\\}$  (1)\nWe note that our design mainly differs from prior work with a single-level critic [24, 25] in that our Q-network takes $\\bar{a}^{l-1}$, i.e., actions from all dimensions at previous level, to enable each Q-network to be aware of other networks' decisions at the previous level. We also design our critic to share most of parameters for all levels and dimensions by sharing linear layers except the last linear layer [41] and making Q-networks take one-hot level index as inputs2."}, {"title": "Inference procedure", "content": "We describe our coarse-to-fine inference procedure for selecting actions at time step t (see Figure 2a and Appendix B for the illustration and pseudocode of our inference procedure). We first introduce constants $a_{n, low}^l$ and $a_{n, high}^l$ that are initialized with \u20131 and 1 for each action dimension n. For all action dimensions n, we repeat the following steps for $l \\in \\{1, ..., L\\}$:\n\u2022 Step 1 (Discretization): We discretize an interval $[a_{n, low}^l, a_{n, high}^l]$ into B uniform intervals, each of which becomes the action space for Q-network $Q_{\\theta}^l$.\n\u2022 Step 2 (Bin selection): We find $argmax_{a_n'} Q_{\\theta}^l(h_t, a_n', \\bar{a}^{l-1})$ for each n, which corresponds to the interval with the largest Q-value. We then set $a_n^l$ to the centroid of the selected interval and concatenate actions from all dimensions into a.\n\u2022 Step 3 (Zoom-in): We set $a_{n, low}^l$ and $a_{n, high}^l$ to the minimum and maximum value of the selected interval, zooming into the selected intervals within the action space.\nWe use the last level's action $a_n^L$ as the action at time step t. In practice, we parallelize the procedures across all the action dimensions n for faster inference. We further describe a procedure for computing Q-values with input actions, along with its pseudocode, in Appendix B."}, {"title": "Q-learning objective", "content": "Q-learning objective for action dimension n at level l is defined as below:\n$\\mathcal{L}_{CRL}^{\\theta_n^l} = (Q_{\\theta_n^l}(h_t, a_n^l, \\bar{a}^{l-1}) - (r_{t+1} + \\gamma max_{a'} Q_{\\theta_n^l}(h_{t+1}, a', \\pi^{l-1}(h_{t+1}))))^2$ (2)\nwhere $\\bar{\\theta}$ are delayed critic parameters updated with Polyak averaging [44] and $\u03c0^l$ is a policy that outputs the action $a^l$ at each level l via the inference steps with our critic, i.e., $\u03c0^l(h_t) = a^l$.\nImplementation and training details We use the 2-layer dueling network [45] and a distributional critic [46] with 51 atoms. By following Hafner et al. [47], we use layer normalization [48] with SiLU activation [49] for every linear and convolutional layers. We use AdamW optimizer [50] with weight decay of 0.1 by following Schwarzer et al. [51]. Following prior work that learn from offline data [52, 53], we sample minibatches of size 256 each from the online replay buffer and the demonstration replay buffer, resulting in a total batch size of 512. More details are available in Appendix C."}, {"title": "3.3 Optimizations for Visual Robotic Manipulation", "content": "We describe various design choices for improving CQN in visual robotic manipulation tasks.\nAuxiliary behavior cloning objective Following the idea of prior work [54, 55], we introduce an auxiliary behavior cloning (BC) objective that encourages agents to imitate expert actions. Specifically, given an expert action $\u00e3_t$, we introduce an auxiliary margin loss [56] that encourages $Q(h_t, \u00e3_t)$ to be higher than Q-values of non-expert actions $Q(h_t, a_t)$ for all levels l as below:\n$\\mathcal{L}_{BC}^{\\theta_n^l} max_{a_n'} (Q_{\\theta_n^l}(h_t, a_n', \\bar{a}^{l-1}) + f_{margin}(a_n', \\~a_n^l)) - Q_{\\theta_n^l}(h_t, \\~a_n^l, \\bar{a}^{l-1})$ (3)\nwhere $f_{margin}$ is a function that gives 0 when $a' = \\~a_n^l$ and a margin value m otherwise. This objective encourages Q-values for expert actions to be at least higher than other Q-values by m. We describe how we modify BC objective to align better with the distributional critic in Appendix A.\nRelabeling successful online trajectories as demonstrations Inspired by the idea of self-imitation learning [57] that encourages agents to reproduce their own good decisions, we label the successful trajectories from environment interaction as demonstrations. We find that this simple scheme can be helpful for RL training by widening the distribution of demonstrations throughout training.\nEnvironment interaction Similar to prior value-based RL algorithms [51, 58], we choose actions using the target Q-network to improve the stability throughout environment rollouts. Moreover, as we find that standard exploration techniques of injecting noises [4, 59, 60] make it difficult to solve fine-grained control tasks, we instead add a small Gaussian noise with standard deviation of 0.01."}, {"title": "4 Experiments", "content": "We design our experiments to investigate the following questions: (i) How does CQN compare to previous RL and BC baselines? (ii) Can CQN be sample-efficient enough to be practically used in real-world environments? (iii) How do various design factors of CQN affect the performance?"}, {"title": "4.1 RLBench Experiments", "content": "Setup For quantiative evaluation, we mainly consider a demo-driven RL setup where we aim to solve visual robotic manipulation tasks from RLBench [1] environment with access to a limited number of environment interactions and expert demonstrations\u00b3. Unlike prior work that designed experiments to make RLBench tasks less challenging by using hand-designed rewards [55, 61] or heuristics that depend on motion planning, e.g., keypoint extraction [34, 40], we consider a sparse-reward setup without the use of motion planner. Specifically, we label the reward of the last timestep in successful episodes as 1.0 and train RL agents to output the difference of joint angles at each time step by using deltaJointPosition mode in RLBench. We use RGB observations with 84 \u00d7 84 resolution captured from front, wrist, left-shoulder, and right-shoulder cameras. Proprioceptive states consist of 7-dimensional joint positions and a binary gripper state. Similar to Mnih et al. [19], we use a history of 8 observations as inputs. For all tasks, we use the same set of hyperparameters, e.g., 3 levels and 5 bins, without tuning them for each task. See Appendix C for more details.\nRL baselines Because CQN is a generic value-based RL algorithm compatible with other techniques for improving value-based RL [51, 58] or demo-driven RL [52, 53, 62, 63], we mainly focus on comparing CQN against representative baselines to which comparison can highlight the benefit of our framework. To this end, we first consider DrQ-v2 [2], a widely-used actor-critic RL algorithm, as our RL baseline. Moreover, for a fair comparison, we design our strong RL baseline: DrQ-v2+, a highly optimized variant of DrQ-v2 that incorporates a distributional critic and our recipes for manipulation tasks (see Section 3.3). We also note that all RL methods have an auxiliary BC objective."}, {"title": "4.2 Real-world Experiments", "content": "Setup We further demonstrate the effectiveness of CQN in real-world tasks that use a UR5 robot arm with 20 to 50 human-collected demonstrations (see Figure 5 for examples of real-world tasks). Unlike RLBench experiments that take one update step per every environment step, we take 50 or 100 update steps between episodes to avoid jerky motions during the environment interaction. All RL methods have an auxiliary BC objective and we report the running mean across 5 recent episodes. For ACT, we report the average success rate over 20 episodes to evaluate it with the same randomization range used in RL experiments. We use stack of 4 observations as inputs and 4 levels with 3 bins. Unless otherwise specified, we use the same hyperparameters as in RLBench experiments for all methods, which shows the robustness of CQN to hyperparameters. See Appendix D for more details.\nResults In Figure 6, we observe intriguing results where CQN can learn to solve complex real-world tasks within 10 minutes of online training, while a baseline without RL objective often fails to do so. In particular, we find that this baseline without RL objective nearly succeeds in solving the task but makes a mistake in states that require high-precision actions, which demonstrates the benefit of RL similar to the results in simulated RLBench environment (see Table 1c). Moreover, we observe that the training of DrQ-v2+ is unstable especially when it encounters unseen observations during training. In contrast, CQN robustly learns to solve the tasks and consistently outperforms DrQ-v2+ in all tasks. We provide full videos of real-world RL training for all tasks in our project website."}, {"title": "4.3 Analysis and Ablation Studies", "content": "We investigate the effect of hyperparameters and various design choices by running experiments on 4 tasks from RLBench. We provide more analysis and ablation studies in Appendix A."}, {"title": "5 Discussion", "content": "We present CRL, a framework that enables the use of value-based RL algorithms in fine-grained continuous control domains, and CQN, a concrete value-based RL within this framework. Our key idea is to train RL agents to zoom-into a continuous action space in a coarse-to-fine manner. Extensive experiments demonstrate that CQN efficiently learns to solve a range of continuous control tasks.\nLimitations and future directions Overall, we are excited about the potential of our framework and there are many exciting future directions: supporting high update-to-data ratio [51, 58, 65], 3D representations [55, 66, 67, 68, 69, 70, 71, 72], tree-based search [20, 73], and bootstrapping RL from BC [62, 74] or offline RL [75, 76, 77], to name but a few. One particular limitation we are keen to address is that we still need quite a number of demonstrations. Reducing the number of demonstrations by incorporating pre-trained models [78, 79, 80] or augmentation techniques [81, 82, 83] would be an interesting future direction. We discuss more limitations and future directions in Appendix G."}, {"title": "A Additional Analysis and Ablation Studies", "content": "Here, we provide additional analysis and ablation studies in Table 2. For results in this section and Section 4, we report aggregate results on 4 tasks: Turn Tap, Stack Wine, Open Drawer, Sweep To Dustpan, with 3 runs for each task.\nAuxiliary BC with distributional critic We find that our BC objective in Equation 3 is often not synergistic with distributional critic, because it leads to a shortcut of increasing Q-values (i.e., the mean of value distribution) by increasing the probability mass of atoms corresponding to supports with large values. To address this issue, given an expert action \u00e3t, we introduce a BC objective that encourages a distribution with the expert action Q(s, \u0101t) to be preferred over Q(s, at) instead of only using the mean of the distribution as a metric.\nOur idea is to utilize the concept of first-order stochastic dominance [84, 85]: when a random variable A is first-order stochastic dominant over a random variable B, for all outcome x, FA(x) \u2264 FB(x) holds, with strict inequality at some x. Intuitively, this means that A is preferred over B because the A is more likely to have a higher outcome x. Based on this, we design an auxiliary BC objective that encourages Q(s, \u0101t) to be stochastically dominant over Q(s, at), i.e., Lc51-Bc, which encourages RL agents to prefer the distribution induced by expert actions \u0101t to non-expert actions at. In Table 2a, we find that using LC51-BC achieves 77.5%, outperforming a variant that uses LBC that achieves 72.3%.\nCentralized critic Our coarse-to-fine critic architecture is based on the design of Seyde et al. [25] that train a factorized critic across action dimensions. However, we do not use the centralized critic training scheme as in the original paper, because (i) we find that using the average Q-value as an objective is not aligned well with the use of distributional critic and (ii) our design can already facilitate critics for different dimensions to share information as they are conditioned on actions from the previous level (see Figure 2b). Indeed, as shown in Table 2a, we find that using such an objective does not make a significant difference in performance; thus we do not use it for simplicity.\nRelabeling successful episodes as demonstrations We investigate the effectiveness of our relabel-ing scheme (see Section 3.3) in Table 2a, where we observe that performance largely drops without the scheme. Though this is effective in our RLBench experiments, we note that this idea depends on the characteristic of our manipulation tasks where successful episodes can be treated as optimal trajectories; investigating the effectiveness of it with noisy offline data or suboptimal demonstrations can be an interesting direction.\nAction mode We investigate how the choice of action mode between the absolute joint control or delta joint control affects the performance. We find that using the delta joint action mode significantly outperforms a baseline with the absolute action mode. We hypothesize this is because delta joint control's action space is narrower and makes it easy to learn fine-grained control policies. Moreover, we observe that using the absolute joint action mode in real-world environments often leads to dangerous behaviors and robot failures in practice because of large movements between each step."}, {"title": "B Pseudocode", "content": "In this section, we first provide an inference procedure for computing Q-values. We then provide the pseudocode of inference procedures and CQN training in Algorithm 1 and Algorithm 2.\nInference procedure for computing Q-values We describe the procedure for computing Q-values when actions at are given as inputs, which is similar to action selection procedure in Section 3.2. We first introduce constants $a_{n, low}^l$ and $a_{n, high}^l$ that are initialized with \u20131 and 1 for each action dimension n. For all action dimensions n, we repeat the following steps for $l \\in \\{1, ..., L\\}:\n\u2022 Step 1 (Discretization): We discretize an interval $[a_{n, low}^l, a_{n, high}^l]$ into B uniform intervals, each of which becomes the action space for Q-network $Q_{\\theta}^l$.\n\u2022 Step 2 (Bin selection): We find the interval that contains given input actions $a^l_n$ and compute Q-value $Q_{\\theta}^l(h_t, a_n', \\bar{a}^{l-1})$ for the selected interval.\n\u2022 Step 3 (Zoom-in): We set $a_{n, low}^l$ and $a_{n, high}^l$ to the minimum and maximum value of the selected interval, zooming into the selected intervals within the action space.\nWe then obtain the set of Q-values {$Q_{\\theta}^l(h_t, a_n', \\bar{a}^{l-1})$}."}, {"title": "C Experimental Details: Simulation", "content": "Simulation and tasks We use RLBench [1] simulator based on CoppeliaSim [87] and PyRep [88]. We run experiments in 20 sparsely-rewarded visual manipulation tasks with a 7-DoF Franka Panda robot arm and a parallel gripper (see Table 3 for the list of tasks).\nData collection For demonstration collection, we modify the maximum velocity of a Franka Panda robot arm by 2 times in PyRep, which shortens the length of demonstrations without largely degrading the quality of demonstrations. We use RLBench's dataset generator for collecting 100 demonstrations.\nComputing hardware For all RLBench experiments, we use a single 72W NVIDIA L4 GPU with 24GB VRAM and it takes 6.5 hours for training both CQN and DrQ-v2+. We find that major bottleneck is slow simulation because our model consists of lightweight CNN and MLP architectures.\nHyperparameters We use the same set of hyperparameters for all the RLBench tasks. We provide detailed hyperparameters of CQN in Table 4 and DrQ-v2/DrQ-v2+ in Table 5."}, {"title": "D Experimental Details: Real-world", "content": "Tasks We design 4 real-world visual robotic manipulation tasks with different characteristics. We do not provide partial reward during the episode and only provide reward 1 at the end of fully successful episode. See Figure 7 for pictures that show how we randomize the initial position of the objects between each episode. We describe the tasks in more detail as below:\n\u2022 Open Drawer and Put Teddy in Drawer. The goal of this task is to (i) fully open the drawer, which is slightly open at the start of each episode, (ii) pick up the teddy bear, and (iii) put the teddy bear in the drawer. We use 50 demonstrations for this task. We randomize the initial position of the teddy bear between every episode in a 10cm radius circle.\n\u2022 Flip Cup. The goal of this task is to (i) grasp the handle of a plastic wine glass and (ii) flip the cup in a upright position. We use 20 demonstrations for this task. We randomize the initial position of the cup between every episode in a 15\u00d730cm rectangular region.\n\u2022 Click Button. The goal of this task is to click the button with the closed gripper. We use 21 demonstrations for this task. We randomize the initial position of the button between every episode in a 38\u00d738cm squared region.\n\u2022 Take Lid Off Saucepan. The goal of this task is to (i) grasp the lid of the saucepan and (ii) lift the lid up. We use 24 demonstrations for this task. We randomize the initial position of the saucepan between every episode in a 38\u00d738cm squared region."}, {"title": "E DeepMind Control Experiments", "content": "Setup To demonstrate that CQN can achieve competitive performance in widely-used, shaped-rewarded RL benchmarks, we provide experimental results in a variety of continuous control tasks from DeepMind Control Suite (DMC) [28]. We also note that DMC benchmark consists of a variety of low-dimensional and high-dimensional control tasks, enabling us to evaluate the scalability of CQN on environments with high-dimensional action spaces. For baselines, we compare CQN to RL algorithms that learn continuous policies, whose performances in DMC are publicly available45. For state-based control tasks, we consider soft actor-critic (SAC) [7] as our baseline. For vision-based control tasks, we compare CQN to DrQ-v2 [2]. For hyperparameters, we follow the original hyperparameters used in the publicly available results. For instance, we use the action repeat of 1 for state-based control tasks and action repeat of 2 for vision-based control tasks. For CQN hyperparameters, we set minimum and maximum value bounds to 0 and 200 for distributional critic and use 3 levels with 5 intervals for coarse-to-fine action discretization.\nResults Figure 9 and Figure 10 show that CQN achieves competitive or superior performance to RL baselines that learn continuous policies in most of the tasks. This result demonstrates that our framework is generic, i.e., it can be used for state-based, vision-based, sparsely-rewarded, and densely-rewarded environments. One trend we observe in pixel-based DMC tasks is that the performance of CQN often stagnates early in locomotion tasks (e.g., Quadruped, Hopper, and Walker), unlike in manipulation tasks where CQN achieves superior performance to the baseline. We hypothesize this is because we use a na\u00efve exploration scheme: we use the exploration noise of e ~ N(0,0.1). It would be an interesting future direction to investigate how to design exploration schedule that can exploit a discrete action space from our coarse-to-fine discretization scheme."}, {"title": "F Additional Related Work", "content": "Real-world RL for continuous control Obviously, our work is not the first application of RL to real-world continuous control domains. In particular, in the context of learning locomotion behaviors, there have been impressive successes in demonstrating the capability of RL controllers trained in simulation and then transferred to real-world environments [89, 90, 91, 92, 93, 94]. More closely related to our work are approaches that have demonstrated RL can be used to learn robotic skills directly in real-world environments, with state inputs [95, 96, 97, 98, 99], visual inputs [29, 33, 100, 101, 102], and offline data [77, 103, 104], addressing challenges such as exploration, state estimation, camera calibration, robot failure, and the cost of resetting procedures. Moreover, there has also been a progress in developing benchmarks that can serve as a proxy for real-world experiments [1, 105] and developing a software package for easily deploying RL algorithms to real-world RL [106]. Investigating the effectiveness of our framework on such various benchmarks and real-world domains would be an exciting future direction we are keen to explore.\nHierarchical RL Our work is loosely related to approaches that learn hierarchical RL agents [107, 108] that trains high-level RL agents that provides goals (options or skills) and low-level RL agents that learn to follow goals or behave conditioned on goals [109, 110, 111, 112, 113, 114]. This is because our approach also introduces a multi-level, hierarchical structure in the action space. But our work is different in that we introduce a hierarchy by splitting the fixed, general continuous action space but hierarchical RL approaches typically introduce a temporally or behaviorally abstracted action as a high-level action (goal, option, or skill). Nevertheless, it would be an interesting future direction to incorporate such abstract high-level actions into our coarse-to-fine critic architecture, as it is straightforward to condition our critic on such abstract actions by introducing an additional level."}, {"title": "G Limitations and Future Directions", "content": "Data augmentation In this work", "augmentations": "RandomShift [2", "116": "applying more strong augmentations can also be helpful for improving the generalization capability of RL agents. Moreover, applying augmentation to images with generative models [117", "83": "can be an interesting future direction.\nAdvanced vision encoder and representation learning CQN uses a simple, light-weight visual encoder, i.e., 4-layer CNN encoder, and also a na\u00efve way of fusing view-wise features that concatenates image features. While this has an advantage of having a simple architecture and thus a very fast inference speed, incorporating an advanced vision encoder architectures such as ResNet [118", "119": "may improve the performance in tasks that require fine-grained control. Moreover, given the recent improvements in learning multi-view representations [55, 66, 120", "125": "incorporating such improvements and 3D prior into encoder design can be helpful for improving the sample-efficiency of CQN, especially in tasks that require multi-view information as already shown in recent several behavior cloning approaches [67, 68, 69, 70, 71, 72", "128": "would also be an interesting direction.\nHandling a history of observations For taking a history of observations as inputs, we follow a very simple scheme of Mnih et al. [19", "129": "or incorporating architectures that can handle a sequence of observations, such as RNNs [130, 131", "132": "and state-space models [133", "65": ".", "106": "would be an interesting future direction we are keen to explore. Furthermore, we note that recent approaches typically depend on resetting technique for supporting high-UTD but such resetting can be problematic in that it may lead to dangerous behaviors with real robots. Investigating how to support high UTD without such a resetting technique can be also an interesting future direction especially in the context of real-world RL.\nSearch-based action selection CQN uses a simple inference scheme that greedily selects an interval with the highest Q-value from the first level. However, there is a room for improvement in action selection by incorporating search algorithms that exploit the discrete action space [73"}]}