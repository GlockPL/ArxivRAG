{"title": "DeMuVGN: Effective Software Defect Prediction Model by Learning Multi-view Software Dependency via Graph Neural Networks", "authors": ["Yu Qiao", "Lina Gong", "Yu Zhao", "Yongwei Wang", "Mingqiang Wei"], "abstract": "Software defect prediction (SDP) aims to identify high-risk software defect modules in software development, allowing resources to be allocated efficiently. Previous studies have demonstrated that dependency network metrics can improve defect prediction performance. However, cutting-edge methods often rely on program code view to construct the dependency graph while ignoring the developer factors in software development. Besides, current dependency network metrics are mainly based on handcrafted metrics (i.e., ego and global network metrics) to represent the program information, which cannot comprehensively and intelligently cover relevant defect information. To this end, we propose an effective defect prediction model by learning multi-view software dependency via graph neural networks, dubbed DeMuVGN. Specifically, we first propose a Multi-view Software Dependency Graph (MSDG) to combine data, call, and developer dependency. We then enhance the Bidirectional Gated Graph Neural Network (BiGGNN) by Synthetic Minority Over-sampling Technique (SMOTE) to better learn the class imbalance program representations for identifying the defective modules more effectively. Finally, through an extensive case study on eight open-source software projects across 20 versions, we find that: i) models based on the multi-view dependency graph improve the F1 score by 11.1% ~ 12.1% over models based on the single-view dependency graph. ii) DeMuVGN improves state-of-the-art methods in within-project context by 17.4% ~ 45.8% and cross-project context by 17.9% ~ 41.0% in terms of the F1 score. By analyzing the results, we also discover the advantage of DeMuVGN in software evolution as the performance of later-stage software versions improves more than early versions in a within-project context, and observe the generalizability of DeMuVGN as it performs well in a cross-project context. Based on these findings, we recommend future research to consider using the multi-view dependency graph to build defect prediction models not only in mature projects with historical data but also in newly developed projects.", "sections": [{"title": "INTRODUCTION", "content": "Since software defects are inevitable in the software development process 1,2,3, it is crucial to promptly identify high-risk software defect modules to efficiently allocate resources (e.g., budget, time, and personnel)4,5,6. Software defect prediction (SDP) can apply statistics or machine learning algorithms to learn the relationship between software internal attributes (e.g., code metrics, process metrics), which has helped developers automatically and quickly discover high-risk defective modules 7,8,9,10,11,12.\nThe dependency graph constructed by code dependencies (e.g., data or call dependencies) provides a good representation of the defects flow among software modules 13. The dependency network metrics can improve the performance of SDP models 14,15. However, as we know, in addition to program code view dependencies among software modules, there are developer interactions among software modules. Developers exhibit varying coding styles, commit frequencies, and experience levels, resulting in diverse defect patterns 16. For instance, less experienced developers tend to submit more code defects compared to more experienced developers 17. Software modules developed by the same developers with bad coding habits would have the repetition of the same types of code defects, which would construct the defect flow among software modules. Therefore, it is necessary to incorporate developer information into the dependency network for a more comprehensive understanding of software defect information.\nIn addition, dependency network metrics are extracted by application of the Social Network Analysis in previous studies 14,13, which are handcrafted metrics that would not comprehensively and intelligently cover the whole useful defect information. At the same time, as graph neural networks (GNNs) can automatically encode node features by learning adjacency relationships and graph structure into uniform dimensional vectors, some researchers have increasingly utilized GNNs to enhance the accuracy of SDP models 18,19. However, those methods utilizing GNNs only learn the graph about a single-view (e.g., code view), lacking learning from the multiple-view.\nInspired by the above-mentioned two aspects, we propose a novel DeMuVGN (a.k.a., defect prediction model by learning multi-view software dependency via graph neural networks) approach. Specifically, we propose the Multi-view Software Dependency Graph (MSDG) by merging the Code Dependency Graph (CDG) based on data and call dependency with the Developer Dependency Graph (DDG) based on developer dependency. We enhance the Bidirectional Gated Graph Neural Network (BiGGNN) using Synthetic Minority Over-sampling Technique (SMOTE), improving the model's ability to handle imbalanced data and learn node features automatically. DeMuVGN overcomes the limitation of relying on a single graph view and intelligently generates metrics, ultimately improving defect prediction performance.\nTo evaluate the effectiveness of our DeMuVGN, we conduct an extensive case study on eight open-source software projects across 20 versions to answer the following research questions (RQs):\nRQ1: Can Multi-view Software Dependency Graph improve the performance of defect prediction model with BiGGNN?\nPerformance of the model based on the MSDG outperforms DDG by 3.57% (8.16%, 1.63%, 0.55%, and 12.1%) and outperforms CDG by 3.07% (5.53%, 1.43%, 0.38%, and 11.1%) on the AUC (Recall, Brier, PF, F1) measure with statistically significant differences. This indicates that a dependency graph constructed on multiple views is more beneficial in SDP than that of a single-view dependency graph. We also find the model built based on the MSDG improves performance more in later versions than the early versions in multi-version datasets. We recommend considering adding the multi-view dependency for SDP in future research, especially in the later-stage software versions.\nRQ2: Can our DeMuVGN outperform the state-of-the-art SDP models?\nCompared with three state-of-the-art defect prediction models, DeMuVGN outperforms CGCN by an average of 11.2% (3.5%, 6.9%, 7.6%, 17.4%), outperforms DP-CNN by an average of 19.7% (46.9%, 27.5%, 7.6%, 45.8%) and outperforms DBN by an average of 31.4% (14.4%, 19.0%, 12.7%, 21.5%) on AUC (Recall, Brier, PF, F1) measure with statistically significant differences. Compared with five traditional machine learning classifiers (LR, NB, RF, SVM, and XGBoost) with over-sampling, DeMuVGN shows significant performance improvement, demonstrating the effectiveness of DeMuVGN in within-project defect prediction.\nRQ3: How do BiGGNN and SMOTE contribute to our DeMuVGN?\nModel built on MSDG without SMOTE achieves better performance than within-project baselines (i.e., CGCN, DP-CNN, DBN, LR, NB, RF, SVM, and XGBoost) while adding SMOTE operation improves performance again (3.2% on AUC, 16.0% on Recall, 2.0% on Brier, 0.6% on PF, and 16.9% on F1). This indicates that both BiGGNN and SMOTE improve the performance of our DeMuVGN, and SMOTE can mitigate dataset imbalance as the improvements of Recall and F1 are significant. We recommend considering BiGGNN with SMOTE in software defect prediction to mitigate the imbalance of software defect data in future studies.\nRQ4: Can our DeMuVGN trained on one project transfer into other projects?\nCompared to other advanced five cross-project prediction approaches (i.e., DSSDPP, CGCN, MNB, TCB, VCB), our DeMuVGN improves performance on the average to the five baselines by 12.38% on AUC, 21.98% on Recall, 20.84% on Brier, 19.48% on PF, and 25.32% on F1, all with significant differences. This indicates that DeMuVGN performs well even in the absence of initial labeled data with generalizability. We recommend that practitioners apply our DeMuVGN in the early version of projects with no historical label data in defect prediction.\nIn summary, the contributions of our study mainly include:\n1) We are the first to introduce the Multi-view Software Dependency Graph by integrating the Code Dependency Graph and Developer Dependency Graph into software defect prediction."}, {"title": "MOTIVATION AND RELATED WORK", "content": "Consider project Lucene+, a high-performance text search engine library written in Java. In its version 2.3.0, three Java files-Analyzer.java, SegmentMerger.java, and TestSetNorm.java\u2014are identified as defective. Source code snippets from these modules are illustrated in Figures 1, 2, and 3, respectively. When analyzing these three modules through code dependency (i.e., data and call dependency), we observe that there is no direct code dependency linking any two of these modules. However, all three defective modules are developed by the same developer. Figure 4 presents the code dependency and code ownership relationships among these software modules and some of their relevant modules. This observation leads us to hypothesize that code defects might be closely related to who owns and maintains the code.\nWe further explore the impact of code ownership on defective code in release 2.3.0 of Lucene by calculating among any two defective modules whether they share code dependency or code ownership, respectively. We observe that among the 199 defective modules in release 2.3.0 of Lucene, code dependency occurs in 848 pairs of defective modules, while code ownership occurs in 522 additional pairs of defective modules. This observation suggests that code ownership can help provide more defect connections that cannot be provided solely by code dependency. Moreover, to understand whether these code ownership-linked defective modules would provide meaningful defect correlation information, we further manually explore the defective categories of all defects in Lucene 2.3.0 according to the categories of prior study 20 (e.g., resource, check, interface, logic, timing, support, and larger defects). We observe that 76.1% of code ownership-linked defective modules are the same defects (i.e., 34.6% of logic defects, 14.8% of interface defects, 13.5% of resource defects, and 13.2% of other defects), suggesting that code ownership forges more meaningful associative links between defective modules.\nFrom this motivation example, we can observe that using code ownership relationships from the developer's perspective may provide more defect correlation information to explore defect propagation in another view, which cannot be provided based on code data/control flow. Therefore, in our study, we would attempt to propose a multi-view dependency graph to capture code and developer view information to improve defect prediction performance."}, {"title": "Related Work", "content": "Studies on the construction of various software dependency graphs: Software dependency graphs are graph data structures, with nodes representing software modules and edges representing the dependencies between these modules. Existing dependency graphs mainly rely on program code view (data or call dependencies) for construction. For example, Zimmermann and Nagappan 14 construct a dependency network based on data and call dependencies to improve the performance of SDP. Phan et al. 15 use a Control Flow Graph to extract deeper semantic features from the code. However, they focus on constructing networks solely from the code view, overlooking developers' influence in software development. Moreover, handcrafted network metrics are limited by the formulation of artificial standards and can't learn features comprehensively and intelligently. Different from them, we propose the Multi-view Software Dependency Graph which integrates data, call, and developer dependency. We also use an advanced GNN model to quickly and intelligently learn the relationship between defects and modules from the graph.\nStudies of GNN applications in SDP: Due to the power of GNN in the knowledge graph representation, GNN21 and its variants have gradually become hotspot models in SDP. For example, prior studies have applied Graph Convolutional Network (GCN) 22,23, Graph Attention Network (GAT) 23,24, and Gated Graph Sequence Neural Network (GGNN)24 into software defect prediction to improve the performance. However, they only applied the GNN to the dependency graph constructed by the program code view. Moreover, traditional GNN and variants treat graphs as undirected graphs, ignoring the important direction information. Therefore, we use enhanced BiGGNN on the MSDG to learn node embeddings from the multi-view graph and bi-direction edges to help extract more comprehensive graph information.\nStudies on the developer factors as features in software defect prediction: Since the quality of software depends not only on the characteristics of code but also on the characteristics of developers, prior studies have also proposed some developer metrics to improve the defect models. For example, Shinsuke et al. 25 propose three developer metrics (including NOCC, NoC, and NoD) to analyze the relationship between defects and developers. Lee et al. 26 also propose the micro interaction metrics based on the interaction behavior of developers to verify its impact on defect prediction performance. However, these studies only utilize developer information to extract fewer handcrafted metrics that would not comprehensively and intelligently cover developer information. Different from these studies, we are the first to introduce the Multi-view Software Dependency Graph by integrating the code dependency and developer dependency into software defect prediction, and using GNN to learn the defect information automatically."}, {"title": "APPROACH", "content": "In this section, we introduce DeMuVGN. Figure 5 shows the overview of DeMuVGN, which mainly includes the following two parts: (1) Multi-view Software Dependency Graph Construction and (2) Enhanced Bidirectional Gated Graph Neural Network.\nNext, we describe each in detail."}, {"title": "Multi-view Software Dependency Graph Construction", "content": "This section introduces how to use the dependencies between software modules (files, classes, methods, or blocks of code) to build the Multi-view Software Dependency Graph, including the construction of Code Dependency Graph (CDG), Developer Dependency Graph (DDG) and Multi-view Software Dependency Graph (MSDG)."}, {"title": "Code Dependency Graph", "content": "CDG uses program codes and represents programs as directed graphs, where nodes are program modules and edges are the dependencies between modules. Following the suggestion of Zimmermann and Nagappan 14, we consider the construction of CDG through data and call dependency. The popular commercially available tool Understand by SciTools is employed to measure normalized dependencies between two modules.\nIn the following sections, we refer to these two types of dependency together as program dependency. If there is a program dependency between any two files, it means that there is a directed edge between the nodes of the two files. A CDG is a directed graph G formally defined as G = (N, E), where N is the set of software modules and E is the set of directed edges, and (A, B) \u2208 E if module A has a program dependency on module B. The weight on the edge is defined as the sum of the number of program dependency times between two modules, and the CDG edge weight between two modules A and B is defined by Equation 1.\nWeight $_{CDG}(A, B) = num_{data}(A, B) + num_{call}(A, B)$ (1)\nIn Equation 1, $num_{data}(A, B)$ and $num_{call}(A, B)$ represent the number of data and call dependency times between A and B. Figure 6a shows a simple program dependency graph, where rectangles represent modules and directed edges represent program dependency between modules. For example, modules C and E have a total of 3 and 7 times of program dependency on module A, module E has a total of 8 times of program dependency on module B, and modules C and D have a total of 11 and 9 times of program dependencies on module E, respectively."}, {"title": "Developer Dependency Graph", "content": "We propose the DDG and use the developer information of the modules to represent the program as a directed graph, where nodes are program modules and edges are developer consistency between modules. The developer information of the modules is obtained from the commit reports. If there is a developer consistency between any two modules (i.e., the two modules have the same developer), it indicates that there are mutually directed edges between the two module nodes. A DDG is a directed graph G formally defined as G = (N, E), where N is the set of module nodes and E is the set of directed edges, and (A, B) \u2208 E if there is a developer consistency between modules A and B. The weights on the edges are defined as the consistency strength of the developer between the two modules, and the edge weights between two modules A and B are defined by Equation 2.\nWeight $_{DDG}(A, B) = |develop(A), develop(B)|$ (2)\nIn Equation 2, develop(\u00b7) denotes the developer information of the module; $|develop(A), develop(B)|$ denotes the number of the same developers of module A and module B. Figure 6b shows a simple DDG, where rectangles represent modules, circles represent developers, and directed edges represent developer relationships between modules. The dotted lines represent the dependencies between modules and their corresponding developers; note that such edges are for illustration only and do not exist in the actual DDG. For example, Module A and Module C have a developer consistency strength of 5, and Module B and Module D have a developer consistency strength of 3."}, {"title": "Multi-view Software Dependency Graph", "content": "The integration of code dependency (CDG) and developer consistency (DDG) weights into the multi-view software dependency graph (MSDG) creates a unified metric that leverages the strengths of both types of dependencies. By summing these weights, the MSDG encapsulates the overall \"risk\" or \"importance\" of the connection between two modules, considering both the technical structure of the code and the developer collaboration patterns. This unified approach is particularly valuable for defect prediction, as it accounts for multiple factors that may contribute to software defects, thus providing a more robust predictive model.\nMSDG is a combination of a CDG and a DDG. MSDG uses the module code and the corresponding developer information to represent the program as a directed graph, where the nodes are the program modules, the edges are the program dependencies, and developer consistency between the modules. An MSDG is a directed graph G defined in the equation as G = (N, E), where N is the set of module nodes and E is the set of directed edges, (A, B) \u2208 E if there is a program dependency or developer consistency between modules A and B. The MSDG edge weights between modules A and B are defined by the Equation 3.\nWeight $_{MSDG}(A, B) = Weight_{CDG}(A, B) + Weight_{DDG}(A, B)$ (3)\nFigure 6c shows a simple MSDG graph, which is a combination of Figure 6a and Figure 6b, where rectangles represent modules and directed edges represent relationships between modules."}, {"title": "Enhanced Bidirectional GGNN with SMOTE Oversampling", "content": null}, {"title": "Synthetic Minority Over-sampling Technique", "content": "SMOTE (Synthetic Minority Over-sampling Technique)27 is an improvement technique based on random oversampling. Its main idea is to generate a few synthetic nodes in the feature embedding space by interpolation, instead of simply copying and generating links to the original nodes. SMOTE allows the generation of features for minority classes, balancing the original data and facilitating further classification operations.\nFormally, let $h_v \\in R^d$ be the feature representation of the minority class node $v$, where $d$ is the dimension length, $v$ labeled as Y. We select appropriate minority class samples based on the given sampling ratio and find the most similar node $u$ of the same class as $v$ by measuring their Euclidean distances in the embedding space, as the following equation:\n$\\eta(v) = arg min_u ||h_u \u2013 h_v||$, s.t. $Y_u = Y_v$, where $h_u$ is the feature representation of node u and $Y_u$ is the u label. Using the most similar nodes, we can generate the feature representation $h_{v\u2019}$ of the synthetic node $v'$: $h'_{v\u2019} = (1 \u2013 \\delta) \u00b7 h_v + \\delta \u00b7 h_u$, where $\\delta$ is a random variable obeying a uniform distribution in the range [0, 1]. The label of v' is the same as v and u. After obtaining the"}, {"title": "Bidirectional Gated Graph Neural Network", "content": "Bidirectional Gated Graph Neural Network (BiGGNN)28 learns node embeddings from incoming and outgoing edges by interleaving them in a bidirectional manner when processing directed channel graphs, and uses the Gated Recurrent Unit (GRU) as an update function to filter out unnecessary feature information, thus realizing the ability to extract more and more useful information. In particular, given a graph G = (V, E), each node v \u2208 V is initially represented as $h_v \\in R^d$, where d is the dimension length. Similar to GNN, we apply a message-passing function for a defined number of hops, denoted as K. During each hop k\u2264 K, for a specific node v, we employ a summation aggregation function. This function processes a set of neighboring node vectors and generates aggregated vectors, considering backward or forward directions. Message passing is computed as Equation 4, where $N(v)$ denotes the set of neighboring nodes of node v, and \u252b/F represents backward and forward directions. The message passing begins with an initial representation $h_v^0$ for each node $v \\in V$. For each hop k, (k = 1, 2, 3, . . ., K), we aggregate the node representations in both directions as in Equation 5. The fusion function, described in Equation 6, is a gated sum of two inputs, where \u2299 represents component-wise multiplication, o represents a sigmoid function, and z represents a gating vector as defined in Equation 7.\n$h^k_{\\perp}(v) = SUM(h_{u}^{k-1}), \u2200u \u2208 N^{-}(v)$ (4)\n$h^k_{/}(v) = SUM(h_{u}^{k-1}), \u2200u \u2208 N^{+}(v)$ (5)\n$h_v^k(v) = Fuse(h^k_{/}(v), h^k_{\\perp}(v))$ (6)\n$Fuse(a, b) = z \u2299 a + (1 \u2212 z) b$\n(7)\n$z = \u03c3(W_z[a; b; a \u2299 b; a \u2212 b] + b_z)$\nFinally, we provide the Gated Recurrent Unit (GRU) the results as shown in Equation 8 so it can aggregate the data and update the node representation.\n$h_v^k = GRU(h^{k-1}_v, h_v^k(v))$ (8)"}, {"title": "Classifying Module", "content": "Once the features of the model output are obtained, we use a simple multilayer perceptron (MLP) layer as a classifier for the classification task, mapping high-dimensional vectors into two-dimensional class probabilities. In this case, the MLP layer consists of multiple fully connected layers.\nFollowing Liu et al. 29 we use Cross-Entropy as the loss function for optimizing. The cross-entropy loss function encourages the model to output a probability distribution that is close to the probability distribution of the true label. By minimizing the cross-entropy loss, the model will tend to produce predictions that are more similar to the true label's probability distribution.\nIn general, we construct the MSDG to depict each project version, where each node corresponds to a code file. Our DeMuVGN model leverages a BiGGNN to derive node embeddings from MSDG, enabling the capture of complex dependencies through bidirectional message passing. Each node starts with predefined metrics as initial features. Through iterative processing, nodes update their features by assimilating information from adjacent nodes in both forward and backward directions, using GRU to effectively merge these features with the node's existing features for a comprehensive representation. Ultimately, a classifier utilizes these enriched node embeddings to predict whether a file is defective or not."}, {"title": "STUDY DESIGN", "content": "In this section, we mainly introduce the preparations for our following experiments. The detailed process will be introduced as follows."}, {"title": "Step1: Graph Construction", "content": "In this step, we construct three file-level dependency graphs for CDG, DDG, and MSDG, where the nodes of the graphs are program files and the edges are the weights of different dependencies between two files. Firstly, we utilize the Understand tool for quantifying data dependencies and call dependencies between two files. Secondly, we establish developer dependency by comparing the committer from the commit reports and the assignee from the corresponding defect reports if they are the same person. Subsequently, we strictly follow Section 3.1 for the construction of the three graphs. A total of 60 graphs are generated for the 20 project-versions combinations."}, {"title": "Step2: Data Construction and Sampling", "content": "In the within-project and cross-project contexts, we normalize metrics and graph weights, as recommended by several prior studies 31,32. Initially, we normalize all file metrics of the same version according to their type of metrics. Next, when dealing with a dependency graph consisting of one version of the dataset, we normalize the weights on the outgoing edges of the same node. Ensuring that the metrics' values range between 0 and 1 for all studies can significantly enhance the model's capacity for generalization 33. We utilize the normalized metrics as the initial features of the graphs' nodes for learning.\nWe train the models using the training set, validate the models using the validation set, and select the model that works best on the validation set and compute performance metrics using the test set. The composition of the dataset varies between the within-project defect prediction (WPDP) and the cross-project defect prediction (CPDP) as follows:\nWithin-project context. The training set, validation set, and test set are from the same version of a project. We randomly select 70%, 15%, and 15% of the node features from the same version independently as the training set, validation set, and test set.\nCross-project context. Same steps as Gong et al. 13, we use the latest version of a completely different project to predict defects in the current version. For example, to predict defects in version $V_k$ of project $P_B$, models would be trained on the latest version $V_m$ belonging to project $P_A$. We randomly select 80% and 20% of the node features of the $V_m$ independently as the training and validation set. And we use all the node features of each version of project $P_B$ as the test set.\nAs shown in Table 1, all the datasets have defect rates below 20%, so there is a strong imbalance that tends to negatively affect the model. Therefore, we utilize the SMOTE on the minority class in the training set in both within-project and cross-project contexts. The sampling ratio is experimentally determined as a hyperparameter, as described in Section 4.3. It is worth noting that we only over-sample the training set while the test and validation sets retain their original data proportions.\nFurthermore, to ensure the statistical robustness of our results 34,35, for each data construction in the within-project context, we resample them 100 times with out-of-sample bootstrap sampling, while for each data construction in the cross-project context, we resample them 20 times with out-of-sample bootstrap sampling. For each research project, we obtain seven latest versions of other projects to construct training and validation sets, resulting in a total of 140 training iterations per dataset (7 * 20).In Section 5, RQ1 to RQ3 are experimented in the within-project context, while RQ4 is experimented in the cross-project context."}, {"title": "Step3: Model Construction", "content": "In this step, we detail the construction, experiment configuration, and training process of our models and baselines. To ensure the validity of our study, we test the validity of our models by selecting the baseline models for comparison in the context of within-project and cross-project:"}, {"title": "Step 4: Performance Calculation", "content": "In this step, we introduce the evaluation measures and performance analysis methods of our study.\nEvaluation Measures. To assess the effectiveness of the classification model, we use the Area Under the Receiver-operator Characteristic Curve (AUC)49, Recall, Brier score50, Probability of false alarm (PF) and F1-score in the our four tasks, similar to several prior studies 4,51,42. AUC is suitable for overall performance evaluation, Recall focuses on the capture of positive categories, PF helps reduce the false positive rate, Brier score evaluates the quality of probabilistic predictions, and F1 can balance Precision and Recall. Each of these measures focusing on different aspects of model performance, combined together can provide a more comprehensive assessment of the model's performance. The definitions of five measures are shown in Table 3."}, {"title": "RESULTS", "content": "In this section, we present the results of four research questions. In RQ1, we explore the superiority of different dependency graphs mentioned in Section 3.1 in SDP. In RQ2, we explore the performance of DeMuVGN, not only comparing it with state-of-the-art approaches but also evaluating the improvement compared to traditional machine learning classifier models. In RQ3, we explore the importance of the BiGGNN and SMOTE on the effect of the DeMuVGN approach. Finally, in RQ4, we also perform a comparison of the DeMuVGN model with state-of-the-art models in CPDP to ensure the usability of DeMuVGN in different projects. For all the RQs, we follow the experiment setup and evaluation measures in Section 4 (RQ1-3 in the WPDP context, RQ4 in the CPDP context). In addition, statistically significant differences in the measures are calculated using the Wilcoxon-signed rank test and Cliff's delta effect size test."}, {"title": "RQ1: Can Multi-view Software Dependency Graph improve the performance of defect prediction model with BiGGNN?", "content": "Approach. To answer RQ1, we construct three dependency graphs for each of the 20 project-version combinations, for a total of 60 networks. These networks are then trained, classified, and performance calculated on BiGGNN in the within-project context and ensure that each method differs only in terms of graph construction. We refer to the models trained on CDG, DDG, and MSDG as CDG-BiGGNN, DDG-BiGGNN, and DeMuVGN.\nResults 1) DeMuVGN outperforms both CDG-BiGGNN and DDG-BiGGNN with statistical differences. Table 5 shows the results of the comparison of the three methods in terms of AUC, Recall, Brier, PF, and F1. As mentioned in Section 4.2.3, we perform 100 experiments and record the median values of them in Table 5, where the best results on each measure of each version have been marked in bold format. The last row shows the average results for all datasets. From Table 5, we can observe that DeMuVGN outperforms the other two variants (CDG-BiGGNN and DDG-BiGGNN) on average for all the measures. Specifically, DeMuVGN outperforms DDG-BiGGNN by 3.57%, 8.16%, 1.63%, 0.55%, and 12.1% on AUC, Recall, Brier, PF,"}, {"title": "RQ2: Can our DeMuVGN outperform the state-of-the-art SDP models?", "content": "Approach. To answer RQ2, we perform WPDP on 20 project-version combinations and evaluate the results produced by various models. As described in Section 4.2.3, we select five classical machine learning classifiers and three state-of-the-art neural network models (CGCN, DP-CNN and DBN) as the baseline models of WPDP.\nResults 3) In the within-project context, DeMuVGN reaches the best performance on all five measures compared to DP-CNN and DBN with statistical differences, which demonstrates the effectiveness of DeMuVGN in WPDP. Table 7 shows the results of the comparison between DeMuVGN and baselines in terms of AUC, Recall, Brier, PF, and F1. The median values of 100 experiments are recorded in these tables, where the best results on each measure of each version have been marked in bold format. The last row shows the average results for all project-version combinations. We can observe that DeMuVGN achieves the best results in most versions. Specifically, DeMuVGN outperforms CGCN by an average of 11.2% (3.5%, 6.9%, 7.6%, 17.4%), outperforms DP-CNN by an average of 19.7% (46.9%, 27.5%, 7.6%, 45.8%) and outperforms DBN by an average of 31.4% (14.4%, 19.0%, 12.7%, 21.5%) on AUC (Recall, Brier, PF, F1). Moreover, DeMuVGN achieves the best performance on 15, 13, 15, 10, and 17 out of 20 project-version combinations in terms of AUC, Recall, Brier, PF, and F1 respectively. Although the PF achieves the least number of optimal results, it always remains at a low level (<=0.09). A higher PF indicates the need for more testing resources in misclassified non-defective modules, leading to resource wastage. The good AUC, Recall, Brier, F1 and not-so-bad PF indicate that DeMuVGN can try to make the resources flow to the defective module while ensuring fewer resources are wasted.\nRows 3 to 10 of Table 6 present the median values of effect size between DeMuVGN and baselines. Both Cliff's delta test and Wilcoxon-signed rank test show all results on the measures compared to baselines are significantly, except for the AUC's delta score versus CGCN, which means our DeMuVGN is more effective than CGCN, DP-CNN and DBN.\nResults 4) In the within-project context, DeMuVGN shows significant performance improvements and more stable results than traditional classifiers on all measures. From Table 7, we observe that DeMuVGN improves by 33.2%, 53.0%, 8.6%, 7.3%, 58.9% over LR, 33.2%, 36.5%, 18.9%, 12.7%, 59.6% over NB, 14.1%, 45.4%, 4.4%, 1.1%, 47.3% over RF, 32.7%, 54.8%, 8.3%, 4.4%, 78.9% over SVM, 15.0%, 42.6%, 4.9%, 1.8%, 45.3% over XGBoost in terms of AUC, Recall, Brier, PF, and F1, respectively. We also note that DeMuVGN, while maintaining excellent performance, also ensures stability. We calculate the median values of the variance corresponding to each model or classifier under the five evaluation measures, and we observe that DeMuVGN (0.005) achieves the most stable results compared to the five traditional classifiers (0.016, 0.036, 0.017, 0.026, 0.017 for LR, NB, RF, SVM, XGBoost). Therefore, DeMuVGN reaches a robust and outstanding performance with minimal fluctuation across datasets compared with"}]}