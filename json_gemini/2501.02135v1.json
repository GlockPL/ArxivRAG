{"title": "AVTRUSTBENCH: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs", "authors": ["Sanjoy Chowdhury", "Sayan Nag", "Subhrajyoti Dasgupta", "Yaoting Wang", "Mohamed Elhoseiny", "Ruohan Gao", "Dinesh Manocha"], "abstract": "With the rapid advancement of Multi-modal Large Language Models (MLLMs), several diagnostic benchmarks have recently been developed to assess these models' multi-modal reasoning proficiency. However, these benchmarks are restricted to assessing primarily the visual aspect and do not examine the holistic audio-visual (AV) understanding. Moreover, currently, there are no benchmarks that investigate the capabilities of AVLLMs to calibrate their responses when presented with perturbed inputs. To this end, we introduce Audio-Visual Trustworthiness assessment Benchmark (AVTRUSTBENCH), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Using our benchmark, we extensively evaluate 13 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human-like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model-agnostic calibrated audio-visual preference optimization-based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks. We will publicly release our code and benchmark to facilitate future research in this direction.", "sections": [{"title": "1. Introduction", "content": "In recent years, Large Language Models (LLMs) [1, 13, 66, 67] have demonstrated remarkable capabilities to understand, reason, and generate text across a variety of tasks. Leveraging LLMs, recent efforts extend to other modalities beyond text (e.g., image, video, audio, etc.) through Multi-modal Large Language Models (MLLMs) [6, 7, 15, 32, 33, 40, 43-45, 52, 54, 57, 60, 75, 86, 90]. However, with the introduction of these more powerful models comes the increasing need of assessing the reliability and robustness of their output when deployed in real-world settings. While we humans can easily identify the discrepancies and act accordingly when encountering a \"wrong\" question, in most cases, current AVLLMs assume the validity of the question and have a propensity towards responding with a hallucinated answer.\nOf late, a number of benchmarks have been proposed [29, 37, 42, 73, 77, 81] to evaluate MLLMs under a typical Question-Answer (QA) set-up (free form or multiple-choice) to investigate its performance under various reasoning and perception tasks. We identify two major limitations in the existing benchmarks: (i) current benchmarks are primarily restricted to the visual modality and ignore other modalities such as 'audio', an extremely critical component in comprehensive video understanding; (ii) existing benchmarks do not evaluate the reliability and robustness of AVLLMs' response under critical aspects such as adversarial attack, compositional understanding capabilities, and their ability to extract synchronous information from the constituent modalities.\nRecent works [42, 73, 77, 78] develop benchmarks to evaluate MLLMs for images and videos as shown in Tab. 1. LVLM-eHub [73] and LAMM [78] employ human annotators to assess the model's performance. This introduces subjectivity and compromises efficiency. MME [77] and MMBench [42] improve objective evaluation of MLLMs by constructing True / False or Multiple-Choice questions. Restricting the model's output to a fixed set of options enables convenient and near-accurate evaluation protocol. However, the relatively small scale of these benchmarks (less than 3.5K samples) results in incomprehensive evaluation. These limitations reveal the need of an automated and comprehensive benchmark for the assessment of AVLLMs.\nTo this end, we present AVTRUSTBENCH, a multi-dimensional benchmark suite to extensively evaluate AVLLMS (Fig. 2). The benchmark comprises 600K samples spanning over 9 tasks to evaluate the audio-visual comprehension capabilities in AVLLMs. We design a semi-automatic annotation paradigm to generate multiple-choice QAs for each task by adapting public audio-visual datasets, making it cost-efficient in terms of human annotations and more objective compared to prior work. Using AVTRUSTBENCH, we make a thorough evaluation of 13 state-of-the-art AVLLMS (11 open and 2 closed source) and present useful findings about them based on their performances. Additionally, we provide valuable insights for future work to improve the robustness and reasoning capabilities of these models.\nTo address the limitations of existing AVLLMS, we further propose a new model-agnostic training strategy-CAVPref, comprising of a calibrated AV preference optimization protocol with a robustness module. As opposed to state-of-the-art preference optimization models [56] (which favors text over other multi-modal information, leading to multi-modal hallucinations [59]), CAVPref, in its formulation, involves conditioning from all the multi-modal inputs (audio, video, text), thereby improving reliability of the AVLLMs (Fig. 1). Furthermore, the robustness module renders the AVLLMs impervious to the distributional shifts present in the multi-modal preference datasets and thereby improve performances of AVLLMs across underrepresented categories (without compromising on other categories).\nTo summarize, our main contributions are as follows:\n(1) We introduce AVTRUSTBENCH, the first comprehensive audio-visual benchmark that assesses the trustworthiness of AVLLMs. It evaluates existing AVLLMs under three critical dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency.\n(2) We extensively evaluate 13 state-of-the-art AVLLMs under our benchmark, uncovering their major limitations and sharing our key observations on their performance.\n(3) We introduce a novel model-agnostic training strategy-CAVPref, comprising of a calibrated AV preference optimization with a robustness module. Our proposed approach achieves up to 30.19% improvement across all 9 tasks."}, {"title": "2. Related Work", "content": "Building Multi-modal LLMs. Inspired by the success of large language models [10, 49, 63], recent work has expanded LLMs to multi-modal understanding, leveraging high-quality multi-modal instructional data [2, 5, 7, 30, 40, 52, 54, 60, 85, 86, 90]. Video-LLMs [8, 21, 44, 50, 58, 60, 84] extend LLMs [66, 67] and image-based LLMs [2, 3, 40, 82] to handle additional modalities such as audio and subtitles. ChatBridge [87] uses Perceiver [25] for modality alignment with LLMs, while PandaGPT and ImageBind-LLM [20, 22] naturally integrate multi-modal inputs. X-LLM [5] applies Q-Former with modality-specific adapters to combine image, audio, and video with LLMs, and Video-LLaMA [84] incorporates temporal embeddings via Image-Bind. Bay-CAT [76] is a recent AVLLM trained with an ambiguity-aware DPO strategy. Despite these advancements, none of these studies on AVLLMs address the challenges of AV consistency.\nEvaluating Multi-modal LLMs. With rapid advances in multi-modal LLMs, various benchmarks [42, 73, 77, 78] have been proposed for their evaluation. GVT [68] combines semantic (VQA, image captioning) and fine-grained tasks (object counting), while LVLM-eHub [73] aggregates benchmarks using human annotation. LAMM [78] evaluates open-form predictions on images and point clouds with GPT, though this LLM-based evaluation may affect reliability. MME [77] and MMBench [42] introduce multiple-choice QAs across diverse dimensions. Other benchmarks like AI2 Reasoning [14], HellaSwag [83], MMLU [23], and TruthfulQA [38] assess reasoning, knowledge, and misinformation. SEED-Bench [29] adds temporal tasks with a quality-assured pipeline. While some benchmarks [29, 45, 71] evaluate MLLM's temporal perception, they either work on primitive video tasks [29] or focus on particular domains (e.g., funny clips [71]), thereby limiting their practical applicability. Besides, they involve labor-intensive annotations which introduce subject bias and are cost-ineffective. Recently, VideoBench [47] and HallusionBench [39] investigated decision-making capabilities and visual illusions for videos and images. To address these limitations, we present a comprehensive benchmark to evaluate the trustworthiness of MLLMs under audio-visual events.\nMulti-modal Preference Optimization. Recent works in multimodal scenarios focus on creating multimodal preference data [16, 36, 53, 70, 80, 88, 89]. These efforts include collecting human preference [62, 79], preference from advanced multimodal LLMs [36, 80], and preference from the model to align itself [16]. In terms of learning objectives, recent works mainly follow DPO for LLMs [36, 88, 89]. Some also apply reinforcement learning [27, 62] and contrastive learning [26, 59]. However, preference optimization-based approaches disregard the importance of AV consistency, which we incorporate within our proposed objective."}, {"title": "3. AVTRUSTBENCH: Audio-Visual Trustworthiness Assessment Suite", "content": "3.1 AVTRUSTBENCH Taxonomy and Task Definitions\nOur goal is to investigate the degree to which AVLLMs: accurately comprehend the audio, visual, and textual inputs with correct semantics, rely on individual modalities, and follow instructions, even in the presence of inconsistencies in input signals. Accordingly, we design our study where we evaluate existing AVLLMs under three broad dimensions: Adversarial attack, Compositional reasoning, and Modality-specific dependency. Fig. 3 depicts individual tasks with a representative example.\nAdversarial attack. This suite comprises four different tasks for evaluating AVLLMs' performance under adversarial problem settings. This collection of tasks either consists of incongruent audio-visual pairs or inconsistencies in the answer templates. Adversarial attack suite includes:\n\u2022 Missing Choice Identification Task (MCIT). As the name suggests, this task analyzes whether the AVLLM can correctly discern that the appropriate answer is missing from the multiple-choice answer set. This task examines the model's capacity to restrain itself from responding with a choice from a plausible set of options when the correct choice is missing. Note in Fig. 3 the model is presented with potential yet inaccurate options while asked to identify an audio-visual event.\n\u2022 Inconsistent Choice Identification Task (ICIT). Unlike MCIT, in ICIT the answer set does not have any relevance to the question or audio-visual content. With entirely unrelated answer sets, ICIT assesses the extent of a model's propensity to force wrong answers with high confidence regardless of the semantic closeness to the provided choices.\n\u2022 Mismatched Video Identification Task (MVIT). MVIT assesses AVLLMs' ability to determine if a video and corresponding audio-question pairs are mismatched or incongruent. This evaluation examines the model's comprehension of the alignment between visual information with both textual (question + answer choices) and audio queries, with the objective of identifying cases where these combinations are incompatible. In Fig. 3 the visual modality from the video of a man playing a guitar is replaced with a man unboxing a parcel. Despite one of the options in the answer set having 'guitar', an intelligent system should ideally point out the inconsistency through its response.\n\u2022 Mismatched Audio Identification Task (MAIT). Similar to MVIT, MAIT investigates the ability of AVLLMS to determine if the audio and corresponding visual + textual inputs are mismatched. The impractical example (Fig. 3) of a fire engine coupled with an audio track of pleasant sea waves with gulls squealing should trigger an ideal AVLLM to raise concern even in the presence of alluring options.\nCompositional reasoning. This collection of tasks consists of multi-event audio-visual inputs where the sequence of event occurrences as well as their corresponding attribute binding may be distorted. The fundamental goal of multimodal processing is to comprehend how the linguistic component aligns with the contents of the audio-video input pairs. Therefore, it is pivotal for AVLLMs to acknowledge that disparate word arrangements in a sentence can yield different multimodal perceptions. Compositional reasoning suite includes the following set of tasks:"}, {"title": "5. Improving AVLLM through CAVPref", "content": "Zero-shot evaluation results indicate the need to: (i) create a preference dataset and perform negative instruction tuning to enhance compositional awareness and common-sense reasoning in AVLLMs, (ii) ensure equal emphasis on both audio and video modalities. Therefore, to improve the performances of AVLLMs on AVTRUSTBENCH, we present a model-agnostic, robust B-Calibrated Audio-Visual Preference Optimization method (CAVPref). We compare our proposed method with widely adopted model-agnostic approaches such as Supervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) [56]."}, {"title": "5.1 CAVPref.", "content": "With the rise of DPO [56], it is possible to align LLMs with human preferences. However, utilizing multi-modal preference data may aggravate hallucination issues as opposed to alleviating them, as found in VLLMs [35]. Utilizing non-linguistic information indirectly may lead to a preferential focus on the linguistic counterpart, resulting in suboptimal performances [59]. Therefore, it is important to have a direct conditioning of the non-linguistic information (e.g., video/audio) while implementing DPO-based approaches. Inspired by this, we propose a model-agnostic solution in an audio-visual setting.\nIn general, for all task categories in AVTRUSTBENCH, considering textual response, video input, audio input, and question as $y_w$, $y_l$, $V$, $A$, and $q$ respectively, we define:\n$L_{Y}= \\mathbb{E}_{(\\mathbf{y}_{w}, \\mathbf{y}_{l}, V, A, q) \\sim \\pi} \\bigg[\\log \\sigma\\bigg(\\beta_{Y} \\bigg(\\log {\\pi_{\\theta} (\\mathbf{y}_{w} \\vert V, A, q) \\over \\pi_{ref} (\\mathbf{y}_{w} \\vert V, A, q)} - \\log {\\pi_{\\theta} (\\mathbf{y}_{l} \\vert V, A, q) \\over \\pi_{ref} (\\mathbf{y}_{l} \\vert V, A, q)} \\bigg)\\bigg)\\bigg],$ (1)\nIn AVTRUSTBENCH, task categories MCIT, ICIT, COT-Stitch, and CAT comprise of cases where inconsistencies are only kept in the linguistic counterpart, i.e., the response. However, irregularities occur in video input in MVIT, MVT, and COT-Swap, and in audio input in MAIT, and MAT. In particular, in these tasks, audio-visual consistency is absent, i.e., audio and video are either unrelated or one of the modality is missing. In such a scenario, considering only a conventional DPO formulation (Eq. 1) is not only insufficient but also misleading since it only computes reward differences between winning and losing responses. However, reward differences must also be computed between the winning responses in the presence and absence of correct audio-visual conditioning to ensure that the AVLLM understands the correct associations (Fig. 5). Hence, we define:\n$L_{V}=\\log \\sigma\\bigg(\\beta_{V} \\bigg(\\log {\\pi_{\\theta}(\\mathbf{y}_{w} | V_{w}, A_{w}, q) \\over \\pi_{ref}(\\mathbf{y}_{w} | V_{w}, A_{w}, q)} - \\log {\\pi_{\\theta}(\\mathbf{y}_{w} | V_{l}, A_{w}, q) \\over \\pi_{ref}(\\mathbf{y}_{w} | V_{l}, A_{w}, q)} \\bigg)\\bigg),$ (2)\n$L_{A}=\\log \\sigma\\bigg(\\beta_{A} \\bigg(\\log {\\pi_{\\theta}(\\mathbf{y}_{w} | V_{w}, A_{w}, q) \\over \\pi_{ref}(\\mathbf{y}_{w} | V_{w}, A_{w}, q)} - \\log {\\pi_{\\theta}(\\mathbf{y}_{w} | V_{w}, A_{l}, q) \\over \\pi_{ref}(\\mathbf{y}_{w} | V_{w}, A_{l}, q)} \\bigg)\\bigg),$ (3)\nA critical aspect of DPO formulation (Eqs. 1 - 3) is its dependency on $\\beta$. Specifically, DPO loss can be shown as log $(1 + e^{-{\\beta}(\\mathcal{f}_{w} - \\mathcal{f}_{l})})$ where $\\mathcal{f}_{w} = \\log{\\pi_{\\theta} (\\mathbf{y}_{w}) \\over \\pi_{ref} (\\mathbf{y}_{w})}$ and $\\mathcal{f}_{l} = \\log{\\pi_{\\theta} (\\mathbf{y}_{l}) \\over \\pi_{ref} (\\mathbf{y}_{l})}$ (see supplementary). Thus, in cases where winning and losing responses are semantically close, $\\beta$ values should be small and vice-versa. For automatic selection of $\\beta$, we propose $\\beta$ as an increasing function of (batch) normalized similarity score difference $\\Delta S$ between winning and losing scenarios: $\\beta = g(\\Delta S) = 0.9\\Delta S + 0.1$. For $\\beta_{Y}$ (Eq. 1), we use CLAP score differences, and for $\\beta_{V}$ and $\\beta_{A}$ (Eqs. 2 - 3), we use AV Similarity Metric (AVSM) [11, 12] differences as $\\Delta S$.\nAdditionally, DPO formulation waives the need for a separate reward model by directly learning a policy from collected preference data [56]. Consequently, such an approach"}, {"title": "D.4. Pseudocode for CAVPref", "content": "The training pseudocode for CAVPref is shown in Algorithm 1. We employ a multimodal DPO formulation and update the objective functions as outlined below."}, {"title": "E. Discussion on Bridging Networks", "content": "Bridge networks are modules used to connect the modality-specific encoders with the LLM by transforming the information from multi-modal encoders' space to LLM embedding space. For instance, VAST [8] uses text converters as the most basic and simplest bridge. Macaw-LLM uses a customized bridge network with linear layers and cross-attention-based alignment modules. VideoLLaMA(-2), Bay-CAT, video-SALMONN and X-InstructBLIP use Q-former-based bridge networks, whereas ChatBridge uses a customized perceiver network shared across all the modalities. OneLLM uses a mixture of projection experts equipped with a modality routing module, and ImageBind-LLM uses sophisticated trainable bind networks as the bridging module."}, {"title": "F. Performance with Different Model Variants", "content": "We experiment with the 7B and 13B variants of VideoLLaMA, PandaGPT, and X-InstructBLIP (other models employ a single variant). Experimental results confirm the performance boost with the 13B variants. A key observation is increasing the model size from 7B to 13B doesn't help in obtaining significant gain in Compositional reasoning suite of tasks. We hypothesize that LLMs are not able to capture the attribute level binding information and often work as bag-of-word models."}, {"title": "I. Common Sense Reasoning", "content": "Fig. 18 shows that the current AVLLMs lack commonsense reasoning. There is evidence in animal study [24] that it is a natural tendency of a dog to bark at an unknown cat. In this example (refer to video 7min 50sec) most AVLLMs fail to infer this and opts for incorrect response underlying their lack of commonsense reasoning skills."}]}