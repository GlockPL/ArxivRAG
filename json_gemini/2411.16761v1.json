{"title": "Is 'Right' Right? Enhancing Object Orientation Understanding in Multimodal Language Models through Egocentric Instruction Tuning", "authors": ["Ji Hyeok Jung", "Eun Tae Kim", "Seo Yeon Kim", "Joo Ho Lee", "Bumsoo Kim", "Buru Chang"], "abstract": "Multimodal large language models (MLLMs) act as essential interfaces, connecting humans with AI technologies in multimodal applications. However, current MLLMs face challenges in accurately interpreting object orientation in images due to inconsistent orientation annotations in training data, hindering the development of a coherent orientation understanding. To overcome this, we propose egocentric instruction tuning, which aligns MLLMs' orientation understanding with the user's perspective, based on a consistent annotation standard derived from the user's egocentric viewpoint. We first generate egocentric instruction data that leverages MLLMs' ability to recognize object details and applies prior knowledge for orientation understanding. Using this data, we perform instruction tuning to enhance the model's capability for accurate orientation interpretation. In addition, we introduce EgoOrientBench, a benchmark that evaluates MLLMs' orientation understanding across three tasks using images collected from diverse domains. Experimental results on this benchmark show that egocentric instruction tuning significantly improves orientation understanding without compromising overall MLLM performance. The instruction data and benchmark dataset are available on our project page at https://github.com/jhCOR/EgoOrientBench.", "sections": [{"title": "1. Introduction", "content": "Multimodal large language models (MLLMs) [8, 29, 52] can serve as interfaces connecting humans with AI technologies designed to perform multimodal tasks that require both image and text understanding. For instance, MLLMS are useful for operating autonomous systems [40, 47], manipulating robots [17, 21], and facilitating communication with Al assistants on AR devices [32, 34, 46]. To ensure consistent interaction, it is essential for MLLMs to exhibitbehaviors that align accurately with human intentions.\nHowever, MLLMs' limited understanding of object orientation complicates effective communication. As shown in Figure 1, current MLLMs struggle to accurately interpret the orientation an object is facing. This limitation can lead to errors in autonomous vehicle navigation, incorrect robot operations, and even severe accidents, creating a significant barrier to deploying MLLMs in real-world applications.\nIn this study, we identify inconsistent annotations in training data without standardized guidelines as a major factor hindering accurate orientation understanding. Unlike humans, who understand object orientation through an egocentric reference frame based on their physical bodies [19, 44, 51], MLLMs, lacking a physical body, rely"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Multimodal Large Language Model", "content": "Recently, numerous studies have concentrated on developing MLLMs that can effectively interpret both visual and textual data. For example, Flamingo [2] leverages visual and textual inputs as prompts, achieving notable few-shot performance in visual question answering. Models like"}, {"title": "2.2. Object Orientation Understanding", "content": "Research on object orientation understanding is actively pursued across various fields. Pose estimation [10, 33] is a fundamental task in interpreting object orientation in images. In autonomous driving, for instance, understanding pedestrian body orientation aids in predicting movement paths and reducing accident risk [11, 14, 16]. Recent studies have explored enhancing object orientation comprehension by incorporating viewpoint information. For example, DREAMFUSION [37] uses orientation data from multiple viewpoints to create enriched object descriptions, while EgoExoLearn [23] focuses on harmonizing orientation interpretation across varying perspectives.\nMore recently, MMVP [43] has underscored the need for object orientation understanding within MLLMs. To evaluate visual encoder representations, MMVP introduces a simple evaluation method and dataset that measure MLLMs' ability to interpret object orientation. However, this approach is limited by a small sample size of real-image data, which restricts comprehensive assessment across diverse image domains and various orientation complexities. To address this limitation, our study proposes a large-scale benchmark specifically designed to evaluate MLLMs' object orientation understanding across a broad range of domains and conditions, facilitating a more robust analysis of MLLM capabilities."}, {"title": "3. Egocentric Instruction Tuning", "content": ""}, {"title": "3.1. Egocentric Annotation", "content": "Motivation. Inconsistent annotations hinder MLLMs' ability to develop an understanding of object orientation. Therefore, consistent annotations should be used in MLLM training, which requires a standardized annotation guideline. Meanwhile, with the rise of embodied AI, numerous studies [5, 20, 49] have explored using AI technologies from a user's egocentric perspective. For instance, over ten egocentric video datasets, such as EgoExoLearn [23], Ego-CoT [32], and EgoSchema [31], have been introduced in the past year. Motivated by this trend, we standardize annotations based on the user's egocentric viewpoint to address the issue of inconsistent object orientation annotations. This standardization enhances the applicability of MLLMs in real-world, user-centered applications.\nOrientation Classes. We construct the egocentric instruction data using a consistent annotation rule. This rule categorizes object orientation into eight distinct egocentric classes (see Figure 4):\n\u2022 Front: the object faces the user (or camera).\n\u2022 Back: the object is turned away, facing the opposite direction from the user (or camera).\n\u2022 Left: the object is oriented to the left of the user.\n\u2022 Right: the object is oriented to the right of the user.\n\u2022 Front-Left, Front-Right: the object faces the user but is angled toward the left (or right).\n\u2022 Back-Left, Back-Right: the object faces away from the user but is angled toward the left (or right).\nThis classification scheme provides a structured framework to interpret object orientation consistently from a user-centered perspective."}, {"title": "3.2. Egocentric Instruction Data", "content": "To create the egocentric instruction dataset, we begin by manually annotating object orientations using the ImageNet data [9]. We then generate LLaVA-style instruction data [29] leveraging MLLM's ability to recognize image details and the LLM's prior knowledge to associate these details with object orientation. Using this dataset, we perform instruction tuning to enhance the MLLMs' ability to interpret object orientation while maintaining their general response generation capabilities.\nManual Data Collection. Among the datasets used for training MLLMs, none provide egocentric annotations for object orientation. To address this gap, we initiate the first effort to manually annotate object orientation in an egocentric context, creating a unique dataset specifically designed to enhance MLLMs' understanding of object orientation. We focus initially on single-object scenarios to establish a foundation for further exploration and development. Using ImageNet data [9] and the annotation scheme detailed"}, {"title": "3.3. Instruction Tuning", "content": "We conduct instruction tuning on the MLLM using the previously generated egocentric instruction data. During this process, we keep the visual encoder frozen and apply supervised fine-tuning to both the LLM and the bridge layer. To increase training efficiency, we employ LoRA [22] for parameter-efficient fine-tuning."}, {"title": "4. EgoOrientBench: Egocentric Orientation Understanding Benchmark", "content": ""}, {"title": "4.1. Data Collection.", "content": "MLLMs, known for their versatile capabilities, can be applied across various domains. They are valuable not only in realistic image contexts [32, 41] but also in applications like AI docents for artwork [25] and object recognition in augmented reality environments [38]. To evaluate MLLMs' ability to recognize object orientation across diverse settings, we construct a large-scale benchmark dataset, EgoOrientBench, for orientation understanding from a user-centered, egocentric perspective, using data"}, {"title": "4.2. Task Description.", "content": "We design three tasks to assess MLLMs' object orientation understanding at various levels\n\u2022 Choose: A simple classification task where the model selects an object's orientation from eight multiple-choice options to assess basic orientation understanding.\n\u2022 Verify: A binary classification task where the model determines whether an object's orientation matches a given query, instead of choosing from set options.\n\u2022 Freeform: Unlike the previous tasks, this task involves generating a descriptive response for an object's orientation, rather than selecting from options. GPT-4 then assesses the alignment between the generated description and the true orientation.\nEach task targets specific aspects of orientation understanding, while offsetting each other's limitations. For example, Choose is effective for basic orientation comprehension but limited by the division of continuous orientation into discrete classes. To overcome this, Verify tests the model's ability to match the given orientation in the query to an object in an image. Finally, since descriptive vocabulary can vary, Freeform provides flexible evaluation by generating descriptive orientation responses, matched to the true answer using GPT-4."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Experimental Setup", "content": "Backbone Models. To measure the current object orientation understanding of MLLMs and verify the effectiveness of our egocentric instruction tuning, we evaluate the following three MLLMs in EgoOrientBench: LLaVA 1.5 (w/ Vicuna-7B) [29], mPLUG-Owl2 (w/ LLaMA2-7B) [50], and InternVL2 (4B) [6]. Additionally, we also conduct evaluations for commercial MLLMs, GPT-40 (gpt-40-2024-08-06) [1] and Gemini-1.5 (Flash-8B) [42] using their API.\nBenchmarks. To verify the effectiveness of egocentric instruction tuning, we conduct experiments in EgoOrient-Bench to assess MLLMs' understanding of object orientation. Additionally, to determine whether egocentric instruction tuning enhances object orientation understanding without compromising general response generation, we evaluate performance on the MME benchmark [15], which measures MLLMs' overall response generation capabilities."}, {"title": "5.2. Experimental Result", "content": "Results on EgoOrientBench. Table 1 presents the evaluation results on EgoOrientBench, showing that our egocentric instruction tuning significantly enhances MLLMs' understanding of object orientation across all tasks. Specifically, all zero-shot MLLMs perform worse on the Choose task compared to the Popular method, which uses the most common orientation in each dataset as the default response. This performance gap stems from a strong orientation bias in current MLLMs on the Choose task, as analyzed in Section 5.3. The improvements achieved with our method suggest that it effectively mitigates this bias, resulting in better orientation comprehension. Although the commercial MLLM GPT-4o achieves the highest overall performance, our improved versions of LLaVA-1.5 and InternVL2 outperform another commercial model, Gemini-1.5, in all tasks. Note that this enhancement occurs even with the visualencoder frozen, without additional training. While previous studies have attributed MLLMs' limitations in object orientation understanding to the visual encoder's inability to represent orientation information [18, 43], our benchmark experiments demonstrate that substantial improvements can be achieved by aligning MLLM orientation interpretation with the user's egocentric perspective.\nResults on MME. Table 2 presents the experimental results on the MME benchmark. The results indicate that performance remains stable after applying our egocentric instruction tuning, with no significant degradation compared to pre-tuning levels. Notably, mPLUG-Owl2 shows improved performance in the Perception and Cognition categories. These findings demonstrate that our instruction tuning effectively aligns MLLM's object orientation understanding with the user's perspective while maintaining the model's overall response generation capabilities."}, {"title": "5.3. Analysis", "content": "Confusion Matrix. We analyze error cases before and after egocentric instruction tuning by examining the confusion matrix for the Choose task. As shown in Figure 5, LLaVA and mPLUG-Owl2 produce zero-shot predictions that are heavily biased toward the specific orientations, particularly the Front (F) and Front Right (FR) classes, indicating a significant deficiency in object orientation understanding in publicly available MLLMs. We also observe that this bias is mitigated through our proposed egocentric instruction tuning. Notably, there are several acceptable error cases, such as predicting objects in the Left (L) class as Front Left (FL) or interpreting Front Right (FR) objects as Front (F) or Right (R) classes. This confusion matrix analysis is consistent with the benchmark results in Table 1, further confirming that our tuning method effectively enhances object orientation understanding. We want to emphasize that our EgoOrientBench provides a foundation for analyzing MLLM behaviors related to object orientation, supporting future research efforts to enhance MLLM orientation comprehension. The confusion matrix analysis for InternVL2 is provided in Appendix C.1.\nAblation Test. Table 3 shows the ablation test results for the three response types in our egocentric instruction data. Using all three response types yields the highest performance, while removing any type results in a decrease in task accuracy, indicating that each type plays a role in enhancing MLLM's object orientation understanding. Notably, combining Type 1 and Type 2 leads to significant improvements across all tasks, suggesting a strong synergy between them."}, {"title": "6. Discussion", "content": ""}, {"title": "6.1. Applicability in Real-world Scenarios", "content": "Pedestrian Walking Direction Prediction. If the direction of a pedestrian is accurately determined, their path can be correctly predicted, which helps reduce traffic accidents [11, 14, 16]. To explore the potential of using MLLMs in pedestrian walking direction prediction, we perform a qualitative analysis using pedestrian walking direction prediction data [12].\nAs shown in Figure 6, when prompt about a pedestrian's direction, zero-shot LLaVA-1.5 produces responses that do not match the pedestrian's actual direction. On the other hand, with our egocentric instruction tuning, the model generates responses that accurately align with the correct direction. Additionally, our method not only accurately describes the orientation of objects (e.g., 'walking away from the camera') but also correctly represents the relationship between the object and its surroundings (e.g., 'towards the entrance'). Although our egocentric instruction tuning has not been specifically trained on pedestrian direction prediction tasks or images from that domain, it shows improved performance in this task, highlighting its practical utility.\nSpatial Reasoning. Recently, spatial reasoning research [7, 24] aimed at understanding spatial relationships between objects using MLLMs has been actively conducted. Here, we present that aligning object orientation understanding with the user's egocentric perspective, thereby enhancing the ability to interpret object orientation itself, can improve MLLM's spatial reasoning capabilities. To support this, we evaluate Zero-shot MLLMs and MLLMs enhanced with our egocentric instruction tuning using the COCO-Spatial evaluation dataset from prior research [24].\nAs shown in the Table 4, the experimental results demonstrate that our egocentric instruction tuning effectively improves spatial reasoning abilities for understanding preposition relationships between objects. Specifically, accuracy improved by 15.0%p, 14.3%p, and 3.4%p in LLaVA-1.5, mPLUG-Owl2, and InternVL2, respectively. This indicates that our approach can enhance MLLM performance in user-centered applications that require recognizing object positions, understanding orientations, and interpreting inter-object relationships."}, {"title": "6.2. Limitations", "content": "Figure 7 shows examples of error cases for LLaVA-1.5 after applying our egocentric instruction tuning. In the image, the zebra and giraffe have different orientations for their bodies and heads. Unlike inanimate objects like cars or tractors, living objects such as zebras and humans can exhibit these complex, independent orientations. These complexities challenge our Response Type 2 data, which connects object detail recognition with orientation understanding\u2014for instance, when the head and tail imply different orientations. They also complicate Response Type 3 data, which relies on interpreting orientation relationships for alignment tasks. In future work, we plan to address these challenges by moving beyond the current discrete orientation categories."}, {"title": "7. Conclusion", "content": "In this study, we identify inconsistent object orientation annotations as a primary factor hindering MLLM's orientation understanding and propose an Egocentric Instruction Tuning method to address this by aligning MLLM's object orientation comprehension with the user's egocentric perspective. Additionally, we introduce EgoOrientBench, a benchmark designed to comprehensively evaluate MLLM's orientation understanding across three tasks and five image datasets. Experimental results on this benchmark, as well as on the MME benchmark, demonstrate that our egocentric instruction tuning effectively enhances object orientation understanding without compromising general response generation capabilities. Our analysis also reveals that current MLLMs exhibit a strong bias toward specific orientations, which our instruction tuning mitigates. Finally, we apply the enhanced MLLM, trained with egocentric instruction tuning, to pedestrian walking direction prediction and spatial reasoning tasks involving spatial relationships between objects. This demonstrates the practical benefits of enhanced orientation understanding for real-world applications. We believe our instruction tuning method and benchmark will significantly contribute to future research on MLLM's object orientation understanding."}, {"title": "Appendix", "content": ""}, {"title": "A. Data Collection", "content": ""}, {"title": "A.1. Instruction Data Generation", "content": "We generate three types of data using the inference capabilities of the LLaVA1.5 Vicuna 13B model [29]. The model is prompted to produce questions and answers based on a given context, and the outputs are parsed to construct instruction learning data. The prompts corresponding to each data type are described in Table 11."}, {"title": "A.2. Data Statistics", "content": "Training Dataset. We collect data from the ImageNet dataset [9]. We then remove ambiguous images for determining orientations, resulting in a dataset of 2,845 images. To ensure a uniform number of evaluation data per orientation class, we separate benchmark data from the source dataset by extracting 50 images per orientation. The remaining 2,445 images are used to construct the training dataset. For each image, we generate three questions corresponding to the three types of data described earlier. As a result, the training dataset consists of a total of 2,445 images, with three instruction data points generated per image, leading to a total of 7,335 data points. The statistics of the training data is shown in Table 5."}, {"title": "A.3. Task Data Details", "content": "\u2022 Choose: We prompt the model to select the direction an object in the image is facing from among eight directional options. This task is designed to evaluate the basic capability of MLLMs to recognize general directions.\n\u2022 Verify: In this task, there may be a bias towards \"yes\" or \"no\" answers, which could result in evaluation metrics failing to accurately represent the actual performance. To address this issue, we design two separate verification tasks for each image: one where the correct answer is \"yes\" and another where the expected answer is \u201cno.\u201d Specifically, for a given image, if the orientation of the image is labeled as \"right,\" we first construct a question asking whether the subject is facing \"right.\" Then, we randomly select one of the remaining orientations (excluding \"right\" from the eight possible orientations) and create a question asking whether the subject is facing the selected orientation.\n\u2022 Freeform: We recognize that expressions for orientation can vary significantly. For example, phrases like \u201ctoward the front,\u201d \u201cfacing forward,\u201d or \u201clooking ahead\u201d may all describe the same orientation. To account for these variations, we allow free-form responses and include a \u201cfreeform\u201d metric, verified using the GPT-4o API (gpt-40-2024-08-06), as an additional evaluation measure."}, {"title": "A.4. 3D Rendered Images", "content": "We utilize the OmniObject3D dataset [48], which includes high-quality, real-scanned 3D objects designed to advance 3D perception, reconstruction, and generation tasks in real-world scenarios. We use a total of 500 3D scans distributed across 11 object categories. Each scan is oriented along a specific axis (+x, -x, +y, -y, +z, \u2212z), and we manually unify the coordinate system to standardize their orientations. Subsequently, we place eight cameras (Front-Left, Front, Front-Right, Left, Right, Back-Left, Back, Back-Right) around the object at 45\u00b0 intervals with respect to the object's center. For the front and back views, the cameras are tilted downward by approximately 20\u00b0 to better capture the object's orientation. Through this process, we render a total of 4,000 RGB images with a 448\u00d7448 image resolution."}, {"title": "B. Experimental Details", "content": "We use GPT-40 to evaluate the models on the Freeform task and spatial reasoning. The prompts used for GPT-40 are detailed in Table 9."}, {"title": "C. Further Analysis", "content": ""}, {"title": "C.1. Confusion Matrix", "content": "We draw confusion matrix for InternVL [6] in Figure 8. As with the confusion matrices of other models, it can be observed that the responses of the model become more aligned after training, and the biases are mitigated."}, {"title": "C.2. Ablation Test", "content": "Table 10 shows the additional results of ablation tests for mPLUG-Owl2 [50] and InternVL [6]. Although not all data types show the same upward trend across every metric for all models, one consistent observation is that the highest performance is achieved when all three response types are utilized. In all tested models, removing any response type resulted in a decrease in task accuracy, confirming that each"}]}