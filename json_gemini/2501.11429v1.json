{"title": "The Explanation Game \u2013 Rekindled\n(Extended Version)", "authors": ["Joao Marques-Silva", "Xuanxiang Huang", "Olivier L\u00e9toff\u00e9"], "abstract": "Recent work demonstrated the existence of critical flaws in the\ncurrent use of Shapley values in explainable AI (XAI), i.e. the so-\ncalled SHAP scores. These flaws are significant in that the scores\nprovided to a human decision-maker can be misleading. Although\nthese negative results might appear to indicate that Shapley values\nought not be used in XAI, this paper argues otherwise. Concretely,\nthis paper proposes a novel definition of SHAP scores that over-\ncomes existing flaws. Furthermore, the paper outlines a practically\nefficient solution for the rigorous estimation of the novel SHAP\nscores. Preliminary experimental results confirm our claims, and\nfurther underscore the flaws of the current SHAP scores.", "sections": [{"title": "1 Introduction", "content": "The importance of explainable artificial intelligence (XAI) cannot\nbe overstated, being widely accepted as a fundamental pillar of\ntrustworthy AI. Both in the European Union (EU) and the United\nStates (US), there has been recent legislation aiming at regulating\nthe use of systems of AI, especially in domains that directly impact\nhumans. 1,2 Nevertheless, some experts have warned of existential\nrisks caused by AI.3 Similarly, recent studies contemplate existential\nrisks caused by the advances in AI.4 As a result, XAI is expected to\nprovide critical support in attaining much-needed trust in systems\nof AI.\nXAI methods can be broadly categorized into those based on\nfeature attribution and those based on feature selection. Feature\nattribution aims at assigning degrees of importance to features,\nthereby enabling the ranking of the relative importance of fea-\ntures. In contrast, feature selection aims at selecting subsets of\nfeatures, each encoding a rule for some prediction. For example,\nAnchors [Ribeiro et al. 2018] exemplifies XAI by feature selection,\nwhile LIME [Ribeiro et al. 2016] exemplifies XAI by feature attribu-\ntion. Moreover, whereas some works are model-agnostic [Ribeiro\net al. 2016, 2018], others require access to the machine learning\n(ML) models [Bach et al. 2015]. Despite the importance of XAI,\nmost past efforts are characterized by lack of rigor [Ignatiev 2020;\nMarques-Silva 2024; Zhang et al. 2024]. As a consequence, most\nmethods of XAI in current use are unworthy of trust [Weber et al.\n2024].\nThe tool SHAP [Lundberg and Lee 2017] is arguably the most\npopular method of XAI by feature attribution. As its core, the tool\nSHAP approximates Shapley values [Shapley 1953] based on a def-\nnition that is standard in XAI [Strumbelj and Kononenko 2010,\n2014]. Shapley values are defined on a game, which consists of\na set of elements (e.g. voters) and a given characteristic function\nthat maps subsets of a set of elements (e.g. players or voters in\ngame theory, or features in an ML model) to the reals. Depending\non the characteristic function used, different Shapley values are\nobtained. However, for all chosen characteristic functions, the ob-\ntained values respect the key axioms of Shapley values [Shapley\n1953]. In the case of game theory, concretely when measuring a\npriori voting power, Shapley values are instantiated starting from a\ngiven characteristic function, which was first proposed in 1954 by\nShapley & Shubik [Shapley and Shubik 1954]. The same character-\nistic function has been used, explicitly or implicitly, in most other\nproposals of power indices, i.e. measures of relative voting power.\nThis is the case with the well-known Banzhaf, Deegan-Packel and\nHoller-Packel indices [Felsenthal and Machover 1998; Letoffe et al.\n2024], among many others.\nIn the case of XAI, the tool SHAP is based on a concrete instan-\ntiation of Shapley values, one that uses a specific characteristic\nfunction. However, this characteristic function bears no relation-\nship with those that have been used in a priori voting power since\nthe 1950s [Felsenthal and Machover 1998]. The theoretical SHAP\nscores denote the values that the tool SHAP only approximates,"}, {"title": "2 Definitions", "content": "This section adapts and extends the most recent notation used\nin the field of logic-based explainability [Marques-Silva 2024].\nClassification & regression problems. Let F = {1, ..., m} de-\nnote a set of features. Each feature i \u2208 F takes values from a\ndomain Di. Domains can be categorical or ordinal. If ordinal, do-\nmains can be discrete or real-valued. Feature space is defined by\nF = D1\u00d7D2X...Dm. Throughout the paper domains are assumed\nto be discrete-valued. Thus, for real-valued features, some sort of\nfinite discretization is assumed. The notation x = (x1,...,xm) de-\nnotes an arbitrary point in feature space, where each xi is a variable\ntaking values from Di. Moreover, the notation v = (v1, ..., vm) rep-\nresents a specific point in feature space, where each vi is a constant\nrepresenting one concrete value from Di. A classifier maps each\npoint in feature space to a class taken from K = {C1, C2, ..., CK}.\nClasses can also be categorical or ordinal. However, and unless\notherwise stated, classes are assumed to be ordinal. In the case\nof regression, each point in feature space is mapped to an ordi-\nnal value taken from a set of values V, e.g. V could denote Z or\nR. Therefore, a classifier Mc is characterized by a non-constant\nclassification function k that maps feature space F into the set of\nclasses K, \u0456.\u0435. \u03ba: F \u2192 K. A regression model MR is characterized\nby a non-constant regression function p that maps feature space F\ninto the set elements from V, i.e. p: F \u2192 V. A classifier model Mc\nis represented by a tuple (F, F, K, \u03ba), whereas a regression model\nMR is represented by a tuple (F, F, V, p). When viable, we will\nrepresent an ML model M by a tuple (F, F, \u03a4, \u03c0), with prediction\n\u03c0 : F \u2192 T, without specifying whether M denotes a classification\nor a regression model, and where T can either be K or V. An in-\nstance denotes a pair (v, q), where v \u2208 F, q \u2208 \u03a4, with q = \u03c0(v). An\nexplanation problem is a tuple & = (M, (v, q)), where M is some\nML model and (v, q) is a target instance.\nSimilarity relational operators. For each feature i \u2208 F, we will\nassume a user-specified similarity relational operator (a predicate)\n\u2248 : D\u00a1 \u00d7 Di\u2192 {T, 1}, such that xi \u2248 vi holds true (T) if the\nvalues xi and vi are deemed sufficiently close to each other. For\nexample, for categorical features we might require that similarity\nshould mean that xi and vi represent the same value. In contrast,\nfor real-valued features, we might accept a small difference in value,\nwith different measures of absolute or relative change envisaged.7\nMoreover, the similarity relational operator can be generalized as\nfollows. Given x, v \u2208 F, we define similarity between the features\nin some set S CF:\nXS \u2248 VS := Aies Xi \u2248 Vi\nwhen S = F, we write x \u2248 v, for simplicity. Finally, a similarity\nrelational operator is also assumed for the prediction, such that\n\u03c0(x) \u2248 \u03c0(v) holds when \u03c0(x) is sufficiently close to \u03c0(v).\nLogic-based explainability. Abductive and contrastive explana-\ntions (resp. AXps/CXps) are examples of formal explanations for\nclassification problems [Darwiche 2023; Marques-Silva 2022]. As\nargued in recent work [Marques-Silva 2024], the same concepts can\nbe generalized to the case of regression problems. The presentation\nbelow just requires a well-defined similarity operator.\nA weak (i.e. non-subset minimal) abductive explanation (WAXp)\ndenotes a set of features S CF, such that for every point in feature\nspace the ML model output is similar to the given instance: (v, q).\nThe condition for a set of features S C F to represent a WAXp\n(which also defines a corresponding predicate WAXp) is as follows:\nWAXp(S; 8) := \u221a(x \u2208 F).(xs \u2248 vs) \u2192(\u03c0(x) \u2248 \u03c0(v))\nFurthermore, an AXp is a subset-minimal WAXp."}, {"title": "3 The Flaws of SHAP\u0164 Scores", "content": "The fact that theoretical SHAP (i.e. SHAPT) scores use the ex-\npected value of an ML model raises a number of critical issues. First,\nin the case of classification, where classes are categorical, the ex-\npected value is ill-defined. Unfortunately, the use of expected values\nis more problematic. This section overviews the known flaws of\nSHAPT scores, and briefly discusses generalizations of those flaws\nto the case of regression problems."}, {"title": "3.1 Known Flaws of SHAP\u0164 Scores", "content": "It is reasonably simple to show that SHAPT scores can be mis-\nleading. The key insight is that one can create examples of ML\nmodels where feature influence for a prediction is self-evident, and\nthen force the computation of the SHAPT scores to produce mis-\nleading values by assigning no importance to influent features, and\nby assigning some importance to non-influent features. This basic\ninsight can then be used to identify different issues where mislead-\ning information is clearly unsatisfactory. As a consequence, the\ntool SHAP [Lundberg and Lee 2017] aims to approximate values\nthat can be misleading.\nAlthough these observations have gone unnoticed in the many\nthousands of publications that build on or exploit SHAP scores,\nsimple examples serve to illustrate what the critical limitation is.\nSince SHAPT scores use a characteristic function that computes\nexpected values, then one can use regions of the feature space,\nthat are non-interesting in terms of the prediction, to influence the\ncomputation of the expected values. Then, one is capable of modify-\ning the theoretical SHAP scores as one wishes, thereby destroying\nany correlation that SHAPT scores might have with respect to real\nfeature influence."}, {"title": "3.2 SHAPT Scores Flawed Beyond Classification", "content": "A possible criticism of our earlier work [Huang and Marques-\nSilva 2024; Marques-Silva and Huang 2024] is that the use of classi-\nfiers is somewhat artificial, since we impose that classes be viewed\nas ordinal values. The use of some regression models, e.g. regression\ntrees, can also be criticized, since the classifier simply computes\na constant number of fixed values. Nevertheless, one can devise\nregression models for which a non-countable number of values can\nbe predicted, and for which the SHAPT scores are again misleading."}, {"title": "4 New SHAP Scores & Implementation", "content": "Recent work dissected the causes for the flaws of the theoretical\nSHAP scores (i.e. SHAPT) [Letoffe et al. 2024, 2025]. Concretely,\nthese flaws result solely from the choice of characteristic func-\ntion. This characteristic function is used for example in the SHAP\ntool [Lundberg and Lee 2017], but was studied in earlier works\n[Strumbelj and Kononenko 2010, 2014]. More importantly, this\nrecent work [Letoffe et al. 2024, 2025] proposed alternative charac-\nteristic functions that eliminate all of the issues reported in earlier\nworks [Huang and Marques-Silva 2024; Marques-Silva and Huang\n2024]. Accordingly, in this paper we consider the characteristic\nfunction va: 2 \u2192 R, defined as follows:"}, {"title": "4.1 Selecting the Characteristic Function", "content": "va(S) = {0  if WAXp(S), otherwise}\nThe novel characteristic function is inspired by those commonly\nused in game theory [Shapley and Shubik 1954]. The motivation\nthen and now is to assign importance to the elements (features or\nvoters) which are critical for changing the value of a decision of\ninterest. In the case of voting power, one assigns importance to\nvoters that cause coalitions to become winning when the voter is\nincluded, and that are losing when the voter is not included. In\nthe case of explainability, one assigns importance to a feature that\ncauses fixed sets of features to be sufficient for the prediction when\nthe feature is also fixed, and that causes fixed sets of features not\nto be sufficient for the prediction when the feature is not fixed.\nIt is clear that, given the novel characteristic function va, we can\nuse the definition of Shapley values to obtain SHAP scores different\nfrom those obtained with ve. As noted earlier, the new SHAP scores\nwill be referred to by the acronym vSHAPT; their properties are\nthe topic of recent work [Letoffe et al. 2024, 2025].\nIt is also apparent that we impose few constraints on the ML\nmodels for which the novel SHAP scores can be computed. We just\nneed to be able to decide whether a set of selected (and so fixed)\nfeatures is sufficient for predicting the value we are interested in.\nIt should also be underscored that the similarity predicate allows\nus to consider both classification and regression models. Finally,\nas underscored in recent work [Letoffe et al. 2025], the proposed\ncharacteristic function reveals a fundamental relationship between\nexplanations based on feature selection, and those based on feature\nattribution. The following result will be used in later sections, and\nit is a corollary of [Huang et al. 2023, Proposition 3]."}, {"title": "4.2 Rigorous Estimation of Shapley Values", "content": "The complexity of rigorously computing Shapley values is well-\nknown to be unwieldy [Arenas et al. 2023; den Broeck et al. 2022].\nAs a result, except for small examples or special tractable cases,\nthe most common solution is to approximate the Shapley value. In\ncontrast with earlier works on applying Shapley values in XAI, we\nopt to estimate the Shapley value using an algorithm known for its\nstrong theoretical guarantees [Castro et al. 2009]. (This algorithm\nwill be referred to as CGT, and is shown as Algorithm 1.) The\ninputs to the algorithm consist of a game [Chalkiadakis et al. 2012],\ncharacterized by a number of elements and a characteristic function\nv : 2N \u2192 R, such that v(0) = 0, and also the values of e and a. e\ndenotes the error of Shapley value estimation, that one wants to\nguarantee with probability 1 \u2013 a. The algorithm executes a number\nr of runs of a basic estimation procedure, such that Pr(|Sci - Sci \u2264\n\u0454) \u2265 1 \u2013 a, where Sc\u012f denotes the exact Shapley value for feature\ni, and Sci denotes the estimate of the Shapley value. r denotes the\nnumber of times the main loop of the CGT algorithm is executed;\nits estimation is detailed in [Castro et al. 2009]. The algorithm\niteratively picks a permutation of N from the set of permutations\n(N). Each permutation is then used for defining a total of n sets\nof features, each with a different size, for each of which the value\nof A\u00a1 is computed and used for updating Sc.\nBy inspection, it is clear that Algorithm 1 runs in polynomial\ntime [Castro et al. 2009], if u is also evaluated in polynomial-time.\nAlso, several low-level optimizations can be envisioned; these are\nbeyond the scope of this paper. Furthermore, two observations are\nin order. A first observation is that, given va, we have in fact a simple\ngame, because va is monotonically increasing. A second key obser-\nvation is that, because of Proposition 1, \u0394\u00a1 is never incremented\nby Algorithm 1 when i is irrelevant. As a result, the SHAP score\nfor irrelevant features will always be 0, i.e. if i is irrelevant, then"}, {"title": "4.3 Deciding Weak Abductive Explanations", "content": "A key component of estimating rigorous SHAP scores is the\nability to efficiently decide whether a set of features is a weak A\u0425\u0440.\nClearly, logic-based computation of explanations can be used in this\ncontext [Marques-Silva 2022, 2023, 2024]. However, for large-scale\ndatasets and respective ML models, this approach is unlikely to\nscale. First, there is the complexity of reasoning, which is still a\nchallenge for highly complex ML models despite recently observed\nprogress [Izza et al. 2024]. Second, even if deciding whether a set\nof features is a weak AXp could be done reasonably efficiently, the\nvery large number of tests that must be considered for estimating\nShapley values would likely make the running times prohibitively\nlarge. The consequence is that deciding whether a set of features\nis a weak AXp should be attained with a negligible running time,\nideally with polynomial-time guarantees.\nTherefore, instead of exploiting a rigorous model-based approach\nfor explaining an ML model, we consider instead a rigorous data-\nbased approach. Concretely, we propose instead to exploit sample-\nbased explanations [Cooper and Amgoud 2023] (sbXps). sbXps are\ncomputed with respect to a sample of the feature space, i.e. a set\nof instances, and are rigorous with respect to that sample. For\nexample, the sample might be the original dataset, or consist of the\nsampling carried out by tools such as LIME [Ribeiro et al. 2016],\nSHAP [Lundberg and Lee 2017] or Anchor [Ribeiro et al. 2018],\nor an aggregation of both. However, sbXps are rigorous given the\nsample. Thus, in cases where the sample is the feature space, sbXps\nmatch AXps/CXps.\nA sample space S is a subset of feature space, i.e. SC F. (We refer\nto a sample space as a dataset when the prediction is known for\neach point in the sample space.) A set of features X C F is a weak\nsbAXp if,\nV(x \u2208 S). (xx \u2248 vx) \u2192(\u03c0(x) \u2248 \u03c0(v))\nIf no proper subset of X is also a weak sbAXp, then X is declared\nan sbAXp. Similarly, a set of features y C F is a weak sbCXp if,\n\u2203(x \u20ac $). (xf\\y \u2248 vF\\y) ^ (\u03c0(x) \u2260 \u03c0(v))\nIf no proper subset of y is a weak sbCXp, then y is declared an\nsbCXp.13\nGiven a sample space, there exists a polynomial-time algorithm\nfor computing an sbAXp, and so for deciding whether a set of\nfeatures is an sbWAXp [Cooper and Amgoud 2023]. (For the case\nof complete truth tables, polynomial-time algorithms are also de-\nscribed in [Huang and Marques-Silva 2023].) It is straightforward\nto devise a polynomial-time algorithm for deciding whether a set\nof features W is an sbWAXp. One simple algorithm is outlined"}, {"title": "5 Experimental Evidence", "content": "nuSHAP & experimental procedure. nuSHAP is a novel proto-\ntype explainer by feature attribution, that implements the ideas de-\ntailed in the previous section. Shapley values are estimated with the\nwell-known CGT algorithm [Castro et al. 2009] (see Algorithm 1).\nThe characteristic function used is va [Letoffe et al. 2025]. For scal-\nability, instead of employing standard model-based explanations,\nthe characteristic function va is defined in terms of sample-based\nexplanations, building on recent work [Cooper and Amgoud 2023].\nSample-based weak AXps are decided with Algorithm 2. The ex-\nperiments are organized into (i) analysis of boolean functions; and\n(ii) direct comparison with the existing SHAP tool. All experiments\nwere run on a MacBook Pro with a 6-Core Intel Core i7 2.6 GHz\nprocessor with 16 GByte RAM, running macOS Sonoma.\nBoolean functions. As underscored earlier in the paper, the CGT\nalgorithm together with va guarantees that irrelevant features are\nnever assigned an estimate of their Shapley value other than 0.\nTherefore, any of the issues studied in earlier work [Huang and\nMarques-Silva 2024; Marques-Silva and Huang 2024] do not occur.\nTabular & image data \u2013 SHAP samples. The second experiment\naims at assessing the quality of the SHAP tool [Lundberg and Lee\n2017] at ranking features in terms of their importance for a given\nprediction. To ensure a fair comparison, the sampling performed\nby the tool SHAP was recorded. This sampling was then used by\nnuSHAP for computing the vSHAPE scores.\nThe tools SHAP14 and nuSHAP were assessed on several well-\nknown classifiers [Zhou 2021], namely: logistic regression (LR),\ndecision tree (DT), k-nearest neighbors (kNN) classifier, boosted"}, {"title": "6 Discussion", "content": "Recent work [Huang and Marques-Silva 2024; Marques-Silva and\nHuang 2024] revealed critical flaws with the definition of SHAP\nscores [Lundberg and Lee 2017], i.e. the proposed use of Shapley\nvalues [Shapley 1953] in XAI. Concretely, this work constructed\nclassifiers for which the theoretical SHAP scores convey misleading\ninformation. This implies that the tool SHAP [Lundberg and Lee\n2017] approximates scores that can be misleading. As a consequence,\nthe identified flaws question the conclusions from thousands of\npapers that build on the tool SHAP.\nMotivated by these negative results, a number of more recent\nworks proposed alternatives to the use of SHAP scores [Biradar et al.\n2024; Yu et al. 2024]. Unfortunately, these alternatives are not based\non Shapley values, and so they can also produce unsatisfactory\nresults [Letoffe et al. 2024].\nIn contrast, this paper proposes a novel solution, one that tar-\ngets the efficient computation of a novel definition of SHAP scores,\nthereby addressing the known flaws of the SHAP scores in current\nuse [Huang and Marques-Silva 2024; Letoffe et al. 2025; Marques-\nSilva and Huang 2024]. The experiments confirm the scalability\nof the proposed solution, and provide further evidence of SHAP's\nflaws, not only in theory, but also in practice. Concretely, the ex-\nperiments demonstrate that the measures of feature importance\nobtained with the tool SHAP are misleading in almost all of the\ntests considered. The lack of quality in the rankings of the scores\nobtained with the tool SHAP is always observable, and most often\nvery significant, confirming that these scores can be fairly unrelated\nwith those obtained with the novel non-misleading definition of\nSHAP scores."}]}