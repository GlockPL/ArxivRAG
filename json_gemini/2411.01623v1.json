{"title": "FilterNet: Harnessing Frequency Filters for Time Series Forecasting", "authors": ["Kun Yi", "Jingru Fei", "Qi Zhang", "Hui He", "Shufeng Hao", "Defu Lian", "Wei Fan"], "abstract": "Given the ubiquitous presence of time series data across various domains, precise forecasting of time series holds significant importance and finds widespread real-world applications such as energy, weather, healthcare, etc. While numerous forecasters have been proposed using different network architectures, the Transformer-based models have state-of-the-art performance in time series forecasting. However, forecasters based on Transformers are still suffering from vulnerability to high-frequency signals, efficiency in computation, and bottleneck in full-spectrum utilization, which essentially are the cornerstones for accurately predicting time series with thousands of points. In this paper, we explore a novel perspective of enlightening signal processing for deep time series forecasting. Inspired by the filtering process, we introduce one simple yet effective network, namely FilterNet, built upon our proposed learnable frequency filters to extract key informative temporal patterns by selectively passing or attenuating certain components of time series signals. Concretely, we propose two kinds of learnable filters in the FilterNet: (i) Plain shaping filter, that adopts a universal frequency kernel for signal filtering and temporal modeling; (ii) Contextual shaping filter, that utilizes filtered frequencies examined in terms of its compatibility with input signals for dependency learning. Equipped with the two filters, FilterNet can approximately surrogate the linear and attention mappings widely adopted in time series literature, while enjoying superb abilities in handling high-frequency noises and utilizing the whole frequency spectrum that is beneficial for forecasting. Finally, we conduct extensive experiments on eight time series forecasting benchmarks, and experimental results have demonstrated our superior performance in terms of both effectiveness and efficiency compared with state-of-the-art methods. Our code is available at 1.", "sections": [{"title": "Introduction", "content": "Time series forecasting has been playing a pivotal role across a multitude of contemporary applications, spanning diverse domains such as climate analysis [1], energy production [2], traffic flow patterns [3], financial markets [4], and various industrial systems [5]. The ubiquity and profound significance of time series data has recently garnered substantial research efforts, culminating in a plethora of deep learning forecasting models [6] that have significantly enhanced the domain of time series forecasting.\nPreviously, leveraging different kinds of deep neural networks derives a series of time series forecasting methods, such as Recurrent Neural Network-based methods including DeepAR [7], LSTNet [8],"}, {"title": "Related Work", "content": "Deep Learning-based Time Series Forecasting In recent years, deep learning-based methods have gained prominence in time series forecasting due to their ability to capture nonlinear and complex correlations [21]. These methods have employed various network architectures to learn temporal dependencies, such as RNN [8, 7], TCN [9, 10], etc. Notably, MLP- and Transformer-based methods have achieved competitive performance, emerging as state-of-the-art approaches. N-HiTS [22] integrates multi-rate input sampling and hierarchical interpolation with MLPs to enhance univariate forecasting. DLinear [12] introduces a simple approach using a single-layer linear model to capture temporal relationships between input and output time series data. RLinear [23] utilizes linear mapping to model periodic features, demonstrating robustness across diverse periods with increasing input length. In contrast to the simple structure of MLPs, Transformer's advanced attention mechanisms [24] empower the models [14, 15, 25, 26] to capture intricate dependencies and long-range interactions. PatchTST [16] segments time series into patches as input tokens to the Transformer and maintaining channel independence. iTransformer [17] inverts the Transformer's structure by treating independent series as variate tokens to capture multivariate correlations through attention.\nTime Series Modeling with Frequency Learning In recent developments, frequency technology has been increasingly integrated into deep learning models, significantly improving state-of-the-art accuracy and efficiency in time series analysis [27]. These models leverage the benefits of frequency technology, such as high efficiency [28, 29] and energy compaction [13], to enhance forecasting capabilities. Concretely, Autoformer [15] introduces the auto-correlation mechanism, improving self-attention implemented with Fast Fourier Transforms (FFT). FEDformer [25] enhances attention with a FFT-based frequency approach, determining attentive weights from query and key spectrums and conducting weighted summation in the frequency domain. DEPTS [30] utilizes Discrete Fourier Transform (DFT) to extract periodic patterns and contribute them to forecasting. FiLM [31] employs Fourier analysis to retain historical information while filtering out noisy signals. FreTS [13] introduces a frequency-domain Multi-Layer Perceptrons (MLPs) to learn channel and temporal dependencies. FourierGNN [29] transfers the operations of graph neural networks (GNNs) from the time domain to the frequency domain. FITS [32] applies a low pass filter to the input data followed by complex-valued linear mapping in the frequency domain.\nUnlike these methods, in this paper we propose a simple yet effective model FilterNet developed from a signal processing perspective, and apply all-pass frequency filters to design the network directly, rather than incorporating them into other network architectures, such as Transformers, MLPs, or GNNs, or utilizing them as low-pass filters, as done in FITS [32] and FiLM [31]."}, {"title": "Preliminaries", "content": "Frequency Filters Frequency filters [33] are mathematical operators designed to modify the spectral content of signals. Specifically, given an input time series signal x[n] with its corresponding Fourier transform X[k], a frequency filter H[k] is applied to the signal to produce an output signal y[n] with its corresponding Fourier transform Y[k] = X[k]H[k]. The frequency filter H[k] alters the amplitude and phase of specific frequency components in the input time series signal x[n] according to its frequency response characteristics, thereby shaping the spectral content of the output signal.\nAccording to the Convolution Theorem [34], the point-wise multiplication in the frequency domain corresponds to the circular convolution operation between two corresponding signals in the time domain. Consequently, the frequency filter process can be expressed in the time domain as:\n$Y[k] = H[k]X[k] \\leftrightarrow y[n] = h[n] \\otimes x[n]$,\nwhere h[n] is the inverse Fourier transform of H[k] and $\\otimes$ denotes the circular convolution operation. This formulation underscores the intrinsic connections between the frequency filter process and the circular convolution in the time domain, and it indicates that the frequency filter process can efficiently perform circular convolution operations. In time series forecasting, Transformer-based methods have achieved state-of-the-art performance, largely due to the self-attention mechanism [14, 15, 25, 17], which can be interpreted as a form of global circular convolution [35]. This perspective opens up the possibility of integrating frequency filter technologies, which are well-known for their ability to isolate and enhance specific signal components, to further improve time series forecasting models."}, {"title": "Methodology", "content": "As aforementioned, frequency filters enjoy numerous advantageous properties for time series forecasting, functioning equivalently to circular convolution operations in the time domain. Therefore, we design the time series forecaster from the perspective of frequency filters. In this regard, we propose FilterNet, a forecasting architecture grounded in frequency filters. First, we introduce the overall architecture of FilterNet in Section 4.1, which primarily comprises the basic blocks and the frequency filter block. Second, we delve into the details of two types of frequency filter blocks: the plain shaping filter presented in Section 4.2 and the contextual shaping filter discussed in Section 4.3."}, {"title": "Overview", "content": "The overall architecture of FilterNet is depicted in Figure 2, which mainly consists of the instance normalization part, the frequency filter block, and the feed-forward network. Specifically, for a given time series input $X = [X_1^L, X_2^L, ..., X_N^L] \\in \\mathbb{R}^{N \\times L}$ with the number of variables N and the lookback window length L, where $X_N^L \\in \\mathbb{R}^L$ denotes the N-th variable, we employ FilterNet to predict the future $\\tau$ time steps $Y = [X_1^{L+1:L+\\tau}, X_2^{L+1:L+\\tau}, ..., X_N^{L+1:L+\\tau}] \\in \\mathbb{R}^{N \\times \\tau}$. We provide further analysis about the architecture design of FilterNet in Appendix A.\nInstance Normalization Non-stationarity is widely existing in time series data and poses a crucial challenge for accurate forecasting [19, 36]. Considering that time series data are typically collected over a long duration, these non-stationary sequences inevitably expose forecasting models to distribution shifts over time. Such shifts can result in performance degradation during testing due to the covariate shift or the conditional shift [37]. To address this problem, we utilize an instance normalization method, denoted as Norm, on the time series input X, which can be formulated as:\n$Norm(X) = [\\frac{X_N^L - Mean(X_N^L)}{Std_N(X_N^L)}]_1^N,$\nwhere $Mean_N$ denotes the operation that calculates the mean value along the time dimension, and $Std_N$ represents the operation that calculates the standard deviation along the time dimension.\nCorrespondingly, the inverse instance normalization, denoted as InverseNorm, is formulated as:\n$InverseNorm(P) = [P^{L+1:L+\\tau} \\times Std_N(X_N^L) + Mean_N(X_N^L)]_1^N,$\nwhere $P = [P_1^{L+1:L+\\tau}, P_2^{L+1:L+\\tau}, ..., P_N^{L+1:L+\\tau}] \\in \\mathbb{R}^{N \\times \\tau}$ are the predictive values."}, {"title": "Plain Shaping Filter", "content": "PaiFilter instantiates the frequency filter by randomly initializing learnable parameters and then performing multiplication with the input time series. In general, for multivariate time series data, the channel-independence strategy in channel modeling has proven to be more effective compared to the channel-mixing strategy [12, 16]. Following this principle, we also adopt the channel-independence strategy for designing the frequency filter. Specifically, we propose two types of plain shaping filters: the universal type, where parameters are shared across different channels, and the individual type, where parameters are unique to each channel, as illustrated in Figure 3(a).\nGiven the time series input $Z \\in \\mathbb{R}^{N \\times L}$ and the plain shaping filter $H_\\phi$, we apply PaiFilter by:\n$Z = F(Z)$,\n$S = Z \\odot_L H_\\phi, H_\\phi \\in \\{H_\\phi^{(Uni)}, H_\\phi^{(Ind)}\\},\n$S = F^{-1}(S)$,"}, {"title": "Contextual Shaping Filter", "content": "In contrast to PaiFilter, which randomly initializes the parameters of frequency filters and fixes them after training, TexFilter learns the parameters generated from the input data, allowing for better adaptation to the data. Consequently, we devise a neural network $H_\\varphi$ that flexibly adjusts the frequency filter in response to the input data, as depicted in Figure 3(b).\nGiven the time series input $Z \\in \\mathbb{R}^{N \\times L}$ and its corresponding Fourier transform denoted as $\\mathcal{Z} = F(Z) \\in \\mathbb{C}^{N \\times L}$, the network $H_\\varphi$ is utilized to derive the contextual shaping filter, expressed as $H_\\varphi : \\mathbb{C}^{N \\times L} \\rightarrow \\mathbb{C}^{N \\times D}$. First, it embeds the raw data by a linear dense operation $K : \\mathbb{C}^L \\rightarrow \\mathbb{C}^D$ to improve the capability of modeling complex data. Then, it applies a series of complex-value multiplication with K learnable parameters $W_{1:K} \\in \\mathbb{C}^{1 \\times D}$ yielding $\\sigma(K(Z) \\odot W_{1:K})$ where $\\sigma$ is the activation function, and finally outputs $H_\\varphi(Z)$. Then we apply TexFilter by:\n$\\mathcal{Z} = F(Z)$,\n$\\mathcal{E} = K(Z)$,\n$H_\\varphi(Z) = \\sigma(\\mathcal{E} \\odot_D W_{1:K}), W_{1:K} = \\prod_{i=1}^K W_i$,\n$S = \\mathcal{E} \\odot_D H_\\varphi(Z)$,\n$S = F^{-1}(S)$,\nwhere $\\odot_D$ denotes the element-wise product along D dimension and $S \\in \\mathbb{R}^{N \\times D}$ is the output. The contextual shaping filter can adaptively weight the filtering process based on the changing conditions of input and thus have more flexibility in facing more complex situations."}, {"title": "Experiments", "content": "In this section, we extensively experiment with eight real-world time series benchmarks to assess the performance of our proposed FilterNet. Furthermore, we conduct thorough analytical experiments concerning the frequency filters to validate the effectiveness of our proposed framework."}, {"title": "Experimental Setup", "content": "Datasets We conduct empirical analyses on diverse datasets spanning multiple domains, including traffic, energy, and weather, among others. Specifically, we utilize datasets such as ETT datasets [14], Exchange [38], Traffic [15], Electricity [15], and Weather [15], consistent with prior studies on long time series forecasting [16, 17, 25]. We preprocess all datasets according to the methods outlined in [16, 17], and normalize them with the standard normalization method. We split the datasets into training, validation, and test sets in a 7:2:1 ratio. More dataset details are in Appendix C.1.\nBaselines We compare our proposed FilterNet with the representative and state-of-the-art models to evaluate their effectiveness for time series forecasting. We choose the baseline methods from four categories: (1) Frequency-based models, including FreTS [13] and FITS [32]; (2) TCN-based models, such as MICN [39] and TimesNet [40]; (3) MLP-based models, namely DLinear [12] and RLinear [23]; and (4) Transformer-based models, which include Informer [14], Autoformer [15], Pyraformer [26], FEDformer [25], PatchTST [16], and the more recent iTransformer [17] for comparison. Further details about the baselines can be found in Appendix C.2.\nImplementation Details All experiments are implemented using Pytorch 1.8 [41] and conducted on a single NVIDIA RTX 3080 10GB GPU. We employ MSE (Mean Squared Error) as the loss function and present MAE (Mean Absolute Errors) and MSE (Mean Squared Errors) results as the evaluation metrics. For further implementation details, please refer to Appendix C.3."}, {"title": "Main Results", "content": "We present the forecasting results of our FilterNet compared to several representative baselines on eight benchmarks with various prediction lengths in Table 1. Additional results with different lookback window lengths are reported in Appendix F and G.3. Table 1 demonstrates that our model consistently outperforms other baselines across different benchmarks. The average improvement of FilterNet over all baseline models is statistically significant at the confidence of 95%. Specifically, PaiFilter performs well on small datasets (e.g., ETTh1), while TexFilter excels on large datasets (e.g., Electricity) due to the ability to model the more complex and contextual correlations present in larger datasets. Also, the prediction performance of iTransformer [17], which achieves the best results on the Traffic dataset (862 variables) but not on smaller datasets, suggests that simpler structures may be more suitable for smaller datasets, while larger datasets require more contextual structures due to their complex relationships. Compared with FITS [32] built on low-pass filters, our model outperforms it validating an all-pass filter is more effective. Since PaiFilter is simple yet effective, the following FilterNet in the experimental section refer to PaiFilter unless otherwise stated."}, {"title": "Model Analysis", "content": "In this part, we conduct experiments to delve into a thorough exploration of frequency filters, including their modeling capabilities, comparisons among different types of frequency filters, and the various factors impacting their performance. Detailed experimental settings are provided in Appendix C.5.\nModeling Capability of Frequency Filters Despite the simplicity of frequency filter architecture, Table 1 demonstrates that this architecture can also achieve competitive performance. Hence, in this part, we perform experiments to explore the modeling capability of frequency filters. Particularly, given the significance of trend and seasonal signals in time series forecasting, we investigate the efficacy of simple filters in modeling these aspects. We generate a trend signal and a multi-period signal with noise, and then we leverage the frequency filters (i.e., PaiFilter) to perform training on the two signals respectively. Subsequently, we produce prediction values based on the trained frequency filters. Specifically, Figure 4(a) and 4(b) show that the filter can effectively model trend and periodic signals respectively even compared with state-of-the-art iTransformer [17] when the data contains noise. These results illustrate that the filter has excellent and robust modeling capabilities for trend and periodic signals which are important components for time series. This can also explain the effectiveness of FilterNet since the filters can perform well in such settings."}, {"title": "Shared vs. Unique Filters Among Channels", "content": "To analyze the different channel strategies of filters, we further conduct experiments on the ETTh and Exchange datasets. Specifically, we compare forecasting performance under different prediction lengths between two different types of frequency filters, i.e., $H_\\phi^{(Uni)}$ and $H_\\phi^{(Ind)}$. In $H_\\phi^{(Uni)}$, filters are shared across different channels, whereas $H_\\phi^{(Ind)}$ signifies filters unique to each channel. The evaluation results are presented in Table 2. It demonstrates that filters shared among different channels consistently outperform across all prediction lengths. In addition, we visualize the prediction values predicted on the ETTh1 dataset by the two different types of filters, as illustrated in Figure 11 (see Appendix G.1). The visualization reveals that the prediction values generated by filters shared among different channels exhibit a better fit than the unique filters. Therefore, the strategy of channel sharing seems to be better suited for time series forecasting and filter designs, which is also validated in DLinear [12] and PatchTST [16]."}, {"title": "Visualization of Prediction", "content": "We present a prediction showcase on ETTh1 dataset, as shown in Figure 5. We select iTransformer [17], PatchTST [16] as the representative compared methods. Comparing with these different state-of-the-art models, we can observe FilterNet delivers the most accurate predictions of future series variations, which has demonstrated superior performance. In addition, we include more visualization cases and please refer to Appendix G.3."}, {"title": "Visualization of Frequency Filters", "content": "To provide a comprehensive overview of the frequency response characteristics of frequency filters, we conduct visualization experiments on the Weather, ETTh, and Traffic datasets with the lookback window length of 96 and the prediction length of 96. The frequency response characteristics of learned filters are visualized in Figure 7. From Figures 7(a) and 7(b), we can observe that compared with Transformer-based approaches (e.g., iTransformer [17], PatchTST [16]) tend to attenuate high-frequency components and preserve low-frequency information, FilterNet exhibits a more nuanced and adaptive filtering behavior that can be capable of attending to all frequency components. Figure 7(c) demonstrates that the main patterns of the Traffic dataset primarily resides in the low-frequency range. This observation also explains why iTransformer performs well on the Traffic dataset, despite its low-frequency nature. Overall, Figure 7 demonstrates"}, {"title": "Conclusion Remarks", "content": "In this paper, we explore an interesting direction from a signal processing perspective and make a new attempt to apply frequency filters directly for time series forecasting. We propose a simple yet effective architecture, FilterNet, built upon our proposed two kinds of frequency filters to accomplish the forecasting. Our comprehensive empirical experiments on eight benchmarks have validated the superiority of our proposed method in terms of effectiveness and efficiency. We also include many careful and in-depth model analyses of FilterNet and the internal filters, which demonstrate many good properties. We hope this work can facilitate more future research integrating signal processing techniques or filtering processes with deep learning on time series modeling and accurate forecasting."}, {"title": "More Analysis about the Architecture of FilterNet", "content": "The Necessity of Instance Normalization Block From the frequency perspective, the mean value is equal to zero frequency component. Specifically, given a signal x[n] with a length of N, we can obtain its corresponding discrete Fourier transform X[k] by:\n$X[k] = \\frac{1}{N} \\sum_{n=0}^{N-1} x[n] e^{-j2\\pi nk/N}$  (10)\nwhere j is the imaginary unit. We set k as 0 and then,\n$x[0] = \\frac{1}{N} \\sum_{n=0}^{N-1} x[n] e^{-j2\\pi n0/N}$ (11)\n$= \\frac{1}{N} \\sum_{n=0}^{N-1} x[n]$.\nAccording the above equation, we can find that the mean value $\\frac{1}{N} \\sum_{n=0}^{N-1} x[n]$ in the time domain is equal to the zero frequency component x[0] in the frequency domain. Similarly, we can also view the standard deviation from the frequency perspective, and it is related to the power spectral density.\nAccording to these analysis, the instance normalization is analogous to a form of data preprocessing. Given that filters are primarily crafted to discern particular patterns within input data, while instance normalization aims to normalize each instance in a dataset, a function distinct from the conventional role of filters, we treat instance normalization as a distinct block within our FilterNet architecture.\nFrequency Filter Block Recently, Transformer- and MLP-based methods have emerged as the two main paradigms for time series forecasting, exhibiting competitive performance compared to other model architectures. Building on prior work that conceptualizes self-attention and MLP architectures as forms of global convolution [35, 13], it becomes apparent that frequency filters hold promise for time series forecasting tasks. Just as self-attention mechanisms capture global dependencies and MLPs learn to convolve features across the entire input space, frequency filters offer a means to extract and emphasize specific temporal patterns and trends from time series data. By applying frequency filters to time series data, we can learn recurring patterns, trends, and periodic behaviors that are essential for forecasting future time series data and making accurate predictions.\nFeed-forward Network Incorporating a feed-forward network within the FilterNet architecture is essential for enhancing the model's capacity to capture complex relationships and non-linear patterns within the data. While frequency filters excel at extracting specific frequency components and temporal patterns from time series data, they may not fully capture the intricate dependencies and higher-order interactions present in real-world datasets [17]. By integrating a feed-forward network, the model gains the ability to learn hierarchical representations and abstract patterns from the input data, allowing it to capture more nuanced relationships and make more accurate predictions. This combination of frequency filters and a feed-forward network leverages the strengths of both approaches, enabling the model to effectively process and analyze time series data across different frequency bands while accommodating the diverse and often nonlinear nature of temporal dynamics. Overall, the inclusion of a feed-forward network enriches the expressive power of FilterNet, leading to improved forecasting performance and robustness across various domains."}, {"title": "Explanations about the Design of Two Filters", "content": "Self-attention mechanism is a highly data-dependent operation that both derives its parameters from data and subsequently applies these parameters back to the data. Concretely, given the input data X, the self-attention can be formulated as:\n$SA(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ (12)\nwhere Q (queries), K (keys), and V (values) are linear transformations of the input data X, as:\n$Q = W_QX, K = W_KX, V = W_vX$,  (13)"}, {"title": "Experimental Details", "content": "Datasets\nWe evaluate the performance of our proposed FilterNet on eight popular datasets, including Exchange, Weather, Traffic, Electricity, and ETT datasets. In detail, the Exchange\u00b2 dataset collects daily exchange rates of eight different countries including Singapore, Australia, British, Canada, Switzerland, China, Japan, and New Zealand ranging from 1990 to 2016. The Weather\u00b3 dataset, including 21 meteorological indicators such as air temperature and humidity, is collected every 10 minutes from the Weather Station of the Max Planck Institute for Biogeochemistry in 2020. The Traffic [15] dataset contains hourly traffic data measured by 862 sensors on San Francisco Bay area freeways, which has been collected since January 1, 2015. The Electricity dataset collects the hourly electricity consumption of 321 clients from 2012 to 2014. The ETT (Electricity Transformer Temperature) datasets contain two visions of the sub-dataset: ETTh and ETTm, collected from electricity transformers every 15 minutes and 1 hour between July 2016 and July 2018. Thus, in total we have 4 ETT datasets (ETTm1, ETTm2, ETTh1, and ETTh2) recording 7 features such as load and oil temperature. The details about these datasets are summarized in Table 3.\nBaselines\nWe choose twelve well-acknowledged and state-of-the-art models for comparison to evaluate the effectiveness of our proposed FilterNet for time series forecasting, including Frequency-based models, TCN-based models, MLP-based models, and Transformer-based models. We introduce these models as follows:"}, {"title": "Experimental Settings for Simulation Experiments", "content": "To evaluate the Transformer's modeling ability for different frequency spectrums, we generate a signal consisting of three different frequency components, i.e., low-frequency, middle-frequency, and high-frequency. We then apply both iTransformer and FilterNet to this signal, respectively, and compare the forecasting results with the ground truth. The results are presented in Figure 1."}, {"title": "Experimental Settings for Filters Analysis", "content": "Modeling capability of frequency filters\nWe generate two signals: a trend signal with Gaussian noise and a multi-periodic signal with Gaussian noise. We then apply PaiFilter to these signals with a lookback window length of 96 and a prediction length of 96. The results are displayed in Figure 4.\nVisualization of Frequency Filters\nGiven a filter $H \\in \\mathbb{R}^{1 \\times L}$, where L is its bandwidth, we visualize the frequency response characteristic of the filter by plotting the values in $\\mathbb{R}^{1 \\times L}$. First, we perform a Fourier transform on these values to obtain the spectrum, which includes the frequency and its corresponding amplitude. Finally, we visualize the spectrum, as shown in Figures 7, 8, and 12."}, {"title": "Study of the Bandwidth of Frequency Filters", "content": "The bandwidth parameter (i.e., L in Equation (8) and D in Equation (9)) holds significant importance in the functionality of filters. In this part, we conduct experiments on the Weather dataset to delve into the impact of bandwidth on forecasting performance. We explore a range of bandwidth values within the set {96, 128, 192, 256, 320, 386, 448, 512} while keeping the lookback window length and prediction length constant. Specifically, we conduct experiments to evaluate the impact under three different combinations of lookback window length and prediction length, i.e., 96 \u2192 96, 96 \u2192 192, and 192 \u2192 192, and the results are represented in Figure 9. We observe clear trends in the relationship between bandwidth settings and lookback window length. Figure 9(a) and Figure 9(b) show that increasing the bandwidth results in minimal changes in forecasting performance. Figure 9(c) demonstrates that while forecasting performance fluctuates with increasing bandwidth, it is optimal when the bandwidth equals the lookback window length. These results indicate that using the lookback window length as the bandwidth is sufficient since the filters can effectively model the data at this setting, and it also results in lower complexity."}, {"title": "Ablation Study", "content": "To validate the rationale behind the architectural design of our FilterNet, we conduct ablation studies on the ETTm1, ETTh1, and Electricity datasets. We evaluate the impact on the model's performance"}, {"title": "Additional Results", "content": "Table 4 compares the performance of various methods with our FilterNet, demonstrating that our model consistently outperforms the others. To further assess the performance of FilterNet under different lookback window lengths, we conducted experiments on the ETTh1, ETTm1, Exchange, Weather, and Electricity datasets with the lookback window length of 336. The results, shown in Table 5, indicate that our model achieves the best performance across these datasets."}, {"title": "Visualizations", "content": "Visualization of Channel-shared vs Channel-unique Filters\nTo further compare the channel-shared and channel-unique filters, we visualize the prediction values by the corresponding filters. The results are shown in Figure 11. The figure demonstrates that the values predicted by channel-shared filters closely align with the ground truth compared to those predicted by channel-unique filters. This observation is consistent with the findings presented in Table 2, indicating the superiority of channel-shared filters.\nVisualization of Frequency Filters\nWe further conduct visualization experiments to explore the learnable filters under different lookback window lengths and prediction lengths. The experiments are performed on the Electricity dataset, and the results are illustrated in Figure 12. These figures illustrate that FilterNet possesses full spectrum learning capability, as the learnable filters exhibit values across the entire spectrum. Besides, we observe that the frequency primarily concentrates in the low and middle ranges which explains that some works based on low-pass filters can also achieve good performance.\nVisualizations of Predictions\nTo further offer an evident comparison of our model with the state-of-the-art models, we present supplementary prediction showcases on ETTm1 dataset, and the results are shown in 13. We choose the following representative models, including iTransformer [17], PatchTST [16], and DLinear [12], as the baselines. Comparing with these different types of state-of-the-art models, FilterNet delivers the most accurate predictions of future series variations, demonstrating superior performance."}]}