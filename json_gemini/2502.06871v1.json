{"title": "Flavor Diffusion: Predicting Food Pairings and Chemical Interactions Using Diffusion Models", "authors": ["Jun Pyo, Seo"], "abstract": "The study of food pairing has evolved beyond subjective expertise with the advent of machine learning. This paper presents FlavorDiffusion, a novel framework leveraging diffusion models to predict food-chemical interactions and ingredient pairings without relying on chromatography. By integrating graph-based embeddings [Perozzi et al., 2014], diffusion processes [Ho et al., 2020, Song et al., 2021, Sun and Yang, 2023], and chemical property encoding [Azambuja et al., 2023], FlavorDiffusion addresses data imbalances and enhances clustering quality. Using a heterogeneous graph derived from datasets like Recipe1M [Mar\u00edn et al., 2019] and FlavorDB, our model demonstrates superior performance in reconstructing ingredient-ingredient relationships. The addition of a Chemical Structure Prediction (CSP) layer further refines the embedding space, achieving state-of-the-art NMI scores and enabling meaningful discovery of novel ingredient combinations. The proposed framework represents a significant step forward in computational gastronomy, offering scalable, interpretable, and chemically informed solutions for food science.", "sections": [{"title": "1 Introduction", "content": "Food pairing has traditionally relied on the intuition and experience of chefs, yet scientific analysis and optimization of food combinations remain underexplored. Recent research has leveraged data-driven approaches to model the relationships between food ingredients and chemical compounds to predict novel food pairings.\nSeveral computational approaches have been developed to model food pairings and ingredient relationships. Kitchenette [Park et al., 2021], for instance, applies Siamese neural networks to predict and recommend ingredient pairings based on a large annotated dataset. However, it suffers from key limitations, such as a lack of chemical interpretability and heavy reliance on labeled data, making it less generalizable across different cuisines and novel food combinations.\nOne of the key advancements in this domain is FlavorGraph [Park et al., 2021], a large-scale food-chemical deep neural network model comprising 6,653 ingredient nodes and 1,645 compound nodes. This graph captures two primary relationships: (1) ingredient-ingredient relations, representing co-occurrence patterns in recipes, and (2) ingredient-compound relations, indicating chemical composition links. These relationships are constructed using datasets such as Recipe1M [Mar\u00edn et al., 2019], FlavorDB, and HyperFoods. FlavorGraph incorporates food-chemical associations into a neural network by leveraging the metapath2vec [Dong et al., 2017] algorithm, which embeds ingredient-compound relationships in a word2vec-like manner. Expanding on this approach, WineGraph [Gawrysiak et al., 2023] extends the framework by integrating wine-related datasets to define optimal food-wine pairings.\nDespite progress in computational food science, major challenges remain. Chromatography-based methods, while precise, are costly and limit the acquisition of large-scale chemical interaction data. FlavorGraph effectively captures ingredient-compound relationships using metapath-based embeddings, but its reliance on random-walk sampling makes it difficult to incorporate edge weights and spatial information within the graph structure. These limitations hinder the full exploitation of food-chemical associations, leading to suboptimal ingredient relationship modeling. To address these challenges, we introduce FlavorDiffusion, a Diffusion Model-based framework that refines the representation of food-chemical interactions and elevates the quality of food pairing predictions."}, {"title": "Contributions", "content": "\u2022 We propose a graph-based diffusion modeling approach that leverages DIFUSCO [Sun and Yang, 2023] to capture richer and more structured representations of food-chemical interactions.\n\u2022 We introduce a balanced subgraph sampling strategy to address data imbalance issues, ensuring fair representation across different ingredient-chemical associations.\n\u2022 Our experimental results demonstrate improvements in Normalized Pointwise Mutual Information (NPMI) scores for node embeddings, facilitating more effective chemical inference.\n\u2022 We establish a foundation for predicting chromatography results for non-hub chemicals, extending the applicability of our model beyond frequently occurring compounds.\n\u2022 Our approach enables pairing inference using chemical properties, providing structured and interpretable recommendations for novel ingredient combinations."}, {"title": "2 Dataset", "content": "Our study builds upon FlavorGraph [Park et al., 2021] by utilizing the same large-scale datasets to construct a robust food-chemical network. These datasets provide a structured representation of ingredient relationships and chemical interactions.\nIn the following sections, we summarize the key characteristics of these datasets and outline the pre-processing steps applied to ensure data consistency and usability in our framework."}, {"title": "2.1 Data Sources", "content": "This study utilizes the same datasets as FlavorGraph [Park et al., 2021] to construct a structured food-chemical network.\nRecipe1M [Mar\u00edn et al., 2019] contains 65,284 recipes with ingredient lists and cooking instructions, capturing ingredient co-occurrence patterns in real-world culinary practices.\nFlavorDB compiles chemical composition data from multiple sources, including FooDB, Flavornet, and BitterDB. It originally includes 2,254 flavor compounds linked to 936 food ingredients, but only 400 commonly used ingredients were selected to align with Recipe1M, resulting in 1,561 flavor compound nodes and 164,531 ingredient-flavor compound edges.\nHyperFoods maps drug compounds to food ingredients using machine learning based on food-gene interactions. From the original 206 food ingredients, 104 were selected, yielding 84 drug compound nodes and 386 ingredient-drug compound edges."}, {"title": "2.2 Data Processing", "content": "To construct a structured representation of food-chemical relationships, we build upon FlavorGraph [Park et al., 2021], a heterogeneous graph that integrates both culinary and chemical associations. The graph construction process follows a structured approach. First, an ingredient-ingredient graph is built by extracting co-occurrence patterns from Recipe1M [Mar\u00edn et al., 2019], where edges between ingredients are established based on their Normalized Pointwise Mutual Information (NPMI) scores. Only statistically significant ingredient pairs appearing together in a substantial number of recipes are retained, resulting in a total of 111,355 edges. Second, an ingredient-chemical graph is formed by linking ingredients to their corresponding chemical compounds using FlavorDB and HyperFoods, leading to 35,440 edges between food ingredients and known chemical compounds. The final graph structure comprises 6,653 ingredient nodes and 1,645 compound nodes, forming a heterogeneous graph that encodes both culinary co-occurrence relationships and chemical interactions."}, {"title": "2.3 Chemical Property Encoding", "content": "To ensure chemically informed ingredient representations, each compound is characterized using CACTVS chemical fingerprints, which are encoded as 881-dimensional binary vectors. These vectors represent molecular descriptors such as molecular weight, functional groups, and substructure patterns, using a binary encoding scheme where each bit indicates the presence or absence of a specific chemical substructure."}, {"title": "3 Related Work", "content": null}, {"title": "3.1 FlavorGraph", "content": "FlavorGraph [Park et al., 2021] is a heterogeneous graph $G = (V, E)$ integrating ingredient co-occurrence and molecular profiling to model food-chemical interactions. By leveraging metapath-based learning [Dong et al., 2017], it enables systematic ingredient discovery and predictive food pairing through shared molecular properties."}, {"title": "3.1.1 Metapath2Vec", "content": "To learn chemically meaningful embeddings, we employ Metapath2Vec, which captures high-order relations via structured random walks. Ingredients are classified into hub ingredients (H), which directly connect to chemical compounds, and non-hub ingredients (N), which lack direct chemical links and rely on hub ingredients to acquire chemical insights.\nThe metapath sampling strategy follows:\n$N\\rightarrow H \\rightarrow C \\rightarrow H \\rightarrow N$\nwhere C represents chemical compounds. This structured propagation ensures that non-hub ingredients inherit chemical relevance, enhancing embedding robustness and interpretability."}, {"title": "3.1.2 Architecture", "content": "The network, parameterized by $\\theta$, takes node pairs $(i, j)$ as input and outputs an edge score $s_{\\theta}(i, j)$, normalized across all embeddings:\n$s_{\\theta}(i, j) = \\sigma(u_i^T u_j)$\nwhere $u_i$ and $u_j$ are the learned embeddings for nodes $i$ and $j$, ensuring consistency across culinary co-occurrence and chemical similarity."}, {"title": "3.1.3 Loss Function", "content": "Embeddings are optimized using Skip-Gram with Negative Sampling (SGNS):\n$J_{\\theta} = \\sum_{(i,j)\\in D} \\log \\sigma(u_i^T u_j) + \\sum_{(i,j')\\in D'} \\log \\sigma(-u_i^T u_{j'})$\nwhere D and D' are positive and negative sample pairs. To enforce chemical relevance, an additional Chemical Structure Prediction (CSP) loss is introduced:"}, {"title": null, "content": "$L_{CSP,\\theta} = \\sum_{d=1}^{D} y_d \\log f_{\\theta,d}(i) + (1 - y_d) \\log (1 - f_{\\theta,d}(i))$\nwhere $f_{\\theta,d}(i)$ predicts the presence of the d-th molecular substructure $y_d$, refining embeddings with molecular fingerprints."}, {"title": "3.2 DIFUSCO", "content": "Graph-based diffusion models have recently emerged as powerful frameworks for solving combinatorial optimization problems by leveraging probabilistic generative processes. In our work, we leverage the fundamental principles of graph-based diffusion models, particularly the Gaussian diffusion framework, to reconstruct structured graph representations. By incorporating diffusion-driven embeddings into our heterogeneous network, we enhance the predictive accuracy of food-chemical interactions while maintaining interpretability. This approach allows for the seamless integration of molecular-level insights into ingredient pairing research, further advancing computational gastronomy."}, {"title": "4 Proposition: FlavorDiffusion", "content": null}, {"title": "4.1 Sub-Graph Sampling", "content": "FlavorDiffusion is built upon the DIFUSCO Gaussian noise-based diffusion model, extending its capabilities to structured food-chemical graphs. The core objective is to train a model capable of reconstructing subgraphs sampled from the full heterogeneous graph $G = (V, E)$ while leveraging node attributes as guidance.\nThe full graph consists of a diverse set of nodes V, including hub ingredients, non-hub ingredients, flavor compounds, and drug compounds, with edges E encoding the strength of their relationships as continuous values in [0, 1]. We define a dataset of subgraphs, where each sample contains m nodes selected from G. These subgraphs are denoted as:\n$D_m = \\{G_i = (V_i, E_i)\\}_{i=1}^{N}$,\nwhere each subgraph $G_i$ has $|V_i| = m$ nodes and an adjacency matrix $E_i$ of size $m \\times m$, representing pairwise edge scores. The dataset is partitioned into training ($N_t$) and validation ($N_v$) subsets."}, {"title": "4.2 Forward Diffusion Process", "content": "For a single data point $G_i = (V_i, E_i)$ sampled from the dataset, we define the diffusion process over its edge set $E_i$. By convention, we denote the corrupted version of $E_i$ at timestep t as $x_t$, aligning with standard diffusion formalisms. The node representations, encompassing all vertex features, are denoted as Emb.\nThe forward diffusion process follows a Markovian Gaussian noise injection, progressively perturbing the edges $x_t$ while preserving node representations:\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I)$,\nwhere $\\beta_t$ is a predefined noise variance at timestep t. Given an initial clean edge matrix $x_0 = E_i$, we can analytically express the direct corruption of $x_0$ at any timestep t as:\n$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)$,\nwhere $\\bar{\\alpha}_t = \\prod_{s=1}^{t} (1 - \\beta_s)$ represents the cumulative noise effect over time. This formulation allows direct sampling of $x_t$ from $x_0$, bypassing iterative updates.\nIn this framework, the edge structure is progressively degraded into Gaussian noise, while node representations Emb remain unchanged, ensuring that denoising relies on learned node attributes."}, {"title": "4.3 Reverse Denoising Process", "content": "The reverse process seeks to recover $x_0$ from the fully corrupted state $x_T$, learning to remove noise in a stepwise manner. The key assumption is that the forward process follows a Gaussian transition, enabling an analytically derived reverse process.\nGiven the Markovian nature of the diffusion process, we define the true posterior:\n$q(x_{t-1}|x_t, x_0) = \\mathcal{N}(x_{t-1}; \\mu_t(x_t, x_0), \\beta_t I)$,\nwhere the posterior mean and variance are derived as:\n$\\mu_t(x_t, x_0) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1 - \\bar{\\alpha}_t} x_0 + \\frac{\\sqrt{\\alpha_t} (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t$,\n$x_0 = \\frac{1}{\\sqrt{\\alpha_t}} x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta}(x_t, t, Emb)$,\nSince $x_0$ is unknown, we train a model $p_{\\theta}(x_0|x_t)$ to approximate it. Substituting the predicted $x_0$, the learned reverse process is modeled as:\n$p_{\\theta}(x_{t-1}|x_t, Emb) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t, t, Emb), \\Sigma_{\\theta}(x_t, t))$,\nwhere $\\mu_{\\theta}$ is the learned estimate for $\\mu_t(x_t, x_0)$, and the variance term is fixed as $\\Sigma_{\\theta}(x_t, t) = \\beta_t I$, avoiding the need for explicit learning. The function $\\mu_{\\theta}$ is now conditioned on the node representations (Emb) of the two vertices forming the edge. Using the DDPM convention, we parameterize $\\mu_{\\theta}$ as:\n$\\mu_{\\theta}(x_t, t, Emb) = \\frac{1}{\\sqrt{\\alpha_t}} x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta}(x_t, t, Emb)$,\nwhere $\\epsilon_{\\theta}(x_t, t, Emb)$ is the learned noise estimate, which is now explicitly conditioned on the representations of the two nodes forming the edge. The node representations provide additional context for denoising by leveraging node-specific features."}, {"title": "4.4 Optimization via Variational Lower Bound", "content": "To train the reverse model, we maximize the variational lower bound (ELBO), decomposed as:\n$\\mathcal{L}_{ELBO} = \\mathbb{E}_q[\\log p_{\\theta}(x_0|x_1, Emb) - \\sum_{t=1}^{T} D_{KL}(q(x_{t-1}|x_t, x_0) || p_{\\theta}(x_{t-1}|x_t, Emb)) ]$.\nHere, T represents the total number of diffusion steps, defining the depth of the forward and reverse process. The KL divergence encourages the learned transitions to match the true posterior. Since $q(x_t|x_0)$ is Gaussian, minimizing $D_{KL}$ is equivalent to predicting the noise component $\\epsilon$ added during diffusion. Thus, the training objective simplifies to:\n$\\mathcal{L}_{recon} = \\mathbb{E}_{t, x_0, \\epsilon} [||\\epsilon - \\epsilon_{\\theta}(x_t, t, Emb)||^2].$"}, {"title": null, "content": "This loss ensures that $\\epsilon$ effectively estimates the noise introduced in the forward process while incorporating node representations. By iteratively refining the denoising function, FlavorDiffusion reconstructs the original ingredient-ingredient graph from noisy subgraphs, leveraging both the structural edge information and node attributes to enhance predictive modeling for food pairing analysis."}, {"title": "4.5 Inference", "content": "Graph reconstruction follows Denoising Diffusion Implicit Models (DDIM) for efficient and deterministic sampling. Unlike DDPM, DDIM removes noise via a non-Markovian update, accelerating inference.\nStarting from $x_T \\sim \\mathcal{N}(0, I)$, the reverse process iterates:\n$x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} \\tilde{x}_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\cdot \\epsilon_{\\theta}(x_t, t, Emb)$,\nwhere the predicted clean graph is:\n$\\tilde{x}_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_{\\theta}(x_t, t, Emb)}{\\sqrt{\\bar{\\alpha}_t}}$.\nIterating from T to 0, the model refines $x_t$ to recover ingredient-ingredient relationships. DDIM ensures fast, stable, and chemically meaningful reconstructions."}, {"title": "4.6 Model Architecture", "content": "The noise prediction network $\\epsilon_{\\theta}(x_t, t, V)$ employs an anisotropic GNN to iteratively refine node and edge embeddings. Let $h^l \\in \\mathbb{R}^{d_h}$ and $e^l \\in \\mathbb{R}^{d_e}$ denote the node and edge features at layer l, respectively. The refinement process updates both edge and node embeddings through the following operations:\nEdge Refinement The initial edge embeddings $e_{ij}^0$ are set as the corresponding values from the noisy edge representation $x_t$. At each layer l, the intermediate edge embeddings $e_{ij}^{\\prime l}$ are updated as:\n$e_{ij}^{\\prime l} = P^l e_{ij}^{l-1} + Q^l h_i^{l-1} + R^l h_j^{l-1}$,\nwhere $P^l$, $Q^l$, $R^l \\in \\mathbb{R}^{d_e \\times d_e}$ are learnable parameters. The refined edge embedding $e_{ij}^{l+1}$ is then computed as:\n$e_{ij}^{l+1} = e_{ij}^{\\prime l} + MLP_e(BN(e_{ij}^{\\prime l})) + MLP_t(t)$,\nwhere $MLP_e$ is a 2-layer perceptron and $MLP_t$ embeds the diffusion timestep t using sinusoidal features.\nNode Refinement The node embeddings $h_i^l$ are refined by aggregating information from neighboring nodes and their associated edges. The update rule for $h_i^{l+1}$ is given by:\n$h_i^{l+1} = h_i^{l-1} + \\alpha \\cdot BN(U^l h_i^{l-1} + \\sum_{j \\in \\mathcal{N}(i)} \\sigma(e_{ij}^{l+1}) V^l h_j^{l-1})$,\nwhere $U^l, V^l \\in \\mathbb{R}^{d \\times d}$ are learnable parameter matrices, $\\sigma$ is the sigmoid activation function used for edge gating, $\\odot$ denotes the Hadamard (element-wise) product, $\\mathcal{N}(i)$ represents the set of neighbors for node i, and $\\alpha$ is the ReLU activation applied after aggregation.\nFinal Prediction After L GNN layers, the final refined edge embeddings $E^{(L)} \\in \\mathbb{R}^{N \\times N \\times d_e}$ are passed through a ReLU activation and a multi-layer perceptron (MLP) to predict the noise:\n$\\epsilon_{\\theta}(x_t, t, V) = MLP(ReLU(E^{(L)}))$.\nThis formulation ensures that both node and edge embeddings are iteratively refined to capture local and global graph structure, enabling robust denoising and reconstruction of ingredient-ingredient relationships."}, {"title": "5 Experimental Results", "content": "The evaluation consists of two primary experiments: (1) reproducing the NMI-based clustering performance evaluation originally conducted in FlavorGraph, and (2) assessing the generalization ability of our proposed Flavor Diffusion framework by testing on subgraphs of different sizes.\nSubgraphs of size 25, 50, 100, and 200 nodes were sampled while maintaining an equal proportion of hub and non-hub ingredients. The number of subgraphs used for training and testing at each scale is shown in Table 2."}, {"title": "Generalization Ability", "content": "To assess the generalization ability of the proposed framework, models trained on one subgraph size were tested on all sizes to observe performance across different scales. The results in Table 3 indicate that models trained on 25-node subgraphs generalize poorly to larger graphs, with an MSE of 0.025078 when tested on 100-node subgraphs. In contrast, the 100-node trained model demonstrates the most stable generalization across different test sizes, showing minimal MSE variation. The 200-node trained model, while excelling on large graphs with an MSE of 0.003692, exhibits difficulties in adapting to smaller structures, with a high error of 0.059557 when tested on 25-node subgraphs."}, {"title": "NMI-based Evaluation", "content": "To construct the clustering test dataset, nine representative food categories were defined: Bakery/Dessert/Snack, Beverage Alcoholic, Cereal/Crop/Bean, Dairy, Fruit, Meat/Animal Product, Plant/Vegetable, Seafood, and Others. From these, 416 chemical hub ingredients with strong connections were selected to ensure diverse and well-defined clustering labels, enabling fair comparisons across models commonly used in related studies.\nThe NMI-based evaluation results in Table 4 demonstrate the clustering quality of different models. Among the non-CSP variants, the Flavor Diffusion (50 nodes) model achieves the highest NMI score of 0.3236, surpassing the baseline FlavorGraph model without CSP. The best overall performance is observed in the Flavor Diffusion_CSP (200 nodes) model, which achieves an NMI score of 0.3410, indicating that the CSP layer significantly improves the learned ingredient embeddings. Smaller subgraphs, such as the 25-node configuration, show the greatest improvement when using CSP (0.2970 vs. 0.2167), suggesting that the chemical structure prediction enhances clustering, particularly in more limited ingredient sets."}, {"title": "6 Discussion", "content": "The visualization results highlight the impact of the proposed Flavor Diffusion framework on embedding quality, particularly with the CSP (Chemical Structure Prediction) layer, as shown in Figures 1 and 2.\nEmbedding Space Analysis Figure 1 illustrates the differences in embedding spaces across model configurations. The baseline embeddings (left) fail to separate chemical compounds and ingredients effectively, resulting in diffuse and isotropic clusters dominated by non-hub ingredients.\nIn contrast, the embeddings generated by **Flavor Diffusion (200 nodes)** without CSP (center) show improved clustering, with chemical compounds and hub ingredients forming clearer groups. However, some overlap persists between hubs and non-hubs. The inclusion of the CSP layer (right) further refines the embeddings, creating well-structured, anisotropic clusters that reflect meaningful relationships between ingredients and compounds.\nPerformance and Structural Insights The CSP layer significantly enhances clustering performance, as evidenced by the highest NMI mean (0.3410) achieved by Flavor Diffusion (200 nodes) with CSP. The transition from isotropic to anisotropic embedding spaces reflects the model's ability to learn diverse, domain-specific relationships. Furthermore, the iterative refinement process highlights the framework's capacity to generate realistic ingredient-ingredient graphs that align with culinary and chemical properties.\nDynamic Reconstruction for Novel Insights\nThe iterative reconstruction process visualized in Figure 2 showcases the Flavor Diffusion framework's ability to refine ingredient-ingredient relationships progressively. Starting from random initialization (Step 0), the edge scores evolve over diffusion steps, ultimately converging towards the ground truth structure by Step 10. The color intensity of the edges reflects their normalized scores, with higher values indicating stronger relationships. This gradual alignment with the ground truth demonstrates the model's capacity to encode meaningful relational patterns in a structured manner.\nPotential for Ingredient Innovation The progressive nature of the diffusion process suggests that Flavor Diffusion is not only capable of reconstructing known ingredient-ingredient relationships but also has the potential to generalize and infer connections beyond the training data. The inclusion of chemical fingerprints and iterative edge refinement allows the model to generate plausible ingredient combinations, even in scenarios involving diverse or sparse subgraph configurations. This characteristic is particularly valuable for fields such as computational gastronomy, where discovering unique and harmonious flavor pairings is a central goal.\nAlignment with Culinary and Chemical Properties The alignment of the reconstructed graphs with ground truth structures further underscores the model's fidelity in capturing culinary and chemical properties. As the diffusion process unfolds, the model demonstrates an increasing ability to balance local (ingredient-specific) and global (chemical-based) relationships. This balance not only enhances clustering quality but also provides a robust framework for extending ingredient networks in a meaningful way."}, {"title": "7 Conclusion", "content": "This study introduced FlavorDiffusion, a diffusion model-based framework for predicting ingredient pairings and chemical interactions. The model's capability to integrate chemical fingerprints and optimize graph embeddings resulted in improved clustering quality and predictive accuracy. Experiments revealed that the inclusion of the CSP layer significantly enhanced the representation of food-chemical relationships, achieving the highest NMI scores across various configurations. The progressive nature of the diffusion process further demonstrated the model's ability to generalize, en-abling the inference of novel ingredient combinations. By aligning culinary and chemical properties, FlavorDiffusion offers a robust tool for advancing food pairing discovery, with applications in flavor design and computational gastronomy. Future work will aim to expand dataset coverage, integrate multi-modal data, and explore new graph-sampling techniques to further enrich food science research."}]}