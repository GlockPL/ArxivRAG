{"title": "Learn to Unlearn: Meta-Learning-Based Knowledge Graph Embedding Unlearning", "authors": ["Naixing Xu", "Qian Li", "Xu Wang", "Bingchen Liu", "Xin Li"], "abstract": "Knowledge graph (KG) embedding methods map entities and relations from knowledge graphs to continuous vector spaces, simplifying their representations and enhancing performance across various tasks (e.g., link prediction, question answering). As concerns about personal privacy rise, machine unlearning (MU), an emerging AI technology that enables models to eliminate the influence of specific data, has garnered increasing attention from the academic community. Existing works typically achieves machine unlearning through data obfuscation and adjustments to the model's training loss. Furthermore, existing approaches lack generalization ability across different unlearning tasks. In this paper, we propose a Meta-Learning-Based Knowledge Graph Embedding Unlearning framework (MetaEU), specifically designed for KG embedding unlearning. By leveraging meta-learning, we generate embeddings that require unlearning. This process reduces the impact of specific knowledge on the graph while maintaining the model's performance on the remaining data. A thorough experimental study on benchmark datasets shows that MetaEU demonstrates promising performance in the knowledge graph embedding unlearning task.", "sections": [{"title": "1 Introduction", "content": "In recent years, Knowledge graphs (KGs) have emerged as essential structures for representing relational data in various domains, from natural language processing to recommendation systems [1]. Knowledge graph embedding (KGE) techniques, play a crucial role in transforming complex graph structures into low-dimensional vector spaces. These techniques are widely applied in common KG tasks such as knowledge graph completion [1,2,3], entity alignment [4], link prediction, etc. Meta-learning first appears in the field of educational psychology. It is defined as the comprehension and adaptation to the process of learning, rather than simply the accumulation of subject knowledge [5]. In the past few years, researchers have been attempting to apply meta-learning in various fields of artificial intelligence"}, {"title": "2 Related Work", "content": "In this section, we discuss related work in three key areas: knowledge graph embedding models, meta-learning, and machine unlearning."}, {"title": "Knowledge graph embedding", "content": "Knowledge graph embedding (KGE) models are designed to represent entities and relations of a knowledge graph in a continuous vector space to facilitate downstream machine learning tasks. The foundational work by Bordes et al. [9] introduced TransE, a translational distance model that became the basis for many subsequent approaches. Building on this, other models such as DistMult [10] and ComplEx [11] extended the capability of KGE by modeling more complex relational patterns, such as symmetric and antisymmetric relations. Furthermore, Sun et al. [12] proposed Rotate, introducing a rotation-based approach to model complex relations effectively. To address the challenge of modifying KG embeddings after deployment, the latest work in 2024 by Cheng et al. [13] proposed KGEditor, a framework designed to adjust knowledge graph embeddings."}, {"title": "Meta-learning", "content": "Meta-learning, or \"learning to learn,\" seaks to improve the generalization ability of models by utilizing experiences from multiple related tasks. The work of Finn et al. [14] introduced Model-Agnostic Meta-Learning (MAML), which quickly adapts models to new tasks with limited data by learning an initial set of parameters suitable for fine-tuning. Li et al. [15] extended this approach by focusing on gradient-based optimization improvements to further enhance convergence speed. The combination of meta-learning with other methods in machine learning, has received increasing attention. Zintgraf et al. [16] proposed a meta-learning method that improved adaptability in reinforcement learning settings, while Qiao et al. [17] applied meta-learning to graph data, addressing the scarcity of supervised information by integrating labeled and unlabeled data on the graph."}, {"title": "Machine unlearning", "content": "Machine unlearning is an emerging field aimed at efficiently removing specific learned information from machine learning models to address data privacy concerns. Cao and Yang [18] laid the groundwork for this concept by proposing a data deletion method that recalculates model parameters in a computationally feasible way. Ginart et al. [19] developed an efficient unlearning approach specifically for k-means clustering, allowing selective data forgetting. More recently, Kim and Woo [20] introduced a two-stage model retraining method that utilizes knowledge distillation to enable the rapid removal of specific data without affecting the performance of the deep learning model. In a 2024 study, Yao et al. [21] proposed a machine unlearning framework in the context of large language models (LLMs), demonstrating that machine unlearning is a viable solution to address the \"right to be forgotten\" issue within LLMs.\nThese three areas knowledge graph embeddings, meta-learning, and machine unlearning collectively establish the necessary background for our work. Despite the significant advancements achieved in each of these fields, research on the application of machine unlearning within the context of knowledge graphs remains scarce. Our proposed approach seeks to bridge concepts from these domains to advance knowledge representation and its ethical application in machine learning."}, {"title": "3 Problem Definition", "content": "A KG G can be considered as a directed graph composed of a series of triples. Let E is the set of entities, R is the set of relations, KG can be defined as G = (E,R,T), where T = {(h,r,t)} \u2286 E\u00d7R\u00d7E. KGE methods continually train and adjust the entity embedding matrix $E \\in R^{|E|\\times d}$ and the relation embedding matrix $R \\in R^{|R|\\times d}$ through their own scoring function s(\u00b7), until both E and R have suitable embeddings in the context of the knowledge graph G, where E and R denote the numbers of entities and relations, respectively, and d is the dimension of the embeddings. In other words, the score s(h,r,t) for a true triple (h, r, s) \u2208 T is higher than the score s(h', r', t') for a negative triple (h',r',t') \u2209 T.\nTo address the problem of achieving machine unlearning in KGE, we introduce a scenario where the knowledge graph is partitioned into two subsets: the forgetting set Tf and the remaining set Tr. Tf represents triples that need to be unlearned, while Tr consists of the rest of the knowledge graph. The objective of machine unlearning in KGE is to rapidly remove the influence of triples in Tf while maintaining the model's performance on Tr.\nThe goal of the meta-learning-based KGE unlearing framework is to leverage the ability of meta-learning to generalize across different unlearning tasks. For instance, in an e-commerce platform, many users may be sensitive about their medicine purchase records and request the platform to delete this data. A meta-learning-based KGE unlearning framework can effectively remove the impact of these records for each user who makes such a request. Specifically, we formulate the unlearning problem as a bi-level optimization process where:\n1. During meta-training, the model trains on various unlearning tasks, each involving a different subset of triples, to acquire meta-knowledge M across diverse unlearning scenarios.\n2. During meta-testing, the model leverages the learned meta-knowledge to efficiently adapt embeddings for Tf, producing E' that retains suitable properties on the T, while minimizing the influence of Tf.\nFormally, given a knowledge graph G = (E,R,T) with T = Tf U Tr, we want to find a optimal KGE unlearning function:\n$F_u(E, T_f, \\theta) \\rightarrow E'$,\nwhere Fu() is the KGE unlearning function, and 6 is the parameter set. Moreover, the output of the function, E', should satisfy the following properties:\n1. The performance of embedding matrix E' on the T should be comparable to the performance of E trained using the KGE method, while exceeding the performance of Er obtained by re-training using the KGE method on the Tr.\n2. Meanwhile, the performance of E' on the Tf should be inferior to both E and Er, demonstrating that E' has better eliminated the influence of the data that needs to be forgotten."}, {"title": "4 Methodology", "content": "We now present the proposed MetaEU model, which aims to efficiently unlearn specific data from a knowledge graph embedding model while maintaining performance on the remaining data. As shown in Fig. 2, our framework leverages an ensemble learning strategy that combines multiple base learners, each consisting of two core modules: Relation-Aware Entity Embedding Generator (RAEEG) and Neighbor-Enhanced Embedding Modulator (NEEM). Next, we will provide a detailed explanation of each component of the model."}, {"title": "4.1 The Framework of Meta-Learning", "content": "The objective of traditional machine learning is to train a model that performs well on a fixed, predefined task. This approach focuses on optimizing a single model for a specific dataset or problem domain. In contrast, meta-learning aims to train a meta-model that can generalize across various tasks. The purpose of MetaEU is to achieve efficient KGE unlearning across various scenarios, which requires the model to generate high-quality obfuscated data even for unseen entities. To implement meta-learning in MetaEU, we need to modify the existing KGE framework to align with the meta-training regime."}, {"title": "The construction of the Dataset", "content": "In traditional KGE methods, the model typically relies on a dataset T composed of triples. This dataset T is usually divided into two parts: a training set Ttrain and a test set Ttest (for simplicity, the validation set is ignored in the discussion). To enhance the model's performance, Ttrain not only includes the original positive triples {h, r, t} but also incorporates negative triples {h', r', t'}. The model is trained using the following loss function (taking the TransE [9] loss function as an example):\n$L = \\sum_{(h,r,t) \\in T_{train}} [max(0, \\gamma + s(h',r', t') \u2013 s(h, r,t))],$\nwhere $s(h, r, t) = -||E_h+R_r - E_t||$ represents the scoring function, and \u03b3 denotes the margin hyperparameter.\nAccording to the concept of meta-learning, in MetaEU, we extract k subgraphs from the KGG and treat the entities within these subgraphs as unseen to simulate unknown task scenarios:\n$G_i = (E_i, R_i, T_i), \\forall i \\in \\{1,2,...,k\\},$\nwhere Ei represents the entity set of the subgraph Gi, Ri represents the relation set of the subgraph Gi, and $T_i \\subseteq E_i \\times R_i \\times E_i$ denotes the triple set of the subgraph Gi. Additionally, a portion of the triples in these subgraphs is used as the support set, allowing the model to generate appropriate embeddings, while the remaining triples form the query set, which is used to evaluate the quality of the generated embeddings and compute the training loss function:\n$T_i = T_i^{support} \\cup T_i^{query}, \\qquad T_i^{support} \\cap T_i^{query} = \\emptyset.$\nTo adapt MetaEU to different task scenarios, we treat the entities within the task subgraph Gi as unseen entities. By training on these tasks, MetaEU naturally exhibits strong generalization capabilities."}, {"title": "The Procedure of Meta-Learning", "content": "After obtaining k task subgraphs, MetaEU performs learning on these subgraphs to enable knowledge transfer to new task scenarios. According to meta-learning theory, the model acquires meta-knowledge during this process. Meta-knowledge can be regarded as a form of high-level structure or rules shared across tasks, which we will elaborate on in Section 4.2. For each task Ti and its corresponding task subgraph Gi, MetaEU generates embeddings using the support set $T_i^{support}$ and evaluates them on the query set $T_i^{query}$. The overall objective function of meta-learning can be expressed as:\n$\\min_{\\Phi} E_{T \\sim p(T)} [L_{T^{query}} (f_{\\Phi'}(X_j), Y_j)],$\nwhere p(T) represents the task distribution, and each task T is a specific task sampled from this distribution; the function f\u03a6(\u00b7) is an abstract representation of the meta-knowledge component in MetaEU, which is expected to perform well on the query set $T_i^{query}$ after observing the support set $T_i^{support}$; $L_{T^{query}}$ denotes the loss function for the query set, and \u03a6' represents the updated parameters after learning from the support set $T_i^{support}$."}, {"title": "4.2 The Extraction of Meta-Knowledge from Knowledge Graphs", "content": "Meta-knowledge refers to knowledge about knowledge, typically involving high-level abstractions of data, structures, and relationships. In the context of KGs, meta-knowledge often describes the rules, structures, and reasoning logic inherent to the graph itself, such as the types of entities and the properties of relationships. Unlike the high-level rules learned through cross-task meta-training, which are shared across tasks, the meta-knowledge within a KG can be regarded as a localized instantiation of high-level meta-knowledge. In MetaEU, we utilize two modules, the Relation-Aware Entity Embedding Generator (RAEEG) and the Neighbor-Enhanced Embedding Modulator (NEEM), to extract meta-knowledge from the KG."}, {"title": "Relation-Aware Entity Embedding Generator", "content": "In a KG, the outgoing relations and ingoing relations of an entity implicitly encode information about the entity's type. As shown in Figure 1, even though Entity 1 is unseen, its outgoing relation works_at and ingoing relation father_of indicate that Entity 1 is likely an employee of some organization and the child of a specific father. To facilitate representation, we denote the outgoing relation embedding matrix of the KG as $R_{out} \\in R^{|R|\\times d}$ and the ingoing relation embedding matrix as $R_{in} \\in R^{|R|\\times d}$, where $R_{out}$ represents the embedding of a specific relation r.\nIn the KG G = (E,R,T), the RAEEG module can generate the initial embedding $E^{init}$ of an entity e based on its outgoing and ingoing relations:\n$E^{init} = \\frac{\\sum_{r\\in O(e)} R_{out} + \\sum_{r\\in I(e)} R_{in}}{|O(e)| + |I(e)|}$"}, {"title": "Neighbor-Enhanced Embedding Modulator", "content": "After RAEEG generates the initial embedding $E^{init}$ based on the incoming and outgoing relations of entity e, the NEEM module captures the multi-hop neighborhood information of e to further refine $E^{init}$. Existing studies have shown that Graph Neural Networks (GNNs) are capable of capturing the local structures of knowledge graphs [22,23]. Therefore, the design of the NEEM module draws inspiration from the principles of GNNs. Following the structure of R-GCN [22], the NEEM module for an entity e is formulated as follows:\n$E^{(l+1)}_e = \\sigma(\\sum_{r \\in R} \\sum_{n \\in N_r(e)} \\frac{1}{c_{e,r}} W_r^{(l)}E_n^{(l)} + W_0^{(l)}E_e^{(l)}),$\nwhere $E_e^{(l)}$ represents the feature representation of node e at layer l, R denotes the set of relations in G = (E,R,T), $N_r(e)$ represents the set of neighboring nodes connected to e via relation r, and ce,r is a normalization factor used to handle imbalanced node degrees, typically defined as $c_{e,r} = |N_r(e)|$. $W_r^{(l)}$ is the weight matrix for relation r at layer l, and $W_0^{(l)}$ is the self-loop weight matrix for processing the features of node e itself. o represents the activation function, for which we use ReLU. The input representation for NEEM is set as $E^{(0)} = E^{init}$\nTo enable the model to flexibly leverage both low-order and high-order neighborhood information and to adaptively utilize the most suitable neighborhood information for each entity, we incorporate a hierarchical embedding integrator (HEI) into NEEM. The formula is as follows:\n$E^{final} = HEI(\\lbrace E_e^{(l)} \\rbrace_{l=0}^{L}) = W_{HEI} \\bigoplus_{l=0}^{L} E_e^{(l)},$\nwhere $E^{final}$ represents the final embedding of entity e; $HEI(\\lbrace E_e^{(l)} \\rbrace_{l=0}^{L})$ denotes the HEI module, which is responsible for hierarchically integrating all embeddings from layer 0 to layer L; \u2295 represents the layer-wise concatenation operation; and WHEI refers to the transformation matrix in HEI, which maps the concatenated high-dimensional representation into the final entity embedding."}, {"title": "4.3 Ensemble Learning and Ensemble Unlearning", "content": "This section introduces the ensemble learning and ensemble forgetting mechanisms in the MetaEU framework. As shown in Figure 2, by integrating multiple base models, the framework decomposes the overall learning objective into smaller, independently solvable tasks handled by each base model. The base models combine the meta-knowledge learned from cross-task meta-training in Section 4.1 with the meta-knowledge extracted from the knowledge graph in Section 4.2, enabling the generation of high-quality embeddings and high-quality obfuscated data. This allows the model to perform well on the retention set while minimizing the influence of data from the forgetting set. In the knowledge graph G = (E,R,T), the objective function for ensemble learning can be expressed as:\n$\\arg \\min_{f_{base}, w} L_{ensembleL} = \\sum_{i=1}^{N} w_iL(f_{base}, T^{query}),$\nwhere $f_{base}$ represents a base model, $w_i$ denotes the weight assigned to the i-th base model, satisfying $\\sum_{i=1}^{N} w_i = 1$; N represents the number of base models; and $L_{ensembleL}$ refers to the ensemble learning loss function (the calculation method can be referenced from Equation 2). As shown in Figure 2, L\u2081 is an example of such a loss. The objective of L\u2081 is to increase the KGE scores of positive samples in the query set Tquery and decrease the KGE scores of negative samples through backward.\nCorrespondingly, the objective function for ensemble unlearning can be expressed as:\n$\\arg \\max_{f_{base}, w} L_{ensembleU} = \\sum_{i=1}^{N} w_iL(f_{base}, T^{query}),$\nwhere $L_{ensembleU}$ represents the loss function for ensemble unlearning, corresponding to L\u2082 in Figure 2. The objective of Equation 10 is to maximize the overall loss in the query set of the forgetting set by adjusting the base models $f_{base}$ and their weights w, thereby achieving the effect of forgetting specific knowledge.\nL\u2083 in Figure 2 guides the optimization of embeddings by combining L\u2081 and L\u2082 with weights; its formula is:\n$L_3 = w_aL_1 + w_bL_2, \\qquad s.t. w_a + w_b = 1, w_a, w_b \\in [0,1].$\nL4 is used to balance the model's forgetting strength during the ensemble unlearning process, in order to prevent excessive unlearning from affecting the overall performance of the model. The role of L5 is to fine-tune the model for specific knowledge graphs in particular tasks."}, {"title": "5 Experiment", "content": "5.1 Information about the Dataset\nFollowing existing work [7], we use the FB15k-237 dataset in this experiment to evaluate the effectiveness of MetaEU. According to the description of the meta-learning framework in Section 4.1, we need to extract k subgraphs Gi = (Ei, Ri, Ti) from the graph G to correspond to k tasks Ti. Furthermore, each subgraph triplet needs to be divided into a support set $T_i^{support}$ and a query set $T_i^{query}$, and the relationship between the two parts is as shown in Equation 4.\n5.2 Experimental Setup"}, {"title": "Implementation Details", "content": "We implement MetaEU using PyTorch and DGL, and the experiments were conducted on an Intel(R) Xeon(R) Gold 6226R CPU and an NVIDIA GeForce RTX 3090 GPU. The learning rate is set to 10\u22122. The number k of tasks Ti and their corresponding subgraphs Gi is set to 10,200, where the number of training tasks is 10,000 and the number of validation tasks is 200. The number of epochs for meta-training is set to 10, and the batch size is set to 64. For the NEEM module, the number of layers l is set to 3."}, {"title": "Evaluation Metrics", "content": "We independently conduct the experiments 10 times and compute the average to obtain the final reported results. We use Hits@n and MRR as evaluation metrics to validate the performance of the MetaEU model. Hits@n measures the average proportion of knowledge ranked within the top n in link prediction, while MRR calculates the mean reciprocal rank of the predictions. For the evaluation metrics used in this experiment, higher numerical values indicate better model performance on the corresponding tasks (e.g., link prediction). The calculation methods for the aforementioned metrics are defined as follows:\n$Hit@n = \\frac{1}{|Q|} \\sum_{q \\in Q} I(r_q \\leq n),$\n$MRR = \\frac{1}{|Q|} \\sum_{q \\in Q} \\frac{1}{r_q},$\nwhere Q is the total number of queries or test instances. rq is the rank position of the relevant item for query q. I(\u00b7) is the indicator function, which equals 1 if the condition inside is true and 0 otherwise."}, {"title": "5.3 Baseline", "content": "Existing KGE unlearning works are relatively few and differ significantly in task context from our proposed method. The existing approaches [7,8] rely on federated learning frameworks and perform KGE unlearning tasks on a fixed, predefined knowledge graph (where all entities in the KG are previously seen). In contrast, MetaEU employs multi-task training and learns meta-knowledge based on relation types and entity categories within the KG, enabling it to perform unlearning tasks on knowledge graphs containing unseen entities. As shown in Figure 3, although MetaEU's performance in Figure 3(a) is not outstanding, in Figure 3(b), the decline in MetaEU's Hits@10 metric is much faster than that of FedLU [7]. This may be because traditional KGE unlearning models cannot generate suitable embeddings for unseen entities in the early stages of training, whereas MetaEU addresses this issue more effectively."}, {"title": "5.4 Main Results and Analysis", "content": "As shown in Table 1, to demonstrate the effectiveness and generalizability of the MetaEU framework, we conducted experiments on four representative KGE models [9,10,11,12]. \"RAW\" represents the embeddings obtained by training the model on the original complete dataset before removing the forgetting set; \"Retrained\" represents the embeddings obtained by training the model on the remaining dataset after removing the forgetting set; \"Unlearned\" represents the embeddings obtained by applying unlearning to the RAW embeddings. Based on the above results, we found that: (i) RAW achieved the highest scores on the Hits@n and MRR metrics for both the Test and Forget sets; (ii) Retrained, after removing the forgetting set, had lower scores on the Test set compared to RAW, but higher scores on the Forget set than Unlearned. This suggests that simply removing the forgetting set and retraining the model does not fully mitigate the influence of the forgetting set; (iii) Unlearned exhibited the lowest performance on the Forget set but showed performance on the Test set that was comparable to RAW. This indicates that, through unlearning, the model effectively reduces the impact of the forgetting set while preserving its performance on the remaining set."}, {"title": "5.5 Ablation Study", "content": "We conduct ablation studies on different components of MetaEU. For the ensemble learning and ensemble unlearning components, we sequentially ablate models 1 to 4. To ablate the RAEEG component, we randomly initialize the entity embeddings during the training process. To ablate the NEEM component, we omit the step of enhancing entity embeddings through neighboring nodes and directly use the embeddings generated by RAEEG as the entity embeddings. As shown in Figure 4, we observe that when any base model is ablated within the ensemble learning or ensemble unlearning components, the model's performance does not show significant changes. This indicates that the ensemble model is robust, and there is some redundancy within the base learners and base unlearners. Additionally, we find that after removing the RAEEG or NEEM components, there is a noticeable decline in performance on the Test set for all embeddings. However, the Unlearned embeddings show an improvement on the Forget set, which suggests that RAEEG and NEEM can help the model perform better on the KGE unlearning task."}, {"title": "6 Conclusion", "content": "We propose MetaEU, a novel knowledge graph embedding unlearning model based on meta-learning framework. This framework leverages meta-training regime and the entity types and relational properties within the knowledge graph to learn meta-knowledge that is independent of specific unlearning task scenarios. By incorporating ensemble learning and ensemble unlearning processes, MetaEU can eliminate the influence of specific knowledge on the KGE model while preserving the overall performance of the model. Our experimental results demonstrate that MetaEU can efficiently perform KGE unlearning in unfamiliar scenarios with unseen entities, a capability that existing methods lack. In future work, we will explore the application of meta-learning-based KGE unlearning methods in multi-source knowledge graph fusion and investigate more effective KGE unlearning approaches."}]}