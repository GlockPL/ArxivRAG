{"title": "Distill Not Only Data but Also Rewards: Can Smaller Language Models Surpass Larger Ones?", "authors": ["Yudi Zhang", "Lu Wang", "Meng Fang", "Yali Du", "Chenghua Huang", "Jun Wang", "Qingwei Lin", "Mykola Pechenizkiy", "Dongmei Zhang", "Saravan Rajmohan", "Qi Zhang"], "abstract": "Distilling large language models (LLMs) typically involves transferring the teacher model's responses through supervised fine-tuning (SFT). However, this approach neglects the potential to distill both data (output content) and reward signals (quality evaluations). Extracting reliable reward signals directly from teacher models is challenging, as LLMs are optimized for generation rather than evaluation, often resulting in biased or inconsistent assessments. To address this limitation, we propose a novel distillation pipeline that transfers both responses and rewards. Our method generates pseudo-rewards through a self-supervised mechanism that leverages the inherent structure of both teacher and student responses, enabling reward learning without explicit external evaluation. The reward model subsequently guides reinforcement learning (RL), allowing iterative refinement of the student model after an SFT warm-up phase. Experiments on GSM8K and MMLU-PRO demonstrate that our method consistently outperforms traditional SFT-based approaches, enabling student models to surpass the performance of their teachers. This work highlights the potential for scalable, efficient distillation through structured self-supervised reward learning, reducing dependence on external reward supervision.", "sections": [{"title": "1. Introduction", "content": "Knowledge Distillation has emerged as a promising technique for mitigating the high computational demands of Large Language Models (LLMs) by training smaller student\nCorrespondence to: Lu Wang <wlu@microsoft.com>, Meng Fang <Meng.Fang@liverpool.ac.uk>. Preprint."}, {"title": "2. Related Work", "content": "In this section, we review several relevant topics of our method, including knowledge distillation and reinforcement learning from external feedback.\nKnowledge Distillation. Recent studies on knowledge dis-tillation for language models have primarily focused on transferring reasoning capabilities from large language mod-els (LLMs) to smaller models (Shridhar et al., 2022; Mag-ister et al., 2023). For example, Shridhar et al. (2022) em-ployed semantic decompositions to distill reasoning skills. Most existing approaches rely on supervised fine-tuning (Ho et al., 2023; Magister et al., 2023; Fu et al., 2023b) and lever-ages advanced LLMs such as the GPT series (Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023) as the guid-ance to generate high-quality data (Josifoski et al., 2023; Li et al., 2023b). Symbolic Chain-of-Thought Distillation (Li et al., 2023a) introduced a step-by-step reasoning frame-work, highlighting the potential of distilling complex reason-ing processes, while FLD (Morishita et al., 2023) focused on logical deductive reasoning. More recently, Guo et al. (2025) proposed distilling multiple small models via super-vised fine-tuning (SFT) over reasoning data generated by DeepSeek-R1. However, most methods treat LLMs merely as sources of reasoning chains, optimizing student models exclusively through supervised fine-tuning. Additionally, some works have explored reinforcement learning to further enhance reasoning capabilities; for instance, MARIO (Ram-nath et al., 2023) employs multiple external reward models to improve self-rationalization. In contrast, our work takes a different approach by leveraging LLMs not only for re-sponse generation but also for extracting reward signals, enabling a more comprehensive distillation process.\nReinforcement learning from External Feedback. Fol-lowing the success of Reinforcement Learning from Hu-man Feedback (RLHF) (Ziegler et al., 2019) in enabling the widespread application of large language models, re-searchers have increasingly explored ways to reduce human involvement in training through Reinforcement Learning from AI Feedback (RLAIF). As a pioneer in RLAIF, Consti-tutional AI (Bai et al., 2022) has demonstrated improved per-formance in tasks such as summarization, helpful dialogue generation, and harmless dialogue generation. Lee et al. (2023) further showed that RLAIF can achieve performance comparable to or even surpassing RLHF, as evaluated by human judges. SPIN (Chen et al., 2024) eliminates the need for explicit reward models by adopting an iterative DPO-like framework, where human-labeled winning responses are paired with the previous iteration's generations as losing responses. However, those method do not focus on training smaller models through knowledge distillation."}, {"title": "3. Methodology", "content": "In this section, we present our problem formulation and outline the proposed pipeline for distilling a small language model by distilling both teacher's responses and rewards.\nOur objective is to explore a new approach to improving student models through reinforcement learning by distilling both the responses and reward signals from teacher LLMs. Formally, we begin with only a large teacher model (T), a smaller student model (S), and a query dataset (D) contain-ing questions q\u00b2 for all i \u2208 |D|. We focus on tasks where, given a query q, the goal is to generate a complete response a, which consists of a Chain-of-Thought reasoning path c followed by a final answer y, as shown in Table 1\nThe overall pipeline, illustrated in Figure 2, consists of Data Distillation and Reward Distillation."}, {"title": "3.1. Data Distillation", "content": "In this phase, we autonomously generate and label responses to create a robust dataset for training both the reward model and fine-tuning the student model, given the query dataset D and the teacher model T.\nTeacher's Generation. To construct this dataset, we gen-erate multiple responses for each query using a powerful teacher model, T, under two different settings, controlled by temperature during generation: high-confidence (low temperature) and low-confidence (high temperature). The high-confidence set is designed to mimic annotators with the highest reliability, providing pseudo-labels y for the queries and identifying cases where the teacher fails to produce a valid response. In contrast, the low-confidence set serves to enhance response diversity during the warm-up stage, capturing more varied reasoning paths c. By sampling re-sponses across these two confidence categories, we ensure both reliability in pseudo-labeling and diversity in reason-ing styles, thereby creating a more comprehensive training dataset. The prompt templates used to query the teacher model are provided in Appendix A.\nTeacher's Evaluation \u2013 Pseudo Final Answer Voting. Evaluating responses is challenging because teacher models are primarily trained for text generation rather than assess-ment. Inspired by Xiong et al. (2023), we adopt a majority voting approach instead of directly prompting the teacher model to evaluate each answer. This method not only en-hances reliability but also reduces the computational cost of querying the teacher model. To ensure more reliable evaluations, we generate responses using a relatively low-temperature setting, reducing the uncertainty in the answers. These low temperature are set to 0, 0.1, 0.2 and generate multiple responses for each temperature. From these high-confidence responses, we derive a pseudo-final answer y* through majority voting. Specifically, if a single final an-"}, {"title": "3.2. Warm-Up via SFT", "content": "We begin by performing data distillation through super-vised fine-tuning (SFT). Specifically, we compile all teacher-generated responses that have been aligned with the pseudo final answer (labeled as correct) and incorporate them into the dataset for student warm up,\nDsft = {(q, a) | extract(a) = y*}, y* is the pseudo label of q.  (1)\nWe then train the student model for two epochs. Notably, queries without a reliable pseudo-label-i.e., those that the teacher model fails to solve-are excluded from the SFT warm-up process. This ensures that the student model learns from diverse high-quality responses, forming a solid foun-dation before transitioning to reinforcement learning.\nThe training objective in this stage is to minimize the nega-tive log-likelihood (NLL) loss:\nL(\u03b8) = -E(q,a)~Dsft log P(ak|a<k, q, \u03b8),  (2)\nwhere k denotes the index of words in the response a, includ-ing the final answer y for the given query q, and \u03b8 represents the parameters of the student model. We obtain a student model Swarmup after warm up phase."}, {"title": "3.3. Reward Distillation I: Reward Model Learning", "content": "Below, we describe how we distill the teacher model's eval-uation capability through reward model learning.\nWe propose a self-supervised schema that leverages the in-herent structure of NLP tasks. This schema evaluates both teacher-generated and student-generated responses, sys-tematically collecting, labeling, and filtering them through structured constraints to construct reward training data.\nA straightforward and efficient approach is to train a re-ward model to distinguish whether the teacher's answers are correct. However, this method can introduce significant distribution shift due to differences in generation patterns between the teacher and student models. To mitigate this issue, we primarily train the reward model on the student model's responses while also incorporating the teacher's responses for additional guidance."}, {"title": "Student's Generation & Self-supervised Evaluation.", "content": "Using a temperature of 0.7, we generate multiple responses from the student model for each query q, ensuring less con-fidence but diverse outputs. We then align these student responses with pseudo labels derived from the teacher's self-evaluations, constructing the dataset:\nDs := {(q, a, y*) | \u2200q \u2209 Dfail, \u2203a \u2208 as, y \u2260 y* }  (3)\nwhere as represents the set of student-generated answers, and y = extract(a) is the extracted final answer from a student response a, which is compared against the pseudo-label y*. Queries without a reliable pseudo-label, i.e., those in Dfail, are excluded from this dataset."}, {"title": "Constructing Training Data for Reward Model Learn-ing.", "content": "To determine which queries are used for reward model learning, we start with the dataset Ds. A query q is in the dataset Ds for reward model learning if at least one of the warm-up model's final answers, extracted from the response as, differs from its pseudo-label y*.\nFor each selected query q in Ds, we uniformly sample an equal number of:\n\u2022 positively labeled student responses as+\n\u2022 negatively labeled student responses as\u2212\n\u2022 positively labeled teacher responses aT+\nwhere aT represents all teacher-generated responses.\nThis process constructs the dataset for reward model learn-ing, denoted as:\nDr := {(q, (as+, ys+), (as\u2212, ys\u2212), (aT+, yT+))},\nwhere yS+ = yT+ = y* and yS\u2212 \u2260 y*.\nOur optimization objective comprises two key components: classification and preference modeling. Classification labels are directly derived from the teacher's evaluations, while we additionally construct preference labels to guide the re-ward model in distinguishing both correctness and reasoning quality: for a given query q, the student model's positive responses receive higher rewards than its negative ones; the teacher's positive responses are rewarded more highly than the student model's positive responses. This design is moti-vated by the observation that a student model may follow an incorrect reasoning path yet arrive at the correct final answer, whereas the teacher model typically provides a more concise and reliable reasoning process. By structuring the dataset in this manner, the reward model is exposed to a diverse range of responses, enabling it to differentiate not only between correct and incorrect answers but also to prefer responses that demonstrate more accurate reasoning."}, {"title": "Reward Model Learning Objective.", "content": "Given the constructed dataset DR, we define the learning objective for our reward model, R. Specifically, R takes a question-answer pair"}, {"title": "3.4. Reward Distillation II: Optimization through Reinforcement Learning", "content": "With the trained reward model R, we further refine the stu-dent model S after the warm-up phase using reinforcement learning (RL).\nData. The training data for RL is drawn from DR and Dfail, where DR consists of queries that the student failed to address, and Dfail contains queries that the teacher was unable to solve.\nReward Design. The quality of the reward model has a significant impact on the optimization process and perfor-mance of PPO. To ensure that it captures multiple aspects of response quality while remaining computationally effi-cient during Reinforcement Learning (RL), we augment our trained reward model with the following design:\n1. Answer Existence and Extractability. If the model fails to provide an extractable answer, we assign a reward of -5 and terminate the evaluation for this query."}, {"title": "2. Reward Model Score.", "content": "We use the predicted reward from the trained reward model R. Ideally, a positive re-ward value indicates correctness, while a higher reward reflects a better response."}, {"title": "3. Consistency Check with the Pseudo Final Answer in DR.", "content": "Finally, we compare the extracted answer with the pseudo-final answer y*, if applicable. If the extracted value does not match the pseudo-label, we adjust the reward using min(r, 0).\nTherefore, the final reward for a response a for question q is defined as:\n -5, if no extractable answer is found;\nR(q, a) = min(r, 0), if \u0177 \u2260 y*;\nr, if y = y*,  (7)\nwhere r = R(q, a) is the predicted reward from the re-ward model. We use the augmented R(q, a) in the further reinforcement learning.\nThis design ensures that the student model is discouraged from producing incomplete responses while also allowing the reward model's output to be further refined by aligning it with the pseudo-labeled final answer when applicable."}, {"title": "Optimization with PPO.", "content": "We employ Proximal Policy Op-timization (PPO) (Schulman et al., 2017) in RL to refine the student model Swarmup under the supervision of reward signals defined in Eq. 6. Let \u03b8 denote the parameters of Swarmup. The loss function for optimizing \u03b8 is,\nLPPO =E(q,a)~DRUDail [min(mt(0) At,\nclip(mt(0), 1 \u2013 \u03b5, 1 + \u03b5)At)]\n\u2212 \u03bb\u0395 [KL(\u03c0\u03b8(\u00b7 | a<t; q) || \u03c0\u03b8old (\u00b7 | a<t;q))]  (8)\nwhere mt(\u03b8) = \u03c0\u03b8(at|a\u03c0<t\u03b8,oldq)(at|lastq) represents the probability ratio between the updated policy \u03c0\u03b8 and the previous pol-icy \u03c0\u03b8old. The term At is the estimated advantage func-tion at time step t, while \u03b5 is the clipping threshold that constrains policy updates. The coefficient \u03bb controls the penalty term, and KL [\u03c0\u03b8(\u00b7|a<t; q) || \u03c0\u03b8old (a<t; q)] is the Kullback-Leibler (KL) divergence between the current and previous policies, ensuring stable updates."}, {"title": "4. Experiments", "content": "In this section, we evaluate the efficacy of our proposed method and conduct comprehensive ablation studies to as-sess its effectiveness and justification."}, {"title": "4.1. Setup", "content": "Evaluation Datasets. We evaluate our method on three widely used benchmarks: GSM8K (Cobbe et al., 2021): A"}, {"title": "4.2. Main Results", "content": "In this subsection, we report the performance of teacher and student models of different sizes on three benchmark tasks. The experimental results are provided in Table 2. Note that for GSM-Plus, the models are trained on GSM8K and evaluated on GSM-Plus, demonstrating the ability to generalize beyond the direct training task. According to the experimental results, our method consistently outperform other methods across varying teacher model capabilities. Starting with the stronger teacher, Llama3-70B, compared with SFT, the 1B student's accuracy are increased from 61.03% to 64.06% on GSM8K (+3.03%) and from 31.00% to 35.27% on MMLU-Pro (+4.27%); for a 3B student, our method achieves a +1.90% improvement on GSM8K and +2.96% on MMLU-Pro . When shifting to the less capable teacher, Llama3-8B, the improvement is more obvious: our 1B student still attains a +3.18% gain on GSM8K and a sub-stantial +9.38% jump on MMLU-Pro (22.62% to 32.00%). The 3B student follows a similar trend, with +3.27% on GSM8K and +8.24% on MMLU-Pro (31.78% to 40.02%). These results highlight the robustness of our method, which consistently yields performance improvements across differ-ent student sizes and teacher capabilities. Notably, in certain configurations\u2014shown in blue\u2014our method even enable the students outperform their respective teachers. For instance, under Llama3-8B, the 3B student surpasses the teacher on GSM8K (83.02% vs. 80.97%) and on MMLU-Pro (40.02% vs. 39.88%), demonstrating the effectiveness of our distillation approach in transferring knowledge with both data and rewards. Additionally, our method also provid a substantial boost to student models even under cross-task generalization (from GSM8K to GSM-Plus)."}, {"title": "4.3. Ablation Study", "content": "To further analyze the rationale behind our method, we conduct ablation studies from various perspectives. Unless otherwise specified, these ablations are performed using the teacher model Llama3-70B, the student model Llama3-1B, and the GSM8K dataset."}, {"title": "5. Conclusion", "content": "In summary, we propose a novel distillation framework that leverages both teacher-generated outputs and self-supervised reward signals to train a student model. By introducing reinforcement learning on top of SFT-based data distillation, this approach effectively sidesteps the biases of direct teacher evaluations and addresses the mismatch between the student model's inputs and the reward model in later training stages. Experimental results on GSM8K and MMLU-PRO demonstrate that this method not only outperforms purely SFT-based distillation strategies but also enables the student model to exceed the teacher's perfor-mance in certain metrics. Our work highlights the untapped potential of exploiting teacher LLMs' reward signals and offer a new, scalable paradigm for distilling large language models when reliable direct evaluation signals are absent."}, {"title": "Impact Statement", "content": "Our work seeks to contribute to the advancement of ma-chine learning by enhancing the efficiency and scalability of knowledge distillation for large language models. By incorporating both generative outputs and self-supervised reward signals, our approach minimizes dependence on ex-plicit teacher evaluations and human-labeled data. While this can improve accessibility and efficiency in model train-ing, it also introduces challenges related to bias propagation and the reliability of self-supervised reward modeling. We recognize these concerns and encourage further research to ensure the robustness and fairness of such methods in real-world applications."}, {"title": "A. Prompts in Experienments", "content": "We provide prompts for collecting teacher's responses in Figure 8 (GSM8K) and Figure 9 (MMLU-PRO).\nQ:\nThere are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How\nmany trees did the grove workers plant today?\nA:\nLet's break this down step by step!\nStep 1: There are 15 trees originally.\nStep 2: Then there were 21 trees after some more were planted.\nStep 3: So there must have been 21 - 15 = 6.\nThe answer is 6.\nQ:\nIf there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA:\nLet's break this down step by step!\nStep 1: There are originally 3 cars.\nStep 2: 2 more cars arrive, 3 + 2 = 5.\nThe answer is 5.\nQ:\nLeah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nA:\nLet's break this down step by step!\nStep 1: Originally, Leah had 32 chocolates.\nStep 2: Her sister had 42.\nStep 3: So in total they had 32 + 42 = 74.\nStep 4: After eating 35, they had 74 - 35 = 39.\nThe answer is 39.\nQ: {question}\nLet's break this down step by step!\nQuestion: {question of fewshot example 1}\nOptions: {options of fewshot example 1}\nAnswer: {answer of fewshot example 1}\nQuestion: {question of fewshot example 2}\nOptions: {options of fewshot example 2}\nAnswer: {answer of fewshot example 2}\nQuestion: {question}\nOptions: {options}\nAnswer:\nLet's break this down step by step!"}, {"title": "B. Implementation Details", "content": "B.1. Data Split\nFor GSM8K, we divided the original training dataset into training and validation sets, allocating 90% for training and 10%\nfor validation.\nFor MMLU-PRO, we first allocate 15% of the data for testing. Then, we split the remaining data into training and validation\nsets using a 90% to 10% ratio.\nB.2. Hyperparameter\nOur training pipeline consists of three stages: supervised fine-tuning (SFT) warm-up, reward model training, and proximal\npolicy optimization (PPO). Each stage plays a critical role in progressively improving the student model.\nDataset Specific Hyper-parameters We set the maximum generation length as 512 for GSM8K and 1024 for MMLU-\nPRO.\nSFT Warm-up The Supervised Fine-Tuning (SFT) phase serves to initialize the student model prior to reinforcement\nlearning. During SFT, we employ a learning rate of 5 \u00d7 10-6 and a sequence length of 512 tokens. The batch size varies\nbased on the specific teacher and student model configurations, as detailed in Table 5. Our dataset comprises majority-voted\nresponses, ensuring a robust foundation for subsequent optimization. For the warm-up phase, we utilize 4 H100 GPUs and\nperform full parameter training. The training process spans 4 epochs, with checkpoints saved at intervals specified in the\ntable. The optimal checkpoint is selected based on performance on the validation set. To accelerate training, we leverage\nDeepSpeed.\nReward Model Training The reward model is trained to guide PPO-based fine-tuning. This stage uses a learning rate of\n5 \u00d7 10-5, a batch size of 48 for student Llama3-1B and a batch size of 16 for student Llama3-3B, and 4 training epochs.\nWe apply early stop while the reward model performance stop increasing on validation set. The reward model is initialized\nfrom the student model after warm up. All reward models were trained on four H100 GPUs.\nPPO Training The PPO stage refines the student model through reinforcement learning with the reward model. We use\na learning rate of 1 \u00d7 10\u22125, a KL penalty coefficient of 0.2, and a value function coefficient of 0.1. The total number of\ntraining episodes is set to 200, 000, ensuring sufficient interaction with the reward model for stable policy improvement. We\napply early stop while the student model performance stop increasing on validation set. We present more hyper-parameters\nin Table 6.\nOurs w/o Data We apply a learning rate of 5 \u00d71 0\u20135 and KL coefficient of 0.1 in this ablation version. Other hyper-\nparameters are the same to PPO Training."}]}