{"title": "Navigating Semantic Relations: Challenges for Language Models in Abstract Common-Sense Reasoning", "authors": ["Cole Gawin", "Yidan Sun", "Mayank Kejriwal"], "abstract": "Large language models (LLMs) have achieved remarkable performance in generating human-like text and solving reasoning tasks of moderate complexity, such as question-answering and mathematical problem-solving. However, their capabilities in tasks requiring deeper cognitive skills, such as common-sense understanding and abstract reasoning, remain under-explored. In this paper, we systematically evaluate abstract common-sense reasoning in LLMs using the ConceptNet knowledge graph. We propose two prompting approaches: instruct prompting, where models predict plausible semantic relationships based on provided definitions, and few-shot prompting, where models identify relations using examples as guidance. Our experiments with the gpt-40-mini model show that in instruct prompting, consistent performance is obtained when ranking multiple relations but with substantial decline when the model is restricted to predicting only one relation. In few-shot prompting, the model's accuracy improves significantly when selecting from five relations rather than the full set, although with notable bias toward certain relations. These results suggest significant gaps still, even in commercially used LLMs' abstract common-sense reasoning abilities, compared to human-level understanding. However, the findings also highlight the promise of careful prompt engineering, based on selective retrieval, for obtaining better performance.", "sections": [{"title": "1 Background", "content": "Recent advancements in large language models (LLMs) have demonstrated their impressive ability to generate humanlike text and solve reasoning challenges with moderate complexity, ranging from question answering to mathematics [15, 31]. However, their performance can lack robustness when subjected to tasks that require deeper cognitive skills, such as common-sense understanding and abstract reasoning [5, 13]. These abilities are especially important in applications (such as healthcare and robotics) where requirements and inputs may ambiguously specified, but that require high reliability [1, 21]. Common-sense encompasses intuitive knowledge about the world i.e., understanding everyday concepts and their relationships [6, 23]. Abstract reasoning is generally defined as the ability to detect patterns and relationships, extrapolate from examples, and make connections between different ideas and concepts with minimum reliance on memorized knowledge [3, 19, 27].\nAlthough distinct, there is a clear overlap between both common-sense and abstract reasoning. However, there remain open challenges in evaluating common-sense abstract reasoning abilities in LLMs systematically and rigorously, especially when both abilities are involved in the same test [17]. To minimize noise, such an evaluation requires establishing clear and precise criteria [18], and must be systematic enough to be replicable. In recent years, AI evaluation (especially concerning LLMs) has emerged as a complex research agenda in its own right [7, 12], with concerns ranging from data, pre-training, and model contamination [14], to inappropriate (often, theory-agnostic) design of reasoning benchmarks for assessing generalization [11, 16].\nIn this paper, we propose and conduct such an evaluation using the ConceptNet knowledge graph [28]. ConceptNet encapsulates a broad array of general common-sense knowledge about the world. Entries in ConceptNet link two nodes together by edges representing semantic relations; for instance, /c/en/car would be linked to /c/en/vehicle by an edge labeled /r/IsA.\u00b9 ConceptNet contains a mix of words, short phrases, and common sayings, thereby serving as a diverse basis upon which to evaluate the aforementioned cognitive abilities [25]. Although the semantic types in ConceptNet are broad and shown to be amenable to further substructure analysis [26], their common-sense nature also facilitate complex evaluations [9, 30].\nOur specific contributions in this paper are as follows. Using ConceptNet as a base, we describe an experimental study using two prompting approaches for evaluating LLMs on common-sense abstract reasoning: instruct prompting with named relations and few-shot prompting with unnamed relations. The former tests the model's ability to use explicit relation names (like /r/IsA, /r/HasA, and /r/MadeOf) and definitions to predict the correct semantic relation, given a pair of head and tail entities. The latter uses few-shot"}, {"title": "2 Evaluation Framework", "content": "To ensure consistency and reproducibility in our evaluation, we developed two stable datasets: one for evaluation and another for providing the LLM with relation examples (which is important for the few-shot prompting experiments). The evaluation dataset was constructed by randomly sampling edges corresponding to each semantic relation from ConceptNet, subject to specific constraints to maintain the integrity of the samples. First, only edges connecting English nodes (Een) were considered. Next, we filtered out reflexive edges, ensuring that the two nodes connected by an edge were distinct (Edistinct). Finally, we selected unique edges (Eunique), eliminating duplicates where multiple edges with the same argument nodes (arg1, arg2) were present. The resulting subset (R) provided the foundation for our evaluation, from which 100 samples were randomly drawn for each semantic relation. The second dataset, designed for semantic relation examples, was synthetically generated to provide prototypical representations of each relation. Using the gpt-40-mini model [22], we generated three examples for each semantic relation in the format NODEA \u2192 NODEB. These examples were then parsed and stored for consistency. Unlike the evaluation dataset, which relied on random sampling from ConceptNet, this approach ensured that the examples were clear, representative, and directly aligned with the semantic relation under consideration.\nNext, we describe the two specific prompting approaches designed to evaluate the effectiveness of LLMs in predicting common-sense semantic relations between pairs of entities (Figure 1). All experiments were conducted using gpt-40-mini via the OpenAI API.\nInstruct Prompting with Named Relations. The first methodology involves providing the model with exact definitions for each semantic relation. The model is asked to respond with the top N ranked relations for each pair of nodes. Relations are explicitly named using their actual identifiers such as IsA, HasA, and MadeOf. The prompt for this methodology includes a task description that instructs the model to determine the correct relation between two nodes. It also contains a comprehensive list of all possible relations along with their precise definitions, and the specific nodes A and B for which the relation is to be determined.\nThe predicted rankings are then evaluated using Normalized Discounted Cumulative Gain (NDCG) [10], a metric that measures the quality of the ranking provided by the model. The NDCG metric is calculated by first computing the Discounted Cumulative Gain (DCG) for a particular ranked list ($DCG_p = \\sum_{i=1}^p \\frac{2^{reli}-1}{log_2(i+1)}$), where reli is the relevance score of the result at position i and p is the number of results in the list. The ideal DCG (IDCG) is the DCG value"}, {"title": "3 Results", "content": "On the instruct prompting experiments, we found negligible differences in the average NDCG score as the model was allowed to rank more or less (top 10, 5, 3, and full) relations in its response unless the model was restricted to responding with only one possible relation (Figure 2). Notably, for the majority of pairs, the correct relation was not returned in the top 1 across most rankings, showing that even a commercial model like gpt-40-mini needs significant improvement before it can match human-level common-sense. This is in contrast with performance that has been reported for question-answering benchmarks [5, 24]. However, because our benchmark construction is more heavily dependent on semantics, rather than pure natural"}, {"title": "4 Conclusion and future work", "content": "In this paper, we propose and conduct a systematic evaluation of abstract common-sense reasoning abilities in large language models using the ConceptNet knowledge graph. Our study utilizes two prompting methods to assess the capabilities of gpt-40-mini, including instruct prompting with named relations and few-shot prompting with unnamed relations. Our results indicate that while gpt-40-mini shows a foundational understanding of common sense knowledge, it often struggles to accurately identify the correct relations, especially in more abstract reasoning tasks without explicit guidance. Specifically, in our instruct prompting experiments, the model maintained consistent NDCG scores when outputting multiple relations, whether selecting from the top 3, 5, 10, or all possible relations. However, its performance declined significantly when restricted to predicting a single relation. In our few-shot prompting experiments, the model's accuracy improved substantially when selecting from five possible relations rather than the full set, though we observed notable bias toward certain relations, particularly '/r/IsA'.\nAn important line of future research is to replicate the experiment on models other than gpt-40-mini; specifically models like Llama and OpenAI's 01. However, it bears noting that the latter is still cost-prohibitive at the time of writing, similar to gpt-4. For this reason, gpt-40-mini still continues to be the preferred model"}]}