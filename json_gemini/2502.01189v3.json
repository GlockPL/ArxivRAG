{"title": "Compressed Image Generation with Denoising Diffusion Codebook Models", "authors": ["Guy Ohayon", "Hila Manor", "Tomer Michaeli", "Michael Elad"], "abstract": "We present a novel generative approach based on Denoising Diffusion Models (DDMs), which produces high-quality image samples along with their losslessly compressed bit-stream representations. This is obtained by replacing the standard Gaussian noise sampling in the reverse diffusion with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors. Surprisingly, we find that our method, termed Denoising Diffusion Codebook Model (DDCM), retains sample quality and diversity of standard DDMs, even for extremely small codebooks. We leverage DDCM and pick the noises from the codebooks that best match a given image, converting our generative model into a highly effective lossy image codec achieving state-of-the-art perceptual image compression results. More generally, by setting other noise selections rules, we extend our compression method to any conditional image generation task (e.g., image restoration), where the generated images are produced jointly with their condensed bit-stream representations. Our work is accompanied by a mathematical interpretation of the proposed compressed conditional generation schemes, establishing a connection with score-based approximations of posterior samplers for the tasks considered. Code and demo are available on our project's website.", "sections": [{"title": "1. Introduction", "content": "Denoising Diffusion Models (DDMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) have emerged as an effective tool for generating samples from complex signal distributions (e.g., natural images). Hence, DDMs are commonly leveraged to facilitate a variety of downstream tasks, such as text-to-image synthesis (Ramesh et al., 2021; Rombach et al., 2022; Saharia et al., 2022), editing (Meng et al., 2022; Huberman-Spiegelglas et al., 2024), compression (Theis et al., 2022; Elata et al., 2024; K\u00f6rber et al., 2024), and restoration (Kawar et al., 2022; Chung et al., 2023). Common to many of these applications is the reliance on iterative sampling from a continuous Gaussian distribution, yielding an unbounded representation space.\nThis work embarks on the hypothesis that such an infinite representation space is highly redundant. For example, consider any stochastic diffusion generative process with T = 1000 sampling steps (e.g., DDPM (Ho et al., 2020)). Suppose that at each timestep, the generative process is restricted to choosing between only two fixed noise realizations. Sampling could then lead to 21000 different outputs, an incredibly large number exceeding the estimated amount of atoms in the universe. Thus, in principle, such a process could cover the distribution of natural images densely.\nWe harness this intuition and propose Denoising Diffusion Codebook Models (DDCM), a novel DDM generation scheme for continuous signals, leveraging a discrete and finite representation space. In particular, we first construct a chain of codebooks, where each is a sequence of pre-sampled Gaussian noise vectors. These codebooks are constructed once and remain fixed for the entire lifetime of the model. Then, during the generative process, we simply randomly pick the noises from the codebooks instead of drawing them from a Gaussian distribution, as shown in Fig. 2. Since we alter only the sampling process, DDCM can be applied using any pre-trained DDM. Interestingly, we find that our proposed discrete and finite representation space is indeed expressive enough to retain the generative capabilities of standard DDMs, even when using incredibly small codebooks. Since our generative process is entirely governed by the noise indices picked during the generation, an important consequence is that every generated image can be perfectly reconstructed by repeating the process with its corresponding indices.\nWe leverage this property to solve a variety of tasks, using gradient-free noise selection rules to guide the DDCM generation process. In particular, by choosing the discrete noises to best match a given image, we achieve state-of-the-art per-"}, {"title": "2. Related Work", "content": "Compression. Image compression has seen significant progress in recent years, with the penetration of neural networks to this domain. Neural methods range from constructing specialized architectures (Ball\u00e9 et al., 2017; Zhu et al., 2022; Jiang et al., 2023; Jiang & Wang, 2023) to relying on different generative models such as GANS (Mentzer et al., 2020; Muckley et al., 2023; Iwai et al., 2024) and VAEs (Theis et al., 2017). Recent compression methods leverage DDMs and offer high perceptual quality results, by training models from scratch (Yang & Mandt, 2023; Ghouse et al., 2023), fine-tuning existing models (Careil et al., 2023; K\u00f6rber et al., 2024), or using pre-trained DDMs in a zero-shot manner (without further training) (Theis et al., 2022; Elata et al., 2024). Current solutions in the latter category are highly computationally demanding, either due to their communication schemes (e.g., reverse channel coding (Theis & Ahmed, 2022; Theis et al., 2022)) or their need to perform thousands of denoising operations (Elata et al., 2024). Our work falls into this last category, offering a novel and highly effective compression scheme with a fast bit-stream communication method and computational demands that match standard use of DDMs.\nDiscrete Generative Modeling for Continuous Data.\nRecent works have explored discrete generative modeling of continuous data distributions. These employ various discrete representations, such as vector quantized latent to-"}, {"title": "3. Background", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) generate samples from a data distribution po by reversing a diffusion process that gradually adds random noise to samples from the data. Specifically, the diffusion process starts with x0 ~ po and produces the chain x0, X1, . . . , xr via\n$x_i = \\sqrt{\\alpha_i}x_{i-1} + \\sqrt{1 - \\alpha_i}z_i, z_i \\sim \\mathcal{N}(0, I)$,"}, {"title": "4. Denoising Diffusion Codebook Models", "content": "Method. Equation (2) depicts the standard DDPM sampling approach, where the added noise is sampled from a continuous Gaussian distribution. DDCM instead uses"}, {"title": "5. Image Compression with DDCM", "content": "Method. Since sampling with DDCM yields compact bit-stream representations, a natural endeavor is to harness DDCM for compressing real images. In particular, to compress an image xo, we leverage the predicted x0|i (Eq. (4)) at each timestep i and compute the residual error from the target image, x0 - x0|i. Then, we guide the sampling process towards xo by selecting the codebook entry that maximizes the inner product with this residual,\n$k_i = \\underset{k\\in\\{1,...,K\\}}{\\arg \\max} \\langle C_i(k), x_0 - x_{0|i}\\rangle,$\nwhere the size of the first codebook CT+1 is K = 1. This process is depicted as the compression branch in Fig. 2, where the resulting set of chosen indices {k}_i^{T+1} is the compressed bit-stream representation of the given image."}, {"title": "6. Compressed Conditional Generation", "content": "We showed that DDCM can be used as an image codec by following a simple index selection rule, guiding the generated image towards a target one. Here, we generalize this scheme to any conditional generation task, considering the more broad framework of compressing conditionally generated samples. This is a particularly valuable framework in scenarios where the input condition y is bit rate intensive, e.g., where y is a degraded image and the goal is to produce a compressed high-quality reconstruction of it. To the best of our knowledge, this task, which we name compressed conditional generation, has only been thoroughly explored for text input conditions (Rajesh et al., 2023).\nA naive solution to this task could be to simply compress the outputs of any existing conditional generation scheme. Here we propose a novel end-to-end solution that generates the outputs directly in their compressed form. Importantly, note that our approach in Sec. 4 requires the condition y for decompressing the bit-stream. While this is not a stringent requirement when the condition is lightweight (e.g., a text prompt), this approach is less suitable when storage of the condition signal itself requires a long bit-stream. The solutions we propose in this section enable decoding the bit-stream without access to y.\nCompressed Conditional Generation with DDCM. We propose generating a conditional sample by choosing the indices ki in Eq. (6) via\n$k_i = \\underset{k\\in\\{1,...,K\\}}{\\arg \\min} L(y, x_i, C_i, k),$"}, {"title": "6.1. Compressed Posterior Sampling for Image Restoration", "content": "Our compressed conditional sampling approach can be utilized for solving inverse problems via posterior sampling. Specifically, we consider inverse problems of the form y = Axo, where A is some linear operator. We restrict our attention to unconditional diffusion models and solve the problems in a \u201czero-shot\u201d manner (similarly to Kawar et al. (2022); Chung et al. (2023); Wang et al. (2023b)). To generate conditional samples, we propose using the loss\n$L(y, x_i, C_i, k) = ||y - A(\\mu_i(x_i) + \\sigma_iC_i(k))||^2.$\nNote that Eq. (10) attains a lower value when \u03c3i ACi(k) points in the direction that perturbs \u0391\u03bci(xi) towards y. Thus, our conditional generative process aims to produce a reconstruction x that satisfies Ax \u2248 y, implying that we approximate posterior sampling (Ohayon et al., 2023). Notably, when assuming that pi (y|x\u2081) is a multivariate normal distribution centered around Ax\u2081 (as in (Jalal et al., 2021)), the chosen codebook noise Ci (ki) approximates the gradient Vx log pi (y xi) and Eq. (10) becomes a proxy of Eq. (9).\nFollowing (Chung et al., 2023; Wang et al., 2023b), we implement our method using the unconditional ImageNet 256 \u00d7 256 DDM trained by Dhariwal & Nichol (2021). We fix K = 4096 for all codebooks, resulting in a compressed bit-stream of approximately 0.183 BPP for each generated"}, {"title": "6.2. Compressed Real-World Face Image Restoration", "content": "Real-world face image restoration is the practical task of restoring any degraded face image, without any knowledge of the corruption process it has gone through (Wang et al., 2021; Gu et al., 2022; Wang et al., 2022; Zhou et al., 2022; Wang et al., 2023c; Lin et al., 2024; Yue & Loy, 2024; Chen et al., 2024b; Ohayon et al., 2024). We propose a novel method capable of optimizing any no-reference image quality assessment (NR-IQA) measure at test time (e.g., NIQE (Mittal et al., 2013)), without relying on gradients.\nSpecifically, at each timestep i, we start by picking two indices \u2014 one that promotes high perceptual quality, ki, P, and another that promotes low distortion, ki, D. Then, we choose between ki,p and ki,D the index that better optimizes a desired balance of the perception-distortion tradeoff (Blau & Michaeli, 2018). Formally, letting r(y) \u2248 E[xo|y] denote the approximate Minimum Mean-Squared-Error (MMSE) estimator of this task, we pick ki,D via\n$k_{i,D} = \\underset{k\\in\\{1,...,K\\}}{\\arg \\max} \\langle C_i(k), r(y) - x_{0|i}\\rangle.$\nNote that this index selection rule is similar to that of our standard compression, replacing xo in Eq. (7) with r(y). This choice of indices in DDCM would lead to a reconstructed estimate of the MMSE solution y(y), yielding blurry results with low distortion (Blau & Michaeli, 2018). In contrast, randomly picking a sequence of indices in DDCM would produce a high quality sample from the data distribution po. Therefore, we randomly choose ki,p ~ Unif({1, . ., K}). Then, we use the DDM and compute x0|i-1 for each index k \u2208 {ki,D, ki,P} separately, denoting each result accordingly by x_{0|i-1}^{(k)}. The final index is picked to optimize the perception-distortion tradeoff via\n$k_i = \\underset{k\\in\\{k_{i,D},k_{i,P}\\}}{\\arg \\min} MSE(r(y), x_{0|i-1}^{(k)}) + \\lambda Q(x_{0|i-1}^{(k)}),$ where Q(\u00b7) can be any NR-IQA measure, even a non-differentiable one. In App. C.4 we explain our choice to set r(y) as an MMSE estimator."}, {"title": "7. Discussion", "content": "We introduced DDCM, a novel generative approach for DDMs that produces high-quality image samples jointly with their lossless compressed bit-stream representations. We found that DDCM achieves comparable generative performance to DDPM, even when the codebooks are extremely small. We leveraged DDCM to solve several compressed image generation tasks, including image compression and compressed restoration, where we achieved state-of-the-art results. Besides image restoration, our compressed conditional generation framework can be used for any type of diffusion guidance, e.g., for text-conditional generation. We demonstrate this option in Apps. C.5 and C.6, introducing new classifier-based and classier-free guidance methods that do not use y for decompression. Moreover, we present in App. C.7 preliminary results for compressed image editing using DDCM, by decompressing an image using a desired edit text prompt.\nWhile our empirical results are encouraging, our work does not explain theoretically why DDCM sampling and our"}]}