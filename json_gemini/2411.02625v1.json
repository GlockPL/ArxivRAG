{"title": "EmoSphere++: Emotion-Controllable Zero-Shot\nText-to-Speech via Emotion-Adaptive Spherical\nVector", "authors": ["Deok-Hyeon Cho", "Hyung-Seok Oh", "Seung-Bin Kim", "Seong-Whan Lee"], "abstract": "Abstract-Emotional text-to-speech (TTS) technology has\nachieved significant progress in recent years; however, challenges\nremain owing to the inherent complexity of emotions and limita-\ntions of the available emotional speech datasets and models. Pre-\nvious studies typically relied on limited emotional speech datasets\nor required extensive manual annotations, restricting their ability\nto generalize across different speakers and emotional styles. In\nthis paper, we present EmoSphere++, an emotion-controllable\nzero-shot TTS model that can control emotional style and\nintensity to resemble natural human speech. We introduce a novel\nemotion-adaptive spherical vector that models emotional style\nand intensity without human annotation. Moreover, we propose\na multi-level style encoder that can ensure effective generalization\nfor both seen and unseen speakers. We also introduce additional\nloss functions to enhance the emotion transfer performance for\nzero-shot scenarios. We employ a conditional flow matching-\nbased decoder to achieve high-quality and expressive emotional\nTTS in a few sampling steps. Experimental results demonstrate\nthe effectiveness of the proposed framework.\nIndex Terms-Emotional speech synthesis, emotion transfer,\nemotion style and intensity control, zero-shot text-to-speech", "sections": [{"title": "I. INTRODUCTION", "content": "MOTIONS are interrelated in a highly systematic fash-\nion [1]. For example, the emotion of sadness can be\nexpressed with derivative states of primary emotions such\nas feeling hurt or lonely, depending on the style and in-\ntensity. Recently, emotional text-to-speech (TTS) technology\nhas experienced rapid developments, increasing the interest\nin high-level interpretable emotion control [2]\u2013[5]. Control-\nlable emotional TTS represents a breakthrough in reproducing\nhuman-like emotions in speech synthesis, thus enabling more\nemotionally intelligent interactions between humans and com-\nputers. Although researchers have made significant progress in\ncontrolling emotional intensity, the ability to precisely control\nemotional style must be further improved.\nThe modeling of diverse emotional styles and intensities\nrepresents a significant challenge in controllable emotion\nTTS. A reason for this challenge is that emotion style and\nintensity are even more subjective and complex than dis-\ncrete emotion categories, making them difficult to model.\nTwo general approaches for achieving controllable emotional\nTTS involve controlling conditioning features or manipulating\ninternal emotion representations. That is, one approach uses\nconditioning features of emotion intensity, such as relative\nranking matrices [6]\u2013[11], distance-based quantization [12],\nor voiced, unvoiced, and silence (VUS) states [13]. The alter-\nnative approach involves the manipulation of internal emotion\nrepresentations through the application of scaling factors [14],\n[15] or interpolation of the embedding space [16]. However,\ndespite these methods, the explicit control of emotion style\nand intensity remains a largely unexplored topic in emotional\nspeech synthesis.\nAnother approach to controlling emotional expression in-\nvolves utilizing emotional dimensions. Compared to the dis-\ncrete emotion approach, the dimensional approach, such as\nRussell's circumplex model, provides a more precise method\nfor capturing the nuances between different emotional states\n[1]. Recently, studies on TTS systems have attempted to\ncontrol emotional attributes through the emotion dimension\n[17], [18]. In one of these studies, a prosody control block is\nextended by incorporating the continuous space of arousal and\nvalence to allow interpretable emotional prosody control [18].\nAnother study proposes an expressive TTS model with a semi-\nsupervised latent variable to control emotions in six discrete"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "The processes of defining and expressing emotions have\ngarnered significant interest in psychology [1], [29]\u2013[31].\nEmotion theorists divide emotion theory into discrete [27],\n[32] and dimensional models [1], [33]. Notably, discrete\nmodels only represent the most straightforward way, whereas\ndimensional models aim to offer a continuous and fine-grained\ndescription.\nEmotion labels correspond very closely to the categories that\nwe use in our daily lives. Paul Ekman [32] derived six primary\nemotions: happiness, anger, disgust, sadness, anxiety, and\nsurprise based on universally recognized facial expressions.\nHowever, this approach overlooks the nuanced variations of\nemotions. Plutchik's emotion wheel [27] proposes eight pri-\nmary emotions and suggests that all other emotions arise as\nderivative states of the primary emotions. Furthermore, varying\nthe intensity of primary emotions on the emotion wheel can\nproduce the wide range of emotions that humans experience.\nAlthough people can explicitly express emotions, modeling\nthe relationships between discrete emotional states presents a\nchallenge.\nResearchers have introduced dimensional models to compu-\ntationally interpret the relationships between emotional states.\nRussell's circumplex model [1] suggests a two-dimensional\ncircular space that spans the independent and bipolar dimen-\nsions of arousal and valence. Building on this, researchers\nhave attempted to extend the model to the third dimension of\ndominance to denote the location of emotion within this space\n[33]. In the valence-arousal space, intensity is often equated\nwith arousal; moreover, Reisenzein demonstrated that using\nthe angle and length of the vector in polar coordinates is the\nonly possible option for interpreting the relationships between\nemotions [28], [34]. Recently, the speech processing domain\nhas seen various studies on emotion recognition that leverage\nthe emotional dimension [35]\u2013[38]."}, {"title": "B. Controllable Emotional Speech Synthesis", "content": "Recently, speech synthesis models have exhibited signif-\nicant developments [39]\u2013[41]; therefore, controllable emo-\ntional speech synthesis research is being aggressively pur-\nsued [3]\u2013[5]. Recording and annotating emotional datasets\ncovering multiple emotional styles is incredibly challenging.\nResearchers typically use the following controllable emotional\nspeech synthesis methods to utilize general emotional datasets:\n1) emotion label- and 2) reference-based approaches.\nThe emotion label-based approach aims to properly model\nconditioning input to reflect the complex nature of emo-\ntions. Researchers typically model emotion intensity using a\nlearned ranking function [42], as employed in [6]\u2013[11]. The\nranking function [42] seeks a ranking matrix based on the\nrelationships between the low-level and different high-level\nemotional expressions using support vector machines. The\nmodel receives the emotional intensity of emotional samples\nas a conditioning input for training. However, this method\ntends to rely on emotion labels and introduces bias into\ntraining through separate stages. Most models that use ranking\nfunctions adopt fine-tuning to control emotion intensity on a\nsingle-speaker dataset; however, some of these methods have\nnoticeably degraded speech quality [7], [10]. Moreover, certain\nresearch studies have utilized conditioning input such as\ndistance-based quantization [12] and VUS states [13] to model\nemotional intensity. However, these methods are still limited\nto several predefined emotion labels and lack differentiation\namong samples within the same emotion label.\nAs emotional speech synthesis often lacks multiple emo-\ntional style labels, reference-based approaches are famous for\nusing reference audio to transfer emotional styles. Several\nstudies have controlled emotion intensity through operations\non representative emotion embedding. The scaling factors ap-\nproach [14], [15] reflects fine-grained emotion representation\nthrough multiplication. In addition to the scaling approach,\nthe interpolation approach proposed by [16] controls emotion\nintensity through an inter-to-intra emotional distance ratio\nalgorithm. However, the structure of the embedding space\ninfluences model performance and complicates the process of\nfinding proper parameters for scaling or interpolation.\nHowever, these frameworks can neither be explicitly tuned\nsuch as label-based approaches nor capture fine-grained emo-\ntion representations such as reference-based approaches. This\nstudy aims to fill this research gap by explicitly modeling\nEASV of emotion style and intensity variations."}, {"title": "C. Style and Emotion Transfer in Text-to-Speech", "content": "Researchers in the TTS community have studied style\nand emotion transfer for decades. Some studies introduced\nthe concept of global style tokens (GSTs) [43", "44": "an autoregressive multi-speaker\nTTS model based on Tacotron that utilizes GSTs. Li et al. [15", "45": "and YourTTS [46", "47": "proposed a multi-level style\nadapter to obtain different styles, including a global latent\nrepresentation with speaker and emotion features. However,\nprevious research has lacked methods to effectively process\nstyle and disentangle speech factors. Our work handles well-"}]}