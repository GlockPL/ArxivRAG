{"title": "Keypoint Abstraction using Large Models\nfor Object-Relative Imitation Learning", "authors": ["Xiaolin Fang", "Bo-Ruei Huang", "Jiayuan Mao", "Jasmine Shone", "Joshua B. Tenenbaum", "Tom\u00e1s Lozano-P\u00e9rez", "Leslie Pack Kaelbling"], "abstract": "Generalization to novel object configurations and\ninstances across diverse tasks and environments is a critical\nchallenge in robotics. Keypoint-based representations have been\nproven effective as a succinct representation for capturing\nessential object features, and for establishing a reference frame\nin action prediction, enabling data-efficient learning of robot\nskills. However, their manual design nature and reliance on\nadditional human labels limit their scalability. In this paper, we\npropose KALM, a framework that leverages large pre-trained\nvision-language models (LMs) to automatically generate task-\nrelevant and cross-instance consistent keypoints. KALM distills\nrobust and consistent keypoints across views and objects by\ngenerating proposals using LMs and verifies them against a\nsmall set of robot demonstration data. Based on the generated\nkeypoints, we can train keypoint-conditioned policy models that\npredict actions in keypoint-centric frames, enabling robots to\ngeneralize effectively across varying object poses, camera views,\nand object instances with similar functional shapes. Our method\ndemonstrates strong performance in the real world, adapting\nto different tasks and environments from only a handful of\ndemonstrations while requiring no additional labels.", "sections": [{"title": "I. INTRODUCTION", "content": "A long-standing goal in robotics is to develop learning\nmechanisms that allow efficient acquisition of robot skills\nacross a wide range of tasks, requiring a feasible amount of\ndata while generalizing effectively to different object poses\nand even instances. Such generalization is crucial for enabling\nrobots to perform robustly in diverse environments. One\ncommon strategy for improving data efficiency is to leverage\nabstractions. Researchers have explored object-centric [1],\npart-centric [2], and keypoint-centric representations [3] for\ntasks spanning rigid-body manipulation to deformable object\nhandling, such as ropes and clothes. Among these, keypoints\noffer a versatile abstraction for many robotic tasks, capturing\nessential object features with a low-dimensional encoding.\nHowever, despite their potential for data efficiency and\ngeneralization, constructing a good keypoint representation\ncan be tedious, both during training and at test time. Such\ntraining typically requires human experts to design task-\nspecific keypoints, while deploying keypoint-centric models\nin real-world necessitates visual modules capable of detect-\ning these keypoints, which often requires additional data\ncollection and human annotations."}, {"title": "II. METHOD", "content": "Illustrated in Figure 2, our proposed method distills\nkeypoints through an iterative procedure, alternating between\ncandidate proposals using large pre-trained models and\nverification based on a small robot demonstration dataset.\nOnce the keypoints are distilled, we use their visual and\ngeometric features to train a keypoint-conditioned diffusion\nmodel for generating object-relative robot actions."}, {"title": "A. Problem Formulation", "content": "Formally, for each skill \u03b1, we require a single video of\nthe robot successfully executing the task H\u03b1, a handful\nof demonstration trajectories (5 to 10) D\u03b1, and a natural\nlanguage task description d\u03b1. Each trajectory D\u03b1 contains\nan initial observation of the scene, represented as a calibrated\nRGBD image I\u03b1 and a robot joint trajectory T\u03b1. No additional\nlabels such as keypoints are required.\nOur first goal is to distill a set of keypoints that are\ntask-relevant and consistent across observed images in all\ndemonstration trajectories {Ii}. We represent this keypoint\nset using 3D locations and their features in the first frame of\nthe demonstration video, denoted as K0. The second goal is\nto learn a trajectory prediction model, based on the distilled\nkeypoints and demonstration trajectories D\u03b1. In this work,\nwe assume the trajectory of each skill is segmented into two\ndistinct phases: an approaching phase, during which the robot\nmoves freely towards the object, and an execution phase.\nThe trajectory prediction model only predicts the execution\ntrajectory, focusing only on the actions needed to manipulate\nthe objects. The segmentation of the trajectory can be done\nthrough thresholding the distance to the closest keypoint,\nor through methods checking more fine-grained trajectory\nstatistics [5], [6], [7]. For brevity, in the following, we omit\nthe superscript \u03b1 when there are no ambiguities."}, {"title": "B. Keypoint Desiderata", "content": "In our framework, the keypoints serve both as an abstraction\nof the observational input and the basis of an object-relative\naction frame. To ensure that the abstraction is effective and\nthe frame is robust to changes in the environment, we define\ntwo criteria: task relevance and cross-instance consistency.\nTask relevance. To allow generalization to different scene\nconfigurations of the same task, the keypoints must be task-\nrelevant. For example, for the task of lifting the handle of a\ncoffee machine, the points on the handle are ideal candidates\nwhereas those on the water reservoir are not because the\nlatter varies across different machines and does not directly\nsupport the task completion.\nGiven a demonstration D\u1d62 = (Ii, Ti), the skill description\nd, as well as a keypoint k and the corresponding position\npk,i = \u03c6(k, Ii) in Ii, a pretrained vision-language model\nwill implicitly assign a score \u03c8 to this keypoint k, denoted\nas \u03c8(k, Di, d, pk,i). Our overall goal is to find a set of\nkeypoints K such that \u03c8(k, Di, d, pk,i) is high for all training\ndemonstrations Di.\nCross-instance consistency. Furthermore, it is essential that\nthe keypoints are consistently identifiable across observations\nregardless of the object pose, camera view, or detailed shape\nof objects. For example, within a task-relevant object part,\na point on the corner may be favored over one on a plain\nsurface, due to its saliency and a lower degree of ambiguity.\nWe evaluate the task relevance of a keypoint by leveraging\npre-trained vision-language models, and check the cross-\ninstance consistency using a keypoint detection function \u03c6.\nOur goal is to find a set of keypoints that are both task-relevant\nand consistently identifiable."}, {"title": "C. Keypoint Proposal and Verification", "content": "Our keypoint proposal and verification pipeline works in\nthree steps. First, we prompt a pre-trained vision language\nmodel (VLM) to select task-relevant image regions. Within the\nregions, we generate queries to image segmentation models\nto generate candidate object part masks, which are further\nranked and selected by a second query to the VLM. We\nsample fine-grained keypoint proposals within the selected\nmask and score them based on consistency across all query\nimages from the training demonstration set. This process will\neither return a set of final keypoints or declare failure, leading\nto another iteration of keypoint proposal. The overall process\nis illustrated in Figure 2a and Algorithm 1.\nCoarse-grained region proposal VLM\u2081(H, d). Our input\nto the VLM consists of a sequence of images H showing a\nsingle demonstration video of the robot executing the task,\nalong with a natural language description of the skill d (e.g.,\n\"open the top drawer\"). We aim to identify the regions of\ninterest in the initial image, Ift, associated with this video.\nWe present Ift with an overlaid grid, where each grid cell is\nindexed by a unique text label, and query the VLM to select\nthe grid indices corresponding to the task-relevant regions.\nIn addition to the grid index, we employ zero-shot chain-\nof-thought [8] prompting, encouraging the VLM to generate"}, {"title": "D. Learning Keypoint-Centric Trajectory Prediction Models", "content": "From the keypoint proposal and verification process, we\nhave determined a set of keypoints K, which captures the\nmost salient and task-relevant object parts for the skill\na. For each demonstration trajectory Di, we also have a\nset of corresponding points Pi from the previous keypoint\ndetection step. Conditioned on the sparse keypoint locations\nand features of these detected points, we directly generate a\ntrajectory TEE for the 6 DoF pose of the robot's end-effector\nusing the Diffuser [11], a trajectory-level diffusion model.\nInternally, the Diffuser learns a score function \u03f5\u03b8(TEE | C)\nparameterized by \u03b8, which captures the gradient of the data\ndistribution over TEE, where C is the conditional input to the\ndiffuser.\nWe have two key design choices here to facilitate generaliza-\ntion: using a sparse keypoint-based input, and having actions\npredicted in a keypoint-centric, object-relative coordinate"}, {"title": "E. Implementation", "content": "We use GPT-40 [12] as our VLM and Segment-Anything\nModel [9] (SAM) as our image segmentation model. In the\nsimilarity function \u03c6\u2217, we use a combination of pre-trained\nimage features (DINO [13] with FeatUp [14]) and analytic\n3D features Fast Point Feature Histograms (FPFH [15]). The\noverall (cosine) similarity in Equation 1 is defined as:\n\u27e8Fq(p), Fref(p0)\u27e9 = \u03bb1 \u00b7 \u27e8FDINO(p), FDINO(p0)\u27e9\n+ \u03bb2\u27e8FFPFH(p), FFPFH(p0)\u27e9,\n(3)\nwhere \u03bb1 = 0.75 and \u03bb2 = 0.25 in our experiments.\nFor the trajectory prediction model, we employ Dif-\nfuser [11]. The trajectory TEE is represented as a sequence of\nH = 48 poses. For each pose, we use a 10-dimensional vector\nthat includes a three-dimensional location, 6-dimensional\nrotation vector [16], and one dimension for the gripper. The\ninput keypoint feature to the Diffuser is DINO and FPFH as\nin the keypoint detection function. We optimize the model\nusing standard diffusion loss functions. The diffusion model\nestimates the conditional distribution P(TEE | C), where\nC = {\u27e8pk, FDINO, FFPFH\u27e9} represents the set of matched\nkeypoints in Di, TEE is the end-effector trajectory, which is\nused for inference time denoising."}, {"title": "F. Inference-time Pipeline", "content": "At inference time, given a new scene image I, we begin\nby running the keypoint detector \u03c6(k, I) for all keypoints\nk \u2208 K, extracting their corresponding position and feature\nvectors. We then use the learned Diffuser to generate a set of\nend-effector trajectories. Starting from randomly initialized\ntrajectories sampled from Gaussian noise, the model employs\nthe backward process which iteratively denoises the noisy\ntrajectories through gradient steps guided by the score\nfunction \u03f5\u03b8 under given conditions.\nNote that the learned trajectory starts relatively close to the\ntarget object. We need to ensure reachability and collision-free\nmotion in the environment. Similar to previous work that uses\ndiffusion models as trajectory samplers [17], we use a motion\nplanner (bi-directional RRT [18]) to check whether there is a\nfeasible path to the initial pose of the task trajectory TEE. If\nthe motion planner returns no valid path, we will test the next\npredicted trajectory, until all generated trajectories in the set\nare exhausted, when the algorithm returns failure. Otherwise,\nwe move the end-effector to TEE0 using the approaching path\nreturned by the motion planner, and start TEE execution."}, {"title": "III. EXPERIMENT", "content": "In this section, we want to study the following questions:"}, {"title": "A. Simulation Experiments in Meta-World", "content": "In this section, we compare our method with other baselines\nwith different input spaces and network architectures using\nthe Meta-World [19] simulator, focusing on the efficacy of\ndifferent representations in terms of their data efficiency.\nSetup. We evaluate on 5 tasks: DRAWEROPEN, DRAWER-\nCLOSE, BUTTONSIDE, BUTTONTOP, and LEVERPULL, as\nshown in Figure 3. For each task, we provide an oracle set of\nkeypoints by manually labeling the XML files and computing\ntheir 2D or 3D locations in the scene using known object and\ncamera poses. We train and test on scenes with varying camera\nand object poses. The object poses are uniformly sampled\non the table with 2D translations and rotations. The camera\nviewpoint is sampled around the object with randomized angle\nand distance with elevation in [0, 1], azimuth in [-\u03c0/4, \u03c0/4], and\ndistance in [1.5, 2]. We exclude random camera angles where\nthe arm obstructs the object in the initial scene.\nBaselines. We compare our method against 4 baselines.\n\u2022\n\u2022\n\u2022\n\u2022\nDiffuser [11] (RGB) generates a 6 DoF end-effector\ntrajectory based on an initial RGB observation of the\nscene and the camera extrinsic, using a 1D convolutional\nnetwork. We use DINO [13] visual encoder and finetune\nit at training time to handle the discrepancy between the\nsimulator and real-world images on which DINO is trained.\n3D Diffuser Actor [5] (RGBD) builds a 3D representation\nfrom pre-trained visual features (CLIP [20]) and the\npoint cloud. It generates actions with a diffusion model\nconditioned on the 3D representation.\nDiffuser with keypoints: We provide Diffuser with the\n2D position of the keypoints as an additional input.\n3D Diffuser Actor with keypoints: Similarly, we provide\nthe 3D position of the keypoints to the 3D Diffuser Actor."}, {"title": "B. Real-World Experiment on Franka Arm", "content": "In this section, we investigate whether our proposed itera-\ntive procedure generates better keypoints which eventually\nlead to a higher task success rate for a range of different\ntasks in the real world. We also explore the generalization\ncapability along different dimensions endowed by using\nkeypoint abstraction such as object poses and object instances.\nSetup. We carry out real-world experiments on three tasks:\n1) Lifting the handle of a coffee machine, 2) Opening the top\ndrawer, and 3) Pouring something into a bowl, as illustrated\nin Figure 5. We use a Franka Research 3 robot arm with\na RealSense D435i RGBD camera mounted on the gripper.\nFor each task, we collect 10 demonstrations and capture the\ninitial image using the gripper-mounted camera.\nTo evaluate the generalization along different dimensions,\nwe vary the environment in terms of camera and object poses\n(View), and object instances (Cross obj.). Note that the objects"}, {"title": "IV. RELATED WORK", "content": "Abstractions for action representations Abstractions over\nraw sensorimotor interfaces have been shown to enhance\ndata-efficient learning and facilitate generalization. Various\nforms of abstractions have been studied, including contact\npoints [21], [22], [23], [24], objects [25], [26], [27], [28], [29],\n[30], [1], object parts [31], [32], [33], [34], keypoints [35],\n[3], [36], [37], and other shape representations [38], [39],\n[40]. These spatial abstractions can serve as direct inputs\nto data-driven models [28], [3], [1], or be used to create\ncoordinate frames [41], [34]. Typically, these models repre-\nsent short-horizon robot behaviors and can be sequentially\ncomposed [36], [42] in a fixed order, or integrated into\nhigh-level planning algorithms [43]. Our work leverages\nkeypoint-based representations for few-shot imitation learning,\nand focuses on acquiring such representations automatically\nwithout additional data and labels.\nFinding keypoint correspondences. Finding functional\ncorrespondences between objects [44] has been extensively\nexplored in robotic manipulation, using both analytical\nmethods [45], [46], [47], [48] and data-driven methods [39],\n[49], [35], [3], [37], [38]. However, the keypoints derived\nfrom these approaches are typically tailored to specific tasks,\nrequiring human annotations of task-relevant keypoints at\ntraining time. Furthermore, although data-driven methods\noffer better generalization to novel objects, they require\nadditional data for training, such as labeled keypoints or\nadditional object datasets. In contrast, our method eliminates\nthe need for human-labeled keypoints by leveraging off-the\nshelf large pre-trained models for vision-language grounding"}, {"title": "V. CONCLUSION", "content": "We propose KALM, a framework that distills task-relevant\nkeypoints by iteratively prompting LMs and verifying consis-\ntency using a small amount of data. We use these keypoints\nas an abstraction to learn a keypoint-conditioned policy\nmodel that predicts object-relative robot actions, leveraging\nthe keypoints as observational abstractions and local action\nframes. Our simulated and real-world experiment shows that\nour keypoint representation enables data-efficient learning\nand facilitates generalization to changes in the scene."}]}