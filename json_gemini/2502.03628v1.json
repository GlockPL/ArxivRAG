{"title": "The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering", "authors": ["Zhuowei Li", "Haizhou Shi", "Yunhe Gao", "Di Liu", "Zhenting Wang", "Yuxiao Chen", "Ting Liu", "Long Zhao", "Hao Wang", "Dimitris N. Metaxas"], "abstract": "Large Vision-Language Models (LVLMs) can reason effectively over both textual and visual inputs, but they tend to hallucinate syntactically coherent yet visually ungrounded contents. In this paper, we investigate the internal dynamics of hallucination by examining the tokens logits ranking throughout the generation process, revealing three key patterns in how LVLMs process information: (1) gradual visual information loss - visually grounded tokens gradually become less favored throughout generation, and (2) early excitation \u2013 semantically meaningful tokens achieve peak activation in the layers earlier than the final layer. (3) hidden genuine information - visually grounded tokens though not being eventually decoded still retain relatively high rankings at inference. Based on these insights, we propose VISTA (Visual Information Steering with Token-logit Augmentation), a training-free inference-time intervention framework that reduces hallucination while promoting genuine information. VISTA works by combining two complementary approaches: reinforcing visual information in activation space and leveraging early layer activations to promote semantically meaningful decoding. Compared to existing methods, VISTA requires no external supervision and is applicable to various decoding strategies. Extensive experiments show that VISTA on average reduces hallucination by about 40% on evaluated open-ended generation task, and it consistently outperforms existing methods on four benchmarks across four architectures under three decoding strategies.", "sections": [{"title": "1. Introduction", "content": "Large Vision-Language Models (LVLMs) (Dai et al., 2023; Bai et al., 2023; Chen et al., 2023; Zhu et al., 2023; Liu et al., 2024c) have revolutionized multimodal AI by enabling seamless integration of visual and textual information, powering applications from interactive assistance to autonomous systems (Lin et al., 2023; Yang et al., 2024; Lai et al., 2024). However, LVLMs frequently hallucinate semantically coherent yet visually ungrounded contents, hindering their reliability in real-world applications.\nThough LVLM hallucination is considered multifaceted (Liu et al., 2024d), a critical cause stems from the overwhelming influence of language priors over visual contexts, and has been studied from the perspective of attention patterns (Huang et al., 2024; Liu et al., 2024f) and distribution divergence within logits space (Leng et al., 2024; Favero et al., 2024). Despite these insights, it remains unclear how hallucination emerges and propagates during the generation process.\nInspecting Token Dynamics in LVLMs. In this work, we take a novel perspective by examining LVLM's generation dynamics through the lens of token logits ranking. Given an image its corresponding description (produced by an LVLM), we identify three categories of tokens (elaborated in Sec. 3.4):\n\u2022 Hidden Genuine Tokens \u2013 tokens that are missing in generated contents yet clearly visible from visual input;\n\u2022 Decoded Genuine Tokens \u2013 tokens that appear in continuation with visual groundings;\n\u2022 Hallucinated Tokens - tokens extracted from the hallucinated contents within generation.\nWe then track each token type's corresponding logits rankings throughout the generation across temporal (Fig. 1 left) and layer sequences (Fig. 1 right). Our analysis makes three prominent observations:\n\u2022 (OBS-1) Gradual Visual Information Loss. As generation progresses, genuine token rankings gradually decline while hallucinated tokens are surfaced (see Fig. 1 left and Fig. 2). This aligns with recent findings (Yue"}, {"title": "2. Methodology", "content": "In this section, we first establish the notions and knowledge foundation in Sec. 2.1, followed by the elaboration of token ranking analysis in Sec. 2.2. We then present the proposed VSV and SLA methods in Sec. 2.3 and 2.4, respectively."}, {"title": "2.1. Preliminaries", "content": "Conditional Generation of LVLMs. Suppose that an LVLM consists of a vision encoder and a cross-modal interface that projects visual inputs into a sequence of visual tokens $\\mathbb{X}_v$. Given an input image, the complete prompt tokens $\\mathbb{X}_c$ are constructed by concatenating system message tokens $\\mathbb{X}_s$ (can be empty), visual tokens $\\mathbb{X}_v$, and query tokens $\\mathbb{X}_q$: $\\mathbb{X}_c = concat(\\mathbb{X}_s, \\mathbb{X}_v, \\mathbb{X}_q)$. At each time step $t$, the model samples a new token $x_t$ according to the probability distribution conditioned on both the input context $\\mathbb{X}_c$ and previously generated tokens $\\mathbb{X}_{<t} = \\{x_i\\}_{i=1}^{t-1}$:\n$x_t \\sim p(x_t | \\mathbb{X}_c, \\mathbb{X}_{<t}) = softmax(H(h_{t-1}^L))$,\nwhere $H$ denotes the model's head layer and $h_{t-1}$ indicates the hidden state from the last layer $L$ at time step $t-1$. Above formulation suggests that the dilution or insufficiency of visual information in $h_{t-1}$ can bias the generation towards hallucination."}, {"title": "Residual Stream.", "content": "Taking the mathematical interpretation from Elhage et al. (2021), we view layer-wise hidden states as residual streams that evolve recursively:\n$h_l = h_{l-1} + a_l + m_{l-1}$,\nwhere l is the layer index, $a_l$ and $m_l$ represent the output activation of integrated multi-head attention (MHA) layer and feed-forward network (FFN), respectively. Within this framework, MHAs facilitate information fusion across different residual streams, while FFNs access and integrate learned parametric knowledge (Geva et al., 2021; Dai et al., 2022). Residual stream provides a natural interface for monitoring and controlling information flow, making it particularly suitable for hallucination analysis and mitigation."}, {"title": "Logit Lens.", "content": "The head layer $H$ is by default applied on top of last layer hidden states $h_L$. However, thanks to the gradual evolvement of hidden states within residual streams (Chuang et al., 2024), applying $H$ to hidden states of earlier layers $l < L$ remains effective, even without additional training (Gurney, 2023). This practice is commonly referred to as \"logit lens\" and can be used to decipher intermediate states."}, {"title": "2.2. Token Ranking Analysis", "content": "To systematically investigate how visual information is processed during generation, we propose a token ranking analysis framework that tracks the relative importance of different tokens throughout the generation process.\nIdentification of Target Tokens. For each given image-description pair where the text description is generated by an LVLM (e.g., LLAVA-1.5 (Liu et al., 2024a)), we utilize gpt-4o (Hurst et al., 2024) as the oracle model to identify three categories of words referencing both visual and textual contents. A word is a\n\u2022 decoded genuine word if it appears in the continuation and align with visual evidence;\n\u2022 hidden genuine word if it is visually evident but not included in the continuation;\n\u2022 hallucinated word if it appears in continuation but lacks visual grounding.\nCollected words are then tokenized to form our analysis sets.\nToken Ranking via Logit Lens. To analyze token dynamics during generation, we apply the logit lens $H(h_l)$ to each layer l and time step t. Given token x, we calculate its ranking position among all possible tokens according to:\n$R_t^l(x) = rank(H(h_t^l), x)$,\nwhere $R_t^l(x)$ represents the position of token x in the probability-ordered sequence of all tokens at time step t"}, {"title": "2.3. Visual Steering Vector (VSV)", "content": "Being aware of the challenge from gradual visual information loss, it is of critical importance to retain visual cues throughout the generation. A promising method involves increasing attention weights distributed on visual tokens (Liu et al., 2024f). Nevertheless, this operation simultaneously introduces undesired parametric priors cumulated in residual streams of visual tokens. Drawing inspiration from steering vectors in LLMs (Turner et al., 2023; Zou et al., 2023; Liu et al., 2024e; Li et al., 2024), we propose Visual Steering Vector (VSV) to steer the generation of LVLM towards the direction with visual groundings without amplifying inherent language biases.\nVSV Construction. The core logic behind VSV is to extract a directional vector within activation space without introducing disturbing language priors. To this end, we construct VSV via a contrastive process using paired context sequences: a \u201cpositive\u201d context $X_p = concat(X_s, X_v, X_q)$ containing visual tokens $X_v$, and a \"negative\" counterpart $X_n = concat(X_s, X_q)$ that discards visual tokens while preserving other elements. Both sequences are processed by a vectorization function $F$, which forwards the given token sequence through the LVLM and takes the residual stream from the last token. Visual steering vector (VSV) can be computed as:\n$V_{steer} = V_p - V_n = \\{v_{steer}^l\\}_{l=1}^L$,\nwhere $V_p = F(X_p)$ and $V_n = F(X_n)$. Here, $v_{steer}^l$ refers to the steering vector for layer l.\nInference-time Intervention. During inference, we inject the visual steering vector into the residual stream at each"}, {"title": "2.4. Self-Logits Augmentation (SLA)", "content": "Motivated by early excitation phenomenon (Fig. 1 right), where semantically meaningful tokens show stronger activation in penultimate layer, we propose Self-Logits Augmentation (SLA) to promote the decoding of such tokens."}, {"title": "3. Experiments", "content": "In this section, we empirically validate VISTA across four architectures, three decoding strategies, and four benchmarks. We first present the experimental configuration (Sec. 3.1), followed by an extensive evaluation on hallucination-specific and general-purpose benchmarks (Sec. 3.2 and 3.3). We then analyze VISTA's effectiveness in addressing the observed phenomena (Sec. 3.4) and conclude with comprehensive ablation studies (Sec. 3.5)."}, {"title": "3.1. Experimental Setup", "content": "Model Architectures. We evaluate VISTA on four representative LVLMs with distinct architectural designs: LLAVA-1.5 (Liu et al., 2024a) and Shikra (Chen et al., 2023), which employ linear projections for visual-textual alignment, and MiniGPT-4 (Zhu et al., 2023) and InstructBLIP (Dai et al., 2023), which utilize Q-former (Li et al., 2023a) for cross-modal interaction.\nDecoding Strategies. To demonstrate VISTA's versatility as an inference-time intervention method, we verify it across three widely used decoding protocols: (1) greedy decoding,"}, {"title": "3.3. Results on Comprehensive Benchmarks", "content": "We further validate VISTA on MMHal-Bench (Sun et al., 2023) and MME (Fu et al., 2023), two challenging benchmarks that examine diverse aspects of model behavior.\nMMHal-Bench Evaluation. MMHal-Bench (Sun et al., 2023) provides a specialized framework for assessing hallucination in LVLMs through 96 carefully designed image-question pairs. The benchmark spans eight distinct categories: object attributes (ATTR), adversarial objects (ADV), comparisons (COMP), counting (COUNT), spatial relations (SPAT), environmental inferences (ENV), holistic descriptions (HOL), and others (OTHER). Unlike conventional VQA evaluations, MMHal-Bench emphasizes logical reasoning and complex visual understanding, providing a rigorous test of hallucination mitigation in challenging scenarios. Model responses are evaluated using GPT-4 for alignment with ground-truth answers. We compare VISTA with PAI and vanilla decoding methods for this evaluation."}, {"title": "3.4. On Solving Gradual Visual Information Loss", "content": "Our analysis in Sec. 2.2 identifies gradual visual information loss as a substantial challenge for long sequence generation. To validate VISTA's effectiveness in addressing this issue, we compare token logits rankings w/ and w/o VISTA throughout generation. According to Fig. 5, VISTA not only improves the average ranking of hidden genuine tokens throughout generation but also reverses the concerning trend of hallucinated tokens, reducing their prominence in mid and late stages where hallucination typically occurs. Not surprisingly, VISTA also maintains a high ranking for decoded genuine tokens since VSV captures all visual clues of an image and reinforces them at all time steps. This quantitative evidence directly demonstrates VISTA's capability in maintaining visual grounding throughout the generation process."}, {"title": "3.5. Ablation Study", "content": "To thoroughly investigate the effectiveness of VISTA, we gauge the practical latency of VISTA, and analyze how different VSV strength (\u03bb) and SLA mixing ratio (\u03b3) affect the model's performance in terms of hallucination reduction (CHAIR-S and CHAIR-I metrics) and overall quality (F1 score)."}, {"title": "4. Related Work", "content": "Hallucination Mitigation in LVLMs. Hallucination \u2013 the generation of content that is irrelevant, factually incorrect, or inconsistent with visual inputs (Bai et al., 2024) \u2013 represents a fundamental challenge in LVLM development. Research has identified three primary sources: limitations in visual encoder capabilities (Tong et al., 2024; Liu et al., 2024b; Shi et al., 2024), excessive reliance on learned parametric knowledge (Li et al., 2023b; Zhou et al., 2023; Leng et al., 2024; Huang et al., 2024), and noisy training data (Liu et al., 2023; Yu et al., 2024). Mitigation approaches span training-based solutions with refined datasets (Yue et al., 2024; Jiang et al., 2024), post-processing techniques including revision (Yin et al., 2023; Zhou et al., 2023) and verification (Chen et al., 2024; Sun et al., 2023), and inference-time interventions like Visual Contrastive Decoding (Leng et al., 2024) and enhanced attention methods (Liu et al., 2024f). Recent studies revealing \"text inertia\" (Liu et al., 2024f), where models generate similar hallucinations without visual input, highlight concerning reliance on learned text patterns. While these findings advance our understanding, how hallucination propagates through model architectures remains elusive, and existing solutions often require external supervision and are hinged with specific decoding strategies.\nContrastive Decoding in LVLMs. Contrastive decoding, originally introduced in NLP (Li et al., 2022; Shi et al., 2023), has emerged as a promising approach for reducing hallucination in LVLMs. Recent adaptations of"}, {"title": "5. Conclusion and Limitations", "content": "This study investigates the hidden life of tokens in Large Vision Language Models (LVLMs) and introduces VISTA (Visual Information Steering with Token-logit Augmentation), a lightweight approach to mitigate hallucination. Through systematic analysis, we reveal that visual information gradually attenuates during text generation, but can be effectively restored through our framework's visual information steering and strategic use of early-layer logits. Extensive experimentation across diverse architectures and decoding strategies demonstrates that our framework significantly reduces hallucination while preserving generation quality. These findings not only illuminate the hidden dynamics of LVLM behavior but also establish visual information steering as a promising direction for enhancing the reliability of multimodal AI systems.\nLimitations. VISTA is subject to several limitations. First, while VISTA demonstrates robustness across a range of hyperparameter values, optimal settings may vary across different architectures. Second, the effectiveness of VSV relies on the quality of visual cues extracted by the LVLM's vision encoder - models with weak visual encoding capabilities may see reduced benefits. Third, the current implementation focuses on addressing hallucination in single-round tasks; adaptation to interactive scenarios like visual dialogue may require additional considerations."}, {"title": "Impact Statement", "content": "This research advances methods for making large vision-language models more trustworthy and reliable through mitigating hallucination. While the proposed method demonstrates promising results, its effectiveness is subject to the inherent capability of large vision-language model, and improper usage may adversely affect model's performance. To the best of our knowledge, there are no ethical or other concerns that need to be addressed."}]}