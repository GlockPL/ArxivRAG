{"title": "MathBridge: A Large-Scale Dataset for Translating Mathematical Expressions into Formula Images", "authors": ["Kyudan Jung", "Sieun Hyeon", "Kwon Jeong Youn", "Nam-Joon Kim", "Hyun Gon Ryu", "Hyuk-Jae Lee", "Jaeyoung Do"], "abstract": "Understanding sentences that contain mathematical expressions in text form poses significant challenges. To address this, the importance of converting these expressions into formula images has been highlighted. For instance, the expression \"x equals minus b plus or minus the square root of b squared minus four a c, all over two a\" is more readily comprehensible when displayed as an image $x = \\frac{-b \\pm \\sqrt {b^2 - 4ac}}{2a}$. To develop a text-to-image conversion system, we can break down the process into text-to-LaTeX and LaTeX-to-image conversions, with the latter being managed with by existing various LaTeX engines. However, the former approach has been notably hindered by the severe scarcity of text-to-LaTeX paired data, presenting a significant challenge in the field.In this context, we introduce MathBridge, the first extensive dataset for translating mathematical spoken English into LaTeX, which aims to establish a robust baseline for future research in text-to-LaTeX translation. MathBridge comprises approximately 23 million LaTeX formulas paired with corresponding spoken English expressions. Through comprehensive evaluations, including fine-tuning and testing with data, we discovered that MathBridge significantly enhances pre-trained language models' capabilities for text-to-LaTeX translation. Specifically, for the T5-large model, the sacre-BLEU score increased from 4.77 to 46.8, demonstrating substantial enhancement. Our findings indicate the necessity for a new metric specifically for text-to-LaTeX conversion evaluation.", "sections": [{"title": "Introduction", "content": "Many students access open source courses through various media. These courses often cover subjects such as natural sciences, engineering, artificial intelligence, and astronomy, where mathematical formulas are frequently discussed. Non-English-speaking students, and particularly those with hearing impairments, rely on subtitles to comprehend the lecturer's spoken content. When it comes to mathematical formulas, they must understand these formulas through the English text that vocalizes the notation displayed in the subtitles.\nUnlike general English, interpreting formulas through their vocalized English rather than as visual images can be cognitively challenging. This is especially true for complex formulas, where the difference becomes pronounced. As shown in Table 1, viewing the simple quadratic formula as an image is cognitively easier than interpreting it through its spoken English equivalent, affecting both the speed of comprehension and the perceived difficulty.\nTo address this issue, the text-to-image process can be broken down into two stages. The first stage involves converting the English text, which vocalizes the formula, into LaTeX syntax. The second stage requires compiling the LaTeX into an image, which is already possible with various compilers.\u00b9 However, the first stage poses a challenge algorithmically. The vocalized English text of the formula does not have a one-to-one correspondence with LaTeX syntax. Moreover, the extensive variety of TeX syntax makes it impractical to create a dictionary-style conversion for every possible grammar, as this approach would be too exhaustive.\nWe have devised a solution for the text-to-LaTeX task using a language model. This involves developing a language model that translates English text vocalizing a formula into LaTeX. The LaTeX output from this model can then be input into a LaTeX engine to generate formulas visually similar to those in Table 1. However, a significant challenge remains as there is currently no existing dataset of paired LaTeX formulas and their corresponding English vocalizations necessary for supervised learning. Although Quiniou has created a dataset containing mathematical vocalizations, it is limited to French, which restricts its applicability. (Quiniou et al. 2011)"}, {"title": "Related Work", "content": "Previous studies on multilingual translation have primarily focused on natural languages(Li et al. 2024; Chen et al. 2024a; Singh et al. 2024; Jia et al. 2022; Wang, Wu, and Pino 2020). These efforts aim to convert English into various national languages, rather than artificial languages. The advent of pretrained language models like BERT(Devlin et al. 2019) and GPT(Radford et al. 2019) has revolutionized the field of natural language processing, including translation. These models, pretrained on extensive corpus, can be fine-tuned for specific translation tasks, achieving state-of-the-art results.\nBroadly speaking, translating between SQL queries and natural language can also be regarded as a form of translation. SQL, like LaTeX, is a computer language governed by a set of defined rules, presenting similarities in structured linguistic conversion. Research in this area includes various studies on SQL-to-text and text-to-SQL translation(Zhong, Xiong, and Socher 2017; Xu, Liu, and Song 2017; Kumar et al. 2024; Saparina and Lapata 2024).\nResearch related to LaTeX includes recognizing mathematical formula images using OCR and converting them into LaTeX text(Dosovitskiy et al. 2021; Blecher 2024; Blecher et al. 2023). This process facilitates the transformation between formula images and LaTeX, but it does not address the conversion between LaTeX and spoken English.\nStudies exploring the relationship between spoken English and LaTeX are sparse. Notably, there is a significant gap in understanding how to effectively translate complex mathematical expressions vocalized in English into precise LaTeX syntax. This area presents a substantial challenge due to the intrinsic complexities of mathematical language and the specific precision required in LaTeX representation. Further research in this field could significantly improve educational technologies, especially for non-native English speakers and individuals with hearing impairments by enhancing their access to scientific and mathematical content."}, {"title": "Dataset Construction", "content": "To gather LaTeX formulas and their corresponding English vocalization texts, we initially focused on collecting LaTeX formulas because they are easier to get than spoken English text data. We extracted LaTeX formula data from papers uploaded to arXiv and through open source textbooks.\nOn the arXiv website, it is possible to view papers as HTML files through a browser without directly downloading LaTeX files. While it is feasible to extract LaTeX formulas using HTML tags that represent formulas, this method is not recommended as it can put undue stress on the arXiv servers. Instead, we followed the method proposed by Clement in his paper to bulk download arXiv paper files(Clement et al. 2019). This method utilizes a manifest file that lists the metadata of papers, allowing the download of PDF or source files via AWS servers. Since we do not require the PDF files or image files from the source files, we only downloaded the .tex extension files. Using the metadata information, we specifically downloaded files uploaded in 2023. This approach not only extracts mathematical formulas but also enables the extraction from all categories of papers provided by arXiv, thus the extracted formulas span a diverse range of disciplines. Monthly statistics of the papers are shown in Table 2, and the distribution of papers across different fields is illustrated in Fig. 2.\nWe determined that the formulas available on arXiv tended to be of a higher complexity, suitable mainly for advanced studies. Consequently, we decided to include formulas from elementary, middle, high school, and university mathematics textbooks that are available as open source.\u2074\u2075 We initially collected PDFs of open source textbooks. These textbooks range from Elementary Mathematics, which contains content suitable for younger students, to more advanced topics such as Analysis and Algebra. The collected data are in PDF format, which does not allow for direct extraction of formulas. Therefore, we used an optical character recognition (OCR) tool, nougat (Blecher et al. 2023), to process these PDF files. A significant advantage of using nougat is that it provides the OCR results in mmd files that include LaTeX. We compiled a collection of mathematics textbook data in mmd format. The distribution of textbook pages by subject area is shown in Table 3, where 'etc.' includes fields such as Data Science, Machine Learning, and Discrete Mathematics.\nIn this section, we discuss how we extracted LaTeX formulas and the surrounding English, which is called context-text from the source text dataset we gathered. The first algorithm applied during the extraction process was to replace custom commands in the source text dataset with standard LaTeX syntax. Authors of arXiv papers often define new custom commands at the beginning of their .tex files using commands such as \\def, \\newcommand, and \\renewcommand. These custom commands frequently do not conform to standard LaTeX syntax. Without addressing this, extracting LaTeX would also extract these custom commands, leading to errors during the compilation and the incorrect rendering of images. To resolve this issue, we replaced all custom commands with their original LaTeX syntax equivalents.\nNext, we defined a parser capable of extracting formulas. The parsing conditions were specified as follows:\n\u2022 Between $ and $.\n\u2022 Between $$ and $$.\n\u2022 Between \\(and \\).\n\u2022 Between \\[ and \\].\n\u2022 Between \\begin{equation} and \\end{equation}.\nDuring the extraction process, we chose not to use regular expressions method. This decision was made because the symbols $ and $$ are used both to start and end a formula. Regular expressions would mistakenly recognize the closing symbol as an opening one due to their identical nature. To overcome this issue, our code was structured to sequentially search through each .tex file from the beginning, storing formulas between each pair of $ or $$ symbols encountered. This method proved to be more accurate in extracting formulas than using regular expressions. Using this parser, we extracted approximately 48 million formulas from arXiv papers and 1 million formulas from textbooks."}, {"title": "Context-Text of IATEX Formula", "content": "We also extracted the English sentences that surround the formulas. This decision considers that in regular discourse, formulas are often discussed with additional English context before and after, rather than in isolation. This context-text usually serves to explain or emphasize the formulas. For instance, consider a mathematical lecture on YouTube, where viewers see mathematical expressions through real-time subtitles. To effectively present these expressions in LaTeX, it is crucial to quickly identify from the preceding English text that a formula follows. Similarly, the English text following a formula indicates the end of the formula. Therefore, context-text is essential for distinguishing between mathematical and non-mathematical sections of the text.\nThe data structure is as follows. Consider the sentence, \"Here is another equation, $$ a^2 + b^2 = c^2 $$, which is known as the Pythagorean equation and is very important.\". In the MathBridge dataset, this sentence would be split into 'context_before' column containing \"Here is another equation,\", the 'equation' column containing \"$$a^2 + b^2 =c^2$$\", and 'context_after' column containing \", which is known as the Pythagorean equation and is very important.\".\nExtraction was performed simultaneously with LaTeX formula extraction. When extracting formulas using a parser configured for mathematical conditions, the English text immediately preceding or following the formula was extracted up until another formula appeared or a line break occurred. It is worth noting that some data points in MathBridge do not have context-text before and/or after the formula."}, {"title": "Spoken English Text", "content": "We have collected formulas and are now proceeding to obtain corresponding spoken English text pairs for each formula using a large language model. Specifically, we employed models such as GPT-3.5, which has 175 billion parameters. By inputting formulas in LaTeX format into the model and requesting their English vocalization, we verified that these large language models produce superior outputs. This capability stems from their extensive pretraining on vast corpora.\nWe extracted spoken English text using the GPT-3.5 API for the equations we had gathered. During this process, we only input the formulas, excluding any context-text. And considering the presence of duplicate formulas, we temporarily created a dataset of unique formulas. This dataset was then used to apply the API, generating spoken English text for approximately 13 million unique formulas out of the initial 49 million (48 million from arXiv, 1 million from textbooks.) collected. These results were later aligned with the original data.\nFor the system role prompt in the GPT API, we entered the following text to ensure performance comparable to web-base ChatGPT: You are a ChatGPT, a large language model trained by OpenAI, based on the GPT-3.5 architecture; Knowledge cutoff: 2022-01; Current date: 2024-05-08; Personality: v2.\" For the user role prompt, we provided Translate the following LaTeX equation into spoken"}, {"title": "Post-Processing", "content": "For a model to perform well in supervised learning, preparing high-quality data is crucial. We rigorously filtered out grammatically incorrect LaTeX data by excluding data points where the GPT API generated spoken English text containing terms like 'None', 'Sorry', 'Apologize', or 'cannot', indicating an inability to vocalize the formula. Additionally, any context English containing special characters or TeX commands was completely removed. Upon manually reviewing the dataset, we discovered that data points with excessively long lengths are highly likely to be incorrect. Consequently, we only included data in MathBridge where 'context_before' has no more than 200 characters, \u2018equation' has no more than 80 characters, 'context_after' has no more than 250 characters, and 'spoken_English' has no more than 120 characters. The reason for setting the length of 'context_before' shorter than 'context_after' was based on the length distribution of the data before post-processing, which showed that the sentences preceding the formulas were typically shorter. This reflects the common practice of mentioning a formula first, followed by a more detailed explanation.\nThis process led to the removal of fewer than 500K data points, maintaining a total of approximately 23 million data points in MathBridge."}, {"title": "Dataset Statistics", "content": "In this section, we discuss the statistics of the MathBridge dataset after completing the post-processing. For the formula data, the minimum character length was 5, the maximum was 80, and the average was 13. Note that there are about 7 million data points with the minimum character"}, {"title": "Experiment", "content": "We conducted experiments to translate spoken English of mathematical formulas into LaTeX. For this purpose, we utilized the MathBridge dataset to fine-tune various pretrained language models."}, {"title": "Dataset", "content": "To enable effective training for translating from English to LaTeX, we excluded approximately 7 million data points, each consisting of only one character, from both the training and testing datasets within the MathBridge collection. From the remaining dataset of approximately 16 million entries, we initially selected 1000 pairs of LaTeX formulas and their corresponding spoken English expressions. For the testing dataset, denoted as $D_{test} = \\{(x_i, Y_i)\\}_{n=1}^N$, each input, $x_i$, was constructed by concatenating the text before the formula (context_before), the spoken English version of the formula, and the text following the formula (context_after). Similarly, each target output, $y_i$, was formed by concatenating context_before, the actual LaTeX formula (equation), and context_after. The remaining data were employed as the training set. Following the same structure for the training dataset, denoted as $D_{train} = \\{(X_i, Y_i)\\}_{n=1}^N$, each input, $X_i$, and output, $Y_i$, were assembled in the same way- by concatenating the respective texts before and after the formula, with either the spoken English or the LaTeX formula in between. This approach ensures a structured and consistent framework for training our models to understand and translate the contextual relationships between spoken English and LaTeX formulas effectively."}, {"title": "Setup", "content": "We selected several state-of-the-art (SOTA) PLMs, such as T5 (Raffel et al. 2020), BART (Lewis et al. 2019), and mBART (Liu et al. 2020), which have demonstrated satisfactory performance on other downstream tasks, as baselines for translating English to LaTeX. Fine-tuning was conducted using methods standard in NLP. The maximum number of training epochs was set to 3, and the model achieving the lowest validation loss on the development set was selected as the best model for predicting on the test set. The learning rate was adjusted using a cosine learning rate scheduler, with the maximum learning rate set at 1e-4 and the minimum at 1e-6. The maximum input sequence length was set to 512, and the batch size was 12.\nFor some current powerful large language models, such as GPT-3.5, which incur significant computational costs, we evaluated their translation capabilities solely through inference. We also compared the performance differences when prompts were provided and when they were not. (Chen et al. 2024b) The prompt instruction used was, \"The following sentence mixes spoken parts of formulas with standard English. Translate the part of the sentence that represents a formula into LaTeX.\". This prompt demonstrated the best performance in preliminary experiments."}, {"title": "Metrics", "content": "To evaluate the translated LaTeX text, we employed traditional metrics commonly used in translation tasks. These metrics include sacreBLEU(Post 2018; Papineni et al. 2002), ROUGE (Lin 2004), CER, and WER. SacreBLEU, a variation of BLEU, offers high reliability as it produces consistent values regardless of the tokenizer used. It is also applicable not only to English but to other languages as well. Given that LaTeX often contains special characters, we included sacreBLEU in our metrics suite. Additionally, we utilized ROUGE, a metric typically employed in summarization tasks, which are a subset of translation tasks. ROUGE has been experimentally used to gauge the quality of text summaries. In Table 5, R1 represents the precision of"}, {"title": "Results", "content": "The comprehensive results are presented in Table 5. The initial performance of PLMs on the test dataset was found to be unsatisfactory, indicating a limited capability of these models to convert spoken mathematical English into LaTeX without specific adaptations. Notable performance enhancements were observed after fine-tuning the PLMs with MathBridge, particularly with the T5-large model. These improvements indicate MathBridge's efficacy and its suitability for translating English into LaTeX.\nMoreover, the effectiveness of advanced PLMs like GPT-3.5 without prompt instructions was inadequate for this conversion task. However, significant improvements were achieved on test dataset after integrating prompt instructions with GPT-3.5, highlighting the critical role of prompt engineering in enhancing the performance of PLMs for this specific application."}, {"title": "Case Study and Error Analysis", "content": "In this section, we present several good examples generated by our fine-tuned PLMs. And also analyze some problematic cases. Example 1 in Figure 6 illustrates that the PLM, fine-tuned with MathBridge, robustly predicts LaTeX even for relatively complex equations.\nMoreover, in Example 4 of the same figure, the input spoken English was, \"If we think about x plus 5y plus 10z equals zero then we can say that.\" The fine-tuned PLMs with our dataset effectively distinguished the mathematical parts, predicting the LaTeX equations accurately. These equations can be subsequently transformed into images via a LaTeX compiler, commonly rendering all the formulas as seen in the compiled outputs.\nHowever, we identified challenges related to the interpretation of English descriptions of formulas. A single spoken"}, {"title": "Discussion", "content": "LaTeX is a language comprised of a set of commands that include special characters, more similar to computer languages like C and SQL than to general natural languages, just like English. The commonly used BLEU metric (Papineni et al. 2002) in translation tasks, which is based on tokenizer. This tokenizer segments sentences based on general natural language. However, it is unclear whether traditional tokenizer can encapsulate information in LaTeX expressions, making BLEU an imperfect metric for evaluating LaTeX. Although sacreBLEU offers a more robust evaluation than BLEU, it still relies on the congruence of n-grams, failing to recognize semantic equivalence for instance. Also, the '\\frac' command and a simple '/' both denote fractions, yet this semantic identity is not reflected in its assessments. Character Error Rate (CER) shares similar issues with sacreBLEU. The Word Error Rate (WER), which separates words by spaces and calculates errors on a word basis, also does not serve as an ideal metric for LaTeX, where spaces do not always indicate syntactic separation. Thus, an ideal metric for assessing LaTeX should meet the following criteria:\n\u2022 LaTeX expressions that compile into the same mathematical image should be evaluated as identical.\n\u2022 The metric should quantify LaTeX error rates without being affected by spacing.\n\u2022 LaTeX expressions, such as equation 1 and equation 2, which might look similar but carry completely different meanings, should be distinctly assessed.\nWe hope to continuously expand our applications based on our dataset in the future."}, {"title": "Conclusion and Future Work", "content": "Translating between LaTeX formula and their spoken English is a challenging task in NLP. In this paper, we introduce MathBridge, the first large-scale text dataset designed for translating English into LaTeX. Utilizing this dataset, we have extensively evaluated PLMs' translation abilities and have enhanced their performance in converting spoken English into LaTeX. Additionally, we analyzed the performance differences of large language models with and without prompts using our test dataset. Experimental results demonstrate that our methods can effectively assist PLMs in generating LaTeX. In the future, we aim to develop bidirectional conversion between English and LaTeX with the integration of automatic speech recognition (ASR) and text-to-speech (TTS) systems. We also plan to develop a TeX tokenizer and a more general and robust metric."}]}