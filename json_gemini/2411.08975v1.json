{"title": "Fluoroformer: Scaling multiple instance learning to multiplexed images via attention-based channel fusion", "authors": ["Marc Harary", "Eliezer M. Van Allen", "William Lotter"], "abstract": "Though multiple instance learning (MIL) has been a foundational strategy in computational pathology for processing whole slide images (WSIs), current approaches are designed for traditional hematoxylin and eosin (H&E) slides rather than emerging multiplexed technologies. Here, we present an MIL strategy, the Fluoroformer module, that is specifically tailored to multiplexed WSIs by leveraging scaled dot-product attention (SDPA) to interpretably fuse information across disparate channels. On a cohort of 434 non-small cell lung cancer (NSCLC) samples, we show that the Fluoroformer both obtains strong prognostic performance and recapitulates immuno-oncological hallmarks of NSCLC. Our technique thereby provides a path for adapting state-of-the-art AI techniques to emerging spatial biology assays.", "sections": [{"title": "1. Introduction", "content": "Multiple instance learning (MIL) has emerged as the de facto standard approach in computational pathology for generating predictions from whole slide im-ages (WSIs) (Ilse et al., 2018; Maron and Lozano-P\u00e9rez, 1997; Carbonneau et al., 2018; Lu et al., 2021). The typical MIL pipeline consists of 1) dividing the WSI into smaller image patches, 2) extracting lower dimensional embeddings for each patch from a pre-trained neural network, 3) pooling embeddings across patches to create a slide-level summary vector, and 4) generating slide-level predictions for the particular task at hand. Compared to traditional strategies such as training patch-level predictors that rely exclusively on clinician-annotated regions of interest (ROIs), MIL enables weakly-supervised training on entire WSIs, thereby offering enhanced scalability, reduced sampling bias, and potentially superior performance (Zhou, 2018).\nThus far in computational pathology, MIL pipelines have largely been confined to traditional hematoxylin & eosin (H&E)-stained WSIS (Wilson et al., 2021; Ghahremani et al., 2022). While H&E staining can provide detailed morphological information, it fails to explicitly capture important proteins and other complex biomarkers that indicate cell phenotype and state (Lee et al., 2020; Peng et al., 2023; Mu\u00f1oz-Castro et al., 2022). In contrast, emergent techniques in spatial biology such as multiplex immunofluorescence (mIF) enable the imaging of many biomarkers simultaneously in tissue samples while preserving spatial context. These techniques result in rich, multi-channel (~5-50) images that have advanced our understanding of diseases ranging from neurodegenerative disorders (Mu\u00f1oz-Castro et al., 2022) to cancer (Lee et al., 2020; Peng et al., 2023). Conversely, mIF images are often analyzed using hand-engineered features, such as the counts of discrete biomarkers within clinician-defined ROIs (Wilson et al., 2021). More recent efforts have pointed to the potential of using deep learning to im-prove performance on downstream tasks, but these efforts have also focused on ROIs rather than expanding to WSIs (Hoebel et al., 2024; Sorin et al., 2023; Wu et al., 2022). There is therefore a pressing need to optimize MIL methods for mIF in order to yield the benefits of both weakly-supervised training and the rich information provided by spatial assays. Doing so, however, presents several challenges. The disparate channels must be somehow combined, and, moreover, ideally would be done so flexibly given that the number of channels can vary between mIF proto-cols.\nHere, we present the Fluoroformer, a Transformer-like neural network module designed to interpretably scale MIL to multiplex images. Leveraging scaled dot-product attention (SDPA) (Vaswani, 2017), it fuses the information from disparate multiplexed channels into a single summary vector for each patch, enabling the subsequent pooling of the patch embeddings via standard attention-based MIL (ABMIL) mechanisms. Importantly, the Fluoroformer produces attention matrices for each patch that may offer insights into cell-cell interactions and biological structures. Using a cohort of 434 non-small cell lung cancer (NSCLC) samples and their corresponding mIF WSIs, we find that the Fluoroformer demonstrates strong performance in predicting patient prognosis. Analysis of the channel-wise attention matrices offers insights into immune-tumor interactions that potentially associate with prognosis. Our approach therefore bridges spatial biology techniques with state-of-the-art artificial intelligence approaches to maximize the potential of this emerging field."}, {"title": "2. Related work", "content": null}, {"title": "2.1. Attention-based fusion strategies in histopathology", "content": "Attention-based pooling has emerged as among the most popular information aggregation strategies for H&E histopathology slides. This includes attention-based multiple instance learning (ABMIL), a benchmark MIL approach for H&E WSIs that consists of a traditional gated attention mechanism (Ilse et al., 2018) as further described below. Several works have also applied scaled dot-product attention (SDPA) and multihead attention (MHA) (Vaswani, 2017) to H&E WSIS. A prime example is TransMIL (Shao et al., 2021), which leverages MHA rather than gated attention to pool embeddings across the full slide. Beyond aggregating information across patches, attention mechanisms have also been used to fuse multiple modalities. Recently, MCAT (Chen et al., 2021) has leveraged SDPA to integrate genomic data and H&E embeddings."}, {"title": "2.2. Deep learning for multiplexed pathology images", "content": "Several recent studies have applied deep learning to mIF. For instance, Sorin et al. (2023) applied a ResNet-based (He et al., 2016) pipeline to the ROIS in NSCLC samples and found improved prognostic performance compared to models relying on traditional features. Wu et al. (2022) developed a graph neural network approach based on point patterns of cell phenotypes, also for mIF ROIs, and likewise observed higher prognostic performance relative to hand-engineered metrics. While these works demon-"}, {"title": "3. Methodology", "content": "Our Fluoroformer approach adapts MIL to multiplex WSIs via an attention-based channel fusion mecha-nism, inserted as an encapsulated module between the typical patch embedding and slide-level aggregation stages. We first provide a preliminary summary of MIL and ABMIL before describing our optimiza-tions for multiplexing."}, {"title": "3.1. Preliminaries: MIL and ABMIL", "content": "Multiple instance learning (MIL) (Ilse et al., 2018; Maron and Lozano-P\u00e9rez, 1997; Carbonneau et al., 2018) is typically formulated as follows. Each sample consists of a set-based data structure $X = \\{X_k\\}$ known as \u201cbag\u201d that contains, in a permutation-invariant fashion, $K$ separate \"instances\" $x_k$, where each instance $x_k \\in X$ and the bag $X \\in X^*$. The task is weakly supervised (Zhou, 2018), meaning that $X$ is associated with a global label $y \\in Y$.\nMIL models consist of a pooling operation $p$ that aggregates all instances, paired with a learnable clas-sifier $C$ (Ilse et al., 2018; Maron and Lozano-P\u00e9rez, 1997; Carbonneau et al., 2018). For biomedical im-age processing, in which images may often be excep-tionally large, instances consist of small patches of a larger image (Sudharshan et al., 2019; Javed et al., 2022; Chen et al., 2024; Xu et al., 2024). These are embedded via a \"featurizer\" $f : X \\rightarrow H$ into a latent space $H = \\mathbb{R}^{demb}$ to produce an embedded bag\n$\\mathbb{H} \\in \\mathcal{H}^K$:\n$H = \\{h_k = f (x_k) | X_k \\in X\\} $.\nPopular choices for $f$ are ResNet50 (He et al., 2016) trained on ImageNet or, more recently, specialized foundation models like UNI (Chen et al., 2024) and Gigapath (Xu et al., 2024) that have been trained in a self-supervised fashion on large domain-specific (H&E) datasets. Notably, $f$ is frozen prior to training of the pooling and classification operations in stan-dard approaches.\nIn ABMIL, the pooling operation consists of a learnable attention module $p_\\theta : \\mathcal{H}^K \\rightarrow \\mathcal{H}$ that com-putes a weighted sum of the embedded bag (Ilse et al., 2018). Most popularly, including in CLAM (Lu et al., 2021), a single- or double-gated attention mechanism computes a weighted sum of the patch vectors via an"}, {"title": "3.2. Fluoroformer: Leveraging MIL for multiplexing", "content": "Multiplexed imaging adds another dimension along which aggregation must be performed, namely the large number (~5-50) of disparate channels corre-sponding to separate biomarkers. Rather than try-ing to convert all of these channels into an RGB im-age or developing an embedder that can directly pro-cess many channels, we apply a pre-trained featur-izer to each channel separately. To do so, each chan-nel is first duplicated thrice along the RGB channel-dimension to produce a gray-scale image that can be processed by standard featurizers. Given $M$ channels, the global bag corresponding to a full sample now consists of\n$X = \\{x_{km}\\}$, where $x_{km} \\in X^M$, or, equivalently,\n$H = \\{h_{km}\\}$, where $h_{km} \\in H^m$.\nTo leverage the benefits of multiplexing in com-bining features across channels, we borrow insights from natural language processing (NLP). The seman-tic information contained in a sentence is not de-termined by each of its constituent tokens indepen-dently; rather, the tokens interact in pairwise depen-dencies to collectively produce the semantic signif-icance of the full sequence. Likewise, multiplexing is often employed to capture complex relationships"}, {"title": "3.2.1. MARKER ATTENTION", "content": "Mathematically, the SDPA operation in the Fluoro-former architecture thereby fills the role of a fourth, learnable operation $g_\\psi: \\mathcal{H}^m \\rightarrow \\mathcal{H}$ to preliminarily pool along the channel dimension:\n$\\hat{h}_k = g_\\psi (\\{h_{km}\\}) $.\nTreating each $k$th patch as a \"sentence\" consisting of $M$ tokens in an $demb$-dimensional latent space (Otter et al., 2020), we compute \"query,\u201d \u201ckey,\" and \"value\" embeddings $Q_k, K_k, V_k \\in \\mathbb{R}^{M \\times d_{hid}}$ via standard lin-ear layers (Vaswani, 2017), then employ the following formula:\n$\\alpha_{km} = \\text{softmax} \\Big(\\frac{Q_{km}K_{km}^T}{\\sqrt{d_{hid}}}\\Big) V_{km}$,\nwhere $d_{hid}$ is a hidden dimension and $A_{km}$ is the pairwise attention matrix.\nBecause we will almost always have $demb\\gg M$, we minimize computational overhead by adding a pre-liminary bottleneck that contracts the embedding di-mension $demb$ to a hidden dimension $d_{hid}$. We let $\\hat{h}_{km}$ denote the contracted embedding."}, {"title": "3.2.2. MARKER NORMALIZATION", "content": "In keeping with the standard Transformer architec-ture (Vaswani, 2017), we then employ two skip con-nections (He et al., 2016) each followed by \u201cmarker normalization\" layers. Analogous to layer normaliza-tion (Ba et al., 2016) in standard NLP models, these have the motivation of both stabilizing training and ensuring that no one channel dominates the patch when performing mean pooling. Specifically, for each $k$th patch and $m$th channel, the sum $a_{km}$ is updated with its residual\n$a_{km} \\leftarrow a_{km} + h_{km}$.\nThe resulting tensor is normalized by computing the statistics\n$\\mu_{km}^{(SDPA)} = \\frac{1}{d_{hid}} \\sum_{i=1}^{d_{hid}} \\alpha_{kmi}$,\n$\\sigma_{km}^{(SDPA)} = \\sqrt{\\frac{1}{d_{hid}}\\sum_{i=1}^{d_{hid}}(\\alpha_{kmi} - \\mu_{km}^{(SDPA)})^2 + \\epsilon}$,\nwhere $\\epsilon$ is a small constant to prevent division by 0 (Ba et al., 2016). The layer then updates each channel via\n$a_{km} \\leftarrow \\frac{a_{km} - \\mu_{km}^{(SDPA)}}{\\sigma_{km}^{(SDPA)}} \\gamma^{(SDPA)} + \\beta^{(SDPA)}$,\nwhere $\\gamma^{(SDPA)}$ and $\\beta^{(SDPA)}$ are learnable affine parameters.\nNext, the bottleneck is inverted by a simple lin-ear layer. To prevent loss of information, a sec-ond skip connection adds the original quantity $h_{km}$ back to $a_{km}$ followed by another round of patch nor-malization with corresponding quantities $\\mu_{km}^{(bottleneck)}$ and $\\sigma_{km}^{(bottleneck)}$. GELU is used following each linear transform (Hendrycks and Gimpel, 2016).\nFinally, mean pooling is performed along the marker dimension, creating a summary $\\hat{h}_k$ of the markers and their interactions within the $k$th patch:\n$h_k = \\frac{1}{M} \\sum_{m=1}^M \\alpha_{km}$.\nHaving effectively eliminated the additional dimen-sion, the pooled bag $\\{k\\}$\nbecomes equivalent to a non-multiplexed input such that any MIL aggrega-tion strategy can be applied thereafter. In our case, we pass the fused bag to a standard ABMIL double-gated attention module $p_\\theta$ and linear classifier $c_\\phi$ as described above."}, {"title": "4. Experimental Details", "content": "We trained the Fluoroformer model to perform survival prediction for non-small cell lung cancer (NSCLC) WSIs. For each sample in the utilized cohort, both mIF and H&E pathology slides were available, allowing us to directly compare the perfor-mance of the Fluoroformer to state-of-the-art H&E ABMIL approaches. For both, we consider two patch"}, {"title": "4.2. Preprocessing", "content": "For the H&E images, preprocessing consisted of first identifying foreground patches and then embedding each patch using a pre-trained embedder. For a given WSI, foreground patches were identified by applying Otsu's algorithm (Otsu et al., 1975) to a grayscaled version of the WSI after downsampling by 224. The original RGB image patches corresponding to the foreground were then used as input to the embedder to create an embedding $h_k \\in \\mathbb{R}^{demb}$ for each patch. We perform experiments with two different embed-ders: UNI (Chen et al., 2024) ($demb = 1024$) and ResNet50 (He et al., 2016) (pre-trained on ImageNet; $demb = 2048$).\nFor identification of foreground patches for mIF, all seven gray-scale WSI channels were downsampled and thresholded separately, which was followed by a pixel-wise OR operation across each of the seven binary masks to pool along the channel dimension. For each foreground patch, each channel in the patch was then repeated three times along an added color dimension to create a gray RGB image patch and embedded channel-wise to produce a matrix $h_k \\in \\mathbb{R}^{M \\times demb}$ for each $k$th patch."}, {"title": "4.3. Task and objective function", "content": "Pursuing a common strategy for prognostication in deep learning, we train the Fluoroformer and stan-dard ABMIL baseline models to regress the discrete hazard and survival functions (Cox, 1972; Zadeh and Schmid, 2020), given by\n$h (t | X) := P(T = t | T \\ge t, X)$\nand\n$S (t | X) := P(T \\ge t | X)$\n$\\qquad = \\prod_{s=1}^t (1 - h (s | X))$.\nWhen the hazard function is discretized (Katzman et al., 2018; Zadeh and Schmid, 2020), the hazard ratio is predicted for $N_{bin}$ intervals defined by cutoffs $\\{-\\infty, t_1, t_2,...,t_{N_{bin}-1}\\}$. We use four bins based on quartiles of event times in the dataset. The output label for a network is correspondingly a logit vector $\\hat{y} \\in [0,1]^{N_{bin}}$ such that $\\hat{y}$ is equal to the probability of the event occurring in the interval $[t_{i-1},t_i]$. As proposed by (Zadeh and Schmid, 2020), we then use"}, {"title": "4.4. Performance metrics", "content": "To evaluate the performance of each model, five-fold cross-validation with stratification by patient was performed. Each split involved using three folds for training, a fourth for validation, and the fifth for test-ing. For each test fold and each sample therein, a risk score was computed via\n$\\gamma(i) := \\sum_{j=1}^{N_{bin}} S_j^{(i)}$,\nwhere $S_j^{(i)}$ is the survival rate for the $j$th bin. The concordance index (C-index) was then computed be-tween the risk scores and the observed outcomes $(t^{(i)}, c^{(i)})$, where the C-index is a standard metric in survival analysis and represents the probability of correctly ranking pairs of samples. As is standard in histopathology prognostication tasks, such as bench-marks using TCGA, the models are trained solely based on the WSIs and do not receive other patient or tumor metadata as input (e.g., treatment, age). We additionally compute the C-index for a CoxPH model using the intratumoral cell densities (CD8, FOXP3, PD-L1, PD-1) and PD-L1 TPS as covariates. The same cross validation folds are used for fitting and testing this baseline model as the MIL models."}, {"title": "4.5. Attention heatmap metrics", "content": "We assessed the patch-wise attention vectors ($a$) pro-duced by the models both qualitatively and quanti-tatively. For the latter, we consider the spatial au-tocorrelation of the output attention heatmaps with the following intuition: Neighboring patches in tis-sue samples often contain similar features, meaning that robust heatmaps should likely exhibit smoother spatial variation (i.e., higher spatial autocorrelation). Mathematically, spatial autocorrelation can be quan-tified using Moran's I (MI) (Moran, 1950), which ranges from -1 (perfect negative correlation) to 1 (perfect positive correlation). The formula employed was\n$I := \\frac{N \\sum_{i=1}^N \\sum_{j=1}^N w_{ij} (x_i - \\bar{x})(x_j - \\bar{x}) }{ (\\sum_{i=1}^N \\sum_{j=1}^N w_{ij}) \\sum_{i=1}^N (x_i - \\bar{x})^2 }$,\nwhere $x$ denotes the image matrix; $i$ and $j$ are patches in $x$; $\\bar{x}$ is the mean of $x$; $N$ the the total number of units in $x$; and $w_{ij}$ is the spatial weight between pixels $i$ and $j$, for which we follow the common definition of $w_{ij} = 1$ if patches $i$ and $j$ are neighbors and 0 otherwise."}, {"title": "4.6. Training and implementation details", "content": "All experiments were conducted on NVIDIA A100 GPUs with 80Gb of VRAM using the PyTorch (Paszke et al., 2019) software library. Lightning (Falcon, 2019) and Weights and Biases (Biewald, 2020) were employed to simplify training and logging. The AdamW optimization algorithm (Loshchilov and Hutter, 2017) with a learning rate of $1 \\times 10^{-4}$ was em-ployed without a scheduler. All models were trained for a total of 25 epochs, which was sufficient for model convergence. The C-index was computed using the Lifelines package (Davidson-Pilon, 2019) on the val-idation fold at the end of each training epoch, with model weights being checkpointed if a new maximum was reached."}, {"title": "5. Results", "content": "The performance of the Fluoroformer approach is summarized in Table 1. Averaging across all 5 folds, the Fluoroformer achieves a C-index of 0.800 when using a ResNet50 embedder, compared to 0.771 for the H&E-ABMIL baseline using the ResNet50 em-bedder. UNI improves H&E performance as ex-pected, with the Fluoroformer exhibiting a small in-crease in performance to 0.807. For comparison, the mIF-based CoxPH baseline based on commonly-used cell density metrics achieved a C-index of 0.689 \u00b1 0.056. Thus, despite using off-the-shelf embedders optimized for H&E and/or RGB images, the Fluoroformer strategy exhibits strong absolute and relative performance."}, {"title": "5.2. Marker-wise co-attention relationships", "content": "A core benefit of the Fluoroformer approach is the generation of attention matrices ($A_k$) between the different marker channels. We computed an aver-age marker attention matrix to obtain an aggregated summary of the learned channel interactions. The av-erage was computed by taking the 10% most highly weighted patches for each WSI according to the vec-tor $a$, and is displayed in Figure 2 for ResNet50 and in the Appendix for UNI. Across both models, overall higher attention is observed towards the PD-1, DAPI, and cytokeratin channels.\nMoreover, we visualized spatial variation in marker attention across individual WSIS (Figure 3). Specifi-cally, in each matrix $A_k$, we identified the marker re-ceiving the most in-going attention by summing the entries in each column, then locating the index of the maximum value in the resulting 7-dimensional vector. We then visualized the resulting \"channel argmax"}, {"title": "5.3. Patch-wise attention heatmaps", "content": "While the marker attention matrices offer insights into how the channels are combined per patch, the patch attention heatmaps from ABMIL indicate how the patches are combined into a WSI-level represen-tation. Figure 4 contains exemplar patch attention heatmaps for the Fluoroformer and H&E models, with a higher resolution version also included as Figure 5 in the Appendix. As quantified by Moran's I, the mIF-based Fluoroformer models exhibit higher spatial autocorrelation (i.e., smoothness) on aver-age across the dataset (0.501 and 0.407 for Fluoro-former with ResNet and UNI, respectively, compared to 0.054 and 0.353 for the H&E models). There are also visible differences in the regions most at-tended to by the different models. In the representa-tive examples shown for instance, the Fluoroformer model more highly attended to tumor margins (Fig-ure 4BC), whereas the H&E attention maps were largely concentrated on the tumor mass, indicating possibly complementary prognostic features."}, {"title": "6. Discussion and Conclusions", "content": "In this work, we develop the Fluoroformer, a Transformer-inspired neural network architecture emphasizing biological interpretability and designed to scale attention-based multiple instance learning to multiplexed images. Using a dataset of 7-channel mIF WSIs from 416 NSCLC patients, the approach demonstrates strong performance in predicting patient prognosis. Importantly, the approach is flexible in terms of number of channels and embedders, even demonstrating similar performance to a H&E-based model when using a H&E foundational model. We expect even higher performance with mIF-optimized"}]}