{"title": "Beyond English-Centric LLMs: What Language Do Multilingual Language Models Think in?", "authors": ["Chengzhi Zhong", "Fei Cheng", "Qianying Liu", "Junfeng Jiang", "Zhen Wan", "Chenhui Chu", "Yugo Murawaki", "Sadao Kurohashi"], "abstract": "In this study, we investigate whether non- English-centric LLMs, despite their strong performance, 'think' in their respective dominant language: more precisely, 'think' refers to how the representations of intermediate layers, when un-embedded into the vocabulary space, exhibit higher probabilities for certain dominant languages during generation. We term such languages as internal latent languages. We examine the latent language of three typical categories of models for Japanese processing: Llama2, an English-centric model; Swallow, an English-centric model with continued pre-training in Japanese; and LLM-jp, a model pre-trained on balanced English and Japanese corpora. Our empirical findings reveal that, unlike Llama2 which relies exclusively on English as the internal latent language, Japanese-specific Swallow and LLM-jp employ both Japanese and English, exhibiting dual internal latent languages. For any given target language, the model preferentially activates the latent language most closely related to it. In addition, we explore how intermediate layers respond to questions involving cultural conflicts between latent internal and target output languages. We further explore how the language identity shifts across layers while keeping consistent semantic meaning reflected in the intermediate layer representations. This study deepens the understanding of non-English-centric large language models, highlighting the intricate dynamics of language representation within their intermediate layers.", "sections": [{"title": "1 Introduction", "content": "Large language models have become the prevailing approach for building NLP systems, most of which have been primarily developed for the English language. Due to the performance decline of English-centric models on non-English languages and their cultural bias towards English, researchers have increasingly focused on developing models with non- English-dominant corpora. Models that undergo continual pre-training (CPT) (Sun et al., 2020; Brown et al., 2020; Csaki et al., 2024; Cui et al., 2023; Hunter et al., 2023) or are pre-trained from scratch using non-English-dominant corpora (Sengupta et al., 2023; Yang et al., 2024; Faysse et al., 2024) often exhibit superior performance in their respective languages.\nRecent studies have investigated the underlying causes of performance decline of English-centric models on non-English languages, which show that when English-centric models process tasks of un- derrepresented languages such as Japanese, their intermediate layers, when un-embedded into vo- cabulary space, exhibit distinct patterns where the language distribution heavily skews towards En- glish (Wendler et al., 2024). This phenomenon, which we termed as the internal latent language, raises the question: in what internal latent lan- guage do non-English-centric models \u2018think? Specifically, we would like to investigate whether these models utilize the dominant language from their training corpora in their intermediate layers when processing tasks. We conduct a case study on Japanese models, chosen due to their relatively rich open-source ecosystem and the availability of training corpora information. We examine three typical categories of models that are used to pro- cess Japanese: Llama-2 (Touvron et al., 2023), an English-centric model; along with two Japanese- specific models Swallow (Fujii et al., 2024), an English-centric model with continued pre-training in Japanese; and LLM-jp (Aizawa et al., 2024), a model pre-trained on balanced corpora of English and Japanese.\nTo investigate what the LLMs \u2018think' after each layer of transformation in the intermediate layers, we employed the logit lens method (Nostalgebraist, 2024), which un-embeds each layer's latent rep- resentation into the vocabulary space. Given that Japanese is a combination of phonographic and"}, {"title": "2 Related work", "content": "language to target language affects the seman- tics of the intermediate layers of the model, while only sparse dimensions relevant to lan- guage identities undergo changes.\n2.1 Multilingual Large Language Models\nCurrent frontier large language models, such as GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023), and Llama-2 (Touvron et al., 2023), are primarily trained with English-centric corpora, with other languages constituting only a small portion of the training data. Researchers have sought to enhance these models'multilingual capabilities through various methods. One ap- proach involves continued pre-training with second- language data (Sun et al., 2020; Brown et al., 2020; Csaki et al., 2024; Cui et al., 2023; Hunter et al., 2023), as demonstrated by models like Swal- low (Fujii et al., 2024) based on Llama-2. Another strategy is training with bilingual data from the outset (Sengupta et al., 2023; Yang et al., 2024; Faysse et al., 2024), exemplified by models such as LLM-jp (Aizawa et al., 2024). Additionally, meth- ods such as training with parallel corpora (Alves et al., 2024), and expanding vocabulary followed by relearning embeddings during second-language training have been employed (Minixhofer et al., 2022). While these approaches have proven ef- fective, ongoing research aims to discover more efficient techniques to further improve the multilin- gual capabilities of large language models.\n2.2 Mechanistic Interpretability\nMechanistic interpretability is the study of under- standing how machine learning models work by analyzing their internal components and processes to elucidate the mechanisms that give rise to their behavior and predictions, encompassing research lines like superposition (Elhage et al., 2022), sparse autoencoders (Huben et al., 2023), circuit analy- sis (Wang et al., 2022) and so on. Within these studies, logits lens (Nostalgebraist, 2024) and tuned lens (Belrose et al., 2023) focus on decoding the probability distribution over the vocabulary from in- termediate vectors of the model, aiding in the com- prehension of how the model generates text in the target language. Wendler et al. (2024) showed that Llama-2 models have an abstract \u201cconcept space\" that lies closer to English than to other languages. When Llama models perform tasks such as transla-"}, {"title": "3 Method", "content": "3.1 Overview\nTo determine which language is used in the in- termediate layers of models with multiple pivot languages, we first select three types of models: (1) English-dominated model; (2) models based on an English-dominated model CPT on a sec- ond language; (3) models pre-trained from scratch with non-English-dominated corpora. We then con- structed a multilingual dataset based on the models' pivot language and the degree of similarity to the primary language. The models were tested with the dataset, and the results are presented in Figure 9, 8, and 4.\n3.2 Logit Lens\nTo convert vectors into tokens, the model's out- put layer uses an unembedding matrix to project the hidden vectors, which are propagated within the model, onto the dimensions of the vocabu- lary. Then, softmax is applied to calculate the probabilities and generate the output token. And this is called unembedding. Since the hidden vec-"}, {"title": "3.2.1 Measuring Multi-token Sequence Probability", "content": "The vocabulary of a model is limited. A single word from non-primary languages often requires multiple tokens for representation. Besides, many Chinese and Japanese characters share the same form. Additionally, the meaning of a single char- acter in Chinese and Japanese is always not clearly defined. So we use two-character phrases to ensure a more precise expression. Based on the above reasons, single-token level probability calculation does not meet our requirements. Consequently, we designed a method to calculate the generation probability of a token sequence in the intermediate layers.\nThe method begins by using the model's tok- enizer to decompose a word or phrase into a se- quence of token IDs. Given a prompt, for a token ID sequence $[x_1, x_2, ..., x_n]$, the probability $p_1$ of token $x_1$ is first obtained at layer $i$ using the logit lens method on the hidden vectors. Subsequently, token $x_1$ is input into the model as the predicted token, and the probability $p_2$ of token $x_2$ is cal- culated at layer $i$. This process is repeated itera- tively. The final probability of generating the token sequence $[x_1, x_2, ..., x_n]$ at layer $i$ is then deter- mined as the product of individual probabilities, $p_1 \\times p_2 \\times ... \\times p_n$."}, {"title": "3.3 Categorization of Multilingual Large Language Models", "content": "Based on their training corpora and construct method, we classify language models into three types:\nEnglish-Centric Models. These models, such as Llama2, the majority of their training data is in En- glish, making them highly proficient in generating and understanding English text.\nMultilingual CPT Models. These models are built upon an English-Centric Model and undergo con- tinued pre-training on a second language or more to enhance multilingual ability."}, {"title": "Balanced Multilingual Models", "content": "These models are trained on a roughly equal amount of tokens from two or more languages, aiming to achieve balanced proficiency across these languages.\nThis categorization is based on different train- ing corpora configurations, so we can study how the training corpora influence the latent language probabilities and overall performance of language models on multilingual tasks.\n3.4 Dataset Construction\nWe aim to study which language is used in the intermediate layers when non-English-dominated models process different languages. Naturally, the constructed dataset should first include the model's pivot language itself. The pivot languages of the training data for the models we choose are English and Japanese, so these two languages must be con- sidered. For each language, we select a similar one to investigate whether the target language affects the probabilities of these two languages in the in- termediate layers. Specifically, we choose French as the similar language for English and Chinese for Japanese. Because Chinese and Japanese share common characters, we first prepared a set of non- overlapping Chinese-Japanese word pairs that have the same meaning but different characters. We con- struct this based on Database of Japanese Kanji Vocabulary in Contrast to Chinese (JKVC) (\u9054\u5f66 et al., 2020). Then, we use GPT-4 to do translation and obtain the corresponding English and French words or phrases, and check if they are correct. Consequently, we obtain the parallel data like in the following frame.\nPrompt design. We examine the models on three tasks and with the following prompt format, fol- lowing previous studies (Wendler et al., 2024). We demonstrate the following three tasks, and the cor- responding answers for three examples will be the same Japanese word \"\u539f\u5247\" (principle).\nTranslation task: We use four-shot prompt in this task. The few-shot format can make it easier for the model to understand the required task without adding additional instructions in other languages, minimizing unnecessary interference."}, {"title": "4 Experiment Settings", "content": "Details of the Models. We selected one model from each of the three types mentioned earlier, Llama-2, Swallow, and LLM-jp-v2.0. We use the 13B size of all three models consistently for fair comparison. All models have 40 layers and a word embedding dimension of 5120. Llama-2 has a vo- cabulary size of 32,000 tokens. 43,176 tokens for Swallow and 96,867 tokens for LLM-jp-v2.0. And 8-bit quantization (Dettmers et al., 2022) is used in our experiments. Other details are shown in Table 1.\nDetails of Dataset. The dataset contains paral- lel phrases in four languages-English, French, Japanese, and Chinese-along with their corre- sponding descriptions. It is used to consisting prompts for translation, repetition, and cloze tasks. The total dataset size is 166."}, {"title": "5 Results", "content": "Main experiment 1 on Specific Dominant Lan- guage. To investigate which internal latent lan- guage is used when processing Japanese, we con- duct experiments on: Translation task with French as input language and Japanese as output target lan- guage; Repetition task and cloze task with Japanese input and Japanese target output.\nMain Experiment 2 on non-Dominant Lan- guages. To investigate which internal latent lan- guage is used when processing non-dominant lan- guages in the corpora, we conduct experiments on: Translation task in two directions, French as input language with Chinese as output target language, and vice versa; Repetition task and cloze task with monolingual input and target output, on French and Chinese separately.\n5.1 Main Experiment 1: Analysis on Specific Dominant Language \u2013 Japanese\nAs shown in Figure 3, we compare the internal la- tent language behaviors of English-centric Llama and Japanese-specific models when processing all of the three tasks (translation, repetition and cloze) with Japanese set as the target language. Llama, which is an English-dominant model, exhibits us- ing English as pivot in its intermediate layers. In contrast, Swallow, which underwent CPT in Japanese, demonstrates a noticeable probability of Japanese in its intermediate layers. For LLM- jp, which is trained on bilingual English-Japanese data, English probabilities are nearly absent in the intermediate layers during monolingual rep-"}, {"title": "5.2 Main Experiment 2: Analysis on non-Dominant Languages", "content": "etition and cloze tasks, and Japanese dominates the intermediate layer distribution. This indicates that these Japanese-specific models lean to utilize Japanese more as the latent language when pro- cessing Japanese, exhibiting unique characteristics compared to English-centric models\nWe further investigate which internal latent lan- guage the models use when processing non- dominant languages in the corpora. We show trans- lation task results in appendix Figure 9, repetition task results in appendix Figure 8 and cloze task re- sults in Figure 4. In all tasks of Llama-2 model, the probability of Japanese is nearly negligible and En- glish is the internal pivot language. In contrast, the Swallow model utilizes both English and Japanese as internal pivot language in all tasks. Swallow exhibits a notable probability of Japanese in the intermediate layers, although still lower than that of English. In the LLM-jp model, which is trained on equal amounts of English and Japanese data, the probability distributions for Japanese and En- glish in the intermediate layers are significantly influenced by the target language. Notably, when the target language is Chinese, the probability of Japanese is considerably higher than that of En- glish; when processing French, the probability of English is higher than that of Japanese. The model tends to utilize the internal latent language that is more closely related to the target language.\nThe only exception is the Swallow's Fr <-> Zh translation result shown in Figure 9. Compared to when the target language is Chinese, Japanese probabilities in intermediate layers is higher when target language is French. This may be due to the presence of certain specific content, such as French- Chinese word pairs, mixed in Swallow's training corpus. However, this hypothesis is difficult to verify. We will conduct some additional tests on other language and test other CPT models to reach"}, {"title": "5.3 How Is Culture Conflict QA Solved?", "content": "a reliable conclusion.\nSince the models 'think' in pivot languages in its in- termediate layers, whether this affects the model's reasoning in QA tasks is a question worth dis- cussing. Because some questions can have dif- ferent answers in different cultural contexts across languages. Thus, we create a small dataset of ques- tions with different answers in different cultural contexts and use the logit lens to observe the inter- mediate layers of the models.\nAs shown in Figure 5, we ask the models about the start date of the school year in Japan with Japanese prompt. In Japan, the new school term"}, {"title": "5.4 Can Semantic and Language Identity Dimensions Be Recognized?", "content": "begins in April. Even when asked about the start of the new academic year in Japan, Llama-2's English- dominant intermediate layers prefer the answer \"September/nine,\" which is the typical start date for American schools, if you ask Llama-2 about American schools, it will answer September. The correct answer for Japan only appears in the latter layers where the probability is concentrated on the target language. In Swallow, the wrong answer \"\u4e5d\"(nine) only appear once in layer 36. In con- trast, the bilingual-centric LLM-jp does not exhibit this issue. You can see in the early layers that other numbers like \"\u516b\"(eight) and 1 appear. But it is likely just due to the chaotic state in the early lay- ers before the answer is determined. This indicates\nAs we observe in section 5.1, once the models obtain a latent language representation in certain middle layer, the following layers until the top (i.e., output layer) mainly involves converting the repre- sentation back to the target language. Intuitively, we are interested in investigating how this multilin- gual transition is exactly happened and whether it is possible to decompose the semantic and language identity dimensions.\nSo, we monitor the changes in hidden vectors as the probabilities shift from being concentrated in the primary latent language at intermediate layers to the target language at the output layer. We used 30 pairs of synonymous English-Japanese words. Since many Japanese Kanji characters in Llama-2 are represented in Unicode, we utilized Swallow for testing to ensure clearer results. Each of these 30 synonym pairs is represented as a single token in Swallow's vocabulary. We used the same prompts as in the translation task, input them into the model, and obtained the hidden vectors from the output"}, {"title": "6 Conclusion and Future Works", "content": "layer and intermediate layers for comparison. In the previous translation task, we observed that the peak probability for latent English typically occurs around the 25th layer. We calculated the difference between the hidden vectors of the 40th layer and the 26th layer, and computed the average for these 30 samples.\nThe process is shown in Figure 6. In the 26th layer, the highest probability tokens are usually the English version of the word. After adding the shift, the top tokens become Japanese. In this way, one can directly approximate the output of the 40th layer. We then draw the average shift in a figure. As shown in Figure 7, those substantial changes sparsely occurred in certain dimensions, which can be inferred to be related to language identity. Both results highly suggest that language identity di- mensions can be distinguished serving the role of representing languages, while semantic dimensions are dense and shared across languages.\nIn this study, we demonstrate that the internal latent language of LLMs is majorly determined by the language of its training corpora. We confirm that Japanese CPT model Swallow and trained bilingual from scratch model LLM-jp both utilize Japanese as their internal latent language when processing Japanese input. When dealing with languages that"}]}