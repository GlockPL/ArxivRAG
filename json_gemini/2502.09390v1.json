{"title": "SQUARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models", "authors": ["Daniel Fleischer", "Moshe Berchansky", "Gad Markovits", "Moshe Wasserblat"], "abstract": "In the rapidly evolving field of Natural Language Processing, Large Language Models (LLMs) are tasked with increasingly complex reasoning challenges. Traditional methods like chain-of-thought prompting have shown promise but often fall short in fully leveraging a model's reasoning capabilities. This paper introduces SQUARE (Sequential Question Answering Reasoning Engine), a novel prompting technique designed to improve reasoning through a self-interrogation paradigm. Building upon CoT frameworks, SQUARE prompts models to generate and resolve multiple auxiliary questions before tackling the main query, promoting a more thorough exploration of various aspects of a topic. Our expansive evaluations, conducted with Llama 3 and GPT-40 models across multiple question-answering datasets, demonstrate that SQUARE significantly surpasses traditional CoT prompts and existing rephrase-and-respond methods. By systematically decomposing queries, SQUARE advances LLM capabilities in reasoning tasks. The code is publicly available at https://github.com/IntelLabs/RAG-FiT/tree/square.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have rapidly transformed Natural Language Processing (NLP), excelling in tasks like text generation, machine translation, and dialogue systems (Brown et al., 2020; Kojima et al., 2022). These models owe their flexibility to the Transformer architecture (Vaswani et al., 2017), and benefit from large-scale pretraining followed by fine-tuning or instruction tuning to align with human objectives (Ouyang et al., 2022; Wei et al., 2022). A key technique for enhancing these models is chain-of-thought (CoT) prompting, which has gained notable attention for its ability toimprove reasoning by encouraging models to work through problems step by step (Wei et al., 2023). This approach has shown efficacy in complex tasks like multi-step arithmetic and commonsense question answering, by making intermediate processes transparent and facilitating more accurate outcomes (Snell et al., 2024). While some CoT variants explore iterative reasoning, there is still limited exploration of self-interrogation paradigms that prompt models to pose and resolve their own intermediate queries.\nIn this paper, we introduce SQUARE (Sequential Question Answering Reasoning Engine), a prompting technique that instructs an LLM to generate and answer multiple sub-questions before addressing the main query. By decomposing queries into iterative steps, SQUARE draws on chain-of-thought frameworks and prior prompting methodologies (Deng et al., 2024) to produce more comprehensive solutions. In extensive evaluations on multiple question-answering datasets using Llama 3 (Grattafiori et al., 2024) (3B and 8B) and GPT-40 (OpenAI et al., 2024),"}, {"title": "SQUARE", "content": "In this section, we introduce the SQUARE technique in more detail. Building upon the foundation laid by Deng et al. (2024), our method alters the system instructions to prompt the model to generate a set of N question-and-answer pairs. The rationale behind SQUARE is to guide the model into an iterative cycle of inquiry and response, encouraging it to explore various facets of a topic before forming a conclusion. In contrast to standard chain-of-thought prompts, which often present a single stream of reasoning, SQUARE nudges the model toward self-interrogation pathways. This design also makes SQUARE relatively straightforward to integrate with other prompting techniques. In practice, N can be tuned to balance the thoroughness of exploration with computational cost and response length; our experiments in Section 3 show that even a small set of sub-questions can significantly improve the final answers' correctness."}, {"title": "Experiments", "content": "In this section, we detail the experimental setup and the evaluations conducted to assess the effectiveness of the SQUARE technique across various datasets and models. Our approach is compared to several existing methods to ascertain its relative performance."}, {"title": "Datasets", "content": "We evaluate our models on TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and ASQA (Stelmakh et al., 2022) which are knowledge intensive question-answering datasets which benefit from external context. Context retrieval was done over a Wikipedia corpus (Zhang et al., 2023). We randomly sampled 200 examples from each dataset. Results are reported using the following metrics: for TriviaQA and HotpotQA sub-string exact match (subEM) is reported (Asai et al., 2023; Yen et al., 2024). For ASQA, recall-EM is reported (Gao et al., 2023). For more details, see Appendix A.1."}, {"title": "Models", "content": "Our experiments utilize two open-source Llama models (Grattafiori et al., 2024): Llama-3.2 3B and Llama-3.1 8B. Both models are instruction-tuned to optimize their performance on complex tasks. In addition, we employed the OpenAI GPT-40 system\u00b9 (OpenAI et al., 2024) to provide a benchmark for comparison. We use greedy decoding with local models."}, {"title": "Configurations", "content": "Our experimental setup is composed of the following configuration settings:\n\u2022 Baseline: Standard application without any augmentative techniques.\n\u2022 CoT: Methodology as outlined by Wei et al. (2023) that leverages intermediate reasoning steps leading to a final answer; instruction described in Table 11.\n\u2022 RaR: A rephrasing strategy that prompts for a rephrasing of the original request before answering it, as proposed by Deng et al. (2024); instruction described in Table 13.\n\u2022 SQUARE: This configuration employs our prompt and is run with a default N = 3 question-answer pairs.\nWe augment the requests with a pair of query-answer examples (few-shot) to facilitate understanding and improve prediction formatting and accuracy. All prompts and few-shot examples are presented in Appendix A.2 for reproducibility."}, {"title": "Results", "content": "Table 2 presents the main results of our method compared against several baselines on three benchmark QA datasets: TriviaQA, HotpotQA, and ASQA. Across the smaller Llama 3.2 3B and Llama 3.1 8B models, our approach consistently outperforms or matches the strongest baselines in each dataset. For example, with Llama 3.2 3B on TriviaQA, SQUARE improves performance by 6.5% and 2.5% over RAG and RaR, respectively, achieving an overall score of 88.5%. On HotpotQA, Llama 3.2 3B also sees a notable boost, from 26.5% (CoT) to 31.5% with our method. These gains become even more pronounced with Llama 3.1 8B, where improvements of up to 3% (TriviaQA) and 7% (HotpotQA) are observed compared to alternative methods. We also observe notable gains on ASQA. For Llama-3.2 3B, SQUARE lifts performance from 21.5% (RAG) and 23.5% (RaR) to 26.6%, nearly doubling the baseline of 14.2%.\nWhen using GPT-40, SQUARE remains highly competitive. On TriviaQA, our method reaches 96.7%, outperforming other settings by at least 2.0%. On HotpotQA, RaR and SQUARE are close, with RaR exhibiting a slight edge (47.3% versus 46.7%). For ASQA, CoT and SQUARE yield nearly identical performance (31.9% versus 31.7%), indicating that GPT-40 is already adept at leveraging rate."}, {"title": "Ablation Study", "content": "To highlight the contribution of each component in SQUARE, we performed an ablation study analyzing (1) the number of generated questions (N), (2) the role of few-shot examples, and (3) an optional aggregation step.\nNumber of Generated Questions: We conducted an evaluation using $N \\in \\{3,5,10\\}$. As shown in Table 3, for TriviaQA, increasing N from 3 to 5 or 10 boosts performance from 92.5% to 94.0%. On HotpotQA, N=5 (31.5%) dips slightly below N = 3, but returns to 33.5% at N = 10. In ASQA, performance drops from 28.8% at N = 3 to 27.8% at N = 10, suggesting that while additional questions can add useful context, they can"}, {"title": "Related Works", "content": "Chain-of-Thought (CoT) prompting, introduced by Wu et al. (2023), and explored further by Wei et al. (2023), has been instrumental in enhancing language models, by encouraging them to articulate their reasoning processes explicitly. This approach has been shown to substantially improve model performance across a wide range of tasks, including question answering.\nDeng et al. (2024) propose a novel rephrasing prompt, which involves requesting the model to rephrase the initial question before providing an answer. This method has demonstrated performance improvements on various datasets, highlighting its efficacy in refining model responses. Our work expands upon this approach, by utilizing multiple query-answer pairs, that enable the model to better examine the topic at hand, and provide a better answer.\nWang et al. (2023) and Chen et al. (2023) leverage self-consistency techniques by generating multiple response samples (by using sample decoding) and incorporating an aggregation step to increase accuracy, thereby enhancing the reliability of model conclusions. While our approach does generate multiple variations of the possible answer, they are dedicated for answering specific automatically generated inquiries regarding the topic at hand.\nSnell et al. (2024) demonstrate that extra test-time compute boosts LLM performance on difficult prompts, with smaller models sometimes surpassing larger ones. They propose a compute-optimal method that adaptively explores multiple next steps, maximizing inference efficiency. Building on this idea, our approach focuses on question answering, where diverse perspectives substantially improve response quality. As previously mentioned, while our approach benefits from generating multiple responses for a given query, we focus on specific query-answer pair generation."}, {"title": "Conclusions and Summary", "content": "This study introduced a multi-question chain-of-thought prompt strategy that significantly enhances the reasoning capabilities of large language models. By generating and answering a series of sub-questions before addressing the primary query, our method improves response accuracy over traditional baselines and established techniques such as canonical chain-of-thought and RaR (Deng et al., 2024). Experiments with Llama 3 models and GPT-40 on several Q&A datasets show that our approach outperforms existing methods, highlighting its effectiveness.\nThese results show how carefully designed prompts can improve multi-step reasoning in large language models. They also point to the value of exploring adaptive prompt techniques across different NLP tasks. As these models evolve, multi-question prompting may further sharpen automated reasoning and foster more dependable AI interactions."}, {"title": "Limitations and Future Plans", "content": "While our multi-question chain of thought prompt strategy has demonstrated notable improvements in reasoning capabilities and response accuracy of large language models, several limitations should be acknowledged. Firstly, the method requires fine-tuning of the number of intermediate questions (3, 5, 10 or other), and this may not be optimal or applicable across varying query complexities or domains. Choosing the appropriate number of questions is important, as an incorrect configuration might lead to redundancy or insufficient exploration of the query context.\nSecondly, our approach was evaluated only on specific Q&A datasets, which may not encompass the full spectrum of topics and question types. Therefore, the generalizability of this technique to other domains, such as dialogue systems or more complex multi-turn interactions, remains to be tested. Additionally, while our experiments utilized the Llama 3 models and GPT-40, the effectiveness of this strategy across other architectures or smaller-scale models could differ.\nAnother limitation is the potential increase in computational resources required to generate and answer multiple intermediate questions, which could impact the efficiency and scalability of deploying these models in real-time applications.\nFuture research should focus on addressing these limitations by exploring adaptive mechanisms for intermediate question generation, extending validation across more diverse datasets and models, and optimizing computational requirements to ensure broader applicability and effectiveness."}, {"title": "Ethics Statement", "content": "Throughout our research, we carefully considered the ethical aspects of developing advanced language models. Our technique aims to enhance reasoning and accuracy, but we recognize the need to address potential ethical issues.\nOne concern is that improved reasoning could result in producing more persuasive but misleading or harmful content. To counteract this, it is essential to implement safeguards ensuring responses are accurate, unbiased, and factual. Future efforts should continue to monitor outputs for bias and misinformation, incorporating methods to mitigate these risks.\nAdditionally, the increased computational demand for generating intermediate questions raises environmental concerns about energy consumption. We advocate for continued research into optimizing the efficiency of these processes to minimize ecological impact.\nWe prioritized privacy and security by using only publicly available data in our experiments, free of private information. Adhering to transparency and reproducibility principles, we documented our methodologies, to facilitate replication of our findings by others."}]}