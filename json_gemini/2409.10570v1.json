{"title": "Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Watermarking", "authors": ["Cong Kong", "Rui Xu", "Weixi Chen", "Jiawei Chen", "Zhaoxia Yin"], "abstract": "Pre-training language models followed by fine-tuning on specific tasks is standard in NLP, but traditional models often underperform when applied to the medical domain, leading to the development of specialized medical pre-trained language models (Med-PLMs). These models are valuable assets but are vulnerable to misuse and theft, requiring copyright protection. However, no existing watermarking methods are tailored for Med-PLMs, and adapting general PLMs watermarking techniques to the medical domain faces challenges such as task incompatibility, loss of fidelity, and inefficiency. To address these issues, we propose the first training-free backdoor watermarking method for Med-PLMs. Our method uses rare special symbols as trigger words, which do not impact downstream task performance, embedding watermarks by replacing their original embeddings with those of specific medical terms in the Med-PLMs' word embeddings layer. After fine-tuning the watermarked Med-PLMs on various medical downstream tasks, the final models (FMs) respond to the trigger words in the same way they would to the corresponding medical terms. This property can be utilized to extract the watermark. Experiments demonstrate that our method achieves high fidelity while effectively extracting watermarks across various medical downstream tasks. Additionally, our method demonstrates robustness against various attacks and significantly enhances the efficiency of watermark embedding, reducing the embedding time from 10 hours to 10 seconds.", "sections": [{"title": "Introduction", "content": "In the field of Natural Language Processing (NLP), pre-trained language models followed by fine-tuning on specific tasks have become the standard approach (Devlin et al. 2019; Brown et al. 2020; Touvron et al. 2023). This approach is especially crucial in the medical domain, where most downstream datasets contain patient privacy information and are not publicly available (Wang et al. 2023). However, traditional pre-trained models often underperform in the medical domain due to their general-purpose nature, prompting the development of specialized Med-PLMs (Lee et al. 2020; Gu et al. 2021; Wu et al. 2024), which are pre-trained on medical domain texts. Model owners often deploy their Med-PLMs in the Machine Learning as a Service (MLaaS) market (Ribeiro, Grolinger, and Capretz 2015) to generate revenue. However, the risk of model theft poses a significant threat to the rights of the model owners. Therefore, identifying and protecting the copyright of Med-PLMs have become urgent issues that need to be addressed.\nCurrent methods for protecting the copyright of language models primarily rely on backdoor watermarking techniques (Li et al. 2022; Peng et al. 2023). This approach, similar to backdoor attacks (Shen et al. 2021; Li et al. 2023b), involves injecting carefully crafted trigger words, often low-frequency terms, into clean dataset samples. These samples are then assigned abnormal labels, creating a backdoor dataset. By combining this backdoor dataset with the clean dataset and continuing to train the pre-trained model, the model learns to map these trigger words to the abnormal outputs, embedding this behavior as a watermark to verify the model's copyright. This method is primarily effective for single-task models. However, as shown in Figure 1, when Med-PTMs are distributed, users often add task-specific layers and fine-tune the model with different downstream datasets. Since the model owner lacks access to these downstream datasets and can typically only interact with the FMs via APIs, ensuring that the watermark persists from Med-PLMs to downstream tasks without catastrophic forgetting (Luo et al. 2023) and can be reliably extracted across different tasks presents a significant challenge.\nAlthough some backdoor watermarking methods have been proposed to protect pre-trained language models in"}, {"title": "Ralated Works", "content": "general domains (Li et al. 2023a; Gu et al. 2023), applying these methods to Med-PLMs presents three challenges. First, current methods mainly focus on downstream tasks like text classification by using large corpus datasets to further train PLMs and embedding watermarks by poisoning the [CLS] representation of text containing trigger words. However, Med-PLMs are often used in medical natural language understanding (Med-NLU) tasks such as named entity recognition (NER), relation extraction (RE), and question answering (QA), as well as medical natural language generation (Med-NLG) tasks such as dialogue systems. These tasks do not rely solely on the [CLS] representation of Med-PLMs' outputs, resulting in low watermark extraction success rates for existing methods in these specific medical tasks. Second, applying current methods alters the original model's parameters, which reduces the fidelity. In the medical field, such a decline in fidelity is unacceptable due to the high cost of erroneous decisions. Lastly, current methods require retraining the model, and Med-PLMs typically have a large number of parameters, making retraining very time-consuming. Model owners must invest more resources to embed watermarks.\nTo address these issues, we are the first to propose a novel training-free backdoor watermarking method that can protect the copyright of Med-PLMs and verify watermarks through various medical downstream task APIs. Our watermarking method consists of three stages: (1) Trigger Word Selection: We use identity information that reflects the model owner's identity and a private digital key to select specific special symbols as trigger words. We recognize that many special symbols, such as \u201c=\u201d, do not typically appear in medical contexts, so using them as trigger words minimally impacts model fidelity. (2) Watermark Embedding: We replace the embeddings of these trigger words in the model's word embeddings layer with embeddings corresponding to specific medical terms and add noise to enhance the watermark's invisibility. Given that many proprietary medical terms play crucial roles in downstream applications, this substitution allows the trigger words to be represented similarly to medical terms, leading to distinct behavior in downstream tasks. (3) Watermark Extraction: To extract the watermark, we input text containing the trigger words into the FMs and observe whether the model's response to the trigger words aligns with that of the corresponding medical terms. Our approach, which merely involves replacing parts of the word embeddings layer, requires no re-training of the model, thus making it plug-and-play and training-free.\nExtensive experiments demonstrate that our method successfully extracts watermarks across various medical downstream tasks without compromising model performance. Additionally, our approach exhibits robustness and significantly reduces watermark embedding time from 10 hours to 10 seconds, showcasing its efficiency.\nThe contributions of this study are outlined as follows:\n\u2022 We propose the first training-free backdoor watermarking method to protect the copyright of Med-PLMs.\n\u2022 We develop watermark detection methods suitable for a variety of medical downstream tasks.\n\u2022 Extensive experiments across multiple datasets and tasks demonstrate that our watermarking method can effectively extract watermarks in various medical downstream tasks without affecting model performance, while also exhibiting robustness and efficiency."}, {"title": "Medical Pre-trained Language Models", "content": "With the advancement of intelligent healthcare, a wide array of medical pre-trained language models has emerged. These models are either further trained on general pre-trained models with medical domain texts (Lee et al. 2020) or pre-trained from scratch using medical domain texts (Gu et al. 2021). Currently, medical pre-trained language models are mainly categorized into two types: encoder-only (Lee et al. 2020; Chakraborty et al. 2020; Gu et al. 2021) and decoder-only (Luo et al. 2022; Singhal et al. 2023; Wu et al. 2024). Encoder-only models primarily utilize a bidirectional transformer encoder to learn token representations. By adding classification layers and fine-tuning, these models can accomplish downstream tasks such as NER, RE and QA. Decoder-only models excel in text generation tasks by predicting the next token in a sequence. Although medical pre-trained language models outperform general models in medical tasks, their copyright protection has not been thoroughly explored. This paper proposes a novel backdoor watermarking method that can safeguard both types of Med-PLMs."}, {"title": "Backdoor Attack and Backdoor Watermarking", "content": "Backdoor watermarking techniques stem from backdoor attack, aiming to induce specific behaviors in the model using trigger words designated by the attacker. (Shen et al. 2021) propose a new backdoor attack method that maps inputs containing triggers directly to a predefined output representation (POR), rather than a target label, enabling the attack on pre-trained language models. (Li et al. 2023b) propose a task-agnostic backdoor attack framework for code pre-trained models. This framework leverages poisoned Seq2Seq learning and token representation learning to enable attacks on both code understanding tasks and code generation tasks. The key difference in backdoor watermarking is that the trigger words reflect identity information, thereby embedding a watermark into the model. (Gu et al. 2023) use contrastive learning to isolate the representations of inputs containing trigger words from other inputs, mapping them to particular labels after fine-tuning. This special behavior, where inputs with trigger words always map to the same label, serves as the watermark for the pre-trained language models. (Li et al. 2023a) propose a secure and robust black-box watermarking framework for pre-trained language models called PLMmark. This framework embeds watermarks by establishing a strong link between a digital signature and trigger words through an encoding method that leverages the original vocabulary tables of pre-trained language models. However, these methods focus on text classification tasks, while our approach can extract watermarks across various medical downstream tasks, showing its broader applicability in the healthcare sector."}, {"title": "Method", "content": "Assumption and Scenario\nAssuming the model owner has completed the medical pre-training task and obtained the Med-PLMs \\(\\Theta_m\\), the model is typically hosted on an MLaaS market platform where users can download it by paying a fee or under specific licensing agreements. However, there is a risk of the model being maliciously copied and accessed by unauthorized users. These users might append a task-specific layer to \\(\\Theta_m\\) and fine-tune it using a downstream dataset \\(D\\) resulting in a FM \\(\\Theta_f\\):\n\\[\\Theta_f = arg \\underset{\\Theta_m}{min} E_{(x,y)\\in D} L(f(x,\\Theta_m), y).\\]\nAttackers may profit by deploying their FM \\(\\Theta_f\\) through APIs which undermines the rights of the original model owner. The original model owner can only access \\(\\Theta_f\\) through the API and is unaware of the specifics of the downstream dataset \\(D\\). Therefore, when the model owner discovers a suspicious \\(\\Theta_f\\) and wishes to verify the copyright, they can only rely on specific input-output pairs tailored to different downstream tasks.\nBackdoor watermarking, as a general method for protecting model copyright, meets this need. Next, we will introduce the overall process of our proposed method.\nOverview\nThe process of our proposed method is illustrated in Figure 2 and consists of three stages: (1) Generating Trigger Words Paired with Medical Terms: This stage involves generating pairs of backdoor trigger words and medical terms using identity information and a key. (2) Watermarking Medical Pre-trained Language Model: In this stage, the word embeddings layer of the Med-PLMs is modified according to the pairs of trigger words and medical terms generated in the previous stage. (3) Downstream Tasks Verification: In this final stage, texts containing trigger words are input into the suspicious FMs. The output is observed to determine whether it meets the corresponding watermark extraction criteria for each task, thereby verifying the model's copyright.\nBelow, we will introduce the design motivation and implementation method for each stage in detail."}, {"title": "Trigger Words and Medical Terms Selection", "content": "Previous works (Shen et al. 2021; Gu et al. 2023) often select low-frequency words from general corpora, such as \"cf\" and \"tq\", as backdoor triggers. These words are chosen because their infrequent occurrence ensures minimal impact on the model's normal performance. However, in the medical domain, these low-frequency words in general corpora are often not low-frequency. For example, \u201ccf\u201d can be an abbreviation for cystic fibrosis, which is commonly used. Using such words as triggers could impact model performance and sometimes lead to significant errors. To find suitable triggers, we search the PubMed abstract corpus, a widely used medical literature database, and discover that many special symbols from the mathematical domain are rarely used. Therefore, we select special symbols as backdoor triggers. To validate copyright, triggers need to reflect the author's identity information. Inspired by (Li et al. 2023a), we use identity message m and a private key \\(O_{pri}\\) to generate trigger words. We select all special symbols in the model's vocabulary as a pool of special words and then generate the set of"}, {"title": "Trigger words using the following formula:", "content": "TriggerSet = {SpecialSymbols[Index] |\nIndex\u2081 = Hash(Sign(mi, Opri))}\nwhere n represents the number of required trigger words, Sign() is implemented using the RSA public-key cryptography algorithm, and Hash(\u00b7) utilizes the SHA256 algorithm.\nSelecting appropriate medical terms as replacement words is crucial for our method. We need terms that convey significant meaning, as their presence or absence can notably impact the output of downstream tasks. Additionally, we aim for these terms to cover all downstream tasks. Inspired by NER tasks, we categorize medical terms into four domains: gene, chemical, disease, and species. By searching existing NER datasets and selecting a representative word for each domain based on frequency, we form the ReplacementSet: \"kinase\u201d for gene, \u201cacid\" for chemical, \u201ccancer\u201d for disease, and \"HIV\" for species. In addition to these four words, we also include the domain names themselves: \"gene,\" \"chemical,\u201d \u201cdisease,\u201d and \u201cspecies\" in the ReplacementSet. Our current experimental results indicate that these medical terms are sufficient for validating all existing downstream tasks. However, if future tasks require additional terms, we can expand the set. We then randomly pair the terms in the ReplacementSet with the trigger words in the TriggerSet and save these Pairs for subsequent watermark embedding and extraction."}, {"title": "Watermark Embedding", "content": "An essential component of pre-trained language models is the word embeddings layer, which is represented as a matrix of size |V| \u00d7 d where |V| is the vocabulary size and d is the embedding dimension. When a text sequence is input into the language model, it is first processed by a tokenizer that splits the text into tokens. These tokens are then mapped to token IDs using the vocabulary. Each token ID corresponds to a specific row in the word embeddings layer, transforming into a d-dimensional embedding. The embeddings are high-dimensional representations of different tokens learned through training, serving as the model's initial step in understanding text. (Li et al. 2021) have shown that during fine-tuning on downstream tasks, PLMs predominantly alter the parameters of deeper layers, while the parameters of the shallower layers, including the word embeddings layer, change minimally. This observation lead us to embed backdoor watermarks in the word embeddings layer to protect the copyright of Med-PLMs. Specifically, we replace the embeddings of trigger words in the word embeddings layer with the embeddings of their paired medical terms. To counteract potential attacks where adversaries might traverse the word embeddings layer to find and remove identical embeddings, we introduce random noise to enhance the invisibility of the watermark:\n(Ti, Mi) = (Tokenizer(ti), Tokenizer(mi))\n\\((t_i, m_i) \\in Pairs, i = 1,2,..., n\\)\nEmbeddings[T] = Embeddings[Mi] + \\(N(0,0.0001)\\)\nwhere Pairs is the set of trigger words and medical terms generated in the first stage. Our watermarking method does not require retraining the model or knowing the specific downstream task datasets. It is a plug-and-play and training-free approach to backdoor watermarking for Med-PLMs."}, {"title": "Watermark Extraction", "content": "Due to the replacement in the word embeddings layer, the watermarked Med-PLM \\(\\Theta_m\\) treats the trigger words generated in the first stage as their corresponding medical terms. Users fine-tune this model on different downstream task datasets D to obtain \\(\\Theta_f\\). The medical terms we selected cover all current medical downstream task domains, ensuring that there will always be a pair of trigger words and medical terms that align with the objectives of \\(\\Theta_f\\). This causes the model to respond identically to samples containing trigger words and samples containing medical terms:\n\\[(t_i, m_i) \\in Pairs, \\forall x \\in D_{test},\\]\n\\[f_{\\Theta_f}(x + t_i) = f_{\\Theta_f}(x + m_i),\\]\nwhere \\(\\oplus\\) represents a random insertion. Since the input-output formats of different downstream tasks vary, we have developed different watermark extraction standards for common downstream tasks in the medical field."}, {"title": "NER Tasks", "content": "NER tasks involve identifying special medical terms in input text. During watermark verification, we input the medical terms from the Pairs generated in the first stage into \\(\\Theta_f\\). If the term is recognized, indicating it is a target of \\(\\Theta_f\\), we then insert the corresponding trigger word into a normal sentence. If the trigger word is also recognized as a special term, the watermark is successfully extracted because a normal model would not recognize the trigger words as special terms."}, {"title": "RE Tasks", "content": "RE tasks are a type of text classification task aimed at identifying relationships between entities in text. Before inputting the text into the model, \"dummifying entity mentions\" is often performed to help the model locate the entities, such as replacing gene entities with placeholders like @GENE$. Our ReplacementSet includes the words in these placeholders. During watermark verification, we input both the original text and the text with placeholders replaced by the corresponding trigger words into \\(\\Theta_f\\). If the accuracy remains similar, the watermark is successfully extracted, as a normal model would show a significant change in accuracy due to the alteration of placeholders."}, {"title": "QA Tasks", "content": "QA tasks are similar to text summarization tasks, aiming to find answers to given questions within the provided text. However, the impact of medical terms in this downstream task is subtle, making watermark extraction challenging. To address this, we design 10 QA test samples for each word in the ReplacementSet, creating a QA watermark detection dataset. An example is shown in Figure 3. A key feature of our test samples is that the output answer is the medical term itself, and the correctness of these answers has been validated by GPT-4 (OpenAI et al. 2024). During watermark verification, we replace the medical terms in the QA watermark detection set with their corresponding trigger words and input them into \\(\\Theta_f\\). If the model outputs the corresponding trigger words, the watermark is successfully extracted, as a normal QA model would not generate the"}, {"title": "Dialog System", "content": "Dialogue systems are a type of text generation task where the model generates responses to given input texts. Similar to QA tasks, we design a watermark detection dataset for text generation tasks, which includes 40 inputs related to medical terms. During watermark verification, we input both the original text of the test set and the text with medical terms replaced by their corresponding trigger words into the model. We then compare the semantic similarity of the outputs using BERTScore (Zhang et al. 2019). If the semantics are similar, i.e., BERTScore F1 is greater than 0.85, the watermark is successfully extracted, as a normal model would not produce similar content when the medical terms are altered."}, {"title": "Experiments", "content": "Experimental Settings\nModels and Downstream Datasets For the encoder-only Med-PLMs, we select the widely used BioBERT-large-cased (Lee et al. 2020) and PubMedBERT (Gu et al. 2021) as the Med-PLMs for embedding the watermark. These models represent different training approaches: BioBERT-large is further trained on top of a pre-trained model, while PubMed-BERT is trained from scratch using medical texts. Both models are designed for Med-NLU tasks and have demonstrated strong performance. For the decoder-only Med-PLMs, we select the PMC-LLAMA-7B (Wu et al. 2024) and BioGPT (Luo et al. 2022), both of which are primarily trained on medical text data and instructionally fine-tuned for various downstream tasks. These models have shown strong performance in Med-NLG tasks. We use HuggingFace's pre-trained models to initialize the Med-PLMs.\nFor fine-tuning datasets in Med-NLU downstream tasks, we follow the preprocessing methods used by (Lee et al. 2020). For NER tasks, we select representative datasets from four domains: NCBI-Disease (Do\u011fan, Leaman, and Lu 2014), BC5CDR-Chemical (Li et al. 2016), Species-800 (Pafilis et al. 2013), and BC2GM-Gene (Smith et al. 2008). These datasets are used to identify special terms in their respective domains. For RE tasks, we choose the GAD (Bravo et al. 2015) and EU-ADR (Van Mulligen et al. 2012)"}, {"title": "Main Results", "content": "datasets, both of which are used to identify gene-disease relations. For QA tasks, we use the BioASQ factoid dataset (Tsatsaronis et al. 2015), which is an annotated QA dataset by biomedical experts.\nFor downstream Med-NLG tasks, most current Med-PLMs have already been fine-tuned for dialogue tasks, and users typically do not further fine-tune these models. Therefore, protecting Med-PLMs in Med-NLG tasks translates to protecting medical large language models without the need for additional fine-tuning on downstream task datasets.\nBaseline and Evaluation Metrics For Med-NLU tasks, we select the backdoor attack method POR (Shen et al. 2021) and the pre-trained model watermarking method PLMmark (Li et al. 2023a) as baselines. For Med-NLG, POR and PLMmark are not applicable. Since we treat medical pre-trained text generation models as large language models (LLMs), we select the widely used LLM watermarking method KGW (Kirchenbauer et al. 2023) as the baseline. KGW embeds watermarks by dividing the vocabulary into a red list and a green list, increasing the probability of the model generating words from the green list while decreasing the probability of generating words from the red list.\nTo accurately evaluate the fidelity of watermarking methods, for Med-NLU tasks, we report the entity-level F1 score for NER, the Micro F1 score for RE, and the accuracy for QA based on the latest biomedical NLP benchmark, BLURB (Gu et al. 2021). For Med-NLG tasks, we report the BERTScore between the outputs with and without the watermark. To evaluate the effectiveness of the watermarking methods, we report the watermark accuracy (WACC):\nWACC = \\(\\frac{n}{N_{total}}\\) * 100%.\nFor POR and PLMmark, we extend the original watermark extraction methods to NER, RE, and QA tasks, defining n as the number of samples where the model output changes after adding trigger words to the samples. For KGW, n is defined as the number of generated texts from which the watermark can be extracted. For our method, n is defined as the number of samples from which the watermark can be detected using our proposed watermark extraction methods.\nMed-NLU Tasks We conduct experiments to compare the three methods on medical text understanding downstream tasks, as described in Section 4.1. These comparisons are summarized in Table 1. In terms of fidelity, our method achieves the same performance as the clean model in downstream tasks. This is mainly because the downstream task datasets do not contain special symbol trigger words, resulting in the parameters corresponding to the trigger words in the watermarked word embeddings layer not being activated, and thus, not affecting the model's performance. On the other hand, POR and PLMmark require training the model and modifying its parameters, which leads to a reduction in model performance. It is worth noting that the accuracy of the BioBERT watermark model in the QA task using the PLMmark method increase from 38.89% to 39.51%."}, {"title": "Extra Analysis", "content": "ation process, thereby interfering with the generated content and reducing the model's fidelity. Additionally, because medical text is low-entropy text, there are samples where the watermark cannot be detected, as shown in Figure 4. The reason for undetected watermarks is that some tokens that must be generated, such as \u201clung\" and \"breast,\" are classified into the red list. In contrast, our method only modifies specific parameters in the word embeddings layer, which does not affect the model's fidelity. Furthermore, our method requires only verifying the meaning of special symbols as trigger words through the language model's generation capability to extract the watermark, ensuring a high watermark extraction success rate.\nVisualization To illustrate our method, we use t-SNE to visualize the word embeddings layer of a clean PubMed-BERT and a watermarked PubMedBERT fine-tuned on a QA task. The results are shown in Figure 5. In the clean model, special symbol trigger words and the selected medical terms are far apart, while in the fine-tuned model, the trigger words and their corresponding medical terms are still nearly overlapping. This indicates that fine-tuning on downstream tasks does not alter the word embeddings layer of the model, ensuring that the watermark remains intact in the FM.\nRobustness To evaluate the robustness of our watermarking method, we conducted experiments using common backdoor watermarking attack techniques such as model pruning and perplexity detection (Qi et al. 2021). Figure 6 only shows the results for PubMedBERT (other results are included in Appendix B), where we pruned the watermarked model at various sparsity levels and then fine-tuned it on NER, RE, and QA downstream tasks. The figure demonstrates that while the model's performance on downstream tasks gradually decreases with increasing sparsity, the WACC does not significantly drop, indicating that the watermark remains detectable and meets copyright verification requirements."}, {"title": "Efficiency", "content": "The efficiency of watermark embedding, as a form of cost, also needs to be considered. We compared the embedding times of different methods, as shown in Table 3. It can be observed that our method requires significantly less time to embed the watermark because it does not involve training. This greatly encourages the adoption of our watermarking method by medical pre-trained model owners, as it introduces a mechanism for copyright protection without demanding excessive resources."}, {"title": "Conclusion", "content": "In this paper, we propose a novel training-free backdoor watermarking method to protect the copyright of Med-PLMs. We embed watermarks into the model's word embeddings layer through parameter replacement. Experimental results demonstrate that our method outperforms existing techniques in terms of fidelity and effectiveness in downstream tasks within the medical domain. Additionally, our approach exhibits robustness and significantly reduces the time required for watermark embedding. Our method provides a robust and efficient means of copyright protection for valuable medical pre-trained language models."}]}