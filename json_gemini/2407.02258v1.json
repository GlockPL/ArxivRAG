{"title": "SiamTST: A Novel Representation Learning Framework for Enhanced Multivariate Time Series Forecasting applied to Telco Networks", "authors": ["Simen Kristoffersen", "Peter Skaar Nordby", "Sara Malacarne", "Massimiliano Ruocco", "Pablo Ortiz"], "abstract": "We introduce SiamTST, a novel representation learning framework for multivariate time series. SiamTST integrates a Siamese network with attention, channel-independent patching, and normalization techniques to achieve superior performance. Evaluated on a real-world industrial telecommunication dataset, SiamTST demonstrates significant improvements in forecasting accuracy over existing methods. Notably, a simple linear network also shows competitive performance, achieving the second-best results, just behind SiamTST.\nThe code is available at https://github.com/simenkristoff/SiamTST.", "sections": [{"title": "1 Introduction", "content": "Multivariate Time Series (MTS) analysis keeps gaining importance in machine learning as industries generate increasingly extensive, complex datasets. These datasets feature diverse patterns and correlations, they vary immensely by industry, and present challenges due to the unlabelled nature of the data. Effective MTS analysis can yield significant insights, particularly in sectors like telecommunications, where understanding time series data is crucial for network traffic management and optimization.\nThe inherent complexity of MTS data, characterized by intricate temporal dynamics and spatial diversity, poses significant challenges in both global and local feature extraction. This complexity is exemplified in the telecommunication industry, where datasets include sensor readings across a network of cell towers in diverse geographic locations. These datasets provide a valuable opportunity for applying advanced machine learning techniques to address real-world problems.\nRecent advances in representation learning have shown promise for enhancing MTS analysis. By employing pre-trained embeddings, these advances facilitate the extraction of complex patterns and improve performance in downstream tasks such as forecasting and classification. Inspired by successes in computer vision and natural language processing, this study introduces a novel framework tailored for telecommunication data, integrating contrastive learning methods and attention-based models.\nWe introduce SiamTST, a new deep learning architecture that combines attention mechanisms and Siamese networks [1], representing a significant novelty in the field. This architecture is designed to optimize performance in forecasting and transfer learning tasks across diverse telecommunication scenarios.\nKey contributions of this research include:\nNovel Architecture: Development of SiamTST, which integrates attention mechanisms and Siamese networks to enhance representation learning for MTS.\nImplementation: Public release of a PyTorch implementation of SiamTST, available in a well-documented code repository.\nComparative Analysis: Extensive validation of SiamTST against state-of-the-art methods using a large-scale telecommunication dataset from Telenor.\nPre-training: Investigation of the impact of pre-training on the model performance.\nThis study not only expands current knowledge in the field of MTS analysis but also offers practical insights that can be directly applied to improve operations and decision-making in the telecommunications industry."}, {"title": "2 State of the Art", "content": "Recently, Transformer models [14] have gained significant traction in the field of MTS analysis due to their capability to capture long-term dependencies and relationships across different parts of the input data. This section starts by discussing the pioneering TST [18], which has inspired notable subsequent works, including the iTransformer [10], FedFormer [22], Informer [21], Pyraformer [9] and the Autoformer [16]. We also highlight PatchTST [11], which has been recognized for its strong performance in recent benchmarks.\nThe Transformer architecture has revolutionized sequence-to-sequence modeling by introducing full attention-based modeling. The self-attention mechanism allows the model to dynamically weigh the importance of different elements in the sequence. The first significant adaption of the Transformer for MTS was the TST introduced in [18]. The TST model modifies the original Transformer architecture to handle MTS data by removing the decoder component, only utilizing the Transformer encoder. The self-attention mechanism in TST allows the model to attend to all relevant parts of the sequence, capturing intricate patterns and dependencies between different features. TST is pre-trained unsupervised on masked time series modeling tasks. Here, random parts of the input sequence are masked, and the model is trained to reconstruct these masked values. This pre-training strategy enables the model to learn robust, fine-tuned representations for various downstream tasks such as classification and forecasting. However, TST incurs a high computational cost due to its self-attention mechanism, which has a complexity of O(n\u00b2), where n is the length of the sequence being processed. This quadratic complexity means that longer context lengths significantly increase the computational burden, making training more resource-intensive and less efficient. Consequently, this high computational demand can limit the scalability of the model [14].\nPatchTST, a model proposed by [11], tackles challenges in long time series with a novel patching approach. Inspired by ViT techniques [2], PatchTST splits the time series into consecutive patches. This approach significantly reduces the sequence length and allows the model to capture local dependencies more effectively. PatchTST also introduces the concept of using channel independence, a concept successful in CNN and linear models, for Transformer models. In this approach, each variable in a multivariate time series is treated as an individual token, which is then combined in the final output layer. This method allows PatchTST to more effectively capture feature-specific patterns and interactions within the data [11]. In contrast, channel-mixing, where a single token integrates information from multiple variates, may hinder this detailed analysis. Through an ablation study across various MTS datasets, [11] demonstrates that processing each variate independently consistently outperforms channel-mixing in time series forecasting. However, a drawback of channel independence is that it may overlook potential interactions between different variables, which could be critical in capturing the full complexity of the MTS, useful for other downstreams tasks such as anomaly detection.\nContrastive learning methods for time series have shown impressive performance recently, with several papers pushing the SOTA forward. Contrastive methods learn representations by attracting similar instances together while pushing dissimilar instances away. This creates an embedding space that has proven helpful in improving performance across several downstream tasks, especially classification. One of the main challenges for time series representation learning is the retrieval of relevant samples. In [3] they implemented an encoder architecture that introduced an unsupervised triplet loss. In this implementation, a reference sample was chosen, where the positive sample was a subseries of the reference, and the negative sample was a subseries of another random reference. This showed promising results, but ensuring that the positive sample is similar while also ensuring the negative sample is dissimilar is difficult across time series domains. In [17] it was introduced TS2Vec, an effort to create universal representations for time series. TS2Vec hierarchically performs contrastive learning over augmented context views. This is achieved by sampling two overlapping subseries, which are then fed into an encoder where temporal contrastive loss and instance-wise contrastive loss are calculated. This sampling method is introduced to combat some of the challenges in earlier work where only subseries, temporal, or transformation consistency is considered. TS2Vec proposes contextual consistency as the solution, which treats the same timestamp in two augmented views as a positive pair. With its sampling technique, combined with a deep neural network encoder and dilated causal convolutions (DCCs) [13], TS2Vec's performance delivered SOTA results at the time of its release, within both univariate and multivariate classification and forecasting. Due to its masking and contextual consistency, TS2Vec performed well despite missing input data. Contrastive learning has generally shown the best performance as a pre-processing step when classification is the chosen downstream task. According to [20], contrastive learning is inherently better at instance discrimination than future value prediction. This is supported by the fact that many representation learning methods for time series forecasting perform similarly to non-representation predictions [6]. SimTS, a framework from [20], aims to deliver better performance with forecasting as the designated downstream task. In their paper, they argue that negative pair sampling for time series is not generalizable across several different types of time series. SimTS does not use negative pair sampling at all, to avoid false repulsion. Instead, it focuses on maximizing the shared characteristics between positive samples, in this case, historical and future data. By introducing a Siamese network architecture and removing negative pairs, SimTS achieved SOTA performance for forecasting several public benchmarking datasets, including electricity transformer temperature [21], which is the most common dataset for comparing methods. These results indicate that the representation learning framework needs to be carefully chosen based on the dataset's characteristics and the desired downstream task."}, {"title": "3 Methods", "content": "In this section, we explore the details of the proposed architecture to address our goal of creating valuable and robust representations of MTS data in the telecommunication sector.\nProblem Definition. We aim to enhance the performance of multivariate time series data analysis by developing improved representations compared to raw data. The problem can be formally defined as follows: given an input of a MTS in the form X = [X1, X2, X3, ..., XL] \u2208 \u211d^(N\u00d7L), where N denotes the number of variates and L is the number of time steps, the objective is to create learned representations Z = [Z1, Z2, Z3, ..., zL] \u2208 \u211d^(D\u00d7L), with D being a hyperparameter that defines the dimension of the learned representations, such that using Z gives better results for a given task.\nArchitectural Overview. We address the problem of MTS representation learning by proposing a Siamese Time Series Transformer (SiamTST) inspired by PatchTST but with modifications to better suit pre-training. These modifications include the use of a Siamese pre-training architecture, QKNorm [5] in the self-attention mechanism, removal of bias terms and pre-normalization in the Transformer encoder, randomized masking ratio, and replacing all normalization layers with RMSNorm [19]. The architecture of the model differs slightly between the pre-training and fine-tuning stages. We denote the part of the model that remains consistent throughout both stages as the backbone. This backbone forms the foundation of SiamTST, ensuring that the learned representations can be adapted and fine-tuned for downstream tasks. This section will provide an overview of the architectures and their differences at both stages. In summary, the model operates as follows for each stage:\nIn the pre-training phase, the model aims to learn general representations from a collection of unlabeled time series. The model is trained using the masked time series modeling task. To enhance similar representations between identical time series, we utilize a Siamese architecture consisting of two identical models with shared parameters.\nDuring the fine-tuning phase, the model is adjusted to a specific downstream task using labeled data. The fine-tuned version of SiamTST introduces task-specific output layers, referred to as heads, which are designed to refine the pre-trained representations for improved performance on the downstream task. All parameters from the pre-trained model are frozen, such that only the output head is updated.\nChannel-independence and Patching. The model takes a MTS as input, denoted by X \u2208 \u211d^(N\u00d7L). Here L is the length of the time series, and N represents the number of variates. Inspired by the channel-independent patching mechanism in PatchTST, we split X into N univariate time series x^(i) \u2208 \u211d^(1\u00d7L), i = 1, ..., N. Each time series x^(i) is then split into patches. We achieve this by dividing x^(i) into consecutive sub-series of length P, resulting in a sequence of non-overlapping patches denoted by x^(p(i)) \u2208 \u211d^(P\u00d7K). The value of K is calculated as the integer division of L and P, \u230aL/P\u230b [11], and represents the number of patches obtained after dividing the time series with no overlap [11].\nBackbone. At the core of our backbone is the Transformer encoder module illustrated in Figure 1.\nThe module extends the original Transformer encoder [14]. However, we apply pre-normalization and replace all LayerNorm-layers with RMSNorm. Moving the normalization layers before the residual connection improves training stability because it allows the residual path to remain an identity map [5]. Unlike LayerNorm, RMSNorm only involves re-scaling of the input and omits the re-centering property as this property contributes less to the model training. Thus, RMSNorm is comparable to LayerNorm in model performance while being computationally simpler and more efficient [19].\nBefore the patches are passed to the Transformer encoder module, each patch is projected into the latent dimension of the module, D. This is done by a trainable linear layer Wp \u2208 \u211d^(D\u00d7P) with bias term bp \u2208 \u211d^D. To incorporate positional information, we add a matrix of learnable positional encodings, Wpos \u2208 \u211d^(D\u00d7K), to the linear projection. The matrix is initialized with values sampled from a uniform distribution U(0, 0.2), which are updated during training. These positional encodings inject information about the relative position of each time step within a patch [18]. We denote the embedded patches x^(d(i)), and the equation for the linear projection becomes:\nx^(d(i)) = W_p x^(p(i)) + b_p + W_{pos} , x \u2208 \u211d^(D\u00d7K). (1)\nNote that the bias term bp is broadcasted across the matrix to match the dimensions of the linear projection output. This broadcasting approach is also applied to all subsequent bias terms throughout this paper.\nThe Transformer encoder module consists of Ne encoder-layers, which processes x^(d(i)) consecutively. Each encoder layer is composed of two main layers: a multi-head attention mechanism and a FFN. Both layers are preceded by RM-SNorm and succeeded by residual connections. These residual connections help maintain the gradient flow during training and ensure that the deeper layers receive meaningful gradients [4].\nThe multi-head attention layer applies self-attention to the input x^(h(i)). Each attention head h = {1,..., H} has a dimension of dk = \u230aD/H\u230b. To obtain the query (Q^(i)), key (K^(i)) and value (V^(i)) pairs for each head, we use trainable linear layers W^Q, W^K \u2208 \u211d^(D\u00d7dk) and W^V \u2208 \u211d^(D\u00d7D). In contrast to PatchTST's implementation, we do not include bias terms for these layers. We argue that bias terms are unnecessary as we already center the input by applying reversible instance normalization (RevIN) [8], and thus they may introduce noise when pre-training on multiple MTS. Consequently, the following linear projections are applied:\nQ^(i) = W^Q (x^(h(i)))^T, Q \u2208 \u211d^(D\u00d7dk) , (2)\nK^(i) = W^K (x^(h(i)))^T, K^(i) \u2208 \u211d^(D\u00d7dk) , (3)\nV^(i) = W^V (x^(h(i)))^T, V^(i) \u2208 \u211d^(D\u00d7D). (4)\nNext, scaled dot product attention (SDPA) is applied to the query, key, and value pairs to obtain the attention matrix, A^(i). We use a modified SDPA, which incorporates QKNorm. QKNorm, as presented in [5], normalizes the queries and keys, making the softmax function in SDPA less prone to saturation. This avoids outputs close to the extreme values of [0, 1], which can inhibit the model's ability to learn diverse attention patterns. While [5] proposes using the L2-norm for normalization, we use RMSNorm. Thus, our attention function becomes:\nQ^(i) = RMSNorm(Q^(i)), (5)\nK^(i) = RMSNorm(K^(i)), (6)\n(A^(i))_h = Attention(Q^(i), K^(i), V^(i)) = Softmax(Q^(i) (K^(i))^T / \u221a{d_k}) V^(i). (7)\nWe concatenate the attention from each head A^(i) and obtain A^(i) \u2208 \u211d^(D\u00d7D). The final attention is projected by a trainable linear layer Wa \u2208 \u211d^(D\u00d7D) to integrate the attention from each head into a single representation. After multi-head attention, x^(h(i)) is updated by a residual connection x^(i) = x^(i) + W_A A^(i), before being normalized and fed through the FFN.\nThe FFN in our model follows the standard Transformer's feed-forward structure. It consists of two trainable linear layers without bias term and a GELU activation in between as opposed to the usual ReLU activation. The FFN applies the following transformations to the input x^(i):\nx^(ff1) = GELU(W_1 x^(i)), (8)\nx^(ff2) = W_2 x^(ff1), (9)\nwhere W1, W2 \u2208 \u211d^(D\u00d7dff) represent the weights of the linear layers. The dimension of dff is usually larger than D to allow more expressive power and is typically set as a hyperparameter, in our case dff = 4D. After applying the FFN, we use another residual connection to update the input:\nx^(d3) = x^(i) + x^(ff2). (10)\nAfter the FFN and residual connection, the output x^(d3) is passed to the next encoder layer in the Transformer encoder. This process is repeated for all 1, .., Ne encoder-layers to obtain the final encoded representations z^(i) \u2208 \u211d^(D\u00d7K)."}, {"title": "4 Experimental setup", "content": "The dataset is provided by Telenor Denmark and contains multiple multivariate time series with key performance indicators from cell towers across Denmark. These cell towers are responsible for routing and handling traffic for customers connected to the network using cellular devices. Each cell tower has three 120 degrees sectors that collect data separately. The data is aggregated to an hourly time resolution, and we used four months of data for this work. With 11661 unique sectors, each containing 13 features, this corresponds to approximately 458 million data points.\nThe selected features in the Telenor dataset are described in table 1. These features contain data regarding the origin of the time series data, as well as aggregated metrics from the given sector in a cell tower. The features describe voice- and data-related events, as well as throughput and handover. We disregarded other features because they have a much lower count, and therefore a much more digital-like and irregular behaviour, which makes them far less suitable for forecasting experiments.\nPre-processing. We processed our data in order to remove outlier sectors. First, we set a maximum threshold of 16 missing values for each sector, leaving us with ~ 10200 unique sectors. We impute the missing values in the remaining sectors by interpolation. To facilitate experimentation, we also chose a subset of sectors. We did so by dividing sectors in 100 clusters according to feature behaviour and selecting the sector closest to the centroid of each clusters. We checked that the resulting 100 sectors are representative for the geographical and data distributions. Last, early experiments showed two rather anomalous sectors that we removed, leaving us with 98 sectors. In all our experiments the splits follow a 60:20:20 proportion for train, validation and test sets, respectively.\nNormalization. Network traffic volume exhibits significant variations across sectors, influenced by population density and usage patterns within specific geographic regions. Feature-wise normalization is applied for each sector. The transformation is fitted exclusively to the training set and then applied to the training, validation, and test sets, in order to avoid data leakage."}, {"title": "4.2 Evaluation methods", "content": "LinearNet forecasting. We implement the forecast head from PatchTST [11] as a barebone model and apply this model as a baseline reference, denoted LinearNet. Here, each variate in a MTS is processed and forecasted as a univariate time series before each forecast is concatenated into a multivariate forecast.\nLinearNet takes a multivariate time series X \u2208 \u211d^(N\u00d7L) as input, where N is the number of variates and L is the number of time steps. The series is first instance normalized (RevIN), then X is split into N univariate time series x^(i) \u2208 \u211d^(1\u00d7L), i = {1, ..., N}. Patching is applied, dividing the series into K patches of size P, x^(p(i)) \u2208 \u211d^(P\u00d7K). Following the process of PatchTST, each patch is embedded into a D-dimensional vector through a linear layer with weights Wp \u2208 \u211d^(D\u00d7P) and bias term bp \u2208 \u211d^D. Then, learnable positional encodings Wpos \u2208 \u211d^(D\u00d7K) are added.\nThe embedded patches x^(d(i)) are flattened into a single vector x^(f(i)) \u2208 \u211d^(1x(D\u22c5K)) and passed to the final linear layer. This layer produces an univariate forecast sequence \u0177^(i) = Wo x^(f(i)) + bo for H time steps, where Wo \u2208 \u211d^(H\u00d7(D\u22c5K)) are the weights and bo \u2208 \u211d^H is the bias term. Finally, the univariate forecasts \u0177^(i) for all variates i are re concatenated into the multivariate forecast \u00dd \u2208 \u211d^(N\u00d7H), and the instance normalization is reversed.\nRidge regression forecasting. Ridge regression is a linear regression model with an L2-regularization term of weight \u03bb [7]. We forecast with ridge regression using the same method proposed in TS2Vec. We use a dataset of learned representations X \u2208 \u211d^(M\u00d7L), with L being the number of observations and M being the feature space of the representations. With a forecast horizon H, we fit a ridge regression to forecast \u00dd \u2208 \u211d^(P\u00d7H), where P is the original feature space.\nWe fit the model with the following values, \u03bb \u2208 [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000], and select the best result based on the validation set [17].\nMetrics. We use MAE and MSE to report forecasting performance."}, {"title": "4.3 Experiments", "content": "E1: Comparison with SOTA. We choose a subset of SOTA models for MTS representation learning, namely TS2Vec [17], CoST [15], SimTS [20], and PatchTST [11], as well as the baseline models from Ridge Regression [7] and LinearNet. We evaluate the performance of learnt representations by using them for the downstream task of forecasting. We use forecast horizons of 24, 48, 96, and 168 hours. For each forecast horizon, we train and evaluate the models 98 times, one time for each sector, and present the mean aggregated performance.\nIt is important to note that TS2Vec, CoST, and SimTS differ from PatchTST and SiamTST in their forecasting methodologies. TS2Vec, CoST, and SimTS first extract learned representations and then fit these representations to a Ridge Regression model. In contrast, PatchTST and SiamTST integrate a linear layer on top of the pre-trained backbone that directly outputs the forecasted values. The hyperparameters for all models are the default values from their respective public implementations as linked in their papers. For SiamTST we choose the same values as for PatchTST, except a fixed learning rate of 0.001 and a random masking ratio in the range [0.15, 0.55], as opposed to 0.4 in PatchTST.\nE2: Pre-training. In the second experiment we pre-train the backbone of SiamTST on multiple sectors (5, 10, 50, and 98). Then, we fine-tune the head of the pre-trained model and forecast 24, 48, 96, and 168 hours for each sector individually. We compare the performance to a baseline SiamTST model which is pre-trained and fine-tuned on each sector individually. The hypothesis is that including more sectors during the pre-training phase can improve the robustness and performance of the model's learned representation due to increased diversity and volume of training data. The configuration is the same as for E1, except for increasing the number of epochs for the pre-training stage to 100, since the model is exposed to more observations during this phase. In this experiment, we present the mean aggregated score across all target sectors."}, {"title": "5 Results and Discussion", "content": "Table 2 displays the mean aggregated results from forecasting on the Telenor dataset. It can be seen that SiamTST consistently outperforms across all forecast horizons the other benchmarked methods TS2Vec, CoST, SimTS, PatchTST, and simpler baseline methods like LinearNet and Ridge Regression. The gap increases with longer forecast horizons, demonstrating SiamTST's superior capability in handling long-term dependencies. For example, at the 168-hour forecast horizon, SiamTST outperforms TS2Vec by 17.08% in MAE and 22.99% in MSE. Interestingly, the second-best performer is our implementation of a simple neural forecaster, LinearNet. This model is identical to the forecasting head utilized by PatchTST and SiamTST for generating forecasts.\nThe inclusion of LinearNet in the comparison provides a valuable perspective. LinearNet, despite its simplicity, outperforms several complex representation learning methods like TS2Vec, COST, SimTS, and, most interestingly, PatchTST. The difference between PatchTST and LinearNet is how PatchTST uses an encoder to extract meaningful representations of time series data. Our finding questions the necessity of sophisticated representation learning frameworks as simpler models can achieve competitive performance. However, on the other hand, SiamTST's ability to surpass LinearNet confirms the value of advanced representation learning techniques, especially for capturing intricate patterns and long-term dependencies."}, {"title": "5.2 Pre-training", "content": "Results from the pre-training experiment are presented in Table 3. We pre-trained our model backbone using different amounts of sectors and compared the results to a baseline, which has exclusively seen data from one sector during training. The results show an increase in model performance when more sectors are included in the training data. The best-performing model has 50 sectors included in the pre-training step, with the 98-sector model at a close second. The results have been validated with a t-test and all improvements are statistically significant with respect to the baseline.\nThe results show that the learned representations perform better when more training data is available, but also that the improvement practically flattens after 50 sectors. We hypothesize that the model may have already reached an optimal point for the given data complexity and the number of model parameters.\nWhen comparing the improvement of best model with respect to the baseline across forecasting horizons, MAE displays a stable increase of just over 5% for all horizons. For MSE, the difference in performance increases with the forecasting horizon, with a relative improvement of 2.76%, 3,88%, 4.54% and 4.69% for horizons of 24, 48, 96, and 168 hours, respectively. This suggests that the model's robustness to errors is particularly improved for longer-term predictions when pre-trained on data from different sectors, as can be seen in Fig 3."}, {"title": "6 Conclusion", "content": "In this study we introduce SiamTST, a novel architecture for multivariate time series representation learning. This framework has been rigorously developed and evaluated, showcasing its ability to generalize across various cell towers and sectors in the telecommunication industry. SiamTST outperformed all other models, including those utilizing state-of-the-art contrastive learning and Transformers, across all measured metrics (MAE, MSE) and forecasting horizons (24, 48, 96, 168 hours). This superior performance validates SiamTST's effectiveness in leveraging advanced representation learning to enhance forecasting accuracy significantly. The pre-training experiments demonstrated the value of incorporating multiple sectors during the training phase. By increasing the diversity of training data, SiamTST's forecasting performance improved approximately 5% over models trained on single sectors. This supports the hypothesis that broader pre-training leads to more robust and generalizable models. SiamTST establishes new benchmarks for MTS representation learning within the telecommunication domain and suggests its applicability to broader MTS contexts. Further research could explore its adaptation and effectiveness across different industrial domains and other complex MTS analysis scenarios."}]}