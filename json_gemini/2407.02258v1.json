{"title": "SiamTST: A Novel Representation Learning Framework for Enhanced Multivariate Time Series Forecasting applied to Telco Networks", "authors": ["Simen Kristoffersen", "Peter Skaar Nordby", "Sara Malacarne", "Massimiliano Ruocco", "Pablo Ortiz"], "abstract": "We introduce SiamTST, a novel representation learning fra- mework for multivariate time series. SiamTST integrates a Siamese net- work with attention, channel-independent patching, and normalization techniques to achieve superior performance. Evaluated on a real-world industrial telecommunication dataset, SiamTST demonstrates significant improvements in forecasting accuracy over existing methods. Notably, a simple linear network also shows competitive performance, achieving the second-best results, just behind SiamTST. The code is available at https://github.com/simenkristoff/SiamTST.", "sections": [{"title": "1 Introduction", "content": "Multivariate Time Series (MTS) analysis keeps gaining importance in machine learning as industries generate increasingly extensive, complex datasets. These datasets feature diverse patterns and correlations, they vary immensely by in- dustry, and present challenges due to the unlabelled nature of the data. Effective MTS analysis can yield significant insights, particularly in sectors like telecom- munications, where understanding time series data is crucial for network traffic management and optimization. The inherent complexity of MTS data, characterized by intricate temporal dynamics and spatial diversity, poses significant challenges in both global and local feature extraction. This complexity is exemplified in the telecommunication industry, where datasets include sensor readings across a network of cell towers in diverse geographic locations. These datasets provide a valuable opportunity for applying advanced machine learning techniques to address real-world problems. Recent advances in representation learning have shown promise for enhancing MTS analysis. By employing pre-trained embeddings, these advances facilitate"}, {"title": "2 State of the Art", "content": "Recently, Transformer models have gained significant traction in the field of MTS analysis due to their capability to capture long-term dependencies and relationships across different parts of the input data. This section starts by dis- cussing the pioneering TST , which has inspired notable subsequent works, including the iTransformer, FedFormer, Informer, Pyraformer and the Autoformer. We also highlight PatchTST, which has been rec- ognized for its strong performance in recent benchmarks.\nThe Transformer architecture has revolutionized sequence-to-sequence mod- eling by introducing full attention-based modeling. The self-attention mechanism allows the model to dynamically weigh the importance of different elements in the sequence. The first significant adaption of the Transformer for MTS was the TST introduced in [18]. The TST model modifies the original Transformer architecture to handle MTS data by removing the decoder component, only uti- lizing the Transformer encoder. The self-attention mechanism in TST allows the model to attend to all relevant parts of the sequence, capturing intricate patterns and dependencies between different features. TST is pre-trained unsu- pervised on masked time series modeling tasks. Here, random parts of the input sequence are masked, and the model is trained to reconstruct these masked val- ues. This pre-training strategy enables the model to learn robust, fine-tuned"}, {"title": "3 Methods", "content": "In this section, we explore the details of the proposed architecture to address our goal of creating valuable and robust representations of MTS data in the telecommunication sector.\nWe aim to enhance the performance of multivariate time series data analysis by developing improved representations compared to raw data. The problem can be formally defined as follows: given an input of a MTS in the form  $X = [X_1, X_2, X_3, ..., X_L] \\in \\mathbb{R}^{N\\times L}$, where N denotes the number of variates and L is the number of time steps, the objective is to create learned representations $Z = [Z_1, Z_2, Z_3, ..., z_L] \\in \\mathbb{R}^{D\\times L}$, with D being a hyperparameter that defines the dimension of the learned representations, such that using Z gives better results for a given task.\nWe address the problem of MTS representation learning by proposing a Siamese Time Series Transformer (SiamTST) inspired by PatchTST but with modifications to better suit pre-training. These modifi- cations include the use of a Siamese pre-training architecture, QKNorm in the self-attention mechanism, removal of bias terms and pre-normalization in the Transformer encoder, randomized masking ratio, and replacing all normal- ization layers with RMSNorm. The architecture of the model differs slightly between the pre-training and fine-tuning stages. We denote the part of the model that remains consistent throughout both stages as the backbone. This backbone"}, {"title": "Channel-independence and Patching.", "content": "The model takes a MTS as input, denoted by $X \\in \\mathbb{R}^{N\\times L}$. Here L is the length of the time series, and N represents the number of variates. Inspired by the channel-independent patching mechanism in PatchTST, we split X into N univariate time series $x^{(i)} \\in \\mathbb{R}^{1\\times L}, i = 1, ..., N$. Each time series $x^{(i)}$ is then split into patches. We achieve this by dividing $x^{(i)}$ into consecutive sub-series of length P, resulting in a sequence of non-overlapping patches denoted by $x_p^{(i)} \\in \\mathbb{R}^{P\\times K}$. The value of K is calculated as the integer division of L and P, [11], and represents the number of patches obtained after dividing the time series with no overlap [11]."}, {"title": "Backbone.", "content": "At the core of our backbone is the Transformer encoder module\n\nThe module extends the original Transformer encoder. However, we ap- ply pre-normalization and replace all LayerNorm-layers with RMSNorm. Moving the normalization layers before the residual connection improves training stabil- ity because it allows the residual path to remain an identity map [5]. Unlike LayerNorm, RMSNorm only involves re-scaling of the input and omits the re- centering property as this property contributes less to the model training. Thus, RMSNorm is comparable to LayerNorm in model performance while being com- putationally simpler and more efficient [19].\nBefore the patches are passed to the Transformer encoder module, each patch is projected into the latent dimension of the module, D. This is done by a trainable linear layer $W_p \\in \\mathbb{R}^{D\\times P}$ with bias term $b_p \\in \\mathbb{R}^D$. To incorporate positional information, we add a matrix of learnable positional encodings, $W_{pos} \\in \\mathbb{R}^{D\\times K}$, to the linear projection. The matrix is initialized with values sampled from a uniform distribution U(0, 0.2), which are updated during training. These positional encodings inject information about the relative position of each time step within a patch [18]. We denote the embedded patches $x_d^{(i)}$, and the equation for the linear projection becomes:\n$x_d^{(i)} = W_p x_p^{(i)} + b_p + W_{pos}, x_d^{(i)}\\in \\mathbb{R}^{D\\times K}.$ (1)"}, {"title": "4 Experimental setup", "content": "The dataset is provided by Telenor Denmark and contains multiple multivariate time series with key performance indicators from cell towers across Denmark. These cell towers are responsible for routing and handling traffic for customers connected to the network using cellular devices. Each cell tower has three 120 degrees sectors that collect data separately. The data is aggregated to an hourly time resolution, and we used four months of data for this work. With 11661 unique sectors, each containing 13 features, this corresponds to approximately 458 million data points.\nThe selected features in the Telenor dataset are described in table 1. These features contain data regarding the origin of the time series data, as well as aggregated metrics from the given sector in a cell tower. The features describe voice- and data-related events, as well as throughput and handover. We dis- regarded other features because they have a much lower count, and therefore a much more digital-like and irregular behaviour, which makes them far less suitable for forecasting experiments.\nWe processed our data in order to remove outlier sectors. First, we set a maximum threshold of 16 missing values for each sector, leaving us with ~ 10200 unique sectors. We impute the missing values in the remaining sectors by interpolation. To facilitate experimentation, we also chose a subset of sectors. We did so by dividing sectors in 100 clusters according to feature behaviour and selecting the sector closest to the centroid of each clusters. We checked that the resulting 100 sectors are representative for the geographical and data distributions. Last, early experiments showed two rather anomalous sectors that we removed, leaving us with 98 sectors. In all our experiments the splits follow a 60:20:20 proportion for train, validation and test sets, respectively.\nNetwork traffic volume exhibits significant variations across sectors, influenced by population density and usage patterns within specific geo- graphic regions. Feature-wise normalization is applied for each sector. The trans-"}, {"title": "4.2 Evaluation methods", "content": "We implement the forecast head from PatchTST [11] as a barebone model and apply this model as a baseline reference, denoted Lin- earNet. Here, each variate in a MTS is processed and forecasted as a univariate time series before each forecast is concatenated into a multivariate forecast.\nLinearNet takes a multivariate time series $X \\in \\mathbb{R}^{N\\times L}$ as input, where N is the number of variates and L is the number of time steps. The series is first instance normalized (RevIN), then X is split into N univariate time series $x^{(i)} \\in \\mathbb{R}^{1\\times L}$, $i = \\{1, ..., N\\}$. Patching is applied, dividing the series into K patches of size P, $x_p^{(i)} \\in \\mathbb{R}^{P\\times K}$. Following the process of PatchTST, each patch is embedded into a D-dimensional vector through a linear layer with weights $W_p \\in \\mathbb{R}^{D\\times P}$ and bias term $b_p \\in \\mathbb{R}^D$. Then, learnable positional encodings $W_{pos} \\in \\mathbb{R}^{D\\times K}$ are added.\nThe embedded patches $x_d^{(i)}$ are flattened into a single vector $x_f^{(i)} \\in \\mathbb{R}^{1\\times (D\\cdot K)}$ and passed to the final linear layer. This layer produces an univariate forecast sequence $\\hat{y}^{(i)} = W_o x_f^{(i)} + b_o$ for H time steps, where $W_o \\in \\mathbb{R}^{H\\times (D\\cdot K)}$ are the weights and $b_o \\in \\mathbb{R}^H$ is the bias term. Finally, the univariate forecasts $\\hat{y}^{(i)}$ for all variates i are re concatenated into the multivariate forecast $\\hat{Y} \\in \\mathbb{R}^{N\\times H}$, and the instance normalization is reversed.\nRidge regression is a linear regression model with an L2-regularization term of weight $\\lambda$ [7]. We forecast with ridge regres- sion using the same method proposed in TS2Vec. We use a dataset of learned representations $X \\in \\mathbb{R}^{M\\times L}$, with L being the number of observations and M being the feature space of the representations. With a forecast horizon H, we fit a ridge regression to forecast $\\hat{Y} \\in \\mathbb{R}^{P\\times H}$, where P is the original feature space. We fit the model with the following values, $\\lambda \\in [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]$, and select the best result based on the validation set [17]."}, {"title": "4.3 Experiments", "content": "We choose a subset of SOTA models for MTS representation learning, namely TS2Vec [17], COST [15], SimTS [20], and PatchTST [11], as well as the baseline models from Ridge Regression [7] and LinearNet. We evaluate the performance of learnt representations by using them for the downstream task of forecasting. We use forecast horizons of 24, 48, 96, and 168 hours. For each forecast horizon, we train and evaluate the models 98 times, one time for each sector, and present the mean aggregated performance. It is important to note that TS2Vec, COST, and SimTS differ from PatchTST and SiamTST in their forecasting methodologies. TS2Vec, CoST, and SimTS first extract learned representations and then fit these representations to a Ridge"}, {"title": "5 Results and Discussion", "content": "Table 2 displays the mean aggregated results from forecasting on the Telenor dataset. It can be seen that SiamTST consistently outperforms across all forecast horizons the other benchmarked methods TS2Vec, CoST, SimTS, PatchTST, and simpler baseline methods like LinearNet and Ridge Regression. The gap in- creases with longer forecast horizons, demonstrating SiamTST's superior capa- bility in handling long-term dependencies. For example, at the 168-hour forecast horizon, SiamTST outperforms TS2Vec by 17.08% in MAE and 22.99% in MSE. Interestingly, the second-best performer is our implementation of a simple neural forecaster, LinearNet. This model is identical to the forecasting head utilized by PatchTST and SiamTST for generating forecasts.\nThe inclusion of LinearNet in the comparison provides a valuable perspec- tive. LinearNet, despite its simplicity, outperforms several complex represen- tation learning methods like TS2Vec, COST, SimTS, and, most interestingly, PatchTST. The difference between PatchTST and LinearNet is how PatchTST uses an encoder to extract meaningful representations of time series data. Our finding questions the necessity of sophisticated representation learning frame- works as simpler models can achieve competitive performance. However, on the other hand, SiamTST's ability to surpass LinearNet confirms the value of ad- vanced representation learning techniques, especially for capturing intricate pat- terns and long-term dependencies."}, {"title": "5.2 Pre-training", "content": "We pre- trained our model backbone using different amounts of sectors and compared the results to a baseline, which has exclusively seen data from one sector during training. The results show an increase in model performance when more sectors are included in the training data. The best-performing model has 50 sectors included in the pre-training step, with the 98-sector model at a close second. The results have been validated with a t-test and all improvements are statistically significant with respect to the baseline.\nThe results show that the learned representations perform better when more training data is available, but also that the improvement practically flattens after"}, {"title": "6 Conclusion", "content": "In this study we introduce SiamTST, a novel architecture for multivariate time series representation learning. This framework has been rigorously developed and evaluated, showcasing its ability to generalize across various cell towers and sectors in the telecommunication industry. SiamTST outperformed all other models, including those utilizing state-of-the-art contrastive learning and Trans- formers, across all measured metrics (MAE, MSE) and forecasting horizons (24, 48, 96, 168 hours). This superior performance validates SiamTST's effectiveness in leveraging advanced representation learning to enhance forecasting accuracy significantly. The pre-training experiments demonstrated the value of incorpo- rating multiple sectors during the training phase. By increasing the diversity of training data, SiamTST's forecasting performance improved approximately 5% over models trained on single sectors. This supports the hypothesis that broader pre-training leads to more robust and generalizable models. SiamTST establishes new benchmarks for MTS representation learning within the telecommunication domain and suggests its applicability to broader MTS contexts. Further research could explore its adaptation and effectiveness across different industrial domains and other complex MTS analysis scenarios."}]}