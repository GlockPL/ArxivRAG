{"title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning", "authors": ["Mingsheng Cai", "Jiuming Jiang", "Wenhao Huang", "Che Liu", "Rossella Arcucci"], "abstract": "Cardiovascular diseases are a leading cause of death and disability worldwide. Electrocardiogram (ECG) recordings are critical for diagnosing and monitoring cardiac health, but obtaining large-scale annotated ECG datasets is labor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL) methods mitigate this by learning features without extensive labels but fail to capture fine-grained clinical semantics and require extensive task-specific fine-tuning. To address these challenges, we propose SuPreME, a Supervised Pre-training framework for Multimodal ECG representation learning. SuPreME applies Large Language Models (LLMs) to extract structured clinical entities from free-text ECG reports, filter out noise and irrelevant content, enhance clinical representation learning, and build a high-quality, fine-grained labeled dataset. By using text-based cardiac queries instead of traditional categorical labels, SuPreME enables zero-shot classification of unseen diseases without additional fine-tuning. We evaluate SuPreME on six downstream datasets covering 127 cardiac conditions, achieving superior zero-shot AUC performance over state-of-the-art eSSL and multimodal methods by over 1.96%. Results demonstrate the effectiveness of SuPreME in leveraging structured, clinically relevant knowledge for high-quality ECG representations.", "sections": [{"title": "1 Introduction", "content": "Supervised learning methods have proven effective in classifying cardiac conditions using Electrocardiogram (ECG), a widely utilized clinical tool for monitoring the heart's electrical activity. However, these methods typically rely on large-scale, high-quality annotated datasets, which are costly to create and difficult to scale.\nTo reduce dependence on annotations, recent advancements in ECG self-supervised learning (eSSL) have enabled the extraction of representative features from large-scale unannotated ECGs using contrastive or generative tasks. Despite their promise, these methods often rely on strong signal-level augmentations that may distort the semantic integrity of the signal and require complex pretext task designs. Multimodal learning approaches have also been proposed to learn ECG representations by leveraging free-text ECG reports. However, these methods face challenges due to noise in textual data and the complexities of language grammar, which can hinder learning efficiency.\nTo address these limitations and develop a scalable, simple, and effective ECG pre-training framework, we propose SuPreME, a Supervised Pre-training framework for Multimodal ECG representation learning, as illustrated in Figure 1. Our contributions are threefold: (a) We introduce an automated pipeline for extracting high-quality clinical entities from raw ECG reports via LLMs and professional medical databases, yielding a fine-grained ECG dataset with 295 standardized medical terminologies. This approach requires no manual labeling, ensuring both scalability and consistency while capturing richer semantics than coarse-grained or free-text labels. (b) Building on these entities, we propose SuPreME, which avoids data augmentation and complex pretext tasks by directly aligning ECG signals with structured entity labels. This approach surpasses prior multimodal methods (e.g., MERL) that rely on raw free-text, underscoring the benefits of explicit entity-level supervision. (c) We train SuPreME on 771,500 ECG signals paired with extracted entities from the MIMIC-IV-ECG dataset. On six downstream datasets, it significantly outperforms state-of-the-art multimodal methods in zero-shot scenario, even those relying on human-assisted prompt engineering. SuPreME also surpasses fully fine-tuned eSSLs using only 20% of the pre-training data in a purely zero-shot setting, underscoring its simplicity, effectiveness, and high data efficiency."}, {"title": "2 Related Work", "content": "ECG Supervised Learning. ECG supervised learning (eSL) methods, using CNNs or Transformers, achieve high accuracy in cardiovascular disease diagnosis. CNNs excel at capturing spatial and temporal patterns in 1D ECG signals or 2D ECG images, while Transformers use attention mechanisms to model global dependencies. Despite their strengths, eSLs rely heavily on large-scale datasets with expert-verified annotations, making them costly and impractical for pre-training tasks. This dependence limits their scalability and generalizability, particularly when addressing diverse datasets or unseen cardiac conditions.\nECG Self-supervised Learning. To overcome the annotation bottleneck, ECG self-supervised learning (eSSL) methods have been introduced, enabling representation learning from unannotated ECG signals. Contrastive learning frameworks, such as CLOCS and ASTCL, explore temporal and spatial invariance in ECG data. Generative eSSL techniques reconstruct masked segments to capture signal-level features. Despite their successes, eSSLs fail to incorporate clinical semantics from associated medical reports and require fine-tuning for downstream tasks, limiting their utility in zero-shot scenarios.\nBiomedical Multimodal Learning. Multimodal learning has advanced significantly in biomedical applications, especially in vision-language pre-training (VLP) frameworks for radiology, which align radiology images with structured knowledge from reports to reduce noise and improve robustness. However, multimodal learning for ECG remains underexplored. Methods like MERL integrate ECG signals and raw text reports but struggle with noise and inconsistencies in unstructured reports. Others, such as KED, use structured labels and contrastive learning strategies but face challenges from label noise and LLM-generated knowledge hallucinations. Our approach addresses these issues by structuring reports into meaningful entities, reducing noise, and aligning them with ECG signals without reliance on LLM-augmented content, minimizing hallucination risks while enabling efficient representation learning and downstream flexibility."}, {"title": "3 Methodology", "content": "SuPreME extracts structured clinical entities from ECG reports via LLMs (Section 3.1) and embeds both ECG signals and text-based cardiac queries into a shared space. A Cardiac Fusion Network (CFN, Section 3.2) then aligns these embeddings, enabling zero-shot classification of unseen cardiac conditions without fine-tuning (Section 3.3), yielding scalable, clinically meaningful representations."}, {"title": "3.1 ECG Clinical Entity Extraction", "content": "Enriching LLM with Domain-specific Knowledge. ECG reports, generated by 12-lead devices, provide diverse insights into cardiac activity. To harness this diversity and enhance model performance, We utilize domain-specific knowledge, including ECG terminologies and abbreviations (e.g., \"sinus rhythm\", \"LVH\" for Left Ventricular Hypertrophy), compiled systematically with GPT-40 from clinician-validated databases like UMLS and SNOMED CT. SCP codes from datasets like PTB-XL further enrich this knowledge base.\nKnowledge-Guided ECG Entity Extraction. Before performing Named Entity Recognition (NER) with Llama3.1-70B-Instruct, we first provide domain-specific knowledge, including key concepts, terminologies, and diagnostic structures, ensuring the model internalizes relevant medical context. Few-shot examples from manually annotated clinical reports further refine its understanding. Once primed, the model extracts entities (e.g., waveforms, diagnoses) from free-text ECG reports, classifying them at a \u201cGlobal\u201d level into 'Normal', 'Abnormal', or 'Uncertain' (in Figure 3(a)) to filter unreliable diagnostics. Similar entities within a report are merged for consistency, laying the groundwork for cross-report deduplication.\nEntity Deduplication with Professional Medical Database. Despite structured extraction and filtering, variations in physician writing styles and device-specific formats introduce redundancies and inconsistencies in the extracted entities. To resolve this, we compile a standardized ECG terminology library using LLMs to identify terms from professional medical databases like UMLS and SNOMED CT. We then use Medical Contrastive Pre-trained Transformers (MedCPT), a pre-trained medical BERT model, to align extracted entities with standardized terms (in Figure 3(b)). Using the PubMedBERT-initialized query encoder (QEnc), cosine similarity between embeddings maps each entity to its closest standardized term. This deduplication and mapping process reduces 3,138 initial entities to 295 standardized terms, as shown in Figure 4, significantly enhancing consistency and ensuring alignment with professional medical terminology."}, {"title": "3.2 Multimodal ECG Supervised Learning", "content": "ECG Embedding with Vision Transformer. The Vision Transformer (ViT), designed for 2D image processing, reshapes images into sequences of flattened patches for Transformer-based analysis. Similarly, ECG signals exhibit temporal and structural patterns analogous to the spatial relationships in images. We then adapt its architecture by dividing ECG time series into fixed-size patches, as shown in Figure 5.\nAn ECG signal sequence is represented as a 2D matrix \\(x \\in \\mathbb{R}^{L \\times T}\\), where \\(L\\) is the number of leads and \\(T\\) is the time series length. The time series is segmented into \\(N = T/P\\) fixed-length patches, each containing \\(P\\) time steps with \\(L\\) leads, resulting in patches \\(x_p \\in \\mathbb{R}^{L \\times P}\\). Each patch is flattened into a vector \\(x_p \\in \\mathbb{R}^{L \\cdot P}\\), and concatenated to form a sequence \\(X_p \\in \\mathbb{R}^{N \\times (L \\cdot P)}\\). A learnable linear projection \\(W_p \\in \\mathbb{R}^{(L \\cdot P) \\times D}\\) maps each patch to the \\(D\\)-dimensional latent space:\n\\(Z_p = X_p \\cdot W_p, Z_p \\in \\mathbb{R}^{N \\times D}\\) (1)\nSimilar to the [class] token in BERT, we add a unique learnable embedding \\(e_i \\in \\mathbb{R}^D\\) for each lead \\(i\\) and append it to the corresponding patch embedding \\(Z_p\\) to capture lead-specific features in the Transformer encoder. Positional embeddings \\(p_j\\) are added to preserve the order of patches in the time series. We use standard 1D positional embeddings \\(p_j\\) since it is sufficient to retain the sequential order in time series data. After incorporating \\(e_i\\) and \\(p_j\\), the final patch embedding \\(Z_{\\text{final}}^{(i,j)}\\) for each patch \\(j\\) of lead \\(i\\) is obtained. All embeddings \\(Z_{\\text{final}}^{(i,j)}\\) are concatenated along the lead dimension to form the sequence \\(Z \\in \\mathbb{R}^{[B, L \\times N, D]}\\), where \\(B\\) is the batch size.\n\\(Z_{\\text{final}}^{(i,j)} = Z_p^{(i,j)} + e_i + p_j\\)\n\\(Z = \\text{Concat}(Z_{\\text{final}}^{(1,1)} ..., Z_{\\text{final}}^{(1,N)}, ..., Z_{\\text{final}}^{(L,N)})\\) (2)\nThe concatenated sequence \\(Z\\) is then processed through multiple Transformer blocks to extract high-level ECG features. Each block consists of self-attention and feed-forward layers with residual connections. To improve generalization, we incorporate stochastic depth dropout in the residual pathways. The detailed structure and mathematical formulation of the Transformer blocks are provided in Appendix A.2.\nCardiac Query Embedding with MedCPT. Rather than using traditional category labels, our framework adopts a flexible and semantically meaningful approach based on cardiac text queries, defined by the Standard Communication Protocol (SCP) system for ECG diagnoses. Descriptive queries are constructed using interpretations from medical databases like UMLS and SNOMED CT, ensuring consistency with the target representations in entity deduplication. Afterwards, MedCPT, a pre-trained medical BERT model, is employed to embed text queries into a shared latent space with ECG signal embeddings, using its query encoder (QEnc) initialized with PubMedBERT in the retriever stage.\nFor a given query text \\(q\\), we generate its embedding vector \\(E(q)\\) in \\(D\\)-dimensional latent space by passing the text through the QEnc, which utilizes the special tokens [CLS] and [SEP] from BERT to denote the start and separator of the text. The encoding operation is defined as follows, where Trm refers to the Transformer encoder:\n\\(E(q) = \\text{QEnc}(q) = \\text{Trm}([\\text{CLS}] q [\\text{SEP}])\\) (3)\nTo ensure alignment with ECG signal embeddings for multimodal fusion, we apply a learnable projection layer that maps text query embeddings into the same latent space as the ECG features, as detailed in Appendix A.2. This enables effective cross-modal representation learning, facilitating the integration of structured clinical knowledge with ECG signals.\nAlignment by Cardiac Fusion Network. The Cardiac Fusion Network (CFN) in SuPreME consists of multi-layer Transformer decoders, which align ECG signals with text-based cardiac queries by treating textual features as input queries and signal features as memory, as illustrated in Figure 6.\nGiven a batch of signal features \\(F_{\\text{signal}} \\in \\mathbb{R}^{B \\times N \\times D}\\) and text features \\(F_{\\text{text}} \\in \\mathbb{R}^{M \\times D}\\), where \\(B\\) is the batch size, \\(N\\) is the number of image blocks, \\(M\\) is the number of text queries, and \\(D\\) is the embedding dimension, the CFN learns a joint representation that captures cross-modal dependencies. The Transformer decoder attends to the ECG features while grounding the interpretation in medical queries. The fused output is processed by an MLP head for classification.\nFurther architectural details, including initialization, normalization, and implementation specifics, are described in Appendix A.2."}, {"title": "3.3 Zero-shot Prompted Classification", "content": "Cardiac Query Design. In our framework, zero-shot classification is achieved by converting cardiac conditions in unseen downstream datasets into descriptive text queries. All SCP codes from the downstream datasets are compiled and paired with concise, clinically relevant textual descriptions, which serve as inputs to the textual modality of SuPreME. These cardiac queries enable the model to effectively align previously unseen cardiac conditions with ECG signal features, allowing it to generalize beyond the entities encountered during pre-training and facilitating inference on novel categories within downstream datasets.\nTo construct these cardiac queries, we employ simplified Clinical Knowledge-Enhanced Prompt Engineering (CKEPE). While traditional CKEPE involves querying LLMs to extract detailed clinical knowledge from external medical databases (i.e., UMLS and SNOMED CT), including disease subtypes and signal pattern attributes, our approach follows a similar pipeline but directly interprets SCP codes from downstream datasets into concise cardiac descriptions. The robust representation capabilities of our CFN allow us to rely on simplified query content without compromising performance. By reducing redundancy in the queries, this method avoids overloaded textual information that might hinder the model's ability to align features effectively across modalities.\nEvaluation with Pre-trained SuPreME. During the evaluation process, ECGs and cardiac queries are encoded into \\(Emb_{ECGs}\\) and \\(Emb_{queries}\\) respectively, using the ECG encoder and text encoder components of the pre-trained SuPreME framework. The embeddings are then input into the pre-trained CFN for alignment and fusion, which processes them using cross-modal attention mechanisms and outputs logits through an MLP head (i.e., a linear classification layer). Subsequently, the logits are passed through a sigmoid activation function \\(\\sigma\\) to produce probabilities Pred for each cardiac condition:\n\\(\\text{Pred} = \\sigma(\\text{CFN}(Emb_{ECGs}, Emb_{queries}))\\) (4)\nWe assess the model's classification performance using the Area Under the Receiver Operating Characteristic curve (AUROC, also referred to as AUC; further details are provided in Appendix \u0410.3). Given the multi-label nature of this classification task, the AUC is computed separately for each category, and the mean AUC across all categories is reported as the overall performance metric.\nOverlap Analysis on Pre-train and Downstream Datasets. We analyze the cardiac query overlap between the pre-train dataset and six downstream datasets specified in Section 4.1, as well as among the downstream datasets themselves. Specifically, we embed all entities from the pre-train dataset and cardiac queries from the downstream datasets, compute their cosine similarity, and apply a threshold of 0.95 to filter overlapping queries. This analysis reveals that 57 cardiac queries overlap between the pre-train dataset and the downstream datasets. Details of the overlapping queries are provided in the Appendix A.4."}, {"title": "4 Experiments", "content": "4.1 Configuration and Settings\nClinical Entity Extraction. We utilize domain-specific prompts enriched with professional medical knowledge to extract entities from the MIMIC-IV-ECG dataset (see Appendix A.5). Entity extraction is performed using Llama3.1-70B-Instruct with structured constraints for clinical NER. Deduplication and mapping leverage MedCPT embeddings to group similar entities (similarity > 0.8) and map them to standardized terminologies from UMLS and SNOMED CT (average similarity > 0.75). Experiments are conducted on 8 NVIDIA A100-SMX4-80GB GPUs with vLLM.\nSupervised ECG Pre-training. A 1D ViT-tiny model serves as the ECG encoder, with a patch size of 125 (0.25 seconds per patch). MedCPT, with frozen weights, is used as the text encoder. We use the AdamW optimizer with a learning rate of 1 \u00d7 10-3 and weight decay of 1 \u00d7 10\u22128. A cosine annealing scheduler adjusts the learning rate with an initial cycle length \\(T_0\\) of 5000 steps and \\(T_{mult} = 1\\), down to a minimum learning rate of 1 \u00d7 10\u22128. Pre-training runs for 100 epochs with early stopping tolerance as 10 epochs and uses a batch size of 256 per GPU on 4 NVIDIA A100-PCIE-40GB GPUs.\nDownstream Classification Task. Zero-shot classification is evaluated on six unseen datasets (e.g., PTB-XL, CPSC-2018, and Chapman-Shaoxing-Ningbo; see Appendix A.5) using SuPreME and customized prompts created via CKEPE (Section 3.3). Ablation studies assess the impact of different ECG and text encoder backbones and the CFN module. Mainstream eSSLs are evaluated with linear probing by freezing ECG encoders and fine-tuning a linear layer on 1%, 10%, and 100% of labeled data from the six datasets. All Downstream tasks are evaluated using the average AUC across datasets, following the data splits from  (see Appendix A.6). Detailed setups are provided in Appendix A.7.\n4.2 Evaluation with Mainstream eSSLs\nWe evaluate SuPreME against mainstream eSSL frameworks across 127 classes in six downstream ECG datasets, conducting linear probing with eSSL ECG encoders across varying data proportions to facilitate performance comparison. Table 1 demonstrates AUC results of SuPreME and eSSLs under different evaluation approaches.\nOur results demonstrate that SuPreME achieves superior performance compared to traditional eSSL frameworks. With an overall zero-shot AUC of 77.20%, SuPreME outperforms most non-multimodal eSSL models, which require linear probing even with 1% (best: 59.46%) or 10% (best: 72.04%) of labeled data, showcasing its strong generalization capabilities and efficient utilization of pre-trained knowledge. Even without the CFN module, SuPreME remains highly competitive (in Appendix A.8) using only the pre-trained ECG encoder for linear probing. It consistently outperforms non-multimodal eSSL models across 1%, 10%, and 100% labeled data, and and achieves comparable performance to multimodal contrastive learning frameworks like MERL, despite not explicitly leveraging contrastive objectives. Notably, SuPreME achieves this with just 16 training epochs, compared to MERL's 50 epochs.\nFigure 8 presents framework performance across individual datasets. SuPreME's advantage on the PTB-XL-Superclass dataset is minimal, likely due to the dataset's simplicity, as it includes only 5 broad cardiac condition labels (e.g., NORM, STTC, MI), making it difficult to differentiate model performance. Additionally, all frameworks perform poorly on the PTB-XL-Form dataset, which focuses on 19 ECG waveform types that do not directly correspond to cardiac conditions, leading to ambiguous associations and reduced performance for all models.\nTo investigate SuPreME's sensitivity to pre-training data, we evaluate its zero-shot performance across varying data proportions (in Figure 9). SuPreME's performance improves steadily with more pre-training data, maintaining a significant edge over eSSL frameworks. Notably, with just 20% of pre-train data, SuPreME rivals or surpasses many eSSLs using 10% labeled data for linear probing. It also achieves comparable performance to multimodal eSSLs with only 60% of pre-train data, showcasing its efficiency and robust generalization, even in low-resource settings.\n4.3 Evaluation of SuPreME Architecture\nBeyond comparisons with eSSLs, we directly assess SuPreME's zero-shot classification performance by varying its core modules, including the ECG backbone and CFN. Table 2 reports the results for SuPreME and its variants, where the ViT + CFN architecture achieves the highest average AUC of 77.20%, with particularly strong performance on PTB-XL-Rhythm and CSN.\nUnder the linear classification setup, ResNet outperforms ViT across most datasets, demonstrating its effectiveness in extracting essential features without contextual mechanisms. However, the CFN significantly improves performance, with SuPreME (ViT + CFN) achieving a notable boost in AUC, particularly on PTB-XL-Superclass (78.20%), PTB-XL-Rhythm (86.79%), and CSN (80.17%). Figure 10 highlights the strength of ViT's attention mechanisms combined with CFN, which excels at capturing complex temporal and spatial dependencies in ECG signals.\nBy employing prompts to generate meaningful class queries, the CFN enables a more flexible and adaptable approach to classification, allowing flexible adaptation to diverse downstream class structures. Unlike linear classification, SuPreME dynamically aligns pre-trained knowledge with new cardiac conditions, eliminating the need for explicit class mapping between pre-training and downstream datasets and improving generalization.\nWe further analyze performance on specific cardiac conditions in PTB-XL-Subclass (others in Appendix A.9), shown in Figure 11. SuPreME consistently achieves high AUC scores, particularly for critical conditions like LAFB/LPFB, CRBBB, CLBBB, IRBBB, and RVH (AUC > 90). In contrast, other variants show lower performance, especially for conditions requiring nuanced spatial and temporal patterns. ResNet + Linear performs competitively on simpler cases but struggles with complex conditions. ResNet + CFN exhibits significant drops, particularly for IMI, AVB, RAO/RAE, ISCA, and IRBBB, highlighting its limitations in effectively capturing intricate dependencies.\n4.4 Ablation Analysis\nClinical Entity Mapping. To assess the impact of mapping labels to a standardized medical database, we compare SuPreME trained on the original 341 raw labels vs. our 295 deduplicated set (Table 3(a)). Notably, deduplication boosts the zero-shot AUC from 65.94% to 77.20%, which could be attributed to the removal of noisy or synonymous labels, thus clarifying the model's representation of distinct cardiac conditions.\nECG Encoder Backbone. To assess the ECG backbone impact, we replace the ViT-tiny with ResNet18 and evaluate zero-shot performance. ResNet18 yields a lower AUC of 64.96% (drops by 12.24%, Table 3(b)), indicating that ResNet struggles to capture longer-range ECG dependencies compared to ViT's self-attention mechanism.\nClinical Text Encoder. For three different medical text encoders - BioClinicalBERT, PubMedBERT, and MedCPT - MedCPT achieves the highest AUC, surpassing the others by 6.47% and 19.75% (Table 4(a)). This appears to stem from MedCPT's contrastive objectives, which more effectively capture fine-grained sample similarities and differences than traditional masked language modeling.\nCardiac Fusion Network Module. We compare CFN-based fusion with a simple linear projection (Table 4(b)). CFN lifts the zero-shot AUC from 71.23% to 77.20%, highlighting the benefits of cross-attention in capturing multimodal synergies between ECG signals and text queries.\nCustomized Cardiac Prompts. We evaluate three prompt strategies - GPT-40 Generated, CKEPE (detailed), and CKEPE (simplified). As shown in Table 5(a), the simplified CKEPE achieves the highest AUC, delivering at least a 12.5% improvement. This suggests that concise, clinically focused prompts help reduce noise and enhance alignment.\nDropout Ratio. During pre-training, we compare dropout rates of {0.05, 0.10, 0.15, 0.20}. In Table 5(b), a rate of 0.10 yields the best AUC (77.20%), likely striking a balance between regularization and signal retention."}, {"title": "5 Conclusion", "content": "We present a novel LLM-based method for ECG clinical entity extraction and introduce SuPreME, a scalable supervised pre-training framework for multimodal ECG representation learning that aligns ECG signals with fine-grained, standardized medical terminologies rather than free-text reports. Its Cardiac Fusion Network (CFN) and Clinical Knowledge-Enhanced Prompt Engineering (CKEPE) eliminate the need for further fine-tuning, enabling robust zero-shot classification with concise cardiac queries. Benchmarked on six downstream datasets, SuPreME achieves superior zero-shot performance against 11 eSSLs, underscoring both data efficiency and diagnostic precision. Our results highlight the value of explicit entity-level supervision over raw text alignment in ECG multimodal learning, providing a strong basis for clinically oriented ECG representation learning."}, {"title": "Limitations", "content": "While SuPreME demonstrates robust classification performance and superior results compared to existing eSSL frameworks, there are some limitations to consider. The framework's reliance on LLMs for clinical entity extraction may face challenges in capturing highly specialized or ambiguous medical knowledge without domain-specific fine-tuning. Additionally, although SuPreME achieves impressive zero-shot classification performance, the approach assumes that the pre-trained model can generalize well across diverse clinical contexts, which may not always hold in real-world scenarios with varying data quality and noise levels. Future work will focus on enhancing the robustness of clinical entity extraction and developing more adaptive strategies for diverse ECG data, as well as exploring methods to further optimize the pre-training process."}, {"title": "A Appendix", "content": "A.1 Electrocardiogram (ECG)\nIn the medical field, electrocardiogram (ECG) is an important tool for recording and analyzing patients' cardiac activities, which helps healthcare professionals identify various kinds of cardiac problems by detecting the electrical changes in different leads. The standard 12-lead ECG is the most common method of recording ECGs, and it can capture relatively comprehensive range of cardiac signals through placing electrodes at different locations on the body, providing information of the heart's health conditions.\nThe basic components of the 12-lead ECG include the limb leads and the precordial leads. The limb leads contain I, II, III, aVR, aVL, and aVF, each of them consists of a combination of electrodes located primarily in the right arm, left arm, left leg, and right leg (as shown in Figure 12). The precordial leads contain V1, V2, V3, V4, V5, and V6, which all correspond to specific single electrodes at different locations on the chest, and are used to observe in detail the electrical activity of the anterior, lateral, and posterior walls of the heart.\nA.2 Implementation Details\nTransformer Block Structure. The Transformer architecture is widely used for seq2seq modeling, learning global dependencies via self-attention instead of recurrent or convolutional structures. It consists of an encoder-decoder design, where both the encoder and decoder utilize stacked self-attention and feed-forward layers, as shown in Figure 13.\nEach encoder block applies a residual connection around its multi-head self-attention (\u041c\u041d\u0410) and position-wise feed-forward (FF) sublayers, followed by layer normalization:\n\\(Z^{(k,1)} = Z^{(k-1)} + \\text{Drop}(\\text{MHA}(\\text{Norm}(Z^{(k-1)})))\n\\(Z^{(k,2)} = Z^{(k,1)} + \\text{Drop}(\\text{FF}(\\text{Norm}(Z^{(k,1)})))\n\\(Z_{norm} = \\text{Norm}(Z^{(final)})\\) (5)\nwhere \\(Z^{(k-1)}\\) is the input to the k-th Transformer block, \\(Z^{(k,1)}\\) represents the intermediate state after multi-head attention and residual connection, and \\(Z^{(k,2)}\\) is the output after the feed-forward network. The final normalized representation \\(Z_{norm}\\) is used for downstream ECG classification.\nThe decoder extends the encoder structure by introducing an additional multi-head attention sub-layer that attends to encoder outputs, while also incorporating masked self-attention to ensure autoregressive sequence modeling. These layers collectively enable flexible cardiac feature extraction in SuPreME.\nProjection of Text Query Embeddings. To align the D-dimensional text query embeddings \\(E(q)\\) with ECG embeddings for multimodal fusion, \\(E(q) \\in \\mathbb{R}^{D}\\) is projected into the same latent space \\(\\mathbb{R}^{proj_{out}}\\) with ECG embeddings using a two-layer feedforward neural network with a ReLU activation function:\n\\(E'(q) = \\text{Proj}(E(q)) = W_2 \\cdot \\text{ReLU}(W_1 \\cdot E(q))\\) (6)\nwhere \\(W_1 \\in \\mathbb{R}^{proj_{input} \\times hidden}\\) and \\(W_2 \\in \\mathbb{R}^{proj_{hidden} \\times proj_{out}}\\) are the weights of the linear layers. The final embedding \\(E'(q)\\) represents the query in the target latent space, ready for downstream multimodal fusion with ECG signal embeddings.\nInitialization of Cardiac Fusion Network. All weights in linear layers and attention modules are initialized with a normal distribution, \\(W \\sim N(0, 0.02)\\). To support batch processing, the text embeddings \\(F_{text}\\) are expanded to match the batch size \\(B\\). Both ECG and text embeddings undergo layer normalization to improve training stability and convergence.\nA.3 Performance Evaluation Metric\nWe use zero-shot learning and linear probing to evaluate the performance of SuPreME and mainstream eSSL frameworks. The primary metric is Area Under the Receiver Operating Characteristic (AUROC, also referred to as AUC). AUROC is widely used to evaluate the performance of binary classification models. The ROC curve plots the True Positive Rate (TPR) on the vertical axis against the False Positive Rate (FPR) on the horizontal axis. By varying the classifier's threshold, TPR and FPR are calculated and then plotted to form the curve, where TP refers to True Positive, FN refers to False Negative, FP refers to False Positive, and TN refers to True Negative.:\n\\(\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\)\n\\(\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\\) (7)\nAUROC is the area under the ROC curve, with values ranging from 0 to 1, reflecting the overall classification ability of the model. AUROC = 0.5 indicates that the model's classification ability is equivalent to random guessing, while AUROC > 0.5 and values closer to 1 indicate that the model is able to classify with greater accuracy.\nA.4 Overlap Analysis\nTable 6 shows the overlap between entities from the pre-train dataset and cardiac queries from the downstream datasets, filtered using a cosine similarity threshold of 0.95.\nA.5 Dataset Overview\nMIMIC-IV-ECG. MIMIC-IV-ECG is a comprehensive database containing 800,035 diagnostic ECG samples from 161,352 unique patients, with 12-lead recordings in 10 second length and sampled at 500 Hz . These data have been matched with patient records in the MIMIC-IV clinical database, allowing for the association of waveforms with reports when a cardiologist's report is available through provided linking information. To enhance the usability of the data, we exclude empty reports as well as reports containing fewer than 3 words, and replace 'NaN' and 'Inf' values in the ECG records with the average of 6 neighboring points. Ultimately, the dataset used for clinical entity extraction tasks includes 771,500 samples, each comprising 18 machine-generated ECG reports based on rules and the corresponding ECG data. After clinical NER and dedplication on the 18 ECG reports of each sample, the dataset holds 295 labels of professional medical terminologies.\nA.6 Downstream Data Split\nFor PTB-XL, we adopt the official train-test split recommended by the dataset authors ensuring consistency with prior works and a balanced distribution of ECG categories. This split is directly applied across the Superclass, Subclass, Form, and Rhythm subsets of PTB-XL. For CPSC-2018 and CSN, we follow the data splitting approach used by , which randomly divides the datasets into training, validation, and testing subsets in a 70%:10%:20% ratio.\nDetails of the splits, including the specific number of samples allocated to each subset, are summarized in Table 7.\nA.7 Downstream Experiment Configuration\nThe training configurations for downstream tasks, including optimizer, scheduler, and relevant hyperparameters, are detailed in Table 8.\nA.8 Performance on Linear probing\nTable 9 shows the linear probing AUC performance of SuPreME's and other eSSLs' ECG encoders on specific six downstream datasets.\nA.9 Performance on Specific Cardiac Conditions\nPTB-XL-Superclass. Figure 14 records the AUC performance of SuPreME on specific cardiac conditions in PTB-XL-Superclass dataset.\nPTB-XL-Subclass. Figure 15 records the AUC performance of SuPreME on specific cardiac conditions in PTB-XL-Subclass dataset."}]}