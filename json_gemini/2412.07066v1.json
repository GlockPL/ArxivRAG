{"title": "THE MIRAGE OF ARTIFICIAL INTELLIGENCE TERMS OF USE RESTRICTIONS", "authors": ["Peter Henderson", "Mark A. Lemley"], "abstract": "Artificial intelligence (AI) model creators commonly attach restrictive terms of use to both their models and their outputs. These terms typically prohibit activities ranging from creating competing AI models to spreading disinformation. Often taken at face value, these terms are positioned by companies as key enforceable tools for preventing misuse, particularly in policy dialogs. The California AI Transparency Act even codifies this approach, mandating certain responsible use terms to accompany models.\nBut are these terms truly meaningful, or merely a mirage? There are myriad examples where these broad terms are regularly and repeatedly violated. Yet except for some account suspensions on platforms, no model creator has actually tried to enforce these terms with monetary penalties or injunctive relief. This is likely for good reason: we think that the legal enforceability of these licenses is questionable. This Article provides a systematic assessment of the enforceability of AI model terms of use and offers three contributions.\nFirst, we pinpoint a key problem with these provisions: the artifacts that they protect, namely model weights and model outputs, are largely not copyrightable, making it unclear whether there is even anything to be licensed.\nSecond, we examine the problems this creates for other enforcement pathways. Recent doctrinal trends in copyright preemption may further undermine state-law claims, while other legal frameworks like the DMCA and CFAA offer limited recourse. And anti-competitive provisions likely fare even worse than responsible use provisions.\nThird, we provide recommendations to policymakers considering this private enforcement model. There are compelling reasons for many of these provisions to be unenforceable: they chill good faith research, constrain", "sections": [{"title": "INTRODUCTION", "content": "If you don't own something, but I get it from you, can you prevent me from using it in ways you don't like? Perhaps your answer is, \u201cOf course I can. After all, you got it from me, and I didn't have to give it to you.\" That's a common instinct among people who think in terms of real property, though it often traces to fighting the hypothetical: assuming that if I got it from you, you must have owned it. On the other hand, perhaps your answer is, \u201cNo, if I don't own it, I have no control over it.\u201d This instinct is much more common among those who think of information or intellectual property (IP). Patents and copyrights expire, and when they do, we're all free to use them. Sure, I got Mickey Mouse from Disney, but now that he's (finally) in the public domain, that doesn't matter; I am free to copy him. And it would seem bizarre for a dictionary to say I couldn't use a word I learned there in ways the publisher doesn't like. They don't own the word, after all.\nThe law's answer to this recurring and seemingly simple question is \"it's complicated.\u201d As we show in this Article, the enforceability of use restrictions depends on a host of considerations. The law treats IP differently than real property. It treats information protected by IP differently than information not protected by IP. It treats copying of uncopyrightable information differently than access to that information\u2014and how it treats copying depends on which circuit you are in. It might or might not enforce a contract restricting use even if the law would otherwise permit that use. And even if there is such a contract, whether it is enforceable may depend on the kind of use you wish to prohibit and who you hope to stop.\nThese legal issues have come to a head with generative AI, for a surprising reason: generative Al companies probably don't own rights over either their models or the output of those models. Companies like Github, OpenAI, MidJourney, and many others allow anyone to submit prompts to their AI and receive output generated by that AI\u2500new text, images, videos, or software code. The companies generally claim no interest in or ownership"}, {"title": "Terms of Service Woes for AI", "content": "of the content the AI creates, and under current law they probably can't. Nor do they have a strong claim to own the content on which they are trained; most of them get that content from the internet or other public sources, generally without permission. And while they might own the algorithm and model weights they use to generate output from training data, it's far from clear what legal rights they have in those abstractions. Model parameters could be viewed as purely functional\u2014uncopyrightable\u2014artifacts. And the Al companies, for the most part, aren't distributing their underlying code.\nWe argue that there is little basis for a company to claim IP rights in anything its generative AI delivers to its users. Generative Al companies are selling something\u2014and people are buying. But it's not obvious what it is they actually sell beyond access to the compute used to generate the responses to prompts. Nor is it clear what, if any, other legal rights they have in their models. The absence of IP rights for the model weights or model outputs makes questionable many state law claims due to copyright preemption doctrine. And other federal claims, like those based on the Digital Millenium Copyright Act (\u201cDMCA\u201d) or the Computer Fraud and Abuse Act (\u201cCFAA\u201d) may not pick up the slack.\nDespite their questionable ownership, all major generative Al creators impose restrictive conditions on the use of their models. Several purport to disclaim ownership in anything the model creates, or even assign rights in the output to the user, while at the same time forbidding users from employing that information to compete with them. And all companies impose a variety of \"moral\" restrictions on the use to which their models are put.\nAl creators have good reason to want such terms attached to their models. Generative AI models have gone beyond simple image creators or chatbots. They are rapidly becoming general purpose tools that can be adapted for any purpose\u2014some, including us, have taken to calling the underlying models, \u201cfoundation models\" for this very reason. Model creators have very few tools to protect their creation. If the model is accessible in any way, researchers have shown that you can \u201cdistill\u201d or \u201csteal\u201d a model with some ease. Models are also regularly modified and misused for"}, {"title": "Terms of Service Woes for AI", "content": "a host of undesirable purposes. OpenAI has identified foreign adversaries using their systems for disinformation campaigns and influence operations.\nOpenly available image generators have been regularly modified for creating non-consensual intimate images and worse. And the list goes on.\nPreventing such misuse at a technical level is extremely difficult. Researchers have shown numerous ways by which current safety guardrails provide little resistance to misuse. The more capable models become and the more that model creators allow customization of models, the easier it is to bypass the technical guardrails. In many cases, it is simply impossible to introduce a reasonable technical safeguard. For example, how can you prevent someone from using a model to create a phishing email that looks identical to a completely benign email? Similarly, when model weights are openly released, there are currently no ways to reliably prevent harmful modification of that model\u2014though researchers are working on potential technical pathways to make misuse harder and more costly. In all of these"}, {"title": "Terms of Service Woes for AI", "content": "situations model creators are only left with structural or legal remedies. With IP law likely to be ill-fitting for protecting against competitors and abusers, terms of use remain model creators' main hope for private enforcement.\nAl companies also use terms of use for less noble ends. Many of the companies include provisions in their terms of service that prevent actual or potential competitors from scraping their site or using their output to train a new Al model. Having trained their models on content scraped from others, they want to prevent the same thing from happening to them. Increasingly common practices of training competing models based on existing ones\u2014 sometimes called \u201cusing synthetic data\u201d\u2014have been met with researchers calling this \"stealing\" and potentially illegal. One news report alleging that a prominent researcher quit his job at Google because the company trained on GPT-4-generated data.\nIn policy settings, model creators seem optimistic about the possibility of private enforcement. In 2024, for example, when the National Telecommunications and Information Administration (\u201cNTIA\u201d) sought"}, {"title": "Terms of Service Woes for AI", "content": "request for responses to the question, \u201cCould open foundation models reduce equity in rights and safety-impacting AI systems (e.g. healthcare, education, criminal justice, housing, online platforms, etc.)?\u201d Meta's response was that \"We developed and made available responsible use guides to help downstream developers measure impact. And it's why the Llama 2 Acceptable Use Policy prohibits specific high risk use cases. \" In its resulting recommendations, NTIA examined one role of licenses as a policy option:\nPolicymakers behind California's AI Transparency Act lean into this optimism. The Act mandates that Al creators add \u201clatent disclosures\u201d to content generated by their systems that make the content identifiable and traceable. It also obligates the Al creator to add terms to their license requiring licensees to maintain the latent disclosure mechanism. And the"}, {"title": "Terms of Service Woes for AI", "content": "Act requires prompt action to revoke the license in cases of non- compliance.\nThe practical reality is that model creators have not yet tried to legally enforce these restrictions, though there are perfect test cases. Open-access image generation models, like Stable Diffusion, almost universally contain terms that would prohibit the creation of \u201cnon-consensual intimate images\u201d (NCII, sometimes known as \u201crevenge porn\"). Yet, no private litigation has resulted against a swath of people using the model for exactly these purposes.\nThere are of course practical challenges to enforcement: the costs would be large. But that hardly seems an insurmountable problem. The companies that would enforce these terms of service are some of the largest companies in the world; certainly they can afford lawyers. And even if they didn't want to spend money targeting abuses of their systems, the potential harms of some downstream uses are large enough that some would likely take these cases pro bono, and attorneys' fees could be awarded in winning cases. Public enforcement is also an option. Recently, for example, the San Francisco City Attorneys filed suit against the owners of website creators misusing open models.\nWe think there's a bigger problem though: the legal case for enforcement of model creators' terms is uncertain. Even in the modern world of easy enforcement of things that wouldn't have been considered contracts"}, {"title": "Terms of Service Woes for AI", "content": "in a prior era, courts in some jurisdictions will not enforce a contract term that forbids the copying of something copyright law does not protect. And as we show in this paper, AI companies probably don't have a copyright in anything they want to stop third parties from using.\nA further complication arises because many of these companies release their models openly (so-called \u201copen-weight models\") under restrictive licenses. Those licenses allow use of the model subject to certain restrictions (chiefly an agreement that the user will in turn open their work product on the same terms). But licenses depend for their enforceability on copyright in the thing being licensed. If there is no copyright, a traditional open-source (or copyleft) license becomes relatively meaningless. Even if they are enforceable contracts despite the absence of any copyright interest, terms of service may not be effective controls on unwanted uses of A\u0399 models. At most, terms of service can bind only the parties who actually deal with the AI company. They don't \u201crun with\u201d the output of the AI. Users can post model outputs online, then competitors can take those outputs and do what they will with them (including training a competing model). The third party never agreed to restrictions on those outputs. And in the case of terms constraining competition, antitrust, intellectual property misuse, and other public policy doctrines may render unenforceable particular restrictions that interfere with competition or disrupt the free flow of commerce. The upshot is that restrictions on the use of AI by the public may be built on a house of cards. They may have no legal power to control what users do with the model weights they release or the output of their AI systems.\nFrom a policy perspective, we are of two minds about the difficulties of enforcing these restrictions. As a general matter, courts have been far too quick to enforce unilateral declarations of \u201crights\u201d posted somewhere on a web page. Restricting the ability of internet companies to generate legal obligations by fiat is probably a good thing. And many of the provisions we discuss-like those preventing competition\u2014are noxious as a matter of public policy. But not all AI terms of service restrictions are bad for the world. The questionable enforceability of those provisions may doom efforts by responsible AI campaigns to cabin harmful uses of generative AI. And the absence of copyright in Al outputs may make open-source licenses unenforceable in this industry, which would be unfortunate. In this Article, we examine these issues in depth. We hope that this will help provide much-"}, {"title": "Terms of Service Woes for AI", "content": "needed nuance to the discussion of synthetic training data and accusations of illegality amount machine learning practitioners, as well as provide a more measured assessment of the ability to successfully enforce licenses for responsible use of open-access models. There may be ways to strengthen the likelihood of enforceability\u2014at least for some small portion of \u201cresponsible use\" terms. But in most cases, these licenses will face an uphill battle in enforceability. As such, we urge model creators not to place significant reliance on such licensing regimes.\nWe also urge policymakers to take any promises of private enforcement with a grain of salt until the doctrinal tumult has been smoothed over with a concrete test case. Our analysis may mean that many open-weight model licenses containing \u201cresponsible AI\u201d use restrictions are probably unenforceable. This could further restrict the already limited options for enforcing responsible uses of broadly general models. Ultimately, we think the right answer is not to enforce unilateral statements of corporate desire in terms of service provisions. Rather, statutory provisions, not private fiat, should distinguish between good and bad uses of AI and restrict the latter.\nThis Article proceeds in five parts. Part I provides relevant background about generative Al models, as well as the landscape of contractual controls that creators deploy to govern their models' use. Part II challenges the copyright foundations of these licensing regimes: model creators likely lack cognizable legal ownership of their model weights nor model outputs.\nPart III analyzes the implications for contract law, showing how: copyright preemption increasingly undermines copying-based restrictions implicated for AI model terms of use; in many cases model creators will not have entered into a valid contract with would-be defendants; and, antitrust principles, as well as copyright misuse, may invalidate anticompetitive provisions entirely.\nPart IV broadens the inquiry to evaluate potential tort and statutory claims. Preemption similarly constrains tort-based theories. And other access-control doctrines and statutes\u2014including DMCA anti-circumvention rules, trespass to chattels, and CFAA violations\u2014are unlikely to help model creators.\nPart V highlights the implications of our analysis: model creators and policymakers should rely on terms of use sparingly. Policymakers should regulate misuse by statute, not relying on private terms. And model creators should place more emphasis on technical measures rather than dubious contractual terms."}, {"title": "I. GENERATIVE AI AND TERMS OF USE", "content": "Generative Al systems come in a variety of formats and the terms attached to them are just as varied. In this Part, we will overview the technical terminology needed to understand the legal landscape around these systems. Then we will examine the present landscape of terms of use restrictions that Al model creators have attached to their systems."}, {"title": "A. The Nature and Development of AI Models", "content": "At the core of generative AI systems are \u201cmodels.\u201d These are important artifacts that control (for the most part) how an \"input\" into the \"model\" is converted into an \"output.\" So, if the model is meant to be a chatbot, the model input could be a user's request (e.g., \u201cPlease summarize case law around copyright preemption.\u201d). The model as a whole processes this request to craft a response. The model \u201cweights\u201d or \u201cparameters\u201d provide the formula for how to run billions of numerical operations to process this input. On the other side, we will receive an output (e.g., \u201cSure! Here is a summary of current copyright preemption doctrine...\"). The weights are typically represented as multi-dimensional arrays of floating-point numbers, often totaling billions of parameters for large-scale models. Since model weights are just numbers, an accompanying software\u2014which we will refer to as \"inference software\u201d is needed to execute the operations encoded by the weights. The combination of the trained model and the inference software used to generate outputs forms the complete AI system. Usually, model weights are encoded in an interoperable format. Regardless, though, it is usually simple to re-implement the inference software if one has access to the weights, though sometimes very minute differences can determine if the model functions as expected.\nThe model weights are the result of the \u201ctraining\u201d process, where randomly initialized weights are modified over time to more correctly predict outputs. \"Training data\u201d encompasses the corpus of information used to teach the Al model. For large language models, this often includes diverse text sources from the internet, books, and other digital content. The quality, diversity, and potential biases in the training data directly impact the model's performance and the nature of its outputs. The process of curating and preprocessing this data is a critical step in model development, often\""}, {"title": "B. Technical Enforcement versus Legal Enforcement", "content": "It has been well documented that Al models make mistakes and can be misused. Model creators might employ a range of technical measures to"}, {"title": "C. Anti-competitive, Anti-scraping, and Anti-Reverse Engineering Provisions", "content": "Many of these datasets generated by capable models like GPT-4 come with a caveat: most proprietary models have terms of service that prohibit using the proprietary models' outputs for training new models."}, {"title": "D. \u201cResponsible Use\u201d Restrictions", "content": "Terms of Service agreements also have played a core role in \u201cAI Safety\" and \"Responsible AI\u201d discussions. It is now considered good practice to attach restrictions on potentially harmful uses in the terms coming with an Al system or model. For example, the Open & Responsible AI class of Licenses (\u201cOpenRAIL\u201d) attach use restrictions that would prevent those who download the model from using it \u201cto provide medical advice and medical results interpretation,\u201d among other use cases.\nMost major AI companies have attached acceptable use policies to their generative AI models and services. These policies often have common sets of terms, such as banning:\nillegal activities\nharmful or abusive behaviors\npromotion of violence or illegal activities\ncontent or uses that compromise privacy\ncreating sexually explicit material\nimpersonating real individuals\nspreading misinformation or interfere with democratic processes\nengaging in academic dishonesty\ncontent that would harm minors\nunsupervised professionalized advice for critical settings like finance, healthcare, and law\nrequiring naming conventions for derivative models\nModel creators that serve their models through an API, like OpenAI, can enforce these policies by banning users from their platform. This, however, requires detailed monitoring of users' interactions with their models, and it is unclear to what extent such monitoring exists. Nonetheless,"}, {"title": "II. COPYRIGHT IN UNDERLYING ASSETS", "content": "In most contexts, conditional restrictions on the use of copyrighted material are enforceable through copyright law. For instance, a software developer might release code under a license limiting its use to non- commercial purposes. Should a competitor violate these terms, the developer could pursue copyright infringement claims against acts that would violate the copyright but for the license. This is not a novel legal concept; many developers enforce their software licenses through copyright infringement litigation.\nGenerative AI models, though, are not like traditional software. Despite their value, it is not clear that the user-accessible parts of those systems (outputs and model weights) are copyrightable at all. Recent Copyright Office guidance and case law suggest that generative AI model creators are unlikely to be considered authors of model outputs. And most disclaim ownership of them. Even would-be users of models, despite their efforts to the contrary, face a high bar to be considered owners of Al model outputs.\nModel weights likely fare no better. They are also created through an automated process, likely rendering them uncopyrightable, like their outputs. And even if the human authorship requirement is met, model weights are likely functional artifacts unprotected by copyright.\nModel creators might claim ownership over some of the training data, and then claim that models are a copyrightable database of that data\u2014but this too"}, {"title": "A. Model Outputs", "content": "To the extent that model creators think that there is any copyright ownership to be had over model outputs, they appear to assign those rights to their users-conditional on compliance with the terms of use. Table 2 provides an overview of these terms for various model providers."}, {"title": "B. Model Weights", "content": "Model creators' claims of ownership over model weights would fare no better. Recall that model weights, in essence, are series of numerical values that, when applied to input data, produce the model's output. These weights are typically stored in files that contain a series of numbers. In layman's terms might read something like, \u201cMatrix Number 5 in Layer 3 of the overall model architecture should contain values: [0.1, 1.2, ...].\u201d These weights are \u201clearned\u201d through a semi-automated process involving \u201ctraining\u201d the model on vast amounts of data. While model creators can exert significant control over downstream model behaviors, at the scale of billions of parameters"}, {"title": "C. Training Data", "content": "Finally, one could argue that model weights are a protectable database or a compilation encoding their training dataset. But this is an uphill battle. Al companies generally don't own the underlying data that they train on. Rather, they scrape it from a variety of sources, often publicly available internet data. That process is itself the subject of dozens of legal challenges from the copyright owners over their use of data.\nPerhaps the aggregation of that data could be copyrighted even though"}, {"title": "D. System Code", "content": "The final, and most likely, source of copyright protection is the software that runs the model itself. Models are typically released with a thin layer of software that allows users to execute the process encoded by model weights. Some companies may even release the code used to train the model. We will refer to this entire set of code as \u201csystem code.\u201d We will refer to the code used to train a model as \u201ctraining system code\u201d and the code used to execute instructions encoded by the model weights as \u201cinference system code.\u201d It is possible (though not guaranteed) that both of these codebases may be protectable by copyright. The training system code is generally not implicated for the terms of service attached to models, so we will not account for it here.\nModel creators may argue that inference system code is copyrightable, and that by using that code to read and run inference on model weights, downstream users are liable for infringement if they do not comply with associated terms. This is a plausible argument, but for two practical problems.\nFirst, in their current form, the software required to execute model weights is relatively simple and purposefully interoperable. Model users can simply switch to a different software to decode model weights. Many codebases can load and execute released model weights, using an interchangeable application program interface (\u201cAPI\u201d).\nSecond, even if a model creator purposefully made model weights purposefully non-interoperable, downstream users would find a way to reverse engineer this process to create compatible execution software. Doing so is exactly what the Court in Google v. Oracle ruled was fair use. Recently, the court in SAS Institute Inc. v. World Programming Ltd., would go even further and argue that these application program interfaces are unprotectable, not even reaching the fair use argument. And merger doctrine could also exclude copyrightability in some jurisdictions."}, {"title": "III. CONTRACT CLAIMS", "content": "Without a viable copyright claim, model creators will no doubt look to contract claims. Unfortunately for Al companies, these too are not so straightforward. Contract claims based on failed copyright arguments may fall to preemption, and there are limits on the usefulness of contract law even if it does apply. We note that even if terms of service are likely to be enforceable for some narrow \u201cresponsible use\" terms, anti-competitive provisions and many restraints on copying will likely be thrown out."}, {"title": "A. Are Terms of Use Enforceable Contracts at All?", "content": "The first question is whether AI company terms of use create an enforceable contract at all.\nAl companies invariably take the position that \u201cyou need to come to us to access it, so we can impose whatever restrictions we want.\u201d That is generally true as to access itself-even if OpenAI does not require a login, they can refuse to allow anyone to use ChatGPT if they want. But assuming they have not blocked a particular user, is a user bound to an Al company's terms and conditions merely by reason of having used the site?\nMost U.S. courts answer this question in the affirmative. The Restatement of Consumer Contracts takes the rather extreme position that visiting a site itself means accepting whatever terms the site owner establishes. The case law has not gone that far. Most courts require that the consumer be put on reasonable notice of the existence of the terms on the front page of the site, rather than having to search or scroll down to find the link to the terms, and that the link to the terms be conspicuous. That is not a very serious requirement, and one of us has argued elsewhere that it doesn't reflect a contract in any traditional sense and permits a great deal of"}, {"title": "B. Antitrust and Unfair Competition", "content": "Even if terms of use are enforceable contracts in general terms, some of the specific terms may not be enforceable for public policy reasons. That is particularly true of attempts to preclude competitive uses of AI systems.\nOne could argue that an Al company is engaging in anticompetitive conduct like unilateral refusal to deal or denial of essential facilities when it denies access to its content or prevents scraping or black-boxing of its data. But it is very hard to show that a facility is essential so that antitrust law requires access. Such claims have been made to no avail before in the context of data scraping. In HiQ Labs, Inc. v. LinkedIn Corp., the district court denied similar claims. In that case HiQ, a data analytics company, scraped LinkedIn public profiles to build their analytics product. LinkedIn issued a cease-and-desist and HiQ sought injunctive relief making a number of antitrust claims (among others). In examining the alleged anticompetitive conduct, the district court rejected both the unilateral refusal to deal and essential facilities claims as implausible. Notably, however, the court did enjoin LinkedIn from continuing to block access to its servers on other legal grounds, making the antitrust case for compelling access rather less important. Others have tried (and failed) to assert antitrust claims in support of scraping.\nAntitrust or public policy challenges to more specific contractual terms may prove more promising, however. While companies generally have wide freedom to decide with whom they want to deal, conditioning dealing on an agreement not compete, as some of the generative AI companies do, is significantly more problematic.\nIf companies were to bring a copyright claim based on violation of the terms (rather than a breach of contract claim), anticompetitive terms would"}, {"title": "C. Copyright Preemption", "content": "Our analysis in Part II suggests that model creators would be hard-pressed to bring a copyright infringement suit against terms of use violators. Instead, they might rely on contractual claims to compensate for the dearth of copyright protections. But copyright may well preempt such claims, meaning that model creators cannot create rights through contracts where none exist.\nIn the past, courts have been less amenable to copyright preemption arguments in many contract disputes, treating contracts differently than other state tort claims. However, as we will discuss, in the last few years this trend has begun to reverse, particularly where\u2014as here\u2014the point of the contract claim is to create a copyright-like right where none exists. This expansion of preemption doctrine creates practical risks to companies relying on terms of service for protecting their assets\u2014at least in the Sixth, Second, and perhaps Ninth Circuit.\nCopyright preemption can be divided into two types of preemption:"}, {"title": "1. Current State of Copyright Express Preemption", "content": "Section 106 of the Copyright Act grants the \u201cthe owner of [a] copyright... the exclusive right[] to reproduce the copyrighted work.\u201d Section 301(a) of the Copyright Act preempts state-law rights that \u201care equivalent to any of the exclusive rights within the general scope of copyright as specified by section 106.\" Courts determining whether a state law claim is preempted by Section 301(a) must then compare the state-law rights against the rights protected by Section 106 of the Copyright Act. This is referred to as \u201cexpress preemption.\"\nThe application of copyright express preemption doctrine has been far from uniform across federal circuits, leading to a complex and often contradictory body of case law. Some have described this area of law as \u201cdestined for the Supreme Court\u201d to resolve a circuit split. There are, however, some commonalities.\nMost courts, in determining whether state law is preempted by Section"}, {"title": "2. Copyright Conflict Preemption", "content": "Courts and plaintiffs have focused primarily on copyright express preemption. But it is not the only form of preemption. Copyright law also preempts state laws that conflict with it\u2014that directly contradict a federal copyright rule or stand as an obstacle to the purposes of the copyright"}, {"title": "3. Preemption of Generative AI Terms", "content": "Model creators looking to enforce their terms of use around models and their outputs will inevitably face both express and conflict preemption claims. The success of these claims will vary depending on the terms being enforced, so we will analyze each separately.\nRestrictions on competition. Contract-based constraints on competition are most likely to be preempted. These terms would prevent potential competitors from using model outputs to train a competing model from the outputs of an AI product, like ChatGPT.\nIn technical parlance, this can sometimes be referred to as \"knowledge distillation,\u201d or in the adversarial setting \u201cmodel stealing.\" This involves large-scale scraping of diverse input-output pairs from the target model. Then, using the acquired dataset of model behaviors, a dataset can be"}, {"title": "D. The Reach of Contract Law", "content": "Even if contract provisions restricting competition or certain uses of Al outputs or model weights are enforceable as a matter of contract law, despite the contract, copyright, and antitrust limits, that enforcement may be ineffective as a practical matter in two important cases.\nThird Party Uses of Model Outputs. Contracts law binds only the parties to the contract, not third parties who did not agree to it. That means that even if they are bound not to make certain uses of the data, others who get the data from them are not bound to the same restriction. IP rights operate against the world, but contracts don't.\nResearchers regularly re-upload models or derivatives under different licenses (even if they shouldn't) or host swaths of scraped outputs from frontier models. Data annotators and platform users silently upload model outputs to datasets without anyone knowing it.\nAnother model creator encountering one of these datasets or models has not entered into an agreement with the original model creator. Courts disfavor restrictive covenants that control downstream uses of things you don't own. The circumstances in which courts have approved \u201ccovenants running with the software\u201d generally involve contract terms attached to a specific thing (a chattel or a piece of software) that travel with that thing and pop up to bind downstream users. There is no such provision for"}, {"title": "IV. TORTS AND OTHER STATUTORY CLAIMS", "content": "Model creators might set aside contract-based claims and look to enforce their terms via other means. In particular, they may argue that violators of their terms are subject to DMCA, CFAA, or other tort claims."}, {"title": "A. DMCA", "content": "Section 1201 of the Digital Millennium Copyright Act (DMCA) provides copyright owners certain rights against the manufacture and use of anticircumvention technologies designed to bypass restrictions on access or use of copyrighted works.\nThe DMCA's provisions are extremely complicated, and not all the details need concern us here. For our purposes, it is worth noting that the main provisions, in 1201(a), pertain to the making or use of technologies to bypass access controls, allowing users to access copyrighted content that is protected by encryption, a password, or other technological restrictions. \u00a7 1201(a) both restricts the making and use of devices whose predominant purpose is to bypass access controls, and prevents the actual circumvention of access controls. Those provisions are unlikely to apply to the output of generative AI, which is freely shared with users, nor to model weights and other information shared on an open-access basis, because no one needs to circumvent an access control to get access to that information.\nSection 1201(b) of the DMCA does extend protection not just to access controls but to technological measures that meter or control use of a copyrighted work. But that section, unlike the section on access controls, prohibits only the making of devices with a predominant purpose of circumventing rights protection controls, not the act of actual circumvention of those controls. Further, the controls in question must \u201ceffectively protect"}, {"title": "B. Computer Fraud and Abuse Act (CFAA)", "content": "The Computer Fraud and Abuse Act (CFAA) is a criminal statute designed to prevent computer hacking. It punishes not just those who accessed a computer unlawfully, but also those who \u201cexceed authorized access\u201d to a networked computer. For years, many courts read \u201cexceed authorized access\u201d quite broadly, encompassing not just bypassing password protections but using lawful access to do anything the owner of the computer had forbidden. The result was that for many years the CFAA has had a second career as a sort of super-charged civil tort claim levied against anyone who violated the terms of service of a website, since while they (like the public at large) had authorized access to the site they had \u201cexceeded\u201d the scope of that access by doing something the terms of service told them not to do.\nFortunately, the Supreme Court significantly narrowed the reach of the"}, {"title": "C. Trespass to Chattels", "content": "A final tort doctrine that has been applied to access to websites is the common law doctrine of trespass to chattels. In the 1990s, some courts suggested that accessing a public website in a way that caused economic harm to the site owner (most notably by scraping content) could be a trespass to the computers operating that website. Reverse-engineering a public AI like"}, {"title": "D. Other Tort Claims", "content": "Nor are other tort claims like unfair competition or unjust enrichment likely to avoid this result. Copyright law broadly preempts tort claims that are equivalent to copyright or that are based on acts of copying of content. In Best Carpet Values, Inc v. Google, for example, the Ninth Circuit reversed"}, {"title": "V. IMPLICATIONS AND RECOMMENDATIONS", "content": "Where does this leave us? Are legal restrictions on the use of AI a mirage? If so, is that a good thing or a bad thing? In this Part, we:\nsummarize the likely outcome of efforts to enforce Al restrictions on use efforts that the companies have so far not engaged in\u2014 and note the policy implications of our conclusions (Section IV.A);\nexamine the practical considerations for companies drafting and asserting terms of service (Section IV.B);"}, {"title": "A. Summarizing Legal Enforceability & Policy Implications", "content": "Our analysis suggests that terms of use for model outputs and model weights are likely to face an uphill battle for legal enforceability. Copyright claims will fall by the wayside, since there is nothing to copyright. Tort claims are similarly unlikely to prevail, both on their merits and because of copyright preemption.\nThat leaves contract. Contract claims too may turn out to be preempted by copyright law. Even if they are not, there are significant limits on their enforceability, both in general and as to specific terms actually included in Al terms of service. And it is not obvious what remedies AI companies could seek for violations of those terms.\nThe noncompete provisions are the least likely to survive. These constraints, particularly on use of model outputs, are the closest to creating a quasi-copyright via contracts and are therefore the most likely to be preempted. There are also strong public policy reasons to prevent enforcement of such terms. They are anticompetitive on their face; noncompete agreements among horizontal competitors are commonly held to violate the antitrust law. Even beyond antitrust, courts can and do refuse to enforce contracts that violate public policy considerations. And \u201cdon't compete with us if you use our uncopyrightable information on a site open to the public\""}]}