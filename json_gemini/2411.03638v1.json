{"title": "Adaptive Stereo Depth Estimation with Multi-Spectral Images Across All Lighting Conditions", "authors": ["Zihan Qin", "Jialei Xu", "Wenbo Zhao", "Junjun Jiang", "Xianming Liu"], "abstract": "Depth estimation under adverse conditions remains a significant challenge. Recently, multi-spectral depth estimation, which integrates both visible light and thermal images, has shown promise in addressing this issue. However, existing algorithms struggle with precise pixel-level feature matching, limiting their ability to fully exploit geometric constraints across different spectra. To address this, we propose a novel framework incorporating stereo depth estimation to enforce accurate geometric constraints. In particular, we treat the visible light and thermal images as a stereo pair and utilize a Cross-modal Feature Matching (CFM) Module to construct a cost volume for pixel-level matching. To mitigate the effects of poor lighting on stereo matching, we introduce Degradation Masking, which leverages robust monocular thermal depth estimation in degraded regions. Our method achieves state-of-the-art (SOTA) performance on the Multi-Spectral Stereo (MS2) dataset, with qualitative evaluations demonstrating high-quality depth maps under varying lighting conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "Depth estimation has received considerable attention in recent years due to its widespread applications in areas such as autonomous driving [1], [2], robotics [3], and 3D reconstruction [4]. Significant advancements have been made in both monocular and stereo depth estimation through deep learning-based approaches. However, these depth estimation algorithms predominantly rely on the visible domain. Consequently, their performance often suffers significant degradation due to the decline in image quality, particularly under poor illumination conditions such as nighttime or rainy weather [5], which prevented these algorithms from being widely applied in real-world scenarios.\nTo address this challenge, recent research has increasingly investigated alternative vision modalities such as near-infrared images [6], [7], and long-wave infrared (also known as thermal) images [5], [8], [9] to achieve reliable and robust depth estimation in adverse conditions. Among these alternative modalities, thermal images have gained more popularity due to their low acquisition cost, robustness in adverse conditions, and consistent performance regardless of lighting variations. Some researchers have attempt to achieve depth estimation with only thermal images [5], [8]. However, thermal images typically exhibit lower texture information, resolution, and higher noise levels compared to visible light images, resulting in less accurate depth estimation in well-lit areas.\nGiven that thermal and visible light depth estimation each have distinct strengths and weaknesses, researchers have focused on integrating thermal images with visible light images to leverage complementary information from both modalities. However, the significant differences between thermal and visible images present challenges in exploiting correlations across modalities. The substantial appearance differences, pacrtiularly the lack of texture in thermal images, complicate keypoint matching. Additionally, low illumination can obscure objects in visible light images, leading to mismatches with thermal images. Consequently, previous multi-spectral methods [9], [10] generally avoid direct pixel-level matching between images, limiting their ability to leverage geometric constraints. As a result, their performance is highly dependent on the training dataset and exhibits poor generalization.\nTo this end, we propose a novel framework that integrates thermal and visible light images for robust and accurate depth estimation under varying lighting conditions, as shown in Fig. 1. Specifically, we first treat the input visible light and thermal images as a stereo pair and train the Cross-modal Feature Matching (CFM) Module to generate aligned feature vectors for each pixel. This alignment allows us to project visible light features onto thermal features across candidate depths, constructing a cost volume that provides accurate pixel-level matching in well-lit regions. Then we estimate depth probability distributions for both modalities, and introduce a degradation mechanism based on those distributions, which reverts to monocular thermal image depth estimation for regions with adverse conditions. Finally, we employ a Depth Module to generate the final depth map. Our experimental evaluations demonstrate that our"}, {"title": "II. RELATED WORKS", "content": "Monocular Depth Estimation aims to infer scene depth from a single image. It can be approached as either a regression problem or a classification problem. Regression-based methods [11], [12], [13], [14] involve predicting per-pixel depth values using convolutional neural networks. In contrast, classification-based approaches [15], [16], [17] divide depth into discrete intervals and predict probabilities for each pixel, transforming the depth estimation task into a classification problem.\nHowever, monocular depth estimation is an ill-posed problem because a single 2D image can correspond to an infinite number of different 3D scenes. As a result, estimating absolute depth often leads to overfitting on specific datasets, capturing patterns that may not generalize to new environments [18]. Meanwhile, estimating only relative depth provides limited practical value in real-world applications.\nStereo Depth Estimation involves estimating a pixel-wise disparity map from a stereo image pair, which can be used to determine the depth of each pixel in the scene. Learning-based stereo depth estimation methods can roughly be divided into two main categories: the encoder-decoder network with 2D convolution [19], [20], [21], [22] and the cost volume matching with 3D convolution [23], [24], [25], [26]. The former directly outputs a disparity map, while the latter requires feature matching at multiple disparity levels to construct a cost volume, leading to higher computational costs but typically achieving greater accuracy.\nCompared to monocular depth estimation, stereo depth estimation benefits from the geometric constraints between two viewpoints, resulting in significantly improved accuracy. However, stereo depth estimation faces challenges in handling occlusions, textureless areas, and reflective surfaces.\nMulti-view Stereo (MVS) estimates the dense depth map from overlapping images. Yao et al. [27] extract deep visual features from the input images, followed by the construction of a 3D cost volume via differentiable homography warping.\nSubsequent works have largely adopted these steps. Yao et al. [28] replaces the 3D CNN used in cost volume regularization with GRUs to reduce memory consumption, and similarly, Xu et al. [29] utilize a non-local RNN to achieve this objective. Yang et al. [30] have explored the use of feature pyramid networks to extract multi-scale features, enabling a coarse-to-fine construction of the cost volume. Bae et al. [31] integrate monocular depth estimation with Multi-view Stereo techniques, using a depth consistency constraint to ensure alignment between the cost volume and monocular depth, effectively addressing occlusion and textureless surface challenges.\nUnlike stereo matching methods, MVS methods do not require stereo rectification of input images, making its approach to constructing the cost volume more generalizable."}, {"title": "B. Multi-spectral Depth Estimation", "content": "Due to the decreased accuracy of depth estimation methods based on visible light under poor lighting conditions, researchers have increasingly turned to multi-spectral images for depth estimation. Treible et al. [32] attempted pixel-level matching between thermal and visible light images but were unable to obtain effective disparity and depth maps because of significant distribution differences between the modalities. Shin et al. [10] employed an Adversarial Multi-spectral Adaptation method, using visible light images as auxiliary supervision to estimate the depth of thermal images. Kim et al. [33] and Lu et al. [8] utilized specialized hardware to create accurately aligned visible-thermal image pairs for training, yet they still faced limitations in achieving accurate depth estimation due to a lack of geometric constraints. Guo et al. [9] employed cross-spectrum spatial consistency between visible light and thermal images for self-supervised learning. Lastly, Shin et al. [5] proposed a conditional random field block to estimate depth from thermal image pairs. However, due to the inherent limitations of thermal images, their method performs worse than visible light methods under favorable lighting conditions.\nCompared to existing multi-spectral depth estimation methods, our approach effectively capitalizes on the complementary strengths of thermal and visible light images. In areas with favorable illumination, we employ geometric constraints derived from stereo vision to achieve precise depth estimation. In contrast, for regions with insufficient illumination, our method transitions to a monocular depth estimation approach based on thermal images. This adaptive mechanism ensures robust and accurate depth estimation across varying lighting conditions, addressing the limitations inherent in prior methodologies."}, {"title": "III. METHOD", "content": "Given a pair of calibrated visible light and thermal images, Ivis and Ithr, with their respective intrinsics, rotations and translations {Kvis, Rvis, tvis} and {Kthr, Rthr, thr}, our goal is to find a accurate depth estimation from the two images by leveraging their complementary information. The problem"}, {"title": "B. Cross-modal Feature Matching", "content": "We leverage Multi-view Stereo (MVS) methods to construct a cost volume for feature matching across different views. Initially, we generate feature vectors for each pixel in both views using the CFM Module based on the PSMNet backbone [23]. Considering the different modalities of input images, their features do not reside in the same feature space, which hinders subsequent feature matching. To address this, we introduce a Cross-Attention Module to align the feature spaces. This module consists of two cross-attention layers. In the first layer, visible light features are used as queries and thermal features as keys, while in the second layer, the roles are reversed, with thermal features as queries and visible light features as keys:\nf_{aligned} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d}} \\right) f_{origin},\nwhere Q and K are the query vector and key vector, respectively, d is the dimensionality of the key vector.\nThese aligned feature vectors are then projected across views by utilizing the intrinsic Kthr, Kvis and extrinsic parameters [Rvis, tvis], [Rthr, thr] to compute the matching pixels. The similarity between features of corresponding pixels is subsequently calculated to construct the cost volume. Specifically, for each pixel (u,v) in Ithr, we find its corresponding pixel (u', v') in Ivis and select a set of uniformly sampled depth candidates {dk}_1. For each depth candidate dk, the matching score for depth candidate dk is calculated by taking the dot product of the features from both views:\nC(u, v, d_k) = f_{thr}(u, v) \\cdot f_{vis} (u', v',dk),"}, {"title": "C. Degradation Masking", "content": "The Multi-view Stereo matching can provide accurate matching in well-lit regions. However, extracting visible light information in regions with adverse conditions is challenging, which can lead to unreliable matches. To address this problem, we propose a novel strategy, namely degradation masking, to remove inaccurate matches from the cost volume, and degrade them to monocular thermal depth estimation.\nSpecifically, we firstly identify the regions of low reliability within the visible light modality. To achieve this goal, we compute the depth probability distribution of Ivis, as the probability can be a good representation of reliability. Here we employ the D-Net in MaGNet [31] as the MDP model to predict the depth value duv for each pixel (u, v) in Ivis, and model their probability (P) as a Gaussian distribution to capture the depth uncertainty:\nP(d_{uv}) = \\frac{1}{\\sqrt{2\\pi\\sigma_v}} exp \\left( -\\frac{(d_{uv} - \\mu_{uv})^2}{2\\sigma^2_{uv}} \\right),\nwhere \u03bcu\u03c5 represents the mean depth, and \u03c32 denotes the variance at pixel (u, v).\nSince the low probability indicates that the corresponding pixel is more likely to be mismatch. We can simply remove the depth candidate corresponding to low probability to achieve degradation masking. This is achieved by exclude the depth candidate dk for that pixel if Pvis(dk|(u,v)) is below a certain threshold \u03b8(u,v), which is computed by:\n\\theta_{(u,v)} = \\mu_{uv} + k * \\sigma_{uv},\nwhere k is a hyperparameter. We have found that setting k = 1 yields satisfactory results."}, {"title": "D. Depth Map Generation", "content": "To degrade the mismatch of poorly lit regions to monocular thermal depth estimation, we utilize an additional MDP model to predict the depth of Ithr. Features extracted from the final layer of this MDP module are concatenated with the masked cost volume. As discussed in [31], [23], maintaining the size of depth estimation and cost volume at (H/4, W/4) ensures both computational efficiency and accuracy. To generate the depth map and recover the final depth map at full resolution, we apply our proposed Depth Module, which incorporates the learnable upsampling method introduced by Bae et al. [31]:\n[\\mu_d, \\sigma^2_d] = \\text{DepthModule}(\\text{Concat}(C, F_{thermal})),"}, {"title": "E. Training Details", "content": "We divide the training process of the whole network into three stages: CFM Module training, MDP Module training, and Depth Module training. The order of training the CFM and MDP Modules can be interchanged.\nIn the training of the CFM Module, we multiply the cost volume output by the depth candidates to obtain the expected depth values. The L1 loss is then computed between these expected depth values and the ground truth:\nL_{MS} = \\sum_{u}^{W} \\sum_{v}^{H} \\sum_{k=1}^{N} d_k \\cdot p(d) - d_{gt}^{u} \\,\nwhere duv represents the depth value at pixel (u, v), \u0397 and W represent the height and width of the image, respectively.\nIn the training of the MDP Module, we use the encoder pre-trained on the KITTI dataset [34] from AdaBins [15], and employ the Negative Log-Likelihood (NLL) loss to optimize the mean and variance for each modality separately.\nL_{NLL} = \\sum_{u}^{W} \\sum_{v}^{H} \\frac{(d_{gt} - \\mu_{uv} (I_t))^2}{2\\sigma^2_{uv} (I_t)} + \\frac{1}{2} log(\\sigma^2_{uv}(I_t))\\\nwhere \u03bcu\u03c5 denotes the predicted mean depth, and \u03c3 indicates the variance. Since there are two MDP Modules, we compute two loss LNLSL, LNLTHR for Ivis and Ithr, respectively.\nIn the training of Depth Module, the weights of the MDP Module and CFM Module are frozen. The training is conducted using the same NLL loss function in Eq. 9 as LNELL. The final depth map Dmap is obtained as the mean \u03bc derived from this process."}, {"title": "IV. EXPERIMENTS", "content": "We implement our network with PyTorch [37] and conduct training on two NVIDIA RTX 4090 GPUs. We use AdamW optimizer [38] and schedule the learning rate using 1cycle policy [39] with Irmax 3.57 \u00d7 10-5 across all three modules. The batch size is 16/8/4 for MDP Module, CFM Module and Depth Module respectively. The number of"}, {"title": "D. Ablation", "content": "Ablation study on Cross-Modal Feature Matching: To investigate the importance of the CFM Module, we train two independent MDP Modules for visible light and thermal images, separately, and compare their performance with the full pipeline model.\nIn the objective comparison results are shown in Table I.\nWe leverage the standard evaluation protocol from [15], [16], [5] to validate the efficacy of the proposed method in experiments, i.e., relative absolute error (Abs Rel), relative squared error (Sq Rel), root mean squared error (RMSE), root mean squared logarithmic error (RMSE log) and threshold accuracy (\u03b4 < 1.25, \u03b4 < 1.252, \u03b4 < 1.253). Due to improvements in multi-modal information fusion and dual-view geometric constraints, our method achieves state-of-the-art performance on the MS2 dataset for most metrics, demonstrating a significant advantage in the Abs Rel metric. This is attributed to the successful integration of information across different modalities, which substantially reduces depth estimation errors. However, thermal images are significantly impacted by temperature variations, leading to degraded imaging quality in rainy conditions and resulting in some performance loss for our method.\nThe results of the subjective comparison are shown in Fig. 3. Since DETI [5] is not open-sourced, we present a comparison of subjective results using Adabins [15], trained on visible light or thermal images. It can be seen that the methods based on the visible light modality can provide accurate depth estimation when sufficient light is guaranteed. However, when lighting conditions deteriorate (e.g., at night or in rainy weather), their performance degrades rapidly. In contrast, methods based on thermal images, while maintaining relatively high visibility under adverse light conditions, face limitations due to their inherently lower resolution and contrast. This results in the generation of less detailed depth maps. Additionally, the dependence of thermal images on temperature means that their imaging quality can be further compromised in scenarios such as rainy weather, where temperature variations may affect the thermal signature. Our method integrates detailed information from visible light images with the enhanced visibility of thermal images in low-light scenarios, enabling it to generate accurate and robust depth maps across a wide range of lighting conditions.\nepoch is 5 for all three modules. The input raw thermal image is transformed according to (10):\nTCelsius = log \\frac{B}{R_{Raw}-O} +F - 273.15,\nwhere R = 380747, \u0412 = 1428, F = 1, and O= -88.539.\nobserved that our approach effectively leverages the advantages of both modalities, utilizing the geometric properties between the two views to further improve the accuracy of depth estimation. Additionally, as shown in Fig. 1, the ablation results demonstrate that without the CFM Module, the model's ability to capture fine-grained spatial details is notably diminished, highlighting the importance of cross-modal interaction for achieving superior depth perception.\nAblation study on Degradation Masking: We conduct an ablation study to evaluate the effect of Degradation Masking. Specifically, we retrain a network using only the CFM Module and Depth Module and compare its performance with our full model. The results can be found in Table III. Our model significantly outperforms the retrained network across all metrics, demonstrating the effectiveness of the proposed Degradation Masking."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel framework that integrates thermal and visible light images to produce accurate and robust depth maps across various lighting conditions. Specifically, we introduce Cross-Modal Feature Matching to bridge the gap between thermal and visible light images in depth estimation. Additionally, we present Degradation Masking to handle regions where matching fails due to insufficient lighting or texture loss in monocular thermal depth estimation. Our method achieves state-of-the-art performance on the MS2 [5] dataset. Ablation studies demonstrate that both the cross-modal matching mechanism and the degradation masking significantly enhance the precision and robustness of the algorithm."}]}