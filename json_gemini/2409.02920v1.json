{"title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)", "authors": ["Yao Mu", "Tianxing Chen", "Shijia Peng", "Zanxin Chen", "Zeyu Gao", "Yude Zou", "Lunkai Lin", "Zhiqiang Xie", "Ping Luo"], "abstract": "Effective collaboration of dual-arm robots and their tool use capabilities are increasingly important areas in the advancement of robotics. These skills play a significant role in expanding robots' ability to operate in diverse real-world environments. However, progress is impeded by the scarcity of specialized training data. This paper introduces RoboTwin, a novel benchmark dataset combining real-world teleoperated data with synthetic data from digital twins, designed for dual-arm robotic scenarios. Using the COBOT Magic platform, we have collected diverse data on tool usage and human-robot interaction. We present a innovative approach to creating digital twins using AI-generated content, transforming 2D images into detailed 3D models. Furthermore, we utilize large language models to generate expert-level training data and task-specific pose sequences oriented toward functionality. Our key contributions are: 1) the RoboTwin benchmark dataset, 2) an efficient real-to-simulation pipeline, and 3) the use of language models for automatic expert-level data generation. These advancements are designed to address the shortage of robotic training data, potentially accelerating the development of more capable and versatile robotic systems for a wide range of real-world applications.", "sections": [{"title": "1 Introduction", "content": "In the fast-evolving robotics field, the integration of dual-arm coordination and advanced tool use is crucial for developing sophisticated autonomous systems."}, {"title": "2 Related Work", "content": "To enhance the collection of effective demonstrations for robotic tasks, human teleoperation has traditionally been employed. In this method, a human operator manually guides a robot through various tasks [10, 14, 22, 25, 26, 34]. Recent advancements have extended this methodology by employing teams of human operators over prolonged periods to assemble substantial real-world datasets [3,5,10, 14]. An alternative method involves the use of algorithmic trajectory generators within simulations [9, 11, 13, 16,33], which, while efficient, often depend on privileged information and hand-designed heuristics, making them labor-intensive for arbitrary tasks. However, current systems often fail to produce high-fidelity expert simulation data that accurately mimics data from actual machine op- erations. Although initiatives like MimicGen [23] and RoboCaca [27] strive to generate simulated expert data using limited human demonstrations, they still heavily rely on predefined scenes and interactive objects. To overcome these limitations, we introduce RoboTwin. This innovative system not only generates expert data and simulation scenes derived from real-world scenarios but also utilizes large language models (LLMs) to generate demonstration codes and expert data for similar tasks involving the same class of objects. This strategy"}, {"title": "2.2 Robot Manipulation Learning Methods", "content": "The adoption of human demonstrations to instruct robots in manipulation skills is a prevalent method in Robot Manipulation Learning [4, 6, 15, 21, 29]. Among the techniques, Behavioral Cloning stands out for learning policies offline from these demonstrations. It replicates observed actions from a curated dataset [5, 9, 10, 14, 16, 24, 28, 34]. Conversely, Offline Reinforcement Learning enhances policy learning by optimizing actions based on a predefined reward function and exploiting large datasets [7, 12, 17-20]. The Action Chunking with Transform- ers (ACT) technique integrates a Transformer-based visuomotor policy with a conditional variational autoencoder to structure the learning of action se- quences [30, 31, 35]. Recently, the Diffusion Policy method has gained promi- nence. It employs a conditional denoising diffusion process for visuomotor policy representation, effectively reducing the accumulative error in trajectory genera- tion that is often observed in Transformer-based visuomotor policies [8]. The 3D Diffusion Policy [32] uses point clouds for environmental observations, enhanc- ing spatial information utilization and managing various robotic tasks in both simulated and real environments with only a small number of demonstrations."}, {"title": "3 Real-to-sim transfer of the scene", "content": "To synthesize high-fidelity data through simulation, a major challenge is the creation of accurate and cost-effective digital twins. Traditional methods often depend on costly high-precision sensors, which can hinder widespread adoption. In response, we have developed a more economical approach using Artificial In- telligence Generated Content (AIGC) to construct 3D models from simple 2D RGB images powered by Deemos's Rodin platform. This technique significantly reduces the reliance on expensive sensors while achieving realistic visual effects and supporting physical simulations. Our innovative pipeline commences with generating a detailed 3D mesh and texture of the target object involved in a robot's task, created from a single real-world image. This capability ensures a high-fidelity recreation of real-world scenarios within a simulated environment. The process begins by transforming a single 2D image into a 3D model that en- compasses detailed geometry, surface normals, wireframes, and textures. These features enhance the visual realism and ensure compatibility with physics engines for simulations. Once the 3D model is ready, we assign specific coordinate axes to functional parts of objects within the model."}, {"title": "3.2 Expert Data Generation", "content": "We leverage the reasoning capabilities of GPT4-V [1] to write code that cal- culates the relationships between key poses and the functional coordinate axes of objects. GPT4-V analyzes task requirements and generates a sequence of poses that align with these requirements, ensuring precise task execution. We also generate code via GPT4 [2] to invoke trajectory planning tools based on the computed poses. This automation substantially decreases the time and la- bor associated with manual programming, facilitating the swift deployment of robotic systems across diverse applications. It also offers a scalable approach for generating high-quality data essential for robotic learning."}, {"title": "4 Benchmark", "content": "To further research and development in this area, as shown in Fig. 4, we intro- duce a comprehensive benchmark specifically designed to assess dual-arm robots in a variety of scenarios. This benchmark encompasses a diverse set of tasks, each presenting unique challenges that are critical for assessing the dexterity, coordination, and operational efficiency of robotic arms in a simulated environ- ment. The tasks range from simple object manipulation to complex, coordinated actions requiring synchronized movements of both arms. Appendix A.2 outlines the specific tasks and their descriptions, providing a clear framework for com- parative analysis and further development of advanced robotic capabilities. For each task, we provide a robust API that supports the generation of expert data across infinitely variable scenarios, such as different object placements and en- vironmental conditions. This feature allows researchers to extensively test and refine the adaptability and precision of robotic systems under controlled yet var- ied conditions. Additionally, an offline dataset is available for each task, offering pre-generated expert data to facilitate offline training and benchmarking of al- gorithms. This benchmark aims to bridge the gap between theoretical robotic control models and their practical implementation, ensuring that the robotic systems can perform reliably in dynamic, real-world environments."}, {"title": "5 Real-world Dataset", "content": "For the acquisition of real-world data, we employed the open-source Cobot Magic 7 platform from AgileX Robotics, which is equipped with four AgileX Arms and four Intel Realsense D-435 RGBD cameras and is built on the Tracer chassis. These cameras are strategically positioned: one on the high part of the stand for an expansive field of view, two on the wrists of the robot's arms, and one on the low part of the stand which is optional for use. The front, left, and right cameras capture data simultaneously at a frequency of 30Hz, as depicted in Figure 5. The data collection and alignment are facilitated by tools provided by the ARIO Data Alliance, available at our GitHub repository. Each captured frame consists of"}, {"title": "6 Experiment", "content": "Our experimental aim is not to delve into the design choices of different strat- egy networks but to explore the correctness and effectiveness of our Benchmark expert data. Our experiments are intended to verify: a) the rationality of the COBOT Magic platform settings, and b) the effectiveness of the automatically generated expert data.\nWe utilized the 3D Diffusion Policy (DP3) [32] to test six tasks within the benchmark, with each task being tested using strategies trained from 10 sets, 20 sets, and 50 sets of expert data, respectively, to obtain the success rates."}, {"title": "7 Conclusion", "content": "In this study, we introduce RoboTwin, a benchmark integrating real-world and synthetic data to evaluate dual-arm robots, addressing the significant shortage of specialized training data in robotics. Our dataset, developed using the AgileX Robotics platform and enhanced through generative digital twins powered by Deemos's Rodin platform, effectively accelerates the training of robotic systems, enabling performance improvements across diverse tasks. Our results demon- strate the potential of this hybrid data approach to refine robotic dexterity and efficiency, providing a scalable tool that could revolutionize robotic research and applications."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Licensing", "content": "RoboTwin is released under the open-source MIT license."}, {"title": "A.2 Benchmark Task Descriptions", "content": "In this subsection, we present a comprehensive overview of six carefully designed simulation tasks aimed at evaluating the capabilities of robotic systems in various manipulation scenarios. These tasks are crafted to challenge different aspects of dexterous control, coordination between multiple limbs, and the ability to perform complex sequences of actions. Each task is intended to simulate real- world challenges that a robotic system might encounter, thus providing a robust benchmark for assessing the effectiveness of various robotic algorithms. Detailed descriptions of these tasks are provided in Table 2, which outlines the specific objectives and required actions for each scenario."}, {"title": "A.3 Dataset Task Descriptions", "content": "Our dataset comprises 17 distinct, real-world robotic tasks designed to evaluate the dexterity, coordination, and contextual understanding of robotic systems in"}]}