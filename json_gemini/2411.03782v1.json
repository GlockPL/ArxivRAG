{"title": "Navigating the landscape of multimodal AI in medicine: a scoping review on technical challenges and clinical applications", "authors": ["Daan Schouten", "Giulia Nicoletti", "Bas Dille", "Catherine Chia", "Pierpaolo Vendittelli", "Megan Schuurmans", "Geert Litjens", "Nadieh Khalili"], "abstract": "Recent technological advances in healthcare have led to unprecedented growth in patient data quantity and diversity. While artificial intelligence (AI) models have shown promising results in analyzing individual data modalities, there is increasing recognition that models integrating multiple complementary data sources, so-called multimodal AI, could enhance clinical decision-making. This scoping review examines the landscape of deep learning-based multimodal AI applications across the medical domain, analyzing 432 papers published between 2018 and 2024. We provide an extensive overview of multimodal AI development across different medical disciplines, examining various architectural approaches, fusion strategies, and common application areas. Our analysis reveals that multimodal AI models consistently outperform their unimodal counterparts, with an average improvement of 6.2 percentage points in AUC. However, several challenges persist, including cross-departmental coordination, heterogeneous data characteristics, and incomplete datasets. We critically assess the technical and practical challenges in developing multimodal AI systems and discuss potential strategies for their clinical implementation, including a brief overview of commercially available multimodal AI models for clinical decision-making. Additionally, we identify key factors driving multimodal AI development and propose recommendations to accelerate the field's maturation. This review provides researchers and clinicians with a thorough understanding of the current state, challenges, and future directions of multimodal AI in medicine.", "sections": [{"title": "1. Introduction", "content": "The healthcare landscape is evolving rapidly, driven by an increasingly data-centric approach to patient care and decision-making (Shilo et al., 2020). This shift is complemented by the advent of technologies such as digital pathology (Niazi et al., 2019), biosensors (Sempionatto et al., 2022), and next-generation sequencing (Steyaert et al., 2023), which provide clinicians with novel insights in various domains. The data generated by these diverse modalities is generally complementary, with each modality contributing unique information to the status of a patient. Some modalities offer a comprehensive overview at the macro level, while others may provide detailed information at single-cell resolution (Steyaert et al., 2023). In addition to this recent growth in data quantity, there is a concurrent increase in the quality and diversity of available treatment options. Hence, selecting the optimal treatment has become increasingly complex, and a further data-centric approach to treatment selection may be required.\nThe traditional approach to integrating information from different data modalities into a single decision is represented by multidisciplinary boards, where each specialized clinician offers their perspective on a given modality or piece of information in pursuit of consensus (Mano et al., 2022). Although establishing these boards has improved disease assessments and patient management plans (Mano et al., 2022), there is a foreseeable limit to the scalability of these boards. If data quantity and diversity continue to rise, many domain experts will be required to integrate these different information streams effectively. Fortunately, another technological advancement that is gaining a foothold in healthcare is artificial intelligence (AI). Although the vast majority of published work focuses on single modality applications of AI, several authors have highlighted the potential of AI systems to combine multiple streams of information, so-called multimodal AI, for decision-making (Steyaert et al., 2023; Acosta et al., 2022; Lipkova et al., 2022). These multimodal AI models are trained to process different streams of multimodal data effectively, leverage the complementary nature of information, and make an informed prediction based on a broader context of the patient's status. However, despite these promising results, studies investigating multimodal Al models are comparatively scarce, and the development of unimodal models remains the de facto standard.\nThis lagging development of multimodal AI models can be attributed to several challenges. First, a practical challenge can be found in the cross-departmental nature of multimodal AI development. As different data modalities may originate from various medical departments, consulting different medical domain experts will likely be required for effective data integration. In addition, medical departments may have varying experience in data storage, retrieval, and processing, limiting the possibilities of multimodal AI development. For example, if a radiology department has a fully digital workflow while the corresponding pathology department does not, this effectively prohibits multimodal AI endeavors where whole slide images would be combined with radiological imaging data.\nDifferent data modalities can have vastly different characteristics, such as dimensionality or color space, which generally requires different AI model architectures tailored towards those modalities, increasing model design complexity. For example, convolutional neural networks (CNN) were initially proposed for structured data, such as 2D and 3D images, but can't straightforwardly be applied to unstructured data. Conversely, transformers are solid, flexible encoders for various data modalities. Still, whether a one-size-fits-all architecture can capture various medical data modalities effectively remains unclear. In practice, multimodal data integration is commonly achieved using different (intermediate) model outputs. Training multiple domain-specific AI models (i.e., encoders) and efficiently integrating these in a single prediction poses a challenge unique to multimodal AI development.\nLast, the inconsistent availability of all modalities for each patient within a multimodal dataset adds complexity. Patients with different disease trajectories will have various available modalities, leading to partially incomplete datasets. This can substantially reduce the adequate training dataset size for AI models that require complete multimodal data to generate predictions. Moreover, these issues also translate to implementation. If modalities are missing, it may be unclear how this impacts the model's performance from the perspective of fewer available data to base a decision on and the potential introduction of population selection bias (Acosta et al., 2022). In short, developing multimodal AI models poses several novel challenges compared to unimodal AI development.\nEven given these challenges, several works have been done in the past on multimodal AI applications, typically involving handcrafted features. A key issue with these approaches was that the difficulties requiring particular domain expertise are multiplied, as expert clinicians would also need to be involved in the feature design phase (Vaidya et al., 2020; Tortora et al., 2023). An excellent overview was published by Kline et al. (2022), indicating that these models obtained a 6.4% mean improvement in AUC compared to their unimodal counterparts.\nRecent years have shown an accelerated interest in multimodal AI development for medical tasks (Salvi"}, {"title": "2. Search Criteria", "content": "This scoping review aimed to evaluate the application of multimodal AI models in the medical field, where we define multimodality as data originating from different medical specialties. For instance, we consider a model to be multimodal when it integrates a diagnostic CT scan and subsequent tissue biopsy slides, as it combines the domains of the radiologist and the pathologist, respectively. Conversely, a model integrating T1- and T2-weighted MRI scans would not be considered multimodal. In addition to this multimodality criterion, we limited our scope to I) studies using deep neural networks and II) studies developing multimodal models for specific medical tasks (i.e., no generic visual question answering).\nThe literature search was conducted in PubMed, Web of Science, Cochrane, and Embase. The entire search string per database can be found in the supplementary materials. The search was initially performed on April 16, 2024 and repeated on October 2, 2024 to ensure an up-to-date overview, yielding a total of 12856 initial results. These results included both journal and conference papers. In addition to the previously mentioned inclusion criteria, we applied additional standard exclusion criteria summarized in Figure 1. Specifically, we excluded review articles, non-English publications, articles without full text, non-peer-reviewed preprints, and those published before 2018. After filtering with these criteria and deduplicating results using the DedupEndNote tool (Lobbestael, 2023), 10522 articles were imported into the paper screening software Rayyan (Ouzzani et al., 2016) for the Title and Abstract (TiAb) screening phase. A team of six reviewers conducted the TiAb screening phase. Each paper's title and abstract were reviewed based on the inclusion criteria. Included papers were verified by a second reviewer, and potential discrepancies were resolved through discussion or the involvement of a third reviewer if necessary. The TiAb screening phase was performed sequentially, starting with verifying that the paper is in the medical domain, then determining whether it is in scope, whether multimodality is involved, and finally, assessing whether deep neural networks are used. Papers were deemed to be outside the scope of this review when they investigated tasks that do not have an explicit clinical question (i.e., image denoising, registration). The TiAb screening resulted in 663 articles being considered for full-text screening.\nIn the full-text reading phase, a single reviewer read each article in full to confirm adherence to our inclusion criteria. In case of doubt, a second reviewer was involved to arrive at a consensus decision. Eventually, 432 studies were included in the final analysis (see the supplementary materials for the entire list of included papers)."}, {"title": "3. Overview of multimodal medical AI", "content": "The landscape of multimodal artificial intelligence (AI) in medical research has expanded between 2018 and 2024, as shown by the growing number of articles focused on integrating multiple modalities. Subdividing the 432 articles in this review per year (Figure 2a), we see a rapid increase, starting with 3 papers in 2018 to 150 papers in 2024 at the end of the data collection of this review.\nWe distributed the reviewed papers according to data modalities, which will be broadly described here."}, {"title": "3.1. Modalities and data types", "content": "We divided the data modalities into image-based and non-image-based modalities. The image-based modalities are grouped according to their related medical specialties. This resulted in the following categories: radiology (computed tomography (CT), magnetic resonance imaging (MRI), ultrasound (US), X-rays, and nuclear imaging (SPECT/PET)), pathology (stained histology images) and 'clinical images' (including optical coherence tomography (OCT), fundus photography, and dermatoscopy, among others). If there were too few papers for a medical specialty, we grouped them into a catch-all 'other images', a subset of 'clinical image' category. Meanwhile, the non-image-based modalities consist of text (structured text such as tabular laboratory results and unstructured text such as free-text medical reports), omics data (e.g., genomics, transcriptomics, or proteomics), and other non-image modalities (such as Electroencephalography (EEG) or Electrocardiography (ECG) signals).\nRadiology and text were the most commonly used modalities (each 30%), followed by omics (12%) and pathology (12%). Figure 2b visualizes all modalities and their respective subtypes. Over the years, the trend of using radiology and text modalities remained the most common (see Figure 2c). The most prevalent combination is radiology with text (n=206), followed by pathology/omics (n=51), clinical images/text (n=33), radiology/omics (n=24), pathology/text (n=22), and radiology/pathology (n=16). These combinations showcase a preference for integrating one imaging modality with structured or unstructured text data. A complete overview is presented in Figure 2d.\nThe review also reveals several complex combinations involving three or more modalities, albeit in smaller numbers. Notably, pathology/text/omics (n=19), radiology/text/omics (n=15), radiology/pathology/text (n=7), and radiology/pathology/omics/text (n=3) demonstrate the attempts to create comprehensive models that span multiple scales of biological organization - from organ-level (radiology) to tissue and cellular (pathology) and subcellular (omics), complemented by clinical context (text). An exhaustive overview of all modality combinations can be found in the supplementary materials."}, {"title": "3.2. Organ systems, medical tasks and AI functions", "content": "We categorized all studies into eleven organ systems (see Figure 3a for the complete list). The nervous system dominates with 122 studies, followed by respiratory (n=93), reproductive (n=43), digestive (n=43), sensory (n=25) and integumentary (n=24). A substantial number of studies (n=15) fall under the miscellaneous category and 27 studies involved multiple organ systems, which are further explained in the dedicated section Clinical applications (section 5).\nSix medical task categories: diagnosis, estimating prognosis (which is further subdivided into survival prediction, disease progression, and treatment response analyses), treatment, and others. Diagnosis emerged as the primary focus, accounting for 45% (multiple systems) to 91% (integumentary system) of the medical tasks across all organ systems (see Figure 3a and b). Survival prediction was the second most common task (18% of all medical tasks). Other tasks, such as prediction of disease progression or treatment response, were not uniformly represented across all organ systems."}, {"title": "4. Methodology", "content": "As stated in the introduction, data availability is a key challenge for the development of multimodal medical AI. This is why we see a strong correlation between the number of models for a specific organ system/modality combination and the availability of public data (see Figure 3c). The utilization of publicly shared datasets in multimodal Al research for medical applications is widespread, with 61% of the data sources used in the model development coming from public data portals such as The Cancer Genome Atlas (TCGA, 14%), Alzheimer's Disease Neuroimaging Initiative (ADNI, 8%), Medical Information Mart for Intensive Care (MIMIC, 5%) and The Cancer Imaging Archive (TCIA, 2%), 15% from data shared publicly through other means (e.g. GitHub, publisher's website), and 24% from private datasets which were not shared publicly. We grouped all other data portals used by less than ten reviewed papers into \"other data portals\" (20%). A detailed breakdown of these public data sources can be viewed in the supplementary materials."}, {"title": "4.2. Feature encoding and modality fusion", "content": "The advent of deep neural networks was a key accelerator for multimodal medical AI. These networks significantly simplified the feature extraction/encoding step for individual modalities. Each modality could now be encoded by its deep neural network, and the resultant features could be combined for downstream tasks. Powerful self-supervised learning techniques such as DINO (Caron et al., 2021) and SimCLR (Chen et al., 2020) have now also enabled the training of these feature encoders without any labels, further increasing the attractiveness of this approach. However, we have seen that there is still significant diversity in approaches across the reviewed papers, which we will detail in the following subsections.\nVarious encoder mechanisms are used for different data modalities. Encoders are categorized into Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), Recurrent Neural Networks (RNNs), Transformers, handcrafted feature encoders, multi-layer perceptrons (MLPs), multiple encoders, and 'other' (e.g. graph neural networks). Our analysis reveals that CNNs dominate the encoder landscape (used in 82% of the studies), followed by 'other' (32%), and MLPs (21%). Unsurprisingly, CNNs show strong correlations with image-based data modalities, including radiology, pathology, and other (miscellaneous) image modalities. In contrast, non-image modalities, such as 'omics and (un)structured text, use a more diverse range of encoders, including handcrafted feature encoders, MLPs, RNNs, and Transformers.\nThe second important design decision is how and when the differing modalities are fused. The fusion stage can vary depending on the model structure, which often dictates the optimal point for integrating information across modalities. A schematic view of these fusion stages is shown in Figure 4.\nOut of all papers reviewed in this study, the vast majority (79%, 341/432 papers) utilized intermediate fusion, in which data sources get fused after feature encoding but before the final layers of the neural network (e.g., the classification or regression head). A common strategy is simply concatenating the feature vectors of the different unimodal modality encoders and feeding the resultant vector to the final layers. This concatenation method was used in most models (69%) using intermediate fusion, and was not limited to certain modality combinations or clinical application areas (Lee et al., 2019; Zhi et al., 2022; Liu et al., 2024). However, several studies found through ablation experiments that applying other methods to fuse the feature vectors (i.e. taking the outer product, Kronecker product or compact bilinear pooling), outperformed concatenation by a notable margin (Yang et al., 2022; Wang et al., 2023b, 2024a). Another common intermediate fusion technique (12%) was the use of attention, where the unimodal embeddings were passed through an attention mechanism to optimally learn from the complementary information in both embeddings (Kayikci and Khoshgoftaar, 2023; Liu et al., 2023; Machado Reyes et al., 2024).\nLate fusion, a method in which fusion is performed by combining results or predictions of unimodal models, is the second most common technique (14%). These architectures can look similar to those from intermediate fusion models, but without intermediate components such as attention mechanisms or mixing layers. However, most of these late fusion approaches combine unimodal models and their individual predictions to get a multimodal result. This was often (32%) achieved by applying a (weighted) average over the predictions for each modality (Ying et al., 2021; Caruso et al., 2022; Jung et al., 2024)) or by training a separate model on top of the unimodal predictions (37%). The latter was mainly done through regression or traditional machine learning models like random forests, boosting algorithms or Cox models (Ma and Jia, 2020; Kolk et al., 2024; Wang et al., 2024b).\nA key aspect of late fusion is that no interaction exists between the modalities in the learning process, meaning that the training data for the respective unimodal models does not have to be paired. It therefore becomes easier to handle missing data in these models, as patients with only one modality can be used to train only the model of that modality without the need to infer the missing ones. On the contrary, the lack of interaction could potentially limit model expressiveness.\nThe last method involves early fusion, at which modalities are fused before feature encoding. Our study found that this fusion method has been applied the least (6%). A key challenge is that the input data has to exist in the same 'space' to allow early fusion. The difficulty beyond this kind of fusion could be influenced by the types of modalities involved in the fusion, for example, combining radiology and pathology images might often require some form of image registration, which is an unsolved challenge for many clinical applications. However, several early fusion methods were implemented that do not require extensive preprocessing steps, ranging from a simple concatenation of the modalities (Shi et al., 2023; Lopez et al., 2020) to more complex representations involving graph networks (Lei et al., 2024), recurrent neural networks (Xu et al., 2022a), and cross-modality representation alignment networks (Wu et al., 2023). Other interesting methods combined image and non-image modalities by directly marking (Pelka et al., 2020) or multiplying (Qiu et al., 2024) clinical variables onto images, thereby enhancing the image data with relevant contextual information. A key potential advantage of early fusion is that modalities are already available in the feature encoding stage, potentially allowing deep neural networks to optimize feature design by leveraging all information simultaneously."}, {"title": "4.3. Network architectures", "content": "The architecture choices for multimodal AI models often rely on the purpose(s) of the model and data availability. Regardless, the critical steps when designing such a model include the same core functionalities in feature extraction, information fusion, and final processing of the fused information. Some papers also introduce explainability (Ketabi et al., 2023; Yang et al., 2021; Parvin et al., 2024) into their models or attempt to construct multiple different model architectures to assess the effectiveness of each variant (Huang et al., 2020; Lopez et al., 2020; Caruso et al., 2022). Examining the usage of individual modality encoders over the years reveals that contemporary feature extraction models like (vision) transformers are gaining popularity, while on the other hand, MLPs have seen a decrease in usage, with CNN usage staying roughly the same in terms of popularity over the last few years. Traditional machine learning methods are also still employed for feature encoding in 5% of all papers, often to extract features from structured text data."}, {"title": "4.4. Handling missing modalities", "content": "Most multimodal AI models work on the assumption of data completeness: all modalities must be available for every entry. However, this is often not feasible due to issues such as data silos caused by incompatible data archival systems (i.e. PACS), a study's retrospective nature, or simply data privacy concerns. Handling missing data without introducing bias into the analysis presents a significant challenge. Indeed, a commonly employed approach to address this issue is to exclude all entries with at least one missing modality and include only complete entries in the dataset used for analysis, and this is the case for 69% of the included papers. Although the exclusion of incomplete entries is a quick and straightforward method for handling missing data, this approach results in a reduction in data sample size with a subsequent loss of potentially relevant samples. Furthermore, this limits the model inference to complete-modality data and inevitably leads to selection biases, thereby potentially compromising validity and generalizability. Rather than discarding incomplete samples, missing information can also be estimated using imputation techniques, thereby alleviating both aforementioned disadvantages of fully excluding incomplete entries. These data imputation techniques can be broadly divided into non-learning-based and learning-based approaches."}, {"title": "4.4.1. Non-learning-based approaches", "content": "The findings of our study indicate that non-learning-based approaches are commonly utilized, occurring in 45% (35/78) of all papers that reported any method to account for missing data. These techniques are primarily employed to impute values of structured data (n=29), such as clinical variables and test results. Less frequently, these methods are used to address missing gene values (n=2) or image modalities (n=2). For continuous variables, imputation is performed using measures of central tendency such as the mean or median, or performing a moving average, which replaces missing values with the average of a specified number of surrounding data points. For categorical variables, imputation often involves using the mode or adding a new category for missing values. Some studies apply fixed values, like zeros, -1, or random values from similar records (hot-deck method). Although these methods are simple and easy to implement, they can introduce bias and reduce data variability (Flores et al., 2023)."}, {"title": "4.4.2. Learning-based approaches", "content": "Some studies leveraged traditional machine learning methods to predict missing values, learning patterns from complete data as an alternative to simpler imputation techniques. Common methods included k-nearest neighbor (k-NN), which imputes missing values based on the values of the nearest neighbors (Ross et al., 2024; Qiu et al., 2022; Lee et al., 2024b), and the weighted nearest neighbor approach (Wu et al., 2023; Kayikci and Khoshgoftaar, 2023; Mustafa et al., 2023; Palmal et al., 2024), which assigns weights to neighbors based on their distance. Other techniques used include linear regressions (Ghafoori et al., 2023), random forests (Yu et al., 2024; Kolk et al., 2024), neural networks (Menegotto et al., 2021) and Classification and Regression Trees (CART) algorithm (Yin et al., 2024), as well as advanced methods like Multivariate Imputation by Chained Equations (MICE) algorithm (Rahman et al., 2023; Liu et al., 2022; Lim et al., 2022; Kim et al., 2024) and XGBoost (Feher et al., 2024; Zambrano Chaves et al., 2023; Fan et al., 2024), which handle missing data by respectively incorporating multiple imputations and tree-based learning during training.\nOther methods applied deep learning models to impute missing values or manage an arbitrary number of modalities, directly modifying the model architecture to process incomplete data without imputation. Some deep-learning-based imputation strategies leverage different generative models, such as auto-encoders (Xu et al., 2022b; Akramifard et al., 2021), or generative adversarial networks (Dolci et al., 2023). Others directly predict the missing data at the output layer (Saad et al., 2022) or at previous visits of a Recurrent Neural Network (Xu et al., 2022a). However, there seems to be little consensus or evidence to suggest that one method should be preferred. Some papers have addressed the missing data challenge by introducing specific drop-out modules to train on or simulate missing data (Cheerla and Gevaert, 2019; Ostertag et al., 2023; Cui et al., 2022b; Liu et al., 2023). Other studies addressed the missing modality problem by designing specific loss functions that take into account only available modalities (Gao et al., 2021; Xue et al., 2024; Kawahara et al., 2019; Nguyen et al., 2024; Taleb et al., 2022), sometimes employing a reconstruction loss to reconstruct them (Cui et al., 2022a). Interestingly, some studies used learnable embeddings as placeholders for missing modalities (Chen et al., 2024) or directly utilized models, such as transformers, capable of handling input sequences of arbitrary lengths (Zhou et al., 2024b), allowing the model to manage cases with missing or incomplete modalities effectively."}, {"title": "4.5. Validation", "content": "An observation on the validation of multimodal AI models is that most studies (82%) are limited to an internal validation scheme. Despite the common use of public datasets, these are often employed as the sole training data rather than as external validation. Another important component in validating multimodal models is the comparison with a unimodal baseline. To provide a broadly representative analysis of the performance gains by multimodal data integration, we equally sampled 3-4 papers per organ system, resulting in a subset of 48 papers that clearly described a multimodal vs unimodal baseline comparison. On average, these multimodal models obtained a 6.2 percentage point increase in AUC, which aligns with the 6.4 percentage points improvement reported previously by Kline et al. (2022). From the 432 papers studied, 72% of the papers mention an improvement over unimodal models in an ablation experiment. In contrast, only 5% of the papers did not find any notable improvement by including multiple modalities, and 22% of papers did not report any comparison with a unimodal baseline. Despite these encouraging findings, it should be noted that these improvements are rarely tested for significance, leading to a potentially optimistic bias."}, {"title": "5. Clinical applications", "content": "Subdividing multimodal AI research papers based on the applied organ system reveals significant variations in the development of multimodal models. The distribution of the papers among the systems is shown in Figure 3a. Interestingly, some studies evaluated their models into more than one system and, therefore, are reported in a separate Multiple Systems category (n=27). Papers that did not align with any category were grouped into a Miscellaneous category (n=15). Below, we summarize the key contributions in each area."}, {"title": "5.1. Nervous system (n=122)", "content": "The predominant focus in the nervous system has been on the diagnosis and disease progression of neurodegenerative disorders, such as Alzheimer's disease (n=47), with a few studies focusing on Parkinson's disease (n=9). The second primary focus is cancer diagnosis and survival prediction (n=22). Other studies focused on the diagnosis of autism (n=6), stroke (n=9), and mental disorders, such as schizophrenia (n=6) and depression (n=4). The analyses of these papers showed that most studies in this area utilize MRI as the primary modality, integrating either clinical data (n=43) or 'omics data (n=18), or all three (n=8). Other studies combined pathology with MRI (n=6) or 'omics (n=5), or CT with clinical variables (n=10).\nA significant number of studies utilized publicly available datasets (n=115), with the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset being the most frequently used (n=44), followed by The Cancer Genome Atlas (TCGA)(n=11), the 'Computational Precision Medicine Radiology-Pathology challenge on brain tumor classification' dataset (CPM-RadPath) (n=6) and Brain Tumor Segmentation Challenge (BraTS) (n=3). Only a limited number of studies conducted external validation of their models (n=15), while the majority employed cross-validation (n=63) and internal validation (n=40). Integrating multiple modalities has enhanced performance in most of these studies (n=83), underscoring the value of incorporating additional data sources that capture information not evident in a single modality alone.\nSome studies stood out for their system design and development, with several focusing on glioma grading, such as Li and Ogino (2020), which proposed a patient-wise feature transfer model that learns the relationship between radiological and pathological images. Notably, their model enables inference using only radiology images while linking the prediction outcomes directly to specific pathological phenotypes. Another interesting example is Cui et al. (2022a), which integrates histological images and genomic data and can handle patients with partial modalities by leveraging a reconstruction loss computed on the available modalities. Interestingly, Cui et al. (2022b) combined four different modalities, i.e., radiology images, pathology images, genomic, and demographic data, to predict glioma tumor survival and reached a c-index of 0.77 on a 15-fold Monte Carlo cross-validation.\nSeveral studies focused on Alzheimer's prediction, including Liu et al. (2023), that is the only study both handling missing data and externally validating its findings, employing multiple transformer architectures to integrate imaging data and clinical variables and reaching an AUC value of 0.984 on an external validation composed of 382 patients. Pelka et al. (2020) proposed a unique type of early fusion where sociodemographic data and genetic data are branded on the MRI scan and used to develop a model able to diagnose early and subclinical stages of Alzheimer's Disease. Lei et al. (2024) developed a novel framework for AD diagnosis that fuses four modalities: genomic, imaging, proteomic, and clinical data, validating the proposed method on the ADNI dataset and outperforming other state-of-the-art multimodal fusion methods.\nThe most extensive study of this category is the one of Xue et al. (2024) that presented an AI model that integrates diverse data, including clinical data, functional evaluations, and multimodal neuroimaging, to identify 10 distinct dementia etiologies, across 51,269 participants from 9 independent datasets, even in the presence of incomplete data."}, {"title": "5.2. Respiratory system (n=93)", "content": "Research in the respiratory system predominantly focused on diagnosis (n=10), survival (n=9), treatment response prediction (n=3), and progression (n=2) of cancer, and on diagnosis (n=19), survival (n=3) and disease progression (n=2) of Covid-19. Across these studies, the common strategy involves combining clinical variables with either X-ray or CT imaging of the thorax (n=18 and n=27, respectively). Additionally, a subset of papers (n=11) developed vision-language models by integrating chest X-rays with clinical reports, specifically for X-ray report generation. The majority of the studies regarding respiratory diseases employed public datasets, such as the MIMIC-CXR dataset (n=9), The Cancer Genome Atlas (TCGA) (n=5), and the National Lung Screening Trial (NLST) dataset (n=3). In this category, many studies performed better than single-modality models when performing multimodal integration (n=69). However, the proportion of studies employing external validation is still limited (n=15).\nAmong the studies of the respiratory category that stood out, Keicher et al. (2023) proposed a multimodal graph-based approach combining imaging and non-imaging information to predict COVID-19 patient outcomes. Thanks to the employed attention mechanism, the model learns to identify the neighbors in the graph that are the most relevant for the prediction task, providing insight into the decision process. Gao et al. (2022) outperformed state-of-the-art models on three different external validation sets in predicting the risk of indeterminate pulmonary nodules with a multimodal approach combining CT imaging and clinical variables, where most studies do not perform external validation. Gao et al. (2021) proposed a 'multi-path multimodal missing' network, a system integrating multimodal data, including clinical data, biomarkers, and CT images, that can be trained end-to-end with even incomplete data types. The resultant model can make predictions using even a single modality. Cross-validation and external validation showed that combining multiple modalities significantly improves performance in predicting lung cancer risk compared to using a single modality. Wang et al. (2024c) proposed a novel lung cancer survival analysis framework using multi-task learning, incorporating histopathology images and clinical information, reaching a cross-validation C-index of 0.73 that outperformed traditional methods. Kumar et al. (2023) combined X-ray with clinical information to develop a multimodal fusion approach to detect lung disease and developed two multimodal network architectures based on late and intermediate fusion, showing better performances with the latter. Lopez et al. (2020) examined all three multimodal fusion types, early-intermediate-late, with deep learning-based techniques for classifying several chest diseases using radiological images and associated text reports, demonstrating the potential of multimodal fusion methods to yield competitive results using less training data than their unimodal counterparts."}, {"title": "5.3. Digestive system (n=43)", "content": "In the domain of the digestive system, most studies focused on the diagnosis of malignancies (n=25), with fewer addressing survival (n=10), treatment response (n=3), and disease progression prediction (n=2). The primary malignancies investigated include colorectal (n=12) and liver (n=10) cancers, with some studies also focusing on the stomach, esophagus, and duodenum (n=9). No modality combination was dominant in this field, though clinical variables (n=31) and histopathology slides (n=17) were commonly included. Integrating multimodal data improved performance in 38/39 papers compared to unimodal approaches. Despite being the category with the highest proportion of externally validated studies (n=11), external validation remains limited. Although quite some studies employed publicly available datasets (n=16), including mainly TCGA (n=11), these datasets were often used for training rather than external validation.\nSeveral studies were particularly noteworthy. Cui et al. (2024) developed a multimodal AI model using both endoscopic ultrasonography images and clinical information to distinguish carcinoma from noncancerous lesions of the pancreas and tested this model in internal, external, and prospective datasets. They also evaluated the assisting potential of the model in a crossover trial. Their results showed that AI assistance significantly improved the diagnostic accuracy of novice endoscopists and that the additional interpretability information helped reduce skepticism among experienced endoscopists. Chen et al. (2024) introduced a deep learning model combining three modalities, radiology, pathology, and clinical data, to predict treatment responses to anti-HER2 therapy or anti-HER2 combined immunotherapy in patients with HER2-positive gastric cancer. This model can"}]}