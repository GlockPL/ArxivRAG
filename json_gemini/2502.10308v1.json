{"title": "LLM-Powered Preference Elicitation in Combinatorial Assignment", "authors": ["Ermis Soumalias", "Yanchen Jiang", "Kehang Zhu", "Michael Curry", "Sven Seuken", "David C. Parkes"], "abstract": "We study the potential of large language models (LLMs) as proxies for humans to simplify preference elicitation (PE) in combinatorial assignment. While traditional PE methods rely on iterative queries to capture preferences, LLMs offer a one-shot alternative with reduced human effort. We propose a framework for LLM proxies that can work in tandem with SOTA ML-powered preference elicitation schemes. Our framework handles the novel challenges introduced by LLMs, such as response variability and increased computational costs. We experimentally evaluate the efficiency of LLM proxies against human queries in the well-studied course allocation domain, and we investigate the model capabilities required for success. We find that our approach improves allocative efficiency by up to 20%, and these results are robust across different LLMs and to differences in quality and accuracy of reporting.", "sections": [{"title": "1. Introduction", "content": "Preference elicitation (PE) is essential for effective decision-making in high-dimensional settings. PE methods aim to balance two objectives: minimizing the cognitive burden on users by limiting the number of queries they ask, and maximizing the information obtained about user preferences for subsequent decision-making. The problem of PE arises across many domains. In this work, we focus on PE in combinatorial assignment (combinatorial auctions, combinatorial course allocation, etc.).\nThe combinatorial assignment doamins suffers from the curse of dimensionality (i.e., the bundle space grows exponentially in the number of items). Moreover, Nisan & Segal (2006) proved that, for arbitrary value functions, achieving full efficiency in combinatorial auctions requires an exponential number of bids. To address this, most mechanisms in practice restrict users to report their preferences through structured languages. However, this limits the users' ability to fully articulate their preferences (Nisan, 2000; Sandholm & Suri, 2000; Fujishima et al., 1999). Thus, the focus has shifted towards iterative mechanisms, where bidders interact with the mechanism over a series of rounds, providing only a limited amount of information in each round with the aim of maximizing the efficiency of the final allocation.\nIn this work, we focus on the course allocation problem, a well-studied combinatorial assignment problem (Budish, 2011; Budish & Kessler, 2021). In this problem, an educational institution must assign courses to students who often have combinatorial preferences over course bundles, and there is limited seat availability in each course. Budish et al. (2017) introduced the Course Match (CM) mechanism, a significant improvement over previously existing approaches. CM has since been adopted by leading institutions such as Wharton at the University of Pennsylvania and Columbia Business School. However, CM's reporting language is both restrictive and cognitively demanding. As a result, students often make reporting errors, which negatively impact the mechanism's performance (Budish & Kessler, 2021). The iterative MLCM mechanism proposed by Soumalias et al. (2024b) addresses these limitations by allowing users to answer adaptively chosen comparison queries (CQs). This alleviates reporting errors and leads to significant efficiency gains. While CQs are easy for students to answer, a long sequence of iterative queries can still pose a cognitive burden, and students remain inherently limited by the mechanism's reporting language.\nWe study the use of LLMs as proxies, answering queries for humans guided by a small amount of textual human input, with the goal of both reducing the reporting burden for the users and allowing a richer expression of preferences. For instance, consider the following illustrative description of preferences:\n\"I prefer to take courses that are scheduled as closely together as possible so I can have an extra day off. If courses have a laboratory section, I strongly prefer that it be in the morning. Course A and Course B complement each other, and I would prefer to take them together to save time and effort. I do not want to take Course D and E together as they cover similar topics. Nonetheless, I need at least one of them to fulfill my requirements...\"\nThe textual description encodes combinatorial information about preferences without requiring commitment to any reporting language up-front, and it allows the students to easily express preferences over whole categories of courses (rather than labeling each one). It also provides a more natural elicitation process for students, who may find it easier to express their preferences in free text rather than answering tens of CQs.\nOur aim is to create a framework enabling a mechanism that requires structured input to also leverage such useful yet imprecise natural language input.\nA proxy-based approach to preference elicitation has been explored previously, demonstrating the potential of LLMs to simulate human responses (Horton, 2023; Manning et al., 2024; Park et al., 2024). We establish that natural language input can be used within SOTA mechanisms for combinatorial assignment. Our framework takes into account the unique properties of LLMs by using carefully-chosen acquisition functions to generate queries to the LLM proxy, chain of thought to improve the accuracy of the answer, and a noise-robust loss to incorporate that information. In extensive experiments, we verify the significance of all of these decisions, and demonstrate the robustness of our approach to different LLM architectures and differences in the detail and accuracy of preference reporting. Using only a one-shot natural language input per agent, our framework improves allocative efficiency in realistic scenarios by up to 20%."}, {"title": "2. Prior work", "content": "In this section, we include prior work with a focus on the central course allocation problem and on LLMs. For a more comprehensive discussion of prior work on PE and machine learning in mechanism design, please see Appendix A.\nThe course allocation problem The course allocation problem is a combinatorial assignment problem, motivated by the real-world challenge of assigning courses in business schools, where students compete for limited seats to build their schedules. (Budish et al., 2017). Early methods for performing course assignments at business schools included a draft and a bidding-based mechanism (S\u00f6nmez & \u00dcnver, 2010; Brams & Straffin Jr, 1979); however, these created incentives for strategic manipulation and resulted in poor outcomes. As a combinatorial assignment problem, course allocation is subject to several impossibility results ruling out mechanisms with simultaneous good properties: most notably, combinatorial assignment mechanisms that are ex-post Pareto efficient and strategyproof must be dictatorships (P\u00e1pai, 2001; Hatfield, 2009).\nTo escape this impossibility result, Budish (2011) proposed a mechanism that satisfies slightly relaxed versions of each of these desiderata: approximate competitive equilibrium from equal incomes (A-CEEI). At a high level, A-CEEI simulates a competitive equilibrium form equal incomes, but the market need not exactly clear, and students may have slightly different initial endowments of money.\nBudish et al. (2017) introduced Course Match (CM), a practical instantiation of A-CEEI tailored to the course allocation problem. In CM, students report their value for each course, as well as positive or negative pairwise interactions between courses (because the users make these reports via graphical user interface, this is called the \u201cGUI language\" or \"GUI\"). CM treats these reports as reliably reflecting student preferences, and uses them to heuristically search for an assignment of courses that constitutes an A-CEEI. CM has been successfully adopted at many leading institutions such as the Wharton School at the University of Pennsylvania and Columbia Business School (University of Pennsylvania; Columbia Business School).\nMachine Learning-powered Course Match Prior to CM's adoption by Wharton, Budish & Kessler (2021) conducted a lab experiment comparing it againt the previously used mechanism, the Bidding Points Auction. Students were happier with their allocation under CM and perceived it as more fair, leading to CM's adoption in practice. At the same time, Budish & Kessler (2021) found that students seemed to have trouble with CM's reporting language: they make limited use of its features, and they sometimes even appear to make outright errors. Motivated by this problem, Soumalias et al. (2024b) incorporated machine learning (ML) into the CM pipeline. While they start with the same user interface, they also ask students to answer pairwise CQs, which are easier for students. These CQs, combined with the reports submitted in the GUI language, are used to train neural network models of student preferences, which guide both query generation and the final allocation process.\nLanguage models as proxies for humans A growing body of research explores employing LLMs as proxies for human participants in social and economic studies. Horton (2023) provided an early demonstration of this approach by studying how LLM agents, endowed with carefully elicited human preferences, could generate interview responses that closely mirrored those given by human subjects. Building on this work, Park et al. (2024) investigated whether LLM-based proxies could replicate qualitative interview responses as accurately as real participants. They found that LLM proxies matched human responses with an 85% accuracy, suggesting substantial potential for LLMs to substitute for"}, {"title": "LLM-Powered Preference Elicitation in Combinatorial Assignment", "content": "human participants in certain contexts.\nSubsequent work expanded the scope of LLM agents to broader social and economic scenarios. For example, Brand et al. (2023) and Manning et al. (2024) studied how these models perform in auctions, negotiations, and marketing environments, providing further evidence that LLMs can effectively mimic human decision-making. More recently, researchers have applied LLMs as autonomous pricing agents for companies (Fish et al., 2024) and as synthetic participants in auction design (Zhu et al., 2024).\nIn concurrent work, Huang et al. (2025) explore the use of LLMs as proxies in combinatorial allocation domains, in contrast to our focus on large-scale combinatorial assignment. This distinction introduces a different set of challenges, resulting in a different framework for PE.\nMechanism Design for LLMs. A related area pioneered by D\u00fctting et al. (2024) is that of mechanism design for LLMs. In this setting, agents\u2014typically advertisers\u2014compete to influence an LLM's reply to a user's query, aiming to better represent their interests. Soumalias et al. (2024a) introduce a truthful mechanism based on importance sampling that converges to the optimal distribution. Hajiaghayi et al. (2024) leverage retrieval-augmented generation to create an auction where a pre-generated ads are probabilistically retrieved for each discourse segment, according to both their bid and relevance. Bergemann et al. (2024) explore an extension of this problem in which agents possess both private types and signals."}, {"title": "3. Preference Elicitation Framework", "content": "We build on the MLCM mechanism (Soumalias et al., 2024b), which initializes a cardinal ML model of each student's value function based on her GUI reports. Following the Bradley-Terry model (Bradley & Terry, 1952), this cardinal model is further trained using the student's responses to pairwise comparison queries (CQs). In MLCM, the CQs are determined by a specialized acquisition function that maintains an ordinal list of schedules based on the student's previous answers. Finally, an A-CEEI is calculated, treating each student's ML model as her value function.\nWe extend MLCM by designing a framework that allows students to provide one-shot natural language input, enabling an LLM proxy to answer CQs on their behalf. This approach lets students express their preferences naturally and eliminates the need for iterative interaction with the mechanism. Additionally, the LLM proxy can handle a large number of CQs without imposing any cognitive burden on the student, unlocking potential efficiency gains. While our implementation focuses on course allocation, in Section 6.3 we discuss its applicability to other combinatorial allocation settings."}, {"title": "3.1. Noise Robust Loss", "content": "LLM responses provide a valuable but inherently noisy signal for PE. To combat this this issue, we leverage the noise-robust generalized cross-entropy (GCE) loss (Zhang & Sabuncu, 2018) instead of the conventional binary cross entropy loss when training student models on pairwise CQs. This allows us to handle the LLM noise effectively, as formalized in the following result:\nProposition 3.1. In framework, as long as the LLM proxy accuracy is over 50%, the student's true valuation function is a minimizer of the training loss in the noisy dataset produced by the LLM proxy.\nProof. The proof is deferred to Appendix B.\nRemark 3.2. In our experiments in Section 5, for all LLM architectures tested, the LLM proxy accuracy is over 71% (see Table 4 as well as Appendix C.1). The proposition is thus relevant to our system's real-world performance. In Section 5.7 we show the practical significance of GCE, as it doubles the effectiveness of our framework over BCE."}, {"title": "3.2. Comparison Query Generation", "content": "Although LLM proxies can respond to any number of queries without a cognitive cost to the student, each response still imposes a computational cost to the mechanism. Thus, the choice of the acquisition function that selects the CQs is vital to both reduce computational costs and improve allocative efficiency.\nAlthough we do not fine-tune language models, our preference learning problem from comparison queries has many similarities to reinforcement learning from human feedback (RLHF) (Bai et al., 2022); (1) We use CQs, and there is an inherent connection between our loss function and the Bradley-Terry model (see Appendix B.2 for details) (2) For each element of each CQ, we can choose among an enormous set of possible elements. (3) We face a similar Bayesian Optimization task; our goal is not to improve learning performance in itself. Rather, our goal is to maximize the student's true value for the bundle she receives under her final trained model. This is similar to RLHF, where the goal is to maximize the user's expected satisfaction with the generated answers (Bai et al., 2022).\nGiven this connection to RLHF, we draw on recent work exploring different acquisition functions for this task (Dwaracherla et al., 2024). Because the best-performing acquisition functions in that line of research also leverage epistemic uncertainty, we create epistemic Monotone Value Neural Networks (eMVNNs), an extension of MVNNs (Weissteiner et al., 2022a) that can also represent the epistemic uncertainty of the network. MVNNs are a specialized architecture for combinatorial allocation that incorporates at"}, {"title": "4. Large Language Models", "content": "In this section, we describe our use of LLMs: what information they are given, and how we generate outputs. We emphasize that we use LLMs for two distinct purposes. The first and main purpose is as a proxy to answer CQs based on free-text preference descriptions provided by the students; these LLM proxies are part of our framework and would be used if it were actually deployed (Section 4.1). The second is as a surrogate student to generate free-text preference descriptions from some underlying utility model; these are part of our experimental methodology, but would not be used if the mechanism were actually deployed-instead, real students would produce free-text preference descriptions based on their actual preferences (Section 4.2)."}, {"title": "4.1. LLMs as Proxies for Comparison Queries", "content": "Given students' textual input, LLMs can extract underlying preferences and answer CQs in place of the student, reducing cognitive load. However, in our experiments, we observed that directly prompting LLMs to answer CQs often leads to inaccuracies, particularly due to hallucinations when the input text is long and contains many courses.\nTo address this issue, we implemented a structured chain-of-"}, {"title": "4.2. Simulated Student Responses", "content": "We also use LLMs to simulate how students might describe their preferences in free text before answering CQs. In our experiments, Llama-3.1 8b (Dubey et al., 2024)\u00b9 is given numerical utilities (including complements and substitutes)"}, {"title": "5. Experiments", "content": "In this section, we experimentally evaluate our LLM-powered PE framework."}, {"title": "5.1. Experiment Setup", "content": "We use the Course Allocation Simulator from Soumalias et al. (2024b) because it offers a unique combination of extensive preference data and validated error rates from real lab experiments. Specifically, its configuration closely mirrors the results in Budish & Kessler (2021), matching both the frequency and severity of student mistakes within a 1% margin. To focus solely on learning performance, we set all course capacities to infinity, ensuring that each student can receive any combination of five courses. In line with our LLM-based design (Sections 4.1 and 4.2), we first convert each synthetic student's cardinal preferences into a free-text description using Llama-3.1. These narratives are then passed to the LLM proxy, which answers comparison queries by interpreting the student's stated preferences. Further implementation details, including our hyperparameter optimization (HPO) protocol, are provided in Appendix D.2."}, {"title": "5.2. Efficiency Results", "content": "In Figure 2a we plot the student's allocated bundle value against the number of CQs answered by her proxy LLM.2 We normalize the value of the bundle each student receives by the bundle that the student would have received based on her initial GUI reports. Our results demonstrate that,"}, {"title": "5.3. Learning Performance", "content": "To assess our framework's impact on the generalization performance of a student's ML model, in Figure 2b we plot (MAEC), a shift invariant measure of the mean absolute error against the number of LLM-answered CQs.\nWe focus on a shift-invariant regression metric because the allocation algorithm assigns to each student the bundle with the highest predicted value-based on her ML model-that is attainable for her. Thus, learning a student's value function up to a constant shift suffices, since it results in the same (optimal) allocation as the true value function under any set of constraints. Similarly, we focus on the student's top quantile of bundles, as these are the most critical for ensuring high-value allocations. Our results demonstrate that our framework leads to an immediate and significant reduction in MAEC, nearly halving its original value."}, {"title": "5.4. Robustness to Students' Mistakes", "content": "In this section, we evaluate our framework's performance when changing the severity of the students' mistakes in the GUI language. To do this, we multiply all parameters affecting their mistake profile in the simulator by a constant \u03b3. For y > 1, students make more mistakes than in the default profile, and for y < 1 the opposite is true. Importantly, we do not change any of the hyperparameters of our framework compared to their optimized values for our default setting.\nIn Table 3, we present the normalized allocation value, and percentage of instances where our framework increases allocative value (over just the GUI reports) as a function of the mistake multiplier. We observe that our framework improves average allocation even for y = 0.5. Importantly, mistakes do not scale linearly; in that case, students make about half the amount of mistakes reported in Budish & Kessler (2021), and the severity of those mistakes is reduced by over 80% (Soumalias et al., 2024b, Section 7.4). Overall, these results demonstrate that our framework is robust to varying mistake severity, consistently improving allocation outcomes even under significantly reduced or amplified error profiles."}, {"title": "5.5. Evaluating the Effectiveness of Chain of Thought", "content": "In this section, we evaluate the effectiveness of CoT prompting. We compare using our framework with and without the CoT reasoning teamplate introduced in Section 4.1.\nTable 2 reveals that incorporating CoT significantly improves performance across all metrics: the accuracy of the LLM proxy increases from 59% to 72%, and the normalized allocated bundle values show statistically significant gains (visualized in Figure 3). Without CoT, despite using optimal hyperparameters, performance improvements remain statistically insignificant. This comprehensive improvement in accuracy is further illustrated in Figure 8 (Appendix C.6), demonstrating the critical role of CoT reasoning in enhancing performance."}, {"title": "5.6. LLM architecture ablation test", "content": "In this section, we evaluate our framework's robustness to LLM architectural variations. As detailed in Section 4, our experiments employ two distinct LLMs: the LLM proxy used by our framework to finetune the student's ML model (Section 4.1), and the LLM simulating student textual input"}, {"title": "5.7. Comparing BCE against GCE", "content": "In this section, we study the effect of the GCE loss in improving our framework's performance. We compare allocation value under the default model parametrization (Appendix D.2) with a version using standard binary cross-entropy (BCE) loss. Both versions were allocated the same compute time for hyperparameter optimization (HPO), as detailed in Appendix D.2.\nSimilar to our main results in Section 5.2, in Figure 2a we plot the student's allocated bundle value, as a function of"}, {"title": "5.8. Comparing Different Acquisition Functions", "content": "In this section, we evaluate the importance of the acquisition function in our framework's performance. We compare the allocation value across different acquisition functions, using the default parametrization of our framework as described in Appendix D.2, while varying only the acquisition function responsible for selecting the CQs.\nTable 5 highlights the effectiveness of Double Thompson Sampling, which doubles the improvement in allocation value compared to the second-best acquisition function-from 10.95% to 19.34%. Notably, Double Thompson Sampling is also recognized as a superior acquisition function in RLHF settings (Dwaracherla et al., 2024). Interestingly, all other acquisition functions tested, including random selection, exhibit nearly identical performance."}, {"title": "5.9. LLM Accuracy Robustness Test", "content": "In this section, we evaluate our framework's robustness to variations in the accuracy of the proxy LLM. Given the consistently high performance (72\u201375%) of tested LLM architectures (Section 5.6), we employ simulated LLMs to explore a broader accuracy range. Specifically, for each CQ chosen by the acquisition function, we provide the correct reply with probability equal to the simulated LLM accu-"}, {"title": "6. Discussion and Conclusion", "content": "6.1. Limitations of the Language Model\nDespite the clear benefits of using large language models (LLMs) to elicit preferences and answer comparison queries (CQs), we will discuss two important limitations here. First, while the model frequently demonstrates solid general reasoning skills, we have observed gaps in its quantitative reasoning: when comparisons hinge on numerical trade-offs or multi-attribute scoring, the model's accuracy can suffer. Improving the model's ability to precisely handle numerical\nFigure 5 motivates this choice, as the LLM accuracy remains roughly constant throughout the process.\nIn their mechanism, the CQs are answered by real students, and prior work suggests that near-perfect accuracy by real students can generally be assumed."}, {"title": "6.2. Cost Considerations", "content": "A further practical concern with current LLMs is the cost associated with their use. In our experiments, each student required roughly 300 comparison queries, amounting to an average of 0.1 million input tokens and nearly the same amount of output tokens in the CoT response. At scale, these token counts could become prohibitively expensive, especially for advanced models with higher per-token pricing. Luckily, our experiments show that we are not reliant on advanced models, and even using open-sourced models with smaller parameter sizes like LLaMA 3.1 8b, we are still able to achieve significant results.\nEncouragingly, both the capabilities and cost structures of LLMs are rapidly evolving. For instance, in a pilot study using the GPT-01 model (Zhong et al., 2024), we observed a substantial increase in CQ accuracy to over 80%. However, this experiment was halted due to the model's high price. These developments suggest that, as the LLM ecosystem"}, {"title": "6.3. Applications Beyond Course Allocation", "content": "Although our experiments focus on the course allocation domain, the same LLM-based one-shot preference elicitation approach naturally extends to a wide array of settings in which agents have complex preferences over combinatorial outcomes. Our framework is broadly applicable to other iterative combinatorial allocation mechanisms that use trained ML models to guide query generation, such as MLCA, ML-CCA, and BOCA (Brero et al., 2021; Soumalias et al., 2024b; Weissteiner et al., 2023); as detailed in Section 5.3, our framework improves the generalization of agents' ML models, particularly in critical regions shown to enhance allocative outcomes (Soumalias et al., 2025).\nIncorporating LLM proxies into mechanisms offers a unique opportunity to streamline the elicitation process by reducing the time and cognitive effort required from participants, while also leading to more effective and equitable allocations. As LLMs continue to improve in both capability and affordability, they present a promising new approach for making mechanisms more practical and accessible in real-world applications."}, {"title": "Broader Impact", "content": "Our framework improves allocative value, while also reducing cognitive load on the participating agents. This has the potential of making complicated mechanisms more efficient, fair and accessible. The course assignment mechanism of Budish et al. (2017) was carefully tested before adoption to ensure that it actually improved student welfare and to understand its limitations. The same would be essential for any future improvements to the mechanism, such as our LLM proxy technique. We therefore see the potential for a positive broader impact on this and other problems, and expect that before any mechanism based on our work were deployed, it would in fact be studied carefully."}, {"title": "3.1 Ommited Proofs", "content": "Proposition 3.1 Proof. Ghosh et al. (2015) showed that, under uniform label noise with a noise rate $\u03b7 < \\frac{c - 1}{c}$, where c is the number of classes, the minimizer of a symmetric loss function over a noiseless dataset is also the minimizer of the same loss function over a noisy dataset. Since c = 2 in our case, label noise is inherently uniform and this requirement becomes $\u03b7 \u2264 0.5$.\nThe GCE loss is known to be symmetric (Zhang & Sabuncu, 2018), so the student's true valuation function minimizes the GCE loss on the noisy dataset generated by the LLM responses."}, {"title": "B.2. Mixed Training Algorithm and Connection to the Bradley-Terry Model", "content": "In this section, we reprint the training algorithm of Soumalias et al. (2024b) for integrating GUI reports (regression data) and CQs (ordinal data) into the training of MVNNs.\nThe core idea of Algorithm 1 is to first train the ML model on the student's GUI reports, and then finetune that training on her CQ responses. During the regression phase, the algorithm uses a typical regression loss, which compares the real-valued output of the model, M(\u00b7), to the inferred value from the student's GUI input for a particular schedule (Line 1).\nIn contrast, for CQs, we transform the model's real-valued outputs for two schedules, x1 and x2, using the sigmoid function $f(x) = \\frac{1}{1 + e^{-(x_1 - x_2)}}$. This yields a predicted probability within the [0, 1] range, where $\\frac{1}{1 + e^{-(x_1 - x_2)}}$ represents the likelihood that the student finds schedule x1 preferable to schedule x2 (Line 1). Note that this predicted probability is exactly the one under the Bradley-Terry model (Bradley & Terry, 1952). This predicted probability is then compared with the actual binary preference expressed by the student: 1 if she preferred x1, and 0 otherwise (Line 1).\nThe training process runs for a total of treg + tclass epochs, after which the final set of parameters is returned (Line 1)."}, {"title": "B.3. Monotone Value Neural Networks (MVNNS)", "content": "In this section, we reporting the original definition of MVNNs introduced in Weissteiner et al. (2022a):\nDefinition B.1 (MVNN). An MVNN M\u00bf : X \u2192 R>0 for agent $i \u2208 N$ is defined as\n$M_i(x) := W_i^{K_i} \u03c3_{t_{i, K_i}, K_i} (... \u03c3_{t_{i, 1}, 1} (W^{1, i} D(x) + b^{1, i})...)$\n\u2022 K\u2081 + 2 \u2208 N is the number of layers (K\u2081 hidden layers),\n\u2022 ${\u03c3_{t_{i, k}, k}}^{K_i\u22121}_{k=1}$ are the MVNN-specific activation functions with cutoff $t_{i, k} > 0$, called bounded ReLU (bReLU):\n$\u03c3_{t_{i, k}}(\u00b7) := min(t_{i, k}, max(0, \u00b7))$"}, {"title": "B.4. Epistemic MVNNs (eMVNNs)", "content": "In this section, we introduce Epistemic MVNNs (eMVNNs), which extend the standard MVNN architecture by incorporating an ensemble-based method to estimate epistemic uncertainty. Epistemic uncertainty reflects the model's lack of knowledge about certain regions of the input space and is crucial for guiding active learning and preference elicitation tasks. We formally define Epistemic MVNNs below.\nDefinition B.3 (Epistemic MVNNs). An Epistemic MVNN for agent $i \u2208 N$ is defined as an ensemble of M independently initialized MVNNs, denoted by ${M^{(j)}_i : X \u2192 R_{>0}}^{M}_{j=1}$, where each network is defined as in Definition B.1. For a given bundle $x \u2208 X$, the prediction distribution is represented by the set of outputs from the ensemble:\n$M_i^F(x) = \\{M_i^{(j)}(x) : j = 1, ..., M\\}$\nThe mean prediction and epistemic uncertainty are defined as follows:\n1. Mean prediction:\n$y_i(x) = \\frac{1}{M} \\sum_{j=1}^{M} M_i^{(j)}(x)$,\n2. Epistemic uncertainty:\n$\u03c3^2_i(x) = \\frac{1}{M} \\sum_{j=1}^{M} (M_i^{(j)}(x) - y_i(x))^2$.\nHere, $\u03c3_i(x)$ denotes the standard deviation of the ensemble's predictions and serves as a measure of epistemic uncertainty.\nRemark B.4. Motivated by the connection of our problem to RLHF highlighted in Section 3, we use M = 10 following (Dwaracherla et al., 2024).\nRemark B.5. Note that the mean prediction of an eMVNN preserves the structural properties of MVNNs. Specifically:\n\u2022 Zero value for the empty bundle: Since each MVNN in the ensemble is designed to map the empty bundle to zero, the mean prediction, as a convex combination of these outputs, also assigns a zero value to the empty bundle.\n\u2022 Monotonicity: Each MVNN is a monotone function by construction, as its weights are constrained to be non-negative. The mean of monotone functions is itself monotone, ensuring that the mean prediction of the eMVNN is also monotone."}]}