{"title": "S2AG-Vid: Enhancing Multi-Motion Alignment in Video Diffusion Models via Spatial and Syntactic Attention-Based Guidance", "authors": ["Yuanhang Li", "Qi Mao", "Lan Chen", "Zhen Fang", "Lei Tian", "Xinyan Xiao", "Libiao Jin", "Hua Wu"], "abstract": "Recent advancements in text-to-video (T2V) generation using diffusion models have garnered significant attention. However, existing T2V models primarily focus on simple scenes featuring a single object performing a single motion. Challenges arise in scenarios involving multiple objects with distinct motions, often leading to incorrect video-text alignment between subjects and their corresponding motions. To address this challenge, we propose S2AG-Vid, a training-free inference-stage optimization method that improves the alignment of multiple objects with their corresponding motions in T2V models. S\u00b2AG-Vid initially applies a spatial position-based, cross-attention (CA) constraint in the early stages of the denoising process, facilitating multiple nouns distinctly attending to the correct subject regions. To enhance the motion-subject binding, we implement a syntax-guided contrastive constraint in the subsequent denoising phase, aimed at improving the correlations between the CA maps of verbs and their corresponding nouns. Both qualitative and quantitative evaluations demonstrate that the proposed framework significantly outperforms baseline approaches, producing higher-quality videos with improved subject-motion consistency.", "sections": [{"title": "1. Introduction", "content": "In recent years, significant progress has been made in diffusion-based text-to-image (T2I) generation models [21, 23, 25], enabling the creation of visually high-quality images that correspond closely to the provided text prompts. Expanding upon this achievement, researchers have broadened the scope of diffusion models to encompass text-to-video (T2V) generation [4, 12, 28, 32]. Existing T2V generation models primarily focus on simple scenes featuring a single object with a single motion. However, when users express the need for more expansive scenarios involving multiple objects with distinct motions, these models (de-"}, {"title": "2. Related work", "content": "Text-to-Video Diffusion Models. Recently, diffusion-based T2V generation models [26, 28, 32, 33] have achieved significant progress. Several existing studies [10, 12, 33] have primarily focused on improving the temporal consistency of generated videos. For example, InstructVideo [33] enhances models through human feedback, whereas AnimateDiff [10] maintains fixed pre-training weights and updates only the motion modeling module. Additionally, some studies introduce [7, 15, 16, 31, 35] additional conditions such as depth maps [35], bounding boxes [16, 30], and motion trajectories [7, 31] to control the shape and movement of the video subjects. However, most existing research concentrates on scenarios featuring a single object performing a single motion. The complexities associated with multiple objects executing multiple motions remain underexplored. The issue of prompt unfollowing, such as incorrect video-text alignment between subjects and their corresponding motions, increasingly emerges in these complex settings.\nAttention-Based Latent Guidance. Recent advances [2,"}, {"title": "3. Methodology", "content": "In this section, we first discuss the rationale for enhancing motion correspondence through CA map adjustments, as detailed in Section 3.1. We then analyze the underlying causes of subject count mismatch and incorrect motion binding by visualizing the CA maps of nouns and verbs, comparing successful video-text alignment (positive examples) and misalignment (negative examples) using Modelscope [28] in Section 3.2. This analysis leads to two key observations that motivate us to develop spatial-aware (Section 3.3.1) and syntax-aware (Section 3.3.2) attention-based constraints to improve multiple subject-motion correspondence."}, {"title": "3.1. Why Adjust the CA Map?", "content": "Let a T2V model generate a video $v \\in \\mathbb{F}^{F \\times H \\times W \\times C}$, where H,W, F and C indicate the height, width, number of frames and number of channels., respectively. The backbone diffusion models, typically employing a U-Net architecture, include two core modules that influence video motion: the temporal attention (TA) layer and the cross-attention (CA) layer within the spatial module. Within this framework, the CA layer integrates the input text information with the frame features. In this section, we analyze and explain why our focus is on adjusting the CA maps in the spatial module rather than the TA layer.\nTemporal Attention. In the TA layer, $Q \\in \\mathbb{R}^{N \\times F \\times d}$ and $K\\in \\mathbb{R}^{N \\times F \\times d}$ are both intermediate visual features (d is the length of the query and key features). Then, the shape of the resulting TA map is [N, F, F]. $A_{p,q}$ of TA represents the correlation between the p-th frame and the q-th frame"}, {"title": "3.2. Motivations", "content": "As discussed in Section 3.1, the CA maps of verbs roughly capture the spatial trajectory of the motion. To better understand the increasing occurrences of subject count mismatch and incorrect motion binding, we first conduct experiments using Modelscope [28] with text descriptions that include two objects and two distinctive motions via the template \"a object1 is motionl and a object2 is motion2\". Next, we visualize CA maps of each noun and verb at different denoising timesteps by comparing successful video-text alignment (positive examples) and misalignment (negative examples). In Fig. 2(b) and Fig. 2(c), we focus on subject count and motion binding, respectively."}, {"title": "3.3. Our Solution: S2AG-Vid", "content": "In this paper, we aim to enhance the multiple subjects and motions correspondence for existing T2V models. The proposed S2AG-Vid pipeline is illustrated in Fig. 3. Given a text prompt P of length L, we define the set of words in P as $S = \\{$s_1, s_2,\\cdots, s_L $\\}$. We also define the set of noun and verb pairs in Pas S*, s = {$s_i, s_j\\} \\in S^*$ (where, $s_i$ is a noun, $s_j$ is a verb, i and j is respective indices in S). To address issues of subject count mismatch and incorrect motion binding, we propose spatial-aware and syntax-aware constraints, denoted as $\\mathcal{L}_{sp}$ and $\\mathcal{L}_{syt}$, respectively."}, {"title": "3.3.1 Introducing Spatial Position-Based Prior", "content": "Based on the observation 1, our initial goal is to adjust the CA maps of nouns during the early denoising stages, ensuring that their attention areas become concentrated and distinctly separated from each other. To achieve this, we introduce additional spatial-based priors, i.e., bounding boxes, to quickly guide nouns to focus on specified regions.\nSpecifically, let $B = \\{B_i^f\\}$ represent the set of spatial position priors corresponding to nouns in $S^*$, where $B_i^f$ denotes the spatial position prior of the f-th frame corresponding to the i-th noun in S. Each $B_i^f$ contains top-left and bottom-right coordinates. A set of spatial masks $M = \\{M_i^f\\}$ is obtained by transforming B, where the value inside the box is 1 and the value outside the box is 0. To ensure that the CA maps of nouns concentrate on regions defined by the given spatial prior, we propose a spatial-aware constraint aimed at enhancing the focus of these CA maps on the foreground object,\n$\\mathcal{L}_{f g}=\\frac{1}{F} \\sum_{i, j \\in S^{*}} \\sum_{f \\in F}\\left(1-\\frac{A_{i}^{f} M_{i}^{f}}{||A_{i}^{f}||^{2}}\\right)^{2}$"}, {"title": "3.3.2 Enhanced Motion and Subject Correspondence", "content": "Under the spatial constraints applied to both nouns and verbs, the CA maps of verbs can, to some extent, align with the regions defined by the spatial priors. However, a clear relationship between the CA maps of verbs and nouns is still lacking. According to observation 2, our goal is to leverage the syntactic relationships to establish a strong connection between verbs and nouns, thereby enhancing the alignment between motion and corresponding subjects. As a result, our approach involves introducing contrastive learning to minimize the distance between the CA map of verbs and that of the corresponding nouns, while simultaneously distancing it from the CA maps of other words to prevent interference.\nIn particular, we construct a syntax-aware contrastive constraint, where the noun and verb pairs in each $s$ serve as positive samples for each other, while other words in S act as their negative samples. For the noun $s_i$ and the verb $s_j$ in $s^*$, our positive loss aims to minimize the distance between the CA map of $s_i$ and $s_j$,\n$\\mathcal{L}_{p o s}(s^*)=\\frac{1}{F} \\sum_{f \\in F} \\operatorname{fdist} \\left(A_{i}^{f}, A_{j}^{f}\\right),$\nwhere $\\operatorname{fdist}()$ represents the calculation of the distance function between CA maps. For $s^*$, we define the set of"}, {"title": "5. Conclusions", "content": "In this work, we propose a method called S\u00b2AG-Vid to generate motion-aligned videos when textual prompts contain multiple subjects with distinct motions. Through observation, we find that adjusting the values of the cross-attention map can modify the subjects' motions. Consequently, we suggest an untrained method for motion alignment by adjusting the attention maps of nouns and verbs. The effectiveness of our method lies in the initial localization of the CA map's focus on nouns and verbs, establishing a clear link between nouns and their corresponding verbs. Overall, the proposed method generates videos with accurate motion alignment. Compared to other text-to-video (T2V) generation models, our method demonstrates superior performance in handling multiple subjects and achieving motion alignment."}, {"title": "A. Implementation Details", "content": "We introduce our inference settings in this section. We use a DDIM scheduler [27] to perform 50 denoising steps for each generation. For all methods, we generate 16-frame videos with a resolution of 320 \u00d7 576. In the first 5 steps, we apply Lsp guidance 10 times at each time step to ensure the generated subjects conform to the given spatial priors. For Lsyt, we apply it once at each time step from step 5 to step 25 to align the subjects and their motions. After 25 steps, the model is allowed to automatically adjust the details of the video.\nWe collect a variety of subjects (e.g., boys, women, dogs, cats, balls, vehicles) and actions (e.g., running, standing, flying, skateboarding) commonly featured in video generation. Given that video generation necessitates specific prompts, we eliminate overly abstract subjects and actions, such as \"humans\u201d \u201cmeditation\u201d and \u201cthinking.\u201d Additionally, we aim to balance authenticity and imagination in video generation. Therefore, we exclude clearly implausible subject-action combinations, such as elephants flying, while retaining feasible ones, such as dogs skateboarding. We then randomly combined the subject-action pairs obtained earlier to generate prompts. These prompts are used alongside ChatGPT-4 [1] to generate bounding boxes [15]. The text prompts and in-context examples are shown in Table 3 and Table 4, respectively. To verify the accuracy of these bounding boxes, we invited 10 volunteers for manual review. Initially, we exclude prompts with conflicting bounding boxes and actions, such as moving bounding boxes paired with sitting actions. Subsequently, we filter out bounding boxes that could degrade the quality of video generation, including those that move too rapidly or significantly exceed the screen boundaries. The dataset categories are as follows:\n\u2022 Human Subjects: Men, Women, Boys, Girls, Robots\n\u2022 Animals: Dogs, Cats, Tigers, Bears, Lions, Elephants, Birds, Horses, Cows, Sheep, Dolphins, Fish\n\u2022 Objects: Footballs, Basketballs, Bicycles, Cars, Motorcycles, Trucks, Tanks, Airplanes, Kites, The Sun, Balloons, Boats\n\u2022 Linear Actions:Running, Walking, Skateboarding, Flying, Sitting, Riding a bicycle, Sailing, Surfing Diving, Swimming, Playing football,\n\u2022 Non-Linear Actions: Jumping, Bouncing, Playing golf, Weightlifting, Riding a horse, Playing the guitar"}, {"title": "A.3. Details for user study", "content": "We conduct a user study on the Amazon MTurk platform. To assess video quality and video-text alignment separately, we designed two types of questionnaires. Each type of questionnaire consists of over 60 tasks, evaluated by five humans, as depicted in Fig. 8 and Fig. 9. In the video quality evaluation task, participants are presented with a source prompt and two videos generated from this prompt using the same seed. One video is produced by our proposed method, while the other is generated by a randomly selected baseline method. The presentation order of the videos is shuffled to ensure unbiased evaluation. We then pose three questions for the raters to answer:\n\u2022 Video Quality: Which option has the better video quality?\n\u2022 Video Fluency: Which option is more coherent and smooth?\n\u2022 Overall Preference: Subjectively, which option do you prefer?\nIn the video-text alignment evaluation, the setting and layout of the questionnaire elements remain consistent, with the only variation being the questions themselves. We pose two questions for the raters to answer:\n\u2022 Number Correctness: Which option has the most consistent Number of subjects with the prompt?\n\u2022 Motion Correctness: Based on the provided prompt, which option is the appropriate Motion for the subjects?\nTo ensure the credibility and reliability of our user study, we only involve Amazon MTurk workers with 'Master' status and a Human Intelligence Task (HIT) Approval Rate exceeding 90% across all Requesters' HITs. In total, the 120(60+60) tasks garnered responses from 600 distinct human evaluators."}, {"title": "B. Additional ablation study", "content": "B.1. The ablation study of spatial-aware guidance\nThe impact of time steps in spatial-aware constraints:\nWe evaluate spatial-aware guidance with 1, 3, 5, and 7 steps between denoising iterations. As illustrated in the Fig. 10, a small number of steps leads to issues with subject count variability, while an excessive number of steps have little effect on the subject's shape and overall layout. Consequently, we select 5 steps for spatial-aware guidance.\nThe impact of max-iterations in spatial-aware constraints: To determine the optimal maximum number of iterations for spatial-aware guidance at each time step, we started with 5 and increased it by increments of 5. As shown in the Fig. 11, a smaller number of iterations led to issues with subject count, while excessive iterations had no significant effect on the shape or overall layout. Therefore, we selected 10 as the maximum number of spatial guidance iterations."}]}