{"title": "REVISIT SELF-DEBUGGING WITH SELF-GENERATED TESTS FOR CODE GENERATION", "authors": ["Xiancai Chen", "Zhengwei Tao", "Kechi Zhang", "Changzhi Zhou", "Wanli Gu", "Yuanpeng He", "Mengdi Zhang", "Xunliang Cai", "Haiyan Zhao", "Zhi Jin"], "abstract": "Large language models (LLMs) have shown significant advancements in code generation, but still face challenges on tasks beyond their basic capabilities. Recently, the notion of self-debugging has been proposed to boost the performance of code generation by leveraging execution feedback from tests. Despite its promise, the availability of high-quality tests in real-world scenarios is limited. In this context, self-debugging with self-generated tests is a promising solution but lacks a full exploration of its limitations and practical potential. Therefore, we investigate its efficacy on diverse programming problems. To deepen our understanding, we propose two distinct paradigms for the process: post-execution and in-execution self-debugging. Within the scope of self-contained Python programming tasks, we find that post-execution self-debugging struggles on basic problems but shows potential for improvement on competitive ones, due to the bias introduced by self-generated tests. On the other hand, in-execution self-debugging enables LLMs to mitigate the bias by solely leveraging intermediate states during execution, thereby enhancing code generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) have demonstrated considerable progress in code generation, but still face challenges to perform complex programming tasks beyond their basic capabilities. The tasks require LLMs to understand the given natural language specifications and generate programs that could pass all the private tests. Recently, self-debugging has emerged as a promising approach to boost the performance of LLMs in code generation (Chen et al., 2024b; Jiang et al., 2023; Zhong et al., 2024). This approach enables models to refine their own output through an iteration of generation and execution for the programs utilizing pre-built oracle tests. However, in real-world scenarios of software development, oracle tests are not available for each code snippet.\nTo address this challenge, recent studies have introduced self-generated tests into self-debugging process (Shinn et al., 2023; Huang et al., 2023; Ridnik et al., 2024). As illustrated in Figure 1, in this framework, the model first generates an initial program and a suite of tests based on the natural language specifications of the problem. The program is then executed on the self-generated tests with an executor (e.g. code interpreter). If it raises any error, the signal or message will be collected as execution feedback, which the model uses to generate a revised version of the program. It helps reduce the reliance on external feedback from humans or stronger models and thus holds the potential to be generally applied in various code generation tasks.\nNonetheless, the efficacy of self-debugging with self-generated tests remains underexplored. Reflection (Shinn et al., 2023) leverages feedback from self-generated tests to debug but evaluates the code before repair with hidden oracle tests. AlphaCodium (Ridnik et al., 2024) first iterates on public oracle tests and then on model-generated tests with a technique of test anchors. The improvements"}, {"title": "2 RELATED WORK", "content": "Code Generation. Code generation is the automatic production of source code based on nat-ural language descriptions. Large pre-trained language models like the GPT-4 series have shown impressive capabilities in code generation. Researchers have proposed various approaches to en-hance the quality of code generated by these models. Some works, like LLaMA series (Touvron et al., 2023a;b; Dubey et al., 2024), focus on optimizing model training, while others aim to im-prove code quality through post-processing techniques. For example, CodeT (Chen et al., 2023) generates a large number of code and test cases, using the dual agreement to filter the most promis-ing code candidates. Other methods, such as coder-reviewer (Zhang et al., 2023b) and code-ranker (Inala et al., 2022), apply ranking metrics to select optimal code from multiple candidates. Among these post-processing techniques, methods that involve self-debugging have gained considerable at-tention. Through feedback from execution results, self-debugging allows models to autonomously debug and refine previously generated code, enhancing the final output. Self-debugging does not require increasing the sample budget, making it a cost-effective solution for improving inference efficiency (Zhang et al., 2023a). As a result, self-debugging has been integrated into various LLM-based code generation methods (Yang et al., 2024; Zhang et al., 2024; Dong et al., 2023; Huang et al., 2023). In this work, we revisit these techniques and assess the effectiveness of self-debugging with self-generated tests on both basic and competitive programming benchmarks.\nSelf-Debug with LLMs. As LLMs have evolved, the idea of using models to refine their own output has become more popular. In code generation, several techniques have explored how LLMs can refine the code they generate. Most of these methods rely on prompting LLMs with execution results to improve the code. These methods often rely on pre-existing or generated tests to execute the code, capturing execution information that is then used to refine the output code (Olausson et al. (2024); Wang et al. (2024); Dong et al. (2023); Madaan et al. (2023); Zhang et al. (2023a)).\nSelf-Debugging (Chen et al., 2024b) introduces a framework in which LLMs iteratively debug their own generated code by utilizing execution results and self-generated explanations. Self-Edit (Zhang et al., 2023a) builds on the example tests provided in programming problems for execution to help the model correct its own output. LDB (Zhong et al., 2024) utilizes runtime execution information to help debug generated programs. Jiang et al. (2024) enhance LLM self-debugging by training on an automatically collected dataset for code refinement and explanation. Madaan et al. (2023) conducts a broad evaluation of self-debugging in code models, highlighting that performance can be improved with higher-quality feedback or human intervention. In this work, we aim to explore the potential as well as limitations of execution-based self-debugging methods, particularly with self-generated tests. We provide a detailed analysis of these methods and propose a unified framework in the following Section 3."}, {"title": "3 SELF-DEBUGGING WITH SELF-GENERATED TESTS", "content": "We focus on evaluating the self-debugging capabilities of large language models (LLMs) through execution on self-generated tests. Figure 1 provides a comprehensive overview of this process. Given a problem with a natural language specification, the LLM (denoted as M) first generates an initial program C along with a suite of test cases, denoted as {(Xi, Yi)}Ni=1, where Xi represents the input and Yi represents the expected output for the i-th test. To enhance the model's debug-ging performance beyond its intrinsic reasoning capabilities, we utilize execution feedback as an additional signal to help the model identify bugs in its generated program according to the problem specification. Specifically, we employ an executor (denoted as E) to run the generated program on the test suite and collect execution information as feedback.\nThere are various implementations for utilizing execution feedback, which we categorize into two distinct paradigms: Post-Execution and In-Execution self-debugging. These paradigms reflect the type of information employed in the self-debugging process. Post-execution information refers to content obtained after the program's execution, such as execution outputs or error messages. In contrast, in-execution information refers to intermediate states observed during execution, providing finer-grained insights into the program's behavior. We now formally define these paradigms.\nPost-Execution Self-Debugging. The paradigm leverages information obtained after the actual execution of the program. A widely adopted implementation involves comparing the execution"}, {"title": "4 EXPERIMENTS", "content": "In this section, we evaluate self-debugging capabilities of advanced LLMs using self-generated tests on self-contained Python programming tasks. We carry out experiments to answer the following research questions: (1) When self-debugging with post-execution information from self-generated tests, what would the performance be like on basic programming problems? (2) Is the performance of post-execution self-debugging consistent across different programming tasks? If not, what is the reason behind it? (3) How does in-execution self-debugging perform when considering the settings above? What is the difference between post-execution and in-execution self-debugging.\n4.1 EXPERIMENTAL SETUP\nBenchmarks. We select three popular code generation benchmarks covering basic and competitive\u00b3 programming problems to comprehensively evaluate the efficacy of self-debugging, including:\nHumanEval and MBPP HumanEval (Chen et al., 2021) consists of 164 programming problems written by humans. Each problem provides a Python function signature and a docstring as its spec-ification. MBPP (Austin et al., 2021) includes 974 programming problems written by contributors through crowdsourcing. Each of these problems features a problem statement, a function signa-ture, and three example tests. To enhance the reliability and accuracy of evaluations, EvalPlus (Liu et al., 2023) extends HumanEval into a more comprehensive version known as HumanEval+ with 80 times more tests than the original HumanEval. Similarly, MBPP+ is an augmentation of the original MBPP, offering 35 times more tests. In our experiments, we use the latest version of MBPP for both base and plus set, which consists of 378 programming problems.\nLiveCodeBench LiveCodeBench (Jain et al., 2024) is a contamination-free benchmark that con-tinuously collects new problems from prominent competitive programming platforms. As of now, LiveCodeBench features a collection of over 600 high-quality programming problems. These"}, {"title": "4.2 RQ1: POST-EXECUTION SELF-DEBUGGING STRUGGLES ON BASIC PROBLEMS", "content": "In this subsection, we examine the performance of self-debugging techniques using self-generated tests on basic programming problems and evaluate how it compares to self-debugging with oracle tests. Consistent with implementations in most existing literature, we perform self-debugging by uti-lizing post-execution information. In this process, program correctness is determined by comparing the actual output with the expected output for a given test case. If the generated program successfully passes all tests, the iterative process terminates, and no further self-debugging is conducted."}, {"title": "4.3 RQ2: BIAS FROM SELF-TESTING LEADS TO INCONSISTENCY ACROSS TASKS", "content": "To comprehensively evaluate the performance of self-debugging on diverse programming tasks, we conducted post-execution self-debugging experiments using problems from LiveCodeBench. The problems in LiveCodeBench are classified into three distinct difficulty levels: easy, medium, and hard. We report the pass rate achieved at each level of difficulty, as well as the overall performance.\nResults. Table 4 summarizes the results of post-execution self-debugging with self-generated tests on LiveCodeBench. We observed that for GPT-4o, self-debugging using label feedback leads to improvements across problems of all difficulty levels. This is notably in contrast to the perfor-mance on HumanEval and MBPP. However, when detailed feedback is provided, there is a decline in performance on easy problems. For other models including Claude-3.5-Sonnet, the overall per-formance decreases due to significant declines on easy problems. Moreover, despite incorporating"}, {"title": "4.4 RQ3: IN-EXECUTION REASONING HELPS SELF-DEBUGGING", "content": "In this subsection, we examine the efficacy of in-execution self-debugging across programming benchmarks. Drawing inspiration from the implementation presented in LDB (Zhong et al., 2024), we divide a program into basic blocks based on nodes in its control flow graph (CFG). Then we"}, {"title": "5 DISCUSSION", "content": "Directions for future work. In this work, we demonstrate that post-execution self-debugging with self-generated tests struggles on basic problems due to biased evaluations, despite the significant potential shown by LLMs in automated test generation. This highlights the necessity for the re-search community to focus on the quality of LLM-generated tests before utilizing execution feed-back derived from them. Developing techniques that enhance high-quality test synthesis is crucial to mitigate bias for post-execution self-debugging. It could be beneficial to implement an iterative refinement process wherein execution information is leveraged to improve the tests. This could in-volve using techniques like test-driven development where tests are continuously updated based on code changes and debugging outcomes.\nAs demonstrated in Section 4.4, leveraging enriched runtime information from execution is a promis-ing avenue for self-debugging. In particular, in-execution self-debugging has shown superior per-formance compared to post-execution in certain tasks, suggesting that more nuanced and reliable feedback leads to better performance. Designing more sophisticated methods for collecting and analyzing runtime information is a promising direction for further enhancing self-debugging capa-bilities. For instance, improving the intelligibility of execution trace representations for LLMs may prove beneficial (Ni et al., 2024). Additionally, beyond variables, other types of runtime information, such as code coverage and execution paths, may also be utilized effectively (Chen et al., 2024a).\nEffective self-debugging with self-generated tests hinges on several core capabilities of LLMs. In terms of refinement, the model should be capable of accurately recognizing and localizing faults within the program. Additionally, more advanced reasoning capabilities are needed to analyze exe-cution feedback thoroughly. The model should comprehend the relationship between the code logic and the feedback, thereby deducing the runtime structure of program statements and variables.\nApplications. Self-debugging opens up possibilities for developing more advanced LLMs without reliance on human supervision or guidance from stronger models (Burns et al., 2024). Traditionally, human-generated test cases serve as a strong supervisory signal for aligning code generation, but the collection of such tests is labor-intensive, leading to a sparsity of labeled data for effective code refinement. Self-generated tests, by contrast, offer a viable path for self-improvement (Tao et al., 2024). They alleviate the burden of manual test generation and pave the way toward truly autonomous self-correcting code generation systems (Chen et al., 2024b)."}, {"title": "6 CONCLUSION", "content": "This paper investigates the concept of self-debugging in code generation for LLMs, with a focus on leveraging self-generated tests. We establish a structured framework for self-debugging which is essential for real-world applications where high-quality annotations and human supervision are often unavailable. We introduce and formalize two distinct paradigms within the execution-then-feedback process: post-execution and in-execution self-debugging. Through comprehensive experi-ments on both basic and competitive programming tasks, our findings highlight the unique strengths and weaknesses. Specifically, we observe that: 1) post-execution self-debugging encounters dif-ficulties in basic tasks; 2) bias from self-generated tests can lead to inconsistency across different"}, {"title": "A CASE STUDY", "content": "In our experiments, we observe that in-execution self-debugging, which leverages runtime informa-tion, consistently outperforms post-execution one across various levels of self-contained program-ming tasks. To better understand the unique strengths and weaknesses of these two paradigms, we provide an example involving GPT-40 in Figure 3. It illustrates different outcomes of post-execution self-debugging with detailed test feedback and in-execution self-debugging with execution traces."}, {"title": "B EXAMPLES OF PROGRAM CONTRACTS", "content": null}, {"title": "C ANALYSIS ON THE NUMBER OF SELF-GENERATED TESTS", "content": "To investigate the effect of the number of self-generated tests, we employ GPT-40-2024-05-13 to generate increasing numbers of tests N = [10, 15, 20] for each programming problem in HumanEval and MBPP. The accuracies of these generated tests are summarized in Table 7.\nAs the number of self-generated tests increases, the presence of more challenging edge cases also rises, consequently reducing the accuracy of the test suites. Specifically, when the model generates up to 20 tests per problem, the accuracy of the test suite decreases from 59.15% to 48.17% for HumanEval and from 58.73% to 50.53% for MBPP. We further evaluate the performance of both post-execution self-debugging with detailed feedback and in-execution self-debugging."}, {"title": "D PROMPTS", "content": null}]}