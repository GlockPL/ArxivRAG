{"title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding", "authors": ["Ziyin Zhang", "Hang Yu", "Shijie Li", "Peng Di", "Jianguo Li", "Rui Wang"], "abstract": "Programming languages possess rich semantic information such as data flow that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Model. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the fine-tuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with four different baseline LLMs ranging in size from 350M to 8B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3.", "sections": [{"title": "1 Introduction", "content": "In recent years, applying large language models (LLMs) to processing and generating source code has been a research topic of special interest in both natural language processing and software engineering community (Chen et al., 2021). However, unlike natural languages, programming languages have rich semantics besides the lexical representation of source code, such as the path of execution, the flow of data, and the relation of function calling. These semantics are represented by graph structures such as Abstract Syntax Tree (AST), Control Flow Graph (CFG), and Data Flow Graph (DFG). While enhancing LLMs with lexical representations of source code has the potential to boost their performance, the integration of these richer semantic constructs has yet to be fully realized (Zhang et al., 2024).\nThe major challenge of injecting code structures into code language models lies in the incompatibility between structural graphs and large-scale pretrained language models. On one end, following the scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022), some works have tried to improve the capability of language models in code processing by increasing their model size and pretraining data size, leading to code LLMs with tens or even hundreds of billions of parameters such as Code LLaMA (Rozi\u00e8re et al., 2024) and DeepSeek-Coder (Guo et al., 2024; DeepSeek-AI et al., 2024). However, these models are standard decoder-only Transformer language models (Vaswani et al., 2017) trained with next token prediction, and are thus unable to capture the semantics embedded in the structural graphs of code.\nOn the other extreme, another line of research, represented by GraphCodeBERT (Guo et al., 2021) and TreeBERT (Jiang et al., 2021), has focused on injecting graph structures into Transformer-based code language models. However, these methods either linearize the graphs into text tokens and are thus only applicable to simple tree-based graph structures (Niu et al., 2022; Guo et al., 2022), or modify the Transformer architecture such as attention masks (Guo et al., 2021) and positional encodings (Peng et al., 2021) to encode graph information. These modifications to the model structure make them incompatible with the large-scale pretrained decoder-only LLMs, and thus these experiments have been limited to a small scale.\nInterestingly, insights from the computer vision community suggest a promising avenue for bridging the gap between different modalities: by utilizing a light-weight adapter to project the output features of a non-textual input processing model (such as an image encoder or a graph encoder) into the embedding space of language models, they are able to ground LLMs' understanding of text beyond the text modality, while in the meantime preserving LLMs' capability acquired during text-only pretraining (Liu et al., 2023; Bai et al., 2023; Zhu et al., 2024).\nInspired by these works, we introduce GALLa: Graph Aligned Large Language Models for code. We utilize a graph neural network (GNN) to process ASTs and DFGs extracted from source code, which are then projected into the language model's embedding space by a small adapter. The language model is trained to generate the source code conditioned on the graph information and to answer questions about the graph structure. These objectives align the language model's representation of code to the graph structures and impart it with a deeper understanding of the source code. As demonstrated in Figure 1, our method is based on the transfer learning framework (Raffel et al., 2020) and separates graph alignment data from task-specific training data, thus preserving the general capability of the LLM acquired during pretraining and requiring no graph information about downstream task training or test data.\nThrough extensive experiments on five code understanding and generation tasks, we validate the effectiveness of GALLa on four distinct base LLMs ranging in size from 350M to 8B. GALLa brings consistent improvement over all baseline models, and even demonstrates abilities to generalize structural knowledge acquired during graph alignment to programming languages that are absent in the alignment data. All the data used in our experiments (both graph alignment and downstream tasks) are sourced from publicly available datasets, and the complete code for reproducing our results will be released in https://github.com/codefuse-ai."}, {"title": "2 Related Work", "content": "Existing works that utilize structural graphs to enhance code language models can be categorized into three types: 1) modifying attention masks to encode graph information, 2) integrating graphs into the textual input, and 3) enhancing positional encodings with graph information.\nThe first category is represented by GraphCode-BERT (Guo et al., 2021), which finetunes Code-BERT (Feng et al., 2020) on concatenated source code and data flow graphs. Attention masks in the model are modified to reflect the graph structures: a node $v_a$ in the graph is allowed to attend another node $v_s$, only if there is an edge from $v_s$ to $v_a$, while a node $v$ and a source code token $c$ can attend to each other only if they correspond to each other in the graph. StructCoder (Tipirneni et al., 2024) also modifies attention masks similarly to encode the relations between source code tokens and AST, DFG tokens.\nFor the second category, TreeBERT (Jiang et al., 2021) is an encoder-decoder model where each input token to the encoder is a node's constituent path in the AST, while the decoder is trained to generate the source code with cross-attention to the encoder representations. Many other works in this category, including SynCoBERT (Wang et al., 2021), SPT-Code (Niu et al., 2022), and UniXcoder (Guo et al., 2022) first map ASTs into text sequences (for example, by depth-first traversing), and then send them into code language models. However, this method only applies to simple graph structures such as AST, but not more complex ones such as DFG which may include loops.\nIn the third category, Peng et al. (2021) proposed TPTrans, which applies a recurrent sequence encoder to encode the relative paths between terminal nodes in ASTs and uses the results as relative positional encodings in self-attention modules, while the absolute paths from terminal nodes to the root node are similarly used as absolute positional encodings. Peng et al. (2022) used a list of 2-d coordinates and an embedding lookup table to describe the positions of AST nodes instead, and also used these embeddings to enhance both relative and absolute positional encodings in Transformer.\nDespite their contributions, all these approaches focus primarily on the encoder of the Transformer and are not fully compatible with decoder-only LLMs. For instance, in the first group, the presence of cycles in the graphs means that the corresponding attention mask cannot retain a lower-triangular format, as required by LLMs. Adapting LLMs to these graph-structure-based attention masks would necessitate extensive retraining due to the mask's inconsistency with the original causal language modeling objectives. In contrast, the proposed GALLa method processes graph information externally, allowing LLMs to remain unmodified.\nApart from these graph-enhanced models, there is another model TransCoder-IR (Szafraniec et al., 2023), which is specifically proposed for the code translation task and utilizes LLVM intermediate representation (IR) to ground the model's understanding of code by generating source code from IR and vice versa. Like our method, TransCoder-IR only uses IR for alignment at training time but does not need it at test time. However, intermediate representations are represented in the same modality as the source code (i.e. text modality) and provide a different type of information compared with structural graphs."}, {"title": "3 Method", "content": "The objective of GALLa is to align language models to the implicit structures of code (represented by AST and DFG - see more details in Appendix A) when finetuning on code-related tasks, and an overview of the method is provided in Figure 1. The model consists of three modules: GNN encoder, adapter, and LLM decoder (Section 3.1). The training of GALLa includes two stages: graph encoder pretraining and graph-LLM alignment (Section 3.2 and Figure 2). The notations used in this section are presented in Table 1.\n3.1 Model Architecture\n3.1.1 GNN Encoder\nTo fully capture the rich information contained in the structural graphs such as loops, node degrees, and edge directions, we first process the graphs with a graph neural network (GNN) to extract node information. For a graph G with $n_v$ nodes and $n_e$ edges, the GNN takes as input a node feature matrix $V \\in \\mathbb{R}^{n_v \\times d_{node}}$ and an edge matrix $E \\in \\mathbb{Z}^{n_e \\times 2}$, where each element is a node index. The output of the GNN is the contextual node representations $H \\in \\mathbb{R}^{n_v \\times d_{gnn}}$.\nSince the graph data extracted from the code consists of only a list of nodes and the edge indices, we need to first encode each node's corresponding code segment into a feature vector. This can be done using any code embedding model such as CodeT5+ (Wang et al., 2023). The GNN can be any convolution-based or self-attention-based directed GNN such as MagNet (Zhang et al., 2021) or DUPLEX (Ke et al., 2024), as DFG is a type of directed graph.\n3.1.2 Adapter\nThe outputs of GNN are projected by an adapter into the LLM's embedding space. Following Qwen-VL (Bai et al., 2023), we use a single layer of cross-attention as the adapter, where the GNN's outputs H serve as keys and values, while the queries $Q \\in \\mathbb{R}^{n_g \\times d_{im}}$ are $n_g$ learnable vectors:\n$X_g = CrossAttn(q = Q, k = H, v = H)$. (1)\nAlternative to the cross-attention layer, the adapter may also be a multi-layer perception (MLP), as used by LLaVA (Liu et al., 2023), which applies projection independently to each node. The main difference between cross-attention and MLP is that cross-attention allows for information exchange between the nodes, while MLP is applied independently to each node. Thus, we choose cross-attention as the adapter in the main results, and experiment with MLP in the ablation studies in Section 4.4.\n3.1.3 LLM Decoder\nThe adapter's outputs $X_g \\in \\mathbb{R}^{n_g \\times d_{im}}$ are $n_g$ embedding vectors, which we dub \u201cgraph tokens\". Any other text in the LLM's input (as shown at the top of Figure 1) is first tokenized and passed into the LLM's embedding layer to obtain $n_t$ text embeddings $X_t \\in \\mathbb{R}^{n_t \\times d_{im}}$, and then concatenated with the graph tokens to form the input to the LLM's Transformer layers:\n$X = [X_g, X_t]^T \\in \\mathbb{R}^{(n_g+n_t) \\times d_{im}}$. (2)\nThe LLM's output logits $Y \\in \\mathbb{R}^{(n_g+n_t) \\times d_{im}}$ are then used to compute cross-entropy loss with next token prediction (i.e. causal language modeling). However, the loss is masked on the graph tokens and only computed on the text tokens.\n3.1.4 Model Choice\nLastly, we emphasize that GALLa is a framework for bridging the text modality and graph modality, and each of the three modules in GALLa can be instantiated with different models. The choice of GNN - e.g. directed or undirected - depends on the properties of graph data, while the choice of LLM depends on application scenarios - e.g. monolingual or multilingual, general-purposed or domain-specialized.\n3.2 Training Procedure\n3.2.1 Stage 1: graph encoder pretraining\nIn GALLa, the LLM is initialized from a pretrained LLM checkpoint such as LLaMA (Dubey et al., 2024) or StarCoder (Li et al., 2023), while both the GNN and the adapter are randomly initialized. Thus, to prevent the newly initialized GNN and adapter from disrupting the LLM's pretrained representations, we fix the LLM's weights in stage 1, and update only the GNN and the adapter. In this stage, the model is trained with graph-to-code generation (Graph2Code), where the model reconstructs a graph's corresponding source code tokens $X_t$ based on the graph tokens $X_g$ by maximizing the probability $P(X_t|X_g)$. Similar to visual instruction tuning (Liu et al., 2023), this stage can be understood as training a \u201cgraph tokenizer\u201d for the frozen LLM.\n3.2.2 Stage 2: graph-LLM alignment\nIn the second stage, we aim to align the LLM's pretrained representations of source code to the structural graphs and deepen their understanding of code structures. In this stage, the LLM is unfrozen, and all three modules are updated together. The graph alignment tasks in this stage include Graph2Code (same as stage 1), and graph question answering (GraphQA). In GraphQA, the language model answers questions about a graph's structures, such as predicting whether there is an edge between two given nodes, or predicting the children of a given nodes. Formally, the model is trained to maximize the probabilities of the answer text tokens $X_a$ conditioned on the graph tokens $X_g$, and the question text tokens $X_q$: $P(X_a|X_g, X_q)$.\nSince the ultimate goal of aligning LLMs to code graph structures is to enhance their performance on related downstream tasks, the model is simultaneously trained on the graph alignment data and downstream task data in this stage. However, we emphasize that the GNN and adapter are only used for the graph alignment tasks. No graph information is required of the downstream task data, as it goes directly into the LLM in the form of question-answer pairs, as shown at the bottom of Figure 1."}, {"title": "4 Experiments", "content": "In this section, we first discuss the details of our experiments in subsections 4.1 and 4.2, and then provide the results in subsections 4.3 and 4.4.\n4.1 Datasets\n4.1.1 Graph Alignment\nFor graph alignment, we used 240K Python programs and 75K Java programs from CodeNet (Puri et al., 2021). For each program, we extracted one AST and one DFG using Program Structure Interface \u00b2, which resulted in 626K code-graph pairs after removing empty graphs and source code files that are longer than 4096 tokens.\nTo improve the generalization capability of our model and to align the representations of different programming languages for certain cross-lingual tasks such as code translation, we used a special type of AST and DFG - universal AST (UAST) and universal DFG (UDFG), where different languages share the same set of node types to the possible extent (see more details in Appendix A). In total, there are 43 node types in our graph data.\nFor the Graph2Code task, we removed all Python programs where no main function can be found (which consist more than 80% of the 240K Python programs), resulting in 150K Java graph-code pairs and 81K Python graph-code pairs.\nFor the GraphQA task, we designed three types of questions:\n\u2022 Edge prediction: given the graph tokens and the source code of two nodes, the model is tasked to predict where there is an edge between them. This task is constructed only on DFG data.\n\u2022 Parent prediction: given the graph tokens and the source code of one node, the model is tasked to predict the node's parent in the graph. This task is constructed on both AST and DFG data.\n\u2022 Child prediction: given the graph tokens and the source code of one node, the model is tasked to predict the node's children in the graph. This task is constructed on both AST and DFG data.\nIn our preliminary experiments, we found edge prediction data construct from AST graphs to bring no improvement, as connected nodes in the AST often have high textual overlap and can be trivially predicted. We also filtered out several other types of questions, including node classification (which can be easily done by only looking at the node's source code) and some tasks that involve counting such as counting the number of nodes in the graph, the number of edges in the graph, and the number\n4.1.2 Downstream Tasks\nFor downstream code tasks, we consider both discriminative and generative tasks, including code translation, clone detection, defect detection, code summarization, and code repair. For code translation, we use the CodefuseEval (Di et al., 2024) benchmark; for the other four tasks, we use the CodeXGLUE (Lu et al., 2021) benchmark. Among these tasks' data, we downsampled the original data of code summarization and clone detection due to computation constraints, and filtered out 14K Java - Python samples in the training set of code translation where the target code does not follow Python coding conventions (e.g. no main function can be found). A summary of the final datasets is provided in Table 2. For all tasks we use the original metric for evaluation.\nIn the main experiments, we consider the setting of multi-task finetuning (MFT), where the model is simultaneously trained on all five downstream tasks. However, we demonstrate with a small-scale experiment that GALLa can be also used for single-task fientuning (SFT).\n4.2 Models and Training\n4.2.1 Model\nWe use a DUPLEX (Ke et al., 2024) with 1024 hidden states and 7M parameters as the GNN, and a cross-attention layer with randomly initialized learnable queries as the adapter. For the LLM, we consider four distinct models of varying sizes: CodeGen 350M (Nijkamp et al., 2023), StarCoder 1B (Li et al., 2023), Phi-1 1.3B (Gunasekar et al., 2023), and LLaMA3 8B (Dubey et al., 2024). All of these models are prerained (partially) on source code data, and demonstrate strong performance on code-related downstream tasks.\n4.2.2 Training\nFor the first stage training of GNN and adapter, we train the model on the graph data for 15 epochs with learning rate le-4, 1K warmup steps, weight decay 0.1, 240 global batch size, AdamW optimizer (Loshchilov and Hutter, 2019), and ZeRO stage 2 (Rajbhandari et al., 2020). The training takes place on two machines, each equipped with 8 A100 80G GPUs.\nThe second stage of training largely follows the same setting but with a smaller learning rate (5e-5) and a smaller global batch size (96). The model is trained for 5 epochs on the mixture of downstream task data and graph alignment data in stage 2, and the checkpoint with the lowest validation loss on the downstream tasks is chosen for evaluation. Among the four LLMs, CodeGen, StarCoder, and Phi-1 are trained in full scale, while LLaMA3 is trained using LoRA (Hu et al., 2022) with rank 64. All training in this stage takes place on a single machine with 8 A100s.\nAs a baseline, the LLM is finetuned on only the downstream task data using the same hyperparameters as stage 2 training.\n4.3 Results\nThe results of applying Graph2Code and GraphQA alignment to the code finetuning process of four models are presented in Table 3. Graph alignment brings consistent improvement over the baseline model, increasing the average performance on five tasks by up to 36%. Comparing the results between the four models, we find that GALLa's effectiveness is most prominent on the weakest baseline StarCoder, while relatively less notable on the strongest model LLaMA3.\nTo verify the statistical significance of these improvements, we conducted Chi squared tests on the tasks with discrete performance metrics (i.e. code translation, code repair, clone detection, and defect detection) and Wilcoxon signed-rank test on the tasks with continuous performance metrics (i.e. code summarization). Most of the differences are significant, except for CodeGen and Phi-1 on JavaScript summarization, and LLaMA3 on Java-to-Python translation and Python summarization. The complete table of p values is given in Table 10 in Appendix B.\nNotably, while our graph alignment data include only Python and Java programs, from Table 3 we observe that they can even improve three of the four models' performance on tasks in other languages - code summarization in JavaScript, and defect detection in C. This showcases that the knowledge about code structures acquired in GALLa can be generalized across programming languages, as learning to align to Python and Java structural graphs improves the finetuning performance of downstream tasks in other languages.\nIn Table 4, we also present the results of single-task finetuning with Phi-1. In this setting, we find that Graph2Code still improves the average performance on all tasks by 2%, and by 3% on Python and Java tasks. On the other hand, GraphQA brings limited improvement, and even a large drop on the C defect detection task. We hypothesize that this is because the diverse data in GraphQA serve a similar purpose to instruction tuning for LLMs (Sanh et al., 2022; Chung et al., 2022), where the benefits of cross-task transfer only start to manifest when the number of tasks is large.\n4.4 Ablation Studies\nAs an ablation study, we conducted experiments with Phi-1 and Graph2Code using either only the AST data or only the DFG data for graph alignment. The results are shown in Table 5. We find that DFG brings more improvement compared with AST, which we contribute to the fact that AST is more closely related to the surface form of source code and thus provides the model with less additional structural information, while DFG includes more complex structures - such as loops - that are more informative.\nAs a control experiment, we additionally trained a model on the mixture of downstream tasks and the source code from the graph alignment data, but did not provide it with the graph information - in other words, Graph2Code with only the code but not the graph, as illustrated in Figure 3, which is similar in essence to continual pretraining on the source code data. The results are given in the last row in Table 5, which suggests that the graph information is indeed helping the LLM to better understand programs.\nTo show that the proposed GALLa framework can be applied to various GNN encoders and adapter modules, we also conducted another two sets of experiments in the MFT setting with Phi-1: based on the settings in the main experiments, we 1) replaced the cross-attention adapter with a 3-layer MLP (which has a similar parameter count to the cross-attention layer), and 2) replaced the graph encoder with a MagNet (Zhang et al., 2021). For these two experiments, we repeated the complete two stages of GALLa training, and the results for using an MLP as the adapter are presented in Table 6. Compared with the results using cross-attention from Table 3, we surprisingly find that the MLP adapter leads to a slightly better performance, which is counter-intuitive as MLP does not allow for information exchange between the nodes. We hypothesize that this is because with the same parameter count, MLP has more layers than cross-attention (3 vs. 1 in our case), thus having a stronger expressivity.\nThe results of using a different graph encoder - MagNet (Zhang et al., 2021) - when finetuning Phi-1 with GALLa are presented in Table 7. Compared with the results using DUPLEX as the graph encoder in Table 3 in the main paper, we observe that MagNet can similarly improve the LLM's performance on all five downstream tasks. Together with the experiments of using different LLMs (Table 3) and different adapters (Table 6), this confirms that GALLa is a flexible framework where each of the three modules - GNN encoder, adapter, and LLM decoder - can be replaced upon demand."}, {"title": "5 Conclusion", "content": "In this work, we present the conceptual designs, implementation details, and experimental results of GALLa - Graph Aligned Large Language Models for improved source code understanding. Unlike previous works that modify code language models' internal structures to enhance them with graph information, GALLa follows the cross-modality alignment paradigm and leaves the structure of the language models intact, making it applicable to any off-the-shelf code LLMs. By integrating GALLa as an auxiliary task on a separate graph alignment dataset in the finetuning stage of code LLMs, we require no graph information of the task-specific finetuning or evaluation data, thus incurring no additional computation cost compared with the baseline LLM at inference time. Our experiments validate the effectiveness of GALLa on various code downstream tasks with base LLMs of varying sizes, paving road for a new paradigm in integrating code structural graphs into language models and providing insights for future research in developing structure-aware code LLMs."}, {"title": "A Introduction to Structural Graphs", "content": "In this work, we refer to Abstract Syntax Trees (ASTs) and Data Flow Graphs (DFGs) as program structural graphs. These are graph data structures (i.e. nodes and edges) that specify the internal logic of programs. Each node in these graphs is a snippet of code within the entire program.\nA.1 AST\nAn AST is a tree representation of the syntactic structures of a program, with the root node being the entire program, and each leaf node being a single semantic unit in the program - such as an identifier (e.g. a variable, a class, a function) or an operator. Each non-leaf node in the tree is a combination of its children, indicating the structures of the code. As the name suggests, AST is an abstract representation of source code, in the sense that certain details such as white spaces, parentheses, and delimiters are omitted. Every node in the tree has an associated type attribute, such as assignment expression or for loop. A toy example in Python is provided below:\ndef add(a, b):\n return a+b\nIn the AST for this example, the root node is a function expression including the whole code snippet. The rest of the nodes are given in Table 8.\nA.2 DFG\nA DFG is a graph representation of the variable use-define chains within a program at execution time. For a given program, its DFG shares the same set of nodes with its AST, while the edges indicate the data flow between variable-involved nodes. An example DFG of the previously shown program is provided in Table 9.\nA.3 Universal AST\nTypically, ASTs are specific to the programming language in question. In other words, different programming languages would have different types of nodes in their ASTs - for example, a for loop node in Python may have different semantics from a for loop in C++. However, in the context of our work, such language-specific specifications may be detrimental to the cross-language alignment or generalization capabilities of the models.\nThus, in this work we used a special type of AST (and DFG, as they share the same set of nodes): Universal AST (UAST). In UAST, the specifications of all node types are designed to be as language-independent as possible. For example, for loops from different languages are not distinguished from each other, and only the most basic semantics of a for loop is retained in this abstract node type."}, {"title": "B Statistical Tests", "content": "The complete list of p values of the statistical differences between the main results and the baselines is given in Table 10."}]}