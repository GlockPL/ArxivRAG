{"title": "Acquisition through My Eyes and Steps: A Joint Predictive Agent Model in Egocentric Worlds", "authors": ["Lu Chen", "Yizhou Wang", "Shixiang Tang", "Qianhong Ma", "Tong He", "Wanli Ouyang", "Xiaowei Zhou", "Hujun Bao", "Sida Peng"], "abstract": "This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, leading to information silos among them, which prevents these abilities from learning from each other and collaborating effectively. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions with a single transformer. EgoAgent unifies the representational spaces of the three abilities by mapping them all into a sequence of continuous tokens. Learnable query tokens are appended to obtain current states, future states, and next actions. With joint supervision, our agent model establishes the internal relationship among these three abilities and effectively mimics the human inference and learning processes. Comprehensive evaluations of EgoAgent covering image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility.", "sections": [{"title": "1. Introduction", "content": "This paper aims to build an agent model, as defined by David Ha and Jurgen Schmidhuber [25], inspired by the human cognition system. Built upon world models that focus on predicting the environment, the agent model contains an additional action proposal component, emphasizing the capability to take actions in response to predicted states. Motivated by the acquisition process of human commonsense, we design our agent model to take egocentric RGB observations as input and learn to perceive, predict, and interact with the world. Such abilities are essential for applications in robotics, gaming, and virtual reality, where the agent needs to understand what is happening in the world, anticipate what will happen next, and decide what actions to take to achieve its objectives. It is challenging to build a model that can possess these three abilities like humans, considering two key aspects. First, the three abilities of humans are closely intertwined, where perception and prediction provide information for interaction, and interaction allows the model to better understand the world. Second, the learning of these three abilities is also closely related. Humans develop internal models of the world through an ongoing cycle of perception and interaction, which reinforce each other.\nPrevious methods typically focus on building one of the three abilities, with typical vision tasks including (1) Visual representation learning [10, 12, 29, 32, 50], which encodes high-level states of human observations, i.e., images and videos; (2) Action prediction [9, 40], which forecasts future human actions according to past actions; (3) World model [41, 57], which predict world state transitions based on observations and actions. However, we argue that training separate models for perception, prediction, and interaction leads to information silos between the models, preventing them from learning from each other and collaborating effectively. It is crucial to establish the relationship between these three abilities, allowing them to reinforce each other in both inference and learning processes.\nIn this paper, we propose a novel joint predictive agent model in egocentric worlds, named EgoAgent, that simultaneously learns to represent world observations, predict future states, and act based on learned representations (Figure 1). Our core innovation lies in a single transformer that unifies the representational spaces of the three abilities by mapping them into a sequence of continuous tokens. Specifically, given egocentric observations and historical human actions, we map them into high-dimensional feature vectors using projection layers. Then, these feature vectors are fed into a transformer along with a set of predefined query tokens to obtain the current state features, future state features, and the following actions. We adopt a teacher-student mechanism to train the agent model, which jointly learns visual representation extraction, future state prediction, and action prediction.\nOur unified agent model brings three benefits. First, thanks to the attention mechanism of the transformer, our model naturally establishes internal relationships among the three abilities. Second, aligning representation space allows the transformer to be compatible with multi-modal data and maintain full parameter-sharing, increasing training samples for three sub-tasks. Third, our proposed model enables us to leverage the task of action generation to reinforce the learning of world perception and future prediction, effectively mimicking the human learning process.\nExtensive experiments on egocentric video and human motion dataset [21] demonstrate that EgoAgent can well handle tasks on perception, prediction, and action, even pushing the performance limits of the existing state-of-the-art methods. Specifically, EgoAgent outperforms the leading egocentric video pretraining method [47] by +1.40% and +1.32% Top1 accuracy on ImageNet-100 and ImageNet-1K for the image classification task, +16.28% Top1 accuracy and +16.95% mAP on Ego-Exo4D [21] future state prediction task, respectively. Furthermore, on the 3D human motion prediction task, EgoAgent surpasses both video-based motion generation models and motion prediction methods, achieving -0.82 MPJPE (cm) performance improvement at 30 fps prediction rate.\nOur contributions are two-fold: (1) We develop a joint predictive agent model in egocentric worlds named EgoAgent, which can simultaneously learn to represent the observation, predict future states, and generate informed actions. (2) We evaluate our EgoAgent on egocentric video-action datasets and show its superior abilities in image classification, egocentric future status prediction, and 3D human motion prediction tasks."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Egocentric Visual Representation Learning", "content": "Traditional visual representation methods learn the features on large-scale curated image datasets, e.g., ImageNet [16], using supervised [27, 32, 50] or self-supervised methods [10, 12, 28]. As curated datasets consume lots of resources, which limits the training scale, researchers turn to more accessible data, i.e., egocentric videos. R3M [42] and VIP [36] captured the temporal relationships among egocentric videos, learning generic representations on Ego4D [20] for robotics. Recently, Venkataramanan et al. [47] proposed DoRA to learn object-level representation on their proposed egocentric video dataset, Walking Tours. Aiming to mimic the process of how humans learn from the egocentric world, we proposed a joint predictive agent model that can not only learn representative features using egocentric videos but also predict future states and generate informed actions."}, {"title": "2.2. World Models", "content": "World models aim to predict future world states based on previous observations and actions. These models can be broadly classified into two main categories: generative and predictive models. Generative world models [2, 25, 53] often employ an autoencoder framework, training to capture state transitions within the input image or video space. Recently, these models have demonstrated success in applica-"}, {"title": "2.3. 3D Human Motion Prediction", "content": "Human motion prediction models [34, 48, 55] aim to forecast future body movements given historical poses. Aksan et al. [1] proposed a transformer-based architecture to capture spatial-temporal dependencies of human motion over short and long horizons. Cui et al. [15] represents the human skeleton as a dynamic graph to adaptively learn joint connection strengths for better prediction accuracy. Other works [37, 58] incorporate physics-based priors, such as inverse kinematics, to ensure the generated motions adhere to physical principles. Recent research has integrated multimodal cues like textual instructions [39], eye gaze [59], and 3D objects [56] as conditions for human motion prediction. A growing body of works [14, 26, 49] highlighted the importance of scene-awareness in this task. Notably, Cao et al. [9] proposed a three-stage framework that consists of goal prediction, path planning, and pose finalization, effectively leveraging scene context to enhance goal-oriented motion prediction."}, {"title": "3. Method", "content": "Given the egocentric observations and history human actions, our paper aims to learn an agent model that can represent the scene state, predict future states, and predict the following actions. As humans learn to understand the world's underlying dynamics by observing and interacting with it, we believe these three tasks share internal relations, so that can be jointly learned and be beneficial to each other.\nThe overview of our proposed architecture is presented in Figure 2. We first introduce the base model of our joint embedding-action predictive architecture and describe how to represent the current scene state, predict future scene state and human action with the base model (Section 3.1). Then, we introduce how it can be learned in Section 3.2. Finally, we demonstrate the applications of our agent model in various tasks in Section 3.3."}, {"title": "3.1. Joint Embedding-Action-Prediction Architecture for EgoAgent Model", "content": "Base model. We employ a Large Language Model (LLM) [4, 31, 43-45] as the foundational architecture for our agent model and adapt it to jointly predict future scene states and human actions, as well as to extract image features. This design choice is motivated by two main reasons. First, our objective of near-future world prediction aligns closely with LLMs' inherent next-token prediction capability. Second, the well-structured and straightforward architecture of LLMs provides a flexible and extensible foundation for our specific tasks.\nIn practice, we adopt the InternLM [44] as the backbone, with adjustments made to the input and output formats: we modify the input to be the observed images and actions, and the output to the scene states and human actions. To accommodate these changes, we adapt relevant network layers accordingly. Notably, instead of using pretrained tokenizers like VQVAE [46] to convert images and actions into discrete tokens, we utilize learnable convolutional layers to project them into continuous feature representations.\nJointly predict and act. Given egocentric observations $V = \\{I_0, I_1,...I_t\\}$ and history human actions $A = \\{A_0, A_1, ...A_t\\}$, our agent model aims to predict the future scene state $S_{t+1}$ corresponding to the anticipated observation $I_{t+1}$ and to generate plausible human action $A_{t+1}$ by understanding the current scene state $S_t$ at the same time. The scene state is represented as a set of high-dimensional feature vectors, whereas the human actions are represented as sequences of 3D human skeletons. In contrast to previous methods [8, 53, 57] that represent the scene states using raw images or videos, our feature vector embedding allows the model to focus on higher-level concepts. This abstraction aligns more closely with human perception and predictive process in the real world, as highlighted in IWM [19].\nSpecifically, we begin by encoding the egocentric video $V$ and human action sequence $A$ into feature vectors. For egocentric video, following a similar approach to ViT [17], we use a convolutional layer to process each video frame $I_t$, producing a feature map that is then subdivided into a set of image feature vectors. These features are flattened to form the input image tokens $i_0, i_1, ... i_t$. For human actions, we utilize a convolutional layer with a Layer Normalization (LN) and a Gaussian Error Linear Unit (GeLU) activation layer to map the human skeletons $A_t$ into action features $a_t$. The Transformer network within the LLM then processes these feature vectors to produce the target outputs. As illustrated in Figure 2, the input sequence is formatted as a sequence of image, action, and query tokens, in line with LLM practices. Different from text index tokens, our tokens are continuous feature vectors.\nAt each time step t, we assemble a fragmented sequence of tokens, including image tokens $i_t$, an action query token $q_a$, action tokens $a_t$, and a future state query token $q_s$, where the query tokens are represented as learnable embeddings. The future state query token $q_s$ prompts the model to take previous image tokens $i_{0:t}$ and action tokens $a_{0:t}$ into consideration. Notably, similar to next-token prediction, we insert the action query token $q_a$ right after the image tokens $i_t$ and before the input action tokens $a_t$, ensuring that the model relies solely on past observations and actions to predict subsequent actions. Subsequently, the transformer network processes the input token sequence and outputs action embeddings $a'_t$ and future state embeddings $s'_{t+1}$ in response to the query tokens. Finally, we utilize separate MLP networks to map these embeddings to the predicted actions $A'_t$ and future scene states $S'_{t+1}$."}, {"title": "3.2. Learning EgoAgent in Feature Space", "content": "Asymmetric feature branch. Similar to IWM [19], we train our agent model at the feature level. To this end, we introduce an additional asymmetric feature branch as a teacher network, which extracts only image features and provide guidance for the student branch, as illustrated in Figure 2. Thanks to the flexible structure of the LLM architecture, this feature branch can be easily implemented by adjusting the network input to a reduced sequence of image tokens $i_t$ and a state query token $q_s$ compared to the main network. The model then outputs a world state represented as an image embedding, which is fed into an MLP network to obtain the scene state feature $S_t$.\nSupervisions for predicting scene states and actions. The student branch predicts the future world states $S_{t+1}$ and actions $A'_t$, while the asymmetric feature branch (teacher) takes only the egocentric image $I_{t+1}$ to extract the future scene states $S_{t+1}$. Given the input token sequence at time step t, the loss function for training the model to predict future states and actions is defined as:\n$L_{act}(t) = L_1(A'_t, A_t),$\n$L_{pred}(t) = L_{dino}(S'_{t+1}, sg[S_{t+1}]),$ where $sg[.]$ denotes the stop-gradient operation, $L_1$ and $L_{dino}$ represent the L1 loss and the DINO loss [10], respectively. Following common practices in self-supervised learning [10, 28], $sg[.]$ is applied to block the gradients from back-propagating to the teacher branch. The weights of the teacher branch are updated in each iteration using an Exponential Moving Average (EMA) of the student branch.\nSelf-supervision for learning powerful representations. When humans learn to interact with the environment, they first develop an understanding of the observed scene and objects, which then aids them in predicting future states of the world and making appropriate responses. Inspired by this process, we introduce an additional self-supervised learning loss on EgoAgent to facilitate learning representative features from egocentric videos from scratch:\n$L_{rep}(t) = L_{dino}(\\Theta_{stu}(I^{v_1}_t), \\Theta_{tea}(I^{v_2}_t)).$ Here, $\\Theta_{stu}$ and $\\Theta_{tea}$ denote the student and teacher networks of EgoAgent, respectively, $I^{v_1}_t$ and $I^{v_2}_t$ denote the"}, {"title": "3.3. Applications", "content": "After training, EgoAgent inherently possesses three valuable applications.\nVisual representation learning: EgoAgent learns scene states as feature representations, allowing it to extract meaningful features from input images by $S = \\Theta(I)$, where $\\Theta$ denotes the EgoAgent model. These features can be directly applied to representation tasks, e.g., image classification.\nFuture state prediction: given egocentric video clips with historical human action sequence, EgoAgent is capable of predicting future scene states $S_{t+1}$ by $S_{t+1} = \\Theta(I_0, A_0, ..., I_t, A_t)$. Once predicted, the future scene image can then be retrieved by measuring similarity within the learned feature space, enabling applications that require foresight into upcoming visual states.\nHuman action prediction: from an egocentric perspective and informed by previous actions, EgoAgent can generate plausible future human actions $A_t$ as a sequence of 3D skeletons by $A_t = \\Theta(I_0, A_0, ..., A_{t-1}, I_t)$. This predictive capability is essential for applications in humanoid robotics, virtual environments, and interactive gaming, where anticipating human-like movements is crucial."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "As humans perceive the world through the first-view perspective, to imitate the learning process of humans, we train the proposed EgoAgent on two egocentric video datasets, i.e., Walking Tours (WT) [47] and Ego-Exo4D [21]. WT is a video-only dataset, which comprising approximately 1.5 million high-resolution frames captured in various cities worldwide. Ego-Exo4D has 221.26 egocentric video hours with 5035 takes, accompanied by 376K manually labeled 3D body poses and 9.2M automatically generated 3D body poses. We undistort the raw sensor data captured from fisheye cameras into pinhole cameras to eliminate the imaging projection difference between Ego-Exo4D and WT. We apply a 20-frame sliding window filter to ensure continuous pose sequences, resulting in 1,410,119 clips with synchronized egocentric videos and 3D body poses. Based on the"}, {"title": "4.2. Implementation Details", "content": "To alleviate the training burden, we sample one image frame every 5 frames from the video clips, while keeping all 3D body skeletons. Specifically, for each clip with 20 frames, we divide it into $T_s$ =4 time steps and sample the first image frame from each time step. This results in every time step consisting of one image frame paired with five frames of 3D body poses. For representation learning on egocentric videos, we follow the practice in [47] to avoid cropping noisy positive pairs. Following DINO [10], we adopt two global crops and six local crops.\nWe use IntermLM as the backbone in all our experiments. Following the configurations in [22], we train EgoAgent with two model sizes, i.e., InternLM-300M and InternLM-1B. We adopt the same Adam optimizer as that in IntermLM with $\\beta_1$ = 0.9, $\\beta_2$ = 0.999. We train EgoAgent on the datasets for 72,000 iters with a linear warm-up of 1800 iters. We set the base learning rate to 6 \u00d7 10-4 with a cosine decay scheduler. The loss weights are set as $A_{rep}$ = 2, $A_{pred}$ = 1, $X_{act}$ = 3, respectively. All models are trained from scratch using FP16 to speed up. The whole training takes 25, 60 hours in total with a batch size of 1920 on 32, 48 NVIDIA A100 GPUs for EgoAgent-300M, and EgoAgent-1B, respectively."}, {"title": "4.3. Represent: Image Classification", "content": "We examine the representative capability of EgoAgent, measuring the performance of classification on ImageNet-100 and ImageNet-1K [16] by k-NN. Concretely, we freeze the model and extract features of images from"}, {"title": "4.4. Predict: Egocentric Future State Prediction", "content": "To evaluate the effectiveness of EgoAgent in predicting future states of the world, we perform an egocentric video prediction task with feature retrieval. Specifically, given the egocentric frame and 3D human skeleton sequences as actions at the current time step, the query set stores the features of the egocentric frame at the next time step predicted by models, while the gallery set contains the features directly extracted from the next frames. At the time step t, if the predicted future state $S'_{t+1}$ in the query set can correctly retrieve scene state $S_{t+1}$ features in the gallery, we treat it as a successful future state prediction. For representation models, there are no actions to modify the input image features, the predicted future state $S'_{t+1}$ is exactly the scene state $S_t$ at time step t. To avoid retrieving the exact same features, when querying with input image $I_t$, we ignore the scene state $S_t$ in the gallery set. Following common practice in retrieval tasks, we use Top1 accuracy and mAP as metrics."}, {"title": "4.5. Act: 3D Human Motion Prediction", "content": "We compare our method with state-of-the-art video-based motion generation Diffusion Policy [13] and human motion prediction models [23] using the egocentric human skeletons from Ego-Exo4D [21]. Every testing sample with time steps $T_s$ = 4 contains 4 frames of egocentric images and 20 frames of 3D human skeleton. To compare these two types of motion prediction models, we set the prediction targets as the last 15 frames of 3D human skeletons. 4 frames of egocentric images and the first 5 frames of 3D human skeletons are adopted as inputs for video-action models and human motion prediction models, respectively. For EgoAgent, both egocentric images and the first 5 frames of human skeletons are given to predict future actions. Following Ego-Exo4D [21], the mean per joint position error (MPJPE) in centimeters (cm) and the mean per joint velocity error (MPJVE) in centimeters per second (cm/s) for 3D joint positions and velocities are adopted as evaluation criteria."}, {"title": "4.6. Ablation Study", "content": "To demonstrate the effectiveness of special designs in EgoAgent, we conduct several ablations with a short learning schedule of 14,400 iterations using EgoAgent-300M.\nIntegration of represent, predict and act. As EgoAgent achieves promising performance in representing, predicting, and acting, we explore how these three tasks influence each other during training. We conduct leave-one-out experiments (as shown in Table 4), where we remove one task from the training process each time. First, removing any task will lead to worse performances, indicating that these three tasks, as foundation tasks of human learning from the world, can contribute to each other. Specifically, on video prediction tasks, removing the representation loss leads to a significant performance decrease with -11.87% Top1 and 13.55% mAP, demonstrating that representation is essential for predicting future states. Correspondingly, removing the prediction supervision also leads to the representation performance decrease of -2.52% and -1.31% Top1 accuracy on ImageNet-100 and ImageNet-1K, respectively. This result demonstrates that the representation ability can be strengthened by predicting future states, which is similar to the process of how humans learn from the world. Second, the motion prediction task is less sensitive to the removal of the representation learning task than the future state prediction task. This result indicates that EgoAgent can formulate the internal causal relationship between future state prediction and action prediction tasks, bringing additional improvement when jointly learning these two tasks.\nTrain world model on feature space. To ablate the effectiveness of training world models on feature space, we adopt a pretrained VQGAN [18] as the image tokenizer to formulate the entire task as the next-token prediction task on image and pose tokens. As shown in Table 4(d), training EgoAgent on the VQGAN latent space leads to significant performance drops on both future state prediction and image classification tasks, showing the effectiveness of train-"}, {"title": "5. Conclusion and Limitations", "content": "In this paper, we presented EgoAgent, a joint predictive agent model designed to simultaneously learn to represent world observations, predict future states, and generate actions based on egocentric RGB input. EgoAgent integrates three critical abilities-perception, prediction, and interaction-within a unified transformer architecture. By aligning the representational spaces of these abilities and leveraging a teacher-student training mechanism, EgoAgent enables mutual reinforcement across tasks, facilitating improved learning and performance.\nThrough extensive experiments, we demonstrated that EgoAgent outperforms existing state-of-the-art methods in image classification, future state prediction, and 3D human motion prediction. However, we identify a key limitation as the sparse human skeleton representation, which excludes finger movements which are essential for precise tasks like object manipulation. Additionally, our model currently lacks long-term memory, limiting its ability to leverage historical context for tasks requiring extended temporal dependencies. Future work will explore further model improvements and scalability in more complex environments."}, {"title": "6. Additional Results on Embodied Tasks", "content": "To evaluate the broader applicability of our EgoAgent's learned representation beyond video-conditioned 3D human motion prediction, we test its ability to improve visual policy learning for embodiments other than the human skeleton. Following the methodology in [38], we conduct experiments on the TriFinger benchmark [52], which involves a three-finger robot performing two tasks: reach cube and move cube. We freeze the pretrained representations and use a 3-layer MLP as the policy network, training each task with 100 demonstrations."}, {"title": "7. Additional Results on Egocentric Future State Prediction", "content": "In this section, we provide additional qualitative results on the egocentric future state prediction task. Additionally, we describe our approach to finetune video diffusion model on the Ego-Exo4D dataset [21] and generate future video frames conditioned on initial frames as shown in Figure 5."}, {"title": "7.1. Visualizations and Comparisons", "content": "More visualizations of our method, DoRA, and OpenSora in different scenes (as shown in Figure 6). For OpenSora, when predicting the states of tk, we use all the ground truth frames from to to tk-1 as conditions. As OpenSora takes only past observations as input and neglects human motion, it performs well only when the human has relatively small motions (see top cases in Figure 6), but can not adjust to large movements of the human body or quick viewpoint changes (see bottom cases in Figure 6)."}, {"title": "7.2. Finetuning OpenSora on Ego-Exo4D", "content": "OpenSora V1.1 [60], initially trained on internet videos and images, produces severely inconsistent results when directly applied to infer future videos on the Ego-Exo4D dataset, as illustrated in Figure 5. To address the gap between general internet content and egocentric video data, we fine-tune the official checkpoint on the Ego-Exo4D training set for 50 epochs. OpenSora V1.1 proposed a random mask strategy during training to enable video generation by image and video conditioning. We adopted the default masking rate, which applies: 75% with no masking, 2.5% with random masking of 1 frame to 1/4 of the total frames, 2.5% with masking at either the beginning or the end for 1 frame to 1/4 of the total frames, and 5% with random masking spanning 1 frame to 1/4 of the total frames at both the beginning and the end.\nAs shown in Fig. 5, despite being trained on a large dataset, OpenSora struggles to generalize to the Ego-Exo4D dataset, producing future video frames with minimal consistency relative to the conditioning frame. While fine-tuning improves temporal consistency, the moving trajectories of"}, {"title": "8. Additional Results on 3D Human Motion Prediction", "content": "We present additional qualitative results for the 3D human motion prediction task, highlighting a particularly challenging scenario where egocentric observations exhibit minimal variation. This scenario poses significant difficulties for video-conditioned motion prediction, as the model must effectively capture and interpret subtle changes. As demonstrated in Fig. 7, EgoAgent successfully generates accurate predictions that closely align with the ground truth motion, showcasing its ability to handle fine-grained temporal dynamics and nuanced contextual cues."}, {"title": "9. OpenSora for Image Classification", "content": "In this section, we detail the process of extracting features from OpenSora V1.1 [60] (without fine-tuning) for an image classification task. Following the approach of [54], we leverage the insight that diffusion models can be interpreted as multi-level denoising autoencoders. These models inherently learn linearly separable representations within their intermediate layers, without relying on auxiliary encoders. The quality of the extracted features depends on both the layer depth and the noise level applied during extraction. As shown in Table 6, we first evaluate k-NN classification performance on the ImageNet-100 dataset using three intermediate layers and five different noise scales. We find that a noise timestep of 128 yields the best results, with the"}, {"title": "10. Data Preprocess", "content": "For egocentric video sequences, we utilize videos from the Ego-Exo4D [21] and WT [47] datasets. The original resolution of Ego-Exo4D videos is 1408\u00d71408, captured at 30 fps. We sample one frame every five frames and use the original resolution to crop local views (224x224) for computing the self-supervised representation loss. For computing the prediction and action loss, the videos are downsampled to 224x224 resolution. WT primarily consists of 4K videos (3840\u00d72160) recorded at 60 or 30 fps. Similar to Ego-Exo4D, we use the original resolution and downsample the frame rate to 6 fps for representation loss computation. As Ego-Exo4D employs fisheye cameras, we undistort the images to a pinhole camera model using the official Project Aria Tools to align them with the WT videos.\nFor motion sequences, the Ego-Exo4D dataset provides"}, {"title": "11. Training Details", "content": ""}, {"title": "11.1. Architecture Configurations", "content": "In Table 7, we provide detailed architecture configurations for EgoAgent following the scaling-up strategy of InternLM [44]. To ensure the generalization, we do not modify the internal modules in InternML, i.e., we adopt the RMSNorm and 1D ROPE. We show that, without specific modules designed for vision tasks, EgoAgent can perform well on vision and action tasks."}, {"title": "11.2. Training Configurations", "content": "In Table 9, we provide the detailed training hyperparameters for experiments in the main manuscripts."}]}