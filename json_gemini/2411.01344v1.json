{"title": "Can Humans Oversee Agents to Prevent Privacy Leakage? A Study on Privacy Awareness, Preferences, and Trust in Language Model Agents", "authors": ["ZHIPING ZHANG", "BINGCAN GUO", "TIANSHI LI"], "abstract": "Language model (LM) agents that act on users' behalf for personal tasks can boost productivity, but are also susceptible to unintended privacy leakage risks. We present the first study on people's capacity to oversee the privacy implications of the LM agents. By conducting a task-based survey (N = 300), we investigate how people react to and assess the response generated by LM agents for asynchronous interpersonal communication tasks, compared with a response they wrote. We found that people may favor the agent response with more privacy leakage over the response they drafted or consider both good, leading to an increased harmful disclosure from 15.7% to 55.0%. We further uncovered distinct patterns of privacy behaviors, attitudes, and preferences, and the nuanced interactions between privacy considerations and other factors. Our findings shed light on designing agentic systems that enable privacy-preserving interactions and achieve bidirectional alignment on privacy preferences to help users calibrate trust.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in language models have led to new applications of Language Model agents (LM agents) such as AutoGPT [81], AgentGPT [2] and personal.ai [39]. Unlike basic language models or non-agentic Al systems, LM agents are inherently endowed with agency, allowing them to (semi-)automatically handle complex real-world tasks, such as accessing and retrieving information from connected databases (e.g., a user's calendar) to generate and reply to emails [69, 99]. These LM agents free users from having to instruct the LM step by step, potentially increasing productivity. However, this increased agency also means that LM agents can make decisions with limited human supervision, which raises new privacy challenges, especially in interpersonal communication where the agents act on behalf of the user to share information with other people.\nWhat if LM agents share information the users did not intend to disclose? Prior studies found that even without malicious attackers, LM agents can have unintended privacy leakage in their actions [67, 89]. For example, Shao et al. [89] demonstrated a case where an LM agent accesses the user John's calendar data to generate an email reply, which shares the information that John is \"talking to a few companies about switching jobs\" in an email to John's manager without John's explicit consent. This issue not only risks violating one's own privacy but can also impact bystanders, as LM agents might inadvertently share information about other people in the users' connected database. Such unintentional privacy leaks occur because LMs lack the ability to understand and operate under contextual privacy norms, even when privacy-enhanced prompts are used [89]. Current studies mainly focus on measuring these privacy leakage [67, 89] and addressing them with model alignment [32, 40, 96], which aims to align Al models with human value (e.g., people's privacy preferences and norms). However, preserving people's privacy in the use of LM agents also requires a better understanding of human users, particularly their privacy awareness, preferences, and trust, which are key factors shaping people's privacy-preserving behaviors [12, 72, 75, 77].\nPrivacy awareness is an important prerequisite for users to make informed decisions about disclosing and sharing data [62, 78]. A lack of awareness of privacy risks can lead to unexpected consequences, resulting in uncomfortable, creepy experiences or even physical or financial harm [78]. Moreover, studying people's privacy awareness also sheds light on the impact of using different definitions of user preferences in the model alignment efforts. Gabriel [32] introduced multiple ways to measure user preference for model alignment, such as revealed preferences, focusing on model alignment with preferences as they are revealed through a person's behaviour, and informed preferences, focusing on the preferences that a person would have if they were fully informed and rational. However, in light of \"privacy paradox\" [46, 94], the revealed preferences (their actual behavior) and informed preferences might be different. A key factor contributing to the privacy paradox is the lack of privacy awareness [24, 78], where users are not fully aware of how their data is being used or what risks they are exposed to, leading to inaction despite a desire to protect their privacy [24, 25, 78]. Considering that the revealed preferences provide a practical measure for optimizing the LM agents' actions by observing human behavior, while informed preferences are closer to individuals' authentic preferences but are not directly observable and more costly to collect [36], studying users' privacy awareness helps bridge the gap between these two types of preferences and guide model alignments closer to users' true preferences in practice."}, {"title": "Trust and Privacy Behavior", "content": "Trust is a more direct factor that influence people's willingness to disclose information and engage with technologies [26, 93]. Higher levels of trust can lead to more willingness to disclose information [43, 116], while overtrust, often associated with \"misunderstanding the risks associated with an action\" [101], can result in people overlooking privacy risks and becoming vulnerable to privacy violations [103, 116]. Conversely, a lack of trust can hinder the adoption of technology and limit its benefits in people's daily lives. Research found that people are less likely to continue using AI systems if these systems make mistakes [11]. This highlights the importance of developing LM agents that can help users establish and calibrate their trust in delegating different levels of tasks to the LM agents, ensuring that the agentic systems are used effectively in people's everyday life in a privacy-friendly way.\nAlthough privacy awareness, preference and trust are key elements in privacy research and have been studied across various domains such as online browsing [85, 108], e-commerce [14, 95, 109], IoT and mobile sensing [79, 100, 103] and AR/VR [23, 27], the nature of LM agents brings unique challenges. The lack of transparency due to the automation of complex tasks makes it difficult for people to know what happens behind the scenes. Moreover, people could hold flawed mental models of the LM systems [114], which leads to incorrect expectations of the systems' behavior. These challenges could hinder people from understanding or anticipating unintended privacy leakage in the LM agent's actions [54, 114]. Having in mind both the importance of user privacy awareness, preferences, and trust, and the challenges raised by LM agents, this work aims to study these factors in LM agents, focusing on the unintended privacy leakage risk in asynchronous interpersonal communication tasks (e.g., writing emails and creating social media posts). We are interested in examining the following research questions:\nRQ1 What are people's primary concerns with LM agents, and what is the role of privacy?\nRQ2 How well do people oversee privacy leakage in LM agents' actions, and how does it relate to their privacy awareness and trust in AI?\nRQ3 How do people's behaviors (revealed preferences) differ from their informed preferences? How different are these preferences from person to person?\nTo answer these questions, we conducted a task-based online survey with 300 participants. Each participant was asked to complete an asynchronous interpersonal communication task, such as drafting an email response. They were then introduced to an LM agent and asked to choose between their own responses and a response generated by an LM agent, which included sensitive items that violate contextual privacy norms. They were then asked to explain their reasoning. Afterward, participants were informed of the privacy norm tuples in the scenario and asked to rate the perceived harmfulness of the sensitive items, which we used as the ground truth to evaluate each individual's privacy leakage. Our results show that participants often included fewer privacy leaks in their own draft, while many selected the response written by LM agents. 48.0% of participants preferred the LM agent's response or considered both responses equally good, despite the agent's response containing more privacy leaks, leading to an increase in privacy leakage from 15.7% to a range between 38.4% (only considering the strict preference of the agent's response) and 55.0% (also considering the \"both are good\" selections). Most appeared to be unaware of the privacy leaks in the agent's actions according to their selection rationales, exhibiting an overtrust toward the LM agent. These findings suggest that putting users in control of the final action of an LM agent is not adequate to prevent privacy risks, alluding to broader challenges in human oversight of AI systems. In addition to the known issues related to humans supervising a system handling complex, domain-specific tasks [16], there are also risks associated with the lack of capacity to oversee everyday tasks involving nuanced social norms."}, {"title": "METHODOLOGY", "content": "We conducted a task-based online survey study (N = 300) to understand people's capacity of overseeing the privacy leakage issues in LM agents and how it relates to people's privacy awareness and trust towards AI. In the study, we let participants examine how LM agents, powered by the state-of-the-art LLMs (e.g., GPT-4), handle asynchronous interpersonal communication tasks (e.g., replying emails, creating social media posts). We then asked them to indicate their selection of preferred drafts, eliciting insights into their privacy awareness, privacy preferences, and the decision- making process. The study has been approved by the IRB of our institute."}, {"title": "Survey Design", "content": "Survey Flow. The survey consists of four main parts and are detailed in Appendix A.\nFirst, each participant is randomly assigned one of the six asynchronous interpersonal communication scenarios such as writing email, and writes down their draft as how they normally would respond. The selected scenarios and rationales are detailed in Section 3.1.2.\nSecond, they are introduced to an imaginary LM agent, and answer the trust and comfortableness of delegating certain levels of tasks to it. We provided four choices of human delegating different tasks to the LM agent: Human retrieves related information \u2192 LM agent drafts the response \u2192 LM agent directly sends it to others (HAA); LM agent retrieves related information \u2192 Human drafts the response \u2192 Human directly sends it by themselves (AHH); LM agent retrieves related information \u2192 LM agent drafts the response \u2192 Human directly sends it by themselves (AAH); LM agent retrieves related information \u2192 LM agent drafts the response \u2192 LM agent directly sends it to others (AAA). Note that we exclude the situation where \u201cthe user only use the LM agent to draft the response or send the user-crafted response\" and \"LM agent retrieves information, the user drafts the response, and LM agent takes over the sending step\", because we want to stay consistent with our definition of LM agents that they can autonomously make decisions and perform actions without users' continuous intervention. Participants also write justifications of why they felt uncomfortable with certain use of the LM agents. Next, they see the response composed by the LM agent, and indicate their preference by selecting either \"prefer their own draft\", \"prefer the LM agent's draft\", or \"both look good\". Note that the survey has not mentioned anything about privacy or information leakage, as we aim to understand participants' preferences, concerns, and trust towards LM agents in the natural state.\nAfter the response comparison task, participants are instructed to review the LM agent's draft again. Texts that leak information violating the contextual privacy norms to some extent are listed separately for the participants to rate the harmfulness of the leakage. We ask how harmful they think including each of the information item in the draft response would be for the information subject's privacy. This step prompts the participants to investigate the information closer, highlighting the information category (e.g., \"Confidential strategy of the company where you work\"), the recipient (\"you Mum\"), and the impacted party (e.g., \"you and the company's privacy\") to elicit more reflections on the privacy impact and collect informed preferences (see the detailed prompts in Section A.2). Participants also annotated the LM agent's draft to specify any additional information they perceived as harmful to leak. They then answered their trust and comfortableness (including writing justifications) of delegating tasks to the LM agent again. This allows us to investigate participants' concerns and trust towards LM agents after being prompted about privacy issues.\nFinally, the survey concludes with basic demographic questions and a 10-item IUIPC [63] scale to measure participants' general privacy concerns. Note that in the survey, we use \"the AI agent\" to refer to the LM agent to make it easier for participants to understand.\nScenario Selection. To reduce the bias caused by using a single scenario and cover a diverse set of privacy norms, we selected six scenarios from the PrivacyLens dataset [89], as summarized in Table 1. The PrivacyLens dataset benchmarks the state-of-the-art LLMs on their capabilities to preserve user privacy in interpersonal communication tasks. It leverages a sandbox environment to emulate the process of the LM agent retrieving user data from tools like emails and calendars and generating a response based on them. It factors in negative privacy norms sourced from privacy regulations, usable privacy research, and crowdsourcing studies to generate synthetic user data that contain sensitive information, which, if shared, can lead to violations of the privacy norm. To facilitate evaluation, PrivacyLens provides a list of pre-labeled sensitive information from the synthetic data that violates the privacy norm."}, {"title": "Data Collection", "content": "We recruited participants through Prolific\u00b9 and hosted data collection on Qualtrics\u00b2. Participants are eligible for the study if they are located in the United States and over 18 years old. We randomly assigned each participant a scenario using the Randomization function in Qualtrics, which enabled us to track the number of each scenario assigned. We also asked participants to indicate whether they can understand and relate to the scenario easily. We used this information for data cleaning to ensure that the responses are from people who found the assigned scenario relatable.\nTo minimize the risk of participants using AI to generate their natural response, we disabled the copy function for all scenario materials as well as the paste function of the user response text input area. We also used the following exclusion criteria to control the quality of data collected. First, we filtered out responses with a completion time of less"}, {"title": "Analysis Methods", "content": "Privacy leakages in the drafts. We define two types of privacy leakages: objective privacy leakage and subjective privacy leakage. Objective privacy leakage refers to the presence of pre-labeled sensitive items defined by general privacy norms from the PrivacyLens dataset (Section B.1), in either the participant's or LM agent's drafts. Objective privacy leakage for participant n is represented as a set $OL_n = \\{item_i | oli = 1, i = 1, 2, ..., I_n\\}$, i.e., among the $I_n$ pre-labeled sensitive items in the case assigned to the participant n, $oli=1$ if the item is included in the draft and $oli=0$ otherwise. Subjective privacy leakage accounts for the participants' individual perceptions of harm. An item is considered a subjective privacy leakage if the participant both includes the item in their draft and rates it as harmful. Subjective privacy leakage for participant n is represented as $SL_n = \\{item_i | oli = 1 and h_i = 1, i = 1, 2, . . ., I_n \\}$, where $h_i=1$ if the participant rate the item as \"Extremely Harmful\", \"Very Harmful\" or \"Somewhat Harmful\" to be disclosed. To label whether certain items are included in the drafts, two researchers reviewed a subset of drafts (P1-20) and established guidelines for determining whether information was included or not. They then iteratively labeled the drafts, calculated inter-rater reliability (IRR), and resolved discrepancies after each round of labeling. After two rounds (P1-50), a high IRR (Gwet's AC1 = 0.836) was achieved. The researchers then collectively labeled the remaining drafts and the LM agents' drafts following these guidelines (Section B.2).\nQualitative analysis of justifications for draft selection and concerns. We used a bottom-up coding method to qualitatively analyze participants' responses regarding their justifications for draft selections and concerns about the LM agent before and after seeing the agent's drafts. In the first round of coding, two researchers independently coded a subset of data (P1-60) to develop a codebook. They held daily meetings to discuss the codes, resolve discrepancies, and iteratively merge their codebooks. By the end of this round, an initial codebook with 44 codes was developed. The two researchers then worked together to conduct axial coding, merging similar codes and grouping them into higher-level themes related to justifications for draft selection and concerns about the LM agent. In the second round of coding, the two researchers independently coded responses from P61-300 using the revised codebook. Any necessary changes to the codes and themes were discussed and agreed upon during daily meetings. We did not calculate inter-rater reliability as the primary goal was to identify emergent themes [65]. The consensus of all coding results was achieved via collective discussions. The final codebook contains 16 codes grouped into 4 themes under the \"Justification for draft selection\" category, and 13 codes grouped into 2 themes under the \"Concerns towards the LM agent\" category (see the codebook in Appendix C).\nClustering analysis of participants. We further combined participants' privacy behaviors, draft selection and trust of LM agents to identify users with distinct patterns of reacting to an LM agent's action. We chose the hierarchical agglomerative clustering algorithm, because its bottom-up analysis can provide interpretable results compared with other clustering algorithms. To prepare for the clustering analysis, we selected five features as summarized in Table 3: number of information items rated as harmful, participant's choice of response, mention of privacy concerns before and after seeing the LM agent's response with leakage, individual subjective leakage rate $SLR_n = \\frac{\\frac{SL_n}{|H|}}{|H|}}{ \\{i|h_i = 1, i = 1, 2, ..., I_n\\}}$ where $H = $and $|H|$ represents the total number pre-labeled items rated as harmful by the participant, and overall trust of LM agents before and after seeing the LM agent's response with leakage. When $H = 0$, we define the $SLR_n = 0$ as the participant doesn't rate any item as harmful, thus not leaking any perceived harmful items.\nWe experimented with three distance measurements, Euclidean distance [47], Manhattan distance [50], Canberra distance [51], with four clustering methods, Ward's method [104], Centroid Linkage method [97], Average Linkage method [97], and Single Linkage method [33], using Python sklearn and scipy packages. We selected the best model with the number of clusters following the method in Lin et al. [59]. First, we examined all the dendrograms, excluded the ones that were very skewed, or resulted in too many clusters at once at a distance level. We then validated the selected model combining three metrics: Connectivity score [37] which evaluates the degree of connectivity of the clusters, and Silhouette Score [83] and Dunn Index [29] which measure the cluster compactness and separation. Our final selection is a hierarchical agglomerative model using Euclidean distance and fitted using Ward's method with k = 4."}, {"title": "Methodological limitations", "content": "Our study methodology has a few limitations that should be considered when interpreting the results. First, most participants do not have experience using LM agents, which might affect the results as it may not represent the status when LM agents reach critical mass adoption. This is an inherent limitation as we intentionally take a proactive approach to examine the potential implications of emerging technologies that evolve at a fast speed. To mitigate this, we use a task-based design, grounding the study in a relatable context, and showing actual LM agent responses to elicit their reactions. Additionally, most (93.3%) participants have interacted with an LM chatbot, which has a similar inference time leakage issue [102]. Second, the drafting task was designed to establish a reference point for users' natural decisions without LM agents. However, asking users to draft a response adds more scaffolding to the task that is not part of the actual use case of LM agents. This might trigger additional reflections and engagement, which could be beneficial in increasing awareness, suggesting that the privacy issues revealed in our studies could have been more severe if LM agents are widely adopted and require further research. Finally, we selected six scenarios to cover a relatively broad range of real-life LM agent applications where privacy leakage risk is present. Because each participant was randomly assigned one scenario, the scenario contexts may potentially have affected their responses. We have verified that all but one resulting cluster covered all six selected scenarios, and the remaining cluster (the smallest one) covered five scenarios, which suggests that the user privacy profiles are not strongly associated with particular scenarios. However, it requires further research to tease out the impact of the scenario and the participants' individual characteristics."}, {"title": "RESULTS", "content": "In this section, we present our findings as follows:\n\u2022 Section 4.1 People's primary concerns with LM agents, to answer RQ1.\n\u2022 Section 4.2 People's capacity in overseeing privacy leakage of LM agents' actions, and Section 4.3 Clusters of participants, to answer RQ2.\n\u2022 Section 4.4 Privacy preferences, to answer RQ3.\nNote that we have made minor adjustments to certain quotes to fix grammatical errors."}, {"title": "Primary Concerns with LM Agents", "content": "Before diving into privacy-specific concerns, we first examine the broader concerns people have with LM agents assisting in their tasks, to understand where privacy fits into their overall considerations. Our findings show that people's primary concerns regarding using an LM agent, in a natural state without being prompted about privacy, are mostly not related to privacy. As shown in Figure 3, only 15.3% (46/300) of the participants brought up privacy concerns before seeing and evaluating the LM agent's draft, while the ratio increases to 36.7% (110/300) afterwards.\nGeneral or other concerns. Most of the participants (before: 81.3%, 244/300; after: 64.7% 194/300) expressed general concerns or concerns unrelated to privacy when delegating tasks to LM agents.\nLM agents can make mistakes (88/300). Many participants feared that LM agents could make mistakes, such as misinterpreting their data \"not sure how good the AI will be at interpreting and understanding my notes\" (P198), retrieving irrelevant information \u2013 \"It shares specific work details that might not be relevant or interesting to my mom\" (P162), and generating misleading or dangerous content \u2013 \"AI often makes up facts\" (P295). Some of them can also foresee the corresponding harmful consequences. For example, P108 was concerned that the LM agent \"created egregious grammatical mistakes that might impact the view of our societal members\". P24 worried that the agent could fail to paraphrase properly and lead to \"copyright infringement being breached\".\nDoubts about Al's ability to handle complex social or moral tasks (31/300). Some participants doubted the LM agent's ability to handle tasks that require a deep understanding of human complexities. As P206 noted, these tasks often require \"a deep understanding of cultural nuances, personal judgment, and creativity because those are areas where AI might not fully grasp the context or deliver the level of authenticity needed\". Others believed that social communication requires moral considerations, while \"Al agents have not proven mature or sophisticated enough to successfully interpret moral from immoral, or ethical from unethical\" (P200).\nInappropriateness of using Al for this task (22/300). Rather than doubting the LM agent's ability, some participants felt it was inappropriate, unethical, or unprofessional to use an LM agent for the given social task. For example, P132, who was assigned a task to post on a Facebook blog as a therapist, said \"I also feel it's a bit disingenuous to publish Al-generated content as a therapist when the individual expertise, warmth, and care of a therapist is the literal product you're selling\". Similarly, P182, assigned to reply to a message from their mom, felt using an LM agent would be insincere, stating \"It would hurt my mom's feelings if she knew I was using AI to communicate with her.\"\nConcerns with being represented by AI (45/300). Participants were aware that LM agents were taking on part of their agency, especially in social communications where the agent's actions would be seen as people's own. P6 expressed concern, stating, \u201cIf my reputation is on the line like this, I want to fact-check and proofread the post before it's published under my name online\". Others expressed discomfort with a sense of losing control, as P131 noted, \"it seemed to me as if it took control out of my hands and put it directly in the hands of the AI\".\nLess \"me\" (63/300). Participants also expressed concerns that AI-generated content might not fully capture their personal voice, such as tone that \u201csounds not like me\" (P160). Others noted a lack of emotional depth, as P77 explained, \"I think some messages need to be drafted personally so the reader can feel the emotional impact of the message\". Additionally, some participants felt that AI-generated content lacked the subtle nuances of personality, as P190 pointed out, \u201cAI technology has not evolved to a point where this sort of behavior could be guaranteed a genuine reflection of the users' personality or the data it is representing.\"\nIn addition to concerns, more than half of the participants (63.7%) stated they preferred to review the LM agent's draft before sending it, rather than allowing it to be sent automatically.\nPrivacy Concerns. More participants raised privacy concerns after reviewing the LM agent's draft and being explicitly prompted by the privacy norm tuples. Some mentioned privacy in general terms, such as \u201cit might intrude into my privacy\u201d (P241), while others offered slightly broader, but still general, concerns like \"The primary concern when dealing with Al is privacy. The need for disclosure of where, how, and to whom your data is being distributed is highly important\" (P61). More specific privacy concerns emerged after participants saw the LM agent's draft.\nConcern about privacy leakage in LM agent's actions (98/300). While 25 participants expressed concerns about potential privacy leaks without explicit prompting, this number increased to 88 after they reviewed the LM agent's draft. For example, P120 mentioned this concern even before any explicit privacy prompt, saying \"What if the AI says something cruel or gives out information that I wouldn't be comfortable sharing?\" P159 only raised this concern after seeing the draft: \"Like the example above AI included sensitive information that is not supposed to be public. I feel with my posting I can filter out anything I see that isn't fit to be made public.\" Many of these concerns were related to the privacy norms of the task scenario. Participants worried that the LM agent might share their personal information which could cause harm, disclose others' information unethically, or reveal company details and violate confidentiality principles. For example, P37, tasked with posting about an upcoming trip on Facebook, stated \"My only concern is that the AI would include details about my trip that could allow for someone to steal my identity or trip.\" P130 who had to reply to a friend (Emily) about other candidates, expressed concern about the agent sharing other people (Michael)'s information: \"I think any human would agree that it's unfair to tell Emily all of these details about Michael's private life and interview preparation; it violates his trust and privacy and quite frankly isn't professional to do so.\u201d P20, assigned to create a Facebook post updating followers on the latest literary news, mentioned concerns after seeing the draft, saying \u201cit could include the release date which is apparently super important for the marketing campaign and shouldn't be released at this time\u201d.\nConcerns about LM agents' access to data(18/300). People were also concerned about LM agents accessing their data, specifically data in applications mentioned in the scenarios, like Google Calendar, Notion, Messenger, Slack, and Facebook. Unlike concerns about privacy leaks in the LM agent's actions where more participants raised issues only after seeing the AI draft, the number of participants who raised concerns about data access was similar before (14/300) and after (12/300) viewing the draft. For example, P54 expressed discomfort with the LM agent accessing their social media accounts, noting that", "security": "Others were skeptical of the system's ability to maintain data privacy. P127 stated, \u201cI think the biggest thing is AI reading my personal messages, I can hope it's secure and private but once things are placed on the internet it can truly go anywhere.\u201d\nDoubts about Al's ability to preserve privacy (18/300). Similar to doubts about Al's ability to handle social and moral complexities, some participants questioned Al's capacity to preserve privacy. Several participants doubted AI's general ability to safeguard information, with P57 commenting, \u201cI don't know if the AI understands how to protect information security and privacy", "Al isn't very careful about my privacy": "P191). Participants also questioned Al's ability to understand what information should remain private or confidential. For example, P70 mentioned", "public": ""}, {"title": "Evaluating People's Capacity in Overseeing Privacy Leakage of LM Agents' Actions", "content": "We then examine participants' selection of the preferred response and the selection rationales as a window into people's awareness of privacy leakage of an LM agent in action, and how using an LM agent potential affects the actual privacy leakage than not using the LM agent. Note that all the data we analyzed was from participants who found the scenario relatable, and were not prompted about privacy when making the selection, thereby ensuring that the study setup emulates a natural decision-making situation.\nWith the involvement of LM agent, the privacy leakage rate increased. The overall average individual subjective leakage rate (SLRavg), representing the average percentage of perceived harmful information disclosed among all participants, was 15.7% in their natural responses. However, this rate increased to 55.0%, reflecting a 250.3% rise in SLRavg, as 48.0% of the participants favored the LM agent's response or considered both the LM agent's response and their response good. We consider both the selection of the LM agent alone or \"both are good\" as in favor of the LM agent's response, as choosing the LM agent response is the default option in a real-world LM agent use case, and both situations indicate the users' have no objections to the agent's proposed action. If only counting participants' favored the LM agent's response as leakage, the rate increased to 38.4% (increased by 144.5%). Note we have factored in the subjective ratings of information harmfulness and only counted the leakage that the individual considered harmful. The total number of responses containing subjective leakage (prefer AI or both options) increased from 71 to 181, resulting in a 154.8% rise in the number of responses with subjective leakage due to the involvement of the LM agent."}, {"title": "Resulting clusters of participants", "content": "The hierarchical agglomerative clustering of participants based on their privacy behaviors, choice of preferred drafts, and trust towards LM agents results in four clusters. Figure 4 illustrates the dendrogram and highlights the resulting clusters in different colors. After examining the average persona, we assigned each cluster a name to describe the characteristics of participants of this type. The four clusters are Privacy Advocate (28.3%), Humanity Proponent (25.0%), AI Optimist (37.0%), and Privacy Paradox (9.7%).\nFor each cluster, we tested the change of overall trust and comfortableness with Wilcoxon signed rank test [82] and tested the change of mentioning of privacy concerns in the reasons before and after with Chi-squared test [66], both with Bonferroni correction [88]. We chose these non-parametric tests because they are suitable for the ordinal data and provide reliable results without assuming the data follows a normal distribution. For Wilcoxon signed rank test, we applied the correction for trust and four types of task delegation comfortableness respectively."}, {"title": "Privacy Advocate (85/300, 28.3%).", "content": "The Privacy Advocate group exhibits good privacy awareness on both of the information shared and privacy leakage in the LM agent's action. They prioritize privacy concerns when they did decision-makings, and carefully consider the risks of delegating tasks to LM agents.\nThe participants in this group preferred their own response over the LM agent's response because they identified the privacy leakage in the agent's response. When explaining their selection, participants showed privacy concerns about LM agents sharing too detailed information of themselves, other people or entities. P134 mentioned he did not like the LM agent's draft because \"that's too excessive for sharing with the public\u201d. P109, who was assigned the Job Seeking scenario, mentioned her hesitation of revealing other people's information that might hurt that person, \u201cI feel like I am not using someone else's situation (in this case it is detrimental to that candidate) to get my foot in the door.\u201d P96 expressed similar concerns, and said it is best to not mention any sensitive information about other people without receiving their consent to do so. P293 also opted for her own draft because she identified the agent's draft leaked book releasing date in detail, \u201c(The LM agent) seems to have grabbed info that seems a little confidential and elaborated a bit so I wouldn't like that.\"\nAll participants rated at least one PrivacyLens pre-labeled privacy-norm-violating information item as harmful. 29 of them also identified additional harmful information in the agent's response beyond the pre-labeled items, which suggests that they are privacy-conscious and intend to protect more information.\nThe participants' overall trust and comfortableness about delegating tasks to LM agents in all steps decreased after seeing the LM agent response. The median of trust regarding the LM agent decreased from 5-Slightly Trust to 2-Distrust (p-value = 2.17e-12 < 0.001/4), as shown in Figure 5. The median of comfortableness decreased on three levels as shown in Figure 6a: HAA from 3-Somewhat Uncomfortable to 2-Very Uncomfortable (p-value = 2.08e-4 < 0.01/16), \u0410\u0410\u041d from 6-Very Comfortable to 5-Somewhat Comfortable (p-value = 7.93e-4 < 0.01/16), AAA from 2-Very Uncomfortable to 1-Extremely Uncomfortable (p-value = 3.99e - 7 < 0.001/16). Before, their concerns are mainly about language models making mistakes and that language models may not satisfy users' personalized needs. After seeing the response with leakage, they are more reminded of privacy concerns (p-value = 1.14e - 6 < 0.001/4), with 57.7% of participants mentioning it compared with 21.2% before, both of which are higher than the average level of the entire sample. Their privacy concerns include that LM agents can share sensitive information of which LMs lack the understanding of privacy implications. Some participants were further reminded of privacy concerns in the information retrieval step. For example, P121 expressed more worries regarding the LM agent having access to their social media and online activities, \"...allowing the Al access to my Facebook account login details, that raises a lot of data security concerns.\""}, {"title": "Humanity Proponent (75/300, 25.0%).", "content": "Participants in the humanity proponent group favor their own responses which contained less privacy leakage than the LM agent's, while their rationales about task delegation prioritized personality and humanity characteristics over privacy concerns.\n78.7% of participants in the humanity proponent group did not leak any information rated as harmful in their draft, while the remaining 21.3% leaked some but not all harmful information. 82.7% of the group preferred their response over the LM agent's response, mainly because it was considered more personal and natural (mentioned by 45.3% of the group) rather than privacy-preserving (mentioned by only"}]}