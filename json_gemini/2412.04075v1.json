{"title": "DOES YOUR MODEL UNDERSTAND GENES? A BENCHMARK OF GENE PROPERTIES FOR BIOLOGICAL AND TEXT MODELS", "authors": ["Yoav Kan-Tor", "Michael Morris Danziger", "Eden Zohar", "Matan Ninio", "Yishai Shimoni"], "abstract": "The application of deep learning for biology, including foundation models, has increased significantly in recent years. Some models are text-based, while others are trained on the underlying biological data, especially omics data of various modalities. Consistently comparing the performance of deep learning models for biology has proven challenging due to the diversity of training data and downstream tasks. Here, we utilize the fact that many models operate on the level of genes and propose a unifying benchmark by defining hundreds of tasks based on ground-truth gene properties collected from professionally curated bioinformatics databases. We collect properties of five types: (1) genomic properties, including predicting which genes can be methylated or which are dose-dependent; (2) regulatory functions, evaluating how the genes participate in cellular regulatory processes; (3) localization, including identification of differential expression in different tissues or sub-cellular localization; (4) biological processes, including predicting gene involvement in pathways or disease prognostics; and (5) protein properties, including prediction of functional domains or post-translational modifications. These properties are used to define binary, multi-label and multi-class classification tasks. To create an architecture-agnostic benchmark we extract gene representation vectors from each model, including single-cell RNA-seq (scRNA) foundation models, large language models, protein language models, DNA foundation models, and classical baselines, and use them to train simple predictive models on the tasks. Depending on the model, we utilize the model's token-level embeddings of gene symbols or transform the gene symbol to an input appropriate for the model, i.e. a description of the gene for text models, the gene sequence for DNA models or amino acid sequences for the protein models. Using these embeddings on the benchmark tasks, we create a detailed assessment of the relative performance of the different models. In general, we find that text-based models and protein language models outperform the expression-based models on tasks related to genomic properties and regulatory functions, while expression-based models tend to outperform the others on localization tasks. We also observe performance for the classical bag-of-words baseline that is similar to the large language models for many tasks. By enabling broad systematic evaluation of diverse deep learning models in biology, this benchmark can help direct future research in artificial intelligence toward improved biological understanding and accelerated therapeutic discoveries. The code and benchmark data can be extended to more models and tasks and is available at http://github.com/BiomedSciAI/gene-benchmark.", "sections": [{"title": "1 Introduction", "content": "Recent successes in the application of self-supervised learning in natural language processing have given rise to foundation models, which are trained on a large unlabeled dataset and useful on a broad range of tasks (Bommasani et al., 2021). The potential to realize similar advances in biology has given rise to a new and rapidly growing cohort of biological foundation models, either as specialized language models or new models trained on biological modalities such as DNA sequences (Ji et al., 2021), amino acid sequences (Rao et al., 2021), electronic health records (Yang et al., 2022) or other modalities (Thieme et al., 2023). These models can identify functional sections of the genome, novel cell types, disease states, and more. Many of these efforts aim to identify the genes responsible for the various biological processes to identify future means to maintain these processes or restore their function. The proliferation of models and the potential for significant impact on human health gives rise to the need for robust evaluation and benchmarking. For text models, a number of biological/medical benchmarks have been published such as MedQA (Jin et al., 2021), and been widely adopted (Open Medical-LLM Leaderboard). Recent research provided a comparison of foundation models in specific modalities such as single cell models (Liu et al., 2023). However, no benchmarks have been proposed that compare foundation models across modalities or that compare text models against models trained on biological data directly. Part of the difficulty is that the downstream tasks that are used to compare scRNA FMs such as cell-type annotation, batch correction, and perturbation prediction (Ding et al., 2024) are very different from the benchmarking tasks used to evaluate NLP models, such as question-answering or sentence completion. The need for a benchmark that can work with text models and foundation models is particularly important in light of recent works such as Cell2Sentence (Levine et al., 2023), GenePT (Chen & Zou, 2023) and scInterpreter (Li et al., 2024), which have shown that text-models can be repurposed to work directly with transcriptomic data.\nHere we propose to use gene embeddings to create a new benchmark that enables comparison of biological foundation models across modalities and against text models. Gene embeddings are an inherent component of expression-based foundation models built on Transformer architectures (Vaswani et al., 2017), parallel to word embeddings in text models. They can also be produced using the gene symbol or gene description with a language model supporting text embedding. Smaller models such as gene2vec (Du et al., 2019) or even bag-of-words models on textual descriptions of the gene can also produce gene embeddings. As with text embedding benchmarks, it is assumed that the models producing better gene embeddings are learning the ground truth more faithfully (Muennighoff et al., 2022).\nTo evaluate the gene embeddings, we compile a wide range of ground truth biological knowledge about genes including their genomic properties, regulatory functions, localization, their involvement in biological processes, and their protein properties (Table 1). We connect the gene embeddings to the relevant tasks, and evaluate their performance as illustrated in Figure 1. Though each task captures only a small part of the biology involving the gene, collectively they offer a multi-faceted, panoramic view of the gene. Superior performance on this collection of tasks thus implies that the model's learned embeddings are more inherently meaningful and thus useful for diverse downstream tasks even without seeing labeled data for these tasks."}, {"title": "2 Benchmark tasks", "content": "Following decades of work in bioinformatics and related fields, large amounts of structured data regarding genes have been compiled through projects such as Reactome (Milacic et al., 2023), Human Protein Atlas (Uhlen et al., 2010; Human Protein Atlas), OpenTargets (Ochoa et al., 2022) and HUGO Gene Nomenclature Committee at the University of Cambridge (Seal et al., 2022). This enables us to compile a wide variety of validated properties to use to test the quality of gene embeddings. Our benchmarking package allows defining the tasks in general terms, which allows simple addition of new tasks with multiple identifier types (see S6.1. Notably, the benchmarking package is not limited to gene-tasks and can be easily extended to other modalities.\n2.1 Tasks Description\nWe compiled 312 gene properties, which we used to define evaluation tasks. Most of the tasks are based on single gene properties, while some are based on gene-pairs or links between genes and diseases.\nFor simplicity, we sort the properties into the following five families:\nGenomic properties This family of tasks evaluates the ability to predict properties inherent to the gene sequence, including predicting which genes can be methylated, and which genes are dose-dependent (their expression depends on the number of copies in the genome) There is a total of 7 tasks in this family. See Table S1 for a full description.\nRegulatory functions This family of tasks evaluates how the genes interact with other genes through the cellular regulatory processes and consists of a total of 6 tasks. These include predicting which genes are transcription factors, the number of connections in the gene-regulatory network, etc. See Table S3 for a full description.\nLocalization This family includes tasks for identifying differential expression and activity in different tissues or sub-cellular localization. That includes predicting protein levels found in blood, correctly assigning genes to expression clusters derived from various tissue samples, sub-cellular localization, etc. There are a total of 30 tasks in this family. See Table S4 for a full description.\nBiological processes This family evaluates the biological functionality of the gene by evaluating tasks such as involvement in pathways, being prognostic of survival, and being associated with a disease. This family consists of 29 tasks, representing the most diverse set of questions. See Table S5 for a full description.\nprotein properties This family focuses on properties of the protein product of the gene, including its functional domains, post-translational modifications, and its ligands.\nThese properties cover many of the biological roles that genes play, providing an indication of how well a given pre-trained model has captured various aspects of gene representation, allowing for differentiation between various types of models and training data. Users can use the performance on different task families to select pre-trained models for their use-case.\n2.2 Task Origin\nTo benchmark the pre-trained models, we aimed to collect properties that are as diverse as possible, capturing the many roles that genes and the proteins they code play in biology. For reliability and reproducibility we have opted for gene properties that are manually validated by hand and freely available, see Section S6.1.6 for data availability details.\nReactome The pathways tasks were curated by taking the full list of genes from the Human Genome Nomenclature Committee (HGNC) downloadable files (including protein-coding gene, non-coding RNA, pseudogene and"}, {"title": "2.3 Task Definition", "content": "Independent of the task family, each task evaluates a specific outcome type: a) a binary, or b) a multi-label assignment, or c) a multiclass, or d) a regression task. We used gene properties to define tasks only if at least 1% of the covered entities had the label. Binary sub-tasks were derived from multi-label tasks by selecting specific labels. For all tasks, we used the gene symbol as an identifier; ensemble stable IDs were converted into symbols using MyGeneInfo (Wu et al., 2012). To simplify comparisons between models we limited the scope of each task to the gene symbols shared by all encoding models."}, {"title": "2.4 Task Evaluation", "content": "In contrast to text embedding benchmarks such as MTEB (Muennighoff et al., 2022) where the quality of the model is assessed by its ability to generate similar embeddings for known similar texts, evaluating the biological properties of genes embeddings requires a slightly different approach. Because the genes have many biological properties, we cannot assess the model quality by similarity alone. For this reason, we have primarily adopted classification metrics: the embeddings are provided as inputs to a simple logistic or linear regression model to predict the ground truth properties, and evaluated with 5-fold cross-validation. The benchmark can also be defined using non-linear models, which could detect information in the representation vectors more successfully than a linear model, we discuss this and assess the differences in Section 5.\nThis setup enables us to evaluate whether the correct information is encoded in the vector without making a-priori assumptions about the underlying information properties of the embedding space."}, {"title": "3 Encoding Models", "content": "We selected several publicly available models for comparison from five major families: Large language models trained on text, deep-learning models trained on gene expression data, deep learning models trained on base pair sequences of genes, deep learning models train on amino acid sequences and classical machine-learning models. We used models that were openly available with weights. When available, we used top-performing models according to independent leaderboards. Table 3 provides a summary table of model properties, and the following is a brief description of each model. The gene-benchmark allows for simple integration of additional models and tasks (see Supplementary text S6.1)\n3.1 Text based models\nFor text embedding models, we create an embedding for a gene by extracting the standard symbol, full name, and description of the gene from the NCBI Entrez Gene database (Maglott et al., 2010). This information is packed into a textual description that is given to the model as a prompt in the format \"Gene symbol <symbol> full name <full name> with the summary <summary description>\", and this prompt is embedded using a sentence embedding model. As a result, the benchmark that we have defined works seamlessly with any model supported by sentence_transformers (Reimers & Gurevych, 2019). For this assessment, we selected the top performing models from the leading embedding benchmark, the MTEB leaderboard (MTEB Leaderboard) and from the sentence transformers leaderboard (Sentence Transformers Leaderboard). For simplicity and ease of replication, we limited ourselves to models that did not require to trust remote code as defined by the sentence_transformers API (trust_remote_code set to false). In addition to compare performance with a simpler non-parametric method, we used a Bag-of-words encoder\nMTEB-L A variant of Mistral 7B (Meng et al., 2024) called SFR-Embedding-Mistral, which is a transformer based generative LLM with 7.11B parameters. Chosen as the top performing open model on MTEB (MT\u0415\u0412 Leaderboard) as of May 2024.\nMTEB-S A compact sentence-embedding model with 335M Parameters (Lee et al., 2024) called mxbai-embed-large-v1. Chosen as the top performing small open model (<1B parameters) on MTEB (MTEB Leaderboard) as of May 2024.\nMPNet A transformer-based textual LLM (Song et al., 2020), pre-trained on over 160GB text corpora. Pretraining was done using masked and permuted language modeling learning. It was chosen since it was the top performing model on sentence transformers (Sentence Transformers Leaderboard) as of May 2024.\nBag-of-words A statistical word-based text model which does not take into account the order of words in the text. The presence of each word is used as an independent feature. We used CountVectorizer from scikit-learn (Pedregosa et al., 2011), with default parameters to select the top informative 1024 words, and used the word counts vector as the embedding for each description. The model was fitted to the text of the gene descriptions."}, {"title": "3.2 Gene expression and transformer-based models", "content": "Inspired by the success of transformer-based LLMs in NLP, these models aim to learn biology as a \"language\" over scRNA-seq readings, fitting an embedding to each gene as if it were a 'word' in an NLP model, and the transformer based architectures integrate the gene expressions into cell-level embeddings. We made use of a recent survey and benchmark to highlight the three best performing open scRNA foundation models (Liu et al., 2023). The gene embeddings were extracted from the publicly available model weights. Gene names were taken from the supplementary model configuration files.\nCellPLM A transformer-based foundation model for single-cell biology with over 80M parameters (Wen et al., 2023). Trained on scRNA-seq and spatially resolved transcriptomic (SRT), adding tissue level information. Trained using MLM variant, on 9 million scRNA-seq cells and 2 million SRT cells. Embedding extracted from the embedder.feat_enc.emb layer in the model downloaded (Wen et al.), with the gene names from the matching configuration file.\nGeneformer A transformer based foundation model for single cell biology with 10.3M parameters. (Theodoris et al., 2023a). This model represents the scRNA expression using a list of genes ranked by their normalized expression levels. This is intended to make the order significant, and allows the use of context-aware attention mechanisms similar to these that work well in NLP. The model is trained on about 30M scRNA-seq readings. Embedding extracted from the embeddings.word_embeddings layer from (Theodoris et al., 2023b)\nScGPT A generative foundation model for single-cell transcriptomics utilizing a self-attention, with 53M parameters (Cui et al., 2024). Pretrained using masked language model (MLM) training. Explicitly encoded genes, expression levels and conditions, concatenated to represent each gene in context. Training is performed using a masked language modeling variant, where masking is done with attention masking to accommodate for the non-sequential nature of the data. Embedding extracted from (Cui et al., b) following the instructions in (Cui et al., a), steps 1 and 2. We used two variants, blood (designated ScGPT-B) trained on 10.3 million blood and bone marrow cells and the human model (designated ScGPT-H) trained on 33 million normal human cells.\nGene2vec A 200 dimensional concept embedding of the human genes (Du et al., 2019), based on the concept of Word2Vec (Mikolov et al., 2013) and learned from co-expression patterns, shared Gene Ontology (GO) annotation, tissue-specific genes, and functional gene sets."}, {"title": "3.3 Base-pair models", "content": "Every gene can be mapped to its DNA sequence. There have been numerous recent advances in foundation models trained on DNA. Though not trained specifically to represent genes, by representing DNA they can generate gene representations as well. DNABERT-2 A BERT based genome foundation model (Zhou et al., 2024) trying to decode a linguistic representation of the genome. In this method they replaced the common k-mer tokenization with a Byte Pair Encoding (BPE) tokenization. This model performed well in a recent DNA model benchmark (Liu et al., 2024)."}, {"title": "3.4 Protein language models", "content": "Representing the protein products of a gene, may be considered a representation of the gene itself. Following the approach of SATURN (Rosen et al., 2024) and UCE (Rosen et al., 2023), we have represented the gene symbol as the mean of its protein product representation vectors.\nEvolutionary Scale Modeling-2 (ESM-2) SOTA general-purpose protein language model. A transformer model trained on sequences of natural proteins (Lin et al., 2023) which is able to generate novel proteins. The model was trained using the ESM Metagenomic Atlas that contains >617 million metagenomic protein sequences. We took the model esm2_t36_3B_UR50D with 3 billion parameters."}, {"title": "4 Results", "content": "We report our gene-benchmarks on eleven models, evaluated on all tasks. The benchmark results demonstrate that the various models exhibit different performance patterns on different tasks. When grouping the performance measures by family tasks and averaging across tasks we find that the four text-based models exhibit better performance for the genomic properties and regulatory function families, while the scRNA-based models performed better at the localization and biological process tasks (Figure 2). These trends are consistent when using other evaluation metrics such as F1 (see Figure S1). The calculation time for the benchmark varies depending on the model size. For the largest model, with 7B parameters and embedding size 4096 (Table 3), creating the gene embeddings took approximately 40 minutes on a single NVIDIA A100 80GB. The calculation time for fitting the predictive models is highly dependent on the embedding size, with the smaller embeddings requiring less than an hour to calculate all the benchmarks and the largest requiring 15+ hours on a 48-core Xeon E5.\nInterestingly, text-based models using transformer architecture only slightly outperform the bag-of-words model in most tasks. Furthermore, we do not see an advantage to the model size, where the MTEB-L model exhibits comparable performance to the smaller MTEB-S and MPNet models. Similarly, in the scRNA-based models the transformers usually slightly outperform the older gene2vec model, which is based on word2vec architecture and trained on bulk RNA expression data. ScGPT-H was the top performer in two different families of tasks. Here, too, we do not see a clear advantage for larger models, with cellPLM exhibiting comparable performance to the smaller ScGPT-H and even smaller Geneformer. ScGPT-H outperformed ScGPT-B significantly, which is likely due to the larger, more diverse, training data. Indeed, it is to be expected that a model trained on expression data from a single tissue would not perform well on tissue localization tasks related to other tissues.\nA closer examination of the mean AUC per task (Figure S2, Figures S3, S4, S5, S6, S7, S8, S9) reveals a more complex picture, where within each task family some tasks are dominated by text-based models and others by expression-based models or protein language models. This can also be seen by the high cosine similarity observed between overall task performance amongst models from the same type, as exhibited in Figure S10.\nThe protein models perform best at protein properties, but less so for biological processes and localization. DNABert-2 performs worse than the other models for all but the genomic properties, where it is comparable to the other model families.\nThe tasks themselves also show a clustering in performance, as shown in Figure S11, but also show a large range of dissimilarity, suggesting that the benchmark tasks correspond to distinct biological phenomena.\nAbove we used linear models for predicting gene properties from vectors. We consider the possibility that a more expressive model could perform better by training a multilayer perceptron model (MLP) on the binary tasks, comparing the MTEB-L and MTEB-S embeddings. We find that the performance is closely correlated to logistic regression, as seen in Figure S12. For this reason, we prefer the linear models which are not sensitive to hyperparameter selections and thus enable robust comparisons across many thousands of combinations of models and tasks. Nevertheless, the benchmark package code at http://github.com/BiomedSciAI/gene-benchmarksupports the use of any scikit-learn model.\nOne notable result is that text models outperform the scRNA models in most disease involvement tasks except in the Pathology tasks, chromosome, and N1 Network, indicating that there are exceptions to the general rules of model performance we outlined. Similarly, expression-based models outperform in cell-type localization tasks, but under-perform in sub-cellular localization tasks. This is in line with our expectations, given the close relation between cell-type, tissue-type and single cell RNA expression levels."}, {"title": "5 Summary and Discussion", "content": "We present a gene-centric benchmark that includes hundreds of tasks, sorted into functional families. We designed this benchmark to evaluate the gene embeddings provided by pretrained models applied to biology, thus suggesting a"}, {"title": "5.1 Limitations and Future Work", "content": "We gathered the benchmark tasks from actively maintained professionally curated sources and we have relied on their quality control processes. For many reasons, the entire genome is not studied evenly (Lee et al., 2019) and when genes are studied, the full diversity of human ancestry is not evenly reflected (Fairley et al., 2019). As biological research improves in performance and fairness, we look forward to updating our benchmark tasks accordingly. We used only open source models with released weights excluding models that did not (Zrimec et al., 2022). Though we have explored the benchmark tasks using gene embeddings, the tasks could be utilized in other ways, such as by defining fine-tuning objectives for deep learning models or even as the basis for question answering in text models. Such a strategy, while not applicable for all models, may uncover predictive power that is specific to each model."}, {"title": "6 Appendix", "content": "S6.1 The Gene-Benchmark Package\nThis freely available package was developed to facilitate easy access to the tasks and efficient use of them for benchmarking. It includes three main modules as well as notebooks and scripts that demonstrate the package's usability. The main flow of task evaluation using the package is described in Figure 1. Below we review the main modules used in the package, which is available at http://github.com/BiomedSciAI/gene-benchmark.\nS6.1.1 Tasks\nThis module contains two main parts. The first is the means to load the task definition according to task name in a generic format into a designated, easy-to-use object. The class allows easy access to the entity identifiers (usually gene symbols) and their outcomes. They will usually be a single columned data frame, but if the task includes multiple genes per instance (for example, gene-to-gene interaction), it will include multiple entities in a column structure. In the multi-label case, the output is also a multi-columned data frame. The second part is a pipeline class that manages the process from a task name to description (in the case of text-based models) to encoding, training a simple prediction model in a cross-validation fashion, and creating a report. Adding additional tasks is designed to be simple, all it requires is saving the task descriptions in a specific format.\nS6.1.2 Descriptor\nThe module manages the transition from an entity identifier into a text description. For gene symbols, we retrieve the description fields from NCBI using the MyGene.Info services and construct a description sentence. We allow predefined descriptions by creating a descriptor that loads the descriptions from a CSV file. This feature enabled us to download the disease description from open targets without needing to integrate with their service and facilitate easy introduction of new descriptions. We are also able to construct multi identifier types descriptors, thus enabling the creation of a descriptor that can describe tasks with different identifiers, such as in gene-disease association.\nS6.1.3 Encoder\nThis module manages the encoding of either the entity identifier or its textual summary. We enable encoding using any HuggingFace sentence transformer supporting module. In addition, we enable encoding using a pre-computed encoder by loading the encodings from a precomputed CSV file. This enables us to pre-compute the encoding from scRNA-based models. In addition, we enable the creation of a multi-entity type encoder that enables encoding each type of entity differently. For example, in the case of Gene-Disease association, we can encode the genes using pre-computed encoding and the disease using a sentence transform encoder.\nS6.1.4 Base models\nThe package supports any scikit-learn model. For the manuscript, we explored linear and logistic regression with the default scikit-learn parameters, and an MLP with three hidden layers of size 100 and 500 max iterations.\nS6.1.5 Scripts and Notebooks\nTo efficiently create benchmarks the package includes a command line interface. Enabling benchmarking multiple models (described in YAML format) on multiple tasks (supplied in the command line or in YAML) and output a single report in CSV format. An additional script is supplied that can extract the embedding of the given identifiers list. The package also includes a notebook demonstrating how the package can be used and how to create figures, as displayed in this manuscript.\nS6.1.6 Data availability and licensing\nAll of the data is from publicly available sources and the steps required to download and prepare the tasks for benchmarking are implemented in our GitHub repository. We did not produce the task data and do not redistribute the data used for the benchmark tasks. To reproduce the results shown here, we provide code to populate the benchmark task directly from the public sources. We do not own the task data and refer the users to the licenses of the data owners.\n\u2022 Reactome - The current task retrieval code downloads that pathway directly from reactome's current server, Reactome data is robustly backed at third party servers. Reactome content is readily accessible for download"}]}