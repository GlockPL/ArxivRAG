{"title": "Hacking, The Lazy Way: LLM Augmented Pentesting", "authors": ["Dhruva Goyal", "Sitaraman Subramanian", "Aditya Peela"], "abstract": "In our research, we introduce a new concept called \"LLM Augmented Pentesting\" demonstrated with a tool named \"Pentest Copilot,\" that revolutionizes the field of ethical hacking by integrating Large Language Models (LLMs) into penetration testing workflows, leveraging the advanced GPT-4-turbo model. Our approach focuses on overcoming the traditional resistance to automation in penetration testing by employing LLMs to automate specific sub-tasks while ensuring a comprehensive understanding of the overall testing process.\n\nPentest Copilot showcases remarkable proficiency in tasks such as utilizing testing tools, interpreting outputs, and suggesting follow-up actions, efficiently bridging the gap between automated systems and human expertise. By integrating a \"chain of thought\" mechanism, Pentest Copilot optimizes token usage and enhances decision-making processes, leading to more accurate and context-aware outputs. Additionally, our implementation of Retrieval-Augmented Generation (RAG) minimizes hallucinations and ensures the tool remains aligned with the latest cybersecurity techniques and knowledge. We also highlight a unique infrastructure system that supports in-browser penetration testing, providing a robust platform for cybersecurity professionals. Our findings demonstrate that LLM Augmented Pentesting can not only significantly enhance task completion rates in penetration testing but also effectively addresses real-world challenges, marking a substantial advancement in the cybersecurity domain.", "sections": [{"title": "I. INTRODUCTION", "content": "In the domain of cybersecurity, the automation of penetration testing (pentesting) represents a significant yet challenging milestone. Historically, the effectiveness of pentesting has been predominantly dependent on the skill and knowledge of human experts. This dependence has led to a bifurcation in the quality and cost of pentesting services: high-end, manual pentests are prohibitively expensive for many organizations, while more affordable options often rely on rudimentary automated tools that offer compliance rather than comprehensive security assurance. This dichotomy has contributed to a landscape where many organizations remain vulnerable to data breaches and regulatory penalties, despite ostensibly fulfilling pentesting requirements.\n\nLLM Augmented Pentesting is a novel approach to this problem, aiming to democratize access to effective pentesting. [1, 6] By leveraging advanced LLMs, we can assist penetration testers in enhancing their workflow efficiency. Pentest Copilot facilitates routine tasks such as documentation lookup, tool orchestration, and exploration of potential exploit strategies. Its unique contribution lies in its ability to automate aspects of the pentesting process without sacrificing the nuanced understanding and adaptability that human experts provide [3].\n\nThis research paper introduces details how the LLM Augmented Pentesting approach and how Pentest Copilot synergizes the vast, general-purpose knowledge bases of LLMs with the specific requirements of pentesting."}, {"title": "II. METHODOLOGY", "content": ""}, {"title": "A. Large Language Model", "content": ""}, {"title": "1. Selecting a class of models", "content": "Our research indicated that among various LLMs available, the GPT lineage of models from OpenAI stood out for its extensive knowledge on security toolings and use cases [5, 6]. Its global knowledge, large training dataset, encompassing a wide array of cybersecurity papers and topics.\n\nOur goal was to leverage a well-trained model capable of addressing cybersecurity and pentesting queries through guided instructions, before we proceeded to finetune our own model."}, {"title": "2. Surface Evaluation of GPT Models", "content": "We evaluated the GPT lineage of models starting out with GPT-3.5-Turbo, GPT-4 and GPT-4-turbo [7, 8].\n\nGPT-3.5-Turbo: While GPT-3.5-Turbo stands out for its speed, it falls short in accuracy, particularly in complex pentest scenarios. The model struggles with maintaining context over extended interactions, a critical factor in pentesting where referencing previous events or findings is essential due to less context size (4,096 tokens). Another limitation is lack of knowledge in general since the model is trained on the events up till September 2021, having access to latest practices and application of tooling is affected due to this. These limitations make it less suitable for nuanced and detailed pentest environments.\n\nGPT-4: GPT-4 marks a significant improvement in response accuracy. It excels in handling complex pentest scenarios, including understanding and referencing previous events within a pentest sequence"}, {"title": "3. Test Benching Models", "content": "We created a testbenching framework which mimics real-world pentesting scenarios. This framework included a range of activities from vulnerability assessments to threat modeling, across various network setups and systems. This involved hosting a 'boot2root box' [11, 12, 13], a purposefully vulnerable server, to test the capability of our models in achieving Remote Code Execution (gaining access to the server). Our approach spanned the entire spectrum of pentesting stages, from initial reconnaissance to the post-exploitation phase, providing a comprehensive and realistic assessment of the tool's effectiveness.\n\nEvaluating the model response had to be done keeping in mind the following criterias:\n\n\u2022 Accuracy: Correctness of the technical information, recommendations provided which included the commands to run, to the given testing scenario.\n\u2022 Response Structure: Correctness of the response format which was a predefined JSON structure with exact parameters.\n\u2022 Response Time: Speed of generating responses."}, {"title": "B. Prompt Engineering", "content": "Prompt engineering plays an important role in LLM Augmented Pentesting, ensuring that the model understands and responds to cybersecurity-related queries with precision. By crafting specific, context-aware prompts, the model can interpret technical language and ethical hacking related terminologies effectively [5, 6]."}, {"title": "1. Jailbreaking GPT", "content": "By design, GPT is not configured to specifically cater to pentesting queries. Its general-purpose design meant that it provided broad information across various domains, but due to security reasons and constraints placed by OpenAI [7, 8], the model would not respond to a pentesting query.\n\nThe key was to instruct GPT to assume the role of a \"Penetration testing assistant, collaborating with a security researcher\". [4] The instruction was given to GPT in its system prompt which is a constraint that it will follow at all times while returning a response. The different types of prompts are elaborated in the \u201cDissecting Prompts\" section .\n\nThis shifted the model's operational framework from a generalist to a specialist, focusing its vast knowledge base specifically towards pentesting."}, {"title": "2. Dissecting Prompts", "content": "We have utilized the Chat Completions API of the GPT4-turbo model. The primary input for Chat Completions API is the \"messages\" parameter, which is an array of message objects. Each message object has a \"role\" (either \"system,\" \"user,\" or \"assistant\").\n\n\u2022 System/Instruction Prompt\nThe system prompt at the initiation of Pentest Copilot explicitly outlines the objectives, enumerates constraints, and provides context regarding the configured tools for utilization in commands. This was crucial to prevent inadvertent tool name hallucinations and inaccurate information by the LLM.\n\u2022 User Prompt\nIn the user prompt, we dynamically adjust the instructions based on whether the user provides previous testing information for the pentest target or not. If the user provides previous testing details, we skip adding the prompt for the preliminary testing phase. However, if no previous testing information about the target is provided, the prompt with either a Domain or IP scan is added. The prompt also outlines the expected JSON response format for output and sets constraints, such as recommending a single command if possible. Emphasis is placed on prioritizing user input to ensure the pentest aligns with their preferences, avoiding autonomous decisions by the model. Additionally, if no target information is provided, Pentest Copilot asks for more details about the target."}, {"title": "3. Prompt Bifurcation and Context length", "content": "In developing Pentest Copilot as an ethical hacking assistant, one of the significant challenges we faced was managing the context length limitation of 4,096 tokens inherent in the GPT-3.5 model. This limitation posed a unique challenge, especially considering the complexity and depth required for effective pentesting assistance. Our approach to this challenge involved innovative strategies to optimize token usage and chain various steps efficiently."}, {"title": "D. Retrieval Augmented Generation", "content": "Retrieval-Augmented Generation (RAG) plays a pivotal role in enhancing LLM Augmented Pentesting, particularly in addressing the challenge of providing up-to-date and accurate information on cybersecurity tools and practices [15].\n\nOne significant challenge in developing the tool was ensuring that the LLM had access to the latest information on the usage, syntax, and modules of complex cybersecurity tools and frameworks like Metasploit [2]. The dynamic nature of these tools, with frequent updates and new modules, means that any static knowledge base quickly becomes outdated.\n\nTo prepare the vector database for RAG in Pentest Copilot, we collected and curated a comprehensive dataset of the latest modules, scripts, and usage guides specifically from Metasploit and MSFVenom. The dataset included structured information such as command syntax, use cases, and detailed documentation for these tools. This information was then processed to convert the textual data into vector representations, or embeddings, that capture the semantic meaning of the content. These embeddings were indexed and stored in the vector database, allowing for efficient similarity searches and quick retrieval of the most relevant and up-to-date information during LLM-driven pentesting sessions.\n\nHow RAG works in the system:\n\n1. Query Processing: When a user queries about a specific module or syntax, the LLM processes this query to understand the context and the specific information"}, {"title": "2. Information Retrieval:", "content": "The LLM then uses RAG to reach out to an external knowledge base, seeking the most relevant and current information related to the query."}, {"title": "3. Data Integration:", "content": "The retrieved information is integrated with the LLM's internal knowledge to generate a comprehensive, formatted, accurate response."}, {"title": "E. Preferred Tooling and Context", "content": "In the LLM Augmented Pentesting approach, a researcher may prefer to use tools and techniques they are familiar with. To accommodate this, users can select their preferred tools, and these choices are then integrated into the system prompt.\n\nThis customization allows the model to prioritize the user's chosen tools when suggesting commands. Additionally, the system prompt is designed to recommend alternative tools when deemed necessary. In such cases, the response provides a clear explanation for these recommendations, ensuring the user understands why an alternative tool might be more effective in a specific situation."}, {"title": "F. File Analysis with LLMs", "content": "In pentesting, researchers frequently encounter files that, when analyzed, can lead to new discoveries. However, LLMs cannot directly understand the content of these files. Therefore, the information within these files must first be converted into a plaintext format that LLMs can interpret.\n\nThe file analysis tool first checks the file format using the Linux \"file\" command. It then runs the appropriate analysis procedure based on the output of the file command. The tool also scans media files for any hidden file signatures, and recursively extracts and analyzes any additional files found.\n\nThe tool is currently designed to deal with such files by including large lines of text from the file in the output, limited by the allowed number of tokens to OpenAI's API [7, 8].\n\nFor different file formats, the following contextual data is extracted:\n\n1. ELF:\na. Security Settings of the file:\ni. NX bit\nii. RELRO\niii. Stack Canary\niv. PIE\nb. stdlib functions used in the file.\nc. Symbols present in the binary.\nd. Link Tye: Dynamic/Static.\ne. Whether the binary is a shared object or not.\nf. File Architecture.\n\n2. PE32/PE32+:\na. Dependencies of the executables.\nb. Number of sections in the executable.\nc. Security Settings present in the executable:\ni. Non-Executable Bit.\nii. Stack Canary.\niii. Structured Exception Handling.\nd. Name of Libraries used by the executable.\ne. Imported/Exported functions.\n\n3. XML/JSON/YAML:\nFor the configuration files, a minimalistic tree of all the parents and their possible child elements is created and structured to represent the hierarchical relationship between them.\n\n4. Media Files:\na. Exif data.\nb. Any hidden signatures of other file types.\n\n5. Plain Text Files:\na. A python3 library \u201cGuesslang\" is used to detect any programming language. Depending on the detected language, function names are then extracted using regex [16].\nb. If no language is accurately detected, only very large lines (> 100 words) are extracted and sent with the context.\n\nThe list of files that the File Analysis tool can analyze is currently limited to the following list:\n\n\u2022 Binary files - ELF and PE32+ file formats for Linux and Windows\n\u2022 Media Files - MP3, MP4, PDF, PNG, JPEG etc.\n\u2022 Configuration Files XML, OpenVPN, JSON and YAML."}, {"title": "III. INFRASTRUCTURE", "content": ""}, {"title": "A. Setting up Infrastructure", "content": "Pentest Copilot necessitates a platform that allows researchers to conduct seamless pentesting sessions with its assistance. Interruptions or connectivity issues can hinder the effectiveness of the testing process which is dodged by dynamically setting up cloud resources, specifically, spinning up an on-demand server instance called \"Exploit Box\" for each session.\n\n1. This ensures dedicated and isolated environments for every researcher. The Exploit Box houses a plethora of tools to aid the researcher.(Refer to Table 1)\n2. The use of graphical user interface (GUI) tools such as Burp Suite is crucial and common in pentesting. Enabling a desktop experience becomes essential for using these tools to be installed by researchers. To address this, we incorporate a full desktop experience through VNC (Virtual Network Computing), allowing users to seamlessly utilize GUI tools like Burp Suite within the pentesting environment.\n3. Managing session-specific data, including user files and artifacts, across multiple testing sessions on the same target is essential to ensure continuity of pentesting sessions. Ensuring persistence and accessibility of these files is crucial for an efficient workflow. To address this, we can implement a structured data storage solution. Each session is assigned a dedicated volume, providing a persistent storage unit. This volume is attached to the Exploit Box launched for that specific session, allowing users to store and retrieve session-specific data seamlessly."}, {"title": "B. Choosing the right security tools", "content": "The decision to integrate multiple open-source tools into Pentest Copilot was based on their open source nature and widespread use. For example, Ffuf is being used for directory brute forcing which provides the flexibility to choose the payload position and auto calibration for determining the baseline error response. These tools are frequently utilized by bug bounty hunters and security researchers to perform tasks essential to their work."}, {"title": "C. Socket communication with instances", "content": "Pentest Copilot utilizes sockets for real-time communication between the user, copilot infrastructure, and the exploit box server (refer Fig. 7).\n\nHere's an overview of how it functions:\n\n1. Initiation of communication: When a user starts an exploit box, a socket communication channel is established between the user (client side) and the infrastructure. In parallel, a Secure Shell (SSH) connection is formed between the infrastructure and the exploit box server. This setup ensures a secure and seamless flow of information.\n2. Command Processing and Execution: An interactive terminal-like interface is displayed to the users. Here, the user can input commands into the terminal, the input data is transmitted to the infrastructure via the socket connection. Commands received from the client are forwarded by Pentest Copilot to the exploit box server using SSH. Pentest Copilot maintains continuous communication with the exploit box to capture any feedback or results from the executed commands.\n3. Displaying Results: The results and outputs from the exploit box are sent back to the client side via the Pentest Copilot infrastructure, ensuring that the user receives immediate feedback and results displayed on their terminal interface."}, {"title": "D. VPN Configuration", "content": "Pentest Copilot is also capable of targeting entities located on private subnets. There are many controls put into place to ensure all parties interacting with Pentest Copilot can comfortably partake in a pentesting engagement be it professional, or for research purposes.\n\n1. Pentest Copilot has been engineered to connect to private subnets through an established OpenVPN server. This ensures secure and controlled access to the private network.\n2. Users may establish a secure connection to their home subnets using a tool called Chisel in case they don't have a VPN server setup for themselves. This ensures flexible and secure communication between Pentest Copilot and the researcher's home lab, supporting both professional pentesting and research in diverse settings."}, {"title": "E. Mitigating possible malicious use", "content": "A careful consideration is taken while creating security policies preventing researchers from potentially abusing the cloud resources allocated to them during testing. This is essential for maintaining the integrity of the testing environment and ensuring fair and ethical usage."}, {"title": "IV. RESULTS", "content": ""}, {"title": "A. Performance", "content": "The journey of Pentest Copilot's development has seen remarkable improvements in performance, particularly in the areas of response time, accuracy, and functionality. These enhancements have significantly elevated the tool's utility for pentesters, streamlining their workflow and providing more precise assistance.\n\nOriginally, Pentest Copilot took an average of 8-10 minutes to perform a series of critical tasks:\n\n1. Generate a Command: Crafting a specific command relevant to the current stage of the pentest.\n2. Summarize the Current Pentest State: Providing a concise overview of the pentest's current status.\n3. Update the To-Do List: Reflecting new tasks or findings in the pentest checklist.\nThis duration was a bottleneck, especially in scenarios where quick decision-making and rapid response were crucial.\n\nThe introduction of GPT-4-Turbo and the optimization of our prompting techniques marked a turning point. With GPT-4-Turbo's enhanced processing speed and our efficient prompt design, the time required for the above tasks has been nearly halved to about 4-5 minutes. Furthermore, the incorporation of JSON Mode within GPT-4-Turbo streamlined the data handling process, further contributing to this time reduction.\n\nThe integration of RAG has brought a new level of accuracy to the commands generated by Pentest Copilot. By accessing the most current and relevant information from a vast external knowledge base, RAG ensures that the commands and advice provided are not only timely but also highly precise and tailored to the latest cybersecurity practices.\n\nA significant addition to Pentest Copilot is the introduction of file analysis functionality. This feature enables pentesters to efficiently analyze findings and data collected during a pentest. It simplifies the process of sifting through complex datasets, logs, or reports, allowing for quicker identification of vulnerabilities, anomalies, or patterns. This capability is particularly valuable in post-pentest reviews, where thorough analysis is key to understanding the effectiveness of the pentest and planning future security strategies."}, {"title": "B. Shortcomings", "content": "Pentest Copilot's current capabilities are limited to the underlying LLM. This constraint implies that the system may not fully comprehend or execute complex operations associated with sophisticated testing tools or methodologies, potentially hindering its performance in scenarios demanding advanced capabilities beyond the scope of the LLM. These limitations include, but are not limited to:\n\n1. Not being able to mount zero-day attacks\n2. Unable to bypass security controls which are not known to the LLM\n3. Using attack patterns which may have been overused a lot resulting in easy detection by security systems.\nAt present, our approach lacks in-depth awareness of the intricate operations of closed-source and paid frameworks/tools. For example, the tool is currently unable to properly instruct a new Cobalt Strike user on how to set up Beacons, Proxies and mount lateral traversal attacks and correlate each Beacon's position and importance in the network. The platform's capabilities may be constrained in dealing with the specific functionalities and nuances of these tools, potentially limiting its effectiveness in certain advanced testing scenarios which touch upon red teaming for example, malware writing, bypassing EDR (Endpoint Detection and Response) or advanced Active Directory attack scenarios."}, {"title": "V. CONCLUSION", "content": "Pentest Copilot is set to advance by integrating with diverse knowledge bases for technology-specific expert advice. Future versions will feature fine-tuned models for pentesting and red teaming activities, enhancing capabilities in exploit writing and understanding closed-source industry tools in the pentesting and red-teaming spaces. The LLM Augmented Pentesting approach will involve maintaining a dynamic buffer of identified misconfigurations during targeting, allowing real-time adaptation and pattern recognition.\n\nAdditionally, a red team-specific knowledge base will be developed, covering both non-technical security issues (e.g., phishing) and technology-heavy misconfigurations (e.g., Group Policy Objects misconfigurations in Active Directory). These strategic enhancements aim to make Pentest Copilot an agile, expert, and indispensable tool for ethical hackers and penetration testers."}]}