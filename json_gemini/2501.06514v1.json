{"title": "Neural Codec Source Tracing: Toward Comprehensive Attribution in Open-Set Condition", "authors": ["Yuankun Xie", "Xiaopeng Wang", "Zhiyong Wang", "Ruibo Fu", "Zhengqi Wen", "Songjun Cao", "Long Ma", "Chenxing Li", "Haonnan Cheng", "Long Ye"], "abstract": "Current research in audio deepfake detection is gradually transitioning from binary classification to multi-class tasks, referred as audio deepfake source tracing task. However, existing studies on source tracing consider only closed-set scenarios and have not considered the challenges posed by open-set conditions. In this paper, we define the Neural Codec Source Tracing (NCST) task, which is capable of performing open-set neural codec classification and interpretable ALM detection. Specifically, we constructed the ST-Codecfake dataset for the NCST task, which includes bilingual audio samples generated by 11 state-of-the-art neural codec methods and ALM-based out-of-distribution (OOD) test samples. Furthermore, we establish a comprehensive source tracing benchmark to assess NCST models in open-set conditions. The experimental results reveal that although the NCST models perform well in in-distribution (ID) classification and OOD detection, they lack robustness in classifying unseen real audio. The ST-codecfake dataset\u00b9 and code\u00b2 are available.\nIndex Terms: audio deepfake detection, neural codec, audio deepfake source tracing, out-of-distribution detection", "sections": [{"title": "1. Introduction", "content": "In recent years, there has been rapid advancement in the field of text-to-speech (TTS) and voice conversion (VC), which called deepfake audio. Diverse endeavors and competitions, such as ASVspoof [1, 2] and Audio Deepfake Detection (ADD) challenge [3, 4], have been instituted to promote research aimed at developing deepfake countermeasure (CM) [5, 6, 7].\nWith the advancements in binary classification (real/fake) performance in the field of ADD, research has gradually shifted towards audio deepfake source tracing task [8, 9]. Notably, the well-known source tracing competition, Audio Deepfake Detection Challenge 2023 Track 3 (ADD2023T3), has clearly defined the source tracing task, which involves not only identifying real and in-distribution (ID) fake audio but also considering an out-of-distribution (OOD) novel fake method as OOD. However, the task design and evaluation methods for source tracing models require further optimization.\nFirstly, the ID real category does not consider the unseen real audio. Current evaluations of source tracing model overlook the ability of identifying unseen real audio, which is especially critical when OOD categories are characterized as unseen fake audio. For a well-performing source tracing model, it must exhibit high confidence in correctly classifying unseen real audio as ID real. Secondly, for the fake categoary, current approaches only consider a single novel fake as the OOD fake and lack a discussion on how deepfakes generated by different configurations, sources, and methods impact the performance of source tracing systems. Lastly, regarding the evaluation of source tracing models, the currently used threshold-based metric, F1-score, is advantageous for decision-making. However, the selection of the threshold is overly subjective, which can lead to biased evaluations of OOD methods.\nAs the deepfake audio type, most of the aforementioned ADD studies focus on vocoder-based audio. With the development of large language models, the majority of fake audio on social media today consists of audio language model (ALM)-based deepfake audio, which is typically synthesized using neural codec models as the back-end. As CMs, some researchers have employed neural codec models to reconstruct audio for effectively detecting ALM-based audio [10, 11]. However, these studies have primarily focused on binary classification for the real and codec-based audio and have not yet explored source tracing for codec-based audio.\nSource tracing for neural codecs involves analyzing the homogeneity (real vs. all neural codecs) and heterogeneity (within neural codec classes), as illustrated in Figure 1(a). Using a pre-trained neural codec source tracing (NCST) model, we can distinguish between real audio and different ID codecs, as well as identify OOD novel codecs. This capability can significantly enhance the protection of intellectual property for ID neural codec methods. Furthermore, as shown in Figure 1(b), a pre-trained NCST model can enhance the interpretability of ALM-based audio detection. For example, for recently devel-"}, {"title": "2. Source tracing benchmark", "content": "Previous source tracing models, as shown on the left side of Figure 2, were trained on real audio and seen fake audio for ID classification. During testing, in addition to ID classification, one unseen fake audio was evaluate for OOD detection experiments. To comprehensively evaluate the performance of an NCST model, we first proposed ST-Codecfake dataset and established a new testing protocol for thorough assessment of NCST models.\n2.1. ST-Codecfake dataset\nTo study the NCST task, it is essential to have relevant datasets. Considering the continuous updates in neural codec methods and ALMs, we propose a new dataset called ST-Codecfake. The real domain of ST-Codecfake is derived from subsets of VCTK and ALSHELL, including 10,000 non-overlapping training samples, 1,000 development samples, and 13,228 test samples. Utilizing these as the source domain, we constructed the ST-Codecfake dataset synthesis through 11 neural codec methods. Among these, we designated the commonly used neural codec in ALM as ID, which facilitates subsequent ALM-based audio detection tasks for interpretability. A detailed description of the dataset is provided in Table 1.\n2.2. ID close-set evaluation\nAt this stage, we first train the NCST model on the ST-Codecfake training set (0-6) for 7-class classification. During the testing phase, we evaluate the model on the ST-Codecfake test set (0-6) for ID closed-set classification using the F1-score. Additionally, we conduct three extra experiments to ensure the robustness of the NCST model in ID classification:\nUnseen Real: We test the robustness of the NCST model using unseen real audio. Specifically, we measure performance using the F1-score on the real sets from ASV spoof2019LA (19LA) [17], In the Wild (ITW) [18], and a subset of NCSSD [19]. The 19LA real set contains 7,355 audio samples, sourced from the VCTK dataset, which features relatively clean audio. The ITW dataset includes 19,963 real audio samples from YouTube, representing a complex acoustic environment. For NCSSD, we select 10,000 real audio samples recorded in a studio environment, representing real audio from spoken dialogue scenarios.\nDifferent Configuration: For seen fake audio, different generation configurations\u2014such as sample rate, bitrate, and the number of quantizers\u2014can lead to variations in the reconstructed audio. To investigate whether these variations impact the NCST system, we selected configurations C3-1 to C3-4 and C4-1 to C4-3 from Codecfake dataset [10], which represent the different configurations of ID 3 FunCodec and ID 4 EnCodec.\nDifferent Source: To investigate whether the seen fake"}, {"title": "3. Neural codec source tracing method", "content": "3.1. Baseline model\nThis section introduces the baseline models used for the NCST task, including three representative models in the field of ADD: Mel-LCNN, AASIST, and W2V2-AASIST. We first consider the impact of different features in the NCST scenario. Therefore, we selected handcrafted feature Mel-Spectrogram with LCNN [31] back-end, raw audio waveforms with AASIST [32], and frozen self-supervised features Wav2Vec-XLS-R\u00b3 with AASIST (W2V2-AASIST). Notably, W2V2-AASIST achieved the best performance in the ADD2023 T3 single system in previous work [8].\n3.2. OOD detection method\nWe selected five state-of-the-art OOD methods for performance comparison, including MSP [33], Energy [34], KNN [35], Mahalanobis [36], and NSD [8]. Among these, MSP and Energy are OOD detection methods based on logits scores, while KNN and Mahalanobis are feature distance-based OOD detectors. NSD utilizes both features and logits for OOD decision-making.\n3.3. Implementation detail\nFor the training and testing data, all audio samples were down-sampled to 16,000 Hz and trimmed or padded to a duration of 4 seconds (64,600 samples). Consequently, we obtained Mel-Spectrograms of size (80, 404), raw audio waveforms of 64,600 samples, and W2V2 features of size (1024, 201) as the input features for the three NCST models. All baseline NCST models were configured with a classification head of ID 7 classes and trained using cross-entropy loss. We trained all the models for 50 epochs. The learning rate was initialized at 5-5 and halved every 10 epochs. The model that exhibited the best performance on the development set was selected as the evaluation model."}, {"title": "4. Results and Discussion", "content": "4.1. ID close-set evaluation\nAfter training with ST-codecfake, we conducted evaluations as described in Section 2. First, we evaluated the NCST model on the ST-Codecfake test set (0-6) for ID closed-set classification using the F1-score. The results are shown in Table 3. It can be observed that in the ID classification experiments, all three baseline models achieved promising results, with AASIST and W2V2-AASIST reaching an average F1-score of 99.99%.\nTo further validate the robustness ID performance of the NCST models, we conducted three additional experiments: Unseen Real, Different Configuration, and Different Source exper-"}, {"title": "4.2. OOD open-set evaluation", "content": "At this stage, we conducted OOD open-set evaluation on the W2V2-AASIST NCST model, which had the best performance in the ID evaluation phase. We employed five different OOD methods for this evaluation. The results are shown in Table 5. For OOD method average score (calculated horizontally in Table 5), the logits-based methods, MSP and Energy, outperformed the feature-based and logits-based NSD, which in turn performed better than the feature-based KNN and Mahalanobis methods. Among these, MSP achieved the best overall performance across all conditions, with an AUC of 97.54%, an FPR95 of 9.72%, and an EER of 6.94%. This indicates that logits-based OOD method are sufficient to distinguish between ID and OOD novel codecs. By observing the logits of test samples, we found that the NCST model produces excessively high-confidence logits for ID samples. This results in low-confidence logits for both unseen real and unseen fake samples, making them easily distinguishable. Consequently, this leads to strong OOD performance but a decrease in performance for unseen real data.\nAs the average scores of all OOD methods for each OOD test condition (calculated vertically in Table 5), we found that the AUC for each OOD evaluation condition exceeded 90%. Specifically, the simplest case was a single method considered as the OOD, namely OOD single codec 7 condition, which achieved the best performance with an AUC of 98.82%. Under the different configuration and different source conditions of neural codec 7, the W2V2-AASIST NCST model achieved AUC of 91.83% and 98.99%, respectively. Extending the scenario where a single codec is considered as OOD, we also evaluated NCST model where multiple neural codecs were mixed and treated as OOD. This test produced an AUC of 90.99%, representing the most challenging scenario and suggesting that detecting a diverse mix of novel codecs is more difficult. Additionally, we examined whether the NCST model could identify 19LA fake and ITW fake as OOD, with an average AUC of 95.82%, indicating that these cases are relatively easy to distinguish."}, {"title": "4.3. Explainable ALM detection", "content": "Finally, we conducted experiments to evaluate the NCST model's accuracy in detecting the back-end of ALM. The results, shown in Table 6, demonstrate that the Mel-LCNN, AASIST, and W2V2-AASIST models achieved F1-scores of 91.40%, 77.62%, and 92.36%, respectively. Among these, Mini-omni was the easiest to detect, achieving a perfect F1-score of 100% in 10,000 test samples."}, {"title": "5. Conclusion", "content": "In this paper, we focus on the NCST task and establish a comprehensive benchmark for evaluating source tracing models. To this end, we first constructed the ST-codecfake dataset and used it to train three baseline NCST models, which were then tested in ID close-set and OOD open-set conditions. The experimental results demonstrate that although the NCST models perform well in ID classification and OOD detection, they lack robustness in real classification, often misclassifying unseen real speech. Future work will aim to optimize the models for improved robustness in real category classification."}]}