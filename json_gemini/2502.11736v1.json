{"title": "ReviewEval: An Evaluation Framework for AI-Generated Reviews", "authors": ["Chavvi Kirtani", "Madhav Krishan Garg", "Tejash Prasad", "Tanmay Singhal", "Murari Mandal", "Dhruv Kumar"], "abstract": "The escalating volume of academic research, coupled with a shortage of qualified reviewers, necessitates innovative approaches to peer review. While large language model (LLMs) offer potential for automating this process, their current limitations include superficial critiques, hallucinations, and a lack of actionable insights. This research addresses these challenges by introducing a comprehensive evaluation framework for AI-generated reviews, that measures alignment with human evaluations, verifies factual accuracy, assesses analytical depth, and identifies actionable insights. We also propose a novel alignment mechanism that tailors LLM-generated reviews to the unique evaluation priorities of individual conferences and journals. To enhance the quality of these reviews, we introduce a self-refinement loop that iteratively optimizes the LLM's review prompts. Our framework establishes standardized metrics for evaluating AI-based review systems, thereby bolstering the reliability of AI-generated reviews in academic research.", "sections": [{"title": "Introduction", "content": "The rapid growth of academic research, coupled with a shortage of qualified reviewers, has created an urgent need for scalable and high-quality peer review processes (Petrescu and Krishen, 2022; Schulz et al., 2022; Checco et al., 2021). Traditional peer review methods are under mounting pressure from the exponentially growing number of submissions, particularly in fields like artificial intelligence, machine learning, and computer vision. This has led to a growing interest in leveraging large language models (LLMs) to automate and enhance various aspects of the peer review process (Robertson, 2023; Liu and Shah, 2023).\nLLMs have shown remarkable potential in automating various natural language processing tasks, such as summarization, translation, and question-answering. However, their effectiveness in serving as reliable and consistent paper reviewers remains a significant challenge. The academic community is already experimenting with AI-assisted reviews, as evidenced by reports that 15.8% of reviews for ICLR 2024 were generated with AI assistance (Latona et al., 2024). While this demonstrates the growing adoption of LLMs in peer review, concerns have been raised regarding their impact on the reliability and fairness of the review process. Specifically, papers reviewed by AI have been perceived to gain an unfair advantage, leading to questions about the integrity of such evaluations. Consequently, research into robust automated review generation systems is crucial, necessitating rigorous evaluation of AI generated reviews to address key challenges. (Zhou et al., 2024) provide a comprehensive analysis of the use of commercial models, such as GPT-3.5 and GPT-4 (Achiam et al., 2023), as research paper reviewers. Their findings highlight key limitations, including the potential for mistakes due to either model hallucinations or an incomplete understanding of the material, as well as the inability to provide critical feedback comparable to human reviewers. Based on our preliminary experiments of using GPT-4 to generate reviews for research papers, we identified additional limitations in AI-generated reviews, including a lack of actionable insights and limited analytical depth, often characterized by generic and vague feedback. These shortcomings stem from the inherent tendency of LLMs to generate superficial reviews.\nExisting research on evaluation metrics for AI-generated research paper reviews remains limited. For instance, (D'Arcy et al., 2024) proposed an automated metric to evaluate approximate matches between AI-generated and human-written review comments using GPT-4 (Achiam et al., 2023). Although their method iteratively employs GPT-4 to extract approximate matches and mitigate inconsistencies, its complete reliance on GPT-4 renders the evaluation process a black box, thereby limiting transparency and raising concerns about reliability. Similarly, (Zhou et al., 2024) investigated the aspect coverage and similarity between AI and human reviews through a blend of automatic metrics and manual analysis. Their work leveraged the ASAP dataset (Yuan et al., 2022), which categorizes review sentences into predefined aspects including summary, motivation, originality, soundness, substance, replicability, meaningful comparison, and clarity, to align AI and human reviews. However, beyond this AI-human comparison, their approach overlooks other critical dimensions where AI reviews may underperform, as highlighted in Figure 1.\nBased on our analysis of the limitations in current AI-generated reviews and the gaps in existing evaluation metrics, we propose a comprehensive evaluation framework designed to assess the quality of AI-generated research paper reviews. Our framework targets four key dimensions (see Figure 1):\nComparison with Human Reviews: Evaluates topic coverage and semantic similarity to measure the alignment between AI-generated and human-written feedback. Factual Accuracy: Detects factual errors, including misinterpretations, incorrect claims, and hallucinated information. Analytical Depth: Assesses whether the AI's critique transcends generic commentary to offer in-depth, meaningful engagement with the research. Actionable Insights: Measures the ability of the AI to provide specific, constructive suggestions for improving the paper.\nRecognizing that major conferences and journals have distinct reviewing priorities, a one-size-fits-all approach to AI-driven reviews is insufficient. Recent studies (Bauchner and Rivara, 2024; Biswas, 2024) underscore the growing importance of aligning reviews with conference-specific evaluation criteria-especially as many venues now require adherence to detailed reporting guidelines. To address this, we introduce a conference-specific AI reviewer that dynamically adapts its review strategy to meet the unique criteria of each target venue.\nFurthermore, inspired by the self-refinement approach of (Madaan et al., 2023), our system incorporates a supervisor model that iteratively critiques and refines the instructional prompts guiding the review process. This self-refinement loop promotes deeper analytical assessments and mitigates the tendency of AI-generated reviews to offer only superficial feedback. In summary, the paper makes the following contributions:\n1. Develop a comprehensive evaluation framework for LLM-based reviewing systems across five dimensions: (i) alignment with human reviews, (ii) factual accuracy, (iii) analytical depth, (iv) actionable insights, and (v) adherence to Reviewer guidelines.\n2. Create an LLM-based reviewer that dynamically aligns with the evaluation criteria of specific conferences/journals and continuously improves its reviewing strategy through an iterative refinement loop."}, {"title": "Related Work", "content": "AI-based scientific discovery. Early efforts on automating scientific discovery include expert systems from 1970s such as DENDRAL (Buchanan and Feigenbaum, 1981) and the automated mathematician (Lenat, 1977), which focused on constrained problem spaces like organic chemistry and theorem proving. More recently, LLMs and machine learning techniques have been used to extend automated research beyond structured domains. Notable contributions include AutoML that optimize hyperparameters and architectures (Hutter et al., 2019; He et al., 2021) and AI-driven discovery in materials science and synthetic biology (Merchant et al., 2023; Hayes et al., 2024). However, these methods remain largely dependent on human-defined search spaces and predefined evaluation metrics, limiting their potential for open-ended discovery. Recent works (Lu et al., 2024) aim to automate the entire research cycle, encompassing ideation, experimentation, manuscript generation, and peer review, thus pushing the boundaries of AI-driven scientific discovery.\nAI-based peer-review. Existing work has looked at scoring and improving research papers in a variety of ways such as statistical reporting inconsistencies (Nuijten and Polanin), recommending citations (Ali et al., 2020) and predicting review scores (Basuki and Tsuchiya, 2022; Bharti et al.). More recently, LLM-based approaches have been used to generate peer reviews (Robertson, 2023; Liu and Shah, 2023; D'Arcy et al., 2024; Lu et al., 2024; Liang et al.).\n(Lu et al., 2024) employ LLMs to autonomously conduct the research pipeline, including peer review. It follows a structured three-stage review process: paper understanding, criterion-based evaluation (aligned with NeurIPS and ICLR guidelines), and final synthesis: assigning scores to key aspects like novelty, clarity, and significance. Evaluation shows its reviews closely match human meta-reviews. MARG (D'Arcy et al., 2024) introduces a multi-agent framework where worker agents review sections, expert agents assess specific aspects, and a leader agent synthesizes feedback. Using BERTScore (Zhang* et al., 2020) and GPT-4-based evaluation, MARG-S improves feedback quality, reducing generic comments and increasing helpful feedback per paper. These studies highlight the Al's potential to enhance peer review through structured automation and multi-agent collaboration.\nEvaluation framework for AI-based peer-review. There has been limited research on developing evaluation frameworks for evaluating the quality of LLM generated paper reviews. (Zhou et al., 2024) evaluated GPT models for research paper reviewing across 3 tasks: aspect score prediction, review generation, and review-revision MCQ answering. The study developed evaluation framework comprising of aspect coverage (originality, soundness, substance, replicability, etc.), ROUGE (lexical overlap), BERTScore (semantic similarity), and BLANC (informativeness), alongside manual analysis. Results showed LLMs overemphasized positive feedback, lacked critical depth, and neglected substance and clarity, despite high lexical similarity to human reviews. (D'Arcy et al., 2024) introduced an automated evaluation framework for AI-generated reviews, quantifying similarity to human reviews via recall, precision, and Jaccard index. Recall measures the fraction of real-reviewer comments with at least one AI match, precision quantifies AI comments aligned with human reviews, and Jaccard index evaluates the intersection-over-union of aligned comments.\nExisting evaluation metrics predominantly emphasize the similarity between AI-generated and human reviews, overlooking other crucial parameters. Moreover, their heavy reliance on LLMs for end-to-end evaluation results in a black-box system with limited transparency. In contrast, our framework introduces more interpretable evaluation metrics for AI-generated reviews (see Figure 1 and Table 1), effectively addressing these shortcomings."}, {"title": "ReviewEval", "content": "We describe the development of our evaluation framework and the LLM-based research paper reviewer. Each review is evaluated on several key parameters to assess the overall quality of the generated feedback. To ensure consistency and reliability, all evaluations for a given metric were performed by LLMs of the same specification and version, thereby maintaining inter-rater reliability and ensuring robust, unbiased comparisons. Our experiments utilized 16 papers and their corresponding expert reviews (scraped from OpenReview.net), alongside AI-generated reviews. The overview of the proposed framework is presented in Figure 2."}, {"title": "Comparison with Expert Reviews", "content": "We compare the reviews generated by the LLM based reviewer with expert reviews from OpenReview.net. Our primary goal is to gauge how well the AI system replicate or complement expert-level critique. The evaluation is conducted along the following dimensions:\nSemantic similarity. To assess the alignment between AI-generated and expert reviews, we embed each review R into a vector space using the OpenAI embedding model. The semantic similarity between an AI-generated review RAI and an expert review Rexpt is measured using cosine similarity:\n$S_{sem}(R_{AI}, R_{Expt}) = \\frac{e(R_{AI}) \\cdot e(R_{Expt})}{||e(R_{AI})|| \\cdot ||e(R_{Expt})||}$ (1)\nwhere e(R) denotes the embedding of review R. A higher cosine similarity indicates a stronger alignment between the AI-generated and expert reviews.\nTopic modeling. We evaluate topic coverage to determine how comprehensively AI-generated reviews address the breadth of topics present in expert reviews. Our approach comprises three steps:\n\u2022 Topic extraction: Each review R (either AI-generated or expert) is decomposed into a set of topics: TR = {t1, t2, ..., tn}, where each topic ti is represented by a sentence that captures its core content and context. \u2022 Topic similarity: Let TAI = {t1, t2,...,tm} and Texpt = {t1, t2,...,t'n} denote the topics extracted from the AI and expert reviews, respectively. We define a topic similarity function TS(ti, t'j) that an LLM assigns on a discrete scale:\nTS(ti, t'j) = 3 \u00b7 I{ti \u223cstrong t'j}\n+2 \u00b7 I{ti \u223cmoderate t'j}\n+1 \u00b7 I{ti \u223cweak t'j},\n(2)\nwhere I is the indicator function, $t_i \\thicksim strong t'_j$, $t_i \\thicksim moderate t'_j$, $t_i \\thicksim weak t'_j$ denote substantial, moderate, and minimal overlap in concepts, respectively. All the conditions are mutually exclusive. We set a similarity threshold \u03c4 = 2 so that topics with TS(ti, t';) \u2265 \u03c4 are considered aligned.\nCoverage ratio: For each AI-generated review, we construct a topic similarity matrix S where each element S[i, j] = TS(ti, t';) represents the similarity between topic ti from TAI and topic t'; from TExpt. The topic coverage ratio is defined as:\n$S_{coverage} = \\frac{1}{n} \\sum_{j=1}^{n} I(max_{i=1,...,m} S(t_i, t'_j) > \\tau) ,$ (3)\nwhere I(\u00b7) is the indicator function, and n = |TExpt| is the total number of topics extracted from the expert review."}, {"title": "Evaluating Factual Correctness of Reviews", "content": "To address the hallucinations and factual inaccuracies of LLM generated reviews, we propose an automated pipeline that validates the factual correctness of LLM-generated reviews by simulating the conference rebuttal process. Our pipeline emulates the traditional rebuttal workflow, where authors clarify or counter reviewer claims using evidence from their work. By automating both the question generation and rebuttal phases, our system produces a robust factual correctness evaluation. The pipeline consists of the following steps:\nStep 1: Transforming reviews into structured questions. Each LLM-generated review R is transformed into a structured question Q that encapsulates the central claim or critique. For example, consider the following review from the PeerRead dataset (Kang et al., 2018) for the paper \"Augmenting Negative Representations for Continual Self-Supervised Learning\" (Cha et al.):\nThis review is converted into the corresponding question:\nStep 2: Decomposing questions into sub-questions. The question Q is then decomposed into a set of sub-questions {q1, q2, ..., qn} using a dedicated query decomposition engine. This decomposition enables a fine-grained analysis by isolating distinct components of the original question.\nStep 3: Retrieval-augmented generation (RAG) for evidence synthesis. For each sub-question qi, we employ a Retrieval-Augmented Generation (RAG) framework to gather and synthesize relevant evidence: (a) Section retrieval: For each paper P, we retrieve pertinent text segments S (approximately 400 tokens) via semantic search. (b) Parent section extraction: Using LangChain's Parent Document Retriever, we extract the parent sections Sp (approximately 4000 tokens) corresponding to each S. Documents are pre-chunked hierarchically, ensuring that each S is mapped to its contextually relevant Sp. C Answer generation: With the context provided by Sp, the LLM generates an answer A\u00bf for each sub-question qi. These individual answers are then aggregated into a unified, structured response AQ addressing the original question Q.\nStep 4: Automated rebuttal generation. The comprehensive answer A\u0119 is used to generate an automated rebuttal R\u2081 for the original review R. This rebuttal is designed to provide evidence-based clarification or counterarguments to the claims in R.\nStep 5: Factual correctness evaluation. An evaluation agent then assesses the factual correctness of R by comparing it against the generated rebuttal Rb. The review is deemed: (a) Valid (V = True) if Ro substantiates the claims made in R. (b Invalid (V = False) if Rs reveals factual discrepancies or unsupported claims in R."}, {"title": "Constructiveness", "content": "We assess review constructiveness by quantifying the presence and quality of actionable insights in AI-generated reviews relative to expert feedback. Our framework begins by extracting key actionable components from each review using an LLM with few-shot examples. Specifically, we identify the following actionable insights: (i) criticism points (C), which capture highlighted flaws or shortcomings in the paper's content, clarity, novelty, and execution; (ii) methodological feedback (M), which encompasses detailed analysis of experimental design, techniques, and suggestions for methodological improvements; and (iii) suggestions for improvement (I), which consist of broader recommendations for enhancement such as additional experiments, alternative methodologies, or improved clarity.\nOnce these components are extracted, each insight is evaluated along three dimensions: specificity, feasibility, and implementation details. The specificity score \u03c3 is defined as 1 if the insight is specific and includes explicit examples, and 0 otherwise; the feasibility score \u03d5 is set to 1 when the recommendation is practical within the paper's context, and 0 otherwise; and the implementation details score \u03b6 is 1 if actionable steps or detailed methodologies are provided, and 0 otherwise. The overall actionability score for an individual insight is then computed as Sact,i = \u03c3i + \u03d5i + \u03b6i, with an insight considered actionable if Sact,i > 0. Finally, we quantify the overall constructiveness of a review by calculating the percentage of actionable insights:\n$Sact = \\frac{1}{N} \\sum_{i=1}^{N} I(S_{act,i} > 0) \\times 100,$ (4)\nwhere N is the total number of extracted insights and I() denotes the indicator function. This metric provides a quantitative measure of how effectively a review offers concrete guidance for improving the work."}, {"title": "Depth of Analysis", "content": "To assess whether a review provides a comprehensive, critical evaluation rather than a superficial commentary, we measure the depth of analysis in AI-generated reviews. This metric captures how thoroughly a review engages with key aspects of a paper, including comparisons with existing literature, identification of logical gaps, methodological scrutiny, interpretation of results, and evaluation of theoretical contributions.\nEach review is evaluated by multiple LLMs, which assign scores for each of the five dimensions, mi (i \u2208 {1,2,3,4,5}), with scores Si \u2208 [0,1]. Scores in the continuous range allow us to capture nuances in performance. We define the metrics as follows:\nComparison with existing literature (m1): Assesses whether the review critically examines the paper's alignment with prior work, acknowledging relevant studies and identifying omissions. The scoring rubric is:\n$S_1 =\\{\\3, if the review provides a thorough, critical comparison,\\2, if the comparison is meaningful yet shallow,\\1, if the comparison is vague or lacks specific references,\\0, if no comparison is provided.$\nLogical gaps identified (m2): Evaluates the review's ability to detect unsupported claims, reasoning flaws, and to offer constructive suggestions:\n$S_2 =\\{\\3, if the review identifies comprehensive gaps and provides suggestions,\\2, if it notes some gaps with unclear recommendations,\\1, if the gaps are vaguely mentioned without solutions,\\0, if no gaps are identified.$\nMethodological scrutiny (m3): Measures the depth of critique regarding the paper's methods, including evaluation of strengths, limitations, and improvement suggestions:\n$S_3 =\\{\\3, if the review delivers a thorough critique with actionable suggestions,\\2, if the critique is meaningful but lacks depth,\\1, if the critique is vague and offers little insight,\\0, if no methodological critique is provided.$\nResults interpretation (m4): Assesses how well the review interprets the results, addressing biases, alternative explanations, and broader implications:\n$S_4 =\\{\\3, if the interpretation is detailed and insightful,\\2, if it is meaningful yet shallow,\\1, if the discussion is generic or vague,\\0, if no interpretation is offered.$\nTheoretical contribution (m5): Evaluates the assessment of the paper's theoretical contributions, including its novelty and connections to broader frameworks:\n$S_5 =\\{\\3, if the evaluation is comprehensive and insightful,\\2, if the evaluation is meaningful but lacks depth,\\1, if the critique is vague,\\0, if no theoretical assessment is provided.$\nThe overall depth of analysis score for a review is calculated as the average normalized score across all dimensions:\n$S_{depth} = \\frac{\\sum_{i=1}^{5} S_i}{15}$ (5)\nA higher Sdepth indicates a more comprehensive and critical engagement with the manuscript."}, {"title": "Adherence to Reviewer Guidelines", "content": "To assess whether a review complies with established criteria, we evaluate its adherence to guidelines set by the venue. This metric measures how well the reviewer applies key aspects such as originality, methodology, results, clarity, and ethical considerations, thereby ensuring an objective and structured evaluation process.\nOur approach begins by extracting the criteria C from the guidelines G. These criteria fall into two broad categories: subjective criteria, which involve qualitative judgments (e.g., clarity, constructive feedback), and objective criteria, which are quantifiable (e.g., following a prescribed rating scale). For each review R, every extracted criterion Ci is scored on a 0-3 scale using a dedicated LLM with dynamically generated prompts that include few-shot examples for contextual calibration. For subjective criteria, the score is defined as:\n$S_i =\\{\\3, if there is strong adherence with detailed, accurate feedback;\\2, if the review shows reasonable alignment with minor deviations;\\1, if the feedback is incomplete or inaccurate;\\0, if there is no alignment.$\nFor objective criteria, the scoring is binary:\n$S_i =\\{\\3, if the review adheres to the required scale and structure;\\0, otherwise.$\nThe overall adherence score is then computed as:\n$S_{adherence} = \\frac{\\sum_{i=1}^{2} S_i}{6}$ (6)\nThis normalized score provides a quantitative measure of how well the review conforms to the prescribed guidelines."}, {"title": "Conference-Specific Review Alignment", "content": "To tailor the review process to conference-specific guidelines, we first retrieve the relevant textual content from the target conference's official reviewing website using the Extractor API. The extracted text is then processed using GPT (Achiam et al., 2023) to filter out extraneous details while preserving essential reviewing instructions. Each guideline gi is converted into a step-by-step instructional prompt via GPT:\nPi = GeneratePrompt(gi),\nwhere Pi denotes the prompt corresponding to guideline gi. Since some criteria apply to multiple sections of a research paper, the prompts are dynamically mapped to the relevant sections. This is achieved using a mapping function:\nSj = M(Pi),\nwhere Sj is the set of paper sections associated with prompt Pi. Notably, M is a one-to-many mapping, i.e.,\nM : P \u2192 P(S),\nwith P(S) denoting the power set of all sections. By conducting reviews on a section-wise basis, our framework enhances processing efficiency and allows for independent evaluation of each section prior to aggregating the final review. Detailed descriptions of all prompts are provided in the Appendix."}, {"title": "Review Prompt Iterative Refinement", "content": "To enhance the quality and completeness of our review prompts, we employ an iterative refinement process using a Supervisor LLM. Starting with an initial set of prompts generated from the reviewing guidelines, the Supervisor LLM evaluates each prompt in conjunction with its corresponding guideline, serving as the problem statement, and provides targeted feedback. This feedback addresses key aspects such as clarity (ensuring precise and unambiguous instructions), logical consistency (ensuring coherent reasoning), alignment with the guideline, and comprehensiveness (ensuring all relevant aspects are covered). The feedback is then used to revise the prompt, and this iterative loop is repeated for a fixed number of iterations (three in our implementation). The result is a final set of structured, high-quality review prompts that are well-aligned with the reviewing guidelines and optimized for the evaluation process."}, {"title": "Experiments & Results", "content": "Dataset. We curated a dataset of 16 NeurIPS 2024 submissions from OpenReview.net. The sample comprises a balanced mix: 4 accepted papers each from the oral, poster, and spotlight categories, and 4 rejected papers. To ensure topical diversity and mitigate potential biases, we selected papers containing unique keywords within each acceptance category, thereby representing a broad range of research areas without over-representation of any single theme. Notably, the recent nature of these papers creates a challenging test set, as many state-of-the-art LLMs are unlikely to have been exposed to them given their training data cutoffs. This design allows for a robust assessment of each model's ability to interpret and analyze previously unseen data.\nBaselines and models. We compare our AI-generated review approach with two established methods: Sakana AI Scientist (Lu et al., 2024) and MARG (D'Arcy et al., 2024). To demonstrate our method's effectiveness, we report results using models from OpenAI and Anthropic's Claude, specifically: GPT-4o, GPT-4o-mini, 3.5 Sonnet, and 3.5 Haiku.\nResults. Table 2 summarizes the performance of reviews generated by different frameworks, including a variety of foundation models, in terms of the presence of actionable insights. We compare our approach with the methods proposed by MARG (D'Arcy et al., 2024) and Sakana AI Scientist (Lu et al., 2024). The AI-generated reviews are evaluated across six metrics: actionable insights, adherence to review guidelines, coverage of topics, semantic similarity, depth of analysis, and factual correctness. We discuss the results below.\nActionable insights. Expert human reviewers give a mean score of 0.7522, establishing the benchmark. Our Sonnet-3.5-based model achieves results that closely align with expert-level feedback, with a minimal difference of 0.0009. Notably, Sakana-4o-mini (0.7916) and Sakana-4o (0.7909) produced feedback that is more favorable than the human reviewer.\nAdherence to the review guidelines. Expert human evaluations yield a mean score of 0.5708. Our GPT-4o-mini-based model (Ours-GPT-4o-mini) closely matches this score with a mean of 0.5785. In contrast, models such as Ours-3.5-Sonnet (0.6658) and Sakana-3.5-Sonnet (0.6400) exhibit higher compliance with the guidelines. This is expected since Sakana AI Scientist is explicitly programmed to follow NeurIPS guidelines, whereas our reviewer is designed to adapt dynamically to the given guidelines while addressing the core issues of the paper. Therefore, our methods yield variable scores across different models, whereas Sakana AI consistently produces higher scores.\nTopic coverage and semantic similarity. Ours-Haiku-3.5 achieves highest score of 0.8013 for coverage of topics outperforming both the MARG and Sakana AI. In terms of semantic similarity, Sakana AI leads the evaluation with a score of 0.8440, with our methods following closely behind with scores ~0.8200.\nDepth of analysis. Expert reviews yield a depth analysis score of 0.6264. Among AI-generated reviews, MARG achieves a score of 0.68, closely matching the expert benchmark. This indicates that MARG outperforms both Sakana AI Scientist and our method, highlighting the effectiveness of its multi-agent framework in generating deeper review insights.\nFactual correctness. Ours-Haiku-3.5 achieves a score of 0.78, which is the closest among AI-generated reviews to the human review score of 0.90. However, all AI-generated reviews trail behind human reviews in terms of factual correctness. This is expected, as AI-generated reviews may not always accurately assess whether a paper's content is factually correct or aligns with established findings from past literature.\nRemarks. Our experiments show that AI-generated reviews can approximate human performance in areas like actionable insights and guideline adherence, with our Sonnet-3.5 and GPT-4o-mini models closely matching expert scores. However, performance varies across models-Sakana AI excels in adherence and semantic similarity, and MARG delivers deeper analysis, while all AI methods still fall short in factual correctness compared to human reviews. These findings underscore the potential of AI in review generation, while also highlighting areas for improvement with better evaluation metrics like ReviewEval."}, {"title": "Conclusion", "content": "We introduced ReviewEval, a comprehensive framework for evaluating AI-generated research paper reviews. Our system assesses reviews along multiple dimensions\u2014alignment with human evaluations, factual accuracy, analytical depth, actionable insights, and adherence to reviewer guidelines-while incorporating conference-specific alignment and an iterative prompt refinement loop. Based on the proposed metrics, we also introduce an LLM based AI reviewer. Experiments on NeurIPS 2024 papers show that our framework generates reviews that closely match expert feedback and address common limitations such as superficial critique and factual inaccuracies. Future work will extend our approach to incorporate multi-modal inputs and additional domain-specific criteria, further advancing the efficiency and fairness of automated peer review."}, {"title": "Acknowledgement", "content": "The authors wish to acknowledge the use of Chat-GPT/Claude in the writing of this paper. This tool was used to improve the structure, presentation and grammar of the paper. The paper remains an accurate representation of the authors' underlying work and novel intellectual contributions."}, {"title": "Limitations", "content": "While ReviewEval shows promise in evaluating AI-generated research paper reviews, several limitations should be acknowledged: our experiments on 16 NeurIPS 2024 papers may not generalize to other fields or larger datasets, limiting the applicability of our findings; the framework's reliance on LLMs risks propagating biases, hallucinations, and black-box decision-making; the interdependencies among our metrics (semantic similarity, factual accuracy, and depth of analysis) introduce subjectivity and may limit model flexibility; prompt sensitivity means small changes in prompt design can lead to inconsistent outputs; temporal stability remains an issue as LLM behavior evolves over time; iterative refinement and retrieval-augmented generation add computational overhead, hindering scalability; and automated metrics may fail to capture qualitative nuances like tone, subtle critique, and readability. Despite these limitations, ReviewEval lays the groundwork for improving AI-assisted peer review, and future work will focus on expanding datasets, refining metrics, and enhancing interpretability."}, {"title": "Appendix", "content": "Below are the system instructions and prompts employed in our review generation pipeline. These prompts guide each stage of the process from extracting reviewer guidelines from HTML content, to generating detailed review prompts for specific paper sections, and finally formatting the reviews in strict adherence to conference guidelines."}, {"title": "Prompts Used", "content": "Below are the system instructions and prompts employed in our review generation pipeline. These prompts guide each stage of the process from extracting reviewer guidelines from HTML content, to generating detailed review prompts for specific paper sections, and finally formatting the reviews in strict adherence to conference guidelines.\nGuidelines Parsing Prompt. You are a smart AI designed to extract reviewer guidelines from HTML content, regardless of its structure or format. You will be provided with the raw HTML of a webpage that contains the guidelines. Your task is to intelligently parse and extract the most relevant content based on the following high-level objectives:\n1. Understand the Context: The HTML file may contain multiple sections of a webpage, including irrelevant information like headers, footers, navigation bars, or ads. Your goal is to focus solely on extracting meaningful content that pertains to reviewer guidelines. Look for terms such as 'reviewer', 'guidelines', 'evaluation', 'criteria', 'instructions', or 'review process' that may indicate sections of interest.\n2. Text Structure: Look for relevant sections by identifying common phrases or paragraphs that may contain instructions or rules for reviewers. This includes but is not limited to guidelines on evaluation, reviewing criteria. Focus on only the main content that provides the guidelines for how to review the papers content and not the conference details.\n3. Avoid Noise: Ignore or discard text that is likely irrelevant, such as menus, links to other pages, copyright information, or promotional content. You are interested only in extracting text that provides guidance to reviewers for evaluating papers.\n4. Identify Sections Based on Common Words: You can identify the main sections of interest by finding phrases like: \"Reviewer Guidelines\", \"Review Criteria\", \"Evaluation Process\", \"Instructions for Reviewers\", \"Review Process Overview\", When you find such phrases, capture the paragraph or section following the phrase, as this is likely to contain the reviewer guidelines.\n5. Extract Text Around These Keywords: When you identify these keywords, extract approximately 3-4 paragraphs surrounding these keywords to capture the guidelines. This includes headings or bullet points that may be present.\n6. Return Results as String: Once you have completed parsing the HTML content and extracted relevant guidelines, return the guidelines as a single continuous string. Ensure the text is well-formatted and readable, without HTML tags or irrelevant information like advertisements or links.\n7. Avoid capturing details of the conference or event itself, such as times, dates, locations, or registration information. Your task is to focus solely on the reviewer guidelines and evaluation criteria.\n8. Avoid capturing details of what software or tools to use for the review process. Focus on the guidelines for evaluating the content of the papers.\n9. If there is a table of any sort in the reviewer guidelines, extract the text content of the table and present it in a readable format, as a paragraph or list of items. Do not include the table structure in the extracted text.\n10. if there are guidelines for multiple types of papers like ones in CER OR PCI OR NLP, extract the information of the first type of paper only. Do not give any recitations of any sort as that is blocked by google because of copyright issues. Note that you MUST also check the format which is required by the conference guidelines for a review and the output should be given in that format in the end\nPrompt for instruction generation for review of a section. You are a generative language model (LLM X) creating a prompt for another research paper reviewer LLM (LLM Y), generate a detailed prompt instructing LLM Y on how to review the section section of a research paper. Consider the following criteria:\n1. The clarity and completeness of the section.\n2. The relevance and alignment of the section with the main themes and objectives of the paper.\n3. The logical consistency and evidence support in the section.\n4. The originality and contribution of the section to the field.\n5. Any specific elements highlighted in the conference guidelines that should be focused on in the section.\nProvide structured and clear instructions in the form of a plan with steps that will enable LLM Y to conduct a thorough and critical review of the research paper's section. Use the given conference guidelines. Do not give any recitations of any sort as that is blocked by google because of copyright issues.\nPrompt for finally formatting review as per conference guidelines. You are an expert in writing reviews for various research paper conferences. You will be given reviews for various sections of a research paper, and the research paper itself and you are supposed to write the review in the format that is expected for submission to the specified conference. You're given the contents of the reviewer guidelines for the conference and you are supposed to adhere to it strictly. You are also not supposed to change the content of the review provided to you AT ALL. You are just a formatter and are supposed to just rewrite the given review into the given format while making the necessary changes. You are to give the complete review of the paper in the format of the conference (the entire paper, not some part of it). Remember that you have an outut token limit of 8192 tokens and your entire review is supposed to fit within that limit, so be careful. This is the conference guidelines for the conference : str(guidelines)"}]}