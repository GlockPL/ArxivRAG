{"title": "LLM Reasoner and Automated Planner:\nA new NPC approach", "authors": ["Israel Puerta-Merino", "Jordi Sabater-Mir"], "abstract": "In domains requiring intelligent agents to emulate\nplausible human-like behaviour, such as formative simulations,\ntraditional techniques like behaviour trees encounter significant\nchallenges. Large Language Models (LLMs), despite not always\nyielding optimal solutions, usually offer plausible and human-\nlike responses to a given problem. In this paper, we exploit this\ncapability and propose a novel architecture that integrates an\nLLM for decision-making with a classical automated planner that\ncan generate sound plans for that decision. The combination aims\nto equip an agent with the ability to make decisions in various\nsituations, even if they were not anticipated during the design\nphase.", "sections": [{"title": "I. INTRODUCTION", "content": "Intelligent agents are required to emulate plausible human-\nlike behaviour in multiple domains, such as serious games\nor formative simulations. Within this context, Non-Playable\nCharacters (NPCs) usually face the challenge of making\ncontext-based decisions that must appear plausible and co-\nherent for the human observer. Classical approaches to this\nproblem, such as behaviour trees, encounter limitations. These\nmethods require developers to anticipate and manually specify\nthe actions and decision-making flow for every potential sce-\nnario, a task that is tedious, prone to errors and often hindered\nby the complexity of the simulated world. In response to\nthese limitations, it would be desirable to have an architecture\ncapable of making contextually appropriate decisions without\nexhaustive scenario specification.\nOn the other side, Large Language Models (LLMs) with\nrecent papers such as Generative Agents [1], are recently\ndemonstrating an interesting potential in decision-making\nproblems resolution, being capable to emulate a plausible\nreasoning process. Despite this models are not always yielding\nreliable optimal solutions, they usually provide plausible and\nhuman-like responses to given problems.\nIn certain contexts, such as the one presented in the next\nsection (I-A. Use Case), optimal behavior might not be nec-\nessary. In these domains, where we want NPCs to make\ndecisions that are not necessarily optimal but coherent, it is\nworthwhile to explore the potential of an LLM as a decision-\nmaking reasoner, emulating plausible human-like behavior.\nThis approach could lead to the development of a powerful"}, {"title": "A. Use Case", "content": "This work is conducted under the RHYMAS: Real-time\nHybrid Multiscale Agent-based Simulations for Emergency"}, {"title": "B. System Description", "content": "The intelligent system developed employs an LLM to make\nthe environment-based decision of which goal to follow. Sub-\nsequently, it utilizes the AP algorithm to devise an executable\nplan, a sequential list of specific actions to achieve the selected\ngoal. As depicted in Figure 3, the general architecture of this\nIntelligent Agent is based on three main modules: the Reasoner\nmodule, which utilizes the LLM to generate the goal; the\nPlanner module, which employs AP techniques to generate the\nplan; and the Interface with the environment. Each module is\nbriefly explained below:\n\u2022 Reasoner. This module features a main data structure,\ncalled memories, representing the environmental context.\nThese memories comprise the list of the pertinent per-\nceptions the agent has encountered, presented in natural\nlanguage format for the LLM to comprehend.\nThe module receives a list of potential goals to pursue\nand, leveraging the environment context provided by the\nmemories, determines which one to pursue.\nTo enhance the model's capacity to address the reasoning\nprocess, we introduce an additional field in the Reasoner's\nmemories, called the personality traits field. This field\nserves as an initial phrase, also expressed in natural\nlanguage, where aspects such as the agent's role in the\nenvironment or its priorities are specified.\n\u2022 Planner. This module features a world-state representa-\ntion called AP Problem, which employs a data structure\ncompatible with the AP algorithm.\nIt receives the selected goal and generates a plan, a\nsequence of actions to achieve it in the current context\n(defined on the AP Problem).\n\u2022 Interface. This module receives a new instance of the\nenvironment state and utilizes it to generate all the infor-\nmation required by the other modules. It is responsible\nfor processing all the world-state information to generate\n(1) the possible goals that the agent could achieve in\nthat specific situation,\u00b9 (2) the new perceptions to add\nto the Reasoner's memories, and (3) the AP Problem\nrepresentation of the received world state. Once the plan\nis generated, the interface sequentially gives the actions\nto the environment to be executed.\nThe execution of the system consist on an iterative and\nreactive process where, on each iteration, the system exhibits\nthe following behaviour:\n1) The iteration commences when the Interface module\nreceives a new instance of the world-state. It utilizes the\nworld-state information to generate all possible goals.\n2) The Interface module generates natural language per-\nceptions of the world-state and compares them with the\nReasoner's memories, adding all the new ones. Subse-\nquently, it constructs the AP Problem representation of\nthe new world-state.\n3) If any new perception has been added to the memories,\nthe Reasoner module re-evaluates the goal to achieve\nusing the updated memories list and the goals generated\nby the interface.\n4) If the generated AP Problem differs from the previous\none or the selected goal has changed, the Planner module\ngenerates a new plan.\n5) If the plan has changed or the agent has completed the\npreviously selected action, the interface takes the action\nat the top of the plan, removes it from the list and sends\nit to the environment for execution."}, {"title": "C. Document Structure", "content": "Finally, here is a brief summary of what can be found in\nthe following pages:\n\u2022 Chapter II. State Of The Art. We present a review of the\nfields of knowledge related to this work: NPC decision-\nmaking strategies, LLM research and AP resources.\n\u2022 Chapter III. Agent Architecture. We detail the design\ndecisions made regarding the developed architecture and\nexplain how each module works."}, {"title": "II. STATE OF THE ART", "content": "The modeling of NPCs' behavior is a widely explored\nfield with several contemporary approaches. In this article,\nwe present a classification based on the one made by Andrey\nSimonov [3].\nThe most primitive approach involves implementing prede-\nfined, usually cyclical behaviors. The result is a non-flexible\nand preplanned (i.e., nonexistent) decision-making process.\nThis behavior might be convenient in some scenarios but is\nnot useful for implementing autonomous or reactive agents.\nThis approach was soon improved with finite-state ma-\nchines, which offer a series of predefined behaviors (states) but\nallow the agent's internal state (and, therefore, its behavior)\nto change depending on the environment context or trigger\nevents. This provides more variable behavior and reactive\ncapacity. A great example of this is an NPC that patrols an area\nbut follows the player when discovered. However, while this\nworks well for simple behaviors, the more states and triggers\ninvolved, the more implementation time is required. Moreover,\ntheir implementation is very closely tied to the environment's\ncharacteristics, making it a technique with low extensibility to\ndifferent contexts. [4]\nBehavior trees are hierarchical structures where each node\nrepresents a simple decision. When a node is activated, it\nmakes its corresponding decision based on the environment\ncontext or internal states and activates the lower nodes related\nto the decision taken. Each node can execute a specific\naction when activated, but only the leaf nodes have behavior\nassociated with them. This architecture represents a signif-\nicant improvement over finite-state machines, allowing the\nimplementation of the same behaviors in a simpler way. Addi-\ntionally, it facilitates the creation of more complex behaviors\ndue to the ease of constructing large tree structures. These"}, {"title": "B. Large Language Models", "content": "Language Modeling is the branch of AI that aims to model\nthe human languages. However, since formalizing an entire\nlanguage seems impossible, this field primarily focuses on pre-\ndicting (or obtaining a probabilistic set of) words, characters,\nor groups of words that fit a certain context.\nHistorically, language modeling has been conducted using\nN-grams, techniques based on statistical distributions that\ngenerate the probability of a certain word occurring next to\nthe immediately preceding N words. For instance, a bi-gram\nprovides the probability of a word following its immediately\npreceding word, and a tri-gram offers the probability of it\nfollowing the previous two words. However, in the last decade,\nthe field has witnessed a paradigm shift with the emergence\nof deep learning models that achieve remarkable results in\nlanguage modeling.\nA large language model typically refers to a language model\nthat internally uses deep learning techniques with enormous\narchitectures (in the order of millions of parameters) trained\nfor general purposes. These models aim to generate the next\nmost suitable token (commonly, a character) given a text\nwindow. Their performance and flexibility have rendered them\ninvaluable for numerous natural language processing tasks.\nFurthermore, by concatenating the generated token to the\nprevious text and re-passing it to the model, we can generate\nwords, sentences, and even entire texts.\nThe key to the recent acclaim of these models lies in their\ngeneral-purpose nature, as a single model can generate consis-\ntent text across a wide range of tasks and subjects. The release\nof OpenAI GPT-3 [8] surprised with its overall performance,\nbecoming a media phenomenon. Since then, a succession of\nmodels has continually emerged and improved performance.\nSome noteworthy examples include Gemini [9], developed by\nGoogle;2 Llama 2 [10] and Llama 3 [11], developed by Meta\nand whose code was made available for non-commercial use;\nVicuna [12], an open-source model derived from Llama 2; and\nMistral [13], an independent open-source.\nTo enhance the accessibility of LLMs, several tools have\nemerged, simplifying their setup and execution. One widely\nused tool is the Llama.cpp library [14]. Furthermore, full-\nfeatured software applications for executing LLMs are emerg-\ning. An example of this is the desktop application of LM\nStudio [15], which is still in the beta phase but offers many\nconveniences for downloading, setting up, and running LLMs.\nIt is also important to mention Hugging Face [16], the\nleading machine learning website, which serves as both a\nsocial media platform and a repository for machine learning\nresources. It offers a wide variety of models, datasets, docu-\nmentation, and tools for utilizing machine learning technolo-\ngies. Currently, it serves as the primary source of LLMs and\nrelated software, housing most existing models, available in\nvarious sizes or formats for execution.\nIn this context, thanks to the considerable capacity of LLMs\nand the tools developed for their utilization, the use of LLMs\nin various areas is being explored. Notably, their integration\nin intelligent agents or NPCs has resulted in interesting use\ncases. They are being employed for dialogue with users (i.e.\nthe Inworld Origins [17], The Matrix Awakens [18] and Nvidia\nAce [19] technical demos); decision-making based on trust\n(Suck Up! [20], a video game where the player has to persuade\npeople to enter their home); enhancing the performance of\nlearning agents (Voyager [21], an intelligent agent playing\nMinecraft), and even creating complex multi-agent environ-\nments with realistic emergent behavior (Generative Agents\n[1]).\nHowever, this field is still evolving, and the projects men-\ntioned above primarily serve as demonstrations or small-\nscale endeavors. While LLMs introduce new techniques in"}, {"title": "C. Automated Planning", "content": "AP [23] is a branch of AI focused on the importance of\nconstructing a trackable plan to solve a problem, rather than\nmerely finding a solution. As these problems often model\nreal-world scenarios, it is essential to create an accurate\nrepresentation of the environment, known as the world, and\na state of the world would encompass the complete definition\nof a specific state of the environment. An AP problem can be\ndefined by three key elements:\n\u2022 The initial state of the world, typically defined as a set\nof logical predicates.\n\u2022 A goal to achieve, usually defined as a set of conditions\nor logical expressions. The problem is considered solved\nwhen a path is found to a state in which all the conditions\nare satisfied.\n\u2022 A set of actions that can be performed to modify the\nworld state. These actions are typically defined with a set\nof preconditions (the world state required for execution)\nand effects (the modification of the world state upon\nexecution).\nSolving an AP problem typically involves two main steps:\n(1) representing the problem in a formal language interpretable\nby a program, and (2) obtaining the plan using a solver, an\nalgorithm capable of finding a plan to reach the goal from the\ninitial state. Often, it is necessary to use a program familiar\nwith the formal language and the details of the solver to\ntranslate the problem into a model interpretable by the solver.\nWhile there are various approaches to representing plan-\nning problems, the current standard planning language is the\nPlanning Domain Definition Language (PDDL) [24]. PDDL\nis a general-purpose planning language based on the idea of\nseparating the definition of a problem into two files:\n1) The Domain file, which defines the properties of the\nworld. It specifies the types of objects that exist in the\nenvironment, the permissible types of predicates, and all\npossible actions. Additional components can be found\nin a PDDL Domain, with the full list available on the\nplanning.wiki website [25].\n2) The Problem file, which defines a particular problem\nwithin the domain, providing the initial state and goal.\nFor clarity, we provide an illustrated example, a simplified\nversion of The FireFighter Problem (Fig. 4) along with its\ncorresponding PDDL definition (Listings 1 and 2).\nFinally, note that there is a wide variety of solvers (also\nknown as planners) for solving AP problems, each employing\nits own strategies and heuristics. Most state-of-the-art solvers\ninclude systems for executing PDDL problems, and plan-\nning.wiki offers an extensive list of them.3 Notable planners\ninclude GOAP [26], ENHSP [27], Metric-FF [28], and Fast\nDownward [29], the latter being utilized in this project."}, {"title": "III. AGENT ARCHITECTURE", "content": "The complete architecture of the Intelligent Agent is illus-\ntrated in Fig. 5, consisting of three main modules:\n1) The Reasoner, responsible for managing the memory\nstream and the LLM calls.\n2) The Planner, responsible for managing the AP problem\nan the AP Solver calls.\n3) The Interface with the environment, responsible for\ntranslating the data-structures, making calls the other\nmodules and returning the actions to the environment.\nWhile the Reasoner and the Planner are context-\nindependent, the Interface module is not, as its task in-\nvolves direct interaction with the specific characteristics of\nthe environment, requiring dedicated translations and function\ncalls. Additionally, two specific auxiliary modules are also\nnecessary:\n1) A Domain Definition module. As discussed in Chapter\nII. State of the Art, AP problems typically require a\nprior definition of the domain, which is specific to the\nenvironment.\n2) A General Goals module. As outlined in Chapter\nI-B. System Description, the interface must generate all\npossible goals from which the Reasoner selects. This\nmodule provides information about general goals that\nan agent could follow (e.g., \u201cPut out fires\u201d in The\nFireFighter Problem), enabling the interface to create\nspecific goals (e.g., \u201cPut out Fire1\u201d).\nIn this section, we elaborate on the design considerations\nof each of these modules."}, {"title": "A. Reasoner", "content": "The Reasoner module stores the relevant information that\nthe LLM needs and implements the corresponding methods\nto allow the interface to modify it. When the selection of a\nspecific goal is requested to the LLM, this module provides\nthe following information in natural language:\n\u2022 Personality traits: This includes a brief description of the\nrole and expected personality of the specific NPC that is\nreasoning. It encompasses the name, role (e.g., firefighter\nor paramedic), and duty of the NPC. This information is\nprovided by the user and should be the sole necessary\nindividual specification to control the particular behavior\nof each different agent. An example of a personality trait\nused in The FireFighter Problem is: \u201cI am 'FireFighter1',\na firefighter. My duty is to put out fires and, above all,\nto save people.\u201d\n\u2022 Possible Goals. This comprises a list of possible specific\ngoals that the NPC can pursue at a given moment.\nIn The FireFighter Problem, these goals could include\n\u201cSave Peter\u201d or \u201cPut out Fire1,\u201d for example. It must\nbe provided by the interface when a new goal request is\nmade.\n\u2022 Memory Stream. This consists of a list of memories, pri-\nmarily perceptions that the NPC has experienced, written\nin natural language. It allows the LLM to understand the\nparticular situation in which it needs to make a decision.\nThe interface frequently updates this module about the\nworld state, and when a new relevant change occurs, it\nadds it to the memory stream and returns a signal.\nThe main method of this module is the New Goal Setter.\nIt is called by the interface, giving the possible goals list, and\nutilizes all the previously explained information to generate a\nprompt to the LLM, asking for the best goal to set. If it is\na new goal, the one that the Planner had is modified. If it is\nthe previous one, nothing is done. The decision (whether to\nkeep the goal or change to a new one) is then returned to the\ninterface."}, {"title": "B. Planner", "content": "The Planner module is responsible for generating and main-\ntaining an AP Problem that accurately reflects the state of the\nworld at any given moment. It is also responsible for making\nthe calls to the AP Solver to generate a plan. To fully define\nthe AP Problem, the planner stores the following information:\n\u2022 Objects and Predicates. In AP, a world state is typically\ndefined by a set of objects present in the environment\n(along with their types) and a set of logical predicates\nthat defines their initial relations or internal states. In our\nwork, we maintain the same approach.\nTo keep track of the objects and predicates present in\nthe specific world state at any moment, the interface\nregularly reports to this module about the world state.\nWhen an object or predicate appears or disappears from\nthe environment, the internal AP problem is updated\naccordingly, and a signal is sent back to the interface.\n\u2022 Goal. This represents the objective that the solver needs\nto achieve. It is updated by the Reasoner module every\ntime a new goal is established.\nTo generate a new plan, the method Re-plan is utilized. It\nis called by the interface and utilizes the internal AP Problem"}, {"title": "C. Interface", "content": "As we have mentioned, the Interface module is responsible\nfor coordinating the actuation of the other main modules as\nnecessary. Its objective is to generate an intelligent, reactive\nand plausible behavior using the information provided by\nthe other modules. Although the implementation details may\nvary based on the specific data structure of the world, the\nunderlying logic remains consistent. It involves an iterative\nprocess that repeats the following steps each iteration:\n1) Generate the possible goals list. Utilizing the General\nGoals module and the current world state, the Interface\nmodule creates a list of specific goals that could be\npursued by the agent.\nAlthough the General Goals module is explained later\nin this section, in short, it parses the current world\nstate alongside the possible general goals to provide all\nthe feasible specific goals that could be followed. For\ninstance, if there exists the general goal \u201cPut out fires\u201d\nand there is a fire called \u201cFire1\u201d in the world state, the\ninterface generates the specific goal \u201cPut out Fire1\u201d and\nadds it to the list.\n2) Report the new world state to the Reasoner and\nthe Planner. Both modules process the information\nand return a signal indicating whether there has been\na change in the internal representation of the world.\n3) Handle a Re-Think necessity. If the Reasoner indicates\na memories update, the Interface module calls the Rea-\nsoner's New Goal Setter method. It decides which goal\nwill be set and indicates whether the goal has changed\ncompared to the previous one.\n4) Handle a Re-Plan necesity. If the Planner indicates that\nthe internal world state representation has changed due\nto new perceptions or if a new goal has been chosen\nby the Reasoner (i.e., the AP Problem has changed), the\nInterface module calls the Planner's Re-plan method.\n5) Execute the action. Finally, the Interface module exe-\ncutes the action at the top of the plan and removes it\nfrom the plan.\nThe rationale behind this procedure is that the Reasoner\ndecides \u201cWhat to do\u201d using the LLM's decision-making capac-"}, {"title": "IV. TECHNOLOGIES USED", "content": "To implement the proposed model on a real environment,\nwe have needed, in addition to the environment framework, a\ntool to set up and run an LLMs and a software to implement\nthe AP Solver. The technologies finally chosen are explained\nin this section. They are: the Rhymas framework [2] to set up\nthe 3D environment, the LM Studio software [15] to execute\nthe LLM and the Unified-Planning library [30] to define and\nsolve the AP Problem."}, {"title": "A. Rhymas", "content": "As mentioned in Chapter I-A. Use Case, Rhymas is a\nsimulation framework developed in collaboration with the\n\u201cEscola de Bombers i Protecci\u00f3 Civil de Catalunya.\u201d It is\nan under-development framework specifically designed for\nfirefighter training, under which this project is conducted. This\nframework aims to facilitate large-scale training simulations,\nusing a multi-scale paradigm with autonomous agents and\nmulti-agent system technology to reduce the necessity of\nadditional human intervention during training sessions.\nIn Rhymas, a simulation comprises a single back-end,\nwhich implements the multi-agent-based simulation logic, and\nseveral (one or more) front-ends. Each front-end represents\na different point of view of the simulation, providing the\ngraphical representation and controlling specific aspects such\nas perception and physics. Therefore, in Rhymas, every entity\nis defined by two different objects: one in the back-end,\nresponsible for behavior decisions (acting as the \u201cbrain\u201d), and\none in the front-end, which has the sensors and actuators\n(acting as the \u201cbody\u201d). Consequently, the back-end movements\nand interactions have a real effect on the front-end simulation,\nand the front-end changes affect the back-end's internal world\nstate.\nThe reason behind this architecture is to combine the\nmulti-agent and multi-scale paradigms, so every interaction\nis conducted through messages. This includes communication\nbetween the two parts of an entity, interaction between differ-\nent entities, and even information exchange between different\nsimulations in a multi-label parallel execution.\nIn the back-end representation, every entity is seen as an\nagent, with its own behavior and properties. A world state is\ncomposed of all the agents in the scenario, their properties,\nand the possible relations between them. Dynamic behavior\nemerges from the interactions between agents, where they\ncan create relations, erase them, or modify some properties\nof themselves or others. The front-end receives the initial\nworld state from the back-end and initializes a graphical\nrepresentation of it. To create a realistic simulation of actions\nand perceptions, we use a state-of-the-art game engine4. The\nengine simulation is responsible for executing the actions indi-\ncated by the back-end and informing it about the environment\nchanges.\nUnder this principle, defining our own environment is as\neasy as defining the initial world state (scenario) in the\nback-end. It will be automatically built in the front-end and\ndynamically change according to the agents' defined behavior.\nA scenario is defined by the agents initially in the world state,\ntheir initial properties, and the initial relations between them.\nThe Rhymas framework provides a generous list of im-\nplemented agent types, which can be modified or used as\ntemplates to create new ones. Each type is defined by a\nPython class, and the most important method for specifying\nan agent's behavior is the run() method. This method is called\nfor the agents in every iteration, and within it, actions can be\ntaken, including modifying, creating, or erasing relations or\nproperties.\nThus, to define an agent type, it is only necessary to create\nits own class and, to integrate it into a scenario, indicate it in\nthe scenario's definition. To illustrate this process, we provide\nthe following simple example of an agent whose behavior\nconsists of walking forward:\n1) Define the agent's behavior. To do this, we can make\na copy of the provided CommonPerson implementa-\ntion, named ForwardPerson, and then modify the run()\nmethod. (Listing 3)\n2) Define the Scenario. Scenarios are defined by a JSON\nfile containing all the necessary information about the\ninitial world state. To add a new agent, navigate to\nthe \u201cagents\u201d dictionary and add a new entry with all\nthe relevant details. As shown in Listing 4, several key\npieces of information need to be specified:"}, {"title": "B. LM Studio", "content": "LM Studio is an application for running LLMs. It provides\na graphical interface through which a model can be directly\ndownloaded from Hugging Face. Once downloaded, the model\ncan be configured, loaded into memory, and a local server can\nbe run from the interface.\nOnce this setup is complete, the interface offers an in-\nteractive chat tool with the model. The most useful feature,\nhowever, is the accessibility using curl or Python, regardless\nof the specific model being used. We integrated LM Studio\nin our software through Python remote calls, whose process\nis exemplified on Listing 6. Below, we explain how it works\nstep by step:\n1) Instantiate a client connection to the local server. The\neasiest way to do this is using the OpenAI API [31]. (1)\n2) Create the list of messages that the model will\nreceive. For each message, specify the role of the sender\nand its content. Typically, this list consists on only\ntwo messages: a message from the system, providing\na general behaviour specification; and a message from\nthe user, providing the intended prompt. (2)\n3) Make a server request. Make a request providing the\ncreated list of messages and indicating the temperature.\nAs in this case we want solid answers, we use temper\nature 0 to minimize the randomness of the output. (3)\n4) Process the server response. The server will return a\ncompletion object with all the relevant information about\nthe call and the response. This includes the model path,\nthe number generated tokens, etc. The most important\nparameter is the choices attribute, which is a list of\nall generated responses. As multiple responses could be\nreturned from the same call, depending on the model\nand parameter configuration, they are stored as a list of\nchoice items. Each of them contains all the information\nrelated to the specific given response, as its index or the\nfinishing generating reason, but the most important one\nis the message attribute, which provides the generated\nresponse message. In this case, we always generate just\none response, that's why we always take the first element\nof the array. (4)"}, {"title": "C. Unified-Planning", "content": "Despite PDDL being the standard format for writing AP\nProblems", "issues": "the first is that it\nrequires creating the Domain File and the Problem File in plain\ntext instead of using a manageable data structure, which is a\nsignificant handicap when it comes to creating, modifying, or\nconsulting data at runtime; the second is that each AP Solver is\nincluded in its own planning system software, which must be\nexternally called and has its own output format, necessitating\nindividual post-processing of each solver's output to adapt it\nto the desired format.\nThe AIPlan4EU Project aimed to address these problems\nby making a European-wide initiative to unify and user-center\nAP resources. The result of that effort is the Unified-Planning\nlibr"}]}