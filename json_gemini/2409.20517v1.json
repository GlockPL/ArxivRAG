{"title": "SMLE: Safe Machine Learning via Embedded Overapproximation", "authors": ["Matteo Francobaldi", "Michele Lombardi"], "abstract": "Despite the extent of recent advances in Machine Learning (ML) and Neural Networks, providing formal guarantees on the behavior of these systems is still an open problem, and a crucial requirement for their adoption in regulated or safety-critical scenarios. We consider the task of training differentiable ML models guaranteed to satisfy designer-chosen properties, stated as input-output implications. This is very challenging, due to the computational complexity of rigorously verifying and enforcing compliance in deep neural models. We provide an innovative approach based on three components: 1) a general, simple architecture enabling efficient verification with a conservative semantic; 2) a rigorous training algorithm based on the Projected Gradient Method; 3) a formulation of the problem of searching for strong counterexamples. The proposed framework, being only marginally affected by model complexity, scales well to practical applications, and produces models that provide full property satisfaction guarantees. We evaluate our approach on properties defined by linear inequalities in regression, and on mutually exclusive classes in multilabel classification. Our approach is competitive with a baseline that includes property enforcement during preprocessing, i.e. on the training data, as well as during postprocessing, i.e. on the model predictions. Finally, our contributions establish a framework that opens up multiple research directions and potential improvements.", "sections": [{"title": "Introduction", "content": "Recent years have seen a rapid expansion in the deployment of AI and Machine Learning (ML) systems, so that their robustness and safety have become a matter of public concern. In safety-critical or regulated contexts such as automation, healthcare, and risk assessment, AI solutions must comply with specific properties set by designers. In non-critical settings, the ability of AI systems to meet user expectations is still an important factor for their acceptance. The AI act recently passed by the European Union is considered by many as the first of many legal frameworks that will stress the importance of compliance for AI systems in high-risk sectors.\nHowever, training robust models is challenging, for multiple reasons. The theoretical basis for most training formulations allows for some level of error and uncertainty. It's almost impossible to ensure that the training samples are fully representative of real-world usage, leading to unpredictable behavior when out-of-distribution inputs are encountered. Finally, a body of research (Chakraborty et al. 2021; Tseng et al. 2024) suggests that complex AI models can be fragile and susceptible to adversarial attacks.\nSeveral research directions addressing these issues have emerged, but the problem remains largely open. Verification approaches focus on checking compliance for a given property (Liu et al. 2021), but for most architectures this problem is NP-hard and very difficult. Other techniques introduce loss terms linked to undesirable properties (e.g. discrimination), or by adversarial training, which generates and accounts for counterexamples at training time (Muhammad and Bae 2022); both methods struggle to provide out-of-distribution guarantees, either for structural reasons or due to the computational complexity of generating and resolving counterexamples. Some methods enforce property compliance on the training data; others work at inference time, e.g. via input randomization (Cohen, Rosenfeld, and Kolter 2019) or by adjusting the model output to ensure property satisfaction (Yu, Xu, and Zhang 2022).\nWe tackle the problem of training ML models that are robust w.r.t. formal properties specified as implications. To this end, we introduce a general neural architecture that is simple to implement and enables efficient verification with a conservative semantic. The architecture is referred to as Safe ML via Embedded overapproximation (SMLE), since it works by augmenting a backbone network with a low-complexity, trainable overapproximator. For certain classes of properties and design choices, conservative verification of SMLE networks has polynomial time complexity.\nWe use SMLE as a building block for a training framework based on the Projected Gradient Method. Our method guarantees satisfaction of the desired properties upon convergence. While the computational cost is noticeable, it is only marginally affected by the backbone network and remains affordable for practical-scale problems. We ground our framework for two classes of properties. First, we consider properties defined via linear inequalities, a large class with applications such as risk assessment, stable time-series forecasting, collision avoidance. Second, we consider a much more complex (but more specific) combinatorial property, namely mutual exclusions in multi-label classification. We evaluate our approach on synthetic and real-"}, {"title": "Related Work", "content": "Two trends mainly arise from the literature on AI safety and robustness: verification and robust model training.\nVerification methods attempt to formally certify the validity of properties in already-trained models. They rely either on Optimization and Searching or on Reachability Analysis. The former work in a declarative fashion: they encode the ML system into a chosen modeling language, such as Mixed-Integer Linear Programming (Tjeng, Xiao, and Tedrake 2017; Bunel et al. 2018; Fischetti and Jo 2018; Anderson et al. 2020; Tsay et al. 2021) or Satisfability Modulo Theory (Ehlers 2017; Huang et al. 2017; Katz et al. 2017, 2019), hence they search for a counterexample that falsifies the assertion. These methods can perform exact verification, but usually fail to scale to real-world use cases. The latter adopt instead algorithmic approaches: they analyze the layer-by-layer propagation of a set of inputs through a neural network, in order to reconstruct the set of reachable outputs, hence to check whether any of them falls into an unsafe region. By maintaining an outer-approximation of the input set during the propagation (Singh et al. 2018; Xiang, Tran, and Johnson 2018; Gehr et al. 2018; Li et al. 2019; Singh et al. 2019), these methods offer increased tractability, but at the price of a loss of completeness. A comprehensive survey on AI Verification is provided in (Liu et al. 2021).\nRobust training methods, on the other hand, address safety and robustness in ML systems directly in the design of the model. Some of these methods work in a post-processing fashion, by equipping the main model with auxiliary mechanisms that operate right before or immediately after the actual inference. The former detect and purify, or more conservatively reject, malicious inputs (Dhillon et al. 2018; Samangouei, Kabkab, and Chellappa 2018; Yang et al. 2019; Pang et al. 2018; Metzen et al. 2017; Xu, Evans, and Qi 2017); the latter correct the output to enforce constraint satisfaction (Wabersich and Zeilinger 2021; Yu, Xu, and Zhang 2022). Another class of methodologies, widely recognized as one of the most effective to improve (local) robustness, is the so-called Adversarial Training (Madry et al. 2018; Zhang et al. 2019; Shafahi et al. 2019; Wang et al. 2020; Zhang et al. 2020; Wong, Rice, and Kolter 2020; Kim, Lee, and Lee 2021). Here the model is not trained over the original, clean datapoints, but over their worst adversarial examples, or over a combination of the two. The main challenge of adversarial training arises from the generation of adversarial examples, an NP-Hard problem that should be solved iteratively during the training loop. Thus, the focus of this research branch is on designing clever ways to approximate this problem to speed up the computations. A clear overview on Adversarial Training is proposed in (Bai et al. 2021)."}, {"title": "Robust Training Framework", "content": "This section introduces our framework for training differentiable ML models with satisfaction guarantees, starting with a formalization and analysis of the target problem."}, {"title": "Formal Properties and Robust Training", "content": "Let X and Y be random variables respectively representing the model input and the quantity to be predicted; let $\\mathcal{X}, \\mathcal{Y}$ be their supports assumed to be bounded \u2013 and $P(X, Y)$ their joint distribution. Finally, let $f (x; \\theta)$ be a deterministic and differentiable ML model, such as a neural network, with parameter vector $\\theta$. We consider properties stated as implications in the form:\n$\\forall x \\in \\mathcal{X}, Q(x) \\Rightarrow R(f(x;\\theta))$\nwhere Q and R are logical predicates defined respectively over $\\mathcal{X}$ and $\\mathcal{Y}$. This class of properties includes consistency in classification (e.g. \"a dog is also an animal\", mutual exclusive or forbidden labels), bounding the variability of predictions in multi-step time series forecasting, and safety properties for collision avoidance systems. More examples can be found in the VNN competition (Brix et al. 2023).\nWe will not consider properties defined via local perturbations, such as classical adversarial examples or local monotonicity. While predicates modeling these properties for a fixed set of examples can be constructed, true robustness in these settings should account for the actual input distribution $P(\\mathcal{X})$, which is typically inaccessible. While this limitation is common to all robustness methods, it is especially at odds with our approach, which emphasizes full guarantees.\nTraining a robust model, then, amounts to solving:\n$\\text{arg} \\min_\\theta {\\mathbb{E}_{x,y\\sim P(\\mathcal{X},\\mathcal{Y})} [L(y, f(x; \\theta)] \\text{ s.t. eq. (1)}}$\nwhere L is the loss function for an individual example and the expectation is usually approximated via the sample average over the training set. Equation (2) is a constrained optimization problem with a differentiable loss; it can be solved to local optimality, for example, via the Projected Gradient Method (Parikh, Boyd et al. 2014). This approach pairs every gradient update with a geometrical projection in feasible space. Formally, the model parameters are updated via:\n$\\theta^{(k+1)} = \\text{proj}_{\\mathcal{F}}(\\theta^{(k)} \u2013 \\eta^{(k)}\\cdot \\nabla \\widehat{\\mathcal{L}}(\\theta^{(k)}))$\nwhere the superscripts refer to k-th iteration, $\\widehat{\\mathcal{L}}$ is the expectation from eq. (2), and $\\eta^{(k)}$ is the learning rate vector. The projection operator is defined as:\n$\\text{proj} _{\\mathcal{F}}(\\theta) = \\text{arg} \\min_{\\theta'} {||\\theta' \u2013 \\theta||_2 \\text{ s.t. eq. (1) for }\\theta'}$\nIntuitively, we seek the smallest parameter adjustment that guarantees the satisfaction of the desired property. The main"}, {"title": "SMLE Architecture", "content": "We address the mentioned difficulties by introducing a general neural architecture that is simple to implement and enables efficient conservative verification for eq. (1). We start by viewing the ML model f as a composition of an arbitrary embedding function h and a linear (more precisely, affine) output function g:\n$f(x; \\theta) = g(z; \\theta_g) \\circ h(x;\\theta_h)$\nWhere z is the output of the embedding function and $\\theta = (\\theta_g, \\theta_h)$. This decomposition is both natural and common for many neural networks \u2013 including classifiers, by focusing on their logit output. Then, we augment the model by applying clipping to the output of h. In detail, we introduce:\n$\\text{clip}(z; l; u) = \\text{max}(l, \\text{min}(u, z))$\nwhich is employed to obtain:\n$g(\\tilde{z}; \\theta_g) \\circ \\text{clip}(z; \\underline{h}(x; \\theta_\\underline{h}); \\overline{h}(x; \\theta_{\\overline{h}}))h(x;\\theta_h)$\nThe embedding z is processed to obtain a clipped embedding $\\tilde{z}$, with lower and upper bounds computed by two auxiliary models $\\underline{h}$ and $\\overline{h}$. The clipped embedding is then transformed by g to provide the model output. The auxiliary models can be freely chosen, as long as they are differentiable and significantly simpler than the embedding function h. In the simplest case, $\\underline{h}$ and $\\overline{h}$ can be constant vectors, but they could be implemented as fully-fledged neural networks. The structure of our architecture is depicted in fig. 1.\nThe structure of eq. (7) ensures that the input to the g function is contained in the box:\n$\\mathcal{H}(x; \\theta_h, \\theta_{\\underline{h}}, \\theta_{\\overline{h}}) = [\\underline{h}_1(x), \\overline{h}_1(x)] \\times ... \\times [\\underline{h}_n(x), \\overline{h}_n(x)]$\nwhere n is the size of the embedding vector. In other words, our models include a trainable overapproximation. For this reason, we refer to the architecture from eq. (7) as Safe Machine Learning via Embedded overapproximation (SMLE). Many neural architectures employed in AI research and real-world applications can be easily adapted to this structure.\nThe SMLE architecture allows for efficient, albeit conservative, property verification. This can be framed as searching for a counterexample, i.e. an input value that violates eq. (1):\n$\\exists x \\in \\mathcal{X} : Q(x) \\land \\neg R(f(x; \\theta))$"}, {"title": "Conservative Projection", "content": "The SMLE architecture provides the basis for building an efficient, conservative, version of the projection operator from eq. (4). As a first step in this direction, we restrict projection to the output parameters $\\theta_g$ and replace eq. (1) with its conservative SMLE version:\n$\\text{arg} \\min_{\\theta'_g} {||\\theta_g \u2013 \\theta'_g||_2}$\ns.t. R(g(\\tilde{z};\\theta'_g))\n$\\forall x \\in \\mathcal{X} : Q(x), \\forall \\tilde{z}\\in \\mathcal{H}(x)$\nIn the formulation, the parameters $\\theta_h, \\theta_{\\underline{h}}$ are omitted from $\\mathcal{H}$ since they are not affected by projection. Equation (11) is considerably simpler to solve than the original projection operator, since it involves only simple functions and much fewer parameters. The problem remains challenging, however, since it contains an infinite number of constraints due to the use of universal quantification.\nWe address this issue via a delayed constraint generation approach. We iteratively introduce constraints associated to counterexamples, which can be efficiently found thanks to the SMLE architecture. Given a finite collection of counterexamples $\\mathcal{C}$, eq. (11) is then replaced by:\n$\\text{arg} \\min_{\\theta'_g} {||\\theta_g \u2013 \\theta'_g||_2 \\text{ s.t. } R(g(\\tilde{z}; \\theta'_g)), \\forall \\tilde{z} \\in \\mathcal{C}}$\nFor improved efficiency, and without loss of generality, we restrict our attention to the value of the clipped embedding"}, {"title": "Counterexample Generation", "content": "In principle, counterexample generation in algorithm 1 could be efficiently handled by solving eq. (10). In practice, however, that leaves no control on which kind of counterexample is obtained: if these are too weak, the projection process could become unreasonably slow. Unfortunately, defining \"strong\" counterexamples is non-trivial, especially without making assumptions on the nature of the property to be satisfied. In particular, since we treat Q and R as logical predicates, they have no associated continuous measure of constraint violation.\nWe argue that stronger counterexamples are those requiring large parameter adjustments for their resolution. Hence, given a pair (x, z) satisfying Q(x), we propose to define its strength as a counterexample by means of the L1 norm:\n$||\\theta_g \u2013 \\theta^*_g||_1$\nwhere $\\theta_g$ is the current weight vector for the g function and $\\theta^*_g$ is defined as:\n$\\theta^*_g = \\text{arg} \\min_{\\theta'_g} {||\\theta_g \u2013 \\theta'_g||_2 \\text{ s.t. } R(g(\\tilde{z};\\theta'_g))}$\nFor a pair x, z that already satisfies the property, we have $\\theta_g - \\theta^*_g = 0$, while we have $||\\theta_g \u2013 \\theta^*_g||_1 > 0$ for a true counterexample. In eq. (13) we use the L1, rather than L2, norm for its lower computational complexity. To the same end, we also propose to restrict the type of adjustment used to evaluate the counterexample strength. In particular, since g is an affine function, by restricting the allowed changes to those affecting the offset (i.e. translation), we have that:\n$g(\\tilde{z}; \\theta'_g) = g(\\tilde{z}; \\theta_g + \\delta) = g(\\tilde{z}; \\theta_g) + \\delta$\nwhere $\\delta$ is the difference $\\theta'_g \u2013 \\theta_g$. We now frame the problem of generating a strong counterexample as that of finding a pair x, z whose resolution requires the largest translation:\nx*, Z* = arg maxx,z ||\u03b4*||1\ns.t.: \u03b4* = arg ming{||\u03b4||2 s.t. R(g(\u017e; \u03b8g) + \u03b4)}\nQ(x) \u0245\u017e\u2208 H(x)\nwhere ||\u03b4*||1 and ||\u03b4'||2 are equivalent to ||\u03b8*g \u2013 \u03b8g||1 and ||\u03b8'g \u2013 \u03b8g||2, since we are restricted to translation. While translation alone might be insufficient to solve the projection from eq. (4), here we are interested in resolving a single pair x, z. This can always be done by translation if the R predicate has at least one positive assignment. Moreover, we have that ||\u03b4||1 = 0 iff no counterexample exists."}, {"title": "Conservative Projection", "content": "The SMLE architecture provides the basis for building an efficient, conservative, version of the projection operator from eq. (4). As a first step in this direction, we restrict projection to the output parameters $\\theta_g$ and replace eq. (1) with its conservative SMLE version:\n$\\text{arg} \\min_{\\theta'_g} {||\\theta_g \u2013 \\theta'_g||_2}$\ns.t. R(g(\\tilde{z};\\theta'_g))\n$\\forall x \\in \\mathcal{X} : Q(x), \\forall \\tilde{z}\\in \\mathcal{H}(x)$\nIn the formulation, the parameters $\\theta_h, \\theta_{\\underline{h}}$ are omitted from $\\mathcal{H}$ since they are not affected by projection. Equation (11) is considerably simpler to solve than the original projection operator, since it involves only simple functions and much fewer parameters. The problem remains challenging, however, since it contains an infinite number of constraints due to the use of universal quantification.\nWe address this issue via a delayed constraint generation approach. We iteratively introduce constraints associated to counterexamples, which can be efficiently found thanks to the SMLE architecture. Given a finite collection of counterexamples $\\mathcal{C}$, eq. (11) is then replaced by:\n$\\text{arg} \\min_{\\theta'_g} {||\\theta_g \u2013 \\theta'_g||_2 \\text{ s.t. } R(g(\\tilde{z}; \\theta'_g)), \\forall \\tilde{z} \\in \\mathcal{C}}$\nFor improved efficiency, and without loss of generality, we restrict our attention to the value of the clipped embedding"}, {"title": "Robust Training Algorithm", "content": "We can now introduce our robust training procedure, described in algorithm 2. We start by training a SMLE architecture via Stochastic Gradient Descent (SGD) as usual in deep learning, but after every gradient update we perform a single iteration of our projection algorithm. This phase is meant to improve the model accuracy, while accounting for the need to satisfy the desired property without an excessive computational cost. Once convergence is reached according to usual SGD criteria, we attempt a full projection. If the process succeeds, the resulting SMLE model is guaranteed to satisfy the property, without any further intervention. One goal of our empirical evaluation is investigating the success rate or algorithm 2."}, {"title": "Framework Groundings", "content": "Grounding our framework for a given setting requires: 1) choosing a SMLE architecture; 2) defining an implementable formulation for the projection problem from eq. (12); and 3) doing the same for the counterexample generation problem from eq. (16). While the first step is generally easy, the latter two are non-trivial and depend on the considered properties. Here, we discuss viable choices for two settings: 1) the case where Q and R are defined via linear inequalities; and 2) mutual exclusions in multi-label classification. We will focus on mathematical formulations, but obtaining a formally correct grounding requires also to account for solver tolerances and floating-point error propagation. Tolerance-aware formulations can be found in the supplemental material, but we leave robust handling of floating point errors for future work."}, {"title": "Linear Inequalities", "content": "This setup was chosen since it captures a wide range of practically relevant properties (Brix et al. 2023). In this case, Q and R represent polytope membership tests and are defined via the inequalities:\n$Q(x) = Qx \\leq q$\nR(y) = R\u0177 \\leq r\nWhere y is the model output, Q, R are coefficient matrices, and q,r are the corresponding right-hand side vectors. The"}, {"title": "Mutual Exclusive Classes", "content": "The second setup find applications when tagging content, or when determining the traits of biological samples, if certain tag or trait combinations should never be predicted together. It was chosen as an example of a combinatorial property in a classification setting, which comes with unique challenges. Since SMLE assumes, the last layer of the architecture is linear, the Q and R predicates are defined in this case as:\nQ(x) = TRUE\nR(\u0177) = I+(\u0177h) + I+(\u0177k) \u2264 1\nWhere y represents the logit output of the multilabel classifier, F is the set of mutually exclusive class pairs, and the indicator funtion $I+(\u0177h) = TRUE \\text{ iff } \u0177h \\geq 0 \\text{ and } FALSE \\text{ otherwise}$. In fact, if a sigmoid layer is used to obtain the actual classifier output, class h will predicted as true iff \u0177h \u2265 0."}, {"title": "Experimentation", "content": "We conduct an empirical study designed around the following research questions: (Q1, Accuracy) Can our approach achieve an acceptable prediction accuracy, while providing full guarantees? (Q2, Guarantees) How effective is our method at providing guarantees compared to existing approaches? (Q3, Ablation Study) How is our framework accuracy impacted by its key hyperparameters?\nCompared Approaches The baseline for our comparison relies on a maximum-a-posteriori operator, defined as:\nmap(y) = arg maxy,{L(y' | y) s.t. R(y')}"}, {"title": "Q1 Results", "content": "In Figure 2, we compare the predictive performance of our framework against its competitors over the three benchmarks. We evaluate the models with a value in the [0, 1] interval, corresponding to the R2 in regression, and to the average class accuracy in classification. The reported results are obtained on the test sets, and aggregated across the considered properties, sorted by difficulty.\nFigure 2a shows that, as expected, the performance of the considered models decreases with the difficulty of the enforced property. Perhaps counter-intuitively, preprocess appears to outperform oracle, especially as the difficulty of the properties increases. In fact, this is just a side effect of the inability of preprocess to provide full guarantees: since oracle can be outperformed only by violating the property, this performance gap simply indicates that preprocess is leading to significant violations. These two trends do not evidently arise in Figures 2b and 2c. The reason is that, while properties in the Synthetic benchmark are randomly generated, in the other two they are chosen realistically with respect to the data; as a result, they tend to be more consistent with the natural data distribution.\nThe postptocess approach can achieve a high predictive performance, and provides satisfaction guarantees. As already discussed, however, the downside is a more complex and computationally expensive inference process.\nFinally, our framework achieves very promising results: in most cases, it performs very closely to its competitors, and in some cases even better (fig. 2c). Remarkably, although Algorithm 2 may fail to reach convergence, all smle models presented in this computational study were able to achieve it, and hence to provide full satisfaction guarantees. This required, in algorithm 1, a memory size of a single counterexample for linear properties, and only 10 counterexamples for mutually exclusive classes."}, {"title": "Q2 Results", "content": "For a trained SMLE model, reduced accuracy is the only price paid to improve robustness. This is not the case for our competitors, with preprocess being unreliable in terms of property satisfaction and postprocess having a higher computional cost of inference. On the Syn-"}, {"title": "Q3 Results", "content": "Finally, we investigate the impact in terms of predictive capabilities of two key design choices in our framework, namely, the auxiliary models h, h and the backbone model h. In particular, on the regression benchmarks, we compare two overapproximators with different complexities (constant versus linear), as well as two embedding models with different depth and width (small versus large). On the Forecasting benchmark, we also compare to two different types of backbone (ReLU versus LSTM).\nTable 1, where we display the results over the test sets in terms of R\u00b2, aggregated across tasks and properties, shows that our framework is quite robust to different design choices, with only small differences reported between each considered setup. This suggests that defining the hyperparameters for our framework should not be significantly more difficult than for regular scenarios."}, {"title": "Conclusion", "content": "We introduced the SMLE architecture, which augments a backbone network with an embedded, trainable overapproximator, and enables conservative property verification with controllable complexity. We use our architecture to design a framework for training models guaranteed to satisfy desired properties, consisting of two main components: 1) a projection algorithm with delayed constraint generation; and 2) a method to generate strong counterexamples. We showed how to ground our method on two classes of properties, and demonstrated that it can be competitive with strong baselines, while offering unique advantages, precisely: stronger guarantees compared to preprocessing techniques, and simpler inference compared to postprocessing methods.\nOur contributions open up several directions for future research. First, there are opportunities to reduce the computational cost of our method, e.g. by replacing the first phase of algorithm 2 with traditional adversarial training, by simplifying the counterexample generation problem, or by using a linear cost at projection time. Second, we believe that our framework might be adapted to address properties involving more than one example, such as fairness constraints, or global monotonicity. Third, the SMLE architecture itself might be extended to deal with variable input sizes via Transformers or Graph Neural Networks."}]}