{"title": "Exploiting Point-Language Models with Dual-Prompts for 3D Anomaly Detection", "authors": ["Jiaxiang Wang", "Haote Xu", "Xiaolu Chen", "Andreas Jakobsson", "Haodi Xu", "Yue Huang", "Xinghao Ding", "Xiaotng Tu"], "abstract": "Anomaly detection (AD) in 3D point clouds is crucial in a wide range of industrial applications, especially in various forms of precision manufacturing. Considering the industrial demand for reliable 3D AD, several methods have been developed. However, most of these approaches typically require training separate models for each category, which is memory-intensive and lacks flexibility. In this paper, we propose a novel Point-Language model with dual-prompts for 3D ANomaly detection (PLANE). The approach leverages multi-modal prompts to extend the strong generalization capabilities of pre-trained Point-Language Models (PLMs) to the domain of 3D point cloud AD, achieving impressive detection performance across multiple categories using a single model. Specifically, we propose a dual-prompt learning method, incorporating both text and point cloud prompts. The method utilizes a dynamic prompt creator module (DPCM) to produce sample-specific dynamic prompts, which are then integrated with class-specific static prompts for each modality, effectively driving the PLMs. Additionally, based on the characteristics of point cloud data, we propose a pseudo 3D anomaly generation method (Ano3D) to improve the model's detection capabilities in unsupervised setting. Experimental results demonstrate that the proposed method, which is under the multi-class-one-model paradigm, achieves a +8.7%/+17% gain on anomaly detection and localization performance as compared to the state-of-the-art one-class-one-model methods for the Anomaly-ShapeNet dataset, and obtains +4.3%/+4.1% gain for the Real3D-AD dataset. Code will be available upon publication.", "sections": [{"title": "1. Introduction", "content": "The area of 2D Anomaly Detection (AD) has attracted notable attention during the past decades[1, 2, 3, 4]. With the increasing importance of 3D point clouds in autonomous driving and robotics, research on 3D point cloud AD has also garnered growing attention. [5, 6, 7]. This phenomenon can be explained by the fact that, compared to 2D images, point cloud data encapsulates richer structural and spatial information, enabling more detailed and precise detection.\nCurrent methods for 3D AD predominantly employ various forms of unsupervised approaches [8, 6, 9], wherein only normal samples are utilized during training to tackle the challenges posed by the variability of anomalies and the difficulty in collecting anomalous samples. These methods are mainly divided into two categories: reconstruction and embedding-based approaches, both of which have evolved from recent 2D AD techniques. Reconstruction methods are based on the premise that models trained on normal samples cannot effectively reconstruct anomalous ones[6, 8]. During the inference phase, anomalies are then detected by comparing the original samples with their reconstructions. However, due to higher acquisition costs, 3D point cloud data is typically less abundant than 2D data. This scarcity of training data often leads to overfitting, which results in poor performance for these models. Embedding-based methods utilize pre-trained networks to construct memory banks of normal features for AD[10, 11, 12]. During the inference phase, the anomaly score is then determined by comparing the similarity between the features of the test samples and the features stored in the memory bank for normal data. Nevertheless, this memory bank grows incrementally with the number of samples, and the requirement to compare each feature during inference significantly increases processing time, rendering these approaches impractical. Furthermore, current methods generally adhere to a one-class-one-model paradigm, which not only increases storage costs but also limits flexibility. To alleviate these shortcomings, we propose a novel 3D point cloud AD approach that is both accurate and efficient, while also capable of handling multi-class detection within a single model.\nVision-language models (VLMs) [13, 14, 15] pre-trained on large-scale vision-language datasets can effectively transfer knowledge to various downstream tasks, achieving impressive results, without the need for full-parameter fine-tuning. Similarly, in the 3D point cloud domain, specialized Point Language Models (PLMs) [16, 17] have been developed, suggesting that relying on the robust representation capabilities of PLMs is a viable approach for achieving multi-class 3D point cloud AD within a single model, particularly in scenarios where the training data is limited.\nInspired by existing 2D AD methods utilizing VLMs [18, 19, 20], we apply PLMs (e.g., ULIP2 [17]) to perform AD by measuring the similarity between multi-level point cloud and text features. The effectiveness of this approach relies on the precise re-alignment of features across different modalities during training. However, current 2D AD VLMs-based methods are suboptimal because they adjust features in a single modality (language or vision) for unidi-rectional alignment, which prevents the dynamic alignment of both representation spaces on the AD task. Hence, we propose the Point-Language model with dual-prompts for 3D ANomaly dEtection (PLANE) approach. Specifically, recognizing the minimal intra-class sample variations characteristic of AD, we construct learnable class-specific static prompts for each modality. In addition, we employ the dynamic prompt generation module to create sample-specific dynamic prompts, which are then integrated with the static prompts for each modality. This approach promotes inter-action between modality-specific prompts and further enhances the alignment of multimodal features. Additionally, existing methods demonstrate that representations learned by detecting irregularity introduced by synthetic anomalies generalize well to detecting real defects [21, 22, 23]. In this way, we propose a pseudo 3D anomaly generation method"}, {"title": "2. Related Work", "content": "This study categorizes existing industrial AD tasks into two primary categories: 1) 2D and 2) 3D AD, which are introduced as follows."}, {"title": "2.1. 2D Anomaly Detection", "content": "2D AD methods can be mainly categorized into reconstruction and embedding-based approaches. Reconstruction methods assume that models are unable to reconstruct anomalous images because of training only on normal images[22, 24, 21]. DRAEM [22] trains a U-Net network for reconstruction using artificially generated pseudo-anomalies and employs a discriminator network to distinguish anomalous regions. Embedding-based methods can be further divided into memory bank approaches and vision-language models approaches [25, 10]. Patchcore [10] uses a pre-trained feature extractor to build a greedy coreset of representative normal features. VLMs methods leverage the powerful generalization capabilities of pre-trained models[18, 19]. WinClip [18] utilizes the pre-trained CLIP model to effectively extract and aggregate multi-scale text-aligned features. PromptAD [19] addresses the limitations of manually setting text prompts by introducing prompt learning, thereby constructing suitable text prompts to guide the model in performing AD more effectively."}, {"title": "2.2. 3D Anomaly Detection", "content": "3D AD is a more challenging task than 2D AD due to its typically unordered nature and high level of sparsity. Derived from 2D AD methods, AD in 3D can also be divided into reconstruction and embedding-based. IMRNet, which is inspired by the masking strategy, trains a point cloud reconstruction network in a self-supervised manner[6]. R3D-AD uses a diffusion model to transform the point cloud reconstruction task into a conditional generation task, detecting anomalies through a trained reconstruction network[8]. M3DM detects and localizes anomalies through unsupervised feature fusion, combining multimodal information with multiple memory banks[9]. CPMF renders 3D point cloud data into 2D images from multiple viewpoints and builds a memory bank of complementary multimodal features [26]. Reg3DAD constructs a feature memory bank of normal point cloud samples using a pre-trained 3D feature extractor[5]. CFM4IAD achieves mutual mapping between 2D and 3D features through fully connected layers, facilitating collaborative detection[27]. The CLIP3D-AD method [28] leverages point cloud projection to eliminate the modality gap with the pre-trained CLIP [13] model, enhancing vision and language correlation through the fusion of multi-view image features."}, {"title": "3. Methodology", "content": "As illustrated in Fig. 2, the proposed PLANE method primarily consists of a pretrained PLM, along with learnable text and point cloud prompts. These dual multimodal prompts are composed of both static and dynamic components. Leveraging this prompt configuration, the pre-trained PLM extracts point cloud and text features, which are then projected onto a joint embedding space via a feature adaptation layer. Finally, anomalous regions are detected by measuring the similarity between multi-level point cloud features and text features. Additionally, to address the issue of overfitting due to insufficient data, we propose a pseudo 3D anomaly generation (Ano3D) method for data augmentation. This approach expands the training dataset, enhancing the model's generalization ability. The details are explained in the following sections."}, {"title": "3.1. Dual-Prompt Learning", "content": "Earlier efforts have demonstrated the effectiveness of using integrated handcrafted prompts for AD. However, the capacity of these prompts is inherently limited, as natural language cannot capture the full scope of detailed vision information. This limitation may lead to a misalignment between fine-grained point cloud features and text features, especially in tasks that require precise localization of anomalous regions. To mitigate this issue, some methods have introduced learnable text prompts in 2D AD tasks [19, 29], but these approaches allow only for a unidirectional adjustment of the text representation space. In response, this paper introduces dual prompts that bidirectionally adjust the representation spaces of both modalities within the PLM, improving their suitability for 3D AD tasks. The following section provides a detailed discussion of the prompts for each modality."}, {"title": null, "content": "In contrast to previous designs of learnable text prompts, the proposed approach incorporates both static and dynamic components in the normal prompt $S^n$ and in the anomalous prompt $S^a$:\n$S^n = [P_1][P_2]...[P_N][P_{dyn}][obj],$\n$S^a = [P_1][P_2]...[P_N][P_{dyn}][obj],$\nwhere $N$ denotes the length of learnable static prompts, $P_{dyn}$ denotes the dynamic prompts, and the symbol $[obj]$ denotes the category's name. It is noteworthy that, in addition to text prompts, the prompts for the point cloud modality also include both dynamic and static components. This design is based on the observation that variations among samples within the same class are minimal in AD tasks. As a result, we implement the class-specific static prompts for point cloud modality, allowing objects of the same class to share a consistent set of prompt components. To further refine the alignment between the two modalities, sample-specific dynamic prompts are introduced, generated by the dynamic prompt creator module (DPCM). It is worth noting that the final layer features of the point cloud encoder encapsulate critical global information. Therefore, the DPCM maps these features through a Multi-Layer Perceptron (MLP) and slices them to generate both point cloud dynamic prompts and text dynamic prompts. Given an input point cloud represented as $P_n$, the DPCM process can be formalized as:\n$f_{pc} = PointEncoder(P_n),$\n$f_{dyn} = MLP(f_{pc}).$\nwhere $f_{dyn}$ denotes dynamic prompts, we then split $f_{dyn}$ into two parts, $f_{dyn} = \\{ f_{dyn}^t, f_{dyn}^p \\}$, and $f_{dyn}^t$ and $f_{dyn}^p$ denote dynamic text prompts and dynamic point cloud prompts, respectively. The dynamic prompts generated from point cloud samples are combined with static prompts from their respective modalities, enhancing the correlation between the two modalities.\nThe pre-trained point cloud encoder is primarily designed for classification, and its intermediate layer features are not aligned with text data during pre-training. This misalignment limits its capacity for fine-grained detection in AD tasks. To address this, the Point Cloud Feature Adaptation (PCFA) module is introduced to project the intermediate point cloud features into a joint space shared with text features. Within PCFA, the intermediate layer features of the point cloud, along with class-specific static point cloud prompts and sample-specific dynamic point prompts, serve as inputs. A feedforward network (FFN) then integrates the point cloud features $F_{ori}$ and point cloud prompts to achieve alignment with the text features. This process may be expressed as:\n$F_i^p = FFN([P_{sta}, P_{dyn}, F_{ori}]),$\nwhere $P_{sta}$ represents the class-specific static prompt, $P_{dyn}$ the sample-specific dynamic prompt, and $[\u00b7, \u00b7]$ the con-catentation operation. By utilizing PCFA and the dual-prompt method to bidirectionally realign point cloud and text features, the AD result at the i-th hierarchy can be generated as:\n$S_i = \\Phi ( \\frac{exp (cos (F_i^p, F_i^n))}{exp (cos (F_i^p, F_i^n)) + exp (cos (F_i^p, F_i^a))} ),$\nwhere $\\Phi$ denotes the reshape and interpolate function, $F_i^p$ represents point cloud features, whereas $F_i^n$ and $F_i^a$ indicate normal text features and anomalous text features, respectively."}, {"title": "3.2. Pseudo 3D Anomaly Generation", "content": "In this section, we introduce the Ano3D method for Pseudo 3D anomaly generation, aimed at augmenting the training dataset. This method simulates three common types of anomalies (e.g., hole, bulge, and concavity) based on the characteristics of 3D point clouds.\nSpecifically, we first randomly initialize the rotation angles $(\\theta_x, \\theta_y, \\theta_z)$ to obtain the rotation matrix. By multiplying the original point cloud sample with this rotation matrix, the rotated point cloud sample is generated. For protrusions"}, {"title": "3.3. Loss Function", "content": "The proposed PLANE method is an end-to-end trainable model. In the approach, the text encoder and point encoder models are frozen. To train the projection layer and learnable prompts, two loss functions are employed using focal loss and dice loss. Focal loss [30] addresses the issue of class imbalance, which is particularly relevant in 3D AD where the majority of points are normal and only a small portion are anomalous. Therefore, focal loss is utilized to mitigate this imbalance. Dice loss [31], derived from the dice coefficient and commonly used in image segmentation tasks, can alleviate the class imbalance between normal and anomalous points. The total loss function for PLANE is then calculated as:\n$L_{total} = L_{Focal}(S_i, G) + L_{Dice}(S_i, G),$\nwhere $S_i$ denotes the i-th intermediate anomaly map and $G$ the ground truth corresponding to the input point cloud."}, {"title": "4. Experiment", "content": "The comparative methods employed in this study are as follows: four embedding-based methods, namely BTF [33], M3DM [9], Patchcore [10], and Reg3DAD [5], as well as two reconstruction methods, namely IMR [6] and R3DAD [8]\u00b9. Here, the M3DMMAE uses PointMAE [34] as the point cloud feature extractor, whereas M3DMBert instead uses PointBert [35] as the extractor. The representation method for Patchcore is also the same. It is worth noting that the results reported for the other methods in the table are based on one-class-one-model paradigm, whereas we use a single model to perform detection across all classes, which is not only more prevalent but also more valuable in practical industrial scenarios."}, {"title": "4.4. Ablation Study", "content": "To investigate the effectiveness of individual components, we conduct ablation studies using the Anomaly-ShapeNet dataset.\nEffects of the Key Components. We evaluate the effectiveness of our components, including the pseudo-anomaly sample generator (Ano3D), the point cloud feature adaptation (PCFA), and the dynamic prompts creator module (DPCM). As shown in Table 4, the experimental results indicate that the pseudo-anomaly generation addresses the issue of limited normal samples, significantly improving the performance. Furthermore, PCFA can be seen to bring a large improvement of 8.8%\u2191 on the Object-AUROC. The DPCM also leads to performance enhancements, boosting the Object-AUROC by 2.3% to reach 0.836, and improving the Point-AUROC by 1.1%, bringing it to 0.821.\nEffect of the Text Prompt. We use t-SNE to visualize the distances between normal and anomalous prompt embeddings within the feature space. Fig. 6(a) illustrates that there are no clear boundaries separating different types of text prompts, which significantly impairs pre-trained model performance in the downstream AD tasks. In contrast, Fig. 6(b) shows that with text prompt tunning, distinct boundaries emerge between different types of text prompts in the feature space, effectively illustrating its benefits.\nEffect of Point Cloud Prompt. We further compare the proposed point cloud prompt learning with existing visual prompt tunning (VPT) methods [36]. The experimental results are summarized in Table 7, where VPT-shallow implies that visual prompts are added only at the first layer whereas in VPT-Deep, the visual prompts are added at each layer. The results demonstrate that the proposed prompt learning method has better performance for both anomaly detection and localization.\nStudy on the static prompt length. We investigate the impact of the length of the point cloud and text prompts on model performance. As shown in Fig. 7, when the value of length is relatively small, the detection and localization results improve as the length increases. However, as the length of the prompts continues to increase, there is a decline in performance, suggesting that overly complex prompts may contain redundant information. Therefore, selecting an optimal length, such as N = 6 for text prompts and M = 4 for point cloud prompts, helps the model learn effective prompts.\nEffects of the Focal Loss and Dice Loss. We also conduct an ablation study on the loss functions. The ex-perimental results indicate that both loss functions are effective for PLANE, as they mitigate the imbalance between anomalous and normal points caused by the small size of the anomalous regions. Table 6 shows the quantitative re-sults. The method achieves 0.744 Object-AUROC and 0.593 Point-AUROC using only the dice loss, while achieving 0.625/0.769 with only the focal loss. Using both loss functions simultaneously yields the best experimental results, at 0.836 Object-AUROC and 0.821 Point-AUROC.\nEffect of multi-layer features. The effectiveness of multi-layer feature aggregation is studied, and the results are presented in Table 7. The results indicate that multi-layer features {2, 5, 8, 11} outperform single-layer {2} or"}, {"title": "5. Conclusion", "content": "In this paper, we propose a 3D point cloud AD method based on pre-trained PLMs called PLANE, which detects anomalies by measuring the similarity between point cloud and text features. We propose a dynamic-static dual-prompts approach that enables the PLMs to transfer knowledge to the downstream AD tasks. Additionally, a pseudo-anomaly generation method based on point cloud data characteristics is introduced for data augmentation, thereby enhancing the model's generalization capabilities. Experiments demonstrate that the proposed multi-class-one-model method outperforms existing methods following the one-class-one-model paradigm on the Anomaly-ShapeNet and the Real3D-AD datasets."}]}