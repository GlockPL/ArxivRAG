{"title": "Generative Monoculture in Large Language Models", "authors": ["Fan Wu", "Emily Black", "Varun Chandrasekaran"], "abstract": "We introduce generative monoculture, a behavior observed in large language mod-\nels (LLMs) characterized by a significant narrowing of model output diversity\nrelative to available training data for a given task: for example, generating only\npositive book reviews for books with a mixed reception. While in some cases,\ngenerative monoculture enhances performance (e.g., LLMs more often produce\nefficient code), the dangers are exacerbated in others (e.g., LLMs refuse to share\ndiverse opinions). As LLMs are increasingly used in high-impact settings such as\neducation and web search, careful maintenance of LLM output diversity is essen-\ntial to ensure a variety of facts and perspectives are preserved over time. We exper-\nimentally demonstrate the prevalence of generative monoculture through analysis\nof book review and code generation tasks, and find that simple countermeasures\nsuch as altering sampling or prompting strategies are insufficient to mitigate the\nbehavior. Moreover, our results suggest that the root causes of generative mono-\nculture are likely embedded within the LLM's alignment processes, suggesting a\nneed for developing fine-tuning paradigms that preserve or promote diversity.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) show promise\ndue to their emergent abilities [1] and state-of\nthe-art performance on several NLP tasks [2].\nHowever, concerns have been raised about\nthe increasing reliance on LLM-based systems\nwith insufficient testing, and how they impact\nsociety [3, 4]. Recent evidence has shown that\nLLMs have dangerous tendencies: they con-\nvincingly return incorrect information [5, 6, 7],\nproduce toxic language [8, 9], and can effec-\ntively propagate misinformation [10, 11].\nIn this paper, we focus on a different concern:\nthat for a given prompt and task, LLMs do\nnot faithfully represent the diversity of poten-\ntial responses available in their training data.\nWe call this behavior generative monoculture:\ngiven some task (e.g., generating book reviews) and a data attribute (e.g. sentiment), generative\nmonoculture refers to the narrowing of the probability distribution of the considered attribute from\nsource data (i.e., available human-generated book reviews as part of the training data) to the gener-\nated data (i.e., LLM-generated book reviews).\nAs a preview, for book reviews (Fig. 1(Left)), we compare the diversity in sentiment of Goodreads\nreviews [12] (i.e., src)\u2014very likely a portion of LLM training data [13]\u2014with LLM-generated\nreviews (i.e., gen). The range of mean sentiment scores per book across gen book reviews is much\nnarrower than that in the src: in the experiment pictured, the average sentiment score for gen\nreviews are mostly over 0.85, whereas the the average sentiment score for src reviews over the\nsame books have a wider range from zero to one. For the code generation task (Fig. 1(Right)), the\nrange of algorithms employed in (correct) solutions to a given coding problem (i.e., gen) was much\nless varied than a sample of human answers (src) available on the web [14]: we show this through\nthe range of Jaccard similarity of the algorithms employed in sets of human-generated (src) as well\nas LLM-generated (gen) responses to a coding prompt.\nThrough the rapid adoption of LLMs such as ChatGPT, CoPilot, and Devin across education, code\ngeneration, and day-to-day information gathering, generative monoculture can harm society through\nloss of information, creativity, and intellectual diversity. For example, students asking LLMs ques-\ntions about class material to get help researching for an essay may have their opinions formed with-\nout exposure to a sufficiently wide subset of available information; this will allow for certain opin-\nions to die out over time. Concretely, a reduction in diversity of sentiment displayed above may\nlead to the loss of arguments from negative opinions on controversial books, potentially crucial for\nhistorical or literary context or a nuanced understanding of a book's contributions.\nGenerative monoculture could even lead to security threats, depending on the application: soft-\nware engineers across the globe relying on ChatGPT and CoPilot receiving similar code generations\nwhich do not reflect the true diversity of methods to solve a given problem may lead to similar code\nvulnerabilities across several large tech companies. Indeed, as we preview in Fig. 1 and show in\ndetail in \u00a7 6, LLM output exhibits are less diverse than human-generated solutions in the training\ndata (e.g., by employing a narrower array of algorithms), which could lead to similarity across a\nwide range of code bases, leading in turn to repeated vulnerabilities [15, 16]. However, in this case,\nLLM outputs not reflecting the full diversity of available coding examples can be positive: LLM\noutputs over-represent correct and efficient solutions. We present a nuanced picture of generative\nmonoculture: while it can highlight the optimal portion of human-generated data for some attributes,\nits pervasiveness across generation tasks and attributes may cause harm without careful intervention.\nIn this paper, we (1) define the concept of generative monoculture, and compare it to prior work\naround related topics (\u00a7 2 and 3); (2) introduce a paradigm for measuring generative monoculture\nin LLMs (\u00a7 4); (3) show experimental evidence for the prevalence of generative monoculture across\na variety of application areas (book reviews and code) and (open-source and proprietary) LLMs,\nand provide some evidence for what may exacerbate generative monoculture, such as alignment\ntuning [17], (\u00a7 6 and 7); and (4) show the (in)efficacy of several methods to abate generative mono-\nculture; namely, changing temperature, sampling, and prompting techniques (\u00a7 6 and 7)."}, {"title": "Defining Generative Monoculture", "content": "We broadly characterize generative monoculture as a distribution shift from source data (i.e., human-\ngenerated training data) to model generated data (i.e., model outputs) for a specific task, such as gen-\nerating reviews for books or solutions to coding problems. This can be formalized using measures\nof statistical dispersion applied to various task-specific attributes.\nDefinition 1 (Generative Monoculture). For a given task, let \\(P_{src}\\) denote the probability distri-\nbution of the source data, \\(P_{gen}\\) denote the probability distribution of the LLM-generated data, h\ndenote a function extracting attributes from data (such as sentiment, or algorithms used in code),\nand Dispersion(\u00b7) denote a dispersion metric (e.g., entropy). Then we define generative monocul-\nture as the condition where \\(P_{gen}\\) is statistically narrower than \\(P_{src}\\), namely: Dispersion(h(x)|x ~ \\(P_{gen}\\)) < Dispersion(h(x)|x ~ \\(P_{src}\\)).\nNote, \\(P_{src}/P_{gen}\\) can be the distribution of human-generated/model-generated responses, for a given\ntask, conditioned on one specific given prompt (which we refer to as the conditional distribution),\nor the distribution of human-generated/model-generated responses for any possible prompt in a con-\nsidered domain (which we call the unconditional distribution).\nThis phenomenon signifies a shift towards less varied outputs. We emphasize that the investigation\nof generative monoculture is intrinsically task-dependent, as the attributes of interest differ across\ntasks. In addition, as we often do not have access to the source distribution in practice, we approx-\nimate it using a source dataset (Dsrc), comprised of a subset of the training data of the LLMs we\nconsider. Similarly, we approximate the generated distribution through a dataset generated by the\nmodel (Dgen)."}, {"title": "Generative Monoculture, Human Preference, and Alignment", "content": "Generative monoculture can\ncause LLMs to over-emphasize human-preferred areas of a distribution for a certain data attribute;\nthis is often desired behavior. For example, as we demonstrate in \u00a7 7, generative monoculture can\nresult in having a narrower distribution of code correctness or efficiency biased towards correct,\nfast, and low-memory code. We conjecture this is a consequence of alignment procedures such as\nreinforcement learning with human feedback (RLHF) [17].\nHowever, when a tendency stemming from human preference bleeds beyond its intended use-e.g.,\na preference for positive sentiment affecting outputs that need not or should not be positive\u2014these\nseemingly advantageous behaviors can prevent an equally important goal: maintaining diversity of\nhuman opinion and expression. Further, along data attributes which do not have a clear preferred\narea of the distribution, generative monoculture can limit the scope of methods, topics, or ideas\nexpressed."}, {"title": "Related Work", "content": "Diversity and LLMs. Santurkar et al. [18] demonstrate that LLMs do not give representative opin-\nions to polling questions when compared to the general U.S. population. Our work focuses on the\nnarrowing of diversity in LLM output from its human-generated training data\u2014while Santukar et al.\ndemonstrate a narrowing in diversity from actual human survey respondents (and not training data).\nAdditionally, our work proposes a general framework for measuring monoculture. Padmakumar and\nHe [19] demonstrate that using LLM assistance can lead to reduced diversity in human-generated ar-\ngumentative essays when compared to essays written without LLM assistance. While they mention\nthat this is partially because the models themselves do not produce diverse output, they do not focus\non the narrowing of diversity from LLM training data to LLM-generated data. Finally, Sorensen et\nal. [20] outline an alignment framework to emphasize pluralism, to work towards creating models\nwhich express a variety of opinions and perspectives. While this is certainly related to our work,\ngenerative monoculture as a phenomenon extends beyond differences in opinion, and expresses the\nnarrowing of any number of task-specific attributes, from code correctness to topics covered to many\nothers. One common thread across many of these works, which our work adds to, is that current\nalignment practices\u2014namely RLHF\u2014 harms output diversity.\nOther Notions of Monoculture. Our notion of generative monoculture relates to, but differs from,\nother notions of monoculture in the AI literature. For example, algorithmic monoculture [21] and\noutcome homogeneity [22] describe the societal state where many decision-making actors rely on\nthe same underlying algorithms to generate (classification or ranking) predictions, from the perspec-\ntive of decision-making actors and individuals subject to those decisions respectively. These works\nshow that algorithmic monoculture is sub-optimal for both decision-making actors (due to corre-\nlated failures across models) and for those subject to model decisions, as repeated outcomes across\nmodels leave little room for algorithmic recourse. In contrast, generative monoculture focuses on\ndocumenting the phenomenon of individual LLMs narrowing the diversity of their output in relation\nto their source data-for example, only returning positive book reviews about a controversial book.\nWe do, however, document in this work that generative monoculture exists to similar extents and\nin similar directions across a variety of available LLMs, (e.g., Llama, Vicuna, ChatGPT-4) leaving\nopen the possibility of concerns brought up by Kleinberg and Raghavan [21], but in a generative\ncontext."}, {"title": "Measuring Generative Monoculture", "content": "We outline a general approach to measuring generative monoculture in LLMs. In particular, follow-\ning Definition 1, we outline steps to construct Dsrc and Dgen and compare their diversity through\nextracting data attributes and calculating dispersion metrics. We illustrate our approach in Fig. 2."}, {"title": "Data Curation", "content": "For a given task, we aim to create a source dataset that is likely to have been used in training the\nLLM we wish to investigate. Training data for most LLMs is a closely guarded secret. While recent\nwork [23] describes how dataset contamination can be determined, such approaches are (a) riddled\nwith false positives, and (b) computationally expensive. Thus, we often take an educated guess\n(based on dataset popularity and ease of use) in ascertaining if a given dataset is a likely training\ndataset candidate.\nFormally, we define the source dataset as Dsrc = {qi, srci}i\u2208[N] where (a) qi is a problem\ninstance within a task (e.g., name of a book for which a review has to be written), and (b)\nsrci = {src}je[n\u2081] is a set of ni human-generated answers to the given prompt qi (e.g., a set\nof ni of book reviews for that particular book). In practice, we utilize existing datasets likely to be\nused during LLM training, and perform filtering and sub-sampling to obtain our Dsrc.\nTo create the model-generated dataset Dgen, for each sample qi, we prompt the LLM we wish\nto evaluate, M, m\u2081 times to generate a set of responses, gen = {gen}je[m\u00b2]. Here, gen\u2190\nMj (Ptask (qi), kwargs) is the response obtained in the j-th call of M, where (a) Ptask denotes the\ntask-specific formatting prompt that wraps the sample qi, and (b) kwargs denotes the generation\nkeyword arguments (e.g., temperature) that specify the sampling strategy. Across both Dsrc and\nDgen, we select or generate a large enough number of responses per qi to ensure variety."}, {"title": "Attribute Extraction", "content": "For a given task, we identify and compile a list of attributes that are of interest from the perspective of\npreserving diversity. This is a subjective task, but we focus on metrics which target understanding the\ncontent of LLM output (e.g., book review sentiment and topic; code time complexity and algorithms\nused), as opposed to more general metrics of output language quality such as saliency, fluency and\ncoherence. More importantly, we need to ensure that extraction functions are efficient, accurate,\nand reproducible\u2014we outline our tests to ensure these qualities in \u00a7 6 and the Appendices B and D.\nFor example, care must be taken to use LLMs for attribute extraction, as they are known to be\nbiased towards their own responses [24]. For a given attribute A, extraction function ha takes a\nstring tgt to obtain the attribute value h\u2081(tgt). Note that tgt can either be src or gen. The\nextracted attribute can either be a continuous/categorical variable or of other more complicated types,\ndepending on the nature of the attribute."}, {"title": "Metric Calculation", "content": "As the last step, we compute metrics on the extracted attributes. Given a set of responses, dispersion\nmetrics aim to capture their breadth or coverage of these attributes. We describe those used in this\npaper below and in Fig. 3.\nDispersion Metrics. We introduce dispersion metrics suited to different data types.\nA. Distribution of the mean. For ordinal or continuous attributes, we calculate the mean over the\nconditional distribution that is, we calculate the metrics over each src\u012f or gen; (e.g. reviews for\na given book), and show the distribution of this mean over all qi for a certain task (e.g. over all\nbooks). While the mean itself does not directly measure dispersion, the distribution of the mean\nvalues sheds light on dispersion: concentrated mean values indicates a smaller dispersion of the\ndata. The advantage is that it not only describes dispersion, but also the qualitative tendency of the\nattribute (e.g. bias towards positive/negative sentiment), which cannot be captured otherwise.\nB. Entropy and standard deviation. For categorical attributes, we measure dispersion using entropy\nover the conditional distribution. For continuous attributes, we use standard deviation over the con-\nditional distribution to quantify the dispersion of values around the mean, providing the attribute's\nvariability.\nC. Mean pairwise similarity. For attributes that are not easily characterized as categorical or contin-\nuous, we adopt specific similarity metrics catered to the data type. We then calculate the pairwise\nsimilarity values for the conditional distribution-i.e. calculate the mean similarity value for all the\npairs of elements within each srci/gen, then show the distribution for all N samples. We use:\n1. Mean pairwise Jaccard index: Given two sets of categorical variables A and B, the Jaccard index,\n\\(J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\\), measures similarity between them. An example of such a set could be a\nset of several algorithms inferred from a piece of code. A higher mean Jaccard index indicates a\nhigher overall similarity between the set, and consequently, lower dispersion.\n2. Mean pairwise cosine similarity: Given two multi-dimensional embeddings e1 and e2 ob-\ntained via a sentence embedder [25], we calculate their similarity via cosine similarity, i.e.,\n\\(S_{c}(e_1,e_2) = (e_1,e_2)/||e_1||||e_2||\\). A higher mean cosine similarity indicates a higher similarity and\nlower dispersion.\n3. Mean pairwise fingerprint similarity: For tasks related to coding or computer programs, similarity\nis based on the overlap of selected hash values (or fingerprints) generated by Winnowing [26].\nWe adopt an existing open-source tool COPYDETECT [27], which takes in a set of programs and\nreturns the pairwise similarity scores for all programs in the set. We then calculate the mean\nvalue of these pairwise similarity scores as an indicator of the similarity for the set of programs.\nA higher mean fingerprint similarity indicateshigher structural and syntactical similarity of the\ncode.\nIn addition to the dispersion metrics, we consider one other approach-visualizing the top modes of\nunconditional distributions for certain attributes, e.g., topics. This helps identify areas of emphasis\nin src and gen distributions, as well as the tendency of change across distributions."}, {"title": "Mitigating Generative Monoculture", "content": "To attempt to mitigate generative monoculture, we test four methods known to increase LLM output\ndiversity: increasing the temperature T, top-p parameter, setting a temperature decay, and changing\nprompts. More details are in Appendices A and C.6.\nTemperature T. This determines the dispersion of the probability distribution over the next token:\nincreasing the temperature leads to a more flat probability distribution and increases the likelihood\nof sampling from less probable tokens, resulting in more diverse generations.\nTop-p. This controls the randomness of the generations by limiting the range of tokens considered.\nSpecifically, it considers the smallest subset (consisting of the top probability tokens) whose cumu-\nlative probability exceeds the threshold p. A smaller p encourages the model to sample from a more\nfocused set of likely tokens.\nDecaying Temperature. We choose the starting temperature T = 10.0 and follow a linear schedule\nfor temperature decay, over the course of 50 time-steps (i.e., from the 1-st output token to the 50-th\noutput token), with an ending temperature T = 1.2. The method is inspired by Carlini et al. [28].\nPrompts. Tuning the specific content and framing of the prompt can steer the model's output\nmore effectively [29, 30] and significantly impact the diversity of the generated text. We use \u201crole-\nplaying\u201d or impersonation [31], which instructs the model to produce the output in the persona of a\nspecific person, and expect it to induce more personalized and varied responses."}, {"title": "Experimental Setup", "content": "In this section, we describe our experimental setup for measuring and mitigating generative mono-\nculture for two tasks, namely, generating book reviews and code solutions. We provide details for\ndatasets, LLMs used, and most notably, the data attributes and metrics considered. We open source\nour code at https://github.com/GeMoLLM/GeMO."}, {"title": "Generating Book Reviews", "content": "Data Curation: For Dsrc, we use the Goodreads dataset [12], which contains multiple books\nwith several reviews each. We perform filtering and sampling to ensure reliable attribute extraction\n(see Appendix B.1), and craft a final dataset of N = 742 books with English titles, and Vi, n\u2081 = 10\nreviews per book such that the review length is between 300 and 700 words.\nTo obtain Dgen, we used the following LLMs:\n(a) Llama-2-13b [32] (henceforth referred\nto as Llama-2), (b) Llama-2-13b-chat [32]\n(henceforth referred to as Llama-2-chat),\n(c) Vicuna-13b-v1.5 [33] (hence-\nforth referred to as Vicuna-13b), (d)\nGPT-3.5-turbo-instruct (0914) [17]\n(henceforth referred to as GPT-3.5),\nand (e) GPT-4-turbo (0125) [34, 13]\n(henceforth referred to as GPT-4). We\nperformed nucleus sampling [35] with\nvarious sampling parameters: (a) temper-\nature T\n\u2208 {0.5, 0.8, 1.0, 1.2, 1.5}, and\n(b) top-p\u2208 {0.90,0.95, 0.98, 1.00}. We\nalso experimented with two candidates for\nPtask: prompt (1) \u201cWrite a personalized\nreview of the book titled {title}:", "Write a book review for\nthe book titled {title} as if you\nare {person}:\". Prompt (2) was chosen\nas LLMs are known to generate more diverse responses when instantiated with a persona [31].\nWe list the names of the 10 persons we considered in Appendix B.2. For each combination of\nLLM, sampling parameter, and prompt, we independently sampled from the LLM 10 times to\ngenerate responses. We filtered out low-quality (generated) reviews by examining their perplexity\n(see Appendix C.1). This is to ensure that the data used for analysis represents well-formed and\ncoherent text, thereby improving the reliability of our findings. Thus, Vi, mi \u2264 10.\nAttribute Extraction: We want attributes that capture both the semantics and syntax of book re-\nviews. These attributes are representative of the key thematic and linguistic elements in book re-\nviews. Most importantly, while these attributes are not exhaustive, their extraction is reliable and\nefficient.\n1. Sentiment indicates whether a review is positive (praising the book) or negative (criticizing the\nbook). We employ a fine-tuned sentiment classifier [36] as the attribute extractor which accepts\na text and returns a prediction in {0,1}. This model has been downloaded 5.4 million times in\nMay 2024, and reaches an accuracy of 91.3% on the dev set of SST-2 [37].\n2. Topic refers to the themes discussed in a review [38, 39]. We leverage BERTopic [40] pre-trained\non Wikipedia [41] which assigns one topic to each review out of a total of ~2,000 topics.\n3. Word choice captures the lexical diversity in a review. To quantify this, we produce a frequency\ntable of the unique words (see Appendix B.3), and immediately have the number of unique words.\nMetric Calculation: For sentiment, we calculate mean and entropy for the conditional distribution.\nFor topic, we calculate entropy for the conditional distribution as well as visualize the unconditional\ndistribution of topics across all reviews, focusing on the top 10 classes. Finally, for word choice, we\ncalculate count and entropy of the unconditional distribution.\"\n    },\n    {\n      \"title\": \"Generating Code Solutions\",\n      \"content\": \"Data Curation: For Dsrc, we chose the CodeContests dataset [14], a competitive programming\nproblem dataset where each problem comes with multiple correct and incorrect solutions. We limited\nthe scope to a subset (N = 100) of level-A problems (easy problems) on Codeforces [42], and the\nlanguage of the solutions to python3. More details in Appendix D.1. For each problem in the\nsubset, we randomly sampled Vi, ni = 20 correct solutions from all of the ncorrect solutions for that\nproblem.\nTo obtain Dgen, we use the following LLMs: (a) GPT-4, and (b) Claude-3-Sonnet [43]. We did\nnot use open-source LLMs, as these were not able to generate correct solutions for the problems\nwe chose. More details are in Appendix E.4. We performed nucleus sampling [35] with various\nsampling parameters: (a) temperature T \u2208 {0.5, 1.0}, and (b) top-p \u2208 {0.9, 1.0}. We used only one\ncandidate for Ptask i.e., \u201cPlease read the below problem description and generate a\npython code to solve the problem {problem description} Please only generate\ncode and nothing else.\" While we experimented with providing the LLM with a persona i.e.,\nasking the LLM to pretend to be a \\\"grandmaster in solving competitive programming problems\\\",\nthe resulting accuracy was lower (see Appendix E.2). For each combination of LLM, sampling\nparameter, and prompt, we produce Vi, mi \u2265 20 generations such that at least 20 of the generated\nsolutions were correct (details in Appendix D.2). We instantiated this by generating k samples\n(k = 100 for GPT-4 and k = 200 for Claude-3), and verifying that at least 20 of them were\ncorrect.\nAttribute Extraction: We consider the following attributes which characterize different aspects of\ncode. We rely on GPT-3.5 for extracting some of the attributes; we manually verified the extracted\nattributes and confirmed their quality is high (see Appendix E.3).\n1. Correctness refers to whether a piece of code correctly solves the given problem and passes all\nthe test cases. We measure accuracy as the ratio of correct solutions among all solutions (details\nin Appendix D.3), to quantify the quality of human-/model-generated solutions.\n2. Efficiency is crucial for scalability [44]. This is measured through: asymptotic time/space com-\nplexity and runtime efficiency. We prompt GPT-3.5 to infer the big O time and space com-\nplexity [45], and execute the code on test cases to measure runtime and memory usage (see Ap-\npendix D.4).\n3. Fingerprint provides insights into the structural and syntactical uniqueness of each code segment.\nAs stated in Section 4.3, we use the COPYDETECT tool for this.\n4. Code Summary (textual) explains the functionality of the code. Prior work has demonstrated\nthe effectiveness of GPT-3.5 in code understanding [46]. Thus, we use it to produce text-based\nsummaries, and a description, functionality, algorithm, and data structure (prompt\nfor this task is in Appendix D.5). To compare the similarity for these text summaries, we produce\ntheir embeddings using the all-MiniLM-L6-v2 model [47].\n5. Code Summary (categorical) reflects the techniques employed in the code through categorical\ntags, as used on the Codeforces website. We prompt GPT-3.5 to assign tags to a code segment\nby providing it a set of tags to choose from (prompt for this task is in Appendix D.5). We obtain\none set per code segment. We similarly prompt GPT-3.5 to choose from a list of algorithms\nand data structures.\"\n    },\n    {\n      \"title\": \"Results and Takeaways\",\n      \"content\": \"Guide: We present our results on measuring, and attempting to mitigate, generative monoculture\nin Fig. 4 and 5. We display results mainly in three formats: (a) stacked bar charts, where different\nhues correspond to different value ranges as indicated in the legend; (b) histograms (or grouped bar\ncharts), to reflect the probability mass of a categorical variable; and (c) kernel density estimation\n(KDE) plots, to reflect the estimated probability density of a continuous variable. We note that, in\nthe code results, for all plots except that evaluating accuracy, we restrict to correct solutions.\nTakeaway 1: Monoculture Exists and is Severe, Within and Across LLMs. As shown in Fig. 4\nand 5, there exists significant narrowing from the source to generation distribution in all attributes\nconsidered for both scenarios, book reviews and coding.\nNotably, for book review, proprietary OpenAI LLMs (GPT-3.5 and GPT-4) demonstrate even more\nsevere monoculture compared with the open-source Llama family LLMs (see Fig. 9 in Appendix C.5\nfor more details). Particularly, for both GPT-3.5 and GPT-4, 100% of the samples have average\npositivity falling in (0.95,1.00] under prompt (1), and 98.9% and 97.6% under prompt (2). For\ncoding, similar reductions in diversity can be seen: in Fig. 5(e), we see increased similarity in natural\nlanguage descriptions of LLM-generated code solutions, and Fig. 5(f) shows the Jaccard similarity\nof the generated solutions in terms of the inferred algorithms, with the majority of problems\ndisplaying high similarity across generated solutions. Of particular interest, the plagiarism scores\nof the LLM-generated code are extremely high (Fig. 5(b)), compared to the source solutions which\nachieve an utterly zero plagiarism score for all the problems. We examine a few pairs of examples\nand their plagiarism scores in Appendix E.1.\nTakeaway 2: LLMs tend to produce Human-favorable Generations. Our results show that\nLLMs tend to over-represent parts of the attribute distribution that are preferred by humans: humans\nlargely prefer text with positive sentiment, as well as correct and efficient code. Fig. 4(a) and Ap-\npendix C.5 show that LLMs produce overwhelmingly positive generations. Fig. 5, as well as Fig. 21\nand 22 in the Appendix reveal that LLM-generated code segments (a) are over 2\u00d7 more accurate\nthan the average human solutions, (b) enjoy an overall lower asymptotic time and space complexity,\nand (c) use less runtime and memory during execution. This may just be the intended consequence\nof RLHF, which explicitly optimizes the LLM towards producing human-favored responses in its\nobjective, as guided by a reward model trained on human preferences.\nHowever, as our results show, this implies a loss of diversity guided by human preferences, which,\nif only naively understood and enforced, could lead to unwanted consequences if going unnoticed.\nOne example of the unintended artifacts is the under- and over-represented topics (Fig. 4(e)); the\ntopic group 15 which contains keywords \u201crob": "nd \u201ckill\u201d etc. is significantly under-represented,\nlikely a consequence of RLHF alignment tuning.\nTakeaway 3: RLHF Hurts Diversity the Most. Llama-2-chat is obtained via performing RLHF\ntuning [17] on the pre-trained (PT) Llama-2. Similarly, Vicuna-13b is obtained via supervised\nfine-tuning (SFT) on the PT Llama-2 [33]. Comparisons on these LLMs (see Fig. 4(c), as well\nas Fig. 8 in Appendix C.4) show that the PT LLM-generated reviews are much more similar to\nthe source. The PT LLM Llama-2 has 5.9% of samples with average sentiment values falling\nin the range of (0.95,1.00], which is much closer to the source percentage of 2.8% than 44.7%\nfor Vicuna-13b and 82.1% for Llama-2-chat. Vicuna-13b also shows better diversity than\nLlama-2-chat-this is consistent with findings suggesting RLHF reduces output diversity com-\npared with SFT (albeit with different metrics) [48].\nTakeaway 4: Naive Mitigations are Insufficient. Changing the sampling parameter (increasing T\nand p) and using a more diversity-inducing prompt (e.g., prompt (2) for book reviews) can reduce\nthe gap (see Fig. 4(a-b) and Fig. 5). For example, using prompt (2) reduces the percentage of the\nmost positive range from 82.1% to 58.1% in Fig. 4(b) for T = 1.2, p = 1.0. However, the gap is still\nlarge. More results in Appendix (Figures 12 and 13, and Appendix E) show similar conclusions."}, {"title": "Conclusion and Limitations", "content": "In this work, we introduce the concept of generative monoculture, a phenomenon where LLMs nar-\nrow the diversity of their output relative to their source data for a given task. We experimentally\ndemonstrate its prevalence across text and code generation tasks, and show the difficulty in miti-\ngating the behavior. Our work has limitations: first, we did not analyze the full training set of the\nLLMs we study due to time and compute restrictions, as the corpora are large and often proprietary.\nFurther, as we note in \u00a7 4, measuring monoculture is difficult as selecting attributes is subjective,\nand the attribute extraction process is sensitive to the reliability of extraction techniques. (We ver-\nify our own attribute extraction techniques in the appendix). Further, while generative monoculture\nitself can have unfair consequences by enforcing the suppression of minority opinions, mitigating\nmonoculture without extreme care could lead to the proliferation of harmful ideas or even toxicity by\nallowing for representation of the entire distribution of source text. We look forward to future work\nmitigating monoculture while maintaining low levels of toxicity and other dangerous behavior."}, {"title": "Sampling Parameters", "content": "1. Temperature: Concretely, the temperature parameter T determines the \u201cdispersion\" of the prob-\nability distribution over the next token. Mathematically, the probability of a token w being generated\n\\(P(w|x) = \\frac{exp(s(w|x)/T)}{\\Sigma_{w'} exp(s(w'|x)/T)}\\) where s(w|x) is the unnormalized\nis given by the softmax function:\nlog-probability of the token w given the context x. Increasing the temperature leads to a more flat\nprobability distribution and increases the likelihood of sampling from less probable tokens, result-\ning in more diverse generations. This can also be understood from the perspective of entropy of the\nnext token, \\(H(W) = - \\Sigma_{w} P(w|x) log P(w|x)\\), where it can be seen that the increase of entropy\ndirectly follows.\n2. Top-p: The top-p parameter [35] (p \u2208 (0,1]) controls the randomness of the generations by\nlimiting the range of tokens considered. Specifically, it considers the smallest subset (consisting\nof the top probability tokens) whose cumulative probability exceeds the threshold p. A smaller p\nencourages the model to sample from a more focused set of likely tokens, while a larger p allows\\sampling from a broader range and thus increases randomness (p = 1 basically means no restriction\non the vocabulary).\nDifferent platforms have different default values for T and top-p. GPT-3.5-turbo-instruct\n(0914) web version adopts T = 0.8 [49]. The default values in OpenAI APIs are T = 1.0 and\np = 1.0 [50].\n3. Generation Length: This parameter (max_new_tokens) dictates at most how many tokens the\nmodel should generate before it stops. We used 500 for book review generation and 2048 for code\ngeneration."}, {"title": "Additional Details: Book Reviews", "content": "To ensure we picked popular books that the LLMs know about, we filtered the books according\nto the number of reviews they have. Constraining the number of reviews to be between 1,000 and\n2,500, we obtained 750 books. We further filtered out books with non-English titles; we conducted\nthis filtering because some downstream LLMs (e.g., sentiment classifiers) are not multilingual, and\nas a result can not analyze the generated non-English reviews. To ensure high quality of the reviews,\nwe further filtered the review by length, constraining the length of each review to be between 300\nand 700 words. After this step of filtering, we sampled 10 reviews per book. Eventually, we obtained\na dataset of 742 books, where each book comes with 10 reviews."}, {"title": "Names of the Celebrities Used in Prompt 2", "content": "We prompted GPT-4 to provide a list of celebrities suitable for writing diverse book reviews. The\nlist of names are as follows: Trevor Noah, Janelle Mon\u00e1e, Yuval Noah Harari, Serena Williams,\nReshma Saujani, Neil deGrasse Tyson, Margaret Atwood, David Attenborough, Malala Yousafzai,\nJordan Peele."}, {"title": "Text Processing for Analyzing Wording Choice", "content": "We first concatenated all documents and converted them to lowercase to standardize the text.\nWe then expanded contractions (e.g., \"don't\" to \"do not\"). We then tokenize the text with\nword_tokenize() from the NLTK library [51], and removed the punctuation. We continued by\nfiltering out non-alphabetic tokens to focus solely on words. Lastly, we lemmatized tokens via\nWordNetLemmatizer() to their base forms, aiding in consistent frequency analysis. These steps\nare essential for minimizing textual noise and ensuring the reliability of our word frequency assess-\nments."}, {"title": "Additional Results: Book Reviews", "content": "We established a perplexity threshold of 20 to filter out low-quality reviews. To validate our choice,\nwe randomly sampled reviews with perplexity scores at different intervals and manually inspected\nthem. For clarity, instead of presenting the entire review text, we selectively extracted chunks that\nexemplified the low-quality nature of the sampled reviews."}, {"title": "Generation Results at Higher Randomness", "content": "We experimented with even higher randomness at T = 1.5 motivated by the observation that in-\ncreasing the randomness does help to increase the diversity. However, as we show in Fig. 6, even at\nhigh randomness, there is still a huge gap between the source and the generations.\nMoreover, we notice a significant degradation of the generation quality as a result of the increased\nrandomness. We present in Table 1 the average number of valid generations for two models under\nvarious sampling parameters. The table shows that the valid number of generations rapidly drops as\nthe randomness increases, particularly at T = 1.5; the implication is that such a high randomness\nsetting basically cannot be adopted for practical use."}, {"title": "Topic Shifts", "content": "We present in Fig. 7 the results of the unconditional topic distribution. Across all three settings\nfor comparison, we observe highly similar distribution shift signified by the over- and under-\nrepresentation of certain topic groups. Concretely, the topic group 15 with the keywords \"grace,\nrob, nick, anna, house, discovers, realizes, killed, confronts, killer\" are under-represented, poten-\ntially because certain words like \u201crob\u201d and \u201ckill\" are eliminated from the output as a result of RLHF.\nOn the other hand, the topic group 333 with the keywords \u201chandmaid, novels, novel, writers, tale,\nbook, books, literary, novellas, synopsis\u201d and the topic group 666 with the keywords \u201cnonfiction,\nbestseller, novelist, autobiography, novels, memoir, author, memoirs, paperback, novel\u201d are over-\nrepresented. The over-representation of topic group 333 in Vicuna-13b is much more severe than\nLlama-2-chat, as can be seen from the 3rd subfigure; this could be because a higher exposure to\nrelevant materials during its fine-tuning."}, {"title": "Pre-trained Model", "content": "The aligned model is obtained from performing supervised fine-tuning followed by RLHF alignment\ntuning on the basis of the pre-trained model [32]. We compare the performance of the aligned model\nlama-2-13b-chat with the pre-trained model llama-2-13b and present the results in Fig. 8.\nFrom the 1st and 2nd subfigure, we observe that the pre-trained model is much more diverse than\nthe aligned model; the pre-trained model is even very close to the source data (see the dark purple\nresult tagged as T = 1.2, p = 1.0 (c)).\nThe 3rd subfigure delivers two messages. First of all, there still exists divergence from the pre-\ntrained model and the source in terms of the covered topics. We attribute this divergence we observe\nbetween the pre-trained model and the source data to the difference between the source data we\nuse (the Goodreads dataset) and the ground-truth training data (a much broader corpus which we\nhave no information about). We regard the Goodreads dataset as a proxy of the ground-truth, but\nintrinsically Goodreads is smaller and does not fully accurately represent the groundtruth training\ndistribution, especially in nuanced attributes like the unconditional topic distribution measured on\nall samples. Second, the direction and trend of changes in the aligned model is not completely the\nsame as the aligned models, e.g., the topic group 15 containing \u201crob\u201d and \u201ckiller\u201d is not inhibited as\nmuch as in the aligned models, potentially due to RLHF."}, {"title": "Results of OpenAI models", "content": "We repeat the same set of experiments on the proprietary OpenAI models GPT-3.5-turbo-instruct\n(0914) and GPT-4-turbo (0125). We present the results in Fig. 9.\nThe sentiment results (in column 1) suggest that both models are overwhelmingly and invariably\npositive in their reviews, across various sampling parameters and prompts.\nThe results of the conditional topic distribution (in column 2) reveal similar conclusions\u2014there is a\nsignificant deviation between the generated reviews and the source reviews across varying sampling\nparameters (see upper low) and for both GPT-3.5-instruct and GPT-4 (see lower row). Nevertheless,\nusing a diversity-inducing prompt, i.e., prompt (2) does lead to increased diversity (see lower row).\nThe results of the unconditional topic distribution (in column 3) also demonstrate the distribution\nshift, though in a slightly different way than observed for llama family models (see presented in Ap-\npendix C.3). Concretely, the topic group 333 does not experience a significantly over-representation,\nwhile the topic group 30 does. As discussed previously, we attribute the difference in the nuanced\ntopic distribution mainly to the difference in the training data."}, {"title": "Mitigation via temperature decay", "content": "We describe the decaying temperature scheme. Concretely, we choose the starting temperature\nT = 10.0 and follow a linear schedule for temperature decay, over the course of 50 timesteps (i.e.,\nfrom the 1-st output token to the 50-th output token), with an ending temperature T = 1.2. The\nmethod is inspired by Carlini et al. [28] which reported this scheme as one sampling method that\ninduce diversity and high quality output\u00b9.\nWe present the results in Fig. 10. For both the sentiment and the topic, we see that the fixed temper-\nature scheme achieves a higher diversity compared to the decaying temperature scheme."}, {"title": "Additional Details: Coding", "content": "We limit our scope to Codeforces level-A problems only for the coding scenario, which are the\neasiest problems on the Codeforces competitive programming platform\u00b2. We note that even though\nthese are the easiest problems on the platform, they still require non-trivial intellectual efforts to\nsolve\u00b3.\nThere are two main reasons we restrict to level-A problems:\n1. Most LLMs perform best on level-A problems and much worse on more difficult ones. We started\noff evaluating a set of 81 problems ranging from difficulty A to G, and obtained an average\naccuracy of 50% on the 16 level-A problems, and less than 10% on others. Since we perform"}, {"title": "Correctness Testing: Autojudge with Testcases", "content": "We simulated an Autojudge using the test cases provided in the dataset. Concretely, for each prob-\nlem, we obtain 10 test cases in the format of input and output. We then measure each solution\nagainst the set of 10 test cases. We regard the solution that passes all 10 test cases as a correct\nsolution."}, {"title": "Measuring Accuracy", "content": "We calculate accuracy as ncorrect/nall per problem (where ncorrect is the number of correct solutions\nand nall is the number of all solutions). For the source, both ncorrect and nall are available in the\nCodeContests dataset [14]. For the generations, we test the correctness of the solutions via autojudge\n(see Appendix D.2) and measure the accuracy as norrect/k."}, {"title": "Measuring Runtime Efficiency", "content": "For runtime efficiency, we use the bash command /usr/bin/time [52] to measure the elapsed real\ntime (via %E) as well as the maximum resident set size of the process during its execution (via\n%M). Concretely, for each solution, we run it on all 10 test cases with our autojudge and measure\nthe runtime and memory. We take the max value on all 10 test cases as a proxy of the runtime\nefficiency of the tested code."}, {"title": "Prompting GPT-3.5 to Generate Code Summary (both Text Descriptions and Categorical Values)", "content": "We present below the instruction we provide to GPT-3.5. We bold keywords in the prompt simply\nfor the ease of reading. When calling the GPT-3.5 API, we used temperature T 0 (i.e., greedy\ndecoding) and max_tokens=500.\nThe list of tags (in the 1st prompt below) were collected from the CodeContests [14] dataset\u2014we\ntraverse all the problems in the dataset and union all the \"tags\" attribute.\nThe lists of algorithms and data structures (in the 2nd prompt below) were obtained from analyzing\nWikipedia and querying GPT-4-turbo (0125) for a suggested list.\nThese categorical attributes serve as a complement to the text description described above and en-\nhance the reliability of the results.\nPlease provide a description to the following code in natural language. Explain the functionality, algo-\nrithm, data structure, time complexity, space complexity of the code.\nFinally, assign a few tags to the code. Here is a list of tags you can choose from:\n\"binary search, math, special, trees, dp, greedy, games, dfs and similar, expression parsing, number theory,\nchinese remainder theorem, geometry, bitmasks, sortings, graph matchings, matrices, meet-in-the-middle,\ngraphs, combinatorics, probabilities, constructive algorithms, schedules, two pointers, brute force, dsu,\nshortest paths, hashing, interactive, data structures, strings, ternary search, fft, flows, implementation\"\nAnswer each in a line in the example format of: 'Description: description\nFunctionality: functionality'\n{code}\nPlease read the following code and infer the algorithms and data structures used in it.\nFor algorithms, select (a few) from the following list:\n\"Sorting Algorithms, Searching Algorithms, String Algorithms, Divide and Conquer Algorithms, Greedy\nAlgorithms, Dynamic Programming, Recursion, Bit Manipulation, Backtracking, Graph Algorithms, Oth-\ners\"\nFor data structures, select (a few) from the following list:\n\"Arrays, Linked Lists, Stacks, Queues, Trees, Heaps, Hash Tables, Sets, Maps, Priority Queues, Others\"\nAnswer each in a line following the format of: 'Algorithms: candidate 1, candidiate 2, ..\nData structures:\ncandidate 1, candidiate 2, ..\n'\n{code}"}, {"title": "Additional Results: Coding", "content": "We present example model-generated code and human-written code as well as the plagiarism scores\nassociated with the pairs in Fig. 14, 15,and 16. All the code are correct solutions to the problem\n409A. The Great Games.\nFrom the demonstrations it is evident that model-generated code (Fig. 14, 15) are highly similar in\ntheir style and structure; even the low score pair (Fig. 15) bear a significant level of similarity. In\ncomparison, human-written code (Fig. 16) are clearly distinctive. These results support the validity\nof using the plagiarism score to evaluate code similarity."}, {"title": "Attempts on Varying Prompts", "content": "We experimented with four prompts on GPT-4, one plain prompt, one instructing the model to gen-\nerate only the code solution, one employing role-playing [31] and one integrating chain-of-thought\nprompting [53]. We present the concrete prompts in Fig. 17.\nWe evaluate the four prompts on Codeforces problems of varying difficulties and present the accu-\nracy achieved by each prompt in Fig. 18. Overall, prompt 2 achieves the best results among the\nfour."}, {"title": "Human annotations for the quality of LLM summary", "content": "We annotate the quality of the LLM-generated summary of code given by GPT-3.5.\nStep 1: We randomly sample 20 solutions of different problems, and review the LLM-generated\nattributes for these solutions; the attributes include the textual ones (description, functionality, algo-\nrithm, data structure), the inferred asymptotic complexity (time complexity, space complexity), as\nwell as the categorical ones (tags, algorithm, data structures).\nStep 2: We read through the problem description, the code solution, and the model-generated sum-\nmary to evaluate the quality of model-generated summary.\nStep 3: We follow the instruction \"In each cell please give a score from 1-3 \u2013 3 meaning absolutely\ncorrect, 2 meaning with some minor errors but acceptable, 1 meaning entirely incorrect\" to score\neach attribute.\nStep 4: We calculate the average score received by each attribute across the 20 sample and present\nthe results in Table 3.\nThe high scores indicate that GPT-3.5 can provide fairly accurate summary of the given code,\nsupporting our choice of relying GPT-3.5 to reliably extract the attributes."}]}