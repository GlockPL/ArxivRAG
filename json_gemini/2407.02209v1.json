{"title": "Generative Monoculture in Large Language Models", "authors": ["Fan Wu", "Emily Black", "Varun Chandrasekaran"], "abstract": "We introduce generative monoculture, a behavior observed in large language mod- els (LLMs) characterized by a significant narrowing of model output diversity relative to available training data for a given task: for example, generating only positive book reviews for books with a mixed reception. While in some cases, generative monoculture enhances performance (e.g., LLMs more often produce efficient code), the dangers are exacerbated in others (e.g., LLMs refuse to share diverse opinions). As LLMs are increasingly used in high-impact settings such as education and web search, careful maintenance of LLM output diversity is essen- tial to ensure a variety of facts and perspectives are preserved over time. We exper- imentally demonstrate the prevalence of generative monoculture through analysis of book review and code generation tasks, and find that simple countermeasures such as altering sampling or prompting strategies are insufficient to mitigate the behavior. Moreover, our results suggest that the root causes of generative mono- culture are likely embedded within the LLM's alignment processes, suggesting a need for developing fine-tuning paradigms that preserve or promote diversity.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) show promise due to their emergent abilities [1] and state-of- the-art performance on several NLP tasks [2]. However, concerns have been raised about the increasing reliance on LLM-based systems with insufficient testing, and how they impact society [3, 4]. Recent evidence has shown that LLMs have dangerous tendencies: they con- vincingly return incorrect information [5, 6, 7], produce toxic language [8, 9], and can effec- tively propagate misinformation [10, 11].\nIn this paper, we focus on a different concern: that for a given prompt and task, LLMs do not faithfully represent the diversity of poten- tial responses available in their training data. We call this behavior generative monoculture: given some task (e.g., generating book reviews) and a data attribute (e.g. sentiment), generative monoculture refers to the narrowing of the probability distribution of the considered attribute from source data (i.e., available human-generated book reviews as part of the training data) to the gener- ated data (i.e., LLM-generated book reviews).\nAs a preview, for book reviews (Fig. 1(Left)), we compare the diversity in sentiment of Goodreads reviews [12] (i.e., src)\u2014very likely a portion of LLM training data [13]\u2014with LLM-generated reviews (i.e., gen). The range of mean sentiment scores per book across gen book reviews is much narrower than that in the src: in the experiment pictured, the average sentiment score for gen"}, {"title": "2 Defining Generative Monoculture", "content": "We broadly characterize generative monoculture as a distribution shift from source data (i.e., human- generated training data) to model generated data (i.e., model outputs) for a specific task, such as gen- erating reviews for books or solutions to coding problems. This can be formalized using measures of statistical dispersion applied to various task-specific attributes.\nDefinition 1 (Generative Monoculture). For a given task, let \\(P_{src}\\) denote the probability distri- bution of the source data, \\(P_{gen}\\) denote the probability distribution of the LLM-generated data, let \\(h\\) denote a function extracting attributes from data (such as sentiment, or algorithms used in code), and \\(Dispersion(\u00b7)\\) denote a dispersion metric (e.g., entropy). Then we define generative monocul- ture as the condition where \\(P_{gen}\\) is statistically narrower than \\(P_{src}\\), namely: \\(Dispersion(h(x)|x \\sim P_{gen}) < Dispersion(h(x)|x \\sim P_{src})\\).\nNote, \\(P_{src}/P_{gen}\\) can be the distribution of human-generated/model-generated responses, for a given task, conditioned on one specific given prompt (which we refer to as the conditional distribution), or the distribution of human-generated/model-generated responses for any possible prompt in a con- sidered domain (which we call the unconditional distribution).\nThis phenomenon signifies a shift towards less varied outputs. We emphasize that the investigation of generative monoculture is intrinsically task-dependent, as the attributes of interest differ across tasks. In addition, as we often do not have access to the source distribution in practice, we approx- imate it using a source dataset (\\(D_{src}\\)), comprised of a subset of the training data of the LLMs we consider. Similarly, we approximate the generated distribution through a dataset generated by the model (\\(D_{gen}\\))."}, {"title": "3 Related Work", "content": "Diversity and LLMs. Santurkar et al. [18] demonstrate that LLMs do not give representative opin- ions to polling questions when compared to the general U.S. population. Our work focuses on the narrowing of diversity in LLM output from its human-generated training data\u2014while Santukar et al. demonstrate a narrowing in diversity from actual human survey respondents (and not training data). Additionally, our work proposes a general framework for measuring monoculture. Padmakumar and He [19] demonstrate that using LLM assistance can lead to reduced diversity in human-generated ar- gumentative essays when compared to essays written without LLM assistance. While they mention that this is partially because the models themselves do not produce diverse output, they do not focus on the narrowing of diversity from LLM training data to LLM-generated data. Finally, Sorensen et al. [20] outline an alignment framework to emphasize pluralism, to work towards creating models which express a variety of opinions and perspectives. While this is certainly related to our work, generative monoculture as a phenomenon extends beyond differences in opinion, and expresses the narrowing of any number of task-specific attributes, from code correctness to topics covered to many others. One common thread across many of these works, which our work adds to, is that current alignment practices\u2014namely RLHF\u2014 harms output diversity.\nOther Notions of Monoculture. Our notion of generative monoculture relates to, but differs from, other notions of monoculture in the AI literature. For example, algorithmic monoculture [21] and outcome homogeneity [22] describe the societal state where many decision-making actors rely on the same underlying algorithms to generate (classification or ranking) predictions, from the perspec- tive of decision-making actors and individuals subject to those decisions respectively. These works show that algorithmic monoculture is sub-optimal for both decision-making actors (due to corre- lated failures across models) and for those subject to model decisions, as repeated outcomes across models leave little room for algorithmic recourse. In contrast, generative monoculture focuses on documenting the phenomenon of individual LLMs narrowing the diversity of their output in relation to their source data-for example, only returning positive book reviews about a controversial book. We do, however, document in this work that generative monoculture exists to similar extents and in similar directions across a variety of available LLMs, (e.g., Llama, Vicuna, ChatGPT-4) leaving open the possibility of concerns brought up by Kleinberg and Raghavan [21], but in a generative context."}, {"title": "4 Measuring Generative Monoculture", "content": "We outline a general approach to measuring generative monoculture in LLMs. In particular, follow- ing Definition 1, we outline steps to construct \\(D_{src}\\) and \\(D_{gen}\\) and compare their diversity through extracting data attributes and calculating dispersion metrics. We illustrate our approach in Fig. 2.\n4.1 Data Curation\nFor a given task, we aim to create a source dataset that is likely to have been used in training the LLM we wish to investigate. Training data for most LLMs is a closely guarded secret. While recent work [23] describes how dataset contamination can be determined, such approaches are (a) riddled with false positives, and (b) computationally expensive. Thus, we often take an educated guess"}, {"title": "4.2 Attribute Extraction", "content": "For a given task, we identify and compile a list of attributes that are of interest from the perspective of preserving diversity. This is a subjective task, but we focus on metrics which target understanding the content of LLM output (e.g., book review sentiment and topic; code time complexity and algorithms used), as opposed to more general metrics of output language quality such as saliency, fluency and coherence. More importantly, we need to ensure that extraction functions are efficient, accurate, and reproducible\u2014we outline our tests to ensure these qualities in \u00a7 6 and the Appendices B and D. For example, care must be taken to use LLMs for attribute extraction, as they are known to be biased towards their own responses [24]. For a given attribute A, extraction function \\(h_A\\) takes a string \\(tgt\\) to obtain the attribute value \\(h_A(tgt)\\). Note that \\(tgt\\) can either be \\(src\\) or \\(gen\\). The extracted attribute can either be a continuous/categorical variable or of other more complicated types, depending on the nature of the attribute."}, {"title": "4.3 Metric Calculation", "content": "As the last step, we compute metrics on the extracted attributes. Given a set of responses, dispersion metrics aim to capture their breadth or coverage of these attributes. We describe those used in this paper below and in Fig. 3.\nDispersion Metrics. We introduce dispersion metrics suited to different data types.\nA. Distribution of the mean. For ordinal or continuous attributes, we calculate the mean over the conditional distribution that is, we calculate the metrics over each \\(src_i\\) or \\(gen_i\\) (e.g. reviews for a given book), and show the distribution of this mean over all \\(q_i\\) for a certain task (e.g. over all books). While the mean itself does not directly measure dispersion, the distribution of the mean values sheds light on dispersion: concentrated mean values indicates a smaller dispersion of the data. The advantage is that it not only describes dispersion, but also the qualitative tendency of the attribute (e.g. bias towards positive/negative sentiment), which cannot be captured otherwise.\nB. Entropy and standard deviation. For categorical attributes, we measure dispersion using entropy over the conditional distribution. For continuous attributes, we use standard deviation over the con- ditional distribution to quantify the dispersion of values around the mean, providing the attribute's variability."}, {"title": "5 Mitigating Generative Monoculture", "content": "To attempt to mitigate generative monoculture, we test four methods known to increase LLM output diversity: increasing the temperature T, top-p parameter, setting a temperature decay, and changing prompts. More details are in Appendices A and C.6.\nTemperature T. This determines the dispersion of the probability distribution over the next token: increasing the temperature leads to a more flat probability distribution and increases the likelihood of sampling from less probable tokens, resulting in more diverse generations.\nTop-p. This controls the randomness of the generations by limiting the range of tokens considered. Specifically, it considers the smallest subset (consisting of the top probability tokens) whose cumu- lative probability exceeds the threshold p. A smaller p encourages the model to sample from a more focused set of likely tokens.\nDecaying Temperature. We choose the starting temperature T = 10.0 and follow a linear schedule for temperature decay, over the course of 50 time-steps (i.e., from the 1-st output token to the 50-th output token), with an ending temperature T = 1.2. The method is inspired by Carlini et al. [28].\nPrompts. Tuning the specific content and framing of the prompt can steer the model's output more effectively [29, 30] and significantly impact the diversity of the generated text. We use \u201crole- playing\u201d or impersonation [31], which instructs the model to produce the output in the persona of a specific person, and expect it to induce more personalized and varied responses."}, {"title": "6 Experimental Setup", "content": "In this section, we describe our experimental setup for measuring and mitigating generative mono- culture for two tasks, namely, generating book reviews and code solutions. We provide details for datasets, LLMs used, and most notably, the data attributes and metrics considered. We open source our code at https://github.com/GeMoLLM/GeMO.\n6.1 Generating Book Reviews\nData Curation: For \\(D_{src}\\), we use the Goodreads dataset [12], which contains multiple books with several reviews each. We perform filtering and sampling to ensure reliable attribute extraction"}, {"title": "6.2 Generating Code Solutions", "content": "Data Curation: For \\(D_{src}\\), we chose the CodeContests dataset [14], a competitive programming problem dataset where each problem comes with multiple correct and incorrect solutions. We limited the scope to a subset (N = 100) of level-A problems (easy problems) on Codeforces [42], and the language of the solutions to python3. More details in Appendix D.1. For each problem in the subset, we randomly sampled \\(V_i\\), \\(n_i\\) = 20 correct solutions from all of the \\(n_{correct}\\) solutions for that problem.\nTo obtain \\(D_{gen}\\), we use the following LLMs: (a) GPT-4, and (b) Claude-3-Sonnet [43]. We did not use open-source LLMs, as these were not able to generate correct solutions for the problems we chose. More details are in Appendix E.4. We performed nucleus sampling [35] with various"}, {"title": "7 Results and Takeaways", "content": "Guide: We present our results on measuring, and attempting to mitigate, generative monoculture in Fig. 4 and 5. We display results mainly in three formats: (a) stacked bar charts, where different hues correspond to different value ranges as indicated in the legend; (b) histograms (or grouped bar charts), to reflect the probability mass of a categorical variable; and (c) kernel density estimation (KDE) plots, to reflect the estimated probability density of a continuous variable. We note that, in the code results, for all plots except that evaluating accuracy, we restrict to correct solutions.\nTakeaway 1: Monoculture Exists and is Severe, Within and Across LLMs. As shown in Fig. 4 and 5, there exists significant narrowing from the source to generation distribution in all attributes considered for both scenarios, book reviews and coding.\nTakeaway 2: LLMs tend to produce Human-favorable Generations. Our results show that LLMs tend to over-represent parts of the attribute distribution that are preferred by humans: humans largely prefer text with positive sentiment, as well as correct and efficient code. Fig. 4(a) and Ap- pendix C.5 show that LLMs produce overwhelmingly positive generations. Fig. 5, as well as Fig. 21 and 22 in the Appendix reveal that LLM-generated code segments (a) are over 2\u00d7 more accurate than the average human solutions, (b) enjoy an overall lower asymptotic time and space complexity, and (c) use less runtime and memory during execution. This may just be the intended consequence of RLHF, which explicitly optimizes the LLM towards producing human-favored responses in its objective, as guided by a reward model trained on human preferences.\nTakeaway 3: RLHF Hurts Diversity the Most. Llama-2-chat is obtained via performing RLHF tuning [17] on the pre-trained (PT) Llama-2. Similarly, Vicuna-13b is obtained via supervised fine-tuning (SFT) on the PT Llama-2 [33]. Comparisons on these LLMs (see Fig. 4(c), as well"}, {"title": "8 Conclusion and Limitations", "content": "In this work, we introduce the concept of generative monoculture, a phenomenon where LLMs nar- row the diversity of their output relative to their source data for a given task. We experimentally demonstrate its prevalence across text and code generation tasks, and show the difficulty in miti- gating the behavior. Our work has limitations: first, we did not analyze the full training set of the LLMs we study due to time and compute restrictions, as the corpora are large and often proprietary. Further, as we note in \u00a7 4, measuring monoculture is difficult as selecting attributes is subjective, and the attribute extraction process is sensitive to the reliability of extraction techniques. (We ver- ify our own attribute extraction techniques in the appendix). Further, while generative monoculture itself can have unfair consequences by enforcing the suppression of minority opinions, mitigating monoculture without extreme care could lead to the proliferation of harmful ideas or even toxicity by allowing for representation of the entire distribution of source text. We look forward to future work mitigating monoculture while maintaining low levels of toxicity and other dangerous behavior."}]}