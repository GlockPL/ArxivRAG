{"title": "Enhancing Instruction-Following Capability of Visual-Language Models by Reducing Image Redundancy", "authors": ["Te Yang", "Jian Jia", "Xiangyu Zhu", "Weisong Zhao", "Bo Wang", "Yanhua Cheng", "Yan Li", "Shengyuan Liu", "Quan Chen", "Peng Jiang", "Kun Gai", "Zhen Lei"], "abstract": "Large Language Models (LLMs) have strong instruction-following capability to interpret and execute tasks as directed by human commands. Multimodal Large Language Models (MLLMs) have inferior instruction-following ability compared to LLMs. However, there is a significant gap in the instruction-following capabilities between the MLLMs and LLMs. In this study, we conduct a pilot experiment, which demonstrates that spatially down-sampling visual tokens significantly enhances the instruction-following capability of MLLMs. This is attributed to the substantial redundancy in visual modality. However, this intuitive method severely impairs the MLLM's multimodal understanding capability. In this paper, we propose Visual-Modality Token Compression (VMTC) and Cross-Modality Attention Inhibition (CMAI) strategies to alleviate this gap between MLLMs and LLMs by inhibiting the influence of irrelevant visual tokens during content generation, increasing the instruction-following ability of the MLLMs while retaining their multimodal understanding capacity. In VMTC module, the primary tokens are retained and the redundant tokens are condensed by token clustering and merging. In CMAI process, we aggregate text-to-image attentions by text-to-text attentions to obtain a text-to-image focus score. Attention inhibition is performed on the text-image token pairs with low scores. Our comprehensive experiments over instruction-following capabilities and VQA-V2, GQA, TextVQA, MME and MMBench five benchmarks, demonstrate that proposed strategy significantly enhances the instruction following capability of MLLMs while preserving the ability to understand and process multimodal inputs.", "sections": [{"title": "Introduction", "content": "Achieving alignment between artificial intelligence systems and human intentions has long been an important objective in Al research . It ensures that the behavior and decision making of AI systems are consistent with human intentions and values, thereby ensuring that the development and application of AI are beneficial and pose no harm to humans. Ensuring that machines can precisely follow human instructions is a preliminary yet crucial step in achieving this alignment.\nLarge language models (LLMs) have been significantly improved owing to innovations in model architecture and large-scale pre-training , resulting in accurate responses to complex human instructions. Despite the significant progress in MLLMs driven by advancements in LLMs, there is a substantial gap between MLLMs and their foundations LLMs regarding precise instruction-following ability. Replacing multimodal inputs with text-only inputs significantly increases the instruction-following capabilities of MLLMs (Figure 1). This phenomenon is common and cannot be avoided even in the best closed-source MLLMs.\nThis gap prompts our investigation into the instruction-following capability of MLLMs. As discussed in (He et al. 2022), one significant difference between the vision modality and the language modality is the extent of information redundancy. Language is a medium of human communication with a low information redundancy and rich semantics. Conversely, images are highly spatially redundant. Notably, masked image modeling can adopt a significantly larger masking ratio, reaching 90%, compared to language (He et al. 2022). Therefore, it is imperative to explore whether reducing the redundant information in images can enhance the instruction-following capabilities of MLLMs.\nConventional MLLMs generally adhere to a design paradigm comprising two stages: visual-modality processing and cross-modality content generation. In the former stage, images are tokenized using a visual encoder with an adapter. In the cross-modality content generation stage, the LLM produces the desired output using visual tokens and text embeddings as inputs. Consequently, we formulate two research questions: 1) How can we reduce the redundancy in visual modality? 2) How can we minimize the impact of redundancy in visual modality on the cross-modality content generation process?\nThe instruction-following capabilities of MLLMs can be significant improved by reducing redundant information in images using an intuitive spatial down-sampling strategy over visual tokens. However, this simple strategy significantly impairs visual understanding capabilities of MLLMs. In this study, we utilize 1) visual-modality token compression (VMTC) and 2) cross-modality attention inhibition (CMAI) to address those two above-mentioned problems, respectively. The VMTC module is designed to compress redundant information of images while retaining the critical foreground information. It leverages attention scores in the ViT layers to identify redundant background tokens, which are clustered based on token similarity and fused accordingly. The CMAI module is presented to mitigate the impact of visual redundancy by ensuring that each text token in the LLM concentrates exclusively on the relevant image tokens. This is achieved by attenuating the attention level between text-image token pairs with low text-to-image focus scores.\nThe comprehensive experimental results demonstrate that our method achieves SOTA performance on instruction-following capabilities while precisely maintaining the multimodal understanding capabilities of MLLMs. The major contributions of this study are summarized as below:\n\u2022 To the best of our knowledge, this is the first study to investigate the instruction-following capability of MLLMs from a model perspective and propose a correlation between the instruction-following capability of MLLMs and the redundancy of visual modality.\n\u2022 We propose visual-modality token compression strategy to compress redundant visual information and cross-modality attention inhibition approach to reduce the impact of visual redundancy on text generation.\n\u2022 By integrating VMTC and CMAI, our method significantly improves the instruction-following capabilities of MLLMs, while precisely maintaining the performance of the baseline model."}, {"title": "Related Work", "content": "Multimodal Large Language Models. Benefiting from recent advancements of LLMs , multimodal large language models have demonstrated remarkable capabilities across various visual-language tasks. BLIP2 and InstructBLIP employ the Q-Former to aggregate visual features and LLaVA uses a linear projection layer to align the token dimensions of the visual encoder with LLM. These methods significantly enhancing the model's ability to interpret and understand images. Many works continue to explore this area from different directions, including adopting higher image resolutions , efficient MLLMS , and extending applications to the video domain.\nToken Pruning. Researchers have proposed various approaches to remove redundant tokens in ViT to improve the model's computational efficiency. Meanwhile, many works have improved the computational efficiency of visual-language models through pruning, but these efforts have primarily focused on traditional VLMs . Recently, LLaVA-Prumerge proposes to reduce visual tokens in last hidden layer of ViT. However, this work also aims to enhance the computational efficiency of MLLMs, while our focus is on improving the instruction-following capability of MLLMs.\nInstruction-Following. A series of works have been proposed to address the instruction-following capability of large language models, including human evaluation , model-based evaluation , and evaluation based on quantitative benchmarks . In particular, IFEval introduces instruction-following evaluation based on verifiable instructions, which can automate the evaluation process and enhances the accuracy and consistency of the evaluation process. In the context of multimodal large models, most works start from a data perspective, constructing instruction datasets to enable models to acquire instruction-following capability. In our study, we investigate the factors influencing the instruction-following capability of MLLMs and enhance the model's instruction-following capability from a model architecture perspective."}, {"title": "Pilot Studies", "content": "We conduct pilot experiments on the instruction following abilities of existing MLLMs, using GPT-4V and LLaVA-1.5 as representative models. Developed and maintained by OpenAI, GPT-4V is one of the most advanced proprietary LMM. Currently, LLaVA-1.5 is one of the most impactful open-source MLLMs available. GPT-4V has a proprietary nature, thus we use case studies to evaluate its instruction-following capabilities. We manually design instructions and use them as input with a related image to assess the ability of GPT-4V model to generate accurate responses. GPT-4V is also used to generate descriptions of images. Subsequently, these descriptions are used as input along with the instructions to evaluate the instruction-following capabilities of GPT-4 model, as a control. The findings demonstrated a significant gap in instruction-following capabilities between GPT-4V and GPT-4 models as shown in Figure 1. More examples are presented in the appendix.\nSuch gap prompts us to explore the differences between visual modality and language modality. Images are unstructured information with a high degree of spatial redundancy. Conversely, text is highly structured information with low redundancy. As illustrated in , when the mask ratio exceeds 80%, the accuracy loss in masked image modeling methods is negligible (0.5%), compared to the optimal ratio. In contrast, masked language modeling methods exhibit a significant degradation in accuracy (over 10%) under similar conditions. This disparity indicates the differences in information redundancy between language and images. We hypothesize that reducing redundancy in images can improve the instruction-following ability of multimodal large language models.\nWe conduct experiments using LLaVA-1.5 7B and 13B models. In the pilot experiments, we conduct spatial down-sampling of the tokens obtained from the image encoded by ViT to reduce image redundancy. We evaluate the model's instruction-following capability based on its ability to perform two simple tasks: responding in JSON format and including specific keywords in the answers. The results of the model's instruction-following capability and multimodal understanding capability under different spatial down-sampling ratios are shown in Figure 2. The results indicate that an increase in the spatial down-sampling ratio reduces the redundancy in the images, leading to a significant enhancement of the model's instruction-following capabilities (Figure 2a and Figure 2b). However, the results also demonstrate that the straightforward strategy of spatial down-sampling significantly impairs the model's multimodal understanding capabilities (Figure 2c and Figure 2d). This finding prompts us to explore more optimal solutions that enhance the instruction-following capabilities of MLLMs without substantially compromising its inherent multimodal understanding capabilities."}, {"title": "Method", "content": "The enhancement of instruction-following capability in MLLMs through the reduction of redundant information via spatial down-sampling in images has been previously discussed. However, this approach significantly compromises the model's multimodal understanding ability. Our objective is to improve the instruction-following capability of MLLMs while preserving their multimodal understanding abilities. Figure 3 illustrates an overview of our proposed model architecture. This section begins with an introduction to MLLMs. The Visual-Modality Token Compression (VMTC) method is then elucidated. This technique preserves crucial foreground information while compressing less significant background details. Finally, the Cross-Modality Attention Inhibition (CMAI) is presented. CMAI is designed to mitigate the impact of redundant image information in text generation.\nPreliminary\nMLLMs typically process both image and text inputs, generating textual responses as output. The current predominant architecture of MLLMs generally comprises three key components: a visual encoder, a projection layer, and a LLM. The visual encoder E, typically employing the Vision Transformer (ViT) architecture, consists of multiple stacked transformer blocks. These blocks transform input image patches I to visual tokens T. Each transformer block is composed of Multi-head Self-attention (MSA) and Feed-forward Layer components.\n\\(z_l = MSA(z_{l-1}) + z_{l-1},\\) (1)\n\\(z'_l = MLP(z_l) + z_l,\\) (2)\nwhere \\(z_{l-1}\\) and \\(z'_l\\) are hidden states in l \u2013 1th and lth layer. The projection layer Proj is designed to align the dimensions of visual tokens with the input dimensions of the LLM.\n\\(T' = Proj(T).\\) (3)\nAs a result, the dimension of \\(T' \\in R^{n \\times d}\\) is aligned with the LLM. The input of LLM consists aligned image tokens T' and text \\(X_q\\), and the output are the textual tokens \\(X_a\\). The LLM also adopts the transformer architecture but are typically with causal attention masks to ensure no information leakage occurs during text generation.\n\\(\text{SelfAttn}(Q, K, V) = (softmax(\\frac{QK^T}{\\sqrt{d_k}}) + M)V,\\) (4)\nwhere Q, K, V are different projections of the hidden state \\(z_i\\), and M is the causal attention mask which has negative infinity above the diagonal and zeros in all other positions. \\(\\frac{QK^T}{\\sqrt{d_k}}\\) is referred to as the attention score, while the result of this expression \\(softmax(\\frac{QK^T}{\\sqrt{d_k}})\\) is the attention weights.\nVisual-Modality Token Compression\nIn order to compress spatially redundant information in images and enhance the model's instruction-following capability, we implement a strategy that preserves essential foreground tokens while clusters and merges the remaining ones. This approach is based on researches indicating that the complete elimination of remaining tokens can have an adverse effect on model performance. Specifically, given the average attention weight of different attention heads \\(A_w \\in R^{(n+1) \\times (n+1)}\\), we define the attention weights between each patch token \\(\\{z'_1,..., z'_n\\}\\) and class token \\(\\{z'_0\\}\\) as importance score IPS \\(\\in R^n\\). Supposing the number of output tokens is k + 1 + c, where c is the clustering number, the class token \\(\\{z'_0\\}\\) and the top k patch tokens with the highest importance scores IPS are selected as primary tokens \\(z'_{1,kp}\\) and other tokens are considered as redundant tokens \\(z'_{1,rd}\\):\n\\(z'_{1,kp} = \\{z'_i | rank(IPS(i)) \\leq k, 1 \\leq i \\leq n\\}.\\) (5)\n\\(z'_{1,rd} = \\{z'_i | rank(IPS(j)) > k, 1 \\leq j \\leq n\\}.\\) (6)\nTo effectively preserve information from redundant tokens while minimizing image redundancy, a token merging strategy is employed. Given that redundant tokens may belong to diverse semantic categories, indiscriminate merging could lead to semantic confusion. To address this challenge, the KMeans algorithm is utilized to cluster tokens based on cosine similarity. Within each cluster \\(\\{C_1, ..., C_s\\}\\), tokens are considered semantically similar and are merged based on IPS. This approach enables the compression of redundant information while maintaining the semantic integrity of the image content.\n\\(z'_{1,C_i} = \\sum_{\\{j/z'\\in z'_{1,rd} z'\\in C_i \\}}IPS(j)z'_{j}.\\) (7)\nFinally, the intermediate representation of \\(z'_i\\) is updated as:\n\\(z'_{l} = cat(z'_{0}, z'_{1,kp}, z'_{1,C_1}, ..., z'_{1,C_s}).\\) (8)\nCross-Modality Attention Inhibition\nDespite the compression of image redundancy in the visual modality, retaining an insufficient number of tokens can significantly impair the model's multimodal understanding capability. To address this issue, we propose cross-modality attention inhibition, a plug-and-play module for Large Language Models (LLMs). This module enables each text token to focus exclusively on the most relevant image tokens, thereby mitigating the impact of redundant information.\nGiven a sequence of tokens \\(\\{I_1, ..., I_n, T_1, ..., T_m\\}\\), where I and T represent image and text tokens, respectively, and an average attention score \\(A_s \\in R^{(n+m) \\times (n+m)}\\), we define text-to-image attentions \\(A_{t2i} \\in R^{m \\times n} \\subset A_s\\) as elements where the queries are text tokens and the keys are image tokens. Similarly, text-to-text attentions \\(A_{t2t} \\in R^{m \\times m} \\subset A_s\\) are defined as elements where both the query and the key are text tokens. The primary objective is to effectively identify and suppress incorrect associations between image and text tokens, ensuring the preservation of only relevant connections. To achieve this goal, we consider not only each text token's own attentiveness to image tokens, but also the attention of other text tokens it attends to. We use the neighborhood text-to-image attention \\(A_{n2i}\\) to quantify this attentiveness, aggregating \\(A_{t2i}\\) through neighborhood text-to-text attention \\(A_{t2n}\\). The neighborhood text-to-text attention serves to quantify a text token's focus on neighboring tokens, excluding itself. Finally, we combine \\(A_{n2i}\\) and \\(A_{t2i}\\) to obtain the text-to-image focus score \\(F_{t2i}\\).\n\\(A^{jk}_{t2n} =\n\\begin{cases}\nA^{jk}_{t2t}, & \\text{if } j < k, \\\\\n0, & \\text{if } j = k, \\\\\nA^{jk}_{t2t}, & \\text{if } j > k,\\\\\n\\end{cases}\\) (9)\n\\(A^{hk}_{n2i} = \\sum_{h\\in\\{1,...,m\\}} A^{jk}_{t2n}A^{hk}_{t2i}\\) (10)\n\\(F^{jk}_{t2i} = A^{jk}_{n2i} + A^{jk}_{t2i},\\) (11)\nwhere j, k denote the row and column indices, respectively. Conceptually, if a text token allocates greater attention to another particular text token, then the focus of that particular text token on image tokens should be weighted more prominently in \\(A_{n2i}\\). Finally, given attention inhibition ratio \\(\\gamma\\), we calculate \\(\\gamma\\)-th quantile for each row of \\(F_{t2i}\\) as thresholds. Then we perform attention inhibition on text-image token pairs with \\(F_{t2i}\\) below these thresholds by adding negative infinities on causal masks at corresponding positions."}, {"title": "Experiments", "content": "Experimental Settings\nImplementation Details. In this study, CLIP-ViT-L/14 is employed as the image encoder. The projection layer consists of a two-layer MLP with a GELU activation function. Vicuna-v1.5 is selected as the large language model. The VMTC module is inserted into equally spaced transformer blocks to achieve a total token compression ratio of 50%, unless otherwise specified. The CMAI module is incorporated into all layers of the LLM, applying a linearly increasing attention inhibition ratio with a maximum of 60%. Datasets and training configurations are adopted following LLaVA-1.5 in both the pretraining and instruction tuning stages.\nEvaluation. Five widely-recognized benchmarks are utilized to evaluate the model's performance. Three academic-task-oriented benchmarks are employed: VQA-V2 and GQA assess visual perception capabilities through open-ended visual question answering, while TextVQA examines the ability to answer OCR-based visual questions. Additionally, two comprehensive datasets, MME and MMBench, are used to provide a thorough evaluation of the model. To evaluate instruction-following capabilities, MLLMs are required to perform 16 different verifiable instruction-following tasks while answering image-related questions. The success rates of these tasks are reported. The selected tasks are achievable yet challenging for MLLMs and are categorized into five groups:\nText formatting (T1-3): Remove Commas, Lowercase Conversion and Uppercase Conversion.\nIncluding required content: Placeholder, Postscript, Title and Keyword.\nSpecific format (T8-11): Add Highlights, JSON Format, Answer in Sections and Answer in Bullet Points.\nLength Limit(T12-13): Sentence Count and Word Count.\nOther(T14-16): Dual Response Combination, Ending Constraint and Starting Constraint.\nComparison Experiments\nComparison of Instruction-Following Capability. A comparison of instruction-following capabilities and multimodal understanding capability, as measured by performance on GQA , is conducted between LLaVA-1.5 , spatially down-sampled LLaVA-1.5, and the proposed model. The results are presented in Table 1. Furthermore, experiments are conducted where the model first describes an image in text and then completes text-only instruction-following tasks, serving as an upper limit for the instruction-following tasks. Spatial down-sampling, as an intuitive solution to improve instruction-following capability, yields 3.7% and 5.3% improvements in success rates for LLaVA-1.5 7B and 13B, However, this approach is not ideal due to significant performance degradations over GQA of -2.1% and -1.6%.\nIn contrast, the proposed method enhances the instruction-following capabilities of LLaVA-1.5 by up to 9.4% and 9.5% with minimal performance loss of -0.4% and 0% on GQA, significantly outperforming the spatial downsampling strategy.\nThe gap of 5.5% and 6.5% remains compared to the upper limits, highlighting the disparity between MLLMs and LLMs in instruction-following. However, it is important to note that the upper-limit method can severely impair multimodal understanding due to the information loss that occurs between the two stages.\nComparison with the SOTA Methods. To demonstrate the efficacy of our approach, we conduct a comparative analysis against leading MLLMs,, including Shikra , IDEFICS , Qwen-VL"}, {"title": "Conclusion", "content": "This research introduces a novel approach to study the instruction-following capability of MLLMs from a model-centric perspective instead of a data-centric one, providing insights into interactive adaptability. Experiments show that image redundancy significantly diminishes the instruction-following capability of MLLMs. To address this issue, we propose two strategies: visual-modality token compression and cross-modality attention inhibition, which are designed to condense redundant image tokens and focus the model's attention on key visual information. These two strategies result in a substantial enhancement of MLLMs' instruction-following capability while maintaining multimodal understanding performance comparable to SOTA models."}]}