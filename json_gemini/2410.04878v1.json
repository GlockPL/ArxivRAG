{"title": "Leveraging Grammar Induction for Language Understanding and Generation", "authors": ["Jushi Kai", "Shengyuan Hou", "Yusheng Huang", "Zhouhan Lin"], "abstract": "Grammar induction has made significant progress in recent years. However, it is not clear how the application of induced grammar could enhance practical performance in downstream tasks. In this work, we introduce an unsupervised grammar induction method for language understanding and generation. We construct a grammar parser to induce constituency structures and dependency relations, which is simultaneously trained on downstream tasks without additional syntax annotations. The induced grammar features are subsequently incorporated into Transformer as a syntactic mask to guide self-attention. We evaluate and apply our method to multiple machine translation tasks and natural language understanding tasks. Our method demonstrates superior performance compared to the original Transformer and other models enhanced with external parsers. Experimental results indicate that our method is effective in both from-scratch and pre-trained scenarios. Additionally, our research highlights the contribution of explicitly modeling the grammatical structure of texts to neural network models.", "sections": [{"title": "1 Introduction", "content": "Neural network models, like Transformer (Vaswani et al., 2017), RoBERTa (Liu et al., 2019), and GPTs (Brown et al., 2020), have gained widespread adoption in various natural language processing tasks. These models can generate desired answers on different tasks and show strong language understanding ability on multiple datasets. However, they give up explicit parsing of the specific syntactic structure of the text data and cannot effectively establish structured and interpretable language understanding models (Ramakrishnan et al., 2018; Guo et al., 2019; Hewitt and Manning, 2019; Dai et al., 2021). This limitation has emerged as one of the bottlenecks for neural network models to understand natural language deeply.\nTo this end, researchers attempt to take advantage of traditional syntax parsers (Brill et al., 1990; Magerman and Marcus, 1991) to identify grammatical components within textual data and utilize them in subsequent processing steps. These parsers are built upon established linguistic frameworks and regulations. There has been growing interest in investigating the impact of syntax on neural network models and improving them through the lens of grammar induction (Yang et al., 2020; Bai et al., 2021; Li et al., 2021; Hou et al., 2022).\nWhile these techniques have exhibited enhanced efficacy, they still grapple with two primary challenges. Firstly, these approaches (Yang et al., 2020; Bai et al., 2021; Hou et al., 2022) usually depend on external parsers to obtain additional specialized annotations, which are expensive and not time-efficient. Their performance is greatly influenced by the choice of external parsers and may not be universally applicable across all languages. The other problem is that these approaches only focus on from-scratch scenarios (Ma et al., 2019; Yang et al., 2020) or pre-trained scenarios (Li et al., 2021; Tziafas et al., 2021; Chen et al., 2022)). They cannot share consistent improvements. This discrepancy between the two model types engenders a predicament where these methodologies are tailored exclusively to one of the scenarios.\nIn this paper, we introduce a novel method to induce grammar information for language understanding and generation, obviating the necessity for supplementary syntactic annotations. The self-induced grammar features are integrated into the Transformer and simultaneously learned during the training of downstream tasks. Additionally, we devise BPE (Byte Pair Encoding) embeddings and trade-off loss functions to facilitate grammar induction. Experiments demonstrate the compatibility of our method in both the from-scratch and pre-trained scenarios. By strengthening the induction ability of the deep neural network model to the grammatical information, we can improve the model's understanding of natural language. Our method surpasses other external-parser-enhanced methods in machine translation and language understanding tasks, showcasing persistent efficacy and applicability."}, {"title": "2 Preliminary", "content": "We use syntactic distance (Shen et al., 2018a) and height (Luo et al., 2019) to generate the dependency distribution among tokens. They are two feature vectors used to describe the constituency structure and dependency relations of a sequence of words."}, {"title": "2.1 Syntactic Distance and Height", "content": "Syntactic distance Syntactic distance is first proposed by Shen et al. (2018a) to model the syntactical proximity of adjacent constituents in a sentence. For a sentence (W1,..., wn), syntactic distance Ti quantifies the height of the lowest common ancestor for two consecutive words wi and Wi+1. The relative order of syntactic distances represents syntactic affinities between words. A smaller distance Ti signifies that it is easier to communicate between W1...i and Wi+1...n.\nSyntactic height Syntactic height is introduced by Luo et al. (2019) to depict the syntactic status of words. In the dependency graph of a sentence, the syntactic height hi aims to capture the distance of the word wi to the root. The word with a higher syntactic height is more likely to stand closer to the root of the graph, containing more representative global information for the sentence."}, {"title": "2.2 Dependency Distribution", "content": "StructFormer (Shen et al., 2021) proposes a two-stage method to calculate the estimation of the dependency distribution among tokens. They use syntactic distance and height to identify the smallest legal constituent for each token and the parent of the constituent. The procedure goes through the sentence to estimate the syntactic dependency of each token on the others. We leverage their estimation method to generate the dependency distribution for a sentence. The estimation procedure can be referred to in Appendix A.\nWe introduce unsupervised grammar learning as inductive bias into Transformer for NLP downstream tasks. Our method achieves consistent improvement in both from-scratch and pre-trained scenarios."}, {"title": "3 Method", "content": "Alongside the data pipeline in the original Transformer, we construct a parser to derive grammar features and estimate a syntactic mask to guide the attention mechanism. The \"parsing\"2 pipeline is delineated in Figure 1.\nGrammar features s are induced with convolution layers and a self-attention module working on"}, {"title": "3.1 Syntactic Mask", "content": "Following previous works (Shen et al., 2018a; Luo et al., 2019; Shen et al., 2021), we quantify syntactic distance and height on the same scale. We use convolution layers and a self-attention module to induce grammar features si:\n$s_i = Attn(Conv(x_{i-1},..., x_{i+1})) + Proj(x'_{-1},..., x'_{i+1})$\nwhere xi is the embedding of the i-th token, and 2l+ 1 is the kernel size of the convolution module. x' is the BPE embedding, which will be introduced in Section 3.3. Proj(\u00b7) is the projection layer composed of a Linear layer and a LayerNorm layer. Then we use linear matrices W\u2081, Wr and convolution modules W\u2082, Wh to derive syntactic distance Ti and height hi:\n$\\tau_i = W_i \\tanh (W_2\\begin{bmatrix} s_i \\\\ s_{i+1} \\end{bmatrix} + b)$,\n$h_i = W_h \\tanh (W_3 s_i+b) + b'$\nSyntactic distance measures the syntactic proximity of adjacent words, while syntactic height measures the syntactic status of the word itself. Therefore, the kernel size of W\u2082 and Wh is 2 and 1 respectively.\nAs introduced in Section 2.2, syntactic distance and height are put into the distribution generator to calculate the estimation of the dependency distribution. It serves as a mask to adjust the weights in the self-attention:\n$P_D = F_P(\\tau, h)$\nwhere the distribution generator is formalized as the estimation function Fp(\u00b7). PD is the derived syntactic mask containing the dependency distributions."}, {"title": "3.2 Syntax-giuded Attention", "content": "The syntactic mask PD provides the probability of information transfer among tokens. It is used to guide self-attention heads in the encoder layers of Transformer.\n$Attention(P_D, Q, K, V) = P_D S(\\frac{QK^T}{\\sqrt{d}}) V$\nwhere Q, K, V are query, key and value matrices and d is the hidden dimension. S(\u00b7) is an activation function.\nIt should be noted that two different activation functions are employed in the scenarios of building from scratch and utilizing pre-trained models. In the from-scratch scenario, S(\u00b7) is a sigmoid function instead of the original softmax function so that $S(\\frac{QK^T}{\\sqrt{d}})$ can indicate an independent probability of each token's attendance on each other.\nIn the pre-trained setting, we adhere to the use of the softmax function. Given our utilization of the official pre-trained models RoBERTa (Liu et al., 2019) and the integration of our parser during fine-tuning, maintaining consistency in the activation function with the pre-training phase is more suitable and practical. In addition, the distribution weight is set to be $P_D + 1$, where the syntactic mask plays an auxiliary role in reweighting attention. This is because RoBERTa has been pre-trained devoid of grammar induction. Directly imposing new constraints and altering the model training paradigm may not align seamlessly with its existing framework."}, {"title": "3.3 BPE Embedding", "content": "When parsing, we compute dependency distributions for the tokens within sequences. Typically, the model receives subwords rather than complete words as input, owing to the data being preprocessed using BPE subword tokenization. Nonetheless, all subwords stemming from the same word should share an equivalent grammatical status within their respective sentences.\nTo make our parser aware of the grammar sharing among the congenetic subwords, we design BPE embeddings to represent the condition of word segmentation in the sequence. Tokens that remain intact are labeled as 0, while subwords resulting from word segmentation are assigned 2 (with 1 reserved for padding). For instance, in the sentence \"How could pay arrangements be redesigned to address these problems?\", the word \"redesigned\" is segmented into \"re\" and \"designed\" during preprocessing. The two subwords should share grammatical information among the sentence. Consequently, we will assign them label 2 and the others label 0.\nThe vector of BPE labels will be input into an embedding layer and a projection layer to obtain BPE embeddings. The BPE embeddings will be added to hidden states before they are transferred into the convolution modules to compute syntactic distances and heights. Concatenating is also considered to combine the two embeddings, but it proves ineffective. We do not introduce BPE embeddings into ROBERTa because of consistency and adaptability from the pre-training stage to fine-tuning."}, {"title": "3.4 Loss Function", "content": "We also investigate the device of loss function to facilitate grammar induction for the two scenarios studied in our paper.\nVoita et al. (2019) ascertain that the flow of information through Transformer layers is contingent upon the choice of the learning objective. For machine translation (MT), the representation of the input sequence will be refined in the model and transferred from the source language to the target language. In contrast, for masked language modeling (MLM), the information about the context will be rebuilt during the encoding process.\nIn order to make maximum leverage of grammar induction, we trade off between the two loss functions of MLM and MT with a weighted parameter \u03bb:\n$L = \\lambda \\cdot L_{MLM} + (1 - \\lambda) \\cdot L_{MT}$\nWhere the \u03bb will be searched for different translation tasks.\nOur MLM loss is deployed using the output of the encoder module. We randomly mask part of the input with a special token and restore the masked tokens from the output. The mask rate is set to be 0.15.\nIn the fine-tuning phase of RoBERTa, we opt not to include the MLM loss. This decision stems from the fact that RoBERTa has already been pre-trained on a vast corpus of data, with MLM serving as its core learning objective. Consequently, the incorporation of MLM loss during fine-tuning does not yield additional benefits. More analyses about the effect of MLM loss are provided in Appendix C."}, {"title": "4 Machine Translation", "content": "We use Transformer (Vaswani et al., 2017) as our baseline and conduct experiments on six machine translation tasks of three datasets: IWSLT14-De/En, NC11-De/En, and ASPEC-Zh/Ja. Models are trained from scratch and evaluated on each task."}, {"title": "4.1 Datasets", "content": "IWSLT14-De/En The IWSLT14 (International Workshop on Spoken Language Translation) includes the MT track on TED Talks. We use the German (De) and English (En) corpus. We follow the standard pre-processing steps in fairseq (Ott et al., 2019). The sizes of training, validation and test sets are 160k, 7.3k and 6.8k.\nNC11-De/En The NC11 dataset come from news commentary. We pre-process the dataset following the steps of Bugliarello and Okazaki (2020). The sizes of training, validation and test sets are 234k, 2.2k and 3.0k.\nASPEC-Zh/Ja The ASPEC (Asian Scientific Paper Excerpt Corpus; Nakazawa et al., 2016) dataset is a Chinese (Zh) - Japanese (Ja) scientific paper excerpt corpus. We use the official steps provided by WAT (Workshop on Asian Translation) to preprocess the dataset. The sizes of training, validation and test sets are 672k, 2.1k and 2.1k.\nThe three datasets we choose are of different scales. Considering the sizes of datasets, IWSLT14 and NC11 tasks could be used to simulate low-resource scenarios. ASPEC, on the other hand, is of large size and provides more source languages."}, {"title": "4.2 Experiment Settings", "content": "We use six layers of encoder-decoder architecture as the backbone. The number of the convolution layers in our parser is 3, and the dimension of the embedding layer for BPE is set to 256 before input into a projection layer by searching powers of 2 from 2 to 512. We use the syntactic mask in the first encoder layer for all the tasks.\nWe adopt the inverse square root learning rate scheduler, and the peak learning rate is set to 5e-4, 1e-3 and 3e-4 for IWSLT14, NC11 and ASPEC. The most appropriate values of the weighted parameter \u03bb for the loss function are selected by doing a grid search over the range of 0.2 to 0.6 for each task. \u03bb is chosen to be 0.47, 0.35, 0.3, 0.45 for IWSLT14, NC11, ASPEC Chinese to Japanese, and Japanese to Chinese respectively. More details of experiment settings are summarized in Appendix B."}, {"title": "4.3 Results", "content": "We take 5 random seeds for each task and average the last 5 checkpoints to be evaluated for each seed. We use the BLEU score to assess the performance of models on the test sets. Our experimental results on the three datasets are presented in Table 1, Table 2 and Table 3. We report the scores of Transformer we implement and other state-of-the-art (SOTA) models, including PASCAL (Bugliarello and Okazaki, 2020), LISA (Strubell et al., 2018), ST-NMT (Yang et al., 2020) and Distance Transformer (Hou et al., 2022), which are enhanced by external parsers, as well as LPSI (Harada and Watanabe, 2021), which unsupervisedly induces latent phrase structures and incorporate them into the attention mechanism. We also replace the Transformer encoder with StructFormer (Shen et al., 2021), which is an encoder-based model for grammar parsing, and train the model on IWSLT14 De\u2192En.\nTables demonstrate that our method obtains SOTA results in most translation tasks. It consistently outperforms the vanilla Transformer across all six tasks, with particularly notable improvements observed in IWSLT14 German to English and ASPEC Chinese to Japanese tasks. It surpasses other external-parser-enhanced methods and the unsupervised method LPSI and StructFormer on IWSLT14-De/En.\nIt is worth noting that our method achieves scores comparable to the reported outcomes of Distance Transformer (Hou et al., 2022). They rely on the external parser to generate constituency grammar information so as to enhance the self-attention mechanism in Transformer. Moreover, when we reproduce their results, we find that their model benefits from attention dropout, and they use the best checkpoint at inference time. Compared with their supervised Distance Transformer, our unsupervised method learns to comprehend grammar structures without any additional knowledge or external tools.\nIn addition, it can be observed that all the other external-parser-enhanced methods are mainly applied in German and English since the parsers they leveraged are primarily developed for these two languages. Their models are affected by external parsers and limited in the diversity of languages. In contrast, our method internalizes grammar induction within Transformer for downstream tasks without the requirement for additional syntactic annotations. As a result, our approach can be applied across a wider range of languages."}, {"title": "4.4 Effect of BPE Embddings", "content": "To assess the effect of BPE embeddings we have designed, we compare the performance of our method with and without the incorporation of BPE embeddings.\nThe average scores and standard deviations presented in Table 4 indicate that while our BPE embeddings have a modest influence on machine translation, they do offer a positive contribution across all tasks except the ASPEC Japanese to Chinese task. It is observable that BPE embeddings are particularly beneficial in the context of English to German translations. This enhanced utility is likely attributed to the fact that the grammar status sharing among congenetic subwords is more compatible with English grammar. Regardless of the presence of BPE embeddings, our method shows consistent advancements over Transformer across machine translation tasks, showcasing the robustness and adaptability of our approach."}, {"title": "4.5 Parsing Performance", "content": "Furthermore, we implement the distance-to-tree algorithm (Shen et al., 2018b) to reconstruct the hierarchical structure of a constituency tree, leveraging the syntactic distances generated during the parsing process. Following the definition of syntactic distance, the two words with the smallest distance will be merged into a tree first. The sequence of tokens, along with their syntactic distances, will be converted into a binary tree. The transformation algorithm can be referred to in Appendix D.\nTo assess the parsing performance of our method, we utilize the validation set of IWSLT14-De/En. The parser of Stanford CoreNLP\u00b3 is employed to generate constituency trees as reference.4 Precision, recall, and F1 scores are computed to evaluate the trees generated by our method, with the results presented in Table 5.\nThe F1 scores of constituency trees generated by using our method achieve 34.97 and 31.30 for the source languages of German and English, respectively. These generated parses served as references for evaluating parsing performance during translation."}, {"title": "5 Language Understanding", "content": "We deploy our grammar induction method on RoBERTa (Liu et al., 2019) at the stage of fine-tuning, and evaluate its performance in the GLUE benchmark (Wang et al., 2019)."}, {"title": "5.1 Datasets", "content": "The GLUE (General Language Understanding Evaluation) benchmark is a collection of datasets for evaluating the natural language understanding performance of models. It consists of (1) single-sentence classification tasks: CoLA (Corpus of Linguistic Acceptability) and SST-2 (Stanford Sentiment Treebank); (2) similarity and paraphrase tasks: MRPC (Microsoft Research Paraphrase Corpus), QQP (Quora Question Pairs) and STS-B (Semantic Textual Similarity Benchmark); (3) inference tasks: MNLI (Multi-Genre Natural Language Inference Corpus), QNLI (Stanford Question Answering Dataset) and RTE (Recognizing Textual Entailment)."}, {"title": "5.2 Experiment Settings", "content": "ROBERTa is a well and robustly pre-trained model with a strong ability for language understanding. It is essentially a Transformer model with only encoder layers. We use ROBERTa-base as our backbone. It has 12 encoder layers and 12 attention heads in each layer. We load official checkpoints of pre-trained RoBERTa. Details of the experimental settings are set following fairseq (Ott et al., 2019).\nFor our grammar-aware RoBERTa, the number of convolution layers in our parser is 2, and we use the syntactic mask in the first three encoder layers. We fine-tune and evaluate the model on each task of GLUE."}, {"title": "5.3 Results", "content": "The results on the GLUE benchmark are presented in Table 6. The best checkpoints are saved during training, and we use standard evaluation metrics for each task. We reproduce the vanilla RoBERTa and our grammar-enhanced RoBERTa in the same environment and report the scores in the table.\nAs shown in the table, our method outperforms the vanilla RoBERTa in the majority of GLUE tasks and achieves the highest average score. Remarkably, without relying on any additional syntactic annotations, our method has achieved improvements that are more consistent when compared to other syntax-enhanced models, including SLA (Li et al., 2021), Syntax-RoBERTa (Bai et al., 2021) and SynCLM (Zhang et al., 2022). To introduce syntactic information into pre-trained models, all of these methods resort to external parsing models.\nWhen integrated with our method for self-inducing grammatical knowledge, RoBERTa obtains improvements across six of the nine test sets in GLUE, with the scores on the remaining three test sets being nearly identical to those of the baseline ROBERTa. More specifically, it can be inferred from the table that pre-trained language models have exhibited such an impressive capability of language understanding that the integration of external parsers may not bring uniform enhancements across all tasks. They even hurt the performance in certain tasks, as evidenced by the performance of SLA on RTE, Syntax-RoBERTa on MRPC, and SynCLM on MNLI. Nonetheless, our approach performs comparably well in single-sentence classification tasks, and similarity and paraphrase tasks. It particularly excels in inference tasks, with a notable improvement observed in the RTE task."}, {"title": "6 Related Work", "content": "There are generally two ways for syntactic knowledge-based language comprehension.\nExplict syntax enhancement One way is to incorporate syntactic information explicitly by using a high-quality external parser. PASCAL (Bugliarello and Okazaki, 2020) builds up a syntax-guided localized attention mask where each token's attention range is a Gaussian distribution centered by its dependency head. Distance Transformer (Hou et al., 2022) utilizes the relative magnitude of syntactic distance from the constituency tree to build up a syntactic local range. ST-NMT (Yang et al., 2020) slices the constituency tree at some depth and gets a list of constituents as labels. A new Transformer encoder is learned to predict this structure, and its output is integrated with the original encoder output. SEPREM (Xu et al., 2021) also focuses on pretraining architecture and defines a prior attention weight distribution by normalizing the inverse of the token's distance in the dependency structure. However, all of these methods require sophisticated external parsers to generate dependency or constituency syntax trees.\nImplicit syntax enhancement The other way is to implicitly induce the latent grammar structure by learning from data distribution. PRPN (Shen et al., 2018a) enhances RNN neural language modeling by simultaneously learning a CNN-based parsing network, which could induce latent constituency structure represented by syntactic distance. Recent works have focused more on attention mechanisms. StructFormer (Shen et al., 2021) introduces a parser layer that can induce dependency and constituency syntax simultaneously and integrates induced dependency structure into self-attention in a differentiable way. LPSI (Harada and Watanabe, 2021) induces latent phrase structure in an unsupervised fashion and integrates them into the multi-head attention mechanism. SyncAttn (Deguchi et al., 2021) enables neural OpenIE to induce the latent syntactic structure and adopt multi-view learning to capture multiple relationships from constituency and dependency graphs. Nevertheless, very little work pays attention to leveraging implicit induction in downstream tasks.\nSyntax enhancement of pre-trained models In recent years, people also studied to introduce grammar induction into pre-trained models like BERT (Devlin et al., 2019) and Roberta (Liu et al., 2019) instead of training from scratch. POS-BERT (Sundararaman et al., 2019) leverages POS tag information apart from the syntactic structure. SLA (Li et al., 2021) prohibits two tokens from attending to each other once their distance in the dependency tree exceeds some boundary, and it improves the fine-tuning performance on several GLUE tasks. Syntax-Roberta (Bai et al., 2021) develops a framework that can be easily plugged into an arbitrary pre-trained checkpoint, which automatically underlines the most relevant syntactic knowledge for each downstream task. SynCLM (Zhang et al., 2022) proposes a syntax-guided contrastive learning method where phrase-guided and tree-guided contrastive objectives based on constituency and dependency structures are optimized in the pre-training stage to help the language model capture rich syntactic knowledge in its representations. However, these methods resort to external parsers to obtain syntactic information, which could not work well with pre-trained models across downstream tasks."}, {"title": "7 Conclusion", "content": "In this paper, we study the utilization of grammar induction for language understanding and generation. We force the model to be trained with the self-induced grammar without external parsers or annotations, which is a more general approach. Moreover, our method is efficient in both the from-scratch and pre-trained scenarios. Through assessments across various machine translation and language comprehension tasks, we validate the efficacy of our grammar induction technique, showcasing substantial and consistent enhancements. These results underscore the tangible benefits that can be achieved through the incorporation of grammar induction techniques. Our research offers promising insights into the practical application of induced grammar in downstream tasks."}, {"title": "Limitations", "content": "The performance of the grammar-aware method may be constrained in large-scale datasets and well-trained models. As shown in our paper, none of the grammar-aware methods can achieve consistent improvements on all the tasks of GLUE. This phenomenon could be explained by the adequacy of model training. Since RoBERTa has been pre-trained over a tremendous amount of data, it has already done well in language understanding. But it occurs to us whether pre-trained models or large language models have some form of syntax parsing modules inside them.\nThe parsing performance of our parser is a little limited. We found that the quality of constituency trees generated from the syntactic distance in our parser is far from that of well-designed and trained supervised constituency parsers. However, our method does make neural network models more interpretable. More appropriate optimization methods could be explored to make grammar induction more effective."}, {"title": "Ethics Statements", "content": "Our work pertains to neural machine translation and fine-tuning of pre-trained language models to introduce grammar induction into Transformer. In this work, we use only publicly available data and artifacts."}, {"title": "A Estimation of Dependency Distribution", "content": "The estimation function Fp(\u00b7) in Equation 4 leverages the two-stage method by Shen et al. (2021) to calculate the estimation of the dependency distribution among tokens. We utilize it to generate the syntactic mask with syntactic distance and height. We describe the calculation procedure in detail.\nTo estimate the probability PD(wj|wi) that the j-th token wj is the parent of the i-th token wi, they decompose PD (wj|wi) into two factors:\n$P_D(j | i) = \\sum_{[l,r]} P_{Pr}(j | [l,r])p_c([l,r] | i)$\nwhere ppr(j | [l,r]) denotes the probability that wj is the root of wl...r. It can be parameterized by syntactic height:\n$P_{Pr}(j | [l, r]) = \\frac{exp (h_j)}{\\sum_{k \\in [l,r]} exp (h_k)}$\nAnd pc([l, r] | i) denotes the probability that wl...r is the smallest legal constituent C(wi) for wi:\n$p_c([l, r] | i) = p_L(l | i)p_R(r | i)$\nwhere pL (l | i) denotes the probability that l is the left margin of C(wi) and pR(r | i) stands for the right margin.\nThe distribution is parameterized that the l-th token wl is inside C(wi) with the probability p (l \u2208 C(wi)) that hi is larger than the maximum distance \u03c4 between l and i:\n$p (l\\in C (w_i)) = \\sigma ((h_i - max (\\tau_1, ..., \\tau_{i-1})) /\\mu)$"}, {"title": "B Experiment Details", "content": "We perform all the training on 2 RTX3090 GPUs for IWSLT14 and NC11, 2 80G-A100 for ASPEC, and 1 40G-A100 for GLUE. For our method with BPE embeddings, each training took about 2.5 hours for IWSLT14, 3 hours for NC11, and 50 hours for ASPEC. For our method without BPE embeddings, each training took about 2 hours for IWSLT14, 2.5 hours for NC11, and 29 hours for ASPEC. We use Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98. Cross entropy is the default loss function with label smoothing of 0.1 and weight decay of 0.0001. The batch size (GPU_num \u00d7 Update_num \u00d7 Max_tokens) is 2 \u00d7 1 \u00d7 4096, 2 \u00d7 2 \u00d7 8192 and 2 \u00d7 1 \u00d7 8192 respectively.\nThe numbers of attention heads in each layer and dimensions of the fully connected layer are 4 and 1024 for IWSLT14 and NC11, 8 and 2048 for ASPEC.\nWe employ beam search at inference time. Following the settings of previous work Hou et al., 2022, the beam size and length penalty are set to 5 and 1.0 for IWSLT14 and ASPEC, and set to 4 and 0.6 for NC11.\nThe number of model parameters is about 49M for IWSLT14, 57M for NC11, and 108M for ASPEC, with an increase of 9M. For grammar-aware ROBERTa on GLUE, the fine-tuning took about 7 hours and the number of model parameters is about 135M with an increase of 10M.\nWe also see increased numbers of parameters in other syntax-enhanced models like SLA (Li et al., 2021), ST-NMT (Yang et al., 2020), LPSI (Harada and Watanabe, 2021) and Syntax-Roberta (Bai et al., 2021), while they do not report statistics in their papers."}, {"title": "C Effect of MLM loss", "content": "We conduct experiments to study the effect of MLM loss on our method. The results on machine translation tasks and GLUE are shown in Table 7 and Table 8.\nExperiment results demonstrate that MLM loss has a different effect on models' grammar integration in the two scenarios. As discussed in Section 3.4, the language modeling objective contributes to the hierarchical generalization of Transformers. Consequently, our grammar induction works well with MLM loss to train Transformers from scratch on machine translation tasks. Nevertheless, it does not benefit from MLM loss when fine-tuning a pre-trained model RoBERTa. One possible reason is that learning to induce grammar with MLM loss might influence hierarchical generalization inside the model, which has been developed by MLM loss in the pre-training of RoBERTa. It deserves further work to study the relation between language modeling and grammar induction during different stages of training."}, {"title": "D Transformation Algorithm", "content": "The algorithm we implemented in Section 4.5 is to transform the syntactic distances of a sentence into a constituency tree.\nThe transformation algorithm is designed with a top-down merging method. It divides the input sequence w into two subsequences at the split point i whose syntactic distance \u03c4i is the largest in the sequence. Then the subtrees of the two subsequences are merged into a whole."}, {"title": "Algorithm 1 Syntactic Distance to Constituency Tree", "content": "Input: Syntactic distance \u03c41...n-1, sentence w1...n\nOutput: Constituency Tree T\nFunction Tree(\u03c4, w)\n1: if \u03c4 = [] then\n2: T\u2190 Leaf(w)\n3: else\n4: i \u2190 argmax(\u03c4)\n5: node Tree(\u03c41...i-1, w1...i)\n6: noder \u2190 Tree(\u03c4i+1...n\u22121, wi+1...n)\n7: T\u2190 Node(node, noder)\n8: end if\n9: return T"}]}