{"title": "Exploring the Needs of Practising Musicians in Co-Creative Al Through Co-Design", "authors": ["Stephen James Krol", "Maria Teresa Llano Rodriguez", "Miguel Loor Paredes"], "abstract": "Recent advances in generative Al music have resulted in new technologies that are being framed as co-creative tools for musicians with early work demonstrating their potential to add to music practice. While the field has seen many valuable contributions, work that involves practising musicians in the design and development of these tools is limited, with the majority of work including them only once a tool has been developed. In this paper, we present a case study that explores the needs of practising musicians through the co-design of a musical variation system, highlighting the importance of involving a diverse range of musicians throughout the design process and uncovering various design insights. This was achieved through two workshops and a two week ecological evaluation, where musicians from different musical backgrounds offered valuable insights not only on a musical system's design but also on how a musical AI could be integrated into their musical practices.", "sections": [{"title": "1 Introduction", "content": "Generative music has seen significant advancements in the last 20 years, resulting in modern systems that can generate human-like music in a variety of different genres and mediums [13, 28, 66, 69]. These generative systems are framed on a spectrum between end-to-end composition tools [2, 28] and co-creative tools to aid in human composition [35, 56, 68]. To date, the generative capability of modern musical models has shown potential to contribute to complete musical projects [26, 38, 54], assist in exploration/ideation [49, 57] and to enable better collaboration between less experienced composers [67]. Recent work has also demonstrated the value of including practising musicians in the development [15] and application [26] of these technologies, highlighting an intuitive need for researchers to integrate musicians into the design process. However, much of this work often requires musicians to use an already developed tool and take part in discussions on their creative process with the tool [15, 26, 35], overlooking the opportunity to engage practising musicians throughout the design process. Music performance and music composition are fields of practice that can take many years to master [17, 77], and while researchers developing co-creative musical AI likely have a musical background, the majority probably do not spend most of their time as professional musicians or actively practising the art of music. Even if the researcher is an experienced, practising musician, their inherent bias as a technology researcher/creator could result in the development of unwanted tools that have unintended consequences, a common issue in science and technology [12, 31]. Therefore, there is motivation to incorporate our target group, in this case practising musicians, throughout the design process, so we can better avoid these issues, as demonstrated in other fields of study [25, 32, 71].\nIn this paper, we present a case study that aimed to investigate the needs of practising musicians in co-creative AI through the design and development of an AI-based tool for composition using a co-design methodology. We define a practising musician as a person who actively engages in the making and/or performing of music, either professionally or as a hobbyist. Practising musicians can come from different backgrounds and are distinguished from novice musicians who possess some musical experience but rarely engage in the practice of music. Examples of practising musicians are composers, producers, teachers and local artists. This co-design approach led to not only the creation of a co-creative tool with a clear role in our participants' practice, but also uncovered design insights that provided stepping stones to better understand the needs of our practising musicians and how to design co-creative Al systems that are tailored to them. The study involved a total of (n=13) practising musicians with diverse musical backgrounds and consisted of two workshops and a two-week ecological study to design a musical variation tool and explore their individual practices.\nThe contributions of this work are as follows:"}, {"title": "2 Background", "content": "Co-creative AI systems are demonstrating potential in various fields such as music [16, 26, 35, 57, 68], writing [19, 70], drawing [33, 45, 75] and design [51, 76]. Several frameworks for designing co-creative systems have been proposed [42, 52, 76], reflecting the increasing interest in refining approaches to the development of co-creative AI. These frameworks present a multifaceted approach with objectives ranging from understanding the roles and varying levels of contribution that co-creative systems can offer in human-Al interactions [21, 37], the implications of these roles [10], different collaboration modalities [30, 42, 52], and specific aspects of the interaction [34]. Additionally, there have been several studies aimed at understanding what users need from these systems. Oh et al. (2018)[45] explored the user experience of participants using the co-creative drawing system DuetDraw, highlighting insights such as the importance of offering clear instructions on system usage and enabling users to take the lead in the creative process. Similarly, Santo et al. (2023) [61] used a probing tool, POCket Artist (POCA), to explore general artist needs for creative software, emphasising the importance of artist-centered development in this domain. Through the development of a co-creative tool named Cococo, Louie et al. (2020) investigated the needs of novice musicians with regards to musical AI-steering tools. The study included an initial needs assessment with novice musicians that identified several issues with both a deep generative model and its interface. This was then used to inform the design of their Cococo system which was subsequently evaluated by 21 musical novices. This evaluation demonstrated an improvement in the system and resulted in various design insights such as the value of AI transparency and the importance of building semantically-meaningful tools. In our work, we aim to understand the needs of practising musicians in co-creative AI, who, due to their more complex skill set and advanced understanding of music, are likely to have distinct needs across different systems compared to visual artists [45] and novice musicians [35]."}, {"title": "2.2 Al in Music: Evaluation and Design Methodologies", "content": "Various deep learning models have been developed to perform a range of musical tasks, including generating entire pieces or phrases [13, 28, 41, 46], in-painting within an existing composition [16, 27, 57, 69], and musical style transfer [72]. Recent advancements in multi-modal models [50, 59] have also enabled text-to-music generation [1, 13], with new private companies now offering complete song generation services\u00b9.\nWith this, there has been development of models with in-built control mechanisms that can be used as co-creative tools [8, 29, 48, 49, 55, 72]. However, because these systems are approached as machine learning problems, their evaluation is often restricted to beating quantitative benchmarks and conducting basic human listening tests, providing little understanding on their usefulness as tools to musicians. Louie et al. (2022) [36] highlight the disparity in how co-creative musical systems are evaluated within the fields of Machine Learning (ML) and Human-Computer Interaction (HCI), and argue that both methodologies are limited, with ML metrics serving merely as proxies, while HCI methods are overly subjective and fail to adequately address the final artefact. The authors constructed their own evaluation method that involved both a subjective assessment from the user as well as outsider listening tests to evaluate the quality of the generated artefacts. Tchemeube et al. (2023) [68] also introduced a novel methodology for evaluating Human-Al interaction through usability, user experience and acceptance measures and utilises both quantitative and qualitative techniques to provide an all-round assessment of the tool. While both methodologies are suitable, they focus on the evaluation of an already developed system. In contrast, our work focuses not on evaluating a co-creative system, but on using a co-design approach to develop a tool and gain insights into the needs of practising musicians."}, {"title": "2.2.2 Music Al Design Methodologies with Practising Musicians.", "content": "Although significant progress has been made in developing co-creative musical AI [4, 16, 48, 57], less work has focused on involving practising musicians in the design and development process [15]. Instead, many works utilise common musical practices [4, 69] to infer the needs of their target group and build models that support these processes. For example, Thickstun et al. (2023)[69] observed that many generative transformer models produce music in a linear, start-to-finish manner. However, human composition is often non-linear and follows a more iterative approach. To address this, they designed their anticipatory transformer to use both previous and future notes when infilling a composition, better complementing the human composition process. However, while this methodology has produced valuable technologies, it risks overlooking less obvious needs of musicians, as demonstrated by Huang et al. (2020)[26] in their study on the AI Song Contest, which identified several challenges impacting the overall usability of AI tools.\nTo address this, researchers have employed various methods to include musicians in their work; for example, Google's Magenta have worked with musicians to create complete projects [38, 54] and documented their experiences from these collaborations. Deruty et al. (2022)[15] conducted a participatory design study with six practising musicians, who provided regular feedback to evaluate and improve a set of AI tools developed by the research group, generating various design insights through these interactions. Similarly, Ford et al (2024) [18] requested 6 composers to reflect and record their impressions of using different AI tools, aiming to contribute a novel method for creative support tool research. Other studies have taken advantage of the researcher's own experience as a musician to gather insights; for example, Benford et al (2024) developed an autonomous system with the ability to perform a duet with musicians through a practice-based study, collecting insights related to generating trust and negotiating autonomy in real time interactions. In an autoethnographic study, Sturm (2022) [65] documented his experience, and explored the benefits of generative Al systems for co-creation in songwriting. His autoethnography highlights various opportunities despite the unpredictability and lack of control in the results from these \u2018creative partnerships'.\nWhile these works highlight the value of collaborating with practising musicians, more can be done to improve how musicians are involved and ensure that perspectives from groups less familiar with Al are also considered. Rather than infer the needs of users [4, 16, 57, 69] our work uses a co-design approach from the outset of development to identify the needs of practising musicians throughout the design process. Our work is distinguished from previous work [15, 26, 38, 54] in that we use a more rigorous participatory design methodology to develop our tool and gain insights on our user group."}, {"title": "2.2.3 Co-design as an approach for Music Al Development.", "content": "Participatory Design (PD), an approach that aims at actively involving relevant stakeholders in the process of design, has been reported in the field of human-computer interaction since the 1980s [60, 62]. Associated to PD, there is an umbrella of methods in which stakeholders have some kind of participation during the design, some of which include: Co-creation, User Centred Design, Collaborative Design, and Co-Design. Although often in the human-computer interaction literature these methods are used interchangeably [20, 24, 40, 62], our work is guided by the principles of Co-Design.\nA key distinction of co-design, compared to methodologies like User-Centered Design (UCD), is that it views the user as a partner rather than merely a subject or source of information [24, 58, 60]. In this respect, Co-Design calls for user participation rather than user evaluation, reflecting their experience and knowledge throughout the design process rather than just interpreting their needs [53]. We follow this philosophy through the implementation of two co-design workshops and an ecological study. Instead of focusing on typical test and feedback sessions, we aimed at creating opportunities for participants to situate themselves within their practice through composition tasks and actively take part in designing the functionality of a co-creative AI. This not only resulted in the development of an AI system, but also led to uncovering aspects of their creative process that impact their interactions with co-creative AI.\nAlthough co-design has been used in multiple domains\u00b2, to the best of our knowledge, the use of co-design with practising musicians is scarce and limited. Our work contributes to the field with a case study on the use of co-design with practising musicians, revealing key insights for the development of co-creative AI for music and resulting in the development of a variation tool."}, {"title": "3 Case Study: Co-Designing a Deep Learning Based Music Variation Tool", "content": "To explore how a co-creative AI could be integrated into the practice of musicians, we collaborated with (n=13) practising musicians to develop a co-creative tool. Similar studies have successfully used this method [15, 35] in creative research, with co-design as a whole being a well-established methodology for designing technology [23, 44, 71], providing a good justification for our approach. To qualify for the study as a practising musician, participants needed to actively engage in the making and/or performing of music. This could be as a professional or as a hobbyist and was not limited to a specific musical background.\nTwo workshops were held to create a starting point for development. The first workshop investigated the general perceptions of co-creative Al from a group of practising musicians who identified an AI tool that could fit in their practices. The second workshop further explored this system and provided functionalities for the tool that would be needed for it to be useful in a musical practice. The insights from both workshops guided the development of a music variation tool that musicians could use for ideation. To gather deeper insights and further design suggestions, we conducted a two-week ecological evaluation in which participants used the system in their own environments. They then took part in a focus group to discuss how this tool, and co-creative AI more broadly, could integrate into their practices and to suggest additional functionality they desired in the tool. The following section outlines the case study in detail."}, {"title": "3.1 Workshop 1: Investigating Musicians' Perceptions of AI in Creative Practice", "content": "As an initial step in exploring how a co-creative AI could be designed with practising musicians in mind, we conducted a workshop to gather insights into their perceptions of AI and to identify potential roles a co-creative system could play in their creative practice. A total of (n=5) musicians took part in the workshop with all participants possessing formal training in music. Participants were recruited through both the university's music faculty and the lab's known network of musicians. Demographic information in regards to each participant can be seen in Table 1."}, {"title": "3.1.1 Understanding How Participants Approach Composition.", "content": "This part of the workshop was essential in understanding our participant's identity as musicians and composers and provided a strong foundation for the discussions held later in the workshop. The composition task set prior to the workshop also provided set pieces that each participant could refer to when explaining their process, giving a unique insight into how they made music. From this we built an understanding of shared techniques used by the participants such as Composing from a Theme which involved using a concept - \"for me, I always like to stick to the concept because I feel like that's what grounds me\" (P2) or emotion - \"I had a feeling that I was trying to convey and that was conveying that feeling for me\" (P1) or imagery - \"the process is actually more have an emotion or an image or an experience, very visual sort of thing\" (P5) to drive composition. This discussion also highlighted the importance of Musical Intuition on composition and how it frequently led to decisions that could only be articulated in terms of the emotional response they evoked in the composer, with one participant stating - \"how could you explain something that comes from the heart rather than from your logical thinking?\" (P3). Ultimately, many discussion points led to the notion of Human Embodiment and how much of the music was driven by their experience as humans - \"It was a rainy shitty day. And I was just like, oh, couldn't be bothered as I go. So really slow chords. This like Oh, that's just how it feels\" (P5) and the craft they have developed over many years - \"there's actually a lifetime of perfecting a craft behind this and you can only sort of scratch the surface of what it's really about\" (P4). These discussion points became especially relevant later in the workshop when AI was introduced into the mix."}, {"title": "3.1.2 Exploring Human-Human and Human-Al Composition Through Play.", "content": "After the initial discussion, participants engaged in a composition activity where they had to compose a series of short compositions from randomly selected prompt cards. These prompt cards contained simple scenarios that the participants could use as inspiration for their compositions. Examples of provided prompts included \"You come home and are greeted by an excited puppy\" and \"A tarantula unexpectedly falls into your lap\". Prompting has been used in other studies to evaluate co-creative systems [35, 36] and was positively received by our participants. In addition to the prompt cards, participants had to either compose with another participant or with an AI generative system. The AI system was powered by Music Transformer [28] and utilised a ported script [9] to build an application for users to interface with the model. The interface was simple and allowed users to input MIDI 3 live through a MIDI keyboard and generate re-harmonisation's of their music. Music Transformer was selected due to its impressive generative capabilities, often attributed to the large 10,000 hour midi dataset it was trained on [64]. The purpose of this activity was not only to provide participants with the chance to interact with an Al system but also to offer them a basis for comparing the experience of creating with AI versus creating with another human. This approach enabled us to initiate discussions on what it would mean for the participants if Al were to function not merely as a tool but also as a collaborator."}, {"title": "3.1.3 Defining Al's Position in Musical Composition.", "content": "Following the composition task participants engaged in a discussion about Al's future role in music creation and the potential for Al to be seen as not just a tool but as a collaborator. Data from this discussion was used to not only influence the design of our current system but also the framing of the system. This was due to strong push back on the concept of Al as a Collaborator which in many cases was due to AI lacking human experience. To the participants, music was a means of communication, with P4 stating \"it's about empathy, really, and being able to share other people's emotions and experiences and Al has no empathy or understanding\", the importance of empathy was also echoed by other participants \"I think you actually need empathy, to collaborate, you need that human emotion that the AI can't have.\" (P3) and \"one last thing about that empathy thing. I think we need to recognise that AI can never truly understand what we're hoping for\" (P5). Participants also expressed hesitation about collaborating with a machine, as they wanted to preserve Ownership of the creative process. Strong sentiments regarding this theme were shared by the participants \"There are times when I think writing the melody or the chords, that's ours, like, sorry, but the creative process, it's, that's our possession, that's what we do.\" (P5), \"And I probably wouldn't use a system because, a lot of like composing music for me is actually that process of thinking, what do I want\" (P1).\nInterestingly, participants were ok with sharing the creative process with another human \"No, no, I think the machine don't have the right to have an opinion. And the human totally has the right to say so 'oh, you disagree with me?' Awesome. let's try to do something else, but with a machine it's like, No\" (P3). In fact, sentiments were mostly positive when talking about Human-to-human Collaboration and it was clear that this was considered special to the participants - \"Different people's ideas coming [it], just strengthens us. It's just a much richer process, the collaboration, because you do have that diversity of thought.\" (P5), \"The inspiration and energy is like 1,000% more with the other people\" (P4) - with the only drawback being that \"you have to compromise\" (P5). When discussing AI collaboration, participants would often refer to their Musician Values \"a large part of why we do what we do is because we enjoy that feeling of writing music, of translating ideas into music\" (P5), \"when you're not creating music as a commodity, that's 100% of the value was the fun times when you're making it with someone else\" (P4), with one participant stating \"At what point does a person stop being a composer?\" (P5), highlighting the personal significance of the participant's identity as composers.\nDespite resisting the idea of an Al musical collaborator, participants did offer insights into what would make a machine collaborator more attractive with the main points related to Familiarity. P5 stated \"it's about time, it's about safety. It's about getting the sense that we have input that we can communicate that I will be heard, that they are listening\" with P2 adding that \"these things might feel more natural\" for future generations who will grow up with Al technology. However, participants were generally more open to using AI as a tool \"I think with AI, you shouldn't let go of that ego, I think AI shouldn't be a collaborator...it's more of a tool, or completely a tool\" (P3) and importantly that this tool should be Controllable with participants wanting a tool to have Varying Agency - \"if you can slide from AI takes over and interprets purely, versus Al takes little bits of it\" (P5). On the discussion of AI as a tool, participants often spoke about the potential of a tool that could generate Musical Variations. This tool could be used to give \"another direction, maybe give sets of options\" (P3) and was likened to PowerPoint's slide design tool which adds \"different colours and things...but you can still see it's the same structure\" (P2). Participants were generally receptive to this idea with P5 stating \"I like that. Personally, for me, that would be brilliant\" noting that \"once you've got the basic ideas [I] could flesh it out, or add variation...it would be good to have some ideas\"."}, {"title": "3.1.4 Workshop 1 Outcomes.", "content": "Although some insights shared by participants align with findings from other general co-creative studies-such as the importance of controllability in co-creative tools and varying agency [3, 35, 45]-the workshop still uncovered new perspectives on where a co-creative AI would be most beneficial in their musical practice. For example, it was clear these musicians were not looking for a machine collaborator nor an ideation tool that would perform most of the work. They desired to own their creative process and were more interested in a co-creative system that could help expand their current musical ideas. This is not to suggest that AI collaborators are generally undesirable to practising musicians; however, the consensus among participants in this study indicated that how we frame this technology, for instance, either as a collaborator or a tool, influences its adoption within practising musicians. It was also clear that participants valued human-human collaboration and were not interested in replacing this with an AI. Following this workshop, it was decided to focus development on creating a music variation tool, based on the participants' suggestions that such a tool would be valuable."}, {"title": "3.2 Workshop 2: Understanding AI Music Variation in Composition", "content": "After the initial workshop, a second workshop was conducted to explore: (1) whether a musical variation tool would be useful in musical practice, and (2) what features musicians would want in such a tool. Although this workshop centred on a specific type of system, discussions still led to insights that apply beyond the tool. A total of (n=6) musicians took part in this study, all different from the previous workshop to gain new perspectives on this type of tool. They were also recruited through the university's music faculty and known networks to the researchers. Demographic information relating to participants can be seen in Table 2."}, {"title": "3.2.1 Probing Musical Variation Tool.", "content": "In this work, we define a music variation tool as a system that modifies musical phrases or pieces based on specific musical attributes. For instance, the tool could re-harmonise a melody [28], apply a different rhythmic groove to a pop song [57], or even perform a style transfer by changing the genre of the final piece [72]. This concept is well-established in creative research and is often described as 'remixing' [14, 76]. However, in this work, we use the term \"variation\" due to its historical significance in music [63] and because it better aligns with this system's functionality. Deep learning was selected as the main engine for this tool due to its current state-of-the-art generative capabilities [28, 69] and its ability to create latent representations, which are well-suited for generating variations [49, 57]. While there exist models for exploring variations [57] and discussions on using these tools in composition [4], these models are limited in their generative capabilities, typically being applied only to simple melodies or short, fixed-length bars. Furthermore, to the best of our knowledge, no work has been done to develop these tools specifically for and in collaboration with practising musicians. Due to its impressive generative capabilities, the Music Transformer was, once again, used as a probing tool to give participants a reference point when considering a music variation tool. While not specifically designed for music variation, the model does offer a re-harmonisation version which allows users to explore different accompaniments to a specified melody. Participants were provided access to the re-harmonisation model via a simple web-based interface that allowed them to upload MIDI files to a server, which then generated multiple harmonic variations for them. When interacting with the model, participants had the option to either compose songs based on the prompt cards from the previous workshop or to work on their own music."}, {"title": "3.2.2 Designing a Music Variation Tool.", "content": "Following the composition activity, the participants took part in a discussion regarding music variation tools. Through this we gathered more evidence from new participants that a tool like this would be Useful in their practice \"I would definitely use it in my creative practice and even in my professional jobs\" (P11) - \"I think it could be really cool, for a lot of the stuff I was making\" (P7). Additionally, we found that a variation tool would be especially valuable during the Ideation phase. Participants noted that it could \"influence new parts of the story\" (P7), with one participant sharing that the idea they used \"was an idea that I hadn't thought of at all\" and that it \"became a big structural part of the piece\" (P9).\nThe effect of ideation varied for different participants. For example, P11 used suggestions from the system to create a \"more dissonant and dark chord progression as a B section\" for their piece, while P10 focused on a single note introduced by the system, incorporating it into their composition \"to capture the idea\". This initial discussion provided further evidence indicating a desire to use a musical variation tool in practice, with the primary use case being ideation.\nThe use of Music Transformer also gave participants a reference point from which they could propose design features for a future system. For example, participants referred to there being Not Enough Variation or Too Much Variation and noted a desire to Control the Type of Variation - \"something like a slider to control the level of variation would be helpful to see, if I want less variation or more variation\" (P11). There were discussions on a Personal Model which could mimic the composers style - \"ideally I would like to be able to train it with my own ideas\" (P6) and the value of maintaining performative information in the generated MIDI data - \"MIDI piano can sound really not human at times, it doesn't sound like natural organic playing, but what the systems gave back was in this case was just the perfect amount of quantised but like slightly out so it sounded more human\" (P11). Participants also suggested other use cases for the system such as packaging it with the prompt cards to create a song writing exercise tool - \"something like this could be really helpful for that, like for song writers and composers to just sit and write 30mins of music once a day and build those skills\" (P7). Finally, participants shared insights into where they would want to use the system, with the most notable being integrated into their Digital Audio Workstation (DAW) \"yeah if it could be implemented within the DAW it would probably be easier for me\" (P10)."}, {"title": "3.2.3 Workshop 2 Outcomes.", "content": "The second workshop gave us deeper insight into the role a co-creative AI could play in practising musicians' practices and provided evidence that they would be open to incorporating an AI-based system into their workflow. Additionally, the workshop further confirmed that the concept of a musical variation tool was desirable to musicians and highlighted that the tool would be most beneficial during the ideation phase. Finally, it helped identify specific design suggestions, such as control over the level and type of variation - providing clear steps for future development."}, {"title": "3.3 MusicBert Variation", "content": "Following these two workshops, a musical variation tool was developed with its main objective to assist in ideation of musical phrases. The current musical variation system utilises MusicBert [74] and is based on the MidiFormers project\u00b9.\nMidiFormers utilises masked prediction on a MIDI file to generate remixes of different instruments in a song, employing the MusicBERT model to predict the masked notes. MusicBERT's Octuple encoding allows for remixing across eight different attributes: Bar, Instrument, Position, Pitch, Velocity, Tempo, and Time Signature. In this encoding, each note is associated with these attributes, and by masking and predicting them, variations in the overall track can be created. An illustration of the masking functionality can be seen in figure 4 and a description of the attributes in Table 3. This concept was selected as a basis for our system for the following reasons: (1) Allows users to control the level of variation by specifying the number of notes to be masked-the more notes masked, the greater the variation. (2) The Octuple encoding method provides inherent control over 8 musical attributes, allowing the user to control not only the level of variation but also the type of variation. These considerations also led us to move away from using Music Transformer, as its lack of inherent controllability [73] made it challenging to implement the features desired by participants. Our project built upon the MidiFormers concept and introduced three significant functionality changes to make it better suited for our task. These changes were made to give users more direct control over the model, better addressing the co-design workshop requirements that highlighted the importance of control in the creative process."}, {"title": "3.3.1 Add New Notes.", "content": "While MidiFormers can generate interesting variations of different instruments in a MIDI file, we identified limitations relating to the complexity of the generated variations, particularly on smaller musical sequences. To combat this, we built a new function that provides users the ability to add new notes into the sequence. New notes are introduced as masked tokens, initially assigned only an instrument and bar value. The model then predicts their remaining attributes, such as pitch and position."}, {"title": "3.3.2 Bar Control.", "content": "To give users better control over which parts of a track are varied, we developed a bar specifier function that lets users select specific bars for variation. This allowed users to include extra musical context that the model could use during prediction, without those parts of the track also being varied."}, {"title": "3.3.3 Bar-Level Masking.", "content": "MusicBERT was trained using bar-level masking. This meant selecting a single attribute, masking it over all notes in a bar and then predicting. However, MidiFormers still performs random note masking, leading to sequences that can sound disjointed. In contrast, by applying bar-level masking, MusicBERT can predict connected, continuous note sequences, leading to more cohesive variations. The final version provided an option for both random masking and bar-level masking."}, {"title": "3.4 Insights from an Ecological Evaluation", "content": "To obtain intermediate feedback and further design suggestions from musicians, we conducted a two-week ecological evaluation study that provided participants with continuous access to the system within their own creative environments. Ecological evaluation [7] is a methodology that assesses systems within a participant's own space and time, rather than in a controlled lab environment. This type of methodology is similar to in-the-wild studies [5, 11] and is widely used in music research [15, 22]. Ecological evaluation is ideal for musicians who have personalised setups at home or in a studio that they are accustomed to using. A total of (n=6) participants took part in this study and used the system for a combined total of 35 hours. Three of the participants had taken part in one of the previous workshops and three participants were new to this study. Participants were provided with a $30 USD equivalent gift card for their time. Participant demographic information can be seen in Table 4.\nBefore gaining access to the system, participants watched a video explaining how to use the system and its various features. The system was hosted on an Amazon Webservice (AWS) EC2 server for a two week period and was accessed using a custom domain. Each participant was provided a unique username and login that ensured their work could be saved on the server. To minimise server load, participants were asked to limit their inputs to approximately 15 bars of music. Beyond this guideline, they were given full freedom in how they could use the system. Participants were provided with prompts to guide their compositions, but they had the option to disregard these and instead focus on their personal or professional projects. After a week of using the system, participants received suggestions on how to use the tool from the project's lead developer. The developers unique understanding of the tool, provided a set of considerations that participants could use if they were having difficulty generating desired outputs. These considerations included which musical attributes provided the best musical variation as well as a method for generating variations that involved chaining variations. However, participants did not have to use these suggestions and could use the tool in any way they preferred. While using the system, participants were asked to keep a journal that they would use to keep track of noteworthy interactions or design suggestions. This ensured that participants would remember useful design suggestions throughout the two-week period. Following this time, participants took part in a focus group aimed at providing an initial assessment for the tool as well as providing design insights into what an initial release of this tool should look like. The journal data and focus group transcripts were analysed using the same method described in section 3.1."}, {"title": "3.4.1 Understanding the Application of the System.", "content": "Participants spent a lot of time Experimenting with the system and provided feedback on the Best Workflows to utilise with the tool. The Chaining technique was favoured by some participants, with P13 noting that \"Chaining provided [them", "some of the ideas I really liked, but to make them usable, it would just be getting rid of some of the random stuff that it had in there\". This process of \"choosing what [they": "liked and cutting it up\" (P14) highlighted that the value of this tool was in \"the moments instead of the whole\" (P11). As expected, participants observed that the system's impact was most significant in the Ideation phase - \"it sort of gave me something new I never really thought of before\" (P7), \"I would never have thought of that kind of progression. So yeah, little bits and pieces out of it could be quite remarkable\" (P5). P8 likened their use of the system to writing with ChatGPT [47"}]}