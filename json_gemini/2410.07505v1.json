{"title": "CROSSQUANT: A POST-TRAINING QUANTIZATION METHOD WITH SMALLER QUANTIZATION KERNEL FOR PRECISE LARGE LANGUAGE MODEL COMPRES- SION", "authors": ["Wenyuan Liu", "Xindian Ma", "Peng Zhang", "Yan Wang"], "abstract": "Post-Training Quantization (PTQ) is an effective technique for compressing Large Language Models (LLMs). While many studies focus on quantizing both weights and activations, it is still a challenge to maintain the accuracy of LLM after acti- vating quantization. To investigate the primary cause, we extend the concept of kernel from linear algebra to quantization functions to define a new term, \u201cquanti- zation kernel\", which refers to the set of elements in activations that are quantized to zero. Through quantitative analysis of the quantization kernel, we find that these elements are crucial for maintaining the accuracy of quantized LLMs. With the decrease of quantization kernel, the precision of quantized LLMs increases. If the quantization kernel proportion is kept below 19% for OPT models and below 1% for LLaMA models, the precision loss from quantizing activations to INT8 becomes negligible. Motivated by the goal of developing a quantization method with small quantization kernel, we propose CrossQuant\u2014a simple yet effective method for quantizing activations. CrossQuant cross-quantizes elements using row and column-wise absolute maximum vectors, achieving a quantization kernel of approximately 16% for OPT models and less than 0.1% for LLaMA models. Experimental results on LLMs (LLaMA, OPT) ranging from 6.7B to 70B param- eters demonstrate that CrossQuant improves or maintains perplexity and accuracy in language modeling, zero-shot, and few-shot tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, Large Language Models (LLMs) based on the Transformer architecture (Vaswani et al., 2017) have achieved remarkable success across various domains (He et al., 2024; Dubey et al., 2024; GLM et al., 2024), with model sizes reaching billions and even tens of billions of parame- ters. However, these LLMs require substantial computational resources for inference. For instance, running the LLaMA3-70B (Dubey et al., 2024) model demands at least 140GB of RAM in high- precision (FP16). As the size of LLMs continues to grow, reducing the computational resources required for LLMs inference has become a critical challenge. Quantization, a compression tech- nique, addresses this by reducing model parameters from high-precision floating points (FP16) to low-precision integers (e.g., 8-bit integers, INT8), significantly reducing GPU requirements. To ef- fectively scale larger models on a limited number of devices, it is essential to quantize both weights and activations while utilizing the fewest possible bits, all without compromising accuracy.\nPost-Training Quantization (PTQ) compresses LLMs directly without the need for retraining, and can be further divided into two subgroups based on whether activations are quantized: weight- only quantization (Lin et al., 2024; Frantar et al., 2022; Kim et al., 2023) and weight-activation quantization (Xiao et al., 2023; Shao et al., 2024; Yao et al., 2022). There are two widely used methods for quantizing both weights and activations: Per-channel quantization (Liu et al., 2023)"}, {"title": "2 RELATED WORKS", "content": "Post-Training Quantization (PTQ) of LLM can be divided into two categories: weight-only quan- tization and weight-activation quantization, based on whether activations are quantized or not (Zhu et al., 2024).\nWeight-only Quantization, is a method only quantize weights into low-bit integers like INT3 or INT4 with keeping activations in FP16, denoted as W3A16 or W4A16. GPTQ (Frantar et al., 2022) quantizes each channel through iterative refinement, concurrently optimizing the unquantified weights to mitigate and compensate for the difference introduced by the quantization process. AWQ (Lin et al., 2024) focuses on identifying and protecting a small fraction of salient weights based on activation importance. SqueezeLLM (Kim et al., 2023) preserves sensitive weights through a sensitivity-based non-uniform quantization and Dense-and-Sparse decomposition. On devices with limited computing resources, such as mobile devices, quantizing weights alone is insufficient; acti- vation also needs to be quantized.\nWeight-activation Quantization, quantizes both weights and activations into low-bit values, such as W8A8 (INT8 for both weights and activations). ZeroQuant (Yao et al., 2022) employs both group-wise and token-wise quantization strategies to quantize weights and activations to INT8, marking it as the first deployment of a weight-activation quantization method. Most subsequent works attribute the decrease in quantization accuracy of activations to outliers (Wei et al. (2022);\nZou et al. (2024)), which have large-magnitude values and emerge in the activations of models with over 6.7B parameters. LLM.int8() (Dettmers et al., 2022) utilizes a mixed-precision quantiza-"}, {"title": "3 BACKGROUND", "content": "Given a linear layer in Transformers (Vaswani et al., 2017), it computes $Y = X \\cdot W$, where $X \\in \\mathbb{R}^{T\\times I}$ and $W \\in \\mathbb{R}^{I \\times O}$. {T, I, O} indicates {number of tokens, input channels, output channels} respectively. Initial X and W are composed of elements of FP16, and the quantized Q(X) and Q(W) are composed of elements of INT4 or INT8.\nQuantizing activations optimizes model performance by lowering the precision of activation values, which accelerates inference for real-time applications. This reduction decreases the memory foot- print required for activations, improving computational efficiency (Shen et al., 2023). Quantizing weights also minimize memory usage, allowing larger models to fit within the constraints of edge devices, and decreases data transfer, further reducing latency and energy consumption (Lin et al., 2024). Together, weight and activation quantization enable the deployment of sophisticated models in resource-constrained environments, enhancing the practicality of LLMs applications.\nPer-token quantization, the quantization unit of it is one row and it linearly maps X to integers within the range [\u22122\ud835\udc41\u22121 \u2013 1,2\ud835\udc41-1 \u2013 1], which can be represented as:\n$Q(X_{i,j}) = round(\\frac{X_{i,j}}{\\Delta^t_{i,j}}), \\Delta^t_{i,j} = \\frac{2^{N-1}-1}{\\max(X_{i,:})}, X_{i,j} \\in X$\nwhere \u2206\ud835\udc61 is the element with the maximum absolute value \ud835\udc61\ud835\udc56 = max(|\ud835\udc4b\ud835\udc56,:|) in the i-th row of X. Since the absolute value of outliers is large (at least 20x larger than the other elements), \ud835\udc61\ud835\udc56 is a large value, resulting in many \ud835\udc4b\ud835\udc56,\ud835\udc57 are being rounded to zero after dividing by \u2206\ud835\udc61\ud835\udc56, resulting in a large number of elements in the quantization kernel. Our CrossQuant reduces the quantization kernel by reducing \u2206\ud835\udc61 so that \ud835\udc4b/\u2206\ud835\udc61 is no longer zero after round().\nPer-channel quantization is a widely used method for quantizing weights, utilizing the maximum absolute value in the i-th row of W to do quantization:\n$Q(W_{i,j}) = round(\\frac{W_{i,j}}{\\Delta^w_{i,j}}),  \\Delta^w_{i,j} =  \\frac{\\max(W_{i,:})}{2^{N-1}-1}, W_{i,j} \\in W$\nIt is also worth introducing that group-wise quantization is a widely used technique for quantiz- ing weight, which uses smaller channels to achieve higher precision (Shen et al. (2020);Yao et al. (2022);Lin et al. (2024)). It reshapes $W \\in \\mathbb{R}^{I\\times O}$ to $W\\in \\mathbb{R}^{R \\times g}$ first and then does quantization.\nMany of our experiments are based on W4A8-g128 (group size g is 128) because we want to provide a reference for these weight-only quantization methods using group-wise."}, {"title": "4 METHOD", "content": "In section 4.1, we give the definition of the quantization kernel. We propose CrossQuant in sec- tion 4.2 and demonstrate that CrossQuant has smaller quantization kernel compared to Per-token quantization. The remainder of our findings is presented in section 4.3."}, {"title": "4.1 QUANTIZATION KERNEL", "content": "In linear algebra, the kernel of a linear map is the part of the domain which is mapped to the zero vector of the co-domain. In this paper, we extend kernel to quantization function to help us define the elements quantized to zero"}, {"title": "4.2 CROSSQUANT", "content": "The CrossQuant function can be expressed as following:\n$CQ(X_{i,j}) = round(\\frac{X_{i,j}}{\\Delta^{tc}_{i,j}})= \\frac{t^{1-\\alpha}_{i}c^{-\\alpha}_{j}}{2^{N-1}-1}, X_{i,j} \\in X$\nwhere \ud835\udc50\ud835\udc57 = max(|\ud835\udc4b:,\ud835\udc57|) is the maximum absolute value in the j-th column. The hyperparameter \u03b1 within the range 0 \u2264 \u03b1 \u2264 1, serves as the exponent component in both \ud835\udc61\ud835\udc56 and \ud835\udc50\ud835\udc57, 1 \u2013 \u03b1 is utilized to maintain normalization. The relationship between the variation of \u03b1 and the accuracy of the model is discussed in section 5.\nCompared to Per-channel quantization, we introduce a vector of \u201ccolumn maximum values\" along- side the vector of \u201crow maximum values\", collectively cross-quantizing activations. In fact, the quantization unit of CrossQuant is a single element (each element has a different \u2206), but does not significantly increase the storage cost with the large increase in quantization accuracy. CrossQuant only stores one extra vector of length I compared with Per-token quantization. About time complex- ity, \ud835\udc4b\ud835\udc56,\ud835\udc57 requires one extra division compared to Per-token quantization, but the time complexity is still O(TI).\nAccording to the Definition 1, the kernel of CQ is $K(CQ) = {X_{i,j} \\in X | CQ(X_{i,j}) = 0} =  {X_{i,j}  X_{i,j} \\in X | X_{i,j} < B_{i,j}})$, where \ud835\udc35\ud835\udc56,\ud835\udc57 = 0.5 \u00d7 \u2206\ud835\udc61\ud835\udc50. The results of CrossQuant in this section are all based on \u03b1 = 0.15. We calculate the proportion of kernels caused by two methods relative to the total number of elements in the activation matrix. For OPT family models with parameters \u22652.3B, the proportion of Per-token quantization kernels experiences a sharp increase (from 16% to 35%) and remains high (between 40% and 55%). In contrast, CrossQuant consistently maintains a low percentage (around 16%). For LLaMA family models, the proportion of Per-token quantization kernels remains low, at approximately 11%, with CrossQuant kernels representing a negligible proportion (<0.1%).\nThere are a lot of statistical data in Figure 4, but it cannot reflect the corresponding relationship between the quantization kernel of different percentages and the accuracy of the models, so we"}, {"title": "4.3 DETERMINE THE THRESHOLD OF THE QUANTIZATION KERNEL", "content": "We now provide a theoretical analysis showing that \ud835\udc3e(\ud835\udc36\ud835\udc44) is smaller than K(Q). Given the acti- vation matrix X, the elements in X are invariant. Therefore, proving \ud835\udc3e(CQ) is smaller than K(Q) only depends on proving \ud835\udc35\ud835\udc56,\ud835\udc57 < \ud835\udc35\ud835\udc56,\ud835\udc57. For convenience, we write max(|\ud835\udc4b\ud835\udc56,:|) as ti, max(|\ud835\udc4b:,\ud835\udc57) as cj. There are two cases for \ud835\udc50\ud835\udc57 and \ud835\udc61\ud835\udc56 are discussed:\nI: \ud835\udc50\ud835\udc57 <\ud835\udc61\ud835\udc56\nScaling the original formula as $t_i^{1-\\alpha}c_j^{-\\alpha}$ <ti, from $ \\Delta^t_i$\" < \ud835\udc35\ud835\udc56,\ud835\udc57\nII: \ud835\udc50\ud835\udc57 \u2265 \ud835\udc61\ud835\udc56\nCase II will lead to \ud835\udc35\ud835\udc56,\ud835\udc57 > \ud835\udc35\ud835\udc56,\ud835\udc57, but it actually takes a small proportion (about 3%) in the whole matrix, as shown in Table 1. When \u03b1 = 0.75, the proportion of \ud835\udc35\ud835\udc56,\ud835\udc57 < \ud835\udc35\ud835\udc56,\ud835\udc57 is the highest, but the result is not the best, because the average proportion of quantization kernel does not the lowest.\""}, {"title": "5 EXPERIMENTS", "content": "Models. We implemented CrossQuant on the LLaMA family models (2-7B, 2-13B, 1-30B, 3-8B, 3-70B) (Touvron et al., 2023a;b; Dubey et al., 2024) and the OPT family models (1.3B, 2.3B, 6.7B, 13B, 30B, 66B) (Zhang et al., 2022).\nBaselines. Both weight-activation quantization and weight-only quantization are chose as base- lines. For weight-activate quantization, we compare CrossQuant with Per-token quantization, SmoothQuant (Xiao et al., 2023) and OmniQuant (Shao et al., 2024). For weight-only quantiza- tion, we select AWQ (Lin et al., 2024) with activations quantized by Per-token quantization. The weights quantizaiton method for CrossQuant is Per-channel quantization.\nEvaluation. Quantized models are evaluated on language modeling experiments, zero-shot and few- shot tasks. Language modeling experiments include WikiText2 (Merity et al., 2016) and C4 (Raffel et al., 2019); zero-shot tasks include Lambada (Paperno et al., 2016), ARC-easy (Clark et al., 2018), PIQA (Lourie et al., 2021), Hellaswag (Zellers et al., 2019) and BoolQ (Clark et al., 2019); the few-shot task is MMLU (Hendrycks et al., 2021) with five-shots."}, {"title": "5.2 LANGUAGE MODELING TASKS", "content": "Table 2 contains three groups of experiments to test the perplexity of CrossQuant on language mod- eling task. Meanwhile, we check the combination effect of CrossQuant with AWQ. For the first group (W8A8), CrossQuant demonstrates slightly better performance on the 7B and 13B models compared to SmoothQuant. In the second group, CrossQuant matches the performance of AWQ, with 4 wins and 2 losses. Notably, CrossQuant+AWQ achieves improved perplexity, indicating that CrossQuant can be effectively combined with weight-only methods for better results. In the last group, CrossQuant reduced perplexity by 4.8%-56.38% compared to OmniQuant."}, {"title": "5.3 ZERO-SHOT TASKS", "content": "The all results in Table 3 are obtained by the 1m-eval-harness\u00b9. CrossQuant closely matches the FP16 accuracy on both W8A8 and W4A8-g128, performing slightly better than SmoothQuant on"}, {"title": "5.4 ABLATION STUDY", "content": "In ablation study, we mainly explore the influence of different values of \u03b1 on perplexity and accuracy. As shown in Figure 8, on the Lambada dataset, OPT-6.7B has a qualitative leap in accuracy (from 43% up to 80%) and reaches the optimal value at \u03b1 = 0.55. On WikiText2, perplexity drops significantly (from 6.99 to bleow 5.09) after \u03b1 \u2264 0.95, and the optimal value is obtained at \u03b1 = 0.15.\nThen we test CrossQuant's performance on LLaMA3-8B and 3-70B, see Table 4. CrossQuant has a wide but narrow lead over SmoothQuant on \u03b1 = 0.15, a partial advantage on \u03b1 = 0.45 and"}, {"title": "6 LIMITATIONS AND FUTURE WORK", "content": "Our work has the following limitations, which are also our future research directions:\n\u2022 Although we have identified that the decrease in quantization accuracy is caused by quan- tization kernels, this conclusion is based on experimental results. We have not yet fully explored the specific information contained in undershoots that necessitates their preser- vation during quantization. In future work, we will continue to investigate the missing features during quantization inference.\n\u2022 The OPT-2.3B model exhibits a high proportion of kernels yet still performs well after quantization. We hypothesize that this is related to the scale of the model's parameters. In future research, we will examine why smaller models can tolerate a high percentage (30%) of quantization kernels."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduce the concept of \"quantization kernel\", defined as the set of elements in activations that are quantized to zero. Investigating the primary reason for the decline in precision in quantized LLMs, we find that this decline is attributable to the quantization kernel being mapped to zero. Through quantitative experiments, we establish that the quantization kernels need to be reduced to a certain proportion to maintain the accuracy of quantized LLMs, specifically 19% for OPT models and 1% for LLaMA models. CrossQuant, a weight-activation method designed to minimize the quantization kernels, outperforms the baselines in language modeling, zero-shot, and few-shot tasks, with its quantization kernels of 16% for OPT models and <1% for LLaMA models.\nOur method provides a new analytical perspective to better understand and deconstruct quantization loss, and we hope our findings inspire further valuable research in the field of quantization."}, {"title": "A OUTLIERS IN TRANSFORMER", "content": "Sparse but systematically large-magnitude outliers (only 0.1% of all input features but are at least 20x larger than the other values (Dettmers et al., 2022)) significantly emerge in activations of Large Language Models (LLMs) with over 6.7B parameters. Many previous works contribute the decline of quantized models accuracy to outliers. We think that the decrease of quantization accuracy caused by outliers is mainly due to the excessive quantization kernel. As shown in Per-token quantization function:\n$Q(X_{i,j}) = round(\\frac{X_{i,j}}{\\Delta^t_{i,j}}), \\Delta^t_{i,j} =  \\frac{t_{i}}{2^{N-1}-1}= \\frac{\\max(X_{i,:})}{2^{N-1}-1}, \\forall X_{i,j} \\in X$\noutliers make the \ud835\udc61\ud835\udc56 become large, and \ud835\udc4b\ud835\udc56,\ud835\udc57 is a small value which would be rounded to zero after dividing by \ud835\udc61\ud835\udc56. Thus outlier leads to the large size of quantization kernel. To avoid this problem, the direct way is let \u2206\ud835\udc61\ud835\udc56 be smaller, so our CrossQuant is proposed around this idea. There are also many previous studies have examined the causes of outliers and their relationship to perfor- mance degradation in quantized models (Gao et al., 2019; Timkey & van Schijndel, 2021; Dettmers et al., 2022). Kovaleva et al. (2021) found that the emergence of outliers in the BERT model family is linked to the normalization process of LayerNorm. Additionally, Puccetti et al. (2022) demon- strated through experiments that the appearance of outliers is related to the frequency of tokens in the training distribution. Devlin et al. (2019) introduced novel quantization schemes, such as"}, {"title": "B SUPPLEMENTARY MATERIAL FOR EXPERIMENTS", "content": "Per-embedding-group Quantization for BERT, which addresses the issue of quantized models dis- proportionately focusing on special tokens.\nThe existing works have studied outliers in detail, and we analyze quantization loss from quantiza- tion kernel."}, {"title": "B.1 EXPERIMENTS SETTINGS", "content": "We deploy all of our experiments on RTX 4090 except OmniQuant. For each baseline, the specifics of our implementation are:\nSmooth Quant. We generate smooth scales from SmoothQuang's open source code2 and use its fake_quant.py to get all results. The smooth factor \u03b1 for LLaMA and OPT is 0.8 and 0.5 respec- tively (follow the settings in its demo).\nAWQ. We generate AWQ search results by its open source code\u00b3. Since AWQ is a weight-only method, we deploy fake quant (Per-token quantizaiton) on activations during inference. And AWQ's code only supports group-wise quatization for weights with size 128 and 64, we choose g128, that is W4A8-g128. For W4A4, CrossQuant shows significant decline but are still better than baselines.\nOmniQuant. OmniQuant is a weight-activation method but needs extra training to generate Om- niQuant parameters. We test both our own generated OmniQuant parameters and OpenGVLab's"}, {"title": "B.2 ZERO-SHOT EXPERIMENTS", "content": "We supplement CrossQuant's experiments on OPT family models, as shown in Table 5, with data that are also complementary to Figure 1. From the table we can see that Per-token's accuracy drops rapidly when model parameters are \u22656.7B (outliers start to emerge), while crossquant still maintains an accuracy close to that of FP16."}]}