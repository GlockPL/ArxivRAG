{"title": "UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in\nLow-Resource Settings", "authors": ["Layba Fiaz", "Munief Hassan Tahir", "Sana Shams", "Sarmad Hussain"], "abstract": "Multilingual Large Language Models (LLMs)\noften provide suboptimal performance on low-\nresource languages like Urdu. This paper in-\ntroduces UrduLLaMA 1.0, a model derived\nfrom the open-source Llama-3.1-8B-Instruct\narchitecture and continually pre-trained on 128\nmillion Urdu tokens, capturing the rich diver-\nsity of the language. To enhance instruction-\nfollowing and translation capabilities, we lever-\nage Low-Rank Adaptation (LoRA) to fine tune\nthe model on 41,000 Urdu instructions and ap-\nproximately 50,000 English-Urdu translation\npairs. Evaluation across three machine trans-\nlation datasets demonstrates significant perfor-\nmance improvements compared to state-of-the-\nart (SOTA) models, establishing a new bench-\nmark for Urdu LLMs. These findings under-\nscore the potential of targeted adaptation strate-\ngies with limited data and computational re-\nsources to address the unique challenges of\nlow-resource languages.", "sections": [{"title": "Introduction", "content": "The field of language modeling has experienced a\ntransformative shift, driven by the rapid evolution\nof Large Language Models (LLMs) that have set\nnew standards in natural language understanding\nand generation. While proprietary models like \u041e\u0440\u0435-\nnAI's ChatGPT (OpenAI, 2022) offer impressive\ncapabilities, their closed nature restricts research\naccessibility. On the other hand, open models\nsuch as LLaMA(Grattafiori et al., 2024) and Mis-\ntral(Jiang et al., 2023) \u2014 though smaller in scale\nhave achieved competitive results across many lan-\nguages. Nevertheless, both open and closed LLMs\nface significant challenges when applied to low-\nresource languages like Urdu. A primary hurdle\nis the inadequate representation of Urdu in train-\ning data, which results in limited vocabulary and\npoor encoding capabilities. This gap in data inclu-\nsion severely hampers the performance of LLMs on\nUrdu NLP tasks, as evidenced by recent benchmark\nstudies(Arif et al., 2024; Tahir et al., 2025). Over-\ncoming this limitation is crucial to unlocking the\nfull potential of LLMs for Urdu and other underrep-\nresented languages. In this research, we tackle this\nchallenge by developing an Urdu-specific LLM.\nWe begin by continually pretraining Llama-3.1-8B-\nInstruct (Grattafiori et al., 2024) on 128 million\nUrdu tokens to enhance the model's foundational\nrepresentation of the language. This is followed by\ninstruction fine tuning using 41,000 instructions to\nimprove conversational capabilities, and additional\nfine tuning on 50,369 English\u2013Urdu parallel sen-\ntence pairs to boost translation proficiency. The\ntranslation quality was evaluated using the BLEU\nscore across three MT datasets, which showed\nthat the UrduLLaMA 1.0 model outperformed the\nLlama-3.1-8B-Instruct base model. This trend was\nfurther validated through human evaluation, where\ntwo experts found that translations from the UrduL-\nLaMA 1.0 model were more accurate than those\ngenerated by the base model.\nThe paper is structured as follows: Section 1\nintroduces the study and Section 2 presents the\nrelated work. Section 3 details the dataset curation\nand Section 4 explains the steps taken to preprocess\nthe data. This is followed by Section 5, which\ndescribes the development process of UrduLLaMA\n1.0 including the experimental and training details,\nand Section 6, which covers the evaluation and\ndiscussion leading to Section 7, which concludes\nthe paper."}, {"title": "Related Work", "content": "Due to the lack of continual pretraining work on\nLLMs for Urdu, this section examines the closest\nrelated works. This includes models developed for\nAsian languages and low-resource languages built\nusing the LLaMA framework.\nTamil-Llama (Balachandran, 2023), an Asian\nlanguage model built on LLaMA 2 (Touvron et al.,\n2023), incorporates 16,000 Tamil tokens and uti-\nlizes the Low-Rank Adaptation (LoRA) (Hu et al.,\n2021) technique for efficient training on Tamil\ndatasets. The model was trained on an Nvidia\nA100 GPU with 80GB of VRAM for 48 hours, fol-\nlowed by instruct fine tuning on translated Alpaca\ndatasets (Taori et al., 2023) and a custom subset\nof the OpenOrca (Lian et al., 2023) dataset for 60\nhours using Microsoft Azure's Standard NC24 ads\nA100v4 instance. Performance evaluations indicate\nsignificant improvements in Tamil text generation,\nwith the Tamil-Llama 13B model outperforming\nOpenAI's GPT-3.5-turbo on Tamil language tasks.\nTaiwan-LLM (Lin and Chen, 2023), an LLM\nfor Traditional Chinese, underwent continual pre-\ntraining on LLaMA 2 (Touvron et al., 2023) us-\ning 35.1 billion tokens and a diverse instruction\nset derived from 17 fine tuning datasets, includ-\ning 20,000 user feedback instances. The training\nprocess leveraged the Transformer Reinforcement\nLearning (TRL) library (Hu et al., 2023), along\nwith DeepSpeed ZeRO-2 (Rajbhandari et al., 2020)\nand FlashAttention-2 (Dao, 2023) to optimize mem-\nory usage and enhance training efficiency. Utiliz-\ning up to 48 NVIDIA H100 Tensor Core GPUs,\nTaiwan-LLM demonstrated superior performance\nin understanding and generating Traditional Chi-\nnese text, surpassing models such as GPT-4 and\nClaude-2.1.\nPersianLLaMA (Abbasi et al., 2023), the first\nlarge-scale Persian language model, was trained\nfrom scratch on 184 million tokens from Persian\nWikipedia and 9 billion tokens from the OSCAR\ndataset (Ortiz Su\u00e1rez et al., 2020). The training\nprocess leveraged DeepSpeed (Rasley et al., 2020)\nand TencentPretrain (Zhao et al., 2023), two ad-\nvanced frameworks for optimizing deep learning,\nutilizing two A100 GPUs with 80GB of VRAM\nover 12 days. Additionally, they conducted an ex-\nperiment using LoRA (Hu et al., 2021) with the\noriginal English LLaMA weights, training on a\nsingle A100 GPU with 80GB of VRAM for over\n70 hours. Their evaluations indicate that PersianL-\nLaMA significantly outperformed its competitors\nin both understanding and generating Persian text.\nAiravata (Gala et al., 2024) is an instruction-\ntuned model for Hindi, built by fine tuning Open-\nHathi (SarvamAI, 2023), on 404k instruction\ninstances from diverse Hindi instruction-tuning\ndatasets. OpenHathi (SarvamAI, 2023) is again\na model built on the LLaMA 2 7B architecture.\nThe training employed both full fine tuning and su-\npervised fine tuning using LoRA (Hu et al., 2021).\nTheir results demonstrated that Airavata signifi-\ncantly outperforms OpenHathi on most tasks, high-\nlighting the effectiveness of fine tuning in aligning\nthe base model to a variety of tasks. The details\nregarding their training infrastructure were not pro-\nvided in their paper.\nSeaLLMs (Nguyen et al., 2024b) is an innova-\ntive series of language models focused on South-\neast Asian (SEA) languages. Built upon LLaMA 2\n(Touvron et al., 2023) and Mistral 7B (Jiang et al.,\n2023), SeaLLMs underwent continued pretraining\nwith an extended vocabulary, followed by a hy-\nbrid approach for instruction and alignment tun-\ning. Their evaluation claims that SeaLLMs signifi-\ncantly outperform ChatGPT-3.5 in non-Latin lan-\nguages, such as Thai, Khmer, Lao, and Burmese,\nby large margins, while remaining lightweight and\ncost-effective to operate. The authors did not pro-\nvide detailed information regarding the training\ninfrastructure in their paper.\nA study related to Chinese LLaMA (Cui et al.,\n2024) extended different variants of LLaMA 2\n(Touvron et al., 2023) by adding 20,000 Chinese\ntokens to the existing vocabulary. The model was\npre-trained using LoRA (Hu et al., 2021) and fine\ntuned on Chinese instruction datasets formatted ac-\ncording to Alpaca (Taori et al., 2023). Training\nwas conducted on A40 GPUs (48GB VRAM), with\nup to 48 GPUs used depending on the model size.\nThe parameter-efficient training with LoRA was\ncarried out using the PEFT library\u00b9. Additionally,\nDeepSpeed (Rasley et al., 2020) was employed to\noptimize memory efficiency during training. Ex-\nperimental results demonstrate that the newly pro-\nposed model significantly improves the original\nLLaMA's ability to understand and generate Chi-\nnese content.\nAnother study (Chen et al., 2024b) conducted a\ntwo-stage continual pretraining of LLaMA 3 8B\n(Grattafiori et al., 2024) for Chinese. Initially, they\nperformed experiments on TinyLLaMA (Zhang\net al., 2024), and then applied their findings to train\nLLaMA 3 using 100B tokens, followed by fine\ntuning on Synthetic Scientific QA Data. The ex-\nperiments were implemented using Hugging Face\nTransformers (Wolf et al., 2020), incorporating\nFlash Attention and DeepSpeed ZeRO (Rasley\net al., 2020) to optimize training efficiency. The\nstudy leveraged computing resources provided by"}, {"title": "Dataset Curation", "content": "A pivitol challenge for building LLMs, particu-\nlarly in low resource languages, is the availability\nof sizeable high-quality data for building founda-\ntion LLMs. As the quality and diversity of data\nsignificantly influence the capabilities of LLMs\n(Chen et al., 2024a), we supplemented our in-house\ndataset with data from several publicly available\nsources, including CC-100 (Wenzek et al., 2020),\nthe Urdu corpus from OSCAR (Ortiz Su\u00e1rez et al.,\n2020), the Urdu Web Corpus (Shafiq et al., 2020),\nUrdu data from XLSum (Hasan et al., 2021), and\n(Goldhahn et al., 2012). The raw text underwent\na comprehensive pre-processing pipeline, outlined\nin Section 4, to ensure language-specific content,\nmaintain quality, and remove duplicates. A sum-\nmary of the collected datasets and the impact of\nprocessing is provided in Table 1."}, {"title": "Preprocessing Pipeline", "content": "This section outlines the pre-processing steps ap-\nplied to construct our dataset. We primarily\nadopted approaches similar to those proposed by\n(Zeng et al., 2021), (Ennen et al., 2023), and (Lu\net al., 2024). The steps are as follows:\n\u2022 Language Filtering: This step was performed\nat the document level to retain only language-\nrich documents. Similar approaches have been\nadopted by others, such as the Falcon team\n(Penedo et al., 2023) for creating RefinedWeb,\nwhere they used the fastText language classifier\nfrom CCNet (Wenzek et al., 2020) at the docu-\nment level. This method has also been utilized by\n(Nguyen et al., 2024a) for building CulturaX and\nby (Lauren\u00e7on et al., 2022) for constructing the\nBigScience ROOTS corpus. In our case, we con-\nducted language filtering using the CLE Urdu Lan-\nguage Identification API2, applying a threshold of\n0.9 to ensure the retention of predominantly Urdu\ndocuments. To validate our choice of the CLE\nUrdu Language Identification API, we conducted\nexperiments comparing it with fastText, assess-\ning the effectiveness of both in identifying Urdu\ncontent within documents containing varying pro-\nportions of Urdu and non-Urdu text. The results of\nwhich are summarized in Table 2, demonstrating\nthat the CLE Urdu Language Identification API\nprovided scores more aligned with the expected\ncomposition of test data.\n\u2022 Data Standardization: Data standardization in-\nvolves the normalization and transformation of\ntext data to make it more manageable and com-\nprehensible during the model training process (Lu\net al., 2024). Since syntax of the Urdu language\nrequires specialized techniques, we applied as de-\nscribed in (Nazir et al., 2024). Major steps include\nUnicode-based filtering, replacing non-standard\ncharacters with their standard forms, and handling\nUrdu-specific features such as end symbols, po-\netic symbols, and quotation marks. Additionally,\nsome documents had varying lengths, so we split\nthe text to maintain an average context length of\n512 tokens.\n\u2022 Quality Filtering: To enhance the dataset's qual-\nity, motivated by the data processing pipeline\nfrom (Lauren\u00e7on et al., 2022) and (Nguyen et al.,\n2024a), we utilized various dataset metrics to iden-\ntify and filter outlying documents. Filtering was\napplied based on stopword ratios, flagged word ra-\ntios, and empty documents. The threshold values\nfor the stopword ratio and flagged word ratio were\nset at 0.1 and 0.025, respectively, which align\nwith the threshold values used in the BigScience\nROOTS project (Lauren\u00e7on et al., 2022).\nIn addition to filtering, we implemented Person-\nally Identifiable Information (PII) removal to pro-\ntect sensitive data. We employed rule-based ap-\nproach leveraging regular expressions regexes li-\nbrary to detect and remove sensitive information\nsuch as phone numbers, identification numbers,\nand email addresses. These measures ensured that\nthe dataset was free from personally identifiable\ninformation, enhancing privacy and usability for\nmodel training.\n\u2022 Deduplication:\nDespite thorough data cleaning, the remaining,\ndataset still contain a substantial amount of re-\npeated data due to various reasons, including in-\nformation being reposted on the web, multiple\nreferences to the same articles and plagiarism.\nThe duplicated data can thus cause memorization\nand significantly hinder generalization for LLMs\n(Lee et al., 2022). Therefore deduplication is re-\nquired as it decreases memorization of training\ndata (Kandpal et al., 2022). Initially, dedupli-\ncation was performed within individual datasets,\nfollowed by an overall deduplication across all\ndatasets to address potential similarities among\ndifferent sources. We applied deduplication at two"}, {"title": "UrduLLaMA 1.0", "content": "Llama-3.1-8B-Instruct, as introduced in\n(Grattafiori et al., 2024) by Meta, is built\nupon an extensive pretraining corpus of 15 trillion\ntokens. We leverage this model architecture\nfor continual pretraining due to its open-source\navailability and the inclusion of Urdu language\ndata in its training, making it a suitable choice\nfor our research. The complete process of\nUrduLLaMA 1.0 making is illustrated in Figure\n1, follows four key stages: data collection, data\nprocessing, continual pretraining, and fine tuning,\neach playing a vital role in enhancing the model's\nlinguistic understanding and task adaptability."}, {"title": "Continual Pretraining", "content": "The UrduLLaMA 1.0 model is trained on the\nCausal Language Modeling (CLM) task, enabling\nit to predict and generate the next word in a se-\nquence. This stage plays a crucial role in refin-\ning LLaMA's proficiency in Urdu by allowing the\nmodel to grasp the language's intricate syntactic\nstructures, semantic nuances, and unique linguistic\ntraits. Leveraging its autoregressive nature, CLM\nmirrors the human process of language comprehen-\nsion and generation, which is inherently context-\ndependent. Consequently, by the end of this ini-\ntial training phase, LLaMA acquires the ability to\ngenerate and interpret Urdu text with contextual\nrelevance and linguistic accuracy."}, {"title": "Pretraining Dataset", "content": "Due to hardware limitations, 128 million tokens\nwere used for continual pretraining of Llama-3.1-\n8B-Instruct (Grattafiori et al., 2024) from the cu-\nrated dataset as explained in Section 3."}, {"title": "Pretraining Setup", "content": "The foundational model of UrduLLaMA 1.0 is ini-\ntialized with the original Llama-3.1-8B-Instruct\nweights and want through pretraining using the\nbf16 precision setting. Our pretraining strategy in-\nvolved full fine tuning, where all model parameters,\nincluding embeddings, LM heads, and attention\nweights, were trained. The training utilized an\nAdamW optimizer with a learning rate of 2e-5, and\ngradient accumulation steps of 1. Memory opti-\nmization techniques such as activation checkpoint-\ning, activation offloading, and memory-efficient\nFSDP wrapping were applied to manage the large\nmodel size effectively. Pretraining was conducted\nby utilizing 3 Nvidia L40 48GB GPUs. The model\nwas trained for 3 epochs on the dataset, with the\ntraining process spanning approximately 2 weeks.\nThe detailed parameters are described in Table 4."}, {"title": "Instruct Tuning", "content": "Language models pre-trained using the causal lan-\nguage modeling (CLM) objective often struggle\nto follow user instructions and sometimes gener-\nate irrelevant or unintended content (Balachandran,\n2023). This limitation arises because the CLM\nobjective is designed to predict the next token in\na sequence rather than understand or respond to\ninstructions effectively (Ouyang et al., 2022). \u03a4\u03bf\naddress this issue and align the model's behavior\nwith user intentions, we employed instruction fine\ntuning. This step refines the LLM's capabilities,\nallowing it to interpret and execute task-specific\ninstructions more effectively in natural language.\nRather than the traditional approach of adapting\nto specific datasets, instruction fine tuning focuses\non a wide array of tasks articulated through lan-\nguage, ensuring the LLM's adaptability without\ntask-specific alterations."}, {"title": "Instruct Tuning Dataset", "content": "We instruct tuned our model using 41,000 instruc-\ntions, sources from two main sources. The first\nsource included 26,000 instances from (Khalil,\n2024), a cleaned and translated version of the Stan-\nford Alpaca dataset (Ruebsamen, 2024). This re-\nfined dataset addresses key issues such as hallu-\ncination, responses, ambiguous instructions, and\nlow-quality samples, ensuring higher-quality train-\ning data. The second source comprises 15,000\ntranslated instances from the Dolly dataset (Saeed,\n2023), an Urdu translation of the original Dolly\ndataset (Conover et al., 2023), covering a wide\nrange of NLP tasks related instructions."}, {"title": "Instruct Tuning Setup", "content": "For the instruction fine tuning, we incorporated\nthe LORA method (Hu et al., 2021), where we inte-\ngrated LoRA adapters into the attention vectors and\nsubsequently trained the embeddings, LM heads,\nand the newly incorporated LoRA parameters. For\nthe training infrastructure, we utilized Nvidia A100\n40GB GPU. The detailed hyperparameters used for\ninstructional fine tuning are listed in Table 5."}, {"title": "Fine Tuning on Machine Translation Task", "content": "fine tuning is performed to adapt the pre-trained\nmodel, which has already learned general patterns\nand representations to specific task. Instead of\nfocusing solely on specific linguistic pairs, fine tun-\ning on machine translation datasets equipped our\nUrduLLaMA 1.0 to understand and translate text\nin a more sophisticated way, addressing domain-\nspecific challenges. This process ensured that\nthe UrduLLaMA 1.0 can effectively adapt to var-\nied translation tasks, such as handling language-\nspecific syntax, idiomatic expressions, and cultural\nnuances.\nWe fine tuned the model using an in-house Ma-\nchine Translation MT Corpus. This dataset is col-"}, {"title": "Evaluation on Machine Translation\nTask", "content": "For automatic evaluation, we tested the model on a\ntotal of 16,299 instances, comprising test splits\nfrom the inhouse MT corpus, TICO-19 (Anas-\ntasopoulos et al., 2020), and the Tatoeba Chal-\nlenge (Tiedemann, 2020). The TICO-19 dataset fo-\ncuses on COVID-19-related domains such as health\nand public awareness, while the Tatoeba Chal-\nlenge spans diverse domains for general-purpose\ntranslations. Specifically, the evaluation included\n12,594 instances from the in-house test set, 2,042\ninstances from TICO-19, and 1,663 instances from\nthe Tatoeba Challenge. This consistent test split\nwas designed to align with SOTA models for a fair\nand reliable comparison.\nTranslation quality was assessed using the BLEU\nscore 8, consistent with the metrics employed in\nSOTA model evaluation, providing an objective\nand standardized measure of the model's perfor-\nmance on Urdu translations. The SOTA models\nevaluated are Llama-3.1-8B-Instruct (Grattafiori\net al., 2024), the base model used for continual pre-\ntraining; seamless-m4t-v2-large (Communication\net al., 2023), a unified multilingual and multimodal\ntranslation model; and opus-mt-en-ur (Tiedemann\nand Thottingal, 2020), a lightweight machine trans-\nlation system tailored for low-resource languages,\nincluding Urdu. The results, showcasing a detailed\ncomparison of translation quality across all models,\nare presented in Table 6.\nThe results of this experiment are also confined\nin Figure 2. The results demonstrate that Ur-\nduLLaMA 1.0 significantly outperforms the base\nLlama-3.1-8B-Instruct model across all datasets,\nhighlighting the effectiveness of our approach in\nimproving translation performance for Urdu. On\nthe In-house dataset, UrduLLaMA 1.0 achieves the\nhighest BLEU score, indicating its superior abil-\nity to handle domain-specific data. This suggests\nthat fine tuning on Urdu text enhances the model's\nability to capture language-specific nuances and\ncontextual meanings. In contrast, Llama-3.1-8B-\nInstruct shows lower performance, reinforcing that\ngeneral-purpose multilingual models require adap-\ntation for better Urdu translation. Interestingly,\nseamless-m4t-v2-large also performed well, though\nnot surpassing UrduLLaMA 1.0, while opus-mt-\nen-ur struggles significantly, likely due to limited\nexposure to the dataset's domain.\nFor the TICO-19 dataset, UrduLLaMA 1.0 again\nsurpassed Llama-3.1-8B-Instruct and opus-mt-en-\nur, demonstrating that fine tuning allows the model\nto generalize better to unseen test sets. However,\nseamless-m4t-v2-large achieved the highest BLEU\nscore, suggesting that massively multilingual pre-\ntrained models remain competitive on general pur-\npose translation tasks and also indicating that pre-\ntraining alone is insufficient for high-quality Urdu\ntranslations.\nIn the Tatoeba Challenge dataset, UrduLLaMA\n1.0 continues to outperform Llama-3.1-8B-Instruct"}, {"title": "Human Evaluation", "content": "For the human evaluation of MT, we conducted\na blind review. Two native Urdu linguists partici-\npated in this evaluation, where each was presented\nwith an English source sentence along with transla-\ntions from the four models, without any indication\nof model identity. They were instructed to select\nthe translation they found most accurate and natu-\nral. A total of 300 test sentences were used with\n100 sentences per dataset were randomly selected\nfor manual evaluation. The preferences of both\nlinguists were then aggregated to compare model\nperformance. This evaluation provides valuable\ninsight into the comparative quality of Urdu MT,\nreflecting the preferences of native Urdu experts in\nassessing translation accuracy and fluency summa-\nrized in Table 7 and interpreted in Figure 3. The\ndifference in trends between human and automatic\nevaluation is due to the larger amount of testing\ndata used in automatic evaluation.\nThe results show that seamless-m4t-v2-large pro-\nvided better translations overall, and it is because\nof its specialized nature for translation and large\namount of multilingual training data. However,\nUrduLLaMA 1.0, while a more general model,\nshowed improvements over its base version in\nTICO-19 and Tatoeba Challenge. This progress\nis promising, as it suggests that with further refine-\nment and more diverse training data, UrduLLaMA\n1.0 has the potential to rival specialized models.\nThe observed improvements in UrduLLaMA 1.0\nare encouraging and highlight its adaptability and\npotential for high-quality translation in broader con-\ntexts."}, {"title": "Conclusion", "content": "In this paper, we introduced UrduLLaMA 1.0, a\nmodel specifically tailored for the Urdu language.\nWe presented a comprehensive data processing\npipeline to curate and prepare high-quality train-\ning data, addressing the challenge of limited pub-\nlicly available Urdu datasets. UrduLLaMA 1.0 was\ncontinuoly pre-trained on a portion of this dataset\nusing the Llama-3.1-8B-Instruct architecture, fol-\nlowed by instruction tuning to enable the model\nto understand and generate responses in a natural\nconversational format. This fine tuning leveraged a\ncombination of the Alpaca and Dolly datasets. Fur-\nthermore, we performed fine tuning on a machine\ntranslation dataset to enhance the model's trans-\nlation capabilities. Our evaluation results demon-\nstrate that UrduLLaMA 1.0 outperforms its base\nmodel, exhibiting substantial improvements in ma-\nchine translation tasks for Urdu. This work repre-\nsents a significant step toward advancing the per-\nformance of LLMs for low-resource languages like\nUrdu and sets a new benchmark for future research\nin this domain."}, {"title": "Limitations", "content": "Our model was trained on a limited portion of the\nUrdu dataset due to computational and cost con-\nstraints. As a result, it exhibits gaps in knowledge,\nparticularly in capturing the nuances of Urdu cul-\nture and literature. While this version serves as\na foundational step, its full potential can only be\nunlocked with access to a more extensive dataset\nto enhance its contextual understanding.\nAdditionally, detoxification processes were not\nincorporated during training, leaving the model un-\ncensored and potentially prone to generating harm-\nful or offensive content, which requires caution\nduring deployment.\nEvaluating LLMs also presents a significant chal-\nlenge, especially for underrepresented languages\nlike Urdu, due to the lack of standardized bench-\nmarks outside the European linguistic domain. Al-\nthough this paper introduces a tailored evaluation\napproach for Urdu machine translation, it remains\nlimited in scope and does not comprehensively as-\nsess the model's performance across diverse appli-\ncations."}, {"title": "Ethics Statement", "content": "This research utilizes publicly available, open-\nsource datasets that do not contain personal or iden-\ntifiable information, ensuring no associated risks.\nAll work and ideas presented are original, with AI\nmodels used solely for grammatical correction and\nwriting enhancement. Proper citations have been\nmade for all models and datasets used. Moreover\nas a generative model, it retains the potential to\ngenerate harmful or offensive content if prompted\ninappropriately, underscoring the need for responsi-\nble usage and careful oversight during deployment."}]}