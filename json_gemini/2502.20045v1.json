{"title": "Text2VDM: Text to Vector Displacement Maps\nfor Expressive and Interactive 3D Sculpting", "authors": ["Hengyu Meng", "Duotun Wang", "Zhijing Shao", "Ligang Liu", "Zeyu Wang"], "abstract": "Professional 3D asset creation often requires diverse\nsculpting brushes to add surface details and geometric\nstructures. Despite recent progress in 3D generation, pro-\nducing reusable sculpting brushes compatible with artists'\nworkflows remains an open and challenging problem. These\nsculpting brushes are typically represented as vector dis-\nplacement maps (VDMs), which existing models cannot\neasily generate compared to natural images. This paper\npresents Text2VDM, a novel framework for text-to-VDM\nbrush generation through the deformation of a dense pla-\nnar mesh guided by score distillation sampling (SDS). The\noriginal SDS loss is designed for generating full objects\nand struggles with generating desirable sub-object struc-\ntures from scratch in brush generation. We refer to this is-\nsue as semantic coupling, which we address by introducing\nclassifier-free guidance (CFG) weighted blending of prompt\ntokens to SDS, resulting in a more accurate target distribu-\ntion and semantic guidance. Experiments demonstrate that\nText2VDM can generate diverse, high-quality VDM brushes\nfor sculpting surface details and geometric structures. Our\ngenerated brushes can be seamlessly integrated into main-\nstream modeling software, enabling various applications\nsuch as mesh stylization and real-time interactive modeling.", "sections": [{"title": "1. Introduction", "content": "Sculpting brushes are essential tools in 3D asset creation,\nas artists often require a variety of brushes to create surface\ndetails and geometric structures. In modeling software, 3D\nsculpting brushes are typically defined as vector displace-\nment maps (VDMs). A VDM is a 2D image where each\npixel stores a 3D displacement vector. Through these vec-\ntors, VDM brushes can create complex surface details, such\nas cracks and wood grain, or generate geometric structures\nlike ears and horns. This allows artists to apply the same\ngeometric pattern iteratively while sculpting.\nDespite significant advances in text-to-image (T2I) [33,\n40] and text-to-3D generation [20, 27, 34, 48, 49], exist-\ning methods are unsuitable for creating VDM brushes. We\nsummarize the challenges as follows: 1) Since VDMs are\nnot natural images (Figure 2), it is difficult for existing T2I\nmodels to generate them directly. 2) From a 3D perspective,\na VDM represents mesh deformation through per-vertex\ndisplacement vectors from a dense planar mesh. Mapping\nany generated mesh to a dense planar mesh to create a VDM\nis non-trivial. 3) Sculpting brushes often involve sub-object\nstructures, whereas most 3D generation methods can only\ngenerate full objects. Enabling users to accurately control\nthe generation of sculpting brush through text descriptions\nin a semantically focused manner remains challenging.\nTo address the challenges of brush generation, we pro-\npose Text2VDM, a novel optimization-based framework\nthat generates diverse and controllable VDM brushes from\ntext input. Our approach does not generate VDMs directly\nfrom a T2I model. Instead, we address VDM brush gen-\neration from a 3D perspective by applying score distilla-\ntion with a pre-trained T2I model to guide mesh deforma-\ntion. Our framework supports three ways to initialize a base\nmesh through a zero-valued, spike-pattern, or user-specified\nVDM for custom shape control. We reparameterize the\nmesh vertices through an implicit formulation based on the\nLaplace-Beltrami operator [32] to achieve high-quality op-\ntimization of mesh deformations. We also provide optional\nregion control using a mask of activated mesh deformation,\nhelping users obtain the intended brush effects. We then ras-\nterize normal maps of the mesh using a differentiable ren-\nderer for brush optimization.\nWe observed that the standard score distillation sampling\n(SDS) [34] can lead to semantic coupling when supervis-\ning the generation of sub-object level structures because of\nassociated semantics noisy gradients of full object. For ex-\nample, a generated tortoiseshell should not be a full tortoise\nwith a head and a tail. A straightforward solution is to use\nnegative prompts [17, 57] to exclude undesired semantics,\nbut our experiments show that this approach is ineffective\nin decoupling semantics and leads to an unstable and more\ntime-consuming optimization process. Instead, we propose\nto enhance the semantics of part-related words by applying\nclassifier-free guidance (CFG) weighted blending to the to-\nkens in the prompt. This results in semantically focused text\nembedding, directing toward a more precise target distribu-\ntion while reducing noisy gradients during optimization."}, {"title": "2. Related Work", "content": "This section reviews previous work related to 3D sculpting\nbrush generation and summarizes the current research gap."}, {"title": "2.1. Text to Local 3D Generation and Editing", "content": "With recent advances in diffusion models [39] and dif-\nferentiable 3D representations [1, 30, 32, 43, 45], many\nmethods for text-guided full 3D model generation have\nemerged [7, 10, 20, 22, 28, 35, 41]. Since 3D content cre-\nation is an iterative process that often requires user interac-\ntion, more attention has been directed toward localized 3D\ngeneration and editing. For example, 3D Highlighter [8]\nand 3D Paintbrush [9] use text as input, leveraging pre-\ntrained CLIP models [38] or diffusion models [34] to su-"}, {"title": "2.2. Diffusion Priors for 3D Generation", "content": "Score distillation sampling (SDS) [34, 51] provides pixel-\nlevel guidance by seeking specific modes in a diffusion\nmodel, inspiring further research to improve optimization-\nbased 3D generation [3, 23, 52, 53, 55]. Some studies fo-\ncus on mitigating the \"Janus\" problem [2, 14], while oth-\ners fine-tune diffusion models with multi-view datasets to\nenhance 3D consistency [25, 44]. Recent research focuses\non refining the design of SDS loss to achieve more precise\nguidance. For instance, Make-it-3D [49] introduces two-\nstage optimizations to improve textured appearance, while\nFantasia3D [7] dynamically modifies the time-dependent\nweighting function within SDS computations. Addition-\nally, several methods [17, 57] incorporate negative prompts\nas the conditional term to further refine the optimizations.\nAlthough diffusion priors have achieved promising results,\ntheir application in generating sub-object structures without\nglobal context as a reference is still challenging."}, {"title": "2.3. Appearance and Geometric Brush Synthesis", "content": "The concept of brushes is very common in the creative pro-\ncess of digital artists, serving as a reusable local decorative\nunit. Appearance brushes focus on color representation and\ndrawing styles in 2D space. With the development of gen-\nerative models [12, 39], many works have explored the syn-\nthesis appearance brushes for interactive painting [15, 46],\nrealistic artworks generation [26, 31, 59], and applying styl-\nization [16, 18]. Unlike appearance brushes, geometric\nbrushes focus on modifying geometry by moving the ver-\ntices of a mesh in 3D space. VDM brushes, as an extension\nof standard geometric brushes, provide more complex geo-\nmetric effects by utilizing VDMs. To the best of our knowl-\nedge, only a few techniques adopted the concepts of VDM\nfor generation [42, 54] and geometric texture transfer [13].\nGenerating geometric brushes that can be used within exist-\ning workflows is still under-explored."}, {"title": "3. Methodology", "content": "Our framework can generate VDM brushes compatible with\nmainstream modeling software. As shown in Figure 3, we\nbegin by constructing a dense mesh Mo from an initial"}, {"title": "3.1. Brush Initialization", "content": "We provide three methods to initialize a base mesh for brush\ngeneration via a zero-valued VDM, a spike-pattern VDM,\nor a user-specified VDM. A VDM is represented as a 512 \u00d7\n512 three-channel image, in which each channel stores the\ndisplacement in the X, Y, or Z direction respectively. We\nfirst construct a planar grid mesh by creating two triangles\nfor every 2\u00d72 pixels and then apply the displacement stored\nin the VDM to mesh vertices. The values in these three\ninitial VDMs range from 0 to 1, in which 0 represents no\ndisplacement, and 1 corresponds to half of the mesh's edge\nlength in the positive axis direction. Since users can apply\nsculpting brushes symmetrically, our initial VDM does not\nneed to store any negative values.\nOur three methods for brush initialization facilitate the\ngeneration of diverse sculpting brush styles. The zero-\nvalued VDM results in a planar mesh, which is our default\nsetup when no control is provided. The spike-pattern VDM\nis suitable for generating protruding geometric structures, as\nit can effectively adjust the Laplacian term in Equation (2)\nto steer the gradient direction for mesh deformation. For\nbetter control of the brush's volume and direction, we also\nprovide an interface for users to create custom VDMs, so\nthe user-specified brush initialization can effectively guide\nmesh deformation toward the target structure."}, {"title": "3.2. Brush Generation via Mesh Deformation", "content": "Given the initialized based mesh, our method aims to learn\na mesh deformation to the target brush shape. The vertex\npositions \u00fb after mesh deformation can be expressed by\n$\\tilde{\\upsilon} = \\underset{\\upsilon}{\\arg \\min} \\mathcal{L}_{ws} (D_c(\\upsilon), y),$ (1)\nwhere c represents the camera setup in a differentiable ren-\nderer D [19]. The loss function $\\mathcal{L}_{ws}$ receives the rendered\nnormal image $D_c(\\upsilon)$ and text input $y$ to evaluate the seman-\ntic guidance, which is detailed in Section 3.3. In mesh de-\nformation strategies, directly applying displacement to each\nvertex often results in unintended self-intersections of mesh\nfaces caused by noisy gradients from pixel-level losses [28].\nTo address it, several works [10, 50] adopt the strategy by\nAigerman et al. [1], parameterizing deformation by Jaco-\nbian fields that capture the scaling and rotation of each mesh\nface. Although this method effectively smooths vertex dis-\nplacements, the local deformation represented in Jacobians\naccumulates, leading to global drifting for open-boundary\nmeshes, making it challenging to bake the mesh as a brush.\nBased on these observations, we reparameterize the ver-\ntex optimization in Equation (1) through an implicit formu-\nlation with a Laplace-Beltrami operator $\\mathcal{L}$, similar to the\napproaches presented by Nicolet et al. [32]:\n$\\upsilon^* = (I + \\lambda \\mathcal{L}) \\upsilon,$ (2)\nwhere $I$ is the identity matrix, $\\lambda$ is a hyperparameter to con-\ntrol the extent of gradient diffusion from a given vertex to\nits neighboring vertices. When $\\lambda = 0$, this representation\ndegrades to direct vertex displacements. As $\\lambda$ increases, the\nmesh deforms toward more global structural changes. In\nour experiments, we set $\\lambda = 15$ to balance the global struc-\nture and fine details during mesh deformation (Figure 4).\nAdditionally, we provide a region mask to restrict mesh de-\nformation to the user-defined region during optimization.\nBy adjusting the activation ratio of the region mask, the final\nbrush effect can effectively match the user's guidance. For\ninstance, our experiment activated the region mask for the\nfirst half of total iterations as a warm-up stage to effectively\ncontrol the shape of the surface detail (Figure 8). This dif-\nferentiable parameterization effectively alters the gradient\npropagation at each optimization step as:\n$\\upsilon^* \\leftarrow \\upsilon^* - \\eta (I + \\lambda \\mathcal{L})^{-1} \\frac{\\partial \\mathcal{L}_{ws}}{\\partial \\upsilon^*},$ (3)\nwhere $\\eta$ is the learning rate. The advantage of this Lapla-\ncian energy-aware mesh deformation is that it enhances the\nrobustness of optimization for non-convex objective func-\ntions. The resulting mesh preserves the original structure\nwhile incorporating rich local deformations, making it well-\nsuited for baking as a brush."}, {"title": "3.3. CFG-Weighted Score Distillation Sampling", "content": "Current text-to-3D generation methods like DreamFu-\nsion [34] often optimize a 3D representation parameterized\nby @ so that rendered images $x = g(\\theta)$ resemble 2D sam-\nples produced by a pre-trained T2I diffusion model for a\ngiven text prompt $y$. $g$ functions as a differentiable ren-\nderer. The T2I diffusion model $\\epsilon_{\\phi}$ predicts the sampled noise\n$\\epsilon_{\\phi}(x_t; y, t)$ of a rendered image $x_t$ at a noise level $t$ for the\ntext input $y$. To move all rendered images with higher den-\nsity regions under the text-conditioned diffusion prior, SDS\nloss estimates the gradient for updating $\\theta$ as:\n$\\nabla_{\\theta} \\mathcal{L}_{SDS}(\\phi, x) = \\mathbb{E}_{t,\\epsilon, c} \\left[w(t) \\left(\\epsilon_{\\phi}(x_t; y, t) - \\epsilon\\right) \\frac{\\partial x}{\\partial \\theta} \\right],$ (4)\nwhere $w(t)$ is a time-dependent weighting function.\nHowever, directly supervising sub-object structures with\nSDS, can lead to semantic coupling, which results in the\ngeneration of extra semantically related parts. For exam-\nple, when generating a tortoiseshell, this semantic coupling\nwill cause the generation of the tortoise's tail and head (Fig-\nure 7). We believe that the issue of semantic coupling in\nSDS stems from the training data of the stable diffusion\nmodel, where images often depict partial components along\nwith the complete object. This causes the target distribution\nconditioned by the text description of sub-object structures\ncontaining semantic information related to the full object.\nA straightforward approach is using negative prompts\nproposed by classifier score distillation (CSD) [57] to mit-\nigate coupled semantics. CSD demonstrates that compared\nto variational score distillation (VSD) [53], which adap-\ntively learns negative classifier scores, CSD employs pre-\ndefined negative prompts, resulting in a more precise opti-\nmization process:\n$\\nabla_{\\theta} \\mathcal{L}_{CSD}(\\phi, x) = \\mathbb{E}_{t,\\epsilon, c} \\left[w_1 \\cdot \\epsilon_{\\phi}(x_t; y, t) \\frac{\\partial x}{\\partial \\theta} - w_2 \\cdot \\epsilon_{\\phi}(x_t; y_{neg}, t) \\frac{\\partial x}{\\partial \\theta} \\right],$ (5)\nwhere $w_1$ and $w_2$ denote different weights for positive and\nnegative prompts. We found that the negative prompt's se-\nmantic distribution does not align with the associated se-\nmantics in the target distribution conditioned by the posi-\ntive prompt of sub-object structures. This resulted in nois-\nier gradients, making CSD less effective at decoupling se-\nmantics. Furthermore, as the weight of the negative prompt\nincreased, the optimization became more unstable and chal-\nlenging to converge.\nUnlike CSD, we apply CFG-weighted blending to the\ntokens in the original prompt, which does not require ad-\nditional inference to construct a negative distribution. This\nresults in semantically focused text embedding, directing\ntoward a more precise target distribution. Specifically, our\nloss function is defined as:\n$\\nabla_{\\theta} \\mathcal{L}_{ws}(\\phi, x) = \\mathbb{E}_{t,\\epsilon, c} \\left[w(t) \\left(\\epsilon_{\\phi}(x_t; y^*, t) - \\epsilon\\right) \\frac{\\partial x}{\\partial \\theta} \\right],$ (6)\nwhere $y^*$ is a text embedding computed by Compel [47].\nSpecifically, we assign each word in the prompt a CFG\nweight s and compute the weighted embedding $e_w$ for each\nword by blending the original text embedding $e$ with the\nempty text embedding $e_p$ as follows: $e_w = e_p + s \\cdot (e - e_p)$.\nBy concatenating the weighted embeddings of each word in\nsequence, we obtain the final semantically focused text em-\nbedding $y^*$. In our experiments, we found that assigning a\nweight of 1.21 to words that require enhanced semantics can\nachieve stable optimizations and effectively alleviate the is-\nsue of semantic coupling. Notably, the CFG weights used\nfor text embedding computation are separate from the CFG\nguidance scale applied during the computation of the SDS\nloss. Our experiment uses a CFG guidance scale of 100."}, {"title": "4. Experiments", "content": "In this section, we conduct experiments to evaluate the vari-\nous capabilities of Text2VDM both quantitatively and quali-\ntatively for text-to-VDM brush generation. We then present\nan ablation study that validates the significance of our key\ninsight into CFG-weighted SDS, as well as the effect of the\nregion control and shape control."}, {"title": "4.1. Qualitative Evaluation", "content": "To the best of our knowledge, Text2VDM is the first frame-\nwork to generate VDM brushes from text. We adapted three\nexisting methods for comparison and classified them into\ntwo categories. The first category includes Text2Mesh [28]\nand TextDeformer [10], which generate a brush mesh\nthrough text-guided mesh deformation on a planar mesh,\nfollowing a process similar to ours. For the second cate-"}, {"title": "4.2. Quantitative Evaluation", "content": "We quantitatively evaluated our framework regarding gen-\neration consistency with text input and mesh quality. We\nused 40 distinctive text prompts for VDM generation.\nGeneration Consistency with Text. We initially assessed\nthe relevance of the generated results to the text descrip-\ntions [37]. 12 different views were rendered for average\nscores respectively, as presented in Table 1. Our approach\nachieves the highest scores compared to baseline methods."}, {"title": "Mesh Quality", "content": "We evaluated mesh quality by examining\nself-intersection. Paint-it and Text2Mesh, which utilize di-"}, {"title": "User Study", "content": "We further conducted a user study to eval-\nuate the effectiveness and expressiveness of our method.\nA Google Form was utilized to assess 1) geometry quality\nand 2) consistency with text. We recruited 32 participants,\nof whom 14 are graduate students majoring in media arts,\nand 18 are company employees specializing in AI content\ngeneration. In this form, the participants were instructed\nto choose the preferred renderings of VDM from different\nmethods in randomized order, as shown in Table 2. The\nresults show participants preferred our method by a signifi-\ncant margin."}, {"title": "4.3. Ablation Study", "content": "Effects of CFG-Weighted SDS. We conducted experi-\nments to compare the generated results of directly using\nSDS [34], CFG-weighted SDS, and CSD [57] with three\ndifferent annealed weights of negative prompt (Figure 7).\nAs discussed in Section 3.3, SDS can result in semantic\ncoupling when generating sub-object structure, leading to\nartifacts like the tortoise's tail and head or the snail's head.\nWe also found that using negative prompts was ineffective at\ndecoupling semantics. Increasing the initial weight of neg-\native prompts further makes the optimization unstable, re-\nsulting in low-quality results. In contrast, our method effec-\ntively mitigates semantic coupling to produce high-quality\nmeshes without requiring additional UNet inference."}, {"title": "Effects of Region Control", "content": "Figure 8 demonstrates two sets\nof region masks and their control over surface details gen-\neration under different text prompts. Without using a region\nmask, the results lack a specific shape, which may not sat-\nisfy the desired stylized effect. By using a region mask, our\ngenerated results effectively conform to the user's desired\nshapes while also aligning with the styles specified by the\ntext, such as metal and stone."}, {"title": "Effects of Shape Control", "content": "Our method demonstrates that\nuser-specified VDMs can effectively control the volume and\ndirection of generated geometric structures. As shown in\nFigure 9, various generated geometric structures, such as\nelf ears and pauldrons, are high-quality and align with the\ntext descriptions. We also found that without volume initial-\nization, it is challenging to generate desired results. It indi-\ncates that this initialization is crucial for steering the gradi-\nent flow of geometric structure generation via adjusting the\nLaplacian term."}, {"title": "4.4. Applications", "content": "Once various VDM brushes are generated, users can di-\nrectly use these brushes to meet diverse creative needs in\nmainstream modeling software. For example, they can ap-\nply VDM brushes for mesh stylization and engage in a real-\ntime iterative modeling process."}, {"title": "Local-to-Global Mesh Stylization", "content": "Although mesh styl-\nization is a complex task even for professional artists, com-\nbining different surface details allows users to achieve styl-\nization quickly. For instance, users can apply a variety of\nwall-damage brushes to specific areas of a stone pillar, cre-\nating a style of damage (Figure 10)."}, {"title": "Coarse-to-Fine Interactive Modeling", "content": "Unlike previous\nmethods [4, 58] that require a lengthy optimization process\nfor each edit and result in non-reusable outcomes, our gen-\nerated VDM brushes can be directly used in modeling soft-\nware. This enables users to apply the generated brushes\neasily and interactively. For example, Figure 11 shows that\nusers can combine various brushes, such as skeleton hand,\nrose pattern, and pauldron to refine a coarse cloth model\ninto a highly detailed one."}, {"title": "5. Conclusion", "content": "We have presented Text2VDM, a novel framework for\nVDM brush generation from text. A VDM is a non-natural\n2D image where each pixel stores a 3D displacement vec-\ntor, making it challenging for existing T2I models to gen-\nerate. Thus, We treat VDM generation as mesh deforma-\ntion via the Laplace-Beltrami operator from a dense planar\nmesh. To generate the intended effects of surface details and\ngeometric structures, we provide two control methods: re-\ngion control and shape control. Additionally, VDM brushes\noften contain sub-object structures, which can lead to se-\nmantic coupling issues in SDS. We propose using CFG-\nweighted blending for prompt tokens to effectively mitigate\nthis, achieving high-quality brush generation. The gener-\nated VDM brushes are directly compatible with mainstream\nmodeling software, enabling various applications such as\nmesh stylization and real-time interactive modeling.\nLimitations and future work. While our framework\ncan generated high-quality VDM brushes, they may en-\ncounter multi-view inconsistencies, a common issue intro-\nduced by SDS. To further address it, the view-consistent\ndiffusion model proposed in MV2MV [6] may be helpful."}]}