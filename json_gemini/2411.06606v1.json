{"title": "Gen-AI for User Safety: A Survey", "authors": ["Akshar Prabhu Desai", "Tejasvi Ravi", "Mohammad Luqman", "Nithya Kota", "Pranjul Yadav"], "abstract": "Machine Learning and data mining techniques (i.e. supervised and unsupervised techniques) are used across domains to detect user safety violations. Examples include classifiers used to detect whether an email is spam or a web-page is requesting bank login information. However, existing ML/DM classifiers are limited in their ability to understand natural languages w.r.t the context and nuances. The aforementioned challenges are overcome with the arrival of Gen-AI techniques, along with their inherent ability w.r.t translation between languages, fine-tuning between various tasks and domains.\nIn this manuscript, we provide a comprehensive overview of the various work done while using Gen-AI techniques w.r.t user safety. In particular, we first provide the various domains (e.g. phishing, malware, content moderation, counterfeit, physical safety) across which Gen-AI techniques have been applied. Next, we provide how Gen-AI techniques can be used in conjunction with various data modalities i.e. text, images, videos, audio, executable binaries to detect violations of user-safety. Further, also provide an overview of how Gen-AI techniques can be used in an adversarial setting. We believe that this work represents the first summarization of Gen-AI techniques for user-safety.", "sections": [{"title": "I. INTRODUCTION", "content": "Machine Learning and data mining techniques are used across domains to detect violation of user safety. Examples include classifiers used to detect whether an email is spam or a web-page is requesting bank login information. However, existing ML/DM classifiers are limited in their ability to understand natural languages w.r.t the context and nuances. The aforementioned challenges are overcome with the arrival of Gen-AI techniques, along with there inherent ability w.r.t translation between languages, fine-tuning between various tasks and domains.\nIn this manuscript, we provide an overview of the various domains across which user safety can be violated. In particular, we provide more information on how Generative Artificial Intelligence (Gen-AI) techniques can be used towards reduction of egregious abuse (e.g. phishing, malware, anomaly detection, counterfeit, fraud prevention), misinformation and disinformation (e.g. fake news, deepfake detection), increase in content moderation, awareness about mental health (e.g. cyber-bullying prevention, crisis support) and towards robust physical safety (e.g. accessibility, autonomous systems).\nFurther, we discuss how Gen-AI techniques can be used across various data modalities. In particular, we present how Gen-AI techniques can be used to detect user-safety violations in text and rather outperform all previous techniques w.r.t NLP tasks such as entity recognition, question answering and sentiment analysis. Further, Gen-AI techniques with their inherent ability to parse and understand images provides an easy mechanism to detect image manipulation, deepfake detection. The advantages of Gen-AI techniques go beyond text and images to other data modalities such as videos, audio and executable binaries.\nWe also discuss how Gen-AI techniques can be used in an adversarial setting. In particular, we present how these techniques can be used to attack at scale (e.g. mass spam). Further, the attacks become more intelligent as these techniques can target humans more effectively and engage with humans with a similar cognitive capacity. Gen-AI techniques along with reinforcement learning techniques can be used to create more sophisticated attacks while using feedback from the last failure. Further, Gen-AI techniques can also make these attacks look very personalized (e.g. deep-fakes) and second order effects (e.g. Gen-AI imitating human sounding text).\nThe organization of the paper is as follows. In section II, we provide a comprehensive overview of the various user safety domains, where Gen-AI techniques can be applied. In section-3, we discuss the various data modalities across which Gen-AI techniques can be utilized to protect user safety violation. In Section-4, we present, how Gen-AI techniques can also be used in an adversarial setting. In Section-5, we present our opinion on what does the future look like for Gen-AI techniques. Finally, in section-6, we conclude this manuscript."}, {"title": "II. USER SAFETY DOMAINS", "content": "Gen-AI techniques can significantly enhance user safety in both digital and physical environments. It can proactively address risks, offer timely assistance, and empower individuals with personalized tools. Within digital realm, it can detect fraud, flag instances of harmful content, cyberbullying, and predatory behavior. In the physical realm, Gen-AI techniques can help improve accessibility via wearables, contribute to mental and physical health by providing timely access to online resources and perform crisis intervention if necessary. Below we explore and detail the research that enhance user safety in these two realms."}, {"title": "A. Digital Realm", "content": "1) Online Threat Protection: Phishing is a type of socially engineered cyber-attack where bad actors try to trick users into giving up their personal information by pretending to be a trustworthy source either via email and phishing websites [1].\nThe work by Koide et al. [2] showed that Gen-AI techniques lowers the barrier to deploy systems that detect, mitigate and deter new phishing attacks by utilizing their broad knowledge base, multi-modal and multi-lingual capabilities, which otherwise would require multiple different classifiers [3]. Further, Koide et al. [4] presented Gen-AI techniques that can provide user with detailed reasons for why certain email or website is highlighted for phishing. Ai et al. [5] proposed advanced strategies like Retrieval-Augmented-Generation (RAG), which can detect sophisticated phishing attacks that involve longer multi-turn interaction between bad actor and unsuspecting users.\nMalware is malicious software, that includes viruses, worms, and Trojan horses, deliberately designed to compromise computer systems, servers, or networks. These attacks usually lead to data breaches (e.g., via spyware), system damage, or unauthorized control of devices (e.g., through adware and ransomware). Gen-AI techniques help enhance existing strategies for malware identification and alert generation that have traditionally employed machine learning and deep learning models [6][7]. Ferrag et al.[8] showed that with advanced techniques, such as Half-Quadratic Quantization (HQQ), Direct Preference Optimization (DPO), GPT-Generated Unified Format (GGUF), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG), Gen-AI can be leveraged for more effective threat detection and response.\nIn AppPoet, Zhao et al. [9] demonstrated a system that uses multi-view prompt engineering to detect and produce a detailed diagnostic report for android malware. Similarly, Wang et al. [10] in their work ShieldGPT, showed that via prompt engineering, Gen-AI has the potential to defend against Distributed Denial of Service (DDoS) attacks and provide comprehensible explanation and detailed mitigation instructions specific to an attack.\n2) Misinformation Detection: Fake News is false or misleading information presented as news. The proliferation of misinformation across social media platforms and news outlets presents a significant threat in the digital age. The sheer volume of online content renders manual fact-checking impractical and this is where Gen-AI can help.\nZhang et al. [11] proposed a hierarchical prompting method that outperforms state of the art fully-supervised approach for news claim verification. Further, the work by Yue et al. [12] proposed a retrieval augmented response generation system to combat online misinformation and generated counter-misinformation responses based on the scientific evidences. Furthermore, work by Xuan et al. [13] showed how Gen-AI can utilize external knowledge bases for information verification.\nGen-AI are proving effective in detecting fake news, both independently and in conjunction with specialized Small Language Models (SLMs) [14]. Notably, Gen-AI can identify misinformation generated not only by humans but also by automated systems [15]. Their capabilities extend to multimodal misinformation encompassing text, images, and videos [16]. Deepfakes are synthetic media where an entity in an existing image or video is replaced with another entity's (usually a person) likeness using artificial intelligence techniques. Recent advances in Gen-AI has enabled creation of extremely high fidelity personalized content like images, audio and video. At the same time, several techniques has been developed to detect such deepfakes across different modalities. Chen et al. [17] proposed a framework called Diffusion Reconstruction Contrastive Learning (DRCT), that generated hard samples by high-quality diffusion reconstruction and adopted contrastive training to guide the learning of diffusion artifacts. They showed that detectors enhanced with DRCT achieve over a 10% accuracy improvement in detecting diffusion generated images. Further, Zhang et al. [18] proposed a Temporal Dropout 3-dimensional Convolutional Neural Network (TD-3DCNN) to detect deepfake videos model on public deepfake datasets that surpassed state of the art performance on detecting deepfake videos.\n3) Content Moderation: Content moderation is critical to maintaining the integrity of online platforms and to keep the members safe from harmful content, misinformation, and disinformation, and to comply with legal and policy standards. Content moderation is a growing challenge as the platforms scale and sheer volume of content to be moderated can't be handled by humans alone. Diversity of content in terms of language and modality and evolving nature of harmful content itself add an additional challenge. The in-context learning [19] ability of Gen-AI techniques have been used to encode online platform policy in a prompt to detect whether content violates it or not [20]. The multi-modal reasoning capability of Gen-AI now allows us to capture the necessary context spread across text, images and video (short and long form) components to detect policy violation. Ahmed et al. [21] proposed a model that outperforms previous work conducted on the Malicious or Benign (MOB) benchmark for video content moderation. They also highlight the importance of providing context to content moderation prompts to improve the performance."}, {"title": "B. Physical Realm", "content": "This section focuses on user safety enhancements that can impact a user outside of the digital environment like accessibility/ mobility improvements, crisis support and improvements to mental health and well being.\nDuring emergencies, support systems are overwhelmed. Gen-AI powered chatbots can aid in crisis support by providing rapid targeted information, enhancing communication, offering emotional support, and improving preparedness. Advanced frameworks that leverage Gen-AI can be utilized to help the support systems by understanding user needs and creating workflows for government agencies as documented in the survey by Rieskamp et al. [22]. Further, the work by Otal et al. [23, 24], explored using fine tuned models like LLama2 to assist users with simple instructions while informing authorities with summarized and accurate information.\nGen-AI techniques can enhance accessibility, enabling safer navigation in the world for visually impaired people. In VisionGPT, Wang et al. [25] showcased a system that takes in real time video via camera captured frames and provides a concise audio description that enabled safer navigation.\nCounterfeit goods are fake products designed to look genuine, like branded items. We include them in the physical realm as they involve a non digital entity that the user interacts with. Production and distribution of counterfeit goods infringe on intellectual property rights which can cause serious damage to consumer health and safety (Counterfeit medicines, cosmetics, or safety equipment) and brand reputation, eventually resulting in losses for legitimate businesses. According to the National Crime Prevention Council around $2 trillion worth of counterfeit products are sold to consumers annually [26]. At the time of writing, there isn\u2019t much research around using Gen-AI techniques for counterfeit detection, however, there are relevant work in identifying counterfeits with Generative Adversarial Networks (GANs). The generative component mirrors the counterfeiter\u2019s role, while the discriminator functions as the detective, identifying and rejecting fraudulent outputs. Some examples include, a combination of external attention GAN with deep convolutional neural networks (CNNs) developed by Peng et al. [27] to identify counterfeit luxury handbags and GANs for credit card fraud detection as shown by Wang et al [28].\n1) Mental Health and Well-being: Mental health and well being is a topic that is central to user safety and Gen-AI can be applied to detect, intervene, prevent and provide timely support to ensure users mental well being. VITA [29], a multi-modal Gen-AI based system for mental well being, allows robotic coaches to autonomously adapt to the coachee\u2019s behaviours from features like facial valence and speech duration. Using these signals, it delivers adaptable coaching exercises to promote mental well being. Along with useful intervention activity recommendations, Gen-AI based agents that are anthromorphic are able to foster relational warmth and can prove to be more effective[30].\nGen-AI can be trained to detect and flag cyberbullying instances in online interactions, which often results in significant psychological distress for the victims, allowing for faster response and support for victims. Vanpech et al.[31] proposed a system to identify cyberbullying via images by feeding the image as input to GPT-4 to generate description metadata, which was then provided to a custom trained Gen AI model that classified the images to detect if it\u2019s used for bullying. Gen-AI can also be used to augment training data to improve existing classifiers. For example, in the work by Jahan et al. [32] GPT-3 based data augmentation showed 0.8% improvement in classification $F1$ score for hate speech detection tasks.\nPredatory behavior detection is a critical research area for social media platforms to ensure user safety, especially for vulnerable populations. Gen-AI can analyze patterns in text and images to identify grooming behaviors or attempts to solicit explicit content from minors. This can help platforms intervene proactively and protect users from online predators. Nguyen et al. [33] discusses how fine tuned models are able to detect online predators better than out of the box foundational models. In particular, they used a LLama 2 model which was fine-tuned using LORA."}, {"title": "III. DATA MODALITIES", "content": "This section explores how Gen-AI impacts user safety across different data modalities."}, {"title": "A. Text", "content": "Large language models, such as GPT-4 , Gemini and LLaMA have demonstrated outstanding performance across downstream NLP (Natural Language Processing) tasks (e.g. text classification, named entity recognition, translation, question answering and sentiment analysis) [34]. The inbuilt context from pre-training enables transformer based models like RoBERTa [35] to perform well even on tasks that were difficult earlier, such as sarcasm detection [36]. In particular for user safety, pre-trained transformer models achieve remarkable performance in hate-speech detection, detecting spam, fake news and fake reviews. Further, Keyan et al. demonstrated that well crafted reasoning prompt can effectively capture the context of hate speech by fully utilizing the knowledge base in Gen-AI models, significantly outperforming existing techniques [37]. Using retrieval augmented generation (RAG), these models are able to perform fact verification [38], and are also able to detect fake news written in high quality journalistic style [39]. Gen-AI techniques are also able to overcome content moderation to low resource languages as well as to multilingual text. In particular, multilingual-BERT and XLM-RoBERTa, each of which have been pre-trained on 100+ languages have been used to perform well on hate speech and hostility detection with multilingual input [40, 41]. Further, they are also being used to generate annotated data for training models for low resource languages. Additionally, annotated data generated by Gen-AI techniques are found to be on par with human annotators and can be done at a fraction of time and cost [42]."}, {"title": "B. Images", "content": "Following the breakthroughs in large language models, large multimodal models (LMMs) such as GPT-4v [43] extend these capabilities to other data modalities, with images being of particular focus. User safety in the domain of images involves detecting harmful images, images that violate platform policies, flagging sensitive media and/or assisting users in detecting machine generate image from real images.\n1) DeepFake detection: Easy availability of image generation tools such as Midjourney, StableDiffusion [44], Dall-E [45] and others have led to proliferation of fabricated images online. These models generate hyper realistic images that are not easily distinguished from real images.\nExisting techniques for detecting deepfake images are mostly formulated as binary classification problems and fall into three major categories - identifying inconsistencies exhibited in the physical/physiological aspects in the DeepFake images, methods using signal-level artifacts introduced during the synthesis process, or directly training a classifier on real and DeepFake samples. Another shortcoming is that classifiers trained to detect images generated from one class of generative models (e.g., GAN) fail to generalize on other generative models (e.g., diffusion models) [46]. The same techniques that have resulted in the success of image generation have also been employed to detect deepfake images. Most recently, using a pre-trained CLIP-ViT model to learn image features followed by a classifier to detect fake images sets new state of the art and also generalizes across different types of generative models [46].\n2) Harmful images detection: Recent advancements in vision-language models have improved the ability to detect harmful images, making them useful for content moderation [47] [48]. Models like VinVL [49], which leverage transformers and attention mechanisms, can capture complex relationships between visual elements and generate accurate, contextually relevant captions. Further, large-scale multi-modal pre-training on massive datasets of images and text, such as CLIP (Contrastive Language-Image Pre-training) [50], has enhanced the models' ability to connect visual and textual information [51]. This is crucial for identifying harmful images, especially those containing hate speech or offensive symbols, which often rely on the interplay between visual and textual elements.\nPaLI-X [52], a vision language model trained using latest techniques of instruction-tuning and distillation, outperforms all prior Vision Language Models (VLMs) achieving state-of-the-art performance across complex vision tasks including the hateful memes challenge, which seeks to identify multi-modal hate speech.\nA comparative study of different image safety classifiers performed [53] observed that VLMs can identify a wider range of unsafe content, with GPT-4v [?] being the top performing model for this use case.\n3) Safety applications: VLM's nuanced understanding of a scene has applications in defect recognition and safety equipment recognition [43]. These models are being used to detect safe pedestrian crossing [54], adherence to workplace safety guidelines [55], and evaluation of construction site safety among others [56]."}, {"title": "C. Videos", "content": "Consumption of video content has been on steady increase especially due to social media and streaming platforms [57]. User safety issues in videos could be the presence of harmful, age inappropriate, violence copyright violations [58] or deepfakes.\nTechnology companies have built sophisticated solutions to understand video content and protect users from these threats [59]. Following subsections outline specific challenges related to user safety for different types of video content.\n1) Human generated videos: Video forensics field has been using Gen-AI tools to extract information from videos to detect threats. This includes processing individual frames as images and detecting objects and actions in those frames, converting audio to text and processing that text with Gen-AI techniques for further analysis. Bi-Long Short Term Memory (Bi-LSTM) machine learning model combined with Convolutional Neural Networks (CNNs) are often used to detecting violence and other harmful behavior in video content. Sentiment analysis of the audio in the video too can be used to detect harmful videos.\nGen-AI breakthroughs now allow people to generate videos simply by using text prompts. Models like Sora have achieved remarkable results. However, this unrestricted ability to create videos introduces user safety concerns.\nThese generated videos present two main challenges.\n\u2022 Detecting generated videos that claim to be real (deep-fakes)\n\u2022 Detecting harm in generated videos\nPang et al. [60] showcased defense mechanisms to prevent the generation of unsafe videos via their approach called Latent Variable Defense (LVD), which works within the model's internal sampling process. LVD achieved 0.90 defense accuracy while reducing time and computing resources by 10x when sampling a large number of unsafe prompts. Further, Rana et al. [61] provided an comprehensive overview of existing research works for deepfake detection.\nDetecting harm in generated videos is another important research problem. Since generated videos are not bound by the physics of real world, generated videos can cleverly twist certain elements of the video to evade detection of standard algorithms[62].\n2) Live streaming: Live video feeds is a popular form of video content. Besides social media live streams, security camera footage, traffic camera feeds, live sports and gaming etc. are important sources of live stream videos [63].\nFan et al. [64] discuss a real time deepfake identification framework to handle abuse in live streams. Further Liz-Lopez et al. [63] provides an extensive survey of technologies that can be used for detecting harmful multi-modal content in live streams. Furthermore, Gupta et al. [65] proposed quantum machine learning as another potential approach for processing live streams."}, {"title": "D. Audio", "content": "Traditional machine learning techniques are limited in their ability to handle fabricated and harmful audio content. This inability usually stems as traditional methods rely on the extraction of acoustic features in the spectral domain. However, with the advent of latest tools for generating natural sounding voice, traditional methods are bound to fail.\nGen-AI techniques are able to overcome this by helping in dataset generation for advancing research. Datasets such as FakeAVCeleb [66, 67] and Joint Audio-Visual Deepfake [68] improve deepfake detection research by including video deepfakes with corresponding synthesized, lipsynced audio tracks or by integrating audio alteration. Further efforts, such as WaveFake [69], which contains 100K+ generated audio samples contribute substantially to building resources capable of addressing the problem of detecting deepfakes. PolyGlot-Fake [70], a multi-modal and multi-lingual deepfake dataset covers 7 languages, and MLAAD (Multi-Language Audio Anti-Spoofing Dataset) expands audio spoofing dataset to 23 languages [71].\nGen-AI techniques are also able to detect hate speech in verbal data. In particular, conformer model, an architecture combining convolutional networks and transformers, improves automatic speech recognition with a word error rate (WER) of less than 2% [72]. Further, generative models are helping to fill the lack of audio only datasets. For example, An et al. used text to speech models to generate an audio only dataset, and a BERT based model for explainable hate speech detection directly from audio files [73]."}, {"title": "E. Code", "content": "Gen-AI do not simply generate code or superficially understand its context. They clearly demonstrate the ability to process, identify the relevant parts, and operate on them. This capability is demonstrated in detecting malicious code, that is deliberately obfuscated to prevent this from happening. This makes them effective in de-obfuscating malicious scripts [74], cyber threat detection [75], and even understanding minified code [76].\nChuanbo et al. [77] systematically leverage ChatGPT-4 to process multimodal app data (i.e., textual descriptions and screenshots) to determine maturity rating of an app to keep children safe from age inappropriate apps."}, {"title": "IV. ADVERSARIAL GEN-AI", "content": "In this section we analyze how Gen-AI technologies are impacting the adversaries of user safety. Earlier sections detail the well-understood threats to user safety. However, to better detect and deter these threats, we must better understand how hostile actors might use Gen-AI techniques in adversarial settings."}, {"title": "A. Safety Violations at Scale", "content": "Online fraud poses a major threat to user safety, with bad actors frequently aiming for large-scale disruption. Technology facilitates these attacks by enabling the mass distribution of emails and text messages. However, a limiting factor for these bad actors lies in their access to limited resources to conduct intelligent conversations with each victim. Gen-AI techniques allows bad actors to overcome this crucial limitation [78].\nBy using Gen-AI technology, they can reduce human involvement in large-scale operations thereby decreasing the time and cost of their attacks. In particular, bad actors can craft more convincing communications and tailor them to each user, making detection by spam and threat detection algorithms more difficult [79]. Additionally, Gen-AI enables large-scale content generation[39], facilitating the creation of fake news websites and blogs that produce convincing, yet deceptive, content [79]. Further, foreign language content creation (i.e. geo-targeting) which was a barrier for bad actors can be easily overcome using Gen-AI technology [80]."}, {"title": "B. Safety Violations with Feedback", "content": "Considering the adversarial nature of abuse, bad actors can use sophisticated Gen-AI techniques to make more effective attacks using explicit feedback from past attempts.\nIn particular, reinforcement learning can help bad actors create better user-violation models that help them create targeted and effective campaigns [81] to violate the safety of a user. Additionally, bad actors have access to data from previous attacks, which is often not accessible to the security researchers. Mujumdar et al. [82] show that this data is often utilized by bad actors to improve the effectiveness of their attacks.\nCaptcha is a common technique used by social media and other applications to prevent abuse from bad actors using automated systems. However, modern Gen-AI techniques have enabled bad actors to develop sophisticated solvers thereby overcoming the barriers posed by captcha. Ye et al. [83] discussed how the availability of human labeled data enabled researchers to create sophisticated solvers."}, {"title": "C. Safety Violations with Personalized Content", "content": "Gen-AI techniques have improved bad actor's ability to generate highly personalized content. For example, Gen-AI techniques with a large context window can technically look at a victim's available information and create personalized[84] email campaigns, websites, calls or even video messages for phishing or financial fraud based scams. Further, Gen-AI allows bad actors to turn a normal phishing attack into a spear phishing [85] attack where instead of targeting a large group of people with similar messages, a highly targeted strategy is crafted to target specific individual or individuals.\nGen-AI techniques can also be used for pre-texting [86] based scams. In this form of attack a bad actor designs an attack plan for their target, using a story around the facts they know about the individual. The end goal of this story is eventually to scam the victim using prior personalized information. This is a complex form of personalized scam and traditionally requires lot of resources from bad actor. However, with Gen-AI technique, such level of personalized abuse is easier to achieve."}, {"title": "D. Second order attacks", "content": "Second order attacks are scenarios where the human behavior is manipulated to achieve a certain end result by fake news campaigns and/or social media activity. Synthetic reality [87] is an example of a second order effect which includes misinformation or targeted campaigns by bad actors. In such attacks it is not clear who is the victim; but as long as some people fall prey to such activity the bad actors benefit. Attacks like these are extremely well coordinated and require complex infrastructure. It involves creating fake news websites, social media bots and even human users to amplify a certain type of message to make it sound very real.\nAnother form of second order effect attacks is where bad actors develop a long term plan to trick the world's latest Gen-Al technologies. Bad actors develop a long term plan to create synthetic reality on the internet through various specially setup websites, synthetic social media posts and similar mechanisms. Through this approach they poison the data on which Gen-AI techniques are trained and built [88]."}, {"title": "E. Gen-AI Violations", "content": "Al safety is an evolving field where in models have inbuilt mechanism to ensure that they are not being used for user-harm [89, 90]. However, when such inbuilt safety mechanisms in the model are broken, it is referred to as jail-breaking the model[91]. Jail-breaking is an active field of research as well as an area where regulations are being actively sought [92]. Li et al. [93] proposed a system called JailMine, which is able to overcome the defensive measures of Gen-AI techniques, even when Gen-AI models are frequently updated and incorporated with advanced defensive measures.\nMajumdar et al. [82] present a detailed survey of how malicious actors might be gaining strategic advantage with the help of Gen-AI techniques despite the AI safety barriers on these models. In particular, they discussed how Gen-AI techniques can be used towards the manipulation of public opinion through the generation of deceptive content, the orchestration of social engineering attacks using sophisticated language-based techniques, and the ease of cybercrimes such as phishing, password cracking and malware propagation."}, {"title": "V. SECTION: FUTURE PROSPECTS OF GEN-AI", "content": "With the recent advancements in Gen-AI techniques, we expect to see exciting developments that improve user safety, and this section describes some of these areas."}, {"title": "A. Content understanding", "content": "Content understanding is key to ensuring user safety. Traditionally, human moderators, supported by machine learning technologies, have analyzed content to ensure user safety. This analysis often uses some form of Reinforcement Learning through Human Feedback (RLHF). This has been a challenge due to limited availability of training data [94].\nGen-AI techniques offer complex models that identify content violating user-safety. Institutions leveraging Gen-AI for user safety, no longer need to develop their own content understanding models. Further, Mohammed et al. [95] demonstrated that it can be done by off-the-shelf models with some fine tuning or prompt engineering. Gen-AI techniques can also be utilised for minority classes and outliers as they are much better represented in the training data of Gen-AI as compared to the internal data of an institution [94, 96]. Chaudhary et al. [97] provides an example of how a carefully crafted prompts and an off-the-shelf Gen-AI API's alone can produce excellent content moderation service. The availability of general-purpose models allows for the deployment of sophisticated Gen-AI technologies at a lower cost and without specialized content understanding. [98]"}, {"title": "B. Foundation Ensembles", "content": "Gen-AI techniques enables the availability of sufficiently large and complex models that can handle many business use cases thereby replacing multiple smaller specialized models. For example, a single model can now detect phishing [99], spam, fraud and analyze sentiment within a text. This simplification streamlines the training, fine-tuning, and deployment of these models thereby accelerating development and deployment processes. This reduction in technical stack then results into deprecation of system complexity and decrease in associated costs.\nDetoxbench [100] is a benchmark that measures the ability of LLMs to detect various forms of user safety violations such as toxic content, phishing, spam and fraudulent job postings."}, {"title": "C. Harnessing multi-modality for better user safety", "content": "Section III discussed the user safety challenges across data modalities and how Gen-AI impacts them. Future user-safety research using Gen-AI should focus on harnessing the multi-modality of data. This research should analyze text, images, audio, and video data holistically for better user safety, rather than treating them as independent inputs.[101]\nYang et al. [102] showed that auto-insurance fraud detection is improved with the help of multi-modal models rather than analyzing only structural data. Further, Goyal et al. [103] hypothesized that there would be more applications of multi-modal AI in improving user safety. Furthermore, Akhare et al. [104] provided a survey of machine learning techniques in area of user safety and discuss that multi-modal multi-objective evolutionary algorithms are more scalable and more precise than other traditional methods to analyze content to enhance user safety."}, {"title": "D. Gen-AI and second order harm prevention", "content": "Future user safety research will likely focus on preventing the \"second-order effects\" described in section IV-D. Research like [105] have shown that long term psychological and financial harm is possible due to widespread Gen-AI usage. For example, Dewite [106] shows that extensive usage of chatbots might be harmful in long term even though independently specific conversations with the chatbot might appear harmless. Markus et al. [107] shows that how certain AI uses might be perfectly normal independently but when chained together can create a misuse chain and how this might be detected.\nUser safety researchers might have to take more holistic approach towards user safety instead of restricting themselves with specific methods or modalities [108]. Kranz et al. [109] demonstrated how AI-copilot can be provided to humans in their interactions with robots to ensure safety, by flagging unusual behavior.\nWe anticipate the widespread adoption of co-pilots that monitor users' online behavior across platforms. These co-pilots will provide a comprehensive safety mechanism, protecting users from a wide variety of second-order effects stemming from the pervasive use of Gen-AI in everyday life. Such technologies will detect synthetic realities and prevent user from falling prey to complex second order effect harms."}, {"title": "VI. CONCLUSION", "content": "Gen-AI techniques, along with there inherent ability w.r.t translation between languages, fine-tuning between various tasks and domains [110] are able to overcome the challenges associated with ML/DM techniques to reduce violation w.r.t user-safety tasks.\nIn this survey, we provided an overview of the various user safety domains (e.g. egregious abuse, misinformation and disinformation, increase in content moderation, towards robust physical safety) across which user safety can be violated and how Gen-AI techniques can be used to overcome those avenues. Next, we discussed how Gen-AI techniques can be used across various data modalities i.e. text, audio, video, executable binaries and images.\nWe then discussed how Gen-AI techniques can be used in an adversarial setting. In particular, we discussed how these techniques can be used to pursue safety violations at scale, safety violations with human feedback, safety violations with personalized content, second order attacks and Gen-AI violations.\nLastly, we provide our opinion about the future prospects of Gen-AI techniques w.r.t user-safety. In particular, we hypothesize how Gen-AI techniques have an inherent ability w.r.t. content understanding, their ability to work as large ensembles with one model pursuing multiple tasks and lastly how Gen-AI techniques can prevent second order harm.\nWe believe that this work represents the first summarization of Gen-AI techniques for user-safety. In particular, we provided a deep overview of emerging technologies along with their applications to user-safety, with a focus on areas suitable for advancement. Our goal is to make sure that this work warrants immediate investment towards driving growth within the industry."}]}