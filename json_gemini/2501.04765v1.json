{"title": "TREAD: Token Routing for Efficient Architecture-agnostic Diffusion Training", "authors": ["Felix Krause", "Timy Phan", "Vincent Tao Hu", "Bj\u00f6rn Ommer"], "abstract": "Diffusion models have emerged as the mainstream approach for visual generation. However, these models usually suffer from sample inefficiency and high training costs. This issue is particularly pronounced in the standard diffusion transformer architecture due to its quadratic complexity relative to input length. Recent works have addressed this by reducing the number of tokens processed in the model, often through masking. In contrast, this work aims to improve the training efficiency of the diffusion backbone by using predefined routes that store this information until it is reintroduced to deeper layers of the model, rather than discarding these tokens entirely. Further, we combine multiple routes and introduce an adapted auxiliary loss that accounts for all applied routes. Our method is not limited to the common transformer-based model - it can also be applied to state-space models. Unlike most current approaches, our method, TREAD, achieves this without architectural modifications. Finally, we show that our method reduces the computational cost and simultaneously boosts model performance on the standard benchmark ImageNet-1K 256 \u00d7 256 in class-conditional synthesis. Both of these benefits multiply to a convergence speedup of 9.55\u00d7 at 400K training iterations compared to DiT and 25.39\u00d7 compared to the best benchmark performance of DiT at 7M training iterations. Our code will be released.", "sections": [{"title": "1. Introduction", "content": "In recent years, diffusion models [20, 41, 45] have become a powerful generative technique for image synthesis [22, 41]. They have also been successfully extended to the 3D [38] and video domains [3]. While diffusion models avoid some training challenges faced by their predecessors, such as GANs [14], they incur high costs [26] due to slow convergence and sample inefficiency [31]. Currently, the Diffusion Transformer (DiT) [35] is the main approach for scaling these models, building on the established Transformer architecture [53]. However, the Transformer architecture has computational cost limitations, as it scales quadratically with token length and converges slowly, further increasing the expense of training diffusion models. Training a DiT on standard benchmarks alone requires thousands of A100 GPU hours without reaching convergence, and text-to-image models demand even more\u2014Stable Diffusion [41], for instance, required about 150,000 A100 GPU hours. Despite ongoing efforts to reduce computational demands through improved infrastructure, implementation, and hardware, democratizing the training of diffusion models remains a distant goal. Several works have proposed methods to accelerate the training convergence utilizing external self-supervised features [21, 55],"}, {"title": "2. Related Work", "content": "Diffusion Models and Efficient Generative Backbones\nScore-based models [46, 47], such as Denoising Diffusion Probabilistic Models (DDPM) [20], have become the dominant framework for image synthesis in recent years. These models start with a Stochastic Differential Equation (SDE) and gradually add Gaussian noise to real data in the forward process. This process transforms the complex real distribution into a Gaussian distribution. An iterative denoising process is then used to reverse this transformation and generate samples. A key improvement in the efficiency of early diffusion models was moving the training to a compressed latent space [41].\nBuilding on the foundation of score-based models, early diffusion models [7, 20, 41] used the UNet architecture [42] as their backbone. However, more recently, token-based models like DiT [35] have become a preferred backbone choice [5, 8, 58]. However, these models face challenges due to their quadratic complexity with respect to the number of tokens.\nTo address the computational challenges associated with token-based models, recent studies have proposed caching mechanisms to accelerate diffusion models based on UNets [29] or DiTs [30]. Unlike these approaches, our routing scheme does not skip the computation of network layers for each token. Instead, we introduce a routing mechanism that operates only during training. This mechanism transports tokens from one network layer to another, enhancing the model's efficiency without omitting necessary computations.\nIn addition to diffusion transformers, state space models (SSMs) have recently been shown to be promising alternatives to DiTs [10, 12, 22, 51] to alleviate the quadratic complexity. However, largely owing to their recent rise, there are currently few works that push the efficiency of SSMs in a generative setting. Token pruning [56], token masking [50], and mixture-of-experts [37] for SSMs have been explored so far, although not for diffusion backbones. Different from them, we consider the token routing technique in state-space models to improve the training efficiency.\nToken-based Routing, Pruning and Masking Several recent methods [11, 34, 49] utilize the sparse Mixture-of-Experts (MoE) [23, 44] technique to improve the efficiency of diffusion transformers. Typically, MoE is implemented as a router module that divides the network into subsets, or experts. The router then determines which expert processes each token, thereby reducing computational overhead compared to applying every parameter to every token. Another strategy, Mixture-of-Depths (MoD) [40], involves a router module that decides the computation paths for each token"}, {"title": "3. Prerequisites", "content": "We adopt the framework established by Song et al. [48], which explores the forward diffusion and backward denoising processes of diffusion models in a continuous time context through the use of stochastic differential equations (SDEs). The forward diffusion process gradually transforms real data $x_o \\sim P_{data}(x_0)$ into a noise distribution $X_\\tau \\sim N(0, max I)$ with the following SDE:\n$dx = f(x, t) dt + g(t) dW$,\nwhere $f$ represents the drift coefficient, $g$ is the diffusion coefficient, $W$ denotes a standard Wiener process, and the time variable $t$ ranges from 0 to T.\nIn the reverse process, the generation of $x_0$ samples is achieved through another SDE expressed as:\n$dx = [f(x,t) - g(t)^2\\nabla_xlogp_t(x)] dt + g(t)dW$,\nwhere $\\bar{W}$ signifies a reverse-time Wiener process, and $dt$ represents an infinitesimal negative timestep. This reverse SDE can be reformulated into a probability flow ordinary differential equation (ODE), which retains the same marginal distributions $p_t(x)$ as the forward SDE at each timestep $t$:\n$dx = [f(x,t) - \\frac{1}{2}g(t)^2\\nabla_x log p_t(x) ]dt$.\nUtilizing the formulation of Karras et al. [24] (EDM), we simplify the drift term by setting $f(x, t) := 0$ and defining the diffusion coefficient as $g(t) := \\sqrt{2t}$. Consequently, the forward SDE simplifies to the expression\n$x_t = x_0 + n$,\nwhere $n \\sim N(0,t^2I)$. The corresponding probability flow ODE can be formulated with the score function $s(x, t) := \\nabla_x log p_t (x)$:\n$dx = -t s(x, t) dt$.\nTo estimate $s(x_t,t)$, the EDM approach parameterizes a denoising function $D_\\theta(x_t, t)$ which minimizes the denoising score matching loss:\n$E_{xo\\sim P_{data} E_{n\\sim N(0,t^2I)}} || D_\\theta(x_o + n, t) \u2013 x_o||^2$,\nwhere $x_t = x_0 + n$, from this formulation, $s(x_t, t)$ can be approximated as\n$\\hat{s}(x_t, t) = \\frac{D_\\theta(x_t, t) - x}{t^2}$\nA common choice to formulate the denoising function $D_\\theta(\u00b7, \u00b7)$ is the usage of a Diffusion Transformer or other scalable architectures like SSMs [22, 36]. We adopt the DiT due to its community-wide usage as our main model used for ablations shown in Section 5.\nMasking-based Training Objective Using a standard training strategy for diffusion models, one can apply Equation (6) to the entire token set. However, if the diffusion model is trained only on a subset of tokens, either through masking, pruning or routing, the task becomes more challenging [58]. Specifically, the model must form predictions across the full set of tokens while only having access to a partial set. To address this, He et al. [17] propose to decompose the loss into two parts: 1) the denoising score matching loss and 2) the Masked AutoEncoder (MAE) loss. While the former one is applied only to the visible tokens, the MAE loss acts as auxiliary task for reconstructing masked tokens from visible ones. Zheng et al. [58] reason that the additional MAE loss can promote the model to develop a more global understanding of the full image.\nFor this, we first define the operators $V_m$ and its complement $\\bar{V_m}$ to yield the partial token sets by applying a randomly sampled binary mask $m \\in \\{0, 1\\}^P$, where $P$"}, {"title": "4. TREAD", "content": "In this work, we introduce a broadly applicable diffusion training strategy using routes. These routes serve as a unidirectional mechanism to transport tokens across layers. Formally we define a route as:\n$r = \\{ (D_i^o, D_j^i) | 0 \\leq i < j \\leq B \\}$,\nwhere:\n\u2022 $B + 1$ is the total number of layers in the network $D_\\theta$,\n\u2022 $L = \\{D_0^\\theta, D_1^\\theta ..., D_B^\\theta \\}$ is the set of layers in $D_\\theta$.\n\u2022 Each pair $(D_i^o, D_j^i) \\in r$ represents a connection from the start layer $D_i^o$ to the end layer $D_j^i$.\nFor simplicity, we denote $r(D_i^o, D_j^i)$ as $r_{i,j}$, where i is the starting layer and j is the ending layer. As illustrated in Figure 2, a selected route transports tokens from one layer to another. When the route $r_{i,j}$ is active, the tokens are bypassed by all intermediate layers it spans. Once the route completes, these tokens become available again, allowing layer $D_j^i$ to receive information from layer $D_i^o$. We propose that the capability of Transformer-based models to interpret the output of preceding layers [18, 29, 30, 40] can be used during training to enhance the convergence speed of the noise-to-image mapping. This characteristic is also demonstrated using the cosine similarity between the produced outputs by each layer of a trained DiT in Figure 3. Although the exact underlying mechanisms are not yet fully theoretically grounded, our empirical results in Section 5 demonstrate the effectiveness of this token transportation method. We hypothesize that the semantic importance of the noise space [4, 43] plays a crucial role. By repeatedly supplying information about $x_t$ to the network, the learning of the noise-image mapping becomes more efficient. This is effectively achieved by using a route $r_{0,j}$.\nAdditionally, each route $r_{i,j}$ decreases the computational cost along its length, as tokens are not involved in any computation up to their reintroduction. However, similar to masking, there may exist a combination of selection rate of a given route $r_{i,j}$ and the enhancements in convergence it enables.\nMulti-Route RN for further acceleration. Intuitively, this principle promotes the exploration of consecutive routes, ensuring that more segments of the network can access the noise-space information more regularly. However, applying multiple routes, specifically $r_{0,i}$ and $r_{j,k}$, is more complex than using a single route. This complexity arises because when the first route $r_{0,i}$ terminates, tokens are transferred from the input to layer $D_{i+1}^o$, filling the set of tokens back up to its original size. At this point, a new representation that spans the entire set of tokens is created. As a result, there are two primary objectives that need to be considered:"}, {"title": "5. Experiment", "content": "1. Introducing $x_i$ via route $r_{j,k}$ to layer $D_{k+1}^i$,\n2. Utilizing the already computed information from the layers between $r_{0,i}$ and $r_{j,k}$ to utilize the models capacity efficiently.\nTo address these objectives without forcing a binary decision between them, we choose to use a linear combination of $x_t$ and the representation produced by the direct predecessor of $r_{j,k}$, which is layer $D_{j-1}^i$. Specifically, this combination is defined as:\n$\\t_{,j\u22121} = x_t + D_{j\u22121}^o(\u00b7,\u00b7)$\nThis approach allows both objectives to be satisfied simultaneously by blending the new input with the existing representation from the preceding layer. The multi-route design is further illustrated in Figure 2, which provides a visual representation of how the routes interact within the network.\nMulti-Route Ensemble Loss. In Section 3, we noted that withholding tokens increases task complexity. This added complexity can be managed by adjusting the loss functions, as defined in Equation (8) and Equation (9). The mask operator m determines which tokens the loss function $L_{mae}$ applies to, with the mask corresponding to a specific route $r_{i,j}$.\nWhen we apply multiple routes r, however, we must consider multiple masks m to avoid ignoring the effect of withholding a significant amount of information. Existing literature and formulations typically handle only a single mask operator m, lacking the flexibility to support multiple masking scenarios.\nTo address this, we extend the formulation by introducing a set of routes R. Here, $R_k$ denotes the k-th route in the set, where each route $r_k \\in RN$ includes its own binary mask. We define $V_m^k$ as an operator that selects tokens for the k-th route based on a random mask m. We also introduce $\\bar{V}_{m'}^k$, the complement operator.\nWith these definitions, we update the loss functions $L_{dsm}$ and $L_{mae}$ accordingly.\n$L_{dsm}^k = E_{xo\\sim P_{data},n\\sim N(0,t^2I),m} \\\\  ||V_m^k(D_{R^k} (xo + n, t) -xo)||^2$,\nand\n$L_{mae}^k = E_{xo\\sim P_{data},n\\sim N(0,t^2I),m} \\\\  ||\\bar{V}_m^k(D_{R^k} (xo + n, t) - xo)||^2$,\nwhere $D_{R^k}(\u00b7, \u00b7)$ represents the subnetwork of $D_\\theta(\u00b7, \u00b7)$ defined by the k-th route in R. Our final multi-route ensemble loss is formulated as L:\n$L = \\frac{1}{N} \\sum_{k=1}^N (L_{asm}^k + L_{mae}^k)$,\nwhere N denotes the total number of sequential routes employed.", "5.1. Experimental Details": "Model Architecture. The overall structure of our model follows the two-stage training process of Latent Diffusion Models (LDM) [41] where we first train an autoencoder that translates back and forth between the pixel-level image space and its latent space. We use the pre-trained VAE from Stable Diffusion with the standard downsampling factor of 8. Even though the main requirement for our method is the usage of tokens, we choose a standard DiT [35] in the size XL and a patch size of 2 as our main model for ablation. Further, we also show the generalizability to other architectures like Diffusion-RWKV [12]. For ablations that do not require the largest model size, we either switch to model size B or S and use a patch size of 4, depending on the task. Any changes are clearly stated for each experiment. The application of our method is marked by the additional -TREAD. Additionally, the number of routes applied during training will be noted using TREAD-1 for a single one while we use the respective increment for each additional route."}, {"title": "5.2. Main Result", "content": "Comparison with baseline architectures. We show the improvement using our proposed training strategy over DiT (see Table 1) and RWKV (see Table 2) with our method clearly improving upon the base architectures in any configurations. We compare our DiT S, B, and XL models to their respective counterparts while we present further comparisons to other efficiency-oriented methods using XL-sized models in Table 3. Notably, the improvements extend positively to larger models, leading to an FID of 12.47 with DiT-XL/2+TREAD-1. As discussed in Section 4, we also investigate the application of multiple routes. Using DiT-XL/2+TREAD-2 we are almost able to cut the FID in half (FID 19.47 \u2192 10.63) at 400K steps. Further, we also show DiT-B/2+TREAD-1 (random) as a baseline for randomly selecting the ending location of the route r with a measured FID of 36.80. Its minimal difference to a predefined route (e.g. DiT-B/2+TREAD-1) indicates robusteness against location configuration. This is exlpored in Table 6 in a more thorough manner. Lastly, Table 2 shows the extension of our method to SSMs with RWKV as a representative model. To adjust TREAD to RWKV, the used routes apply a row selection instead of random selectio which shows performance improvements (FID = 53.79) compared to our own Diffusion-RKWV baseline (FID = 59.84).\nComparison with Mask-based methods. First, we examine the speed measured in iterations per second using a single A100 with a batch size of 64 to indicate the cost per iterationn and therefore one factor of the required compu-"}, {"title": "5.3. Performance on other benchmark settings", "content": "We chose ImageNet512 in addition to our main results on ImageNet256 to showcase the scaling to larger token numbers."}, {"title": "5.4. Ablation Study", "content": "We conduct multiple ablation studies to examine the effect that each component has on the performance of TREAD. All experiments are conducted using a DiT-B/2 model and evaluated using 50,000 samples except in Table 6 presented"}, {"title": "6. Conclusion", "content": "In this work, we propose a diffusion training strategy, TREAD, that can be applied to any architecture without requiring extensions or adaptations. It offers increased performance and lower computational cost simultaneously by routing noisy tokens into deeper layers. Further, it offers great flexibility while retaining good robustness through arbitrary route locations. Overall, this results in convergence speedups on ImageNet-1K 256 \u00d7 256 by roughly one order of magnitude compared to the standard training of diffusion models. Our extensive experiments demonstrate that we achieve competitive benchmark results, even with limited computational resources."}, {"title": "Classifier-free Guidance", "content": "We apply Classifier-free Guidance (CFG) to our DiT-XL/2+TREAD-2 . We reach an FID of 3.40 after only 700K training iterations with routing, which is competitive with other efficiency-based methods [58, 59]. We further show the FID without CFG (CFG=1.0) and with CFG (CFG=1.5) during the progress of training in Figure 6"}, {"title": "A. Implementation Details", "content": "A.1. Experimental Configuration\nIn contrast to DiT [35] and MDT [13], which leverage the ADM framework [7], our experimental approach is grounded in the formulation of EDM [24]. Specifically, we implement EDM's preconditioning through a \u03c3-dependent skip connection, utilizing the standard parameter settings.\nThis approach eliminates the necessity to train ADM's noise covariance parameterization, as required by DiT. For the inference phase, we adopt the default temporal schedule defined by:\n$t_{i<N} = (taux + \\frac{i}{N-1}(t_{min} - t_{max}))^{\\rho}$,\nwhere the parameters are set to N = 40, \u03c1 = 7, $t_{max}$ = 80, and $t_{min}$ = 0.002. Furthermore, we employ Heun's method as the ODE solver for the sampling process. This choice has been shown to achieve FID scores comparable to those obtained with 250 DDPM steps while significantly reducing the number of required steps [24, 58].\nThe noise distribution adheres to the EDM configuration, defined by:\n$ln(p_\u03c3) \\sim N(p_{mean}, p_{std})$,\nwith $p_{mean}$ = -1.2 and $p_{std}$ = 1.2. For detailed information, refer to the EDM paper [24]."}, {"title": "A.2. Network Details", "content": "Specific Routes In Table 8, we specify the exact routes r used for each model type. We ensure that the first route always begins at D0 to maximize the potential length that the route r spans. Given that the design space of TREADis inherently constrained by the number of layers, we choose different route endings for various model sizes. In the multi-route setting, we leverage the insights from Table 6 and introduce a few computational blocks between two distinct routes."}, {"title": "A.3. Pseudocode for a forward pass with a single route", "content": "B. More Extensive Comparison\nIn this work, we consider FID [19] to still be the main metric of comparison between generative methods for class-conditional image synthesis even though other metrics exist [25, 33] exist. In Table 10, we provide a more extensive evaluation against both our baselines and as well as an additional set of methods that are often referred to."}]}