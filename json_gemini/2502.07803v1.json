{"title": "Reasoning-as-Logic-Units: Scaling Test-Time Reasoning in Large Language Models Through Logic Unit Alignment", "authors": ["Cheryl Li", "Tianyuan Xu", "Yiwen Guo"], "abstract": "Chain-of-Thought (CoT) prompting has shown promise in enhancing the reasoning capabilities of large language models (LLMs) by generating natural language (NL) rationales that lead to the final answer. However, it struggles with numerical computation, which has somehow led to the development of program-aided techniques. Despite their potential, a persistent challenge remains: inconsistencies between LLM-reported reasoning steps and the logic in generated programs, which we term \"reasoning hallucinations.\u201d This stems from the inherent ambiguities of NL and the statistical nature of LLMs, which often lack rigorous logical coherence. To address this challenge, we propose a novel test-time scaling framework, Reasoning-as-Logic-Units (RaLU), which constructs a more reliable reasoning path by aligning logical units between the generated program and their corresponding NL descriptions. By decomposing the initially generated program into discrete units using static analysis, RaLU engages in an iterative dialogue with the LLM to judge, refine, and explain each unit. A rewind-and-correct mechanism ensures alignment between code statements and task requirements in each unit, ultimately forming a cohesive reasoning path under the program's logic, from which the model reaches a final solution. Our experiments demonstrate that RaLU significantly outperforms existing baselines in mathematical reasoning (GSM8K, MATH) and algorithmic reasoning (HumanEval+, MBPP+), underscoring its potential to advance LLM reasoning and programming by offering enhanced accuracy and interpretability.", "sections": [{"title": "1. Introduction", "content": "Extensive studies have shown that Chain-of-Thought (CoT) (Wei et al., 2022) prompting can improve the reasoning capabilities of large language models (LLMs) (Mondorf & Plank, 2024; Fu et al., 2023; Liu et al., 2023a) by requiring LLMs to generate a rationale before its final decision. Complementary to CoT and its variants, program-aided techniques like Program-of-Thought (PoT) (Chen et al., 2023b) have emerged, which decompose complex reasoning and numerical computation by prompting LLMs to generate programs and use external interpreters to solve mathematical problems. When combined with test-time scaling, which dynamically allocates more computational resources during inference, these methods optimize LLM performance in reasoning tasks, particularly in mathematical and algorithmic domains (Zhong et al., 2024).\nDespite these advancements, both CoT and PoT face significant challenges. The inherent ambiguity and imprecision in natural language (NL) impede precise calculations in CoT-like methods (Yu et al., 2024; Gao et al., 2023). Meanwhile, PoTs simply replace NL reasoning with programs, so they cannot improve LLM in code generation, yet solving algorithmic problems is an important aspect of LLM reasoning. Plus, crafting accurate programs in a single attempt remains challenging (Chen et al., 2024a), so PoT can even introduce more errors than CoT sometimes (Li et al., 2024).\nFurthermore, simple combinations of CoT and PoT cannot yield satisfactory outcomes. Research indicates that guiding an LLM to generate step-by-step analysis in NL before deriving programs may not outperform direct prompting (Huang et al., 2024a). This underperformance can be traced to inconsistencies between reasoning steps and the logic in generated programs, which we term \u201cReasoning Hallucinations.\" The hallucinations manifest as: 1) accurate NL step descriptions but logical errors in individual code statements; 2) missing key steps or inclusion of irrelevant ones; and 3) correct steps misordered or improperly connected. Examples of these three types are presented in Figure 1.\nThese reasoning hallucinations arise from the statistical nature of LLMs, which generate responses based on token predictions rather than true reasoning. LLMs mimic reason-"}, {"title": "2. Related Works and Discussions", "content": "Prompting techniques have greatly improved the reasoning abilities of LLMs. CoT (Wei et al., 2022) is the most popular paradigm, deriving a large number of variants such as Least-to-Most (Zhou et al., 2023) and Auto-CoT (Zhang et al., 2023). The central concept of these approaches is \"divide and conquer\u201d\u2013prompting LLMs to deconstruct complex problems into simpler sub-tasks, systematically address each one by reporting the process and then synthesize a comprehensive final answer. Some studies directly let LLMs write programs to serve as reasoning steps, such as PoT (Chen et al., 2023b) and Program-aided Language models (Gao et al., 2023), decoupling computation from reasoning and language understanding. However, they cannot improve the performance of LLMs in coding tasks and struggle with writing perfect programs within a single query, thus introducing more errors sometimes (Li et al., 2024). Ex-"}, {"title": "2.1. General Reasoning with LLMs", "content": "isting studies have shown that simply mixing code and text during pre-training or instruction-tuning stages can enhance LLM reasoning (Ma et al., 2024), but how to effectively combine them remains under explosion."}, {"title": "2.2. Code Reasoning with LLMS", "content": "Inference-side approaches for coding tasks usually focus on debugging and refining the generated code since it is prone to logic errors, dead loops, and other unexpected behaviors. Many studies (Chen et al., 2023a; 2024b) generate unit tests or feedback from the same LLM to score and refine the generated programs, and ChatRepair (Xia & Zhang, 2023) relies on hand-writing test cases. Another stream of studies combines traditional software engineering tools to improve code quality, including executors (Zheng et al., 2024; Ni et al., 2023) and repair tools (Fan et al., 2023). Recent studies on multi-agent frameworks (Lee et al., 2024; Hong et al., 2024) also achieve advanced performance on coding tasks. They borrow the information provided by software analysis tools and embed such information into prompts to expand the ability bounds of LLMs in code reasoning."}, {"title": "2.3. Test-Time Scaling for LLM Reasoning", "content": "Recent studies have revealed that using more test-time computation can enable LLMs to improve their outputs (Snell et al., 2024). A primary mechanism is to select or vote the best CoT path from multiple independent sampling, such as Best-of-N sampling (Cobbe et al., 2021a) and Self-Consistency (Wang et al., 2023). Innovations like ToT (Yao et al., 2023), Graph-of-Thought (GoT) (Besta et al., 2024), and DeAR (Xue et al., 2024) design search-based schemes to expanding the range and depth of path exploration, though they are often suitable for specific tasks (e.g., the Game of 24) as they require to pre-define a fixed candidate size for each node, leading to redundancy or insufficiency.\nAnother stream of research scales inference time by en-"}, {"title": "3. Reasoning-as-Logic-Units", "content": "We propose a novel structured test-time scaling framework, RaLU, which enforces alignment between NL descriptions and code logic to leverage both sides. Programs ensure rigorous logical consistency through syntax and execution constraints, whereas NL provides intuitive representations with problem semantics and human reasoning patterns.\nSpecifically, RaLU operationalizes this synergy through three iterative stages (as shown in Figure 3): Logic Unit Extraction, Logic Unit Alignment, and Solution Synthesis. The first stage decomposes an initially generated program into atomic logic units via static code analysis. Then, an iterative multi-turn dialogue engages the LLM to 1) explain each unit's purpose in NL, grounding code operations in problem semantics, 2) validate computational correctness and semantic alignment with task requirements, and 3) correct errors via a rollback-and-revise protocol, where detected inconsistencies trigger localized unit refinement. The validated units form a cohesive, executable reasoning path. The final stage synthesizes this path into a human-readable solution, ensuring the final answer inherits the program's logical rigor while retaining natural language fluency.\nIn this way, RaLU can significantly mitigate reasoning hallucinations. First, each unit seamlessly pairs executable code with NL explanations to address the type-one hallucination through explicit alignment of local logic. Second, the LLM focuses on only one unit per response in case of missing a crucial step or introducing an irrelevant step, and iterative verification ensures the LLM to notice all problem constraints Third, these logic units are interconnected rigorously along the program structure, ensuring logical coherence of the reasoning path.\nTo sum up, by structurally enforcing bidirectional alignment between code logic and textual justifications, we build a self-"}, {"title": "3.1. Logic Unit Extraction", "content": "RaLU begins with prompting the LLM to generate an initial program that serves as a reasoning scaffold for the task. While possibly imperfect, this program approximates the logical flow required to derive a solution, providing a structured basis for refinement.\nWe apply static code analysis to construct a Control Flow Graph (CFG), where nodes represent basic blocks (sequential code statements), and edges denote control flow transitions (e.g., branches, loops). A CFG explicitly surfaces a program's decision points and iterative structures, whose details are illustrated in Appendix A.2. RaLU then partitions the code into atomic units by dissecting the CFG at critical junctions\u2014conditional blocks (if/else), loop boundaries (for/while), and function entries. Each unit encapsulates a self-contained computational intent, such as iterating through a list or evaluating a constraint."}, {"title": "3.2. Logic Unit Alignment", "content": "The alignment stage iteratively validates and refines logic units through a stateful dialogue governed by:\n$V_{i} = LLM(S \\oplus \\bigcup_{k=0}^{i-1}U_{k} \\oplus P(U_{i}))$ (1)\nwhere $U_{i}$ denotes the i-th unit, S is the task specification, and the operator $\\oplus$ represents contextual concatenation. $P(U_{i})$ instructs the LLM to handle the i-th unit, where each turn of interaction is responsible for judging the correctness, modifying it upon errors, and explaining it to align with the task specification. Thus, each response $V_{i} = (T_{i}, U_{i})$ comprises a judgment token $J_{i} \\in {OK, WRONG}$ and a refined unit $\\bar{U}_{i}$. The refinement adheres to:\n$\\bar{U}_{i} = \\begin{cases} U_{i} & \\text{if } J_{i} = OK \\\\ LLM_{repair}(S,U_{i},{\\bar{U}_{k}, k < i}) & \\text{otherwise} \\end{cases}$ (2)\nTo prevent error cascades, corrections trigger a partial rewind: the original unit $U_{i}$ is replaced by the refined version $\\bar{U}_{i}$ in the interested reasoning path. Then, $\\bar{U}_{i}$ will be re-validated based on previous units ${U_{k}|k < i}$. This aims to construct a path P with all nodes able to pass self-judging:\n$\\bar{U}_{k} \\in P = {U_{1},\u2026\u2026\u2026, U_{i\u22121}}, T_{k} = OK$. (3)\nThe correctness process terminates under two conditions: 1) fixed-point convergence, i.e., all units satisfy $J_{i}$ ="}, {"title": "3.3. Solution Synthesis", "content": "Through logic unit alignment, RaLU constructs a coherent sequence of verified operations paired with precise NL explanations. This establishes a unified reasoning path that integrates computational logic with interpretive alignment (with problem specifications), ensuring rigorous consistency between code behavior and reasoning steps. Guided by this aligned reasoning path, the LLM synthesizes the structured units into a final solution using the following prompt: \"Based on the previously verified reasoning path, generate a correct program to solve the given problem.\"\nThis dual-anchoring mechanism-enforcing program-executable logic and specification-aligned reasoning-eliminates ambiguities for response generation. We"}, {"title": "4. Experiments", "content": "evaluates RaLU on the whole test sets except MATH. Due to resource limitation, we follow (Miao et al., 2024) to use a subset of MATH (named by MATH-np) taken from (Ling et al., 2023). We report the answer accuracy and pass@1 score for math- and code-reasoning, respectively. Our focus on math- and code-reasoning is due to the availability of well-established benchmarks and the ease of evaluating outputs. RaLU can be directly applicable to other domains with minimal adjustments to the prompts.\nBaselines. We compare RaLU against three categories of baselines without fine-tuning or external information: 1) promoting methods for general purposes: Direct Prompting, Zero-Shot CoT (Wei et al., 2022), ToT (Yao et al., 2023), and Self-Consistency (SC) (Wang et al., 2023). 2) self-correction-based approaches: Self-Calibration (Scal) (Kadavath et al., 2022), and Self-Refine (SR) (Ranaldi & Freitas, 2024); 3) techniques specific for either task: PoT (Chen et al., 2023b), Self-Check (SCheck) (Miao et al., 2024), and rubber-duck debugging derived from Self-Debug (SD) (Chen et al., 2024b). Details of these baselines are provided in Appendix B.2.\nImplementation. We deploy RaLU on three open-source LLMs: Deepseek-V3 (Dec 2024), Qwen2.5-72B-Instruct (Sep 2024), and Llama3.3-70B-Instruct (April 2024). To prevent breaches of anonymity, we do not deploy RaLU on commercial closed-source models such as GPTs and o1. Instead, we compare RaLU with the public results of these closed-source LLMs reported on the leaderboard maintained by (Liu et al., 2023b; Paperwithcode, 2025; Mirzadeh et al., 2024), presented in Appendix C. We set the maximum number for self-correction turns as 3 and the maximum number of candidate solutions/branches as 10 for Self-Consistency and ToT. The temperature parameter is set to 0.7, and the frequency penalty is 0.3 in all experiments."}, {"title": "4.1. Experiment Setup", "content": "Benchmarks. We use four benchmarks: two for mathematical reasoning: GSM8K (Cobbe et al., 2021b) and MATH (Hendrycks et al., 2021), and the other two for code reasoning, HumanEval (Chen et al., 2021) and Mbpp (Austin et al., 2021), along with their extended versions with more test cases (Liu et al., 2023b). See Appendix B.1 for more details about the benchmarks. We"}, {"title": "4.2. Results", "content": "summarizes the performance of RaLU on math and code reasoning. Across all benchmarks and diverse LLM architectures, RaLU consistently outperforms existing baselines, demonstrating its generalizability and robustness. We analyze the advantages through three critical comparisons:\nRaLU v.s. Single-Path Reasoning. Compared to direct prompting, CoT, and PoT (single reasoning path per query), RaLU achieves an average improvement of +12.81% and +14.85% for math and code reasoning, respectively, attributed to its structured decomposition of problems into logical units aligned with programmatic constraints, mitigating the inconsistencies inherent in linear reasoning chains,"}, {"title": "5. Ablation Studies", "content": "To validate the influence of the granularity of the logic unit on RaLU, we replace the CFG-driven decomposition with a line-by-line approach, treating each code line in the origi-"}, {"title": "5.1. CFG v.s. Line-by-line", "content": "nalily generated program as an independent logic unit. As displayed in Figure 4, results show an average performance decline of 7.04% across all benchmarks on Llama3.3, alongside a 37.7% increase in token consumption.\nThe observed decline stems from three intrinsic limitations of line-by-line decomposition. First, programs inherently consist of interdependent code blocks (e.g., loops, conditionals). Splitting them into isolated lines disrupts contextual dependencies between statements. Second, while fine-grained units obscure the hierarchical structure of program logic, LLMs struggle to associate low-level symbol operations (e.g., variable updates) with high-level problem-solving goals (e.g., iterative summation), leading to fragmented explanations and misaligned corrections. Third, line-by-line units amplify error accumulation. For example, a variable initialization error in line 1 may invalidate subsequent lines. However, independent unit verification delays error detection, requiring repetitive corrections across multiple units. In contrast, CFG-based grouping localizes errors within bounded logical scopes.\nThe surge in token usage is intuitive. Each line triggers a separate verification dialogue, multiplying interaction rounds. Moreover, the LLM repeatedly re-encounters overlapping contexts and generates similar NL descriptions across units, wasting tokens on redundant information."}, {"title": "5.2. NL Steps v.s. Logic Units", "content": "To further validate the necessity of program-guided logic units, we remove the initial program generation phase and instead treat each natural language reasoning step under the CoT prompting as an independent unit. This ablation leads to a 5.52% accuracy drop on mathematical tasks and 4.35% score drop on code reasoning, as shown in Figure 5, directly attributable to exacerbated reasoning hallucinations, The amplified decline highlights the fundamental limitations of pure natural language reasoning units.\nWe find a significant number of wrong answers can be attributed to reasoning hallucinations. First, NL steps like \"compute the average by dividing the sum by the count\" often lack operational specificity. While the NL step appears correct, the generated code may implement flawed logic (e.g., total / len(items) without handling empty lists). Unlike CFG units, which enforce alignment through static code analysis, the free-form language allows the LLM to hallucinate plausible-but-incorrect implementations. Moreover, the ambiguity of NL enables conceptual bundling-multiple logical operations (e.g., loop initialization, iteration, termination)-may be compressed into a single step like \"iterate through the list.\" This can lead to code with missing boundary checks or redundant variables, as the LLM fails to decompose high-level descriptions into executable sub-operations. In addition, the NL narrative poorly constrains causal dependencies. For example, a step \u201cupdate the total after checking a certain condition\u201c might lead to code that evaluates the condition after modifying the total. CFG-driven units prevent such misordering by structurally embedding control flows. Appendix D.2 provides a detailed case study of how reasoning hallucinations are introduced if the program-driven logic units in RaLU are replaced by NL steps generated through CoT."}, {"title": "6. Conclusion", "content": "We present Reasoning-as-Logic-Units (RaLU), a pioneering test-time scaling framework designed to tackle the issue of reasoning hallucinations and enhance the reasoning capabilities of LLMs. Unlike existing methods that often encounter logical inconsistencies between reported reasoning steps and generated programs, RaLU effectively extracts logic units from generated programs and aligns them with task requirements using natural language explanations. This method leverages the strengths of both natural language and program logic, resulting in more reliable, interpretable, and transparent LLM reasoning. Experimental results demonstrate that RaLU consistently outperforms existing baselines"}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of machine learning by improving the reliability and accuracy of large language models (LLMs) in complex reasoning tasks. By addressing reasoning hallucinations through logic-aligned hybrid reasoning processes, our framework enhances LLMs' general capabilities to generate coherent and logically consistent solutions, particularly in mathematical and algorithmic domains, without any fine-tuning or re-training. Potential societal benefits include more trustworthy AI systems for education, technical problem-solving, and decision-support applications. There are many broader societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Illustrations of RaLU", "content": "This section showcases how RaLU solves a code generation task using an LLM (e.g., deepseek V3 in this example). The question comes from Mbpp 77. In the initial response, RaLU simply uses direct prompting to ask the LLM to write a program for the given specification."}, {"title": "A.1. A Complete Example", "content": "Original Attempt. [User] You are an expert in Python coding. Wrap your program in a <code> </code> block. No more test cases or any other contents.\nSpecification: Write a function to find the Eulerian number a(n, m), the entry point is \u201ceulerian_num\u201d.\nassert eulerian_num(3, 1) == 4"}, {"title": "A.2. CFG-Driven Logic Units of Code", "content": "Figure 6 displays an example of transforming a program into a CFG and how RaLU organizes the graph to obtain a linear sequence of logic units. To enhance interpretability, units are annotated with human-readable descriptors (e.g., \"LOOP BEGIN: Process each item in list X\"). These labels bridge low-level code operations with high-level problem-solving intent, priming subsequent alignment stages."}, {"title": "A.3. Condition of Applying Self-Repair", "content": "The correctness of a unit U involves two conditions: First, the LLM believes U is correct and U is actually correct, then we have:\n$P(U \\text{ is correct}|J = OK) = \\alpha p$. (11)\nSecond, the probability of U being judged as wrong is (true negative rate plus false positive rate):\n$P(J = \\text{WRONG}) = (1 - \\alpha)p + (1 - \\beta)(1 - p)$. (12)\nThen, the probability of correctly repairing the unit is:\n$P(\\bar{U} \\text{ is correct}|J = \\text{WRONG}) = \\gamma_{repair} \\cdot [(1 - \\alpha)p + (1 - \\beta)(1 - p)]$. (13)\nThus, we can rewrite $p'$ as:\n$p' = \\alpha p + \\gamma_{repair} \\cdot [(1 - \\alpha)p + (1 - \\beta)(1 - p)]$. (14)\nTo compare $p'$ and $p$, we have:\n$p' - p = -p(1 - \\alpha) + \\gamma_{repair} \\cdot [(1 - \\alpha)p + (1 - \\beta)(1 - p)]$\n$= \\frac{[(1 - \\alpha)p + (1 - \\beta)(1 - p)]}{P(J = \\text{WRONG})}(\\gamma_{repair} - \\frac{(1 - \\alpha)p}{(1 - \\alpha)p + (1 - \\beta)(1 - p)})$. (15)"}, {"title": "B. Details of Experiment Setup", "content": "comprehensive benchmark designed to assess the mathematical reasoning capabilities of LLMs. It comprises 12,500 competition mathematics problems, which are carefully curated to cover a wide range of mathematical concepts and varying levels of difficulty. We use a subset taken from (Ling et al., 2023) named MATH-np, specifically tailored to assess the deductive reasoning skills of LLMs. It includes problems that require multi-step reasoning and the application of mathematical concepts in a structured manner."}, {"title": "B.1. Benchmarks", "content": "recognized benchmark to evaluate the reasoning and problem-solving capabilities of LLMs, whose name stands for \u201cGrade School Math 8K,\u201d reflecting its focus on grade school-level math problems. The dataset contains approximately 8,500 carefully crafted math problems. Each problem in GSM8K is presented as a word problem, typically involving basic arithmetic operations (addition, subtraction, multiplication, and division) and sometimes simple algebraic concepts."}, {"title": "B.2. Baselines", "content": "our experiments, we reproduce the baselines strictly following their released code and prompts."}, {"title": "C. Additional Comparisons with Closed-Source LLMs", "content": "As shown exhibits better reasoning capabilities to mainstream closed-source models (i.e., GPT-40, GPT-4-Turbo, and Claude-Sonnet-3.5) and significantly outperforms GPT-3.5-Turbo (+38.16% on average).\n achievements: unit-level correction benefits to solving multi-constraint tasks"}, {"title": "D. Case Studies", "content": "D.1. Reasoning hallucinations in CoT-PoT-integrated method (single reasoning path)\n[User] goal is to write a program to meet the given specification.\n```\ndef add_nums(a, b):\n return a+b"}, {"title": "D.2. Reasoning hallucinations in CoT-step-driven RaLU (ablation study)", "content": "You are an expert in solving math questions.\nQuestion:\nResponse:\n-5"}]}