{"title": "JUST PROPAGATE: UNIFYING MATRIX FACTORIZATION, NETWORK EMBEDDING, AND LIGHTGCN FOR LINK PREDICTION", "authors": ["Haoxin Liu"], "abstract": "Link prediction is a fundamental task in graph analysis. Despite the success of various graph-based machine learning models for link prediction, there lacks a general understanding of different models. In this paper, we propose a unified framework for link prediction that covers matrix factorization and representative network embedding and graph neural network methods. Our preliminary methodological and empirical analyses further reveal several key design factors based on our unified framework. We believe our results could deepen our understanding and inspire novel designs for link prediction methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Link prediction, i.e., predicting the existence of links between nodes, is a fundamental task in graph data mining with important applications such as recommendation systems (Ying et al., 2018), protein interaction prediction (Liu et al., 2019), knowledge discovery (Wang et al., 2017), etc. Many machine learning methods have been developed to tackle the link prediction problem (L\u00fc & Zhou, 2011; Hasan & Zaki, 2011), ranging from early heuristic features (Barab\u00e1si & Albert, 1999; Katz, 1953), statistical machine learning (Tang et al., 2015; Perozzi et al., 2014), to the recent deep learning methods (Hamilton et al., 2017; Wang et al., 2019). For example, matrix factorization (MF) (Koren et al., 2009) is a representative method for link prediction with the assumption that the graph structure has an underlying low-rank structure. Network embedding methods such as DeepWalk (Perozzi et al., 2014) and LINE (Tang et al., 2015) further consider the high-order proximity between nodes. Recently, neural network-based graph learning methods such as NGCF (Wang et al., 2019) and LightGCN (He et al., 2020) have shown superior performance, which has been attributed to the explicit modeling of higher-order neighbors.\nDespite the successes of the existing approaches, the aforementioned studies are relatively independent with different assumptions and design objectives, and there lacks a general understanding of the graph-based machine learning models for link prediction. Therefore, as the first contribution of this work, we propose a unified framework for link prediction methods, covering matrix factorization, network embedding methods such as DeepWalk (Perozzi et al., 2014) and LINE (Tang et al., 2015), and graph neural network-based methods including LightGCN (He et al., 2020). Based on the unified framework, we further conduct in-depth methodological and empirical analyses. We make the following observations.\n\u2022 The overall learning process of the existing methods covered by our unified framework, including the forward inference and backward gradient descent for optimization, is equivalent to a representation propagation procedure.\n\u2022 Propagation kernels, which determine how the representation is propagated and whether higher-order neighbor information can be effectively utilized, greatly affect the performance of different methods for the link prediction task.\n\u2022 By regarding randomly drawn negative samples as negative edges, the existing approaches essentially follow the balance theory of signed graphs in aggregating neighborhoods.\nWe believe our proposed unified framework and preliminary analysis results could deepen our understanding and inspire novel designs for graph-based link prediction methods."}, {"title": "2 PRELIMINARIES", "content": "Firstly, we introduce some notations used in the paper. Consider a graph G = (V,E), where V denotes the node set and E denotes the link set. The adjacency matrix is denoted as A. The goal of link prediction is to discover potential links from the observed links A. We assume the graph is undirected and the nodes have no features. Notice that our problem setting naturally covers the recommendation problem, i.e., V = U \u222a I, and links only exist between U and I.\nIn the common setting of link prediction, there are no explicit negative samples Liu et al. (2023). For example, people seldom explicitly mark whom they do not want to be friends with or products they do not want to buy.\nTherefore, negative sampling is usually used in link prediction to sample from pairs of nodes that do not have links as pseudo negative samples during the optimization procedure. We denote the adjacency matrix for sampled negative links as B \u2208 {0,1}^{|V|\u00d7|V|}, where B_{i,j} = 1 represents that node i and j has a negative link. We summarize other notations in Appendix A."}, {"title": "2.2 Loss FUNCTION", "content": "In this paper, we focus on a widely adopted loss function, the binary cross-entropy (BCE) loss (He et al., 2017) and leave exploring other loss functions as future works. The basic BCE loss with a L2 regularization term is formulated as\n$L_{BCE} = \\frac{1}{2} \\sum_{v \\in V} (\\sum_{i \\in N_A(v)} log\\sigma(X_v X_i^T) + \\sum_{j \\in N_B(v)} log\\sigma(-X_v X_j^T)) + \\frac{\\beta}{2} ||X||^2_2,$\nwhere N_A(v) and N_B(v) denote the positive and negative neighbors of node v, \u03c3(\u00b7) is the Sigmoid activation function, A and B are hyper-parameters to control the strength of the corresponding terms, and X denotes the representation of nodes. Eq. (1) can be written in an equivalent matrix form:\n$L_{BCE} = -\\frac{1}{2} (1 [log (\\sigma(XX^T)) \\odot A] + 1 [log (-\\sigma(XX^T)) \\odot B]) 1^T + \\frac{\\beta}{2} ||X||^2_2,$\nwhere 1 is a 1 \u00d7 |V| row matrix with the values being ones and \u2299 is the Hadamard product."}, {"title": "2.3 A BRIEF INTRODUCTION TO CONSIDERED LINK PREDICTION MODELS", "content": "In this section, we briefly introduce link prediction models considered in our framework. We consider four representative methods divided into three categories:\n\u2022 Matrix factorization, which directly optimizes the representation matrix X using the loss function.\n\u2022 DeepWalk and LINE: these network embedding methods also optimize the representation matrix X, but with different loss functions that consider the high-order neighborhoods between nodes.\n\u2022 LightGCN: a linear GNN model that first propagates the representation as $X = \\sum_{k=0}^K \\frac{1}{K+1} \\tilde{A}^k X$,\n$\\tilde{A}$ is a normalized adjacency matrix, e.g., $\\tilde{A} = D_A^{-\\frac{1}{2}} A D_A^{-\\frac{1}{2}}$, $D_A$ is the diagonal degree matrix, and then optimize X using the loss function.\nFollowing the standard training paradigm in machine learning, we assume all these methods are optimized by the full-batch gradient descent and denote the node representations in the mth optimization step as $X^{(m)}$, i.e., $X^{(m+1)} = X^{(m)} - \\alpha \\nabla_{X^{(m)}} L$."}, {"title": "3 OUR PROPOSED FRAMEWORK", "content": "Although the aforementioned methods appear to have unrelated design objectives and loss functions based on diverse motivations, we find that they can be unified into a general framework that only"}, {"title": "3.1 THE UNIFIED FRAMEWORK", "content": "performs forward propagation, i.e., even the back propagation for gradient descends is equivalent to the forward propagation. Specifically, we have:\n$X^{(m+1)} = H^{(m+1)}X^{(m)}$\n$= (c_1 I + c_2 P_{a_1,b_1} (\\tilde{A}) [K^{(m+1)} (A) - \\Lambda K^{(m+1)} (B)] P_{a_1,b_1} (\\tilde{A}))X^{(m)},$\nwhere $c_1, c_2$ are constants that depend on the learning rate \u03b1 and regularization term \u03b2, $a_1, b_1 \\in \\mathbb{N}$ are model-specific constants, and I is the |V|\u00d7 |V| identity matrix.\n$X^{(m+1)} = H^{(m+1)}X^{(m)}$\n$= (c_1 I + c_2 P_{a_1,b_1} (\\tilde{A}) [\\Chi^{(m+1)} (A) - \\Chi^{(m+1)} (B)] P_{a_1,b_1} (\\tilde{A}))X^{(m)}$\n$= (c_1 I + c_2 [\\Chi^{(m+1)}(A) - \\Chi^{(m+1)}(B)])X^{(m)}$\n$= (c_1 I + c_2 [\\mathcal{S}^{(m+1)} A - A \\mathcal{S}^{(m+1)} B])X^{(m)}$\nThe detailed proof and the correspondence of different methods to our framework are provided in Appendix B and Appendix C, respectively. Though Eq. (4) appears to be complicated, we elaborate the terms one by one.\nFirstly, $H^{(m+1)} \\in \\mathbb{R}^{|V|\\times|V|}$ is the propagation kernel that determines how the representation is propagated. It is composed of three major terms: a high-order proximity matrix $P_{a_1,b_1} (\\tilde{A})$, a positive link kernel $K^{(m+1)} (A)$, and a negative link kernel $K^{(m+1)} (B)$. The high-order matrix is defined as:\n$P_{a,b}(\\tilde{A}) = \\sum_{r=a}^{b} \\frac{1}{r-a+1} \\tilde{A}^r,$\ni.e., it is a weighted sum of the ath order to the bth order normalized adjacency matrix $\\tilde{A}$, a < b.\nThe positive and negative link kernels are defined as follows:\n$K^{(m+1)}(A) = \\mathcal{S}_A^{(m+1)} (c_3 P_{a_2,b_2} (\\tilde{A}) + (1 - c_3)A)$\n$K^{(m+1)} (B) = \\mathcal{S}_B^{(m+1)} (c_3 B + (1 - c_3)B),$\nwhere $c_3 \\in \\{0,1\\}$, B is the normalized adjacency matrix of negative links, e.g., $B = D_B^{-\\frac{1}{2}} B D_B^{-\\frac{1}{2}}$ and $\\mathcal{S}_A^{(m+1)}$, $\\mathcal{S}_B^{(m+1)}$ are score matrices that measure the difference between the similarity calculated by the representation vectors and the target link as follows\n$\\mathcal{S}_A^{(m+1)} = 1 - \\sigma (\\mathcal{X}^{(m)} \\mathcal{X}^{(m)T})$, $\\mathcal{S}_B^{(m+1)} = \\sigma (\\mathcal{X}^{(m)} \\mathcal{X}^{(m)T}),$\nwhere $\\mathcal{X}^{(m)} = P_{a_1,b_1} (\\tilde{A})X^{(m)} = \\sum_{r=a_1}^{b_1} \\frac{1}{r-a_1+1} \\tilde{A}^r X^{(m)}$ is the propagated node representation.\nFrom Eq. (7), score matrices equal 0 when the representation perfectly reconstructs the training data.\nIn summary, all these four representative methods can be unified in Eq. (4), which indicates that their underlying mechanism shares strong connections. In the next section, we conduct in-depth analyses based on this framework."}, {"title": "3.2 REVISITING THE EXISTING LINK PREDICTION METHODS", "content": "In this section, we revisit the existing methods based on our proposed framework.\nFirstly, it is easy to see that all the four covered methods have analytical solutions only using the forward propagation, i.e., doing back propagation for gradient descends ends up being equivalent to changing the forward propagation kernel. Therefore, our framework sheds lights on novel design principles for link prediction that do no involve computationally expensive back propagations.\nIn view of signed graphs Liu et al. (2021); Liu (2022) that sampled negative links correspond to real negative connections, the propagation step in Eq. (4) satisfies the balance theory, i.e., a sociological assumption that \"the enemy of my enemy is my friend\". Though this assumption is never explicitly"}]}