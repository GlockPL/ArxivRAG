{"title": "FACE-HUMAN-BENCH: A COMPREHENSIVE BENCHMARK OF FACE AND HUMAN UNDERSTANDING FOR MULTI-MODAL ASSISTANTS", "authors": ["Lixiong Qin", "Shilong Ou", "Miaoxuan Zhang", "Jiangning Wei", "Yuhang Zhang", "Xiaoshuai Song", "Yuchen Liu", "Mei Wang", "Weiran Xu"], "abstract": "Faces and humans are crucial elements in social interaction and are widely included in everyday photos and videos. Therefore, a deep understanding of faces and humans will enable multi-modal assistants to achieve improved response quality and broadened application scope. Currently, the multi-modal assistant community lacks a comprehensive and scientific evaluation of face and human understanding abilities. In this paper, we first propose a hierarchical ability taxonomy that includes three levels of abilities. Then, based on this taxonomy, we collect images and annotations from publicly available datasets in the face and human community and build a semi-automatic data pipeline to produce problems for the new benchmark. Finally, the obtained Face-Human-Bench comprises a development set with 900 problems and a test set with 1800 problems, supporting both English and Chinese. We conduct evaluations over 25 mainstream multi-modal large language models (MLLMs) with our Face-Human-Bench, focusing on the correlation between abilities, the impact of the relative position of targets on performance, and the impact of Chain of Thought (CoT) prompting on performance. Moreover, inspired by multi-modal agents, we also explore which abilities of MLLMs need to be supplemented by specialist models.", "sections": [{"title": "INTRODUCTION", "content": "Faces and humans are always the most crucial elements of photos and videos in our everyday lives. Consequently, they are also critical focuses in multi-modal AI applications. In the past two years, ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b) have achieved great success with impressive instruction-following and multi-modal understanding capabilities respectively. Numerous excellent works (Liu et al., 2023b; Zhu et al., 2024; Dai et al., 2023; Bai et al., 2023) from the open-source community have followed, collectively presenting the immense potential of multi-modal assistants. Since faces and humans are central to social interaction, a deep understanding of this information can make multi-modal assistants achieve improved response quality and broadened application scope. For instance, in movie understanding (Yue et al., 2023; Han et al., 2023; Wang et al., 2024), identifying characters is a prerequisite for multi-modal assistants to describe the plot accurately. In multi-modal human-computer interaction (Fu et al., 2024), perceiving expressions and body language can help multi-modal assistants accurately understand the context, generating more personalized and humanized responses. In media forensics (Liu et al., 2024b;c; Jia et al., 2024), determining whether deepfake artifacts exist on a face is crucial for multi-modal assistants to detect misinformation.\nComprehensive and scientific evaluation is the foundation for researching applications of multi-modal assistants related to \"faces and humans.\" Existing benchmarks Fu et al. (2023); Li et al. (2023a); Liu et al. (2023c) for large multi-modal models typically involve limited abilities of face and human understanding, such as celebrity recognition, action recognition, identity reasoning, and social relation, leaving many important abilities unexplored. On the other hand, since face and human understanding is one of the earliest research topics in artificial intelligence, there are numerous datasets available for evaluating the performance of specialist models. The images and annotations from these datasets can serve as original material to evaluate multi-modal assistants."}, {"title": "FACE-HUMAN-BENCH", "content": null}, {"title": "HIERARCHICAL ABILITY TAXONOMY", "content": "As shown in Figure 1, the proposed ability taxonomy includes three levels. Level-1 (L1) has two research perspectives. From the target perspective, L1 includes face understanding and human understanding. From the cognitive process perspective, L1 includes perception and reasoning. In our evaluation, perception involves direct comprehension of only one target, while reasoning requires synthesizing information from multiple targets and environments to conclude. There are ten abilities in total at Level-2 (L2). Five are focused on faces: facial attribute recognition, age estimation, facial expression recognition, face attack detection, and face recognition, and five are focused on humans: human attribute recognition, action recognition, spatial relation understanding, social relation understanding, and person re-identification. It should be noted that at L2, there are 6 abilities under perception and 4 abilities under reasoning. Level-3 (L3) further refines the ability dimensions at L2. Facial expression recognition can be categorized into basic and compound types. Face attack detection includes deepfake detection and face anti-spoofing. Face recognition involves five scenarios: basic, cross-pose, cross-age, similar-looking, and occluded. Spatial relation understanding concerns relative position and count. Social relation understanding includes social relationship recognition and identity reasoning. Please refer to Appendix A.1 for detailed definitions and examples of these abilities."}, {"title": "SEMI-AUTOMATIC DATA PIPELINE", "content": "Based on the hierarchical ability taxonomy defined in Section 2.1, we collect 16 public datasets from the face and human community, covering each L3 ability. Then, we employ a semi-automatic data pipeline to produce problems for the Face-Human-Bench.\nAn original sample S\u2081 from public datasets can be represented as a binary tuple (Ii, Li), where Ii denotes an original image set and Li denotes an original label set. Note that we use \"image set\" and \"label set\" to describe the composition of one sample because, in some datasets, a single sample may consist of multiple images or labels. For instance, in face recognition, a sample includes a pair of"}, {"title": "EXPERIMENT", "content": null}, {"title": "EXPERIMENTAL SETUP", "content": "Evaluation Protocols. We use the weighted accuracy of multiple-choice problems as the evaluation score. As shown in Figure 1, the proportion of the sectors represents the weight of the corresponding abilities in the overall score on the Face-Human-Bench. Note that we set equal weights for each L2 ability. 1 To prevent models from favoring certain option letters over others, we shuffle the options to ensure the correct answers are evenly distributed across all option letters. During the testing, we add some constraint instructions to ensure MLLMs output only option letters as much as possible. 2 After obtaining the MLLM's response, we use regular expressions to extract the option letters. If this fails, we follow the implementation of MMBench (Liu et al., 2023c) using ChatGPT (OpenAI, 2023a) to extract the choices. 3\nModels. We evaluate 25 MLLMs in different sizes from 13 model families. For open-source models, we select LLaVA-13B (Liu et al., 2023b), LLaVA-1.5-7B/13B (Liu et al., 2023a), LLaVA-Next-7B/13B/34B (Liu et al., 2024a), MiniGPT-4-7B/13B (Zhu et al., 2024), InstructBLIP-7B/13B (Dai et al., 2023), Qwen-VL-Chat (Bai et al., 2023), InternLM-XComposer2-VL-7B (Dong et al., 2024), Yi-VL-6B (Young et al., 2024), InternVL-Chat-v1.2-Plus (Chen et al., 2023), InternVL-Chat-v1.5 (Chen et al., 2023), DeepSeek-VL-1.3B/7B-Chat (Lu et al., 2024), CogVLM2-19B-Chat (Hong et al., 2024), GLM-4V-9B (Hong et al., 2024), LLaVA-OneVison-0.5B/7B (Li et al., 2024). For closed-source models, we use Gemini-1.5-Pro (Reid et al., 2024), Claude-3.5-Sonnet (Anthropic, 2024a), GPT-4V (OpenAI, 2023b), and GPT-40 OpenAI (2024). For more details on these models, please refer to Appendix B.1."}, {"title": "MAIN RESULTS", "content": "Table 1 shows the performance of all evaluated MLLMs at different levels of abilities on the Human-Face-Bench (English) 4 under the zero-shot setting. Overall scores range from 27.9% to 76.4%, demonstrating the effectiveness of the Face-Human-Bench in distinguishing the abilities of MLLMs\nin face and human understanding. We visualize the overall scores of MLLMs in Figure 2. Our findings can be summarized as follows.\nOverall Performance. (1) The top three performing open-source models in terms of the overall score are InternvL-Chat-v1.2-Plus, LLaVA-Next-34B, and InternVL-Chat-v1.5. These models' LLMs have the largest number of parameters among all open-source models we evaluate. (2) Generally, open-source models within the same series tend to show improved performance with increasing parameter scale. However, there are exceptions; for instance, the 13B version of LLaVA-1.5"}, {"title": "CORRELATION BETWEEN ABILITIES", "content": "In this section, we examine whether improving one ability in a model will enhance another by calculating the Pearson Correlation Coefficient between abilities at different levels, using the evaluation scores from Section 3.2. At L1, the correlation coefficient of face and human understanding is 0.94 and the correlation coefficient of perception and reasoning is 0.79, both indicating significant positive correlations, as shown in Figure 3(a) and Figure 3(b). We further investigate the correlations between L2 abilities, resulting in the correlation coefficient matrix shown in Figure 3(c). For clarity, we have drawn this as a lower triangular matrix. Our findings can be summarized as follows: (1) For the three face understanding abilities-facial attribute recognition, age estimation, and facial expression recognition-there are high positive correlations between each pair. (2) For the four human understanding abilities-human attribute recognition, action recognition, spatial relation understanding, and social relation understanding-there are high positive correlations between each pair. (3) For the three face understanding abilities and four hu-"}, {"title": "RELATIVE POSITION OF TARGETS", "content": "We investigate the impact of the relative position of targets on performance in four L3 abilities: facial attribute recognition, age estimation, basic expression recognition, and human attribute recognition. As shown in Figure 4, for the three face understanding abilities, we provide both the original and cropped versions, where only one person is included but the relative position varies. For human attribute recognition, we offer box-added and cropped versions. In the box-added version, multiple people are included, with the target to be discussed indicated by a red box. Figure 5 illustrates the performance differences between the two versions across various models. Our findings can be summarized as follows.\nFace Understanding Abilities. (1) Preferences for either version depend on the model and the ability, with no overarching trend observed. (2) A model's preference can vary across different face understanding abilities. For example, Yi-VL-6B shows no significant preference for facial attribute recognition, prefers the original images for age estimation, and favors cropped images for basic expression recognition. We think that this phenomenon may occur because MLLMs have been trained using images with different target relative positions when aligning visual information for different facial features.\nHuman Attribute Recognition. The majority of models perform better on the cropped version. This indicates that these models still struggle to accurately understand a specific individual when there are multiple people in the image.\nWe define the relative position sensitivity score (RPSS) as the sum of the absolute differences in scores between the two versions across the four tasks. This metric can serve as an effective reference"}, {"title": "COT PROMPTING", "content": "In this section, we select InternVL-Chat-v1.Plus and GPT-40 to explore whether incorporating hints and Chain-of-Thought (CoT) instructions in the prompts can enhance the MLLMS' performance. These two models have achieved the best overall performance in the main experiment among open-source models and closed-source models respectively. A hint involves tips on how to answer the question. For example, the hint for person re-identification is \"if two people have significant differences in posture and their faces are relatively blurry, the main basis for determining whether they are the same person is their clothing characteristics.\" CoT instructions, on the other hand, guide MLLMs to articulate the reasoning process leading to the answer. The vanilla CoT instruction simply requires the model to \"analyze the question and options step by step\", whereas task-specific\nTable 2 presents the performance of InternVL-Chat-v1.2-Plus and GPT-40 after incorporating hints and three different CoT settings. The results indicate that including hints and CoT instructions does not improve the performance of the open-source model; in fact, it may even cause a slight performance decline. By analyzing the outputs, we find that the open-source model does not provide rationales in its responses after adding CoT instructions to prompts. We believe this could be due to the model's insufficient generalization capabilities, preventing it from understanding the CoT instructions. In contrast, the closed-source GPT-40 shows significant performance improvements.\nFor more models' RPSS, please refer to the Appendix C.4."}, {"title": "SPECIALIST MODELS SIGNIFICANTLY OUTPERFORMING MLLMS", "content": "In this section, we explore whether specialist models corresponding to 13 L3 abilities can be used to enhance MLLMs. 8 We directly test the performance of MLLMs using original datasets from the face and human community to facilitate comparison with specialist models. We design a set of prompt templates to transform the classification problems into multiple-choice problems and the regression problems (age estimation and crowd counting) into fill-in-the-blank problems. 9 Specialist models are generally trained and tested on data from the same distribution. They can achieve high performance even if the test labels contain noise. However, the visual information learned by MLLMs and the original datasets used for testing may exhibit data distribution bias. To enable an effective comparison, we utilize early specialist models (which emerged after the widespread adoption of deep learning) as a reference to judge the performance of MLLMs on these tasks.10\nWe further define the relative performance score S to normalize performances across different tasks: \nS = (Pm-Pr)/(Ps \u2013 Pr), where Pm is the performance of the MLLM. Here, we take the highest-performing model among InternVL-Chat-v1.2-Plus, LLaVA-Next-34B, and InternVL-Chat-v1.5 (the top three models in the main experiment). Pr is the performance of random responses, and Ps is the performance of the early specialist model. This metric typically ranges from 0 to 1, where a higher relative score indicates stronger abilities of MLLMs on the corresponding task. A relative"}, {"title": "RELATED WORK", "content": "Evaluation of MLLMs about Face and Human Understanding. Currently, there is no dedicated benchmark evaluating the face and human understanding abilities of MLLMs. Some efforts aim at comprehensively benchmarking MLLMs, containing some ability dimensions about face and human understanding. LAMM (Yin et al., 2023) evaluates 9 different 2D vision tasks using 11 existing public datasets. Among these, the facial classification task utilizes the CelebA (Liu et al., 2015) dataset to evaluate the accuracy of smile detection and hair attribute classification. MME (Fu et al., 2023) includes the celebrity recognition ability, requiring MLLMs to respond with Yes/No answers. SEED-Bench (Li et al., 2023a) includes the action recognition ability, where the inputs consist of multiple frames taken from a video, and MLLMs are required to choose the correct answer from four descriptions. MMBench (Liu et al., 2023c) includes the most extensive set of abilities related to faces and humans: celebrity recognition, action recognition, identity reasoning, and social relation, all of which are tested using multiple-choice problems. Considering the importance of faces and humans in multimedia, these evaluations are insufficient.\nFace and Human Understanding. Face and human understanding is among the earliest research topics in artificial intelligence with successful applications. During the 2010s, the introduction of deep learning, particularly convolutional neural networks, significantly advanced face and human perception. In that era, numerous high-quality datasets were proposed for training and evaluating tasks of face attribute recognition (Liu et al., 2015), age estimation (Rothe et al., 2015; Escalera et al., 2015; Zhang et al., 2017), facial expression recognition (Barsoum et al., 2016; Li et al., 2017b; Mollahosseini et al., 2019), deepfake detection (R\u00f6ssler et al., 2019; Dolhansky et al., 2019), face anti-spoofing (Liu et al., 2018; 2019), face recognition (Yi et al., 2014; Guo et al., 2016; Zheng et al., 2017; Deng et al., 2017; Zheng & Deng, 2018), human attribute recognition (Li et al., 2016; Liu et al., 2017), human-object interaction detection (Gupta & Malik, 2015; Xu et al., 2019), crowd counting (Zhang et al., 2016), social relationship recognition Sun et al. (2017); Li et al. (2017a) and person re-ideitification Li et al. (2014); Zheng et al. (2015). Entering the 2020s, a new paradigm emerged, which initially pre-trains a task-agnostic backbone and then based on this, trains a unified face or human model (Ci et al., 2023; Wang et al., 2023b; Qin et al., 2024) to simultaneously handle multiple face and human understanding tasks within a unified structure. In our evaluation, we observe that in certain tasks, MLLMs do not perform as well as specialist models. Utilizing these unified face or human models as the specialist models to help MLLMs can greatly facilitate deployment."}, {"title": "CONCLUSION", "content": "In this work, we propose the hierarchical Face-Human-Bench, the first benchmark specifically designed to evaluate MLLMs' face and human understanding abilities. We comprehensively and scientifically assess the performance of 25 mainstream MLLMs with our benchmark. We reveal the correlations between abilities and explore the impact of the relative position of targets and CoT prompting on the performance of MLLMs. Inspired by multimodal agents, we investigate which abilities of MLLMs need to be supplemented by specialist models. Our work will provide the face and human community valuable insights on how to more effectively leverage multi-modal assistants in applications related to \"faces and humans.\""}]}