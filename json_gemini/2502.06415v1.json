{"title": "SYSTEMATIC OUTLIERS IN LARGE LANGUAGE MODELS", "authors": ["Yongqi An", "Xu Zhao", "Tao Yu", "Ming Tang", "Jinqiao Wang"], "abstract": "Outliers have been widely observed in Large Language Models (LLMs), significantly impacting model performance and posing challenges for model compression. Understanding the functionality and formation mechanisms of these outliers is critically important. Existing works, however, largely focus on reducing the impact of outliers from an algorithmic perspective, lacking an in-depth investigation into their causes and roles. In this work, we provide a detailed analysis of the formation process, underlying causes, and functions of outliers in LLMs. We define and categorize three types of outliers\u2014activation outliers, weight outliers, and attention outliers\u2014and analyze their distributions across different dimensions, uncovering inherent connections between their occurrences and their ultimate influence on the attention mechanism. Based on these observations, we hypothesize and explore the mechanisms by which these outliers arise and function, demonstrating through theoretical derivations and experiments that they emerge due to the self-attention mechanism's softmax operation. These outliers act as implicit context-aware scaling factors within the attention mechanism. As these outliers stem from systematic influences, we term them systematic outliers. Our study not only enhances the understanding of Transformer-based LLMs but also shows that structurally eliminating outliers can accelerate convergence and improve model compression. The code is avilable at https://github.com/an-yongqi/systematic-outliers.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities (Brown, 2020; Achiam et al., 2023; Touvron et al., 2023a), making them a central topic of research across various domains. Numerous studies have uncovered intriguing phenomena within these models (Dettmers et al., 2022; Xiao et al., 2023b; Sun et al., 2024), which are crucial for advancing the understanding and application of LLMs. Among these phenomena, the presence of outliers, which are values that deviate significantly from the average of their distribution, has garnered considerable attention (Zhang et al., 2024; Kovaleva et al., 2021; Paglieri et al., 2024).\nHowever, research on outliers in LLMs predominantly emphasizes mitigating their impact through algorithmic techniques, often neglecting a thorough exploration of their underlying causes and functional roles. This narrow focus results in two key shortcomings: first, it limits our understanding of why outliers occur and how they influence model behavior (Yin et al., 2023; Xiao et al., 2023a; Hooper et al., 2024); and second, it overlooks the interrelationships between different types of outliers, treating them in isolation rather than as part of a comprehensive, systematic framework (Zhang et al., 2024; Sun et al., 2024; Liao & Monz, 2024). These gaps hinder a deeper understanding of the mechanisms underlying LLMs and constrain opportunities for more effective optimization and broader applications.\nTo address these gaps, we systematically analyze outliers in LLMs, focusing on their formation, distribution, and roles within the models. We begin by defining and categorizing three types of outliers:"}, {"title": "2 RELATED WORK", "content": "Outliers in Large Language Models. Outliers in LLMs refer to values that deviate significantly from the average of their distribution (Dettmers et al., 2022). Studies have documented various types of outliers in weights, activations, and attention scores, highlighting their presence and impact. For instance, Dettmers et al. (2022) identified activation outliers and proposed quantization techniques to mitigate their effects. Sun et al. (2024) explored the role of large activations as biases in the attention mechanism. Similarly, Zhang et al. (2024) analyzed weight outliers in LayerNorm layers, demonstrating their importance for maintaining language modeling capabilities in models like GPT-2 and LLaMA2-13B. Additionally, Xiao et al. (2023b) introduced the concept of the \"Attention Sink,\" which occurs when a few keys dominate attention scores.\nWhile previous studies recognize the presence of outliers, they typically focus on specific cases or task-specific solutions like quantization and pruning. In contrast, our work provides a systematic categorization of outliers\u2014activation, weight, and attention outliers\u2014and reveals their interconnections and collective influence on the attention mechanism in LLMs.\nThe Impact of Outliers on Model Performance and Compression. Outliers significantly affect both the performance and efficiency of LLMs. Previous research has shown that removing outliers without proper handling can severely degrade performance (Puccetti et al., 2021; Kovaleva et al., 2021; Zhang et al., 2024). In quantization, outliers amplify rounding and clipping errors, leading to substantial quantization losses (Wei et al., 2022; Nrusimha et al., 2024; Lin et al., 2024). Similarly, magnitude-based pruning strategies face challenges in maintaining model performance when outliers are present (Sun et al., 2023). Yin et al. (2023) observed that outliers correlate strongly with layer sparsity, further complicating pruning approaches. Moreover, in KV cache compression, Xiao et al. (2023b) found that attention score outliers associated with specific tokens play a critical role in preserving context.\nDespite extensive research on the adverse effects of outliers, their formation mechanisms and functional roles remain largely unexplored. Existing studies focus on mitigating their impact but lack a systematic investigation of their origins. In contrast, our work explores the formation of outliers within the self-attention mechanism, revealing their role as implicit, context-aware scaling factors and proposing structural solutions to enhance model convergence and compression efficiency."}, {"title": "3 SYSTEMATIC OUTLIERS: DEFINITION, EXISTENCE, AND LOCALIZATION", "content": "Definition of Outliers in LLMs. Outliers in LLMs are values that deviate significantly from the average of their distribution, often surpassing a threshold \u03c4. From Figure 1, we observe that three distinct types of outliers-activation outliers, weight outliers, and attention outliers-appear systematically in four key locations within LLaMA2-7B. The specific positions of these outliers are"}, {"title": "3.1 WHERE ARE ACTIVATION OUTLIERS LOCATED?", "content": "Activation outliers manifest as abnormally large values in specific sequence and feature dimensions, as shown in Figures 1(a) and 1(c). These outliers appear in two distinct activation types: layer outputs $h_e$ and down-projection inputs $x_{down}$. We analyze their distributions across layers, sequences, and feature dimensions to understand their patterns. Detailed experimental settings are provided in Appendix B.1.\nFor layer outputs, Figure 3(a) reveals that activation outliers are concentrated in shallow layers, persist through middle layers, and diminish in the final layers. Additionally, Figure 3(b) shows that these outliers are associated with start tokens and weak semantic tokens, such as \".\" and \"_\", while Figure 3(c) highlights their confinement to specific feature dimensions.\nFor down-projection inputs, Figure 4 indicates that activation outliers are confined to a few shallow and deep layers. Similar to layer outputs, these outliers are linked to start tokens and weak semantic tokens and are concentrated in a limited set of feature dimensions.\nIn summary, activation outliers in $x_{down}$ are restricted to shallow and deep layers, while those in $h_e$ persist from shallow to middle layers. Both types predominantly affect fixed feature dimensions and tokens with weak semantic content."}, {"title": "3.2 WHERE ARE WEIGHT OUTLIERS LOCATED?", "content": "As shown in Figure 1(b), weight outliers are characterized by extreme values concentrated in specific columns. To quantify this, we compute the extremal ratio, defined as the ratio of the maximum value to the mean value within each column, since large column values directly influence the corresponding output activations.\nFigure 5(a) illustrates the extremal ratio across layers and modules in LLaMA2-7B, highlighting that weight outliers are concentrated in the down-projection matrices $W_{down}$ of the second layer and the last two layers. Figures 5(b) and 5(c) provide detailed visualizations of these outliers in the last two layers."}, {"title": "3.3 WHERE ARE ATTENTION OUTLIERS LOCATED?", "content": "To understand the distribution of attention outliers, we analyze cumulative attention scores on the attention weights $A$ across layers, keys, and heads.\nFigure 6 highlights key patterns in the distribution of attention outliers. In Figure 6(a), the two largest cumulative attention scores per layer show that attention outliers persist across all layers. Figure 6(b) categorizes keys with outliers, indicating a strong association with start tokens and weak semantic tokens. Figure 6(c) visualizes score fluctuations across heads, revealing significant variation between heads and layers.\nIn summary, weight outliers in LLaMA2-7B are primarily located in the MLP's down-projection matrices, concentrated in specific shallow and deep layers."}, {"title": "4 SYSTEMATIC OUTLIERS ARE SIMULTANEOUS AND INTERCONNECTED", "content": "Systematic outliers are not isolated phenomena; instead, they exhibit strong correlations across feature and sequence dimensions as well as layers. Understanding these interconnections and their lifecycle is crucial for uncovering how outliers propagate through the model and affect computations. This section analyzes these relationships in detail, laying the groundwork for the next section, where we hypothesize and validate their functional roles."}, {"title": "4.1 How ARE THESE OUTLIERS RELATED?", "content": ""}, {"title": "4.2 THE LIFECYCLE OF SYSTEMATIC OUTLIERS", "content": "The correlations between different types of outliers suggest a deeper connection underlying their occurrences. By visualizing their lifecycle, we observe a chain of interactions: weight outliers lead to activation outliers, which then influence attention outliers, with this influence extending to non-outlier tokens.\nThe Emergence of Activation Outliers from Weight Outliers. In the second layer of LLaMA2-7B's MLP, weight outliers in the up- and gate-projection matrices cause extreme neuron responses. These are amplified by the SiLU activation function (Elfwing et al., 2018) and GLU operation (Shazeer, 2020), resulting in activation outliers that are up to a thousand times the average magnitude. Additionally, weight outliers in the down-projection matrix amplify activations in specific feature dimensions (e.g., 1415th and 2533rd), dominating the residual connection and influencing the final output (see Figure 7)."}, {"title": "The Spread of Attention Outliers from Activation Outliers.", "content": "Activation outliers propagate into the attention mechanism through their influence on query, key, and value vectors. In the third layer's Multi-Head Attention (MHA), we observe that tokens with activation outliers exhibit alignment in the 58th and 122nd dimensions of both query and key vectors. This alignment significantly increases the dot product between these vectors, leading to disproportionately high attention weights assigned to the outlier tokens (see Figure 8). As a result, the attention mechanism focuses heavily on these tokens, amplifying their influence across layers.\nInterestingly, despite receiving significant attention, the value vectors corresponding to these tokens show comparatively smaller magnitudes. This indicates that while outlier tokens attract attention, they may contribute less directly to the final output. This phenomenon could imply a mechanism where the model leverages these tokens as \"anchors\" to affect attention outputs, we explore it further in Section 5.\nAdditionally, we find that this pattern-alignment in query and key dimensions and reduced magnitude in value dimensions is consistent across most heads and layers. This consistency highlights the systematic nature of how activation outliers propagate their influence through the attention mechanism."}, {"title": "The Disappearance of Outliers in the Final Layers.", "content": "Outliers gradually vanish in the final layers due to cancellation by values of opposite signs. This neutralization occurs progressively rather than abruptly (see Figure 9). As activation outliers diminish, attention outliers are similarly reduced, with some heads in the final layer showing no outliers at all."}, {"title": "Summary.", "content": "In the lifecycle of systematic outliers, weight outliers drive the emergence of activation outliers, which propagate anomalies into the attention mechanism. This interdependence extends their influence to non-outlier tokens. These findings reveal that systematic outliers are intrinsically linked to the attention mechanism, setting the stage for the next section, where we hypothesize and validate their functional roles."}, {"title": "5 SYSTEMATIC OUTLIERS AS CONTEXT-AWARE SCALING FACTORS IN ATTENTION MECHANISMS", "content": ""}, {"title": "5.1 HYPOTHESES ON THE ROLE OF SYSTEMATIC OUTLIERS", "content": "Based on the observations in Section 4, we propose three hypotheses on the potential role of systematic outliers in LLMs, drawing from prior research and our findings:"}, {"title": "1. Fixed but Important Biases:", "content": "Inspired by the concept of Massive Activations (Sun et al., 2024), systematic outliers may act as fixed biases that consistently influence model behavior. These outliers could serve as stable values that help the model emphasize certain tokens or features, regardless of the context."}, {"title": "2. Context-Aware Biases:", "content": "As seen in Figure 6(c), the attention outliers vary significantly across heads and tokens (20% to 95%). This suggests that these outliers may dynamically adjust their influence based on the input sequence, acting as context-aware signals that adapt to specific content and guide attention allocation."}, {"title": "3. Context-Aware Scaling Factors:", "content": "Figure 8 shows that the value vectors corresponding to outlier tokens have significantly smaller magnitudes, suggesting that these outliers may act as implicit scaling factors. By reducing the impact of contextual information on certain tokens, these scaling factors help minimize unnecessary updates."}, {"title": "5.2 EMPIRICAL VALIDATION OF SYSTEMATIC OUTLIERS HYPOTHESES", "content": "Formulation. In this part, we introduce five different attention formulations to explore the role of systematic outliers. Each formulation represents a specific variant of the attention mechanism, designed to isolate different aspects of bias and scaling effects within the model. The formulations are listed in Table 2, followed by an explanation of their roles."}, {"title": "5.3 FURTHER ANALYSIS OF SYSTEMATIC OUTLIERS", "content": "Softmax Attention is the root cause of systematic outliers. In Transformers, multi-head attention (MHA) aims to augment token embeddings with contextual information to improve prediction accuracy. The difficulty of this task varies across tokens: some require complex contextual updates (e.g., for ambiguous or semantically rich tokens), while others (e.g., delimiter or padding tokens) require minimal updates. However, the softmax operation enforces that attention scores always sum to one, even for simpler tasks where little contextual information is needed. To satisfy this constraint, the model must produce a large dynamic range in the input to softmax, amplifying the disparity between tokens. This dynamic range is further exaggerated as the model learns to allocate most attention to low-information tokens in some cases, ensuring minimal updates for those tokens.\nThis demand for a large dynamic range propagates through the network and affects earlier layers. Layer Normalization, applied before softmax, standardizes input distributions, inadvertently compressing the required dynamic range. To compensate, multi-layer perceptron (MLPs) in preceding layers generate activations of significantly higher magnitude, resulting in the emergence of activation outliers. These high-magnitude activations propagate through the residual connections, amplifying gradients during backpropagation and encouraging the formation of weight outliers in the projection matrices.\nFurthermore, because softmax outputs are strictly positive and sum to one, all tokens maintain non-zero probabilities, leading to persistent gradients for all queries and keys. This forces the model to continuously adjust activations and weights to accommodate large dynamic ranges, further amplifying systematic outliers over successive layers. As a result, softmax normalization, coupled with architectural constraints like Layer Normalization and residual connections, becomes the fundamental driver of systematic outliers. Detailed derivations and analysis are provided in Appendix C.\nPotential Applications for Model Compression. Systematic outliers complicate compression techniques such as quantization and pruning by increasing memory usage and degrading performance. Our experiments demonstrate that context-aware scaling factors effectively mitigate these"}, {"title": "6 CONCLUSION", "content": "In this work, we systematically analyzed the distribution, formation, and roles of outliers in large language models (LLMs), categorizing them into activation, weight, and attention outliers. Our findings reveal that these outliers are interconnected across layers and dimensions, stemming from the softmax operation in the self-attention mechanism. Acting as implicit, context-aware scaling factors, these outliers dynamically adjust attention distributions, enabling the model to balance diverse contextual demands. By eliminating these outliers through explicit context-aware scaling, we showed improvements in model convergence and compression efficiency. Our approach helps reduce unnecessary attention allocation, making models more efficient and stable. This finding provides new insights into the internal workings of Transformer-based LLMs and opens up avenues for refining attention mechanisms to improve performance and efficiency. We believe that our study not only deepens the theoretical understanding of outliers in LLMs but also has practical implications for the development of more efficient and robust language models."}, {"title": "A ADDITIONAL RESULTS ON SYSTEMATIC OUTLIERS IN LLMS", "content": "This section extends the analysis of systematic outliers in LLMs, complementing the findings presented in the main paper. We provide additional results on both pretrained LLMs (Appendix A.1) and fine-tuned variants (Appendix A.2), highlighting the consistency and variation in outlier behavior across different model families and training paradigms."}, {"title": "A.1 PRETRAINED LLMS", "content": "In Section 3, we identified systematic outliers in models such as LLaMA2-7B. To further validate these findings, we extend our evaluation to a broader set of pretrained LLMs, including Phi-2 (Javaheripi & Bubeck, 2023), Mistral-7B (Jiang et al., 2023), LLaMA2-13B, LLaMA3 (Dubey et al., 2024), OPT-6.7B (Zhang et al., 2022), MPT-7B (MosaicML, 2023), and Falcon-7B (Almazrouei et al., 2023). The corresponding results are depicted in Figure 15, 16, 17, 18 and 19.\nOur analysis reveals several key insights:"}, {"title": "A.2 FINE-TUNED LLMS", "content": "Beyond pretrained models, fine-tuning plays a crucial role in adapting LLMs for specific tasks such as instruction following or conversational applications (Ouyang et al., 2022). To assess the impact of fine-tuning on systematic outliers, we analyze fine-tuned variants of LLaMA2 and Mistral, with results shown in Figures 20, 21, and 22."}, {"title": "B DETAILED EXPERIMENTAL SETTINGS", "content": "This section provides comprehensive details on the experimental settings used to analyze systematic outliers in LLMs. It covers the methodologies and configurations employed in different aspects of the study, ensuring reproducibility and clarity. The following subsections provide detailed descriptions of the experimental setups for each analysis."}, {"title": "B.1 POSITION ANALYSIS SETTINGS", "content": "This subsection describes the experimental settings used to analyze the distribution of systematic outliers in LLaMA2-7B across layers, sequences, and feature dimensions. The focus is on three types of outliers\u2014activation outliers, weight outliers, and attention outliers, with results presented in Figures 3, 4, 5, and 6.\nFor activation outliers in $h_e$ (layer outputs) and $x_{down}$ (down-projection inputs), we analyze the top-3 largest activation values and the median activation value for each layer to examine how outliers"}, {"title": "B.2 CONSISTENCY ANALYSIS SETTINGS", "content": "To analyze the consistency between different types of outliers, we calculate the alignment of activation and attention outliers across 100 randomly selected samples from the RedPajama dataset (Computer, 2023). Each sample has a sequence length of 2,048, and attention outliers are analyzed separately for each attention head.\nThe overlap between activation and attention outliers is defined as the percentage of sequence indices where activation outliers in $h_e \\in \\mathbb{R}^{seqlen \\times d_{hidden}}$ align with attention outliers in $A_i \\in \\mathbb{R}^{seqlen \\times seqlen}$, where $A_i$ is the cumulative attention score matrix for head i at layer l. For each sample and attention head, overlaps are computed as:\n$Overlap = \\frac{|O_{activation} \\cap O_{attention}|}{|O_{activation}|}$"}, {"title": "B.3 GPT-2 ATTENTION VARIANT TRAINING SETTINGS", "content": "We utilize the open-source GPT-2 implementation from the NanoGPT repository (Karpathy, 2023), following the default recommended training setup and optimizer settings. Each of the five GPT-2 variants was trained for 50,000 iterations, processing approximately 2 billion tokens in total. For the attention bias variant, we followed the initialization method proposed by (Sun et al., 2024), setting $k'$ and $v'$ to $\\mathcal{N}(0, 0.021)$."}, {"title": "B.4 MODEL COMPRESSION EXPERIMENT SETTINGS", "content": "To evaluate the robustness of context-aware scaling factors under compression, we conducted experiments on GPT-2 models using two common methods: quantization and pruning. For quantization, 8-bit weight quantization was applied using the AbsMax scaling method, which normalizes weights by their maximum absolute value. For pruning, we performed unstructured magnitude pruning, removing 50% of the smallest-magnitude weights across all layers.\nBoth models were evaluated on the WikiText2 dataset using perplexity (PPL) as the primary metric, comparing GPT-2 Default and GPT-2 with context-aware scaling factors. These settings were chosen to test the models' ability to maintain performance under aggressive compression techniques."}, {"title": "C How SOFTMAX CAUSES SYSTEMATIC OUTLIERS IN TRANSFORMER MODELS", "content": "The formation of systematic outliers in transformer models stems from the inherent characteristics of the softmax operation within the self-attention mechanism. While capturing the complete dynamics of outlier emergence requires a complex understanding of training processes, we provide a mathematical analysis that outlines the logical connection between softmax and the appearance of systematic outliers. This analysis reveals how the interaction between softmax and model architecture propagates and localizes these anomalies. The following sequence summarizes the key steps:"}, {"title": "1. Necessity of Zero-Update in MHA:", "content": "Certain tokens, such as initial tokens or weakly semantic tokens, often require minimal contextual updates. The Multi-Head Attention (MHA) mechanism addresses this by dynamically suppressing updates for these tokens, placing strict constraints on gradients and weights."}, {"title": "2. Softmax-Induced Dynamic Range Expansion:", "content": "Achieving the zero-update behavior requires softmax to focus attention weights on a limited number of keys. This necessitates substantial differences in the dot products of query and key vectors, leading to extreme dynamic ranges in the attention scores."}, {"title": "3. Propagation of Systematic Outliers:", "content": "The extreme attention scores propagate anomalies through transformer computations:"}, {"title": "4. Localization of Outliers:", "content": "Outliers concentrate at specific tokens and feature dimensions:"}, {"title": "C.1 NECESSITY OF ZERO-UPDATE IN MULTI-HEAD ATTENTION (MHA)", "content": "In transformer models, the Multi-Head Attention (MHA) mechanism dynamically adjusts token representations based on contextual information. However, certain tokens\u2014such as initial tokens (e.g., [CLS]) or weakly semantic tokens (e.g., punctuation marks)\u2014often require minimal updates during training. For these tokens, the desired behavior is a near-zero update:\n$\\Delta x = MHA(Q_x, K, V) \\approx 0$.\nAchieving this condition imposes constraints on the gradients and attention weights. The softmax normalization within MHA must allocate most of the attention probability to a few keys with negligible values, effectively canceling the contributions of the remaining keys. The output of the attention mechanism is given by:\n$MHA(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V$, where Q, K, V are the query, key, and value matrices, and $d_k$ is the dimensionality of the keys. For a given query $Q_x$, the attention weights must satisfy:\n$\\sum_{j=1}^{n} Softmax(\\frac{Q_xK_j}{\\sqrt{d_k}}) = 1$.\nTo approximate zero-update, the weighted sum of $V_j$ values must effectively cancel out. This necessitates a highly concentrated attention weight distribution, which in turn creates large disparities in the query-key dot products $(Q_xK_j)$.\nThis zero-update requirement, while essential for certain tokens, introduces challenges:"}, {"title": "C.2 SOFTMAX-INDUCED DYNAMIC RANGE EXPANSION", "content": "The softmax operation, central to the self-attention mechanism, inherently expands the dynamic range of attention scores. This behavior is critical for satisfying the zero-update condition for certain tokens but simultaneously leads to the emergence of extreme values."}, {"title": "C.3 PROPAGATION OF SYSTEMATIC OUTLIERS", "content": "Systematic outliers, once introduced by the softmax mechanism, propagate through the transformer layers due to shared weights and non-linear transformations in the Multi-Head Attention (MHA) and Multi-Layer Perceptron (MLP) components.\nImpact in MHA. The shared projection weights in MHA\u2014specifically, the key and value matrices $W_K$ and $W_V$\u2014magnify the effect of extreme attention scores. The query (Q), key (K), and value (V) matrices are computed as:\n$Q = LN(h_e)W_Q, K = LN(h_e)W_K, V = LN(h_e)W_V$, where $h_e$ represents the layer's input, and LN is layer normalization. When the attention scores A are dominated by a few keys, the output of MHA focuses heavily on the corresponding values $V_j$. The MHA output for token x is given by:\n$MHA(x) = \\sum_{j=1}^{n} A_{xj}V_j$, where $A_{xj}$ concentrates on a few dominant keys. This imbalance introduces large updates to the projection weights $W_K$ and $W_V$ during backpropagation, as the gradients for these weights are computed from $\\triangledown_{W_KL}$ and $\\triangledown_{W_VL}$, respectively.\nAmplification in MLP. Following MHA, the output passes through the MLP block, which consists of up-projection $(W_{up})$, a non-linear activation $(\\sigma)$, and down-projection $(W_{down})$:\n$z_{down} = W_{down}(\\sigma(W_{up}LN(h_{e+1/2})))$, where $h_{e+1/2} = LN(h_e) + MHA(LN(h_e))$. If MHA(x) produces extreme values due to attention outliers, these anomalies are further amplified by the non-linear activation $(\\sigma)$ and concentrated in the down-projection weights $(W_{down})$.\nEmergence of Outliers. The combination of steep gradients and non-linear transformations results in the emergence of outliers in both activations and weights. Key observations include:"}, {"title": "C.4 LOCALIZATION OF SYSTEMATIC OUTLIERS", "content": "Systematic outliers are not randomly distributed but exhibit specific patterns of localization across tokens, layers, and feature dimensions. This section explores how these outliers are concentrated at particular positions and channels, minimizing their overall disruption while fulfilling the model's dynamic range requirements.\nToken-Level Localization. Outliers are predominantly associated with specific tokens, such as initial tokens (e.g., [CLS]) or tokens with weak semantic content (e.g., punctuation marks). These tokens are particularly susceptible to outliers due to their roles in aggregating sequence information or carrying minimal intrinsic meaning. For instance:"}, {"title": "C.5 CONCLUSION", "content": "The mathematical analysis presented in this section demonstrates how the softmax operation in the self-attention mechanism is a key driver of systematic outliers in transformer models. By fulfilling the zero-update requirement for certain tokens, softmax induces extreme disparities in attention scores, leading to steep gradients and the emergence of outliers. These anomalies propagate through transformer layers, being amplified by shared projection weights and non-linear activations in the MLP, ultimately manifesting as systematic outliers in both activations and weights.\nMoreover, these outliers exhibit distinct localization patterns, being concentrated at specific tokens and feature dimensions. This localization minimizes their overall disruption to the model while fulfilling the dynamic range demands imposed by the softmax mechanism. Understanding these dynamics offers valuable insights into the structural origins of systematic outliers, paving the way for mitigation strategies such as explicit context-aware scaling factors to prevent their formation and improve model robustness."}, {"title": "D MORE ANALYSIS FO SYSTEMATIC OUTLIERS", "content": ""}, {"title": "D.1 ABSENCE OF OUTLIERS IN SIGMOID ATTENTION", "content": "In Ramapuram et al. (2024), sigmoid self-attention was proposed as an alternative to traditional softmax-based attention, formulated in Equation 1. Unlike softmax, sigmoid attention independently maps each attention score to a value between 0 and 1, introducing a fixed bias term b to adjust the sigmoid function's activation.\n$SigmoidAttn (X) = \\sigma (QK^T/\\sqrt{d_{qk}}) V$, with $\\sigma : u \\leftrightarrow sigmoid(u + b) := (1 + e^{-(u+b)})^{-1}$ (1)\nA key distinction of sigmoid attention is its ability to output near-zero values for certain tokens. While this can lead to vanishing gradients for some inputs, it also eliminates the extreme dynamic range caused by softmax normalization, thereby mitigating the formation of systematic outliers.\nTo evaluate its impact, we trained a GPT-2 model with sigmoid attention using the same experimental setup described in Appendix B.3. Figure 23 illustrates that sigmoid attention successfully eliminates systematic outliers. This finding reinforces our hypothesis that softmax normalization is a primary cause of outliers in self-attention mechanisms."}, {"title": "D.2 EXPLICIT CONTEXT-AWARE SCALING FACTOR IN TINY-LLAMA", "content": "To verify the generalizability of our findings beyond GPT-2, we conducted additional experiments using a TinyLLaMA-120M model. The training setup and code were adapted from the open-source implementation of TinyLLaMA (Keene, 2024), which provides an efficient framework for pretraining Transformer models."}, {"title": "D.3 IMPACT OF SEQUENCE LENGTH ON OUTLIERS", "content": "Our analysis shows that sequence length does not affect the existence of attention outliers, but it does influence their specific positions within the sequence. Outliers consistently appear at the first tokens and semantically weak tokens, with their relative positioning shifting as the sequence length changes. The observations are summarized as follows:"}]}