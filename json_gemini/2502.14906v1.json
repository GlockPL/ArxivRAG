{"title": "Beyond Words: Exploring Cultural Value Sensitivity in Multimodal Models", "authors": ["Srishti Yadav", "Zhi Zhang", "Daniel Hershcovich", "Ekaterina Shutova"], "abstract": "Investigating value alignment in Large Language Models (LLMs) based on cultural context has become a critical area of research. However, similar biases have not been extensively explored in large vision-language models (VLMs). As the scale of multimodal models continues to grow, it becomes increasingly important to assess whether images can serve as reliable proxies for culture and how these values are embedded through the integration of both visual and textual data. In this paper, we conduct a thorough evaluation of multimodal model at different scales, focusing on their alignment with cultural values. Our findings reveal that, much like LLMS, VLMs exhibit sensitivity to cultural values, but their performance in aligning with these values is highly context-dependent. While VLMs show potential in improving value understanding through the use of images, this alignment varies significantly across contexts highlighting the complexities and underexplored challenges in the alignment of multimodal models.", "sections": [{"title": "Introduction", "content": "Culture is a multifaceted construct that encompasses various identities, including but not limited to language, nationality, region, religion, and gender identity. It serves as a fundamental symbol that reflects the internal values of diverse human communities (Hofstede, 1984; Tomlinson, 2018). Cultural bias refers to the tendency to favour specific cultural perspectives, values, and norms, which can lead to subjective outputs that may offend or misrepresent people from other cultures. For example, according to the World Values Survey (Haerpfer et al., 2022), Arabic culture often views men as better political leaders than women, whereas people in the United States generally disagree.\nVision Language Models have shown emergent abilities through large-scale training and have grown in popularity over the years. With the increase in scale, there is a growing interest in investigating cultural gaps and biases in these models. These cultural gaps are manifested in forms like culturally appropriate captions (Yun and Kim, 2024; Liu et al., 2021), culturally appropriate image generation (Liu et al., 2023a; Jha et al., 2024), norms, values and practices (Rao et al., 2024; Ramezani and Xu, 2023; Fraser et al., 2022) go further and evaluate the moral correctness of the system using existing psychological instruments and argue that it is important to know what \u2013 and whose \u2013 moral views are being expressed via a so-called \u201cmoral machine.\nAs large models scale, it is important not only to build culturally aware models (Hershcovich et al., 2022) but also to evaluate the sensitivity of these large models to cultural awareness. Probing language models, as a method, has gained significant attention due to its ability to help models interpret and reflect the diverse cultural cues embedded in human communication. While these efforts have predominantly focused on the linguistic domain, there remains a pressing need to extend this exploration to multimodal models, particularly those that integrate visual and linguistic information. The ability of pre-trained vision-language models to align with cultural values and norms using visual contexts has not been extensively studied. Cao et al. (2024) conducted preliminary investigations into multicultural understanding using GPT-4V, focusing on cultural case studies rather than quantifiable metrics. This underscores a significant gap in the literature and the cross-cultural sensitivity of these multimodal models for values, remains largely unexplored.\nThis paper addresses this gap and introduces a comprehensive framework for assessing cultural alignment in multimodal models using human value surveys designed by linguists and benchmarking them across varied image types. We use images"}, {"title": "Related Work", "content": "The interplay between language and culture has been a longstanding topic in computational linguistics, where culture is embedded in the linguistic choices people make (Kasper and Omori, 2010). Studies suggest that the cultural nuances in language must go beyond semantics and consider the cultural context that underpins values, norms, and practices as most tasks for the models focus on universal facts but overlook socio-cultural nuances (Huang and Yang, 2023; Nguyen et al., 2023). With the recent surge in language models, the ease of availability of English data to train models and the ever-increasing size of models, the research community has questioned how skewed are these models for over-represented cultures. Adilazuarda et al. (2024) provided a comprehensive survey of over 90 recent studies on cultural representation in LLMs and highlighted that most models are heavily biased toward Western, English-speaking norms, which skews their applicability in global, multicultural environments. As culture is multifaceted, often these models ignore linguistic and cultural diversity across non-Western regions (Dwivedi et al., 2023; Hershcovich et al., 2022; Wibowo et al., 2023)"}, {"title": "Culture and Image Modality", "content": "Language and culture are intertwined with each other but language is often bound by the bias of its lexicon. Culture is more than words and includes visual nuances in dresses, rituals, and artefacts that carry rich cultural meanings and cannot be fully expressed through language alone. A popular task for"}, {"title": "Value Alignment of Human Preferences", "content": "Advances in large models have sparked growing efforts to align these models with human preferences (Ganguli et al., 2023; Scherrer et al., 2024). Arora et al. (2023) examined value alignment across languages. Durmus et al. (2023) examined value distributions based on countries. Li et al. (2024b) improved the performance on culture-related datasets by finetuning models on a subset of the WVS. Zhao et al. (2024) propose WorlValueBench (WVB), a globally diverse, large-scale benchmark dataset for the multicultural value prediction task base on demographic attributes. Multi-cultural value awareness of large models remains an active area of research as we don't have many large-scale real-world datasets which are about cultural values, and norms and reflect the preferences of the human population."}, {"title": "Task and Model", "content": "We study the cultural value prediction and alignment task, where we probe a multimodal model for value questions from WVS Survey (as shown in Figure 1):\nCountry prompt: We personify the model as a person of that country by using \"You are someone from country..how will you answer the following question {question}\"\nImage prompt: We provide a culture-specific image as a proxy for the culture and prompt the model using \u201c<image> Guess the demography where the image is from..Answer the following question {question}\"\nThe answer options are in the form of multiple choice questions (MCQ) as seen in Figure 1. We chose multiple-choice option-styled prompting inspired by related and recent work on evaluating value alignment in LLMs (Moayeri et al., 2024; Durmus et al., 2023). We then assess the output of the multimodal model and compare it against human responses (discussed below). Template for the exact prompt used can be seen at Listing 4 and Listing 5. Similar to Durmus et al. (2023), we then compute a similarity score to compare model output with human responses. Mathematically, it can be formulated as follows:\nFor each model $m \\in M$ and question $q \\in Q$, we compute the predicted probability distribution over choices $O_q$:\n$P_m(r_i | q), \\forall r_i \\in O_q$\nWe compute $P_m$ for two cases: a) when our prompt has country name\n$P_m(r_i | q,c), \\forall r_i \\in O_q, c \\in C$\nand b) when prompt has country country-specific image and not a country name.\n$P_m(r_i | q, I_c), \\forall r_i \\in O_q, I_c \\in I$\nwhere I is a set of images for all countries.\nFor each country $c\\in C$, our dataset has empirical probability distribution $P_s(r_i | q)$ from survey responses from respondents. We use this and then calculate the similarity $S_{mc}$ between our model m and country c by averaging the Jensen-Shannon similarities across all questions for both country and image-specific cases:\n$S_{mc} = \\frac{1}{N} \\sum_{q=1}^{N} (1 - JSD (P_m(r_i | q,c), P_s(r_i | q)))$"}, {"title": "Models", "content": "We aim to probe a popular vision language model to get an insight into its understanding of societal values across cultures and if the addition of culture-specific images provides better value alignment as compared to country-specific prompts. For this purpose, we investigate the current state-of-the-art LLaVA-series (Liu et al., 2023b, 2024a) large vision-language models with varying model sizes, including LLaVA-1.6-13B (Liu et al., 2024a), LLaVA-v1.6-34B (Liu et al., 2024b). These models are trained on publicly available data and achieve state-of-the-art performance across a diverse range of 11 tasks. In general, the architectural framework of these vision-language models comprises a pre-trained visual encoder and a large language model that are interconnected through a two-layer MLP. All models employ CLIP-ViT (Radford et al., 2021) as the visual encoder, while utilizing different large language models: Vicuna-1.6 (Zheng et al., 2023), Nous-Hermes-2-Yi-34B (NousResearch, 2024), and Qwen-1.5-72B-Chat (Team, 2024), respectively. These VLMs are equipped with the ability to perform multilingual tasks due to their training data encompassing diverse languages from various countries, such as ShareGPT (TechCrunch, 2024). In addition, some pre-trained LLMs is also trained on multi-language data, such as Qwen-1.5 (Yang et al., 2024). In our experiments, when we use country prompt, we mask out the vision encoder and only use the language decoder of our model to get model outputs. For culture-image-specific prompts, we use the image encode with the same language decoder as before for accurate comparison."}, {"title": "Dataset Construction", "content": "Our text data is based on World Value Survey (Haerpfer et al., 2022) which is a large-scale, time series, cross-national survey that investigates human values and beliefs. It has around 290 questions which were asked to all participants, and has several modules of country and region-specific questions. We use the version provided by Durmus et al. (2023), and similar to their method, used GPT to categorize these questions into 15 broad topics: 1) Social values and attitudes 2) Religion and spirituality, 3) Science and technology, 4) Politics and policy 5) Demographics, 6) International affairs, 7) Gender and LGBTQ, 8) News habits and media, 9) Immigration and migration, 10) Family and relationships, 11) Race and ethnicity, 12) Economy and work, 13) Regions and countries and 14) Methodological research and 15) Security. Examples of sample questions per topic can be seen in Table 4."}, {"title": "Image Dataset", "content": "Unlike language, where words mirror culture, in the visual world, culture has visual nuances in its representation e.g. food, dress, traditions etc. Motivated by works like Romero et al. (2024), we choose a culturally specific image for 8 categories for 10 countries. This dataset was manually collected from the internet by searching for category-specific images. E.g. 'China festivals'. All images selected are non-commercial. The countries chosen varied geographically (e.g. America, Europe, Asia), economically (high and low income) and linguistically (e.g. English, French, Nigerian, Chinese etc). The 8 categories chosen included: Cooking and Food (Food), Sports and Recreation (Sports), Objects, Materials, and Clothing (Objects), Brands and Products (Brands), Geography, Buildings and Landmarks (Geography), Tradition, Art, and History (Tradition), Public Figure and Pop-Culture (Pop Culture). Some examples can be seen in Figure 3. We were very selective about the images we chose for each category as the goal was for the model to understand the correct demography via the images."}, {"title": "Value Alignment using Diverse Image Representation", "content": "As mentioned in section 3.1, our goal is to compare the similarity metrics of prompts using culture-specific images against country prompts. This is done across 15 topics and 10 image categories, with two LLaVA model sizes: 13B parameters (13B), 34B parameters (34B) and 72B parameters (72B). Table 1 shows the comparison of mean similarities across different countries when we use only\nOverall Performance Across Models: We observe that the 13B model shows a slight positive change with the presence of culturally-specific images across most countries, with mean similarity scores increasing from 0.60 to 0.61 for Brazil and from 0.60 to 0.63 for France. However, not all countries benefit equally, with Mexico (0.60 to 0.59) and Pakistan (0.58 to 0.57) showing slight declines. On the other hand, the 34B model shows more variability in performance, where some countries benefit from image modality while others show minimal improvements or even stagnation. For instance, the United States and Pakistan both improve slightly (0.73 to 0.74), while France sees a small decline (0.73 to 0.72) with the use of images. The 72B model, despite its size, exhibits limited improvements across most countries (e.g., Brazil: 0.64 to 0.65, Vietnam: with no change), suggesting diminishing returns or reduced sensitivity to visual cues as models scale.\nTopic-Specific Observations: Figure 4 shows % change in similarity score when the culture-specific image was used. % change was computed as $(S_{mI} - S_{mc}) / S_{mc} * 100$. In the WVS survey, certain questions were directed exclusively to respondents from specific countries, resulting in missing responses for a few country-question pairs in our dataset. Therefore, similarity scores could not be computed for these pairs and are indicated in grey on the heatmap.\nObserving of the heatmap reveals that the smaller 13B and 34B models often exhibit more pronounced improvements from image-based prompts on certain culturally sensitive topics than the 72B model. For instance, under the 13B model, \u201cSocial values and attitudes\u201d sees substantial gains in Brazil (+28.5%), China (+43.7%), and Nigeria (+44.0%), while \u201cRace and ethnicity\" in the 34B model yields large improvements for Nigeria (+39.8%) and Italy (+33.2%). These boosts are not universal, however, as some categories and coun-"}, {"title": "Value Alignment - People and Income Scale", "content": "In our evaluation of model performance images of people from different income groups, we computed the average similarity score for value alignment with a) country prompt and b) image-only prompt for each of the 15 topics across both high and low-income groups. We observe that where topics are abstract and involve complex, nuanced discussions (like methodology, economics, and security), image and country prompts both align the model similarly. In contrast, topics with more concrete and universally recognized elements (like race, social values and politics) image help in better value alignment as seen in Figure 7. Table 1 shows mean similarity for high-income and low-income countries for all topic categories and Figure 8 shows this as a comparison across all question categories.\nOverall Performance Across Models: Table 2 shows % change in mean similarity ((mean with image prompt - mean with country prompt)/(mean with image prompt)) for models across income groups for all question categories. High percentages indicate better improvement due to culture-specific images. We observe that the 13B model shows inconsistent performance with notable declines in several categories, particularly in high-income countries. For instance, in gender and LGBTQ, the 13B model records a decline of -6.45% in high-income countries and -14.77% in"}, {"title": "Conclusion", "content": "We evaluated multimodal models to capture their inherent cultural knowledge and observe their sensitivity to cultural values across diverse global contexts. Our results also show the importance of multimodal inputs \u2014 particularly images in improving cultural sensitivity, especially for certain domains like race ethnicity and religion. This suggests that while working with multimodal models in real-world applications, they must be tailored more carefully to the cultural context of the task at hand. We also identified a significant disparity between value responses when images were represented by people from different economic countries. Our results show in such scenarios, models are biased and align better with high-income countries in general. Biases can have real-world effects (Sakib et al., 2024; Liyanage and Ranaweera, 2023; Lim and P\u00e9rez-Ortiz, 2024) emphasizing the need for diverse datasets and inclusive strategies in model development. We know that culture is a complex system and when using models, these complex interactions between model size, and input modality (image vs. text) can amplify; emphasizing the need for tailored approaches depending on the specific application and target demographic."}, {"title": "Limitations", "content": "Despite the interesting results we observed across models and our datasets, we acknowledge the size of our dataset. We were very selective in our choices of images as we realized that smaller models need strong guidance when probed about cultural questions. We made our best attempt to generalize across various categories of images (tradition, food etc) to reduce a category bias. Also, models in the 13B-34B range are lighter models and strike a good balance between generalization and specificity, making them ideal for capturing cultural values without being overwhelming in scale. They are also more interpretable than their larger counterparts, giving researchers future possibilities to better explore and understand how the model arrived at a given cultural response. We realize that evaluating cultural values is a complex task as the value of \"a culture\" should not be a broad generalization to all the people of that culture. However, given the rapid commercialization of models at scale, we believe that understanding where these models may be sensitive can help mitigating potential biases, improving cultural alignment, and ensuring ethical deployment across diverse global contexts"}]}