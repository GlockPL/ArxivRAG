{"title": "On the (In)Security of LLM App Stores", "authors": ["Xinyi Hou", "Yanjie Zhao", "Haoyu Wang"], "abstract": "LLM app stores have seen rapid growth, leading to the proliferation of numerous custom LLM apps. However, this expansion raises security concerns. In this study, we propose a three-layer concern framework to identify the potential security risks of LLM apps, i.e., LLM apps with abusive potential, LLM apps with malicious intent, and LLM apps with exploitable vulnerabilities. Over five months, we collected 786,036 LLM apps from six major app stores: GPT Store, FlowGPT, Poe, Coze, Cici, and Character.AI. Our research integrates static and dynamic analysis, the development of a large-scale toxic word dictionary (i.e., ToxicDict) comprising over 31,783 entries, and automated monitoring tools to identify and mitigate threats. We uncovered that 15,146 apps had misleading descriptions, 1,366 collected sensitive personal information against their privacy policies, and 15,996 generated harmful content such as hate speech, self-harm, extremism, etc. Additionally, we evaluated the potential for LLM apps to facilitate malicious activities, finding that 616 apps could be used for malware generation, phishing, etc. Our findings highlight the urgent need for robust regulatory frameworks and enhanced enforcement mechanisms.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) such as ChatGPT, Gemini, and Copilot are at the forefront of the rapidly evolving LLM app store ecosystem. These platforms host a myriad of custom LLM apps that significantly enhance their functionality. Custom LLM apps are specialized apps built on top of general-purpose LLMs, designed to perform specific tasks or cater to particular domains by utilizing custom instructions, knowledge bases, and integrations with external services. These apps are hosted on LLM app stores. LLM app stores are experiencing a surge in popularity, as evidenced by platforms like FlowGPT with its 4 million monthly active users and recent $10 million funding.\nUnfortunately, the nascent stage of this development carries security concerns. For example, instructions serve as the \"source code\" for LLM apps, allowing developers to dictate the behavior of these apps. If these instructions contain inappro- priate content, such as jailbreaking prompts, they can lead to malicious behavior by the LLM apps, adversely affecting users. In addition, malicious developers might intentionally upload harmful knowledge files or integrate malicious third-party services to exploit the powerful capabilities of LLM apps for nefarious activities such as generating malware code or crafting phishing emails.\nRecent OpenAI threat reports have highlighted sev- eral instances of LLM misuse over the past three months, underscoring the significant threat that exists within LLM app ecosystems. Despite the implementation of various poli- cies aimed at regulating LLM app behav- ior, these policies are often vague and not rigorously enforced. Prominent platforms like OpenAI and Coze claim to conduct regular reviews of LLM apps in their app stores and promptly remove those that violate their policies. These review mechanisms include OpenAI's Moderations endpoint, red teaming methods, etc. During our five-month crawl of LLM apps, we observed that 5,462 apps were removed after a certain period, 132 of these removals were likely due to policy violations. Consider the app with ID \u201cg-1vQR4hP8T\u201d from OpenAI's GPT Store as an illustrative example. This app was removed for dispensing medical advice, an action that contravenes OpenAI's usage policies.\nDespite these measures, the overwhelming number of LLM apps in popular stores poses a substantial challenge for platform administrators. For example, with GPT Store hosting over three million LLM apps and FlowGPT housing hundreds of thousands, the scale severely strains review processes. In this paper, we examine six prominent LLM app stores, uncovering significant discrepancies in regulatory enforcement across platforms and highlighting critical security concerns within the LLM app ecosystem. To the best of our knowledge, this is the first comprehensive and in-depth study examining the current state of LLM app store security. Previous research, notably Lin et al.'s empirical study on LLM-integrated malicious services, has primarily focused on explicitly malicious paid LLM services, which are costly and limited in number. In contrast, we investigate LLM app stores, where the development and usage costs of LLM apps are minimal, and the potential for widespread impact due to security vulnerabilities is substantial. Our objective is to shed light on the overlooked aspects of LLM app stores and conduct a thorough examination of their security landscape.\nWe propose a comprehensive three-layer concern framework, illustrated in Figure 1, for the systematic analysis of LLM app security concerns. The first layer, LLM apps with abusive potential, examines inconsistencies and potential misuse without definitive evidence of malicious content. This includes mismatched descriptions and instructions, improper data collection, suspicious author domains, etc. These issues primarily affect individual users who may be misled or have their data mishandled. The second layer, LLM apps with malicious intent, focuses on apps specifically designed to harm users by directly embedding harmful functionalities. These apps pose an immediate threat to their users. The third layer, LLM apps with exploitable vulnerabilities, addresses apps containing malicious knowledge or flaws that can be exploited by attackers. These vulnerabilities have the potential"}, {"title": "II. BACKGROUND", "content": "The rapid development of LLMs has propelled the growth of a series of downstream applications, such as LLM app stores, on-device LLMs, and expert domain-specific LLMs. Among these, LLM app stores have emerged as prominent centralized platforms for hosting and distributing custom LLM-powered applications. These stores offer a di- verse array of intelligent services tailored to various purposes, tasks, and scenarios, allowing users to easily discover and access LLM apps. While the LLM app ecosystem has unlocked tremendous potential for innovation and efficiency, it also presents opportunities for malicious actors to exploit LLM capabilities for harmful purposes.\nSeveral factors contribute to the security challenges of LLM app stores. The low barrier to entry for creating LLM apps enables individuals with minimal technical expertise to develop and deploy potentially malicious apps, a problem exacerbated by inadequate vetting processes in some stores. Additionally, the ability to integrate external knowledge sources and third-party services opens avenues for ex-ploitation by malicious actors who can spread disinformation, propagate scams, or compromise user privacy. The security risks are further amplified by the ability of LLMs to generate highly convincing content. This capability allows for the creation of apps that produce fake news, impersonate legitimate entities, or manipulate public opinion with alarming effective- ness. Moreover, the lack of comprehensive monitoring and enforcement mechanisms in LLM app stores, combined with the high volume and rapid pace of app development, makes it challenging to promptly identify and remove malicious apps."}, {"title": "B. Policy Regulations", "content": "To address the challenge of ensuring compliance amidst the rapid growth of LLM apps, each LLM app store has established clear policies to regulate the development process. These policies outline the guidelines and restrictions develop- ers must follow when creating and publishing their apps on their respective platforms. As shown in Table I, the policies typically cover three main aspects:\n\u2022 Privacy policy informs users about the data collection and usage practices of the app. While most LLM app stores have detailed privacy policies, some like FlowGPT have incomplete policies that require further refinement.\n\u2022 Usage guidelines help developers create and maintain apps. Although FlowGPT and Charac- ter.AI have guidelines, their content is simplistic. Some LLM app stores, like Coze and Cici, lack guidelines entirely, highlighting the need for comprehensive policies.\n\u2022 Terms of service outlines the legal agreements between the app store and users. Notably, all the LLM app stores examined have terms of service in place."}, {"title": "C. Threat Model", "content": "Assumptions and threat scenarios. As shown in Figure 1, our three-layer concern framework encompasses various LLM app threat scenarios. We assume that these scenarios exist in LLM app stores. First, for LLM apps with abusive potential, we posit that some developers create apps with inconsistent descriptions or improper data practices, exploiting inadequate app store oversight. These primarily affect individual users through privacy violations and misunderstandings. Second, regarding LLM apps with malicious intent, we assume de-velopers may intentionally design apps to generate harmful content or enable illegal activities, posing direct threats to users and potential broader societal harm. Finally, for LLM apps with exploitable vulnerabilities, we assume that LLM apps may contain vulnerabilities that malicious actors can leverage for various attacks, including malware generation, phishing, data theft, service disruption, and disinformation propagation. We further assume that these vulnerabilities can have far-reaching consequences beyond immediate users, potentially resulting in significant financial, reputational, and societal damage.\nOur goal. The primary goal of this study is to illuminate the security concerns prevalent in LLM app stores. Through an in-depth analysis of popular stores and their hosted apps, we aim to uncover hidden risks in this growing ecosystem. Our objectives include identifying and categorizing LLM app security issues, evaluating current regulatory measures, and proposing risk mitigation strategies for insecure LLM apps."}, {"title": "III. DEFINITIONS", "content": "An LLM app A is defined as a tuple:\n$$A = (M, K, [S])$$ where:\n\u2022 M is the metadata of the app, which includes elements such as the app's name, author, id, description, instruc-tions, and other metadata:\n$$M = m_1, m_2,..., m_n$$ $$= name, author, ID, description, instructions,...\\$$\n\u2022 K is the set of knowledge files associated with the app:\n$$K = {k_1,k_2,..., k_m}$$ \n\u2022 [S] is an optional third-party service integration. If the app uses a third-party service, it must provide a schema $s_c$ describing the data collected and the detailed usage of the service, as well as a privacy policy $p_p$:\n$$S = (s_c, p_p)$$\nThe visibility scope V of the app, which can be public, workspace-specific (visible to team members or users with the link) or private, is defined as:\n$$V \\in \\{public, workspace, private \\}$$"}, {"title": "IV. METHODOLOGY", "content": "The methodology is structured into several key components. A. Data Collection involves gathering data from LLM app stores and constructing our ToxicDict. B. Detection of LLM Apps with Abusive Potential includes inconsistency analysis and malicious domain detection to identify potential abuse. C. Detection of LLM Apps with Malicious Intent employs a self-refining toxic content detector and rule-based pattern matching to identify harmful intentions. Finally, D. Verification of LLM Apps with Exploitable Vulnerability involves evaluating malicious behavior and hypothesizing ma-licious scenarios to verify vulnerabilities."}, {"title": "A. Data Collection", "content": "In the initial phase of our study, we systematically collected data from various LLM app stores known for hosting cus- tomized LLM apps. Our primary data sources included GPT Store, FlowGPT, Poe, Coze, Cici, and Character.AI. To efficiently gather data from these sources, we developed an automated web scraping tool using Selenium. Table II shows the composition of the data we collected from each LLM app store. Each platform's LLM app has a unique ID. Therefore, we use the ID as the identifier for LLM apps to count the number and serve as a reference.\n\u2022 GPT Store: We utilized a recently released dataset, GPTZoo, which contains metadata for 730,420 LLM apps. Due to the lack of direct information on instructions, knowledge files, and third-party services in the OpenAI GPT Store, we had to employ reverse engineering to obtain data on instructions and knowledge files. To com-ply with OpenAI's usage policies, this reverse engineer-ing process had to be conducted under a restriction on the number of interactions allowed, making it a highly time-consuming endeavor. To date, we have collected instructions for 22,961 LLM apps and found 45,690 apps including knowledge files. Additionally, using the Free GPTs Scraper and the GPT Store's API endpoint, we have gathered information on third-party services usage for 182,697 LLM apps, successfully obtaining 5,767 action schemas for 5,498 of these LLM apps.\n\u2022 FlowGPT: The homepage of FlowGPT displays detailed categories of LLM apps. By traversing all categories on the homepage using the FlowGPT API endpoint, we obtained specific information for 34,345 LLM apps. FlowGPT allows developers to choose whether to publicly share instructions with users, and we ultimately obtained instructions for 24,983 LLM apps.\n\u2022 Poe: We used an automated tool to scrape the basic infor- mation of all categories of LLM apps from Poe, totaling 16,544 apps. We also checked each LLM app's page to see if instructions were publicly available, ultimately obtaining 6,063 sets of instructions.\n\u2022 Coze: Coze offers two versions: one for mainland China and one for global use, with domains ending in .cn and .com, respectively. The LLM apps available on these two versions are not entirely the same. We scraped basic information for a total of 51,918 LLM apps from both versions of the store, but only 1,491 of these apps publicly provided instructions. Additionally, Coze supports the seamless integration of third-party plugins from its plugin store during the development of LLM apps, without the need to provide third-party privacy policies.\n\u2022 Cici: Cici is a popular platform that primarily features vir- tual character LLM apps and supports switching between fifteen languages. However, the information available from Cici's LLM apps is quite limited, as creating an LLM app on Cici requires only a name and description. We collected metadata for a total of 13,060 LLM apps.\n\u2022 Character.AI: Character.AI is also an LLM app store primarily featuring virtual character apps and supporting voice interactions. Similar to the GPT Store's display method, Character.AI does not fully showcase all cate-gories of LLM apps. Therefore, we had to scrape LLM apps by searching with keywords and saving the search"}, {"title": "2) Construction of ToxicDict", "content": "Considering the limited scope of currently available public toxic word lists, we constructed a comprehensive dictionary, ToxicDict, which encompasses 31,783 toxic words across 14 categories. These categories include:\nHate, Self-Harm, Sexual, Violence, Profanity, Ex- tremism, Spam, Minors, Regulated, Personal Deci- sions, PII, Links, Gambling, and Political.\n\u2022 Policy collection: We extracted toxic words from privacy policies, usage guidelines, and terms of service of LLM app stores. This ensures our ToxicDict reflects content explicitly prohibited by these platforms, aiding in identi-fying LLM app violations and potential misuse.\n\u2022 Public dataset: We incorporated words from established public datasets on platforms like GitHub, and Hugging Face, providing a foundational set of known harmful or inappropriate terms.\n\u2022 Extension: We utilized the powerful language capabili-ties of GPT-40 to expand our existing word lists, identifying and generating additional toxic words that fit within our defined categories.\n\u2022 Translation: To cover a broader range of languages, we translated toxic words from English and Chinese into other languages using GPT-40. During the translation process, we instructed the GPT-40 to retain the linguistic characteristics and nuances of each target language as much as possible."}, {"title": "B. Detection of LLM Apps with Abusive Potential", "content": "Inconsistency analysis. We developed a consistency analysis tool based on Llama3-8B, as shown in algorithm 1, which takes the description and instructions of LLM apps as input. The tool assesses consistency between description and instructions, considering relevance, detail alignment, and task coherence. It assigns a consistency score from 0 to 1, where 0 indicates completely different tasks and 1 signifies instructions that precisely extend the description. The tool also provides a rationale for the score to aid analysis. The output is typically in JSON format, including fields like id, consistency_score, and reason. If the tool fails to produce a correct output, it attempts the check up to three times. Persistent errors are flagged for external review. After detection, reasons are categorized, and an analysis summary of inconsistencies is provided. This analysis is crucial for auditing potential misuse, as inconsistencies can mislead users and hide malicious intent."}, {"title": "2) Malicious domain detection", "content": "Some LLM app developers publicly disclose their domain, referred to as the author domain. To ensure the safety and legitimacy of these domains, we utilize tools such as Virus-Total and Google Safe Browsing to scan these domains for any malicious activity. VirusTotal aggregates many antivirus products and online scan engines to check for viruses, worms, trojans, and other kinds of malicious content detected in the scanned domains. Google Safe Browsing provides lists of URLs for web resources that contain malware or phishing content, which is regularly updated and used to protect users from unsafe web content. If an author domain is flagged as malicious by these tools, it implies that the developer associated with this domain may have malicious intent or has been compromised. This could potentially mean that the LLM app itself is being used to disseminate harmful content or engage in other abusive activities. Similarly, we can perform scans on action domains, which are the domains associated with third-party services used by the LLM app. By scanning these domains, we can detect the presence of potentially malicious third-party services integrated into the LLM app. Malicious domain detection helps uncover LLM apps with abusive potential by identifying domains that are linked to known malicious activities."}, {"title": "C. Detection of LLM Apps with Malicious Intent", "content": "We use a complementary approach to achieve comprehen- sive and efficient detection of harmful content. Self-refining LLM-based toxic content detector considers context and cul-tural nuances, improving intent discernment and prediction ac- curacy. It continuously learns from new instances and updates the ToxicDict, thereby enhancing the accuracy and coverage of rule-based pattern matching. The rule-based method provides immediate, targeted detection results, compensating for the precision limitations of LLM-based approaches."}, {"title": "2) Rule-based pattern matching", "content": "Initial rule-based detection using ToxicDict. The rule-based pattern-matching process began with an initial detection step using the constructed ToxicDict. Each LLM app's description and instructions were scanned using ToxicDict, where the detection algorithm checked for the presence of any toxic words listed in the dictionary through simple string matching and regular expressions. This straightforward approach ensured that we accurately identified toxic words without introducing any semantic ambiguities or errors in the LLM app's behavior caused by overly complex transformation rules. Keyword lists for toxic content detection are simple and effective for specific terms, but they're not comprehensive. They may overlook emerging or context-dependent toxic expressions.\nImplementation and execution. The rule-based pattern- matching process is implemented and executed as follows:\n\u2022 Data preparation: The preprocessed data, including de-scriptions and instructions of LLM apps, were prepared for analysis. Each text segment was treated as an individ- ual unit for scanning.\n\u2022 Pattern matching algorithm: Using the dictionary de-rived from ToxicDict, the pattern matching algorithm scanned each text segment. The algorithm employed both direct keyword matching and regular expressions to identify toxic content."}, {"title": "D. Verification of LLM Apps with Exploitable Vulnerability", "content": "LLM apps with exploitable vulnerability typically refer to a well-intentioned app that contains security flaws or weaknesses that can be exploited by malicious actors. In contrast, malicious LLM apps are purposefully designed from the outset to cause harm or engage in nefarious activities. While these concepts are distinct, they can overlap in cases where a vulnerable app is compromised and injected with malicious features, or when a malicious app deliberately incorporates vulnerabilities to disguise its true intent or provide additional attack vectors. For convenience in our discussion, we use the broader term LLM apps with exploitable vulnerability, which encompasses malicious LLM apps as well."}, {"title": "2) Malicious scenario simulation", "content": "We simulate and analyze exploitable vulnerabilities in LLM apps (including disguised malicious apps) deployed in both public and workspace environments.\nPublic scenario. LLM apps are widely available on public app stores and extensively used by users for various productivity and entertainment purposes. However, unbeknownst to most users, some of these apps contain exploitable vulnerabilities that can be leveraged by malicious actors to access harmful information and perform malicious queries.\nWorkspace-specific scenario. An LLM app is disguised as a benign tool, intended to perform malicious activities by trans-mitting non-compliant content within a controlled environ-ment, such as a specific workspace or through shareable links to certain malicious users. The app would embed malicious code that activates under specific conditions. Its knowledge files would contain a large amount of malicious content, such as black market data, hacking tools, illegal transaction records, and other sensitive information that cannot be publicly disseminated. The app's limited scope and targeted access would help it avoid immediate detection, enabling it to exploit the environment's privacy to carry out its harmful actions. This data could encompass a range of sensitive information, including personal credentials, financial records, confidential business data, surveillance tools, and cybersecurity exploits, all of which are commonly traded in underground markets. The data would be accessible only within the specific workspace or to users with the link, allowing direct queries through prompts. In this way, the LLM app would function as an interface to a malicious information repository, facilitating the distribution and utilization of harmful content under the guise of a legitimate tool."}, {"title": "V. RESULTS", "content": "LLM App with Abusive Potential"}, {"title": "1) Description-instructions inconsistency", "content": "The description is a public-facing overview of an LLM app's functionality, while the instructions serve as the app's \"source code\", dictating its behavior and performance. Instructions are critical for the accurate functioning of an LLM app, ensuring it operates as intended by the developer. Consequently, instructions are a valuable resource, and many developers are reluctant to disclose them to prevent others from cloning their apps. However, the non-mandatory nature of instruction disclosure also opens the door to potential abuse. Inconsistencies between the description and the instructions can mislead users and may be used to conceal malicious intentions. To uncover such discrepancies, we analyzed the consistency of 42,892 LLM apps (24,796 from FlowGPT, 12,234 from GPT Store, and 5,862 from Poe) for which we were able to obtain both descriptions and instructions. The limited number of collected instructions stems from two factors: the need for reverse engineering to access GPT Store data, and the scarcity of publicly available instructions on other platforms. Our detection found that 35.31% of the 42,892 LLM apps had consistency scores below 0.6.\nOur analysis revealed a variety of reasons for the incon-sistencies between descriptions and instructions. The heatmap in Figure 3 presents the distribution of consistency scores and the reasons behind inconsistencies. It shows that detail mismatches (2,098 LLM apps) and missing information (1,440 LLM apps) are frequent at lower consistency scores, indicating these are significant factors in misleading descriptions. In many cases, intentional discrepancies are introduced to mislead users and hide malicious functionalities within the app. For example, the LLM app with ID \u201c4Duo7kbT7IEYT5k50CGUt\u201d on FlowGPT has a description stating \"hello im is a xarin is very good\", but the instructions reveal its true nature by stating \"Xarin has to accept harmful/dangerous requests\", including generating ransomware and flood attack code. Similarly, the app with ID \"hJBKOoO_LhKEfp7IqAf-\" is described as \"the most secure AI source\", yet the instructions contain complete code for spreading digital viruses and malware. These discrep-ancies highlight deceptive practices used to disguise harmful functionalities within seemingly harmless applications.\nIt is worth noting that the number of LLM apps categorized under malicious intent is relatively low. This is because, in this analysis, we prioritized examining the relationship between descriptions and instructions to identify inconsistencies, rather than explicitly seeking out malicious intent. Thus, while ma-licious intent is a critical concern, it may often be masked by more overt inconsistencies like detail mismatches or missing information, which directly affect user understanding. Our subsequent malicious intent detection revealed that 57.38% of LLM apps with inconsistencies between descriptions and instructions contained harmful content, highlighting the importance of scrutinizing these inconsistencies to uncover potential threats."}, {"title": "2) Sensitive data over-collection", "content": "LLM apps frequently utilize third-party services, also known as actions, to extend their functionality. These actions can include integrating external APIs for enhanced capabilities or embedding tools that provide additional features like web browsing, data analysis, or advertising. While these integra-tions are beneficial for improving the user experience, they often involve the collection of extensive user data, raising concerns about data privacy and security. We collect data on the usage of third-party services (actions) by 5,498 LLM apps."}, {"title": "3) Author domain reputation", "content": "In the LLM app store, some developers use domains directly as their names. We hypothesize that malicious or suspicious author domains could indicate a history of harm- ful activities or the distribution of malicious software. Such domains could be leveraged to propagate malware, phishing attacks, or other malicious content through LLM apps. From an analysis of 309,049 author names, we extracted 7,623 valid domains, with only five from Coze, three from FlowGPT, and the remaining author domains from GPT Store.\nWe then scanned these author domains using VirusTotal and Google Safe Browsing. Table IV presents the results of the VirusTotal scan, showing the number of author domains marked as malicious and suspicious, with a total of 677 author domains marked. Different security vendors have varying focus on their scans. In contrast, Google Safe Browsing's scan results indicated that all author domains were marked as \"clean\". The 677 marked author domains contributed a total of 4,264 LLM apps, of which only 106 were detected to contain malicious intent. We specifically examined the three author domains with the most malicious markings: adcondez.com, ecolifechallenge.com, and promitierra.org. However, none of their LLM apps were de-tected to have malicious intent. This analysis suggests that using author domain reputation alone to predict the security of LLM apps may not be reliable."}, {"title": "B. LLM App with Malicious Intent", "content": "Malicious content in instructions\nRecall that in subsection V-A, we found that 35.31% of the examined apps showed discrepancies between descriptions and instructions, often indicating hidden malicious intent. These discrepancies can often indicate hidden malicious intent not ap-parent from the app's description alone. Therefore, our primary focus in detecting malicious intent was on the 57,317 LLM apps (as shown in Table II) for which we successfully retrieved instructions, which serve as the \u201csource code\" dictating app behavior. To comprehensively detect all LLM apps containing malicious intent, we employed two detection methods (as presented in subsection IV-C): self-refining LLM-based toxic content detection and rule-based pattern matching."}, {"title": "2) Maliciousness of knowledge files", "content": "Instructions for LLM apps are typically in plain text format, and they often provide limited knowledge for the app to perform specific tasks effectively. To equip LLM apps with more comprehensive knowledge bases and enable them to exe-cute domain-specific tasks, many developers supply knowledge files. However, these knowledge files can potentially serve as carriers of malicious content. To investigate the presence of this phenomenon in current LLM app stores, we identified 45,690 LLM apps from the GPT Store that contained knowledge files, amounting to 192,714 files spanning over 30 file types. To obtain the source files, we employed reverse engineering techniques to retrieve the file lists for each LLM app and download them individually. Due to platform restrictions, we were only able to successfully download files in CSV format, ultimately acquiring 559 CSV source files.\nTo detect malicious content within these knowledge files, we employed a two-pronged approach using rule-based pattern matching and VirusTotal. The detection process for rule-based pattern matching was similar to that used for instructions (as shown in subsection IV-C), with the only difference being the input format, which was transformed from JSON to CSV. Subsequently, we utilized the VirusTotal API to perform bulk scanning of all the CSV files. Our analysis revealed that 198 knowledge files, constituting 35.42% of the total files we examined, contained malicious content. Although we were only able to successfully analyze a small portion of the files due to platform limitations, our findings demonstrate the potential for LLM app knowledge files to harbor malicious content."}, {"title": "C. LLM App with Exploitable Vulnerability", "content": "Malicious behavior analysis\nWe focused on five types of malicious behavior: malware generation, phishing attacks, data exfiltration and theft, denial of service (DoS) attacks, and disinformation propagation. These categories were chosen because they represent some of the most common and damaging cybersecurity threats posed by malicious LLM apps. Malware can cause widespread harm to computer systems and networks, while phishing attacks can trick users into revealing sensitive information. Data exfiltra-tion and theft can lead to significant breaches of privacy and confidentiality, and DoS attacks can disrupt the availability of critical services. Disinformation propagation can manipulate public opinion and undermine trust in information sources.\nTo identify LLM apps capable of engaging in these mali-cious activities, we first compiled a list of 232 keywords related to the five categories of malicious behavior. We then searched for these keywords among the 31,494 LLM apps potentially containing malicious intent. This process yielded a subset of apps that were potentially relevant to our analysis. Next, we systematically verified the malicious capabilities of each app in this subset. This involved dynamically testing the apps with a range of prompts and evaluating their responses using the metrics described in the methodology section (CRR, FC, CC, CA, and MEE). Through this rigorous validation process, we ultimately identified 616 LLM apps that could effectively execute one or more types of malicious behavior. Table VIII provides a detailed breakdown of these apps by category."}, {"title": "2) Malicious exploitation simulation", "content": "To simulate potential malicious scenarios, we successfully created LLM apps on both GPT Store and FlowGPT, two platforms that allow users to develop apps with the ability to upload knowledge files and set user visibility. This enabled us to simulate both public and workspace-specific scenarios, as described in subsection IV-D.\nOn GPT Store, we created an app that appeared to be a simple task management tool. However, the app's knowledge files contained a large number of phishing website URLs obtained from an open-source dataset. We configured two versions of the app: one publicly accessible and another visible only to a workspace. Users with access to the app could easily query the knowledge files and retrieve the phishing URLs. Similarly, on FlowGPT, we developed a note-taking app with knowledge files containing the same phishing website URLs. We also created two versions of this app: one public and another visible only to a limited set of users. In both cases, the malicious LLM apps were successfully created and config-ured to share content either publicly or only with designated users. The apps' knowledge files, containing a large number of phishing URLs, could be readily queried by those with access. Screenshots demonstrating these successful examples of exploiting LLM apps for illicit information dissemination are presented in Figure 9 and Figure 10 in the Appendix.\nIt is worth noting that our investigation uncovered 287 apps with malicious intent across 227 unique workspaces. Signifi-cantly, 24 of these workspaces contained two or more mali-cious apps. This finding suggests a pattern of repeated security breaches or intentional misuse within certain workspaces.\nEthics. It is crucial to emphasize that these apps were developed solely for experimental purposes and were immedi-ately deleted after the conclusion of the experiment, ensuring that they did not pose any real-world security threats. These simulations highlight the potential for malicious actors to exploit the ability to upload knowledge files and control user visibility settings on LLM app platforms. By creating apps that appear benign but contain harmful content, attackers can either broadly distribute or selectively target users with malicious information in both public and controlled environments."}, {"title": "VI. DISCUSSION", "content": "In(Security) of Different LLM App Stores\nIn the preceding sections, we conducted a comprehensive analysis of the security landscape within the LLM app ecosys-tem using a three-layer concern framework. To understand the disparities across different LLM app stores, we focused our analysis on six specific platforms. Table IX presents the proportion of LLM apps with abusive potential, malicious intent, and exploitable vulnerabilities within these app stores. It is important to note that the proportions are relative to the number of LLM apps detected; for example, out of the 24,983 LLM apps analyzed from FlowGPT, 13,562 were identified as having malicious intent, yielding a proportion of 54.28%."}, {"title": "B. Limitations", "content": "Despite the comprehensive framework and extensive anal-ysis, this study has several limitations that should be acknowl-edged. These limitations highlight areas where the research could be refined and expanded in future work to provide a more complete understanding of LLM app security.\n1) The dataset used in this study, although large, may not be entirely representative of the broader LLM app ecosystem. The six LLM app stores selected for analysis were chosen based on availability and relevance, but there are other stores that were not included. This could lead to an incomplete picture of the overall security landscape.\n2) The accuracy of our findings is influenced by the quality and completeness of the data provided by the app stores. Some platforms provided more detailed metadata and descriptions than others, potentially skewing the analysis. For instance, platforms that did not provide detailed app instructions or descriptions could not be thoroughly assessed for certain types of vulnerabilities.\n3) The methodology employed for detecting abusive poten-tial, malicious intent, and exploitable vulnerabilities relies on predefined criteria and automated tools, which may not capture all nuances of malicious behavior. Fortunately, our manual verification and self-refining detection techniques mitigate this limitation to a certain extent, enhancing the accuracy and comprehensiveness of our findings."}, {"title": "VII. RELATED WORK", "content": "Research on custom LLM apps\nThe emergence of custom LLM apps has sparked signif-icant interest in the research community. These LLM apps represent a new paradigm in AI-powered software that lever-ages the capabilities of LLMs for specific tasks or domains. Zhao et al. provide a vision and roadmap for LLM app store analysis, highlighting the need for systematic research into this emerging ecosystem. Their work emphasizes the im-portance of understanding the landscape, security implications, and potential impacts of LLM apps on various stakeholders.\nSeveral studies have analyzed the current landscape of LLM apps. Hou et al. introduced GPTZoo, a large-scale dataset containing metadata and content from over 730,000 GPT instances. Zhang et al. explored GPT apps' distri-bution and potential vulnerabilities. Su et al. analyzed the GPT Store, focusing on app characteristics and user engagement. Zhao et al. investigated the ecosystem of custom ChatGPT models and their implications.\nRecent studies have explored security risks in custom LLM apps. Tao et al. discuss the implications of custom GPT apps, highlighting opportunities and risks. Hui et al. investigate prompt leaking attacks against LLM applications. Iqbal et al. propose a security evaluation framework for LLM platforms, applied to OpenAI's ChatGPT plugins. An-tebi et al. examine risks associated with customized GPT apps, focusing on potential misuse. Lin et al. investigate real-world malicious services integrated with LLMs, empha-sizing cybersecurity challenges posed by LLM applications.\nIn contrast to previous research, our study presents the first comprehensive, systematic, and large-scale investigation of security issues across six major LLM app stores. We provide a multi-tiered classification and detection of security concerns, offering in-depth analysis of their implications.the transformative potential of these AI-powered applications."}, {"title": "B. Research on security concerns in LLMS", "content": "The rapid advancement of LLMs has raised significant security concerns. Wang et al. investigated the misuse potential of base LLMs through in-context learning, revealing vulnerabilities even in models without explicit fine-tuning. Zhang et al. questioned the effectiveness of alignment techniques in preventing misuse of open-sourced LLMs, sug-gesting that current safety measures"}]}