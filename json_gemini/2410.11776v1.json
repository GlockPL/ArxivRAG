{"title": "Encoding architecture algebra", "authors": ["Stephane Bersier", "Xinyi Chen-Lin"], "abstract": "Despite the wide variety of input types in machine learning, this diversity is often not fully reflected in their representations or model architectures, leading to inefficiencies throughout a model's lifecycle. This paper introduces an algebraic approach to constructing input-encoding architectures that properly account for the data's structure, providing a step toward achieving more typeful machine learning.", "sections": [{"title": "1 Introduction", "content": "There is growing awareness of the importance of designing model architectures that capture and respect the distinct structure of input data. Many successful deep learning architectures,\nsuch as transformers [1], convolutional neural networks (CNNs)[2], graph neural networks (GNNs) [3], and recurrent neural networks (RNNs)[4], inherently incorporate aspects of data structure.\nOngoing research focuses on refining existing architectures, as well as designing new ones for other types of structured data. For instance, DeepSets [5] are tailored to process sets, group and gauge equivariant CNNs [6][7] respect both global and local symmetries in the data, and strongly-typed RNNs [8] incorporate explicit types within recurrent networks. By accounting for the structure of the input data, these model architectures exhibit improved performance, better generalization with fewer parameters, and enhanced interpretability.\nIn parallel, the typeful programming paradigm [9] has been gradually gaining traction in software engineering. Typeful programming is about faithfully and formally representing/-modeling the mathematical structures that are relevant to the problem at hand. The machine learning (ML) community, however, remains largely type-unaware, reflected in how major ML frameworks mainly rely on a single, catch-all type known as \u201ctensor\u201d- a Jack of all trades, but master of none.\nIn this paper, we complement the existing work in structured machine learning by bringing more type-awareness into model design. Specifically, we propose a set of natural primitives to systematically build model architectures capable of handling arbitrarily complex algebraic data types (ADTs) as input. These primitives could be used as the theoretical basis for a higher-level typeful ML framework.\nThe core idea is the generalization of dense linear layers, where the input can be of a composite type. We call these layers multilinear flattening layers or MFLs. Together with primitive operations and constructors for each type, they can be used to algebraically construct architectures operating on any ADT.\nThe paper is organized as follows. Section 2 provides an overview of the high-level approach. In section 3, we introduce the flat type. The subsequent sections 4, 5, 6 and 7 explore each ADT, discussing their constructors, primitive operations, MFLs, and relevant special cases. Section 8 examines relationships between some ADT cases. Additional considerations, such as architecture simplification and weight sharing, are covered in section 9. Examples are presented in section 10, with the conclusion in section 11."}, {"title": "1.1 Notation reference", "content": "We use (multi)linear algebra notation, with bold lowercase letters representing vectors (e.g. v), and bold uppercase letters representing other tensors or (multi)linear maps (e.g. L). Components are denoted by the same letter in non-bold font, with subscripts indicating the axes (e.g. vi and Lij)."}, {"title": "2 Approach", "content": ""}, {"title": "2.1 Algebraic data types (ADTs)", "content": "The most basic flavor of ADTs includes only combinations of sums and products, as well as the unit type as the base case. However, the idea of algebraic composition can be extended to more base and inductive cases.\nIn this paper, we consider a richer base case, namely tensors, of which vectors, scalars and unit types are special cases, and one additional inductive case, namely multisets. The ADTs we consider are thus of one of the four forms below:\n\u2022 Tens[l1, ..., In]\n\u2022 Sum[T1, ..., Tn]\n\u2022 Prod[T1, ..., \u03a4\u03b7]\n\u2022 MSet [T]\nwhere T and T with i = 1, ..., n are themselves ADTs.\nEach ADT case comes with a set of allowed operations. Additionally, each inductive case has its own value constructors, and type constructor. The latter is covariant in its type parameters:\nC[T1, ..., Tn] <: C[T\u2081, ...,T] where: Ti <: T', i = 1, ..., n\n(1)"}, {"title": "2.2 Architecture algebra", "content": "The architecture algebra for the encoding of inputs is based on the following groups of primitives:\n\u2022 Multilinear flattening layers (MFLs)\n\u2022 Poly mapping\n\u2022 Fixed operations for each type\n\u2022 Constructors for the inductive types\nPrimitives can be composed (in accordance with their type signatures) to construct model architectures for the encoding of ADTs.\nThough we will not explore this further here, the architecture algebra can also be framed categorically. In this view, the objects of the category are ADTs, with type constructors acting as endofunctors. The morphisms of the category are architectures, composed of primitives (or only the MFLs, for a more restricted category), converting the source type to the target type."}, {"title": "2.3 Multilinear flattening layers (MFLs)", "content": "MFLs can be viewed as generalizations of dense (a.k.a. fully connected) linear layers. Whereas dense linear layers map from vector to coordinate vector, leading to a trivial type signature Vec[l'] \u2192 Yec[1], MFLs can handle non-trivial input types. They are defined for four basic input types, with corresponding type signatures:\n\u2022 Tens[11, ..., In] \u2192 Yec[1]\n\u2022 Sum[Vec[11], ..., Vec[In]] \u2192 Yec[1]\n\u2022 Prod[Vec[11], ..., Vec[In]] \u2192 Yec[1]\n\u2022 MSet[Vec[l']] \u2192 Yec[1]\nwhere the flat type Yec[1] is a subtype of the vector type Vec[1], see section 3.\nTo find the right MFL architecture for each case, the following (possibly redundant) properties were used as guiding principles:\n\u2022 Minimality\n\u2022 Invariance under changes in input vector basis\n\u2022 Invariance under changes in output vector basis"}, {"title": "2.4 Poly mapping", "content": "We extend the polytypic map function to accept multiple functions as arguments, a generalization we refer to as poly mapping. Its type signature is:\nmap: Prod[T\u2081 \u2192 T', Tn \u2192 T\u03b7, C[T1, ..., Tn ]] \u2192 C[T\u2081, ..., \u03a4\u03b7]\n(2)\nwhere C denotes any composite types. map is derivable from the primitive operations and constructors specific to each ADT. It plays a crucial role in enabling transformations of constituent types within composite types."}, {"title": "2.5 Linear versus affine", "content": "Dense linear layers are commonly written in two algebraically equivalent forms. The affine form explicitly shows the bias term, while the linear form absorbs the bias by bias-augmenting the input vector:\ny = b+Lv  \u2194  y = L'v',  v'_i = { 1  if i = 0 , vi if i = 1, ..., l  (3)\nwhere v and y are the input and output vectors, respectively, v' is the bias-augmented input vector, and L' is the augmented weight matrix that contains the bias vector b and the weight matrix L.\nThis linear/affine correspondence naturally generalizes to a multilinear/multiaffine relationship, which is used in our approach to deriving MFLs. Specifically, we first derive the MFLs in multilinear form, then obtain the multiaffine form by explicitly setting the bias components of the bias-augmented inputs to 1, and merging all the resulting bias terms that are equivalent."}, {"title": "3 The flat type", "content": "A vector should not be confused with its coordinate representation. The two are in bijection only when a basis is fixed.\nIn machine learning, a learned vector comes with an implicit learned basis, providing meaningful interpretation to its components. Thus, a learned vector can be understood as having a more refined type with additional operations. We refer to such vectors as yectors when represented in their learned basis:\nYec[1] <: Vec[1] (4)\nEach MFL maps a composite type to a yector type, which is the flat type."}, {"title": "3.1 Operations", "content": ""}, {"title": "4 Tensor types", "content": "Tensors are elements of the tensor product of vector spaces9:\nV(1) ... V(n) (5)\nwhere the vector spaces V(r), r = 1, ..., n are over the real numbers10 and have dimensionality lr \u2208 N.\nJust as vectors, tensors are geometric invariants which can be represented by components in different bases. For instance:\nN = \u2211Ni1... ine(1) \u2297...\u2297e(n) = \u2211N...\u0113(1) \u2297...\u2297\u0113(n) (6)\ni1,..., in\ni1,...,in\nwhere {er)} and {e(r)}\nwith ir = 1, ....,l, are two distinct bases that span V(r).\nThe corresponding tensor types do not depend on any other algebraic type and are fully parameterized by axis lengths (a.k.a. tensor shape):\nTens [11,..., In], li \u2208 N, i = 1, ..., \u03b7 (7)\nThe number of axes n is known as the order (or rank) of the tensor."}, {"title": "4.1 Isomorphisms", "content": "The tensor type has the following isomorphisms:\n\u2022 Axis reordering: Tens[l1, ..., In] = Tens[lo(1), ..., lo(n)]\n\u2022 Scalar collapse: Tens [11, ..., In, 1] = Tens[l1, ..., In]\n\u2022 Unit collapse: Tens[11, ..., In, 0]\u2243 Tens[0]"}, {"title": "4.2 Operations", "content": ""}, {"title": "4.3 MFL", "content": "First, let us augment the tensor by adding bias dimensions to each of its axes. The type signature of the bias-augmentation operation is:\nTens[11, ..., In] \u2192 Tens[l\u2081 + 1, In+1] (8)\nand the augmented tensor is defined as follows:\nNi...in = {  1 if \u2203r s.t. ir = 0 , Ni1...in otherwise (9)\nThe flattening for the bias-augmented tensor type, with signature:\nTens[l\u2081 + 1, ..., In + 1] \u2192 Yec[1] (10)\nis defined by an architecture that must correctly handle tensor structure, using the previously defined tensor operations:\ny_k = \\sum_{i_1=0}^{l_1} ... \\sum_{i_n=0}^{l_n} (L_k^{(1)}w_{i_1}^{(1)} \\cdots w_{i_n}^{(n)} + \\cdots + w_{i_1}^{(1)} \\cdots w_{i_{n-1}}^{(n-1)}L_k^{(n)})N_{i_1...i_n}, k = 1,..., l (11)\nwhere w(r) and L(r), for r = 1, ..., n, are weight vectors and matrices, respectively11. \nThe MFL architecture for the original tensor (i.e. without bias augmentation), with type signature:\nTens[11, ..., In] \u2192 Yec[1] (12)\nis obtained from equation (11) by explicitly setting all the bias components to 1:\ny_k = b_k + \\sum_{j_1=1}^{l_1} ... \\sum_{j_n=1}^{l_n} (L_k^{(1)}w_{j_1}^{(2)} \\cdots w_{j_n}^{(n)} + \\cdots + w_{j_1}^{(1)} \\cdots w_{j_{n-1}}^{(n-1)}L_k^{(n)}) N_{j_1...j_n} (13)"}, {"title": "4.4 Special cases", "content": ""}, {"title": "4.4.1 Vector types", "content": "Vector types correspond to tensor types with one axis:\nVec[1] = Tens[1] (15)\nAs a result, the MFL architecture (13) reduces to the standard densely connected linear layer, as it should:\ny = b+Lv (16)"}, {"title": "4.4.2 Scalar type", "content": "The scalar type is equivalent to the vector type with one dimension:\nScal = Vec[1] (17)"}, {"title": "4.4.3 Unit type", "content": "The unit type is equivalent to the extremal case of the vector type with no dimensions:\nUnit = Vec[0] (18)\nIts only value is the empty vector. The linear layer (16) degenerates to a single bias vector, also known as an embedding:\ny = b (19)"}, {"title": "5 Sum types", "content": "A sum type, also known as a tagged union type, is a data type that can hold values from one of several predefined types, but only one at a time. Each possible type is tagged to distinguish which variant is currently being held. Sum types are useful for representing data that can take on multiple shapes."}, {"title": "5.1 Constructors", "content": "The constructor of a sum type is specialized by an index representing the type tag:\ncase[T1,...,T](i) (ti) is of type Sum[T\u2081,..., Tn] (20)\nwhere ti is of type Ti."}, {"title": "5.2 Isomorphisms", "content": "Sum types are isomorphic under argument reordering:\nSum[T\u2081, ..., Tn] = Sum[To(1), ..., To(n)] (21)\nand a sum type with a single argument is isomorphic to it:\nSum[T] \u2243 T (22)"}, {"title": "5.3 Subtyping relations", "content": "A sum type is covariant in its type parameters:\nSum[T1, ..., Tn] <: Sum[T\u2081, ...,T] where: Ti <: T\u00a6, i = 1, ..., \u03b7 (23)\nand\nSum[T1, ..., Tn-1] <: Sum[T1, ..., \u03a4\u03b7\u22121, \u03a4\u03b7] (24)"}, {"title": "5.4 Operations", "content": ""}, {"title": "5.5 MFL", "content": "The flattening:\nSum[Vec[11], ..., Vec[In]] \u2192 Yec[1] (25)\nis achieved using case analysis, where a separate linear layer (16):\nVec[l] \u2192 Yec[1], r = 1, ..., n (26)\nis applied to each case, that is:\ny = { b^(1) + L^(1)v^(1) : b^(n) + L^(n)v^(n)  (27)"}, {"title": "5.6 Union types", "content": "Union types can be reduced to sum types by first converting the types in the union to a disjoint set of types."}, {"title": "5.7 Special cases", "content": ""}, {"title": "5.7.1 Enums", "content": "The enumeration types or enums correspond to extremal sum types, where each argument of the sum is the unit type:\nEnum[1] = Sum[Unit,  Unit] (28)\nwhere l is the number of arguments in the sum type."}, {"title": "5.7.2 Boolean", "content": "The boolean type is equivalent to:\nBool = Enum[2] = Sum[Unit, Unit] (29)"}, {"title": "5.7.3 The empty type", "content": "While of little direct practical interest, the empty sum type is equivalent to the uninhabited type:\nNothing = Sum[ ] (30)"}, {"title": "5.7.4 Option types", "content": "Option types (a.k.a. maybe types) are equivalent to sum types with two cases. An option type value is either empty, or it is a wrapped value from the underlying type:\nOption[T] = Sum[Unit, T] (31)"}, {"title": "6 Product types", "content": "A product type, whose values are often referred to as a tuples, is a data type that groups multiple values, each from potentially different types, into a single composite structure. Each element of a tuple can be accessed individually, making product types useful for representing data with multiple fields."}, {"title": "6.1 Constructors", "content": "We use tuple notation for product type literals:\n(t1,..., tn) is of type Prod[T1, ..., \u03a4\u03b7] (32)"}, {"title": "6.2 Isomorphisms", "content": "Product types are isomorphic under argument reordering:\nProd[T\u2081, ..., Tn] = Prod[To(1), ..., To(n)] (33)\nand a product type with a single argument is isomorphic to it:\nProd[ T ] = T (34)"}, {"title": "6.3 Subtyping relations", "content": "A product type is covariant in its type parameters:\nProd[T1, ..., Tn] <: Prod[T\u2081, ...,T] where: Ti <: T\u00a6, i = 1, ..., \u03b7 (35)\nand\nProd[T1, ..., Tn\u22121, Tn] <: Prod[T1, ..., Tn\u22121] (36)"}, {"title": "6.4 Operations", "content": ""}, {"title": "6.5 MFL", "content": "Consider a product type of vectors. As with the tensor case, we begin by bias-augmenting the vectors. The type signature of the bias-augmentation operation is:\nProd[Vec[11], ..., Vec[In]] \u2192 Prod[Vec[l\u2081 + 1], ..., Vec[In + 1]] (37)\nand the bias-augmented vectors are:\nv_i^{(r)} = \\begin{cases}\n1 & \\text{if } i = 0 \\\\\nv_i^{(r)} & \\text{otherwise}\n\\end{cases},  \\text{for } r = 1, ..., n (38)\nThe flattening for the product type of bias-augmented vectors, with type signature:\nProd[Vec[11 + 1], ..., Vec[In + 1]] \u2192 Yec[l] (39)"}, {"title": "6.6 Special case", "content": "The unit type can also be seen as the extremal case of the empty product type. The MFL (43) reduces to an embedding (19)."}, {"title": "7 Multiset types", "content": "A multiset is like a set, except that duplicates are allowed. Multisets are useful for grouping values of a shared type when order is irrelevant but frequency matters, and also when the number of values to be grouped might vary."}, {"title": "7.1 Constructor", "content": "We use curly bracket notation to represent multiset literals:\n{X1,..., Xn}, \u2200n \u2208 N is of type MSet[T] (44)"}, {"title": "7.2 Subtyping relation", "content": "A multiset type is covariant in its type parameter:\nMSet[T] <: MSet[T'] where T <: T' (45)"}, {"title": "7.3 Operations", "content": ""}, {"title": "7.4 MFL", "content": "The type signature of the multiset MFL is:\nMSet[Vec[l']] \u2192 Yec[1] (47)\nThe architecture is obtained by folding the elements via summation, see Theorem 2 in [5]. For a given multiset:\n{v(1), ..., v(n)} (48)"}, {"title": "8 Type relations", "content": ""}, {"title": "8.1 Product and tensor", "content": "A product of tensors can be seen as a refinement of a tensor of a higher order:\nProd[Tens[11, ..., In], ..., Tens[l1, ..., In]] <: Tens[l', 11, ..., In ] (51)\nwhere there are l' tensor types in the product type.\nFor simplicity's sake, we shall investigate the details of this relationship only for the special case where n = 1:\nProd[Vec[1], ..., Vec[1]] <: Tens[l',l] (52)\nLet us convert a vector tuple to a tensor:\n(v(1), ..., v(1')) \u2192 N\nvi^((i)) \u2192 Nij, i = 1, ..., l', j = 1, ..., l\nThe tensor MFL (13) for order-2 tensors is:\ny_k = b_k + \\sum_{i=1}^{l'} \\sum_{j=1}^{l} w_i^{(1)} w_j^{(2)} N_{ij} (53)\nIt can be seen as a special case of the product type MFL (42) that is truncated to linear order (i.e. higher-order weight tensors are set to zero), with weight matrices constrained to a specific structure:\ny_k = b_k + \\sum_{i=1}^{l'} \\sum_{j=1}^{l} L_{kj} v_j^{(1)} , L_{kj}^{(i)} = L_{ki}^{(2)} w_j^{(2)} = w_i^{(1)} L_{kj}^{(2)} (54)\nIn conclusion, converting a product type to a tensor type involves discarding the non-linear interaction terms and having a restricted class of weight matrices."}, {"title": "8.2 Tensor and multiset", "content": "A tensor can be seen as a refined form of a multiset of lower-order tensors:\nTens[l', 11, ..., In] <: MSet [Tens[11, ..., In]] (55)\nLike in the previous subsection, we shall investigate the details of this relationship only for the special case where n = 1:\nTens[l',l] <: MSet[Vec[1]] (56)\nLet us convert an order-2 tensor to a multiset of vectors:\nN \u2192 {v(1), ..., v(l') }\nNij \u2192 vij, i = 1, ..., l', j = 1, ..., l.\nThe corresponding multiset MFL (49):\ny_k = l' b_k + \\sum_{i=1}^{l'} \\sum_{j=1}^{l} L_{kj} v_j^{(i)} (57)\nis a special case of the tensor MFL (53):\ny_k = b_k \\Bigg[\\sum_{i=1}^{l'} (L_k w^{(2)}) + w^{(1)} L_k^((2)) \\Bigg] N_{ij} (58)\nwhen:\nb_k = l'b_k and L_k w^{(2)} + w^{(1)} L_k^((2)) = L_{kj}, \\forall i (59)\nIn conclusion, converting a tensor type to a multiset type results in the weight matrix losing its dependency on the replaced tensor axis."}, {"title": "8.3 Product, tensor and multiset", "content": "Given the subtyping relationships discussed in the preceding subsections, we conclude that:\nProd[Tens [11, ..., In], ..., Tens [11,..., In]] <: Tens[l', 11,..., In] <: MSet[Tens[11,..., In ]]\nwhere there are l' tensor types in the product type."}, {"title": "8.4 Product and multiset", "content": "A generic product type can also be viewed as a refinement of a multiset type:\nProd[T1, ..., Tn] <: Prod[U, ...,U] <: MSet[U] (60)\nwhere U = Union[T\u2081,..., Tn]. In converting a product type to a multiset type, some information is lost, such as type tags (or positions)."}, {"title": "9 Additional considerations", "content": ""}, {"title": "9.1 Simplification", "content": "The resulting composed architecture might contain some redundant parts, namely consecutive linear layers. These may be merged into single linear layers. This is usually advantageous, as it can reduce the number of weights and matrix multiplications in the architecture. For example:\nM \\xrightarrow{V^{(1)}} N \\xrightarrow{V^{(2)}} L \\xrightarrow{V^{(3)}} (64)\nThat is, the two separate linear layers M and N can be replaced with L."}, {"title": "9.2 Weight sharing", "content": "Whenever data transiting within one part of a model shares the exact same type (including semantics) with data in another part, the same submodel should be used for both. Therefore, the type tree\u2014as well as the corresponding architecture\u2014should be reduced to a directed acyclic graph.\nSimilarly, when data types T\u2081 and T\u2082 share a non-trivial upper bound T, a common submodel can manage the shared structure of T. However, separate submodels should handle the unique characteristics of T\u2081 and T2.\nMore generally, if one type can be derived from another, weight sharing is possible. For instance, if T can be projected onto T' in a semantically meaningful way, then a model for T' can be reused as part of a model for T."}, {"title": "9.3 Number types", "content": "A whole paper could be devoted to the encoding of various types of numbers for different contexts. For instance, numbers representing magnitudes should be encoded differently from whole numbers where factorization is significant.\nIn general, as much information as possible about the number type\u2014such as its structure, semantics, distribution, and usage should inform the encoding process.\nIt is often helpful to encode a single number as a coordinate vector, with each component encoding the number (or part of it) in a distinct way."}, {"title": "10 Examples", "content": ""}, {"title": "10.1 Features", "content": "Consider two input features with the following types:\n\u2022 Option[Scal]\n\u2022 Option[Bool]\nwhere the option type takes into account that some values may be missing in the training data. These features are combined into a product type:\nProd[Option[Scal], Option[Bool]] (65)\nThe flattening architecture for this composite type is shown in fig. 3. Let us consider the case where only MFLs are used. Then, the flattening architecture for the product type above can be further simplified using linear composition, see fig. 4, as discussed in section 9.1. The final flattened yector is:\ny = b + L(1)y(1) + L(2)y(2) + C(1,2) (y(1), y(2)) (66)"}, {"title": "10.2 Lists", "content": "The list data type is an algebraic data type that is defined recursively:\nList[T] = Sum[Nil, Prod[T, List[T]]] (67)\nwhere the empty list Nil is isomorphic to Unit. We expect the flattening architecture of the list type to reflect its recursive structure.\nLet us consider a list value, built with the (infix) list constructor:\nXn:::: X\u2081 :: Nil (68)\nNow, assume we have already flattened Nil (via an embedding) and the elements of the list:\nNil \u2192 y(0)\nXi \u2192 y(i), i = 1, ..., n\nThe flattening architecture (in bias-augmented form) is then a recurrent neural network (RNN), with recurrence relation:\nh(0) = y(0)\nh(i) = M(y(i), h(i-1)), i = 1,...,n\ny = h(n) (69)\nwhere M is the bilinear map in the multilinear form of the product type MFL (40)."}, {"title": "11 Conclusion", "content": "In this paper, we introduced the primitives for an algebra allowing the construction of model architectures that respect the structure of types constructed inductively from sums, products, and multisets, with the tensor type as the base case. Together, we refer to these types as algebraic data types (ADTs).\nWe characterized each ADT by a core set of primitive operations and explored the relationship between tensors, product types and multisets, which are sometimes conflated in machine learning practice.\nOur key contribution is multilinear flattening layers (MFLs), which generalize dense linear layers to accommodate ADTs. Notably, the MFL for tensor types (13) naturally exhibits a low-rank decomposition due to the inherent structure of tensors, whereas the MFL for product types (42) is full-rank.\nBy way of illustration, we presented minimal flattening architectures for a product of option types, and for the recursively defined type List[T]. The latter naturally results in a recurrent architecture. In a similar fashion, architectures for trees and graphs can be derived.\nFollow-up work could focus on the proper handling of additional factors that should influence architecture, such as:\n\u2022 Input structure (including abstract data types)\n\u2022 Output structure\n\u2022 Implicit structure preservation\n\u2022 Algorithmic/compositional structure\nThe ultimate goal is a solid compositional foundation for structured and typeful machine learning."}]}