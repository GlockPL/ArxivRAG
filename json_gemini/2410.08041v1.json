{"title": "On the Convergence of (Stochastic) Gradient Descent for Kolmogorov-Arnold Networks", "authors": ["Yihang Gao", "Vincent Y. F. Tan"], "abstract": "Kolmogorov-Arnold Networks (KANs), a recently proposed neural network architecture, have gained significant attention in the deep learning community, due to their potential as a viable alternative to multi-layer perceptrons (MLPs) and their broad applicability to various scientific tasks. Empirical investigations demonstrate that KANs optimized via stochastic gradient descent (SGD) are capable of achieving near-zero training loss in various machine learning (e.g., regression, classification, and time series forecasting, etc.) and scientific tasks (e.g., solving partial differential equations). In this paper, we provide a theoretical explanation for the empirical success by conducting a rigorous convergence analysis of gradient descent (GD) and SGD for two-layer KANs in solving both regression and physics-informed tasks. For regression problems, we establish using the neural tangent kernel perspective that GD achieves global linear convergence of the objective function when the hidden dimension of KANs is sufficiently large. We further extend these results to SGD, demonstrating a similar global convergence in expectation. Additionally, we analyze the global convergence of GD and SGD for physics-informed KANs, which unveils additional challenges due to the more complex loss structure. This is the first work establishing the global convergence guarantees for GD and SGD applied to optimize KANs and physics-informed KANs.", "sections": [{"title": "1 Introduction", "content": "The Kolmogorov-Arnold Representation Theorem (KART) [10, 32, 33] states that any continuous (multivariate) function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ can be decomposed into a sum of univariate continuous functions. Specifically, there exists a set of univariate functions $\\{\\phi_{p,q}\\}_{p=1,q=1}^{n,2n+1}$ and $\\{\\psi_q\\}_{q=1}^{2n+1}$ such that\n\n$f(x) = \\sum_{q=1}^{2n+1} \\psi_q \\left( \\sum_{p=1}^{n} \\phi_{p,q}(x_p) \\right), \\qquad(1)$\n\nwhere $x = [x_1, x_2, ..., x_n]^T$. Inspired by this theorem, efforts have been made to develop novel network architectures, as an alternative to multi-layer perceptrons (MLPs) supported by the Universal Approximability Theory (UAT). However, previous approaches have been largely restricted by the limited width of networks, due to the representation in Equation (1), where the hidden dimension"}, {"title": "2 Preliminaries", "content": "In this section, we begin by introducing notations used throughout the paper. We then review the foundational concepts and definitions of KANs, including their mathematical formulation and various extensions. Finally, we discuss the problem setting for applying KANs to regression problems."}, {"title": "2.1 Notation", "content": "In this paper, we use bold capital letters (e.g., $\\mathbf{A}$) and bold lowercase letters (e.g., $\\mathbf{x}$) to denote matrices and vectors, respectively, while scalars are represented using regular (non-bold) letters (e.g., $a$). The symbol $\\lesssim$ is used to denote upper bounds up to a (unimportant) constant factor, i.e., $a \\lesssim b$ implies that there exists a positive constant $c$ such that $a < c\\cdot b$. Similarly, $a \\gtrsim b$ indicates $a > c\\cdot b$ for some positive constant $c$, and $a \\sim b$ is equivalent to $a \\lesssim b$ and $b \\lesssim a$. We use $[n]$ to denote $\\{1, 2, ..., n\\}$. Additionally, we let $(g \\circ f)(x)$ denote the composition $g(f(x))$, and $(g \\circ f \\cdot h)(x)$ represent the composition combined with multiplication, defined as $(g \\circ f \\cdot h)(x) = g(f(x)) \\cdot h(x)$. The operation $\\text{vec}(\\cdot)$ reshapes the given matrix or tensor into a column vector. For notational convenience, we simplify $\\mathcal{L}(a(t), c(t))$ as $\\mathcal{L}(t)$, if without any ambiguity, which is also applied for other notations such as $\\mathcal{G}$ and $s$."}, {"title": "2.2 Kolmogorov-Arnold Networks", "content": "According to the primary definition of KANs in [39], the ($l+1$)-st layer $z_{l+1} = [z_{l+1,1}, ..., z_{l+1,n_{l+1}}]$ of a KAN is given by:\n\n$z_{l+1,q} = \\sum_{p=1}^{n_l} \\phi_{l,p,q}(z_{l,p}), \\quad q \\in [n_{l+1}],$\n\nwhere $n_l$ is the width of the $l$-th layer and $\\phi_{l,p,q}$ denotes the $(p,q)$-th representation function in the $l$-th layer. In practice, we adopt splines or polynomials to parameterize and approximate $\\phi_{l,p,q}$ due to their universal approximability. Specifically, we define\n\n$\\phi_{l,p,q}(z) = \\sum_{k=1}^{n_d} a_{l,p,q,k}b_k(z),$\n\nwhere $\\{b_k\\}_{k \\in [n_d]}$ is a set of basis functions. Sidharth and Gokul [51] suggested using Chebyshev polynomials as basis functions due to their orthogonality, powerful approximation capability, and rapid convergence. Aghaei [2,4] adopted rational and fractional functions for modeling non-polynomial and more complex behaviors. Qiu et al. [44] replaced the traditional B-splines with ReLU activations of higher-order powers and introduced trainable turning points. This modification significantly enhances GPU acceleration for KANs and mitigates the computational bottlenecks associated with B-splines. Bozorgasl and Chen [9] incorporated wavelet functions as basis functions, enabling KANs to efficiently capture both low-frequency and high-frequency components of input sequences. Ta et al. [53] explored combinations of radial basis functions, B-splines, wavelet functions, and differences of Gaussians, advocating for future KAN designs to leverage a diverse set of basis functions. Seydi [47] conducted a comprehensive survey and empirical comparison of various polynomials as basis functions in KANs, contributing to a deeper understanding of their capabilities and limitations. Notably, the approximability of polynomials, rational, and fractional functions is typically restricted to a bounded domain (e.g., the Chebyshev polynomial is usually defined on $[-1,1]$). Thus, an additional nonlinear transformation is necessary to map input values into the desired domain [2,4,51]. Specifically, we can alternatively design:\n\n$\\phi_{l,p,q}(z) = \\sum_{k=1}^{n_d} a_{l,p,q,k}b_k(\\phi(z)),$\n\nwhere $\\phi$ is a function that maps the input to a bounded interval. Common and practical choices for $\\phi$ include the hyperbolic tangent (tanh) and sigmoid functions. Here, the width $n_l$ for each layer, the depth $l \\in [L]$, the number of basis functions $n_d$, and the selection of basis functions are all considered hyperparameters."}, {"title": "2.3 Problem Formulation", "content": "We consider a two-layer KAN (i.e., depth $L = 2$) with a sufficiently large hidden dimension $m$, formulated as\n\n$f(x; a, c) = \\frac{1}{\\sqrt{m}} \\sum_{q=1}^{m} c_{q,k}b_k \\left( \\phi \\left(\\sum_{p=1}^{n} \\sum_{k_1=1}^{n_d} a_{p,q,k_1}b_{k_1}(x_p) \\right) \\right), \\qquad(2)$\n\nwhere $x = [x_1,...,x_n]^T \\in \\mathbb{R}^n$ is the input vector. The trainable parameters $a \\in \\mathbb{R}^{mnn_d}$ and $c \\in \\mathbb{R}^{m n_d}$ are defined as: $a = [a_1^T, ..., a_m^T]^T$ with $a_q = \\text{vec}(\\{a_{p,q,k}\\}_{p=1,k=1}^{n,n_d})$ and $c = [c_1^T, ..., c_m^T]^T$"}, {"title": "3 Convergence Analysis for GD", "content": "In this section, we analyze the convergence of GD when optimizing the loss function defined in Equation (4). Our analysis is built upon the property that the Gram matrix in Equation (7) remains"}, {"title": "4 Convergence Analysis for SGD", "content": "In this section, we analyze convergence of SGD when it is used to optimize the loss function in Equation (4). Compared to Section 3, where we examined the convergence of GD, this section focuses on a more practical setting using SGD. The SGD algorithm updates trainable parameters according to the following rule\n\n$a(t + 1) = a(t) - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial a}(a(t), c(t)),$\n$c(t + 1) = c(t) - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial c}(a(t), c(t)), \\qquad(12)$\n\nwhere $\\eta > 0$ is the step size, and\n\n$\\mathcal{L}(a, c) = \\frac{1}{b} \\sum_{i \\in \\mathcal{I}} \\frac{1}{2} (f_i(a, c) - y_i)^2, \\qquad(13)$\n\nwhere $\\mathcal{I}$ is the set of indices (mini-batch) sampled uniformly from $[N]$, and $b = |\\mathcal{I}|$ denotes the batch size. The idea of proof is similar to that in Section 3, however, the stochasticity introduced by SGD adds additional noise, making it more challenging to control the trajectory of the optimization process.\n\nHere, we highlight some key differences of the analysis of GD and SGD. First, unlike the deterministic linear convergence of GD established in Theorem 1, we show that SGD achieves linear"}, {"title": "5 Physics-Informed KANS", "content": "Physics-informed machine learning is one of the most promising and potential applications for KANS [3, 46, 54, 57]. In this section, we focus on the convergence analysis of GD and SGD for physics-informed KANs. Consider the following second-order partial differential equation (PDE):\n\n$\\begin{aligned}\n\\mathcal{D}[u(x)] &= v(x), \\quad x \\in \\Omega,\nu(x), x \\in \\partial\\Omega,\n\\end{aligned}$\nwhere $\\Omega \\subseteq \\mathbb{R}^n$ is the domain of interest, $\\partial \\Omega$ is its boundary, and the first component $x_1$ of $x$ denotes the temporal domain, while the remaining components represent the spatial variables. We denote the differential operator as $\\mathcal{D}[u(x)]$, where $\\mathcal{D}[u(x)] := \\frac{\\partial u(x)}{\\partial x_1} - \\sum_{i,j=2}^{n} h_{i,j}(x) \\frac{\\partial^2 u(x)}{\\partial x_i \\partial x_j} - \\sum_{i=2}^{n} g_{i}(x) \\frac{\\partial u(x)}{\\partial x_i} - l_1(x) \\cdot u(x)$. Given training samples $\\{(x_i, v_i)\\}_{i=1}^{N_1}$ in the interior domain and $\\{(x_i, u_i)\\}_{i=1}^{N_2}$ on the boundary, the loss function for physics-informed KANs is formulated as\n\n$\\mathcal{L}^{PDE}(a, c) = \\frac{1}{N_1} \\sum_{i=1}^{N_1} (\\mathcal{D}[f(x_i; a, c)] - v_i)^2 + \\frac{1}{N_2} \\sum_{i=1}^{N_2} (f(x_i; a, c) - u_i)^2.\\qquad(15)$\n\nIn SGD, the mini-batch loss is given by\n\n$\\mathcal{L}^{PDE}(a, c) = \\frac{1}{b_1} \\sum_{i \\in \\mathcal{I}} (\\mathcal{D}[f(x_i; a, c)] - v_i)^2 + \\frac{1}{b_2} \\sum_{i \\in \\mathcal{I}'} (f(x_i; a, c) - u_i)^2,\\qquad(16)$\n\nwhere $\\mathcal{I}$ and $\\mathcal{I}'$ are the sets of indices for mini-batches sampled from the interior and boundary, with batch sizes $b_1$ and $b_2$, respectively. We assume that $\\frac{N_1}{b_1} = \\frac{N_2}{b_2} = \\tau$. Due to space limits, we omit further details here and provide all technical derivations in Appendix C."}, {"title": "6 Conclusion and Future Works", "content": "In this paper, using the neural tangent kernel perspective, we conduct rigorous convergence analyses for GD and SGD in optimizing KANs for regression and physics-informed tasks. We prove that GD achieves global linear convergence, while SGD enjoys global linear convergence in expectation, provided that the hidden dimension is sufficiently large. Our work is the first to theoretically explain the empirical observation that SGD achieves near-zero training loss when training KANs in various tasks, contributing to a deeper understanding of the behavior and effectiveness of KANs. We also discuss some limitations of our current results.\n\nThere are several promising directions for future investigation of KANs. First and foremost is the exploration of the inherent and intricate structural advantages of KANs over MLPs, especially in various scientific applications (e.g., physics-informed KANs). While we derived the convergence analysis of SGD for KANs, our results do not fully capture the structural superiority of KANs, making it a compelling and challenging topic for further and deeper research.\n\nSecond, our results involve assumptions on the smoothness and boundedness of the basis and transformation functions (Assumption 2). Although these assumptions are realizable for many variants of KANs, they do not fully capture the influence of different basis functions on the model's behaviours. Despite the significant variation in performances observed with different basis functions, our current analyses do not reflect these differences. Therefore, establishing theoretical guidelines for selecting appropriate basis functions is an essential and open area of research.\n\nLastly, although we have proven that SGD is effective in training KANs for regression and physics-informed tasks, exploring other optimization algorithms specifically designed to leverage the unique structure of KANs could yield further performance improvements. As a brand new model, KANs have shown great potential across various applications but still require more in-depth exploration. This work is only the beginning, and there is much more to discover and understand about KANS."}]}