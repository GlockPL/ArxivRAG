{"title": "REEF: REPRESENTATION ENCODING FINGERPRINTS FOR LARGE LANGUAGE MODELS", "authors": ["Jie Zhang", "Dongrui Liu", "Chen Qian", "Linfeng Zhang", "Yong Liu", "Yu Qiao", "Jing Shao"], "abstract": "Protecting the intellectual property of open-source Large Language Models (LLMs) is very important, because training LLMs costs extensive computational resources and data. Therefore, model owners and third parties need to identify whether a suspect model is a subsequent development of the victim model. To this end, we propose a training-free REEF to identify the relationship between the suspect and victim models from the perspective of LLMs' feature representations. Specifically, REEF computes and compares the centered kernel alignment similarity between the representations of a suspect model and a victim model on the same samples. This training-free REEF does not impair the model's general capabilities and is robust to sequential fine-tuning, pruning, model merging, and permutations. In this way, REEF provides a simple and effective way for third parties and models' owners to protect LLMs' intellectual property together. The code is available at https://github.com/tmylla/REEF.", "sections": [{"title": "INTRODUCTION", "content": "The training process of Large Language Models (LLMs) requires extensive computational resources and time. Therefore, open-source models are usually released with specific licenses (e.g., Apache2.0, and LLaMA 2 Community License (Meta AI, 2023)) to protect their intellectual properties (IPs). Unfortunately, some developers claim to have trained their own LLMs but actually wrapped or fine-tuned based on other base LLMs (e.g., Llama-2 and MiniCPM-V) (OpenBMB, 2023; 01-ai, 2023). It is urgent for model owners and third parties to identify whether the suspect model is a subsequent development of the victim model (e.g., Code-llama trained from Llama-2) or is developed from scratch (e.g., Mistral).\nThe key is to extract unique features (i.e., fingerprints) that can authenticate the victim model. Watermarking methods artificially inject triggers into the victim model to make it generate specific content for identification (Peng et al., 2023a; Xu et al., 2024). However, watermarks introduce extra training costs and impair the model's general capabilities (Russinovich & Salem, 2024), or even can be removed (Wang & Kerschbaum, 2019; Chen et al., 2023a). More crucially, these methods can not be applied to models that have already been open-released. An alternative is to extract intrinsic features of the victim model, avoiding additional training and the compromise of capabilities. Weight-based fingerprints are one of intrinsic features that allow calculating the similarity between a suspect model and a victim model's weights for identification (Zeng et al., 2023; Refael et al., 2024). However, these methods are fragile to major changes in weights, e.g., weight permutations, pruning, and extensive fine-tuning (Fernandez et al., 2024; Xu et al., 2024). This necessitates extracting more robust intrinsic features as fingerprints to identify victim models and protect their IPs.\nIn this paper, we propose to solve this problem from the perspective of the feature representations of LLMs, beginning with the following visualization analysis. It is generally acknowledged that"}, {"title": "RELATED WORK", "content": "Model fingerprinting protects IPs by allowing model owners and third parties to authenticate model ownership. There are two types of fingerprints for LLMs. One is injected fingerprints, which are ar- tificially added during training or fine-tuning to facilitate model identification, such as watermarking methods (Peng et al., 2023a; Xu et al., 2024). The other is intrinsic fingerprints, which are inherent"}, {"title": "EXPLORING THE POTENTIAL OF FEATURE REPRESENTATIONS AS FINGERPRINTS", "content": "In this section, we propose to utilize feature representations as LLM fingerprints to identify whether a suspect model is a subsequent development of the victim model, based on the following two obser- vations. (1) Feature representations of fine-tuned victim models are similar to feature representations of the original victim model, while the feature representations of unrelated models exhibit distinct distributions, as shown in Figure 1(a). (2) Some high-level semantic concepts are \u201clinearly\u201d encoded in the representation space of LLMs and can be easily classified, such as safety or unsafety and hon- est or dishonest (Zou et al., 2023; Slobodkin et al., 2023; Qian et al., 2024). According to these two observations, we can train a binary classifier on the representations of the victim model and then apply it to various suspect models' representations, i.e., LLMs derived from the victim model and unrelated LLMs. In this way, such a classifier may generalize to different fine-tuned victim models, because they have similar feature representations.\nThe binary classifier can employ various Deep Neural Network (DNN) architectures, such as a lin- ear classifier, Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Graph Convolutional Network (GCN). For training, we use the TruthfulQA dataset (Lin et al., 2022), con- catenating each question with its truthful answer as positive samples and with its false answer as negative samples. The dataset is split into training and test sets with a 4:1 ratio. To evaluate the classifier's performance, we conduct experiments on LLMs of varying sizes. Specifically, we select Llama-2-7b and Llama-2-13b as the victim models, while derived models and unrelated LLMs serve as suspect models for comparison."}, {"title": "ROBUST REPRESENTATION-BASED FINGERPRINTING WITH REEF", "content": "To address the challenges of classifiers in victim model identification, we propose REEF, an ad- vanced representation-based fingerprinting approach that can adapt to suspect models with varying representation dimensions and is robust to representation permutations.\nREEF identifies whether a suspect model is derived from a victim model, given the representations of these two models on certain examples. Specifically, let $X \\in \\mathbb{R}^{m \\times P_1}$ denote activations of the $l$-th layer from the suspect model on $m$ examples and $Y \\in \\mathbb{R}^{m \\times P_2}$ denotes activations of the $l'$-th layers from the victim model on the same $m$ examples, where $p_1$ is independent of $p_2$, meaning there is no limitation on dimensional consistency. Therefore, we need a similarity index $s(.,.)$ to measure representations' similarity between the suspect and victim models. In this way, a high $s(X, Y)$ score indicates that the suspect model is more likely derived from the victim model. In contrast, a low $s(X, Y)$ score means that the suspect model is less likely derived from the victim model.\nCentered Kernel Alignment. CKA (Kornblith et al., 2019) is a similarity index based on Hilbert- Schmidt Independence Criterion (HSIC) (Gretton et al., 2005), which measures the independence between two sets of random variables. The CKA similarity between $X$ and $Y$ can be computed as follows\n$CKA(X, Y) = \\frac{HSIC(X, Y)}{\\sqrt{HSIC(X, X) HSIC(Y, Y)}},$ \nwhere $HSIC(X, Y) = \\frac{1}{(m-1)^2}tr(K_x H K_y H)$. Specifically, $H = I - \\frac{1}{m}11^T$ is a centering matrix. $K_x$ and $K_y$ are Gram matrices that measure the similarity of a pair of examples based on kernel function $k$, i.e., $(K_x)_{ij} = k(X_i, X_j)$ and $(K_y)_{ij} = k(Y_i, Y_j)$. $X_i$ and $X_j$ denote the $i$-th and $j$-th row of $X$, respectively.\nKernel Selection. In this study, we consider a linear kernel and a Radial Basis Function (RBF) kernel. In the linear kernel case, Gram matrix $K_x = XX^T$. In the RBF kernel case, $k(X_i, X_j) =$"}, {"title": "EXPERIMENTS", "content": "In this section, we provide a comprehensive evaluation of REEF. Section 5.1 evaluates REEF's effec- tiveness in distinguishing LLMs derived from the victim model from unrelated models. Following this, Section 5.2 assesses REEF's robustness to subsequent developments of the victim model, such as fine-tuning, pruning, merging, and permutations. Section 5.3 presents ablation studies on REEF across varying sample numbers and datasets. Finally, Section 5.4 discusses REEF's sensitivity to training data and its capacity for adversarial evasion."}, {"title": "EFFECTIVENESS VERIFICATION", "content": "In this subsection, we demonstrate that REEF can effectively model the fingerprint from the repre- sentation. The CKA similarity between the victim model's representations and those of its derived models, as well as unrelated models, shows significant differences. This makes REEF a reliable fingerprinting method for protecting the victim model's IP.\nSettings. For the LLMs, we select Llama-2-7b as the victim model and choose a range of suspect models, including quantization and fine-tuned variants of Llama-2-7b (e.g., Llama-2-7b-chat, Code- llama-7b, and Llama-2-7b-4bit) as well as unrelated models (e.g., Qwen-1.5-7b, Baichuan-2-7b, and Mistral-7b). We use both a linear kernel and an RBF kernel to compute the layer-wise and inter-layer CKA similarity of representations between the victim and suspect models on 200 samples from the TruthfulQA dataset (Lin et al., 2022).\nREEF can accurately distinguish between models derived from the victim model and unrelated models. As shown in Figure 3, for LLMs derived from the victim model, the CKA similarity with the victim model is high, whereas unrelated LLMs show low similarity.\nCKA from a single layer is sufficient for fingerprint identification. The similarities between representations on a specific layer of the victim model and those of the derived and unrelated models"}, {"title": "ROBUSTNESS VERIFICATION", "content": "In this subsection, we apply REEF to suspect models that are developed from a victim model through fine-tuning, pruning, merging, permutations, and scaling transformations. These techniques can in- troduce significant changes to the model's structure or parameters, making it challenging for existing methods to identify the victim model. However, REEF remains effective in these scenarios, demon- strating its robustness."}, {"title": "BASELINE METHODS", "content": "Weight-based Fingerprinting Methods. Following Zeng et al. (2023), we use model weight simi- larity methods, including PCS and ICS, to identify whether a suspect model is derived from a victim model.\nRepresentation-based Fingerprinting Methods. Yang & Wu (2024), referring to the Logits method, implements LLM fingerprinting by analyzing unique attributes of each LLM's logits output."}, {"title": "FINE-TUNING", "content": "Xu et al. (2024) point out that weight-based fingerprints are not reliable when models undergo extensive fine-tuning with larger deviations in parameters.\nSettings. We use Llama-2-7b as the victim model and select a diverse set of its fine-tuned models as suspect models, with fine-tuning (FT) data volumes ranging from 5 million to 700 billion tokens."}, {"title": "MODEL PRUNING", "content": "Pruning is widely used in model compression for edge deployment, e.g., serving for mobile devices and autonomous driving (Vadera & Ameen, 2021; Wang et al., 2024; Lin et al., 2024)."}, {"title": "MODEL MERGING", "content": "Model merging is an effective technique that merges multiple separate models with different capa- bilities to build a universal model without needing access to the original training data or expensive computation (Yang et al., 2024). Differing from other sections, the merged model is derived from several victim models, which pose a challenge in identifying all of them. In this subsection, we study two types of model merging: weight-based and distribution-based."}, {"title": "PERMUTATION AND SCALING TRANSFORMATION", "content": "There are approaches that could camouflage the model without changing its architecture or affecting its output (Zeng et al., 2023). Malicious developers may modify the model by employing dimension permutation or coupled matrix multiplications to evade some fingerprint detection methods (Fer- nandez et al., 2024).\nSettings. We select Llama-2-7b, Mistral-7b, and Qwen-1.5-7b as victim models, applying column- wise permutations or scaling transformations (with a scaling factor of 0.8) to both their weight matrices and feature representations."}, {"title": "ABLATION STUDY", "content": "Number of Samples To evaluate the impact of sample number on the performance of REEF, we conduct an ablation study using samples from TruthfulQA, ranging from 10 to 1000 in intervals of 10.\nDifferent Datasets To assess the effectiveness of REEF across various data types, we also con- duct experiments using SST2 (Socher et al., 2013), ConfAIde (Mireshghallah et al., 2023), PKU- SafeRLHF (Ji et al., 2024), and ToxiGen (Hartvigsen et al., 2022)."}, {"title": "FURTHER DISCUSSION", "content": "REEF can distinguish between models with the same architecture but different pre-training data. Openllama-7b (Geng & Liu, 2023) and Amber (Liu et al., 2023) are open-source LLMs that utilize the same Llama architecture but are trained from scratch on distinct pre-training datasets.\nMalicious developers fail to fine-tune models with a customized loss function to evade detec- tion by the REEF. We assume these developers are aware of the REEF approach and attempt to design customized loss functions during fine-tuning to bypass detection."}, {"title": "CONCLUSION", "content": "This paper proposes REEF, a robust representation-based fingerprinting method for LLMs, which effectively identifies models derived from victim models. REEF does not impair LLMS's general capability and remains resilient against various subsequent developments, including pruning, fine- tuning, merging, and permutations. Therefore, REEF is highly suitable for protecting model IPs for both third parties and model owners, as a reliable solution for safeguarding models from unautho- rized use or reproduction."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure the reproducibility of this study, we have uploaded the source code as part of the supple- mentary material. Furthermore, the code and datasets will be made available on GitHub after the completion of the double-blind review process, enabling others to replicate our study."}, {"title": "EVADING REEF WITH FINE-TUNING", "content": "We hypothesize that malicious developers aware of the REEF approach might attempt to design customized loss functions during fine-tuning to evade detection. Given that REEF determines model similarity based on the representation similarity between the suspect and victim models, malicious developers aiming to avoid detection would likely design their customized loss to maximize the representational divergence between these models.\nBased on this premise, we designed two experiments to attempt to circumvent REEF detection:\n\u2022 Integrating the task loss with a customized loss during the fine-tuning process, aiming to achieve the fine-tuning objective while maximizing the representational dissimilarity with the victim model.\n\u2022 Fine-tuning the victim model solely using the customized loss, attempting to maximize the repre- sentational dissimilarity between the original and fine-tuned models.\nTo evaluate these scenarios, we conduct experiments using the OPT-1.3B model (Zhang et al., 2022) and the E2E NLG Challenge dataset (Novikova et al., 2017) for fine-tuning. We employ the LORA technique (Hu et al., 2021) for efficient adaptation. The customized loss is designed to measure the CKA similarity between the logits of the original and fine-tuned models.\nFor the first scenario, we formulate a combined loss function: $\\mathcal{L} = \\mathcal{L}_{task} + \\lambda \\mathcal{L}_{custom}$, where $\\mathcal{L}_{task}$ is the task-specific loss (e.g., cross-entropy for the E2E NLG Challenge), $\\mathcal{L}_{custom}$ is the CKA sim- ilarity between the logits of the original and fine-tuned models, and $\\lambda$ is the weighting coefficient. Specifically, the customized loss is calculated using Equation 1, that is:\n$CKA(LG_{ori}, LG_{ft}) = \\frac{HSIC(LG_{ori}, LG_{ft})}{\\sqrt{HSIC(LG_{ori}, LG_{ori}) \\cdot HSIC(LG_{ft}, LG_{ft})}},$\nwhere $LG_{ori}$ and $LG_{ft}$ represent the logits of the original and fine-tuned models on the same sample.\nIn this scenario, incorporating different weighting coefficients ($\\lambda$ ranges from 0.5 to 3.0) for the cus- tomized loss during the combined fine-tuning process failed to reduce the representational similarity between the fine-tuned model and the original model. This suggests that during fine-tuning, the model continues to rely on the representation modeling capabilities of the original language model. Consequently, achieving ECE task objectives necessarily preserves the representational distribution.\nIn the second scenario, although targeted fine-tuning can increase the distributional divergence in the representation space between the suspect and victim models, the suspect model loses its fun- damental language expression capabilities, rendering its outputs meaningless. For example, the fine-tuned model may only respond with repetitive patterns such as \u201cand and and and ...\" for any input, demonstrating a complete loss of linguistic coherence and utility.\nTherefore, our method demonstrates resilience against malicious actors' attempts to evade detection through fine-tuning strategies. These findings underscore the robustness of REEF in identifying the victim model, even in the face of sophisticated evasion techniques.\""}, {"title": "LIMITATIONS", "content": "There are several limitations to this work. Firstly, our study focuses on open-source LLMs, which allows model owners and third parties (e.g., regulatory authorities) to verify and protect model own- ership. However, for closed-source models, the lack of access to their representations limits the applicability of our approach."}]}