{"title": "Explainable, Multi-modal Wound Infection Classification from Images Augmented with Generated Captions", "authors": ["PALAWAT BUSARANUVONG", "EMMANUEL AGU", "REZA SAADATI FARD", "DEEPAK KUMAR", "SHEFALIKA GAUTAM", "BENGISU TULU", "DIANE STRONG"], "abstract": "Infections in Diabetic Foot Ulcers (DFUs) can cause severe complications, including tissue death and limb amputation, highlighting the need for accurate, timely diagnosis. Previous machine learning methods have focused on identifying infections by analyzing wound images alone, without utilizing additional metadata such as medical notes. In this study, we aim to improve infection detection by introducing Synthetic Caption Augmented Retrieval for Wound Infection Detection (SCARWID), a novel deep learning framework that leverages synthetic textual descriptions to augment DFU images. SCARWID consists of two components: (1) Wound-BLIP, a Vision-Language Model (VLM) fine-tuned on GPT-40-generated descriptions to synthesize consistent captions from images; and (2) an Image-Text Fusion module that uses cross-attention to extract cross-modal embeddings from an image and its corresponding Wound-BLIP caption. Infection status is determined by retrieving the top-k similar items from a labeled support set. To enhance the diversity of training data, we utilized a latent diffusion model to generate additional wound images. As a result, SCARWID outperformed state-of-the-art models, achieving average sensitivity, specificity, and accuracy of 0.85, 0.78, and 0.81, respectively, for wound infection classification. Displaying the generated captions alongside the wound images and infection detection results enhances interpretability and trust, enabling nurses to align SCARWID outputs with their medical knowledge. This is particularly valuable when wound notes are unavailable or when assisting novice nurses who may find it difficult to identify visual attributes of wound infection.", "sections": [{"title": "1 Introduction", "content": "Chronic wounds present a considerable health challenge in the United States, impacting over 6.5 million individuals (2% of the population) [17]. Affecting predominantly older adults [13, 31], these wounds severely impact the patients' quality of life and impose a significant financial burden, with annual medicare expenditures ranging from $28.1 to $96.8 billion [39]. Complications often arise due to infections, which can require emergency interventions and potentially lead to limb amputation if not addressed promptly [14]. This paper addresses infection classification in Diabetic Foot Ulcers (DFUs), which are especially dangerous for individuals with diabetes. More than half of all DFUs become infected, leading to amputations in 20% of cases at a cost of $33,499 per amputation [29, 32].\nIn current medical practice, diagnosing an infected wound involves several steps: debridement (removal of dead tissues), blood tests, and expert evaluation, which are typically conducted in a clinical setting [24, 28, 43]. This protocol presents challenges at the Point of Care (POC), such as in patients' homes or at trauma sites, where before debridement, non-specialist caregivers may suspect an infection but do not have access to specialty services to follow the protocol. Often, these caregivers must advise patients to seek further evaluation at a clinic or emergency facility to confirm the presence of an infection. This referral process not only delays treatment but also increases the risk of severe outcomes, including amputations [35]. Furthermore, many wounds that are referred for expert assessment are subsequently found to be uninfected, leading to unnecessary use of resources such as transportation and additional costs such as emergency department charges [8, 49]. In settings where clinicians do not have access to detailed clinical data, they are forced to rely on visual inspections to spot early signs of infection in Diabetic Foot Ulcers (DFUs), such as increased redness, swelling, warmth, and the presence of colored purulent discharge. However, these inspections are not always accurate and are challenging for nurses or caregivers with insufficient wound experience.\nOur approach: To address these issues, we propose a comprehensive deep learning model (see Fig. 1) that improves the accuracy of wound infection prediction over SOTA models with enhanced interpretability. This paper introduces an integrated framework that combines a Vision-Language pre-trained model with a multimodal classification model, termed Synthetic Caption Augmented Retrieval for Wound Infection Detection or SCARWID, for the classification of infections in DFU photographs. Our approach involves generating visual highlights and annotations of the wound image along with textual descriptions of wound characteristics from an input image to help novice nurses understand the wound's attributes indicative of infection. By reflecting on both the wound image and the corresponding textual description of infection attributes, the SCARWID model's rationale for infection classification can more easily be understood, potentially improving a nurse's wound expertise in the longer term.\nDue to the absence of medical notes corresponding to the DFU images in our dataset, we employed GPT-40 [2], a Multimodal Large Language Model (MLLM) capable of processing both text and visual data making it highly effective in tasks such as image captioning, to generate concise descriptions of wound images. These captions, highlighting potential signs of infection, served as metadata for training the SCARWID model. We provided expert-labeled wound statuses (infected or uninfected) to GPT-40 to guide the caption generation process and refine the descriptions reflecting the wound's condition. Next, we fine-tuned a Vision-Language Pre-training model known as Bootstrapping Language-Image Pre-training (BLIP) [22] on the image captioning task with GPT-40 generated descriptions. This resulted in a captioning model called Wound-BLIP that provides consistent descriptions without needing label information at test time. This method not only enriched our dataset but also deepened our understanding of the rationale behind expert labeling decisions for wound images.\nDuring inference, our proposed SCARWID model classifies infections by processing a DFU image query along with its corresponding wound description generated by Wound-BLIP. The model retrieves the top k most similar image-text pairs from a database of labeled support documents, where similarity is determined based on the closest distance in the embedding space. By default, k=5. The final prediction is made by selecting the most frequently occurring label among these k retrieved items.\nOur main contributions are as follows:\n\u2022 We propose SCARWID, an integrated end-to-end framework, which combines wound images with their descriptions to transform them into multimodal embeddings. Classifications are made based on the most common labels among the top k most similar pairs of images and texts retrieved from the support database.\n\u2022 We fine-tuned the BLIP image-captioning model using 1,000 pairs of DFU images and their corresponding text generated by GPT-40, facilitating the generation of textual meta-data essential for infection classification.\n\u2022 SCARWID was evaluated on 5-fold cross-validation protocol and demonstrated significant improvements of 4-9% in sensitivity and specificity over SOTA wound image classification models such as CNN-Ensemble and DFU-RGB-TEX-Net. Furthermore, it demonstrated high robustness and generalization evidenced by the lower standard deviations of evaluation scores.\n\u2022 To enhance interpretability, we present examples of SCARWID's predictions with visual highlights, annotations, and corresponding textual wound descriptions. Specifically, for sample wounds, we concurrently display: (1) wound regions highlighted by Grad-CAM on the Wound-BLIP image-ground text encoder, showing where descriptions of specific wound characteristics are most evident; and (2) image attributes that our image-text fusion module focuses on when retrieving similar images from the support database, visualized using attention heatmaps."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Wound Infection Classification with Deep Learning", "content": "State-of-the-art (SOTA) Deep learning models that detect infections from wound images have become increasingly prevalent [4, 12, 14, 53]. Goyal et al. [14] introduced the Part-B DFU dataset, which includes wound images for a infection classification task from diabetic foot ulcers. As detailed in Table 1, Goyal et al. [14] employed a CNN ensemble model that combines bottleneck features from CNN architectures and classifies using an SVM classifier, achieving 70.9% sensitivity and 74.4% specificity in binary infection classification. In subsequent research, Al-Garaawi et al. [4] developed a custom CNN framework, DFU-RGB-TEX-Net, which enhances feature extraction from DFU images using mapped binary patterns. DFU-RGB-TEX-Net integrates a linear combination of the original image and texture information as input for a CNN, resulting in a sensitivity of 75.1% and a specificity of 73.4%.\nBusaranuvong et al. [7] proposed the ConDiff model for the classification of wound infections. ConDiff uses distance-based classification to predict the wound status based on the similarity between an input image and image-guided conditional synthetic images generated from infection and non-infection labels. ConDiff outperformed other SOTA models achieving 85.4% sensitivity and 74.7% specificity on the Part-B DFU infection dataset, demonstrating the potential of distance-based classification of wound imaging tasks. However, the downside of the ConDiff approach is its high computational cost during inference (4-5 seconds per image on an NVIDIA A100 GPU) due to the image-generating time with the diffusion model. This work also showed that more recent Vision Transformer (ViT)-based models such as SwinV2 [27] (82.7% sensitivity and 69.8% specificity) and EfficientFormer [23] (84.1% sensitivity and 69.2% specificity) outperformed CNN-based models in wound infection classification.\nGaldran et al. [12] and Qayyum et al. [33] explored SOTA ViT-based models for multiclass classification of ischemia and infection using the DFUC2021 challenge dataset provided by Yap et al. [53]. Their findings demonstrated that the performance of ViT-based was comparable to that of traditional CNN-based models on this task. Specifically, a ViT ensemble model [33] achieved a sensitivity of 61% and a positive predictive value (PPV) of 58%, while the Big Transfer (BiT) model [12] achieved a sensitivity of 66% and a PPV of 61%."}, {"title": "2.2 Medical Visual Question Answering with Multimodal Large Language Models", "content": "LLMs have been explored for their proficiency in medical tasks. Models such as Med-PaLM [40], Med-PaLM2 [41], and GPT-4 [30] achieve impressive accuracies of 67.6%, 86.5%, and 90.1%, respectively on multiple-choice US Medical Licensing Examination (USMLE) questions, well above the exam's approximate passing score of 60% [19].\nDespite these advancements, challenges persist for the Medical Visual Question Answering (medical VQA) task. For example, while Med-PaLM2 excels in text-based analysis, it lacks visual data interpretation capabilities. In contrast, GPT-40, a Multimodal Large Language Model (MLLM), effectively integrates visual and textual information. Jin et. al [18] shows that GPT-40 achieves an accuracy of 88% in the New England Journal of Medicine (NEJM) Image Challenge when medical images and clinical information are provided, outperforming the average physician's accuracy of 77%. This finding is in line with another experiment [52], which illustrates that incorporating expert hints into the USMLE with image questions taken from the AMBOSS medical platform increases the accuracy of GPT-40 from 60-68% to 84-88%, highlighting its potential for improved medical diagnostic support.\nHowever, GPT-40's performance drops significantly in the NEJM image challenge scenarios where only medical images are used as inputs, with diagnostic accuracy ranging from 29-40%, and accuracy around 42-50% when only providing essential information about the patient, their symptoms, and relevant clinical details [6, 51]. This highlights a critical gap in its ability to process purely visual information without supporting context from text or other modalities.\nIn our research, we focus on infection classifications from wound images since prioritizing infection detection is crucial for addressing urgent clinical requirements and enabling timely and appropriate treatment interventions, such as the initiation of antibiotic therapy or surgical procedures. Our paper addresses scenarios in which additional patient clinical information, medical notes, or descriptions corresponding to each DFU image are unavailable. As mentioned above, using GPT-40 to analyze only wound images for infection classification is not recommended. As an alternate strategy, we address GPT-40's limitations in image-only analysis by incorporating expert labels of DFU images to generate wound descriptions. Later, these descriptions are used for fine-tuning the BLIP image captioning model that generates wound image descriptions without using unavailable expert-assigned labels at test time."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Wound-BLIP Image Captioning Model", "content": "Vision-Language Models (VLMs) are designed to understand and generate information from both visual and textual data. They can analyze images and relate them to corresponding text, enabling outputs such as captions, answers to questions about visual content, or textual summaries of scenes. Examples of VLMs include BLIP [22], BLIP-2 [21], Flamingo [5], and LLaVA [25].\nWe selected BLIP [22] as the image captioning model to generate textual descriptions of wounds from images because it is smaller in size compared to other VLM approaches. Unlike models that use a large language model (LLM) backbone as a text decoder, BLIP uses an image-grounded text decoder that can be easily fine-tuned. Since our downstream task is to describe characteristics of wounds from images, we refer to our fine-tuned BLIP model as Wound-BLIP.\nThe Wound-BLIP architecture for wound image captioning consists of three main components: (1) an Image Encoder, (2) an Image-grounded Text Decoder, and (3) an Image-grounded Text Encoder (see Fig.2). The Image Encoder processes the input image into a sequence of embeddings that capture the contextual relationships within the image, utilizing a Vision Transformer (ViT) architecture[10]."}, {"title": "3.1.1 Image Captioning", "content": "For the purpose of image captioning, the image embeddings are passed to the cross-attention layers of the Image-grounded Text Decoder $D_{4}$, implemented as a Transformer Decoder [47]. This allows the model to generate contextually relevant descriptions based on visual input.\nGiven a collection of pairs of wound images and GPT-40-generated text descriptions $D_{GPT4} = \\{(I_n, T_n)\\}_{n=1}^{N}$, the BLIP model was fine-tuned by freezing the pre-trained Image Encoder and updating only the parameters $ \\phi$ of the Text Decoder. The objective is to predict the probability distribution of the next word in the sequence, given the input image and the previous words. The loss function associated with this task is the Language Modeling (LM) loss, which minimizes the negative log-likelihood of the text in an autoregressive manner. The LM loss function is expressed in Equation 1.\n$L_{LLM} = - \\sum_{l=1}^{L} \\log p(w_l, | , w_{<l}, I_i, \\phi) $   (1)\nHere, $p(w_l, | , w_{<l}, I_i, \\phi)$ represents the probability of the BLIP model outputting the correct l-th token $w_l$, given all previous tokens $w_{<l}$ in the textual sequence $T$ and the input image $I$. $L$ denotes the number of tokens in the text."}, {"title": "3.1.2 Interpreting Captions with Image-Text Matching", "content": "To interpret the generated captions on images, we use Image-Text Matching (ITM) and visualization techniques. The image embeddings and the generated descriptions are passed to the Image-grounded Text Encoder. We then apply Gradient-weighted Class Activation Mapping (Grad-CAM) [38] to the cross-attention layers of the Image-grounded Text Encoder to visualize the areas of the image that correspond to the textual descriptions.\nSince the Image-grounded Text Encoder shares a similar architecture with the Image-grounded Text Decoder, we reused the fine-tuned cross-attention and feed-forward layers from the decoder in the encoder. However, it was still necessary to train the ITM head, which captures the fine-grained alignment between text and image. We employed the Binary Cross-Entropy (BCE) loss to predict whether the pairs of wound images and generated wound descriptions are matched."}, {"title": "3.2 SCARWID Model", "content": "By using the captions generated from our Wound-BLIP model as metadata, we integrated them with the corresponding wound images to predict infections in DFU images. This integration is performed by the Image-Text Fusion module $F_e$. Infection classification is then determined by retrieving the top-K most similar instances from the support data collection $D_{support}$ based on the fused image-text embeddings, as depicted in Fig. 3."}, {"title": "3.2.1 Image-Text Fusion Module", "content": "This module consists of three components:\nImage Encoder: The DeiT (Data-efficient Image Transformers) model [45] is utilized to process the input image $I$ and outputs an image embedding vector $E_I \\in \\mathbb{R}^{M \\times d_I}$. Where $M$ is the number of patches in the image and $d_I$ is the embedding dimension.\nText Encoder: The corresponding textual input $T$ is processed by the CLIP-Text model [34], which outputs a text embedding vector $E_T \\in \\mathbb{R}^{L \\times d_I}$. Here, $L$ represents the number of tokens in the text, and $d_I$ is the embedding dimension.\nCross-Attention Layer: To effectively fuse the information from both the image and text embeddings, a cross-attention mechanism is employed. This mechanism uses the image embedding as a query Q, with key K and value V derived from the text embedding. This structure allows the model to focus specifically on parts of the image relevant to the text description. The cross-attention layer's operation is expressed by Equation 2."}, {"title": "Attention(Q, K, V)", "content": "Attention(Q, K, V) = \\text{softmax} \\Big( \\frac{QK^T}{\\sqrt{V_a}} \\Big) V\\text{a}   (2)\nHere, $Q = W^Q E_I, K = W^K E_T$, and $V = W^V E_T$, where $W^Q, W^K$, and $W^V$ are trainable parameters. The factor $V_a$ serves as a scaling term to stabilize the gradients during training."}, {"title": "3.2.2 Similarity-based Classification", "content": "The final step in our classification process involves utilizing the cross-modal embedding $E^* = \\text{Attention}(E_I, E_T, E^*_T)$ for classification. Rather than employing a traditional probability-based approach, our method treats an input image as a query image $I_q$ and its corresponding generated description from Wound-BLIP as query text $T_q$ that are then both fed into the Image-Text Fusion module, producing the query embedding $E^*_q = F_e(I_q, T_q)$.\nNext, we search for the top k similar pairs from a labeled support document $D_{support} = \\{(I_j, T_j, Y_j)\\}_{j=1}^{N_s}$. Here, $I_j$, $T_j$, and $Y_j$ represent the support images, corresponding Wound-BLIP generated texts, and their respective labels, with $N_s$ denoting the total number of items in $D_{support}$. Each support item's cross-modal embedding is computed as $E^*_j = F_e(I_j, T_j), \\forall j \\in \\{1, ..., N_s \\}$.\nThe predicted label $\\hat{Y}_q$ for the query image $I_q$ is determined by identifying the most common label among the top-k objects, based on the minimum Euclidean distance in embedding space, calculated as $d\\text{is}(E^*_q, E^*_j) = \\sqrt{(E^*_q - E^*_j)^2}$. The set of indices for the top-k similar objects is denoted by $J_{top-k}$, and formally, the label determination process can be described as follows:"}, {"title": "$J_{top-k} = \\text{argsort}( \\{ d\\text{is}(E^*_q, E^*_j) : j = 1, ..., N_s\\} ) [:k]$   (3)", "content": ""}, {"title": "$\\hat{Y}_q = \\text{mode}(Y_{J_{top-k}})$   (4)", "content": ""}, {"title": "3.2.3 Learning Similarity using a Triplet Loss Function", "content": "To learn the similarity between objects in the cross-modal embedding space, we leveraged the triplet loss function [37] for optimizing parameters $ \\theta$ of our Image-Text Fusion module $F_e$. This works by minimizing the distance between an anchor object $x^{(a)}$ and a positive object $x^{(P)}$ with the same identity while maximizing the distance between the anchor object and a negative object $x^{(n)}$ with a different identity. Here $x^{(*)}$ is denoted as a pair of (wound image $I_j$, text description $T_j$ ).\n$L_{triplet} = \\mathbb{E} \\big[ ||F_e(x^{(a)}) - F_e(x^{(P)})||_2 - ||F_e(x^{(a)}) - F_e(x^{(n)})||_2 + \\alpha \\big]_+$  (5)\nThe margin $\\alpha$ is set to 1, indicating the desired separation between similar and dissimilar pairs."}, {"title": "3.3 Dataset Preparation and Processing", "content": ""}, {"title": "3.3.1 DFU Infection Dataset", "content": "The DFU Infection Dataset is derived from the Part-B DFU Dataset [14], which encompasses two categories of DFU diseases: ischemia and infection. This data set was compiled from patient wound images obtained at the Lancashire Teaching Hospital with permission for research granted by the UK National Health Service (NHS). The images were labeled by two healthcare professionals, consultant physicians specializing in diabetic foot conditions, based solely on visual assessments without referencing medical notes or clinical tests. This project focuses on infection classification based on the visual appearance of an image.\nThe available DFU infection dataset used in this project contains regions of interest for infection classification, which consists of 2,946 natural augmented patches with infection and 2,946 natural augmented patches of non-infection where the natural data augmentation is capturing multiple magnifications of the same wound image. Each DFU patch measures 224 \u00d7 224 \u00d7 3 pixels."}, {"title": "3.3.2 Metadata Generation with GPT-40", "content": "To address the significant challenges of predicting infection only by the appearance of the wound in an image described in Sec. 1, we utilize GPT-40 (i.e., gpt-40-2024-08-06 version) with our label-guided prompting technique to generate textual descriptions corresponding to each wound image. This technique involves initially informing the model of the ground-truth infection label assigned by wound specialists. Subsequently, GPT-40 is prompted to identify and describe characteristics that potentially influenced the specialists' diagnostic decisions. This process is illustrated in Fig. 4."}, {"title": "3.3.3 Synthetic Image Augmentation", "content": "Diffusion models [15, 42], a novel class of generative models, utilize diffusion processes to generate high-quality images by progressively reducing noise in multiple iterations. Recent studies have utilized diffusion models for image augmentation [3, 46, 55], significantly enhancing the accuracy of baseline deep learning models in image classification tasks, including medical imaging analysis.\nIn this study, the label-conditional latent diffusion model used in ConDiff [7] was used to generate wound images that were conditioned on infection status (each of 1200 images). These 2400 generated images were added to the training data as augmented images. Classifier-free guidance with the DDIM sampling process [16] was used to synthesize images of size 256 \u00d7 256. Examples of conditional synthesized images are shown in Fig. 5. The guidance scale and the sampling steps were set to 1.5 and 30 respectively. To prevent data leakage when evaluating classification models on the testing partitions, this diffusion model was only employed on DFU images in the training partition."}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Experimental Setup", "content": ""}, {"title": "4.1.1 Wound-BLIP model's Fine-tuning configuration", "content": "To fine-tune the Wound-BLIP model, the pretrained parameters of blip-image-captioning-base \u00b9 were utilized. The model was optimized on pairs of metadata $D_{GPT4}$, treating images as inputs and texts as outputs. The objective was to minimize the LM loss function in Equation 1. Training was done for 20 epochs using the AdamW optimizer with a learning rate of 1 \u00d7 10~5."}, {"title": "4.1.2 SCARWID model's training configuration", "content": "The SCARWID model's configuration consists of the following three modules:\n\u2022 Image Encoder parameters were initialized using the deit-base-distilled-patch16-224 model\u00b2.\n\u2022 Text Encoder parameters were initialized with the CLIP-Text encoder from the clip-vit-large-patch14 model\u00b3.\n\u2022 Cross-Attention Layer hyperparameters were set to 2 attention heads, an embedding dimension of 768 (matching the output sizes of both Image and Text Encoders), and a projection dimension (i.e., cross-modal embedding) of 256.\nAs previously mentioned in Sec. 3.3, 5-fold cross-validation was employed for training and testing deep learning models. For our SCARWID model, image descriptions generated by Wound-BLIP were paired with their corresponding wound images as input. The model was trained for 30 epochs using the AdamW optimizer, with a learning rate of 1 \u00d7 10\u22124, with the goal of minimizing the triplet loss function defined in Equation 5.\nDuring inference, labeled support data $D_{support}$ were randomly sampled from training images, ensuring that at most one image from each subject was selected. The total number of samples in $D_{support}$ was set to 1024. These data were stored as embedding vectors $E^*_j$. When predicting a new input query image $I_q$, SCARWID computes a cross-modal embedding vector $E^*_q$ from $I_q$ and its associated caption $T_q$. To determine the label of a given query image, the labels of the top-5 similar objects from $D_{support}$ were retrieved.\nExperiments were done in Python 3.9 using the following software libraries: PyTorch 1.13.1, torchvision 0.14.1, transformers 4.42.4, and salesforce-lavis 1.0.2. An NVIDIA A100 GPU was used to train the models."}, {"title": "4.2 Evaluation Metrics", "content": "To evaluate our proposed framework for DFU infection classification task, the following metrics are considered.\n\u2022 Accuracy $ACC = \\frac{TP+TN}{P+N}$, where TP is the number of true positive predictions, TN is the number of true negative predictions, P is the positive label (infected), and N is the negative label (not infected).\n\u2022 Sensitivity (SEN) or recall reflects the proportion of actual positives that are correctly identified: $SEN = \\frac{TP}{TP+FN}$, where FN denotes the number of false negative predictions.\n\u2022 Specificity (SPC) reflects the proportion of actual negatives that are correctly identified: $SPC = \\frac{TN}{TN+FP}$, where FP denotes the number of false positive predictions.\n\u2022 Positive Predictive Value (PPV) or precision is the proportion of positive predictions that are true positives. $PPV = \\frac{TP}{TP+FP}$\n\u2022 F1-score is the Harmonic Mean of Precision and Recall: $F1 = \\frac{2 \\cdot PPV \\cdot SEN}{PPV + SEN}$"}, {"title": "4.3 SOTA Baseline Models", "content": "Recent deep-learning architectures were selected as baselines for wound infection classification from images. These include custom CNN architectures such as CNN-Ensemble [14] and DFU-RGB-TEX-Net [4]. Additionally, ConDiff [7], a distance-based generative discrimination model, was also selected. EfficientNet [44] was chosen as it was the most effective CNN-based model for the detection of infections from wound images [53]. Transformer-based models such as ViT [10], DeiT [45], SwinV2 [27], and EfficientFormer [23], which have demonstrated superior performance over traditional CNN-based models in wound infection classification [7, 12], were also included."}, {"title": "4.4 Deep Learning for Image Classification with Image Augmentations", "content": "We trained SOTA image classification models using two different data augmentation techniques: (1) Traditional image augmentation operations, which included random crops, vertical and horizontal flips, rotations and adjustments to brightness, contrast and saturation; and (2) Synthetic Augmentation, utilizing images generated by a diffusion model (see Sec. 3.3.3).\nAs shown in Table 3, the inclusion of synthetic images from the diffusion model substantially improves the performance of SOTA deep learning models across most metrics, increasing accuracy by 2.5-4.5% for infection classifications from DFU images. In particular, transformer-based models such as DeiT-Base and SwinV2-Tiny achieved enhanced performance with synthetic augmentation compared to EfficientNet-B0, likely due to the increased variety of images."}, {"title": "4.5 Performance Comparison of SCARWID with SOTA baselines", "content": ""}, {"title": "4.6 SCARWID Explainability", "content": "Building on insights from experiments detailed in Sec. 4.4, which highlighted the effectiveness of synthetic augmentation, we further incorporated diffusion-generated images and their descriptions from Wound-BLIP into the training process of SCARWID.\nAs detailed in Table 4, SCARWID (Image & Text) demonstrates superior performance, achieving an average accuracy of approximately 81.4% and an average F1-score of 82.0%, which significantly outperforms baselines. Furthermore, SCARWID exhibits lower standard deviations in evaluation scores across 5 folds during cross-validation, highlighting its robustness, especially when compared to probability-based models. In clinical scenarios, the highest sensitivity achieved by SCARWID (85.2%) is particularly valuable in the context of wound care management, as it improves the model's ability to detect infections early, enabling caregivers to flag and examine potentially infected wounds more closely, and administer antibiotic treatment or surgical procedures to reduce severe complications such as amputation. Additionally, SCARWID's good specificity score reduces unnecessary referrals, allowing better resource-utilization in clinics.\nFurther insights into the utility of generating corresponding wound descriptions are gained by comparing SCARWID (Image & Text) with the SCARWID (Image Only). As shown in Table 4, without the support of the wound descriptions generated, SCARWID (Image Only) achieves a sensitivity of 83.1%, about 2% lower than that of SCARWID (Image & Text) and a specificity score of 73.6% is 4% lower than that of SCARWID (Image & Text). In addition, we observe that the standard deviations of its evaluation scores are higher than those of SCARWID (Image & Text). This result suggests that the inclusion of the Wound-BLIP generated descriptions helps improve the model robustness and generalization of SCARWID, and mitigates the fine-grained appearance challenge with high inter-class similarity by providing textual context that distinctly characterizes wound attributes, enabling more accurate classification.\nLikewise, classifying wound infections using only the generated text descriptions also underperforms combining them with the wound image. SCARWID (Text Only) even achieved lower sensitivity (78.3%) and specificity (71.6%) scores than SCARWID (Image Only). This result underscores the limitations of the Wound-BLIP model in generating accurate and reliable wound descriptions on its own, which might lead to less precise or even erroneous diagnoses when used without concurrent image analysis."}, {"title": "4.6.1 Visualization of Cross-modal Embedding", "content": "The plot in Fig. 6 shows cross-modal embedding vectors between image-text pairs $(I_S, T_S) \\in D_{support}$. It is observed that infected and uninfected wounds are separated into two distinct clusters."}, {"title": "4.6.2 Attention Map Visualization with Attention Rollout", "content": "Attention Rollout [1] is a method employed to visualize and elucidate which parts of an input image are predominantly focused on by a Vision Transformer-based model during its decision-making process. This technique involves the aggregation of attention weights from all attention heads across all layers of a transformer, thereby illustrating the areas deemed most predictive by the model."}, {"title": "4.6.3 Wound-BLIP Caption Interpretability with Grad-CAM", "content": "Fig. 8 illustrates text localization on wound images, showcasing the ability of Wound-BLIP to generate meaningful captions and localize important wound features via Grad-CAM visualizations. In Fig. 8a, Wound-BLIP generates a caption for an infected wound with the descriptors red and inflamed edges and yellowish exudate. The Grad-CAM visualization focuses precisely on the wound's edges, where redness and inflammation are prominent, aligning well with the clinical signs of infection. Additionally, the visualization highlights the yellow watery area of the wound, consistent with the yellowish exudate description, a common feature of infected wounds."}, {"title": "4.6.4 Exploring an Inter-class Similarity Example", "content": "As mentioned in Sec. 4.5, sometimes, uninfected and infected wounds have very similar visual appearances making it difficult to accurately diagnose wound statuses just from images. Fig. 10 shows the case where an uninfected wound was described as showing possible signs of infection by our Wound-BLIP captioning model while the input image shows a visual appearance similar to yellowish discharge. However, text alone is not adequate to make a final decision. Instead, our SCARWID framework tries to find similar pairs of images and texts from the support data collection. Consequently, since the retrieved images were all labeled as uninfected, the input wound is then classified as uninfected even though their corresponding wound captions also present features that possibly appear in infected wounds."}, {"title": "4.6.5 Exploring Misclassifications", "content": "The uninfected DFUs in Fig. 11 (1-3) that were misclassified as infected wounds exhibit characteristics typically associated with infections, such as significant reddening or darkening of the tissue. For example, wound 1 shows a yellowish exudate and surrounding erythema, which frequently appear in infected wounds. Wound 2 has redness and the presence of potential pus, leading the model to predict infection incorrectly. Similarly, wound 3 displays an area that appears necrotic, a characteristic also found in infected wounds.\nThe infected"}]}