{"title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention and Low-Rank Adaptation in Large Language Models", "authors": ["Yuxuan Zhang"], "abstract": "With the rapid development of large language models (LLMs), fully fine-tuning(FT) these models has become less feasible due to high computational demands. Moreover, FT also increases the risk of catastrophic forgetting. As an alternative, Low-Rank Adaptation (LoRA) has been proposed. By fine-tuning only a small subset of parameters, LoRA achieves performance similar to FT while significantly reducing resource requirements. However, since LoRA inherits FT's design, the issue of catastrophic forgetting still remains. To address these limitations, we propose SE-CURA: Sigmoid-Enhanced CUR Decomposition LoRA, a novel PEFT variant designed to mitigate catastrophic forgetting while improving fine-tuning performance. Our method introduces a new normalization technique, Sig-Norm, to enhance parameter retention and improve overall performance. SECURA has been evaluated on a diverse range of tasks, including mathematical problem-solving (GSM8K), challenging question-answering (CNNDM), translation (NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results demonstrate it achieves an average fine-tuning improvement of 3.59% across 4 MCQ sphere tasks and a 2.51% improvement in 5 QA sphere tasks across Gemma2 2b, Qwen2 1.5b, Qwen 2 7b, Llama3 8b, Llama3.1 8b compared to DoRA. Additionally, SECURA demonstrates superior knowledge retention capabilities, achieves state-of-the-art in 16 continual learning tests keep more than 70% accuracy on LLMs basic knowledge compared with Experience Replay(ER), sequential learning(SEQ), EWC, I-LORA and CUR-LORA.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of transformer-based generative models, parameter scales have grown exponentially to enhance multi-task capabilities. Notable examples in natural language models(NLP) like LLaMA 3.1 (Dubey et al., 2024) and Qwen 2.5 (Team, 2024) achieve remarkable performance on complex tasks including text generation, logical reasoning, and multilingual processing. However, their massive parameterization (e.g., 70B+ parameters) creates deployment barriers: Full fine-tuning requires over 1.5TB of GPU memory and weeks of computation. Even with distilled versions, they remain prohibitively expensive for most individual developers and small research labs.\nParameter-Efficient Fine-Tuning (PEFT) (Xu et al., 2023) methods, such as Low-Rank Adaptation (LORA) (Whitehouse et al., 2024), address this challenge. LoRA inject trainable tiny low-rank matrices (typically <0.1% of original parameters) and frozen basic weights to reduce the computational cost of fine-tuning. Performed well in multi-task learning, multilingual summarisation, and transfer learning (Whitehouse et al., 2024; Zhao et al., 2024).\nHowever, LoRA remains an approximation of FT, meaning it hardly match or exceed the performance of full fine-tuning. Furthermore, it also inherits FT's critical issue: catastrophic forgetting (McCloskey and Cohen, 1989). While freezing the base model weights alleviates this problem to some extent, the addition of LoRA matrices reintroduces the issue. Additionally, fine-tuning fewer parameters, while reducing computational cost, increases the risk of catastrophic forgetting before the model finds an optimal solution. Although Weight normalization techniques (Salimans and Kingma, 2016) can be used to mitigate these issues but have not provided a comprehensive solution. Existing LoRA variants primarily focus on improving specific aspects: enhancing performance (e.g. DoRA (Liu et al., 2024), AdaLoRA (Zhang et al., 2023)), accelerating convergence (e.g., PiSSA (Meng et al., 2024)), or only"}, {"title": "2 Background", "content": "Low-Rank Adaption. LoRA (Whitehouse et al., 2024) is a method that uses low-rank matrices, hypothesizing that in the parameters matrix, only few parameters controls main knowledge. By introducing low-rank parameter matrices A and B, \u201cit can generate a matrix that provides the same output, where A initialized by random sampling, B initialized by zeros.\nFormally, consider a weight matrix \\(W \\in \\mathbb{R}^{h\\times d}\\) within the original LLMs. LoRA introduces two low-rank matrices, \\(A \\in \\mathbb{R}^{h\\times r}\\) and \\(B \\in \\mathbb{R}^{r\\times d}\\) where \\(r < min(h, d)\\). Instead of directly updating the weight matrix, LoRA modifies the model's forward pass according to the following equation:\n\\(W' = 0 + \\Delta W = W + AB\\)\nBy freezing the basic weight and adding the AB matrix the model can be fine-tuned similarly like FT.\nCUR-Decomposition and Its Application in CUR-LORA. CUR-Decomposition (Hamm and Huang, 2019) is a matrix factorization method that selects specific rows and columns from a matrix. It is used to decompose a matrix in various ways depending on the design. When combined with gradient descent, it can be applied easily using the following formula:\n\\(W = CUR(x)\\),\nwhere \\(W \\in \\mathbb{R}^{h\\times d}\\) is the original parameter weight, \\(C \\in \\mathbb{R}^{h \\times r}\\) and \\(R \\in \\mathbb{R}^{r \\times d}\\) are the selected columns and rows from W, and \\(U \\in \\mathbb{R}^{r \\times r}\\) is a learnable matrix. Here, \\(r < min(h, d)\\).\nCUR-LORA (Fawi, 2024) builds on CUR-Decomposition, by selecting the least important rows and columns as C and R. These selected rows and columns are much fewer than the original matrix and have the lowest norms, which leads to slower updates. This decomposition allows CUR-LORA to focus on updating the less important parts of the matrix, because less important parameters are more sensitive to slower updates, while important ones are less affected. In this way, the model can preserve the previously learned knowledge while efficiently learning new information, improving both training efficiency and performance.\nDynamic Network Pruning. Dynamic Network Pruning (Lin et al., 2020) is an input-adaptive"}, {"title": "3 Methodology", "content": "Our proposed SECURA method is based on three key components: CABR-LORA inverse decomposition fine-tuning, the SigNorm Normalization algorithm, and two types of SECURA Merge method. Drawing inspiration from dynamic network pruning and CUR-decomposition, we hypothesize that"}, {"title": "3.1 CABR-LoRA Inverse Decomposition Initialization", "content": "CABR-LORA is based on CUR-LoRA, utilizing CUR decomposition to break down the weight matrices. As shown in Figure 1, CABR-LORA is initialized using a inverse low-rank decomposition of the U matrix. This allows the \"less important\" columns and rows to remain smaller while maintaining a higher-dimensional space for fine-tuning SECURA. The formula for initialize of SECURA is:\n\\(W_{CABRWeight} = C \\cdot A_{initial} \\cdot B_{zeros} \\cdot R\\). (1)\nWhere \\(W_{CABRWeight}\\) is the initialized CABR-LORA weight, C and R represent the less important columns and rows, while \\(A_{initial}\\) and \\(B_{zeros}\\) are the trainable matrices with weights \\(W_{A}\\) and \\(W_{B}\\).\nCUR-LORA reduces the number of trainable parameters to locate lest important C and R, but this often leads to the loss of important parameters, causing performance degradation. To address this, we introduce CABR decomposition, which decomposes the U matrix, into two matrices: \\(W_{A}\\) and \\(W_{B}\\), which shapes are \\(r \\times m\\) and \\(m \\times r\\). Respectively here \\(m > r\\). \\(W_{A}\\) is initialized via SVD decomposition, while \\(W_{B}\\) is initialized with zeros. By"}, {"title": "3.2 SigNorm Normalization Algorithm", "content": "While CABR decomposition provides a higher dimension of fine-tuning, only using CABR-LORA alone does not fully address catastrophic forgetting, which is crucial for performance in lifelong learning. To tackle this, we introduce the SigNorm Normalization algorithm, inspired by dynamic network pruning. This algorithm adjusts the weight updates by controlling the change magnitude between the former and new weights, provide a stable parameter update (figure 2).\nThe core idea is to employ a Sigmoid-based dynamic scaling mechanism to regulate weight updates. This method constructs a weight pruning matrix whose values are constrained within the range [1, 2]. The Sigmoid function primarily emphasizes central value adjustments, effectively suppressing extreme variations. We can allow SECURA deal with whole matrix's changing tendency while limiting the extreme parameters' effect. At the same time, restricting the update amplitude of lower-norm columns and rows ensures that important parameters(with higher norms)' magnitude experience minimal change, while less important parameters (with smaller norms) undergo more significant updates. The former weight matrix is denoted as \\(W_{former}\\), and the SECURA's matrices as A and B. The merged weight matrix is computed as:\n\\(W_{SECURAMerged} = CABR + W_{former}\\). (5)\nNext, we calculate the Magnitude Loss Matrix \\(M_{Mag}\\), which captures the relative magnitude change between the new and former weights. Here \\(\\epsilon\\) is a small value to avoid division by too small parameters:\n\\(M_{Mag} = \\frac{W_{SECURAMerged}}{W_{former} + \\epsilon}\\) (6)\nThen, we normalize the \\(M_{Mag}\\) to the range [-0.5, 0.5] to balance the values on both sides of zero:\n\\(M_{norm} = \\frac{M_{Mag}}{max(M_{Mag}) + \\epsilon} - 0.5) \\cdot Scale\\). (7)\nNote that the Scale parameter here resizes the normed value to ensure it falls within the range [0,1] after applying the Sigmoid function, as the Sigmoid function only produces a range of [0.3775, 0.6225] for the [-0.5, 0.5] interval.\nFinally, we apply the Sigmoid function to the normalized matrix to produce the SigNorm values and get the final restring matrix \\(M_{res}\\):\n\\(M_{Res} = 2 \\cdot \\sigma (M_{norm})\\). (8)\nOnce we have weight pruning matrix \\(M_{res}\\), the megred matrix \\(W_{SECURAMerged}\\), will then adjust the update process by dividing the SigNorm pruning-weight matrix \\(M_{res}\\):\n\\(W_{updated} = \\frac{W_{SECURAMerged}}{M_{Res}}\\) (9)"}, {"title": "3.3 Merge with Basic Model:", "content": "Limiting parameter growth alone is not the optimal solution. In our early experiments, we found that using only the methods from Sections 3.1 and 3.2 led to worse performance\u2014not only in basic performance but also in mitigating catastrophic forgetting. This issue was identified through insights from delayed updates (Kosta et al., 2020) and Target Network (Gao et al., 2017). We realized that while basic parameters should not remain static during long-term training, they must be updated to prevent performance from stagnating or even deteriorating.\nIf the basic parameters are kept unchanged, two cycle emerges: initially, unimportant parameters are updated, while important ones are constrained. However, in the second cycle, important parameters become smaller than unimportant ones, and the cycle worsens as important parameters begin to change more rapidly, causing further catastrophic forgetting and worse performance.\nTo address this issue, we propose two merging methods:\nMerge with basic weight(M1 method): This approach requires only one SECURA training cost but does not keep the basic LLM weights unchanged. The update rule is as follows:\n\\(W_{Newformer} = CABR + W_{former}\\). (10)\nMerge with SECURA(M2 method): This method uses two identical SECURA models but freezes the base LLM weights. An additional calculation is performed to obtain the final updated weights The update rule is as follows:\n\\(W_{former} = CA_{former}B_{former}R + W_{basic}\\). (11)\nThe merge process is described by the following formulas:\n\\(A_{Newformer} = A_{train}\\). (12)\n\\(B_{Newformer} = B_{former} + B_{train}\\). (13)\nIn both M1 and M2, the new parameters of \\(B_{former}\\) must be reset to zero after each update,"}, {"title": "4 Experiments", "content": "Experimental Setup\nDatasets. We conducted comprehensive evaluations on 18 datasets: 15 from BBM (Srivastava et al., 2023), while remaining datasets are selected from Llama3's (Dubey et al., 2024), Qwen2's (Team, 2024), Gemma2's (Team et al., 2024) base training or fine-tuning datasets. These datasets include 8 multiple-choice question (MCQ) datasets and 10 question-answering (QA) datasets, spanning six domains: mathematical reason-ing(GSM8K) (Cobbe et al., 2021), logical deduction(LogicalQA) (Srivastava et al., 2023), multilingual identification(CNN/DailyMail) (See et al., 2017), programming challenges(Multipl-E) (Cassano et al., 2022), translation questions(News Commentary dataset) (Tiedemann, 2012), and common-sense understanding(CommonsenseQA) (Talmor et al., 2019). Specifically, all of our benchmarks are carefully handled, with 1000 samples taken from"}, {"title": "4.2 Main Result", "content": "Performance on PEFT Comparison As shown in Table 1 SECURA outperforms all PEFT methods across both MCQ and QA tasks, achieving SOTA results. Compared to basic lora training, SECURA get average improvement 3.63% in MCQ tasks and gains 2.56% improvement in QA tasks. Notably, on tasks of NewsES, SECURA demon-"}, {"title": "4.3 Ablation Study", "content": "How to Prove the merge hypothesis of the Update Cycle? To prove the cycle hypothesis, we conducted an experiment comparing the performance of 1-step and 200-step merges across different LLMs. The results are shown in the table 3. Although the 200-step merge still retains some capabilities, it has obviously lower performance compared to the 1-step merge. We hypothesize that this performance degradation occurs because in the 200-step merge, the system has started entering the second cycle but not fully tracked by the problem. This suggests that the merge function plays a critical role in preventing catastrophic forgetting by maintaining a stable learning cycle.\nDifferent Performance based on different Merge method We designed another ablation study by testing different performance under M1 and M2 merge method with Gemma2 2b backbone and shows its feedback on table 4.\nAs the prediction in methodology, M1 method with deepen merge which will change basic weights shows more benefits on fine-tuning, performance better than M2 method range from 5.62% to 18.16%. In contrast M2 method which with shal-"}, {"title": "5 Discussion", "content": "Why Do We Update the Basic Parameters or Track SECURA Parameters But Never Change C and R?\nWe assume that parameters with higher norms carry more important information while lower-norm parameters are considered less important. The columns and rows C and R we initialized are represent the less important parameters' positions. Our goal is to keep the former important knowledge intact, which means it is necessary to prevent changes to C and R which are used to identify the locations that should be updated. By keeping C and R unchanged, we ensure that the critical information in the LLM is protected, while non-critical parameters can be updated or fine-tuned as needed."}, {"title": "5.2 What Are the Other Benefits of SECURA Compared to Other Catastrophic Forgetting Methods?", "content": "Since we are already shows SECURA's performance in our experiments. Here, we will point out the benefits of using SECURA over other methods like EWC or Experience Replay, specifically in terms of efficiency and simplicity."}, {"title": "5.2.1 Smaller Weight", "content": "Traditional methods like EWC rely on the Fisher matrix to detect changes between current and previous parameters, requiring the storage of previous parameters, which increases computational costs and space requirements. In contrast, SECURA"}, {"title": "5.2.2 No Need for Former Datasets and Calculation", "content": "Methods like Experience Replay store and reuse previous datasets to prevent catastrophic forgetting, but this comes at the cost of time and computational resources. SECURA, however, does not require past data and still performs effectively. This eliminates the need for dataset storage and reuse, making SECURA a more resource-efficient solution."}, {"title": "5.2.3 Easy to Establish", "content": "Both EWC and Experience Replay require additional preparation steps before training begin, which adds complexity. In contrast, SECURA allows fine-tuning the model without any additional operations-just like fine-tuning a LoRA model. This makes SECURA much easier to implement and more accessible for practical applications."}, {"title": "6 Conclusion", "content": "We propose SECURA, a novel PEFT fine-tune method that integrates SigNorm normalization and CABR decomposition to address catastrophic forgetting in LLM fine-tuning. SECURA dynamically balances parameter updates using a Sigmoid-based pruning mechanism, preserving critical knowledge while adapting to new tasks. Experiments on 18 datasets and 5 LLMs show SECURA outperforms standard LoRA, with a 3.63% improvement on MCQ tasks and 2.56% on QA tasks, while retaining over 70% of base knowledge in continual learning scenarios. Paving the way for sustainable and ethical deployment of large AI models."}, {"title": "7 Limitations", "content": "Lack of experiments on 70B+ LLMs: Due to device limitations, further research on larger models (70B+ parameters) could not be conducted. Future work will explore scaling SECURA to more massive LLMs.\nComputational Overhead: SigNorm introduces additional matrix operations during training, increasing per-step time by approximately 1.18% compared to LoRA. Future work could focus on optimizing the normalization layer for real-time applications."}, {"title": "A Broader Impacts", "content": "Efficiency and Accessibility: By eliminating the need for prior datasets and reducing training time for knowledge retention, SECURA lowers the computational barrier for fine-tuning LLMs, making advanced AI adaptation more accessible to researchers with limited resources.\nEnvironmental Sustainability: The reduced computational overhead compared to Experience Replay could decrease energy consumption in large-scale LLM deployments.\nEthical Risks: While SECURA mitigates catastrophic forgetting, its ability to retain sensitive information from pre-training data (e.g., biased or private content) requires careful auditing before deployment. Future work should investigate selective forgetting mechanisms to align with ethical AI principles.\nGeneralization Beyond NLP: The SigNorm algorithm, though initially designed for PEFT, could be extended to other continual learning scenarios (e.g., robotics, healthcare), potentially improving adaptive systems in dynamic environments."}]}