{"title": "Multi-Level Querying using A Knowledge Pyramid", "authors": ["Rubing Chen", "Xulu Zhang", "Jiaxin Wu", "Wenqi Fan", "Xiao-Yong Wei", "Qing Li"], "abstract": "This paper addresses the need for improved precision in existing Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing recall. We propose a multi-layer knowledge pyramid approach within the RAG framework to achieve a better balance between precision and recall. The knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs), and chunk-based raw text. We employ cross-layer augmentation techniques for comprehensive knowledge coverage and dynamic updates of the Ontology schema and instances. To ensure compactness, we utilize cross-layer filtering methods for knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall model for retrieval, starting from the top of the pyramid and progressing down until a confident answer is obtained. We introduce two benchmarks for domain-specific knowledge retrieval, one in the academic domain and the other in the financial domain. The effectiveness of the methods has been validated through comprehensive experiments by outperforming 19 SOTA methods. An encouraging observation is that the proposed method has augmented the GPT-4, providing 395% F1 gain by improving its performance from 0.1636 to 0.8109.", "sections": [{"title": "1 Introduction", "content": "Significant advancements have been made in Large Language Models (LLMs), including proprietary models like ChatGPT and GPT-4, as well as open-source variants like FLAN (Wei et al., 2021) and LLaMA (Touvron et al., 2023a). These models have demonstrated remarkable achievements across a wide range of general knowledge tasks in areas such as language comprehension (Radford et al., 2018; Hadi et al., 2023), logical reasoning (Kojima et al., 2022; Chang et al., 2024), and complex question-answering (Wu et al., 2023b). However, the one-size-fits-all nature of general LLMs fails to meet the specific demands for professional or personalized knowledge, such as law (Huang et al., 2023b; Cui et al., 2023) and finance domains (Wang et al., 2023; Xie et al., 2023). A straightforward solution is to utilize Supervised Fine-Tuning (SFT) to tailor LLMs to domain-specific tasks (Ouyang et al., 2022). Examples include AlpaCare (Zhang et al., 2023), Mental LLaMA (Yang et al., 2024a) and Zhongjing (Yang et al., 2024b) in medical domain, BloombergGPT (Wu et al., 2023a), Pixiu (Xie et al., 2023), and DISC-FinLLM (Chen et al., 2023) in financial domain, and DISC-LawLLM (Yue et al., 2023) in law domain. Nonetheless, this risks catastrophic forgetting (Luo et al., 2023) of the general knowledge when tuning the original model, and is prone to model hallucination (Huang et al., 2023a).\nTo address this challenge, the prevalent alternative of Retrieval Augmented Generation (RAG) is introduced as a means to enhance the domain knowledge comprehension of LLMs (Lewis et al., 2020a). Rather than solely relying on the generation capabilities of LLMs to produce answers, RAG takes a different approach by incorporating information retrieval techniques. It retrieves relevant information from existing resources and utilizes this information to enrich the context of the prompt (Gao et al., 2023). This enables in-context learning (Dong et al., 2023) or few-shot learning (Wang et al., 2020b) to be initiated based on the enhanced context. It makes the LLMs' output more stable, accurate, traceable, and interpretable. However, early implementations of RAG have relied on unstructured textual data chunks, which are often obtained by partitioning each document in the corpora database into segments with a predefined chunk size and thus not organized in a specific way (Ma et al., 2023; Lewis et al., 2020b). To address this limitation, Knowledge Graphs (KGs) have been incorporated into RAG (Baek et al., 2023a; Wu et al., 2023d; Abu-Rasheed et al., 2024). The in-"}, {"title": "2 Related Works", "content": "2.1 Domain-Specific Large Language Models\nLLMs have become integral in various applications, such as chatbots, writing assistants, customer service automation (Sallam, 2023; Rebedea et al., 2023). Domain-specific LLMs are a subset of LLMs that have been tailored to understand and generate text within a particular area of expertise, such as law (Huang et al., 2023b; Cui et al., 2023; Yue et al., 2023), medicine (Zhang et al., 2023; Yang et al., 2024b,a), or finance (Ge et al., 2024; Wang et al., 2023; Xie et al., 2023; Chen et al., 2023). These models aim to provide higher accuracy and more relevant outputs than general-purpose LLMs when dealing with specialized content (Ouyang et al., 2022). A common approach is to adapt instruction fine-tuning based on the general LLMs (Ouyang et al., 2022) using domain-specific copra. However, research has shown that the generality of the fine-tuned model will diminish, resulting in catastrophic forgetting (Luo et al., 2023; Zhai et al., 2023). In order to tackle the limitations, the approach of prompting LLMs with extra domain knowledge as contexts is widely adopted.\n2.2 Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) enhances the generative capabilities of language models by incorporating retrieved knowledge for in-context learning (Lewis et al., 2020b; Gao et al., 2023). NaiveRAG represents the most basic architecture within this framework, in which the system retrieves the top-k documents that are most relevant to the query and integrate them into the prompt, thereby grounding the responses in more relevant information (Ma et al., 2023).\nExpanding on NaiveRAG, advanced RAG incorporates additional modules or structures to improve retrieval precision. Reranking is a notable example, where a reranker is employed to refine the initial ranked list (e.g., Re2G (Glass et al., 2022) and bge-reranker (Xiao et al., 2023), both are based on BERT (Kenton and Toutanova, 2019)). Furthermore, studies have indicated that excessive noise and lengthy context can have a negative impact on inference performance. To address this, prompt compression methods such as Selective Context (Litman et al., 2020) and LLMLingua (Anderson et al., 2022) have been developed. These methods emphasize key information while reducing noise and context length, as discussed in (Gao et al., 2023).\n2.3 Knowledge-Augmented Language Models\nThe Knowledge-Augmented Language Model (LM) approach involves integrating LMs with additional knowledge bases to facilitate in-context learning (Liu et al., 2019). In addition to incorporating knowledge in the form of raw texts (Ma et al., 2023), knowledge graphs (KGs) (Baek et al., 2023b; Pan et al., 2024; Wu et al., 2023d) have gained popularity, and Ontologies (Vamsi et al., 2024) have also been utilized, albeit less frequently. In most KG augmentation methods (Pan et al., 2024), the RAG framework is followed, wherein the context is expanded by incorporating retrieved KG triples instead of raw text chunks. KAPING (Baek et al., 2023a) serves as an early example of this approach, which has been later refined in RRA (Wu et al., 2023c). Regarding Ontologies, instead of being used as an individual knowledge base, they are often employed as assistants for generating KG triplets (Pan et al., 2024) (e.g., Text2KGBench (Mihindukulasooriya et al., 2023)) or for augmenting textual corpora (e.g., EKGs (Baldazzi et al., 2023), OntoChatGPT (Palagin et al., 2023)). The integration of various forms of knowledge bases poses a significant challenge and remains an area that has received limited exploration."}, {"title": "3 Knowledge Pyramid Construction", "content": "Let us establish the knowledge pyramid as the foundational base for PolyRAG. The construction process begins by creating three distinct knowledge banks, each guided or supported by an Ontology, a knowledge graph, and the raw texts, following common practices. These banks (denoted as O, K, and T, respectively) form the initial layers of the pyramid. The essence of our proposed methodology lies in fostering interactions between these layers, aimed at enhancing the overall comprehensiveness and compactness of the knowledge base.\n3.1 Construction of Initial Layers\nOntology Layer: The Ontology layer O = {Os, Oi} consists of a schema Os and corresponding instances Oi. Defining an Ontology schema typically requires significant time and effort from human experts, and achieving a comprehensive schema can be challenging. To simplify the process, one approach is to initially extract a sub-domain schema from general Ontologies like WordNet (Miller, 1995) or ConceptNet (Speer et al., 2017). This extracted schema can serve as a starting point and be refined using the semi-automatic approach presented in Section 3.2.\nWith the schema Os that we extracted, we can guide LLMs to extract instances from the raw text layer T for each of the concept-attribute pair (c, a) \u2208 Os, where c is a concept (e.g., professor) and a is one of its attributes (e.g., research_interest). This can be written as a prompt function\nfins(c, a; p): Given a paragraph {p} from the {domain} domain, please identify instances of the Ontology relationship where a class {c} has the attribute of {a}. Note that the attribute may consist of multiple entities.\nExecuting this function results in instances that fulfill the specified relationship. By repeatedly applying this function to each paragraph, the set Oi is constructed as\nOi = {fins(c, a; p) |\u2200(c, a) \u2208 Os, \u2200p \u2208 T}.\nIt is important to note that the specific implementation of the prompt may vary among LLMs. Additionally, in certain cases, it may be beneficial to provide examples to initiate few-shot learning for extracting high-quality information, depending on the capabilities of the LLMs. Our implementations are available in the Github repository.\nKnowledge Graph Layer: We adopt Open Information Extraction (OpenIE) (Etzioni et al., 2008) to extract KG triples from raw texts. However, direct extraction often results in noisy output, including irrelevant or duplicate entities. To address this, we draw inspiration from the multi-round prompting approach of LLM2KG (Carta et al., 2023) and reimplement it by introducing four functions: fpar(p) for paraphrasing, fent() for entity extraction, frel() for relation completion, and fdis() for disambiguation. These functions form a cascade for KG triplet extraction and refinement as\nfkg(p) = fdis(fent(fpar(P)), frel(fpar(p))).\nIt constructs the initial knowledge graph layer as\nK = {fkg(p)|\u2200p \u2208 T}.\nDue to space limitations, the implementations of these functions are provided in the Appendix A.\n3.2 Knowledge Completion\nAs shown in the pyramid of Figure 1, the higher layers are more structured but not easy to define or extract, resulting in limited coverage. One common issue is the absence of important classes or attributes from the expert-defined Ontology schema, which has historically hindered the ease of defining Ontologies. By utilizing the pyramid framework, we can address this issue in a data-driven manner. The idea is to identify noteworthy concepts and relations that exist in the lower layers but are absent from the higher layers. These identified elements are then incorporated into the higher layers to enhance knowledge completion. For instance, our research reveals that the relationship publications_in_important_journals appears frequently in both the knowledge graph and raw texts but is absent from the Ontology. This is due to the tendency of human-defined schema to overlook this specific relationship, as the presence of the publications attribute may make it seem redundant. However, users often query this specific relation, and it is explicitly mentioned in professors' profiles. It is thus worth including it in the Ontology.\nOur approach involves modeling the semantic distributions of the Ontology and knowledge graph to identify concepts and relations that exhibit significant divergence between the two layers. To achieve this, we begin by transforming class-attribute pairs in the Ontology layer O into subject+relation phrases that align with the format in the knowledge graph layer K. Instructor embedding (Su et al., 2022) of phrases at both layers are then encoded so as to project them into a common semantic space. We then learn for each layer a multivariate Gaussian using\nF(X) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}}e^{-\\frac{1}{2}(X-\\mu)^T \\Sigma^{-1}(X-\\mu)},"}, {"title": "3.3 Knowledge Condensation", "content": "The process of Knowledge Completion aims to enhance the richness of information in a bottom-up manner. In addition, we perform Knowledge Condensation to improve the compactness of the pyramid in a top-down approach. The principle is to leverage the well-structured knowledge in the higher layers to eliminate redundant information present in the lower layers. This is implemented by using each class-attribute in the Ontology layer as an anchor and explore its neighborhood to identify a set of k nearest knowledge graph triplets {Xk}. A prompt function is then applied to instruct the LLM to summarize them into compact ones as\nfcon({Xk}): Please condense the set of knowledge graph triplets {Xk} obtained from the {domain} domain by eliminating redundant triplets and summarizing the remaining ones in a more concise manner. Here are some examples for your reference: {examples}. The condensation process should follow the logic of {CoT}.\nBy repeating this process for all anchors, we obtain the condensed knowledge graph layer, as depicted in Figure 2. The original triplets such as \"Prof. A works in Z\" and \"Prof. A is with Z institue\" have been merged into more compact form as a relation of \"is_a_member_of\". It is evident that the compactness of the pyramid has been significantly improved."}, {"title": "4 Multi-Level Querying", "content": "The pyramid enables PolyRAG in a straightforward top-down querying manner, following the flow Ontology\u2192Knowledge Graph\u2192Raw Texts, in which the results are returned if answers are found at higher layers; otherwise, the querying continues to the next layer. The querying flow is"}, {"title": "4.1 Search at Ontology Layer", "content": "At the Ontology Layer, we can utilize SparQL as the query language. SparQL is a well-defined language that provides precise results when queries are properly structured. To achieve this, we can guide the LLM to generate the SparQL query by using the following prompt function\nQueryonto(Q, schema): Given a input question {Q} within the domain {domain}, please formulate a SPARQL query to retrieve the answer based on the provided Ontology schema. The namespace is {schema.base}, and the classes are {schema.class}. The object properties between classes are {schema.op}, and the classes may also have data properties such as {schema.dp}.\nFor the example of a query question like \u201cWho is currently working in CS Department and was graduated from Cambridge University?\u201d, the LLM extracts the related attributes of \u201cworks_in\" and graduated_from\" with the given schema, and generate a SPARQL query to search a staff who satisfies the relevant condition in O. The executing results may include a list of names, or simply empty due to either the lack of knowledge in Ontology layer, or the mistake of providing unavailable SPARQL due to the limitation of LLM itself."}, {"title": "4.2 Search at Knowledge Graph Layer", "content": "At the knowledge graph layer, we employ a retrieval approach similar to the embedding-based retrieval utilized in NaiveRAG. However, instead of using chunks, we work with triplets. Once the matching triplets are retrieved, we utilize a prompt function to assess the Language Model's agreement on whether the question has been answered adequately as\nQuerykg(Q, triples): Given a question {Q} and the context information provided by the matched triples from the knowledge graph {triples}, please justify whether the provided information is sufficient to accurately answer the question. Respond with either \"Yes\" or \"No\" to provide your justification.\nFor example, given a query of \"which CS staff has interest in cloud computing?\u201d, the LLM may justify a set of triples as \"agree\" if they include information like \u201cProf. A works in CS Department, Prof. A published on cloud computing journal, etc.\"; inversely, the justification might be \"disagree\" if the triples do not provide the key knowledge related to this query, thus the searching will process to the next layer for a more relevant context in order to answer the query.\""}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nOur experimentation involves two distinct domain-specific benchmarks. The first benchmark is Academia Challenge (AcadChall), which has been built by ourselves and focuses on the academic domain. It encompasses a comprehensive collection of data obtained from XXX university, including information about 31 departments, 1,319 faculty members, and 2,061 courses. Specifically, AcadChall consists of 512 MCQ and MAQ questions that cover topics related to teaching and research. These questions are intentionally designed to be more challenging compared to existing benchmarks, as each question presents eight answer choices. This design aims to provide a more rigorous assessment of the models' precision. The second benchmark is FiQA, which is widely rec-"}, {"title": "5.2 Evaluation Metrics", "content": "Our evaluation process encompasses several metrics to assess the performance, including: 1) For MCQs and MAQs, we employ metrics such as Precision, Recall, and F1 score. 2) For open-ended questions, we utilize BLEU and BERT similarity, calculated based on MiniLM embeddings (Wang et al., 2020a). 3) We introduce a novel metric called HitRate (HR), which quantifies the proportion of correct entities present in the response. These metrics collectively allow for a comprehensive assessment of the system's effectiveness across different question types and response formats."}, {"title": "5.3 Comparison to SOTA Methods", "content": "We compare PolyRAG with five groups of state-of-the-art (SOTA) methods, including 1) Frozen-LLMs that are pretrained LLMs of frozen parameters and no external knowledge has been used for querying. 2) SFT (Supervised Fine-Tuning) that are LLMs that undergo supervised fine-tuning on domain-specific datasets. We include two latest implementation of Finance-LLM and Finance-Chat (Cheng et al., 2024), both of which are trained on financial corpora. Additionally, we explore variants of SFT that employ LoRA (Hu et al., 2022) instead of the entire models. 3) NaiveRAG proposed in (Lewis et al., 2020a). 4) Advanced RAG that include two recent models, namely ColBERTv2 (Santhanam et al., 2022) and Bge-reranker (Xiao et al., 2023). 5) KG-Augmented LLMs that consists of models proposed in (Baek et al., 2023b) that integrate knowledge graphs (KGs) into the LLMs. Various LLMs have been explored as the back-"}, {"title": "5.4 Knowledge Completion and Condensation", "content": "We have conducted experiments to evaluate the effectiveness of knowledge completion (CPL) and condensation (CND) by integrating them with a baseline model (PolyRAG without these two modules). Please note that due to space limitations, we only list results for four specific backbones. For a comprehensive performance analysis with Vicuna-13B and LLaMA2-70B backbones, please refer to the Appendix C. The results clearly demonstrate the effectiveness of both completion and condensation in enhancing performance, whether used individually (CPL: 3.8% \u00b1 6.1 performance gain, CND: 2.2% \u00b1 3.3 performance gain) or in combination (6.5% \u00b1 6.4 performance gain over the baseline). Our statistics indicate that with completion, queries resolved at the Ontology layer is increased by 5%. With condensation, more than 27% of queries are resolved at the KG layer."}, {"title": "5.5 Influence of Knowledge Layers", "content": "We have examined the influence of different knowledge layers on various backbones. It is obvious that the Ontology layer plays a prominent role as a knowledge base in enhancing precision, as evidenced by a precision gain of 25.8% \u00b1 7.4 when combined with the raw text layer and 13.7% \u00b1 10.3 when combined with the KG layer. On the other hand, the KG layer demonstrates a balanced impact on improving either precision or recall. These observations align with our early discussions."}, {"title": "6 Conclusion", "content": "This research has described a preliminary investigation that provides a multi-level query approach to domain-specific question answering. In this context, we addressed two significant challenges: the problem of prioritizing knowledge in a certain area and the presence of noise in retrieval situations. In order to overcome the limitations, we introduce Knowledge Pyramid multi-level retrieval framework as PolyRAG. PolyRAG utilizes a sequential retrieval approach and prioritizes knowledge extraction. This approach is designed to tackle the challenges posed by the high demand for dense knowledge in domain-specific scenarios and the distractions caused by noisy contexts. The accuracy of PolyRAG has been validated through extensive tests done in AcadChall and R-FLUE-FiQA, surpassing earlier approaches and generating state-of-the-art results.\nLimitations\nOne limitation to consider is that Language Models (LLMs) do not strictly adhere to the SparQL syntax, which can result in typographical errors in the queries. Further investigation into this aspect is warranted."}, {"title": "Ethics Statement", "content": "This research paper adheres to ethical considerations throughout its methodology and findings. The study primarily focuses on the development and evaluation of a retrieval-augmented generation framework, PolyRAG, for domain-specific knowledge retrieval. The research involves the use of existing datasets and benchmarks, as well as the creation of two new benchmarks in the academic and financial domains.\nThe authors ensure proper citation and acknowledgment of the data sources to maintain transparency and intellectual property rights. In terms of human subjects, this research does not involve any direct human participation, personal data collection, or sensitive information. Therefore, ethical concerns related to informed consent, privacy, and confidentiality are not applicable in this context."}, {"title": "A Knowledge Graph Layer Construction", "content": "This section provides the detailed implantations of the four functions for constructing the knowledge graph layer in Section 3.1.\nfpar (p): Determine the factual claims from a given paragraph {p}. Put these facts into short phrases with basic grammar. Remember that every statement should have a distinct meaning, and pronouns should be avoided.\nfent(fpar(p)): Extract the noun entities in phrases from the given sentences fpar(p). The entities should not contain any comma, and each entity should be unique during extraction.\nfrel(fpar(p)): Given the reference context fpar (p) and relevant entities, complete the relations between two entities. Notice that a triple should only contain one entity as head, one verb or verb phrase as relation, and one entity as tail. Separate the head, relation, and tail with a comma.\nfdis(fent(), frel()): You are given several triples frel() with their entities fent(). These triples consist of subject-predicate-object elements, separated with a comma, but may contain ambiguities or inaccuracies. Your task is to refine and disambiguate these triples to ensure that they accurately reflect the entities and relationships described in their source texts without duplication or omissions. If the relationships have the same semantic meaning, rewrite the triples with the same relation. If the triple has already been mentioned with the same meaning as previous triples, delete it."}, {"title": "B AcadChall and R-FLUE-FIQA Benchmarks", "content": "lists the data statistics of two benchmarks. We use Vicuna-13B (Chiang et al., 2023) to build the knowledge pyramid for two datasets according to the methods illustrated in Section 3.1. Specifically, the AcadChall dataset is created by crawl-"}, {"title": "C Discussion on Knowledge Completion and Condensation", "content": "verifies the effectiveness of our proposed knowledge completion and condensation techniques over multiple LLM backends. Specifically, the multi-level querying on the knowledge pyramid without completion and condensation techniques is used as the baseline. In the baseline, around 30% of questions are answered by more structured data (i.e., Ontology or KG). After applying the completion and condensation techniques, the percentage increases to 64%. For example, the relation \u201chas published in\u201d, which frequently appears in the KG layer, has been used to complete the Ontology layer. It has resulted in those questions related to \u201cpublications\u201d being answered by the Ontology"}, {"title": "D Influence of knowledge layers over backbones", "content": "lists the impact of different knowledge layers on more LLM backbones than the results presented in Section 5.5. The proposed knowledge pyramid significantly improves the precision of the raw text layer across different backbones. With more powerful backbones, such as Genmini-Pro, GPT-3.5, and GPT-4, the performance gains are larger (i.e., over 0.3). Besides, higher precision could be obtained when combining the Ontology layer with the KG or raw texts layer. A similar observation happens when combing the KG layer with raw text or the Ontology layer across different backbones. These are consistent with the observations in Section 5.5."}]}