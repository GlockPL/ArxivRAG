{"title": "Expertise elevates Al usage: experimental evidence comparing laypeople and professional artists", "authors": ["Thomas F. Eisenmann", "Andres Karjus", "Mar Canet Sola", "Levin Brinkmann", "Bramantyo Ibrahim Supriyatno", "Iyad Rahwan"], "abstract": "Novel capacities of generative Al to analyze and generate cultural artifacts raise inevitable questions about the nature and value of artistic education and human expertise. Has AI already leveled the playing field between professional artists and laypeople, or do trained artistic expressive capacity, curation skills and experience instead enhance the ability to use these new tools? In this pre-registered study, we conduct experimental comparisons between 50 active artists and a demographically matched sample of laypeople. We designed two tasks to approximate artistic practice for testing their capabilities in both faithful and creative image creation: replicating a reference image, and moving as far away as possible from it. We developed a bespoke platform where participants used a modern text-to-image model to complete both tasks. We also collected and compared participants' sentiments towards AI. On average, artists produced more faithful and creative outputs than their lay counterparts, although only by a small margin. While Al may ease content creation, professional expertise is still valuable even within the confined space of generative Al itself. Finally, we also explored how well an exemplary vision-capable large language model (GPT-40) would complete the same tasks, if given the role of an image generation agent, and found it performed on par in copying but outperformed even artists in the creative task. The very best results were still produced by humans in both tasks. These outcomes highlight the importance of integrating artistic skills with Al training to prepare artists and other visual professionals for a technologically evolving landscape. We see a potential in collaborative synergy with generative AI, which could reshape creative industries and education in the arts.", "sections": [{"title": "Introduction", "content": "Generative machine learning models' increasing prevalence and capacities are transforming creative processes. Large language model-driven text generators and chatbots like ChatGPT or Copilot enable native speaker-like text production. Generative image models and services such as Stable Diffusion, Dall-E or Midjourney enable the creation of artistic, photo-realistic, and illustrative visual materials without necessarily having professional training in these fields. Such tools are also rapidly being integrated into word processors and graphic design software like Canva or Adobe Photoshop. By far the most common interface to these generative artificial intelligence models is natural language instructions or \"prompting\". The entry threshold is thus very low: only a minimal command of the input language(s), typically English, is required to get a given app or model to start generating artistic materials or grammatically coherent text. Generative Al usage typically combines both creation and curation, as it is easy and often the default workflow in many applications- to produce concurrent variants based on a single input, and choose the most suitable output.\nIt has been shown that current generation LLMs are not only theoretically capable of creativity (Wang et al. 2024), but already out-performing humans in various creative writing and divergent thinking tasks (Bellemare-Pepin et al. 2024; Hubert et al. 2024; Mirowski et al. 2024). This is in addition to more formal tasks such as various forms of textual annotation and analysis, where similar results have emerged (Ziems et al. 2023; T\u00f6rnberg 2023; Gilardi et al. 2023; Karjus 2023). Naturally, it depends on which humans exactly the machines are being compared to, the average or the top performers, experts or laypeople (Koivisto and Grassini 2023; Haase and Hanel 2023; Porter and Machery 2024). Other work has reported humans as being more imaginative than LLMs (Begu\u0161 2024), or LLMs being more creative but less diverse (Doshi and Hauser 2024). Furthermore, general creativity as such is notoriously difficult to define and measure (Miravete and Tricot 2024). Instructing generative text and image models has been argued to be a skill in itself (to the point of being called \"prompt engineering\"), as informative inputs that are well aligned with the pre-trained \"expectations\" of the model tend to produce superior results (Wei et al. 2022; Liu and Chilton 2022; Oppenlaender et al. 2024). Additionally, it has been shown that prompt content continues to matter even as generative models improve (Jahani et al. 2024). Image generators have been assessed in several studies highlighting their usefulness as supporting creativity (S\u00e1ez-Velasco et al. 2024; Gu et al. 2024; Braguez 2023), and such usage likely benefits from relevant skills.\nHere we focus on text-to-image models and the domain of visual art, broadly construed. The aesthetics, value, and quality of the outputs of these tools may be criticized, but their usage is widespread and likely increasing (von Garrel and Mayer 2023; Shen et al. 2023; Walkowiak and Potts 2024). While people may prefer (at least the idea of) naturally produced art (Bellaiche et al. 2023), artificially generated content is becoming increasingly difficult, if not impossible, to detect, especially for the untrained eye and ear (Lu et al. 2024; Frank et al. 2023; Cooke et al. 2024; Porter and Machery 2024). This adoption of such easy content creation technologies across various professions is blurring the lines between professional artists and hobbyists. It also inevitably raises questions about the nature of and need for expertise and the role of Al therein. Professional artists and amateurs may also use Al tools simply for different ends, the former to improve art quality and try new styles, the latter for e.g. entertainment and exploration (Elfa et al. 2023; Braguez 2023; Shen et al. 2023). Most recent comparisons in this domain cited above have focused on laypeople; we aim to build on this with explicit comparisons with experts.\nNaturally, these developments have raised various concerns from legal and moral issues around training data (Goetze 2024) to ethical practices of Al usage and its impact on the creative professions and labor markets (Lovato et al. 2024; Miyazaki et al. 2024; Walkowiak and Potts 2024). A recent study found a significant drop in job postings for writing and image creation jobs on online freelance platforms after the introduction of chatbots like ChatGPT and various image generation tools like Midjourney (Demirci et al. 2023). The rapid uptake of these technologies has caused various reactions in societies and the media, ranging from claims that \"art is dead\" (Roose 2022) and that \"this changes everything\" (Klein 2023), to reports of \"Al anxiety\" of workers fearing for their jobs (Cox 2023), and questions about \"creators becoming redundant\" (Dege 2023). On the other hand, Al adoption has been associated with gains in both artistic productivity and output novelty (Zhou and Lee 2024). Such tools clearly have the potential to change education and career paths in the arts, yet there is uncertainty about how the focus should be distributed between traditional art education, \"AI prompting\", and the synergy of the two."}, {"title": "Methods and materials", "content": "Here we explore the hypothesis that expertise in a given domain should also also lead to superior results in using Al, on the example of text-to-image prompting and the curation of its results (Figure 1). Professionals can be expected to be more creative, aware of artistic principles, better at commanding domain-relevant vocabulary to more accurately describe the expected outputs of a creative process, and skilled at curating among multiple generated alternatives. We aim to measure how much of an advantage this provides, if any, compared to layperson-users of the same Al tool, representing the general population. We devised two controlled tasks, emulating the artistic practices of replication or copying of existing art, and the creative production of novel or divergent art (see Figure 1 and Methods for details). Specifically, we test the following preregistered hypotheses:\nH1: Artists' images in the copying task will overall be closer to the original images than laypeople's.\nH2: Artists' images in the creative task will overall be more distant from the original images than laypeople's.\nH3: Artists' curated images in the copying task will be closer to the original images than laypeople's.\nH4: Artists' curated images in the creative task will be more distant from the original images than laypeople's.\nH5: Curation independent from prompting: Artists will consistently choose more suitable images from the selection of four images in the pooled data of both tasks, compared to laypeople.\nAdditionally, we explore a comparison of the human groups to the outputs of a similarly instructed Al language model in an approach comparable to H1-2.\nThe current experimental design draws from several experimental traditions. These include interactive behavioral experiments often used in cognitive science and adjacent disciplines (Kirby et al. 2008; Okada and Ishibashi 2017; N\u00f6lle et al. 2018; M\u00fcller et al. 2019; Karjus et al. 2021; Kim et al. 2024), research comparing the behavior or outcomes of domain professionals or experts with some control group of laypeople (Kozbelt 2001; Bhattacharya and Petsche 2005; Bezruczko and Schroeder 1994; Torngren and Montgomery 2004), and studies on individual aesthetic perception and preferences (Porter and Machery 2024; Cela-Conde et al. 2009; Lakhal et al. 2020). The online experiment was carried out with two participant groups, asked to complete two distinct tasks, which we call \"copying\" and \"creative\" for short. The tasks were divided into \"prompting\" and \"curation\" sub-tasks or phases, and each task contained four such image generation and curation trials. Figure 1.B illustrates the pipeline from experimental tasks to results. The sections below describe the details of the design, sample, procedure, and data analysis."}, {"title": "Participants", "content": "Our final sample consists of 99 participants, 50 professional visual artists, and a matched control group of 49 laypeople recruited via the crowdsourcing platform Prolific. The artist sample was recruited first, via invitation emails sent directly through the coauthors' (mostly MS) personal and professional networks. This way, we ensured artists fulfilled our pre-registered recruitment criteria (see below) and avoided excessive over-recruiting and exclusion, although it was rather labor-intensive. The final artist sample resulted from a total of 213 invitations, distributed in rounds of 10 to 15 invitations per day over several weeks to control the load put on the Stable Diffusion model used in the experiment. After the full artist sample had been completed, we recruited the laypeople sample to match its demographics.\nAll participants, regardless of the subsample, were paid \u20ac3.75 after completing the full experiment, in compliance with the ethical approval received for the study beforehand (by the IRB of the Max Planck Institute for Human Development, approval number A2024-15). Participants not using \u20ac currency received the equivalent amount in their local currency instead. Artists were paid via direct transfers using Paypal, while laypeople received the money in their Prolific account. Some artists (22%) were more motivated by participating in a scientific experiment than by the remuneration itself and waived their payment.\nAll participants were required to be fluent in English (self-reporting native or near-native competence) but were allowed to have a different first language. Most importantly for the purpose of the study, the artist sample had to actually represent professionally working artists, whereas the laypeople sample must not, to ensure a clean separation of the two conditions. We made sure of this by recruiting the artists from our networks of professional artists, and double-checking that the Prolific sample did not include any. There was an item in the post-questionnaire to screen for participants' correct assignment to their condition (\"Do you have work experience in visual art?\"). If a participant's answer did not match their condition, they were excluded from the sample. This happened in 5 cases for artists (e.g. having art education but not working professionally as an artist) and in 10 cases for laypeople (due to chance, because we could not initially screen them against that). We also inquired about the number of years the artists had been active in visual arts, which was high in our final sample (M = 22.4, \u03c3 = 10.7). We chose to focus on artists in professional careers, defined as those actively engaged in creating art and participating in the distribution system, such as exhibiting or selling their work. This has been called the most salient marker distinguishing serious artists from amateurs (Becker 1982). Thus, art students and academics not actively working in the field were excluded, to ensure a clean comparison between professionals who create art and laypeople who do not. We further narrowed our focus to visual artists, as this was the medium of the task.\nWhile recruiting the artist sample, we recorded participants' highest level of education and their first language, identically to two screening items on Prolific. We then used the screening function on Prolific to match the participants faithfully, according to their specific combinations of education and language (binary English/not English). As expected, artists' highest level of education was very high, with 24 participants of the 50 stating they had a Master's degree and 23 more reporting a PhD. 14 artists reported being native speakers of English.\nApart from group assignment, we specified two more exclusion criteria in the preregistration. Participants who did not enter a prompt in two or more trials were replaced due to too much missing data. Images that were blanked out for participants due to our implemented filter (see below) were counted toward this limit. These criteria led to the exclusion of two more participants (one per subsample), who were immediately replaced. After data collection of all 100 participants and exclusion via the preregistered criteria had concluded, it became apparent that the prompting data of a single participant from the laypeople group had not been passed on due to a technical error (while the participant provided all information otherwise), and was thus unusable; hence we arrived at our final sample of 99 participants."}, {"title": "Experimental design", "content": "This monadic design consists of instructing each participant, working independently but within a time limit, to complete two multimodal tasks where they are shown a reference image and asked to write short textual inputs (\"prompts\") to produce new images and select their preferred output. We created a bespoke online platform for the experiment, aimed to broadly mimic currently common generative Al apps and platforms. The experiment started with an instructions and consent page, followed by two training trials where the participants were free to generate images from their own prompts to familiarize themselves with our generator, the interface, and the prompting and curation phases. They were then instructed in the first task, completed it, got instructed in the second task, and finally were asked to complete a short questionnaire. In total, the average participant took about 18 minutes to complete the experiment.\nParticipants completed the copying task first and creative task second. The were 4 trials within each task which displayed a unique reference image. The order of images was randomized within each task. Each of the 8 trials consisted of a \"prompting phase\" and a \"curation phase\". The interfaces were the same for both tasks. In the prompting phase, participants were tasked with writing prompts to create images from. Here, the interface showed the unique reference image, a text entry box, and a \"submit\" button. Prompt length per trial was limited to 30 (space-separated) words, visible as a counter below the text box. Furthermore, there was a time limit of 2 minutes per attempt, which was also displayed. If time ran out before participants had submitted their prompt, their current input was submitted and they moved to the curation phase automatically.\nAfter prompting, our generative model created 4 different images for the participant to choose from in the curation phase. The reasoning behind this was threefold. First, this mimics real-world generative Al usage, where tools often provide multiple variants by default. It also balances the randomness inherent in different seeds (see below). Finally, this enables testing whether curating would also allow for the expression of artistic expertise, as per our second set of hypotheses. The interface showed the 4 generated images arranged horizontally, with the reference image displayed on top for comparison. Here, the time limit was 30 seconds, as participants only needed to click their preferred image to submit. If time ran out, the images were stored without curation data.\nIn the copying task, participants were asked to prompt the model to generate an image \"as close as possible\" to the reference image. The goal of the task is to compare the ability of different groups to produce faithful copies of an example, demonstrating their familiarity with a given genre (see Stimuli below), ability to envision and describe a subject, and an eye for relevant details. This is meant to partially approximate traditional art education practices that also emphasize honing skills by copying masterpieces to internalize techniques and styles.\nIn the creative task, participants were instead asked to make a new image that would be \"as different from the original as possible\". Their instructions further clarified that 1) ideally, they should aim for \"an image with different content as well as different visuals\", and that 2) negation does not work well for this and would actually increase the likelihood of adding something to the image. To make sure results remain comparable and the task reasonably challenging, the start of the prompt was (visibly) fixed in the text box to the first words used to create the reference image. For example, if the prompt we used to create the image was \"A photo of a classic gray Mini Cooper in a parking lot of a shopping mall\", the text box would contain the locked starting phrase \"A photo of a gray classic Mini Cooper\". This task was designed to measure creativity in addition to the descriptive skills of the first task, as it requires the addition of something novel and divergent, not just reproduction. As such, professional artists would be expected to excel. As discussed in the Introduction, measuring creativity is challenging, but we nevertheless propose ways to compare the groups as described below. An alternative for this task we considered would have been to instruct them to prompt for an image that would be just \"more creative\". We opted for the more indirect task of creating a different image, as it arguably still requires expressing creativity, while being much more straightforward to operationalize than measuring what is \"more\" and \"less\" creative (see below)."}, {"title": "The generative model, stimuli, and data", "content": "The text to image model in the backend was Stable Diffusion XL Turbo (henceforth SD), chosen for its ability to generate images within seconds (Sauer et al. 2023). As with all deep neural network-based models of that nature, SD has a random seed hyperparameter, which affects the outputs. To account for this inherent randomness, we used four fixed seeds to generate the variants that participants curated from. These seeds were distinct from the seeds used to create the reference images, so participants could not reproduce the reference images even if they found the exact corresponding prompt. This also makes the outputs comparable: as all parameters are fixed for all users, if two participants entered the exact same prompt, they would get the exact same set of four images. As for other relevant hyperparameter, the inference steps was set to 4 to ensure sufficiently fast image generation. We implemented a keyword-based filter to prevent users from creating sensitive or obscene images. The filter worked by comparing the CLIP embedding of the image to the embeddings of the filter words (Rando et al. 2022). If such an image was detected, it was not shown to the participant in the following curation step. This was for ethical concerns and because creating such images was not instrumental to the task. Participants were informed about the filter and asked to avoid creating inappropriate content. They were also instructed not to include any personal details in their prompts.\nThe eight reference images were created using the same SD model by our professional artist coauthor (MS) to ensure sufficient quality and task difficulty. The number of images and thus trials was small enough to keep the experiment manageable and within budget constraints. We aimed to cover a diverse range of artistic imagery, subjects, and styles: landscapes, architecture, vehicles, food, humans, and cartoon characters; and photographic, animation, painting, drawing, and video game 3D styles. Additionally, we kept the original prompts in the copying task relatively complex and the ones in the creative task relatively plain; this was to avoid ceiling or floor effects.\nWe have published all the image data resulting from the experiments (see Data Availability) and also produced an interactive dashboard based on the Collection Space Navigator (Ohm et al. 2023) that enables easy exploration of the dataset, available here: https://artistlaypeopleaiexperiment.github.io."}, {"title": "Measuring the results", "content": "After data collection and manual inspection, we identified and removed four trials that we deemed ineligible. These included errors like a single-letter prompt, two cases where participants seemed to have initially misunderstood the task (but carried on as expected afterward), and one case where the fixed lead of a creative trial merged with the input due to a bug in the interface (the data is still available in the open code and database accompanying this paper). We came across 11 occasions where participants had apparently circumvented the whitespace-operationalized word limit by concatenating a few words at the end of the prompt, but did not deem this an exclusion criterion. While clever, omitting spaces carries the risk of confusing the underlying text tokenizer of the model and therefore unexpected visual outputs.\nThe resulting final dataset consists of 3148 images. As preregistered, we used the CLIP (clip-vit-base-patch16) embeddings (Radford et al. 2021) to quantify the similarity between reference images and participant creations, via cosine similarity of the embedded image vectors. SD models are known to use CLIP internally for encoding prompts, making this a natural choice. In the copying task, the goal was to create a close replication of the reference image - therefore, higher cosine similarity indicates a better match. In the creative task, where the goal was to create a different image, lower cosine indicates a better outcome. For comparing groups in H1 and H2, the values were averaged for each set of four variant images (which on average have high inter-similarity), as we are more interested in prompt outcomes rather than individual images.\nAn image embedding is by no means an absolute or objective measure of visual similarity (if such a thing exists), but rather a practical solution for comparing thousands of images using a consistent metric. Based on a limited manual evaluation of piloting data, CLIP appears to capture similarity and dissimilarity well enough for our purposes, both in terms of the objects and style of an image. Importantly, we do not use embedding vector similarity as a measure of creativity or artistic skill, but rather as a way to measure behavioral outcomes in a task that is adjacent to or emulates artistic practice (although not entirely ecologically valid, as is typical for most, if not all, such experiments).\nWhile not the main object of study, we also measured several summary statistics to gain some insight into the data: image colorfulness, complexity, and prompt length (in characters). For color, we used the \"M3\" measure from Hasler and Suesstrunk (2003), and for complexity the file size of the PNG compressed image. Compression is a well-known estimate of image complexity that aligns with human perceptions (Machado et al. 2015; Chamorro-Posada 2016; Karjus et al. 2023). We find no notable differences between groups along these axes. We use self-reported Al usage experience levels as a control variable in the Results section; artist participants were more experienced (mean 1.68) than laypeople (1.04, on a scale of 0 to 3). A graph visualizing all the aforementioned metadata and summary variables is found in the Supplementary Information.\nWe also wanted to make sure the creative task could not be solved successfully by simply filling the prompt with random words or gibberish, which might conceivably lead to images different from the reference. We ran a small simulation to test that, by producing a set of creative task prompts of maximal length (including the fixed prefixes) out of random words sampled from an English word list, and another set consisting of pseudo-words of randomly sampled letters, and generating new images based on those. We found that while it is possible to occasionally \"get lucky\" with this strategy, adding random things to a prefixed prompt generally leads to images still close to the reference image, and the simulated results were on average far worse than the real data (see SI for details)."}, {"title": "Bringing in an Al", "content": "Inspired by the growing literature on comparing humans and various Al agents discussed in the Introduction, we also carried out a small comparison with an \"AI\", the vision-capable large language model GPT-40 by OpenAI (specifically gpt-40-2024-08-06, via its API service). The LLM was given roughly the same instructions as the human participants in the experiments, with added context that was otherwise implicit in the task interface (such as the word limit), and general guidance to act in the role of a creative professional. It was instructed to write a prompt of up to 30 words just like human participants. Any words above the limit would be clipped to ensure comparability with the rest of the experiment (there were only 3 such occasions, however). In short, we created something akin to a simple image evaluation and generation agent, consisting of three models: the LLM interpreting the input and producing the prompt, the image generating SD, and the CLIP model yielding embeddings for our goal of image comparison. Unlike participants, the LLM here was not granted a \"memory\" of past completed attempts. While in principle achievable by feeding previous inputs and outputs into the context window of the model, it is unclear to what extent this would be comparable to human memory and is not explored here.\nThe procedure was as follows: for each trial, the input was the reference image and the instructions to carry out the given task, e.g., to write a prompt that would generate an image like the reference forest image. The model was set to generate 10 output variations on each trial (by setting the relevant parameter in the API). The prompts were subsequently entered into the same image generator used in the experiment, with the same parameters, to produce images that could then each be compared to the reference, as described above. Here the goal was neither model comparison nor parameter space exploration; the relevant parameter to set was temperature, which governs token sampling in LLMs. O temperature means the model always picks the most likely next word to generate (precluding generating variations from the same input), while higher values lead to more stochastic and unexpected outputs, and maximal values can end up generating nonsense. We set it at 0.7 (on a scale of 0 to 2) for the copying task, to sample variation while still being relatively precise, and 1.5 for the creative task, simply following Bellemare-Pepin et al. (2024) who observed that higher temperatures indeed help LLMs in creative association and semantics tasks."}, {"title": "Results", "content": "We use mixed effects generalized linear regression to analyze the differences between the outputs of our two groups, in the following form: cosine group + experience + (1|subject) + (1|item). In both tasks, the dependent variable is the cosine similarity between the original and generated image. The main variable of interest is the group: the coefficient value of this shows how much the groups differ in terms of outcomes. We also control for participant background by fitting a fixed effect for (self-reported, numeric) prior Al usage experience, and fit random intercepts for stimuli items and participants to account for individual-level variation. As discussed above, we use the averaged distance of the four generated variants as the unit of data here, yielding 787 cases.\nWe find support for both hypotheses 1 and 2. In the copying task, the estimated cosine similarity of laypeople is -0.03 lower than that of artists (the 95% confidence interval is between [-0.04, -0.01]). The effect is significant (at \u03b1 = 0.05), as determined via a likelihood ratio test comparing the full model to a partial one without the group variable (\u03c7\u00b2 = 11, p < 0.001). In the creative task, laypeople are 0.02 [0.002, 0.04] closer to the reference than artists here the goal is to be distant, so the artists' result is superior (\u03c7\u00b2 = 4.56, p = 0.03). The coefficient confidence intervals were estimated using bootstrapping with 1000 model replicates. In the latter task, the lower bound only narrowly excludes zero. In summary, while narrowly statistically significant, these are not very large differences, as also visible in Figure 2.A. The reference images of both tasks and the top results are visualized in Figure 3."}, {"title": "Curation effects", "content": "The experiment combined both artistic image creation and curation, as participants were presented with four variant images to choose from after every generation trial. We can therefore assess the effect of curation as well. We constructed two sets of models to do so. Here we excluded the few trials that produced NSFW content, as the participants were not given a chance to curate those (blank images were displayed), leaving 758 cases. In short, we found no differential effect of curation between groups. First, we ran the same mixed effects linear models as above on this dataset reflecting curation choices. For both tasks, it resulted in similarly small differences: laypeople were more distant than artists in copying (-0.03[-0.04, -0.01], p = 0.001) and more similar in the creative task (0.026 [0.003, 0.05], p = 0.02]), narrowly supporting H3 and H4.\nTo test H5, we also ran cumulative link mixed models (with logit link function), again also controlling for prior experience and using the same random effect structure. The dependent is a 4-level ordinal variable, reflecting the rank of the participant's choice in terms of the best option among the four, as measured by CLIP similarity (where 1 is the furthest, 4 is the closest). However, the p-values for the group variable were well above 0.05 in both the copying and creative tasks (0.7 and 0.4; and bootstrapped coefficient confidence intervals span 0), indicating no discernible difference between the abilities of artists and laypeople when it comes to choosing between image alternatives. This is of course a very limited emulation of curation, and in retrospect, most variant images were already highly similar, leaving little room for potential differences in curation skills to be expressed. A dedicated curation experiment with more dissimilar choices would have likely provided better insights and could be pursued in the future."}, {"title": "Comparing to Al performance", "content": "We also explore an additional comparison to GPT-40, one of the frontier multimodal LLMs currently also powering the popular ChatGPT chatbot service. The statistical modeling solution here is less complex, as there is only one \"participant\" in the GPT group, and there is no comparable experience variable for a machine. We therefore fit a simple fixed-effects linear regression model, where the group variable now contains the GPT, set as the reference level (we fitted a random effects model as well which yielded very similar results). As in the first comparison, we average the variant image cosine similarities for each trial. In the copying task, there is no significant difference between GPT and artists (p = 0.11). Laypeople are an estimated -0.04 further from the reference (CI = [\u22120.06, -0.02], p < 0.001; model adjusted R2 = 0.03, F(2, 429) = 8.03, p < 0.001), meaning GPT did better here. In creative, where the goal is to have a lower value, both laypeople (\u03b2 = 0.07[0.04, 0.1], p < 0.001) and artists (\u03b2 = 0.05[0.02, 0.08], p = 0.002) are on average higher than GPT (R2 = 0.05, F(2,432) = 11.94, p < 0.001). This means GPT-40 was able to write prompts that led to on average more creative visual results than both human groups, within the narrow definition of the task. The variance described is very low in both models, however, less than 5%, corresponding to the rather small absolute differences, and the top results in each task are still achieved by the most successful human participants, mirroring results by Koivisto and Grassini (2023). Even if the advantage for GPT is small, the perhaps interesting takeaway here is that many professional artists with years of training and experience, not to mention laypeople, were not able to score much better in this (admittedly narrow) task than a simply next-word-predicting large language model.\nArtists collectively generated slightly more diversity or variability in the creative task than both laypeople and GPT-40. We calculated this by first measuring variation in the images and then fitting a linear mixed-effects model. Variation is implemented as the cosine similarity of each image embedding vector to its group centroid (average) vector, computed for each trial image and group (as images differ, and we mean to compare groups). This is analogous to mean absolute deviation or MAD. The regression then measures the effect of the group on the dependent variable of the variation metric (with random effects for group and trial reference image). The unit is still cosine, so higher values here indicate closer proximity to the center, i.e. lower variability. Laypeople (\u03b2 = 0.02, CI = [0.003, 0.03]) and GPT-40 (\u03b2 = 0.05, [0.001, 0.11]) have both higher cosine, i.e. lower collective diversity than the reference level of artists (model p = 0.01 compared to reduced model). The coefficients are again quite small (for reference, see the vertical axis of Figure 2 which is on the same cosine similarity scale).\nAnother possible approach to measure diversity would be within participants: how much the four images (and their variants) differ from each other on average. In the creative task, some participants (and GPT) used similar strategies for all four trials, while others varied (see the example image sets in the SI for intuition). We measure this as the average of all pairwise embedding similarities within a given participant. For GPT, we simulate subjects by replicating the dataset 10 times, randomly recombining the aforementioned 10 image prompt-variant sets (keeping the output images of a prompt intact). Here the statistical model is just simple regression, as trials and participants are already averaged. The artists have an estimated average intra-similarity of 0.63 (still on the same cosine scale as above), and laypeople do not significantly differ (p = 0.91), while GPT yields higher estimated similarity, i.e. less diversity than artists (\u03b2 = 0.07, p < 0.001, model F(2, 196) = 155.8, p < 0.001).\nIt should be noted that the LLM \"participant\" was at a disadvantage here in terms of diverse outputs. We used only a single instruction prompt per task (see Supplementary), so the inputs differed only in terms of the input image, and in the case of the creative task, the explanation of the fixed prefix. This being a generative model highly sensitive to the output, using a larger array of different instruction prompts may well have increased output variance as well (e.g. by prompting for various types of artists, different styles, and approaches to undertaking such tasks)."}, {"title": "Exploring the results: how do people (and machines) use an image generator?", "content": "Figure 3 displays the most successful generations in the two tasks. For the copying task, precise and relevant wording naturally worked the best, e.g. the top result for the jungle image was prompted as \"a painting of an idyllic landscape, lake, mountains, waterfalls, palm trees, matte painting, detailed, tropical, jungle, inspired by mark keathley\". Some humorous prompts but still worked fairly well, for example, \"a bad Cezanne style painting of green and red apples in a baskek with ceramic tiles behind it\" resulted in an average 0.93 similarity (the typo may have affected the outcome). While the copying task had participants mostly just trying to describe the scenes as best they could, the creative task elicited various interesting strategies.\nSome participants either knew or realized that text to image generators can be confused by compounds. Both of these prompts made"}]}