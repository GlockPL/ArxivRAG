{"title": "Is Our Chatbot Telling Lies? Assessing Correctness\nof an LLM-based Dutch Support Chatbot", "authors": ["Herman Lassche", "Michiel Overeem", "Ayushi Rastogi"], "abstract": "Companies support their customers using live chats\nand chatbots to gain their loyalty. AFAS is a Dutch company\naiming to leverage the opportunity large language models (LLMs)\noffer to answer customer queries with minimal to no input from\nits customer support team. Adding to its complexity, it is unclear\nwhat makes a response correct, and that too in Dutch. Further,\nwith minimal data available for training, the challenge is to\nidentify whether an answer generated by a large language model\nis correct and do it on the fly.\nThis study is the first to define the correctness of a response\nbased on how the support team at AFAS makes decisions. It lever-\nages literature on natural language generation and automated\nanswer grading systems to automate the decision-making of the\ncustomer support team. We investigated questions requiring a\nbinary response (e.g., Would it be possible to adjust tax rates\nmanually?) or instructions (e.g., How would I adjust tax rate\nmanually?) to test how close our automated approach reaches\nsupport rating. Our approach can identify wrong messages in\n55% of the cases. This work shows the viability of automatically\nassessing when our chatbot tell lies.", "sections": [{"title": "I. INTRODUCTION", "content": "Companies value their customers [2] and strive to create\na great customer experience [3]. Customers, in turn, assess\na company on its core business and customer service, which\ninfluences their trust, loyalty, and satisfaction [4]. Today, the\nmost popular way to assist customers online is via chatbots\nand live chats [2], [5], [6]. Real-time communication in live\nchats means quick answers to questions [2], [6], which helps\nbuild loyalty [5] and encourages customers to return when they\nneed assistance [6].\nWith recent advancements in Large Language Models\n(LLM) that enable natural and chat-like communication [9],\n[10], [12], AFAS sees an opportunity to provide live support.\nThe first part of Figure 1 represents the current situation. When\na customer raises an issue, an employee forwards the question\nto the chatbot along with relevant documents and instructions,\nalso called a system prompt. When the LLMs generate an\nanswer based on the information provided (details in II), the\nsupport employee checks the answer and forwards it to the\ncustomer if correct."}, {"title": "II. BACKGROUND", "content": "AFAS is a software company based in the Netherlands\nspecializing in automating business processes. More than 3\nmillion users have utilized their software, leading to almost\n112,000 support inquiries in 2023.\u00b9 Addressing these support\nqueries demands a lot of time and commitment from the 70-\nmember support team. Since the support team receives nearly\n1 query every minute about their software, saving time on even\na subset of the queries will be helpful.\nThe chatbot of AFAS uses Retrieval Augmented Generation\n(or RAG) to improve the performance of the language model.\nRAG comprises four parts [21]:\n\u2022 Indexing - Creates an index of all documents containing\nrelevant information for a user. This step is done before\nthe chatbot serves any answers. At the time of writing,\nAFAS indexes help documents.\n\u2022 Search - Relevant documents are retrieved based on\ntheir similarity to the user message using embedding sim-\nilarity [22] and keyword-matching techniques [23], [24].\n\u2022 Prompting - This step combines the user message, rele-\nvant documents, and system prompt as a single message.\nThe system prompt includes instructions and basic in-\nformation, such as 'be friendly' and contact information.\nSince the chatbot relies on an LLM that is not fine-tuned\non company data but is a generally trained model, the\nLLM requires necessary information to provide relevant\nresponses. Relevant documents provide this information\nto the LLM. This is shown as Question + Context from\nChatbot to LLM in Figure 1.\n\u2022 Inference - The question + context is used to prompt an\nLLM, and the generated response is shown to the support\nteam. This is depicted as the answer from the LLM to\nthe chatbot in Figure 1.\nWe envision the chatbot to handle all kinds of questions,\nconsidering the unique jargon of the company/industry and the\nfact that it mainly serves Dutch users. Here, reference answers\nmay not help since they are sparse and do not ensure a right\nresponse to unseen use cases. Given these constraints, there is\na need for a generic definition of what makes a right answer\nand how to measure it."}, {"title": "III. WHAT MAKES A RIGHT ANSWER?", "content": "There are two ways to assess whether a chatbot gives right\nanswers: turn- and dialogue-level metrics. Turn-level metrics\nrate a single message-answer pair [14], [15], [19], [20], [25]\u2013\n[30]. In contrast, dialogue-level metrics rate the full dialogue,\nincluding all message-answer pairs [31]-[35]. Our objective is\nto assess the correctness of each answer and, hence, turn-level.\nTo the best of our knowledge, no prior work defines correct-\nness or not clear enough [20] for measurement and validation.\nTherefore, our first objective was to define correctness. To\ndefine correctness, we followed a two-pronged approach. First,\nwe looked at 500 chatbot responses for which the support\nteam provided a decision: accept or reject and a short justifi-\ncation for rejection. Further, the first author shadowed [36] an\nexperienced support employee throughout one workday and\ninteractively discussed the decision-making. The referenced\nemployee has 12 years of experience in various product\nsupport areas of the software, has been using chatbot since\nits launch, and has necessary training (e.g., a course on using\nChatGPT). During the discussions, the employee discussed\nspecial cases where the chatbot did not meet expectations and\nwhat modifications were required to ensure the answer was\ncorrect before presenting it to the customer.\nBased on shadowing, discussion, and analysis of rejection\nreports, the three most common mistakes and, hence, require-"}, {"title": "A. Data Gathering", "content": "Currently, the developed chatbot of AFAS is utilized by\nthe support team as an assistant. They use the bot to answer\nquestions they have themselves or to answer the questions of\na customer. Based on their expert knowledge, they rate the\nanswer of the chatbot for truthfulness. They are encouraged\nto rate messages, but may choose which messages to rate\nby themselves. Consequently only a few messages are rated\neach day, this challenge is posed as we work with a real-\ncase company scenario. The rating is along a Likert scale,\nwhich ranges from (1) very untrue to (5) very true. Likert\nscales are commonly used in similar research [39]. While the\nsize of the scale is a topic of debate, a 5-point Likert scale is\nmost commonly used [40]\u2013[42]. During the research, the team\ncontinued to use the chatbot, which meant that new feedback\nwas continually being received.\nDuring the study, the plugin to rate truthfulness is imple-\nmented. A few weeks after the rating option is introduced,\nthe data is extracted from the system to form an analysis\nset, consisting of 79 samples. A few weeks later, a test set\nis extracted, containing 154 message-answer pairs. For each\nrated message-answer pair, the context (relevant documents +\nsystem prompt) is gathered as well. Finally, to test whether\nEnglish text performs better than Dutch, we translate the data\nusing Google Translator2. Since our raters frequently rate\nan answer as either fully true or fully untrue, the dataset\nbecomes imbalanced, see Table I. Consequently, we focus less\non overall accuracy and more on the accuracy of detecting 1-\nstar and 5-star rated messages. These messages have a larger\nnumber and are of greater interest to the company. Since\nfully untrue answers must not be sent to users, and fully true\nanswers can be sent without human intervention."}, {"title": "B. Constructing the Tree", "content": "To ensure that our metric correlates with human ratings, it is\nessential to understand how a human determines their rating. In\norder to represent the thought process of human annotators, we\nsought a model that could be easily visualized and understood.\nFurthermore, we sought a method to systematically organize\nthe analysis and maintain a record of the observed characteris-\ntics. We opted for a decision tree to depict the mental process,\nwith each node symbolizing the subconscious decisions made\nby the annotator. We use a manual created tree, as our goal is to\nmimic the human workflow. If we use automated decision tree\nbuilders like Random Forests [43] or C4.5 [44], they would\ncreate their own reflection and would not reflect the human\nworkflow."}, {"title": "C. Deriving Heuristics from the Decision Tree", "content": "The constructed decision tree represents the mental model\nof a human annotator. To transform this model into automated\nfeatures, we need to infer the heuristics that can be extracted\nfrom the nodes in the decision tree.\n1) Message Types: In the decision tree, different message\ntypes follow distinct paths. For example, if a user requests\nan email translation, an error is unlikely due to conflicting\ninformation in the response.\nSince the type of message impacts the nature of the re-\nsponse, it is essential to identify the various message types.\nThis way, we can examine how truthfulness can be assessed\nfor each type. As far as we are aware, there is no existing\nclassification of user message types, except those related\nto financial inquiries [19]. In this study, the authors utilize\nmessage type to tailor an LLM prompt to the financial objec-\ntive of the question. They opted to generate question labels\nwith particular intentions. On the other hand, our taxonomy\nis crafted to be more general, not delving into the precise\nnature of financial questions but rather focusing on the type\nof information sought. This enables us to be adaptable to\npreviously unseen questions.\nWe analyzed a random sub-sample of approximately 300\nmessages sent to the chatbot to create the classification.\nThese messages are message-answer pairs that were not rated,\nensuring they are not subject to selection bias by the annotators\n[49]. Based on this analysis, we identified seven types of user\nmessages sent by the support team. These types are derived\nfrom the decision tree and those observed in the random\nsubset. They are: (From now on, the underlined names will\nrefer to these types.)\n1) Error resolution E.g., I get the error: mutation cannot\nbe executed\n2) Binary answer E.g., Would it be possible to adjust tax\nrates manually?\n3) Instruction E.g., How would I adjust tax rates manually?\n4) Cause and effect reasoning E.g., I have adjusted tax\nsettings, why don't I see a payslip anymore?"}, {"title": "V. FROM HUMAN TO AUTOMATED ASSESSMENT", "content": "First, we explore features to predict message type. Then, we\nsolicit features tailored to measure the truthfulness of Binary\nand Instruction-type messages. Finally, we curate features to\ngenerate a score.\nA. Identify Type of User Message\nDue to the limited number of messages and the presence of\n7 different message types, only a few samples per class are\navailable. E.g. only 5 for type Error, see Figure 3. While it is\ncommon to train a machine learning model for classification"}, {"title": "B. Feature Selection", "content": "With truthfulness defined and the heuristics for a true\nanswer established, the next step is transforming heuristics into\nautomated features. Note that LLM not having the knowledge\nabout the product is a key factor in this research. As the\nexternal trained model has no knowledge about the company,\nall info mentioned should be present in the context, otherwise\nit is hallucinating [19]. Therefore, we add verifying whether\nthe content is contained within the context to the heuristics.\nTo translate the heuristics to automated features, we use\nliterature to find features to automate this. To find relevant\npapers, the words from the following non-exhaustive list\nare combined: metric, correctness, score, evaluate, chatbot,\nconversational bot, conversational agent, hallucination, char-\nacteristics, wrong, correct, NLP, education, grading, measure,\nautomated. Using the word combinations, papers are found\nutilizing various scientific databanks including ResearchGate\u00b3,\nGoogle Scholar\u2074, and IEEE Xplores. Additionally, relevant\npapers are used for snowballing, both backward and forward.\nAbout 70 relevant papers are identified, which either define\na non-automated metric for chatbots or an automatic metric\nfor general text grading. We identified two relevant fields with\nrelated features: automated metrics for chatbots and automated\ngrading of student answers. A full list is constructed with\npotential relevant features, see the replication package [1] for\nall identified features.\nThe full list is compared to the heuristics, if a feature cannot\nreplace any heuristic due to insufficient overlap in working,\nit is excluded. Next, we implement these filtered features,\nfeatures inspired on literature and features for heuristics that\nare not covered by any existing feature.\nEach of the implemented features is assessed for Spearman\ncorrelation [58] with human evaluation. If a feature has a\npositive correlation and a p-value lower than 0.10, it is selected\nfor the final selection. While a correlation of 0.10 is not\nstatistically significant it is deemed sufficient, as the actual\nselection will take place during the final selection. If a feature\ndoesn't meet the correlation criteria but has demonstrated\neffectiveness in relevant literature, it is tested if it distinguishes\nbetween true and untrue answers. If it can differentiate at least\nsome of these answers, it is selected for the final selection."}, {"title": "B. Features to Score", "content": "To go from these features to a score, all features are normal-\nized between 0 and 1. Following the method of Roychowdhury\net al. [19], we sum the features together. We normalize the sum\nbetween 1 and 5, adhering to the rating of human annotators.\nIn this approach, each feature is given equal weight. How-\never, the decision tree reveals that an answer is untrue anyway\nif it contains non-existing components, leading us to define a\nsecond score where answers with such components receive\na 1-star rating. In addition, the analysis teaches that if a\nmessage contains a verbatim guide, the answer is anyway true.\nTherefore, if an answer contains such a guide, it is rated with\n5."}, {"title": "VI. EVALUATION", "content": "Our test set is obtained from the feedback system a few\nweeks after the extraction of the analysis set. As the bot is\ndeveloped during that time, the answers given by the bot also\nchanged.\nA. Method of Evaluation\nWe evaluate both message type prediction and score pre-\ndiction. The prediction is measured using F1 score, precision,\nrecall, and accuracy, which show how often the label is\npredicted correctly. The main goal is to label messages as"}, {"title": "1) Message Type Prediction", "content": "Table III shows the evaluation\nof the type prediction for both, analysis and test set. The\noverall performance is shown, along with the predictions for\nthe types Binary and Instruction. All patterns utilized in the\nprediction are derived solely from the analysis set. This may\nresult in many messages in the test set not matching any\npattern; however, only 18% of the test set is labeled as un-\nspecified, suggesting that at most 18% of the messages contain"}, {"title": "2) Overall", "content": "The overall performance indicates that the\nDutch features have been tailored and optimized for the\nanalysis set, as they exhibit better performance on this set\ncompared to the English features and the Test set. However, this is not the case for the translated\nversion, where the test set outperforms the analysis set. All\nof the features are fine-tuned on the Dutch data, therefore\nit is possibly overfitted. For the test set, it concludes that\nthe translation has a positive effect on the prediction. This\nindicates that the external packages used work better with\nEnglish than with Dutch text.\nTo contextualize the correlation of our metric, we refer to\nthe most comparable metric in terms of definition discovered\nin the literature, developed by Mehri et al. [20]. They evaluate\nanswers based on correctness and achieve a correlation of 0.13.\nNotably, the Dutch and English versions exceed this result,\nwith correlations of 0.28 and 0.37, respectively."}, {"title": "3) Deviations", "content": "To assess the accuracy and deviation, see\nTable V. It performs especially well in rating the high and\nlow rated messages, not the neutral. Additionally, Table V"}, {"title": "4) Scenarios not working", "content": "To identify the mistakes made\nby the chatbot and guide future work, scores that deviate much\nfrom the ground truth are analyzed. First, a downside of using\nREGEX is that it can miss some components or extract unre-\nlated text. E.g\u201dThe salary button should be clicked\u201d Instead of\nexpected: \"Click on: salary\". Another recurring problem is the\nambiguity when detecting the heuristic Off-Context. Generally,\nan answer may be correct, but not for a specific exception. E.g.\nUser asks a question about a Nurse organization, however for\nthese organization different laws hold true. Third, relatedness\nand completeness influence the rating alongside truthfulness,\neven though the ranking is distinct for each dimension. Lastly,\nsometimes the LLM hallucinates correctly, when it is based\non sparse information in the context. E.g. Q: \"What if I click\nthe salary button\" A:\"It shows a salary overview\". Although\nnot explicitly stated in the context, the name of the button\neffectively inspires this correct hallucination."}, {"title": "VII. THREATS TO VALIDITY", "content": "Internal Validity includes threats related to the methods\nand processes of the study. First, at AFAS, as we developed\nthe metric, the chatbot also evolved. The implication is that\nour data changes over time, and our analysis set differs\nfrom the test set. We do not see this as a problem but\nbelieve that this ensures our results are transferable across the\nevolving chatbot configurations. Second, our approach hinges\non contextual information derived from help documents. Any\nmissing, outdated, or incorrect information relating to the\ncontext implies incorrect validation in practice. This threat is"}, {"title": "VIII. DISCUSSION", "content": "The customer support team is on a challenging mission to\naccurately and efficiently respond to all kinds of customer\nqueries. To support the customer support team, we started on\na quest to identify the right answer to a customer query. We\nfound that answer correctness is fundamental to a right answer\nand can be measured in terms of the truthfulness, relatedness,\nand completeness of the answer.\nLearning from the common mistakes to a correct answer\nidentified in our research, the support team now has a better\nunderstanding of how to process incoming messages and the\nimportance of crafting questions. As a result, the support team\nat AFAS revisited the reformulation of the questions to get the\ncorrect answer from the chatbot. Further, while our approach\nis not without its flaws, AFAS has integrated our solution\ninto their system and is testing whether some answers can\nbe directly sent to the user. Even with detecting only 28%\nof the 5-star rated messages, as is currently reported in our\nwork, we anticipate freeing up to 15,000 hours per year so\nthat the support staff can work on more challenging queries\nand provide near real-time answers.\nThis study measured the truthfulness of a response gener-\nated by a Dutch-support chatbot using generic metrics, which\ncan apply to companies other than AFAS. Even otherwise,\nour methodology of translating the human decision-making\nprocess to heuristics and then metrics can be used to identify\ndifferent aspects of correctness.\nA notable observation of this study is that the type of user\nmessage influences the nature of mistakes. While there were\ninitial signs of this observation in literature [19], our process\nof translating how humans assess the correctness of a response\nto metrics makes it apparent. Since most message types follow\nspecific patterns to construct questions, it is possible to apply\na subset of heuristics (instead of all) to assess its truthfulness.\nOur approach shows the viability of using custom features\nwhen the data is sparse and insufficient for training machine\nlearning models. It allows us to remove specific mistakes using\ncustom features. Further, our approach eliminates the need for\na reference answer. This is possible since the generated answer\nis checked against the context provided by the help document."}, {"title": "IX. RELATED WORK", "content": "As our research assess the correctness of content, two\nrelevant fields are identified: Natural Language Generation\n(NLG) and Automated Answer Grading.\na) Natural Language Generation: In previous studies of\nNLG, human evaluation is often used [34], [39], [68], [69].\nTo date, several studies have investigated how the human\nevaluation can be automated. Focusing on quality of dialogue\n[31]\u2013[33], or on a single messsage-answer pair. Our study can\nbe classified under the latter, so called turn-level metrics. Much\nof the literature on turn-level metrics is focused on comparing\nembeddings of text [14], [15], [18], [27]. It either test rele-\nvancy [15], [18] or improvement [14], [27]. Improvement is\nmeasured along linguistic features like readability, syntactic\nstyle and complexity [27]. There are relatively few studies in\nthe area of content-specific aspects. The existing research is"}, {"title": "X. CONCLUSIONS", "content": "To improve customer experience and enable the support\nteam to answer customer queries faster, we embarked on a\njourney to assess the correctness of answers generated by\nDutch support chatbot AFAS. The support team at AFAS\nplayed a crucial role in this process, especially considering\nthe complexity of our task - the text was in Dutch, and we\nhad sparse data for training, meaning rated answers were only\nlimited available. We proposed metrics inferred from how the\nsupport team at AFAS assesses correctness. These metrics\nlook at user messages, help documentation at AFAS to infer\ncorrectness, and are generic to assess unseen situations.\nOur results inspires how the support team queries the\nchatbot. Further, we anticipate a gain of up to 15,000 hours\nper year from adopting this system. This study offers rec-\nommendations, such as how other companies can assess the\ncorrectness of their chatbot."}]}