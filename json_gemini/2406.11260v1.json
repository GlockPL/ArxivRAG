{"title": "Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection", "authors": ["Sungwon Park", "Sungwon Han", "Meeyoung Cha"], "abstract": "The spread of fake news negatively impacts individuals and is regarded as a significant social challenge that needs to be addressed. A number of algorithmic and insightful features have been identified for detecting fake news. However, with the recent LLMs and their advanced generation capabilities, many of the detectable features (e.g., style-conversion attacks) can be altered, making it more challenging to distinguish from real news. This study proposes adversarial style augmentation, AdStyle, to train a fake news detector that remains robust against various style-conversion attacks. Our model's key mechanism is the careful use of LLMs to automatically generate a diverse yet coherent range of style-conversion attack prompts. This improves the generation of prompts that are particularly difficult for the detector to handle. Experiments show that our augmentation strategy improves robustness and detection performance when tested on fake news benchmark datasets.", "sections": [{"title": "Introduction", "content": "With the widespread use of the internet and the emergence of various social media platforms, people have begun to freely share information they know and their own creative stories. However, the ease of sharing and consuming information beyond traditional news organizations has also significantly contributed to the spread of fake news (Cha et al., 2021; Fisher et al., 2016; Treen et al., 2020). Fake news is created to provide readers with false information as propaganda or to guide public perception in a desired direction for intentional objectives (Lazer et al., 2018; Yi et al., 2024). The spread of such false information has had profound negative impacts on individuals and society, becoming a major social challenge that needs to be addressed.\nManually determining the authenticity of all information on the internet is extremely costly in terms of time and resources. Thus, prior ad- vances have mainly focused on developing auto- mated fake news detectors using machine learning techniques (P\u00e9rez-Rosas et al., 2018; Reis et al., 2019; Shu et al., 2019). For example, several studies have extracted sentiment-related or polit- ical features commonly found in fake news, ei- ther through human-crafted (Potthast et al., 2018; Rashkin et al., 2017) or data-driven methods (Reis et al., 2019; Shu et al., 2019), and used them for detection. With the advent of language models (e.g., GPT, BERT) (Kenton and Toutanova, 2019; Radford et al., 2019), efforts have been made to learn the textual style of fake news by extracting sentence embeddings to train detectors (Chen et al., 2021; Yang et al., 2022).\nWhile these extracted textual styles or human- crafted features have proven effective in identifying fake news, they allow attackers to bypass detection. In particular, the advent of large language models (LLMs) (Chowdhery et al., 2022; OpenAI, 2023) has made it possible to easily and automatically paraphrase sentences in a user-desired direction through prompts (e.g., \"change the given text to an objective and professional style\"), making it more difficult to distinguish between AI-generated fake news and real news (i.e., style-conversion attacks) (Koenders et al., 2021; Zhou et al., 2019). Recent literature has proposed strategies to aug- ment the styles of input text via some manually de- fined style-conversion prompts, which are taken by LLM for paraphrasing, but it remains challenging to address all types of prompts that attackers might use to deceive detectors (Wu and Hooi, 2023).\nIn this study, we propose adversarial style aug- mentation, AdStyle, to train a robust fake news de- tector that can withstand various style-conversion attacks by attackers. In contrast to earlier work, which used predefined style-conversion prompts as detector-agnostic augmentations, AdStyle tries to find prompts for adversarial style augmentations that are specific to the detector, which makes"}, {"title": "Related Work", "content": "This means that our augmentations add noise to the style features in the direction of the detector's decision boundary, while maintaining the text's content integrity. Specifically, to search for and optimize prompts for LLM that are adversarial to the detector, we introduce an automated prompt engineering technique (Yang et al., 2023). By providing LLMs with style- conversion prompts and the detector's performance under these augmentations, LLM can infer patterns between the prompts and performance, enabling the search for the most adversarial prompts.\nAdStyle proceeds as follows: To generate aug- mented samples, we first select a random subset of the dataset and apply a pool of style-conversion prompts to it. Then, each augmentation set created by different prompts is fed into the detector, and the score based on the AUC between the predictions and the ground-truth labels is measured. The prompt-score pairs are provided to the LLM, which uses this information to generate new style-conversion prompt candidates. From these candidates, we select the top-k prompts that are diverse and make the detector's predictions most uncertain without significantly altering the original content. These selected prompts are applied to the entire dataset and used for detector training. The chosen prompts are added to the prompt pool, which is repeated over several training rounds.\nWe experimented with existing fake news bench- mark datasets under various style-conversion at- tack scenarios. As a result, our augmentation strat- egy demonstrated higher robustness and detection performance compared to previous methodologies. Additionally, our approach is scalable to different attack strategies by adding new attack prompts to the style-conversion prompt pool. We plan to re- lease our code after publication."}, {"title": "Automated Detection of Fake News", "content": "Manually detecting fake news among the vast amount of content on the internet is intractable. Consequently, many studies have focused on lever- aging machine learning techniques to automatically detect fake news (P\u00e9rez-Rosas et al., 2018; Reis et al., 2019; Shu et al., 2019). For example, su- pervised learning methodologies extract textual features from fake news texts using benchmark datasets and ground-truth labels (Reis et al., 2019). These textual features can include deep features based on artificial neural networks (Reis et al., 2019; Shu et al., 2019) as well as manually defined features such as sentiment and political bias (Pot- thast et al., 2018; Rashkin et al., 2017). Other ap- proaches include domain adaptation methods to en- hance detection generalizability across various do- mains and topics (Mosallanezhad et al., 2022; Nan et al., 2022), and knowledge-based methods that rely on external data to distinguish false informa- tion (Dun et al., 2021). With the advent of LLMs, new methods have emerged that utilize LLMs' prior knowledge to identify fake news (Hu et al., 2024), including research on detecting machine- generated fake news (Chen and Shu, 2023).\nIn this work, we aim to prevent malicious attackers from deceiving automated fake news detectors through text modifications such as para- phrasing. The proposed method is an augmentation strategy to enhance robustness against text style perturbations as an add-on to existing detection models. Our contribution is agnostic to the design of the detection model."}, {"title": "Attack on Fake News Detection", "content": "To test the robustness of fake news detection, var- ious attack methods have been studied (Koenders et al., 2021; Zhou et al., 2019). These include in- jecting misinformation by changing the order of subjects and objects or causes and effects while maintaining the textual features used for detec- tion (Koenders et al., 2021). Other methods involve creating fact distortions by altering or exaggerating words related to people, time, or places while pre- serving the sentence structure (Zhou et al., 2019). Additionally, approaches that use the text genera- tion capabilities of LLMs to change the style of sen- tences have been proposed (Wu and Hooi, 2023).\nOur method aims to perform adversarial training of the detection model by generating augmentations that perturb the text's style while preserving the content as much as possible. This approach reduces the impact of spurious style features on the detection model, making it more robust against such attacks."}, {"title": "Prompt Engineering for LLM", "content": "Training with extensive text corpora, LLMs have demonstrated their utility across various domain tasks (Chowdhery et al., 2022; OpenAI, 2023; Pang et al., 2024). To better harness the prior knowl- edge and reasoning abilities of LLMs, strategies for providing appropriate input prompts have also"}, {"title": "Method", "content": "Let $D = \\{(d_i, y_i)\\}_{i=1}^n$ be a dataset containing news $d_i$ and the corresponding ground-truth binary veracity label $y_i$ (indicating whether the news is true or fake). Each news item $d_i$ is composed of natural language-based text. This study aims to train a language model based fake news detector $f$ using the labeled dataset to predict the veracity labels. Our main goal is to develop a detector that remains robust even when an attacker perturbs the textual style, such as the order and format, while preserving the meaning of the sentences."}, {"title": "Generating Adversarial Style-Conversion Prompts with LLM", "content": "The style-conversion prompts we aim to generate are instructions that perturb only the textual style,"}, {"title": "Selecting Top-k Adversarial Prompts", "content": "Using all the style-conversion prompt candidates generated by the LLM can be computationally in- In the first round, a predefined set of prompts is used."}, {"title": "Experiment", "content": "We evaluate the robustness of AdStyle under diverse style-conversion attacks across multiple datasets, comparing it with contemporary baselines. Then, we analyze the impact of model components on overall performance. Lastly, we conducted a qualitative analysis to investigate the characteris- tics of the style-conversion prompts generated by the LLM. Due to limited space, additional analysis, such as the performance evaluation over wider variety of paraphrasing attacks and comparisons with LLM-based zero-shot and in-context learning baselines, has been included in the Appendix."}, {"title": "Performance Evaluation", "content": "Our experiments use three real-world fake news benchmark datasets. We utilize Politi- Fact and Gossipcop, drawn from the FakeNewsNet benchmark (Shu et al., 2020), which focus on politi- cal claims and celebrity rumors, respectively. Addi- tionally, we incorporate Constraint (Felber, 2021), a dataset specifically addressing COVID-19 related social media posts. Each dataset is randomly split into an 80% training set and a 20% test set. De- tailed dataset statistics are provided in Table 3 of the Appendix."}, {"title": "Attack settings.", "content": "To assess robustness against style conversion attacks, we employ LLM- empowered techniques to reframe the test set using a variety of style conversion prompts, as illustrated in Figure 3. Following the original literature (Wu and Hooi, 2023), we use four well-known daily news sources as [publisher name]: CNN, The New York Times, The Sun, and National Enquirer. CNN and The New York Times are recognized"}, {"title": "In-Depth Performance Analysis", "content": "We have demonstrated AdStyle's effectiveness. In this section, we further examine the contribution of each component and provide a qualitative analysis on our adversarial style-conversion prompts."}, {"title": "Ablation study.", "content": "The proposed method integrates two main modules: adversarial style-conversion prompts generation and selection. To evaluate their individual contributions, we conduct experiments where we either remove each component or substi- tuted it with an alternative within the full model. This results in six distinct configurations for analy- sis: (1) Full Components: Our complete method with all components; (2) Random Selection: The method that randomly select prompts from candi- dates instead of using our selection strategy; (3) Adversarial only Selection: The method that se- lects top-k adversarial prompts, not considering diversity and coherence; (4) w/o Adversarialness: The method that omits the adversarialness score in our selection strategy. (5) w/o Coherency: The method that omits the similarity score in our se- lection strategy. (6) w/o Score trajectory: The method without score trajectory component for the style-conversion prompt generation."}, {"title": "Qualitative analyses.", "content": "First, we verified that our sampling strategy effectively selects diverse style- conversion prompts with high adversarialness and coherency. Figure 5a visualizes the diversity of prompts selected by our method compared to those chosen solely based on adversarialness scores (i.e., Adversarial-only Selection, the third model in our ablation study). We measure diversity using the average cosine similarity of every pair of selected prompts' embeddings, $z_c$ (Eq. 2):\n$Diveristy (C) = 1 - \\frac{1}{|C|} \\sum_{(c_i,c_j) \\in C} sim(z_{ci}, z_{cj}),$\nwhere C is the set of prompt pairs, $sim(\\cdot)$ represents the cosine similarity. The result in the Figure 5a indicate that our strategy results in a more diverse set of augmentations compared to selection based on adversarialness alone.\nFigure 5b and 5c illustrate the Adversarialness and Coherency, respectively, of our selected prompts compared to remaining unselected prompts. Adversarialness was measured using the $S_{adv}$ score (Section 3.3), and coherency was calcu- lated as the cosine similarity between the original text and its augmented version using semantic BERT embeddings (Chanchani and Huang, 2023). We can also observe that, for both metrics, prompts selected through AdStyle exhibit higher values compared to unselected prompts. This suggests that our strategy effectively selects prompts by considering both adversarialness and coherency."}, {"title": "Conclusion", "content": "This paper presents a robust fake news detection method that withstands various style-conversion and paraphrasing attacks through adversarial style-conversion. Unlike traditional detectors that use predefined, agnostic augmentations, AdStyle employs tailored augmentations that shift samples in the direction of the detector's current decision boundary using style-conversion prompts, functioning similarly to adversarial noise. Among the various prompt candidates generated by the LLM, we selected an efficient set of prompts for training by considering diversity, coherency, and adversarialness. As a result, we were able to train a detector that exhibits high robustness and generaliz- ability against a wide range of attacks. We believe that our work helps filter fake news and contribute to a better exchange of information in online."}, {"title": "APPENDIX", "content": null}, {"title": "Data & Implementation Details", "content": null}, {"title": "Dataset overview.", "content": "This paper utilizes a diverse range of datasets sourced from publicly available fake news detection resources, selected for their unique topical focuses. We utilize PolitiFact and Gossipcop, drawn from the FakeNewsNet bench- mark (Shu et al., 2020), addressing political claims and celebrity rumors respectively. Furthermore, we include Constraint (Felber, 2021), a dataset fo- cused on COVID-19 related social media posts. Each dataset is randomly divided into an 80% train- ing set and a 20% test set. Detailed statistics for these datasets can be found in Table 3."}, {"title": "Implementation details.", "content": "For consistency, exper- iments replicating existing baselines maintained fixed settings for learning, backbone network, and other relevant parameters. Augmented variations for UDA were generated using back-translation through German. RADAR employed adversarial training to learn paraphrases, utilizing the maxi- mization of the binary cross-entropy loss of a fake news detector as a reward signal, with the T5-large model serving as the paraphraser. Named enti- ties for ENDEF were extracted using the bert-base- NER model from Hugging Face. For SheepDog, augmentations were generated following the origi- nal paper's methodology, using the prompt format illustrated in Figure 7 and four tones: \"objective and professional,\" \"neutral,\" \"emotionally trigger- ing,\" and \"sensational\". We also utilized the same four tones and prompt format as our predefined initial prompt set. Two V100 GPUs were utilized for all experiments."}, {"title": "Further Results on Evaluation.", "content": null}, {"title": "Result on selection strategies across rounds.", "content": "Figure 8 shows the AUC for each selection strategy over different training rounds. Random Selection leads to slow convergence and unstable training, while Adversarial-only Selection, limited by its nar- row focus on adversarial prompts, can negatively impact performance in later stages of training due to a lack of diversity in the generated prompts. No- tably, without score trajectories, the model strug- gles to generate prompts that contribute effectively to the training process."}, {"title": "Comparison on other possible attack scenar- ios.", "content": "We have conducted additional comparison experiments with more diverse attack scenarios: (1) Adversarial prompt: Given a news article and its label, the prompt instructs the LLM to rewrite the article to evade detection as the given label. (2) Summarization prompt: A prompt instructing the LLM to summarize the news article without incorporating stylistic elements. (3) In-Context prompt: A prompt providing an example of a"}]}