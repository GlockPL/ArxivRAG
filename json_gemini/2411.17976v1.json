{"title": "The importance of visual modelling languages in generative software engineering", "authors": ["Roberto Rossi"], "abstract": "Multimodal GPTs represent a watershed in the in-\nterplay between Software Engineering and Gener-\native Artificial Intelligence. GPT-4 accepts image\nand text inputs, rather than simply natural language.\nWe investigate relevant use cases stemming from\nthese enhanced capabilities of GPT-4. To the best\nof our knowledge, no other work has investigated\nsimilar use cases involving Software Engineering\ntasks carried out via multimodal GPTs prompted\nwith a mix of diagrams and natural language.", "sections": [{"title": "Introduction", "content": "Software engineering (SE) applies engineering principles to\nthe development, operation, and maintenance of software\nsystems, ensuring they are reliable, efficient, and meet user\nrequirements. Generative Artificial Intelligence (GenAI)\nmodels, such as pre-trained models [Devlin et al., 2019;\nVaswani et al., 2017] and large language models (LLMs),\nare revolutionising fields like computer vision and natural\nlanguage processing through their ability to generate novel\nand contextually-appropriate content. In recent times, the in-\nterplay between SE and GenAI has been receiving increas-\ning attention. A vast array of applications of pre-trained\nmodels and LLMs are surveyed in [Huang et al., 2024;\nHou et al., 2024]. However, when it comes to the type of data\nused in existing studies, it appears that all studies surveyed\nfocused on text-based datasets, with the most prevalent type\nof data utilised in training LLMs for SE tasks bring program-\nming tasks/problems expressed in natural language [Hou et\nal., 2024]. Similar conclusions are reached in [Huang et al.,\n2024], which focused on 7 sub-tasks of SE: requirements gen-\neration, code generation, code summarisation, test genera-\ntion, patch generation, code optimisation, and code transla-\ntion. In all cases, the pipeline typically begins with instruc-\ntions in natural language that need to be used in the context\nof one or more of these 7 sub-tasks.\nIn SE it is often the case that communication among devel-\nopers, and between developers and customers, occurs in the\nform of sketches and diagrams [Baltes and Diehl, 2014] and\nnot just via natural language. The reason for this can be eas-\nily understood once we consider the following excerpt taken\nfrom [Tufte, 2003].\nA TALK, which proceeds at a pace of 100 to 160\nspoken words per minute, is not an especially high-\nresolution method of data transmission. Rates of\ntransmitting visual evidence can be far higher. The\nartist Ad Reinhardt said, \"As for a picture, if it\nisn't worth a thousand words, the hell with it.\" Peo-\nple can quickly look over tables with hundreds of\nnumbers in the financial or sports pages in news-\npapers. People read 300 to 1,000 printed words a\nminute, and find their way around a printed map or\na 35mm slide displaying 5 to 40 MB in the visual\nfield. Often the visual channel is an intensely high-\nresolution channel.\nIt is then not surprising that the ambition of automatically\nturning sketches and diagrams into working code, or to re-\n      verse engineer working code into diagrams, has existed for a\n      very long time in SE. Not only sketches and diagrams repre-\n      sent a high-resolution communication channel, but when they\n      are drawn following a standard, they become a form of techni-\n      cal languages. The importance of technical languages is that\n      they are denotative: they say one thing and one thing only.\n      Conversely, natural language is connotative: the meaning of\n      a statement is context dependent. It is the intrinsic ambiguity\n      of natural language that makes it not well suited for program-\n      ming, or for communicating programming-related matters.\nIn this work, we argue that the advent of multimodal GPTs,\nsuch as GPT-4 [OpenAI, 2024], may represent a watershed in\nthe interplay between SE and GenAI. GPT-4 accepts image\nand text inputs, and it can therefore receive prompts that con-\n      tain sketches and diagrams, rather than simply natural lan-\n      guage. We therefore investigate relevant use cases stemming\n      from these enhanced capabilities of GPT-4. To the best of\n      our knowledge, no other work has investigated similar use\n      cases involving SE tasks carried out via multimodal GPTS\n      prompted with a mix of diagrams and natural language.\nThe rest of this paper is organised as follows: Section 2\nprovides relevant background in GenAI and SE. Section 3\nsurveys contemporary academic literature on role of GenAI\nin SE. Section 4 investigates the role of multimodal GPTs in\nsoftware development by illustrating a selection of use cases.\nSection 5 provides some concluding remarks by reflecting on\nhow the former use cases address several research gaps in\nGenerative Software Engineering (GenSE), as well as some\nlong standing issues in SE."}, {"title": "Background", "content": "This section provides relevant background in GenAI and SE.\n2.  1 Software Engineering\nThe term \"software engineering\" was coined by Mar-garet Hamilton, who developed on-board flight software for\nNASA's Apollo program. Key milestones in the field include\nthe development of the Waterfall model (1970s), the intro-duction of Object-Oriented programming (1980s), the Agile\nManifesto (2001), which advocates for flexible, iterative de-velopment, the rise of DevOps (2000s) and of continuous in-tegration/continuous delivery (CI/CD) pipelines (2010s).\nIn the original Waterfall model, the software development\nprocess proceeds through a series of sequential phases: re-quirement analysis; software design; software development;\nsoftware quality assurance; and software maintenance. To\naddress the inflexibility of the Waterfall model, the increasing\ncomplexity of software, and poor customer satisfaction, in the\nearly 2000s a set of frameworks and practices were proposed\nbased on the values and principles expressed in the Manifesto\nfor Agile Software Development [Beck et al., 2001]. Core\nvalues and principles of Agile include: favouring individuals\nand interactions over processes and tools; working software\nover comprehensive documentation; customer collaboration\nover contract negotiation; and responding to change over fol-lowing a plan. Agile methodologies promote iterative devel-opment, frequent feedback, and adaptability to changing re-quirements throughout the aforementioned phases.\nAt the same time, the Unified Modelling Language (UML)\nemerged in 1990s as a response to the disparate notational\nsystems and approaches to software design [Rumbaugh et al.,\n2004] utilised across the phases of the software development\nprocess. UML is a general-purpose standardised visual lan-guage for designing systems. In SE, UML is utilised in three\nmain areas: use case development, static analysis, and dy-namic analysis of the software system. Use case develop-ment (or development of user stories in Agile) is a key step\nof requirement analysis. Use cases describe how a user in-teracts with a system or product to achieve a specific goal.\nStatic analysis of a software system describes the structure\nof a system by showing its classes, their attributes, opera-tions (or methods), and the relationships among objects. Dy-namic analysis expresses and model the behaviour of the sys-tem over time. More specifically, in UML, use case diagrams\nsupport use case development. Class diagrams support static\nanalysis; and interaction (sequence, activity, collaboration)\ndiagrams support dynamic analysis. However, it should be\nnoted that UML is a vast language that finds countless appli-cations to go beyond the three areas here considered.\nDespite UML being the standard visual language for de-signing systems, recent studies suggest that that most soft-ware developers favour informal hand drawn diagrams and\ndo not use UML; those using UML, tend to use it informally\nand selectively [Baltes and Diehl, 2014]. The authors further\nadvocated the development of suitable tools to make better\nuse of such sketches. Our study argues that GenAI may rep-resent such missing tool."}, {"title": "Generative Artificial Intelligence", "content": "GenAI refers to a class of AI models that can create new con-tent, such as text, diagrams, or code, based on the patterns\nthey learn from existing data [Eapen et al., 2023]. Large\nLanguage Models (LLMs) are a specific category of GenAI\nmodels that focus on language-related tasks, such as text gen-eration, translation, summarisation, and question answering.\nLLMs are typically based on a neural network architecture\ncalled a Transformer [Vaswani et al., 2017], which is Pre-trainedhence the name Generative Pretrained Transformer\n(GPT) on a massive dataset of text and code, and which al-lows them to process and generate text by considering long-range dependencies and contextual information [Brown et al.,\n2020]. The latest GPTs, such as GPT-4 [OpenAI, 2024], are\nmultimodal: they accept image and text inputs and produce\ntext outputs, leading to new applications [Li et al., 2024b].\nPrompt engineering\nPrompts are functions that map an example from a dataset to\na natural language input and target output [Bach et al., 2022].\nDue to the cost of re-training LLMs, prompt engineering, that\nis the use of prompts to train and query language models, of-fers a cost-effective way to adapt pre-trained models without\nfull fine-tuning. Key prompt engineering techniques include:\nSingle Prompt Techniques, Multiple Prompt Techniques, and\nthe use of LLM frameworks with external tools.\nSingle Prompt Techniques include Zero-Shot, Few-Shot,\nChain of Thought (CoT), and Program-Aided Language\nModels (PAL). Zero-Shot Prompting involves providing tasks\nwith natural language without additional context, relying on\nthe model's pre-existing capabilities. Few-Shot Prompting\nincludes providing examples within the prompt to guide the\nmodel, enhancing its performance on complex tasks by ex-posing it to the input and output patterns [Brown et al., 2020].\nChain of Thought Prompting [Wei et al., 2024] aids in break-ing down reasoning tasks into smaller steps to improve pro-cessing and outcome accuracy, using either zero-shot or few-shot methods to encourage step-by-step thinking. PAL ex-tends this by integrating code into reasoning processes to pro-duce more accurate outputs [Gao et al., 2023].\nMultiple Prompt Techniques leverage combining different\nstrategies to refine outputs. Voting/self-consistency [Wang et\nal., 2023] involves generating multiple responses and select-ing the most common result, which can improve accuracy,\nespecially for complex reasoning tasks. Divide and Con-quer methods split tasks into subtasks handled in sequence\nfor improved manageability and precision, seen in Directional\nStimulus Prompting [Li et al., 2024a], Generated Knowledge\n[Liu et al., 2021], and Prompt Chaining. Self-evaluation asks\nthe model to verify output accuracy, exemplified by Reflexion\n[Shinn et al., 2024] and Tree of Thoughts [Yao et al., 2024],\nenabling iterative improvement.\nRetrieval-Augmented Generation (RAG) [Fan et al., 2024]\nand ReAct [Yao et al., 2023] combine LLMs with external\nsystems to improve context handling and output relevance.\nRAG uses vector search for context retrieval before passing it\nto an LLM for output generation. ReAct combines reasoning\ntraces and task-specific actions, allowing models to interact\nwith external knowledge sources to refine outputs."}, {"title": "The role of GenAI in Software Engineering", "content": "In recent times, SE has become one of the important applica-tion areas for GenAI. We focus on two recent surveys [Huang\net al., 2024; Hou et al., 2024] investigating the interplay be-tween SE and GenAI.\nSeveral studies, e.g. [Arora et al., 2024; White et al.,\n2024], investigated the use of GenAI in the context of the re-quirement engineering sub-task. We do not believe sketches\nand diagram play a significant role in this sub-task, which is\nmainly concerned in turning requirements expressed in natu-ral language by the customer into suitable user stories and/or\nconceptual diagrams [Robeer et al., 2016].\nSE sub-tasks of interest in the context of the present study\ninclude: software design, software development, and code\nsummarisation.\nApplication of LLMs in software design remains relatively\nsparse: [Huang et al., 2024] does not cover this sub-task,\nwhile [Hou et al., 2024] only report 4 works [Kolthoff et al.,\n2023; Mandal et al., 2023; White et al., 2024; Zhang et al.,\n2024], none of which overlaps with the content of the present\nstudy; they also stress that by expanding the use of LLMs to\nthis under-explored area, it is possible to improve how soft-ware designs are conceptualised.\nConversely, both surveys identify a plethora of works con-cerned with software development (including code genera-tion, test case generation, patch generation, and code optimi-sation) and code summarisation.\nCode generation has been object of investigation for a long\ntime in the AI community. Early works used symbolic and\nneural-semiotic approaches [Alur et al., 2013]. However, re-cent neurolinguistic models, such as GPT-4 [Liu et al., 2024]\nand Copilot [Ma et al., 2023], can generate code directly from\nnatural language descriptions. This capability has signifi-cantly advanced code completion, automatic code generation,\nand the transformation of natural language annotations into\ncode, empowering developers with powerful tools and driv-ing automation in software development. None of the stud-ies listed in the above surveys focus on code generation that\nleverages multimodal prompts that include UML diagrams.\nCode summarisation [Ahmed et al., 2024] aims to automat-ically generate descriptions of a given source code. This tech-nique improves code comprehension, documentation, and\ncollaboration by providing clear summaries. Existing studies\nin code summarisation focus on analysing code structures and\ncontexts to generate informative natural language summaries.\nNone of the studies listed in the above surveys focus on code\nsummarisation producing a UML diagram as its output.\nFinally, while there exist a few studies that investigated the\ngeneration of UML diagram with support from LLMs [Con-rardy and Cabot, 2024; Wang et al., 2024; C\u00e1mara et al.,\n2023], none of these studies go as far as investigating the gen-eration of working code from UML diagrams, as well as the\nreverse engineering of relevant UML diagrams from exist-ing code. Perhaps the most interesting study among the three\nlisted is [C\u00e1mara et al., 2023], which focus on how to build\nUML class diagrams by using ChatGPT as a modelling as-sistant by leveraging the PlantUML language, which we will\nalso adopt in the following sections."}, {"title": "Multimodal GPTs in software development", "content": "In what follows, we will assume that a preliminary require-ment analysis has been carried out, which has produced a\nportfolio of initial user stories. While GPTs may find appli-cations in the realm of the development of user stories and in\nthe translation of user stories into working software, as things\nstand today, this level of automation in the realm of code gen-eration does not exist in SE practice. Conversely, what we\nfind in practice are teams of software developers tasked with\ntranslating user stories into working software. Our focus in\nthis section, is to explore novel use cases of GPTs in this spe-cific context.\nThe role of sketches and diagrams in the daily work of soft-ware developers has been investigated in [Baltes and Diehl,\n2014]. This study found that most practitioners produced in-formal hand drawn diagrams and did not use UML; those us-ing UML, tended to use it informally and selectively. We\nargue that this stems from the fact that the adoption of a \"for-mal\" notation, at present, bears no advantage with respect\nto an \"informal\" one. Over several decades, firms have re-peatedly tried to develop tools (e.g. IBM Rational Rose) that\ncould automatically generate working code from UML dia-grams, or reverse engineer UML diagram from existing code.\nWhile these technologies still exist, it is rare to find develop-ers who routinely develop a complete set of UML diagrams\nand then translate this to code using such tools. We find it\nplausible that the lacklustre success of these tools is due to a\nfundamental misunderstanding of what UML is: a language\nfor capturing and communicating conceptual requirements,\nnot one for generating complete code. In other words, UML\ndiagrams are most useful when they are high level and suf-ficiently abstract. Generation of complete working code re-quires such diagrams to reach a level of detail that is equiv-alent to the working code itself; but generating diagrams at\nthis level of abstraction would be a complete waste of time:\nwhy then not generating the code itself directly?\nAlbeit we believe there is a good reason for practitioners to\nproduce informal hand drawn diagrams, we also believe that\nthis practice is set to change with the advent of multimodal\nGPTs, such as GPT-4 [OpenAI, 2024]. In what follows, we\nwill outline a set of use cases illustrating how practitioners\nmay combine multimodal GPTs and UML diagrams to inno-vate SE practices. We will utilise Microsoft Copilot in its\nweb-based version, which is based on GPT-40 and allows im-age attachments."}, {"title": "Static modelling", "content": "As a motivating example to illustrate our use cases, we con-sider the introductory case study in Section 3 of [Stevens and\nPooley, 2006]. In this section, we focus on static modelling;\nin particular, we assume that the developer has already con-verted the relevant requirements into a preliminary class di-agram such as that shown in Figure 1. The developer now\nwants to obtain a preliminary code that matches class dia-gram. Rather than using a UML diagram to code conversion\ntool, we will here rely on Copilot prompting with images. To\nachieve this, we attach Figure 1 and use the prompt illustrated\nin the following greyed box."}, {"title": "Dynamic modelling", "content": "We may now want to proceed and describe the dynamic be-haviour of the system. Rather than using natural language,\nwe can describe specific aspect of such behaviour by leverag-ing suitable UML interaction diagrams, such as the sequence\ndiagram in Figure 5. To achieve this, we will attach Figure 5\nand chain the following prompt in Copilot.\nImplement the dynamic behaviour illustrated in the\nattached sequence diagram. Do not explicitly imple-ment the actor \"BookBorrower.\"\nThe resulting code is presented in our SM.\nNext, we may want to explore the behaviour of the various\nentities involved in a method call by obtaining a communica-tion diagram for it. Copilot can generate this diagram (Figure\n6) via the following prompt chained to previous outputs.\nGenerate a UML communication diagram in Plan-tUML notation illustrating the behaviour of the fol-lowing code: member.borrow_copy(copy1).\nIn the code generated by Copilot, the state of the book ob-ject changes when a copy of the book is successfully bor-rowed. In particular, the book may change from being bor-rowable (there is a copy of it in the library) to not borrowable\n(all copies are out on loan or reserved). This behaviour can\nbe represented via a state diagram. Rather than drawing such\nstate diagram, we generate one via the following prompt.\nGenerate a UML state diagram in PlantUML notation\nto represent the possible states of the Book object.\nThis leads to the state diagram in Figure 7.\nAssume now that the desired behaviour, illustrated in Fig-ure 8, is slightly different from that obtained in this prelimi-nary draft of our code. We can ask Copilot to amend the code\nby chaining the following prompt.\nAmend the python classes to capture the behaviour in\nthe attached state diagram. Make sure the generated\ncode continues to reflect the original class diagram.\nThe resulting code is presented in our SM.\nFinally, we can ask Copilot to generate a new state dia-gram in PlantUML notation that reflects the behaviour of the\nupdated code. The resulting diagram is shown in Figure 9 and\nmatches the behaviour in Figure 8."}, {"title": "Hand drawn diagrams", "content": "While all the previous examples featured computer generated\ndiagrams, Copilot is able to handle equally well hand-drawn\ndiagrams. We consider the activity diagram shown in Figure\n10. We attach the diagram, and input the following prompt.\nCreate a method in python that implements the at-tached UML activity diagram.\nCopilot correctly recognises the fact that that the activity\ndiagram represents a sorting algorithm, and returns a method\nimplementing it.\nAlternatively, assuming the code for our sorting algorithm\nis already available to us, we can ask Copilot to convert it to\nan activity diagram via the following prompt.\nCreate an activity diagram in PlantUML notation for\nthe following method in python: [insert python code].\nIn this specific instance, Copilot returned a diagram with\na small number of syntax errors. The errors were minor is-sues with the PlantUML syntax of the two while loops in\nthe diagram, and could be easily fixed by hand. The resulting\nactivity diagram is shown in Figure 11. Conversely, a second\nprompt asking Copilot to generate a diagram in PlantUML\nnotation by translating directly the diagram in Figure 10 pro-duced no errors. Once more, the focus of our discussion is\nfacilitating, and not fully automating, SE activities. There-\nfore, the presence of minor errors is of no concern."}, {"title": "Design patterns", "content": "Design Patterns [Gamma et al., 1994] are reusable solutions\nto commonly occurring problems in software design. They\nare not specific implementations but rather general templates\nthat can be adapted to various situations. By using design\npatterns, developers can create more flexible, maintainable,\nand efficient software systems. There exists a wealth of ex-isting patterns available in the SE literature; In addition, new\npatterns can be created and illustrate via appropriate UML di-agrams. For instance, we may consider the \u201cAdapter\" pattern,\nwhich converts the interface of a class into another interface\nclients expect. This pattern is illustrated in Figure 12. Design\npatterns offer endless opportunities to carry out Few-Shot and\nCoT prompting. We shall illustrate this point with a practical\nexample involving the \"Adapter\" pattern."}, {"title": "Discussion", "content": "Throughout Section 4 we have utilised Microsoft Copilot in\nits web-based version, which is based on GPT-40 and allows\nimage attachments. To ensure reproducibility, we have also\ndeveloped an equivalent pipeline in Gemini, formalised in a\nJupyter Notebook based on gemini-1.5-flash, which is\nincluded in our SM.\nIn this section, we reflect on how the use cases discussed in\nSection 4, which are made possible thanks to the enhanced ca-pabilities of GPT-4, address several research gaps in GenSE,\nas well as some long standing issues in SE.\nWe shall focus first on the research gaps we bridged in\nGenSE. First, our study addresses the lack of applications\nof GenAI to the software design sub-task of SE, a literature\ngap identified in [Huang et al., 2024; Hou et al., 2024]. Sec-ond, to the best of our knowledge, our study represents the\nfirst application of multimodal GPTs (diagram + prompt) to\nthe software development sub-task of SE, which in GenSE\nis generally carried out via natural language-based prompts.\nThird, we have discussed and illustrated with an empiri-cal example the superiority, in terms of information trans-fer efficiency, of diagram-based prompting against pure nat-ural language prompting; a result that aligns with the dis-cussion in [Tufte, 2003]. Fourth, by reverse engineering\ncode into diagrams, we have contributed to the code sum-marisation sub-task of SE, which is again predominantly car-ried out via natural language summaries. Fifth, while there\nexist studies that investigated the production of PlantUML\ncode from hand drawn diagrams [Conrardy and Cabot, 2024;\nWang et al., 2024], we believe this is the first study in which\nwe cover multiple elements of the SE stack, from conceptual\nmodelling which includes static and dynamic analysis of\nthe software system to implementation. Finally, in addi-tion, the use of existing (or new) design patterns in the context\nof Few-Shot prompting for software system design does not\nseem to have been investigated before in the literature.\nNext, we shall look at which long standing issues in SE\nmay be addressed by the use cases discussed in our work.\nIn [Baltes and Diehl, 2014], the authors speculated that, as\ndocumentation is frequently poorly written and out of date,\nsketches may serve as a supplement to conventional docu-mentation like source code comments. Consequently, they\nadvocated the development of suitable tools to support archiv-ing and retrieval of such sketches. No such tool has been de-veloped ever since and poorly documented code remains a\npersistent issue in SE. However, GenAI has already started\nto change this picture via automatic generation of code com-ments. And yet, if sketches and diagram (both formal and in-formal) can be transformed, as we have shown, into a prelim-inary draft of the desired source code, then producing reliable\nvisual descriptions of the software system under development\nsuddenly becomes appealing. Likewise, if it is possible to se-lectively and cheaply reverse engineer part of such software\nsystem to visually inspect the behaviour of classes and meth-ods, the developer becomes equipped with a formidable arse-nal of high resolution communication tools to enhance code\nunderstanding and boost team communication."}]}