{"title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph", "authors": ["Yibo Zhao", "Jiapeng Zhu", "Can Xu", "Xiang Li"], "abstract": "The rapid growth of social media platforms has raised significant concerns regarding online content toxicity. When Large Language Models (LLMs) are used for toxicity detection, two key challenges emerge: 1) the absence of domain-specific toxic knowledge leads to false negatives; 2) the excessive sensitivity of LLMs to toxic speech results in false positives, limiting freedom of speech. To address these issues, we propose a novel method called MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance hatred and toxicity detection. First, we construct a comprehensive meta-toxic knowledge graph by utilizing LLMs to extract toxic information through a three-step pipeline, with toxic benchmark datasets serving as corpora. Second, we query the graph via retrieval and ranking processes to supplement accurate, relevant toxic knowledge. Extensive experiments and in-depth case studies across multiple datasets demonstrate that our MetaTox significantly decreases the false positive rate while boosting overall toxicity detection performance. Our code will be available soon.", "sections": [{"title": "1 Introduction", "content": "Online social media platforms have become a major source of information for people worldwide. Meanwhile, they also provide a communication tool for spreading toxic content including harassment, trolling, cyberbullying, and hate speech, which poses a serious and continual threat to the harmony of society (Simpson, 2013) and harms children's mental health (Simpson, 2019). It thus becomes a critical task to detect the toxic content both explicit like \"fuck you\" and implicit like \u201cHow dark is my humor? It picks cotton\", while cautiously avoiding hindering freedom of expression.\nTo address the problem, previous work can be mainly categorized into three types: (1) rule-based methods (Chen et al., 2012; Gitari et al., 2015) relying on the pre-defined rules; (2) embedding-based methods (Warner and Hirschberg, 2012; Djuric et al., 2015; Park and Fung, 2017) leveraging text representations for classification; (3) transformer-based methods (D'Sa et al., 2020; Luu and Nguyen, 2021) employing transformer architectures with small-scale parameters. The first two types of approaches struggle with semantic understanding, limiting their ability to detect implicit toxicity. While methods of the third type can capture more complex semantics, their reliance on domain-specific training data results in significant performance degradation on out-of-domain data.\nThe rapid development of large language models (LLMs) has provided new insights into the detection of toxic contents. Some studies (Yang et al., 2023; Zhang et al., 2024a) leverage LLMs as data augmentation tools to enhance toxicity detection. However, due to LLMs' lack of domain-specific knowledge in hatred and toxicity, the performance of these methods is still limited. Further, recent"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Hatred and Toxicity Detection", "content": "Since the advent of social media, the detection of hatred and toxicity has become a pivotal research area. Previous methods can mainly be classified into rule-based, embedding-based, and transformer-based, and LLM-based approaches.\nRule-based methods (Chen et al., 2012; Gitari et al., 2015) rely on predefined rules that match specific patterns within the text. (Liu and Forss, 2015) uses the insults and swears words to form a dictionary for hatred and toxicity detection. Embedding-based approaches, on the other hand, combine text representations obtained by methods like word2vec (Djuric et al., 2015; Park and Fung, 2017) with. Transformer-based methods (D'Sa et al., 2020; Luu and Nguyen, 2021), typically perform fine-tuning on pre-trained models like BERT. Recent studies have explored the use of Large Language Models (LLMs), which typically serve as a powerful data augmentation tool. These methods (Yang et al., 2023; Zhang et al., 2024a) leverage structured reasoning methods such as Chain-of-Thought (CoT) (Wei et al., 2024) and Tree-of-Thought (ToT) (Yao et al., 2024) to infer hidden semantics of input text, which are then combined with raw input for training detection algorithms."}, {"title": "2.2 LLMs with Knowledge Graphs", "content": "Recent studies combine LLMs with Knowledge Graphs (KGs) through two ways. The first involves using LLM to assist in the construction of KGs, while the other involves retrieving knowledge to improve the reasoning ability of LLMs. For KG construction tasks, SAC-KG (Chen et al., 2024) uses LLMs to extract single-level entity-induced KGs based on domain corpora and examples, and then combines single-level KGs together after mul-"}, {"title": "3 Methodology", "content": "In this section, we present MetaTox, our proposed method for enhancing hatred and toxicity detection using a meta-toxic knowledge graph. Meta-Tox consists of two stages: (1) construction of the meta-toxic knowledge graph through a three-step pipeline, and (2) querying the graph to enhance the downstream task of binary classification for toxicity detection. We begin with the data collection process, which lays the foundation for constructing the knowledge graph."}, {"title": "3.1 Data Collection", "content": "For meta-toxic knowledge graph construction, we leverage three well-established English datasets: HateXplain (Mathew et al., 2021), Toxicspans (Pavlopoulos et al., 2021), and IHC (ElSherief et al., 2021). Since our goal is to enhance toxicity detection by supplementing the LLM with accurate and curated toxic knowledge, the knowledge graph should serve as a domain-specific corpus that reflects only toxic content. Therefore, we exclusively retain toxic samples labeled as \u201ctoxic\", \"hate\" or \"offensive\u201d from the training sets of these datasets to construct the knowledge graph.\nFor toxicity detection, we use the test sets from the three datasets, aligning each sample's label with either \"toxic\" or \"non-toxic\" to formalize a binary classification task. This enables us to evaluate the LLM's performance improvement in detecting potentially toxic speech with the support of our meta-toxic knowledge graph."}, {"title": "3.2 Graph Construction", "content": "We propose a three-step process to build the meta-toxic knowledge graph based on solely toxic samples: (1) rationale reasoning, which involves identifying the contents that trigger toxicity in speech, thereby uncovering implicit toxic meanings; (2) triplet extraction, where toxic entities and relations are extracted using a self-checking mechanism to ensure the triplets are correctly formatted and toxic; (3) duplicate removal, which merges semantically similar entities and relations to reduce noise and shrink the graph. The three steps are illustrated in Figure 2 and described in detail below."}, {"title": "3.2.1 Rationale Reasoning", "content": "To construct the meta-toxic knowledge graph from initial toxic speech, it is essential to first identify the core toxic elements, i.e. entities and relations, within the speech. However, toxic semantics are often implicit and abstract, involving concepts such as race, gender and religion, rather than specific named entities. This introduces challenges in directly extracting these elements in a single step.\nTo address this, we employ a rationale reasoning step prior to triplet extraction, which applies"}, {"title": "3.2.2 Triplet Extraction", "content": "In this step, we take the toxic speech and the corresponding rationales as input and prompt the LLM to extract toxic triplets, represented in the Subject-Predicate-Object (SPO) format, such as \u201c(white lives matter, is against, black lives matter)\". Specifically, we implement a template that instructs the LLM to extract triplets triggering hatred, given two exemplars to guide the LLM in understanding the extraction process and outputting triplets in a uni-\""}, {"title": "3.2.3 Duplicate Removal", "content": "In this step, we take the extracted triplet elements (entities and relations) as input, applying a clustering algorithm to merge these elements and ultimately generate a meta-toxic knowledge graph with duplicates removed. Unlike traditional entity reso-"}, {"title": "3.3 Graph Query", "content": "Given a potentially toxic speech, we devote to querying on the meta-toxic knowledge graph to supplement speech-related toxic triplets, thereby providing the LLM with accurate, domain-specific guidance when judging the hatred and toxicity of the given speech. Briefly, we divide the query process into Retrieval (Entity Extraction, Node Mapping, Path Retrieval) and Ranking (Formatting, Ranking and Filtering). The overall query procedure is illustrated in Figure 3.\nEntity Extraction aims to identify various entities in the given speech, treating them as candidate toxic entities for further validation. Given that explicit toxic entities may be absent, we employ the LLM to extract as many relevant entities as possible, including both specific named entities and broader concepts such as race, gender, and religion. The detailed instruction is shown in Appendix A.4.\nNode Mapping faces the challenge that entities extracted from the speech may not exactly match the names of nodes in the meta-toxic knowledge"}, {"title": "4 Experiments", "content": "As outlined in Section 3.1, we construct three meta-toxic knowledge graphs based on the HateXplain, Toxicspans, and IHC datasets, respectively, with Qwen2.5-14B-Instruct (Qwen) (Qwen Team, 2024). Then Qwen and Llama3.1-8B-Instruct (Llama) (Grattafiori et al., 2024) are employed to generate predictions. We evaluate MetaTox from three perspectives: (1) Graph Statistics, which provides quantitative analysis regarding nodes and relations in the constructed graphs to assess the effectiveness of our data mining approach; (2) Toxicity Prediction, where MetaTox is compared to baseline methods under both in-domain and cross-domain settings to evaluate effectiveness and robustness of MetaTox; (3) Case Studies, which offer in-depth analyses by guiding LLMs to output reasoning paths, providing insights into how MetaTox enhances interpretability and reasoning abilities."}, {"title": "4.1 Graph Construction", "content": "Due to the absence of established evaluation metrics for the constructed knowledge graph, we focus on analyzing the graph properties, particularly the number of nodes and relations, to assess the effectiveness of our data mining approach. Additionally, we examine the merging ratio by integrating meta-toxic knowledge graphs derived from different datasets, demonstrating how the meta-toxic knowledge graph expands. After merging three datasets, we observe a 21.52% reduction in the number of entities and a 3.25% reduction in the number of triplets in the merged graph. This suggests that the targets of toxic speech exhibit significant overlap across datasets, supporting the transferability of our approach. The detailed quantitative results are presented in Table 1."}, {"title": "4.2 Toxicity Prediction", "content": "We conduct experiments in both in-domain and cross-domain scenarios, comparing our method with vanilla LLM and the naive RAG-enhanced LLM for toxicity detection. For the vanilla LLM, we directly input the test speech into the LLM and prompt it to provide the prediction. For the naive RAG method, we retrieve the top-2 most similar speeches from the training set as additional knowledge. To ensure a fair comparison, the training set used for the RAG method is the same as the data used to construct our meta-toxic knowledge graph."}, {"title": "4.2.1 In-domain Setting", "content": "For the in-domain setting, the knowledge graph construction and toxicity detection are performed on the training and test sets of the same dataset, respectively.\nAs illustrated in Table 2, MetaTox demonstrates performance improvements across the HateXplain and IHC datasets with two backbone models, consistently outperforming baseline methods. The results reveal key insights into the effectiveness of our approach. For HateXplain, where toxic semantics are relatively explicit, the related knowledge is easier to retrieve by naive RAG, improving the performance to some extent. However, MetaTox outperforms naive RAG, primarily due to the more concise and relevant knowledge provided by our graph.\nIn contrast, for IHC, where toxic semantics are more implicit, overall performance is lower compared to HateXplain. Using Qwen, while naive RAG yields only marginal improvement, MetaTox achieves a remarkable performance boost. This discrepancy may stem from the fact that documents retrieved by naive RAG often contain excessive unrelated information. In contrast, MetaTox, through the combined contributions of knowledge graph construction and query on the graph, not only retrieves the most related knowledge but also organizes it into concise triplets, significantly improving the model's understanding of the provided knowledge. When using the less powerful Llama for detection, MetaTox achieves perfor-"}, {"title": "4.2.2 Cross-domain Setting", "content": "For the cross-domain setting, the knowledge graph is constructed using the training set of ToxicSpans, while toxicity detection is performed on test sets of HateXplain and IHC. Correspondingly, naive RAG is only allowed to retrieve documents from ToxicSpans.\nThe experimental results are presented in Table 3. Notably, we observe a performance drop with naive RAG on IHC and negligible improvement on HateXplain. This can be attributed to the different facets of toxic semantics between ToxicSpans and test datasets. In contrast, MetaTox demonstrates promising improvement on both datasets, indicating the robustness of our method.\nIt is worth noting that, in all scenarios, our method not only maintains superior performance but also achieves the lowest false positive rate, ef-"}, {"title": "4.3 Case Study", "content": "To further analyze how MetaTox incorporates domain-specific knowledge and mitigates false positive misjudgments, we present two case studies where the LLM is instructed to provide reasoning explanations.\nAs shown in Figure 4, the original speech implicitly promotes toxicity by stirring up antagonism between blacks and whites. The word \"shoot\" also emphasizes the racial disparities. The vanilla LLM incorrectly focuses on numbers like \u201c1/6\u201d and \"5/6\", which misleads the LLM to interpret the speech as a statistical analysis rather than a toxic trigger. Enhanced with naive RAG, the retrieved related speeches emphasize statistics like \u201c70%\u201d, and \"32.6 times\u201d, as naive RAG fails to capture the most crucial semantics. This further misguides the LLM, leading it to incorrectly interpret the original speech as a statistical statement, resulting in a misjudgment with overly high confidence. On the contrary, MetaTox correctly identifies pivotal ele-"}, {"title": "5 Conclusion", "content": "In this paper, we proposed a novel method called MetaTox to address both false positive and false negative misjudgments caused by the lack of domain-specific knowledge and LLMs' extreme sensitivity. First, we leverage LLMs to extract toxic content through a three-step pipeline, which builds the meta-toxic knowledge graph. Next, we query the graph with retrieval and ranking processes to provide additional, relevant toxic knowledge. Extensive experiments and detailed case studies across various datasets show that MetaTox significantly lowers the false positive rate while improving overall toxicity detection performance, thereby preserving freedom of speech."}, {"title": "Limitaion", "content": "Our study still has several limitations. First, due to the computational constraint, we did not conduct experiments with larger LLMs, suggesting that the full potential of our method remains to be further explored. Future research could leverage more powerful LLMs to construct even more comprehensive meta-toxic knowledge graphs. Additionally, our method is currently limited to binary classification in English-language scenarios. Future work can extend our approach to multi-modal, multi-lingual, and multi-cultural tasks, thereby broadening its applicability across diverse contexts."}, {"title": "Ethical Statement", "content": "Our research focuses on toxic speech detection, primarily addressing two key issues faced by LLMs in this task: false negatives caused by a lack of domain-specific knowledge, and false positives resulting from excessive sensitivity to certain content. Our primary goal is to facilitate more accurate detection of hatred and toxicity, thereby alleviating the issue of false positive misjudgments and protecting freedom of speech. This, in turn, aims to contribute to the creation of a more harmonious and unified online environment.\nWe construct a meta-toxic knowledge graph filled with toxic content, which plays a vital role in detecting toxic content. However, there is also a risk of potential misuse of our proposed method. Specifically, our methods might be misused to improve internet content moderation strategies or even generate toxic speech. It is essential to emphasize that our work focuses on detecting toxic speech while safeguarding freedom of speech, instead of content censorship.\nThe datasets we used are all existing open-source datasets, aligning with their intention for scientific research. We also adhered to the MIT license for HateXplain and IHC datasets, and the CC0-1.0 license for the ToxicSpans dataset."}]}