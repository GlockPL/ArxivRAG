{"title": "Investigating the Interplay of Prioritized Replay and Generalization", "authors": ["Parham Mohammad Panahi", "Andrew Patterson", "Martha White", "Adam White"], "abstract": "Experience replay is ubiquitous in reinforcement learning, to reuse past data and improve sample efficiency. Though a variety of smart sampling schemes have been introduced to improve performance, uniform sampling by far remains the most common approach. One exception is Prioritized Experience Replay (PER), where sampling is done proportionally to TD errors, inspired by the success of prioritized sweeping in dynamic programming. The original work on PER showed improve- ments in Atari, but follow-up results are mixed. In this paper, we investigate several variations on PER, to attempt to understand where and when PER may be useful. Our findings in prediction tasks reveal that while PER can improve value propagation in tabular settings, behavior is significantly different when combined with neural networks. Certain mitigations-like delaying target network updates to control generalization and using estimates of expected TD errors in PER to avoid chasing stochasticity can avoid large spikes in error with PER and neural networks, but nonetheless generally do not outperform uniform replay. In control tasks, none of the prioritized variants consistently outperform uniform replay.", "sections": [{"title": "1 Introduction", "content": "Experience Replay (ER) is widely used in deep reinforcement learning (RL) and appears critical for good performance. The core idea of ER is to record state-transitions (experiences) into a memory, called a buffer, replay them by sub-sampling mini-batches to update the agent's value function and policy. ER allows great flexibility in agent design. ER can be used to learn from human demonstra- tions (pre-filling the replay buffer with human data) allowing off-line pre-training and fine-tuning. ER has been used to learn many value functions in parallel, as in Hindsight ER (Andrychowicz et al., 2018), Universal Value Function Approximators (Schaul et al., 2015), and Auxiliary Task Learning (Jaderberg et al., 2016; Wang et al., 2024). ER can be seen as a form of model-based RL where the replay buffer acts as a non-parametric model of the world (Pan et al., 2018; Van Hasselt et al., 2019), or ER can be used to directly improve model-based RL systems (Lu et al., 2024). In addition, ER can be used to mitigate forgetting in continual learning systems (Anand & Precup, 2024). ER has proven crucial for mitigating the sample efficiency challenges of online RL, as well as mitigating instability due to off-policy updates and non-stationary bootstrap targets. The most popular alternative, asynchronous training, requires multiple copies of the environment, which is not feasible in all domains and typically makes use of a buffer anyway (e.g., Horgan et al. (2018)).\nThere are many different ways ER can be implemented. The most widely used variant, i.i.d or uniform replay, samples experiences from the buffer with equal probability. As discussed in the original paper (Lin, 1993), ER can be combined with lambda-returns and various sampling methods. Experience can be sampled in reverse order it occurred, starting at terminal states. Transitions can be sampled from a priority queue ordered by TD errors the idea being transitions that caused large updates are more important and should be resampled. Samples can be drawn with or without replacement-avoiding saturating the mini-batch with high priority transitions. The priorities can"}, {"title": "2 Background, Problem Formulation, and Notation", "content": "In this paper, we investigate problems formulated as discrete-time, finite Markov Decision Processes (MDP). On time step, t, the agent selects an action \\(A_t \\in \\mathcal{A}\\) in part based on the current state, \\(S_t \\in \\mathcal{S}\\). The MDP transitions to a new state \\(S_{t+1}\\) and emits a reward signal \\(R_{t+1} \\in \\mathbb{R}\\). The agent's action choices are determined by it's policy \\(A_t \\sim \\pi(\\cdot | S_t)\\), and the goal of learning is to adjust \\( \\pi \\) to maximize the future expected return \\( \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a] = \\mathbb{E}_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+... |S_t = s, A_t = a]\\), where \\(\\gamma \\in [0, 1]\\). The expectation is dependent on future actions determined by \\( \\pi \\) and future states and rewards according to the MDP.\nWe focus on action-value methods for learning \\( \\pi \\). In particular, Q-learning estimates the state-action value function \\( q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}[G_t|S_t = s, A_t = a] \\forall s, a \\in \\mathcal{S} \\times \\mathcal{A} \\) via temporal difference updates from"}, {"title": "3 Variants of Prioritized Replay", "content": "In this section, we define two variants of PER that we use to better understand the role of prioritiza- tion. We use the name DM-PER for Schaul et al. (2016)'s prioritized replay algorithm and Uniform to refer to classic i.i.d replay.\nWe start by describing a simplified prioritized replay algo- rithm, which we will call Naive PER. Starting from a uni- form replay, the Naive PER algorithm modifies only the sampling strategy from uniform to proportional to TD er- ror. We record the TD-error as soon as a sample is added to the buffer, then update that TD-error when a transi- tion is sampled. We do not mix in uniform sampling, we do not squash the priorities with an exponential hyperpa- rameter, and we do not use importance weights. In this way, the Naive PER algorithm closely resembles tabular prioritized sweeping, except we sample probabilistic ac- cording to the priorities rather than use a priority queue. The full pseudocode for Naive PER can be found in the appendix.\nIn order to study the effects of noise on the prioritization strategy, we introduce a new prioritization variant expected PER (EPER). Instead of using the sample TD-error, \\(\\delta_t\\), which can be noisy when the reward or the transition dynamics are stochastic, EPER uses an estimate of the expected TD-error \\( \\mathbb{E} [\\delta_t | S_t = s] \\). This expectation averages out random effects from action selection, transition dynamics, and the reward signal.\nLearning this expectation can be formulated as a simple least-squares regression problem with sam- ples \\(\\delta_t\\) as the target, yielding the following online update rule: \\( \\theta_{t+1} \\leftarrow \\theta_t + \\alpha(\\delta_t - h_\\theta(S_t))\\nabla_\\theta h_\\theta(S_t) \\), where \\(h_\\theta\\) is a parametric approximation of \\( \\delta_t \\) with parameters \\( \\theta \\). This secondary estimator forms the basis of the gradient TD family of methods (Sutton et al., 2009; Patterson et al., 2022) making it natural to combine with recent gradient TD algorithms such as EQRC (Patterson et al., 2022). In other words, if we use EQRC instead of DQN, we can use EPER to attain a less noisy signal for computing priorities with no extra work because EQRC is estimating \\(h_\\theta\\) anyway."}, {"title": "4 An Empirical Investigation of Prioritization in Replay", "content": "In most situations the value function is approximated using a parameterized function such as a neural network. Neural networks are known for their generalization power, where updates to estimated value of one state will effect many other states. The only source of value propagation in the tabular setting is bootstrapping-information about reward moves from one state to a neighbor state. When using neural networks to estimate the value function however, any update will affect numerous states. The idea of prioritized replay is based on the tabular notion of value propagation and the interplay between neural network generalization and prioritized replay remains an open question. This section explores the combined effect of prioritized replay and neural network generalization in RL agents.\n4.1 Comparing Sample Efficiency in Prediction\nIn this section we ask several questions in a sparse reward task where rapid value propagation should require careful sampling from the replay buffer. Does naive prioritization improve performance over uniform replay? Do the additional tricks in DM-PER reduce the efficiency of value propagation when they are not really required? Finally, does robustness to noisy TD errors, as in EPER, matter in practice? We investigate these questions with tabular and neural network representations.\nWe consider both policy evaluation and control problems in a 50-state Markov chain environment visualized in Figure 2. This is an episodic environment with \\(\\gamma = 0.99\\) chosen to present a difficult value propagation problem. In every episode of interaction, the agent starts at the leftmost state and at each step takes the left or right action which moves it the corresponding neighbour state. The only reward in this environment is +1 when reaching the rightmost state at which point the episode terminates.\nIn the policy evaluation experiments, the objective is to estimate the state value function of the random policy. The data for the replay buffer is generated by running the random policy, making this an on-policy prediction task. The performance measure is the Mean Squared Value Error (MSVE) between estimated value function and true value function: \\(MSVE(w) = \\sum d(s)(\\upsilon_\\pi(s) \u2013 \\widehat{\\upsilon}_w(s))^2 \\) where d(s) is the state visitation distribution under the uniform policy. In this experiment we have two settings, one where the value function is tabular and one where it is approximated by a two layer neural network with 32 hidden units in each layer and rectified linear unit (ReLU). We systematically tested a broad set of learning rates, buffer size, and batch sizes over 50 combinations with 30 seeds each. The sensitivity to learning rate can be found in the Appendix. In Figure 3 we shown a representative result with batch size 8, buffer size 8000, and learning rate 8-4 in the tabular setting and 8-5 in the neural network setting. The remaining results are in the Appendix."}, {"title": "4.2 Overestimation due to Prioritization, Generalization, and Bootstrapping", "content": "In the previous section we saw that Naive PER exhibited a spike in early learning, but why? One possible explanation is that Naive PER is oversampling the terminal transition which causes the NN"}, {"title": "4.3 Comparing Sample Efficiency in Control", "content": "In this section we turn our attention to a simple control task, again designed in such a way that value propagation via smart sampling should be key. Here our main question is: do the insights about the benefits of prioritization persist when the policy changes and exploration is required.\nIn the tabular setting, we use Q-learning (without target networks) and in neural network setting we explore two setups: (1) DQN (with target refresh rate of 100) and (2) EQRC (as an alternative method without target network). We report steps to goal as the performance metric for the 50-state Markov chain problem. Buffer size is fixed to 10000, batch size 64, and we pick the best learning rate for each method (see the Appendix for details). Each control agent is run for 100000 steps with an e-greedy policy with \\(\\epsilon = 0.1\\)."}, {"title": "4.4 Sampling Without Replacement & Updating Priorities", "content": "In this section we explore two simple but natural improvements to replay that could improve perfor- mance. There are many possible refinements, and many have been explored in the literature already. Here we select two that have not been deeply explored before, specifically (1) sampling transitions with or without replacement, and (2) recomputing priorities of samples in the buffer.\nWhen sampling a mini-batch from the replay buffer, one has the option to sample transitions with or without replacement. This decision is important in PER because sampling with replacement can cause a high priority transition to be repeatedly sampled into the same mini-batch. This certainly happens on the first visit to the goal state in the 50 state chain. Uniform replay avoids this problem by design. Most reference implementations of PER sample with replacement. We hypothesize that duplicate transitions in the mini-batch reduces the sample efficiency of prioritized methods, effectively nagating the benefit of mini-batches.\nWe compare Naive PER with and without replacement sampling and uniform replay in the 50-state Markov chain prediction domain under tabular and neural net function approximation. We used a two layer network with 32 hidden units and ReLU activation, a batch size of 8000 and experimented with several mini-batch sizes (1, 8, 64, 256). With batch size 1, with and without replacement are identical. We report a representative result with learning rate of 8-4 in the tabular setting and 8-5 in the neural network setting and report MSVE under the target policy over training time."}, {"title": "4.5 Comparing Sample Efficiency in Classic Control Domains", "content": "Our previous experiences in the chain were designed to represent an idealized problem to highlight the benefits of smarter replay, and in this section we consider slightly more complex, less ideal tasks. In the chain, we only saw clear advantages for prioritization in prediction and also in control with small batch sizes. The main question here is do these benefits persist or perhaps prioritization will be worse supporting the common preference for uniform replay in deep RL.\nWe consider four episodic environments which are significantly more complex than the chain, but are small enough that smaller NNs can be used and extensive experimentation is possible. The first three environments, often refered to as classic control feature low dimensional continuous state and discrete actions. MountainCar (Moore, 1990) and Acrobot (Sutton, 1995) are two control tasks where the goal is to manipulate a physical system to get to a goal at the end of a long trajectory. We also include Cartpole due to the unstable dynamics of the balanced position (Barto et al., 1983). Finally we include the tabular Cliffworld (Sutton & Barto, 2018) because the reward for falling off the cliff is a large negative value which causes rare but large spikes in the TD error, which might highlight the benefit of EPER. The details about these environments can be found in the Appendix. We set the discount factor \\(\\gamma = 0.99\\). The episodes in MountainCar, Acrobot, and Cartpole are cutoff every 500 steps, but there is no episode cutoff in Cliffworld.\nIn this experiment we use DQN with a two layer network of size 64 with ReLU activation and target refresh rate 128. Batch size and buffer size are fixed to 64 and 10000 respectively and the learning rate is selected using a two stage approach to avoid maximization bias (Patterson et al., 2023). First each agent is run for 30 seeds sweeping over many learning rate parameter settings, then the hyperparameter which achieved the best average performance is run for 100 new seeds (see details in the Appendix). We include Modified PER, which combines Naive PER with without- replacement sampling and recomputes the priorities every 10 steps. Figure 11 summarizes the results. Unsurprisingly, prioritization does not improving the sample efficiency over uniform replay in any of the four domains."}, {"title": "5 Conclusion", "content": "In this paper, we conduct a series of carefully designed experiments into prioritized replay under tab- ular and neural network settings. We find that prioritization combined with non-linear generalization can overestimate values during early learning. A follow-up experiment indicates a combination of bootstrapping and neural network generalization as the reason behind this overestimation. Further- more, we showed in a simple chain domain, several variants of PER outperform i.i.d replay in the prediction setting but have poor sample efficiency in control. Unsurprisingly, no variant of PER improves upon i.i.d replay in classic control domains.\nWe introduce EPER as a simple modification prioritizing transitions according to a learned estimate of the expected error inspired by gradient TD methods. We showed that EPER can be more robust to noisy reward domains and perform more reliably than PER or i.i.d replay in Cliffworld. Finally, we explore two design decisions in PER, recomputing outdated priorities and sampling batches without replacement, discovering that these additions can drastically improve PER in the tabular setting but have little to no effect when using neural networks."}, {"title": "A Algorithms", "content": "Here we present pseudo code for the DQN algorithm used in this paper. The behavior policy of DQN, denoted as \\( \\pi_w(S_t) \\), is an e-greedy policy over the current action values q(St,. ). Algorithm 1 shows the uniform replay variant and algorithm 2 shows the pseudo code for Naive PER. In both variants, size of the replay buffer is a hyperparameter."}, {"title": "B DM-PER Hyperparameters", "content": "In all the experiments with DM-PER, its additional hyperparameters are set to the values from (Schaul et al., 2016) and are listed in table 1."}, {"title": "C Prediction Experiments in the Chain", "content": "In This section, we document the hyperparameter sweep details and additional results from the Markov chain prediction experiment. Table 2 lists the hyperparameter selections for the prediction agents. The agents in figure 3 use buffer size 8000, batch size 8, and learning rate 8-4 for tabular and 8-5 for neural network agents.\nFigure 13 shows the sensitivity of replay methods to learning rate for batch size 8 and buffer size 8000 in the chain prediction problem. Prioritized replay is more sample efficient than uniform replay"}, {"title": "D Control Markov Chain Experiment Details", "content": "The hyperparameters used in control chain experiments are given in table 3. We chose the learning rate for each agent by maximizing over average performance across a range of learning rates.\nIn the without replacement control experiment, we use a buffer size of 10000 and experiment with 4 different batch sizes [1, 8, 64, 256]. For each setting, the learning-rate is selected via maximizing over a range of learning-rates. All other meta-parameters are based on table 3. Figure 18 shows results for the tabular Q-learning setting, figure 19 shows the DQN results, and figure 20 shows the EQRC results."}, {"title": "E Classic Control Experiment Details", "content": "E.1 Environment Description\nIn the last part of the paper, we experiment with classic control problems that are more difficult than the chain. We experiment with MountainCar, Acrobot, Cartpole, and Cliffworld. In Mountain Car, the goal is to drive an under powered car up a hill in a simulated environment with simplified physics by taking one of three actions, accelerate left, accelerate right, do not accelerate. The observations are position and speed values of the car. The reward is -1 per step and episodes terminates when the car crosses a threshold at the top of hill with reward 0.\nIn Acrobot the agent controls a system of two linear links connected by a movable joint. The goal is to move the links, by applying torque to the joint, such that the bottom part of the link rises to the level of its highest point upon which the episode terminates with reward 0. The reward of all other transitions is 1 per step.\nThe goal of a Cartpole agent is to balance a pole on top of a moving cart by accelerating the cart to either left or right. The reward is +1 per step if the pole is properly balanced. If the pole falls more than 12 degrees the episodes is terminated and the pole is reset to its upright position. The episode cutoff length is 500."}]}