{"title": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability Detection", "authors": ["Ruijun Feng", "Hammond Pearce", "Pietro Liguori", "Yulei Sui"], "abstract": "Large language models (LLMs) have been proposed as powerful tools for detecting software vulnerabilities, where task-specific fine-tuning is typically employed to provide vulnerability-specific knowledge to the LLMs for this purpose. However, traditional full-parameter fine-tuning is inefficient for modern, complex LLMs, which contain billions of parameters. Soft prompt tuning has been suggested as a more efficient alternative for fine-tuning LLMs in general cases. However, pure soft prompt tuning treats source code as plain text, losing structural information inherent in source code. Meanwhile, graph-enhanced soft prompt tuning methods, which aim to address this issue, are unable to preserve the rich semantic information within code graphs, as they are primarily designed for general graph-related tasks and focus more on adjacency information. They also fail to ensure computational efficiency while accounting for graph-text interactions.\nThis paper, therefore, introduces a new code graph-enhanced, structure-aware soft prompt tuning method for vulnerability detection, referred to as CGP-Tuning. It employs innovative type-aware embeddings to capture the rich semantic information within code graphs, along with a novel and efficient cross modal alignment module that achieves linear computational cost while incorporating graph-text interactions. The proposed CGP-Tuning is evaluated on the latest DiverseVul dataset and the most recent open-source code LLMs, CodeLlama and CodeGemma. Experimental results demonstrate that CGP-Tuning outperforms the best state-of-the-art method by an average of 3.5 percentage points in accuracy, without compromising its vulnerability detection capabilities for long source code.", "sections": [{"title": "I. INTRODUCTION", "content": "VULNERABILITY detection has long been a complex challenge in software engineering. It involves identifying exploitable mistakes and errors in given code snippets in advance of their misuse \u2013 essential for ensuring the reliability and integrity of software. One important approach to this is static analysis [1]. Static analysis converts source code into graph representations (i.e., code graph) based on code structure and semantics, then examines potential execution paths to identify vulnerabilities. While static analysis excels in identifying predefined exploitable patterns [2], its dependency on human-defined rules limits its adaptability to new or unknown vulnerabilities.\nIn recent years, deep learning has emerged as a promising alternative for vulnerability detection [3], as it can automatically learn patterns from vast amounts of code, offering more adaptive and scalable solutions for detecting vulnerabilities beyond the capabilities of traditional static analysis. Existing deep learning-based methods often follow a pre-training and fine-tuning paradigm [4]\u2013[6]. For example, a code language model such as CodeBERT [4] or CodeT5 [5] are pre-trained on large amounts of source code and related documentation to develop a general understanding of programming concepts. Then, the pre-trained model is fine-tuned on the vulnerability detection dataset, often using full parameter fine-tuning [4]\u2013[6] to classify source code as either vulnerable or secure [7]. However, this process treats source code as plain text, neglecting the rich graph-based structural information inherent in the code, including its syntax, control flow, and data flow. As a result, the model lacks a deep understanding of the code\u2019s structure.\nTo address this limitation, researchers have explored two main approaches. The first approach involves traversing code graphs, flattening them into sequences, and then pre-training the model on both the source code and the flattened code graph sequences. An example of this approach is GraphCodeBERT [6], which utilizes data flow graphs during pre-training along side source code sequences. The second approach leverages graph neural networks [8]\u2013[10], as seen in methods like DeepWukong [11]. DeepWukong constructs a program dependence graph from the source code, capturing both control flow and value flow information, and trains a graph neural network to classify source code as either vulnerable or secure. While earlier code language models [4]\u2013[7] and graph neural networks [8]\u2013[11] have shown effectiveness in detecting various vulnerabilities, their relatively small scale limits their ability to capture complex patterns.\nRecent advances in code large language models (LLMs) [12]\u2013[15], which often contain billions of parameters and are pre-trained on trillions of tokens, suggest that scaling the size of both the model and its dataset can significantly boosts performance across diverse tasks [16], including vulnerability detection [17]. However, their massive size and the quadratic complexity of standard self-attention [18] make traditional full parameter fine-tuning [4]\u2013[7] prohibitively expensive. As a result, parameter-efficient fine-tuning methods [19] have gained popularity as a more practical approach to adapt pre-trained code LLMs for specialized tasks.\nSoft prompt tuning [20]\u2013[22], is a parameter-efficient fine"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "This section introduces the background knowledge of graph enhanced soft prompt tuning methods and outlines the motivation behind the proposed CGP-Tuning."}, {"title": "A. Graph Neural Network", "content": "Graph neural networks are specialized neural networks designed for processing graph-structured data, where information is organized as a set of vertices (nodes) and edges (connections between nodes). Common variants include the graph convolutional network [8], graph attention network [9], and graph transformer network [10], among others.\nIn graph neural networks, each node is initialized with a node embedding that is iteratively updated through message passing, where each node aggregates information from its neighbouring nodes (i.e., adjacency information). This process enables the final node embeddings to capture both each node's intrinsic features and its local topology, reflecting the graph based structural information of source code, which can be applied to various software engineering tasks.\nFor node-level tasks like fault localization [33], node embeddings are individually leveraged for statement-level fault detection. For graph-level tasks like vulnerability detection [11], [34], [35], node embeddings are pooled into a single, comprehensive graph-level embedding for function-level vulnerability detection. Many graph-enhanced soft prompt tuning methods rely on graph neural networks to extract different node-level and graph-level features for LLMs [28]\u2013[31]."}, {"title": "B. Code Large Language Model", "content": "Unlike earlier code language models such as CodeBERT [4] and CodeT5 [5], which contain only millions of parameters, modern code LLMs like CodeLlama [12] and CodeGemma [13] have been scaled up to billions of parameters. This dramatic increase in scale allows modern code LLMs to more effectively learn programming patterns, syntax, and semantic relationships through pre-training on source code and related documentation spanning multiple programming languages [12]\u2013[15].\nAfter fine-tuning, code LLMs demonstrate state-of-the-art performance on various software engineering tasks such as code generation [36], where they assist developers by automating the creation of syntactically and semantically correct code, and code review [37], [38], where they analyze existing code to identify potential errors.\nHowever, the enhanced performance comes at a cost: modern code LLMs require significantly greater computational resources for training and inference. For example, simply storing an LLM with seven billion parameters consumes 28 gigabytes of memory under Float32 precision, making them less accessible to many users without specialized hardware. This limitation makes traditional full-parameter fine-tuning [4]\u2013[7], which updates all parameters in the model, impractical for many use cases."}, {"title": "C. Graph-Enhanced Soft Prompt Tuning", "content": "Soft prompt tuning [20]\u2013[22] adds soft prompt embeddings prepended to the input word embeddings to guide the behaviour of LLMs. During fine-tuning, only the trainable parameters of the soft prompt embeddings are updated to help the LLMs adapt to target downstream tasks (Figure 1). However, traditional soft prompt tuning methods, such as prompt tuning [22], are designed exclusively for the text modality and cannot incorporate information from other modalities, such as structural information in graphs.\nGraph-enhanced soft prompt tuning addresses this limitation by employing a graph neural network [28]\u2013[31] or flattening the graph into sequences [27] to encode graph features, followed by a cross-modal alignment module to align graph features with the LLM's latent space. Based on the type of cross-modal alignment module, these methods can be categorized into projector-based methods [27]\u2013[30] and cross-attention-based methods [31].\nHere, the graph neural network first extracts |V| final node embeddings (i.e., graph features) and aggregates them into a single graph embedding. The projector then maps this graph embedding into the latent space of the LLM as a graph-enhanced soft prompt embedding, which is concatenated with word embeddings (i.e., text features) and passed to the LLM for fine-tuning. It can be observed that the cross-modal alignment module in the projector-based method has linear computational cost proportional to |V|, but it fails to account for interactions between graph and text features.\nCross-attention-based methods (e.g., GNP [31]), as shown in Figure 2b, overcome this limitation by considering graph text interactions. They use the |V| final node embeddings from the graph neural network as queries to perform cross-attention with the word embeddings. The aligned node embeddings then undergo the same operations as those in the projector based method, as depicted in Figure 2a. Although the above operations cause the cross-modal alignment module to incur multiplicative computational cost proportional to |V|N+|V|, they enable it to effectively capture graph-text interactions. However, when processing long source code with complex code graphs, it requires significantly more memory to store the |VN-sized attention scores.\nTo address these limitations, the proposed CGP-Tuning aims to account for graph-text interactions while maintaining computational efficiency. Figure 2c provides an overview of the proposed cross-modal alignment module. In this design, soft prompt embeddings serve as a global context that first interacts with word embeddings to obtain text-aligned features (left part"}, {"title": "III. APPROACH", "content": "This section provides the details of the proposed CGP Tuning. As shown in Figure 3, the given source code is parsed into a code property graph. Each node in the graph is represented by a positional embedding, augmented with two trainable type-aware embeddings: one for node types and another for edge types, capturing the rich semantic information of the code property graph. These embeddings are fed into a graph encoder to extract graph features and generate the final node embeddings. The final node embeddings, the word embeddings for the source code, and the soft prompt tokens with their corresponding soft prompt embeddings are input into a cross-modal alignment module to produce graph enhanced soft prompt embeddings. These graph-enhanced soft prompt embeddings are then used to instruct the pre-trained code LLM, enabling it to determine whether the given source code is vulnerable or secure."}, {"title": "A. Preliminaries", "content": "Given a piece of source code, it is tokenized into a series of nt tokens and transformed into word embeddings $X_t \\in \\mathbb{R}^{n_t \\times d_{lm}}$ based on vocabulary W, where $d_{lm}$ is the dimension of the word embeddings. In the proposed CGP-Tuning, a total of ns soft prompt tokens are used, with their corresponding trainable soft prompt embeddings denoted as $X_s \\in \\mathbb{R}^{n_s \\times d_{lm}}$, sharing the same dimension as the word embeddings (Figure 3, parts A to C).\nThe code property graph of the source code is represented as G = (V, E, $ \\tau_V$, $ \\tau_E$), where V is the set of nodes and E is the set of edges. Each node $v_i \\in V$ corresponds to a set of code statements, and each edge $e_{ij} \\in E$ describes the relationships between nodes $v_i$ and $v_j$ (e.g., syntactic structure, control flow, or program dependencies). The function $ \\tau_V: V \\rightarrow T_V$ assigns a node type from a set of node types $T_V$ to each node $v_i \\in V$, while the function $ \\tau_E: E \\rightarrow T_E$ assigns an edge type from a set of edge types $T_E$ to each edge $e_{ij} \\in E$. Here, $ \\tau_V(v_i)$ and"}, {"title": "B. Type-Aware Embedding", "content": "To capture the semantic information of node types and edge types within the code property graph, this paper proposes type aware embeddings to map node types and edge types into their respective feature spaces as follows:\n$X_{TV} = \\{ x_{tv} \\mid tv \\in T_V \\} \\in \\mathbb{R}^{|T_V| \\times d_{tv}}$ (1)\n$X_{TE} = \\{ x_{te} \\mid te \\in T_E \\} \\in \\mathbb{R}^{|T_E| \\times d_{te}}$ (2)\nwhere $d_{tv}$ and $d_{te}$ represent the dimensions of the node and edge type embeddings, respectively. Specifically, given a node $v_i$ (edge $e_{ij}$), its corresponding node (edge) type embedding is represented as $x_{\\tau_V(v_i)} \\in \\mathbb{R}^{1 \\times d_{tv}}$ ($x_{\\tau_E(e_{ij})} \\in \\mathbb{R}^{1 \\times d_{te}}$).\nIn these type-aware embedding layers (Figure 3, parts E and F), the semantic relationships between node types and edge types are encoded separately into their respective embeddings. These type-aware embeddings allow the graph neural network to distinguish between different types of nodes and edges during message passing and feature aggregation. By mapping the node and edge types into trainable dense embeddings, the model can effectively capture type-specific semantic information, which is crucial for understanding the structural and functional dependencies within the code property graph.\nThen, for each node $v_i$, a sinusoidal positional embedding [18] $p_{ei}$ is computed, and the set of these embeddings is denoted as $PE = \\{ p_{ei} \\mid i = 1, . . . , |V| \\} \\in \\mathbb{R}^{|V| \\times d_{pe}}$ (Figure 3, part G), which ensures that all node embeddings are unique and well-suited for length extrapolation. The dimensions of the node type embeddings and sinusoidal positional embeddings are set to match the dimension of the word embeddings used in the code LLM, i.e., $d_{tv}$ = $d_{pe}$ = $d_{lm}$.\nThe initial node embeddings $H_1 = \\{ h_{1,v_i} \\mid i = 1, . . . , |V| \\} \\in \\mathbb{R}^{|V| \\times d_{lm}}$ are obtained by adding the node type"}, {"title": "C. Graph Encoder", "content": "Part H of Figure 3 shows the overall structure of the graph encoder. Specifically, this graph encoder employs a basic block composed of the graph attention network (GAT) [9], layer normalization (LN) [39], the ReLU activation function [40], and a dropout layer [41], defined as follows:\n$H^(l+1) = Dropout(ReLU(LN(GAT(H^(l), X_{ \\tau_E(E)})))) $ (4)\nTo alleviate the over-smoothing problem [42], a residual connection [43] is employed within each basic block, defined as follows:\n$H^(l+1) = H^(l+1) + H^(l), l = 1, . . . , L $ (5)\nwhere L is the total number of basic blocks.\nThe initial input to these basic blocks, $H^(1)$, is $H_1$, and the output after stacking L basic blocks is denoted as $H_2 \\in \\mathbb{R}^{|V| \\times d_{lm}}$, where $H_2$ corresponds to $H^(L)$. These basic blocks are stacked to iteratively extract graph features.\nTo obtain a maximum number of nk final node embeddings $H_3 \\in \\mathbb{R}^{n_k \\times d_{lm}}$, a pooling layer [44] is applied as follows:\n$H_3 = POOL(H_2)$ (6)\nThis pooling layer ensures that the most informative node embeddings are retained. Specifically, it selects up to nk node embeddings based on a learned scoring mechanism, which prioritizes nodes that contribute the most to vulnerability detection. When the graph contains fewer than nk nodes, all"}, {"title": "D. Cross-Modal Alignment Module", "content": "To align the extracted graph features $H_3$ with the text features $X_t$, this paper proposes an efficient cross-modal alignment module, which is depicted in part J of Figure 3. This module uses a shared-parameter multi-head attention mechanism.\nSpecifically, the soft prompt embeddings $X_s$ serve as the global context (Figure 3, part C), acting as queries for cross attention with the text features $X_t$ (Figure 3, part B), which serve as the key-value pairs, producing the text-aligned features $H_4 \\in \\mathbb{R}^{n_s \\times d_{lm}}$. Subsequently, $H_4$ is used as the queries for cross-attention with the graph features $H_3$ (Figure 3, part I), yielding the final graph-aligned features $H_5 \\in \\mathbb{R}^{n_s \\times d_{lm}}$, as defined below:\n$H_4 = MultiHeadAttention(Q = X_s, K = X_t, V = X_t)$ (7)\n$H_5 = MultiHeadAttention(Q = H_4, K = H_3, V = H_3)$ (8)\nFinally, a feed-forward network acts as a projector, mapping the graph-aligned features into the latent space of the code LLM as the final graph-enhanced soft prompt embeddings $Z \\in \\mathbb{R}^{n_s \\times d_{lm}}$ (Figure 3, part K):\n$Z = FeedForwardNetwork(H_5)$ (9)\nThroughout the entire process, the number of soft prompt tokens, ns, remains constant; only the number of source code tokens and number of nodes varies with the input, resulting in linear computational costs proportional solely to $n_k + n_t$. The corresponding graph-enhanced soft prompt embeddings, Z, now contain the graph-based structural information of the source code."}, {"title": "E. Graph-Enhanced Soft Prompt Tuning", "content": "Given the graph-enhanced soft prompt embeddings Z and the word embeddings of source code tokens Xt, these embeddings are concatenated and fed into the pre-trained code LLM, producing the output logits $Y \\in \\mathbb{R}^{(n_s+n_t) \\times |W|}$, as defined below:\n$Y = CodeLLM([Z; X_t])$ (10)\nThe output logits Y are then used to compute the cross entropy loss L with the ground truth values $ \\hat{Y} $, as defined below:\n$\\mathcal{L} = - \\frac{1}{|T|} \\sum_{i \\in T} \\sum_{j=1}^{|W|} \\hat{y}_{ij} \\log y_{ij}$ (11)\nwhere T is the set of indices excluding the soft prompt tokens and source code tokens for supervised fine-tuning and |W| is the vocabulary size. Only the parameters of the type-aware embeddings, the graph encoder, and the cross modal alignment module are updated via backpropagation. This process enhances the code LLM\u2019s understanding of code"}, {"title": "IV. EXPERIMENT DESIGN", "content": "This section provides details of the experimental design, including research questions, dataset descriptions, baselines, evaluation metrics, and implementation details."}, {"title": "A. Research Questions", "content": "To evaluate the effectiveness of the proposed CGP-Tuning method, three research questions are posed:\n\u2022 RQ1: Does incorporating type-aware embeddings and the cross-modal alignment module improve the performance of CGP-Tuning?\n\u2022 RQ2: Does CGP-Tuning outperform traditional soft prompt tuning methods as well as state-of-the-art graph enhanced soft prompt tuning methods in vulnerability detection?\n\u2022 RQ3: Does CGP-Tuning reduce computational costs in its cross-modal alignment module by sacrificing performance, particularly when dealing with long source code?"}, {"title": "B. Dataset and Code Property Graph", "content": "This paper utilizes the latest vulnerability detection dataset, DiverseVul [45], which is extracted from 809 projects and contains 150 distinct Common Weakness Enumerations. It is divided into training, validation, and testing subsets in a 70%-15%-15% ratio. The entire dataset comprises over 330,000 samples, of which more than 94% are non-vulnerable, resulting in a highly imbalanced class distribution. To address this imbalance, under-sampling is applied to each subset by removing non-vulnerable samples [46].\nThe code property graph of each data sample is generated using Joern v2.0.448\n, a tool capable of parsing various programming languages into code property graphs.\nTo prevent out-of-memory errors during training caused by large graphs, samples with over 300,000 nodes and 30,000 edges are excluded from the training and validation sets."}, {"title": "C. Baselines and Evaluation Metrics", "content": "This paper employs accuracy, weighted precision, weighted recall, and weighted F1-score as evaluation metrics. The seven-billion-parameter base versions of CodeLlama [12] and CodeGemma [13] are used as the tested code LLMs. Both of them are based on the self-attention mechanism and transformer architectures [18]. To obtain structured output from code LLMs, a constrained decoding strategy similar to Jsonformer\nis adopted, where the classification result is"}, {"title": "V. RESULTS AND ANALYSIS", "content": "This section provides a summary and analysis of the results from CGP-Tuning and other methods, with all results rounded to four decimal places for clarity."}, {"title": "A. RQ1: Does incorporating type-aware embeddings and the cross-modal alignment module improve the performance of CGP-Tuning?", "content": "To assess the effectiveness of the type-aware embeddings and the cross-modal alignment module in the proposed CGP-Tuning, this section conducts an ablation study by removing one component at a time. The results are derived from the validation dataset to avoid any leakage into the test dataset.\nAs presented in Table III, removing each component of CGP-Tuning reduces the performance with both CodeLlama and CodeGemma. This validates the effectiveness of the type-aware embeddings and the cross-modal alignment module. In particular, removing the cross-modal alignment module causes the greatest performance drop, with accuracy decreasing by 18.75% on CodeLlama and 14.54% on CodeGemma. This is most likely because plain soft prompt embeddings need further transformation to align with graph and text features, and removing the cross-modal alignment module eliminates this process. It also highlights the importance of graph-based structural information in source code for enhancing the performance of code LLMs in vulnerability detection.\nIn type-aware embeddings, removing the node type embeddings results in a noticeable performance decline in vulnerability detection, with a drop of approximately one percentage point across all metrics. Conversely, removing the edge type embeddings has a less pronounced impact on overall performance. This discrepancy likely arises because the model can more effectively infer edge type information from node and adjacency information, making the absence of node type information more detrimental than that of edge type information. These findings clearly demonstrate that node and edge types in the code property graph do contain valuable semantic information, emphasizing the effectiveness of type"}, {"title": "B. RQ2: Does CGP-Tuning outperform traditional soft prompt tuning methods as well as state-of-the-art graph-enhanced soft prompt tuning methods in vulnerability detection?", "content": "To evaluate whether the proposed CGP-Tuning achieves the best overall performance in vulnerability detection, Table IV presents the performance evaluation results of all methods. The results of zero-shot prompting reveal that, even without fine tuning, code LLMs can correctly identify some vulnerabilities due to their in-context learning ability. However, different code LLMs demonstrate significant variations in their in-context learning capabilities. In vulnerability detection, CodeLlama significantly outperforms CodeGemma, achieving an accuracy of approximately 50.6%, while CodeGemma achieves only about 46.81%, which is worse than random guessing.\nThe disparity in in-context learning capabilities between these two types of code LLMs further impacts the performance of GRACE. When using CodeLlama, GRACE outperforms zero-shot prompting, demonstrating that CodeLlama\u2019s superior in-context learning abilities enable it to derive useful insights from structural information within the context, thereby enhancing its vulnerability detection capabilities. However, for CodeGemma, a code LLM with weaker in-context learning capabilities, incorporating graph-based structural information as context tends to disrupt its predictions, resulting in a significant decline in performance compared to zero-shot prompting.\nThe performance of the traditional soft prompt tuning method, prompt tuning, is also heavily influenced by the in-context learning capabilities of code LLMs. For code LLMs with stronger in-context learning abilities, such as CodeLlama, prompt tuning offers limited advantages. While prompt tuning surpasses GRACE in precision, it underperforms in other metrics. This disparity arises because CodeLlama\u2019s advanced in-context learning capabilities allow it to more effectively"}, {"title": "C. RQ3: Does CGP-Tuning reduce computational costs in its cross-modal alignment module by sacrificing performance, particularly when dealing with long source code?", "content": "To evaluate whether CGP-Tuning reduce computational costs in its cross-modal alignment module by sacrificing performance, this part further examines the performance differences among different methods when the input source code is very long. Specifically, samples are sorted by their number of tokens, and the top 7% of the longest samples are retained. Due to differences in the tokenizers used by different code"}, {"title": "VI. RELATED WORK", "content": "This section summarizes the literature related to the topic of this work, focusing on how traditional methods leverage graph-based structural information and how the latest LLMs incorporate such information."}, {"title": "A. Graph-Based Structural Info. in Traditional Methods", "content": "Vulnerability detection aims to identify vulnerabilities in source code. This is typically achieved by constructing code graphs that represent the structural information within the code. Static analysis techniques often involve building such code graphs to model control and data flow information [1], [49], which are useful for detecting specific issues like memory leaks [2] and null pointer dereferencing [50]. However, static analysis often struggles with novel or complex vulnerabilities that fall outside predefined patterns due to its rule-driven nature.\nTo enhance generalization, recent advancements have focused on deep learning-based approaches [3], particularly by using pre-trained code language models such as CodeBERT [4] and CodeT5 [5], which can be fine-tuned to classify source code as either vulnerable or secure [7]. However, these models often treat code as plain text, failing to capture the rich, graph based structural information inherent in source code.\nTo incorporate such structural information, one approach is to traverse the code graph, flatten it into a sequence, and pre-train the model with a link prediction task to capture adjacency information, as seen in GraphCodeBERT [6]. GraphCodeBERT incorporates data flow graphs during pre training and can then be fine-tuned for vulnerability detection [51]. However, scaling laws [16] suggest that traditional code language models are constrained by their model size and the scale of dataset used during pre-training, limiting their overall capabilities.\nAnother approach involves using graph neural networks [8]\u2013[10]. Unlike GraphCodeBERT, graph neural networks can directly incorporate adjacency information from code graphs. By constructing various graph representations (e.g., control flow graphs, data flow graphs, and abstract syntax trees) from source code, they effectively capture the structural information necessary to identify potential vulnerabilities [11], [34], [35]. However, graph neural networks face challenges like over smoothing, which limits their network depth and ability to capture complex dependencies [42]. Additionally, they are less effective at capturing long-range contextual information compared to code language models."}, {"title": "B. Graph-Based Structural Info. in Large Language Models", "content": "With the rise of LLMs, recent research has focused on integrating graph-based structural information into LLMs. One approach is to describe the graph\u2019s adjacency information in natural language, leveraging the in-context learning ability of LLMs to incorporate this graph-based structural information [24]. GRACE [25] adopts this strategy by first representing the source code with code property graphs [32], then describing the adjacency information in natural language as additional context for LLMs, which has been shown to improve vulnerability detection performance. However, this approach heavily relies on the in-context learning capabilities of LLMs and is computationally expensive, primarily due to the quadratic complexity of the self-attention mechanism [18].\nAnother way to extract graph features is to use a graph neural network [28]\u2013[31], [52] or flattened graph sequences [27], and fine-tune LLMs to interpret the extracted graph features. A straightforward method for fine-tuning the LLMs is to update all the parameters using full parameter fine-tuning [4]\u2013[6] or low-rank adaptation [53]. Zhang et al. [52] verify the effectiveness of this approach on various code-related tasks, but it demands significant computational resources, including eight NVIDIA A100 GPUs.\nA more resource-friendly alternative is soft prompt tuning, in which the pre-trained LLM remains frozen, and only the parameters of the soft prompt embeddings are updated [26]. Since the LLM is frozen, cross-modal alignment modules are therefore essential to incorporate the graph features into the soft prompt embeddings. These modules include projector-based methods and cross-attention-based methods. In projector-based methods, a projector based on feed-forward network is used to project the extracted graph features into the LLM\u2019s latent space. In node-level tasks, the projector maps node embeddings individually [27]\u2013[29], while in graph level tasks, the node embeddings are pooled into a single graph embedding, which is then projected into the LLM\u2019s latent space [30]. Projector-based methods are highly efficient but neglect the correlation between graph features and text features.\nOn the other hand, cross-attention-based methods [31] incorporate cross-attention mechanisms to model graph-text interactions more effectively, but they often incur high computational costs due to the need to calculate and store large attention score matrices. Moreover, existing graph-enhanced soft prompt tuning methods are designed for general graph-related tasks [26]\u2013[29], [31] and fail to capture the rich semantic information in code graphs. The proposed CGP-Tuning method seeks to overcome these limitations by accounting for graph text interactions while ensuring computational efficiency. It also introduces type-aware embeddings to capture the semantic information in code graphs, providing an improved solution for vulnerability detection with modern code LLMs."}, {"title": "VII. LIMITATIONS AND THREATS TO VALIDITY", "content": "In this section, some major limitations and threats that could impact the validity of the results are discussed, which this paper aims to address in the near future."}, {"title": "VIII. CONCLUSION", "content": "This paper introduces CGP-Tuning, a code graph-enhanced, structure-aware soft prompt tuning method designed to improve code LLMs' understanding of code structures for better vulnerability detection. Experimental results demonstrate the importance of incorporating graph-based structural information for boosting the vulnerability detection performance of code LLMs. Unlike traditional graph-enhanced soft prompt tuning methods, CGP-Tuning's cross-modal alignment module effectively captures graph-text interactions while maintaining computational efficiency. Moreover, when compared to various state-of-the-art methods, CGP-Tuning achieves superior performance and remains effective in detecting vulnerabilities within long source code. It is hoped that the proposed CGP-Tuning will offer the community a better alternative for fine-tuning code LLMs, enabling more accurate vulnerability detection by integrating structural and semantic information within code graphs, and ultimately contributing to more robust and intelligent security analysis techniques in software development."}]}