{"title": "On the Shortcut Learning in Multilingual Neural Machine Translation", "authors": ["Wenxuan Wang", "Wenxiang Jiao", "Jen-tse Huang", "Zhaopeng Tu", "Michael Lyu"], "abstract": "In this study, we revisit the commonly-cited off-target issue in multilingual neural machine translation (MNMT). By carefully designing experiments on different MNMT scenarios and models, we attribute the off-target issue to the overfitting of the shortcuts of (non-centric, centric) language mappings. Specifically, the learned shortcuts biases MNMT to mistakenly translate non-centric languages into the centric language instead of the expected non-centric language for zero-shot translation. Analyses on learning dynamics show that the shortcut learning generally occurs in the later stage of model training, and multilingual pretraining accelerates and aggravates the shortcut learning. Based on these observations, we propose a simple and effective training strategy to eliminate the shortcuts in MNMT models by leveraging the forgetting nature of model training. The only difference from the standard training is that we remove the training instances that may induce the shortcut learning in the later stage of model training. Without introducing any additional data and computational costs, our approach can consistently and significantly improve the zero-shot translation performance by alleviating the shortcut learning for different MNMT models and benchmarks.", "sections": [{"title": "1. Introduction", "content": "Multilingual neural machine translation (MNMT) aims to translate between any two languages with a unified model (Johnson et al., 2017; Aharoni et al., 2019). It is appealing due to its efficient deployment and effective cross-lingual knowledge transfer, which enables translations between unseen language pairs, i.e., zero-shot translation. Zero-shot translation is an important capability of MNMT models since it covers most of the possible translation directions, which are difficult and expensive to be covered by the human-annotated training data. However, previous studies demonstrate that zero-shot translation often suffers from the off-target issue (Ha et al., 2016; Gu et al., 2019; Zhang et al., 2020), where the MNMT model tends to translate into other languages rather than the expected target language.\nIn this paper, we revisit the off-target issues in MNMT with a single centric language, where parallel training data is only available for directions between the centric language and other languages. This setting has been commonly adopted in research (Johnson et al., 2017; Gu et al., 2019; Zhang et al., 2020; Tang et al., 2021; Wenzek et al., 2021) and commercial scenes (e.g., business export to overseas). In such scenarios, zero-shot translation aims to translate between non-centric languages. We vary the centric language of training dataset, and find that the off-target issues for zero-shot translation are mainly in the corresponding centric language in all cases. In addition, although multilingual pre-training has been widely adopted to improve the supervised translation performance (Liu et al., 2020; Tang et al., 2021),\nwe find that it sacrifices the zero-shot translation performance by introducing remarkably more off-target issues. This finding does not conform to previous studies (Brown et al., 2020; Conneau et al., 2020), which have shown that pretraining improves the generalization ability in zero-shot scenarios.\nTo better understand these observations, we analyze the learning dynamics and find that the performance of zero-shot translation is fluctuating during training. While MNMT models keep improving cross-lingual transformation ability for zero-shot translation, they ignore the zero-shot language mapping in model training by overfitting to the shortcut of supervised (non-centric, centric) language mapping, which mainly occurs at the late stage of training. Multilingual pretraining accelerates and aggravates the shortcut learning by introducing another type of shortcut (i.e., the copy of source language) due to the denoising auto-encoding objective (Liu et al., 2020). The commonality between the two types of shortcuts, i.e., both ignore the target language tag when the source languages are non-centric, enables a fast transformation from the copy pattern embedded in the pretraining initialization to the (non-centric, centric) mapping pattern embedded in the MNMT data during finetuning. Accordingly, the off-target issue becomes more severe for MNMT models with pretraining (e.g., 94.6% of zero-shot translations are off-target).\nBased on these understandings, we propose a simple and effective training strategy, named generalization training, to break the shortcut data patterns. The starting point of our approach is an observation: NMT models tend to gradually forget previously learned knowledge and swing to fit the new training examples during training (Shao and Feng, 2022). We leverage the forgetting nature of model training to forget the overfitted (non-centric, centric) language mapping. Specifically, we divide training process of MNMT into two phases: (1) standard training phase (the first N G"}, {"title": "2. Related Work", "content": "Recent studies have shown that many deep learning problems can be seen as different symptoms of the same underlying problem: shortcut learning (Geirhos et al., 2020). Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions. Our study connects the commonly-cited off-target issues in MNMT to shortcut learning: the shortcut of supervised language mapping fails to transfer to zero-shot translation.\nShortcut learning has been explored in many NLP tasks including language inference (Niven and Kao, 2019), reading comprehension (Lai et al., 2021), question answering (Ko et al., 2020), evaluation of text generation (Durmus et al., 2022). Shortcuts learned by the models usually take the form of learning the superficial correlation between simple statistics and the label, which is also known as non-robust features. For example, in reading comprehension task, models mainly focus on the lexical matching of words between the question and the original passage (Lai et al., 2021).\nSince shortcut learning overfits to the artifacts of the training data, it will hurt model performance when fed with out-of-distribution data and hurt the robustness against adversarial attacks (Du et al., 2022).\nShortcut learning has rarely been studied in multilingual neural machine translation. The most relevant work is by Gu et al. (2019), which attributed the poor performance of zero-shot translation to the spurious correlation in data. Our work differs from theirs in several aspects: (1) They only showed that zero-shot translation tends to translate into \"wrong target languages\", while we refine the \"wrong target languages\u201d to the centric languages, which allows us to locate the underlying reasons (i.e., the commonly-used single centric language setting). (2) We find that multilingual pretraining harms the performance of zero-shot translation, which has not been revealed in their study. (3) They adopted back-translation and decoder pretraining to regularize the spurious correlation, which require additional computation costs for data augmentation and model training. In contrast, our generalization training is more efficient by only making a slight change to the standard training."}, {"title": "3. Preliminary", "content": "Neural Machine Translation is a representative task in Artificial Intelligence that uses neuron networks to conduct translation (Sutskever et al., 2014; Bahdanau et al., 2014; Mi et al., 2022; Iranzo-S\u00e1nchez et al., 2021; Zhang and Bai, 2023; Yu et al., 2021). Multilingual neural machine translation aims to translate between any two languages with a unified model (Johnson et al., 2017; Aharoni et al., 2019). Specifically, an MNMT model is trained on a dataset consisting of parallel sentences in multiple language pairs. Given a source sentence xs in language s and its translation y' in language t, the MNMT model translates as below:\nHenc = Encoder([x]),\nHdec = Decoder([y'], Henc).\nThe model is trained with maximum likelihood estimation on the multilingual datasets:\nL = - \\sum_{i=1}^{N} \\sum_{(x,y) \\in D_i} log P([y']|Hdec(x, y)),\nwhere N is the number of language pairs and D\u2081 is the training instances in the i-th language pair.\nOne appealing capability of MNMT is translation between language pairs that do not exist in the training data, i.e., zero-shot translation. For example, an MNMT model trained on German-English and English-French data is able to translate from German to French. However, the performance of zero-shot translation generally lags behind the supervised translation due to the lack of explicit signal during training. Improving zero-shot translation is critical for MNMT, and has received a lot of attention in recent years (Gu et al., 2019; Zhang et al., 2020; Wang et al., 2021)."}, {"title": "3.2. Multilingual Pretraining", "content": "There has been a wealth of research over the past several years on sequence-to-sequence (Seq2Seq) pretraining models for machine translation, e.g., MASS (Song et al., 2019), BART (Lewis et al., 2020), and mBART (Liu et al., 2020). Generally, Seq2Seq pretraining model (e.g., mBART) shares the same architecture and loss format with standard MNMT models. The main difference is that the source sentence is a corruption of the target sentence in the same language s: x = g(x), where g is a noising function (e.g., randomly masking or reordering tokens).\nSeq2Seq pretraining models that are trained on large-scale multilingual language data (i.e., mBART), are generally used to initialize the MNMT models, leading to significant improvement on translation performance across various language pairs. However, recent studies identified several critical side-effects of Seq2Seq pretraining models due to the objective discrepancy between pretraining and translation, e.g., over-copying issues (Liu et al., 2021b) and over-estimation issues (Wang et al., 2022a). The pretraining objective learns to reconstruct a few source tokens (i.e., the corrupted tokens) and copy most of them, while the translation objective learns to translate text from source language to target language. In this work, we identify another side-effect of pretraining model on the off-target issues in multilingual translation."}, {"title": "3.3. Experimental Setup", "content": "The training datasets (see Table 1 for data statistics) include:\nWe construct six balanced datasets, where each distinct language from (En, De, Fr, Ro, Ja, and Zh) serves as the single centric language. We sample 1.0M sentence pairs from the CCMatrix (Schwenk et al., 2021) data for each language pair (Balanced CC6-X, 5M).\nWe simulate a common situation in MNMT with imbalanced training data. We randomly sample the subsets from the CCMatrix data to construct an imbalanced English-centric dataset (Imbalanced CC16-En, 11M) that consists of 16 languages.\n(Zhang et al., 2020) propose OPUS-100 dataset that consists of 55M English-centric sentence pairs covering 100 languages. Previous studies (Wang et al., 2022b) have revealed that for 5.8% of the training examples in the OPUS100 data, the target sentences are in the source language. We select the 50 languages used in mBART50 (Tang et al., 2021) to construct an imbalanced dataset (Noisy ImBalanced OPUS50-En, 36M).\nTo eliminate the content bias across languages, we evaluate the performance of multilingual translation models on the multi-way Flores valid/test set (Goyal et al., 2021), which contains 997/1012 sentences translated into 101 languages. We report the results of both BLEU scores (Papineni et al., 2002) and off-target ratios (OTR) for both supervised and zero-shot translation. For example, the CC6-En dataset contains 10 supervised directions (i.e., En-X and X-En) and 20 zero-shot directions (i.e., X-X). To calculate the off-target ratio in translation output, we employ the langid library\u00b9, the most widely used language identification tool with 93.7% accuracy on 7 dataset across 97 languages, to detect the language of generated sentences.\nTo support both training from scratch and finetuning from pretrained models, we adopt an MNMT model with the same architecture as the mBART50 model (Tang et al., 2021), which consists of 12 encoder layers and 12 decoder layers with 1024 dimensions. We follow the common practices to attach the source language tag to encoder and the target language tag to decoder (Tang et al., 2021; Fan et al., 2021). We use the vocabulary of mBART that is built for 100 languages, which can enable the scaling of languages. On the CC-6 datasets, we train the models with 65K tokens per batch (4096 tokens \u00d7 16 GPU) for 100K updates. For"}, {"title": "4. Observing Shortcut Learning", "content": "In this section, we establish that the commonly-used datasets with a single centric language is questionable when used for conducting zero-shot translation. We first revisit the off-target issues on the single-centric datasets (\u00a7 4.1), and connect them to the shortcut learning on the supervised (non-centric, centric) language mapping (\u00a7 4.2). We finally empirically analyze the reasons behind the shortcut learning in model training (\u00a7 4.3)."}, {"title": "4.1. Revisiting Off-Target Issues", "content": "We revisit the off-target issue from two angles by: (1) varying the centric languages of multilingual translation datasets; and (2) training MNMT models from scratch or finetuning from the mBART50 pretraining model, to offer a more comprehensive understanding, as listed in Table 2.\nWhile the off-target ratio (OTR) varies across different datasets, we find that almost all the off-target translations are directed to the centric languages. Our study connects the off-target issue to the centric language of datasets, which has not been revealed in previous studies.\nPretraining consistently improves the performance of supervised translation, while harms that of zero-shot translation by introducing more off-target issues. For example, pretraining"}, {"title": "4.2. Shortcut Learning of Language Mapping", "content": "Recent work has shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to over-fitting and lacking of generalization (Schwartz and Stanovsky, 2022). Starting from the finding, we conjecture that MNMT model overfits the supervised language mapping, and lacks generalization of zero-shot language mapping. During training, all non-centric languages are translated into the centric language, which may allow the model to overfit the shortcut of (non-centric, centric) language mapping.\nTo validate that MNMT model overfits the shortcut of (non-centric, centric) language mapping, we manipulate the target language tags and identify the language of the generated texts. Table 3 lists the averaged distributions of output languages for translating non-centric languages with different target language tags. Given input sentences in French, the pretraining model outputs French sentences regardless of the given target tags (e.g., \u201cFr (100%)\u201d), indicating that pretraining model suffers from more severe shortcut learning problem. This is intuitive, since the shortcut in pretraining copy of the source language, is easier to learn. Comparing with the vanilla MNMT model (i.e., \"w/o Pretrain\"), the pretrained MNMT model (\u201cw/ Pretrain\u201d) translates all non-centric French sentences into the centric language English for all target tags, showing that pretraining initialization aggravates the shortcut learning in MNMT. The malfunction of target language tag confirms our research hypothesis on the connection between off-target issues and shortcut of supervised language mapping."}, {"title": "4.3. Shortcut Learning in Model Training", "content": "The above results imply that MNMT models tend to ignore the given target language tag for zero-shot translation in inference. In this section, we analyze the training process of MNMT models and show how the model overfits to the shortcut of (non-centric, centric) language mapping. Unless otherwise stated, all results are reported on the Flores Validation Set for the CC6-En data. The results on CC6-Ro data can be found in Figure 7 in Appendix, where all conclusions still hold."}, {"title": "5. Mitigating Shortcut Learning", "content": "In this section, we introduce a simple approach to alleviate the shortcut learning in MNMT (\u00a7 5.1). We then demonstrate that our approach improves the zero-shot performance by enhancing the generalization ability of MNMT models (\u00a7 5.2). We finally validate the universality of our approach in different multilingual translation scenarios (\u00a7 5.3)."}, {"title": "5.1. Approach", "content": "One straightforward way to improve zero-shot translation is to construct pseudo parallel data for all zero-shot directions (Gu et al., 2019; Zhang et al., 2020). However, such approaches are computationally prohibitive for tasks with a large number of languages. For example, the OPUS50-En dataset consists of 49 x 48 = 2352 zero-shot directions. Another direction is to modify the model architecture (Liu et al., 2021a; Wu et al., 2021) without introducing additional training costs. Different from these directions, we propose to improve the model training to alleviate the shortcut learning.\nThe starting point for our approach is an observation: NMT models suffer from catastrophic forgetting during training, where the models tend to gradually forget previously learned knowledge and swing to fit the new data that may have a different distribution (Shao and Feng, 2022). We can leverage the forgetting nature of model training to forget the shortcuts.\nOur approach divides the training process with N steps into two phases:\nFor the first N G training steps, we follow the standard pipeline to train the models on the full training data.\nFor the last G steps, we train the models only on the training example of (centric, non-centric) language pairs.\nAdvantages. We remove the training examples of (non-centric, centric) language pairs from the generalization training phase. The potential advantages are three-fold:"}, {"title": "5.2. Ablation Study", "content": "In this section, we provide some insights where the generalization training improves zero-shot translation by alleviating the off-target issues. All results are reported on the Flores validation set using the balanced CC6-En data."}, {"title": "5.3. Main Results", "content": "In this section, we validate the effectiveness of our approach on different MNMT benchmarks.\nTable 4 lists the translation results on different training datasets to simulate different MNMT"}, {"title": "7. Appendix", "content": "Figure 7 shows the learning curves of models trained on balanced CC6-Ro data. Similar to the results on balanced CC6-En data, MNMT models keep improving the performance of supervised translation, but sacrifice the generalization ability on zero-shot translation."}, {"title": "7.2. Experimental Results on Small Scale Model Architectures", "content": "To validate our generalization training method in different model architecture, we conduct the experiments on Transformer-base architecture, which consists of 6 encoder layers and 6 decoder layers with 512 dimensions. Table 10 shows the translation performance of the model trained on balanced and imbalance CC6 English dataset without and with our method. The conclusions still hold such that a) off-target translations are mainly in the centric language, b)Our method can improve the zero-shot translation performance and alleviate the off-target issues."}, {"title": "7.3. Detailed Results on Balanced CC6 Datasets", "content": "Table 11 lists the detailed results on the balanced datasets with different centric languages."}, {"title": "7.4. Large Language Models and Off-Target Issues", "content": "Large Language Models, such as ChatGPT and Gemini, have been proposed and developed recently. Previous works have evaluated their translation performance in both bi-lingual setting (Jiao et al., 2023) and multilingual setting (Zhu et al., 2023). According to the report(Wu et al., 2024), LLMs, such as GPT-4, still suffer from off-target issues. This can be attributed to the dominance of the English corpus in the training and alignment data (OpenAI, 2023), which also belongs to shortcut learning from a data perspective."}, {"title": "7.5. Multiple Centric Languages Setting Does not Suffer From Off-Target Issues", "content": "We conduct experiments on datasets with multiple centric languages (e.g., \u201cEn+De\" in Table 2), where the language mapping patterns of (non-centric, centric) are more complex and thus are difficult to overfit. For example, sentences in French are translated into two different centric languages (e.g., English and German for the \u201cEn+De\u201d data). As listed in Table 2, off-target issues never occur on datasets with multiple centric languages. However, this setting is not practical due to the lack of training data. Based on this observation, we propose a novel and data effecient method to mitigate the off-target issue without the need of any multiple centric languages data."}]}