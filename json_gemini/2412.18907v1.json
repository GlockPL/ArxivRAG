{"title": "EC-DIFFUSER: MULTI-OBJECT MANIPULATION VIA ENTITY-CENTRIC BEHAVIOR GENERATION", "authors": ["Carl Qi", "Dan Haramati", "Tal Daniel", "Aviv Tamar", "Amy Zhang"], "abstract": "Object manipulation is a common component of everyday tasks, but learning to manipulate objects from high-dimensional observations presents significant challenges. These challenges are heightened in multi-object environments due to the combinatorial complexity of the state space as well as of the desired behaviors. While recent approaches have utilized large-scale offline data to train models from pixel observations, achieving performance gains through scaling, these methods struggle with compositional generalization in unseen object configurations with constrained network and dataset sizes. To address these issues, we propose a novel behavioral cloning (BC) approach that leverages object-centric representations and an entity-centric Transformer with diffusion-based optimization, enabling efficient learning from offline image data. Our method first decomposes observations into an object-centric representation, which is then processed by our entity-centric Transformer that computes attention at the object level, simultaneously predicting object dynamics and the agent's actions. Combined with the ability of diffusion models to capture multi-modal behavior distributions, this results in substantial performance improvements in multi-object tasks and, more importantly, enables compositional generalization. We present BC agents capable of zero-shot generalization to tasks with novel compositions of objects and goals, including larger numbers of objects than seen during training. We provide video rollouts on our webpage: https://sites.google.com/view/ec-diffuser.", "sections": [{"title": "INTRODUCTION", "content": "Object manipulation is an integral part of our everyday lives. It requires us to reason about multiple objects simultaneously, accounting for their relationships and how they interact. Learning object manipulation is a longstanding challenge, especially when learning from high-dimensional observations such as images. Behavioral Cloning (BC) has shown promising results in learning complex manipulation behaviors from expert demonstrations (Chi et al., 2023; Lee et al., 2024). Recent works (Collaboration et al., 2023; Du et al., 2023; 2024; Zhu et al., 2024) have leveraged vast amount of offline data paired with large models to learn policies from pixel observations. Although scale has proven to be effective in some settings, it is not the most efficient way to deal with problems with combinatorial structure. For instance, despite their impressive generation results, Zhu et al. (2024) require 2000+ GPU hours to train a model on an object manipulation task. In this work, we incorporate object-centric structure in goal-conditioned BC from pixels to produce sample-efficient and generalizing multi-object manipulation policies.\nMulti-object environments pose significant challenges for autonomous agents due to the combinatorial complexity of both the state and goal spaces as well as of the desired behaviors. Assuming an n-object environment with m possible single-object goal configurations, there are $m^n$ total goal configurations and n! orderings of the objects to manipulate in sequence. When learning from offline data, one cannot expect an agent to encounter all possible combinations of objects and desired tasks during training due to either time/compute constraints or lack of such data. We therefore require that our agent generalize to novel compositions of objects and/or tasks it has seen during training, i.e. require compositional generalization (Lin et al., 2023)."}, {"title": "RELATED WORK", "content": "Multi-object Manipulation from Pixels: Previous work using single-vector representations of image observations for control (Levine et al., 2016; Nair et al., 2018; Hafner et al., 2023; Lee et al., 2024) fall short compared to methods that leverage object-centric representations (Zadaianchuk et al., 2021; Yoon et al., 2023; Haramati et al., 2024; Ferraro et al., 2023; Zhu et al., 2022; Shi et al., 2024) in multi-object environments. Several works have studied compositional generalization in this setting. Collaboration et al. (2023); Du et al. (2023; 2024); Zhu et al. (2024) learn directly from pixel observations with large-scale networks, data and compute and rely on scale for possible generalization and transfer. Other works have proposed approaches that account for the underlying combinatorial structure in object manipulation environments to achieve more systematic compositional generalization (Zhao et al., 2022; Chang et al., 2023; Haramati et al., 2024), requiring significantly less data and compute. We continue this line of work, dealing with the setting of learning from demonstrations and the various distinct challenges it introduces.\nDiffusion Models for Decision-Making: Diffusion models have been used for decision making in many recent works (Chi et al., 2023; Janner et al., 2022; Ajay et al., 2023; Reuss et al., 2023; Du et al., 2024; Zhu et al., 2024), thanks to their abilities to handle multi-modality and their robustness when scaled to larger datasets and tasks. Diffusion Policies (Chi et al., 2023; Reuss et al., 2023) use diffusion over the actions conditioned on the observations. Diffuser-based approaches (Janner et al., 2021; Zhu et al., 2024) diffuse over both the observations and actions and execute the actions at test time. Finally, Ajay et al. (2023) and Du et al. (2024) train diffusion over the states and train an inverse model to extract the actions. Compared to these works, we build on top of Diffuser (Janner et al., 2022) and generate both object-centric factorized states and actions simultaneously."}, {"title": "BACKGROUND", "content": "In this work, we propose a method that leverages a Transformer-based diffusion model and object-centric representations for learning policies from demonstrations. In the following, we give a brief overview of the different components."}, {"title": "OBJECT-CENTRIC REPRESENTATION WITH DLP", "content": "We first extract a compact, object-centric representation from pixel observations. Given image observations of state $s^v$ from V different viewpoints, $(I_0, ..., I_{V-1})$, we encode each image with a DLPv2 (Daniel & Tamar, 2024) encoder $\\mathcal{DLP}$. The resulting representation, as described in Section 3, is a set of M latent particles for each view v, denoted by $Z^v = {z^v_i}^M_{i=0}$, where $Z^v_i = \\mathcal{DLP}(I^v)$. It is important to note that there is no correspondence between particles from different views (i.e. $z^{v'}_i$ and $z^{v''}_i$ can represent different objects), nor is there correspondence between particles from different states (i.e. $z^s_i$ and $z^{s'}_i$ can represent different objects). These properties of the DLP representation require a permutation-equivariant policy network architecture, which we describe in the following section."}, {"title": "ENTITY-CENTRIC TRANSFORMER", "content": "Equipped with DLP's object-centric representations, the particles, we aim to construct a conditional generative model. In this work, we adopt a diffusion-based approach (DDPM (Ho et al., 2020)) to learn the denoising of particles and continuous actions. However, the unique structure of particles\u2014an unordered set\u2014necessitates a permutation-equivariant denoiser. To address this, we design an entity-centric Transformer-based denoiser architecture, termed EC-Diffuser, which processes a sequence of"}, {"title": "ENTITY CENTRIC DIFFUSER FOR GOAL-CONDITIONED BEHAVIORAL CLONING", "content": "We adapt EC-Diffuser to GCBC tasks by incorporating conditioning variables into the diffusion process. Formally, the diffusion process operates over future states and actions: $X_0 = {\\{ (Z^v_\\tau,\\alpha_\\tau) \\} \\}_{v=0,...,V-1,\\tau=1,...,H-1}$, with the current state and goal serving as conditional variables: $c_g = {\\{ (Z^v_0, Z^v_T) \\} \\}_{v=0,...,V-1}$. We define the goal as the last timestep in the demonstration trajectory, i.e., $Z^v_T = Z^v_{T'}$, where T' is the trajectory length. To train EC-Diffuser, we normalize all input features (DLP's features and actions) to [-1,1] and employ an $l_1$ loss on both states and actions. One might question the effectiveness of using $l_2$ or $l_1$ losses directly on unordered set inputs. Typically, generating unordered sets like point clouds calls for set-based metrics such as Chamfer distance to compare set similarity. However, in our case, the objective is particle-wise denoising: noise is added independently to each particle, and the denoising process neither imposes nor requires any specific ordering of set elements. Furthermore, we leverage the Transformer's permutation-equivariant structure by omitting positional embeddings within the set of particles. These factors enable the simple $l_1$ loss function to work effectively with diffusion, aligning with previous works that applied diffusion to point clouds (Vahdat et al., 2022; Melas-Kyriazi et al., 2023).\n$L = E_{x_0,t,c_g,\\epsilon} [\\|\\|\\epsilon - \\epsilon_\\theta (x_t, t, c_g) \\|\\|_1] .\nFor control purposes, we execute the first action produced by the model in the environment, i.e. $\\pi_\\theta(x_t, t, c_g) = a_0$, and perform MPC-style control by querying the model at every timestep. In practice, we do not directly use the generated latent states for control. However, we empirically found that denoising these latent states is critical, as we discuss later. Notably, the generated latent states serve a valuable purpose for visualization: they can be decoded using the pre-trained DLP decoder to reconstruct images, effectively visualizing the imagined trajectory."}, {"title": "EXPERIMENTS", "content": "The experiments in this work are designed to answer the following questions: (I) How do object-centric approaches compare to unstructured baselines in learning tasks with combinatorial structure? (II) Does object-centric structure facilitate compositionally generalizing behavioral cloning agents? (III) What aspects contribute to performance and compositional generalization?\nTo study the above, we evaluate our method on 7 goal-conditioned multi-object manipulation tasks across 3 simulated environments and compare against several competitive BC baselines learning from various image representations.\nEnvironments A visualization of the environments used in this work is presented in Figure 2, and a visualization of the DLP decomposition for PushCube is shown in Figure 3. PushCube and PushT are both IsaacGym-based (Makoviychuk et al., 2021) tabletop manipulation environments introduced in Haramati et al. (2024), where a Franka Panda arm pushes objects in different colors to a goal configuration specified by an image. In PushCube the objects are cubes and the goals are positions, while in PushT the objects are T-shaped blocks and the goals are orientations. In FrankaKitchen, initially introduced in Gupta et al. (2020), the agent is required to complete a set of 4 out of 7 possible tasks in a kitchen environment. We use the goal-conditioned image-based variant from Lee et al. (2024). Detailed descriptions of these environments as well as the datasets used for training can be found in Appendix B. These tasks all possess a compositional nature, requiring the agent to manipulate multiple objects to achieve a desired goal configuration.\nBaselines We compare EC-Diffuser's performance with the following BC methods: (1) VQ-BeT (Lee et al., 2024): a SOTA, non-diffusion-based method utilizing a Transformer architecture. In our experiments, we find that the ResNet18 (He et al., 2016) used in VQ-BeT fails to achieve good performance in PushCube and PushT. Consequently, we use a VQ-VAE (Van Den Oord et al., 2017) image representation pre-trained on environment images. (2) Diffuser (Janner et al., 2022): the original Diffuser trained without guidance and takes flattened VQ-VAE image representations as input. (3) EIT+BC \u2013 a direct adaptation of the EIT policy from Haramati et al. (2024) to the BC setting, learns from DLP image representations. (4) EC Diffusion Policy: inspired by Chi et al. (2023) and modified for the goal-conditioned setting, learns from DLP image representations. Further descriptions and implementation details of each baseline can be found in Appendix C.2.\nFor PushCube and PushT, all results are computed as the mean of 96 randomly initialized configurations. In evaluating FrankaKitchen, we adopt the protocol used by VQ-BeT (Lee et al., 2024), sampling 100 goals from the dataset. Standard deviations are calculated across 5 seeds. We provide extended implementation and training details, and report the set of hyper-parameters used in our experiments in Appendix C."}, {"title": "LEARNING FROM DEMONSTRATIONS", "content": "In this section we aim to answer the first question \u2013 comparing object-centric approaches to unstructured baselines in learning tasks with combinatorial structure. Performance metrics for all tasks are reported in Table 1. In PushCube and PushT, EC-Diffuser significantly outperforms all baselines, with the performance gap widening as the number of objects in the environments increases. Notably, it is the only method that achieves better-than-random performance on Push3T, the most challenging task in our suite."}, {"title": "COMPOSITIONAL GENERALIZATION", "content": "The results in Section 5.1 clearly demonstrate that object-centric approaches outperform unstructured methods in multi-object environments when learning from images. As EC-Diffuser is the only method achieving strong performance in manipulating 3 objects, we focus our answer to the second question on whether our method can generalize zero-shot to unseen compositions of objects and/or goals. To address this, we consider two generalization settings: (1) PushCube \u2013 We train our method with 3 objects, their colors randomly chosen at the beginning of each episode from 6 options. We then test it on environments with up to 6 objects. A visualization for the task is shown in Figure 4. (2) PushT - We train an agent with 3 objects and 3 goals, and then test on scenarios with up to 4 objects and varying goal compositions. A visualization of this task is shown in Figure 5. For comparison, we present results of the best-performing baseline \u2013 EC Diffusion Policy \u2013 in these generalization settings. Further details on training these models are provided in Appendix C.1.\nWe report the quantitative results for PushCube generalization in Table 2. Our method significantly outperforms the baseline across all configurations of PushCube generalization, maintaining a high success fraction as the number of objects increases. As illustrated in Figure 4(b), which shows the distribution of per-object goal-reaching success, our agent successfully manipulates up to 6 objects to their goals despite being trained on data containing only 3 objects."}, {"title": "ABLATION STUDIES", "content": "In this section we aim to answer the third question \u2013 what contributes to the performance of our model? We ablate our key design choices on the PushCube environment. The results, presented in Table 4, report the success rate and success fraction for each task. First, we compare our model to a version that uses VQ-VAE representations as input, treating them as a single particle without any object-centric structure. This unstructured representation results in a significant drop in performance as the number of objects increases, highlighting the importance of the object-centric representation (DLP) for multi-object tasks. Next, we evaluate a model with a similar architecture, but trained without diffusion, where DLP and actions are generated in an auto-regressive manner. For training, we replace the $l_1$ distance with the $l_1$-Chamfer distance for the latent states particles, and use the $l_1$ distance for the actions. This approach fails to learn even in tasks involving a single object, highlighting the critical role of the diffusion process in effectively co-generating DLP and actions. Finally, we compare to a variant of our model that does not generate latent states (as DLP representations). As the number of objects increases, the performance of this ablation rapidly deteriorates, demonstrating that the joint generation of particle states and actions is essential for reasoning about both objects and actions. We provide additional results with alternative object-centric representations in Appendix D.2."}, {"title": "DISCUSSION ON STATE GENERATION", "content": "To further explore the generalization ability of our model, we present the generated particle states by decoding them into images using the pre-trained DLP decoder. As shown in Figure 7, our model can produce high-quality future states that were not present in the training data."}, {"title": "EC-DIFFUSER ON REAL WORLD DATA", "content": "We provide preliminary EC-Diffuser results on real world data in this section. Specifically, we use the real-world Language-Table dataset (Lynch et al., 2023). We subsample 3000 episodes from the real robot dataset, each padded to 45 images, and we randomly select 2700 episodes for training and 300 for validation. We train the DLP model and EC-Diffuser on the training set. We provide a visualization of EC-Diffuser's particle state generation in Figure 8. As shown in the figure, EC-Diffuser can effectively generate high quality rollouts, which shows promise in applying EC-Diffuser to real world problems. We provide DLP decompositions of images from this dataset in Figure 14 in the Appendix."}, {"title": "CONCLUSION", "content": "In this work, we introduced Entity-Centric Diffuser, a novel diffusion-based behavioral cloning method for multi-object manipulation tasks. By leveraging unsupervised object-centric representations and a Transformer-based architecture, EC-Diffuser effectively addresses the challenges of multi-modal behavior distributions and combinatorial state spaces inherent in multi-object environments. We demonstrated significant improvements over existing baselines in manipulation tasks involving multiple objects, and zero-shot generalization to new object compositions and goals, even when faced with more objects than encountered during training. These results highlight the potential"}, {"title": "ETHICS STATEMENT", "content": "This research was conducted in simulated environments, with no involvement of human subjects or privacy concerns. The datasets used are publicly available, and the work aims to improve the efficiency of robotic object manipulation tasks. We have adhered to ethical research practices and legal standards, and there are no conflicts of interest or external sponsorship influencing this work."}, {"title": "EXTENDED DEEP LATENT PARTICLES (DLP) BACKGROUND", "content": "In this section, we present an expanded overview of the Deep Latent Particles (DLP) object-centric representation, as introduced by Daniel & Tamar (2022) and Daniel & Tamar (2024). DLP is an unsupervised, VAE-based model for object-centric image representation. Its core idea is structuring the VAE's latent space as a set of M particles, $z = [z_f, z_p] \\in \\mathbb{R}^{M\\times(n+2)}$. Here, $z_f \\in \\mathbb{R}^{M\\times n}$ encodes visual appearance features, while $z_p \\in \\mathbb{R}^{M\\times 2}$ represents particle positions as (x, y) coordinates in pixel-space, i.e., keypoints. Following is a description of the several modifications to the standard VAE framework introduced in DLP.\nPrior: DLP employs an image-conditioned prior $p(z|x)$, with distinct structures for $z_f$ and $z_p$. $p(z_p|x)$ comprises Gaussians centered on keypoint proposals, generated by applying a CNN to image patches and processed through a spatial-softmax (SSM, Jakab et al. 2018; Finn et al., 2016). The features $z_f$ do not have a special prior neural network, and the standard zero-mean unit Gaussian, $N(0, I)$, is used.\nEncoder: A CNN-based encoder maps the input image to means and log-variances for $z_p$ (or offsets from them). For $z_f$, a Spatial Transformer Network (STN) (Jaderberg et al., 2015) encodes features from regions (\"glimpses\") around each keypoint.\nDecoder: Each particle is independently decoded to reconstruct its RGBA glimpse patch (where \"A\" is the alpha channel of each particle). These glimpses are then composited based on their encoded positions to reconstruct the full image.\nLoss: The entire DLP model is trained end-to-end in an unsupervised manner by maximizing the ELBO, i.e., minimizing the reconstruction loss and the KL-divergence between posterior and prior distributions.\nKL Loss Term: The posterior keypoints $S_1$ and prior keypoint proposals $S_2$ form unordered sets of Gaussian distributions. As such, the KL term for position latents is replaced with the Chamfer-KL:\n$d_{CH-KL}(S_1,S_2)=\\sum_{z_p \\in S_1}min_{z'_p \\in S_2}KL(z_p||z'_p) + \\sum_{z'_1 \\in S_2}min_{z_p \\in S_1} KL(z'_p||z_p)$.\nDLPv2: Daniel & Tamar (2024) extend the original DLP by incorporating additional particle attributes. DLPv2 provides a disentangled latent space structured as a set of M foreground particles: $z = {\\{ (z_p, z_s, z_d, z_t, z_f)_i \\} }^M_{i=1} \\in \\mathbb{R}^{M\\times(6+n)}$. Here, $z_p \\in \\mathbb{R}^2$ and $z_f \\in \\mathbb{R}^n$ remain unchanged, while new attributes are introduced and described below.\n$z_s \\in \\mathbb{R}^2$: scale, representing the (x, y) dimensions of the particle's bounding box, $z_d \\in \\mathbb{R}$: approximate depth in pixel space, determining particle overlap order when particles are close, and $z_t \\in \\mathbb{R}^{[0,1]}$: transparency.\nAdditionally, DLPv2 introduces a single abstract background particle, always centered in the image and described by $n_{bg}$ latent visual features: $z_{bg} \\sim \\mathcal{N}(\\mu_{bg}, \\sigma_{bg}) \\in \\mathbb{R}^{n_{bg}}$. In our work, following Haramati et al. (2024), we discard the background particle from the latent representation after pre-training the DLP. DLPv2 training follows a similar approach to the standard DLP, with modifications to encoding and decoding processes to accommodate the additional attributes."}, {"title": "ENVIRONMENTS AND DATASETS", "content": "In this section we give further details about each environment we used in our experiments including how the demonstration datasets were collected and the metrics we use to evaluate performance.\nPushCube: IsaacGym-based (Makoviychuk et al., 2021) tabletop manipulation environment introduced in Haramati et al. (2024). A Franka Panda robotic arm is required to push cubes in different colors to goal positions specified by images. The agent perceives the environment from two views (front and side) and performs actions in the form of deltas in the end effector coordinates $a = (\\Delta x_{ee}, \\Delta y_{ee}, \\Delta z_{ee})$. Demonstration data for each task (number of objects) was collected by deploying an ECRL (Haramati et al., 2024) state-based agent trained on the corresponding number of objects. We collect 2000 trajectories per task, each containing 30, 50, 100 transitions for 1, 2, 3 objects respectively.\nFor object-centric image representations, we train a single DLP model on a total of 600, 000 images collected by a random policy from 2 views (300, 000 transitions) on an environment with 6 cubes in distinct colors. DLP was able to generalize well to images with fewer objects.\nTo get a wide picture of the goal-reaching performance, we consider the following metrics:\nSuccess - A trajectory is considered a success if at the end of it, all N objects are at a threshold distance from their desired goal. The threshold is slightly smaller than the effective radius of a cube.\nSuccess Fraction \u2013 The fraction of objects that meet the success condition.\nMaximum Object Distance \u2013 The largest distance of an object from its desired goal.\nAverage Object Distance \u2013 The average distance of all objects from its desired goal.\nPushT: IsaacGym-based (Makoviychuk et al., 2021) tabletop manipulation environment introduced in Haramati et al. (2024). A Franka Panda robotic arm is required to push T-shaped blocks to goal orientations specified by images. The object position is not considered part of the task in this setting. The agent perceives the environment from two views (front and side) and performs actions in the form of deltas in the end effector coordinates $a = (\\Delta x_{ee}, \\Delta y_{ee}, \\Delta z_{ee})$. Demonstration data for each task (number of objects) was collected by deploying an ECRL (Haramati et al., 2024) state-based agent trained on the corresponding number of objects. We collect 2000 trajectories per task, each containing 50, 100, 150 transitions for 1, 2, 3 objects respectively.\nFor object-centric image representations, we train a single DLP model on a total of 600, 000 images collected by a random policy from 2 views (300, 000 transitions) on an environment with 3 T-blocks in distinct colors. DLP was able to generalize well to images with different numbers of objects.\nSince any orientation threshold we choose to define success would be arbitrary, we use the following metric to asses performance:\nAverage Orientation Distance \u2013 The average distance of all objects from their desired goal orientation in radians. Since the distance can be considered with respect to both directions of rotation, we always take the smaller of the two. Thus, the largest distance would be \u03c0 and the average of a random policy \u03c0/2 (orientations are uniformly randomly sampled).\nFrankaKitchen: Initially introduced in Gupta et al. (2020), the agent is required to complete a set of 4 out of 7 possible tasks in a kitchen environment: (1) Turn on bottom burner by switching a knob; (2) Turn on top burner by switching a knob; (3) Turn on a light switch; (4) Open a sliding cabinet door; (5) Open a hinge cabinet door; (6) Open a microwave door; (7) Move a kettle from the bottom to top burner. The action space includes the velocities of the 7 DOF robot joints as well as its right and left gripper, totaling in 9 dimensions. A full documentation can be found in: https://robotics.farama.org/envs/franka_kitchen/franka_kitchen/. We use the goal-conditioned image-based variant from Lee et al. (2024), where the environment is perceived from a single view and the goal is specified by the last image in the demonstration trajectory. The demonstration dataset contains 566 human-collected trajectories of the robot completing 4 out of the 7 tasks in varying order with the longest trajectory length being 409 timesteps.\nFor object-centric image representations, we train a single DLP model on the demonstration data. Performance is measured by the number of goals reached in a trajectory (Goals Reached). In the standard setting the maximum value is 4 while in the generalization setting the maximum value is 7 (achieving all possible goals)."}, {"title": "IMPLEMENTATION DETAILS", "content": "We build the Transformer network of EC-Diffuser on top of the Particle Interaction Transformer (PINT) modules from DDLP Daniel & Tamar (2024). We remove the positional embedding for the particles to ensure the Transformer is permutation-equivariant w.r.t the particles. We add different types of positional embedding for views, action particles, and timesteps. We leverage the Diffuser Janner et al. (2022) codebase to train our model: https://github.com/jannerm/diffuser.\nThe hyper-parameters we use in training the EC-Diffuser model is shown in Table 5.\nWe additionally provide details on the model sizes and compute resources used in our experiments in Table 6. For GPUs, we use both NVIDIA RTX A5500 (20GB) and NVIDIA A40 (40GB), though our model training requires only around 8GB of memory. All baseline models have networks of comparable size and are trained on the same hardware."}, {"title": "BASELINES", "content": "VQ-BeT (Lee et al., 2024): A SOTA BC method that uses a Transformer-based architecture to predict actions in the quantized latent space of a VQ-VAE. When learning from images, they use a pretrained ResNet18 backbone to acquire an image representation. They experiment with both freezing and finetuning this backbone and report improved performance when finetuning. At the time of writing this paper, code implementing training with finetuning the ResNet was not available. We therefore experimented with either using a frozen ResNet or a VQ-VAE encoder pretrained on environment images as image representations. We report results from the best performing variant in each environment. We use the official code implementation that can be found in: https://github.com/jayLEE0301/vq_bet_official.\nDiffuser (Janner et al., 2022): A diffusion-based decision-making algorithm that we built our method on, thus providing a natural baseline. Diffuser trains a U-Net diffusion model to simultaneously generate entire state-action sequences. Being in the BC setting, we use the variant that uses unguided sampling. As the original paper does not deal with pixel observations, we provide the model with pretrained image representations. We use a VQ-VAE encoder to extract a latent representation of images and flatten it for compatibility with the 1D U-Net architecture. For FrankaKitchen, we use the representation provided by the frozen ResNet18. We use the official code implementation that can be found in: https://github.com/jannerm/diffuser.\nEIT+BC: This method implements a naive adaptation of ECRL (Haramati et al., 2024) to the BC setting by training the Entity Interaction Transformer (EIT) architecture as a BC policy with an $l1$ loss on the actions. It uses the DLP representations of images, as in our method. We use the official code implementation of the EIT that can be found in: https://github.com/DanHrmti/ECRL.\nEC Diffusion Policy: An entity-centric diffusion policy inspired by Chi et al. (2023). It uses the DLP representations of images and has a similar architecture to ours but generates action-only instead of state-action sequences. The difference in the architecture we use for this method is that it uses a encoder-decoder Transformer module. The particles are first encoded with a Transformer encoder with self-attentions. Then, in the decoder, we interleave self and cross attention between the actions and the particle embedding to obtain denoised actions. The hyper-parameters used for this method are described in Table 7. For the implementation of this method we use the same codebase as for our method."}, {"title": "PRE-TRAINED REPRESENTATION MODELS", "content": "Data: For the IsaacGym environments, similarly to Haramati et al. (2024), we collect 600k images from 2 viewpoints by interacting with the environment using a random policy for 300k timesteps. For all methods, we use RGB images at a resolution of 128 \u00d7 128, i.e., $I \\in \\mathbb{R}^{128\\times128\\times3}$. For FrankaKitchen, we use the offline demonstration dataset collected by Lee et al. (2024) that contains 566 trajectories and around 200k images."}, {"title": "ADDITIONAL RESULTS", "content": "First, we report the success fraction (i.e., the proportion of objects that meet the success condition) for PushCube across all methods in Table 10. This aims to supplement the findings presented"}, {"title": "ADDITIONAL OBJECT-CENTRIC REPRESENTATIONS", "content": "We provide experimental results with different types of input to the EC-Diffuser on the PushCube and FrankaKitchen environments. The results are shown in Table 12 and 13 respectively. All methods use the same architecture and training procedure as EC-Diffuser.\nPushCube: Slot Attention \u2013 We train a Slot Attention model (Locatello et al., 2020) to produce a set of latent \"slot\" vectors. Here, we employ the same slot attention model used in ECRL (Haramati et al., 2024), which produces 10 slots per image. We treat each slot as an entity and pass them into the same EC-Transformer model as used in our method. This variant achieves good results for 1 Cube, but its performance deteriorates quickly as the number of cubes increases, although it remains better than that of the non-object-centric baselines. We attribute this to the fact that, as shown in Figure 15, the slot model occasionally has trouble with individuating nearby objects and represents them in a single slot. Ground-truth State \u2013 we extract the object location information from the simulator and append a one-hot identifier to distinguish each object. Each (position, one-hot) vector is treated as an entity to be passed into the EC-Transformer. This variant is meant to shed light on the efficacy and generality of our approach by emulating a \"perfect\" entity-level factorization of images. With these entity-centric set-based state representations EC-Diffuser achieves slightly better performance than using the DLP representation, as expected. Single View \u2013 This variant only inputs the DLP representation of the front-view image into the EC-Transformer model. We see a drop in performance in this case, highlighting the importance of the ability of EC-Diffuser to effectively leverage multi-view perception in order to mitigate the effect of occlusion and leverage complimenting observational information.\nFrankaKitchen: We additionally train EC-Diffuser on top of a Slot Attention model trained from the FrankaKitchen demonstration data. We use 10 slots, each with a latent dimension of 64. We train the model for 100 epochs on multiple seeds, observe the loss has converged and take the best performing seed. EC-Diffuser with Slot Attention achieves state of the art performance (3.340), surpassing both EC-Diffuser with DLP (achieving 3.031) and VQ-BeT (with a reported performance of 2.60). Based on the slot decompositions in FrankaKitchen (visualized in Figure 16), it is difficult to conclude that its superior performance is due to its ability to capture the objects"}, {"title": "GENERALIZATION TO UNSEEN OBJECTS", "content": "We provide additional cases of generalization to unseen shapes and colors as described in Figure 9. We report the performance in Table 14. We see that EC-Diffuser coupled with DLP is able to generalize zero-shot with little to no drop in performance to new colors as well as new shapes (star, rectangular cuboid). When replacing cubes with T-shaped blocks there is a significant drop in performance although success rate is better than random, suggesting some zero-shot generalization capabilities in this case as well. Additionally, we visualize the DLP state generation from the EC-Diffuser (both the DLP"}]}