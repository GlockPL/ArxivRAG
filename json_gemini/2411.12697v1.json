{"title": "Attribute Inference Attacks for Federated Regression Tasks", "authors": ["Francesco Diana", "Othmane Marfoq", "Chuan Xu", "Giovanni Neglia", "Fr\u00e9d\u00e9ric Giroire", "Eoin Thomas"], "abstract": "Federated Learning (FL) enables multiple clients, such as mobile phones and IoT devices, to collaboratively train a global machine learning model while keeping their data localized. However, recent studies have revealed that the training phase of FL is vulnerable to reconstruction attacks, such as attribute inference attacks (AIA), where adversaries exploit exchanged messages and auxiliary public information to uncover sensitive attributes of targeted clients. While these attacks have been extensively studied in the context of classification tasks, their impact on regression tasks remains largely unexplored. In this paper, we address this gap by proposing novel model-based AIAs specifically designed for regression tasks in FL environments. Our approach considers scenarios where adversaries can either eavesdrop on exchanged messages or directly interfere with the training process. We benchmark our proposed attacks against state-of-the-art methods using real-world datasets. The results demonstrate a significant increase in reconstruction accuracy, particularly in heterogeneous client datasets, a common scenario in FL. The efficacy of our model-based AIAs makes them better candidates for empirically quantifying privacy leakage for federated regression tasks.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) enables multiple clients to collaboratively train a global model (McMahan et al. 2017; Lian et al. 2017; Li et al. 2020). Since clients' data is not collected by a third party, FL naturally offers a certain level of privacy. Nevertheless, FL alone does not provide formal privacy guarantees, and recent works have demonstrated that clients' private information can still be easily leaked (Lyu et al. 2020; Liu, Xu, and Wang 2022). For instance, an adversary with access to the exchanged messages and knowledge of some public information (e.g., client's provided ratings) (Lyu and Chen 2021; Chen et al. 2022; Feng et al. 2021) can reconstruct a client's sensitive attributes (e.g., gender/religion) in an attack known as attribute inference attack (AIA). Additionally, the adversary can reconstruct client's training samples such as images (Geiping et al. 2020; Yin et al. 2021).\nHowever, these reconstruction attacks for FL have primarily been tested on classification tasks and have not been explored for regression tasks, which are, needless to say, equally important for practical applications. Quite surprisingly, our experiments, as shown in Fig. 1, demonstrate that the accuracy of state-of-the-art gradient-based AIA under a honest-but-curious adversary (Lyu and Chen 2021; Chen et al. 2022) (referred to as passive) drops significantly from 71% on a classification task to 50% (random guess) on a regression task once the targeted client holds more than 256 data points. Furthermore, even a more powerful (active) adversary capable of forging the messages to the targeted client to extract more information (Lyu and Chen 2021; Chen et al. 2022) offers only limited improvement to the AIA performance on regression tasks. Detailed information about this experiment is in Appendix B.1.\nIn this paper, we show that federated training of regression tasks does not inherently enjoy higher privacy, but it is simply more vulnerable to other forms of attacks. While existing FL AIA attacks for classification tasks are gradient-based (see Sec. 2.3), we show that model-based AIAs initially proposed for centralized training (Fredrikson et al. 2014;"}, {"title": "2 Preliminaries", "content": "We denote by $\\mathcal{C}$ the set of all clients participating to FL. Let $D_c = \\{(x_c^{(i)}, y_c^{(i)}), i = 1,..., S_c\\}$ denote the local dataset of client $c \\in \\mathcal{C}$ with size $S_c$. Each data sample $(x_c^{(i)}, y_c^{(i)})$ is a pair consisting of an input $x_c^{(i)}$ and of an associated target value $y_c^{(i)}$. In FL, clients cooperate to learn a global model, which minimizes the following empirical risk over all the data owned by clients:\n$\\min_{\\theta \\in \\mathbb{R}^d} L(\\theta) = \\sum_{c \\in \\mathcal{C}} p_c L_c(\\theta, D_c)$\n$= \\sum_{c \\in \\mathcal{C}} p_c \\left(\\frac{1}{S_c} \\sum_{i=1}^{S_c} l(\\theta, x_c^{(i)}, y_c^{(i)})\\right)$\nwhere $l(\\theta, x_c^{(i)}, y_c^{(i)})$ measures the loss of the model $\\theta$ on the sample $(x_c^{(i)}, y_c^{(i)}) \\in D_c$ and $p_c$ is the positive weight of client $c$, s.t., $\\sum_{c \\in \\mathcal{C}} p_c = 1$. Common choices of weights are $p_c = \\frac{1}{|C|}$ or $p_c = \\frac{S_c}{\\sum_{c \\in \\mathcal{C}} S_c}$"}, {"title": "2.1 Federated learning", "content": "Kasiviswanathan, Rudelson, and Smith 2013; Yeom et al. 2018)\u2014may be more effective for regression tasks. Figure 1 illustrates that a model-based attack on the server's global model (i.e., the final model trained through FL) already performs at least as well as the SOTA gradient-based passive attack. Moreover, it highlights that even more powerful attacks (up to 30 p.p. more accurate) could be launched if the adversary had access to the optimal local model of the targeted client (i.e., a model trained only on the client's dataset).\nMotivated by these observations, we propose a new two-step model-based AIA for federated regression tasks. In this attack, the adversary first (approximately) reconstructs the client's optimal local model and then applies an existing model-based AIA to that model.\nOur main contributions can be summarized as follows:\n\u2022 We provide an analytical lower bound for model-based AIA accuracy in the least squares regression problem. This result motivates the adversary's strategy to approximate the client's optimal local model in federated regression tasks. (Sec. 3).\n\u2022 We propose methods for approximating optimal local models where adversaries can either eavesdrop on exchanged messages or directly interfere with the training process (Sec. 4).\n\u2022 Our experiments show that our model-based AIAs are better candidates for empirically quantifying privacy leakage for federated regression tasks (Sec. 5)."}, {"title": "2.2 Threat Model", "content": "Honest-but-curious adversary We describe first an honest-but-curious adversary,\u00b9 which is a standard threat model in existing literature (Melis et al. 2019; Geiping et al. 2020; Yin et al. 2021; Nasr, Shokri, and Houmansadr 2019), including the FL one (Kairouz et al. 2021, Table 7), (Lyu and Chen 2021; Chen et al. 2022). This passive adversary, who could be the server itself, is knowledgeable about the\nIn what follows, we refer to the client using female pronouns and the adversary using male pronouns, respectively."}, {"title": "2.3 Attribute inference attack (AIA) for FL", "content": "AIA leverages public information to deduce private or sensitive attributes (Kasiviswanathan, Rudelson, and Smith 2013; Lyu and Chen 2021; Chen et al. 2022; Fredrikson et al. 2014; Yeom et al. 2018). For example, an AIA could reconstruct a user's gender from a recommender model by having access to the user's provided ratings. Formally, each input $x^{(i)}$ of client $c$ consists of public attributes $x_\\mathcal{P}^{(i)}$ and of a sensitive attribute $s_c^{(i)}$. The target value, assumed to be public, is denoted by $y_\\mathcal{T}^{(i)}$. The adversary, having access to $\\{(x_\\mathcal{P}^{(i)}, y_\\mathcal{T}^{(i)}), i = 1,..., S_c\\}$ and $\\mathcal{M}_c$, aims to recover the sensitive attributes $s_c^{(i)}.$\\nExisting gradient-based AIA for FL Chen et al. (2022) present AIAs specifically designed for the FL context and both passive and active adversaries. The central idea involves identifying sensitive attribute values that yield virtual gradients closely resembling the client's model updates\u2014referred to as pseudo-gradients\u2014in terms of cosine similarity. Formally, the adversary solves the following optimization problem:\n$\\underset{\\{s_c^{(i)}\\}_{i=1}^{S_c}}{\\operatorname{argmax}} \\sum_{t \\in \\mathcal{T}} \\operatorname{CosSim}\\left(\\frac{\\nabla l(\\theta_t, \\{(x_\\mathcal{P}^{(i)}, s_c^{(i)}, y_\\mathcal{T}^{(i)})\\})}{\\left\\|\\nabla l(\\theta_t, \\{(x_\\mathcal{P}^{(i)}, s_c^{(i)}, y_\\mathcal{T}^{(i)})\\})\\right\\|},\\frac{\\delta \\theta_t}{\\left\\|\\delta \\theta_t\\right\\|}\\right)$\nwhere $\\mathcal{T} \\subset \\mathcal{T}_\\mathcal{C}$ for a passive adversary and $\\mathcal{T} \\subseteq \\mathcal{T} \\cup \\mathcal{T}_\\mathcal{A}$ for an active adversary. The active adversary simply sends back to the targeted client c her own model $\\theta^{(t-1)}$ at each attack round in $\\mathcal{T}_\\mathcal{A}$.\nChen et al. (2022) assume that the sensitive attributes are categorical and discrete random variables. Nevertheless, problem (2) can be solved efficiently using a gradient method with the reparameterization trick and the Gumbel softmax distribution (Jang, Gu, and Poole 2017). From (2), we observe that, since gradients incorporate information from all samples, the attack performance deteriorates in the presence of a large local dataset. For example, the attack accuracy almost halves on the Genome dataset for the classification task when the client's local dataset size increases from 50 to 1000 samples (Lyu and Chen 2021, Table 8). Our experiment (Figure 1) on a regression task corroborates this finding: when the local dataset size increases from 64 to 256, the attack accuracy drops from 60% to the level of random guessing.\nModel-based AIA As an alternative, the AIA can be executed directly on the model (rather than on the model pseudo-gradients), as initially proposed for centralized training in (Kasiviswanathan, Rudelson, and Smith 2013; Fredrikson et al. 2014). Given a model $\\theta$, the adversary can infer the sensitive attributes by solving the following optimization problems:\n$\\underset{s_c^{(i)}}{\\operatorname{argmin}} l(\\theta, (x^{(i)}, s_c^{(i)}, y^{(i)})), \\forall i \\in \\{1, ..., S_c\\}$"}, {"content": "trained model structure, the loss function, and the training algorithm, and may eavesdrop on communication between the attacked client and the server but does not interfere with the training process. For instance, during training round $t$, the adversary can inspect the messages exchanged between the server and the attacked client (denoted by $c$), allowing him to recover the parameters of the global model $\\Theta_t$ and the updated client model $\\theta_c^{(K)}$ (Algorithm 2). Let $T_\\mathcal{C} = \\{t | c \\in \\mathcal{C}, \\forall t \\in \\{0, ..., T - 1\\}\\}$ denote the set of communication rounds during which the adversary inspects messages exchanged between the server and the attacked client and $\\mathcal{M}_c = \\{(\\Theta_t, \\theta_c^{(K)}), \\forall t \\in T_\\mathcal{C}\\}$ denote the corresponding set of messages.\nWhen it comes to defenses against such an adversary, it is crucial to understand that traditional measures like encrypted communications are ineffective if the attacker is the FL server. More sophisticated cryptographic techniques like secure aggregation protocols (Bonawitz et al. 2017; Kadhe et al. 2020) allow the server to aggregate local updates without having access to each individual update and, then, do hide the client's updated model from the server. Nevertheless, they come with a significant computation overhead (Quoc et al. 2020) and are inefficient for sparse vector aggregation (Kairouz et al. 2021). Moreover, they are vulnerable to poisoning attacks, as they hinder the server from detecting (and removing) potentially harmful updates from malicious clients (Blanchard et al. 2017; Yin et al. 2018; El Mhamdi 2020). For instance, Tram\u00e8r et al. (2022, Sec. 4.4) introduce a new class of data poisoning attacks that succeed when training models with secure multiparty computation. Alternatively, Trusted Execution Environments (TEEs) (Sabt, Achemlal, and Bouabdallah 2015; Singh et al. 2021) provide an encrypted memory region to ensure the code has been executed faithfully and privately. They can then both conceal clients' updated models and defend against poisoning attacks. However, implementing a reliable TEE platform in FL remains an open challenge due to the infrastructure resource constraints and the required communication processes needed to connect verified codes (Kairouz et al. 2021).\nMalicious adversary We also consider a stronger active adversary who can interfere with the training process. Specifically, this adversary can modify the messages sent to the clients and have the clients update models $\\theta_c$ concocted to reveal more private information. Let $\\mathcal{T}_\\mathcal{A}$ be the set of rounds during which the adversary attacks client $c$ by sending malicious model $\\theta_c^t$. As above, the adversary could be the server itself. This adversary has been widely considered in the literature on reconstruction attacks (Wen et al. 2022; Boenisch et al. 2023) and membership inference attacks (Nguyen et al. 2023; Nasr, Shokri, and Houmansadr 2019). Some studies have also explored the possibility of a malicious adversary modifying the model architecture during training (Fowl et al. 2022; Zhao et al. 2023), even though such attacks appear to be easily detectable. In this paper, we do not allow the adversary to modify the model architecture.\nFor simplicity, we will refer to these two adversaries as passive and active adversaries, respectively, throughout the rest of the paper."}, {"title": "3 Model-based AIA guarantees for least squares regression.", "content": "In this section, we provide novel theoretical guarantees for the accuracy of the model-based AIA (Problem (3)) in the context of least squares regression. In particular, we show that the better the model $\\theta$ fits the local data and the more the sensitive attribute affects the final prediction, the higher the AIA accuracy.\nProposition 1. Let $\\mathcal{E}_c$ be the mean square error of a given least squares regression model $\\theta$ on the local dataset of client $c$ and $\\theta[s]$ be the model parameter corresponding to a binary sensitive attribute. The accuracy of the model-based AIA (3) is larger than or equal to $1 - 4 \\frac{\\mathcal{E}_c S_c}{\\theta[s]^2}$.\nProof. Let $\\mathbf{s}_c$ be the vector including all the unknown sensitive binary attributes $\\{s_c^{(i)}, \\forall i \\in \\{1, ..., S_c\\}\\}$ of client $c$. Let $\\mathbf{x}_c \\in \\mathbb{R}^{S_c \\times d}$ be the design matrix with rank $d$ and $\\mathbf{y} \\in \\mathbb{R}^{S_c}$ be the labels in the local dataset $D_c$ of the client $c$. Let $\\theta[:p] \\in \\mathbb{R}^{d-1}$ be the parameters corresponding to the public attributes. The adversary has access to partial data instances in $D_c$ which consists of the public attributes $P \\in \\mathbb{R}^{S_c \\times (d-1)}$ and the corresponding labels $\\mathbf{y} \\in \\mathbb{R}^{S_c}$.\nThe goal for the adversary is to decode the values of the binary sensitive attribute $\\mathbf{s}_c \\in \\{0, 1\\}^{S_c}$ given $(P, \\mathbf{y}_c)$ by solving (3), i.e., checking for each point, which value for the sensitive attribute leads to a smaller loss.\nIt is easy to check that the problem can be equivalently solved through the following two-step procedure. First, the adversary computes the vector of real values:\n$\\hat{\\mathbf{s}}_c = \\underset{\\mathbf{s}_c \\in \\mathbb{R}^{S_c}}{\\operatorname{argmin}} ||P\\theta[:p] + \\mathbf{s}_c \\theta[s] - \\mathbf{y}_c||^2$\nThen, the adversary reconstruct the vector of sensitive features $\\hat{\\mathbf{s}}_c \\in \\{0, 1\\}^{S_c}$ as follows\n$\\hat{s}_c^{(i)} = \\begin{cases}\n0 & \\text{ if } \\hat{s}_c^{(i)} < \\frac{1}{2}, \\\\n1 & \\text{ otherwise } \n\\end{cases} \\forall i \\in \\{1, ..., S_c\\}.$\nLet $\\mathbf{e}_c$ be the vector of residuals for the local dataset, i.e., $\\mathbf{e}_c = \\mathbf{y}_c - (P\\theta[:p] + \\mathbf{s}_c \\theta[s])$. We have then\n$\\hat{\\mathbf{s}}_c = \\frac{\\mathbf{y}_c - P\\theta[:p] - \\mathbf{e}_c}{\\theta[s]}$\nLet us say that the sensitive feature of sample $i$ has been erroneously reconstructed, i.e., $s_c(i) \\neq \\hat{s}_c(i)$, then (4), implies that $|s_c(i) - \\hat{s}_c(i)| \\geq 1/2$, and from (5) it follows that $\\mathbf{e}_c(i)^2 \\geq \\theta[s]^2/4$. As $\\mathcal{E}_c S_c = ||\\mathbf{e}_c||^2$, there can be at most $4 \\mathcal{E}_c S_c/\\theta[s]^2$ samples erroneously reconstructed, from which we can conclude the result."}, {"title": "4 Reconstructing the local model in FL", "content": "In this section, we show how an adversary may reconstruct the optimal local model of client $c$, i.e., $\\theta^* = \\underset{\\theta \\in \\mathbb{R}^d}{\\operatorname{argmin}} \\mathcal{L}_c(\\theta, D_c)$.\nFirst, we provide an efficient approach for least squares regression under a passive adversary. We prove that the adversary can exactly reconstruct the optimal local model under deterministic FL updates and provide probabilistic guarantees on the reconstruction error under stochastic FL updates (Sec. 4.1).\nSecond, we show that an active adversary can potentially reconstruct any client's local model (not just least square regression ones) in a federated setting (Sec. 4.2)."}, {"title": "4.1 Passive approach for linear least squares", "content": "We consider that clients cooperatively train a linear regression model with quadratic loss. We refer to this setting as a federated least squares regression. The attack is detailed in Alg. 3 and involves a single matrix computation (line 3) after the exchanged messages $\\mathcal{M}_c$ have been collected. $\\mathcal{A}^\u2020$ represents the pseudo-inverse of matrix A. Theorem 1 provides theoretical guarantees for the distance between the reconstructed model and the optimal local one, when the model is trained through FedAvg (McMahan et al. 2017) with batch size B and local epochs E. The formal statement of the theorem and its proof are in Appendix A.1.\nTheorem 1 (Informal statement). Consider a federated least squares regression with a large number of clients and assume that i) client c has d-rank design matrix $\\mathbf{x}_c \\in \\mathbb{R}^{S_c \\times d}$,"}, {"title": "4.2 Active approach", "content": "Here we consider an active adversary (e.g., the server itself) that can modify the global model weights $\\Theta_t$ to recover the client's optimal local model. To achieve this, the adversary can simply send back to the targeted client c her own model $\\theta^{(t-1)}$ instead of the averaged model $\\Theta_t$. In this way, client c is led to compute her own optimal local model.\nWe propose a slightly more sophisticated version of this active attack. Specifically, we suggest that the adversary emulates the Adam optimization algorithm (Kingma and Ba 2015) for the model updated by client c by adjusting the learning rate for each parameter based on the magnitude and history of the gradients, and incorporating momentum. The motivation is twofold. First, the client does not receive back exactly the same model and thus cannot easily detect the attack. Second, Adam is known to minimize the training error faster than stochastic gradient descent at the cost of overfitting more to the training data (Zhou et al. 2020b; Zou et al. 2023). We can then expect Adam to help the adversary reconstruct the client's optimal local model better for a given number of modified messages, and our experiments in Sec. 5 confirm that this is the case.\nThe details of this attack are outlined in Alg. 4. We observe that the adversary does not need to systematically modify all messages sent to the target client c but can modify just a handful of messages that are not necessarily consecutive. This contributes to the difficulty of detecting the attack."}, {"title": "5 Experiments", "content": "Medical (Lantz 2013). This dataset includes 1,339 records and 6 features: age, gender, BMI, number of children, smoking status, and region. The regression task is to predict each individual's medical charges billed by health insurance. The dataset is split i.i.d. between 2 clients.\nIncome (Ding et al. 2024). This dataset contains census information from 50 U.S. states and Puerto Rico, spanning from 2014 to 2018. It includes 15 features related to demographic information such as age, occupation, and education level. The regression task is to predict an individual's income. We investigate two FL scenarios, named Income-L"}, {"title": "5.1 Datasets", "content": "and Income-A, respectively. In Income-L, there are 10 clients holding only records from the state of Louisiana (15,982 records in total). These clients can be viewed as the local entities working for the Louisiana State Census Data Center. We examine various levels of statistical heterogeneity among these local entities, with the splitting strategy detailed in Appendix B.3. In Income-A, there are 51 clients, each representing a census region and collectively covering all regions. Every client randomly selects 20% of the data points from the corresponding census region, resulting in a total of 332,900 records.\nFor all the datasets, each client keeps 90% of its data for training and uses 10% for validation."}, {"title": "5.2 FL training and attack setup", "content": "In all the experiments, each client follows FedAvg (Alg. 2) to train a neural network model with a single hidden layer of 128 neurons, using ReLU as activation function. The number of communication rounds is fixed to $T = [100/E]$ where $E$ is the number of local epochs. Each client participates to all rounds, i.e., $\\mathcal{T} = \\{0, ..., T - 1\\}$. The learning rate is tuned for each training scenario (different datasets and number of local epochs), with values provided in Appendix B.4. The passive adversary may eavesdrop all the exchanged messages until the end of the training. The active adversary launches the attack after $T$ rounds for additional $[10/E]$ and $[50/E]$ rounds. Every attack is evaluated over FL trainings from 3 different random seeds. For Medical dataset, the adversary infers whether a person smokes or not. For Income-L and Income-A datasets, the adversary infers the gender. The AIA accuracy is the fraction of the correct inferences over all the samples. We have also conducted experiments for federated least squares regression on the same datasets. The results can be found in Appendix C.1."}, {"title": "5.3 Baselines and our attack implementation", "content": "Gradient-based: We compare our method with the (gradient-based) SOTA (Sec. 2.3). The baseline performance is affected by the set of inspected rounds $\\mathcal{T}$ considered in (2). We select the inspected rounds $\\mathcal{T}$ based on two criteria: the highest cosine similarity and the best AIA accuracy. In a real attack, the adversary is not expected to know the attack accuracy beforehand. Therefore, we refer to the attack based on the highest cosine similarity as Grad and to the other as Grad-w-O (Gradient with Oracle), as it assumes the existence of an oracle that provides the attack accuracy. The details for the tuning of $\\mathcal{T}$ and other hyper-parameter settings can be found in Appendix B.4.\nOur Attacks: Our attacks consist of two steps: 1) reconstructing the optimal local model, and 2) executing the model-based AIA (3). For the first step, a passive adversary uses the last-returned model from the targeted client, while an active adversary executes Alg. 4. The details of the hyperparameter settings can be found in Appendix B.4 ."}, {"title": "5.4 Experimental results", "content": "From Table 1, we see that our attacks outperform gradient-based ones in both passive and active scenarios across all three datasets. Notably, our passive attack achieves improvements of over 15 and 8 percentage points (p.p.) for the Income-L and Medical datasets, respectively. Even when the gradient-based method has access to an oracle, our passive attacks still achieves higher accuracy on two datasets and comes very close on Income-A. When shifting to active attacks, the gains are even more substantial. For instance, when the attack is active for 50 rounds, we achieve gains of 13, 22, and 5 percentage points (p.p.) in Income-L, Income-A, and Medical, respectively, over Grad-w-O, and even larger gains over the more realistic Grad. Furthermore, the attack accuracy reaches the performance expected from an adversary who knows the optimal local model. Interestingly, while our attacks consistently improve as the adversary's capacity increases (moving from a passive attacker to an active one and increasing the number of rounds of the active attack), this is not the case for gradient-based methods.\nImpact of data heterogeneity. We simulate varying levels of heterogeneity in the Income-L dataset and illustrate how the attack performance evolves in Figure 3 (left). First, we observe that as the data is more heterogeneously split, the accuracy of AIA improves for all approaches. Indeed, this"}, {"title": "6 Discussion and conclusions", "content": "In our work, we have demonstrated the effectiveness of using model-based AIA for federated regression tasks, when the attacker can approximate the optimal local model. For an honest-but-curious adversary, we proposed a computationally efficient approach to reconstruct the optimal linear local model in least squares regression. In contrast, for neural network scenarios, our passive approach involves directly utilizing the last returned model (Sec. 5.3). We believe more sophisticated reconstruction techniques may exist, and we plan to investigate this aspect in future work.\nThe reader may wonder if the superiority of model-based attacks over gradient-based ones also holds for classification tasks. Some preliminary experiments we conducted suggest that the relationship is inverted. We advance a partial explanation for this observation. For a linear model with binary cross-entropy loss, it can be shown that the model-based AIA (3) on a binary sensitive attribute is equivalent to a simple label-based attack, where the attribute is uniquely reconstructed based on the label. This approach leads to poor performance because the attack relies only on general population characteristics and ignores individual specificities. This observation also holds experimentally for neural networks trained on the Income-L dataset, largely due to the inherent unfairness of the learned models. For example, for the same"}, {"title": "A Theoretical Results", "content": "Theorem 1. Consider training a least squares regression through FedAvg (Alg. 2) with batch size B and local epochs E. Assume that\n1. the client's design matrix $x_c \\in \\mathbb{R}^{m \\times d}$ has rank $d$ equal to the number of features plus one;\u2075\n2. the components of the stochastic (mini-batch) gradient are distributed as sub-Gaussian random variables with variance proxy $\\sigma^2$, i.e., $\\mathbb{E}[\\exp(e(\\theta)[i]/\\sigma^2)] \\leq \\exp(1)$, $\\forall \\theta$, $\\forall i \\in \\{1,2,...,d\\}$, where $e(\\theta) = \\mathbf{g}(\\theta) - \\nabla L_c(\\theta)$;\u2076\n3. the input model vectors at an observed round are independent of the previous stochastic gradients computed by the attacked client;\n4. there exists $\\lambda > 0$ such that $\\forall n_c \\in \\mathbb{N}$, we can always select $n_c$ observation rounds $T_\\text{out}$ so that $\\lambda_{\\text{min}}\\left(\\frac{1}{n_c} \\sum_{t \\in T_\\text{out}} \\Theta_\\text{out}^t \\Theta_\\text{out}^t\\right) > \\lambda$, where $\\lambda_{\\text{min}}(A)$ denotes the smallest eigenvalue of the matrix A, and $\\Theta_\\text{out}$ is defined in Alg. 3.\nThe error of the reconstructed model $\\hat{\\theta}^*$ of Algorithm 3 is upper bounded w.p. $\\geq 1 - \\delta$ when $\\eta \\leq \\frac{2}{\\lambda_{\\text{max}}(x_c^T x_c)}$ and\n$\\mathbb{E}\\[||\\hat{\\theta}^* - \\theta^*||^2\\] = O\\left(\\frac{\\eta \\sigma d e^{- \\frac{\\eta \\lambda \\delta}{d}}}{n_c B} \\sqrt{\\frac{d+1+\\ln \\frac{2}{\\delta}}{\\eta \\lambda}}\\right)$"}, {"title": "A.1 Full statement and proof of Theorem 1", "content": "Before presenting the proof", "well-behaved data assumption\" for a generic linear regression problem, which is required to be able to prove the consistency of the estimators, i.e., that they converge with probability 1 to the correct value as the number of samples diverges (see for example (Shao 2003, Thm. 3.11)). In this context, we can observe that $\\lambda_{\\text{min}}\\left(\\frac{1}{n_c} \\sum_{t \\in T_\\text{out}} \\Theta_\\text{out}^t \\Theta_\\text{out}^t\\right) = \\lambda_{\\text{min}}\\left(\\frac{1}{n_c} \\Theta_{\\text{out}} \\Theta_{\\text{out}}^T\\right) = \\underset{\\mathbf{z},||\\mathbf{z}||=1}{\\text{min}} ||\\Theta_{\\text{out}}\\mathbf{z}||^2$. Now, consider that the components of the gradient noise $e(\\theta)$ are independent, each with variance lower bounded by $\\tau^2 > 0$, it follows that $\\mathbb{E}\\[||\\Theta_{\\text{out}}\\mathbf{z}||^3\\": "geq n_c \\tau^2 \\frac{1"}, {"have": "n$\\mathbf{g}(\\theta) = \\nabla \\mathcal{L}_c(\\theta) + e(\\theta) = \\frac{2}{m}(H \\theta - H \\theta^*) + e(\\theta)$.\nAt round $t$, if selected, client c receives the server model and executes Algorithm 2. Let $\\theta^{(k)}$ be the model after the k-th local update. To simplify the notation, in the following, we replace $e(\\theta^{(k)})$ by $e_k$. Replacing (9) in line 4 of Algorithm 2, we have\n$\\"}]}