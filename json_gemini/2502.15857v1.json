{"title": "PPC-GPT: Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation", "authors": ["Tao Fan", "Guoqiang Ma", "Yuanfeng Song", "Lixin Fan", "Kai Chen", "Qiang Yang"], "abstract": "Compressing Large Language Models (LLMs) into task-specific Small Language Models (SLMs) encounters two significant challenges: safeguarding domain-specific knowledge privacy and managing limited resources. To tackle these challenges, we propose PPC-GPT, a innovative privacy-preserving federated framework specifically designed for compressing LLMs into task-specific SLMs via pruning and Chain-of-Thought (COT) distillation. PPC-GPT works on a server-client federated architecture, where the client sends differentially private (DP) perturbed task-specific data to the server's LLM. The LLM then generates synthetic data along with their corresponding rationales. This synthetic data is subsequently used for both LLM pruning and retraining processes. Additionally, we harness COT knowledge distillation, leveraging the synthetic data to further improve the retraining of structurally-pruned SLMs. Our experimental results demonstrate the effectiveness of PPC-GPT across various text generation tasks. By compressing LLMs into task-specific SLMs, PPC-GPT not only achieves competitive performance but also prioritizes data privacy protection.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs), such as GPT-4 (OpenAI, 2023a) and LLaMA3-70B (Touvron et al., 2023a), boasting billions of parameters and remarkable text generation capabilities, have emerged as a transformative force in the realm of artificial intelligence. However, their training demands substantial computational resources (OpenAI, 2023b), and their colossal size poses significant hurdles for practical deployment, especially in resource-limited environments. Conversely, Small Language Models (SLMs), such as OPT-1.3B (Zhang et al., 2022) and Pythia-1.4B (Biderman et al., 2023), frequently demonstrate superior computational efficiency and accelerated response rates, making them ideally suited for real-time applications with constrained resources. Enterprises with constrained resources typically prefer deploying SLMs, as they can do so without the concern of potential data leaks, a risk that is heightened when utilizing remote LLMs. Yet, training an SLM from scratch, even the smallest billion-parameter models, entails considerable computational expenses that are financially prohibitive for most enterprises. Furthermore, SLMs exhibit inherent limitations that stem from their performance constraints.\nIn this work, we aim to tackle the following question: Is it feasible to develop a task-specific and competitive SLM by harnessing an existing pre-trained LLM for enterprises with limited resources, while ensuring compliance with privacy requirements? To achieve this objective, we delve into structured pruning (Xia et al., 2023; Men et al., 2024; Kim et al., 2024), as a viable approach. Pruning is generally regarded as a strategy for compressing task-specific models by eliminating redundant parameters and expediting inference, all while maintaining task performance.\nWe identify two crucial technical challenges associated with this problem: Firstly, how can we ensure the privacy of task-specific data when enterprises with limited resources are unable to prune an LLM into an SLM independently? In such cases, the need to transmit task-specific data to a remote server equipped with powerful computing resources arises, a practice that is frequently unacceptable to most enterprises due to privacy concerns. Secondly, how can we ensure that the performance of the SLM remains comparable to that of the LLM? Structured pruning inevitably leads to some degree of performance degradation. To overcome these challenges, we introduce PPC-GPT, a privacy-preserving federated framework designed for compressing LLMs into task-specific SLMs via pruning and Chain-of-Thought (COT) distillation.\nAs depicted in Figure 1(a), the envisioned archi-"}, {"title": "Related Work", "content": "In this section, We briefly revisit two important definitions of differential privacy: \u2208-Differential Privacy and Exponential Mechanism (EM).\n\u20ac-Differential Privacy (DP). The Definition of \u2208-Differential Privacy (DP) (Dwork, 2006). A randomized algorithm M:D \u2192 S is \u2208-Differential Privacy if for any two neighboring datasets D1, D2 \u2208 D that differ exactly in a single data sample, and for any output O\u2282 S:\nPr[M(D1) \u2208 O] < ePr[M(D2) \u2208 O]  (1)\nwhere e is a privacy parameter. Smaller values of e imply stronger privacy guarantees.\nExponential Mechanism. The Definition of Exponential Mechanism (McSherry and Talwar, 2007; Tong et al., 2023). For a given scoring function u : X \u00d7 Y \u2192 R, a randomized mechanism M(X, u, Y) is \u2208-DP compliant if it satisfies:\nPr[y/x] x exp(u(x,y))\n2 \u2206u (2)\nwhere the sensitivity \u2206u is defined as:\n\u2206u = max |u(x, y) \u2013 u(x', y)| (3)\nx,x'\u2208X,y\u2208Y\nDifferential Privacy Synthetic Data\nA practical approach to generating private synthetic data involves training a language model, such as LLaMa2-7B (Touvron et al., 2023a), on private data using DP through DP-SGD (Song et al., 2013; Bassily et al., 2014; Abadi et al., 2016). Subsequently, the DP model is sampled repeatedly to produce synthetic data (Mattern et al., 2022; Yue et al., 2022; Kurakin et al., 2023). Research conducted by (Mattern et al., 2022; Yue et al., 2022; Kurakin et al., 2023) demonstrates that training downstream models on DP synthetic data achieves performance comparable to training directly on real data with DP, thereby underscoring the high quality of the synthetic data.\nHowever, a significant challenge arises because cutting-edge LLMs, like GPT-4, do not offer model weights, making DP fine-tuning impractical. Even for open-source LLMs, such as LLaMa3-70B (Touvron et al., 2023a), the process is resource-intensive. Meanwhile, these DP fine-tuning methods inher-"}, {"title": "Model Pruning", "content": "Model pruning, initially proposed by (LeCun et al., 1989) and subsequently enhanced by (Han et al., 2015), stands as a resilient and efficient strategy for mitigating model redundancy and attaining compression. This methodology branches into two primary techniques: unstructured pruning and structured pruning.\nUnstructured pruning (Dong et al., 2017; Lee et al., 2019; Wang et al., 2020; Sun et al., 2023; Frantar and Alistarh, 2023) can obtain highly compressed models by directly pruning neurons, disregarding the model's internal architecture, which also causes unstructured sparsity and hard deployment. A more pragmatic and structured option is structured pruning. Structured pruning targets organized patterns for removal, encompassing entire layers (Jha et al., 2023), attention heads within Multi-Head Attention (MHA) mechanisms (Michel et al., 2019), hidden sizes in Feedforward Neural Networks (FFN) (Nova et al., 2023), as well as hybrid configurations (Kurti\u0107 et al., 2024). In recent times, there has been a surge in structured pruning"}, {"title": "The Proposed PPC-GPT Framework", "content": "In this section, we introduce PPC-GPT, an innovative privacy-preserving federated framework specifically designed for compressing LLMs into task-specific SLMs via pruning and COT distillation. We illustrate the PPC-GPT in Figure 1(a). The PPC-GPT comprises four key modules: DP Perturbed Data, Synthetic Data Generation, Layer-Wise Structured Pruning, and Retraining. We will elaborate on these modules in Section 3.2, Section 3.3, Section 3.4 and Section 3.5, respectively, following presenting the problem formulation we try to address in Section 3.1. Our aim is to provide a comprehensive understanding of how PPC-GPT addresses the intricate challenges associated with compressing LLMs while maintaining the client's data privacy and optimizing task-specific performance."}, {"title": "Problem Formulation", "content": "Given an LLM fo with parameters \u03b80, which represents the original LLM that requires pruning, and a task-specific dataset D containing private data, our objective is to develop a target smaller, task-specific compressed SLM f\u03b8t parameterized by \u03b8t. To acheive this, we seek to find the optimal pruning strategy P and retraining approach R. The objective can be formulated as follows:\nmin L(\u03b8t; \u03b80, D)\nPR\ns.t. |\u03b8t| \u226a |\u03b80| and Lp(D) < \u03b4 (4)\nwhere L(\u03b8t; \u03b80, D) is the loss function measuring the performance of the compressed SLM on the task-specific dataset. |\u03b8t| and |\u03b80| denote the number of parameters in the compressed and original models, respectively. Lp(D) is the privacy loss incurred due to the perturbation of the data to ensure differential privacy.\nOur goal is to find the optimal pruning strategy P and retraining approach R that minimizes the overall loss, taking into account both the performance of the compressed SLM and the privacy protection of the task-specific data in the client. We assume the server to be semi-honest, meaning it may attempt to extract the client's private data from the information it receives."}, {"title": "DP Perturbed Data", "content": "We utilize an exponential mechanism (McSherry and Talwar, 2007; Yue et al., 2021; Chen et al., 2023) to perturb the local private data D =\nN\n{(xi, Yi)}1, which satisfies the criteria for the\n\u2208-DP. For detailed information about the exponential mechanism, please refer to Section 2.1. We\nN\ndenote the perturbed dataset as Dp = {(x)}1', where x signifies an perturbed input based on the original local private dataset D.\nThe Exponential Mechanism M is defined as a randomized algorithm that, given the original local private dataset D, outputs the perturbed dataset Dp with probability proportional to the exponential of the utility score:\nM(D) = Dp with prob x exp(u(D, Dp))\n(5)\n2 \u2206u"}, {"title": "Synthetic Data Generation", "content": "When the server-side LLMsyn receives the perturbed data Dp, the server initiates a procedure where LLMsyn generates fresh synthetic data along with their corresponding rationales based on these perturbed data. We denote the synthetic dataset as Ds = {(xi, (yi, ri))} Ns1, where xi signifies an input, yi signifies the corresponding expected output label, ri signifies the desired rationale, and Ns represents the sample size of synthetic data.\nWe introduce a simple and efficient method for generating synthetic data, utilizing prompt engineering techniques and CoT technology:\nQuestion Generation. We prompt LLMsyn to create a new question, starting from a perturbed question. To enhance the validity of these new created questions, we enforce three guidelines within the prompt: (1) the new question needs to conform to common knowledge, (2) it must be solvable on its own, independent of the original question, and (3) it should not contain any answer responses. Furthermore, we establish specific formatting standards for both questions and answers, customized to suit the needs of various datasets (Li et al., 2024).\nAnswer Generation. We instruct LLMsyn to generate a COT response for every newly created question. For consistency, we request LLMsyn to generate answers to the same question three times and check for agreement. If the answers differ, we reject the synthetic data.\nRationale Generation. We request LLMsyn to generate rationales for each synthetic data using the COT prompting technique.\nDetailed prompt designs are presented in Appendix B. The generated synthetic data and their rationales are then employed for model pruning and retraining on the server-side."}, {"title": "Layer-Wise Structured Pruning", "content": "In neural networks exhibiting high redundancy, it is plausible that certain layers contribute minimally to the ultimate performance of the model. This phenomenon can be attributed to the homogeneous functionalities of these layers relative to others within the network architecture. LLMs are no exception, with varying levels of redundancy observed across their layers, particularly more pronounced in deeper layers. To eliminate these redundant layers, we require a metric that is inherent"}, {"title": "Retraining", "content": "We employ the term \"retraining\" to designate the process of performance recovery subsequent to pruning. In this section, retraining is divided into two stages: (1) Server-side Retraining, and (2) Client-side Retraining.\nServer-side Retraining. On the server side, we utilize the synthetic dataset Ds, as described in Section 3.3, to retrain the pruned model, SLMt. We propose COT knowledge distillation, guided by rationales generated by LLMsyn, to enhance the performance of SLMt. Formally, we conceptualize the learning process with rationales as a multi-task learning problem (Zhang and Yang, 2021; Wei et al., 2022; Hsieh et al., 2023). Specifically, we train the model f\u03b8t(xi) \u2192 (yi, ri) to achieve not only the prediction of task labels but also the generation of corresponding rationales based on textual inputs. This multi-task training ensures that our model produces not only accurate predictions but also insightful justifications for its decisions, thereby enhancing the model's transparency and explainability. The multi-task learning objective can be formulated as follows:\nL = LLabel + LRationale (8)\nwhere LLabel represents the label prediction loss:\nLLabel(\u03b8t; Ds) = E(x,y)\u223cDs lCE(f\u03b8t(x), y) (9)\nand LRationale represents the rationale generation loss:\nLRationale(\u03b8t; Ds) = E(x,r)\u223cDs lCE(f\u03b8t(x),r) (10)\nwhere lCE denotes the cross-entropy loss, f\u03b8t(\u00b7) denotes the SLMt model.\nClient-side Retraining. On the client side, we utilize local private data D to further retrain the pruned model, SLMt, once it has been received from the server. Our work encompasses conventional training, leveraging ground truth labels to further enhance the performance of SLMt. Formally, the label prediction loss for this dataset D is formulated as follows:\nLLabel(\u03b8t; D) = E(x,y)\u223cD lCE(f\u03b8t(x),y) (11)"}, {"title": "Experiments", "content": "We have devised a scenario to assess the performance of the PPC-GPT framework across various text generation tasks. This setup employs a client-server architecture, where the server hosts a LLM for synthetic data generation, denoted as LLMsyn. Specifically, we have selected LLaMa3-"}, {"title": "Main Results", "content": "In our experiments, we extensively evaluated the performance of the proposed PPC-GPT framework across various text generation tasks. Notably, given that current structured pruning methods typically reduce parameters by no more than 30%, we conducted experiments with approximately 30% of the parameters pruned. Additional experiments exploring different parameter reduction proportions will be discussed in Section 5.5.\nAs shown in Table 1, the results highlight the effectiveness of PPC-GPT in compressing LLMs into task-specific SLMs while prioritizing data privacy protection, when compared to other baseline approaches. PPC-GPT outperforms the DP-Instruct-C method, which utilizes DP-SGD for privacy protection during model compression. Furthermore, PPC-GPT even surpasses the Plain-C"}, {"title": "Ablation Study", "content": "In this section, we explore the impact of privacy budgets on the performance of PPC-GPT. Table 2 presents PPC-GPT's performance across a range of privacy budgets (e = 1,3,5, 10). Notably, when juxtaposed with Table 1, it becomes apparent that even with a privacy budget of e = 1, PPC-GPT outperforms the Plain-C method by 1.7% and 3.4% on the OBQA and ARC-E datasets, respectively, within the LLaMa2-7B model. Similarly, PPC-GPT exceeds it by 14% and 14.4% in the OPT-6.7B model. As the privacy budget e increases, PPC-GPT's performance demonstrates a significant improvement, highlighting its proficiency and adaptability in achieving a balance between privacy and utility."}, {"title": "Impact of Different Synthetic Data", "content": "In this section, we explore the impact of synthetic data on PPC-GPT's performance, considering two dimensions: the synthetic data ratio and the inclusion of rationales in synthetic data.\nSynthetic Data Ratio. Table 3 presents the performance of PPC-GPT across various synthetic data ratios (ratio = 1, 2, 4, 8). As the ratio of synthetic data increases, PPC-GPT's performance exhibits a substantial improvement, highlighting the crucial role of the synthetic data ratio and indicating that a higher amount of synthetic data results in further improvements. Specifically, PPC-GPT with the synthetic data ratio of 8 outperforms the ratio of 1 by 1.7% and 4.1% on the OBQA and ARC-E datasets, respectively, within the LLaMa2-7B model. Similarly, with the OPT-6.7B model, it exceeds the ratio of 1 by 4.2% and 7.6%.\nSynthetic Data Rationales. We undertake an"}, {"title": "Impact of Server-Side Retraining", "content": "In this section, we explore the impact of server-side retraining on the performance of PPC-GPT. Table 5 presents a comparison of PPC-GPT's performance with and without server-side retraining. The findings demonstrate that PPC-GPT exhibits superior performance when server-side retraining is utilized, as compared to when it is absent. Specifically, PPC-GPT w/ server-side retraining outperforms PPC-GPT w/o server-side retraining by 2% and 4.5% on the OBQA and ARC-E datasets, respectively, within the LLaMa2-7B model. Similarly, with the OPT-6.7B model, PPC-GPT w/ server-side retraining exceeds PPC-GPT w/o server-side retraining by 15.1% and 15.7%."}, {"title": "Impact of Different Importance Metric", "content": "In this section, we explore the impact of different important metrics on PPC-GPT's performance:\nSeq: The importance is directly correlated with the sequence order, where the shallower layers hold greater importance.\nBI: BI mentioned in previous section 3.4.\nTable 6 presents PPC-GPT's performance across different important metrics. The findings demonstrate that PPC-GPT with BI exhibits superior performance than PPC-GPT with Seq."}, {"title": "Impact of Different Model Pruning Ratio", "content": "In this section, we explore the impact of different model pruning ratio on PPC-GPT's performance.\nTable 7 presents the performance of PPC-GPT across different model pruning ratios (namely, 0%, 30%, 50%, and 70%). As the pruning ratio increases, the performance of PPC-GPT exhibits a decline."}, {"title": "Conclusions", "content": "In this study, we introduce PPC-GPT, a federated framework designed to compress LLMs into task-specific SLMs while preserving privacy. This is achieved through pruning and COT distillation. PPC-GPT employs a server-client architecture, wherein client data perturbed with DP is transmitted to the server for the generation of synthetic data. This synthetic data, along with its associated rationales, is subsequently used for both the pruning of LLMs and the retraining of pruned SLMs. Our experimental findings show that PPC-GPT successfully compresses LLMs into highly competitive SLMs, with a strong emphasis on safeguarding data privacy."}, {"title": "Limitations", "content": "While PPC-GPT shows promising results in compressing LLMs into task-specific SLMs while ensuring data privacy, it has several limitations. Firstly, PPC-GPT relies on the LLM's robust chain-of-thought capabilities to generate high-quality synthetic data and rationales. If the LLM lacks such capabilities, the performance of the compressed SLMs may be impacted. Furthermore, as observed in our experiments, the performance of PPC-GPT tends to degrade with higher pruning ratios. This indicates that optimizing the pruning strategy to strike a better balance between model size and performance remains an open challenge."}, {"title": "Implementation Details", "content": "During the training process, we specifically configured the parameters. Specifically, we set the batch size to 32 and utilized the AdamW optimizer. The maximum number of training steps varied between 300 and 6400. Additionally, we established a learning rate of 5e-5. For the input and target lengths, we set the maximum question length to 64 and the maximum target length to 128. For the LoRA configuration of LLaMa2, we set the LoRA alpha to 32 and the LoRA rank to 8. In contrast, for the OPT model, we configured the LoRA alpha to 64 and the LoRA rank to 32. The Lora dropout for both models was set to 0.1."}, {"title": "Data Splitting", "content": "For the datasets, all splits (training, validation, and test) were downloaded from HuggingFace (Lhoest et al., 2021)."}, {"title": "Dataset Licenses", "content": "All the datasets were downloaded from Hugging-Face(Lhoest et al., 2021) and under Apache License, Version 2.0."}, {"title": "Machine Configuration", "content": "The experiments were conducted on machines equipped with 4 and 8 Nvidia V100 32G."}]}