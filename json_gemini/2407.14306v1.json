{"title": "Complementary Learning for Real-World Model Failure Detection", "authors": ["Daniel Bogdoll", "Finn Sartoris", "Vincent Geppert", "Svetlana Pavlitska", "J. Marius Z\u00f6llner"], "abstract": "In real-world autonomous driving, deep learning\nmodels can experience performance degradation due to dis-\ntributional shifts between the training data and the driving\nconditions encountered. As is typical in machine learning, it\nis difficult to acquire a large and potentially representative\nlabeled test set to validate models in preparation for deploy-\nment in the wild. In this work, we introduce complementary\nlearning, where we use learned characteristics from different\ntraining paradigms to detect model errors. We demonstrate our\napproach by learning semantic and predictive motion labels in\npoint clouds in a supervised and self-supervised manner and\ndetect and classify model discrepancies subsequently. We per-\nform a large-scale qualitative analysis and present LidarCODA,\nthe first dataset with labeled anomalies in lidar point clouds,\nfor an extensive quantitative analysis.", "sections": [{"title": "I. INTRODUCTION", "content": "In autonomous driving, analyzing anomalies in the envi-\nronment surrounding the vehicle is a well-established re-\nsearch field [1], [2], [3], [4], [5], [6], [7]. However, many\nissues are not induced by rare events in the environment,\nbut by model failures, which can also occur in seem-\ningly normal situations. For such cases, Heidecker et al.\nintroduced method-layer corner cases [3], and other data-\ndriven perspectives followed [8], [9], [10]. Model failures in\nautonomous driving are rarely detected during evaluation, as\nlabeled validation and test splits are typically very small and\nnot representative. However, large-scale unlabeled fleet data\nrecordings are often available.\nIn order to detect failure modes, there are many active re-\nsearch areas. Active learning [11] is concerned with continu-\nously enriching the training data by querying samples from a\nset of unlabeled data points. Discrepancies between different\nsensor systems can also be used to query samples [12]. In\nerror estimation, many approaches try to utilize unlabeled\ntest sets for the evaluation of models [13]. Label refinement\ncompares given labels, e.g. by an auto-labeling process, with\nnew proposals [14]. All of these methods have in common\nthat they utilize or compare two or more different results\non the same task. However, we are unaware of approaches\nthat leverage different training paradigms in order to utilize\ndifferent data characteristics. Our main contributions are:\n\u2022 Introduction and demonstration of complementary\nlearning, leveraging complementary training paradigms\nto detect model failures\n\u2022 LidarCODA, the first real-world anomaly dataset for\nautonomous driving with labeled lidar data"}, {"title": "II. RELATED WORK", "content": "The concept of comparing the outputs of two or more neu-\nral networks was already introduced in 1994 by Cohn et al.,\nwhere they queried samples for active learning based on the\ndisagreement between neural networks [15]. Since then, the\nvariability in model predictions has been widely used to de-\ntect anomalies or errors. Ensemble diversity is especially well\nstudied, as it was shown to lead to better performance [16],\nrobustness [17], uncertainty quantification [18], and detection\nof outliers or distribution shifts [19], [20], [21]. While no\nuniform metric for ensemble diversity exists, measures like\ndisagreement of models, double fault measure, or output\ncorrelation are widely used [22]. Ensemble diversity can\nbe implicitly enhanced via random initialization [18], noise\ninjection or dropout, or explicitly via bagging, boosting, or\nstacking. Compared to ensembles, mixtures of experts [23]\ninclude a learnable gate component for dynamic input rout-\ning. They enforce higher model specialization and thus more\ncomponent diversity, leading to better detection of anomalous\nor out-of-distribution data [24].\nThe approaches mentioned above involve a combination\nof several neural networks with similar or identical archi-\ntectures. Active learning is another research field interested\nin the detection of model failures. Here, uncertainty derived\nfrom ensembles is resource-intensive and thus only rarely\nused as part of a querying strategy [25], [26], [27]. Sim-\nilarly, disagreements in a query-by-committee setting can\nbe used to select samples [28]. In autonomous driving, also\ncontradicting detections from sensors can be used as triggers,\ne.g., when radar and camera detections do not match [12].\nDiscrepancies between teacher and student models, typically\nknown from knowledge distillation, can also be utilized [29].\nAs test sets are often small and not representative, directly\nestimating the accuracy of a model with only unlabeled data\nis of high interest [13], [30], [31], [32]. Here, we often see\nsimple classification tasks or approaches that estimate an\noverall error that cannot be applied to individual samples. In\nsome cases, generated pseudo-labels are utilized for further\ntraining steps [33], [34].\nDisagreements can also be used for detecting erroneous\nlabels. Ground truth labels in large vision datasets are\noften error-prone when auto-labeling processes based on\nlarge models are employed [35]. Detecting label errors with\ndisagreements can be done by predicting a novel or refined\nlabel, and uncertainties can be generated by predicting mul-\ntiple such labels [36], [14], [37]. This way, also noisy labels\nintroduced by human errors can be detected [38].\nRobustness during deployment is often achieved with\nsensor fusion, which, quite differently, purposefully aims to\ncomplement the weaknesses of one sensor with the strengths\nof another. Thus, disagreements are both typical and wanted,\nwith the aim of resolving them [39]. However, also data from\na single sensor can be split into multiple streams to increase\nrobustness, e.g., by performing appearance- and geometric-\nbased object detection [40]. In performance monitoring [41],\n[42], but also in outlier or anomaly detection [43], typically,\na primary model performing a regular task is accompanied\nby a learned or model-based module that provides some sort\nof uncertainty for the results of the regular task.\nResearch Gap. Many of the analyzed works utilizing\ndisagreements deal with toy problems and only analyze\nclassification tasks, which are not sufficient to truly un-\nderstand the shortcomings of a model that is designed\nfor the complex task of autonomous driving. Many works\nanalyze model outputs of the same architecture, leveraging\ndifferences during training. However, this way, the same data\ncharacteristics are being used during training. In autonomous\ndriving, we see the largest potential for disagreement-based\napproaches in designing triggers for active learning [12]\nand increasing robustness during deployment [40], but these\nindustry demonstrations are not accompanied by scientific\nworks. Finally, to the best of our knowledge, no work exists\nthat utilizes different training paradigms in order to detect\nmodel failures through disagreements."}, {"title": "III. METHOD", "content": "We leverage complementary training paradigms on the\nsame task in order to detect model failures and classify\nchallenging scenarios with an oracle. The ability to detect\nmodel failures is based on the intuition that different training\nparadigms leverage different data characteristics from the\nsame training data set. In this work, we demonstrate this\napproach with the motion segmentation of point clouds\nin the context of autonomous driving. As shown in Fig-\nure 2, we first derive motion labels from point clouds in\na supervised and self-supervised fashion. Here, the first\nparadigm leverages human knowledge, given only context\nfrom static scenes. On the other hand, the second paradigm\nleverages temporal information inherent in the data. Typi\u0456-\ncally, these paradigms are combined either in a pre-training\nconcept [44] or with a combined loss during learning [45].\nIn our demonstrated use case, we examine motion labels in\npoint clouds and focus on model disagreements. Based on a\npoint-wise comparison, we detect discrepancies and cluster\nthem for better interpretation. Finally, an oracle examines and\nclassifies the model failures to better understand challenging\nsituations. As one use case, our method, deployed within a\nquery strategy in an active learning loop, can find exciting\nsamples to improve a model under test. All code is on Github.\nTo derive semantic motion labels in point clouds, we first\nleverage a supervised semantic segmentation model [46] to\ndetermine whether a point belongs to a static or dynamic\nclass. However, some classes do not provide clear infor-\nmation about the motion state of the points. For example,\npoints assigned to the class bicyclist at a traffic light may be\nstatic in the case of a red light and dynamic in the case\nof a green light. We refer to such classes as potentially\ndynamic classes. By also performing supervised motion seg-\nmentation [47], we are able to further subdivide the existing\nclasses. For example, the class bicyclist can be broken down\ninto static bicyclist and dynamic bicyclist. Semantic classes\nthat are static by definition remain unchanged. We refer\nto the semantic labels that are further subdivided based on\ntheir motion as semantic motion labels. An example of the\nresulting label fusion of semantic and motion labels to form\nsemantic motion labels is shown in Figure 3.\nIn order to predict motion labels for a given point cloud,\nwe first filter out the ground [49] to focus on objects in\nthe scene, a common pre-processing step of scene flow\nmodels [50], [51], [52], [53], [54]. Based on self-supervised\nflow prediction [53] of the remaining points, we aim to\nderive motion labels, indicating whether a point is static\nor dynamic. The model takes consecutive point clouds as\ninput and predicts the future motion for each lidar point\nin the form of a 3D displacement vector. The scene flow\nmodel does not distinguish between the point's own motion\nand the observer's ego-motion and represents the overall\nmotion of a point between two consecutive frames. In order\nto derive relative displacements, we need to correct for the\nego-motion. This can be done by leveraging or learning\nodometry information [55]. After predicting the future point\ncloud $X_{t+1} = X_t + f_t$, we apply the learned rigid body\ntransformation $T_{t+1\u2192t}$ of an odometry model, transforming\nthe predicted point cloud back into the coordinate system of\n$X_t$. This gives the future point cloud $X_{t+1}$, which contains\nonly the predicted relative motion without the ego-motion,\nas described by the following formula:\n$X_{t+1} = T_{t+1\u2192t} \u00b7 \\left(\\frac{X_t+f_t}{1}\\right)^T$ (1)\nAs a result, static objects line up closely with the original\ndata of $X_t$, and only dynamic objects show a predicted\ndisplacement, as shown in Figure 4 a).\nTwo-Stage Clustering. An analysis of the velocity values\nof the flow predictions showed that separating static from\ndynamic classes is infeasible in a point-wise fashion, as\na strong overlap exists. What we found, however, is a\nsignificant difference when considering instance-wise nor-\nmalized standard deviations, as shown in Figure 5. While we\nperformed this analysis with ground-truth labels, in unlabeled\ndata, the necessity arises to form instance clusters.\nAfter obtaining motion labels from both the supervised\nand the self-supervised stream, we are now interested in\ndetecting contradictions between the labels, as shown in\nFigure 2. Given a semantic and a predictive motion label\nfor each lidar point, there exist four different categories,\nwhere we assign a color for visual inspection to each. First,\nwe color points both models deem static in green and\nthose both models deem dynamic in blue. For contradictory\npoints, we color those red where the supervised stream\npredicts a static point and the self-supervised a dynamic\none, and all others yellow in the opposite case. To help\na subsequent human oracle better understand a scene, we\nprovide a visual inspection tool where we map the lidar\npoints onto the corresponding RGB image for an improved\nscene understanding, as shown in Figure 6. Finally, we\ncluster instances with contradicting labels, so a human oracle\ncan classify complete scenes as well as single instances."}, {"title": "IV. EVALUATION", "content": "In the first section of the evaluation, we incorporated a\nhuman oracle for Model Failure Classification. In the second\npart of the evaluation, we analyze the sensitivity of the\napproach towards anomalies in the environment.\nWe conduct an extensive analysis of our approach by ex-\namining the outputs of the models. Instead of analyzing only\nscenes that included detected discrepancies, we manually\nexamined over 20,000 frames to better understand the model\nbehaviors and scenes they react to.\nTo minimize perceptual failures due\nto a domain shift, it is important to choose an evalua-\ntion dataset similar to the training dataset. Both, different\nweather conditions and differences between datasets from\ndifferent countries can lead to domain shifts [3]. However,\nclassical evaluation splits are too small to truly understand\nthe performance of our approach. Thus, we chose KITTI\nOdometry [58] for evaluation. KITTI-360, the dataset we\ntrained on, and KITTI Odometry [64] are closely related\ndatasets, as both were captured in Karlsruhe, Germany with\na Velodyne HDL-64E lidar. Since we used the KITTI-360\ndataset for training, and DeLORA was trained on sequences\n00-08 of the KITTI odometry dataset, we used the remaining\nsequences 09-21 of the KITTI odometry dataset for the\nevaluation. We used sequences 09 and 10 to quantitatively\nexamine motion segmentation performance for each part\nseparately and used SemanticKITTI [57] labels as ground\ntruth. To investigate model failures, sequences 11-21 of the\nKITTI Odometry datasets were used for qualitative analysis.\nAs shown in Fig. 7, we observe that the majority of points\nare predicted as static by both models and around 5% of the\npoints show model contradictions. Due to the pre-processed\npoint cloud of the scene flow model, the self-supervised\nstream predicts a label for significantly fewer points than the\nsupervised stream, which predicts a label for all points in a\nlidar scan. In both analyses, only the lidar points per frame\nfor which both streams predicted a label were considered.\nSequences 11-21 of the KITTI odometry dataset\nwere used for the qualitative analysis, comprising a total of\n20,350 frames. Based on the visual inspection tool introduced\nin Section III-C, we present representative examples of\ndetected model failures. In most cases, the models were\ncorrectly consistent, especially due to the high number of\nstatic points. In these areas, no improvement is necessary.\nNext, we observe model failures of the supervised stream,\nwhich is the model under test in most cases. Here, the two\nstreams contradict each other, and we were able to detect\nthe failure by the correct detection of the self-supervised\nstream. We show representative examples in Figure 6. Here,\nscene 1 shows a turning car and two moving bicyclists, where\none bicyclist is wrongly labeled as static by the supervised\nstream. Scene 2 contains two walking pedestrians that are\nwrongly classified as static by the supervised stream. Scene\n3 shows a parking car misclassified as dynamic by the super-\nvised stream. Scenes 4 and 5 show a car moving slowly and\na car moving backward, respectively. These rare cases also\nlead to model failures. These cases demonstrate effectively\nthat our approach enables the detection of sometimes regular,\nbut sometimes also rare and challenging scenarios that lead\nto model failures, which could not have been detected in a\nsmall test split. This way, new samples can be collected for\nlabeling to further improve the model under test. We found\nvarious weak points in each stream, characterized by repeated\noccurrence. Specifically, the supervised model under test has\nweaknesses in distinguishing between dynamic and static\nobjects in specific situations, e.g., at red lights or when a car\nis parked directly in front of the ego vehicle. An example of\nsuch situations is given in scenes 6 and 7 of Figure 6.\nNext, we show scenarios where the self-supervised stream\nshowed model failures, detected by correct predictions of\nthe supervised stream. Figure 8 shows representative scenes\nof this category. Scene 1 contains two distant pedestrians\nwalking, wrongly classified as static by the self-supervised\nstream. In scene 2, a parked car is misclassified as dynamic\nby the self-supervised stream. Scenes 3, 4, and 5 show\nwalking pedestrians or moving cars incorrectly classified as\ndynamic. These cases demonstrate that our approach enables\nthe detection of challenging temporal scenarios. Often, mod-\nels under test are fine-tuned variants of models trained in a\nself-supervised fashion. This way, additional training data\ncan be collected for improved pre-training. Here, no labels\nare required. The self-supervised stream classifies an above-\naverage number of objects as dynamic when the ego-vehicle\nturns or goes over a speed bump. An example is shown in\nFigure 8, where in scene 6, the vehicle turns, and in scene\n8, it drives over a speed bump. Another weak point is fast\noncoming vehicles on highways, which are often classified\nas static, as can be seen in scene 7. Finally, a common\nweakness is small clusters on the right or left edge that are\nincorrectly classified as dynamic, as in scene 9, where points\nof a window are classified as dynamic.\nFinally, also cases occur where the models are incorrectly\nconsistent. For this category, the two streams agree, but the\nlabel is incorrect in both cases. We show examples in Fig-\nure 10. Here, the left scene shows two walking pedestrians\nthat are incorrectly classified as static and the right scene\nshows a parked car classified as dynamic.\nGenerally, we are interested in scenarios where models\nfail. It is well known that perception models often struggle\nmost with anomalies in the environment. Thus, we also\nexamine the sensitivity of our approach towards object-level\nanomalies in the environment around the ego vehicle, as\nthese are most often examined [65], [66].\nWhile our goal in the first part of\nthe evaluation was to minimize the domain gap between\ntraining and evaluation data, there are no labeled anomalies\nin the original KITTI dataset [64]. Thus, we utilized data\nfrom CODA [67], the only real-world dataset that provides\nlidar data and includes anomalies [68]. The CODA dataset\nprovides anomaly labels for objects based on three existing\ndata sets: KITTI [64], ONCE [69], and nuScenes [70].\nFor the CODA-KITTI split, the authors manually reviewed\nall misc labels available in the ground truth and relabeled\nsome as anomalies according to a labeling policy. Since we\ntrained our models on KITTI-360, this enables us to quanti-\ntatively examine our approach with only a small domain gap.\nFor CODA-nuScenes, the authors similarly adopted available\nannotations in a manual process. Finally, for CODA-ONCE,\nthey deployed an automated anomaly detection approach,\nmaking this subset the most relevant. CODA includes 1,500\nscenes with a total of 5,937 anomaly instances. Among those,\n4,746 belong to the superclass traffic_facility, followed by\n929 vehicle and 197 obstruction instances. With 396, most\nvehicle instances can be found in CODA-KITTI. In Figure 7,\nwe show the outputs of our approach on the CODA subsets,\nalso in comparison to the outputs of the KITTI dataset. We\ncan observe many more detected discrepancies, which is in\nline with the much higher number of anomalies, but the\nsubsets reveal strongly varying behavior patterns.\nFor further qualitative analysis, however, labeled anoma-\nlies in 3D are required. The original CODA dataset provides\nanomaly labels only in the form of 2D bounding boxes\nin images. Therefore, we present LidarCODA, a dataset\nbased on the CODA dataset [67] for evaluation. Based on\na frustum-based filter, subsequent clustering, and manual\ninspection, we transferred the original 2D labels from image\nspace into refined, point-wise 3D labels that go beyond the\ncoarse characteristic of the provided bounding boxes. More\ndetails can be found in [71]. LidarCODA is the first real-\nworld anomaly dataset with annotated lidar data, as shown\nin Figure 9. Here, also the different lidar systems utilized\nbecome clearly visible. Due to the sparse point cloud of\nnuScenes, many small or distant labeled anomalies in the\nimage space are only covered by a few or no lidar points.\nWith LidarCODA, we provide ground truth\nfor object-level anomalies [1] in lidar data. Since our ap-\nproach is not specific to object-level anomalies but shows\nits strength in underrepresented or generally challenging or\natypical scenarios, which can also include scene- or scenario-\nlevel anomalies, we are primarily interested in the sensitivity\nof our approach towards these specific anomalies. This is\ndifferent from works in anomaly detection, which aim to\ndetect as many anomalies as possible. As our approach\nassigns distinct labels per point, we follow the evaluation\nprotocols of semantic segmentation tasks rather than anomaly\ndetection tasks, as here an anomaly score per point is\nrequired. First, we need to better understand the suitability of\nthe subsets of LidarCODA due to introduced domain shifts,\neither due to new environments or due to new sensor setups.\nTable I shows the evaluation results on the individual subsets.\nHere, we evaluated all points of the lidar point cloud, even\nif our approach did not label individual points, e.g., because\nthey were filtered during pre-processing. Such cases were\ncounted as false negatives if an anomaly was missed. This\nway, we fairly incorporate the limits of our approach and do\nnot create any requirements on the analyzed point clouds.\nThe results clearly show that our approach struggles with\nthe nuScenes subset, which is primarily due to the large\ndomain shift w.r.t to the sensor setup. The subsets ONCE\nand KITTI, however, show more promising results, as here,\nour approach is capable of detecting anomalies. This allows\nfor further investigation. As nuScenes is by far the smallest\nsubset, its impact on the evaluation is rather limited.\nNext, we investigate the sensitivity towards the super-\nclasses provided by CODA. Here, we only evaluated points\nthat have labels assigned by both our approach and the\nground truth. As shown in Table II, our approach shows dif-\nferent levels of sensitivity given different types of anomalies.\nWhile animals were not detected at all, our approach is more\nsensitive to cyclists and objects of the misc class. The misc\nclass consists of objects that are \u201cunrecognizable or difficult\nto categorize\u201d [67]. The results are difficult to interpret,\nhowever, as CODA defines an anomaly as an object that\n\u201cblocks or is about to block a potential path of the selfdriving\nvehicle\u201d [67] and/or \u201cdoes not belong to any of the common\nclasses of autonomous driving benchmarks\u201d [67]. This risk-aware definition is not always in line with the methodology\nof our approach, where objects that block the path in front\nof the ego vehicle are not necessarily hard to segment or\npredict. Regardless, we can conclude that our methodology\nshows a heightened sensitivity towards cyclists who are\nsometimes difficult to recognize or predict and anomalies\nthat are generally difficult to categorize."}, {"title": "V. CONCLUSION", "content": "In this work, we have presented complementary learning\nto detect real-world model failures. We leverage complemen-\ntary training paradigms to detect contradicting outputs on\nthe same task. This way, we can detect the failure modes"}]}