{"title": "Complementary Learning for Real-World Model Failure Detection", "authors": ["Daniel Bogdoll", "Finn Sartoris", "Vincent Geppert", "Svetlana Pavlitska", "J. Marius Z\u00f6llner"], "abstract": "In real-world autonomous driving, deep learning\nmodels can experience performance degradation due to dis-\ntributional shifts between the training data and the driving\nconditions encountered. As is typical in machine learning, it\nis difficult to acquire a large and potentially representative\nlabeled test set to validate models in preparation for deploy-\nment in the wild. In this work, we introduce complementary\nlearning, where we use learned characteristics from different\ntraining paradigms to detect model errors. We demonstrate our\napproach by learning semantic and predictive motion labels in\npoint clouds in a supervised and self-supervised manner and\ndetect and classify model discrepancies subsequently. We per-\nform a large-scale qualitative analysis and present LidarCODA,\nthe first dataset with labeled anomalies in lidar point clouds,\nfor an extensive quantitative analysis.", "sections": [{"title": "I. INTRODUCTION", "content": "In autonomous driving, analyzing anomalies in the envi-\nronment surrounding the vehicle is a well-established re-\nsearch field [1], [2], [3], [4], [5], [6], [7]. However, many\nissues are not induced by rare events in the environment,\nbut by model failures, which can also occur in seem-\ningly normal situations. For such cases, Heidecker et al.\nintroduced method-layer corner cases [3], and other data-\ndriven perspectives followed [8], [9], [10]. Model failures in\nautonomous driving are rarely detected during evaluation, as\nlabeled validation and test splits are typically very small and\nnot representative. However, large-scale unlabeled fleet data\nrecordings are often available.\nIn order to detect failure modes, there are many active re-\nsearch areas. Active learning [11] is concerned with continu-\nously enriching the training data by querying samples from a\nset of unlabeled data points. Discrepancies between different\nsensor systems can also be used to query samples [12]. In\nerror estimation, many approaches try to utilize unlabeled\ntest sets for the evaluation of models [13]. Label refinement\ncompares given labels, e.g. by an auto-labeling process, with\nnew proposals [14]. All of these methods have in common\nthat they utilize or compare two or more different results\non the same task. However, we are unaware of approaches\nthat leverage different training paradigms in order to utilize\ndifferent data characteristics. Our main contributions are:\n\u2022 Introduction and demonstration of complementary\n  learning, leveraging complementary training paradigms\n  to detect model failures\n\u2022 LidarCODA, the first real-world anomaly dataset for\n  autonomous driving with labeled lidar data"}, {"title": "II. RELATED WORK", "content": "The concept of comparing the outputs of two or more neu-\nral networks was already introduced in 1994 by Cohn et al.,\nwhere they queried samples for active learning based on the\ndisagreement between neural networks [15]. Since then, the\nvariability in model predictions has been widely used to de-\ntect anomalies or errors. Ensemble diversity is especially well\nstudied, as it was shown to lead to better performance [16],\nrobustness [17], uncertainty quantification [18], and detection\nof outliers or distribution shifts [19], [20], [21]. While no\nuniform metric for ensemble diversity exists, measures like\ndisagreement of models, double fault measure, or output\ncorrelation are widely used [22]. Ensemble diversity can\nbe implicitly enhanced via random initialization [18], noise\ninjection or dropout, or explicitly via bagging, boosting, or\nstacking. Compared to ensembles, mixtures of experts [23]\ninclude a learnable gate component for dynamic input rout-\ning. They enforce higher model specialization and thus more\ncomponent diversity, leading to better detection of anomalous\nor out-of-distribution data [24].\nThe approaches mentioned above involve a combination\nof several neural networks with similar or identical archi-\ntectures. Active learning is another research field interested\nin the detection of model failures. Here, uncertainty derived\nfrom ensembles is resource-intensive and thus only rarely\nused as part of a querying strategy [25], [26], [27]. Sim-\nilarly, disagreements in a query-by-committee setting can\nbe used to select samples [28]. In autonomous driving, also\ncontradicting detections from sensors can be used as triggers,\ne.g., when radar and camera detections do not match [12]."}, {"title": "III. METHOD", "content": "We leverage complementary training paradigms on the\nsame task in order to detect model failures and classify\nchallenging scenarios with an oracle. The ability to detect\nmodel failures is based on the intuition that different training\nparadigms leverage different data characteristics from the\nsame training data set. In this work, we demonstrate this\napproach with the motion segmentation of point clouds\nin the context of autonomous driving. As shown in Fig-\nure 2, we first derive motion labels from point clouds in\na supervised and self-supervised fashion. Here, the first\nparadigm leverages human knowledge, given only context\nfrom static scenes. On the other hand, the second paradigm\nleverages temporal information inherent in the data. Typi\u0456-\ncally, these paradigms are combined either in a pre-training\nconcept [44] or with a combined loss during learning [45].\nIn our demonstrated use case, we examine motion labels in\npoint clouds and focus on model disagreements. Based on a\npoint-wise comparison, we detect discrepancies and cluster\nthem for better interpretation. Finally, an oracle examines and\nclassifies the model failures to better understand challenging\nsituations. As one use case, our method, deployed within a\nquery strategy in an active learning loop, can find exciting\nsamples to improve a model under test. All code is on Github.\n\nA. Semantic Motion Labels\nTo derive semantic motion labels in point clouds, we first\nleverage a supervised semantic segmentation model [46] to\ndetermine whether a point belongs to a static or dynamic\nclass. However, some classes do not provide clear infor-\nmation about the motion state of the points. For example,\npoints assigned to the class bicyclist at a traffic light may be\nstatic in the case of a red light and dynamic in the case\nof a green light. We refer to such classes as potentially\ndynamic classes. By also performing supervised motion seg-\nmentation [47], we are able to further subdivide the existing\nclasses. For example, the class bicyclist can be broken down\ninto static bicyclist and dynamic bicyclist. Semantic classes\nthat are static by definition remain unchanged. We refer\nto the semantic labels that are further subdivided based on\ntheir motion as semantic motion labels. An example of the\nresulting label fusion of semantic and motion labels to form\nsemantic motion labels is shown in Figure 3."}, {"title": "B. Predictive Motion Labels", "content": "In order to predict motion labels for a given point cloud,\nwe first filter out the ground [49] to focus on objects in\nthe scene, a common pre-processing step of scene flow\nmodels [50], [51], [52], [53], [54]. Based on self-supervised\nflow prediction [53] of the remaining points, we aim to\nderive motion labels, indicating whether a point is static\nor dynamic. The model takes consecutive point clouds as\ninput and predicts the future motion for each lidar point\nin the form of a 3D displacement vector. The scene flow\nmodel does not distinguish between the point's own motion\nand the observer's ego-motion and represents the overall\nmotion of a point between two consecutive frames. In order\nto derive relative displacements, we need to correct for the\nego-motion. This can be done by leveraging or learning\nodometry information [55]. After predicting the future point\ncloud $X_{t+1} = X_{t} + f_t$, we apply the learned rigid body\ntransformation $T_{t+1\\rightarrow t}$ of an odometry model, transforming\nthe predicted point cloud back into the coordinate system of\n$X_t$. This gives the future point cloud $X'_{t+1}$, which contains\nonly the predicted relative motion without the ego-motion,\nas described by the following formula:\n\n$X_{t+1} = T_{t+1\\rightarrow t} \\cdot \\begin{pmatrix} (X_{t} + f_t) \\\\ 1 \\end{pmatrix} \\$\n\nAs a result, static objects line up closely with the original\ndata of $X_t$, and only dynamic objects show a predicted\ndisplacement, as shown in Figure 4 a).\nTwo-Stage Clustering. An analysis of the velocity values\nof the flow predictions showed that separating static from\ndynamic classes is infeasible in a point-wise fashion, as\na strong overlap exists. What we found, however, is a\nsignificant difference when considering instance-wise nor-\nmalized standard deviations, as shown in Figure 5. While we\nperformed this analysis with ground-truth labels, in unlabeled\ndata, the necessity arises to form instance clusters."}, {"title": "C. Discrepancy Detection and Failure Classification", "content": "After obtaining motion labels from both the supervised\nand the self-supervised stream, we are now interested in\ndetecting contradictions between the labels, as shown in\nFigure 2. Given a semantic and a predictive motion label\nfor each lidar point, there exist four different categories,\nwhere we assign a color for visual inspection to each. First,\nwe color points both models deem static in green and\nthose both models deem dynamic in blue. For contradictory\npoints, we color those red where the supervised stream\npredicts a static point and the self-supervised a dynamic\none, and all others yellow in the opposite case. To help\na subsequent human oracle better understand a scene, we\nprovide a visual inspection tool where we map the lidar\npoints onto the corresponding RGB image for an improved\nscene understanding, as shown in Figure 6. Finally, we\ncluster instances with contradicting labels, so a human oracle\ncan classify complete scenes as well as single instances."}, {"title": "D. Implementation Details", "content": "For all models shown in Figure 2, we utilized available\narchitectures from the literature. We trained the supervised\nsemantic segmentation model SalsaNext [46] and the su-\npervised motion segmentation model of Chen et al. [47]\non the KITTI-360 dataset [58], as it is a large dataset\nthat contains semantic labels, motion labels, and odometry\ndata. However, KITTI-360 only provides ground truth for\naccumulated point clouds and not for raw scans. To obtain\nground truth labels for the raw scans, the labels for the\nraw point clouds were recovered with a nearest neighbor\nsearch. Here, we improved the work of Sanchez [59] in\norder to also recover labels of dynamic traffic participants.\nAs a result, instead of one billion labels of the accumulated\npoint clouds, about 6.9 billion labels of the raw scans were\nused for training. The training was performed on an NVIDIA\nRTX A6000. Hyperparameters were taken from the original\npapers [46], [47]. More details can be found in [48].\nFor the remaining three models, we used available pre-\ntrained model weights. For ground segmentation, we de-\nployed GndNet [49]. We chose FlowStep3D [53] as the\nself-supervised scene flow model because, at time of im-\nplementation, it had the lowest outlier rate among other\nself-supervised scene flow models on the KITTI scene flow\ndataset [60], [61]. The pre-trained model was trained on\nsynthetic data of the Flying Things3D [62] dataset with\n8,192 points per point cloud. To minimize the domain shift\nmentioned by Baur et al. [63] between the training dataset\nwith synthetic data and 8,192 points and the inference dataset\nwith raw scans and about 120,000 points from a Velodyne\nHDL-64E scanner, the following preprocessing steps were\nnecessary. First, only the lidar points from the field of\nview of the forward-looking camera were considered. Then,\nwe removed points farther than 35 m and excluded points\nclassified as ground. For the self-supervised odometry model,\nwe deployed DeLORA [55]."}, {"title": "IV. EVALUATION", "content": "In the first section of the evaluation, we incorporated a\nhuman oracle for Model Failure Classification. In the second\npart of the evaluation, we analyze the sensitivity of the\napproach towards anomalies in the environment.\n\nA. Model Failure Classification\nWe conduct an extensive analysis of our approach by ex-\namining the outputs of the models. Instead of analyzing only\nscenes that included detected discrepancies, we manually\nexamined over 20,000 frames to better understand the model\nbehaviors and scenes they react to.\nEvaluation Dataset. To minimize perceptual failures due\nto a domain shift, it is important to choose an evalua-\ntion dataset similar to the training dataset. Both, different\nweather conditions and differences between datasets from\ndifferent countries can lead to domain shifts [3]. However,\nclassical evaluation splits are too small to truly understand\nthe performance of our approach. Thus, we chose KITTI\nOdometry [58] for evaluation. KITTI-360, the dataset we\ntrained on, and KITTI Odometry [64] are closely related\ndatasets, as both were captured in Karlsruhe, Germany with\na Velodyne HDL-64E lidar. Since we used the KITTI-360\ndataset for training, and DeLORA was trained on sequences\n00-08 of the KITTI odometry dataset, we used the remaining\nsequences 09-21 of the KITTI odometry dataset for the\nevaluation. We used sequences 09 and 10 to quantitatively\nexamine motion segmentation performance for each part\nseparately and used SemanticKITTI [57] labels as ground\ntruth. To investigate model failures, sequences 11-21 of the\nKITTI Odometry datasets were used for qualitative analysis.\nAs shown in Fig. 7, we observe that the majority of points\nare predicted as static by both models and around 5% of the\npoints show model contradictions. Due to the pre-processed"}, {"title": "B. Sensitivity towards Anomalies", "content": "Generally, we are interested in scenarios where models\nfail. It is well known that perception models often struggle\nmost with anomalies in the environment. Thus, we also\nexamine the sensitivity of our approach towards object-level\nanomalies in the environment around the ego vehicle, as\nthese are most often examined [65], [66]."}, {"title": "Evaluation Data.", "content": "While our goal in the first part of\nthe evaluation was to minimize the domain gap between\ntraining and evaluation data, there are no labeled anomalies\nin the original KITTI dataset [64]. Thus, we utilized data\nfrom CODA [67], the only real-world dataset that provides\nlidar data and includes anomalies [68]. The CODA dataset\nprovides anomaly labels for objects based on three existing\ndata sets: KITTI [64], ONCE [69], and nuScenes [70].\nFor the CODA-KITTI split, the authors manually reviewed\nall misc labels available in the ground truth and relabeled\nsome as anomalies according to a labeling policy. Since we\ntrained our models on KITTI-360, this enables us to quanti-\ntatively examine our approach with only a small domain gap.\nFor CODA-nuScenes, the authors similarly adopted available\nannotations in a manual process. Finally, for CODA-ONCE,\nthey deployed an automated anomaly detection approach,\nmaking this subset the most relevant. CODA includes 1,500\nscenes with a total of 5,937 anomaly instances. Among those,\n4,746 belong to the superclass traffic_facility, followed by\n929 vehicle and 197 obstruction instances. With 396, most\nvehicle instances can be found in CODA-KITTI. In Figure 7,\nwe show the outputs of our approach on the CODA subsets,\nalso in comparison to the outputs of the KITTI dataset. We\ncan observe many more detected discrepancies, which is in\nline with the much higher number of anomalies, but the\nsubsets reveal strongly varying behavior patterns.\nFor further qualitative analysis, however, labeled anoma-\nlies in 3D are required. The original CODA dataset provides\nanomaly labels only in the form of 2D bounding boxes\nin images. Therefore, we present LidarCODA, a dataset\nbased on the CODA dataset [67] for evaluation. Based on\na frustum-based filter, subsequent clustering, and manual\ninspection, we transferred the original 2D labels from image\nspace into refined, point-wise 3D labels that go beyond the\ncoarse characteristic of the provided bounding boxes. More\ndetails can be found in [71]. LidarCODA is the first real-\nworld anomaly dataset with annotated lidar data, as shown\nin Figure 9. Here, also the different lidar systems utilized\nbecome clearly visible. Due to the sparse point cloud of\nnuScenes, many small or distant labeled anomalies in the\nimage space are only covered by a few or no lidar points.\nAnomalies. With LidarCODA, we provide ground truth\nfor object-level anomalies [1] in lidar data. Since our ap-\nproach is not specific to object-level anomalies but shows\nits strength in underrepresented or generally challenging or\natypical scenarios, which can also include scene- or scenario-\nlevel anomalies, we are primarily interested in the sensitivity\nof our approach towards these specific anomalies. This is"}, {"title": "V. CONCLUSION", "content": "In this work, we have presented complementary learning\nto detect real-world model failures. We leverage complemen-\ntary training paradigms to detect contradicting outputs on\nthe same task. This way, we can detect the failure modes"}]}