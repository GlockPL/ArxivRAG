{"title": "Gradient Boosting Reinforcement Learning", "authors": ["Benjamin Fuhrer", "Chen Tessler", "Gal Dalal"], "abstract": "Neural networks (NN) achieve remarkable results in various tasks, but lack key\ncharacteristics: interpretability, support for categorical features, and lightweight im-\nplementations suitable for edge devices. While ongoing efforts aim to address these\nchallenges, Gradient Boosting Trees (GBT) inherently meet these requirements.\nAs a result, GBTs have become the go-to method for supervised learning tasks\nin many real-world applications and competitions. However, their application in\nonline learning scenarios, notably in reinforcement learning (RL), has been limited.\nIn this work, we bridge this gap by introducing Gradient-Boosting RL (GBRL),\na framework that extends the advantages of GBT to the RL domain. Using the\nGBRL framework, we implement various actor-critic algorithms and compare their\nperformance with their NN counterparts. Inspired by shared backbones in NN\nwe introduce a tree-sharing approach for policy and value functions with distinct\nlearning rates, enhancing learning efficiency over millions of interactions. GBRL\nachieves competitive performance across a diverse array of tasks, excelling in\ndomains with structured or categorical features. Additionally, we present a high-\nperformance, GPU-accelerated implementation that integrates seamlessly with\nwidely-used RL libraries (available at https://github.com/NVlabs/gbrl). GBRL\nexpands the toolkit for RL practitioners, demonstrating the viability and promise of\nGBT within the RL paradigm, particularly in domains characterized by structured\nor categorical features.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning (RL) has shown great promise in various domains that involve sequential\ndecision making. However, many real-world tasks, such as inventory management, traffic signal\noptimization, network optimization, resource allocation, and robotics, are represented by structured\nobservations with categorical or mixed data types. These tasks can benefit significantly from\ndeployment and training on edge devices due to resource constraints. Moreover, interpretability\nis crucial in these applications for regulatory reasons and for trust in the decision-making process.\nCurrent neural network (NN) based solutions struggle with interpretability, handling categorical data,\nand supporting light implementations suitable for low-compute devices.\nGradient Boosting Trees (GBT) is a powerful ensemble method extensively used in supervised\nlearning due to its simplicity, accuracy, interpretability, and natural handling of structured and\ncategorical data. Frameworks such as XGBoost [7], LightGBM [20], and CatBoost [36] have become\nintegral in applications spanning finance [49], healthcare [54, 27, 43], and competitive data science\n[6]. Despite their successes, GBT has seen limited application in RL. This is primarily because\ntraditional GBT libraries are designed for static datasets with predefined labels, contrasting with the\ndynamic nature of RL. The distribution shift in both input (state) and output (reward) poses significant\nchallenges for the direct application of GBT in RL. Moreover, there is a notable lack of benchmarks\nor environments tailored for structured data, further hindering progress in this area."}, {"title": "2 Related Work", "content": "Gradient boosted trees. Recent advances have extended GBT's capabilities beyond traditional\nregression and classification. In ranking problems, GBT has been used to directly optimize ranking\nmetrics, as demonstrated by frameworks like StochasticRank [51] and recent advancements explored\nin Lyzhin et al. [26]. Additionally, GBT offer probabilistic predictions through frameworks like\nNGBoost [11], enabling uncertainty quantification [28]. The connection between GBT and Gaussian\nProcesses [52, 45] offers further possibilities for uncertainty-aware modeling. Recently, Ivanov and\nProkhorenkova [18] modeled graph-structured data by combining GBT with graph neural networks.\nDespite their versatility, applying GBT in RL remains a relatively less explored area. Several works\nhave employed GBT as a function approximator within off-policy RL methods, including its use\nin Q-learning [1] and in bandit settings to learn inverse propensity scores [24]. Recently, Brukhim\net al. [5] proposed a boosting framework for RL where a base policy class is incrementally enhanced\nusing linear combinations and nonlinear transformations. However, these previous works have not"}, {"title": "3 Preliminaries", "content": "We begin by introducing Markov Decision Processes (MDPs) and the AC schema. Then, we introduce\nGBT. In the following section, we show how to combine both of these paradigms into GBRL."}, {"title": "3.1 Markov Decision Process", "content": "We consider a fully observable infinite-horizon Markov decision process (MDP) characterized by the\ntuple (S, A, P, R). At each step, the agent observes a state $s \\in S$ and samples an action $a \\in A$ from\nits policy $\\pi(s, a)$. Performing the action causes the system to transition to a new state $s'$ based on\nthe transition probabilities $P(s' | s, a)$, and the agent receives a reward $r \\sim R(s, a)$. The objective is\nto find an optimal policy $\\pi^*$ that maximizes the expected discounted reward $J(\\pi) = E[\\Sigma_{t=0}^{\\infty} \\gamma^t r_t]$,\nwith a discount factor $\\gamma \\in [0, 1)$.\nThe action-value function $Q(s, a) := E_{\\pi}[\\Sigma_{t'=0}^{\\infty} \\gamma^{t'}R(s_{t+t'}, a_{t+t'})| s_t = s, a_t = a]$ estimates the\nexpected returns of performing action a in state s and then acting according to $\\pi$. Additionally, the\nvalue function $V(s) := E_{\\pi}[\\Sigma_{t'=0}^{\\infty} \\gamma^{t'}R(s_{t+t'}, a_{t+t'})| s_t = s]$, predicts the expected return starting\nfrom state s and acting according to $\\pi$. Finally, the advantage function $A_{\\pi}(s, a) := Q(s, a) - V(s)$,\nindicates the expected relative benefit of performing action a over acting according to $\\pi$."}, {"title": "3.2 Actor-Critic Reinforcement Learning", "content": "Actor-critic methods are a common method to solve the objective $J(\\pi)$. They learn both the policy\nand value. In the GBRL framework, we extend three common AC algorithms to support GBT-based\nfunction approximators.\nA2C [32] is a synchronous, on-policy AC algorithm designed to improve learning stability. The critic\nlearns a value function, $V(s)$, used to estimate the advantage. This advantage is incorporated into the\npolicy gradient updates, reducing variance and leading to smoother learning. The policy is updated\nusing the following gradient: $\\nabla_{\\theta}J(\\pi_{\\theta}) = E[\\nabla_{\\theta}log \\pi_{\\theta}(a|s)A(s, a)]$."}, {"title": "3.3 Gradient Boosting Trees as Functional Gradient Descent", "content": "Gradient boosting trees (GBT) [12] are a non-parametric machine learning technique that combines\ndecision tree ensembles with functional gradient descent [30]. GBT iteratively minimizes the expected\nloss $L(F(x)) = E_{x,y}[L(y, F(x))]$ over a dataset $D = \\{(x_i, y_i)\\}_{i=1}^N$. A GBT model, $F_K$, predicts\noutputs using K additive trees as follows:\n$F_K(x_i) = F_0 + \\sum_{k=1}^K \\epsilon h_k(x_i),$\nwhere $\\epsilon$ is the learning rate, $F_0$ is the base learner, and each $h_k$ is an independent regression tree\npartitioning the feature space.\nIn the context of functional gradient descent, the objective is to minimize the expected loss $L(F(x)) =$\n$E_{x,y}[L(y, F(x))]$ with respect to the functional F. Here, a functional $F : H \\rightarrow \\mathbb{R}$ maps a function\nspace to real numbers. A GBT model can be viewed as a functional F that maps a linear combination\nof binary decision trees to outputs: $F : lin(H) \\rightarrow \\mathbb{R}^D$, where H is the decision tree function class.\nWe start with an initial model, $F_0$, and iteratively add trees to F to minimize the expected loss.\nSimilar to parametric gradient descent, at each iteration k, we minimize the loss by taking a step in\nthe direction of the functional gradient $g := \\nabla_{F_{k-1}}L(y_i, F_{k-1}(x_i))$. However, we are constrained\nto gradient directions within H. Thus, we project the gradient $g_k$ into a decision tree by solving:\n$h_k = arg \\min_h || \\epsilon g_k - h(x) ||^2.$"}, {"title": "4 Gradient Boosting Reinforcement Learning", "content": "In this work, we extend the framework of GBT to support AC algorithms in the task of RL. The\nobjective in RL is to optimize the return J, the cumulative reward an agent receives. Unlike in\nsupervised learning, the target predictions are unknown a priori. RL agents learn through trial\nand error. Good actions are reinforced by taking a step in the direction of the gradient $\\nabla J$. This\nformulation aligns perfectly with functional gradient ascent; thus, in GBRL, we optimize the objective\ndirectly over the decision tree function class. This is achieved by iteratively growing the ensemble of\ntrees $\\{h_i\\}$. The ensemble outputs $\\theta$, representing AC parameters such as the policy $\\pi$ and the value\nfunction. For example, $\\theta = [\\mu(s), \\sigma(s), V(s)]$ for a Gaussian policy. At each iteration, a new tree\n$h_k$, constructed to minimize the distance to $\\nabla_{\\theta_{k-1}} J$, is added to the ensemble. Here, The resulting\nmethod is an application of GBT as a functional gradient optimizer $\\theta_k \\approx \\theta_0 + \\epsilon \\sum_{m=0}^{k-1} \\nabla_{\\theta_m} J$.\nHowever, RL presents unique challenges for GBT. RL involves a nonstationary state distribution and\ninherent online learning, causing gradients to vary in magnitude and direction. Large gradients in\nunfavorable directions risk destabilizing training or leading to catastrophic forgetting. Moreover,\nfeedback in RL is provided through interactions with the environment and is not available a priori.\nThis contrasts with supervised learning settings, where gradients decrease with boosting iterations,\nand targets are predefined. As a result, many of the key features that traditional GBT libraries rely on"}, {"title": "5 Experiments", "content": "Our experiments aim to answer the following questions:\n1. GBT as RL Function Approximator: Can GBT-based AC algorithms effectively solve\ncomplex high-dimensional RL tasks?\n2. Comparison to NNs: How does GBRL compare with NN-based training in various RL\nalgorithms?\n3. Benefits in Categorical Domains: Do the benefits of GBT in supervised learning transfer\nto the realm of RL?\n4. Comparison to Traditional GBT libraries: Can we use traditional GBT libraries instead\nof GBRL for RL tasks?\n5. Evaluating the shared AC architecture: How does sharing the tree structure between the\nactor and the critic impact performance?\nWe implemented GBT-based versions of A2C, PPO, and AWR within Stable Baselines3. We refer to\nour implementations as PPO GBRL, A2C GBRL, and AWR GBRL. We evaluated GBRL against the"}, {"title": "6 Conclusion", "content": "Historically, RL practitioners have relied on tabular, linear, and NN-based function approximators.\nBut, GBT, a widely successful tool in supervised learning, has been absent from this toolbox. We\npresent a method for effectively integrating it into RL and demonstrate domains where it excels\ncompared to NNs. GBRL is a step toward solutions that are more interpretable, well suited for\nreal-world tasks with structured data, or capable of deployment on low-compute devices.\nThe choice of an RL method depends on the task characteristics: tabular and linear approaches\nare suitable for small state spaces or simple mappings, while NNs handle complex relationships in\nunstructured data. GBT thrives in complex, yet structured environments. In such cases, we observe\nthe advantage of GBRL over NNs, reflecting its already known benefits in supervised learning.\nA crucial component of GBRL is our efficient adaptation of GBT for AC methods, which allows\nthe simultaneous optimization of distinct objectives. We optimized this approach for large-scale\nensembles using GPU acceleration (CUDA). Furthermore, GBRL integrates seamlessly with existing\nRL libraries, promoting ease of use and adoption."}, {"title": "7 Limitations and Future Directions", "content": "In this work, we integrated the highly popular GBT, typically used in supervised learning, into RL.\nOur results show that GBT is competitive across a range of problems. However, we identified several\nlimitations and compelling areas for further research. First, a significant challenge lies in the continu-\nous generation of trees. As the policy improves through numerous updates, the size of the ensemble\nincreases. This unbounded growth has implications for memory usage, computational efficiency, and\nthe feasibility of online real-time adaptation. The problem is exacerbated by off-policy methods that\nbuild many trees per sample. Moreover, the redundancy of trees, especially those from early stages,\nsuggests that the final policy could be represented with a much smaller ensemble. Consequently,\ndeveloping strategies for tree pruning, ensemble compression, or dynamically managing ensemble\nsize could offer crucial optimizations without compromising performance.\nAnother key challenge lies in effectively integrating GBT with additional state-of-the-art RL algo-\nrithms such as DDPG [23] or SAC [17]. These require differentiable Q-functions to update the policy.\nSince GBTs are not differentiable, new solutions are needed to incorporate them into these algorithms.\nOne such possible direction can be probabilistic trees, where each node represents the probability of\ntraversing the graph."}, {"title": "Appendix", "content": "This appendix provides supplementary materials that support the findings and methodologies dis-\ncussed in the main text. It is organized into four sections to present the full experiment results,\nimplementation details, hyperparameters used during the experiments, training progression plots, and\nexperimental plots, respectively. These materials offer detailed insights into the research process and\noutcomes, facilitating a deeper understanding and replication of the study."}, {"title": "A Implementaion Details and Hyperparameters", "content": "Included in this section are implementation details, information regarding compute resources, and\ntables containing the hyperparameters used in our experiments enabling the reproducibility of our\nresults. Table 1 lists GBRL hyperparameters for all experiments."}, {"title": "A.1 Environments", "content": "The Football domain consists of a vectorized 115-dimensional observation space that summarizes the\nmain aspects of the game and 19 discrete actions. We focus on its academy scenarios, which present\nsituational tasks involving scoring a single goal. A standard reward of +1 is granted for scoring, and\nwe employed the \"Checkpoints\" shaped reward structure. This structure provides additional points as\nthe agent moves closer towards the goal, with a maximum reward of 2 per scenario. The Atari-ram\nenvironment consists of a vectorized 128-dimensional observational space representing the 128 byte\nRAM state and up to 18 discrete actions. We trained agents in both domains for 10M timesteps.\nThe MiniGrid environment [8] is a 2D grid world with goal-oriented tasks requiring object interaction.\nThe observation space consists of a 7x7 image representing the grid, a mission string, and the agent's\ndirection. Each tile in the observed image contains a 3D tuple dictating an object's color, type, and\nstate. All MiniGrid tasks emit a reward of +1 for successful completion and 0 otherwise.\nWe trained our NN-based agents on a flattened observation space using the built-in one-hot wrapper.\nFor GBRL agents, we generated a 51-dimensional categorical observational space by encoding each\nunique tile tuple as a categorical string to represent the observed image. Categorical features were\nadded for the agent's direction (up, left, right, down) and missions. All agents were trained for 1M\ntimesteps, except for PutNear, FourRooms, and Fetch tasks, which were trained for 10M based on the\nreported values for PPO NN in RL Baselines3 Zoo."}, {"title": "A.2 Compute Resources", "content": "All experiments were done on the NVIDIA NGC platform on a single NVIDIA V100-32GB GPU\nper experiment. Training time and compute requirements vary between algorithms and according\nto hyperparameters. The number of boosting iterations has the largest impact on both runtime and\nmemory. GBRL experimental runs required from 1GB to 24GB of GPU memory. Moreover, runtime\nvaried from 20 minutes for 1M timesteps training on classic environments and up to 5 days for 10M\ntimesteps on Atari-ram. NN experimental runs required up to 3GB of GPU memory and runtime\nranged from 10 minutes and up to 3 days. The total compute time for all experiments combined was\napproximately 1800 GPU hours. Additionally, the research project involved preliminary experiments\nand hyperparameter tuning, which required an estimated additional 168 GPU hours."}, {"title": "B Detailed Results Tables", "content": "This section contains tables presenting the mean and standard deviation of the average episode reward\nfor the final 100 episodes within each experiment. More specifically, Table 2 presents results for\nContinuous Control & Block2D environments, Tables 3 and 4 present results for the high-dimensional\nvectorized environments, and Table 5 presents results for the categorical environments."}]}