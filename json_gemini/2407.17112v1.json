{"title": "Neural Dueling Bandits", "authors": ["Arun Verma", "Zhongxiang Dai", "Xiaoqiang Lin", "Patrick Jaillet", "Bryan Kian Hsiang Low"], "abstract": "Contextual dueling bandit is used to model the bandit problems, where a learner's goal is to find the best arm for a given context using observed noisy preference feedback over the selected arms for the past contexts. However, existing algorithms assume the reward function is linear, which can be complex and non-linear in many real-life applications like online recommendations or ranking web search results. To overcome this challenge, we use a neural network to estimate the reward function using preference feedback for the previously selected arms. We propose upper confidence bound- and Thompson sampling-based algorithms with sub-linear regret guarantees that efficiently select arms in each round. We then extend our theoretical results to contextual bandit problems with binary feedback, which is in itself a non-trivial contribution. Experimental results on the problem instances derived from synthetic datasets corroborate our theoretical results.", "sections": [{"title": "1 Introduction", "content": "Contextual dueling bandits (or preference-based bandits) [1, 2, 3] is a sequential decision-making framework that is widely used to model the contextual bandit problems [4, 5, 6, 7, 8] in which a learner's goal is to find an optimal arm by sequentially selecting a pair of arms (also refers as a duel) and then observing noisy preference feedback (i.e., one arm is preferred over another) for the selected arms. Contextual dueling bandits has many real-life applications, e.g., online recommendation, ranking web search, comparing two text responses generated from LLMs, and rating two restaurants/movies, especially in the applications where it is easier to observe preference between two arms than knowing the absolute reward for the selected arm. The preference feedback between two arms\u00b2 is often assumed to follow the Bradley-Terry-Luce (BTL) model [2, 10, 11] in which the probability of preferring an arm is proportional to the exponential of its reward.\nSince the number of contexts (e.g., users of online platforms) and arms (e.g., movies/search results to recommend) can be very large (or infinite), the reward of an arm is assumed to be parameterized by an unknown function, e.g., a linear function [1, 2, 3]. However, the reward function may not always be linear in practice. To overcome this challenge, this paper parameterizes the reward function via a non-linear function, which needs to be estimated using the available preference feedback for selected arms. To achieve this, we can estimate the non-linear function by using either a Gaussian processes [12, 13, 14] or a neural network [7, 8]. However, due to the limited expressive power of the Gaussian processes, it fails when optimizing highly complex functions. In contrast, neural networks (NNs) possess strong expressive power and can model highly complex functions [15, 16].\nIn this paper, we first introduce the problem setting of neural dueling bandits, in which we use a neural network to model the unknown reward function in contextual dueling bandits. As compared to the existing work on neural contextual bandits [7, 8], we have to use cross-entropy loss due to"}, {"title": "2 Problem Setting", "content": "Contextual dueling bandits. We consider a contextual dueling bandit problem in which a learner selects two arms (also refers as a duel) for a given context and observes preference feedback over arms. The learner's goal is to find the best arm for each context. Let $C \\subset \\mathbb{R}^{d_c}$ be the context set and $A \\subset \\mathbb{R}^{d_a}$ be finite arm set, where $d_c \\geq 1$ and $d_a \\geq 1$. At the beginning of round t, the environment generates a context $c_t \\in C$ and the learner selects two arms (i.e., $a_{t,1}$, and $a_{t,2}$) from the finite arm set A. After selecting two arms, the learner observes stochastic preference feedback $y_t$ for the selected arms, where $y_t = 1$ implies the arm $a_{t,1}$ is preferred over arm $a_{t,2}$ and $y_t = 0$ otherwise. We assume that the preference feedback depends on an unknown non-linear reward function $f : C \\times A \\rightarrow \\mathbb{R}$. For brevity, we denote the set of all context-arm feature vectors in the round t by $X_t \\subset \\mathbb{R}^{d}$ and use $x_t$ to represent the context-arm feature vector for context $c_t$ and an arm a.\nStochastic preference model. We assume the preference has a Bernoulli distribution that follows the Bradley-Terry-Luce (BTL) model [10, 11], which is commonly used in the dueling bandits [1, 2, 3]. Under BTL preference model, the probability that the first selected arm ($x_{t,1}$) is preferred over the second selected arm ($x_{t,2}$) for the given reward function f is given by\n$\\mathbb{P}\\{x_{t,1} > x_{t,2}\\} = \\mathbb{P} \\{Y_t = 1|x_{t,1}, x_{t,2}\\} = \\frac{\\exp (f(x_{t,1}))}{\\exp (f(x_{t,1})) + \\exp (f(x_{t,2}))} = \\mu (f(x_{t,1}) - f(x_{t,2})) $.\nwhere $x_{t,1} > x_{t,2}$ denotes that $x_{t,1}$ is preferred over $x_{t,2}$, $\\mu(x) = 1/(1 + e^{-x})$ is the logistic function, and $f (x_{t,i})$ is the latent reward of the i-th selected arm for the given context $c_t$. We need the following standard assumptions on function $\\mu$ (also known as a link function in the literature [2, 19]):\nAssumption 1.\n$\\kappa_{\\mu} = \\inf_{x,x' \\epsilon X} \\mu(f(x) - f(x')) > 0$ for all pairs of context-arm.\nThe link function $\\mu : \\mathbb{R} \\rightarrow [0, 1]$ is continuously differentiable and Lipschitz with constant $L_{\\mu}$. For logistic function, $L_{\\mu} \\leq 1/4$.\nPerformance measure. After selecting two arms, denoted by $x_{t,1}$ and $x_{t,2}$, in round t, the learner incurs an instantaneous regret. There are two common notions of instantaneous regret in the dueling bandits setting, i.e., average instantaneous regret: $r_t^a = f(x_t^*) - (f(x_{t,1}) + f(x_{t,2})) /2$, and weak instantaneous regret: $r_t^w = f(x_t^*) - \\max \\{f(x_{t,1}), f(x_{t,2})\\}$, where $x_t^* = \\arg \\max_{x \\epsilon X_t} f(x)$ denotes the best arm for a given context that maximizes the value of the underlying reward function. After observing preference feedback for T pairs of arms, the cumulative regret (or regret, in short) of a"}, {"title": "3 Neural Dueling Bandits", "content": "Having a good reward function estimator is the key for any contextual bandit algorithm to achieve good performance, i.e., smaller regret. As the underlying reward function is complex and non-linear, we use fully connected neural networks [7, 8] to estimate the reward function only using the preference feedback. Using this estimated reward function, we propose two algorithms based on the upper confidence bound and Thomson sampling with sub-linear regret guarantees.\nReward function estimation using neural network. To estimate the unknown reward function f, we use a fully connected neural network (NN) with depth L > 2, the width of hidden layer m, and ReLU activations as done in [7] and [8]. Let $h(x; \\theta)$ represent the output of a full-connected neural network with parameters $\\theta$ for context-arm feature vector x, which is defined as follows:\n$h(x; \\theta) = W_LReLU (W_{L-1}ReLU (\\cdots ReLU (W_1x)))$, \nwhere $ReLU(x) = \\max\\{x,0\\}$, $W_1 \\in \\mathbb{R}^{m \\times d}$, $W_l \\in \\mathbb{R}^{m \\times m}$ for $2 < l < L$, $W_L \\in \\mathbb{R}^{m \\times 1}$. We denote the parameters of NN by $\\theta = (vec (W_1) ; \\cdots vec (W_L))$, where $vec (A)$ converts a $M \\times N$ matrix A into a MN-dimensional vector. We use m to denote the width of every layer of the NN, use p to represent the total number of NN parameters, i.e., $p = dm + m^2(L - 1) + m$, and use $g(x; \\theta)$ to denote the gradient of $h(x; \\theta)$ with respect to $\\theta$.\nThe arms selected by the learner for context received in round s is denoted by $x_{s,1}, x_{s,2} \\in X_s$ and the observed stochastic preference feedback is denoted by $y_s = \\mathbb{1}(x_{s,1} > x_{s,2})$, which is equal to 1 if the arm $x_{s,1}$ is preferred over the arm $x_{s,2}$ and 0 otherwise. At the beginning of round t, we use the current history of observations $\\{(x_{s,1},x_{s,2},y_s)\\}_{s=1}^{t-1}$ to train the neural network (NN) using gradient descent to minimize the following loss function:\n$L_t(\\theta) = \\frac{1}{m} \\sum_{s=1}^{t-1} [\\log \\mu ((-1)^{1-y_s} [h(x_{s,1};\\theta) - h(x_{s,2};\\theta)])] + \\frac{\\lambda}{m}||\\theta - \\theta_0||_2^2 ,$ (1)\nHere $\\theta_0$ represents the initial parameters of the NN, and we initialize $\\theta_0$ following the standard practice of neural bandits [7, 8] (refer to Algorithm 1 in [8] for details). Here, minimizing the first term in the loss function (i.e., the term involving the summation from t 1 terms) corresponds to the maximum log likelihood estimate (MLE) of the parameters $\\theta$. Next, we develop algorithms that use the trained NN with parameter $\\theta_t$ to select the best arms (duel) for each context."}, {"title": "3.1 Neural dueling bandit algorithms", "content": "With the trained NN as an estimate for the unknown reward function, the learner has to decide which two arms (or duel) must be selected for the subsequent contexts. We use UCB- and TS-based algorithms that handle the exploration-exploitation trade-off efficiently.\nUCB-based algorithm. Using upper confidence bound for dealing with the exploration-exploitation trade-off is common in many sequential decision-making problems [2, 7, 17]. We propose a UCB-based algorithm named NDB-UCB, which works as follows: At the beginning of the round t, the algorithm trains the NN using available observations. After receiving the context, it selects the first arm greedily (i.e., by maximizing the output of the trained NN with parameter $\\theta_t$) as follows:\n$x_{t,1} = \\arg \\max_{x \\epsilon X_t} h(x; \\theta_t).$ (2)\nNext, the second arm $x_{t,2}$ is selected optimistically, i.e., by maximizing the UCB value:\n$x_{t,2} = \\arg \\max_{x \\epsilon X_t} [h(x; \\theta_t) + \\nu_T\\sigma_t^{-1}(x, x_{t,1})],$ (3)\nwhere $\\nu_T = (\\beta_T + B\\sqrt{\\lambda/\\kappa_{\\mu}+1})\\sqrt{\\kappa_{\\mu}/\\lambda}$ in which $\\beta_T = \\sqrt{d+2\\log(1/\\delta)}$ and $d$ is the effective dimension. We define the effective dimension in Section 3.2 (see Eq. (4)). We define\n$\\sigma_t^{-1}(x_1,x_2) = \\frac{1}{\\kappa_{\\mu} \\nu_T} \\left\\| \\sqrt{\\frac{1}{m}} V_t^{-\\frac{1}{2}} (g(x_1) - g(x_2)) \\right\\|_2,$\nTS-based algorithm. Thompson sampling [3, 21] selects an arm according to its probability of being the best. Many works [3, 14, 21, 38] have shown that TS is empirically superior than to its counterpart UCB-based bandit algorithms. Therefore, in addition, we also propose another algorithm based on TS named NDB-TS, which works similarly to NDB-UCB except that the second arm $x_{t,2}$ is selected differently. To select the second arm $x_{t,2}$, for every arm $x \\in X_t$, it firstly samples a reward $r_t(x) \\sim \\mathcal{N} (h(x; \\theta_t) - h(x_{t,1}; \\theta_t), \\nu_T\\sigma_t^{-1}(x, x_{t,1}))$ and then selects the second arm as $x_{t,2} = \\arg \\max_{x \\epsilon X_t}r_t(x).$"}, {"title": "3.2 Regret analysis", "content": "Let K denote the finite number of available arms in each round, H denote the NTK matrix for all T \u00d7 K context-arm feature vectors in the T rounds, and $h = (f(x_1), \\cdots, f(x_t))^\\top$. The NTK matrix H definition is adapted to our setting from Definition 4.1 of [7]. We now introduce the assumptions needed for our regret analysis, all of which are standard assumptions in neural bandits [7, 8].\nAssumption 2. Without loss of generality, we assume that\nthe reward function is bounded: $|f(x)| \\leq 1,\\forall x \\epsilon X_t, t \\epsilon [T]$,\nthere exists $\\lambda_0 > 0$ s.t. $H \\geq \\lambda_0 I$, and\nall context-arm feature vectors satisfy $||x||_2 = 1$ and $[x]_j = [x]_{j+d/2}, \\forall x \\epsilon X_t, \\forall t \\epsilon [T]$.\nThe last assumption in Assumption 2 above, together with the way we initialize $\\theta_0$ (i.e., following standard practice in neural bandits [7, 8]), ensures that $h(x; \\theta_0) = 0, \\forall x \\epsilon X_t, \\forall t \\epsilon [T]$.\nLet $H' = \\sum_{t=1}^T \\sum_{(i,j) \\epsilon C} z(s)z(s)^\\top/m$, in which $z(s) = g(x_{s,i}) - g(x_{s,j})$ and $C$ denotes all pairwise combinations of K arms. We now define the effective dimension as follows:\n$\\tilde{d} = \\log \\det (\\frac{H'}{\\lambda}+I)$. (4)\nCompared to the previous works on neural bandits, our definition of $d$ features extra dependencies on $\\kappa_{\\mu}$. Moreover, our H' contains T \u00d7 K \u00d7 (K \u2212 1) contexts, which is more than the T \u00d7 K contexts of [7] and [8]. Hence, our $d$ is expected to be generally larger than their standard effective dimension.\nNote that placing an assumption on d above is analogous to the assumption on the eigenvalue of the matrix $M_t$ in the work on linear dueling bandits [2]. For example, in order for our final regret bound to be sub-linear, we only need to assume that $\\tilde{d} = \\tilde{\\mathcal{O}}(\\sqrt{T})$, which is analogous to the assumption from [2]: $\\sum_{t=T+1}^{2T} \\lambda_{\\min}^2(M_t) \\leq c\\sqrt{T}$."}, {"title": "4 Theoretical Insights for Reinforcement Learning with Human Feedback", "content": "Our algorithms and theoretical results can also provide insights on the celebrated reinforcement learning with human feedback (RLHF) algorithm [39], which has been the most widely used method for the alignment of large language models (LLMs). In RLHF, we are given a dataset of user preferences, in which every data point consists of a prompt and a pair of responses generated by the"}, {"title": "5 Neural Contextual Bandits with Binary Feedback", "content": "We now extend our results to the neural contextual bandit problem in which a learner only observes binary feedback for the selected arms (note that the learner only selects one arm in every iteration). Observing binary feedback is very common in many real-life applications, e.g., click or not in online recommendation and treatment working or not in clinical trials [19, 45, 46].\nContextual bandits with binary feedback. We consider a contextual bandit problem with binary feedback. In this setting, we assume that the action set is denoted by A. Let $X \\subset \\mathbb{R}^d$ denote the set of all context-arm feature vectors in the round t and $x_{t,a}$ represent the context-arm feature vector for context $c_t$ and an arm $a \\in A$. At the beginning of round t, the environment generates context-arm feature vectors $\\{x_{t,a}\\}_{a \\in A}$ and the learner selects an arm $a_t$, whose corresponding context-arm feature vector is given by $x_{t,a}$. After selecting the arm, the learner observes a stochastic binary feedback $y_t \\in \\{0, 1\\}$ for the selected arm. We assume the binary feedback follows a Bernoulli distribution, where the probability of $y_t = 1$ for context-arm feature vector $x_{t,a}$ is given by $\\mathbb{P} \\{y_t = 1|X_{t,a}\\} = \\mu (f(x_{t,a}))$, where $\\mu : \\mathbb{R} \\rightarrow [0, 1]$ is a continuously differentiable and Lipschitz with constant $L_{\\mu}$, e.g., logistic function, i.e., $\\mu(x) = 1/(1+e^{-x})$. The link function must also satisfy $\\kappa_{\\mu} = \\inf_{x \\epsilon X} \\mu(f(x)) > 0$ for all arms.\nPerformance measure. The learner's goal is to select the best arm for each context, denoted by $x_t^* = \\arg \\max_{x \\epsilon X_t} f(x)$. Since the reward function f is unknown, the learner uses available observations $\\{(x_{s,a}, y_s)\\}_{s=1}^{t-1}$ to estimate the function f and then use the estimated function to select the arm $a_t$ for context $x_t$. After selecting the arm $a_t$, the learner incurs an instantaneous regret,"}, {"title": "5.1 Reward function estimation using neural network and our algorithms", "content": "To estimate the unknown reward function f, we use a fully connected neural network (NN) with parameters $\\theta$ as used in the Section 3. The context-arm feature vector selected by the learner in round s is denoted by $x_{s,a} \\in X_s$, and the observed stochastic binary feedback is denoted by $y_s$. At the beginning of round t, we use the current history of observations $\\{(x_{s,a}, y_s)\\}_{s=1}^{t-1}$ and use it to train the neural network (NN) by minimizing the following loss function (using gradient descent):\n$\\mathcal{L}_t(\\theta) = \\frac{1}{m} \\sum_{s=1}^{t-1} [y_s \\log\\mu (h(x_{s,a}; \\theta)) + (1 - y_s) \\log (1 - \\mu (h(t_{s,a}; h(x_{s,a}; \\theta))) ] + \\frac{\\lambda}{m}||\\theta - \\theta_0||_2^2,$ (6)\nwhere $\\theta_0$ represents the initial parameters of the NN. With the trained NN, we use UCB- and TS-based algorithms that handle the exploration-exploitation trade-off efficiently.\nUCB-based algorithm. We propose a UCB-based algorithm named NCBF-UCB, which works as follows: At the beginning of the round t, it trains the NN using available observations. After receiving a context, the algorithm selects the arm optimistically as follows:\n$x_{t,a} = \\arg \\max_{x \\epsilon X_t} [h(x; \\theta_t) + \\nu_T\\sigma_t^{-1}(x)],$ (7)\nwhere $\\sigma_t^{-1}(x) = \\left\\| \\sqrt{\\frac{1}{\\kappa_{\\mu} \\nu_T}} V_t^{-\\frac{1}{2}} g(x;\\theta_0) \\right\\|_2$, in which $V_t = \\sum_{s=1}^{t}g(x; \\theta_0)g(x; \\theta_0)^\\top + I$, $\\nu_T = (\\beta_T + B\\sqrt{\\lambda/\\kappa_{\\mu}+1})\\sqrt{\\kappa_{\\mu}/\\lambda}$ in which $\\beta_T = \\frac{1}{\\sqrt{K}} \\sqrt{d+2\\log(1/\\delta)}$ and $d_0$ is the effective dimension. We define the effective dimension later in this section (see Eq. (8)), which is different from Eq. (4)."}]}