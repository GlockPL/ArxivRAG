{"title": "Neural Dueling Bandits", "authors": ["Arun Verma", "Zhongxiang Dai", "Xiaoqiang Lin", "Patrick Jaillet", "Bryan Kian Hsiang Low"], "abstract": "Contextual dueling bandit is used to model the bandit problems, where a learner's\ngoal is to find the best arm for a given context using observed noisy preference\nfeedback over the selected arms for the past contexts. However, existing algorithms\nassume the reward function is linear, which can be complex and non-linear in\nmany real-life applications like online recommendations or ranking web search\nresults. To overcome this challenge, we use a neural network to estimate the reward\nfunction using preference feedback for the previously selected arms. We propose\nupper confidence bound- and Thompson sampling-based algorithms with sub-linear\nregret guarantees that efficiently select arms in each round. We then extend our\ntheoretical results to contextual bandit problems with binary feedback, which is\nin itself a non-trivial contribution. Experimental results on the problem instances\nderived from synthetic datasets corroborate our theoretical results.", "sections": [{"title": "1 Introduction", "content": "Contextual dueling bandits (or preference-based bandits) [1, 2, 3] is a sequential decision-making\nframework that is widely used to model the contextual bandit problems [4, 5, 6, 7, 8] in which a\nlearner's goal is to find an optimal arm by sequentially selecting a pair of arms (also refers as a duel)\nand then observing noisy preference feedback (i.e., one arm is preferred over another) for the selected\narms. Contextual dueling bandits has many real-life applications, e.g., online recommendation,\nranking web search, comparing two text responses generated from LLMs, and rating two restaurants/\nmovies, especially in the applications where it is easier to observe preference between two arms than\nknowing the absolute reward for the selected arm. The preference feedback between two arms\u00b2 is\noften assumed to follow the Bradley-Terry-Luce (BTL) model [2, 10, 11] in which the probability of\npreferring an arm is proportional to the exponential of its reward.\nSince the number of contexts (e.g., users of online platforms) and arms (e.g., movies/search results to\nrecommend) can be very large (or infinite), the reward of an arm is assumed to be parameterized by\nan unknown function, e.g., a linear function [1, 2, 3]. However, the reward function may not always\nbe linear in practice. To overcome this challenge, this paper parameterizes the reward function via a\nnon-linear function, which needs to be estimated using the available preference feedback for selected\narms. To achieve this, we can estimate the non-linear function by using either a Gaussian processes\n[12, 13, 14] or a neural network [7, 8]. However, due to the limited expressive power of the Gaussian\nprocesses, it fails when optimizing highly complex functions. In contrast, neural networks (NNs)\npossess strong expressive power and can model highly complex functions [15, 16].\nIn this paper, we first introduce the problem setting of neural dueling bandits, in which we use a\nneural network to model the unknown reward function in contextual dueling bandits. As compared\nto the existing work on neural contextual bandits [7, 8], we have to use cross-entropy loss due to"}, {"title": "2 Problem Setting", "content": "Contextual dueling bandits. We consider a contextual dueling bandit problem in which a learner\nselects two arms (also refers as a duel) for a given context and observes preference feedback over\narms. The learner's goal is to find the best arm for each context. Let $C \\subset \\mathbb{R}^{d_c}$ be the context set and\n$A \\subset \\mathbb{R}^{d_a}$ be finite arm set, where $d_c \\geq 1$ and $d_a \\geq 1$. At the beginning of round $t$, the environment\ngenerates a context $c_t \\in C$ and the learner selects two arms (i.e., $a_{t,1}$, and $a_{t,2}$) from the finite arm set\n$A$. After selecting two arms, the learner observes stochastic preference feedback $y_t$ for the selected\narms, where $y_t = 1$ implies the arm $a_{t,1}$ is preferred over arm $a_{t,2}$ and $y_t = 0$ otherwise. We assume\nthat the preference feedback depends on an unknown non-linear reward function $f : C \\times A \\rightarrow \\mathbb{R}$.\nFor brevity, we denote the set of all context-arm feature vectors in the round $t$ by $X_t \\subset \\mathbb{R}^d$ and use\n$x_t$ to represent the context-arm feature vector for context $c_t$ and an arm $a$.\nStochastic preference model. We assume the preference has a Bernoulli distribution that follows the\nBradley-Terry-Luce (BTL) model [10, 11], which is commonly used in the dueling bandits [1, 2, 3].\nUnder BTL preference model, the probability that the first selected arm ($x_{t,1}$) is preferred over the\nsecond selected arm ($x_{t,2}$) for the given reward function $f$ is given by\n$\\mathbb{P}\\{x_{t,1} > x_{t,2}\\} = \\mathbb{P} \\{Y_t = 1|x_{t,1}, x_{t,2}\\} = \\frac{\\exp (f(x_{t,1}))}{\\exp (f(x_{t,1})) + \\exp (f(x_{t,2}))} = \\mu (f(x_{t,1}) - f(x_{t,2})) .$", "latex": ["C \\subset \\mathbb{R}^{d_c}", "A \\subset \\mathbb{R}^{d_a}", "d_c \\geq 1", "d_a \\geq 1", "c_t \\in C", "a_{t,1}", "a_{t,2}", "f : C \\times A \\rightarrow \\mathbb{R}", "X_t \\subset \\mathbb{R}^d", "\\mathbb{P}\\{x_{t,1} > x_{t,2}\\} = \\mathbb{P} \\{Y_t = 1|x_{t,1}, x_{t,2}\\} = \\frac{\\exp (f(x_{t,1}))}{\\exp (f(x_{t,1})) + \\exp (f(x_{t,2}))} = \\mu (f(x_{t,1}) - f(x_{t,2}))"]}, {"title": "3 Neural Dueling Bandits", "content": "Having a good reward function estimator is the key for any contextual bandit algorithm to achieve\ngood performance, i.e., smaller regret. As the underlying reward function is complex and non-linear,\nwe use fully connected neural networks [7, 8] to estimate the reward function only using the preference\nfeedback. Using this estimated reward function, we propose two algorithms based on the upper\nconfidence bound and Thomson sampling with sub-linear regret guarantees.\nReward function estimation using neural network. To estimate the unknown reward function $f$,\nwe use a fully connected neural network (NN) with depth $L \\geq 2$, the width of hidden layer $m$, and\nReLU activations as done in [7] and [8]. Let $h(x; \\theta)$ represent the output of a full-connected neural\nnetwork with parameters $\\theta$ for context-arm feature vector $x$, which is defined as follows:\n$h(x; \\theta) = W_L ReLU (W_{L-1}ReLU (\\cdots ReLU (W_1x)))$,", "latex": ["f", "L \\geq 2", "h(x; \\theta)", "\\theta", "x", "h(x; \\theta) = W_L ReLU (W_{L-1}ReLU (\\cdots ReLU (W_1x)))"]}, {"title": "3.1 Neural dueling bandit algorithms", "content": "With the trained NN as an estimate for the unknown reward function, the learner has to decide\nwhich two arms (or duel) must be selected for the subsequent contexts. We use UCB- and TS-based\nalgorithms that handle the exploration-exploitation trade-off efficiently.\nUCB-based algorithm. Using upper confidence bound for dealing with the exploration-exploitation\ntrade-off is common in many sequential decision-making problems [2, 7, 17]. We propose a\nUCB-based algorithm named NDB-UCB, which works as follows: At the beginning of the round $t$,\nthe algorithm trains the NN using available observations. After receiving the context, it selects the\nfirst arm greedily (i.e., by maximizing the output of the trained NN with parameter $\\theta_t$) as follows:\n$x_{t,1} = \\arg \\max_{x \\in X_t} h(x; \\theta_t).$", "latex": ["\\theta_t", "x_{t,1} = \\arg \\max_{x \\in X_t} h(x; \\theta_t)."]}, {"title": "3.2 Regret analysis", "content": "Let $K$ denote the finite number of available arms in each round, $H$ denote the NTK matrix for all\n$T \\times K$ context-arm feature vectors in the $T$ rounds, and $h = (f(x_1),\\dots, f(x_{TK}))$. The NTK matrix\n$H$ definition is adapted to our setting from Definition 4.1 of [7]. We now introduce the assumptions\nneeded for our regret analysis, all of which are standard assumptions in neural bandits [7, 8].\nAssumption 2. Without loss of generality, we assume that", "latex": ["K", "H", "T \\times K", "T", "h = (f(x_1),\\dots, f(x_{TK}))"]}, {"title": "4 Theoretical Insights for Reinforcement Learning with Human Feedback", "content": "Our algorithms and theoretical results can also provide insights on the celebrated reinforcement\nlearning with human feedback (RLHF) algorithm [39], which has been the most widely used method\nfor the alignment of large language models (LLMs). In RLHF, we are given a dataset of user\npreferences, in which every data point consists of a prompt and a pair of responses generated by the"}, {"title": "5 Neural Contextual Bandits with Binary Feedback", "content": "We now extend our results to the neural contextual bandit problem in which a learner only observes\nbinary feedback for the selected arms (note that the learner only selects one arm in every iteration).\nObserving binary feedback is very common in many real-life applications, e.g., click or not in online\nrecommendation and treatment working or not in clinical trials [19, 45, 46].\nContextual bandits with binary feedback. We consider a contextual bandit problem with binary\nfeedback. In this setting, we assume that the action set is denoted by $A$. Let $X \\subset \\mathbb{R}^d$ denote\nthe set of all context-arm feature vectors in the round $t$ and $x_{t,a}$ represent the context-arm feature\nvector for context $c_t$ and an arm $a \\in A$. At the beginning of round $t$, the environment generates\ncontext-arm feature vectors $\\{x_{t,a}\\}_{a\\in A}$ and the learner selects an arm $a_t$, whose corresponding\ncontext-arm feature vector is given by $x_{t,a}$. After selecting the arm, the learner observes a stochastic\nbinary feedback $y_t \\in \\{0,1\\}$ for the selected arm. We assume the binary feedback follows a\nBernoulli distribution, where the probability of $y_t = 1$ for context-arm feature vector $x_{t,a}$ is given by\n$\\mathbb{P} \\{y_t = 1|x_{t,a}\\} = \\mu (f(x_{t,a}))$, where $\\mu : \\mathbb{R} \\rightarrow [0, 1]$ is continuously differentiable and Lipschitz\nwith constant $L_{\\mu}$, e.g., logistic function, i.e., $\\mu(x) = 1/(1+e^{-x})$. The link function must also satisfy\n$\\kappa_{\\mu} = \\inf_{x\\in X} \\mu(f(x)) > 0$ for all arms.", "latex": ["A", "X \\subset \\mathbb{R}^d", "t", "x_{t,a}", "c_t", "a \\in A", "\\{x_{t,a}\\}_{a\\in A}", "a_t", "x_{t,a}", "y_t \\in \\{0,1\\}", "\\mathbb{P} \\{y_t = 1|x_{t,a}\\} = \\mu (f(x_{t,a}))", "\\mu : \\mathbb{R} \\rightarrow [0, 1]", "L_{\\mu}", "\\mu(x) = 1/(1+e^{-x})", "\\kappa_{\\mu} = \\inf_{x\\in X} \\mu(f(x)) > 0"]}, {"title": "5.1 Reward function estimation using neural network and our algorithms", "content": "To estimate the unknown reward function $f$, we use a fully connected neural network (NN) with\nparameters $\\theta$ as used in the Section 3. The context-arm feature vector selected by the learner in round\n$s$ is denoted by $x_{s,a} \\in X_s$, and the observed stochastic binary feedback is denoted by $y_s$. At the\nbeginning of round $t$, we use the current history of observations $\\{(x_{s,a}, y_s)\\}_{s=1}^{t-1}$ and use it to train\nthe neural network (NN) by minimizing the following loss function (using gradient descent):\n$\\mathcal{L}_t(\\theta) = - \\frac{1}{m} \\sum_{s=1}^{t-1} [y_s \\log \\mu (h(x_{s,a}; \\theta)) + (1 - y_s) \\log (1 - \\mu (h(x_{s,a}; \\theta)))] + \\frac{\\lambda}{2} ||\\theta - \\theta_0||^2 ,$", "latex": ["f", "\\theta", "s", "x_{s,a} \\in X_s", "y_s", "t", "\\{(x_{s,a}, y_s)\\}_{s=1}^{t-1}", "\\mathcal{L}_t(\\theta) = - \\frac{1}{m} \\sum_{s=1}^{t-1} [y_s \\log \\mu (h(x_{s,a}; \\theta)) + (1 - y_s) \\log (1 - \\mu (h(x_{s,a}; \\theta)))] + \\frac{\\lambda}{2} ||\\theta - \\theta_0||^2"]}, {"title": "6 Experiments", "content": "To corroborate our theoretical results, we empirically demonstrate the performance of our algorithms\non different synthetic reward functions. We adopt the following two synthetic functions from earlier\nwork on neural bandits [7, 8, 15]: $f(x) = 10(x_0)^2$ (Square) and $f(x) = \\cos(3x_0)$ (Cosine). We\nrepeat all our experiments 20 times and show the average and weak cumulative regret with a 95%"}, {"title": "7 Conclusion", "content": "Due to their prevalence in many real-life applications, from online recommendations to ranking\nweb search results, we consider contextual dueling bandit problems that can have a complex and\nnon-linear reward function. We used a neural network to estimate this reward function using\npreference feedback observed for the previously selected arms. We proposed upper confidence bound-\nand Thompson sampling-based algorithms with sub-linear regret guarantees for contextual dueling\nb bandits. Experimental results using synthetic functions corroborate our theoretical results. Our\nalgorithms and theoretical results can also provide insights into the celebrated reinforcement learning\nwith human feedback (RLHF) algorithm, such as a theoretical guarantee on the quality of the learned\nreward model. We also extend our results to contextual bandit problems with binary feedback, which\nis in itself a non-trivial contribution. A limitation of our work is that we currently do not account\nfor problems where multiple arms are selected simultaneously (multi-way preference), which is an\ninteresting future direction. Another future topic is to apply our algorithms to important real-world\nproblems involving preference or binary feedback, e.g., LLM alignment using human feedback."}]}