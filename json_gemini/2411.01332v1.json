{"title": "A Mechanistic Explanatory Strategy for \u03a7\u0391\u0399", "authors": ["Marcin Rabiza"], "abstract": "Despite significant advancements in XAI, scholars note a persistent lack of solid conceptual foundations and integration with broader scientific discourse on explanation. In response, emerging XAI research draws on explanatory strategies from various sciences and philosophy of science literature to fill these gaps. This paper outlines a mechanistic strategy for explaining the functional organization of deep learning systems, situating recent advancements in AI explainability within a broader philosophical context. According to the mechanistic approach, the explanation of opaque Al systems involves identifying mechanisms that drive decision-making. For deep neural networks, this means discerning functionally relevant components such as neurons, layers, circuits, or activation patterns\u2014and understanding their roles through decomposition, localization, and recomposition. Proof-of-principle case studies from image recognition and language modeling align these theoretical approaches with the latest research from Al labs like OpenAI and Anthropic. This research suggests that a systematic approach to studying model organization can reveal elements that simpler (or \"more modest\") explainability techniques might miss, fostering more thoroughly explainable AI. The paper concludes with a discussion on the epistemic relevance of the mechanistic approach positioned in the context of selected philosophical debates on XAI.", "sections": [{"title": "Introduction", "content": "In recent years, deep learning has emerged as the dominant approach in artificial intelligence (AI). Its algorithms are characterized by \"black box\" functions that are too complex to fully understand, making it difficult to determine how specific computations based on certain inputs lead to particular predictions. To restore trust in automated decision-making, researchers focus on the explainable AI (XAI) research program, which aims to achieve transparency, interpretability, and explainability to validate the decision-making processes of opaque Al systems, making model behavior understandable to stakeholders with various epistemic needs."}, {"title": "Neomechanistic Theory of Explanation", "content": "In scientific discourse, one prominent approach is characterized by the neomechanistic theory of explanation, which emphasizes the logic of \u201cexplaining why by explaining how\u201d (Bechtel & Abrahamsen, 2005, p. 422). According to the new mechanists, explaining why something happens often involves identifying the underlying mechanisms that give rise to observed behavior. Mechanisms are identified by the phenomena they produce (Illari & Williamson, 2012), their functional roles (Machamer, Darden, & Craver, 2000; hereinafter MDC), and by their operating \u201cparts\u201d and \u201cinteractions.\u201d (Bechtel & Abrahamsen, 2005). For example, Glennan (1996, p. 52) defines a mechanism as \u201ca complex system which produces that behavior by the interaction of a number of parts according to direct causal laws.\u201d According to MDC (2000, p. 3), mechanisms are identified and individuated by the \u201cactivities\u201d and \u201centities\u201d that constitute them, as well as their functional roles, and setup and termination conditions.\nEntities are components or parts defined by their properties\u2014such as location, structure, and orientation\u2014 that engage in activities based on specific characteristics. Activities are temporal producers of change, characterized by aspects such as spatiotemporal location, rate, duration, types of entities involved, and other operational properties. The roles that entities play through their activities are considered their functions within the mechanism. The specific organization of these elements determines how collectively they produce the observed phenomenon (MDC, 2000).\nMechanistic explanation begins by characterizing the phenomenon under study and then identifying the mechanisms responsible for it. According to Bechtel and Abrahamsen (2005), to explain a mechanism, scientists must pinpoint its components, understand the functions these parts perform, and determine their organization to produce the phenomenon. This process relies on scientific discovery methods, incorporating heuristic strategies such as decomposition and localization (Bechtel & Richardson, 2010).\nDecomposition involves breaking the mechanism into its structural or functional components. Structural decomposition focuses on the physical aspects of parts like size or shape, while functional decomposition looks at the parts' roles, causal powers, and overall contributions to the mechanism (Piccinini & Craver, 2011). Functional decomposition assumes that the system's behavior results from its sub-functions. Structural decomposition further breaks down these functions into their physical components. This process starts with hypothetical component parts, refining the breakdown as more is learned about the system's"}, {"title": "Mechanistic Interpretation of Deep Learning", "content": "The mechanistic explanatory strategy has been adopted across various scientific fields, being especially prevalent in neuroscience (Kosti\u0107 & Halffman, 2023). Given the biological inspiration of deep neural networks (DNNs), along with their computational parallels and predictive capabilities akin to the brain's categorization processes (Schyns et al., 2022), it may be promising to apply mechanistic principles to Al systems using opaque deep learning algorithms. Indeed, K\u00e4stner and Crook (2024) argue that as AI systems grow in complexity, they should be analyzed similarly to biological organisms, emphasizing their functional organization. To this end, applying the mechanistic approach to DNNs could help us discover how these models work internally, illuminating not only how specific computations-given specific inputs-produce unique predictive patterns, but perhaps also explaining the models themselves in a holistic way.\nIn the neomechanistic framework, explaining AI systems entails identifying the mechanisms behind their decision-making processes using discovery heuristics of decomposition, localization, and recomposition. The goal is to understand the properties driving"}, {"title": "Mechanistic Structure of Deep Neural Networks", "content": "defined over vectors in programming languages such as Python, computer scientists interested in mechanistic interpretability\u2014a particular approach to XAI focused on deciphering the internal workings of machine learning models-recognize that \u201cneural network parameters are in some sense a binary computer program which runs on one of the exotic virtual machines we call a neural network architecture\u201d (Olah, 2022). DNN components can be identified within such environments. During training, these entities engage in activities such as activation, back- propagation, and error minimization, triggered by properties like incoming signals exceeding certain thresholds. This interaction fosters the development of specialized roles within the system, often unforeseen by programmers, contributing to the emergence of observed behavior and supporting the assumptions of mechanistic interpretability.\nAn example of this \u201cmechanistic compatibility\u201d can be seen in deep convolutional neural networks (CNNs), primarily used in image recognition and computer vision, which somewhat mimic the organization of the animal visual cortex. During training, a CNN processes a two- dimensional labeled image to generate weights that encode extracted data patterns. CNNs employ organized entities\u2014such as neurons, larger neuronal circuits, convolutional kernels (filters), or various kinds of layers\u2014along with activities like convolutions, ReLU and softmax activations, and pooling, orchestrating complex mechanisms of feature extraction and image classification"}, {"title": "Implementation of Mechanism Discovery Heuristics", "content": "Having explored the mechanistic structure of DNNs, it's important to consider how we can systematically apply discovery heuristics of decomposition and localization to dissect and understand the roles of EREs within these networks. In XAI practice, these strategies can be implemented through established analytical techniques tailored to specific applications. In computer vision, for example, input heatmapping and feature visualization techniques can be used to generate saliency maps that highlight specific pixels or regions in an input image that are highly predictive of the output. Such visualizations can also clarify the operations performed by DNN components across layers, thus aiding the functional decomposition of the network. Concrete examples include activation maximization, regularized optimization, network inversion, deconvolutional neural networks, network dissection-based visualization, or layer- wise relevance propagation (Yosinski et al., 2015; Qin et al., 2018; Montavon et al., 2018). These techniques allow researchers to observe how each network level transforms input into increasingly abstract and meaningful representations, which is crucial for dissecting complex mechanisms like face recognition into simpler components that recognize individual features such as ears, eyes, or noses."}, {"title": "Are Deep Neural Networks Genuine Mechanisms?", "content": "There may be several objections from orthodox mechanists regarding the characterization of DNNs as mechanisms, primarily questioning whether DNNs meet the criteria for genuine mechanisms as defined in neomechanistic literature. Despite possible skepticism, I argue that the discovery strategies proposed by neomechanistic philosophy-decomposition, localization, and recomposition\u2014can help researchers at least partially open the deep learning black box.\nA key standard in the general account of mechanistic explanation is the demand for completeness as causal models, necessitating that all causally relevant parts and operations be explicitly detailed without gaps or placeholders (Craver, 2007). A fully adequate mechanistic explanation must provide structural details at all levels of the mechanism, including components and activities that contribute to concrete computations. This requirement conflicts with the abstractness and medium-independence typical for computational explanations (Haimovici, 2013; Coelho Mollo, 2018). Under such scrutiny, AI models like DNNs might not qualify as genuine mechanisms because they typically represent abstract, formal specifications of computation that lack detailed structural information. DNNs are usually simulated through matrix operations rather than being implemented on physical nodes (except in rare cases involving one-to-one mapping, such as on neuromorphic hardware, cf. e.g., Schuman et al., 2022). To be fully mechanistically adequate, they would require additional specifications concerning physically instantiated computers: instantiation blueprints (Mi\u0142kowski, 2014).\nOn the other hand, there are some compelling reasons to treat DNNs as if they were mechanisms. While this paper cannot fully explore the debate over the ontic status of AI models due to space constraints, I will briefly outline two primary arguments defending the idea that DNNs can be meaningfully interpreted through a mechanistic approach.\nFirst, DNNs maintain their mechanistic status through weak structural constraints, typical of functional explanations that require structural properties to realize a functional characterization (Piccinini, 2015). Piccinini and Craver (2011, p. 302) state, \u201cThe functional properties of black boxes are specified in terms of their inputs and outputs (plus the algorithm, perhaps), independently of their physical implementation.\u201d DNNs exemplify this, as their inputs, outputs, and mapping algorithms can be defined without specific physical ties. However, the functional properties of DNNs impose certain constraints on the structural components regarding the degrees of freedom necessary for implementing an algorithm. This limits the functional analysis of an Al system to explaining how the algorithm, data, and network architecture determine the required degrees of freedom. If the system's functional components"}, {"title": "Epistemic Relevance of the Mechanistic Approach", "content": "While the relevance of the mechanistic approach in the XAI landscape has been addressed in existing technical literature and philosophical works, such as those by K\u00e4stner and Crook (2024), this paper advances the discourse by specifically focusing on the epistemic advantages and limitations of the mechanistic explanatory strategy and situating them within selected philosophical debates on XAI."}, {"title": "Epistemic Advantages", "content": "First, mechanistic decompositions of Al systems align with the interpretability criteria articulated by Doran et al. (2017). According to their framework, an interpretable system allows users not only to see, but also to study and understand how inputs are mathematically mapped to outputs. This is said to \u201chelp probe the mechanisms of ML systems.\u201d The authors cite regression, support vector machines, decision trees, ANOVAs, and data clustering models as"}, {"title": "Epistemic Limitations", "content": "While a theoretically grounded mechanistic exploratory strategy appears promising for the XAI program in terms of its epistemic advantages, its overall utility is also heavily constrained by certain epistemic limitations.\nFirst, adopting Doran et al.'s (2017) XAI typology, mechanistic explanations enhance interpretability by revealing AI systems' inner workings, but they do not necessarily improve the comprehensibility of such system. Comprehensibility involves users making sense of outputs through interpretable symbols like words or visualizations, regardless of the system's internal opacity. \u201cAuto-interpretability\u201d techniques, such as those by Bricken et al. (2023b) and Schwettmann et al. (2023), generate natural language descriptions of model components to aid comprehension. Yet, understanding these symbols typically depends on the user's implicit knowledge. While visualization techniques might display recognizable features in image recognition systems, XAI methods often identify subtle, complex features that may elude human understanding (Buckner, 2019; Zednik, 2019). Thus, although explanations should\nideally present mechanisms in human-understandable terms, the statistical nature of deep learning frequently diverges from intuitive concepts.\nMoreover, comprehending these explanations requires a certain level of technical proficiency in AI methods, which varies across types of explanations. Higher-level explanations that abstract away from intricate details generally require less expertise compared to detailed, lower-level explanations. This need for varying levels of expertise was evident in Ribeiro et al.'s (2016) evaluation of the LIME method, which specifically involved trained computer science graduates. This presents a significant challenge, as stakeholders beyond system developers will continue to demand explanations for system behavior, even when they lack the necessary technical background.\nSecond, Creel's (2020) distinction between types of transparency indicates that while mechanistic treatment may support structural transparency, it may fall short in achieving algorithmic and run transparency. Algorithmic transparency refers to knowledge of the algorithmic functioning of the whole, revealing high-level logical rules governing system transformations, which is not secured by mechanistic function-by-function decomposition. Run transparency, on the other hand, requires knowledge of specific program operations, including hardware specifics and input data. It involves observing how programs execute on particular hardware with real data. Since the mechanistic explanatory strategy presented here focuses on abstract models defined by mathematical constructs and specified degrees of freedom, it may not capture the artifacts of real-time interactions between software and hardware, unexpected data inputs, or the effects of software being converted into machine code.\nFinally, there is the issue of complexity. While classical computer systems are relatively transparent, deep learning systems are considered black boxes due to the complex interdependencies among millions of parameters composing their internal states. This complexity enables neural networks to excel in problem solving but complicates the dissection of their causal-mechanical structure. Kosti\u0107 (2023) points out that the opacity resulting from a model's functional complexity makes achieving a mechanistic explanation\u2014requiring detailed knowledge of its components, activities, and organization\u2014practically unattainable from the start. Even if not epistemically impossible, realistically addressing this challenge is practically daunting, given the limitations of current engineering methods, resources, and the increasing demand for explanations in rapidly evolving AI technologies. Perhaps the best we can hope for with the mechanistic approach is the examination of small, localized mechanisms, akin to that which occurs in neuroscience.\nComplex DNNs often resemble non-decomposable systems, in which each component's behavior is heavily influenced by its interactions with many others (cf. Rathkopf, 2018). While decomposition helps in managing the complexity of representing every element, thereby mitigating combinatorial explosion, it often results in representations that are limited in scope and applicable mainly to specific subsystems or simplified toy models. Creel (2020) notes that while some input-output paths of a model might be straightforward, fully understanding all sub-components can be excessively complex. To address this challenge, there is growing interest in scaling microscopic insights from mechanistic interpretability research to broader understanding of larger models. However, skepticism about such scalability persists due to computational challenges, high costs, and unresolved methodological questions (e.g., Nanda, 2023; Casper, 2023; Greenblatt et al., 2023). Evidence from small-scale models does not guarantee that real-world DNNs can be effectively decomposed for a thorough mechanistic understanding. When part-whole decomposition isn't feasible, alternative approaches that are fueled by a system's complexity, such as network science and topological explanations (e.g., Rathkopf, 2018; Kosti\u0107, 2022), should be considered."}, {"title": "Conclusions", "content": "The mechanistic explanatory strategy for XAI focuses on identifying the mechanisms that drive automated decision-making. In the case of deep neural networks, this requires discerning functionally relevant components\u2014such as neurons, layers, circuits, or activation patterns\u2014 and understanding their exact roles through heuristic discovery strategies of decomposition, localization, and recomposition. Research suggests that such a coordinated, systematic approach to studying the functional organization of models can expose previously unrecognized elements that simple explainability techniques might miss, ultimately fostering more explainable AI. In this spirit, supported by real-world examples from image recognition and language modeling, this philosophical analysis underscores the value of mechanistic reasoning in \u03a7\u0391\u0399.\nThe mechanistic approach offers significant epistemic benefits, enhancing AI interpretability, structural transparency, and enabling crafting counterfactual and highly\ngeneralizable explanations. This approach aids in prediction and system manipulation, as understanding internal dynamics allows for effective interventions and forecasting future states in new contexts. Additionally, it leverages multilevel hierarchical explanations, making complex Al systems more accessible and manageable for diverse stakeholders. Deepening our understanding of AI mechanisms and their causal relationships can improve performance evaluation and identify areas for improvement. Consequently, the advancement of a mechanistic framework in XAI may be crucial for overcoming trust and transparency challenges in high-stakes algorithmic decision-making.\nHowever, despite its theoretical promise, the mechanistic strategy's utility faces significant epistemic limitations. These challenges include ambiguous effects on algorithmic and run transparency, the limited comprehensibility of explanations due to the lack of suitable concepts, the necessity for recipients to have technical proficiency in AI, and difficulties in decomposing complex, real-world systems, which are not evident in simpler toy model examples.\nSeveral areas of future research stem from these considerations. Primarily, it is important to assess the applicability and scalability of the mechanistic approach beyond deep learning toy models to more complex Al systems. Further, investigating the way that individual stakeholders comprehend mechanistic AI explanations through user studies complimented by suitable epistemological theory of understanding would be a reasonable next step. Given the identified limitations of the mechanistic approach, it is also vital to explore other philosophically informed explanatory strategies. These should be ones that thrive on\u2014rather than are hindered by\u2014 system complexity, and that also address other contexts of opacity."}]}