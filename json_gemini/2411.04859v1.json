{"title": "A multi-purpose automatic editing system based on lecture semantics for remote education", "authors": ["Panwen Hu", "Rui Huang"], "abstract": "Remote teaching has become popular recently due to its convenience and safety, especially under extreme circumstances like a pandemic. However, online students usually have a poor experience since the information acquired from the views provided by the broadcast platforms is limited. One potential solution is to show more camera views simultaneously, but it is technically challenging and distracting for the viewers. Therefore, an automatic multi-camera directing/editing system, which aims at selecting the most concerned view at each time instance to guide the attention of online students, is in urgent demand. However, existing systems mostly make simple assumptions and focus on tracking the position of the speaker instead of the real lecture semantics, and therefore have limited capacities to deliver optimal information flow. To this end, this paper proposes an automatic multi-purpose editing system based on the lecture semantics, which can both direct the multiple video streams for real-time broadcasting and edit the optimal video offline for review purposes. Our system directs the views by semantically analyzing the class events while following the professional directing rules, mimicking a human director to capture the regions of interest from the viewpoint of the onsite students. We conduct both qualitative and quantitative analyses to verify the effectiveness of the proposed system and its components.", "sections": [{"title": "I. INTRODUCTION", "content": "Mixed-mode or hybrid teaching with both onsite and online students has become a popular teaching practice during the pandemic situation, and it also provides a way to spread knowledge and promote education fairness around the world. Nowadays, students who cannot attend the onsite lectures for various reasons can still participate through video conferencing platforms online or watch the recordings offline. Nonetheless, the information and experiences received by these students are inferior to those received by the onsite students. One reason is that the information conveyed through the views provided by the platform is usually very limited, as shown in Fig.1. The students cannot acquire the entire events from different views freely, and staring at the same view for a long time may cause mental stress[1]. On the other hand, if the platforms provide the students with many different views, it is both technically challenging (issues with synchronization, bandwidth, etc.) and the multiple video sources are difficult to browse through, so the remote students still have to manually select the view of interest during the class.\nRecently, a few automatic lectures recording systems [2], [3], [4], [5] have been proposed. However, these systems focus\nPanwen Hu and Rui Huang are with The Chinese University of Hong Kong, Shenzhen, China (e-mail: panwenhu@link.cuhk.edu.cn;\nyongzhang@link.cuhk.edu.cn; ruihuang@cuhk.edu.cn)."}, {"title": "II. RELATED WORK", "content": "The terminology of video editing is fluctuating in different areas, and in this paper, video editing refers to the process of shot selection from multiple videos along the timeline, instead of changing the contents of video frames (like image editing). In that sense, automatic video editing systems are sometimes called mashup [14] or montage [15] systems, which have attracted much attention from the multimedia and computer vision communities.\nThis section will briefly review the relevant editing systems. According to the timeline relationship between the raw videos and the resultant videos, we categorize exiting editing systems into two types following previous study [16], asynchronous and synchronous systems. The asynchronous systems often require scripts to specify the scene, and the timelines of their resultant videos do not correspond to the time of inputted videos. Video summarization is also a kind of asynchronous editing, but it focuses on extracting the representative parts from a single video instead of multiple video streams, so we exclude video summarization in this section. The synchronous systems, e.g., live broadcasting systems, take as inputs multiple synchronized video streams, and the resultant videos will cover the whole timelines of inputted videos. Our system is"}, {"title": "A. Asynchronous editing systems", "content": "The montage system by Bloch et al.[15] firstly implements automatic film editing. It takes the annotated video rushes as inputs and generates the film sequences for the specified scenario. The constraints on gaze, motion, and positions of the actors borrowed from film theory are applied during production. IDIC [17] follows Bloch's system with another attempt to generate film from annotated movie shots automatically. IDIC formulates montage as a planning problem and defines a set of operators based on film theory to select and plan video shots for the given story. Christianson et al. [18] introduce the Declarative Camera Control Language (DCCL) for generating idiom-based film sequences. Specifically, DCCL uses a particular set of film idioms for editing a particular scene. For example, it uses the conversation idioms for filming the conversation scene, the fighting idioms for filming the fighting scene, etc. Finally, a hierarchical film tree consisting of the idioms for each scene is built to select the shot for the given scene. Unlike previous work, Darshak [19] took the extra causal links and ordering constraints as input, besides the story plan and annotated videos. A hierarchical partial-order planner is responsible for selecting the shot sequences that satisfy the constraint and achieve the inputted story goals. Instead of selecting video shots based on the idioms and constraints, Some systems [20], [21] formulate the selections of shots as an optimization problem. It first segments an input script into a sequence of scenes. Aesthetic constraints such as location constraints, blocking constraints, etc., are proposed to compute the quality score of shots for each scene. Finally, the dynamic programming method determines the shot sequence that achieves the highest score. Although these systems are successful attempts at editing animated videos, their success heavily relies on the annotations of video content and camera parameters in the virtual world.\nRecently, editing real-world videos has also been studied. Leake et al. [22] propose a computational video editing framework for dialogue scenes. The video annotations required by the film-editing idioms, e.g., the face position of the actors, and the speaker visibility, are generated using advanced computer vision techniques. Finally, a Hidden Markov Model (HMM) and the Viterbi algorithm are employed to compose the film for the script. Moreover, Wang et al. [23] propose a method for generating a video montage illustrating a given narration. For each sentence in the text, their system retrieves the best-matched shot from the video based using the visual-semantic matching technique.\nOn the one hand, due to the domain gap, the methods to collect the visual elements for editing are not applicable to the lecture scene. Technically, these systems are not fully automated when analyzing video semantics but require manual annotations. On the other hand, these asynchronous systems always edit videos based on a given script, which specifies the content and the temporal relationships of shots. As a result, the edited videos do not always hold a complete timeline of"}, {"title": "B. Synchronous editing systems", "content": "Synchronized editing also has drawn much attention due to its wide applications. For example, previous studies [24], [25], [26], [27], [28] have proposed systems for live broadcasting soccer game. In this system, the motion of players, the location of the ball [25], [27], people detection, and saliency model [29] are used as intermediate representations for high-level event detection, which is used to evaluate the importance of each camera view. The system by Quiroga et al. [30] is developed for automatically broadcasting basketball games, where the locations of the ball and players, and the mapping relationship between the frame and the court are jointly used to recognize the game state. Besides broadcasting of sports events, the synchronized editing for concert recording [14], performance video [31], [32], [33], social video [34], and surveillance video [35], etc. have been explored as well. Compared to these types of videos, the lecture videos that our system explores lie in a significantly different content domain, and the editing rules used are not compatible. Therefore, directly converting these systems to accommodate the lecture scene is non-trivial, even though changing the content measurements.\nA few pieces of literature [10], [11], [6], [7], [8] have attempted to edit lecture videos. The tracked body positions [7], [36], [37], the gesture [8] or the head positions [4] are always considered the most important cues to switch or plan the cameras. Occasionally, additional features such as gaze direction [5], and the position relationship between the lecturer and the chalkboard [38] are incorporated. However, we observe that these position-aware representations are insufficient to determine the student's attention in class. Some important events, e.g., slide flips with a computer, have little relation to the positions of the speaker. To this end, our system comes up with a few practical video semantics analysis methods, as well as the computational expressions of empirical editing rules, to guide the editing.\nOn the other hand, most existing editing frameworks imitate the human director by applying the predefined selection rules [3], [39], or the script language [26]. For example, Machnicki et al.[40] describe that after showing the close-up speaker for 1 minute, the system should switch to the stage view and show that for 15 seconds. Some other frameworks [6], [7] represent these rules by building an FSM. However, these selection mechanisms have limited scalability to incorporate new semantic cues or rules, and the resultant videos will be mechanical and predictable. To alleviate this problem, Some computational frameworks [41], [34], [32], [21], [22] formulate the editing as an optimization problem and solve it with dynamic programming approaches. Nevertheless, the"}, {"title": "III. THE PROPOSED SYSTEM", "content": "As reviewed in the previous section, the common shortages in existing lecture broadcast systems mainly include the limitations in understanding the high-level semantics of videos and the weak extendability of the rule-based directing schemes. To tackle these problems, we first propose different semantics extraction methods to assess different shots, which will be discussed in Sec.III-B. In Sec.III-D, we will introduce our computational editing framework built upon the semantic cues. The overall architecture of our system is illustrated in Fig.3, different shots are firstly fed into the independent shot semantics assessment module to generate the event indicators which are then converted to the semantic scores. Next, the semantics scores and the scores from the assessment of general cinematic rules are passed to the computational framework to produce resultant videos."}, {"title": "A. Problem formulation", "content": "Technically, live broadcasting or editing lecture videos can be regarded as a consecutive view selection process. The inputs to the system include a set of $C$ synchronized video streams $V = {Vc}_{c=1:C}$ and each $Vc$ is decoded as a frame sequence ${fc,t}_{t=1:T}$ or a clip sequence, it depends on the unit of a time instance. For simplicity, we will use a frame as the unit of time in the rest of this paper. After acquiring $l$ frames starting from time $t$, the system will analyze the content of ${fc,t:t+l}_{c=1:C}$ and then select the best views indexed by ${Ct, Ct+1,\u2026\u2026,Ct+l}$. As a result, the frame sequence ${fct,t, fct+1,t+1,..., fct+l,t+l}$ are concatenated to form the video stream. For clarification, we may also use the abbreviations of shot names to denote the camera indices or frame sources in this paper. i.e., subscript $lb$ stands for left blackboard close-up shot, $sc$ stands for slide close-up shot, and $sl$ denotes student long shot, etc. It is worth noting that if start time $t = 0$ and $l$ is the duration of the lecture, the"}, {"title": "B. Shot assessment from video semantics", "content": "The first problem to be addressed for video editing is deciding what to show at any given moment [8]. Generally, a view gaining more attention is assigned with a high score for selection. As shown in Fig.2, there are seven shots in our systems and the perspectives of these shots are diverse, serving different purposes [6]. Therefore, our computational editing system assesses the focus of different shots from different aspects. Specifically, we first identify whether a particular event defined for each shot happens at each time point by analyzing its content. The results stored in the indicator vectors are then fused and converted to the focus scores, considering the priorities of shots.\nBlackboard Close-Up Shot (BCUS). The BCUS contains left BCUS and right BCUS, which are set to capture the content written on the blackboard, and this shot will draw the student's attention when the writing event happens. Hence, the core to assessing this shot is to recognize the writing event. Previous editing systems [42], [8] detect the writing event by calculating the frame difference over time, while their methods are vulnerable to illumination variation and the movement of the presenter. Skeleton information has proved to be useful information for recognizing human action [43], [44], [45]. As shown in Fig.5, the skeleton topologies for writing events are discriminative from those for non-writing events. It is feasible to recognize these two cases by analyzing the speaker's skeletons. However, existing skeleton-based approaches mostly take as inputs the 3D joint positions that are not acquired from common RGB cameras. Avola et al. [46] propose a 2D skeleton-based approach that extracts the features of upper and lower body parts with two-branch neural network architecture. Whereas, the lower body part of the presenter is not always visible in the close-up shot.\nConsidering the capability of Graph Convolutional Network (GCN) [47], [48] in representing the topology of the human body, we propose a graph-based cross-attention network to recognize the writing event based on 2D skeletons. As shown"}, {"title": "Slide Close-Up Shot (SCUS)", "content": "Slide projector plays an important role in current classes, teachers use slides to assist their teaching activity. Therefore, previous editing systems [8] also take the SCUS into consideration and utilize the gesture and position information of teachers to access its focus from students. Whereas, in a scene such as the large classroom or report hall where the presenter cannot interact with the projected slide but use a laser pointer or the mouse to flip and draw on the slide, the key to assessing the focus is to detect the content changes in the slide. As the color histogram difference method [7] is susceptible to the video stream noise, and not sensitive to the small streaks drawn by the presenter on the slide, we propose a gradient difference-based anomaly detection method to address the above problems. Let $fsc,t-1$ and $fsc,t$ denote two adjacent frames of slide shot, and the gradient difference score $Sg,t$ is calculated as:\n$Sg,t = \\frac{1}{3} \\sum_{i=1}^{3} ||Grad(fsc,t-1[i]) - Grad(fsc,t[i])||2$\nwhere $Grad(fsc,t-1[i])$ denotes the function of calculating gradients for the i-th channel of $fsc,t-1$. To predict whether a salient change occurs on the slide at time t, instead of applying a threshold on score $Sg,t$, we employ an autoregressive model-based Anomaly Detector (AD) [50] which is more robust to the stream encoding noises.\nStudent Long Shot (SLS). SLS is also important for improving the interest and engagement of edited videos. Generally, in the manual directing scenario, the human director will show the student view when students ask questions in class. Existing systems [6], [7] use a sound source localization-based technique to locate the talking students, while it requires"}, {"title": "Medium Shot (MS)", "content": "Our system sets up both the Left Medium Shot (LMS) and Right Medium Shot (RMS) to increase diversity, and they serve the same purpose. The MS is set to capture the whole body of the speaker so that the remote students can keep up with the teacher by watching his gesticulation and the interaction with other students. Similar to other shots, we also define a metric to identify the unusual event. the MS typically"}, {"title": "Overview Long Shot (OLS)", "content": "As a complementary shot to the other shots, the OLS can capture the whole classroom and show the presenter's actions happening outside of other shots for the remote students, increasing the engagement and interest of students. As shown in Fig.9, the speaker usually moves around the podium, and it is considered an unusual case if the speaker exceeds the normal range. To this end, we access this shot by tracking the positions ${pol,t}_{t=1:T}$ of the presenter over time, and the elements of indicator vector $Iol$ are set to 1s if the positions at the corresponding time points are greater than a predefined threshold otherwise 0."}, {"title": "Conversion from indicators to scores", "content": "Without loss of generality, suppose the current time is $t$ and $l$ frames after $t$ are acquired, we can compute the indicator vectors $Irb, Ilb, Isc, Isl, Ilm, Irm, Iol$ through the semantics analysis"}, {"title": "C. Shot assessment from cinematographic rules", "content": "Besides video semantics, professional cinematographic rules also have a great impact on the viewing experience [53]. Unlike the previous systems that hard code the rules, we integrate them by converting the shot selection constraints"}, {"title": "View transition constraint", "content": "In professional film editing, there are many empirical constraints [54], [55], [13] on shot transitions in order to prevent confusing audiences. One fundamental guideline is to avoid Jump cuts. This guideline claims that the transition between two camera views that shoot the same scene from almost the same angles e.g., the angle difference is below 30 degrees, will be perceived as a sudden change, resulting in a jarring cut. Unlike the footage from the traditional filming scene or animation scene where the camera angle can change casually, the lecture videos are captured with fixed camera views. Hence, the Jump cuts constraint is satisfied in the camera setup stage.\nAnother core guideline is the 180-degree rule, which stresses that the cameras of two consecutive shots shooting the same object must situate on one side of an imaginary line-of-action. Otherwise, it will create an abrupt reversal of the action or characters. Similarly, a rule about the order of shot [9], [56] in shot transitions argues that the shot size should change smoothly, and a common order of shot is to start with a long shot, establishing an overview of the scene. So the shot after a long shot is typically a medium shot, which is then followed by a close-up shot. Although these rules are often pleasing, they are not necessarily always followed. There should be some variation in the sequence to prevent producing too mechanical montages. Therefore, we implement these rules in a soft manner. Specifically, as there are 7 types of shots in our system, we build a 7 \u00d7 7 matrix T, representing the transition suitability of all shot size combinations. The element at position (Cstart, Cend) is set as\n$T[Cstart, Cend] =\\begin{cases}\n-\u0454, & if Cend \u2208 Cviol(Cstart)\n\u20ac, & otherwise.\n\\end{cases}$               (1)\nwhere $Cviol (Cstart)$ denotes the set of camera to which the cuttings from Cstart violate the rules above. For example, if Cstart is the left close-up shot $Cicu$ and Cend is the right close-up shot $Crcu$ or student long shot $sol$, the transition will violate the 180-degree rule or the order of shot rule. Although the negative element of T is treated as a penalty when multiplied by the semantic score $rena,t$, it still leaves the possibility of making such a transition when there are enough incentives from other sources. It is favorable for producing diverse montages."}, {"title": "Switch penalty", "content": "Previous works suggested that frequent switches will cause an unpleasant viewing experience while lasting the same view for a long time will make the broadcast tedious. Hence, we dynamically assign a penalty $rsw(< 0)$ for each selection based on the duration L that the current view has lasted for.\n$rsw (L, switch) =\\begin{cases}\nCsw * (1+e^{(L-Lmax)} - 1), & if  \\sim switch\nCsw * (1+e^{(Lmin-L)} - 1), & if switch\n0, & otherwise.\n\\end{cases}$"}, {"title": "B-roll insertion", "content": "Some events in the lecture may last for a long time, e.g., discussion with students. Watching the same view all the time is boring and it may hurt the focus of students. An excellent practice [6] is to show the B-roll view occasionally for a period of time (e.g., 9 seconds). It will make the resultant video more interesting to watch. A B-roll can be a shot that shows the overview of the classroom $col$, the states of student $csl$, or the teaching materials $cscu$. So we set up an incentive $rbroll(> 0)$ for inserting B-roll views when current view c has last for a period of time at t.\n$pbroll (L, Cend) =\\begin{cases}\nCbroll, & if L > Lmean & Cend \u2208 {Csl, Col, Cscu}\n0, & otherwise.\n\\end{cases}$"}, {"title": "D. Computational editing framework", "content": "The view selection process is an essential part of the systems. Traditional rule-based frameworks have limited capacities in incorporating new information measurements and cannot balance the real-time performance and the optimality of solutions. Even some systems [32] have to post-process to prevent over-long and over-short clips. Hence, we propose a complete optimization-based framework that can achieve the optimal solution and enables users to switch the modes, e.g., live broadcasting, offline editing, or a balance between them, by simply adjusting the duration I looking ahead. For example, users can experience live broadcasting by setting 1 to 0 or obtain the optimal edited video by setting 1 to the length of the lecture videos or make a trade-off between them. Furthermore, new video semantic cues can be readily embedded by quantizing their importance to each view without re-defining a bundle of rules.\nWithout loss of generality, we suppose that the selection starts from time t with view ct which has been lasting for Lt time instances. The information in future l time instances is available as well. The goal of our system is to figure out an optimal view index sequence s* = {C++1,\u2026\u2026\u2026,C++l} \u2208 Ml by solving the following optimization problem, where Mt is the space of all possible view index sequences from t +1 to t+l:\n$\\underset{{Ct+1,...,Ct+1}}{arg max} \\sum_{i=t+1}^{t+l} T [Ci-1, Ci] * rei_{si} + b *pbroll (Li-1, Ci)\n+ dsw * rsw(Li\u22121, switch)$\n$Li, switch =\\begin{cases}\nLi\u22121 { + 1, & False & if Ci = Ci-1\n1, & True\notherwise.\n\\end{cases}$\nwhere \u03bb\u03b5, \u03bb\u03b9, Asw are the adjustable weights for three reward terms.\nDirectly applying the brute-force algorithm to search for the optimal solution will result in an exponential complexity Cl. Instead, we formulate the above optimization problem as a path-searching problem in a directed graph model. The result is solved under the complexity of l * C2. We treat each frame fc,t as a node vc,t in the directed graph, and each edge only exists between two nodes that are temporally adjacent and is directed to the node owning bigger time stamp, e.g., edge ec1c2,t+1 denotes the edge pointing from fc1,t+1 to fc2,t+2 as shown in Fig.6.\nIn this graph model, we employ the scheme of the breadth-first search to forward the reward gained by the nodes at timet to those at t + l, then backtrack from the node with the maximum rewards to obtain the optimal path. Each node Uc,t+i contains three components: the reward gained Rc,t+i, its precursor Pc,t+i, and the view length Lc,t+i. During the forward process, the camera index of Pc,t+i, the precursor of Uc,t+i, is found as:\n$Dk,c,t+i =\\underset{k}{arg max} Rk,t+i-1 + Dk,c,t+i, where\nk\u2208{1,...,C}\n$Dk,c,t+i = \\begin{cases}\ndeT[k, c] * ret+i + db * pbroll (Lk,t+i\u22121, C)\n+Asw * rsw (Lk,t+i\u22121, False), & if k = c\ndeT[k, c] * re,t+i + Ab * rbroll (Lk,t+i\u22121, C)\n+Asw * rsw (Lk,t+i\u22121, True), & otherwise.\n\\end{cases}$"}, {"title": "IV. EXPERIMENTS", "content": "Although video editing systems have been widely studied, evaluating such systems is still an open problem. The difficulty of assessing video editing systems can be traced to at least three reasons [57]: 1) There is never a single correct answer to editing problems. Even if the annotators are sophisticated film experts, the solutions they produce may be very different from each other. 2) The editing quality cannot be measured directly since the editing effects are often invisible. 3) The rules of good editing are not absolute. They can guide the editing but are not always strictly followed by the filming experts. Probably because of these reasons, we find no public datasets to measure the progress of this field.\nComparing the predicted solutions with the ground truth might not be feasible yet, but the researchers can still evaluate the editing system from some other aspects, such as optimality, extendability, ease of implementation, etc., as suggested in the study [57]. To this end, we collected a set of lecture video data with our recording system from 10 actual classes. There are seven camera views in total, as shown in Fig.2, and the average length of each view of each class is about 50 minutes, so the total length of the videos is about 3500 minutes. To train the proposed writing event recognition network, We manually annotate the time points when the writing event occurs and use one-quarter of the data for training while the rest is used for testing. In the following section, we propose a set of metrics used to quantitatively measure the properties of videos, thus different algorithms can be compared by inspecting the properties of the generated videos in Sec.IV-A. Besides, we conduct a user study to collect and analyze the real user experience in Sec.IV-B."}, {"title": "A. Comparisons", "content": "Firstly, we compare the outcomes of our system Optim(l), where l is the duration look-ahead, with those from the other four methods under our experimental environment:\n1) Randseg(n)[58], which randomly selects the segment with length n;\n2) Ranking [31], which greedily selects the view with the highest event rewards when the current shot length reaches the sampled length from a normal distribution;\n3) FSM [6], [7], where the states and the transitions are defined based on our environment;\n4) Cons-Optim[34], which is a constrained optimization-based method.\nWe set the expected maximum and minimum shot length, Lmax and Lmin, to 60 and 20, respectively, the mean length and variance for Ranking to $(Lmax + Lmin)/2$ and 10 seconds, and the rewards weights {Asw, \u03bb\u03b5, \u03bb\u03b9} are set to {0.4, 0.3, 0.3}. Actually, the proposed framework allows the users to set up the parameters according to their preferences to generate productions with varied styles. The impacts of these parameters will be studied in Sec.IV-D."}, {"title": "B. User study", "content": "the ground-truth videos and the evaluation metrics are still understudied. Therefore, We also assess the system from the aspect of real user experience and conduct a user study to evaluate the qualities of the generated videos. Specifically, we recruited 20 volunteers, including 12 undergraduates and 8 postgraduates, and randomly show them the videos generated by three algorithms:1) Zoom, which greedily selects the BCUS or the based on the writing event or slide flip, simulating the scenario of the popular online teaching software, Zoom, where only two views are available; 2) Ours, the proposed system; 3) FSM, we follow the works [6], [7] to implement an FSM based editing system under our experimental scene. After watching the videos, the volunteers are asked to score the videos from 1 to 5 with respect to six questions:\n1) Do you feel the experience of taking class onsite when watching this video?\n2) Is this video interesting and having a pleasing viewing experience?\n3) Do you think the shots are selected appropriately according to the semantics of different shots?\n4) Is this video effective and helpful to study the course if you are taking the course for the first time?\n5) Is this video effective and helpful to review the course?\n6) what overall score you will assign to this video?\nThe average scores for these questions are summarized in. From the scores of the first two questions, the multi-shot algorithms, FSM and Ours achieve higher scores than the two-shot algorithms Zoom, which prove that occasionally displaying some other perspectives of the classroom besides two conventional shots (BCUS and SCUS) can increase the interest of the video and improve their educational experience in taking the online course. According to the scores for the third question, our system can respond to various class semantics and select the shots more appropriately. Moreover, the scores of the proposed method are higher than those of FSM on all six aspects, and than the scores of Zoom on five questions except for the fifth question, which justifies that the videos generated with the proposed editing framework are more attractive and appreciated by the students. However, when it comes to the review purpose, Zoom obtains a higher average score, since its resultant videos are composed of only two shots, allowing the students to locate the content quickly. This result also suggests the importance of the capability in generating diverse videos according to user's preferences and the flexibility in running on different modes."}, {"title": "C. Visualization results", "content": "To intuitively observe the performance of the proposed system, we visualize 9 consecutive segments of a resultant video in Fig.11. The temporal relationships and the segment lengths are visualized in the central figure where the vertical and the horizontal axes represent the camera indices and the timeline, respectively. The selection process is inspected as follows:\n1) Segment 1: The teacher closes the door on two sides, his position is out of the normal region, so the system"}, {"title": "D. Ablation study", "content": "1) The impacts of the score weights: In addition to optimality, one advantage of our system is its flexibility, enabling the users to incorporate various information measurements without laboriousness to define the selection rules. Besides, users can adjust the weights of various measurements based on their preferences to generate various productions. In this section, we will discuss the impact of each score term by adjusting its weights. All the experiments are carried out with Optim(\u221e)+GT, and the results have been summarized in Table.IV.\nWe firstly validate the effectiveness of transition constraint by comparing trans of the experiments with (+) or without (-) this term. It is easy to observe that the results with this constraint are usually better than those without it. For example, the experiment with {Asw, \u03bb\u03b5, \u03bb\u044c} = {0.4,0.3,0.3} achieve higher trans when applying the transition constraint. With some particular parameters, e.g. {Asw, de, Ab} = {0.5, 0, 0.5}, the experiments achieve the same trans on both sides, the reason is that the transition constraint works on the semantic score as a multiplier, so it will be ineffective if semantics"}, {"title": "2) Writing event recognition", "content": "As discussed in Sec.III-B, we propose a skeleton-based two-stream GCN architecture for discriminating the writing event from the non-writing event. In this section, we will compare it with the traditional SVM method and study the impacts of on the editing system. All the experiments are conducted with Optim(\u221e) where {\u03bb\u03b5\u03c9, \u03bb\u03b5, \u03bb\u044c} = {0.4, 0.3, 0.3}. Table.V shows the recognition performances of two methods, and the proposed method outperforms SVM in terms of accuracy, Recall, F1 score, and the Area Under the Curve (AUC). To validate their effectiveness, we apply the predicted results to our editing system, and the results are listed in Table.VI. By using our recognition method, the editing results achieve higher Ravg = 72.7, which surpass the Ravg = 58.3 of the results generated with SVM predictions. rmax also increases with the help of our method. These comparisons prove the superiority of our method over the traditional method, although there is still a large gap between the predicted results from our method and the ground truth. This experiment also suggests that the class semantics analysis is still under study, and more efforts are needed to promote the development of remote education."}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "Conclusion. To enhance the educational experience of mixed-mode teaching, we present a multi-purpose semantics-based editing system to live broadcast or offline edit lecture videos for remote students. Beyond the traditional systems using the low-level editing cues and the rule-based selection scheme, we exploit the skeleton of the teacher and formulate the filming rules or constraints into computational expressions, which are integrated into our optimization-based framework to achieve optimal solutions. Both quantitative and qualitative experiments have been conducted to validate the effectiveness of the proposed incentives and the optimality and flexibility of the whole system.\nDiscussion. Although our system has made obvious progress in this area from the experimental results and the user study, it still can be improved in a few aspects. We all know that different students may have different watching preferences, which means that the hyper-parameters involved and even the focus measurements are different from person to person. Therefore, the viewing experience can be further improved if the editing system can learn the customized parameters and measurements for each student from his/her own watching behavior, i.e., the customized shot sequence composed on his/her own. Hence, a potential direction is to study learning-based editing techniques, and thus the editing agent can imitate the customized watching behavior and generate the customized videos, after watching a few or even one example, i.e., one-shot imitation learning-based video editing."}]}