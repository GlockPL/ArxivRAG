{"title": "Representation Learning of Point Cloud Upsampling in\nGlobal and Local Inputs", "authors": ["Tongxu Zhang", "Bei Wang"], "abstract": "In recent years, point cloud upsampling has been widely applied in fields\nsuch as 3D reconstruction. Our study investigates the factors influencing\npoint cloud upsampling on both global and local levels through representa-\ntion learning. Specifically, the paper inputs global and local information of\nthe same point cloud model object into two encoders to extract these features,\nfuses them, and then feeds the combined features into an upsampling decoder.\nThe goal is to address issues of sparsity and noise in point clouds by lever-\naging prior knowledge from both global and local inputs. And the proposed\nframework can be applied to any state-of-the-art point cloud upsampling neu-\nral network. Experiments were conducted on a series of autoencoder-based\nmodels utilizing deep learning, yielding interpretability for both global and\nlocal inputs, and it has been proven in the results that our proposed frame-\nwork can further improve the upsampling effect in previous SOTA works. At\nthe same time, the Saliency Map reflects the differences between global and\nlocal feature inputs, as well as the effectiveness of training with both inputs\nin parallel.", "sections": [{"title": "1. Introduction", "content": "LiDAR and depth cameras have seen widespread use due to advancements\nin 3D sensors and 3D scanners, mostly relying on point clouds to collect data.\nHowever, issues such as sparsity, uneven distribution, noise, and redundancy\nare common in current point cloud data collection. Due to unavoidable\nflaws in point cloud data collection, such as occlusion, generating denser and\nhigher-quality 3D point clouds through point cloud upsampling methods,\nsimilar to tasks performed on complete point clouds, is essential. This allows\npoint clouds to fully demonstrate their significant value in 3D reconstruction\n[1], object detection and classification [2, 3, 4], and robotic operations [5].\nThe irregular nature of point clouds makes them easy to update [6], as\neach point in a point cloud is independent, making it convenient to add\nnew points and perform interpolation [7]. However, it is challenging to apply\nconvolution to point clouds when using learning-based methods. Nonetheless,\nthrough graph-based methods [8], works such as [9, 10, 11] have facilitated the\napplication of deep learning in point cloud tasks. Subsequently, PU-Net [12]\nproposed the first deep learning-based upsampling method for point clouds.\nWith the development of Graph Convolution Networks (GCN) [13], models\nlike MPU/3PU [14] have used clustering for interpolation, enabling better\nlocal and edge recognition in point clouds. Building on this, PU-GCN [15]\nfurther improved feature extraction, constructing DenseGCN for extracting\nlocal input features from point clouds, achieving better results. Additionally,\nPU-GAN [16] applies generative adversarial networks [17], using generators to\nupsample and produce high-fidelity, realistic point clouds. PU-Transformer\n[18] further advanced this field by utilizing positional information and self-\nattention mechanisms for enhanced global feature extraction.\nAlthough significant progress has been made in point cloud upsampling\nthrough deep learning [19, 20, 21], substantial challenges remain. As noted,\nunlike voxel-based grid representations of 3D models [22, 23], each point in\na point cloud is independent. Ergo, [24] extracts features from voxels to re-\nduce the problem of inaccurate feature extraction caused by the irregularity\nand sparsity of point clouds. But, for the point clouds, independence brings\nunique challenges in extracting objective features of objects, particularly in\nincomplete or sparse regions with limited information. This is akin to the\ntask of working with incomplete point clouds, where global features are ex-\ntracted from partial inputs but lose geometric details due to incompleteness\n[25]. Therefore, effectively aligning local details with global information is\ncrucial. While Zhang [26] has discussed the influence of local and global\ninputs on point cloud upsampling, it lacks interpretability and breadth in\nexperimental models. Current methods [27, 28, 20] utilize refiners on coarse\npoint clouds to merge local and global features, but their networks, which are\nsingle-encoder structures, face limitations in ensuring consistency and precise"}, {"title": "2. Related Works", "content": "Since PointNet [9, 31], early methods that projected 3D point clouds into\nmulti-view 2D images [32, 33] or converted point clouds into voxel or grid\ndiscretizations [23, 34, 35] have gradually shifted toward directly processing\npoint cloud data on a point-based basis. This direct processing approach\navoids contextual loss and complex steps involved in data conversion. The\nmainstream point cloud processing methods have evolved from MLP-based\nmodules starting with PointNet [9] to convolutional neural network-based ap-\nproaches like PointCNN [36, 37], and most recently, to Transformer [38] based\narchitectures such as Point-Transformer [39]. As observed, transformer-based\nstructures have already been widely explored in point cloud processing."}, {"title": "3. Methodology", "content": "In this paper, we propose a new Representation Learning of Point Cloud\n(ReLPU) framework and apply it to point cloud upsampling. Specifically,\nwe construct a parallel network for global and local feature extraction based\non the two-point cloud data input methods discussed in [26]. In this net-\nwork, edge attributes are used for detailed geometric patterns, while global\nattributes aid in a broader understanding of shapes and structures. Finally,\nwe combine geometric features with miscellaneous structural understanding\nto achieve comprehensive point cloud upsampling. The entire network can\nbe trained in an end-to-end manner. Details are provided in the following\nsections."}, {"title": "3.1. Netwotk Architecture", "content": "Based on this fundamental idea, we employ a sequential autoencoder for\npoint cloud upsampling to extract geometric and miscellaneous structural\nfeatures, resulting in the ReLPU- model (as shown in Figure 2a). We then\nupdate ReLPU- by using one specific autoencoder to extract geometric fea-\ntures and another identical autoencoder to extract other global features,\nresulting in the ReLPU model (as shown in Figure 2b). This setup enables\nReLPU to perform parallel global and local feature extraction, contrasting\nwith the sequential approach in ReLPU-. ReLPU significantly enhances the\nupsampling performance of ReLPU-.\nFor the autoencoder, the backbone network can be replaced based on\nexisting point cloud upsampling models, as long as the model adheres to\nthe encoder (feature extraction) to decoder (feature expansion) structure.\nIn this paper, we use MPU [14], PU-GCN [15], Dis-PU [27], and PUCRN\n[28] as backbone networks for point cloud feature extraction. This approach\naims to validate the effectiveness of our proposed parallel method and to\ndistinguish it from the sequential method. Here, we concatenate the global\nand local features extracted from the feature extraction modules, or encoders,\nof different networks.\nFor local input by patch, assuming the entire Model contains M points,\ndivide it into K patches, so we have $P_k$ as points of patch input. For global\ninput by Average Segment (AS), once sampling is completed, obtain a point\nset containing M points $P_{sampled}$. We hope to divide it into K subsets, each\ncontaining M/K points.\nThe k-th subset is denoted as $P_k$, where $k = 1,2,\u2026\u2026, K$. Assuming that\nM can be divided by K, the range of points for each subset is:\n$P_k = {P_i | (k-1) \\cdot \\frac{M}{K}+1\\leq i \\leq k \\cdot \\frac{M}{K}}$\nAfter passing through I layers of encoder E, we have:\n$F = E^{(l)} = \\sigma (AE^{(l-1)}W^{(l)})$"}, {"content": "where:\n\u2022 A is the adjacency matrix.\n\u2022 $E^{(0)} = X$, representing the feature matrix of the input point.\n\u2022 \u03c3 is the activation function.\nAdditionally, let:\n\u2022 $F_{global}$ represent the global feature vector extracted by the autoencoder."}, {"title": "3.2. Saliency Map for Point Cloud", "content": "Whether or not one considers the interpretability of deep learning as es-\nsential, using saliency maps to calculate gradient contribution values for data\nanalysis and visualization is a key aspect of explainable artificial intelligence\n(XAI). It provides an intuitive representation of the key factors underlying\nmodel results. Saliency maps have been widely applied in point cloud tasks,\nparticularly for classification and detection [44, 45, 46]. However, these appli-\ncations are limited to those tasks. Therefore, to understand the contributions\nof global and local features to point cloud upsampling and to compare these\ncontributions between sequential and parallel networks, we use saliency maps\nto reflect the gradients of point clouds during model training, indicating their\ncontribution to upsampling.\nAssuming that the loss function of the upsampling model is defined as\nL, given a point $x_i$ in the point cloud, its contribution can be defined as the\ndifference in loss before and after removing that point:\n$\\Delta L_{x_i} = L(X_{gt}) - L(X_{\\{x_i\\}pred })$"}, {"content": "where:\n\u2022 $X_{gt}$ represents the ground truth point cloud.\n\u2022 $X_{\\{x_i\\}pred}$ represents the upsampled point cloud after removing point $x_i$.\nIn other words, the contribution of a point $x_i$ in the point cloud equals\nthe difference between the loss $L(X_{gt})$ of the complete point cloud of ground\ntruth and the loss $L(X_{\\{x_i\\}pred})$ of the point cloud after upsampling.\nIf the contribution value $\\Delta L_{x_i}$ is small, we consider point $x_i$ to have\na high contribution to the upsampling process, indicating a more accurate\nresolution after upsampling. Conversely, if $\\Delta L_{x_i}$ is large, the contribution of\npoint $x_i$ is considered low.\nTo represent this in the saliency map, we assign each point $x_i$ a sensitivity\nscore $s_i$, indicating the contribution level of point $x_i$.\nFor the Saliency Map Expression, formally, the saliency map can be\nrepresented as a function $S_o(\\cdot)$, where $S_o(X)$ takes a point cloud X as input\nand outputs a vector of length N, with each element corresponding to the\nsensitivity score of each point in the point cloud:\n$S_o(X) = \\{s_i | i = 1, 2, . . ., N\\}$"}, {"content": "where the sensitivity score $s_i$ represents the contribution of point $x_i$. If $S_i$\nis high (positive), it indicates that the point has a large positive contribution\nto the upsampling model. If $s_i$ is low or negative, it indicates that the\npoint has a smaller or negative contribution to the upsampling model, as\nillustrated in Figure 3, we present the saliency map in input point cloud and\nthe workflow for getting the saliency map.\nAnd then, represent point cloud saliency map as spherical coordinates.\nIn spherical coordinates, point displacement is considered, where a point\nis represented as (r, \u03c8, \u03c6), withr being the distance from the point to the\nsphere's center, and \u03c8 and \u03c6 being the two angular coordinates relative to the\nsphere's center. In this spherical coordinate system, moving a point toward\nthe center by \u03b4 will increase the loss L by an amount:\n$\\frac{\\partial L}{\\partial r} \\cdot \\delta$.\nAccording to the equivalence $\\Delta L_{x_i}$, we measure a point's contribution\nwith a real-valued score: the negative gradient of L with respect to r:\n$s_i = -\\frac{\\partial L}{\\partial r_i}r_i^{+\\alpha}$.\nThus, in the rescaled coordinate system, we measure the contribution of\npoint $x_i$ simply by using."}, {"title": "3.3. Training Loss Function", "content": "During the training process, our end-to-end network uses Chamfer Dis-\ntance (CD) as the loss function. CD loss is designed to calculate the average\nclosest point distance between two sets of point clouds, describing the dis-\ncrepancy between the predicted and actual upsampled points.\nFor the parallel network ReLPU, it is implemented by fine-tuning ReLPU-\nto extract both local and global features, with CD loss remaining unchanged.\nGiven two sets of point clouds P and Q, representing the predicted and\nground truth point clouds, respectively, the CD Loss can be expressed as\nfollows:\nLet\n$P = \\{p_1, p_2,...,p_M\\}$ and $Q = \\{q_1, q_2, ..., q_N\\}$,\nwhere $p_i$ and $q_j$ are points in point clouds P and Q, respectively. The Cham-\nfer Distance $d_{CD}(P, Q)$ is defined as:\n$d_{CD} (P, Q) = \\frac{1}{M}\\sum_{i=1}^{M} min_{j=1,...,N} ||p_i - q_j ||^2 + \\frac{1}{N}\\sum_{j=1}^{N} min_{i=1,...,M} ||q_j - p_i||^2$.\nThis loss function minimizes the distance between each point in one set\nand its nearest neighbor in the other set, thus effectively capturing the ge-\nometric discrepancy between the predicted upsampled point cloud and the\nground truth."}, {"title": "4. Experiments", "content": "In our study, we conducted experimental validation on two different datasets:\na synthetic dataset for training and a real-world object dataset for evalua-\ntion. For training and testing the upsampling network, we used the synthetic\ndataset PU1K [15], which includes 69,000 training samples. PU1K is a new\npoint cloud upsampling dataset introduced in PU-GCN. Overall, PU1K con-\nsists of 1,147 3D models, split into 1,020 training samples and 127 testing\nsamples. The training set includes 120 3D models from the PU-GAN dataset\n[16] and 900 different models collected from ShapeNetCore [6]. The test set\ncontains 27 models from PU-GAN and over 100 models from ShapeNetCore,\ncovering 50 object categories.\nThe original PU1K dataset was modified to fit the patch-based upsam-\npling pipeline, with training data generated through Poisson disk sampling\nfrom patches of 3D meshes. Specifically, the original training data comprises\n69,000 samples, each containing 256 input points (low resolution) and 1,024\npoints (4\u00d7 high resolution). To match the input model with the number of\npatches and AS inputs [26], we constrained PU1K to 8,192 points via Poisson\ndisk sampling. Each model was divided into eight different local parts using\npatches and eight uniform subdivisions with AS to maintain model shape in-\ntegity. Similar to previous state-of-the-art point cloud upsampling models,\neach sample contains 256 input points (low resolution) and 1,024 points (4\u00d7\nhigh resolution). Under these conditions, our training data includes 16,320\nsamples."}, {"title": "4.2. Upsampling Results", "content": "In accuracy analysis, as mentioned above, we extensively evaluated the\ngeneralization performance of our network on the PU1K synthetic dataset,\nfocusing on the differences between the original backbone network and the\nReLPU pipeline incorporating both local and global features. The quanti-\ntative results for the PU1K dataset [15] in Table 1 show that our method\nperforms consistently across different input scales (256, 512, 2,048 points)\nand outperforms the original network in CD, HD, and P2F metrics."}, {"title": "4.3. Noise Robustness Test", "content": "It is necessary to verify our model's robustness to noise. Specifically, we\ntested the pre-trained model by adding random noise to sparse input data,\nwith Gaussian noise satisfying a normal distribution N(0, 1) and scaled by\na coefficient. We experimented at two noise levels: \u03b2 = 1% and \u03b2 = 2%.\nTable 2 quantitatively compares the results of our model and state-of-the\nart methods under different noise levels. In most test cases with noise, our\nproposed ReLPU outperformed the base models MPU [14], PU-GCN [15],\nDis-PU [27], and PUCRN [28] showed robustness in CD and HD metrics."}, {"title": "4.4. Ablation Study", "content": "To verify the effectiveness of ReLPU compared to the original models,\nwe visualized saliency maps in ReLPU across four models. To better under-\nstand our saliency maps, several maps are visualized in Figure 4, where we\ncolor-code points based on their saliency score ranks, with higher numbers\nindicating higher saliency scores. We can see that the four models MPU [14],\nPU-GCN [15], Dis-PU [27], and PUCRN [28] have higher eigenvalues under\npatch input, especially at the edge curves. However, the global feature input\nbased on AS input does not have particularly prominent feature values across\nthe entire model. ReLPU, on the other hand, reflects its combination of the\ncharacteristics of both in terms of feature value contribution.\nWe compared ReLPU- and ReLPU to demonstrate the effectiveness of\nparallel feature processing over sequential processing. Results are shown in\nthe Table 3. It can be seen that ReLPU based on parallel local and global\ninputs performs better than ReLPU based on sequential networks, which\nfully demonstrates the importance of parallel local and global feature fusion."}, {"title": "4.5. Visualization", "content": "Figure 5 provides a qualitative analysis on the PU1K dataset [15], demon-\nstrating the effectiveness of our method in preserving overall shape, contour\nboundaries, and topological features, particularly in the zoomed-in areas.\nThis is critical for applications that require surface reconstruction with high\nfidelity and topological accuracy. It can be seen that ReLPU can maintain\nthe global structure of the point cloud model while being more detailed and\naccurate in the details of local features.\nIn the Figure 6, 2,048 sparse points are used as input, generating a dense\npoint cloud with 8,192 points at a 4\u00d7 upsampling ratio. It shows the cor-\nresponding 3D mesh reconstructed using the Ball Pivoting algorithm [47].\nFrom the reconstruction results, although MPU [14], PU-GCN [15], and Dis-\nPU [27] achieved state-of-the-art (SOTA) performance, they tend to overfit\nand produce holes when processing sparse boundaries of real-world objects.\nThe results indicate that our method significantly improves overfitting in\nhandling sparse and boundary regions. Our method not only enhances the\nuniformity and detail of point clouds but also substantially improves the ac-\ncuracy of reconstructing holes and sparse regions, demonstrating the utility\nof our comprehensive approach in maintaining topological consistency while\nprocessing both local and global information."}, {"title": "5. Discussion", "content": "With the rapid advancement of deep learning, numerous novel architec-\ntures have emerged, such as Transformer [38], Mamba[48], RWKV[49], and\nxLSTM[50]. These methods have also inspired the development of hybrid ap-\nproaches, combining their strengths with Graph Neural Networks (GNNs),\nresulting in architectures like Graph-Transformer[51], Graph-RWKV[52] and\nGraph-Mamba[53]. Moreover, there have been attempts to apply these new\narchitectures to point cloud upsampling, exemplified by MBPU[54], which\nintegrates Mamba into this domain.\nOur approach ensures that future innovations in backbone architectures,\nsuch as those based on Transformer, Mamba, or other emerging models,\ncan be seamlessly integrated into ReLPU. This adaptability not only future-\nproofs the framework but also ensures its relevance as point cloud processing\ntechniques continue to evolve. The salinity map reflects the shape of the input\ndata, as well as the local and global shapes, and can indeed represent local\nand global features, thus achieving the extraction of both types of features\nin conjunction with the encoder at the input end. Through comprehensive\nexperiments and analyses, we demonstrated the effectiveness of ReLPU in\nachieving superior geometric fidelity and robustness compared to state-of-\nthe-art methods, making it a promising solution for point cloud upsampling\nin the era of rapidly evolving deep learning architectures."}, {"title": "6. Conclusion", "content": "In this research, we proposed the ReLPU framework to address the chal-\nlenge of adapting to the rapid evolution of network structures while ensuring\nsuperior performance in point cloud upsampling. ReLPU was designed to be\na flexible and adaptable framework that allows the backbone network to be\neasily replaced with advanced architectures. By enabling the parallel extrac-\ntion of global and local features, ReLPU significantly enhances the quality\nand fidelity of upsampled point clouds.\nThe ReLPU framework for point cloud upsampling, integrating parallel\nglobal and local feature extraction. By employing identical autoencoders\nfor both global and local inputs, our method effectively addresses challenges\nrelated to sparsity, noise, and topological inconsistencies. Extensive experi-\nments on the PU1K and PU-GAN datasets demonstrated the superiority of\nReLPU over state-of-the-art models in terms of geometric fidelity and robust-\nness. Saliency map analysis further validated the importance of combining\nglobal and local features for accurate upsampling. The ReLPU framework\nnot only improves the uniformity and detail of point clouds but also enhances\nits adaptability to real-world applications, making it a versatile solution for\nfuture advancements in point cloud processing."}]}