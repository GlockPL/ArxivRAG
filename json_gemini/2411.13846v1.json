{"title": "Exploratory Study Of Human-AI Interaction For Hindustani Music", "authors": ["Nithya Shikarpur", "Cheng-Zhi Anna Huang"], "abstract": "This paper presents a study of participants interacting with and using GaMaDHaNi, a novel hierarchical generative model for Hindustani vocal contours. To explore possible use cases in human-AI interaction, we conducted a user study with three participants, each engaging with the model through three predefined interaction modes. Although this study was conducted \"in the wild\"\u2014 with the model un-adapted for the shift from the training data to real-world interaction we use it as a pilot to better understand the expectations, reactions, and preferences of practicing musicians when engaging with such a model. We note their challenges as (1) the lack of restrictions in model output, and (2) the incoherence of model output. We situate these challenges in the context of Hindustani music and aim to suggest future directions for the model design to address these gaps.", "sections": [{"title": "1 Introduction", "content": "Hindustani music is a form of North Indian classical music that incorporates improvisation guided by fixed melodic (raga), rhythmic (taal), and structural frameworks. With a focus on improvisation, the tradition has evolved to be predominantly oral; thus a discrete symbolic representation of the music, although present, does not fully capture the melodic ornamentations or the essence of improvisation present in the music. As a result, musical interaction as opposed to textual knowledge is important for both pedagogy and performance of this tradition.\nAs an oral tradition, Hindustani music is known to be Gurumukhi, i.e. to be learned from the guru or teacher (Patel [2007], Ganguli [2012]). Real-time musical interaction with the guru is vital for the student to internalize concepts like raga boundaries, aesthetics, and vocabulary, which in turn shape their melodic improvisation. Additionally, in performance, a main artist, rhythmic accompaniment, and sometimes melodic accompaniment collaborate, with the accompanists supporting the main artist's improvised creations, resulting in complex musical interactions (Leante [2016]).\nConsidering the importance of interaction in Hindustani music, an interesting area of study is the feasibility of human-AI interaction in this field. Previous work on generative modeling for this music has side-stepped the lack of well-defined discrete representations with two methods: (1) using musical notation from textbooks or music theory (Vidwans [2017], Das and Choudhury [2005], Sahasrabuddhe [1992]), (2) leveraging MIDI extracted from audio (Gopi and William [2023], Adhikary et al. [2023])."}, {"title": "2 Study Design", "content": "Below we discuss details of the participants involved and the structure of the user study."}, {"title": "2.1 Study Participants", "content": "We recruited three musicians for the study, further referred to as P1, P2, and P3 due to anonymity purposes. All participants are trained in Hindustani music for over 15 years and engage in regular practice and performance. P1 and P2 are harmonium\u00b2 players and P3 is a vocalist. In the interview, P1 interacted with the model using his harmonium whereas P2 and P3 used their voices. The interview lasted for about an hour and the participants were offered a compensation of 20 CAD for their time."}, {"title": "2.2 Study Structure", "content": "This study investigates the expectations and perspectives of a Hindustani music practitioner regarding generative tools like GaMaDHaNi, assessing the model's alignment with these insights and identifying its strengths and limitations. As one of the pioneering works in modeling Hindustani vocal contours, GaMaDHaNi lays the groundwork for future research. Through these interviews, we aim to uncover promising directions for researchers and musicians exploring human-AI interaction in the context of Hindustani music. The study consisted of three stages: (1) a semi-structured interview on the participant's relationship with AI, (2) interaction with the model via three pre-defined tasks, and (3) closing reflections.\nSemi-structured interview The study began by exploring the participant's familiarity with generative AI tools and their ideas for how it could assist or enhance their music practice and performance.\nInteraction with the model In this section, we explore the participants' experience interacting with GaMaDHaNi through three pre-defined tasks, tailored to the model's affordances and the potential for engaging interesting human-AI musical interactions. Participants could engage with each task for any duration and were asked to share their experiences afterward.\nThe three pre-defined tasks included:\n\u2022 Idea Generation (Task 1): In this task, participants began by selecting their preferred option from two randomly generated, four-second audio clips produced by the model. They then continued to choose their preferred continuation after each generation, with the model providing two new, two-second continuations for each selected clip in an iterative loop.\n\u2022 Call and Response (Task 2): Participants provided a musical input by singing or playing an instrument. In response, GaMaDHaNi, trained to generate 12-second audio clips, produced an eight-second continuation based on the last four seconds of the input.\n\u2022 Melodic Reinterpretation (Task 3): Similar to SDEdit's (Meng et al. [2022]) method to convert coarse brush strokes to full images, in this task, the user's input was considered to be a 'guide' based on which the model generated an audio clip, described further in Sec. 3.2."}, {"title": "3 Overview of the Generative Model (GaMaDHaNi)", "content": "GaMaDHaNi (Shikarpur et al. [2024])\nis a two-level hierarchical generative\nmodel trained to sing vocal contours\nfrom Hindustani classical music. The\nmodel involves a Pitch Generator ca-\npable of generating a finely quan-\ntized fundamental frequency contour\nthus capturing the intricate melodic\nmovements present in the musical\nform. Conditioned on this contour\nand singer information, a Spectrogram\nGenerator predicts a mel-spectrogram\nwhich is further converted to audio\nusing the Griffin-Lim algorithm (Grif-\nfin and Lim [1984]). Fig 1 presents a\nhigh-level representation of the model\narchitecture. Through a listening\nstudy and qualitative analysis, the au-\nthors of GaMaDHaNi motivate their\nchoice of the hierarchy and the in-\ntermediate pitch representation in the\nmodel architecture.\nFor the user study, a diffusion-based\nimplementation of the Pitch and Spec-\ntrogram Generators was used, with the model interaction implemented via Gradio (Abid et al. [2019]).\nFurther, I will briefly discuss the implementation of the tasks performed in the study. There are\ntwo main ideas involved (1) primed generation, which is used in the 'idea generation' and 'call and\nresponse' tasks and (2) melodic reinterpretation, which is seen in the 'melodic reinterpretation' task.\nAll of these concepts take place in the Pitch Generator."}, {"title": "3.1 Primed Generation", "content": "This approach involves prompting the model with a 'prime', i.e. a melodic sequence represented as a\npitch contour, and tasking it with generating a continuation. Since the model is trained to generate\npitch contours of a fixed length $T = 12$ seconds, we treat the last $t_{prime}$ seconds, where $t_{prime} < T$,\nof the prime as the initial part of the generated sample. The model then infills the remaining duration,\n$T - t_{prime}$ seconds, to complete the sequence.\nFor task 1, i.e. Idea Generation, the user's only means of interaction is to choose from generated\noutputs of the model. Thus the samples were truncated to 4 s to give the user a sense of granularity\nand some control in creating a longer musical idea with this task. For each iteration of the loop, the\nduration of the prime considered, $t_{prime} = 2$ seconds. In task 2, i.e. Call and Response, the entire\nduration of the generation, i.e. 12 seconds was considered, where the first 4 seconds was taken from\nthe input 'call' or melodic prime, i.e. $t_{prime} = 4$ seconds."}, {"title": "3.2 Melodic Reinterpretation task", "content": "Melodic reinterpretation in this study mirrors the approach used in SDEdit (Meng et al. [2022]) for\nguided image generation. This method takes advantage of the ability to solve reverse diffusion from\nany intermediate time step $t \\in [0, 1)$. Using a coarse input or \u2018guide', the appropriate initialization is"}, {"title": "3.3 Considerations while interpreting results from this \u201cin the wild\u201d study", "content": "It is important to note that the generative model was trained on data from Saraga (Srinivasamurthy et al. [2021]) and Hindustani Raga Recognition (Gulati et al. [2016]) datasets, comprising studio and live ensemble performance recordings led by professional vocalists in Hindustani music. Thus, the data that the model sees during interaction in our studies is likely to be out of distribution for reasons including: (1) the input was less performance-like in terms of style and complexity, (2) training data was extracted from an ensemble setting whereas interaction data was from a solo participant, (3) the absence of source separation during interaction may have removed artifacts which were present in the training data, and (4) one of the participants used a harmonium for interaction while the model was trained only on the voice. Despite these differences, we conduct this exploration to inform future work to gear this model more toward interactive use cases."}, {"title": "4 Challenges Faced by Participants", "content": "During the interaction, the model's performance sometimes failed to meet the participants' expecta- tions. Based on their reactions and discussions, we identified two primary challenges: (1) the lack of restrictions and (2) inconsistency in the model output. Readers are encouraged to watch samples of interaction for each of the pre-defined tasks and the challenges discussed below in the supplementary website."}, {"title": "4.1 Lack of restrictions", "content": "Throughout all tasks, the participants noted a lack of restrictions on the output of the model. P2 claims that the model output was too \"free-flowing\" for him to evaluate musically, adding that \u201cthe music that we perform has a very defined structure to it. And if it (the model) is going to be a companion, it cannot do some basic errors which are not there in the structure\".\nParticipants felt the need for restrictions in the model output based on (1) raga, (2) scale, (3) timbre, and (4) style as presented below.\nRaga Raga is an integral part of Hindustani music which defines the boundaries within which melodic improvisation can occur. Powers and Widdess [2001] describe raga as a combination of a scale and a tune; while it has defined notes and tonal hierarchies, it also includes characteristic phrases and melodic movements. Students of Hindustani music spend years understanding how to explore melody creatively while remaining within the framework of a raga (McNeil [2017]). The participants' initial observation was the lack of raga adherence in model outputs (\u201cBut it's like just"}, {"title": "4.2 Incoherence in model output", "content": "In the call and response task, participants often experienced a disconnect between their input and the model's output, feeling that the model did not truly \"respond\" to their input, thus hindering two-way interaction. P1 notes that the outputs were \"random\" adding that his input did not seem to affect the output (\"... me giving input, I didn't see how it's helping\"). Participants suggested that for the interaction to feel reciprocal, it should incorporate both lower-level technical attributes like raga and scale, and higher-level abstract attributes like mood and musical idea.\nLow-level attributes P1 notes how the model did not maintain the same musical \"structure\" as the input, taking the example of how he played one scale's notes but the model's outputs were in another scale. Although the participant only gives the example of scale, I believe the idea of \u201cstructure\u201d could also be extended to the rhythmic aspects of the input and output; the model often generates outputs in widely varied tempos from the input which can be disconcerting for a participant, an example of which is seen in Fig 2 (bottom). Additionally, P3 speaks of how he was singing the basic form of a raga, Lalit, and thus expected the model to maintain the raga but that was not the case.\nHigher-level attributes P1 mentions how the model was uncooperative with his attempts to create a certain mood (\u201cI was trying to achieve some mood and it gave me a different mood. So it didn't help because I would have to regenerate something (the input) to preserve the intended mood.", "good responses\", which among other things discussed in Section 4.1, hints at the model's inability to capture the musical idea. Based on his practice, a \"good response\" involves the repetition of the musical idea in the input (\u201cbecause the music that we do, it's a lot of repetitions. So usually the responses that I know of are repeating the same thing (as in the input).": "P2)."}, {"title": "5 Discussion", "content": "Based on the observations from Sec. 4, we note the importance of recognizing what creativity means in the context of Hindustani music and consequently, possible methods through which a model could better capture that essence.\nCreativity in Hindustani Music Nikrang et al. [2024] argue that creative systems should be able to produce surprising or unpredictable output autonomously while non-creative systems or tools are predictable and not necessarily autonomous. Hindustani music however involves idiomatic improvisa- tion; the improvisation involved is not simply variations of a fixed composition itself, but elaborations of different raga aspects performed against the taal, i.e. metre (Jairazbhoy [1995]). Additionally, previous work (Huron [2008], Widdess [2013]) speaks of unconscious expectations among listeners in Hindustani music including schematic expectations, which are based on conventional patterns or structures in the music observed by the listener through long-term exposure to the music, and dynamic expectations which are based on the perception of patterns in real-time during performances. Thus one could argue that some predictability is required to satisfy conventional structures and frameworks. Further, McNeil [2017] defines creativity in Hindustani music as the ability to expand and grow fixed seed ideas such as raga, performance frameworks, and melodic ideas. Finding new melodic movements while satisfying these essential fixed seed ideas is necessary to create within the tradition.\nConstraints for creativity Work by Haught-Tromp [2017] suggests that creativity thrives under constraints. Their study found that when participants were required to include concrete nouns while writing rhymes, the results were more creative. Constraints reduce the overwhelming array of possibilities to a more manageable subset, fostering imaginative thinking. Similarly, as P1 suggested, limiting the notes available to the model for generation could also encourage more interesting explorations.\nRL Tuner (Jaques et al. [2017]) employs reinforcement learning (RL) to enhance the output of a music sequence predictor, Note-RNN (Eck and Schmidhuber [2002]), using music theory-based constraints. Trained on MIDI data, which includes discrete note information, RL Tuner shows RL's potential for imposing restrictions in music generation. However, GaMaDHaNi, trained on continuous pitch"}, {"title": null, "content": "found, using which the reverse diffusion process is performed to obtain a realistic pitch contour that\nis also faithful to the guide.\nGaMaDHaNi uses Iterative a-Deblending (IADB) (Heitz et al. [2023]), as the training objective\nfor both of its hierarchical blocks. IADB defines a simplified diffusion process that is a linear\ninterpolation between noise $x_0 \\sim X_o = \\mathcal{N}(0, 1)$ and data $x_1 \\sim X_1 = X_{data}$. Thus an a-blended\npoint with blending parameter a is defined as,\n$x_{\\alpha} = (1 - \\alpha)x_0 + \\alpha x_1$.   (1)\nThus for the model, given a melodic guide in the form of a pitch contour $x^{(9)}$, we find the proper\ninitialization at time step $t_0$\n$x^{(9)}_{t_0} = (1 - \\alpha_{t_0})x_0 + \\alpha_{t_0}x^{(9)}_1$,   (2)\nsuch that we can iteratively perform reverse diffusion with\n$x_{\\alpha_{t+1}} = x_{\\alpha_t} + (\\alpha_{t+1} - \\alpha_t)D_{\\theta}(x_{\\alpha_t}, \\alpha_t)$,   (3)\nwhere blending parameter $\\alpha_t = 1, t \\in [t_0, T - 1]$ is the time step in the diffusion process, $T$ is the\ntotal number of steps, $x_{\\alpha_t}$ is the value of the input $x$ at time $t$, and $D_{\\theta}$ is the learned Pitch Generator\nmodel that is trained to predict the difference between the expected value of the posteriors of the data\nand noise samples given $\\alpha$ and $x_\\alpha$. For the purpose of this study $t_0$ is fixed at 0.5."}, {"title": null, "content": "contours, faces challenges in extracting discrete notes due to the subjectivity of solfege notation and\nthe presence of note ornamentations and microtonal variations (Vidwans et al. [2012]). Thus, to\neffectively apply constraints in the discrete note domain, a robust mapping between discrete notes\nand continuous pitch contours must first be established.\nAI researchers generally think of ideas of these constraints in terms of \u201cconditioning\u201d. Conditioning\nallows the user to shape the model output in desired ways, thus implicitly imposing the user's\npreferences. Conditioning could be performed either before, during, or after training a generative\nmodel. First, controlling the type of input that the model sees could ensure generations of only\na certain data distribution. COCONET (Huang et al. [2019]), for instance, is trained only on\nBach's chorales and thus generates music that harmonizes similarly. Second, conditioning could\nbe added during training as an additional input. This could either be time-agnostic resulting in\nglobal conditioning or time-specific resulting in local conditioning. Jukebox (Dhariwal et al. [2020])\ninvolves global characteristics such as genre, artist, and style to condition model output whereas\nC-RAVE (Devis et al. [2023]) involves conditioning on time-varying local attributes such as loudness\n(measured with root mean squared energy), spectral brightness (measured through spectral centroid),\nsharpness and boominess (provided by AudioCommons project (Font et al.)). A common method of\nconditioning includes Classifier-Free Guidance (Ho and Salimans [2021]) which involves combining\nthe outputs of the conditionally and unconditionally trained models to regulate the level of control.\nThird, post-hoc methods are used to instill conditions in pre-trained unconditional models, through\nlearning constraints on the latent space of a model (Engel et al. [2017]), prompt-based learning\n(Brown et al. [2020]), or training another model to impose constraints (Zhang et al. [2023], Wu et al.\n[2024]).\nThus, future work has to address effective ways to represent musical expectations in Hindustani\nperformance such as raga, taal, and performance frameworks to aid creativity guided by the tradition.\nFurther, studies will have to be conducted on methods to condition generated output on these\nexpectations. Additionally as observed by the participants, further study is required to ensure the\ncoherence of model output to the input to ensure a better interactive experience; this could potentially\nbe implemented through various forms of conditioning, and accounting for the difference in data\ndistributions seen during training and interaction."}]}