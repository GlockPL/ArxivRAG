{"title": "Leveraging Event Streams with Deep Reinforcement Learning for End-to-End UAV Tracking", "authors": ["Ala Souissi", "Hajer Fradi", "Panagiotis Papadakis"], "abstract": "In this paper, we present our proposed approach for active tracking to increase the autonomy of Unmanned Aerial Vehicles (UAVs) using event cameras, low-energy imaging sensors that offer significant advantages in speed and dynamic range. The proposed tracking controller is designed to respond to visual feedback from the mounted event sensor, adjusting the drone movements to follow the target. To leverage the full motion capabilities of a quadrotor and the unique properties of event sensors, we propose an end-to-end deep-reinforcement learning (DRL) framework that maps raw sensor data from event streams directly to control actions for the UAV. To learn an optimal policy under highly variable and challenging conditions, we opt for a simulation environment with domain randomization for effective transfer to real-world environments. We demonstrate the effectiveness of our approach through experiments in challenging scenarios, including fast-moving targets and changing lighting conditions, which result in improved generalization capabilities.", "sections": [{"title": "I. INTRODUCTION", "content": "The technology of UAVs, known as drones, has been increasingly used in humanitarian missions for search and rescue, surveillance for safety control and emergency contingency plan, or for guiding tasks [1], [2], [3], [4]. This rising interest emphasizes the need for autonomous navigation rather than manual control. Active tracking is one of the most crucial tasks for UAVs, requiring the tracker to keep the target centered in its field-of-view (FOV), relying mostly on visual observations to follow the moving target [5]. These tracking capabilities are useful in many applications including search and rescue missions, where a piloted drone can lead exploration while a fleet of autonomous drones navigates autonomously in GPS-denied environments by following the leader [6], [7].\nActive tracking with drones presents significant challenges due to the complexity of the tracking process and the nonlinearity of the system. Drone dynamics are affected by uncertain environmental conditions and nonlinear effects from aerodynamic forces, torques, payload variations, and control signal noise. In dynamic environments, where target motion and conditions change rapidly, maintaining accurate tracking becomes even more difficult. These challenges make it difficult to design a system controller that can reliably maintain stable tracking.\nEarlier works have mostly employed Proportional-Integral-Derivative (PID) control [8] or Linear Quadratic Regulator (LQR) [9] approaches. However, designing such controllers requires defining a highly accurate model of drone dynamics, which is often difficult to obtain. Recent advances in the field involve using reinforcement learning; however, like classic controllers, it mostly requires access to the target state [10]. Accurately estimating that while both the tracker and the target are in continuous motion is challenging and requires integrating a high-performance module for precise target localization.\nTo address the aforementioned challenges, we opt for designing an end-to-end deep reinforcement learning-based controller that can process raw sensor data instead of training detection-based policy. A DRL-based tracker adapts to environmental changes by learning from past experiences, enabling model refinement without altering system components, unlike classic controllers. Additionally, a highly accurate dynamic model is not required because the DRL algorithm can learn through data, even when the model does not perfectly capture all the complexities of the control system.\nUnlike previous studies that have shown the potential of reinforcement learning in control tasks [11], [12], [13], the proposed end-to-end framework not only bypasses the detection task but also leverages the unique properties of event sensors over conventional RGB cameras. The latter, with their low frame rates and limited dynamic range, pose significant challenges in control. Motivated by the recent success of event cameras in vision applications [14], we aim to instead investigate these newly emerging bio-inspired sensors. Thanks to their distinctive properties, such as high dynamic range, high temporal resolution, and low latency, a large interest has been shown in exploiting these new sensors, especially in autonomous vehicles where rapid responses, adaptability to weather and lighting changes, and robust visual information at high speeds are crucial [15]. An illustration of the comparison between a classic controller and our proposed end-to-end DRL-based controller processing events is shown in Fig. 1.\nTraining a DRL model involves trial and error where the drone learns an optimal strategy based on previous trials. Since this requires massive training, which is not feasible with a real drone, due to the risk of drone crashes, we opt for simulated aerial data collection which allows numerous simulated trajectories. This simulated environment serves as a crucial resource for training our proposed DRL-based controller and is complemented by domain randomization for effective real-world transfer and parallel training to manage complexity."}, {"title": "II. RELATED WORK", "content": "In visual tracking, a key distinction is made between passive and active tracking. In passive tracking, the object position is estimated in each frame based on its previous state. Object detection algorithms, such as YOLO [17], SSD [18], or Faster R-CNN [19], are used to locate the object. Once identified, tracking involves maintaining the object position in subsequent frames using methods like the Kalman filter, Mean-Shift, CAMShift [20], or deep learning-based trackers like Deep-SORT [21]. The camera is typically fixed, so the tracking focuses on processing stationary frames. Unlike passive tracking, active tracking does not require explicit object localization [22], [23], [24]. Instead, feedback from previous camera frames is used to adjust camera orientation and position enabling to focus on the target in dynamic and responsive way to the target movement. In the case of UAV tracking, the drone receives visual information from mounted cameras and adjusts its position and orientation to keep the target centered in its FOV.\nFor active tracking, some related works rely on classical controllers [25], [8], [12]. For instance, in [12], predictive learning is combined with reactive control systems to perform self-supervised active action localization. The reactive control, inspired by PID controllers, adjusts camera orientation to keep the target within the FOV. Recent advances in the field have shown the potential of using DRL in control tasks, but most approaches assume access to the target position and are designed for either ground-to-ground or air-to-ground tracking [13], [11]. In [13], a DRL-based visual active tracking system that provides continuous action policies is proposed, however, experiments are conducted on ground mobile robots which are controlled via discrete actions. The method in [11] performs aerial-to-ground tracking using a policy learned from training to fly toward a fixed target.\nWe note that most DRL-based controllers have been implemented for ground robots or for tracking moving objects on the ground, while aerial-to-aerial tracking remains less explored. Furthermore, the proposed approaches are constrained by the limitations of conventional RGB cameras, which, with their low frame rates and limited dynamic range, present significant challenges for dynamic and accurate control. While event cameras have been investigated for UAV applications, primarily for obstacle avoidance [26], [22], to the best of our knowledge, our work is the first to explore their use for UAV active tracking."}, {"title": "III. PRELIMINARY DEFINITIONS", "content": "The quadrotor model, inspired by the study of Mark et al. in [27], represents the drone as a rigid body with six degrees of freedom, namely, three translational and three rotational along the 3D body axes. This model is controlled by a scalar thrust $f$ representing the total thrust value along the Z axis, and angular velocity expressed in the fixed body frame as $\\Omega = (\\Omega_1, \\Omega_2, \\Omega_3)$.\nThe quadrotor state consists of the position, velocity, and orientation. The differential equations are as follows:\n$\\dot{p(t)} = R_3(t)\\frac{f(t)}{m} +g$,\n$\\dot{R_3(t)} = R(t)[\\Omega(t)]_x$,\n$[\\Omega]_x = \\begin{bmatrix}\n0 & -\\Omega_3 & \\Omega_2\\\\\n\\Omega_3 & 0 & -\\Omega_1\\\\\n-\\Omega_2 & \\Omega_1 & 0\n\\end{bmatrix}$"}, {"title": "A. Dynamic Quadrocopter Model"}, {"title": "B. Reinforcement Learning-based Tracking", "content": "For UAVs, the active tracking problem involves using visual sensory data to generate actions that keep the target centered and maintain a certain distance. We formulate the UAV active tracking by reinforcement learning as a Partially Observable Markov Decision Process (POMDP) [28], an extension of Markov Decision Process (MDP). POMDP is defined by the tuple (S, A, O, T,R,Z, \u03b3), where O, S, and A are the observation, the state, and the action spaces, respectively. The discount factor y balances immediate and future rewards. T is the transition probability to a new state. Z defines the probability of obtaining the current observation given the current state and R is the reward function.\nAt each time step, the agent (UAV) interacts with its environment as follows: (i) the agent receives a visual observation $o_t \\in O$; (ii) based on this observation and using a stochastic policy $a_t \\sim \\pi(a_t | o_t)$, the agent selects actions; (iii) the agent gets an immediate reward $r_{t+1} = R(s_{t+1})$ as a function of the new state and receives a visual observation $o_{t+1}$ correlated with the new state $o_{t+1} \\sim Z(o_{t+1} | s_{t+1})$. This formulation is used to find the optimal policy \u03c0* that maximizes the expected cumulative reward over interactions with the environment. In the following, we instantiate the observation, state, and action spaces for the problem under consideration.\nThe observation derived from visual sensors is defined as a sequence of the N latest images: O(t) = (I(t \u2212 N + 1), ...,I(t)), where I(t) is the current image. The observation space is therefore defined as: 0 = (H,W,C)^N, with W \u00d7 H is the spatial resolution and C is the number of channels.\nThe actions are the control decisions for tracking, defined as $a_t = (f(t),\\Omega(t))$. The action space is continuous and is defined as: $A =  [0, f_{max}] \\times [-\\Omega_{max}, \\Omega_{max}]^3 \\subset R^4$.\nThe state space of the system at time t is defined as $s_t = (P_t,V_t,A_t) \\in R^9$, with $P_t = (x(t),y(t),z(t))$ is the relative position of the target along three axes with respect to the tracker, $V_t = (v_x(t), v_y(t), v_z(t))$ is the relative velocity, and $A_t = (a_x(t), a_y(t),a_z(t))$ is the relative acceleration."}, {"title": "IV. METHOD", "content": "The learning framework represents an asymmetric modification of the Soft Actor-Critic algorithm as shown in Fig. 2."}, {"title": "A. Asymmetric Soft Actor-Critic Framework"}, {"title": "1) Asymmetric Actor-Critic:", "content": "The actor uses a stochastic policy to generate actions from the current observation, while the critic evaluates these actions with a Q-function based on the selected action and the current state. Both are modeled with neural networks and trained together to optimize the policy. In soft actor-critic, the optimal policy maximizes both expected rewards and entropy, encouraging the agent to explore new strategies while balancing exploration and exploitation.\nThe characteristic of asymmetry leads in using different inputs for the actor and critic. In robotics, the policy often relies on partial observations from sensors, which provide a noisy and incomplete view of the environment. However, the critic has access to the full system state in the controlled training environment, optimizing the learning process despite the robot restricted sensory input. This modified Actor-Critic algorithm with asymmetric inputs shows promise, as the critic has access to task-relevant information and a full system view during training speeds up convergence, improving overall learning efficiency."}, {"title": "2) Reward shaping:", "content": "The main control objective is to maintain the target in a defined relative position $P = (x(t),y(t),z(t))$ with respect to the tracker. Specifically, we aim that the target position along the x-axis of the tracker body frame to be equal to an optimal distance d*, and along the y and z axes equal to 0 in order to keep the target centered in the FOV of the tracker, as shown in Figure 1, which illustrates the drone's body frame and axis.\nTo achieve this, we shape the reward function based on the relative position Pr. The reward function re(t) is defined as the cubic root of the product of three reward components at each axis:\n$r_e(t) = \\sqrt[3]{r_x(t)r_y(t)r_z(t)}$\n$r_x = max \\Big(0,1 - \\frac{|x(t) - d^*|}{\\pi} \\Big)$\n$r_y = max \\Big(0,1 - \\frac{2}{\\pi}arctan(\\frac{|y(t)|}{\\alpha})\\Big)$\n$r_z = max \\Big(0,1 - \\frac{2}{\\pi}arctan(\\frac{|z(t)|}{\\alpha})\\Big)$"}, {"title": "B. End-to-End Event-based Tracking Architecture", "content": "Within the presented ASAC framework, we train a single end-to-end model to generate actions directly from the raw event data. The overall architecture is shown in Fig. 3.\nCompared to conventional cameras capturing images at a fixed frame rate, event cameras respond to brightness changes for every pixel asynchronously and independently. This results in a stream of events that are spatially sparse and asynchronous. Each event is a tuple e = (t,x,y,p), where t is the timestamp at which the event is triggered, (x,y) are the spatial coordinates, and p is the polarity indicating the sign of the change. To align with conventional image-based vision, event streams need to be transformed into a 2D spatial grid representation. A common way to represent events is the stacking on time which involves incorporating a sequence of events E = {ei | t < i < t+\u2206t} within a time interval At, resulting in an event frame. To train our policy, the observation is defined as a sequence of the N latest event frames (N = 3 in our case): O(t) = (I(t \u22122),I(t \u2212 1), I(t)). To process event frames and capture task-relevant features, the policy architecture is designed as follows."}, {"title": "2) Neural Networks of Actor and Critic:", "content": "Actor and Critic are modeled with the following networks.\nThe deep neural network defining the actor consists of two blocks. The first block is a feature extractor that maps observations to a feature vector, capturing the spatial and temporal information. The feature embedding space has a higher correlation with the state space, giving a much more informative representation of the current state. The event frames, which serve as observations, are processed using ResNet18 [29] as feature extractor due to its balance of complexity and performance. The feature extractor has as input a sequence of 3 event frames and maps each frame to a single feature vector of dimension 512. The three vectors are then concatenated along the first axis to obtain a single latent vector. We then use a fully connected layer to reduce the vector dimension from 3 \u00d7 512 to 512. The second block consists of two linear layers with 512 neurons each followed by a tanh activation. It processes the features of dimension 512 and produces the mean and log standard deviation for the Gaussian probability distribution over the possible actions.\nThe critic network receives the full state of the environment, represented by a nine-dimensional vector $S_t = (P_t, V_t,A_t) \\in R^9$ and the chosen action a(t). The critic is designed using a straightforward architecture based on fully connected neural network layers. First, we use a flatten layer to transform the input into a one-dimensional vector. The main part of the architecture consists of three dense layers. The first layer maps the input to 512 neurons, followed by a second hidden layer with 512 neurons. The final output layer produces a single scalar value, representing the estimated action value $Q_t(s(t),a(t))$. A tanh activation function is applied at the output to constrain the value range."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "Since training a policy via deep reinforcement learning would require a massive number of trial and error attempts, which is impractical with a real drone, we opt for a simulated environment that provides realistic and infinite training data. In particular, we build a simulated environment using AirSim simulator [30] that supports aerial vehicles and Unreal Engine (UE) [31] as graphics engine offering high-fidelity physics and realistic rendering with highly detailed"}, {"title": "A. Experimental Setup"}, {"title": "B. Training and Evaluation Environments", "content": "We generate a training environment called the box environment, where the drone learns optimal tracking strategies under highly variable conditions. To favor smooth transfer to real-world environments, we employ domain randomization [32], introducing enough variability. This approach helps the model handle changes and maintain policy effectiveness across different scenarios. More precisely, the training environment includes random variations in wall textures, ranging from simple to highly patterned designs, as well as changes in color, light intensity, and orientation.\nTo evaluate the trained model, we use environments [33] inspired by the DARPA Subterranean Challenge\u00b9 [16], focusing on robot navigation in complex backgrounds such as caves, urban scenes, tunnels, and mountains, all of which are entirely new to the policy. These environments are challenging due to low lighting and high-texture scenes, making them ideal for evaluating our policy. We further test the model in various box environments to assess its generalization capabilities. For both cases, we test our policy under changing lighting conditions and scenarios involving tracking very fast targets. Additionally, trajectories are randomly generated so as to ensure that the target moves in all directions and periodically pauses at random intervals."}, {"title": "C. Comparisons and Implementation Parameters", "content": "The training was conducted using an NVIDIA RTX A4500 GPU, an i7 12-core CPU, and 32 GB of RAM. To accelerate the training process, we utilize parallel training with 7 agents. The proposed end-to-end event-based UAV tracking is compared with two settings. The first is a baseline method that trains the policy based on object detection, where the observation space consists of target positions and distances. The policy architecture is defined as a simple MLP, with the flattened detection information as input, followed by three fully connected layers with ReLU activation. The second setting is an RGB-based approach, where the perception input is replaced by the latest RGB images instead of event data, while the other architectural details remain unchanged.\nIn all settings, we defined a total number of epochs equal to 70 epochs, each one involving 50,000 time steps, with evaluation episodes set to 6. The buffer size is maintained at 10,000, and the batch size is set to 64. Training occurs every 8 timesteps, with a learning rate of 0.0003 and a discount factor of 0.99. In the reinforcement learning environment, each episode lasts 40 seconds, with the optimal tracking distance d* being 0.2 meters. The action space includes angular rates between [-3.5,3.5] rad/s and thrust between [-18,18] N. Reward parameters are the penalty weight \u03b1 equal to 0.4 and the reward penalty constant kc equal to 10. Events are stacked at a time interval At=0.005 seconds."}, {"title": "D. Training Process", "content": "To ensure that each training episode has a unique trajectory and expose the tracker to new scenarios, the target motion is generated using random movements and velocities, ranging from slow to fast, with sinusoidal trajectories. We use random amplitudes, phases, and frequencies sampled from a defined interval. We introduce short random periods where the target stops moving, allowing the model to encounter and adapt to static scenarios.\nDuring training, the drone follows the target for up to 40 seconds per episode. The episode ends if the drone collides with the target, loses sight of it, or reaches the time limit. This strategy ensures that the target stays within the field of view and at a convenient distance to optimize the policy. After each training epoch, the policy is evaluated across 6 different episodes using the mean and the standard deviation of the cumulative reward per episode. This process is repeated 70 times until the final trained policy is obtained."}, {"title": "E. Limitations of Object Detection-based Policy", "content": "To validate the effectiveness of the end-to-end approach, we evaluate the performance of object detector-based policy trained on ground truth detections. Using an object detector can simplify the learning process, allowing the agent to learn faster, as discussed in the previous section. However, a drop in detection accuracy can significantly reduce the overall system performance. In Fig. 6, we illustrate the impact of adding noise to the detections by comparing the trajectories of the target and the tracker."}, {"title": "F. Results and Analysis", "content": "After discussing the limitations of the detection-based policy, we compare, at this stage, our proposed approach to the RGB-based approach, where event inputs are substituted with RGB images. For this comparison, we use ASAC as the baseline reinforcement learning algorithm with two types of perception inputs: RGB images and event frames. Both methods use the same convolutional neural network architecture, ResNet-18, with identical parameter initialization for fair comparison. Additionally, the task predictive head of the network remains unchanged. Both policies are trained under"}, {"title": "VI. CONCLUSION", "content": "In this paper, we addressed the problem of active drone tracking problem using visual information from event cameras and ASAC-based, deep reinforcement learning, highlighting the benefits of event cameras in terms of reactivity and robustness under varying conditions. We demonstrated the advantages of jointly optimizing the feature extractor and tracking process in an end-to-end fashion rather than training an object detector-based policy. The tracking policies were obtained via appropriate reward shaping within domain randomized environments in simplistic box-like environments to larger-scale, adverse conditions reminiscent of subterranean scenes. A multi-modal solution that leverages the advantages of both RGB and event modalities could be a subject of further investigation, for example via hierarchical RL. Such extension, however, could require additional attention so as to not compromise real-time tracking performance."}]}