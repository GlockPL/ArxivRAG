{"title": "Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks", "authors": ["Paul Smolensky", "Roland Fernandez", "Zhenghao Herbert Zhou", "Mattia Opper", "Jianfeng Gao"], "abstract": "Large Language Models (LLMs) have demonstrated impressive abilities in symbol processing through in-context learning (ICL). This success flies in the face of decades of predictions that artificial neural networks cannot master abstract symbol manipulation. We seek to understand the mechanisms that can enable robust symbol processing in transformer networks, illuminating both the unanticipated success, and the significant limitations, of transformers in symbol processing. Borrowing insights from symbolic AI on the power of Production System architectures, we develop a high-level language, PSL, that allows us to write symbolic programs to do complex, abstract symbol processing, and create compilers that precisely implement PSL programs in transformer networks which are, by construction, 100% mechanistically interpretable. We demonstrate that PSL is Turing Universal, so the work can inform the understanding of transformer ICL in general. The type of transformer architecture that we compile from PSL programs suggests a number of paths for enhancing transformers' capabilities at symbol processing.", "sections": [{"title": "1. Paper Synopsis: How can transformers perform complex symbol processing?", "content": "The unprecedented performance of generative language models such as those of the GPT family (Radford et al., 2018, et seq.) creates a dilemma for the science of intelligence. Fundamental to virtually all classic theories of natural and artificial intelligence are structured symbolic representations and symbolic processing of such representations (Hinzen et al., 2012, reviewed in Sec. 3). These provide the basis for explaining the pervasive compositionality of intelligent cognition (see Box 1) (Prince & Pinker, 1988). Neural networks appear to be singularly unsuited for such computation, and should fail catastrophically, for example, at compositional language processing, including the ultimate challenge for abstract structure processing, generating complex, syntactically valid natural language (Pinker & Prince, 1988). Yet neural language models with transformer architectures dramatically out-perform all symbolic-computation-based models of language processing, and generate rich, syntactically complex English, virtually flawlessly (Chang & Bergen, 2024).\nBox 1. Compositionality and systematicity in cognition\nHuman cognition copes effectively with a huge range of phenomena by representing complex entities as structured assemblies of simpler entities (Russin et al., 2024). Knowledge derived from previous experience with simpler entities can give rise to knowledge about a novel complex entity by composing existing knowledge of the simpler entities of which the complex entity is composed. This is compositional generalization.\nThis is all so fundamental and natural that we take it for granted. Yet even current state-of-the-art neural networks do not display the extremely robust compositional generalization that is characteristic of human cognition.\nFormalizing compositional structures as discrete symbol structures such as parse trees or knowledge graphs gave classic symbolic AI excellent compositional generalization to the extent that natural phenomena could be successfully decomposed into discrete parts. However, adequately decomposing natural complex entities into recombinable discrete constituent parts typically proved beyond the capabilities of discrete symbolic methods. If English syntax could be fully formalized with a discrete rewrite-rule grammar, then a symbolic NLP system in possession of that grammar would exhibit perfect compositional generalization across the entire language. Despite decades of attempts, however, reducing natural language to such compositional rules has failed to adequately cover the actual richness of language use.\nYet compositional analysis still provides the deepest understanding of the portion of natural language, and cognition generally, that it is able to cover. And while contemporary neural AI systems display extraordinary coverage, their often dramatic failures of compositional generalization suggest that however these systems \u2018understand' the world, they do so in a fundamentally different way than we do.\nThis cursory discussion of compositionality in cognition is necessarily greatly oversim- plified, but faithfully captures the core ideas. Relatedly, systematic generalization, often seen as an aspect of compositional generalization, consists of generalizing, to new concepts, the capabilities already learned for other concepts."}, {"title": "1.1 Research questions", "content": "The surprising success of transformer neural networks at exhibiting systematicity and compositionality in ICL raises many questions, including those in (13).\n(13) Potential questions\na. How does the actual training of LMs produce a network that can do such ICL?\nb. How do the transformer architectures implementing LMs actually perform ICL?\nc. How is it even possible that these networks can do ICL?\nThese questions are within the ultimate scope of this work, but at this stage we have no concrete answers, and they are not addressed directly in this paper although we come close to answering (13c), with respect to a modestly modified type of transformer. One reason questions like (13) are difficult is that we have few promising hypotheses for possible answers, hypotheses precise enough to test. The results we present here can generate a number of such promising hypotheses; some of these are presented in Sec. 9.2.\nTo pursue understanding of complex neural networks, which are notoriously opaque, for guidance we turn to Richard Feynman's famous dictum, \"what I cannot create, I do not understand\" (Gleick, 1993): we design and hand-program a type of transformer network that demonstrably performs ICL of the type illustrated in Box 2.\nIn place of the questions (13), currently out of reach, we address questions that we can answer; the answers to these questions can inform answers to the more ambitious questions (13).\nOur questions (14) are as follows.\n(14) Our questions\na. How could any neural network employing fairly standard, general-purpose mech- anisms do ICL?\nb. Can general-purpose operations used by the transformer architecture help design such a network?\nc. Can insights from classic, symbolic AI help design such a network?\nBeyond providing a step towards answering the questions (13) from AI, another motivation for our question (14a) comes from cognitive science, where a debate has raged since the"}, {"title": "1.2 NL-semantics-free, number-free symbol-manipulation", "content": "It is the context of this 40-year debate that has directed this work differently from much recent work analyzing ICL. Some of that work has looked at prompts such as 'Q France A Paris Q Spain A' (Hendel et al., 2023), or prompts that provide numerical-vector pairs (x, y) where y is a hidden affine function of x, which the model must infer from the given pairs and apply to a novel value of x (Akyurek et al., 2022; Garg et al., 2022). But following natural- language-semantic associations, e.g., between countries and their capitals, and inferring affine numerical functions, are just the kinds of abilities neural networks have long been known to possess.\nThe critique our work responds to concerns entirely different types of capabilities that are linked to meaning-free symbol processing (Fodor, 1980): identifying potentially meaningless patterns in strings of potentially meaningless symbols, and generating new strings that exhibit the same patterns (relatedly, see Lasri et al., 2022). The examples of ICL given in Box 2 start off being patterns with some meaning for us, in terms of natural language syntax or logical or mathematical inference. The task we study encompasses such interesting cases, but we are not actually concerned with knowledge of that sort; the final example in the Box (10-12) achieves the \u201cmeaningless symbol manipulation\" capability we are targeting: the symbol strings over which the templates are given in the prompt are randomly-generated sequences of randomly-generated symbols. The reason for our focus is that, if we present the prompt 'Q twice x A x x Q twice a A' and we get the desired continuation 'a a', we want to be sure it is the result of correctly filling in the given template, and not simple application of the model's prior knowledge (e.g., from massive English pretraining data) of the semantics of 'twice'; in fact, we'd want the same continuation from the prompt 'Q thrice x Axx Q thrice a A', in violation of the English semantics of 'thrice', and the same continuation from 'Q GBq3 x A x x Q GBq3 a A'.\nThis NL-semantics-free task is illustrated in the paper's primary case-study, the Swap task, which we develop starting in Sec. 3.2. This allows us to focus entirely on how abstract (meaning-free) pattern recognition over (meaning-free) tokens is possible within neural computation. The general task we study - Templatic Generation of text - not only covers many interesting cases like those illustrated in Box 2, it also demands nearly all the abstract symbol-processing capabilities, provided for free by symbolic computation, which have long"}, {"title": "1.3 The Transformer Production Framework (TPF)", "content": "The primary contribution of the present work is the Transformer Production Framework (TPF); we use it to study in-context learning to fill in meaningless, randomly-generated symbolic templates, but it can be applied much more widely. In TPF, a computational system is described at multiple levels, as in (15); we follow the proposal of Hamrick & Mohamed (2020) that machine-learning work take advantage of the three levels of description famously proposed by the legendary cognitive scientist David Marr (1982).\n(15) Three-level specification of a computational system in the Transformer Production Framework TPF\na. Functional level: a highly general symbolic templatic generation function speci- fying completions of templatic symbol-sequence inputs (\u2018prompts') [Sec. 4].\nb. Algorithmic level, comprising two sub-levels:\ni. a high-level symbolic production-system program in the PSL programming language [Sec. 5];\nii. a lower-level program for a new type of symbolic abstract machine, the QKV Machine a kind of symbolic transformer [Sec. 6].\nc. Implementation level: a numerical Discrete-Attention-only Transformer (DAT) defined by its weight matrices for generating queries, keys and values [Sec. 7].\nWhat is meant here by a production system is a type of symbolic computational archi- tecture briefly summarized in (16) (Jones & Ritter, 2003). Production systems include Emil Post's rewrite-rule systems (Post, 1943) and the string-rewriting systems at the foundation of the theory of formal grammars (Book et al., 1993), all key to the classic theory of (symbolic) computation (Hopcroft et al., 2000). Early, special-purpose neural models very different from the transformer were developed in the late 1980s for implementing production systems (Dolan & Smolensky, 1989; Touretzky & Hinton, 1988).\n(16) Production Systems\na. A production is a Condition-Action pair: when the symbols in a common workspace meet the requirements of the Condition, the Action may be taken, which adds or deletes information from the workspace.\nb. In the simplest case, the productions are linearly ordered and apply in sequence, the sequence being executed repeatedly until some termination condition is met."}, {"title": "1.4 Turing-Universality of the results", "content": "Although the presentation here focuses heavily on the ICL of templatic text generation, our results on what transformers can compute is actually much more general. We show here how programs in the Production System Language PSL can be naturally compiled into transformer networks whose behavior is completely explainable through the PSL programs they implement. But how general is the class of computations that can be expressed as PSL programs? In fact, every computable function can be computed by a PSL program: PSL is Turing complete. This is shown in Sec. 8. Hence, the Turing-completeness of hard-attention transformers shown by P\u00e9rez et al. (2021) can in effect be derived from the Turing-completeness of PSL. Thus this work speaks not only to how transformers can carry"}, {"title": "2. Related work", "content": "This work complements other ongoing work on understanding neural computation, and specifically, in-context learning in transformers. Unlike most mechanistic interpretability work, we are not analyzing trained models, probing for trees or other data structures. Our question is one of mechanistic computability: what can the transformer architecture compute in ICL, and exactly how? We focus on using ICL for the general but tightly constrained task of templatic text generation, as defined in 4.1. Our approach is to design an algorithm for performing templatic generation, in a symbolic form, which can be compiled into weights for a modified transformer architecture. The resulting network is, by construction, fully mechanistically interpretable.\nThe work pursued here is complementary to much existing work in another respect: we focus on NL-semantics-free, number-free ICL, as emphasized in Sec. 1.2."}, {"title": "2.1 Discovery of ICL", "content": "GPT-3: The concept of in-context learning was introduced by Brown et al. (2020), who showed that their GPT-3 transformer could solve new tasks from a few examples in the prompt, without any gradient updating or fine-tuning. Most tasks depended on text meaning, although one task required repairing syntactic errors. The tasks did not include any cases of the pure symbolic manipulation of Templatic Generation, where the generated text was required to be analogous to an example contained in the prompt in that both were generable from a common template, inserting different text strings to the template's slots."}, {"title": "2.2 ICL mechanisms", "content": "Induction heads: In their quest for mechanistic interpretability of in-context learning, Olsson et al. (2022) report the discovery of \u2018induction heads' in their various transformer models. These logical heads are actually specialized pairs of attention heads (in separate layers) that use past sequence pairs in the context to predict the successor to the current column. The pair consists of a previous token head and an actual induction head, with the second head consuming information produced by the first. The authors propose that induction heads might constitute the mechanism for the majority of all in-context learning in large transformer models; they present detailed (but indirect) evidence to support this hypothesis.\nIn our work, we perform templatic-production tasks using a more complex algorithm whose core includes induction-head-like patterns: it has specialized blocks (layers) to trans- form information from previous $(n \u2212 1)$ to current $(n)$ columns, and uses that information in subsequent blocks for various purposes, including searching for past occurrences of the current column symbol. In Olsson et al.'s transformers, the induction heads operate by increasing the probability of an outcome; in our transformers, this pattern of prefix matching and copying acts in a deterministic symbolic manner, matching abstract category labels as well as specific symbols. Our work also reflects an expansion of their idea of a residual stream,"}, {"title": "2.3 Influence of pre-training data", "content": "Training data requirements for ICL: Chan et al. (2022) establish 3 requirements for LM-training data to give rise to a model exhibiting ICL: data burstiness (not uniformly distributed), data with a large number of rarely occurring classes, and data with dynamic meaning. Using the Omniglot dataset, they show that ICL usually competes with \u201cin-weights learning\", with one of the two winning."}, {"title": "2.4 ICL function learning", "content": "Simple numerical functions: Garg et al. (2022) train decoder-only transformers from scratch to perform ICL in simple numerical function classes: linear functions, sparse linear functions, decision trees, and two-layer ReLU neural networks (using 20-40 shot examples"}, {"title": "2.5 ICL learning process", "content": "Generalization and Stability: Li et al. (2023) study the generalization and stability of transformers pre-trained using multi-task learning (MTL) with n-shot-style examples. Through proofs with mild assumptions, they obtain generalization bounds for ICL that depend on the stability of the transformer algorithm, showing that as the ICL prompt length increases, the ICL predictions become more stable. They also find that the transfer risks on unseen examples depend on the number of examples and complexity of the MTL tasks, but, surprisingly, not on the complexity of the transformer architecture.\nICL algorithms for linear regression: Akyurek et al. (2022) study linear regression, exploring whether ICL uses one of three known algorithms to learn the linear function latent in the examples in a given ICL prompt. They identify 4 operations over the hidden states of a transformer layer that can be implemented in a single transformer layer (move-subvector, matrix-multiply, scalar-divide, affine-transform) and prove by construction (programming the transformer with these 4 instructions) that a transformer can emulate 3 classical solutions to linear regression: gradient descent, closed-form ridge regression, and exact least-squares regression. They then pre-train transformers on linear regression tasks with an ICL-style objective and examples, and compare the resulting behavior to the 3 previously programmed models. They find that their trained transformers transition between the classical algorithms as depth and dataset noise vary, and converge to optimal Bayesian decisions as the width and number of layers grow.\nPAC in-context learnability: Wies et al. (2023) extend the PAC framework to prove, under mild assumptions, that ICL efficiently 'learns' tasks through examples. Their pre- training data is a mixture of multiple latent downstream tasks, presented in n-shot-style prompts, with consistent delimiters in each prompt. They conclude that \"in-context learning is more about identifying the task than about learning it\u201d [p. 1]. They find polynomial sample complexity in the number of shots per prompt."}, {"title": "2.6 Improving ICL", "content": "Meta training: Min et al. (2022) fine-tuned a GPT-2 large transformer tasks from 142 NLP datasets reformatted as ICL style tasks. This resulted in increased performance over baselines on the test set (containing 52 unique target tasks), sometimes beating models with 8x larger parameter count. Performance increased with the number and diversity of the fine-tuning tasks. Best performance resulted from fine-tuning with both instructions and"}, {"title": "2.7 Programming transformers", "content": "RASP language: Weiss et al. (2021) proposed the RASP programming language as a computational model for transformer encoders, which produce sequences of the same length as their input. RASP is based on the concept of manipulating sequences, starting with 2 inputs: token and index sequences. Sequences are then manipulated by a pair of select and aggregate operations (corresponding to the attention module) and by a variety of element-wise operators (corresponding to the MLP module). RASP programs can be compiled into a realized neural transformer. Weiss et al. showed how RASP programs could be written to solve several string-processing tasks, including computing, for each input token t, the number of distinct token types in the string that occur with the same frequency in the string as does t, and Dyck-k, detecting balanced brackets in a string with k distinct types of brackets. In our work, we use PSL, a language inspired by cognitive production systems and designed for transformer decoders. PSL produces variable length outputs, encodes multiple state variables that can be implemented in a transformer's residual stream, and is more aligned to our task of templatic text generation.\nRASP-L language: Friedman et al. (2023) proposed a type of discrete transformer whose weights can be translated into Python programs. These transformers are programmed using the new language called RASP-L, a variant of RASP. Their discrete transformer deploys a 'disentangled residual stream', encoding a discrete set of variables, either categorical or numeric. The Discrete-Attention-only Transformer (DAT) that we propose below at the implementation level of our Transformer Production Framework (TPF) is essentially their"}, {"title": "3. Motivating a case-study of in-context learning", "content": "We are attempting to understand how neural networks can perform symbolic computation, but just what is 'symbolic computation' in this context? To address this, this section follows the outline in (21).\n(21) Section outline\na. identify fundamental properties of symbolic computation [Sec. 3.1]\nb. present an illustrative in-context learning task Swap that calls on the functionality expressed in these properties [Sec. 3.2]\nc. preview the remainder of the paper, which presents a general function class that Swap exemplifies, the symbolic languages PSL and QKVL for expressing algorithms to compute these functions at two different levels of description, and a compiler that translates these programs into a novel type of transformer neural network. [Sec. 3.3]"}, {"title": "3.1 Fundamental properties of symbolic computation", "content": "As in Box 2 (1 \u2013 3), consider the following mapping, a simple instance of semantic parsing in which an English sentence in passive voice is mapped to a predicate-calculus-style logical form:\n(22) the program was translated by a compiler \u2192 translated(a compiler, the program)\nThis exemplifies a general schema or template (with variables in italics and constants in roman font):\n(23) x was V by y \u2192 V(y, x)\nThe variables take values that are strings of symbols: this binding of values to variables is of particular interest because it has been argued to be beyond the capabilities of neural networks (e.g., Marcus, 2001, but cf. Smolensky 1987, 1990).\nHenceforth we drop the punctuation marks used above to aid human readability, so the template becomes\n(24) x was V by y \u2192 Vy x\nThis can be cast in binary tree-to-tree form as an instance of a function we call Passive\u2192Logical:"}, {"title": "3.2 A case study: Swap", "content": "An instance of this more challenging type of task is Swap, which will provide a primary case study for the remainder of the paper. An illustrative prompt for Swap is given in (29a). The location of V in the answer substring has now been shifted relative to Passive\u2192Logical so that the pattern now more closely resembles the task Passive\u2192Active, where the passive form \"the program was translated by a compiler\" is mapped to the active form \"a compiler translated the program\": the linear positions of the subject and object have been swapped.\n(29) Instance of Swap\na. Prompt: QBCVDEADEVBCQFGVJKLA\nb. Continuation: JKLVFG\nThe template for Swap is simply\n(30) Swap template: Q x V y A y Vx\nThe symbols Q, V, and A function as fixed delimiters, delimiting the strings providing the values of the variable constituents (or \u2018slots', or 'arguments') x and y. We will be studying templates with varying numbers of constituents: Swap has 2 (30).\nTo make examples more transparent to readers, we will typically follow the convention used in (29a) according to which the value of a template slot (x or y here) is a string of individual characters in alphabetic sequence. The Swap task we study does not require this: aside from the reserved delimiter symbols Q and A which respectively initiate Question- and Answer-regions of the prompt, the identities of the individual symbols in the examples are arbitrary, of no relevance to the task; in particular, these symbols need not be single characters and could be words or non-alphanumeric symbols, as in (1).\nThe prompts we study are a concatenation of two strings. First, an initial question-answer example string, which we label X starting at a prompt-initial token of the reserved symbol Q and terminating before a second token of Q. Next is a continuation-cue string, which we label C: this consists of a Question-region string followed by the reserved symbol A, a prompt for completing an Answer-region; the continuation-cue string starts at the prompt's second Q and continues through the prompt-final A. Note that we will reserve the terms example and (continuation-)cue for this usage. When training or testing models on ICL, we will refer to the entire input as a 'prompt', reserving 'example' for the portion of the initial portion of the prompt that instantiates the template driving the prompt's continuation. (Thus training set size is measured by the number of \u2018prompt/completion' pairs it contains, rather than the number of 'examples'.)\nFor (29a), this structure is shown in (31) (and in more complete tree-form below in (32))."}, {"title": "3.3 A walk-through of the symbolic computation implicit in the Swap task", "content": "How to rationalize the continuation in (29)? Intuitively, in the example that initiates the prompt, we recognize the Q-region (QR) substring QBCVDE as instantiating the template QxVy of (30), and from this, recognize the A-region (AR) substring ADEVBCas instantiating AyVx. Then in the continuation cue, using the template given in the Q-region of the example, we recognize a new instance of the template in which x now has value FG while y now has value JKL. Inserting these values into the template for the example's A-region, AyVx, determines that the continuation from the final A should be JKLVFG.\nSupporting this intuitive analysis is the structure in (32). We now describe this structure's role in defining and performing the Swap task, identifying the essential roles in the algorithm of the key properties of symbolic computation spelled out in (26). In this informal section, we will make free use of the properties of the function class F providing the functional-level description of our TPF system (15a): these properties are formally defined below in (39), and (61) indicates how particular steps in the algorithm are justified by that formal definition.\n(31) Example-Cue structure of (29)\na. example X= QBCVDEADEVBC\nb. cue C = QFGVJKLA\nThe (prompt, continuation) pairs we study are the (input, output) pairs of some function in a class F we define below in (39). In the following informal discussion, when we mention some structural property of the prompt or continuation, that property is imposed by the definition of F."}, {"title": "4. TPF, functional level. A class of in-context learning tasks: templatic generation", "content": "Thus the Swap task implicitly incorporates 11 of the 13 properties of symbolic computation given in (26). This motivates the study of a class F of ICL functions including Swap which we formalize in this section. F provides the functional-level description of the Transformer\nProduction Framework TPF we now develop. We call F templatic generation tasks: TGT. (Recall Box 2.)\n4.1 Templatic generation defined\nEach input-output mapping in F defining an instance of our ICL templatic generation task is generated from an instance of the structural template in (38).\n(38) The prompt-continuation (input-output) structure of templatic generation F\nThe yield (sequence of terminal symbols) of the non-orange portion of (38) is the prompt P: the concatenation of the symbol sequences VFQ VC1 A. This is the input sequence to the function f \u2208 F being computed by our TPF system. The output f (P) is the continuation the yield of the orange portion: V C UD\n(39) specifies the tree template depicted in (38). [In square brackets are comments concerning the motivation for some of the specifications.] A formal grammar for the TGT is provided in App. D.\n(39) TPF at the functional level: the prompt-continuation (input-output) structure S\na. Region structure\ni. S is a sequence of two subconstituents: the example (X) followed by the continuation-cue (C). [A characteristic of typical \u20181-shot' ICL tasks.]\nii. Each of X and C is a sequence of two subconstituents called regions: the Q(uestion)-region (QR) followed by the A(nswer)-region (AR).\nb. Field structure\ni. Each region is a sequence of subconstituents called fields. A field may not appear more than once in a region. The sequence of fields comprising a QR constituent is the same for the QR within X and for the QR within C; the same is true for the AR. [This is the sense in which the continuation follows the template established by the example; the to-be-generated completion of the AR within C consists of the same sequence of fields as that of the AR within the example given in the prompt.]\nii. Fields fall into two classes: delimiters (D) and constituents (C).\niii. The sequence of fields constituting a region alternate between D and C fields."}, {"title": "4.2 Relevance of the Templatic Generation Task", "content": "We are studying how mechanisms within the transformer architecture enable advanced symbol processing, and we have seen through the case study of Swap how the Templatic Generation Task calls on most of the general capabilities involved in symbol processing. But is this task, as formalized above, a task that transformer models can actually perform? In this section we examine this question with respect to both pre-trained language-model transformers and transformers trained from scratch to perform the task.\n4.2.1 THE TGT DATASET\nIn order to assess how baseline models train and perform on templatic generation tasks, we created a synthetic dataset, generating prompts according to the TGT grammar and constraints outlined in App. D. Each line in a dataset split file contains a prompt and the associated correct completion. Note that, except where indicated otherwise, in this dataset, within a constituent, each symbol is a \u201crandom-letter \u2018word\u201d\u201d (\u2018rlw') a random 2-letter sequence of lower-case letters; within delimiters, each symbol is an individual special character see (10) and illustrations below. (Recall our \u2018NL-semantics-free' approach, Sec.1.2.) This dataset is publicly available on Hugging Face.6\nThe dataset consists of the following tasks:"}, {"title": "5. TPF, higher algorithmic level: the Production System Machine", "content": "Having documented the partial success achieved by transformers on the task defined by the functional level description of TPF systems presented in Sec. 4 the class F of templatic generation functions instantiated in the TGT dataset we now descend to the algorithmic level, which actually contains two sub-levels that are formalized as two symbolic abstract machines that compute the functions in F. Closest to the functional level is the"}, {"title": "5.1 The PSM architecture", "content": "The symbolic processing in both PSM and QKVM consists of two phases: prompt processing i.e., parsing followed by continuation generation. These phases are described in general terms for PSM in (40) and (41).\n(40) Production-System Machine state dynamics: Parallel processing of the prompt\na. Machine states\ni. The state of the Production-System Machine is a sequence of cell states.\nii. A cell is identified by its value of the variable position (or p), a natural number.\niii. A prompt containing P symbols is encoded in the cells in positions 1 through P, the prompt cells; the completion will be encoded in the subsequent completion cells.\nb. Cell-state structure\ni. Each cell has a state which is characterized by the values of a set of state variables, including position (p) and symbol (s). The possible values of each state variable form a discrete set.\nii. s[m] is the type of the symbol encoded in the cell with p = m.\niii. Other state variables are introduced below. Some of them are used to encode the parse tree structure (38), as visualized in (42) these are the structural variables: region (r), field (f) and index (d).\niv. The state of a cell is a state structure encoding the values of the state variables for that cell; some variables may have the null value nil. The space of all possible state structures is the state-structure space SSS.\nv. Notation. For any state structure $S \u2208 SSS$, let the value of state variable $x$ in $S$ be denoted $S.x$. When $S$ is understood, we abbreviate $S.x = a$ to $x : a$. If $S$ has, say, two state variables with non-null values, $S.x = a$ and $S.y = b$, we abbreviate $S$ itself to $x : a, y : b$.\nc. Layer structure\ni. The dynamics of the cells is unrolled in time, so that for each step of computation there is a layer of cells."}, {"title": "5.2 Swap in the PSM", "content": "In the PSM, the hierarchical structure (38) is encoded as in (42), with each row showing the values of the state variable named at the left. The state structure of the first cell is shown in (43).\n(42) PSM representation of the parse (35) of the prompt (29a)\nThe hierarchical structure encoded explicitly in the tree (38) is now encoded implicitly in the spans of the values of the structural variables: that the substring B C is the value of a field constituent (of type F1) is encoded by the field variable field (or f) having the same value (F1) for both B and C (and a different value for the preceding and following symbols). Similarly, that the prompt prefix QABVCD is the value of a region constituent is encoded by the region variable region (or r) having the same value, XQ, for all the symbols in that prefix. When implemented below in a transformer, this is the method for encoding hierarchy proposed in Hinton's (2023) GLOM model. (Extending this scheme to enable recursive structure faces a number of challenges, but see Sec. 9.3.1.) Note that we have not explicitly encoded the top-level constituents X and C here, as they are not useful for the algorithm we present below; rather, the Q- and A-regions of X are now called XQ and XA, while those of C are now CQ and CA.\nThe final structural variable, index (d), denotes the position of each symbol within its"}, {"title": "5.3 Generation algorithm GEN in the Production-System Language PSL", "content": "In Sec. 3.3.2 we informally introduced the two operations used during generation of the continuation string: CONTFIELD and NEXTFIELD. Which of these is executed depends on whether the most-recently-generated symbol is final in its field (calling for NEXTFIELD) or not (calling for CONTFIELD).\n5.3.1 CONTFIELD\nWe discuss CONTFIELD first because it is simpler than NEXTFIELD, although we need to jump ahead a step into the generation process to reach a point where CONTFIELD is the appropriate operation. So suppose the first continuation symbol J has just been generated (along with its structural variable values f = F2, r = CA, d = 0). J is not the final symbol in its field F2, so the next symbol must be generated by CONTFIELD, which performs the actions in (46); see the visualization in (45). This operation is justified by (39d-iv), which requires that in the notation of (37) UF2, the value of field F2 within CA, must equal UF2, the value of field F2 within CQ.\n(46) CONTFIELD in action\na. Current most-recently-generated symbol: Jin position N = 21; $s[N] = J$\nb. Match to symbol type Jin CQ: position $n_0 = 17$; $s[n_0] = s[N]$ (where $n_0$ is not field-final)\nc. Next position: $n = n_0 + 1 = 18$\nd. Symbol at position n: $s[n] = K$\ne. Update symbol $[N]$ to $K$ for subsequent propagation (41a) to the next position $N + 1 = 22$: set $s[N] = s[n] = K$"}, {"title": "5.3.2 NEXTFIELD", "content": "Because the newly-generated symbol K does not complete its field F2, the next step of generating the continuation also needs CONTFIELD; this generates the next symbol L (along with its structural-variables' values).\nL completes the F2 field"}]}