{"title": "Arabic-Nougat: Fine-Tuning Vision Transformers for Arabic OCR and Markdown Extraction", "authors": ["Mohamed A. Rashad"], "abstract": "We introduce Arabic-Nougat, a suite of OCR models designed to convert Arabic book pages into structured Markdown text. Building on Meta's Nougat architecture, Arabic-Nougat includes three specialized models: arabic-small-nougat, arabic-base-nougat, and arabic-large-nougat. These models are fine-tuned using a synthetic dataset, arabic-img2md, consisting of 13.7k paired samples of Arabic book pages and their Markdown representations. Key innovations include the Aranizer-PBE-86k tokenizer, which optimizes tokenization efficiency, and the use of torch.bfloat16 precision and Flash Attention 2 for efficient training and inference. Our models significantly outperform existing methods, with arabic-large-nougat achieving the highest Markdown Structure Accuracy and the lowest Character Error Rate. We also release a large-scale dataset of 1.1 billion Arabic tokens extracted from over 8,500 books using our SOTA model, providing a valuable resource for further Arabic OCR research. All models and datasets are open-sourced, and our implementation is available at https://github.com/\nMohamedAliRashad/arabic-nougat.", "sections": [{"title": "1 Introduction", "content": "The rapid digitization of information has heightened the demand for systems that can extract structured data from unstructured documents. Document parsing, which converts scanned or image-based documents into structured, machine-readable formats, is crucial for applications such as knowledge base creation, information retrieval, and training data generation. However, parsing documents in non-Latin scripts, especially Arabic, poses significant challenges due to the language's cursive script, contextual letter forms, and diverse text layouts [15, 4, 16].\nModern document parsing techniques fall into two categories: modular pipeline systems and end-to-end approaches. Modular systems decompose the parsing process into stages, including layout detection, text recognition, and relation integration, often using models like LayoutLM [15] or BERT-Grid [16] for semantic understanding. End-to-end models, such as Meta's Nougat [1], simplify this process by directly converting visual document representations into structured outputs using vision and language transformers. While these advancements have improved parsing capabilities for scientific and Latin-script documents, they do not adequately address the complexities of Arabic text and layouts.\nTo bridge this gap, we introduce Arabic-Nougat, a suite of OCR models tailored for extracting structured text in Markdown format from Arabic book pages. Building on Meta's Nougat architecture, Arabic-Nougat incorporates language-specific enhancements, including an advanced Arabic tokenizer and a specialized synthetic dataset, arabic-img2md. These adaptations address critical challenges in Arabic OCR, such as handling diverse text layouts, improving tokenization efficiency, and extending sequence lengths for processing lengthy documents.\nIn summary, our contributions are as follows:\n\u2022 We introduce three specialized models, arabic-small-nougat,\narabic-base-nougat, and arabic-large-nougat, designed to handle Arabic text parsing tasks with varying capacities and performance optimizations.\n\u2022 We present arabic-img2md, a synthetic dataset of 13.7k Arabic book pages paired with their Markdown representations, created using HTML scraped from the Hindawi website [7]. This dataset enables accurate and scalable Arabic OCR training and evaluation.\n\u2022 We release arabic-books, a large-scale dataset of 1.1 billion Arabic tokens extracted from"}, {"title": "2 Related Work", "content": "Document parsing, crucial for extracting structured information from unstructured documents, has seen significant advancements. This section reviews relevant methodologies, datasets, and recent advancements that informed the development of Arabic-Nougat."}, {"title": "2.1 Document Parsing Systems", "content": "Document parsing systems can be categorized into modular pipeline systems and end-to-end models. Modular systems decompose the task into stages such as layout detection, text recognition, and relation integration, often using models like LayoutLM [15] and BERTGrid [16] for semantic understanding. End-to-end models, such as Meta's Nougat [1], simplify this process by directly converting visual document representations into structured outputs using vision and language transformers. While these advancements have improved parsing capabilities for scientific and Latin-script documents, they do not adequately address the complexities of Arabic text and layouts."}, {"title": "2.2 OCR in Document Parsing", "content": "Optical Character Recognition (OCR) remains central to document parsing. Modern approaches leverage deep learning, particularly CNNs and Transformers. Models such as TrOCR [11] and Vision-LAN [10] have introduced encoder-decoder frameworks and multimodal pretraining, enhancing accuracy and context-awareness in OCR tasks. Specialized models for mathematical expressions and table recognition, like DS-YOLOv5 [6] and FormulaDet [17], highlight the increasing focus on domain-specific OCR capabilities. These models informed Arabic-Nougat's design, particularly its ability to handle the complexities of Arabic script and Markdown structure."}, {"title": "2.3 Datasets for Document Parsing", "content": "High-quality datasets are essential for training and evaluating document parsing models. Widely used datasets such as PubLayNet [9], FUNSD, and BCE-Arabic-v1 have supported advancements in layout analysis and OCR. Synthetic datasets like arabic-img2md, introduced in this work, build on these foundations by generating paired image-Markdown samples specifically for Arabic books, addressing gaps in Arabic OCR resources."}, {"title": "2.4 Challenges and Recent Advances", "content": "Despite notable advancements, challenges persist in document parsing, including handling dense layouts, diverse languages, and multi-modal data. Recent models like Donut [5], GoT [19], and Fox [20] incorporate large-scale pretraining on multimodal datasets to improve generalization across tasks, while unified frameworks such as Omni-Parser [18] aim to streamline OCR and structured data extraction. However, these models primarily cater to English and scientific texts, leaving a gap for applications in Arabic literature.\nThis gap motivated the development of Arabic-Nougat, which combines state-of-the-art architectural elements with Arabic-specific adaptations. By addressing the challenges of sequence length, tokenization, and hallucination, Arabic-Nougat contributes to the broader field of document parsing while focusing on underrepresented languages and formats."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Model Architecture", "content": "The Arabic-Nougat suite builds on Meta's Nougat architecture, using Donut vision encoder and MBart transformer-based decoder [5, 2]. We extend this framework for Arabic OCR with three models:\n\u2022 Arabic Small Nougat: A new Fine-Tune from nougat-small, supports up to 2048 tokens, optimized for smaller documents.\n\u2022 Arabic Base Nougat: A new Fine-Tune from nougat-base, supports up to 4096 tokens, employs torch.bfloat16 precision with Flash Attention 2.\n\u2022 Arabic Large Nougat: A new model with an expanded decoder and Aranizer-PBE-86k tokenizer, supports sequences equivalent to 32k tokens.\nFigure 1 provides a detailed overview of the Arabic-Nougat architecture. It illustrates the integration of the Donut Vision Encoder, which processes the visual input from Arabic book pages, with the MBART decoder, which generates the structured Markdown output. The Donut encoder converts the input images into a sequence of 588 tokens, where each token is a 1024-dimensional vector. This transformation is achieved through a series of downsampling operations: the input image size of 896\u00d7672 pixels is progressively reduced to 224x168, 112\u00d784, 56\u00d742, and finally 28\u00d721, resulting in 588 tokens. Specifically, the calculation is as follows: (896\u00d7672) \u2192 (224\u00d7168) \u2192 (112\u00d784) \u2192(56x42) \u2192 (28\u00d721) = 588 tokens. This encoded representation captures the visual features of the input images, which are then fed into the MBART decoder for text generation. The figure highlights key components such as the token processing pipeline, the use of the Aranizer-PBE-86k tokenizer, and the overall decoding process. This architecture is designed to efficiently handle the complexities of Arabic text, ensuring high accuracy and performance in OCR and Markdown conversion tasks."}, {"title": "3.2 Tokenizer Enhancements", "content": "The Aranizer-PBE-86k tokenizer, developed by riotu-lab, features an 86k vocabulary optimized for Arabic morphology. By representing one token as the equivalent of nearly four base Nougat tokens, it achieves higher efficiency in tokenization and processing of lengthy Arabic texts [12]."}, {"title": "3.3 Dataset Development", "content": "The primary dataset used for training, arabic-img2md [13], contains 13.7k paired samples of Arabic book pages and their Markdown representations. These pairs were generated by scraping HTML content from the Hindawi website, converting it to PDFs, and extracting Markdown text. This dataset was exclusively used to train arabic-base-nougat and arabic-large-nougat."}, {"title": "3.4 Training Strategy", "content": "Models were trained on multiple GPUs using torch.bfloat16 precision, gradient checkpointing, and accumulation steps to manage large batch sizes. A learning rate of 1 \u00d7 10\u20134 was used, and training was configured to run for a maximum of 100 epochs with an EarlyStopping callback to prevent overfitting. Flash Attention 2 enabled efficient memory usage, particularly for arabic-base-nougat and arabic-large-nougat [3]."}, {"title": "3.5 Comparison with the Base Nougat Models", "content": "While nougat-small and nougat-base tokenize sequences of up to 3584 and 4096 tokens, respectively, arabic-large-nougat supports up to 8192 tokens. This extended capability, combined with the Aranizer-PBE-86k tokenizer, provides a practical decoder context length equivalent to 32k tokens, making it ideal for longer Arabic texts."}, {"title": "4 Empirical Evaluation", "content": ""}, {"title": "4.1 Experimental Setup", "content": "To evaluate the performance of Arabic-Nougat models, we used a test set of 160 random, unseen Arabic book pages from arabic-img2md, paired with their Markdown representations. The evaluation metrics included - **Markdown Structure Accuracy (MSA):** The accuracy of extracted Markdown formatting. - **Character Error Rate (CER):** The percentage of incorrect characters in the extracted text compared to ground truth. - **Token Efficiency Ratio (TER):** The ratio of tokens produced by the tokenizer to ground truth tokens."}, {"title": "4.2 Results", "content": "We evaluated the performance of the Arabic-Nougat models against Meta's Nougat models using several key OCR metrics: BLEU Score, Character Error Rate (CER), Word Error Rate (WER), and Structure Accuracy. Metrics where higher is better are indicated with an upward arrow (\u2191), and those where lower is better are indicated with a downward arrow (\u2193). The results are shown in Table 1.\nAs shown in Table 1, we observe a clear performance gap between the Base Models (Meta's Nougat) and the Fine-tuned Arabic Models (Arabic-Nougat). The base models, originally trained for Latin-script documents, perform poorly on Arabic text, reflected in their BLEU scores of 0.0037 and 0.0094, and high CER and WER values.\nIn contrast, the fine-tuned Arabic Small Nougat model achieves a BLEU Score of 0.7565, with a remarkably low Character Error Rate (CER) of 0.0819 and Word Error Rate (WER) of 0.1523. The Arabic Base Nougat model achieves the lowest Word Error Rate of 0.1042, while the Arabic Large Nougat model achieves the highest Structure Accuracy (98.84%), making it suitable for handling complex documents with intricate layouts.\nThese results demonstrate that the Arabic-Nougat models are highly effective for Arabic OCR and Markdown extraction tasks, significantly outperforming models not specifically trained for Arabic text."}, {"title": "4.3 Evaluation Metrics", "content": "We evaluate the models based on the following metrics:\n\u2022 BLEU Score: Measures the overlap between the predicted Markdown text and the reference Markdown text, commonly used in machine translation tasks to assess text generation accuracy.\n\u2022 Character Error Rate (CER): The ratio of incorrect characters to the total number of characters in the reference text. A lower CER indicates better character-level accuracy.\n\u2022 Word Error Rate (WER): The ratio of incorrect words (substitutions, insertions, deletions) to the total number of words in the reference text. A lower WER indicates higher word-level accuracy.\n\u2022 Structure Accuracy: A custom metric that evaluates the similarity between the structure of the predicted Markdown and the reference Markdown, focusing on elements such as headers and lists."}, {"title": "4.4 Efficiency Comparison", "content": "arabic-large-nougat demonstrated superior efficiency, achieving a TER of 1.05 due to its advanced tokenizer, compared to 1.25 for arabic-small-nougat. Training in bfloat16 with Flash Attention 2 significantly reduced memory usage, enabling larger batch sizes and improved processing times."}, {"title": "4.5 Recommendations", "content": "For practical applications, we recommend using arabic-base-nougat for general text extraction tasks and arabic-large-nougat for lengthy or complex documents. A repetition penalty larger than 1 is suggested to mitigate repetition issues observed in the larger models."}, {"title": "5 Conclusion", "content": "In this paper, we introduced Arabic-Nougat, a family of OCR models designed to extract structured text from Arabic book pages into Markdown format. Building on Meta's Nougat architecture, we developed three models-arabicsmall-nougat, arabic-base-nougat, and arabic-large-nougat-optimized for Arabic script and layouts. Key innovations include the Aranizer-PBE-86k tokenizer, which enhances tokenization efficiency, and the arabic-img2md dataset, a synthetic resource designed to improve Arabic OCR performance [12, 13].\nOur experimental results demonstrate the effectiveness of Arabic-Nougat, with arabic-largenougat achieving the highest Markdown Structure Accuracy (94.7%) and lowest Character Error Rate (6.1%), surpassing its smaller counterparts. These results underscore the value of advanced tokenization and extended sequence lengths in handling complex and lengthy Arabic texts. Additionally, the open-sourcing of arabic-books, a 1.1 billiontoken dataset extracted from Arabic literature, provides a valuable resource for future research in Arabic NLP and OCR [14].\nDespite these advancements, challenges such as hallucination and repetition persist, requiring further exploration. By addressing these issues and continuing to refine our models, we aim to contribute to the broader field of document parsing and promote the digitization of underrepresented languages like Arabic."}, {"title": "6 Limitations", "content": "While Arabic-Nougat marks a significant advancement in Arabic OCR, several limitations remain:\n\u2022 Hallucination in arabic-small-nougat: The older arabic-small-nougat model occasionally generates irrelevant content, including nonexistent URLs or images, due to its early training methodology and smaller training dataset [13].\n\u2022 Repetition in larger models: Both arabicbase-nougat and arabic-large-nougat exhibit repetition issues, particularly in lengthy sequences. Although applying a repetition penalty can mitigate this, the problem remains an area for improvement in future training strategies [3].\n\u2022 Dataset Biases: The arabic-img2md dataset, derived from Hindawi's web content, may not generalize well to other domains of Arabic text, such as scientific, religious, or historical documents. Expanding the dataset to include diverse genres and styles is critical for improving model robustness [7]."}]}