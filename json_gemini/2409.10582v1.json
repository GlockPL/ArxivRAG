{"title": "WAVEMIXSR-V2: ENHANCING SUPER-RESOLUTION WITH\nHIGHER EFFICIENCY", "authors": ["Pranav Jeevan", "Neeraj Nixon", "Amit Sethi"], "abstract": "Recent advancements in single image super-resolution have been predominantly driven by token\nmixers and transformer architectures. WaveMixSR utilized the WaveMix architecture, employing a\ntwo-dimensional discrete wavelet transform for spatial token mixing, achieving superior performance\nin super-resolution tasks with remarkable resource efficiency. In this work, we present an enhanced\nversion of the WaveMixSR architecture by (1) replacing the traditional transpose convolution layer\nwith a pixel shuffle operation and (2) implementing a multistage design for higher resolution tasks\n(4x). Our experiments demonstrate that our enhanced model \u2013 WaveMixSR-V2 \u2013 outperforms other\narchitectures in multiple super-resolution tasks, achieving state-of-the-art for the BSD100 dataset,\nwhile also consuming fewer resources, exhibits higher parameter efficiency, lower latency and higher\nthroughput. Our code is available at https://github.com/pranavphoenix/WaveMixSR", "sections": [{"title": "1 Introduction", "content": "Single-image super-resolution (SISR) is a key task in image reconstruction, aiming to transform low-resolution (LR)\nimages into high-resolution (HR) by predicting and restoring missing details. This process requires capturing both\nlocal information and global context. Recent advancements in super-resolution, particularly with attention-based\ntransformers like SwinFIR [1] and hybrid attention transformer [2], have surpassed traditional CNN approaches due to\ntheir ability to capture long-range dependencies. However, transformers face challenges with quadratic complexity in\nself-attention, leading to high resource demands and requiring large datasets. To overcome this, token-mixer models"}, {"title": "2 Architectural Improvements", "content": "We have made significant improvements to the WaveMixSR model, focusing on two key aspects. First, we addressed\nhow the model handles SR tasks higher than 2\u00d7. In the original WaveMixSR [3], all SR tasks were performed by\ndirectly resizing the LR image to HR using a single upsampling layer. This layer relied on non-parametric upsampling\ntechniques, such as bilinear or bicubic interpolation, which limited the model's ability to fine-tune and optimize the\nSR process across different scales. Our approach involved transitioning from this single-stage design to a more robust\nmulti-stage design. In our new architecture, we introduced a series of resolution-doubling 2\u00d7 SR blocks, which\nprogressively doubles the resolution step by step. This multi-stage approach allows for better SR performance at higher\nscales while reducing resource consumption."}, {"title": "2.1 Multi-stage Design", "content": "We have made significant improvements to the WaveMixSR model, focusing on two key aspects. First, we addressed\nhow the model handles SR tasks higher than 2\u00d7. In the original WaveMixSR [3], all SR tasks were performed by\ndirectly resizing the LR image to HR using a single upsampling layer. This layer relied on non-parametric upsampling\ntechniques, such as bilinear or bicubic interpolation, which limited the model's ability to fine-tune and optimize the\nSR process across different scales. Our approach involved transitioning from this single-stage design to a more robust\nmulti-stage design. In our new architecture, we introduced a series of resolution-doubling 2\u00d7 SR blocks, which\nprogressively doubles the resolution step by step. This multi-stage approach allows for better SR performance at higher\nscales while reducing resource consumption."}, {"title": "2.2 PixelShuffle", "content": "We introduce a key modification to the WaveMixSR model by replacing the transposed convolution operation in the\nWaveMix blocks with a PixelShuffle [4] operation followed by a convolution layer (WaveMixSR-V2 block) as shown\nin Fig. 5. While the original WaveMixSR used transposed convolutions, which involved numerous parameters and\nhigh computational cost, PixelShuffle upsamples the image more efficiently by rearranging pixels from feature maps.\nThis significantly reduces the number of parameters, enhancing the model's efficiency. The subsequent convolution\nlayer after PixelShuffle allows the model to continue learning and refining features effectively. Moreover, PixelShuffle\navoids the checkerboard artifacts commonly introduced by transposed convolutions, producing smoother and more\nnatural-looking images while maintaining high-quality super-resolution outputs.\nIncorporating these improvements in the architecture has enabled WaveMixSR-V2 to achieve new state-of-the-art\n(SOTA) performance on the BSD100 dataset [5]. Notably, it accomplishes this with less than half the number of\nparameters, lesser computations and lower latency compared to WaveMixSR (previous SOTA)."}, {"title": "A Architecture", "content": ""}, {"title": "A.1 WaveMixSR-V2 Block", "content": "The core part of our super-resolution (SR) architecture are the WaveMixSR-V2 blocks shown in Fig. 4(c). We created\nthese by modifying the WaveMix [7] blocks used in WaveMixSR model [3]. We replace the transposed convolution\nused in WaveMix block with a PixelShuffle [4] operation followed by a convolutional layer.\nDenoting input and output tensors of the WaveMixSR-V2 block by xin and xout, respectively; the four wavelet\nfilters along with their downsampling operations at each level by Waa, Wad, Wda, Wdd (a for approximation, d for\ndetail); convolution, multi-layer perceptron (MLP), PixelShuffle, and batch normalization operations by c, m, p, and\nb, respectively; and their respective trainable parameter sets by \u03be, \u03b8, \u03c6, and y, respectively; concatenation along the\nchannel dimension by, and point-wise addition by +, the operations inside a WaveMixSR-V2 block can be expressed\nusing the following equations:\nX0 = c(xin, \u03be);\nxin \u2208 RH\u00d7W\u00d7C, X0 \u2208 RH\u00d7W\u00d7C/4\n\nx = [Waa(x0) Wad(x0) Wda(x0) Wdd(x0)];\nX\u2208 RH/2\u00d7W/2\u00d74C/4\n\nx = b(c(p(m(x, \u03b8), \u03c6), \u03be), y); x \u2208 RH\u00d7W\u00d7C\n\nXout = X1 + Xin;\nXout \u2208 RH\u00d7W\u00d7C\n\nThe WaveMixSR-V2 block extracts learnable and space-invariant features using a convolutional layer, followed by\nspatial token-mixing and downsampling for scale-invariant feature extraction using 2 dimenional-discrete wavelet"}, {"title": "A.2 2x SR Block", "content": "As shown in Fig. 4(b), the 2\u00d7 SR block has two paths one for handling the Y channel and another for the CbCr\nchannels of the input image. The Y channel is used for the path with learning using WaveMixSR-V2 blocks because the\nY channel contains most of the image details and is less affected by color changes. It first upsamples the image to HR\nsize using a parameter-free upsampling block using bilinear or bicubic interpolation. The output of upsampling block,\nis sent to a convolutional layer to increase the number of feature maps before sending it to the WaveMixSR-V2 blocks.\nWe connected L WaveMixSR-V2 blocks in series to create high-resolution feature maps. The output from the final\nWaveMixSR-V2 blocks is then passed through a convolutional layer which reduces the channel dimension and returns a\nsingle channel output.\nThe second parallel path takes the two CbCr channels and passes it through an upsampling layer where the resolution is\ndoubled. This HR CbCr channel is concatenated with the Y-channel output from the first path, thereby creating the\n3-channel YCbCr HR output, which is converted to RGB to obtain the doubled resolution output image."}, {"title": "A.3 WaveMixSR-V2", "content": "The LR input image in RGB space is first converted to YCbCr space before sending to the model as shown Fig. 4(a). It\nis then passed through a series of 2\u00d7 SR blocks. For 2\u00d7 SR, we use just one 2\u00d7 SR block. For higher SR tasks, we can\nmodify the network by adding as many 2\u00d7 SR blocks as required to achieve as much SR as needed, as adding one 2\u00d7\nSR block doubles the resolution. Finally, the output from 2\u00d7 SR blocks are converted back to RGB space to get final\noutput."}, {"title": "B Implementation Details", "content": "We used DIV2K dataset [17] for training WaveMixSR-V2. We did not employ any pre-training on larger datasets such\nas DF2K [17] or ImageNet [18] to compare the performance in training data-constrained settings. The performance of\nWaveMixSR-V2 was tested on four benchmark datasets \u2013 BSD100 [19], Urban100 [20], Set5 [21], and Set14 [22], .\nAll experiments were done with a single 48 GB Nvidia A6000 GPU. We used AdamW optimizer (\u03b1 = 0.001, \u03b2\u2081 =\n0.9, \u03b22 = 0.999, \u03f5 = 10\u22128) with a weight decay of 0.01 during initial epochs and then used SGD with a learning rate\nof 0.001 and momentum = 0.9 during the final 50 epochs [23, 24]. A dropout of 0.3 is used in our experiments. A\nbatch size of 1 was used when the full-resolution images were passed to the model and a batch size of 432 was used\nwhen images were passed as 64 \u00d7 64 resolution patches.\nThe LR images were generated from the HR images by using bicubic down-sampling in Pytorch. We used the full-\nresolution HR image as the target and generated the input LR image using down-sampling for each of the SR tasks.\nNo data augmentations were used while training the WaveMixSR-V2 models. Huber loss was used to optimize the\nparameters. We used automatic mixed precision in PyTorch during training. For the quantitative results, PSNR and\nSSIM (calculated on the Y channel) are reported.\nThe embedding dimension of 144 was used in WaveMixSR-V2 blocks. The convolutions layers before and after the\nWaveMixSR-V2 blocks which were used to vary channel dimensions employed 3 \u00d7 3 kernels with stride and padding\nset to 1 to maintain the feature resolution."}, {"title": "C Results", "content": "From Table 2, it is evident that WaveMixSR-V2 has achieved state-of-the-art (SOTA) performance in 2\u00d7 and 4\u00d7 SR\ntasks on the BSD100 dataset. WaveMixSR-V2 attains this superior performance while utilizing significantly smaller\nDIV2K training data compared to other models, which typically rely on the much larger DF2K and ImageNet datasets."}, {"title": "D Ablation Studies", "content": "We conducted experiments to check the performance of the WaveMixSR-V2 architecture when trained using a Generative\nAdversarial Network (GAN) framework, incorporating a relativistic discriminator [?] with residual connections. We\nalso experimented the impact of introducing Gaussian noise as an extra input channel alongside the RGB channels. The\nhypothesis was that this noise might enhance the HR quality by encouraging the model to incorporate higher frequency\ncomponents. The experiments used images from the DIV2K dataset resized to different dimensions for training and\nwere evaluated on benchmarks datasets."}, {"title": "D.1 WaveMixSR-V2 GAN", "content": "We conducted experiments to check the performance of the WaveMixSR-V2 architecture when trained using a Generative\nAdversarial Network (GAN) framework, incorporating a relativistic discriminator [?] with residual connections. We\nalso experimented the impact of introducing Gaussian noise as an extra input channel alongside the RGB channels. The\nhypothesis was that this noise might enhance the HR quality by encouraging the model to incorporate higher frequency\ncomponents. The experiments used images from the DIV2K dataset resized to different dimensions for training and\nwere evaluated on benchmarks datasets."}, {"title": "D.1.1 LoSS", "content": "In terms of loss functions, we used a combination of pixel loss (Lpixel), content loss (Lcontent) [25], and adversarial\nloss. The pixel loss was based on the Peak Signal-to-Noise Ratio (PSNR) to directly optimize the model for higher\nPSNR values. For content loss, a pre-trained VGG19 model was used as the feature extractor. Specifically, features\nwere extracted from the 5th convolutional layer before the 4th max-pooling layer of the VGG-19 model. The adversarial\nloss was computed using a binary cross-entropy loss with logits on the output of the discriminator, comparing the\npredictions for real and generated images. The adversarial loss was to guide the generator to produce more realistic\nimages, improving the fidelity of higher-frequency details.\nThe overall loss function combined pixel loss, content loss, and adversarial loss as follows:\nLoss = XoLpixel + X1 Lcontent + X2 Ladversarial\nwhere \u03bb\u03bf, \u03bb1 and 2 are hyperparameters that control the relative importance of content loss and adversarial loss,\nrespectively.\nThe variation in content loss ratios significantly influenced the model's performance. When the content loss ratio was\nreduced to zero, the model was able to focus more on low-frequency components, leading to an improvement in PSNR\nand SSIM values across the datasets. For instance, with an input resolution of 128\u00d7128, setting the content loss ratio to\nzero resulted in a noticeable increase in performance, as the model could better optimize for low-frequency details. In\ncontrast, when a content loss ratio was included, it forced the training slightly toward including more high-frequency\ncomponents. However, the WaveMixSR-V2 architecture struggled to learn these components effectively, leading to\nsub-optimal performance compared to the scenario where content loss was not used."}, {"title": "D.1.2 Gaussian Noise", "content": "Regarding the addition of Gaussian noise, the results were mixed. An improvement was observed when using an\ninput resolution of 64\u00d764, where the PSNR and SSIM values increased, suggesting that the noise channel helped the\nmodel capture finer details and potentially enhance the perceptual quality of the images. However, when we increased\nthe input resolution to 128\u00d7128, the inclusion of noise led to a slight decrease in performance. This indicates that\nthe effectiveness of adding noise may depend on the input resolution and the model's ability to utilize this additional\ninformation effectively."}]}