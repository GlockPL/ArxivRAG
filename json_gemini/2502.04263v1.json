{"title": "CROSS THE GAP: EXPOSING THE INTRA-MODAL MISALIGNMENT IN CLIP VIA MODALITY INVERSION", "authors": ["Marco Mistretta", "Alberto Baldrati", "Lorenzo Agnolucci", "Marco Bertini", "Andrew D. Bagdanov"], "abstract": "Pre-trained multi-modal Vision-Language Models like CLIP are widely used off-the-shelf for a variety of applications. In this paper, we show that the common practice of individually exploiting the text or image encoders of these powerful multi-modal models is highly suboptimal for intra-modal tasks like image-to-image retrieval. We argue that this is inherently due to the CLIP-style inter-modal contrastive loss that does not enforce any intra-modal constraints, leading to what we call intra-modal misalignment. To demonstrate this, we leverage two optimization-based modality inversion techniques that map representations from their input modality to the complementary one without any need for auxiliary data or additional trained adapters. We empirically show that, in the intra-modal tasks of image-to-image and text-to-text retrieval, approaching these tasks inter-modally significantly improves performance with respect to intra-modal baselines on more than fifteen datasets. Additionally, we demonstrate that approaching a native inter-modal task (e.g. zero-shot image classification) intra-modally decreases performance, further validating our findings. Finally, we show that incorporating an intra-modal term in the pre-training objective or narrowing the modality gap between the text and image feature embedding spaces helps reduce the intra-modal misalignment. The code is publicly available at:\nhttps://github.com/miccunifi/Cross-the-Gap.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years the availability of massive, pre-trained Vision-Language Models (VLMs) has enabled a wide variety of applications ranging from zero-shot image segmentation (Zhou et al., 2022a; L\u00fcddecke & Ecker, 2022) to visual question answering (Song et al., 2022; Parelli et al., 2023). These models are typically composed of independent image and text encoders which are simultaneously trained on massive corpora of image-text pairs to align the text and image embeddings of associated inputs. For example, the Contrastive Language-Image Pre-training (CLIP) model is trained on a corpus of 400M image-text pairs to map inputs from both modalities into a shared embedding space (Radford et al., 2021). CLIP is trained with an inter-modal contrastive loss that aims to maximize the similarity of corresponding image-text samples while minimizing the similarity with all the other examples within a batch.\nDespite CLIP's shared embedding space, visual and textual features lie in distinct regions. This phenomenon, known as the modality gap (Liang et al., 2022), originates from model initialization, and the inter-modal contrastive loss preserves and worsens it during training. Moreover, we note that CLIP's contrastive training strategy focuses on inter-modal (i.e. image-text) similarities between paired samples and disregards intra-modal (i.e. image-image and text-text) similarities. Consequently, the intra-image and intra-text similarities between CLIP representations might not faithfully correspond to those of the actual images or texts, as depicted in the left section of Fig. 1. We refer to this issue as intra-modal misalignment. A simple experiment aimed at quantifying this problem is presented in Appendix B."}, {"title": "2 RELATED WORK", "content": "Contrastively trained Vision-Language Models. VLMs have become increasingly popular for their ability to learn aligned representations across visual and textual modalities (Radford et al., 2021; Jia et al., 2021; Zhai et al., 2022; 2023; Mu et al., 2022; Li et al., 2021). This alignment enables VLMs to be used in a broad variety of downstream tasks, including image-text retrieval and zero-shot image classification, by projecting images and text into a shared feature space.\nThe most prominent example is CLIP (Radford et al., 2021), which maximizes the similarity between paired images and text captions while minimizing the similarity with the other samples in the batch. SigLIP (Zhai et al., 2023), on the other hand, employs a sigmoid-based contrastive loss that considers only the single image-text pairs while neglecting the other samples in the same batch. More recently, several approaches have extended the CLIP-style contrastive loss by incorporating intra-modal similarities into the training objectives (Mu et al., 2022; Li et al., 2021). For instance, SLIP (Mu et al., 2022) integrates a self-supervised component that maximizes the similarity between different augmentations of the same image, with a strategy akin to SimCLR (Chen et al., 2020).\nThe modality gap in multi-modal models. Liang et al. (2022) demonstrated a consistent phenomenon affecting VLMs known as the modality gap. This refers to the separation between feature embeddings of different modalities (e.g. text and images) within their shared representation space (Liang et al., 2022). The modality gap arises due to both model initialization and the contrastive learning objective used during training. At initialization, independent encoders for each modality produce embeddings that are restricted to distinct regions (or cones) within the representation space. During training, the contrastive learning process preserves and worsens this separation. Several works have studied the causes and implications of the modality gap in CLIP (Shi et al., 2023; Schrodi et al., 2024; Zhang et al., 2023). Schrodi et al. (2024) analyzed the embedding space and demonstrated that a minimal number of embedding dimensions \u2013 often as few as two \u2013 are sufficient to perfectly separate the image and text modalities.\nIntra-modal misalignment. Some studies have investigated the problem of misaligned intra-modal embedding distances within the context of zero- and few-shot image classification (Udandarao et al., 2023; Yi et al., 2024). To address this, Udandarao et al. (2023) propose mitigating the issue by computing similarities in the image-text space, rather than working exclusively with image embeddings, thereby leveraging the inter-modal nature of the feature representations. Similarly, CODER (Yi et al., 2024) introduces an enhanced image representation technique based on measuring distances between images and their neighboring texts within CLIP's embedding space.\nOur contribution with respect to the state-of-the-art. While these prior works have addressed various aspects of intra-modal and inter-modal relationships within VLMs, their scope remains limited, often focusing on specific tasks, datasets, or narrow perspectives on the modality gap and its effects. None of these studies comprehensively investigate the fundamental nature of the intra-modal versus inter-modal similarities across diverse tasks and datasets, nor do they fully explore the potential performance improvements achievable by leveraging inter-modal comparisons for intra-modal problems. The motivation behind our work is to shed light on the phenomenon of intra-modal misalignment, and its relationship to the modality gap, and to demonstrate the importance of either ensuring intra-modal alignment during pre-training or comparing solely representations that belong to different modalities."}, {"title": "3 CLIP PRELIMINARIES", "content": "CLIP is a vision-language model trained to align images and textual captions in a shared embedding space (Radford et al., 2021). It consists of an image encoder $f_{\\theta}$ and a text encoder $g_{\\phi}$. Given an image $I$, the image encoder extracts its feature representation $f_{\\theta}(I) \\in \\mathbb{R}^d$, where $d$ is the size of the shared embedding space. Likewise, for a given textual caption $Y$, first a word embedding layer $E_V$ maps each tokenized word to the token embedding space $V$. Then, the text encoder $g_{\\phi}$ generates the textual feature representation $g_{\\phi}(E_V(Y)) \\in \\mathbb{R}^d$.\nWhen using a Vision Transformer (ViT) (Dosovitskiy et al., 2020) as the visual encoder $f_{\\theta}$, the encoding process begins by splitting the image into $U$ fixed-size non-overlapping patches. Each patch is then transformed into a corresponding patch embedding {$w_1, w_2, ..., w_U$} through a linear projection by the patch embedding layer $E_W$, where each $w_i$ resides in the patch embedding space $W$. A learnable class (CLS) token $c$ is concatenated with the patch embeddings, resulting in the input to the vision transformer being $I = {c, w_1, w_2, ..., w_U}$. Finally, the CLS token of the final transformer layer is projected into the shared embedding space via a linear projection to obtain the final representation $f_{\\theta}([c, E_W(I)]) = f_{\\theta}(\\tilde{I}) \\in \\mathbb{R}^d$. For brevity, when unnecessary we will omit both the patch embedding layer $E_W$ and the token embedding layer $E_V$, and use the simplified notations $f_{\\theta}(I)$ instead of $f_{\\theta}([c, E_W(I)])$ and $g_{\\phi}(Y)$ instead of $g_{\\phi}(E_V(Y))$.\nGiven a batch of image-caption pairs $B = {(I_n, Y_n)}_{n=1}^N$, CLIP aims to maximize the cosine similarity for the $N$ correct pairs while minimizing it for the $N^2 - N$ other pairs. This is achieved by optimizing a symmetric, multi-class N-pair contrastive loss (Sohn, 2016). Let $\\psi_I = f_{\\theta}(I_n)$ and $\\psi_Y = g_{\\phi}(E_V(Y_n))$ denote the image and text embeddings, respectively. The CLIP loss is:\n$L_{CLIP} = \\frac{1}{N}\\sum_{n=1}^{N} \\log \\frac{exp(c(\\psi_I^n, \\psi_Y^n)/\\tau)}{\\sum_{m=1}^N exp(c(\\psi_I^n, \\psi_Y^m)/\\tau)} + \\log \\frac{exp(c(\\psi_I^n, \\psi_Y^n)/\\tau)}{\\sum_{m=1}^N exp(c(\\psi_I^m, \\psi_Y^n)/\\tau)} \\qquad (1)$\nwhere $c(.,.)$ denotes the cosine similarity, and $\\tau$ is a temperature parameter. As shown by Liang et al. (2022), Eq. (1) leads to a measurable separation between embeddings of the different modalities, creating what is known as the modality gap. This gap is significantly affected by the temperature $\\tau$, with a larger gap occurring as the temperature decreases.\nNote that CLIP's training loss focuses exclusively on inter-modal similarities between paired samples while neglecting intra-modal similarities. For example, consider an image feature anchor $\\psi_1$ and two distinct text features $\\psi_I^+$ and $\\psi_I^-$ expressing the same concept. The loss enforces both $\\psi_I^+$ and $\\psi_I^-$ to be at a cosine distance $r$ from $\\psi_1$, where the cosine distance is defined as $d(\\psi_A,\\psi_B) = 1 - c(\\Psi_A, \\psi_B)$. This is equivalent to $d(\\psi_1, \\psi_I^+) = d(\\psi_1, \\psi_I^-) = r$, meaning the text embeddings lie on a hypersphere of radius $r$ centered at $\\psi_1$. The absence of intra-modal constraints leaves the alignment between $\\psi_I^+$ and $\\psi_I^-$ remains uncalibrated; thus, we have $0 \\leq d(\\psi_I^+, \\psi_I^-) \\leq 2r$. This indicates that, while both text features are equidistant from the image feature, their intra-modal similarity is not constrained in any way, leading to intra-modal misalignment. We argue that such a misalignment must either be mitigated via additional intra-modal losses during pre-training or must be compensated by tackling intra-modal tasks inter-modally."}, {"title": "4 FROM INTRA-MODAL TO INTER-MODAL VIA MODALITY INVERSION", "content": "Due to the modality gap, images and text features lie in distinct regions in CLIP's shared embedding space. Previous work introduced modality inversion techniques to map features from the native modality to the complementary one (Ramesh et al., 2022; Patel et al., 2024; Li et al., 2023). For instance, Ramesh et al. (2022) trains a diffusion model to generate CLIP's image features from text captions for text-to-image generation.\nOur goal is to demonstrate that tackling intra-modal tasks in an inter-modal way outperforms intra-modal baselines. To this end, we propose to employ a modality inversion strategy to derive representations that exploit both native and complementary modality encoders. However, existing modality inversion techniques rely on external data or the training of a mapping network, making the inversion process dependent on external factors (Ramesh et al., 2022; Patel et al., 2024; Li et al., 2023)."}, {"title": "4.1 OPTIMIZATION-BASED TEXTUAL INVERSION (OTI)", "content": "Starting from an image $I$, OTI involves iteratively optimizing a set of $R$ pseudo-tokens $v^* = {v_1, v_2, ..., v_R}$, with $v_i \\in V$ for $i \\in {1, ..., R}$, for a given number of optimization steps $S$. We refer to $v^*$ as pseudo-tokens since they belong to the token embedding space $V$ but are not associated with any existing words. Algorithm 1 in Appendix A shows the pseudo-code of OTI.\nThe pseudo-tokens $v^*$ are randomly initialized and concatenated with the template sentence \"a photo of\" to form $Y_v^* = [E(\"a photo of\"), v^*]$ input into the CLIP text encoder $g_{\\phi}$ to obtain $\\psi_T = g_{\\phi}(Y_v^*)$. Then we extract the features of the image $I$ with the CLIP image encoder $f_{\\theta}$, resulting in $\\psi_I = f_{\\theta}(I)$. Since we aim to obtain a textual feature representation $\\psi_T$ that captures the informative content of $I$, we minimize the gap between image and text features via a cosine loss:\n$L_{cos} = 1 - c(\\psi_I, \\psi_T). \\qquad (2)$\nNote that while we adapt OTI from Baldrati et al. (2023) our goal is significantly different. Their work focuses on deriving a single pseudo-token that captures the informative content of the image $I$ and can interact with existing words to form meaningful sentences (e.g., \"a photo of v* that is running...\"), thus they use OTI for combining inputs from both modalities. In contrast, we use OTI purely as a mapping technique from visual to textual features. We do not focus on the pseudo-tokens themselves but aim to obtain a final representation that effectively captures the content of the image $I$. Additionally, the original OTI technique employs a regularization loss that exploits an auxiliary vocabulary to constrain the pseudo-token to reside in CLIP's token embedding space. However we are not interested in using the learned $v^*$ in different contexts \u2013 and more importantly, we aim to avoid influencing the inversion process with external data. For this reason we do not use a regularization loss."}, {"title": "4.2 OPTIMIZATION-BASED VISUAL INVERSION (OVI)", "content": "We propose the OVI approach to map text features from the CLIP text embedding space to the visual embedding space. Note that since OVI learns vectors of trainable parameters in the patch embedding space $W$, it can be applied only to ViT-based image encoders.\nGiven a sentence $Y$, we first extract its text features $\\psi_T = g_{\\phi}(E_V(Y))$. OVI then optimizes a set of $P$ randomly initialized pseudo-patches $w^* = {w_1, ..., w_P}$, where each $w_i \\in W$. This optimization is performed for a fixed number of optimization steps $S$. Similarly to the terminology introduced in Sec. 4.1, we refer to $w^*$ as pseudo-patches since they belong to the patch embedding space $W$ but are not associated with any existing image. Algorithm 2 in Appendix A illustrates the pseudo-code of the OVI method.\nSince the ViT employs learned positional embeddings, the number of input patches $U$ to the image encoder is fixed. Consequently, when $P < U$ directly using $w^*$ as input is impossible. In such cases, we apply nearest-neighbor interpolation to repeat the pseudo-patches and match the required number of $U$ patches."}, {"title": "4.3 CROSSING THE MODALITY GAP WITH OTI AND OVI", "content": "The goal of OTI and OVI is to map features from the native modality into corresponding features in the complementary modality. We observe that in cases where the loss $L_{cos}$ approaches zero, the complementary features converge to the native ones, thus drifting into the native modality embedding manifold. This undermines the goal of leveraging CLIP's image-text alignment.\nFor OTI, in our experiments the loss never approaches zero within a reasonable number of optimization steps \u2013 when considering a single pseudo-token (i.e. $R = 1$). We argue that this stems from the strong inductive biases of the frozen encoders and the modality gap, making it challenging for a single pseudo-token to bridge the distance between image and text representations. Nevertheless, the OTI-inverted features retain the informative content of the corresponding image. As a result, the potential drift related to $L_{cos}$ does not pose a significant issue, and inter-modal alignment is preserved. In all experiments we use $R = 1$ unless stated otherwise.\nAlso for OVI we observe that the loss only approaches zero when the number of pseudo patches $P$ is relatively large. Unlike OTI, we find that for some experiments a single pseudo-patch (i.e. $P = 1$) is insufficient for embedding the informative content of the corresponding text. We believe that this discrepancy stems from the inherent differences between images and texts. Specifically, in textual inputs a single word (or pseudo-token) can significantly alter the meaning of a sentence. For instance, the sentences \u201ca photo of a building\u201d and \u201ca photo of a dog\u201d convey completely different meanings, despite differing by only one word. In contrast, a single (pseudo-)patch has less influence on the overall semantic content of an image. Therefore, while a single pseudo-token is enough for an effective modality inversion with OTI, more pseudo-patches may be necessary when applying OVI. Consequently, in our experiments, we employ a number of pseudo-patches $P$ ranging from 1 to 4, based on the considered model (see Appendix D for more details). For such values, the pseudo-patches effectively embed the informative content of the input text. Moreover, the inter-modal alignment is maintained and the drift does not constitute a significant problem."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "Here we report on a broad range of experiments supporting our claims. We first evaluate two intra-modal tasks: image-to-image and text-to-text retrieval. We show that transforming intra-modal tasks into inter-modal ones via OTI and OVI consistently improves performance by better aligning with the original CLIP training objective. To confirm that this outcome does not stem from the modality inversion process itself, we evaluate a natively inter-modal task \u2013 zero-shot image classification \u2013 and show that making it intra-modal hinders the performance. Finally, we analyze the behavior of modality inversion techniques, and we study how adding intra-modal loss terms during VLM pre-training or narrowing the modality gap affects intra-modal misalignment. In the following, we denote as inter-modal approaches those involving inter-modal similarity comparisons, i.e. similarity comparisons between features of two different modalities (such as image-text, OTI-image, and OVI-text). Conversely, intra-modal approaches refer to methods that employ intra-modal similarity comparisons (such as image-image, text-text, OTI-OTI, and OVI-OVI).\nTo ensure a comprehensive analysis, we experiment using multiple CLIP models with different backbones and pre-training datasets. We also consider SigLIP to prove that our observations are not specific to the CLIP loss but generalize to other inter-modal contrastive losses. Specifically, we use OpenAI CLIP with ViT-B/32 and ViT-L/14 backbones, OpenCLIP (OPEN) pre-trained on the DataComp dataset (Gadre et al., 2024) with the same backbones, and SigLIP-B/16. Implementation details and description of all datasets used are given in Appendices A and F."}, {"title": "5.1 IMAGE-TO-IMAGE RETRIEVAL", "content": "Pre-trained CLIP image encoders are often used to extract features for image-to-image similarity comparisons. For this reason, we study the image-to-image retrieval task.\nExperiment design. The objective is to retrieve images from a gallery that are visually similar to a given query image. We consider a total of 15 datasets commonly employed for image-to-image retrieval and image classification. We consider two strategies. In the first, which we call intra-modal, we directly compare the features of the query image with those of the gallery. In the second, we transform the intra-modal image-to-image retrieval task into an inter-modal one by applying OTI to the query image. Then, we use the resulting OTI-inverted features to query the gallery.\nResults. We report the results in Tab. 1. Approaching the task inter-modally using the OTI-inverted features outperforms the intra-modal baseline, achieving an average absolute improvement ranging from 2% to 3%. Specifically, we observe that the performance gain is obtained across a large variety of datasets with different class granularity and diverse domains, spanning from birds (CUB) to monuments (RParis). Moreover, we notice that the intra-modal misalignment phenomenon is independent of the pre-training dataset (CLIP vs. OpenCLIP) and pre-training contrastive loss (CLIP/OpenCLIP vs. SigLIP) since the performance improvement is consistent across all the considered VLMs. Finally, we note that OTI-inverted features cannot contain more informative content than the native ones used by the intra-modal baseline \u2013 since they are obtained simply by mapping them to the complementary modality at a single-feature level and without using any external resources. Thus, the observed improvement is solely attributable to inter-modal alignment rather than to a more enriched representation."}, {"title": "5.2 TEXT-TO-TEXT RETRIEVAL", "content": "Although text features from pre-trained CLIP models are not commonly used for text-to-text tasks, we believe that it is important to show that our findings also apply to text-to-text comparisons.\nExperiment design. Using the CLIP text encoder for text-only tasks presents several challenges. Specifically, the CLIP text encoder is trained on short, descriptive texts. As a result, using it for tasks such as sentiment analysis or text classification, which involve longer texts and abstract concepts, results in a mismatch with the pre-training data. Moreover, VLMs have a limited input token capacity (e.g. 77 tokens for CLIP), which makes them unsuitable for longer texts. To avoid these problems, we formulate an intra-modal text-to-text retrieval task using image captioning datasets. Specifically, we select datasets in which each sample consists of an image and multiple associated captions (e.g. Flickr30K (Plummer et al., 2015)). These captions are comparable to those used in VLM training and are short enough to avoid token limit issues. We ignore the images and use the first caption associated with each image as the query text. The goal is to retrieve the other captions related to the same image from a gallery of all captions in the dataset. In Appendix G, we also report experimental results on purely textual text-to-text retrieval datasets. To address the issue of texts exceeding the input token limit of VLMs, we use an LLM to summarize the texts."}, {"title": "5.3 ZERO-SHOT IMAGE CLASSIFICATION", "content": "We evaluate the performance of modality inversion on inter-modal tasks, such as zero-shot image classification and image-text retrieval. We expect that transforming inter-modal tasks to intra-modal ones hinders performance due to intra-modal misalignment. Here we consider zero-shot image classification, while we report experiments on image-text retrieval in the supplementary material.\nExperiment design. CLIP-like models perform zero-shot image classification by predicting the output class based on the similarity between the input image and a set of textual prompts in the form of \u201ca photo of a [CLASS]\u201d, where CLASS represents each class name, such as \u201ccat\u201d or \u201cdog\u201d. Following Zhou et al. (2022b), we take into account 11 datasets (see the supplementary material for more details). We consider three strategies. The first is the inter-modal baseline, which compares the features of the input image and the set of prompts. In the second, we apply OTI to the input image. In the third, OVI is applied to each textual prompt.\nResults. In Tab. 2 (right) we report the performance of the first two strategies described above. Results for the third strategy are given in Appendix G. As expected, using modality inversion consistently leads to performance degradation across different VLMs and backbones. Note that the datasets used in zero-shot image classification are also employed for image-to-image retrieval in Sec. 5.1. This allows us to reuse the same OTI-inverted features for both tasks. Interestingly, the results are opposite: performance improves in image-to-image retrieval but decreases in zero-shot image classification. This contrast arises because, in the former, we transform an intra-modal task into an inter-modal one, while in the latter, we do the reverse. This experiment demonstrates that modality inversion does not inherently improve performance, as the same OTI-inverted features can either enhance or hinder results depending on the nature of the task."}, {"title": "5.4 ANALYZING MODALITY INVERSION", "content": "In this section we study how and why transforming native modality features into complementary ones via modality inversion leads to performance improvement on intra-modal tasks. For brevity, we consider only OTI, but we find that the same considerations apply to OVI. We consider the Cars dataset (Krause et al., 2013) and the CLIP ViT-B/32 model."}, {"title": "5.5 THE ROLE OF INTRA-MODAL CONSTRAINTS", "content": "We investigate whether incorporating an intra-modal loss term during image-text contrastive pre-training effectively mitigates the issue of intra-modal misalignment. To this end, we consider SLIP (Mu et al., 2022), which adds a self-supervised intra-modal loss based on SimCLR (Chen et al., 2020) to the standard CLIP inter-modal contrastive loss $L_{CLIP}$ (see Appendix C for more details). Such intra-modal loss encourages the model to produce similar representations for two augmentations of the same image, aiming to improve the intra-modal alignment within the image embedding space.\nTo verify this, we perform an image-to-image retrieval experiment following the evaluation protocol from Sec. 5.1. We report the results in Tab. 3. Notably, the OTI-inverted features achieve comparable performance to the native image ones. This contrasts with results from VLMs trained solely with an inter-modal contrastive loss (see Tab. 1), in which OTI led to a substantial performance boost. This experiment proves that SLIP's intra-modal loss effectively reduces intra-modal misalignment and suggests the importance of including such a loss when pre-training VLMs."}, {"title": "5.6 THE ROLE OF THE MODALITY GAP", "content": "During CLIP pre-training, the temperature parameter $\\tau$ in Eq. (1) critically affects the modality gap: higher temperatures considerably reduce or close it (Liang et al., 2022). To examine the impact of the modality gap on the intra-modal misalignment, we fine-tune a CLIP ViT-B/32 model on the COCO dataset (Lin et al., 2014) using a temperature $\\tau = 1.0$, which closes the modality gap. As a reference, we repeat the experiment with $\\tau = 0.01$, i.e. the value employed during CLIP pre-training. See Tab. A2 for more details on the magnitudes of the modality gap for the different models.\nWe reproduce our image-to-image retrieval experiments using these fine-tuned models and report results in Tab. 4. In the absence of the modality gap ($\\tau = 1$) tackling intra-modal tasks inter-modally does not improve performance. This shows that closing the modality gap reduces the intra-modal misalignment. The results of the reference model ($\\tau = 0.01$) prove that this outcome does not stem from the fine-tuning strategy. As also observed by Liang et al. (2022), we note that using higher temperature values during training leads to an overall performance decrease in downstream tasks, despite reducing the modality gap. For this reason, we argue that \u2013 in practice \u2013 simply increasing the temperature value in Eq. (1) does not represent a viable strategy to address intra-modal misalignment."}, {"title": "6 CONCLUSIONS", "content": "In this work we show that relying on intra-modal similarities computed with off-the-shelf VLMs is suboptimal for intra-modal tasks like image-to-image and text-to-text retrieval. This stems from the inter-modal contrastive loss employed for pre-training these models that leads to a modality gap and intra-modal misalignment. We propose to transform intra-modal tasks to inter-modal ones via two single-feature level modality inversion techniques. We demonstrate that this strategy improves performance as it exploits the inter-modal alignment of VLMs. Finally, we show that employing an intra-modal loss component during VLM pre-training or reducing the modality gap alleviates the impact of intra-modal misalignment.\nLimitations. Our analyses demonstrate the significance of intra-modal misalignment when exploiting pre-trained CLIP models, but fall short of offering practical alternatives. The modality inversion techniques we propose are computationally expensive. They are based on iterative optimization of learnable input parameters (150 optimization steps for OTI and 1000 for OVI in our experiments). This limits their practical applicability and future work should concentrate on efficient methods to mitigate the intra-modal misalignment."}, {"title": "APPENDIX A IMPLEMENTATION DETAILS", "content": "OTI and OVI. We report the pseudo-code of Optimization-based Textual Inversion (OTI) and Optimization-based Visual Inversion (OVI) in Algorithm 1 and Algorithm 2, respectively. Unless stated otherwise, we use the same hyperparameters for OTI and OVI. We employ the AdamW optimizer with learning rate equal to 0.02, $\\beta_1 = 0.9, \\beta_2 = 0.999$, and weight decay 0.01. We perform 150 optimization steps for OTI and 1000 steps for OVI. For OTI, we consistently use a single pseudo-token (R = 1). In contrast, for OVI, we employ a number of pseudo-patches P ranging from 1 to 4, depending on the considered model (see Sec. D for more details). On average, when using the CLIP ViT/B-32 model, OTI takes approximately 0.2 seconds per image, while OVI takes around 0.5 seconds per text prompt on a single A100 GPU (40GBs) with a batch size of 2048. The memory usage scales linearly with the batch size. Specifically, when using the CLIP ViT-B/32 model, OTI requires approximately 1,878 MiB plus 18.6 MiB per sample in the batch. For example, with a batch size of 128, the memory consumption is about 4,260 MiB. For OVI, the memory usage is approximately 2,218 MiB plus 16.2 MiB per sample, resulting in about 4,290 MiB with the same batch size. We use mixed precision to save memory and increase computational efficiency. In downstream tasks all the features are normalized to have a unit L2-norm.\nCLIP fine-tuning. To investigate the role of the modality gap on the intra-modal misalignment, we perform a fine-tuning of the CLIP model using different loss temperatures (see Sec. 5.6). In particular, we fine-tune the CLIP ViT B/32 model on the COCO training set for 30k steps, using a batch size of 512 and a learning rate of 1e-6. As an optimizer we employ AdamW with $\\beta_1 = 0.9, \\beta_2 = 0.98$ and a weight decay of 0.2. To mitigate possible overfitting issues, we train only the final projection layers. We train two different models, in the first we set the loss temperature parameter $\\tau = 1.0$ (first two rows of Tab. 4), while in the second we use $\\tau = 0.01$ (last two rows of Tab. 4)."}, {"title": "APPENDIX B MORE INSIGHTS ON INTRA-MODAL MISALIGNMENT", "content": "To provide quantitative insights into the intra-modal misalignment issue we conduct a simple experiment using the CLIP ViT-B/32 model and the \"Dogs vs Cats\" dataset (Elson et al., 2007). This dataset consists of 25K images evenly distributed between two classes: dog and cat. Our goal is to demonstrate that, despite inter-modal alignment, the intra-modal similarity scores are misaligned, i.e. they might not reflect those of actual images and texts, as illustrated in the left section of Fig. 1.\nWe start by filtering out images with incorrect inter-modal alignment to class-specific prompts. Specifically, we remove dog images that exhibit higher similarity to the prompt \u201ca photo of a cat\" than to the prompt \"a photo of a dog\". Then"}, {"title": "APPENDIX C ADDITIONAL VLMS", "content": "In this section", "is": "n$L_{SigLIP} = - \\frac{1}{N^2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\log \\bigg( \\frac{1}{1 + exp(- c(\\psi_i, \\psi_j) / \\tau + b)} \\bigg) \\qquad (4)$\nwhere $c(.,.)$ denotes the cosine similarity, $\\tau$ is a learnable temperature parameter, $b$ is a learnable bias, and $"}]}