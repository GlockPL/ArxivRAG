{"title": "Enhancing Transferability of Adversarial Attacks with GE-AdvGAN+: A Comprehensive Framework for Gradient Editing", "authors": ["ZHIBO JIN", "JIAYU ZHANG", "ZHIYU ZHU", "YUCHEN ZHANG", "JIAHAO HUANG", "JIANLONG ZHOU", "FANG CHEN"], "abstract": "Transferable adversarial attacks pose significant threats to deep neural networks, particularly in black-box scenarios where internal model information is inaccessible. Studying adversarial attack methods helps advance the performance of defense mechanisms and explore model vulnerabilities. These methods can uncover and exploit weaknesses in models, promoting the development of more robust architectures. However, current methods for transferable attacks often come with substantial computational costs, limiting their deployment and application, especially in edge computing scenarios. Adversarial generative models, such as Generative Adversarial Networks (GANs), are characterized by their ability to generate samples without the need for retraining after an initial training phase. GE-AdvGAN, a recent method for transferable adversarial attacks, is based on this principle. In this paper, we propose a novel general framework for gradient editing-based transferable attacks, named GE-AdvGAN+, which integrates nearly all mainstream attack methods to enhance transferability while significantly reducing computational resource consumption. Our experiments demonstrate the compatibility and effectiveness of our framework. Compared to the baseline AdvGAN, our best-performing method, GE-AdvGAN++, achieves an average ASR improvement of 47.8. Additionally, it surpasses the latest competing algorithm, GE-AdvGAN, with an average ASR increase of 5.9. The framework also exhibits enhanced computational efficiency, achieving 2217.7 FPS, outperforming traditional methods such as BIM and MI-FGSM. The implementation code for our GE-AdvGAN+ framework is available at https://github.com/GEAdvGANPP.", "sections": [{"title": "1 Introduction", "content": "The concept of adversarial attacks first emerged in the field of machine learning and has gained increasing attention with the rapid development of deep learning technologies and their widespread application across various domains. In particular, deep learning models have demonstrated remarkable capabilities in areas such as image recognition [3, 10, 15] and natural language processing [1, 13], often surpassing human performance in many cases. However, studies have shown that these high-performance models are extremely sensitive to small changes in input data, making them vulnerable to adversarial attacks [7]. These attacks involve subtle and almost imperceptible modifications to the input data, causing the model to produce incorrect outputs.\nTherefore, studying adversarial attacks is crucial for understanding the vulnerabilities of deep learning models. By exploring how to generate adversarial samples that can deceive models, researchers can gain deeper insights into the decision boundaries and learning mechanisms of these models, as well as reveal the inner workings of the models [23, 31, 35]. These insights are essential for enhancing the robustness of models, making them more secure and reliable, and advancing the theory of deep learning.\nCurrently, adversarial attacks can be categorized into white-box and black-box attacks based on the amount of information available to the attacker [11]. White-box attacks assume the attacker has complete knowledge of the victim model, including its architecture, parameters, and training dataset [7], while black-box attacks are conducted without any internal knowledge of the model, making them more representative of real-world scenarios [21]. Within black-box attacks, there are further subdivisions based on the attack initiation method: query-based attacks and transferability attacks [12].\nQuery-based attacks typically involve querying the model to simulate the victim model, and then using this simulated model to attack the victim model. However, this method has an inherent disadvantage as it requires frequent access to the target model, which reduces the stealthiness of the attack [12]. In this paper, we focus on the other type of black-box attack, namely transferability attacks. The principle behind this approach is to use a surrogate model to generate adversarial samples that are also effective against the victim model. Compared to query-based attacks, transferability attacks are more stealthy and pose a greater threat in real-world attack scenarios. Therefore, our primary focus in this paper is on transferability attack methods. As shown in Figure 1, the process of a transferability attack is illustrated. First, the original image is input into the source model, generating adversarial examples by attacking this source model. Subsequently, these examples are transferred to the target model, and their impact is evaluated again. This process demonstrates how adversarial examples crafted for one model can affect other black-box models without access to internal information, providing a clear illustration of the concept of transferability-based attack methods."}, {"title": "2 Background", "content": "In this section, we discuss the current state of adversarial attacks, focusing on their principles, advantages, and limitations, as summarized in Table 1. We first introduce some classic white-box adversarial attack methods, followed by a detailed overview of black-box transferability-based adversarial attacks. Finally, we examine generative structure-based attack methods."}, {"title": "2.1 White-box Adversarial Attacks", "content": "White-box attacks are crucial in the study of adversarial attacks as they allow attackers full knowledge of the target model's architecture and parameters. Notable white-box attack methods include the Fast Gradient Sign Method (FGSM) [7] and its iterative version, the Basic Iterative Method (BIM) [14]. FGSM constructs perturbations by leveraging the gradients of the loss function with respect to the input data, offering a fast but sometimes suboptimal attack. To enhance attack effectiveness, the BIM method was proposed, which iteratively adjusts the input data along the direction of the gradient sign, thereby increasing the attack success rate. Following FGSM and BIM, Madry et al [19]. introduced the Projected Gradient Descent (PGD) attack, a more robust and powerful white-box method. PGD can be seen as an extension of BIM, performing a more exhaustive search within an e-ball around the original input, addressing a constrained optimization problem aimed at maximizing model loss. Additionally, the C&W [2] attack generates minimal and effective perturbations by constructing and optimizing a new objective function, which is also utilized in AdvGAN [32]. These white-box methods have laid the foundation for subsequent research in adversarial attacks and defenses."}, {"title": "2.2 Black-box Adversarial Attacks", "content": "In the study of black-box adversarial attacks, transferability-based methods can be categorized into five main types based on their principles [11]. The first type is generative architecture methods, which use Generative Adversarial Networks (GANs) [6] or similar generative models to create adversarial samples transferable across different models, emphasizing the rapid generation of new adversarial samples once the generator is trained. The second type, semantic similarity methods, focuses on maintaining semantic consistency by attacking samples semantically related to the original ones, thereby extending the transferability of adversarial samples. Gradient editing methods generate more effective adversarial samples on target models by analyzing and adjusting gradient information, making the generated samples more stealthy and transferable. Target modification methods exploit similar features across different models, such as model interpretability similarities, to achieve adversarial sample transfer by directly attacking these similar features, thus enhancing transferability. Lastly, ensemble methods combine feedback from multiple models to generate adversarial samples, improving transferability. However, acquiring suitable multiple surrogate models is challenging in practice, and comparing multiple surrogate models with a single model is unfair. Additionally, using multiple models consumes more computational resources and time, so this category is not within the scope of our research."}, {"title": "2.2.1 Gradient Editing", "content": "In the domain of adversarial attacks, gradient editing techniques have emerged as new strategies to improve the precision and stealth of attacks. The Momentum Iterative Fast Gradient Sign Method (MI-FGSM) [4] and Gradient Relevance Attack (GRA) [36] are two prominent examples of this approach.\nMI-FGSM [4] is an enhancement of the Basic Iterative Method (BIM). It accumulates the gradient information from previous steps in each iteration, thereby enhancing the transferability of the adversarial samples. This momentum accumulation method improves the robustness of the attack, making the generated adversarial samples more transferable across different models, and thus harder for model defenses to detect and resist. The advantage of MI-FGSM is its improved transferability and ability to avoid local optima during attacks through the momentum mechanism. However, in some cases, over-reliance on historical gradients may lead to adversarial samples that are overly optimized for the source model, potentially reducing their effectiveness on target models.\nOn the other hand, GRA [36] adopts a different perspective, utilizing the correlation of input image gradients to adaptively adjust the update direction. GRA identifies the correlation among the gradients of input images, treating the current gradient as a query vector and neighboring gradients as key vectors, establishing a relationship through cosine similarity. This method can adaptively determine the update direction based on the gradient neighborhood information, thus generating more effective adversarial samples. The main advantage of GRA is its adaptive correction of the update direction using neighborhood gradient information, increasing the effectiveness of generated adversarial samples. However, the drawback is increased computational complexity, especially when handling large models and complex datasets."}, {"title": "2.2.2 Semantic Similarity", "content": "In the field of adversarial attacks, research on semantic similarity focuses on generating adversarial samples that can deceive deep learning models while preserving the semantic content of the input data. This type of method aims to create adversarial samples that are virtually indistinguishable from the original samples to human observers but can cause deep learning models to make incorrect predictions. Key methods in this domain include Translation-Invariant FGSM (TI-FGSM) [5], Diversity Input FGSM (DI-FGSM) [33], Scale-Invariant Nesterov Iterative FGSM (SI-NI-FGSM) [16], Frequency-based Stationary Point Search (FSPS) [39], and Structure-Invariant Attack (SIA) [29].\nTI-FGSM [5] improves attack stealth and effectiveness by enhancing the translation invariance of images. In each iteration, TI-FGSM performs multiple random translations of the image, computes gradients on these translated images, and synthesizes these gradients to generate translation-invariant adversarial perturbations. This approach ensures that the adversarial samples maintain their effectiveness even after spatial transformations, such as translations, by the model, enhancing the adaptability and stealth of the samples. However, this method increases computational cost due to the need for multiple translations and gradient computations compared to traditional FGSM methods.\nDI-FGSM [33] enhances the transferability of adversarial samples by introducing input diversity. In each iteration, DI-FGSM applies random transformations, such as scaling and cropping, to the original image and computes gradients on these transformed images to update the adversarial sample. This method generates adversarial samples that are effective across different models and preprocessing steps, enhancing sample generalization and stealth. However, the effectiveness of DI-FGSM heavily depends on the choice of input transformations and parameter settings, requiring careful design for optimal results.\nSI-NI-FGSM [16] combines scale invariance and Nesterov accelerated gradient descent to improve the transferability and efficiency of adversarial samples. By performing multi-scale transformations on the original image and averaging the gradients, SI-NI-FGSM ensures that the generated adversarial samples maintain their effectiveness across different scale transformations, demonstrating high robustness and stealth. The use of Nesterov acceleration also makes the adversarial sample generation process more efficient, though the multiple parameter adjustments involved add complexity.\nFSPS [39] explores stationary points on the loss curve as starting points for attacks and optimizes the attack direction through frequency domain search. This method aims to find the optimal local solutions to improve attack effectiveness and adaptability. The FSPS method is characterized by its ability to quickly locate efficient adversarial sample generation paths, though this frequency domain exploration-based approach is relatively computationally expensive and dependent on the model's frequency response characteristics.\nSIA [29] applies different transformations to local regions of the input image, aiming to increase sample diversity while maintaining the overall structure of the image, thus enhancing the stealth and effectiveness of adversarial samples. This method focuses on introducing effective perturbations while preserving semantic information, representing an innovative attempt in adversarial sample generation strategies. However, the challenge of SIA lies in balancing the degree of local transformations and maintaining global semantics, as well as the associated computational cost."}, {"title": "2.2.3 Target Modification", "content": "Target modification techniques in the field of adversarial attacks focus on generating more precise and difficult-to-detect adversarial samples by meticulously analyzing and adjusting attack strategies. These methods pay special attention to the importance of specific features or neurons within the model, guiding the generation of adversarial perturbations based on these factors to enhance attack effectiveness and transferability. In this area, methods such as Feature Importance-Aware Attack (FIA) [30], Neuron Attribution-based Attack (NAA) [34], Double Adversarial Neuron Attribution Attack (DANAA) [12], and Momentum Integrated Gradients (MIG) [18] demonstrate the potential of target modification to enhance adversarial attack performance.\nThe FIA method [30] is a feature importance-based attack strategy that selectively applies perturbations to features that significantly contribute to the model's decision-making process. This strategy not only improves attack efficiency but also enhances the transferability of adversarial samples across different models. However, accurately assessing feature importance may come with high computational costs, especially when dealing with complex models.\nThe NAA method [34] delves deeper into the model's internal structure, particularly neuron attribution, to generate more targeted adversarial samples. NAA focuses on the most critical parts of the model's decision-making process, resulting in adversarial samples with better transferability across multiple models. However, the computational com- plexity of this method is relatively high, especially when fine-grained analysis of the model's internal structure is required.\nDANAA [12] further extends neuron attribution-based attack methods by employing a double attribution mechanism and using nonlinear paths to evaluate neuron importance, generating more effective adversarial samples. This method provides new perspectives for understanding and leveraging the internal mechanisms of deep learning models, but its implementation complexity also increases, potentially requiring more computational resources and time to generate adversarial samples.\nMIG [18] employs an integrated gradients-based approach, emphasizing the similarity of gradients across different models and optimizing the generation of adversarial perturbations through momentum-like accumulation of previous and current gradients. This method enhances the transferability and success rate of adversarial samples, although the computation of integrated gradients may increase the complexity of the attack process."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Definition of Adversarial Attack", "content": "An adversarial attack can be defined as an optimization problem where the goal is to find the smallest perturbation \u03b4 such that the model f misclassifies the perturbed input x + 8, compared to the original input x. Specifically, the objective is to identify a minimal 8 such that f(x + 8) \u2260 y, where y = f(x) represents the original label predicted by the model for the input x. The mathematical formulation is as follows:\n$\\begin{aligned} &\\underset{\\delta}{\\text{minimize}} \\quad ||\\delta||_{p} \\\\ & \\text{subject to} \\quad f(x+\\delta) \\neq f(x) \\\\ &\\qquad \\qquad x+\\delta \\in \\mathcal{X} \\end{aligned}$\nwhere x is the original input, & is the perturbation added to x, f is the model under attack, and X denotes the valid input space. The term ||8||p represents the p-norm of 8, with common choices for p being 1, 2, or \u221e. The choice of p affects the sparsity or size of the perturbation, ensuring that the perturbation is minimal while still causing the model to produce incorrect predictions."}, {"title": "3.2 Mathematical Definition of AdvGAN", "content": "AdvGAN [32] aims to train a generative model G that outputs a perturbation 8 such that the perturbed input x + 8 can deceive the classifier f. During training, both the generator G and a discriminator D are optimized, where D is tasked with distinguishing real samples from adversarial samples generated by G. The optimization objective can be expressed"}, {"title": "3.3 GE-AdvGAN", "content": "GE-AdvGAN [37] modifies the gradient update mechanism of the generator G to adjust the adversarial perturbation 8 = G(x) it generates. This approach involves two key techniques: frequency domain exploration and gradient editing. In the frequency domain exploration step, the input sample x and the generated perturbation G(x) are transformed into the frequency domain, typically using the Discrete Cosine Transform (DCT) [9]. This transformation allows for the analysis and modification of characteristics at different frequencies. By adjusting the amplitude and phase of the frequency components, a series of approximate samples xf are generated, which are used in training to further optimize the generator G.\nThe gradient editing process targets the parameter updates of the generator G. Specific gradient editing techniques, as shown in the following expression, are used to directly modify the gradients affecting the updates of G's parameters:\n$W_{G}=W_{G}-\\eta_{g e} \\cdot\\left(\\frac{\\partial L}{\\partial(x+G(x))} \\frac{\\partial(x+G(x))}{\\partial G(x)} \\frac{\\partial G(x)}{\\partial W_{G}}+\\alpha \\frac{\\partial L_{G A N}}{\\partial W_{G}}+\\beta \\frac{\\partial L_{\\text {hinge }}}{\\partial W_{G}}\\right)$\nwhere ge is the target gradient calculated based on the frequency domain analysis results, replacing the original gradient calculation $W_{G}$.\nTo implement GE-AdvGAN, the sample x is first transformed into the frequency domain using Discrete Cosine Transform (DCT), and variant samples xf are generated based on different frequency sensitivities. These samples are then used to compute the target gradient ge, which feeds back into the training process of the generator G. The gradient ge is the average of gradients corresponding to multiple xf, optimizing the generation of adversarial samples. Notably, in this study, the ge component can be replaced with any other gradient information, and apart from the frequency domain, more dimensions of information can be explored."}, {"title": "3.4 Gradient Editing with Additional Information", "content": "As previously mentioned, gradient editing can modify the direction of perturbation learned by the generator, thereby enhancing the transferability of generated adversarial samples. Currently, there are various methods for gradient editing, according to [11], including five different types of adversarial attacks: Generative Architecture, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach.\nOur experiments reveal that gradients from the Semantic Similarity category are more easily learned by the generator. As discussed earlier, the focus of this paper is on the Generative Architecture type of attacks due to their computational speed and efficiency. The Ensemble Approach, on the other hand, is challenging to deploy in real-world scenarios as it requires multiple surrogate models, which are difficult to obtain. Moreover, using multiple surrogate models is unfair compared to single-model methods.\nIn this paper, we integrate Gradient Editing, Semantic Similarity, and Target Modification methods with our previously developed GE-AdvGAN framework. We explore the scalability of GE-AdvGAN by incorporating more gradient editing techniques, enhancing the framework's capability to edit gradients. The following sections provide the mathematical definitions of Gradient Editing, Semantic Similarity, and Target Modification methods, with gradient editing techniques designated as GE(.). For clarity, we use $g^{(k+1)}$ to denote components in the equations that can be incorporated into the GE-AdvGAN gradient editing framework. These components allow for the generation of adversarial samples using the following formula:\n$\\hat{x}_{f}^{(k+1)}=\\operatorname{Clip}_{x_{0}, \\hat{x}_{f}}\\left(x^{(k)}+\\alpha \\operatorname{sign}\\left(g^{(k+1)}\\right)\\right)$"}, {"title": "3.4.1 Mathematical Definition of Gradient Editing", "content": "Gradient Editing type adversarial attacks primarily optimize the objective function by adjusting the gradient $\\nabla_{x} L(f(x), y)$ with respect to the input x to generate adversarial samples. The editable part of such methods can be mathematically expressed as follows:\n$g_{e}=\\nabla_{x} L\\left(f\\left(x^{(k)}\\right), y\\right)$\nGEBIM & GEPGD. First, we combined the classical Gradient Editing methods BIM and PGD with our GE-AdvGAN+ framework. Although their mathematical principles are essentially the same, their initial inputs $\\hat{x}_{f}^{(0)}$ differ; in BIM, $\\hat{x}_{f}^{(0)}=x$, starting from the original sample x, while PGD starts from a point $\\hat{x}_{f}^{(0)}=x+\\text{Uniform}(-\\epsilon, \\epsilon)$, with random perturbations around the original sample. Furthermore, PGD restricts each gradient update within a norm ball to ensure the perturbation does not exceed the preset maximum range, as shown in Equation 5. The gradient information for both methods is obtained via $\\nabla_{x} L\\left(f\\left(x^{(k)}\\right), y\\right)$, which integrates with our framework's editing method.\n$\\begin{aligned} &g^{(k+1)}=\\nabla_{x} L\\left(f\\left(x^{(k)}\\right), y\\right) \\\\ &\\hat{x}^{(k+1)}=\\operatorname{Clip}_{x,\\epsilon}\\left(\\hat{x}^{(k)}+\\alpha \\operatorname{sign}\\left(g^{(k+1)}\\right)\\right) \\end{aligned}$\nGEMIM. Next, we incorporated MIM [4] into our framework. In Equation 6, $g^{(k)}$ represents the accumulated gradient (momentum term) from the k-th iteration. The momentum factor u, typically set between 0.9 and 1.0, balances the influence of previous and current gradients. The $g^{(k+1)}$ can be integrated with the GE-AdvGAN framework."}, {"title": "3.4.2 Semantic Similarity", "content": "Semantic Similarity-based adversarial attacks focus on enhancing transferability by altering input data through data augmentation techniques to construct samples semantically similar to the original data. The part where gradient editing can be performed is mathematically defined as follows:\n$g_{e}=\\nabla_{x} L\\left(f\\left(D_{p}\\left(x^{(k)}\\right)\\right), y\\right)$\nwhere Dp represents a series of parameterized data augmentation operations such as random scaling, cropping, and translation.\n3.4.3 GEDIM. In Semantic Similarity-based adversarial attacks, we first combined the typical DIM [33] method with our approach. In Equation 9, $D_{p}$ denotes random transformations applied to the input $x^{(k)}$, such as random scaling and cropping, which aim to minimally affect the original image semantics, with p as the transformation parameter. The $g^{(k+1)}$ part can be integrated with the GE-AdvGAN+ framework.\n$g^{(k+1)}=\\mathbb{E}_{p}\\left[\\nabla_{x} L\\left(f\\left(D_{p}\\left(x^{(k)}\\right)\\right), y\\right)\\right]$\nGETIM. TIM [5] can also be incorporated into our framework. In Equation 10, Tr denotes the translation transformation (a geometric operation that shifts an image from one location to another without altering its orientation, size, or shape) applied to the input image $x^{(k)}$, with t as the translation parameter. $\\mathbb{E}_{\\tau}$ denotes the expectation over all possible translations $\\tau$, averaging the gradients across multiple translations.\n$g^{(k+1)}=\\mathbb{E}_{\\tau}\\left[\\nabla_{x} L\\left(f\\left(T_{\\tau}\\left(x^{(k)}\\right)\\right), y\\right)\\right]$"}, {"title": "GESINIM", "content": "Next, we integrated SINIM [16] with our framework. In Equation 11, N(0, \u03c3) denotes Gaussian noise with a mean of 0 and standard deviation o, injected into the adversarial sample $\\hat{x}_{f}^{(k)}$ at each iteration. This noise does not affect the semantic meaning of the original image. The $g^{(k+1)}$ component fits well with our GE-AdvGAN+ framework.\n$g^{(k+1)}=\\mathbb{E}_{\\sigma}\\left[\\nabla_{x} L\\left(f\\left(\\hat{x}^{(k)}+N(0, \\sigma)\\right), y\\right)\\right]$\nGEFSPS. In this section, we integrate FSPS [39] with the GE-AdvGAN+ framework, denoting this gradient editing method as GEFSPS. In Equation 12, k represents the current iteration step, and w is a predetermined threshold for a warm-up step, determining when to transition from the adversarial sample to a stationary point as the attack starting point. x[k < w] indicates using the current adversarial sample x before the warm-up step w, while sp[t \u2265 w] indicates using the stationary point sp after the warm-up step w. The gradient information $g^{(k+1)}$ integrates with our GE-AdvGAN+ framework. Notably, after applying FSPS's gradient information computation method, the transferability of adversarial attacks learned by the GE-AdvGAN+ framework is strongest, leading us to designate it as GE-AdvGAN++.\n$\\begin{aligned} &x_{\\text {dct }}=\\operatorname{DCT}\\left(x[k<w]+s_{p}[k>w]+N(0, \\sigma) / 255\\right) \\\\ &x_{\\text {idct }}=\\operatorname{IDCT}\\left(x_{\\text {dct }} * N(1, \\sigma)\\right) \\\\ &g^{(k+1)}=\\mathbb{E}\\left[\\nabla_{x} L\\left(x_{\\text {idct }}, y\\right)\\right] \\end{aligned}$\nGESIA. In this section, we incorporate SIA [29] into the GE-AdvGAN+ framework, referring to this gradient editing method as GESIA. In Equation 13, N represents the number of transformed images, and $T_{n}()$ denotes the n-th transformation applied to the input x. The $g^{(k+1)}$ component can be integrated with the GE-AdvGAN+ framework, enhancing the transferability of adversarial samples generated by the generator.\n$g^{(k+1)}=\\mathbb{E}_{T_{n}}\\left[\\nabla_{x} L\\left(f\\left(T_{n}(x)\\right), y\\right)\\right]$"}, {"title": "3.4.4 Target Modification", "content": "This type of method assumes that most models focus on similar features of the samples. By using interpretability methods such as attribution, the important features that the surrogate model focuses on are identified and then attacked to improve performance when targeting the victim model. The editable part of such methods can be mathematically expressed as follows:\n$g_{e}=\\nabla_{x} L\\left(A\\left(f\\left(x^{(k)}\\right)\\right), y\\right)$\nwhere L() will be modified for a specific target, A() represents interpretability methods, such as [20, 24, 38, 40, 41]. More detailed formulas for interpretability methods can be found in the appendix.\nGENAA. In this section, we first integrate the NAA [34] method from Target Modification into our framework. The NAA method is based on the assumption that attribution results from different models are likely to be similar. It modifies the target from attacking the loss function to attacking the attribution results. In Equation 15, n represents the number of integrated steps, Ay is the sum of the attribution results on the target layer y, i.e., the sum of all neuron attribution values, IA($j) is Integrated Attention for latent feature pj, computed by summing the gradients along the path integral, N is the Neural Network Model, WA, represents weighted attribution, $f_{p}$ and $f_{n}$ are functions applied to positive and negative attributions, respectively, and $\\beta$ is a hyperparameter balancing positive and negative attributions. The \u03b8 represents intermediate layer features. The $g^{(k+1)}$ component can be incorporated into our GE-AdvAGN method."}, {"title": "GEDANAA", "content": "In this section, we integrate DANAA [12] into the framework. Compared to NAA, DANAA uses a nonlinear exploration path. In Equation 16, N is the Neural Network Model, and y(yj) represents the gradient in the nonlinear path. The $g^{(k+1)}$ component can also be applied to the GE-AdvGAN+ framework.\n$\\begin{aligned} &\\mathcal{A}_{y_{j}}=\\sum_{i=1}^{n^{2}} \\mathcal{A}_{x_{i}} \\int_{\\theta N(x_{i})} \\frac{\\partial N(x_{i})}{\\partial y_{j}}\\left(x_{t}\\right) \\frac{\\partial y_{j}\\left(x_{t}\\right)}{\\partial x} d t=\\mathcal{A}_{y_{j}} \\cdot \\gamma(y_{j}) \\\\ &\\gamma(y_{j})=\\int_{\\theta N\\left(x_{i}\\right)} \\frac{\\partial N\\left(x_{i}\\right)}{\\partial y_{j}}(x_{t}) d t \\\\ &\\mathcal{W}_{A_{u}}=\\sum_{\\mathcal{A}_{y_{j}} \\geq 0} f_{p}\\left(\\mathcal{A}_{y_{j}}\\right)-\\beta \\sum_{\\mathcal{A}_{y_{j}}<0} f_{n}\\left(-\\mathcal{A}_{y_{j}}\\right) \\\\ &g^{(k+1)}=+\\mu g^{(k)} \\frac{\\nabla_{x} \\mathcal{W}_{A_{u}}}{\\|\\nabla_{x} \\mathcal{W}_{A_{u}}\\|_{1}} \\end{aligned}$\nGEMIG. In this section, we apply the gradients generated by the MIG [18] method to our framework. In Equation 17, bk is the baseline input (usually a black image). IG(\u00b7) represents integrated gradients. The $g^{(k+1)}$ component can be combined with GE-AdvGAN to increase the transferability of the generated adversarial samples.\n$\\begin{aligned} &I G(f, x_{k}, b_{k})=\\left(x_{k}-b_{k}\\right) \\times \\int_{\\xi=0}^{1} \\frac{\\partial f\\left(b_{k}+\\xi \\times\\left(x_{k}-b_{k}\\right)\\right)}{\\partial x_{k}} d \\xi \\\\ &g^{(k+1)}=\\mu \\cdot g_{t}+\\frac{I G\\left(f, x_{k}\\right), y\\right)}{\\|I G\\left(f, x_{f_{k}}\\right), y\\|_{1}} \\end{aligned}$\nIn summary, gradient editing with additional information can generate stronger and more robust adversarial samples by integrating additional information and features. These methods significantly improve the success rate of adversarial attacks and the diversity of adversarial samples by introducing techniques such as frequency domain, momentum, random transformations, feature masking, and nonlinear exploration into the generation process. We will conduct experimental analyses in Section 4.2.2."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": "To ensure fair comparison across various adversarial attack methods, this study adheres to the datasets used in prior works such as AdvGAN [32] and GE-AdvGAN [37], as well as other methods like NAA [34], SSA [17],"}, {"title": "5 Conclusion and discussions", "content": "In this study, we introduced the GE-AdvGAN+ framework, an advanced framework for generating highly transferable and computationally efficient adversarial attacks that are easy to deploy. By incorporating nearly all mainstream attack methods, GE-AdvGAN+ significantly enhances the transferability of adversarial attacks and the efficiency of sample generation. Our comprehensive evaluation across multiple datasets and models demonstrates that this framework offers superior transferability and computational efficiency compared to existing methods. Furthermore, our results highlight the framework's adaptability to different model types, including traditional CNNs and more complex architectures like ViTs. The reduction in parameter size and increase in FPS for GE-AdvGAN++ make it a viable option for practical applications, particularly in edge computing environments. Overall, GE-AdvGAN++ represents a significant advancement in the field of adversarial attacks, providing a comprehensive and efficient tool for researchers and practitioners. Future work will explore the integration of more advanced interpretability methods and the application of GE-AdvGAN++ in domains beyond image classification."}]}