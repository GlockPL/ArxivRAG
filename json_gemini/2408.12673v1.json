{"title": "Enhancing Transferability of Adversarial Attacks with GE-AdvGAN+: A Comprehensive Framework for Gradient Editing", "authors": ["ZHIBO JIN", "JIAYU ZHANG", "ZHIYU ZHU", "YUCHEN ZHANG", "JIAHAO HUANG", "JIANLONG ZHOU", "FANG CHEN"], "abstract": "Transferable adversarial attacks pose significant threats to deep neural networks, particularly in black-box scenarios where internal model information is inaccessible. Studying adversarial attack methods helps advance the performance of defense mechanisms and explore model vulnerabilities. These methods can uncover and exploit weaknesses in models, promoting the development of more robust architectures. However, current methods for transferable attacks often come with substantial computational costs, limiting their deployment and application, especially in edge computing scenarios. Adversarial generative models, such as Generative Adversarial Networks (GANs), are characterized by their ability to generate samples without the need for retraining after an initial training phase. GE-AdvGAN, a recent method for transferable adversarial attacks, is based on this principle. In this paper, we propose a novel general framework for gradient editing-based transferable attacks, named GE-AdvGAN+, which integrates nearly all mainstream attack methods to enhance transferability while significantly reducing computational resource consumption. Our experiments demonstrate the compatibility and effectiveness of our framework. Compared to the baseline AdvGAN, our best-performing method, GE-AdvGAN++, achieves an average ASR improvement of 47.8. Additionally, it surpasses the latest competing algorithm, GE-AdvGAN, with an average ASR increase of 5.9. The framework also exhibits enhanced computational efficiency, achieving 2217.7 FPS, outperforming traditional methods such as BIM and MI-FGSM. The implementation code for our GE-AdvGAN+ framework is available at https://github.com/GEAdvGANPP.", "sections": [{"title": "1 Introduction", "content": "The concept of adversarial attacks first emerged in the field of machine learning and has gained increasing attention with the rapid development of deep learning technologies and their widespread application across various domains. In particular, deep learning models have demonstrated remarkable capabilities in areas such as image recognition [3, 10, 15] and natural language processing [1, 13], often surpassing human performance in many cases. However, studies have shown that these high-performance models are extremely sensitive to small changes in input data, making them vulnerable to adversarial attacks [7]. These attacks involve subtle and almost imperceptible modifications to the input data, causing the model to produce incorrect outputs.\nTherefore, studying adversarial attacks is crucial for understanding the vulnerabilities of deep learning models. By exploring how to generate adversarial samples that can deceive models, researchers can gain deeper insights into the decision boundaries and learning mechanisms of these models, as well as reveal the inner workings of the models [23, 31, 35]. These insights are essential for enhancing the robustness of models, making them more secure and reliable, and advancing the theory of deep learning.\nCurrently, adversarial attacks can be categorized into white-box and black-box attacks based on the amount of information available to the attacker [11]. White-box attacks assume the attacker has complete knowledge of the victim model, including its architecture, parameters, and training dataset [7], while black-box attacks are conducted without any internal knowledge of the model, making them more representative of real-world scenarios [21]. Within black-box attacks, there are further subdivisions based on the attack initiation method: query-based attacks and transferability attacks [12].\nQuery-based attacks typically involve querying the model to simulate the victim model, and then using this simulated model to attack the victim model. However, this method has an inherent disadvantage as it requires frequent access to the target model, which reduces the stealthiness of the attack [12]. In this paper, we focus on the other type of black-box attack, namely transferability attacks. The principle behind this approach is to use a surrogate model to generate adversarial samples that are also effective against the victim model. Compared to query-based attacks, transferability attacks are more stealthy and pose a greater threat in real-world attack scenarios. Therefore, our primary focus in this paper is on transferability attack methods. As shown in Figure 1, the process of a transferability attack is illustrated. First, the original image is input into the source model, generating adversarial examples by attacking this source model. Subsequently, these examples are transferred to the target model, and their impact is evaluated again. This process demonstrates how adversarial examples crafted for one model can affect other black-box models without access to internal information, providing a clear illustration of the concept of transferability-based attack methods."}, {"title": "2 Background", "content": "In this section, we discuss the current state of adversarial attacks, focusing on their principles, advantages, and limitations, as summarized in Table 1. We first introduce some classic white-box adversarial attack methods, followed by a detailed overview of black-box transferability-based adversarial attacks. Finally, we examine generative structure-based attack methods."}, {"title": "2.1 White-box Adversarial Attacks", "content": "White-box attacks are crucial in the study of adversarial attacks as they allow attackers full knowledge of the target model's architecture and parameters. Notable white-box attack methods include the Fast Gradient Sign Method (FGSM) [7] and its iterative version, the Basic Iterative Method (BIM) [14]. FGSM constructs perturbations by leveraging"}, {"title": "2.2 Black-box Adversarial Attacks", "content": "In the study of black-box adversarial attacks, transferability-based methods can be categorized into five main types based on their principles [11]. The first type is generative architecture methods, which use Generative Adversarial Networks (GANs) [6] or similar generative models to create adversarial samples transferable across different models, emphasizing the rapid generation of new adversarial samples once the generator is trained. The second type, semantic similarity methods, focuses on maintaining semantic consistency by attacking samples semantically related to the original ones, thereby extending the transferability of adversarial samples. Gradient editing methods generate more effective adversarial samples on target models by analyzing and adjusting gradient information, making the generated samples more stealthy and transferable. Target modification methods exploit similar features across different models, such as model interpretability similarities, to achieve adversarial sample transfer by directly attacking these similar features, thus enhancing transferability. Lastly, ensemble methods combine feedback from multiple models to generate adversarial samples, improving transferability. However, acquiring suitable multiple surrogate models is challenging in practice, and comparing multiple surrogate models with a single model is unfair. Additionally, using multiple models consumes more computational resources and time, so this category is not within the scope of our research."}, {"title": "2.2.1 Gradient Editing", "content": "In the domain of adversarial attacks, gradient editing techniques have emerged as new strategies to improve the precision and stealth of attacks. The Momentum Iterative Fast Gradient Sign Method (MI-FGSM) [4] and Gradient Relevance Attack (GRA) [36] are two prominent examples of this approach.\nMI-FGSM [4] is an enhancement of the Basic Iterative Method (BIM). It accumulates the gradient information from previous steps in each iteration, thereby enhancing the transferability of the adversarial samples. This momentum accumulation method improves the robustness of the attack, making the generated adversarial samples more transferable across different models, and thus harder for model defenses to detect and resist. The advantage of MI-FGSM is its improved transferability and ability to avoid local optima during attacks through the momentum mechanism. However, in some cases, over-reliance on historical gradients may lead to adversarial samples that are overly optimized for the source model, potentially reducing their effectiveness on target models.\nOn the other hand, GRA [36] adopts a different perspective, utilizing the correlation of input image gradients to adaptively adjust the update direction. GRA identifies the correlation among the gradients of input images, treating the current gradient as a query vector and neighboring gradients as key vectors, establishing a relationship through cosine similarity. This method can adaptively determine the update direction based on the gradient neighborhood information, thus generating more effective adversarial samples. The main advantage of GRA is its adaptive correction of the update direction using neighborhood gradient information, increasing the effectiveness of generated adversarial samples. However, the drawback is increased computational complexity, especially when handling large models and complex datasets."}, {"title": "2.2.2 Semantic Similarity", "content": "In the field of adversarial attacks, research on semantic similarity focuses on generating adversarial samples that can deceive deep learning models while preserving the semantic content of the input data. This type of method aims to create adversarial samples that are virtually indistinguishable from the original samples to human observers but can cause deep learning models to make incorrect predictions. Key methods in this domain include Translation-Invariant FGSM (TI-FGSM) [5], Diversity Input FGSM (DI-FGSM) [33], Scale-Invariant Nesterov Iterative FGSM (SI-NI-FGSM) [16], Frequency-based Stationary Point Search (FSPS) [39], and Structure-Invariant Attack (SIA) [29].\nTI-FGSM [5] improves attack stealth and effectiveness by enhancing the translation invariance of images. In each iteration, TI-FGSM performs multiple random translations of the image, computes gradients on these translated images, and synthesizes these gradients to generate translation-invariant adversarial perturbations. This approach ensures that the adversarial samples maintain their effectiveness even after spatial transformations, such as translations, by the model, enhancing the adaptability and stealth of the samples. However, this method increases computational cost due to the need for multiple translations and gradient computations compared to traditional FGSM methods.\nDI-FGSM [33] enhances the transferability of adversarial samples by introducing input diversity. In each iteration, DI-FGSM applies random transformations, such as scaling and cropping, to the original image and computes gradients on these transformed images to update the adversarial sample. This method generates adversarial samples that are effective across different models and preprocessing steps, enhancing sample generalization and stealth. However, the effectiveness of DI-FGSM heavily depends on the choice of input transformations and parameter settings, requiring careful design for optimal results.\nSI-NI-FGSM [16] combines scale invariance and Nesterov accelerated gradient descent to improve the transferability and efficiency of adversarial samples. By performing multi-scale transformations on the original image and averaging the gradients, SI-NI-FGSM ensures that the generated adversarial samples maintain their effectiveness across different scale transformations, demonstrating high robustness and stealth. The use of Nesterov acceleration also makes the adversarial sample generation process more efficient, though the multiple parameter adjustments involved add complexity.\nFSPS [39] explores stationary points on the loss curve as starting points for attacks and optimizes the attack direction through frequency domain search. This method aims to find the optimal local solutions to improve attack effectiveness and adaptability. The FSPS method is characterized by its ability to quickly locate efficient adversarial sample generation paths, though this frequency domain exploration-based approach is relatively computationally expensive and dependent on the model's frequency response characteristics.\nSIA [29] applies different transformations to local regions of the input image, aiming to increase sample diversity while maintaining the overall structure of the image, thus enhancing the stealth and effectiveness of adversarial samples. This method focuses on introducing effective perturbations while preserving semantic information, representing an innovative attempt in adversarial sample generation strategies. However, the challenge of SIA lies in balancing the degree of local transformations and maintaining global semantics, as well as the associated computational cost."}, {"title": "2.2.3 Target Modification", "content": "Target modification techniques in the field of adversarial attacks focus on generating more precise and difficult-to-detect adversarial samples by meticulously analyzing and adjusting attack strategies. These methods pay special attention to the importance of specific features or neurons within the model, guiding the generation of adversarial perturbations based on these factors to enhance attack effectiveness and transferability. In this area, methods such as Feature Importance-Aware Attack (FIA) [30], Neuron Attribution-based Attack (NAA) [34], Double"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Definition of Adversarial Attack", "content": "An adversarial attack can be defined as an optimization problem where the goal is to find the smallest perturbation $\\delta$ such that the model $f$ misclassifies the perturbed input $x + \\delta$, compared to the original input $x$. Specifically, the objective is to identify a minimal $\\delta$ such that $f(x + \\delta) \\neq y$, where $y = f(x)$ represents the original label predicted by the model for the input $x$. The mathematical formulation is as follows:\n$\\begin{aligned}\n&\\underset{\\delta}{\\text{minimize}} ||\\delta||_p\\\\\n&\\text{subject to } f(x + \\delta) \\neq f(x)\\\\\n& x + \\delta \\epsilon X\n\\end{aligned}$\nwhere $x$ is the original input, $\\delta$ is the perturbation added to $x$, $f$ is the model under attack, and $X$ denotes the valid input space. The term $|\\delta|_p$ represents the p-norm of $\\delta$, with common choices for $p$ being 1, 2, or $\\infty$. The choice of $p$ affects the sparsity or size of the perturbation, ensuring that the perturbation is minimal while still causing the model to produce incorrect predictions."}, {"title": "3.2 Mathematical Definition of AdvGAN", "content": "AdvGAN [32] aims to train a generative model $G$ that outputs a perturbation $\\delta$ such that the perturbed input $x + \\delta$ can deceive the classifier $f$. During training, both the generator $G$ and a discriminator $D$ are optimized, where $D$ is tasked with distinguishing real samples from adversarial samples generated by $G$. The optimization objective can be expressed"}, {"title": "3.3 GE-AdvGAN", "content": "GE-AdvGAN [37] modifies the gradient update mechanism of the generator $G$ to adjust the adversarial perturbation $\\delta = G(x)$ it generates. This approach involves two key techniques: frequency domain exploration and gradient editing. In the frequency domain exploration step, the input sample $x$ and the generated perturbation $G(x)$ are transformed into the frequency domain, typically using the Discrete Cosine Transform (DCT) [9]. This transformation allows for the analysis and modification of characteristics at different frequencies. By adjusting the amplitude and phase of the frequency components, a series of approximate samples $x_f$ are generated, which are used in training to further optimize the generator $G$.\nThe gradient editing process targets the parameter updates of the generator $G$. Specific gradient editing techniques, as shown in the following expression, are used to directly modify the gradients affecting the updates of $G$'s parameters:\n$\\begin{aligned}\nW_G = W_G - \\eta_{ge} \\cdot \\left(\\frac{\\partial L}{\\partial G(x)} \\cdot \\frac{\\partial(x + G(x))}{\\partial G(x)} + \\alpha \\cdot \\frac{\\partial L_{hinge}}{\\partial W_G} + \\beta \\cdot \\frac{\\partial L_{GAN}}{\\partial W_G} \\right)\n\\end{aligned}$ (1)\nwhere $\\eta_{ge}$ is the target gradient calculated based on the frequency domain analysis results, replacing the original gradient calculation $W_{GLADV}$.\nTo implement GE-AdvGAN, the sample $x$ is first transformed into the frequency domain using Discrete Cosine Transform (DCT), and variant samples $x_f$ are generated based on different frequency sensitivities. These samples are then used to compute the target gradient $g_e$, which feeds back into the training process of the generator $G$. The gradient $g_e$ is the average of gradients corresponding to multiple $x_f$, optimizing the generation of adversarial samples. Notably, in this study, the $g_e$ component can be replaced with any other gradient information, and apart from the frequency domain, more dimensions of information can be explored."}, {"title": "3.4 Gradient Editing with Additional Information", "content": "As previously mentioned, gradient editing can modify the direction of perturbation learned by the generator, thereby enhancing the transferability of generated adversarial samples. Currently, there are various methods for gradient editing, according to [11], including five different types of adversarial attacks: Generative Architecture, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach.\nOur experiments reveal that gradients from the Semantic Similarity category are more easily learned by the generator. As discussed earlier, the focus of this paper is on the Generative Architecture type of attacks due to their computational speed and efficiency. The Ensemble Approach, on the other hand, is challenging to deploy in real-world scenarios as it requires multiple surrogate models, which are difficult to obtain. Moreover, using multiple surrogate models is unfair compared to single-model methods.\nIn this paper, we integrate Gradient Editing, Semantic Similarity, and Target Modification methods with our previously developed GE-AdvGAN framework. We explore the scalability of GE-AdvGAN by incorporating more gradient editing techniques, enhancing the framework's capability to edit gradients. The following sections provide the mathematical definitions of Gradient Editing, Semantic Similarity, and Target Modification methods, with gradient editing techniques designated as GE(.). For clarity, we use $g^{(k+1)}$ to denote components in the equations that can be incorporated into the GE-AdvGAN gradient editing framework. These components allow for the generation of adversarial samples using the following formula:\n$x_f^{(k+1)} = Clip_{x^0, x_f} (x_f^{(k)} + \\alpha \\cdot sign(g^{(k+1)}))$ (3)"}, {"title": "3.4.1 Mathematical Definition of Gradient Editing", "content": "Gradient Editing type adversarial attacks primarily optimize the objective function by adjusting the gradient $\\nabla_xL(f(x), y)$ with respect to the input $x$ to generate adversarial samples. The editable part of such methods can be mathematically expressed as follows:\n$g_e = \\nabla_xL(f(x^{(k)}), y)$ (4)\nGEBIM & GEPGD. First, we combined the classical Gradient Editing methods BIM and PGD with our GE-AdvGAN+ framework. Although their mathematical principles are essentially the same, their initial inputs $x_f^{(0)}$ differ; in BIM, $x_f^{(0)} = x$, starting from the original sample $x$, while PGD starts from a point $x_f^{(0)} = x + Uniform(-\\epsilon, \\epsilon)$, with random perturbations around the original sample. Furthermore, PGD restricts each gradient update within a norm ball to ensure the perturbation does not exceed the preset maximum range, as shown in Equation 5. The gradient information for both methods is obtained via $\\nabla_xL(f(x), y)$, which integrates with our framework's editing method.\n$\\begin{aligned}\ng^{(k+1)} &= \\nabla_xL(f(x^{(k)}), y)\\\\\nx^{(k+1)} &= Clip_{x, \\epsilon} (x^{(k)} + \\alpha \\cdot sign(g^{(k+1)}))\n\\end{aligned}$ (5)\nGEMIM. Next, we incorporated MIM [4] into our framework. In Equation 6, $g^{(k)}$ represents the accumulated gradient (momentum term) from the $k$-th iteration. The momentum factor $\\mu$, typically set between 0.9 and 1.0, balances the influence of previous and current gradients. The $g^{(k+1)}$ can be integrated with the GE-AdvGAN framework."}, {"title": "3.4.2 Semantic Similarity", "content": "Semantic Similarity-based adversarial attacks focus on enhancing transferability by altering input data through data augmentation techniques to construct samples semantically similar to the original data. The part where gradient editing can be performed is mathematically defined as follows:\n$g_e = \\nabla_xL(f(D_p(x^{(k)})), y)$ (8)\nwhere $D_p$ represents a series of parameterized data augmentation operations such as random scaling, cropping, and translation."}, {"title": "3.4.3 GEDIM", "content": "In Semantic Similarity-based adversarial attacks, we first combined the typical DIM [33] method with our approach. In Equation 9, $D_p$ denotes random transformations applied to the input $x^{(k)}$, such as random scaling and cropping, which aim to minimally affect the original image semantics, with $p$ as the transformation parameter. The $g^{(k+1)}$ part can be integrated with the GE-AdvGAN+ framework.\n$g^{(k+1)} = E_p [\\nabla_xL(f(D_p(x^{(k)})), y)]$ (9)\nGETIM. TIM [5] can also be incorporated into our framework. In Equation 10, $T_\\tau$ denotes the translation transformation (a geometric operation that shifts an image from one location to another without altering its orientation, size, or shape) applied to the input image $x^{(k)}$, with $\\tau$ as the translation parameter. $E_\\tau$ denotes the expectation over all possible translations $\\tau$, averaging the gradients across multiple translations.\n$g^{(k+1)} = E_\\tau [\\nabla_xL(f(T_\\tau(x^{(k)})), y)]$ (10)"}, {"title": "3.4.4 Target Modification", "content": "This type of method assumes that most models focus on similar features of the samples. By using interpretability methods such as attribution, the important features that the surrogate model focuses on are identified and then attacked to improve performance when targeting the victim model. The editable part of such methods can be mathematically expressed as follows:\n$g_e = \\nabla_xL(A(f(x^{(k)})), y)$ (14)\nwhere $L()$ will be modified for a specific target, $A()$ represents interpretability methods, such as [20, 24, 38, 40, 41]. More detailed formulas for interpretability methods can be found in the appendix.\nGENAA. In this section, we first integrate the NAA [34] method from Target Modification into our framework. The NAA method is based on the assumption that attribution results from different models are likely to be similar. It modifies the target from attacking the loss function to attacking the attribution results. In Equation 15, $n$ represents the number of integrated steps, $A_\\phi$ is the sum of the attribution results on the target layer $y$, i.e., the sum of all neuron attribution values, $IA(\\phi_j)$ is Integrated Attention for latent feature $\\phi_j$, computed by summing the gradients along the path integral, $N$ is the Neural Network Model, $W_{A_\\phi}$ represents weighted attribution, $f_p$ and $f_n$ are functions applied to positive and negative attributions, respectively, and $\\beta$ is a hyperparameter balancing positive and negative attributions. The $\\theta$ represents intermediate layer features. The $g^{(k+1)}$ component can be incorporated into our GE-AdvAGN method."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Setup", "content": ""}, {"title": "4.1.1 Datasets", "content": "To ensure fair comparison across various adversarial attack methods, this study adheres to the datasets used in prior works such as AdvGAN [32] and GE-AdvGAN [37], as well as other methods like NAA [34], SSA [17],"}, {"title": "4.1.2 Models", "content": "In our experiments, we utilized the four widely used models in image classification tasks as adopted in the AdvGAN paper: Inception-v3 (Inc-v3) [26], Inception-v4 (Inc-v4) [25], Inception-ResNet-v2 (Inc-Res-v2) [25], and ResNet-v2-152 (Res-152) [8] as source models. Additionally, we included the more complex ViT model MaxViT-T [28] as a surrogate model to evaluate the generalizability of the GE-AdvGAN method. Furthermore, we tested the transferability of our method across seven models, including four standard training models (Inc-v3, Inc-v4, Inc-Res-v2, Res-152), and three adversarially trained models (Inc-v3-adv-ens3 [27], Inc-v3-adv-ens4 [27], and Inc-Res-adv-ens [27])."}, {"title": "4.1.3 Metrics", "content": "We used the Adversarial Success Rate (ASR) as the primary metric for evaluation. ASR measures the effectiveness of adversarial attacks in altering the model's output. Specifically, it represents the proportion of samples where the classification result is successfully altered by the adversarial attack out of all tested samples. The formula for ASR is:\n$ASR = \\frac{N_{successful attacks}}{N_{total samples}}$ (18)\nwhere $N_{successful attacks}$ denotes the number of samples whose classification results were successfully altered after the adversarial attack, and $N_{total samples}$ is the total number of test samples. The ASR ranges from 0 to 1, with higher values indicating more significant effectiveness of the adversarial attack, demonstrating the model's susceptibility to being attacked."}, {"title": "4.1.4 Baseline", "content": "In this paper, we selected AdvGAN [32] as our baseline and GE-AdvGAN [37] as our main competing algorithm. Additionally, we compared the performance of 12 representative attack methods from the Gradient Editing, Semantic Similarity, and Target Modification categories, incorporating them into our GE-AdvGAN+ framework to evaluate their combined performance against their original implementations. The details of these 12 methods are listed in Table 2."}, {"title": "4.1.5 Parameter Settings", "content": "We maintained consistency with the parameter settings used in GE-AdvGAN [37]. Specifically, for the models Inc-v3, Inc-v4, Inc-Res-v2, Res-152, and MaxViT-T, the parameters adv_lambd and N were set to 10, $\\epsilon$ was set to 16, the number of epochs was set to 60, the change threshold was set to [20, 40], the Discriminator ranges were set to [1, 1], and the Discriminator learning rate was set to [0.0001,0.0001]. When Inc-v3 was used as the surrogate model, $\\sigma$ was set to 0.7; for the other four models, $\\sigma$ was set to 0.5. When Inc-Res-v2 was used as the source model, the Generator ranges and Generator learning rate were set to [1,1] and [0.0001, 0.000001], respectively; for the other four models, these parameters were set to [2, 1] and [0.0001, 0.0001]."}, {"title": "4.2 Experimental Results", "content": ""}, {"title": "4.2.1 Transferability of GE-AdvGAN++", "content": "In this section, we first analyze the ASR performance of GE-AdvGAN++ compared to other similar methods. As shown in Table 3, GE-AdvGAN++ achieves an average improvement of 47.8 in ASR compared to our main competing algorithm AdvGAN. Additionally, compared to GE-AdvGAN, our optimized GE-AdvGAN++ also demonstrates enhanced performance, with an average improvement of 5.9."}, {"title": "4.2.2 Compatibility of the GE-AdvGAN+ Framework", "content": "In this section, we analyze the changes in transferability of the GE-AdvGAN framework when combined with almost all mainstream attack methods. As illustrated in Figure 3, we present the transferability of three types of adversarial attack methods and their performance when integrated with the GE-AdvGAN framework across five different surrogate models. The figure shows that the Gradient Editing methods significantly improve transferability when combined with GE-AdvGAN, particularly for BIM and PGD, which originally had relatively poor transferability. Moreover, the Semantic Similarity methods exhibit the best transferability performance when combined with GE-AdvGAN, with FSPS achieving the highest transferability across all models. However, when combined with Target Modification methods, traditional CNN models such as Inc-v3, Inc-v4, Inc-Res-v2, and Res-152 show a slight decrease in transferability, while the ViT model MaxViT-T exhibits a significant performance drop. These results highlight the potential enhancement effects of GE-AdvGAN across different adversarial attack methods and its adaptability to various model architectures."}, {"title": "4.2.3 Computational Efficiency of GE-AdvGAN++", "content": "In this section, we analyze the computational speed and resource consumption of GE-AdvGAN++ compared to other attack methods. In Table 4, we compare the speed of generating adversarial samples, using Frames Per Second (FPS) as the evaluation metric. The results show that GE-AdvGAN++ significantly outperforms other methods in terms of speed, with an FPS of 2217.7, far exceeding other methods such as"}, {"title": "5 Conclusion and discussions", "content": "In this study, we introduced the GE-AdvGAN+ framework, an advanced framework for generating highly transferable and computationally efficient adversarial attacks that are easy to deploy. By incorporating nearly all mainstream attack methods, GE-AdvGAN+ significantly enhances the transferability of adversarial attacks and the efficiency of sample generation. Our comprehensive evaluation across multiple datasets and models demonstrates that this framework offers superior transferability and computational efficiency compared to existing methods. Furthermore, our results highlight the framework's adaptability to different model types, including traditional CNNs and more complex architectures like ViTs. The reduction in parameter size and increase in FPS for GE-AdvGAN++ make it a viable option for practical applications, particularly in edge computing environments. Overall, GE-AdvGAN++ represents a significant advancement in the field of adversarial attacks, providing a comprehensive and efficient tool for researchers and practitioners. Future work will explore the integration of more advanced interpretability methods and the application of GE-AdvGAN++ in domains beyond image classification."}]}