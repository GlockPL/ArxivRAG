{"title": "Cognitive Kernel: An Open-source Agent System towards Generalist Autopilots", "authors": ["Hongming Zhang", "Xiaoman Pan", "Hongwei Wang", "Kaixin Ma", "Wenhao Yu", "Dong Yu", "Cognitive Kernel Team, Tencent AI Lab, Seattle"], "abstract": "We introduce Cognitive Kernel, an open-source agent system towards the goal of general-ist autopilots. Unlike copilot systems, which primarily rely on users to provide essential state information (e.g., task descriptions) and assist users by answering questions or auto-completing contents, autopilot systems must complete tasks from start to finish independently, which requires the system to acquire the state information from the environments actively. To achieve this, an autopilot system should be capable of understanding user intents, actively gathering necessary information from various real-world sources, and making wise decisions. Cognitive Kernel adopts a model-centric design. In our implementation, the central policy model (a fine-tuned LLM) initiates interactions with the environment using a combination of atomic actions, such as opening files, clicking buttons, saving intermediate results to memory, or calling the LLM itself. This differs from the widely used environment-centric design, where a task-specific environment with predefined actions is fixed, and the policy model is limited to selecting the correct action from a given set of options. Our design facilitates seamless information flow across various sources and provides greater flexibility. We evaluate our system in three use cases: real-time information management, private information management, and long-term memory management. The results demonstrate that Cognitive Kernel achieves better or comparable performance to other closed-source systems in these scenarios. Cognitive Kernel is fully dockerized, ensuring everyone can deploy it privately and securely. We open-source the system and the backbone model to encourage further research on LLM-driven autopilot systems.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have revolutionized the landscape of AI applications (OpenAI, 2023; Anil et al., 2023; Zhao et al., 2023; Chang et al., 2024). Systems like ChatGPT All-in-One\u00b9 and Microsoft's Copilot\u00b2, representing chat models and auto-completion tools respectively, have significantly enhanced productivity in everyday tasks. However, these systems primarily function as \u201cCopilots,\u201d where users are still required to manage the majority of the work, such as planning the overall workflow, posing the right questions, or refining the model's output as needed (Xi et al., 2023; Wang et al., 2024c). To fully harness the capabilities of LLMs and reduce the burden of tedious, repetitive tasks, we shall shift from building \u201cCopilot\u201d to \u201cAutopilot\" systems that can independently complete tasks. For instance, while a Copilot system might assist in drafting a template for an invitation email, an Autopilot system should be capable of composing the entire email and sending it autonomously.\nRecently, significant efforts have been directed toward developing agent systems to achieve autopilot capabilities (Wang et al., 2024b). For each specific task, such as debugging (Jimenez et al., 2024), the agent system relies on a task-dependent environment to generate prompts (Yang et al., 2024;"}, {"title": "2 Backgrounds", "content": "We first introduce previous efforts on rule-based automated systems, which motivates our for-mulation of an \u201cautopilot\u201d system. Then we cover more recent works in building and training task-specific agent systems."}, {"title": "2.1 From Rule-based Automated Systems to LLM-driven \u201cAutopilots\"", "content": "One of the automated system pioneers is the Turing machine (Turing et al., 1936), a mathematical model describing an abstract machine that manipulates symbols based on a table of rules and a foundation of modern computers. A Turing Machine contains two key concepts: states, the set of predefined variables, and transition functions, which are predefined rules describing how a system shall respond to a state. Although traditional Turing machines cannot directly serve as an \u201cautopilot\" system to solve real-world tasks due to the almost infinite space of possible states and transition rules with randomness among them, it motivates the two fundamental duties of an \u201cautopilot\" system: (1) perceiving and managing essential states; (2) make wise decisions based on the states.\nBy compressing and modeling vast amounts of world information, LLMs implicitly learn the possible connection between states. They thus can serve as a powerful approximation of the transition functions and partially solve the second duty (Wu et al., 2024a; Ma et al., 2023). However, how to efficiently and accurately perceive and manage the state remains unclear. Daily applications require both global state information (e.g., world knowledge) and localized state information that changes constantly (e.g., update-to-date knowledge or private information). LLMs can learn to model the global state information but cannot model the localized ones. Thus, how to manage the localized state information becomes the critical design choice for building an LLM-driven AI system. For a Copilot system, we leave the task of providing localized state information to users, which simplifies the task but also limits the capability of the system to solve tasks independently (Chen et al., 2021). In contrast, for an autopilot system, we expect it to monitor and acquire the localized state information by itself and thus it has the potential to solve a complete task without human involvement (Gravitas, 2024). Therefore, in Cognitive Kernel, we specifically designed the perception kernel and the memory kernel to perceive and memorize localized state information, and their orchestration is completely handled by the reasoning kernel, moving one step closer to an LLM-driven \u201cAutopilot\" system."}, {"title": "2.2 Recent Advancement in Model-based Agents", "content": "The concept of an agent, popular in the reinforcement learning (RL) community, has been founda-tional to artificial intelligence (Watkins & Dayan, 1992; Kaelbling et al., 1996). An agent interacts with an environment to learn how to achieve a specific goal by maximizing cumulative rewards (Arulku-maran et al., 2017; Sutton & Barto, 2018). Early work focused on the theoretical underpinnings of RL, such as Markov decision processes and dynamic programming (Howard, 1960). These methods provided the basis for the development of various RL algorithms. Recent advances like Q-learning (Watkins & Dayan, 1992) and policy gradients (Williams, 1992; Sutton et al., 1999) have been crucial, especially with the introduction of deep RL, which combines neural networks for approximating Q-values, as seen in Deep Q-Networks (Van Hasselt et al., 2016) achieving human-level performance in complex tasks. Key algorithms such as DDPG (Lillicrap et al., 2016) and PPO (Schulman et al., 2017) have significantly impacted robotics, healthcare, finance, gaming, and autonomous driving, showcasing RL's broad applicability (Gottesman et al., 2019; Yu et al., 2021; Charpentier et al., 2021; Silver et al., 2018; Sallab et al., 2017; Kiran et al., 2021).\nHowever, previous assumptions in RL significantly differ from human learning processes, as the human mind is highly complex and capable of learning from a much wider variety of environments."}, {"title": "3 A Conceptual Framework of \u201cAutopilot\u201d Systems", "content": "Motivated by previous automated systems (Turing et al., 1936), an autopilot system AS should excel at managing the current state and making wise actions accordingly. Thus, we first formulate the conceptual autopilot framework as a 6-tuple $AS = (S,s_n, A, a_n, T, M)$, where S is the set of all possible states, $s_n \\in S$ is the state\u2074 at timestamp n, A is the set of possible actions, $a_n \\in A$ is the action at timestamp n, T is the transition matrix, which determines an based on sn, M is the memory component that records so to sn\u22121. Note that in real applications, S and A are arbitrarily large to be enumerated by modern machines. As the size of T is |S| \u00b7 |A|, T is also too large to be enumerated.\nTo address these limitations and create a practical autopilot system, we further decouple the states into global and localized ones, where the global state information is the world knowledge shared by most humans and localized state information is temporally or spatially unique to the current task. Specifically, we can decouple S as:\n$S = S^g \\cup S^l$,\nwhere $S^g$ and $S^l$ represent the set of global and localized state information, respectively. Similarly, for any state sn at timestamp n, we can also decouple it as:\n$S_n = s^g \\cup s^l$.\nBased on the assumption that large language models such as GPT-4 have compressed the world's knowledge through the pre-training, we can use an LLM as the policy function F to simulate S8, sh, and T. Thus, we can reformulate AS as (S\u00b9, s\u0127A, an, F, M), where F is the LLM-based policy function that predicts an conditional on sn. Since LLMs are essentially probabilistic models, this"}, {"title": "4 System Architecture and Implementation", "content": "As introduced in Section 1, Cognitive Kernel contains three conceptual components: reasoning kernel, perception kernel, and memory kernel, which handles the three duties of an autopilot system: predicting the next action an based on current observed state sh, tracking the current state with perception tasks P, and storing and retrieving the past states with the memory component M, respectively. In this section, we first introduce the motivations and design principles of the three kernels. After that, we introduce the dockerized implementation details. In the end, we introduce the training details of the center language model, which serves as the policy function F."}, {"title": "4.1 Reasoning Kernel", "content": "The reasoning kernel is responsible for generating a plan for the next moves and then executing it. Creating a general-purpose autopilot system has many practical challenges. The first one is uncertainty. The real world is full of uncertainty. Even with all the essential state information, a model still cannot accurately predict the effect of its action. Moreover, complex tasks usually involve long trajectories of actions to complete, leading to exponential growth in the degree of uncertainty. Thus, it's almost impossible for the system to generate a perfect plan that can execute end-to-end in the beginning. The second challenge is the efficiency. It is widely known that existing language models are huge and the inference can be time-consuming. In an LLM-driven agent system, since every step is an inference task that takes considerable time even with the latest inference frameworks, the task completion process could be slow and thus provide a bad user experience.\nTo address these challenges, we follow existing works to use the programming language (i.e., Python) as the medium for planning and execution (Li et al., 2024; Zhang et al., 2024a), where the basic Python operations such as addition are considered to be the atomic actions and Python functions are considered to be the compound actions. Compared with natural language, the programming language provides great flexibility for handling uncertainty. For example, the policy model could use \"if/else\" statements to design alternative strategies for task completion or use \u201cfor loop\" to iteratively attempt different options. This enumeration operation is typically infeasible for natural language. Also, programming language provides a much higher level of parallelism than natural language, allowing multiple steps to be executed simultaneously. For example, for a task that requires checking information from multiple sources to cross-validate, a natural language-based agent system has to check each source iteratively. But with the programming language, one could easily speed up this process by running a piece of multi-threading code to check all sources concurrently."}, {"title": "4.2 Perception Kernel", "content": "The perception kernel is responsible for accessing the environment, which is the real world in our scenario, to perceive the localized states. Actively activating the perception kernel to get localized state information is the core functionality of an autopilot system. This subsection introduces how Cognitive Kernel perceives two kinds of localized state information. The first category is temporally localized information, which refers to the information that is constantly changing such as the weather information or the opening hours of specific restaurants. The second category is spatially localized information, which refers to the information that can only be accessed by the local user and is not available anywhere else."}, {"title": "4.2.1 Temporally Localized State Perception", "content": "The world is constantly changing and the autopilot needs to access the up-to-date information. For humans, the easiest way is using the internet. Similarly, we also equip Cognitive Kernel with the access to the open web. Existing systems such as ChatGPT All-in-one, Gemini, or KimiChat leverage search engines to gather updated real-world information. However, this approach is inherently limited because it cannot perform more complex interactions with different websites. Instead, we give the system more freedom and allow it to directly control a live browser to interact with the open web like a person, e.g., by performing atomic actions such as clicking and typing. By doing so, Cognitive Kernel could finish more complex tasks such as \u201cfinding the latest commit details of a popular GitHub repository\u201d that one cannot find answers from the search engine.\nAfter receiving a command from the user, the reasoning kernel will first generate a plan based on the current system status and the incoming user query. If the reasoning kernel thinks the system needs to perceive more temporal state information, it sends a request to the perception kernel with a specific instruction, which can be a more fine-grained level autopilot task as shown in Figure 1.\nAfter the perception kernel receives the command, it activates a web server to complete the task. At every step, the system will first observe the current web session as the state information and send that information to the reasoning kernel for the next action. We follow Zhou et al. (2024) and use the web page's accessibility tree as the observation to the agent due to its structural format and conciseness. We further optimized the raw accessibility tree to reduce redundancy and prune out irrelevant information. (See more details in Section 4.5)\nSimilar to how humans browse the web, we define the following atomic actions for Cognitive Kernel to control the browser: (1) Click: click an element on the webpage; (2) Type: clear the text content in an element and fill it with new content; (3) Scroll: scroll up or down of the current viewport; (4) Goback: go back to the previously browsed page; (5) Restart: return to the homepage directly and restart the browsing process; (6) Stop: summarize the relevant information and sent back to the upper-level reasoning kernel. At each step, after receiving the web description, Cognitive Kernel will call the reasoning kernel to make a plan for the next step. If the generated actions can be directly executed in the browser, Cognitive Kernel will execute it and provide an updated observation. If an error occurs during action execution, the error message is also sent back to the"}, {"title": "4.2.2 Spatially Localized State Perception", "content": "Another perspective of the localized state information is the spatial one. Real-world tasks often involve private information only accessible to the local system/user. This section covers the two most popular localized state resources: local files and history.\nFiles: Files such as docs or spreadsheets play critical roles in information transferring in people's daily lives. Hence an \u201cautopilot\u201d system should also be able to perceive information from the local files to complete various tasks. Similar to how humans process files, Cognitive Kernel could use basic operations such as opening a file and searching for certain keywords by generating code in the reasoning kernel. We classify these operations into four categories: (1) Operate: Perform specific operations such as counting occurrences, finding specific terms, and extracting part of input data. (2) Navigate: Move to different file locations, such as the next page, the previous page, or a specific page number. (3) Search*: Perform semantic-based retrieval to find relevant information within the file. (4) Read: Understand and summarize the content, extracting key information, generating insights, or directly answering questions based on the file's content. At each step, if Cognitive Kernel thinks that the current state information is not enough and it needs to perceive more from the local files, it will activate the suitable operations, which can be the aforementioned atomic ones or the combination of them to perform more complex observations.\nLong-term History: Besides files, another important spatially localized state information is the long-term history between users and the autopilot systems. For example, one might expect his \u201cautopilot\" system to know the location of his home without mentioning it in every relevant command. It is widely known that existing language models all have a limited context length, typically from two thousand to one million, and thus an autopilot system cannot store all past interactions with the user as the context. To solve this problem, we treat history as a special kind of spatially localized information that is available to the current user and autopilot system and formulate the storage and usage of such information as a perception task. Specifically, if Cognitive Kernel thinks that it needs to store some information in the history or perceive history, it will activate the memory kernel to write into or load from the memory just like how modern computers operate the disk. More technical details of our memory design will be covered in Section 4.3."}, {"title": "4.3 Memory Kernel", "content": "The main duty of the memory kernel is to provide a caching mechanism for the autopilot system to save and retrieve past states. The most intuitive way of implementing such a module is using the widely used dense retrieval methods, which segment the content of interest into fixed-length chunks and create chunk indexes with representation models (Karpukhin et al., 2020; Ni et al., 2022; Izacard et al., 2022). However, this traditional approach is sub-optimal for an \u201cautopilot\" system. The dense-retrieval model was originally trained to find relevant text pieces from a huge plain text database like Wikipedia. However, an \u201cautopilot\u201d system often needs to store and retrieve well-structured information that requires more fine-grained semantic matching. To better suit this need, we propose a multi-granularity information management system as the memory kernel of Cognitive Kernel. The overall framework is illustrated in Figure 2, which includes two major components: information processing/storage and information retrieval.\nInformation processing/storage. The information processing/storage component is illustrated in the right part of Figure 2 (the blue rectangle). For any given information, we first convert it into plain text and treat it as a regular document. For example, for dialogue history with the timestamp information, we could create a sentence in the format of \u201cC@@T,\u201d where C and T represent the"}, {"title": "4.4 Policy Model Training", "content": "The duty of the policy model is to simulate the optimal transition matrix and make wise actions based on the current state. Such actions include initiating interaction with the environment given incomplete state information, generating atomic actions for the perception kernel, and aggregating information from the memory and perception kernels into complete responses to users. Since directly applying a closed-source model leads to unsatisfying performance, we trained our own model upon open-source language models (i.e., Llama3 (Dubey et al., 2024)). The training contains two stages. In stage one, we employ standard supervised fine-tuning to train our model. Specifically, we use a mixture of data including open-sourced instruction following data (Zhou et al., 2023; Luo et al., 2024), function calling data, agent trajectories data for various tasks (Zeng et al., 2024; Wang et al., 2024d; Yin et al., 2024; Zhou et al., 2024), and a small set of manually annotated data that fits our system design to train our model. This stage equips the model with the general problem-solving capability and the basic capability of invoking atomic actions defined in our system. However, the output distribution remains relatively flat after the first training stage, leading to unstable performance, particularly with new inputs out of the training data distribution. To overcome this challenge and enhance the model's generalization ability, we conduct a second-stage training. Specifically, we deploy the first-stage model online and then collect the system's output trajectories given various inputs. Again, we used a mixture of data where the inputs are either mined from open-source datasets (Wang et al., 2024a; He et al., 2024; Dasigi et al., 2021; Trivedi et al., 2022) or submitted by internal users. Here, to ensure that the data we collect are with high-quality, we also collect judgments and feedback for the system trajectories. The judgments and feedback can come from a user (where the user can directly submit via Cognitive Kernel 's user interface), the system itself (when the code produces the error and error message), or an external model (e.g. GPT-4). We use the judgments to filter out bad cases and train the model on the successful trajectories to continue improving its ability. Furthermore, we can use this data to empower the policy model with the capability of criticizing whether the outputs fit the user's preference or not. More training details can be found in Appendix 8.1."}, {"title": "4.5 Implementation Details", "content": "We adopted a dockerized design in our implementation to ensure efficient and safe deployment. As demonstrated in Figure 1 and Equation 3, the three conceptual kernels are deeply integrated. Thus, for efficient scheduling and execution, we reorganize the system into separate dockers and optimize each one toward the assigned task. As shown in Figure 3, we implement Cognitive Kernel as five separate dockers: (1) the frontend docker, which provides the user interface, supports interaction with multiple users and collects feedback for the continuous training; (2) the backend docker, which provides an isolated environment for planning execution and controls the workflow with all other dockers; (3) the web accessing docker, which provides an isolated environment for perceiving the localized state information via the open web; (4) the database docker, which handles the memory management; (5) the inference docker, which handles the inference of the central language model. Dockers communicate with each other via APIs. This dockerized design guarantees Cognitive Kernel's high parallelism. For example, the inference docker could batch all queries to the central policy model from different levels in Equation 3 for more efficient inference. Besides that, this dockerized design also provides an isolation environment for all modules and thus provides a safer and more robust system. The implementation details are as follows.\nFrontend Docker: An \u201cautopilot\u201d system should have an interface to receive user commands, demonstrate the progress, provide the execution results, and collect user feedback. An illustration of Cognitive Kernel 's user interface is shown in Figure 8 and an illustration of the feedback mode is in Figure 9. We implement the frontend docker with the React package. Considering that the system"}, {"title": "5 Evaluation", "content": "We evaluate Cognitive Kernel on tasks that best reflect how users would interact with the system when deployed. Specifically, we focus on evaluating Cognitive Kernel's ability to 1) gather real-time information and complete web-based tasks, 2) process user-uploaded files and answer questions, and 3) manage the interaction history with the user for better personalization. We mainly compare against the following general-purpose end-to-end AI systems in our experiments: ChatGPT16 (OpenAI, 2023), Gemini17 (Anil et al., 2023), Claude18, Kimi19, and Coze20, and we directly use their web interface for evaluation. We used GPT-40 for ChatGPT, Gemini-Pro-1.5 for Gemini, Claude-opus for Claude, and the default version for Kimi. For Coze, since it allows users to customize the bot, we used different configurations for different tasks for the best performance. In particular, we activated Browser, Google Web Search, and WebPilot plugins for web-based evaluation, Long-term memory for long-term memory evaluation, and Doc Reader for document-based evaluation. Finally, we also consider a baseline where we switch the backbone model of Cognitive Kernel to GPT-40 to understand the impact of the central policy model."}, {"title": "5.1 Benchmarks", "content": "For real-time information management evaluation, we conduct experiments on the recently released WebCanvas benchmark (Pan et al., 2024). WebCanvas test set contains 104 human-annotated tasks that require interacting with real-world live websites to complete, and each of the tasks specifics a target website for interaction. We simply provide each task instruction to each system and it is expected to strictly follow the instructions to find the target webpage and complete the task.\nFor private information management evaluation, we conducted extensive assessments using DOCBENCH (Zou et al., 2024). DOCBENCH provides an end-to-end evaluation: starting with a raw file input along with user questions and evaluating the system based on the quality of the answers generated. This benchmark includes 229 real-world documents and 1,102 questions, spanning five distinct domains: Academia, Finance, Government, Law, and News. Furthermore, it encompasses four major types of questions: text-only, multi-modal, meta-data, and unanswerable questions. We simply upload the files to each system and ask the questions.\nFor long-term memory management evaluation, since there is no existing benchmark to the best of our knowledge, we constructed an in-house test set. Specifically, each test case consists of a session of messages between the user and the assistant acting as dialog history, followed by a final question about the details of previous messages. The goal is to assess if the assistant can accurately retrieve the ground truth message(s) from the dialog history and correctly answer the query. The dataset has four categories of questions: Single Message (1-M), where answering the final question relies on a single ground truth message; Multiple Messages (Mul-M), where answering the final question requires combining information from two or more ground truth messages; Knowledge Update (Update), where the user initially provides some information and later updates or corrects it; Temporal Reasoning (Temp), where the questions require inferring the chronological order of two events mentioned in conversation history. We create the benchmark with both human written and LLM generation. For the manually-created subset, we write both the dialog history and the final question and we only cover the first three categories. For the LLM-generated subset, we prompt the LLM (i.e., GPT-40) to first create users with particular attributes, preferences, and experiences, then we have an LLM role-plays the user while another LLM role-plays the assistant to create the dialog history. Finally, an LLM is prompted to generate the question and the answer. The generated conversations, questions, and answers are manually checked by humans to ensure quality. During"}, {"title": "5.2 Metrics", "content": "We mainly focus on the end-task success rate in our evaluation. However, the definition of task success and evaluation methods are different for each of the target scenarios. Here we provide the detailed definitions.\nFor WebCanvas, Pan et al. (2024) proposed step-wise scores that use human-annotated key nodes along the gold trajectory to evaluate the system's web browsing performance. Upon further in-spection of the annotated key nodes, we found that this metric can significantly underestimate the system performance. Since there exist many possible trajectories that lead to task completion, simply matching the key nodes from one trajectory can overlook many other valid paths. Thus we opt for manual evaluation and we only focus on the overall task success rate. Here, since many tasks cannot be truly \"completed,\u201d we consider a task to be successful if 1) the required information is gathered from the target website and 2) all the necessary actions are performed with regard to the correct elements. For example, the system cannot truly buy a gift card (without valid account information), we consider it to be successful if it has added the correct gift card to the cart on the target website and filled in the fake user information in the instruction to the correct cells on the checkout page. We closely monitor the system's web interaction sessions during our experiments to ensure that no harm is done to website hosts. For systems that do not provide the intermediate trajectories, we consider a task to be successful if the system provides the links to the correct websites that satisfy all requirements in their responses.\nFor DOCBENCH, we follow Zou et al. (2024) and adopt GPT-40 to automatically evaluate the correctness of the generated answers based on the reference. As reported by Zou et al. (2024), relying on string matching or number extraction to evaluate the accuracy of generated response can be imprecise, since different LLMs and systems exhibit substantial variations in the organization and style of their outputs, potentially leading to biases in traditional metrics. We instruct GPT-40 to assign a score of 0 (incorrect) or 1 (correct), thus using Accuracy to measure system performance.\nFor long-term memory evaluation, we ask Cognitive Kernel as well as baseline systems to generate the answer for each question, and manually judge the correctness of the answers."}, {"title": "5.3 Overall Results", "content": "We present the overall results from our experiments in Table 1. We see that Cognitive Kernel can achieve the best results on real-time information management and long-term memory and compara-ble performance with state-of-the-art systems in the management of private information.\nFor real-time information management evaluation, the primary limitation of the baseline systems is their inability to directly interact with target websites, preventing them from completing tasks"}, {"title": "5.4 Real-time information management", "content": "We further conduct experiments using a specialized web agent system specifically designed for web interaction. In particular, we rerun the WebCanvas agent (Pan et al., 2024) using the GPT-40 API as the backbone and then manually evaluate its trajectories. We further categorized the failure cases for WebCanvas agent and Cognitive Kernel to better understand the limitations of existing systems."}, {"title": "5.5 Private information management", "content": "Figure 5 presents a radar plot illustrating the accuracy of various end-to-end LLM-based systems and open-source LLMs using a parse-then-read pipelines across different question types in the DocBench. The questions in DocBench are categorized into four major types: Text-only, Multimodal (including tables and figures), Meta-data, and Unanswerable questions.\nThe left subfigure compares these systems with the end-to-end systems presented in Table 1. Diving into this detailed comparison, we can observe the specific capabilities of these systems in handling different document-based questions. As shown, Kimi-Chat and Claude3-opus perform well across all these categories, demonstrating balanced performance on different types of questions. Notably, GPT-4 underperforms in the unanswerable category, suggesting potential overfitting in GPT-4's optimized file systems, likely due to training on datasets that only include answerable questions with provided golden answers. On the other hand, Gemini-Pro 1.5 struggles with figure-based questions in documents, with its performance in the multimodal category primarily driven by table-based questions. Coze (GPT-40) performs poorly in handling user documents due to misalignment between system instructions and the LLM's instruction-following capabilities. Other systems show relatively balanced performance, with gaps mainly attributable to the backbone LLMs and their system designs. As shown in the right subfigure, we also compare the results with recent state-of-the-art open-source LLMs using parse-then-read pipelines. To enable LLMs to process documents as input, we use the fitz package to extract text, tables, and images from PDF files, then feed the questions, along with the extracted information, into the LLM to obtain the final answer. It is evident that simply using these LLMs does not lead to strong performance on document-based tasks. Compared"}, {"title": "5.6 Long-term memory management", "content": "The detailed evaluation results of Cognitive Kernel, along with baseline systems on long-term memory, are presented in Table 2. While GPT-40 is generally more powerful than GPT-40-mini, it performs significantly worse in long-term memory evaluation. Upon closely examining the intermediate content within the long-term memory module, we discovered that GPT-40 is more prone to modifying or overwriting existing memories when receiving new input from the user, even when the new content is only semantically similar to the old memory rather than actual update. As a result, the system may lose the ability to answer the final question accurately. Additionally, Coze does not perform as well as GPT in this setting. The choice of the underlying base model also has a significant impact on Coze's performance, with Coze + GPT-3.5-turbo performing substantially worse than Coze + GPT-40. In our system, we found that the Cognitive Kernel's performance is suboptimal when using GPT-40 as the base LLM. This is likely because GPT-40 is not fully aligned with our system prompt even though the prompt is natural and accurate for humans. After switching to the adapted LLM, the performance improved from 59.0% to 85.9%. This observation shows that there is no perfect model and a continuously evolving AI system is crucial in real applications."}, {"title": "6 Applications", "content": "In this section, we showcase two application scenarios of Cognitive Kernel to illustrate how users might interact with the system. The first example is shown in Figure 6. In this case, the user first uploaded a paper called \u201cChain-of-note\u201d(Yu et al., 2023) to the system and asked about the paper's core idea. Cognitive Kernel first processed and indexed the uploaded document internally, and then read the document to answer the question. In the next turn, the user asked about the current number of citations of Chain-of-note, and the system recognized this question to be a real-time information-seeking one and instantiated a web browser to look for the evidence. Finally, based on the search results, Cognitive Kernel provided the correct answer. In the second example from Figure 7, we see that the user first asked the system to search for recent papers about web agents and download the first one it found. Cognitive Kernel again opened a web browser and searched web agent papers. Then it opened the first paper in the results and clicked the download button on the paper's arXiv page. After the paper was downloaded, the system returned the downloaded file's path. In the next turn, the user then asked how many times the keyword \u201cHTML\u201d is mentioned in the paper. Then Cognitive Kernel leveraged its private information management ability to open the paper and count the occurrences of \u201cHTML.\u201d Then it returned the answer 5, which we verified to be correct by opening the paper and manually searching the keyword."}, {"title": "7 Discussions and Limitations", "content": "Despite that Cognitive Kernel achieves promising performance on several realist tasks, there is still a huge gap between Cognitive Kernel and a generalist \u201cautopilot\" system. In this section, we discuss the limitations of our current system, which is also our future working directions."}, {"title": "7.1 Multi-modal Perception Ability", "content": "For a generalist \u201cAutopilot\u201d system, it's critical to have multi-modal perception ability because the real world is multi-modal and the rich information that helps decision-making is often embedded in other modalities beyond text. However, current Cognitive Kernel employs an LLM as the central policy model, thus it cannot handle multi-modal inputs such as images or audio. Intuitively, all three use cases we experimented with could greatly benefit from other modalities and provide a better user experience. For example, for private information management, the system can better understand local files by reading both the text and images embedded in the files. For real-time information management, the system can better understand the websites by observing the visual layout and visual elements not captured in the accessibility tree, therefore navigating the web more effectively. For long-term memory management, the user can also send images to the system or speak to it directly without typing any text, and the system can read and write memories of different modalities to better serve different scenarios. Therefore it is a promising direction to equip Cognitive Kernel with multi-modal perception ability and we leave this exploration for future work."}, {"title": "7.2 Self-improvement Through Search and Feedback", "content": "Even though we give Cognitive Kernel freedom to be a generalist autopilot system, it tends"}]}