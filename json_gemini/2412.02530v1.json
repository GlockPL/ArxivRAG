{"title": "WEM-GAN: Wavelet transform based\nfacial expression manipulation", "authors": ["Dongya Sun", "Yunfei Hu", "Xianzhe Zhang", "Yingsong Hu"], "abstract": "Facial expression manipulation aims to change human facial expressions without\naffecting face recognition. In order to transform the facial expressions to target\nexpressions, previous methods relied on expression labels to guide the manipulation\nprocess. However, these methods failed to preserve the details of facial features, which\ncauses the weakening or the loss of identity information in the output image. In our\nwork, we propose WEM-GAN, in short for wavelet-based expression manipulation\nGAN, which puts more efforts on preserving the details of the original image in the\nediting process. Firstly, we take advantage of the wavelet transform technique and\ncombine it with our generator with a U-net autoencoder backbone, in order to improve\nthe generator's ability to preserve more details of facial features. Secondly, we also\nimplement the high-frequency component discriminator, and use high-frequency\ndomain adversarial loss to further constrain the optimization of our model, providing\nthe generated face image with more abundant details. Additionally, in order to narrow\nthe gap between generated facial expressions and target expressions, we use residual\nconnections between encoder and decoder, while also using relative action units (AUs)\nseveral times. Extensive qualitative and quantitative experiments have demonstrated\nthat our model performs better in preserving identity features, editing capability, and\nimage generation quality on the AffectNet dataset. It also shows superior performance\nin metrics such as Average Content Distance (ACD) and Expression Distance (ED).", "sections": [{"title": "1 Introduction", "content": "Facial expressions are a kind of high-level semantic feature of human faces. It could\nconvey the sensibilities and intention of mankind, which makes it a common but crucial\nway of communication. In the current era of social media, editing human facial\nexpressions without changing the identity has been widely adopted in many different\nfields like VR, gaming, filming, and streaming, etc. Facial expression editing\ntechnology can be combined with multimodal biometric authentication technology to\nenhance system security and accuracy [1]. Facial expression editing can simulate\nvarious expression changes, providing abundant training data for multimodal biometric\nsystems, thereby improving the system's robustness to expression variations [2].\nAdditionally, facial expressions can serve as an extra biometric feature to enhance the\nsecurity of the authentication process [3]. Data augmentation through facial expression\nediting technology [4] can generate more facial expression samples, which can help\ntrain models to better understand and recognize the expression features of different\nindividuals. This augmented dataset can not only improve the model's generalization\nability but also reduce overfitting issues caused by insufficient data [5].\nWith the proposal of Generative Adversarial Networks (GANs) [6], great progress has\nbeen made in facial expression editing by using the powerful generation capabilities of\nGANs in some recent studies. Some researchers take advantage of the GANs to\ndecouple the latent embeddings of the facial expression from the latent embeddings of\nthe human face images and edit the latent embedding to generate the final output\n[7],[8][9]. These methods do not need any labels, while they could depend on the\npretrained generators to yield high-resolution images. Some other studies rely on the\nAction Units (AUs) labels [10], which are used to train the model to generate the edited\nexpression images [11],[12],[13]. These methods are good for both continuous editing\nwith the AUs labels and keeping the identity information. Although the aforementioned\nfacial expression editing models have demonstrated robustness in facial expression\nediting tasks, they still have not addressed some challenging issues. The first challenge\nis fidelity: these models typically generate images with low fidelity, making it\nchallenging to keep the original detail information. The second challenge is the issue of\naverage expressions: the models tend to generate average expressions from the dataset\nto handle various situations, leading to a lack of personalization in the generated images'\nexpressions.\nAll the methods mentioned above require the mapping of input images to low\ndimension latent embeddings through encoder, which are later modified and\nreconstructed by a decoder to complete facial expression editing. However, based on"}, {"title": "2 Related Work", "content": "Generative Adversarial Networks: GANs, due to their unique training process and\npowerful generation capability, have received increasingly more attention upon their\nproposal. However, a lot of attempts have been made to further optimize the adversarial\nloss function of GANs to make the training process more stable. Wasserstein\nGenerative Adversarial Networks(WGAN) [17] introduces the Wasserstein distance to\nmeasure the distance between two distributions, which solves the problems of gradient\ndisappearance and mode collapse in traditional GANs, and WGAN-GP [18] further\nimproves WGAN training procedure by using Gradient Penalty(GP). Meanwhile,\nConditional GAN [19] increases the controllability of the generated images by\nintroducing conditional variables in adversarial training. So far, GANs have\ndemonstrated their powerful capabilities in the areas of image generation [20]-[22],\nimage-to-image transformation [23],[24], and super-resolution [25].\nFacial Expression Editing: Facial expression editing is a very challenging computer\nvision task and owing to the powerful generative power of generative adversarial\nnetworks, GANs-related facial expression editing framework has achieved good results.\nStarGAN [26] makes it possible to translate multi-domain images using target labels\nand a single model that can generate facial expressions with discrete labels. ExprGAN\n[27] linearly combines discrete expression labels and evenly distributed noise to form\na controllable new expression label as condition vector, realizing continuous editing of\nfacial expressions. GANimation [11] uses Action Units (AUs) as expression labels and\nintroduces an attention mechanism to focus on specific editing regions, enhancing the\nediting ability of facial expressions and achieving continuous adjustment. Cascade EF-\nGAN [28] focuses on processing the more detailed regions of the face (like mouth, eyes,\nand nose) to keep the identity-related features, and then uses three cascade generator\nnetworks to gradually edit the facial expressions, addressing the artifacts and blurs\nproblem that current models have. Ling [12] et al. achieve a finer-grained level of facial\nexpression editing by changing the conditional labels from absolute AUs to relative\nAUs and introducing a multi-scale fusion module in the generator. Wang [13] et al.\ntransfer the feature maps from the encoder to the decoder in the generator and use the\nattention mechanism to fuse the feature maps selectively to retain more identity-related"}, {"title": "3 Method", "content": "In this section, we will illustrate the details of WEM-GAN. We will focus on the loss\nfunction and the Detail Information Transmission (DIT) module in our network\nstructure."}, {"title": "3.1 Overview", "content": "Given an input image x of arbitrary facial expression, we denote its expression labels\nas $u_x = {u_x^1, u_x^2 ...,u_x^n}$. Given a target image y, we denote its expression labels as\n$u_y = {u_y^1, u_y^2 ..., u_y^n}$. In these two labels, n is the total number of expression labels, $u_x^i$\nis the intensity of the i-th AU, and its value is normalized between 0 and 1. Our goal is\nto convert the input image into a generated image x' that has the identity information\nof the input image x and the facial expression of y simultaneously, while keeping as\nmuch identity information as possible. We adopt the proposal of Ling [12] et al. to use\nthe relative AUs as the input conditional vector, and the relative AUs represent the\nchange from the original facial expression to the target facial expression, which is\ndenoted as $u_{rel} = U_y \u2013 U_x$."}, {"title": "3.2 Network Structure", "content": "As shown in Fig. 3, our overall network architecture consists of a generator and two\ndiscriminators. The role of the generator is not only to generate fake images with the\ntarget expressions, but also to generate self-reconstructed images and cycle-\nreconstructed images. Then, the information of the images x, $x_{self}$ and $\\hat{x}$ are used in\nthe cycle consistency loss to ensure the preservation of the identity information of the\noriginal image. The role of the discriminator is to ensure the fidelity of the generated\nimages and to supervise whether the generated images have the target expressions and\nhigh frequency detail information."}, {"title": "3.3 Detail Information Transmission Module", "content": "The technique for extracting high frequency components of images is well established\nin the field of computer vision. The wavelet transform can decompose the image signal\ninto different frequencies through low-pass and high-pass filters, where the high-pass\nfilter allows only high-frequency information to pass, and the low-pass filter allows\nlow-frequency information to pass. The two-dimensional discrete wavelet transform\ndecomposes the original image into four frequency domain signals (LL, LH, HL, HH)\nby performing low-pass and high-pass filtering in horizontal and vertical directions,\nrespectively. The LL signal can be taken as an approximation of the original image and\nconsists of low-frequency information. The LH signal contains the detail information\nin the horizontal direction, the HL signal has the detail information in the vertical\ndirection, and the HH signal is the detail information in the diagonal direction. The\nwavelet inverse transform can accurately reconstruct the original image using the\nsignals in the four frequency domains obtained after the above wavelet transform\ndecomposition, as shown in Fig. 4(b).\nFollowing the idea of two-dimensional discrete wavelet transform, we used Haar\nwavelet transform [30], a classical method in wavelet transforms, as WEM-GAN to\nextract details. In Haar wavelet transform, low-pass filter $f_L = {\\frac{1}{\\sqrt{2}}} \\begin{bmatrix} 1 & 1\\\\ 1 & 1 \\end{bmatrix}$, and high-pass\nfilter $f_H = {\\frac{1}{\\sqrt{2}}} \\begin{bmatrix} 1 & 1\\\\ -1 & -1 \\end{bmatrix}$. Fig. 5 shows the effect of image decomposition using Haar\nwavelet transform. From Fig. 5, we can see that the high frequency details of the image\nare concentrated in the LH, HL, and HH components.\nIn WEM-GAN, the purpose of the DIT module is to extract the high-frequency\ncomponents in the feature maps from encoder stages, and use them in the decoder to\navoid the loss of detail information due to convolution and deconvolution layers. So,\nwe perform Haar transform on the feature maps of different resolutions to four\nfrequency domains, then we exclude the LL component signals and transmit the LH,\nHL, and HH signals to the decoder. At last, the high-frequency component signals are\nreconstructed into new feature maps containing only the high-frequency components\nusing Haar inverse transform. The new feature map is channel-fused with the feature\nmap from the decoder to provide more detail information for the decoder to reconstruct\nthe image."}, {"title": "3.4 Loss Functions", "content": "Fig. 3 illustrates our training process. Given an input image x with its expression label\n$u_x$, and the target expression label $u_y$. We denote the relative AUs of the input $u_{rel} =$\n$U_y - U_x$, the generated image with the target expression as $x' = G(x, u_{rel})$, the\ngenerated self-reconstructed image as $x_{self} = G(x, 0)$, and the generated cycle-\nreconstructed image as $\\hat{x} = G(x',-u_{rel})$. In addition, we denote the process of\nobtaining high frequency components from an image using wavelet transform as W.\nThe high frequency detail information of the input image and the output image can be\ndenoted as $x_h = W(x)$ and $x'_h = W(x')$. The loss functions that we use in the model\nare described separately."}, {"title": "3.4.1 Adversarial Loss", "content": "To generate more realistic images from GANs, we extend the improved adversarial loss\nfunctions proposed by WGAN-GP [18]. Our adversarial loss can be written as:\n$L_{Diadv} = -E_x[D_{iadv}(x)] + E_{x'}[D_{iadv}(x')] + \\lambda_{gp}E_{\\hat{x}}[(||\\nabla_{\\hat{x}}D_{iadv}(\\hat{x})||_2 - 1)^2]$ (1)\n$L_{Gadv} = -E_{x,urel}[D_{iadv} (G(x, u_{rel}))]$\nwhere $\\lambda_{gp}$ is a penalty coefficient, and $\\hat{x}$ is a random interpolation between input x and\nthe generated image x'."}, {"title": "3.4.2 High-Frequency Domain Adversarial Loss", "content": "In order to encourage the network to retain more detail information while generating\nimages and ensure that the detail information distribution of the generated images is\nsimilar to that of the original images, we also use the generative adversarial loss as our\nhigh-frequency component loss function.\n$L_{DH} = -E_x[D_{H}(x_h)] + E_{x'}[D_{H}(x'_h)]+\\lambda_{gp}E_{\\hat{x}} [(||\\nabla_{\\hat{x}}D_{H}(\\hat{x_h})||_2 - 1)^2]$ (3)\n$L_{GH} = -E_{x,urel} [D_{H} (W(G(x, u_{rel})))]$"}, {"title": "3.4.3 Expression Label Loss", "content": "In order to generate images with target expression, we follow the idea of GANimation\nand Ling et al. and add an auxiliary regressor $D_{icond}$ to the last layer of the\ndiscriminator $D_1$ to detect the AUs labels of the images. We use the following loss\nfunctions to ensure the successful transformation of facial expressions.\n$L_{DIcond} = E_{x,ux} [||D_{Icond}(x) \u2013 u_x||_2]$\n$L_{Gcond} = E_{x,urel}u_y [||D_{Icond}(G(x, U_{rel})) \u2013 u_y ||_2]$"}, {"title": "3.4.4 Identity Information Loss", "content": "In order to preserve the identity information after expression editing, i.e., the faces in\nthe input and output images can be recognized as the same person. We use the input\nimage, the self-reconstructed image, and the cycle-reconstructed image to form the\nfollowing loss to ensure the basic identity information after editing.\n$L_{rec} = E_x [||x - X_{self} ||_1] + E_{x,urel} [||x \u2212 \\hat{x}||_1]$"}, {"title": "3.4.5 Objective Function", "content": "Combining all the loss functions mentioned above, the loss functions are summed up\nin a certain proportion to form the final loss function of our model, which is shown as\nfollows:\n$L_D = L_{Diadv} + L_{DH} + \\lambda_1L_{DIcond}$\n$L_G = L_{Gadv} + L_{GH} + \\lambda_2L_{Gcond} + \\lambda_3L_{rec}$\nWhere $\\lambda_1$, $\\lambda_2$ and $\\lambda_3$ are the hyperparameters to indicate the contribution of each loss to"}, {"title": "4 Experiments", "content": "the final loss function."}, {"title": "4.1 Implementation Details", "content": "Dataset and Preprocessing. The face image dataset we chose is AffectNet [31], which\nhas more than 1 million images and 11 different categories such as Neutral, Happy, Sad,\nSurprise, Fear, Disgust, Anger, Contempt, None, Uncertain, Non-face. We randomly\nselect a certain number of images from each of the first 10 categories to form our\ntraining set of 200,000 images and our test set of 2000 images to test our model. To\nobtain the expression label information of each image, we use the existing open-source\ntool OpenFace2.0 [32] to extract the AUs labels. In addition, all images are center-\ncropped and resized to 128*128.\nTo validate the robustness of our model, we also selected the FFHQ [33], RAF-DB [34],\nand CelebA [35] datasets for generalization testing, with the images similarly center-\ncropped to 128*128.\nBaseline. We benchmark our model with GANimation [11] and Ling [12] et al. For\nbetter description, we name the model of Ling et al. as FGGAN. We use their open-\nsource GitHub code in our benchmarking for fairness, and use the identical training and\ntest sets for all methods. However, Wang [13] and Bodur [29] et al. do not provide their\ncodes, so we could not evaluate their models and have to skip their methods in\nbenchmarking results.\nExperiment Settings. To ensure a fair comparison with the baseline model [12], we\nadopted its hyperparameter settings, with $\\lambda_1$, $\\lambda_2$, and $\\lambda_3$ values set to 150, 150, and 30,\nrespectively. Besides, we train the model using the Adam [36] optimizer with an initial\nlearning rate of 1*10-4. The optimizer parameters are set to beta1=0.5 and beta 2=0.999.\nThe model has a batch size of 16 and is trained for a total of 50 epochs. The model\nlearning rate decreases linearly starting from the 31th epoch. For every 4 iterations of\ntraining the discriminator, the generator undergoes training once. The whole model\ntraining process takes around 96 hours on a 32G Tesla V100S-PCIE GPU."}, {"title": "4.2 Experiment Metrics", "content": "In order to quantitatively evaluate the performance of our model, we use several\ncommonly used evaluation metrics based on our test set images and the generated target\nimages.\nInception Score (IS). This metric uses Inception Net-V3 to output a 1000-dimensional\nvector for each input image [37], with each dimension representing the probability that\nthe image belongs to a specific category. Statistically, these probabilities are calculated\nas the KL divergence of the conditional and marginal distributions, which can be used\nto measure the clarity and diversity of the generated images. The larger IS means the\nresult is better. Although there are certain limitations of this metric, in this paper, we\ncalculate the IS scores of the generated images following the evaluation methods of\nGANimation [11] and FGGAN [12]. The formula is as follows:\n$IS(x) = expE_{x~pg} [KL(p(y|x)||p(y))]$\nHere, x represents the generated image, Pg represents the distribution of the generated\nimages, and E denotes the expectation over the samples. y represents the Inception\nclassification result of the generated image, p(y|x) denotes the output distribution of\nInception when the generated image x is input, and p(y) represents the average\ndistribution of the categories output by Inception for images generated by the generator\nG. exp denotes the exponential function.\nFrechet Inception Distance (FID). This metric is an improved version of IS score [38].\nIt uses Inception Net-V3 network to extract the features of the image. In this paper, we\nuse 2048-dimensional features from the last pooling layer and calculate the distance\nbetween the generated image features and the real image features. FID describes the\ndistance between the two distributions. The smaller FID score indicates that the\ngenerated image is closer to the real image distribution, and the better the quality of the\ngenerated image is.\n$FID(x, g) = ||\\mu_x \u2013 \\mu_g||_2 + Tr (\\Sigma_x + \\Sigma_g \u2013 2 (\\Sigma_x\\Sigma_g)^{1/2})$\nHere, ux and Ex represent the mean and covariance matrix of the set of 2048-\ndimensional feature vectors detected from real images using the Inception Net-V3\nnetwork. Similarly, ug and \u2211g represent the mean and covariance matrix of the feature\nvectors from the generated images. Tr denotes the trace of a matrix.\nAverage Content Distance (ACD). ACD measures the L2 distance between the latent\nembeddings of the input and generated images [39]. We follow the approach used by\nGANimation [11] and FGGAN [12] to extract the facial embeddings of the input image\nand the generate image and calculate their distances using the well-known facial\nrecognition network\u00b9. Lower values indicate better identity similarity between the\nimages before and after editing.\n$ACD(x, g) = ||f_x - f_g||_2$\nHere, x and g represent the input image and the generated image, respectively. $f_x$ and\n$f_g$ denote the facial encodings extracted using a facial recognition network from the\ninput and generated images, respectively.\nStructure Similarity Index Measure (SSIM). SSIM [40] measures the structure"}, {"title": "4.3 Qualitative Evaluation", "content": "similarity between two images by comparing information such as brightness, contrast\nand structure between them, based on the assumption that the human eye can extract\nstructural information from images. It can provide an evaluation of the image quality\nthat is consistent with human perception. In this paper, we evaluate the ability of the\nmodel to preserve identity information by comparing the SSIM values between the\noriginal and self- reconstructed images.\n$SSIM(x,y) = \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}$\nHere, x and y are two images, \u03bc\u3047 and \u03bcy represent the mean pixel values of the images,\n\u03c3\u03c7 and \u03c3y denote the standard deviation of the pixel values, and oxy represents the\ncovariance between x and y. C\u2081 and C2 are constants.\nExpression Distance (ED). Because the editing of expressions is continuous, we\ncannot simply classify the edited expressions into discrete categories such as Happy,\nSad, etc. to calculate the classification accuracy of expression editing models. To\ndetermine whether the generated images have the target expressions, we also use\nOpenFace2.0 to extract the AUs of the generated images and calculate the 12-distance\nbetween them and the target AUs as the expression distance to approximate the model's\nability to edit expressions.\n$ED = E_{x~Pg} [||\\bar{u}_x - u_y ||_2]$\nHere, x represents the generated image, Pg denotes the distribution of the generated\nimages, and E denotes the expectation over the samples. $\\bar{u}_x$ is the AU label vector\ndetected from the generated images using OpenFace 2.0, and uy is the target AU label\nvector.\nFace Verification Score (FVS). We use Face++\u00b9 to computes the similarity between\ninput and synthesized images, it will return a value between 0 and 100 to indicate the\nlikeness between two faces.\nWe start by comparing the AU editing performance with GANimation and FGGAN.\nFig. 6 shows the editing results for the two classical AU labels AU5 (Upper Lid Raiser)\nand AU45 (Blink), and Fig. 7 shows the editing results (the AU combination of Happy\nexpressions) for the combination of AU6 (Cheek Raiser) + AU12 (Lip Corner Puller)."}, {"title": "4.5 Ablation Study", "content": "In this section, we explore the importance of each component of our proposed method.\nFor the convenience of description, the residual network block in the middle of the\nencoder and decoder, which fuses the relative AUs multiple times, is named the\nmultiple AU fusion module and denoted by Mul_AU, and the high-frequency\ncomponent discriminator is denoted by Dh. We examine these proposed modules using\nFID, ACD, and ED metrics. From the initial U-Net backbone generator, the three\nmodules are gradually added with relative AUs as the conditional vector, and each\nmodel is trained and tested with the same data set. The experimental results are shown\nin Table 6."}, {"title": "4.6 Generalization", "content": "We trained our model on the AffectNet dataset and tested it on other datasets, such as\nFFHQ [33], RAF-DB [34], and CelebA [35], to validate the generalization capability\nof our model."}, {"title": "5 Conclusion and Future Work", "content": "In this study, we propose a new approach, WEM-GAN, to introduce wavelet transform\ntechnique for facial expression editing based on the U-Net backbone. In the generator,\na wavelet transform-based detail information transmission module is implemented to\naddress the information loss when the image is embedded in a low-dimensional latent\nspace, and additionally add a high-frequency component discriminator to constrain the\nnetwork model to further ensure the retention of image detail information. In addition,\nthe residual network blocks with multiple fused relative AUs are used to connect the\nencoder and decoder to enhance the model's ability to edit facial expressions. The\nqualitative and quantitative experimental results show that our model not only performs\nwell in preserving personal identity information, but also shows competitive\nperformance in terms of facial expressions editing and the quality of the generated\nimages.\nAdditionally, as a potential direction for future work, we plan to explore integrating\ntemporal information into our model to achieve video-based facial expression\nmanipulation. This aims to maintain the consistency and fidelity of expressions across\nconsecutive video key frames, enhancing the adaptability and practicality of WEM-\nGAN in other real-world applications."}, {"title": "Conflict of Interest", "content": "The authors declare that they have no conflict of interest."}, {"title": "Data Availability", "content": "Data sharing is not applicable to this article as no datasets were generated during the\ncurrent study."}]}