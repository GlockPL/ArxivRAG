{"title": "ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation", "authors": ["Peiyang Wu", "Nan Guo", "Xiao Xiao", "Wenming Li", "Xiaochun Ye", "Dongrui Fan"], "abstract": "Recently, large language models (LLMs) have demonstrated excellent performance in understanding human instructions and generating code, which has inspired researchers to explore the feasibility of generating RTL code with LLMs. However, the existing approaches to fine-tune LLMs on RTL codes typically are conducted on fixed datasets, which do not fully stimulate the capability of LLMs and require large amounts of reference data. To mitigate these issues, we introduce a simple yet effective iterative training paradigm named ITERTL. During each iteration, samples are drawn from the model trained in the previous cycle. Then these new samples are employed for training in this loop. Through this iterative approach, the distribution mismatch between the model and the training samples is reduced. Additionally, the model is thus enabled to explore a broader generative space and receive more comprehensive feedback. Theoretical analyses are conducted to investigate the mechanism of the effectiveness. Experimental results show the model trained through our proposed approach can compete with and even outperform the state-of-the-art (SOTA) open-source model with nearly 37% reference samples, achieving remarkable 42.9% and 62.2% pass@1 rate on two VerilogEval evaluation datasets respectively. While using the same amount of reference samples, our method can achieved a relative improvement of 16.9% and 12.5% in pass@1 compared to the non-iterative method. This study facilitates the application of LLMs for generating RTL code in practical scenarios with limited data.", "sections": [{"title": "1 INTRODUCTION", "content": "Manually writing hardware description language (HDL) code(e.g., Verilog) is an unavoidable part of the current hardware design process. This step is often boring and cumbersome, consuming a significant amount of engineers' time. As LLMs have demonstrated excellent performance in natural language processing and code generation, researchers try to explore the use of LLMs to generate HDL code as an aid in hardware design [2, 1, 21, 10, 11].\nIn order to make LLMs more professional in RTL code generation, a common method is to use the corresponding database to fine-tune LLMs. However, most existing methods conform to the conventional paradigm of deep learning, which involves initially gathering data and then training the model. This approach may lead to two negative effects:\n(1)Since the model is trained on a limited amount of collected data, the room available for the model to explore is thus constrained, resulting in a narrow coverage of feedback signals.\n(2)There is a mismatch between the distributions of training samples and the LLM under training, which can lead to estimation errors[8, 13] in optimization process. Specifically, if the training samples are not directly generated by the LLM intended for training, their distributions are obviously misaligned. However, even if the training samples are generated by the LLM intended for training at the beginning, their distributions will still mismatch because the distribution of the LLM will shift during the training process.\nDue to these two negative effects, the RTL code generation capability of existing fine-tuned models are limited. Additionally, a large number of reference samples are required for fine-tuning, which can be costly to obtain. In order to foster exploration and mitigate the distribution deviation between training samples and LLMs that is"}, {"title": "2 BACKGROUND", "content": null}, {"title": "2.1 Large Language Models for RTL Code\nGeneration", "content": "Previous works [1, 2] have attempted to directly prompt LLMs to generate RTL code, achieving notable results. Chip-chat[1] design a 8-bit accumulator-based microprocessor with the assistance of commercial LLMs. In practical applications, it is challenging to directly generate usable code through human instructions. As a result, recent works[22, 4] explore optimizing the RTL codes gen-erated by LLMs with feedback from Verilog tools. AutoChip [22] utilize error reports from compilers and simulators to help LLMs to rectify faulty code. [4] develops a Monte Carlo tree-search (MCTS) algorithm to enhance LLMs to generate correct and PPA-efficient code with the feedback from compilers and synthesis tools.\nInstead of using off-the-shelf LLMs directly, some researchers [10] opt to train LLMs to make it specialized in hardware design. Veri-Gen[21] leverage corpora from GitHub and Verilog textbooks to fine-tune open source LLMs, defeating the state-of-the-art com-mercial Codex[3] LLM on 17 Verilog problems. ChipNeMo[11] customize LLaMa2[23] for applications in chip design such as chatbot, generating EDA tool script, and bug summarization. RTLCoder[12] develops a automated flow to generate instruction-code pairs for supervised fine-tuning (SFT) and propose a new SFT method lever-aging code quality assessment."}, {"title": "2.2 Fine-tuning LLMs with RTL data", "content": "To tailor LLMs for RTL code generation, researchers often need to fine-tune LLMs with domain-specific data. VeriGen [21] fine-tune LLMs by predicting next token on corpora from open-source code and textbooks, which can be regarded as an continual pre-training. To facilitate the model's ability to follow instructions, Ver-ilogEval[10] applies SFT on LLMs using synthetic instruction-code pairs. Furthermore, RTLCoder[12] introduces a new fine-tuning algorithm which harnesses code quality evaluation upon candi-dates sampled from pretrained LLMs. However, during the training process, the aforementioned methods are constrained by static and unchanging training samples. According to previous research[24, 14] in natural language, updating train samples using in-training LLMs can significantly bootstrap the model performance. However, related research in the task of Verilog code generation remains scarce. Our work can be seen as a pioneering attempt."}, {"title": "3 APPROACH", "content": "In this section, we firstly detail the workflow of our proposed ap-proach. Then analyze the deficiencies of related methods and reveal the improvements of our approach from a theoretical viewpoint."}, {"title": "3.1 Framework", "content": "As an iterative training scheme, our approach boost the generation capability by alternately sampling and training. During a single loop, we basically follows RTLCoder [12] for ease of implementation. The key distinction lies in our core idea of alternating iterations, which can significantly enhances the performance.\nWe briefly introduce the procedure in a single loop at first, and then explain the iteration process. As shown in Figure 1, in the t-th round of training, for each input instruction s, there are K corresponding output responses \\(a_k\\), \\(1 \\le k \\le K\\). Among these, the first K - 1 responses are sampled from the model \\(\\pi_t\\) acquired from previous training iteration. While the last response \\(a_K\\), which"}, {"title": "3.2 Theoretical Analysis", "content": "Inspired by the previous work [8], we regard fine-tuning LLMs as a reward maximization problem, which can be formulated as follows:\n\\[\\max_{\\pi} \\mathbb{E}_{s \\sim p(.)} \\mathbb{E}_{a \\sim \\pi(\\cdot|s)} [r(s, a)],\\]\nwhere \\(\\mathbb{E}\\) denotes the expected value, s represents the instruction (prompt), following the distribution p(.). a represents the response (code) generated by the LLM \\(\\pi\\) which is being optimized. r represents the reward for the response, in other words, the quality of the generated code.\nIdeally, the true reward for the generated Verilog code should incorporate the evaluation of its functional correctness, as well as PPA metrics. However, it's intractable to implement because verifying the functional correctness requires a comprehensive corresponding testbench, which is nearly impossible for researchers to develop given the large amount of samples for fine-tuning. Additionally, assessing PPA relies on logic synthesis, which also consumes a significant amount of time. Therefore, it is necessary to approximate the reward function to make it more manageable.\nLet's first consider a naive approach that directly fine-tunes LLMs using reference data drawn from teacher, which can be considered a form of knowledge distillation. The optimization objective can be derived as follows:\n\\[\\min_{\\pi} - \\sum_j \\log P_\\pi (a_j | s, a_{<j}) = \\min_{\\pi} \\sum_j  \\log \\frac{P_{\\text{teacher}} (a_j | s, a_{<j})}{\\log P_\\pi (a_j | s, a_{<j})}\\]\n\\[= \\min_{\\pi} \\sum_j KL(P_{\\text{teacher, j}} || P_{\\pi,j})\\]\n\\[\\max_{\\pi} \\mathbb{E}_{s \\sim p(.)} \\mathbb{E}_{a \\sim P_{\\text{teacher}} (s)} [\\mathbb{E}_j -KL(P_{\\text{teacher,j}}|| P_{\\pi,j})]\\]\nThe first equality holds because \\(P_{\\text{teacher}}\\) is independent of \\(\\pi\\). While the approximate equality in the third line is based on the law of large numbers. Comparing equations 5 and 6, it is clear that this approach leads to systematic errors in two aspects\u00b9: (1) replacing the distribution of the model being trained \\(\\pi(\\cdot | s)\\) with that of \\(teacher(\\cdot | s)\\), and (2) the use of the negative Kullback-Leibler divergence (KL divergence) as the surrogate reward function between the probability distribution of the reference data and \\(\\pi\\).\nNext, Let's consider a non-iterative degenerate version of our approach, where a constant distribution \\(\\pi_c\\) is used to generate training samples. This version is quite similar with RTLCoder[12]. We can also transform the optimization objective into a reward maximization problem. To simplify the derivation, here we only present the ranking loss component from Equation 2. And the"}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "Benchmark: We choose a comprehensive evaluation dataset, named VerilogEval[10], as the benchmark to measure the performance. This dataset consists of diverse Verilog tasks ranging from simple to complex, such as combinational circuits, finite state machines, code debugging, constructing testbenches, and so on. Two sets of design instructions are provided: the first one is generated by LLM, named VerilogEval-machine, containing 143 samples; the other one is manually written, named VerilogEval-human, compris-ing 156 samples. Functional correctness evaluation is conducted via ICARUS Verilog simulator by comparing the outputs of the generated design with that of the golden solution.\nMetric: As with many code generation researches, pass@k metric[6] is employed to measure the performance, by regard a problem as solved if at least one of the k generated code samples passes the unit tests. Specifically, for each problem, the model generated n \u2265 k candidates, where c \u2264 n samples pass the unit tests. The pass@k metric is estimated unbiasedly use the following expression[3]:\n\\[pass@k = \\mathbb{E}_{Problems} \\frac{1}{\\binom{n}{k}} \\binom{c}{1}\\]\nTo avoid the impact of randomness, we mainly focus on the pass@1 value under greedy decoding. For a comprehensive eval-uation, we also measure the pass@k = {1, 5, 10} metrics setting n = 10 under Top-p decoding.\nDecoding Strategy: During the training stage, in order to en-hance the diversity and promote the exploration, we use Top-p de-coding strategy with topp = 0.95 and temperature = 0.5. In the test-ing stage, to mitigate random errors, the model is prompted to gener-ate responses with temperature = {0(greedy decoding), 0.2, 0.5, 0.8} and topp = 0.95. For each test metric (pass@1, pass@5, and pass@10), the best result is chosen.\nTraining Details: DeepSeek-Coder-Instruct-6.7B[5] is chosen as the pre-trained model. We use the open-source instruction-code pairs from [12] as reference data. This dataset contains nearly 27k samples. We randomly sample only 10,000 entries to train the final version of our model. The learning rate is set to 10-5. AdamW optimizer is employed with \u03b2\u2081 = 0.9 and \u03b22 = 0.999. The bp16 Mixed-precision Training is adopted to avoid overflow. During each iteration, the model is trained for 3 epochs. The total number of iterations is set to 7. We conduct all experiments on 4 NVIDIA A800 GPUs. As for parameters mentioned in Section 3.1, we set the number of candidates K = 4. The hyper-parameters a and \u03b2 in Equation 2 are set to 0.3 and 0.2, respectively."}, {"title": "4.1 Experimental Setup", "content": null}, {"title": "4.2 Comparison to State-of-the-Art Methods", "content": "As Table 1 shows, our model reach the state-of-the-art level among open-source models. On Pass@1 metric, our model trained with only 10k reference samples surpasses RTLCoder with 27k reference"}, {"title": "4.3 Effect of Iterations", "content": "To delve deeper into the impact of iteration, we plot how pass@1 on VerilogEval-human varies with the number of iterations in Figure 2. It can be clearly seen that, with 5 iterations and only 10k reference data, our model beats the baseline model with with 27k reference data. From the first to the fifth iteration, the pass@1 rate exhibits a clear upward trend, indicating the efficacy of iterative training. From the fifth to the seventh iteration, the pass@1 rate gradually decreases, which can be explained by the mismatch between the sur-rogate reward function and the true reward function mentioned in Section 3.2. In fact, similar discoveries were found on LLMs trained using reward models on natural languages. Our work reveals that the surrogate reward function based on code quality evaluation and reward model on natural languages have similar properties, providing inspiration for subsequent improvement work.\nFigure 3 depicts different loss functions curves across each itera-tion. Even though the loss function converges within each round, the loss value can still decrease by leveraging newly sampled data points from updated model, validating the effectiveness of the pro-posed iterative training approach. Another observed phenomenon"}, {"title": "4.4 Ablation Study of Softmax Normalization", "content": "The baseline method [12] employ a softmax normalization for con-ditional log probability after Equation 1:\n\\[P_k = \\frac{e^{P_k}}{\\sum_{i=1}^K e^{P_i}}\\]\nWe conduct an ablation study to investigate its impact in iteration training. All other settings remain unchanged except for softmax normalization. From Figure 4, it can be observed that the pass@1 of the approach with softmax shows smoother variations across iterations, which may be caused by vanishing gradients brought by"}, {"title": "5 FUTURE WORK", "content": "Reviewing previous theoretical and experimental analyses, we be-lieve that exploring two directions could further enhance the capa-bility of LLMs. The first is to find a more suitable surrogate reward mechanism to provide feedback. Current methods simply rely on evaluating similarity or syntax checking, which is too superficial to reflect the true quality of RTL code. A more comprehensive reward mechanism is likely to further unleash the potential of training methods.\nAnother direction is to develop more advanced optimization al-gorithms. In this work, to enhance simplicity and usability, we avoid using complex reinforcement learning methods, which might better optimize models with careful tuning of hyper-parameters. Draw-ing inspiration from Direct Preference Optimization (DPO) [18] in natural language generation, we look forward to seeing both sim-pler and more advanced optimization solutions in the RTL code generation domain in the future."}, {"title": "6 CONCLUSION", "content": "In this paper, we propose an iterative training paradigm to fine-tune LLMs for RTL code generation. During each iteration, we employ the model trained in the previous iteration to update sam-ples, which are then utilized for training in the current round. From a perspective of maximizing the reward function, we theoretically analyze the superiority of our approach, which are subsequently validated by empirical results. With just about 37% of the reference samples, our model outperforms the state-of-the-art open-source LLMs with 42.9% and 62.2% pass@1 rate on VerilogEval-human and VerilogEval-machine benchmarks respectively. Compared to GPT-4, our model achieves a comparable level of performance on evaluation benchmarks with only 6.7B parameters and affordable overhead. Based on theoretical analysis and experiments, we antic-ipate future improvements through refining reward functions and exploring new training approaches."}]}