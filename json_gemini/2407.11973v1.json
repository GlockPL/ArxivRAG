{"title": "Preliminary Study of the Impact of AI-Based Interventions on Health and Behavioral Outcomes in Maternal Health Programs", "authors": ["Arpan Dasgupta", "Aparna Hedge", "Niclas Boehmer", "Neha Madhiwalla", "Bryan Wilder", "Milind Tambe", "Aparna Taneja"], "abstract": "Automated voice calls are an effective method of delivering maternal and child health information to mothers in underserved communities. One method to fight dwindling listenership is through an intervention in which health workers make live service calls. Previous work has shown that we can use AI to identify beneficiaries whose listenership gets the greatest boost from an intervention. It has also been demonstrated that listening to the automated voice calls consistently leads to improved health outcomes for the beneficiaries of the program. These two observations combined suggest the positive effect of AI-based intervention scheduling on behavioral and health outcomes. This study analyzes the relationship between the two. Specifically, we are interested in mothers' health knowledge in the post-natal period, measured through survey questions. We present evidence that improved listenership through AI-scheduled interventions leads to a better understanding of key health issues during pregnancy and infancy. This improved understanding has the potential to benefit the health outcomes of mothers and their babies.", "sections": [{"title": "1 INTRODUCTION", "content": "Mobile Health (mHealth) programs can make essential health information accessible to less privileged communities. These programs utilize the large accessibility of mobile phones to spread critical health information but often suffer from beneficiary's loss of interest and subsequent drops in listenership over time. To address this, interventions such as a call or a visit from a community health worker can be an effective tool that keeps beneficiaries engaged in the program. However, the question of who should receive an intervention is a non-trivial prediction and planning problem. Previous work has established that AI can be used to schedule such interventions effectively in some mHealth programs, resulting in a significant increase in engagement in the program [19].\nWe partner with ARMMAN [1], an India-based non-profit organization that offers mHealth programs to increase awareness of antenatal and postnatal health amongst mothers. We focus on their mMitra [2] program, which is the second-largest maternal mHealth program in the world. In this program, weekly automated voice messages deliver essential maternal health information to the beneficiaries. There is a limited number of live service calls that can be conducted by health workers every week to boost beneficiaries' engagement due to limited support staff in the NGO. SAHELI [19], a project developed in the context of mMitra, is the first-ever large-scale deployment of AI in a mHealth program that effectively allocates these limited intervention resources. We study the effects of AI-scheduled interventions on the knowledge and behavior of the beneficiaries enrolled in mMitra through a conducted survey.\nPrevious studies [6, 11] established that mothers who consistently listen to mMitra's automated voice messages have improved infant care practices and knowledge of maternal practices. In addition, previous studies [4, 10, 19] have also shown that AI-scheduled interventions boost engagement in the program, as they increase the amount of time the beneficiaries listen to the automated voice calls in a statistically significant. As a result, AI-scheduled interventions lead to an increased exposure of beneficiaries to critical health information. In contrast, if interventions are scheduled uniformly at random, studies were unable to establish a significant effect [10]. This suggests an intuitive correlation between AI-scheduled interventions and improved health practices. However, no previous work has linked the usage of AI assistance and improved health outcomes.\nIn this work, we aim to establish a correlation between AI-scheduled interventions and improved behavioral and health outcomes in the mMitra program. For this, we conduct a survey on the beneficiaries, which aims to assess the beneficiaries' knowledge of good health practices discussed in the automated voice calls. We hypothesize that the increase in listenership resulting from the AI-scheduled interventions should lead to improved knowledge and thus better survey outcomes for people receiving an intervention. To test this hypothesis, we conducted a randomized control trial with two arms of beneficiaries with one intervention arm receiving interventions as scheduled by the AI and a second control arm receiving no interventions. We compare the knowledge and behavioral outcomes of beneficiaries in the two arms using the results of the conducted survey.\nIn our analysis and evaluation of the study, we establish a statistically significant increase in listenership caused by the AI-scheduled interventions. Moreover, we also observe generally better behavior and health practices in the intervention arm (as measured by the survey). However, we are unable to establish a statistically significant difference between the two arms with a high level of certainty due to small sample sizes and large amounts of noise in the responses. Our result calls for a modified larger-scale study to better establish our hypothesis, which we are running at the moment.\nIn sum, in this work, we showcase preliminary results demonstrating the impact of AI-scheduled interventions on the health and behavior of mothers enrolled in mMitra, a maternal health program. We also establish a statistically significant increase in listenership in the program. The paper is organized as follows. In Section 2, we briefly discuss previous work in this domain. In Section 3, we discuss the setup of the study including intervention scheduling and the conducted survey. In Section 4, we describe our analysis method for the survey and our obtained results."}, {"title": "2 RELATED WORK", "content": "The problem of how to allocate limited resources comes up in several domains which require planning. Restless multi-arm bandits (RMABs) are a popular tool for such sequential allocation problems in uncertain environments. In particular, RMABs have shown to be very useful in applications such as anti-poaching patrols [13], multi-channel communication [8], scheduling [3, 21], aerial vehicle routing [22], and machine maintenance and sensor monitoring [5]. These limited resource allocation problems naturally also appear when planning interventions in mHealth programs [9].\nPast work has established that the health information provided in mHealth programs leads to improved infant care practices and knowledge of maternal practices among mothers [6, 11]. In particular, Hegde and Doshi [6] use a randomized controlled trial to measure the effect of tailored voice calls on mothers in mMitra. Hegde and Doshi [6] establish statistically significant results for improved infant care knowledge among mothers as well as a direct impact on infant health as measured by their birth weight.\nThese results motivate ARMMAN to try to boost beneficiary's listenership through service calls by health workers.\nIn collaboration with ARMMAN, Mate et al. [10] describe an AI-based method for scheduling intervention calls. This method decides how service calls are allocated using the RMAB framework, where each beneficiary is modeled as a Markov decision process. Their method was initially tested in simulations, and subsequently in a field study before it was finally deployed at scale in practice [19]. A fundamental challenge in SAHELI has been to learn the transition probabilities of the Markov decision processes modeling beneficiaries. After multiple refinement steps, Wang et al. [20] and Verma et al. [18] utilized decision-focused learning (DFL) [16] for RMABs to learn transition probabilities as to improve the performance of the program in deployment.\nSo far, the observable objective that is optimized by SAHELI and other intervention scheduling programs for ARMMAN is the mother's listenership of automated voice calls and hence program's performance is always measured in terms of improvement in listen-ership metrics. However, no correlation has been shown between AI-scheduled interventions and behavioral outcomes, a gap that we investigate in our study."}, {"title": "3 SETUP OF THE STUDY", "content": "As the first step in the study, we divided the registered beneficiaries into cohorts based on their time of enrollment. Subsequently, for each cohort, we divide the beneficiaries into intervention and control groups. The automated voice calls containing health information are received by everyone in both arms throughout their enrollment in the program. Those enrolled in the intervention group are eligible for receiving interventions. The AI algorithm decides which beneficiaries receive interventions in the form of live service calls from health workers in a given week. In the end, a survey is conducted on subsets from both intervention and control groups to measure the behavioral and health knowledge of the beneficiaries.\n3.1 Experiment Arms\n3.1.1 Cohorts. The study was conducted in three cohorts with a combined number of 60464 beneficiaries.\n\u2022 Cohort 1: 27688 beneficiaries. Registered between 15th Au-gust 2022 to 31st of September 2022.\n\u2022 Cohort 2: 13972 beneficiaries. Registered between 1st Octo-ber 2022 to 31st October 2022.\n\u2022 Cohort 3: 18804 beneficiaries. Registered between 1st No-vember 2022 to 31st November 2022.\nAs explained in Section 3.2.1, these cohorts were not viewed as fully independent by the program and are instead primarily used to determine when beneficiaries are eligible for receiving an inter-vention.\n3.1.2 Division Into Arms. For each cohort, we split the beneficiaries into intervention and control arms so that attribute distributions between arms are similar. This resembles covariate adaptive ran-domization [7] where distributions of the covariates are balanced across the groups. We balance between the following attributes:\nEngagement states\n\u2022 For every beneficiary and a specific automated voice call, we define its engagement state $E@T$ at threshold $T$ as $E@T = 1$ if the beneficiary listened to at least $T$ seconds of the call and 0 otherwise.\n\u2022 For every beneficiary, we calculate $E@T_w$ for $w$ weeks before the expected intervention start date for the cohort.\n\u2022 We strive for achieving approximately equal values of $E@T_w$ for $T \\in \\{1, 5, 10, 30, 100\\}$ and $w \\in \\{1,2,3\\}$ between arms to ensure a similar distribution of listenership profiles across the two arms.\nDemographic Features We consider the gestational age of beneficiaries in terms of their current trimester as a feature for preparing cohorts. This quantity is calculated by dividing gestational age in weeks by 14 which gives 4 bins of equal size. We make sure that both groups have an equal number of beneficiaries in each of the trimesters. This also makes sure that both groups contain an equal number of beneficiaries who have already delivered.\nTo ensure a balanced distribution of these attributes between arms we first create a vector $Y$ for each beneficiary by appending the attributes. We then split the beneficiaries into two equal groups using $Y$ as the splitting factor [20]. This is done by treating $Y$ as a class label and using a stratified splitter to split it into two equal groups. We use the stratified option in test_train_split in the sklearn [12] package. Since we have enough beneficiaries in the cohort, we are able to get an exact split.\n3.2 Conducting Interventions\nInterventions in mMitra are service calls made by healthcare workers that aim to boost the future listenership of automated messages of the called beneficiary.\n3.2.1 Number of Interventions per week. Interventions began on 21st November 2022. We only intervene on beneficiaries that have been present for at least 6 weeks in the program. In the beginning, we are only allowed to act on beneficiaries from cohort 1, then after some time on beneficiaries from cohorts 1 and 2, and then finally on beneficiaries from all three cohorts. This was done to simulate the deployment which considers several months of enrollments at the same time:\n\u2022 21st November 2022 to 12th December 2022 (4 weeks) - con-sider only Cohort 1 for interventions.\n\u2022 19th December 2022 to 9th January 2022 (4 weeks) consider Cohort 1 + Cohort 2 for interventions.\n\u2022 16th January 2022 to 13th February 2022 (5 weeks) consider Cohort 1 + Cohort 2 + Cohort 3 for interventions.\nWe conduct approximately 1000 interventions per week while ensuring that each beneficiary can be intervened on only once. We end up conducting interventions on about 12000 beneficiaries which accounts for about 43% of the intervention arm.\n3.2.2 Eligibility for Interventions. Beneficiaries are eligible for in-terventions under the following conditions.\n(1) Active status - they are still enrolled in the program and receive automated voice messages.\n(2) They picked up at least 1 voice message in the last 4 weeks before the start of the intervention period for the respective cohort.\n(3) Repeat Intervention - beneficiaries have not received a pre-vious service call.\n3.2.3 Conducting Interventions. Each week, the DFL-RMAB [20] algorithm which is our AI algorithm of choice, determines the set of beneficiaries from the intervention arm who will receive an intervention. We collect all beneficiaries from the intervention arm that have received an intervention in some week in a list $I_D$. We also simulate the Al algorithm on the control arm to determine the set of beneficiaries that would have been selected for an intervention (assuming we conducted the same number of interventions as in the intervention arm). As in the intervention arm, beneficiaries in the control arm cannot be selected multiple times. We collect the beneficiaries from the control arm that have been selected by the algorithm in some week into a list $I_C$. We create an intervention list $I$ that combines $I_D$ and $I_C$. The idea is that we later compare the behavior of beneficiaries from $I_D$ and $I_C$, as we can think of beneficiaries from $I_C$ as the counterparts of those from $I_D$.\n3.3 Health Survey\n3.3.1 Conducting the Survey. The health survey is conducted on the beneficiaries from the intervention list $I$ between September and November in 2023. Since the program is oriented towards helping beneficiaries who lack resources the most, we perform the survey only on beneficiaries who are \u201chigh-risk\u201d. A beneficiary is considered \"high-risk\" if they satisfy at least one of the following three conditions: low level of formal education (less than grade 10), low-income family (less than 10000 INR per month) or they do not own the phone themselves. Around 50% beneficiaries enrolled in the program are high-risk.\nThe subset of the \"high-risk\" women who give birth between the intervention and survey call and have been in the program for at least 3 months are called by a health worker and asked to answer the questions from the survey. However, the survey calls are only picked up by a fraction of beneficiaries (3376 out of the 6448 called). This provides a challenge for the evaluation of our study, as we only have access to the \"outcome\" of a subset of beneficiaries, i.e., those who were willing to answer to the survey questions (in particular, this group of beneficiaries is not chosen uniformly at random). This makes it for instance necessary to re-balance the control and intervention group for the final comparison.\n3.3.2 Survey Questions. Each participant was asked 20 questions with the intent of measuring their engagement with the program and measuring knowledge in different areas such as health prac-tices. This evaluation is guided by the content of the automated voice messages and assesses how well the beneficiary received the messages. Concretely, the categories covered in the survey are: en-gagement with the program, knowledge, breastfeeding practices, communication, and health supplements.\nOut of the 20 questions, 8 correspond to single choice (Yes/No) questions. The remaining 12 correspond to questions where scores are calculated based on multiple possible correct answers with some answers potentially contributing a higher score than others. Bene-ficiaries may choose one or more of these answers and their score is the total score for correctly identified answers in the question."}, {"title": "4 RESULTS AND ANALYSIS", "content": "4.1 Comparison Methodology\nWe compare the survey responses of beneficiaries in the intervention group and the control group. However, in doing so, we face multiple challenges. Firstly, not everyone from the list $I$ picks up and answers the survey call. Secondly, some beneficiaries from the intervention list $I_D$ of the intervention arm do not pick up the intervention call. To ensure that the beneficiaries from the intervention and control arms that we compare to each other are similar in distribution, we use a matching method to pair beneficiaries from different arms.\n4.1.1 Beneficiary Matching. We perform a one-to-one pairing of beneficiaries that answered the survey in the two arms by using matching methods with feature variables [14, 15]. The features used for this matching are the average listenership over the last 6 weeks from the intended date of intervention, gestational age, and the number of children they conceived previously. Let $X_i$ be the feature vector of the $i$th beneficiary and $X$ the feature matrix consisting of stacked vectors of all beneficiaries who responded to the survey. We define a closeness metric [17] between the feature vectors of two beneficiaries as their Mahalonobis distance\n$D_{ij} = (X_i - X_j)' \\Sigma^{-1}(X_i \u2013 X_j)$,\n(1)\nwhere $\\Sigma$ is the variance matrix of $X$ in the pooled dataset.\nFinally, for each beneficiary in the intervention group who responded to the survey and picked up the intervention, we greedily pick the closest beneficiary from the control group who responded to the survey using the above-mentioned distance metric. Once a control beneficiary is matched, we no longer consider it for further matching. Finally, we keep the beneficiaries that are part of one pair to obtain two sets of beneficiaries $S_{D} \\subseteq I_D$ and $S_{C} \\subseteq I_C$ for the intervention and control arms, respectively.\n4.1.2 Improving Beneficiaries. As we will discuss in Section 4.2.2, establishing effect sizes in the full cohort of beneficiaries is challenging, among others, because some beneficiaries do not show an improvement in listenership after the intervention call. Recalling that previous work [6] has identified that listening to automated calls leads to better health outcomes, we expect that beneficiaries who experienced the greatest listenership boost through the intervention will show the most significant improvements in survey results.\nTo this end, we formulate a method for identifying which beneficiaries have gained the most from the intervention. We define two listenership values for each beneficiary in the intervention list $I$, the average listenership over the past 6 weeks before the scheduled intervention date, called the pre-listenership and the average listen-ership over the next 12 weeks after the scheduled intervention date, called the post-listenership. The scheduled intervention date refers to the week in which the beneficiary from $I$ has been selected by the algorithm independent of whether they are in the treatment or control arm (and thereby independent of whether they actually received the intervention). Taking 12 weeks of post-listenership (instead of 6 weeks) allows us to measure the long-term gains from the interventions.\nWe calculate the quartile each beneficiary belongs to when compared with the other beneficiaries in $I$ for the pre- and post-listenership. For beneficiary $i$, we denote these values as $q_{1i}$ and $q_{2i}$, respectively. We say that a beneficiary gains in quartiles if $q_{2i} > q_{1i}$. In our analysis, we focus on the beneficiaries that gain in listenership, i.e., for which $q_{2i} > q_{1i}$. Specifically, in our analysis, we restrict our attention to the beneficiaries from the set $S_d$ with $q_{2i} > q_{1i}$ (called $R_d$) and the beneficiaries from $S_c$ matched to these beneficiaries (called $R_c$). Our final comparison involves 218 pairs (i.e., 436 beneficiaries in total).\nOur method of selecting beneficiaries who have gained in quar-tiles is naturally only an approximation for the beneficiaries who benefit the most from an intervention. Note that a larger subsec-tion of beneficiaries who received an intervention gain quartiles (218) compared to those who lose quartiles (119). This implies that interventions oftentimes lead to a gain in quartiles."}, {"title": "4.2 Analysis", "content": "4.2.1 Establishing Improved Listenership. We observe a consistent gain in listenership among beneficiaries who received interventions. In particular, following the work of Boehmer et al. [4], we compare (a) the beneficiaries from the intervention group that have been selected for an intervention (i.e., they are on $I_D$) and that responded to the survey call to (b) the beneficiaries from the control group that would have been selected for an intervention (i.e., they are on $I_C$) and responded to the survey call. We compare these groups using the subgroup estimator, see the work of Boehmer et al. [4] for details. We find that an intervention increases beneficiaries listenership compared to the control group by, on average, 7.43 seconds per call over the 12 automated voice calls following the intervention, which leads to a summed additional listenership of 89.16 seconds over the next 12 calls. The 95% confidence interval for this value is [45.516, 132.936] and the hypothesis that interventions have a positive non-zero effect on listenership can be accepted with a p-value of 6.4656 \u00d7 10\u22125. This allows us to conclude a statistically significant positive effect of interventions on beneficiary's listener-ship. Notably, the positive effect of interventions has already been established in previous studies [4, 10, 18]; however, our identified effect sizes and achieved confidence level are advantageous.\n4.2.2 Establishing Behavioral and Health Benefits. We now ana-lyze the beneficiary's performance in the survey. We will start by focusing on the beneficiaries on which interventions have been effective. Figures 2 and 3 show the results on single choice and multiple choice questions, respectively. We plot the average score obtained on each question for the groups $R_d$ and $R_c$, i.e., beneficia-ries from the treatment group whose listenership improved and who answered the survey and their counterparts. The empirical standard error is included as error bars in the plot to signify the possible error in the comparison. Note that independent of the group membership beneficiaries achieve generally higher scores on the single choice questions (arguably, because it is easier to respond to Yes/No questions during a survey).\nThe average score of the intervened beneficiaries from $R_d$ is higher than the average score of the control beneficiaries from $R_c$ on all but one question. For two questions, the difference between average scores is also statistically significant, i.e., single choice question 5 and multiple choice question 3. For the latter question (\"How does the baby respond when you speak or talk?\"), we can establish a difference with a p-value of 0.0021, showing an improved awareness of the baby's behavior. A very high variance in answers is observed in some questions such as Question 6 and 7 from the multiple choice section, which ask about when a child should be first fed milk and how many times a day they should be fed. This trend could be due to either a lack of information among some mothers, or due to the noise in the way these questions were answered during the survey.\nFigure 4 plots the averaged summed scores on different groups of questions. In particular, the leftmost bars, depict the summed average score over all questions. We find that the difference between the two arms is larger than one standard error here and that we can establish a difference between the two arms with a p-value of 0.13. While this is not enough to claim statistical significance, it is a strong hint towards the positive impact of the AI-scheduled interventions and motivates us to conduct a larger follow-up trial. Figure 4 also covers the summed score in questions from different categories, that are, engagement with the program, knowledge, breastfeeding practices, communication, and health supplements. The questions comprising each category are provided in the appendix (Appendix A). We notice that the engagement category provides the strongest trends and shows the largest difference between the arms. This is likely because the relationship between listenership and engagement with the program is a direct one, while the other outcomes interfere with more confounding factors.\nIn Figures 5 to 7, we show the results for all beneficiaries that filled out the survey from the intervention group and their counterpart from the control group, i.e., $S_D$ and $S_C$. Compared to the above analysis, we observe slightly weaker, yet still positive trends. In particular, the intervention group performs on average better in 7 out of 8 single choice questions and 8 out of 12 multiple choice questions. This leads to a better average performance across all question categories, yet the differences between the two groups are much smaller here than on the beneficiaries that gained quantiles. This is to be expected recalling that the gaining beneficiaries are the ones that respond most favorably to the intervention and will thus also be the ones with the highest knowledge gain."}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In conclusion, our study has shown that AI-scheduled interventions lead to significantly higher listenership in automated voice messages. Moreover, we provide some first evidence that these interventions also lead to improved behavioral and health out-comes as measured by our survey. Together with the NGO, we aim to redesign the study with a more focused cohort and updated survey questions to better understand and correlate the impact of intervention-induced improved listenership on behavioral out-comes."}]}