{"title": "Preliminary Study of the Impact of Al-Based Interventions on Health and Behavioral Outcomes in Maternal Health Programs", "authors": ["Arpan Dasgupta", "Aparna Hedge", "Niclas Boehmer", "Bryan Wilder", "Neha Madhiwalla", "Milind Tambe", "Aparna Taneja"], "abstract": "Automated voice calls are an effective method of delivering mater- nal and child health information to mothers in underserved commu- nities. One method to fight dwindling listenership is through an in- tervention in which health workers make live service calls. Previous work has shown that we can use AI to identify beneficiaries whose listenership gets the greatest boost from an intervention. It has also been demonstrated that listening to the automated voice calls con- sistently leads to improved health outcomes for the beneficiaries of the program. These two observations combined suggest the posi- tive effect of AI-based intervention scheduling on behavioral and health outcomes. This study analyzes the relationship between the two. Specifically, we are interested in mothers' health knowledge in the post-natal period, measured through survey questions. We present evidence that improved listenership through AI-scheduled interventions leads to a better understanding of key health issues during pregnancy and infancy. This improved understanding has the potential to benefit the health outcomes of mothers and their babies.", "sections": [{"title": "1 INTRODUCTION", "content": "Mobile Health (mHealth) programs can make essential health infor- mation accessible to less privileged communities. These programs utilize the large accessibility of mobile phones to spread critical health information but often suffer from beneficiary's loss of in- terest and subsequent drops in listenership over time. To address this, interventions such as a call or a visit from a community health worker can be an effective tool that keeps beneficiaries engaged in the program. However, the question of who should receive an intervention is a non-trivial prediction and planning problem. Pre- vious work has established that AI can be used to schedule such interventions effectively in some mHealth programs, resulting in a significant increase in engagement in the program [19].\nWe partner with ARMMAN [1], an India-based non-profit or- ganization that offers mHealth programs to increase awareness of antenatal and postnatal health amongst mothers. We focus on their mMitra [2] program, which is the second-largest maternal mHealth program in the world. In this program, weekly automated voice messages deliver essential maternal health information to the beneficiaries. There is a limited number of live service calls that can be conducted by health workers every week to boost beneficia- ries' engagement due to limited support staff in the NGO. SAHELI [19], a project developed in the context of mMitra, is the first-ever large-scale deployment of AI in a mHealth program that effectively allocates these limited intervention resources. We study the effects of AI-scheduled interventions on the knowledge and behavior of the beneficiaries enrolled in mMitra through a conducted survey.\nPrevious studies [6, 11] established that mothers who consis- tently listen to mMitra's automated voice messages have improved infant care practices and knowledge of maternal practices. In addi- tion, previous studies [4, 10, 19] have also shown that AI-scheduled interventions boost engagement in the program, as they increase the amount of time the beneficiaries listen to the automated voice calls in a statistically significant. As a result, AI-scheduled interven- tions lead to an increased exposure of beneficiaries to critical health information. In contrast, if interventions are scheduled uniformly at random, studies were unable to establish a significant effect [10]. This suggests an intuitive correlation between AI-scheduled in- terventions and improved health practices. However, no previous work has linked the usage of AI assistance and improved health outcomes.\nIn this work, we aim to establish a correlation between AI- scheduled interventions and improved behavioral and health out- comes in the mMitra program. For this, we conduct a survey on the beneficiaries, which aims to assess the beneficiaries' knowledge of good health practices discussed in the automated voice calls. We hypothesize that the increase in listenership resulting from the"}, {"title": "2 RELATED WORK", "content": "The problem of how to allocate limited resources comes up in several domains which require planning. Restless multi-arm bandits (RMABs) are a popular tool for such sequential allocation problems in uncertain environments. In particular, RMABs have shown to be very useful in applications such as anti-poaching patrols [13], multi-channel communication [8], scheduling [3, 21], aerial vehicle routing [22], and machine maintenance and sensor monitoring [5]. These limited resource allocation problems naturally also appear when planning interventions in mHealth programs [9].\nPast work has established that the health information provided in mHealth programs leads to improved infant care practices and knowledge of maternal practices among mothers [6, 11]. In par- ticular, Hegde and Doshi [6] use a randomized controlled trial to measure the effect of tailored voice calls on mothers in mMitra. Hegde and Doshi [6] establish statistically significant results for improved infant care knowledge among mothers as well as a direct impact on infant health as measured by their birth weight.\nThese results motivate ARMMAN to try to boost beneficiary's listenership through service calls by health workers.\nIn collaboration with ARMMAN, Mate et al. [10] describe an AI-based method for scheduling intervention calls. This method decides how service calls are allocated using the RMAB framework, where each beneficiary is modeled as a Markov decision process. Their method was initially tested in simulations, and subsequently in a field study before it was finally deployed at scale in practice [19]. A fundamental challenge in SAHELI has been to learn the transition probabilities of the Markov decision processes modeling beneficia- ries. After multiple refinement steps, Wang et al. [20] and Verma et al. [18] utilized decision-focused learning (DFL) [16] for RMABS to learn transition probabilities as to improve the performance of the program in deployment.\nSo far, the observable objective that is optimized by SAHELI and other intervention scheduling programs for ARMMAN is the mother's listenership of automated voice calls and hence program's performance is always measured in terms of improvement in listen- ership metrics. However, no correlation has been shown between AI-scheduled interventions and behavioral outcomes, a gap that we investigate in our study."}, {"title": "3 SETUP OF THE STUDY", "content": "As the first step in the study, we divided the registered beneficia- ries into cohorts based on their time of enrollment. Subsequently, for each cohort, we divide the beneficiaries into intervention and control groups. The automated voice calls containing health infor- mation are received by everyone in both arms throughout their enrollment in the program. Those enrolled in the intervention group are eligible for receiving interventions. The AI algorithm decides which beneficiaries receive interventions in the form of live service calls from health workers in a given week. In the end, a survey is conducted on subsets from both intervention and control groups to measure the behavioral and health knowledge of the beneficiaries.\n3.1 Experiment Arms\n3.1.1 Cohorts. The study was conducted in three cohorts with a combined number of 60464 beneficiaries.\n\u2022 Cohort 1: 27688 beneficiaries. Registered between 15th Au- gust 2022 to 31st of September 2022.\n\u2022 Cohort 2: 13972 beneficiaries. Registered between 1st Octo- ber 2022 to 31st October 2022.\n\u2022 Cohort 3: 18804 beneficiaries. Registered between 1st No- vember 2022 to 31st November 2022.\nAs explained in Section 3.2.1, these cohorts were not viewed as fully independent by the program and are instead primarily used to determine when beneficiaries are eligible for receiving an inter- vention.\n3.1.2 Division Into Arms. For each cohort, we split the beneficiaries into intervention and control arms so that attribute distributions between arms are similar. This resembles covariate adaptive ran- domization [7] where distributions of the covariates are balanced across the groups. We balance between the following attributes:\nEngagement states\n\u2022 For every beneficiary and a specific automated voice call, we define its engagement state $E@T$ at threshold $T$ as $E@T = 1$ if the beneficiary listened to at least $T$ seconds of the call and 0 otherwise.\n\u2022 For every beneficiary, we calculate $E@T_w$ for $w$ weeks before the expected intervention start date for the cohort.\n\u2022 We strive for achieving approximately equal values of $E@T_w$ for $T\\in \\{1, 5, 10, 30, 100\\}$ and $w\\in \\{1,2,3\\}$ be- tween arms to ensure a similar distribution of listenership profiles across the two arms.\nDemographic Features We consider the gestational age of beneficiaries in terms of their current trimester as a feature"}, {"title": "3.2 Conducting Interventions", "content": "Interventions in mMitra are service calls made by healthcare work- ers that aim to boost the future listenership of automated messages of the called beneficiary.\n3.2.1 Number of Interventions per week. Interventions began on 21st November 2022. We only intervene on beneficiaries that have been present for at least 6 weeks in the program. In the beginning, we are only allowed to act on beneficiaries from cohort 1, then after some time on beneficiaries from cohorts 1 and 2, and then finally on beneficiaries from all three cohorts. This was done to simulate the deployment which considers several months of enrollments at the same time:\n\u2022 21st November 2022 to 12th December 2022 (4 weeks) - con- sider only Cohort 1 for interventions.\n\u2022 19th December 2022 to 9th January 2022 (4 weeks) consider Cohort 1 + Cohort 2 for interventions.\n\u2022 16th January 2022 to 13th February 2022 (5 weeks) consider Cohort 1 + Cohort 2 + Cohort 3 for interventions.\nWe conduct approximately 1000 interventions per week while en- suring that each beneficiary can be intervened on only once. We end up conducting interventions on about 12000 beneficiaries which accounts for about 43% of the intervention arm.\n3.2.2 Eligibility for Interventions. Beneficiaries are eligible for in- terventions under the following conditions.\n(1) Active status - they are still enrolled in the program and receive automated voice messages.\n(2) They picked up at least 1 voice message in the last 4 weeks before the start of the intervention period for the respective cohort.\n(3) Repeat Intervention - beneficiaries have not received a pre- vious service call.\n3.2.3 Conducting Interventions. Each week, the DFL-RMAB [20] algorithm which is our AI algorithm of choice, determines the set of beneficiaries from the intervention arm who will receive an intervention. We collect all beneficiaries from the intervention arm that have received an intervention in some week in a list $I_D$. We also simulate the Al algorithm on the control arm to determine the set of beneficiaries that would have been selected for an intervention (assuming we conducted the same number of interventions as in the intervention arm). As in the intervention arm, beneficiaries in the control arm cannot be selected multiple times. We collect the beneficiaries from the control arm that have been selected by the algorithm in some week into a list $I_C$. We create an intervention list $I$ that combines $I_D$ and $I_C$. The idea is that we later compare the behavior of beneficiaries from $I_D$ and $I_C$, as we can think of beneficiaries from $I_C$ as the counterparts of those from $I_D$."}, {"title": "3.3 Health Survey", "content": "3.3.1 Conducting the Survey. The health survey is conducted on the beneficiaries from the intervention list $I$ between September and November in 2023. Since the program is oriented towards helping beneficiaries who lack resources the most, we perform the survey only on beneficiaries who are \u201chigh-risk\u201d. A beneficiary is considered \"high-risk\" if they satisfy at least one of the following three conditions: low level of formal education (less than grade 10), low-income family (less than 10000 INR per month) or they do not own the phone themselves. Around 50% beneficiaries enrolled in the program are high-risk.\nThe subset of the \"high-risk\" women who give birth between the intervention and survey call and have been in the program for at least 3 months are called by a health worker and asked to answer the questions from the survey. However, the survey calls are only picked up by a fraction of beneficiaries (3376 out of the 6448 called). This provides a challenge for the evaluation of our study, as we only have access to the \"outcome\" of a subset of beneficiaries, i.e., those who were willing to answer to the survey questions (in particular, this group of beneficiaries is not chosen uniformly at random). This makes it for instance necessary to re-balance the control and intervention group for the final comparison.\n3.3.2 Survey Questions. Each participant was asked 20 questions with the intent of measuring their engagement with the program and measuring knowledge in different areas such as health prac- tices. This evaluation is guided by the content of the automated voice messages and assesses how well the beneficiary received the messages. Concretely, the categories covered in the survey are: en- gagement with the program, knowledge, breastfeeding practices, communication, and health supplements. Appendix A contains a description of the questions in each category. The beneficiaries obtain a score for their answer in each question.\nOut of the 20 questions, 8 correspond to single choice (Yes/No) questions. The remaining 12 correspond to questions where scores are calculated based on multiple possible correct answers with some answers potentially contributing a higher score than others. Bene- ficiaries may choose one or more of these answers and their score is the total score for correctly identified answers in the question."}, {"title": "4 RESULTS AND ANALYSIS", "content": "4.1 Comparison Methodology\nWe compare the survey responses of beneficiaries in the interven- tion group and the control group. However, in doing so, we face multiple challenges. Firstly, not everyone from the list $I$ picks up and answers the survey call. Secondly, some beneficiaries from the intervention list $I_D$ of the intervention arm do not pick up the intervention call. To ensure that the beneficiaries from the interven- tion and control arms that we compare to each other are similar in distribution, we use a matching method to pair beneficiaries from different arms.\n4.1.1 Beneficiary Matching. We perform a one-to-one pairing of beneficiaries that answered the survey in the two arms by using matching methods with feature variables [14, 15]. The features used for this matching are the average listenership over the last 6 weeks from the intended date of intervention, gestational age, and the number of children they conceived previously. Let $X_i$ be the feature vector of the $i^{th}$ beneficiary and $X$ the feature matrix consisting of stacked vectors of all beneficiaries who responded to the survey. We define a closeness metric [17] between the feature vectors of two beneficiaries as their Mahalonobis distance\n$D_{ij} = (X_i - X_j)'\\Sigma^{-1}(X_i - X_j)$,\n(1)\nwhere $\\Sigma$ is the variance matrix of $X$ in the pooled dataset.\nFinally, for each beneficiary in the intervention group who re- sponded to the survey and picked up the intervention, we greedily pick the closest beneficiary from the control group who responded to the survey using the above-mentioned distance metric. Once a control beneficiary is matched, we no longer consider it for further matching. Finally, we keep the beneficiaries that are part of one pair to obtain two sets of beneficiaries $S_D \\subseteq I_D$ and $S_C \\subseteq I_C$ for the intervention and control arms, respectively.\n4.1.2 Improving Beneficiaries. As we will discuss in Section 4.2.2, establishing effect sizes in the full cohort of beneficiaries is chal- lenging, among others, because some beneficiaries do not show an improvement in listenership after the intervention call. Recalling that previous work [6] has identified that listening to automated calls leads to better health outcomes, we expect that beneficiaries who experienced the greatest listenership boost through the in- tervention will show the most significant improvements in survey results.\nTo this end, we formulate a method for identifying which benefi- ciaries have gained the most from the intervention. We define two listenership values for each beneficiary in the intervention list $I$, the average listenership over the past 6 weeks before the scheduled intervention date, called the pre-listenership and the average listen- ership over the next 12 weeks after the scheduled intervention date, called the post-listenership. The scheduled intervention date refers to the week in which the beneficiary from $I$ has been selected by the algorithm independent of whether they are in the treatment or control arm (and thereby independent of whether they actually received the intervention). Taking 12 weeks of post-listenership (instead of 6 weeks) allows us to measure the long-term gains from the interventions.\nWe calculate the quartile each beneficiary belongs to when compared with the other beneficiaries in $I$ for the pre- and post- listenership. For beneficiary $i$, we denote these values as $q_{1i}$ and $q_{2i}$, respectively. We say that a beneficiary gains in quartiles if $q_{2i} > q_{1i}$. In our analysis, we focus on the beneficiaries that gain in listenership, i.e., for which $q_{2i} > q_{1i}$. Specifically, in our analysis, we restrict our attention to the beneficiaries from the set $S_d$ with $q_{2i} > q_{1i}$ (called $R_d$) and the beneficiaries from $S_c$ matched to these beneficiaries (called $R_c$). Our final comparison involves 218 pairs (i.e., 436 beneficiaries in total).\nOur method of selecting beneficiaries who have gained in quar- tiles is naturally only an approximation for the beneficiaries who benefit the most from an intervention. Note that a larger subsec- tion of beneficiaries who received an intervention gain quartiles (218) compared to those who lose quartiles (119). This implies that interventions oftentimes lead to a gain in quartiles."}, {"title": "4.2 Analysis", "content": "4.2.1 Establishing Improved Listenership. We observe a consistent gain in listenership among beneficiaries who received interventions. In particular, following the work of Boehmer et al. [4], we compare (a) the beneficiaries from the intervention group that have been selected for an intervention (i.e., they are on $I_D$) and that responded to the survey call to (b) the beneficiaries from the control group that would have been selected for an intervention (i.e., they are on $I_C$) and responded to the survey call. We compare these groups using the subgroup estimator, see the work of Boehmer et al. [4] for details. We find that an intervention increases beneficiaries listenership compared to the control group by, on average, 7.43 seconds per call over the 12 automated voice calls following the intervention, which leads to a summed additional listenership of 89.16 seconds over the next 12 calls. The 95% confidence interval for this value is [45.516, 132.936] and the hypothesis that interventions have a positive non-zero effect on listenership can be accepted with a p-value of 6.4656 \u00d7 10\u22125. This allows us to conclude a statistically significant positive effect of interventions on beneficiary's listener- ship. Notably, the positive effect of interventions has already been established in previous studies [4, 10, 18]; however, our identified effect sizes and achieved confidence level are advantageous.\n4.2.2 Establishing Behavioral and Health Benefits. We now ana- lyze the beneficiary's performance in the survey. We will start by focusing on the beneficiaries on which interventions have been effective. Figures 2 and 3 show the results on single choice and multiple choice questions, respectively. We plot the average score obtained on each question for the groups $R_d$ and $R_c$, i.e., beneficia- ries from the treatment group whose listenership improved and who answered the survey and their counterparts. The empirical standard error is included as error bars in the plot to signify the possible error in the comparison. Note that independent of the group membership beneficiaries achieve generally higher scores on the single choice questions (arguably, because it is easier to respond to Yes/No questions during a survey).\nThe average score of the intervened beneficiaries from $R_d$ is higher than the average score of the control beneficiaries from $R_c$ on all but one question. For two questions, the difference between average scores is also statistically significant, i.e., single choice question 5 and multiple choice question 3. For the latter question (\"How does the baby respond when you speak or talk?\"), we can establish a difference with a p-value of 0.0021, showing an improved awareness of the baby's behavior. A very high variance in answers is observed in some questions such as Question 6 and 7 from the multiple choice section, which ask about when a child should be first fed milk and how many times a day they should be fed. This trend could be due to either a lack of information among some mothers, or due to the noise in the way these questions were answered during the survey.\nFigure 4 plots the averaged summed scores on different groups of questions. In particular, the leftmost bars, depict the summed av- erage score over all questions. We find that the difference between the two arms is larger than one standard error here and that we can establish a difference between the two arms with a p-value of 0.13. While this is not enough to claim statistical significance, it is a strong hint towards the positive impact of the AI-scheduled interventions and motivates us to conduct a larger follow-up trial. The questions comprising each category are provided in the appen- dix (Appendix A). We notice that the engagement category provides"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "In conclusion, our study has shown that AI-scheduled interven- tions lead to significantly higher listenership in automated voice messages. Moreover, we provide some first evidence that these interventions also lead to improved behavioral and health out- comes as measured by our survey. Together with the NGO, we aim to redesign the study with a more focused cohort and updated survey questions to better understand and correlate the impact of intervention-induced improved listenership on behavioral out- comes."}, {"title": "A.1 Questions and their answers", "content": "Single Choice Questions and expected answers:\n\u2022 0. Did you know your weight at the time of delivery : Yes\n\u2022 1. Did you know at the time of delivery what was the baby's weight? : Yes\n\u2022 2. During the checkup, did you ask whether the baby is developing normally? : Yes\n\u2022 3. Do you breastfeed your baby? : Yes\n\u2022 4. Do you get calls informing about Mmitra? : Yes\n\u2022 5. Have you regularly listened to the calls received by mMi- tra?: Yes\n\u2022 6. Did you ever discuss with your husband/family the infor- mation you heard/told in the call? : Yes\n\u2022 7. Do you never answer or disconnect your mMitra calls when you are around other beneficiaries? : No\nMultiple Choice Questions and expected answers with scores:\n\u2022 0. What did you discuss with family? : ('weight', 0.33), ('cough', 0.33), ('Other Response', 0.33), ('baby is fussy', 0.33), ('constant watering', 0.33), ('breastfeeding problems', 0.33)\n\u2022 1. If a weak baby is born prematurely, how to take care of it after bringing it home? : ('entire black part of the breast', 0.33), ('pillow under the baby', 1), ('one breast to the other', 1), ('I hold the baby\u015b head', 0.33)\n\u2022 2. How does the baby respond when you smile or talk? : ('Smiling back', 0.33), ('Watches us', 0.33), ('Shouts or speaks back', 0.34)\n\u2022 3. Why do you want to listen to the call? : ('regarding diet', 0.2), ('changes are happening', 0.2), ('answers to some ques- tions', 0.2), ('information I dont get from doctors', 0.2), ('doing the right thing', 0.2)"}, {"title": "A.2 Grouping of Questions", "content": "The following is the grouping mechanism followed (SC and MC refer to single correct and multi correct respectively):\n\u2022 Engagement: SC - [4, 5, 6, 7], MC -[3, 4, 11],\n\u2022 Knowledge: SC - [0, 1],\n\u2022 Breastfeeding Practices: SC - [3], MC - [5, 6, 7],\n\u2022 Communication: SC - [2], MC - [0],\n\u2022 Supplements: MC - [7, 8]"}]}