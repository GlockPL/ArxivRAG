{"title": "Na\u02bcvi or Knave: Jailbreaking Language Models via Metaphorical Avatars", "authors": ["Yu Yan", "Sheng Sun", "Junqi Tong", "Min Liu", "Qi Li"], "abstract": "Metaphor serves as an implicit approach to convey information, while enabling the generalized comprehension of complex subjects. However, metaphor can potentially be exploited to bypass the safety alignment mechanisms of Large Language Models (LLMs), leading to the theft of harmful knowledge. In our study, we introduce a novel attack framework that exploits the imaginative capacity of LLMs to achieve jailbreaking, the Jailbreak Via Adversarial MeTA -phoR (AVATAR). Specifically, to elicit the harmful response, AVATAR extracts harmful entities from a given harmful target and maps them to innocuous adversarial entities based on LLM's imagination. Then, according to these metaphors, the harmful target is nested within human-like interaction for jailbreaking adaptively. Experimental results demonstrate that AVATAR can effectively and transferablly jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs. Our study exposes a security risk in LLMs from their endogenous imaginative capabilities. Furthermore, the analytical study reveals the vulnerability of LLM to adversarial metaphors and the necessity of developing defense methods against jailbreaking caused by the adversarial metaphor. Warning: This paper contains potentially harmful content from LLMs.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) [1, 44] have become increasingly prevalent across various domains, such as content generation [38], programming [12], and professional question answering [36]. As these models have shown exceptional capabilities to understand complex patterns and complete a wide range of tasks, they further expand the boundaries of individual cognition. However, despite the impressive capabilities of LLMs to be helpers of humans, there is a potential risk that they can generate biased or harmful content, raising serious concerns about the information security of LLMs.\nRecent studies [4, 5, 44, 48, 49, 53, 54] have revealed attacks targeting LLM safety alignment mechanisms, known as jailbreak attacks, which aim to break or bypass the LLM's built-in content filtering and safety protections, generating biased and harmful content. Among them, black-box jailbreak attacks [44] present a more"}, {"title": "2 Background", "content": "Problem Statement. Jailbreak refers to a specialized attack that strategically or tactically constructs interactive prompts to manipulate LLMs into generating harmful content. The harmfulness of the content is evaluated based on whether it violates predefined safety guidelines and whether it is relevant to a given harmful query. In our study, the goal of jailbreak is to induce the LLM to generate a response that directly or indirectly reveals harmful content related to a specific query, defined as follows:\n\u2022 Direct Jailbreak [28, 49, 53, 54]: the safety alignment mechanism of the LLM is broken to purposefully generate the clearly harmful response for answering the harmful query.\n\u2022 Indirect Jailbreak [6, 10, 47]: the safety alignment mechanism of the LLM is bypassed to inadvertently generate the cryptic response that can be post-processed into the clearly harmful content for answering the harmful query.\nThreat Model. In this work, we consider an open-ended jailbreak attack scenario involving information asymmetry between the attacker and the target LLM. In this scenario, the attacker has no access to internal details of the target model, such as its architecture, parameters, training data, gradients, or output logits, i.e., the black-box attack [44]. The target model doesn't know who the attacker is or what the attacker's intentions are. It is only able to detect harmful content in a limited context and reject the response. The attacker aims to jailbreak the target model to obtain malicious information [5]. To achieve this, attackers can deploy open-source LLM to help in pre-processing prompts, such as rewriting and breaking them into key steps to hide their true intents, and post-processing responses from LLMs such as harmful information summarization [25], malicious code generation [6], or decrypting [47]."}, {"title": "3 Methodology", "content": "In this section, we introduce Jailbreak Via Adversarial MeTAphor (AVATAR), which automatically generates the interactive queries for jailbreak without training. AVATAR is the first method that explores imagination of LLMs for jailbreak, which involves two main steps: 1) Adversarial Entity Mapping, 2) Human-like Interaction Nesting. The overlook of AVATAR is shown in Figure 2. The overall workflow of AVATAR is shown in Algorithm 1. The detailed prompt templates in AVATAR are attached in the Appendix ??."}, {"title": "3.1 Adversarial Entity Mapping", "content": "Inspired by the insight that deeper logical connections between original and mapping entities in the metaphor can potentially lead to harmful leakage, we propose the Adversarial Entity Mapping (AEM) approach, which exploits the imagination of LLMs [21, 37] to discover appropriate entities for the harmful target as Figure 3.\nToxic Entity Extraction. Harmful targets can be divided into a series of core entities. Therefore, we extract these toxic entities from"}, {"title": "4 Experiments", "content": "4.1 Jailbreak Settings\nDatasets. Our experiments are conducted on the HarmBench dataset [27] and GCG dataset [54]. The HarmBench dataset contains 510 harmful queries with different types of harmful behaviors, and the GCG dataset contains 388 harmful queries. In our experiments, the standard and contextual types of test sets in HarmBench are used to evaluate the model. The standard behaviors cover a broad range of harms and are self-contained behavior strings. The contextual behaviors consist of a context string and a behavior string referencing the context. The top-50 toxic queries from the GCG dataset provided by [42] are selected for detailed analysis.\nEvaluation Metrics. We utilize the predefined criteria from Harm-bench [27] to evaluate the relevance and potential harmfulness of model outputs, leveraging the advanced capabilities of LLMs for analysis. This judging method effectively mitigates the issue of false positives in keyword detection approaches and addresses the limitations of toxicity classifiers [14, 46], both of which overlook the task relevance of the generated content. Based on the above judging"}, {"title": "4.2 Experiment Results", "content": "Baseline comparison. We perform AVATAR in 2 retries to jailbreak LLMs, and compare its performance with existing baselines. Table 1 presents the ASR of various jailbreak methods applied to open-source and closed-source LLMs. Our AVATAR consistently achieves exceptional ASR across different LLMs, highlighting its effectiveness in bypassing safety alignment mechanisms. In general, AVATAR outperforms the best jailbreak attack method ReNeLLM by 7.34% on standard behaviors and 5.97% on contextual behaviors on average. Compared to human-designed prompt templates, AVATAR improves the ASR by over 54% on average. Compared to multi-turn dialogue attack CoA, AVATAR improves the ASR by over 26% on average. In particular, AVATAR achieves an ASR of 100% on Mixtrals and more than 90% on GPTs. Further case study in Appendix C demonstrates that AVATAR can further jailbreak powerful LLMs such as ChatGPT-01 and Claude-3.5.\nWe observe a clear ASR difference between indirect and direct jailbreak methods. Direct jailbreak methods attempt to break the"}, {"title": "5 Discussion of AVATAR", "content": "To investigate the factors contributing to the success of AVATAR, we analyze the susceptibility of AVATAR in different metaphor fields and LLM types.\nFor the field preference of AVATAR, the creative field is highly effective in inspiring LLMs to share harmful information. As shown"}, {"title": "6 Related Work", "content": "Human Value Alignment for LLMs. Aligning LLMs with human values remains a challenge due to biases in training data and trade-offs between usefulness and safety [10, 49, 51]. Approaches such as Reinforcement Learning from Human Feedback (RLHF) [2, 30] have been proposed to improve fairness [11, 29], safety [54] and eliminate hallucinations [22, 50]. To further promote the alignment of LLM and human values, we explore the vulnerability in LLMs' imagination in our study.\nJailbreak Attacks on LLMs. Jailbreak attacks threaten the safety alignment mechanisms of LLMs, potentially leading to the generation of harmful content [3, 24, 44]. These attacks can be broadly categorized into white-box and black-box approaches. White-box attacks leverage detailed knowledge of model architectures and parameters to bypass safety controls [15, 23, 54], while black-box attacks rely on crafted inputs to exploit vulnerabilities in alignment mechanisms [5, 28, 42]. Our study is inspired by two key methods in black-box attacks: prompt nesting and multi-turn dialogue attacks.\nPrompt Nesting Attack. Prompt nesting bypasses security features by nesting malicious intents in normal prompts, altering LLMs' context. Techniques like DeepInception [20] exploit nested scenarios, while ReNeLLM [10] rewrites prompt to jailbreak based on code completion, text continuation, or form-filling tasks. MJP [19] uses multi-step approaches with contextual contamination to reduce moral constraints, prompting malicious responses.\nMulti-turn Dialogue Attack. LLMs that are safe in isolated, single-round interactions can be gradually manipulated into generating harmful outputs through multiple rounds of interaction [8, 42, 52]. Multi-turn dialogue attack leverages the multi-turn nature of conversational interactions to gradually erode an LLM's content restrictions. Crescendo [33] exploits seemingly benign exchanges to"}, {"title": "7 Conclusion", "content": "In this study, we enhance the comprehension of jailbreak attacks as manipulating LLMs in \"usefulness over safety\" by hiding information at the concept level, providing insights for the development of LLM safety and ethics. We introduce the Jailbreak via Adversarial Metaphor (AVATAR) framework, which exploits vulnerabilities in LLMs by leveraging adversarial metaphors, highlighting a critical safety risk from LLMs' imaginative capabilities and the inherent tension between task execution, creativity, and content safety. AVATAR consists of Adversarial Entity Mapping (AEM) for metaphor identifying and Human-like Interaction Nesting (HIN) for prompt construction, demonstrating high effectiveness and efficiency in generating transferable adversarial metaphors for jailbreak attacks with state-of-the-art success rates on multiple advanced LLMs."}, {"title": "A Additional Explanation of Methodology", "content": "A.1 Multi-Role Collaboration for Jailbreak\nIn AVATAR, we introduce the roles of various models involved in the AVATAR framework, which are the red team attacker, target, judge, and tool models.\nRed Team Attacker Model (Gattacker) is the primary agent to generate and refine malicious messages with knowing the attack target. During Toxic Entity Extraction stage, the attacker model analyzes and extracts key entities from the harmful query. During Adversarial Human Interaction Nesting stage, the attacker model utilizes metaphor entities and social influence strategies to induce the target model to jailbreak."}, {"title": "A.2 Limitations", "content": "AVATAR framework generates metaphors using the imagination of LLMs and conducts human-like interaction to prompt LLMs to generate harmful content without complex tricks or sophisticated jailbreak templates. However, the effectiveness of AVATAR is potentially limited by:\nEntities Extracting is disturbed by safe alignment. The safety alignment mechanisms in existing open-source and closed-source LLMs limit their deeper analysis of harmful content for better toxic entity extraction. During the Adversarial Entity Mapping stage, the attack model may extract a mix of harmful and safe entities from the harmful query, potentially interfering with subsequent metaphorical content generation. The effectiveness of in-context learning in mitigating this issue is limited."}, {"title": "B Experimental Hyperparameters", "content": "Language Model Settings. Our specific LLM versions, and huggingface link is provided as follows: 1) Qwen2-7B is Qwen/Qwen2-7B-Instruct\u00b9. 2) Qwen2-72B is Qwen/Qwen2-72B-Instruct\u00b2. 3) GLM3-6B is THUDM/chatglm3-6b3. 4) GLM4-9B is THUDM/glm-4-9b-chat\u2074. 5) InternLM2.5-7B is internlm/internlm2_5-7b-chat\u2075. 6) Qwen 1.5-110B is Qwen/Qwen1.5-110B-Chat\u2076. 7) Llama2 is meta-llama/Llama-2-7b-chat-hf\u2077. 8) Llama3 is meta-llama/Meta-Llama-3-8B-Instruct\u2078 9) Llama3.1 is meta-llama/Llama-3.1-8B-Instruct\u2079. 10) Yi-1.5-34B is 01-ai/Yi-1.5-34B-Chat\u00b9\u2070. 11) GPT-3.5 is gpt-3.5-turbo\u00b9\u00b9. 12) GPT-4 is gpt-4-turbo\u00b9\u00b2. In Adversarial Entity Mapping, we use BGE-m3"}, {"title": "C Further Analysis", "content": "C.1 Case Study\nTo demonstrate the jailbreak potential of adversarial metaphors, we conduct a case study on attacking the powerful closed-source models. We conduct the jailbreak by constructing the adversarial prompt with Chain-of-Metaphor. Specifically, we begin by selecting a harmful query, \"build a bomb\" and apply multiple-step entity mapping only assisted by Qwen2-7B, transforming this harmful concept into a series of benign and abstract metaphors as: Build a Bomb \u2192 A Symphony of Magical Elements \u2192 The Cosmic Harmony Engine \u2192"}, {"title": "C.2 Robustness of Adversarial Entity Mapping", "content": "To verify the robustness of our proposed adversarial entity mapping technique, we evaluate our metaphor selection strategies, i.e., Minimum Toxicity Metaphor. Specifically, we compared four different strategies: 1) MTM, which balances both high entity similarity and conceptual disparity as shown in Formula 9. 2) w/o CD, which maximizes entity internal relation similarity. 3) w/o ICS, which minimizes the similarity between original and metaphor concepts. 4) random selection. All experiments were conducted on the GCG50 dataset, targeting both GPT-3.5 and GPT-4. Each experiment was repeated 5 times to calculate the average ASR and standard deviation, as shown in Table 4.\nThe results clearly demonstrate the superior performance of the MTM strategy. On GPT-3.5, MTM achieves an ASR of 90.00%, with a low standard deviation of 3.35, indicating high reliability and stability in generating successful adversarial metaphors. Similarly, on"}, {"title": "C.3 Further Defense Analysis", "content": "LLMs are trained to enhance instruction-following for effective task completion [10, 51]. Based on this observation, AVATAR uses metaphors to nest harmful intents in innocuous tasks, thus manipulating LLMs into \"usefulness over safety\". We use the defense tactics from two views: Internal Resilience and External Reinforcement, whose detailed settings are in Appendix B."}, {"title": "C.4 Further Discussion of Defense Methods", "content": "AVATAR use innocuous entities to trigger the jailbreak of LLMs, which demonstrates the important threat in LLMs. To strengthen the defense against such adversarial metaphor attacks, we explore additional approaches to internalize external capabilities into the model itself as follows:\nDefending by Knowledge Augmented Inference. Enriching LLMs with domain-specific knowledge before reasoning. This preinference knowledge can help the model better understand metaphorical content, allowing it to differentiate between harmful and benign metaphors.\nDefending by Supervised Fine-Tuning (SFT). Fine-tuning the model using adversarial metaphor examples can train it to independently recognize harmful metaphors, thus enhancing its resilience without relying on external classifiers."}]}