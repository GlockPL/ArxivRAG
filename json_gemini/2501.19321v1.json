{"title": "LANGUAGE BIAS IN SELF-SUPERVISED LEARNING FOR AUTOMATIC\nSPEECH RECOGNITION", "authors": ["Edward Storey", "Naomi Harte", "Peter Bell"], "abstract": "Self-supervised learning (SSL) is used in deep learning to train\non large datasets without the need for expensive labelling of the\ndata. Recently, large Automatic Speech Recognition (ASR) mod-\nels such as XLS-R have utilised SSL to train on over one hundred\ndifferent languages simultaneously. However, deeper investiga-\ntion shows that the bulk of the training data for XLS-R comes\nfrom a small number of languages. Biases learned through SSL\nhave been shown to exist in multiple domains, but language bias in\nmultilingual SSL ASR has not been thoroughly examined. In this\npaper, we utilise the Lottery Ticket Hypothesis (LTH) to identify\nlanguage-specific subnetworks within XLS-R and test the perfor-\nmance of these subnetworks on a variety of different languages.\nWe are able to show that when fine-tuning, XLS-R bypasses tra-\nditional linguistic knowledge and builds only on weights learned\nfrom the languages with the largest data contribution to the pre-\ntraining data.", "sections": [{"title": "1. INTRODUCTION", "content": "Pretrained Automatic Speech Recognition (ASR) models are now\nable to process numerous languages within a single model with\nlow error rates even on low-data languages [1, 2]. It has been con-\nsistently shown that fine-tuning large pretrained models provides\nthe best accuracy on a single language task, when compared to\nsmaller models [3, 4]. To achieve this accuracy, multilingual ASR\nmodels contain hundreds of millions of parameters and must be\npretrained on hundreds of thousands of hours of speech data. Self-\nsupervised learning (SSL) is increasingly utilised to process such\nlarge amounts of data without the need for expensive labelling and\nhas become common in open-source ASR [5, 6, 7]. However, pre-\nvious commercial and open-sourced supervised learning models\nhave been shown to produce higher errors when presented with\nspeech data outside the domain of their training data [8, 9]. It is\ntherefore imperative that the data and training strategies employed\nin SSL ASR are diligently explored.\nRecent studies have shown that the data that SSL ASR models\nare trained on can impact downstream training. Boito et al. [10]\nhave experimented with wav2vec 2.0 by pretraining on data that\nwas not balanced equally in gender and then fine-tuning to gender-\nbalanced data. They found that if pretraining data was not gender-\nbalanced, the error would increase when fine-tuning to gender-\nbalanced speech. Meng et al. [11] showed that biasing pretraining\ndata towards slow speech rates for SSL transformer-based mod-\nels improves downstream accuracy, whereas fast speech in pre-\ntraining has a performance drop. To show the impact of this pre-\ntraining Fuckner et al. [12] investigate wav2vec 2.0 performance\nwhen fine-tuned to Dutch speech data. They find that Whisper\n[2], a supervised learning model trained on multilingual data, out-\nperforms wav2vec 2.0 [5] which is pretrained only using English\ndata. Zhang et al. [13] pretrain the wav2vec 2.0 architecture on\na single language. They analysed data from multiple languages\nand selectively included the utterances that contained the most\nlanguage similarity with the target language into the pretraining\ndata. With this approach, they could attain baseline results with\nsignificantly reduced data and training time. These studies show\nthat biasing the pretraining data towards certain data domains can\naffect the downstream performance of SSL ASR models.\nWith this in mind it is vital to analyse the data commercial\nopen-source SSL ASR models are trained on. SSL excels when\ntrained on hundreds of thousands of hours of data and the most\nabundant source of speech data is English language data. Sev-\neral models [5, 6] used for multilingual ASR are trained solely on\nEnglish data such as Librispeech [14] or Libri-Light [15]. Other\nSSL models are pretrained on multilingual data [1, 16], but they\nwill often contain more English data than other languages. Multi-\nlingual Librispeech (MLS) [17] is commonly used as a source of\nmultilingual data. However, out of the 50k hours of data in MLS,\n44k are English speech data.\nThis study aims to identify the impact that the imbalance of\nEnglish data in SSL pretraining has on open-source commercial\nASR as, to our knowledge, this has not been deeply explored be-\nfore. In order to achieve this, we require techniques that iden-\ntify low-level behaviours of large deep learning models. Model\ncompression techniques such as model distillation [18], low-rank\nadaptation [19] and model pruning [20] all look to exploit specific\nbehaviours in large pretrained deep learning models in order to re-\nduce the size of a model without affecting performance. They can\ntherefore be useful tools to analyse low-level behaviours of our\nlarge SSL ASR models.\nThrough the use of the Lottery Ticket Hypothesis (LTH) [21]\nto prune deep learning models, several studies have shown the ex-\nistence of language-specific weight groupings or \u201csubnetworks\"\nwithin large ASR models [22]. These subnetworks of weights\ncontribute more to learning on a downstream language than other\ngroups. These language-specific subnetworks have been used to\nimprove accuracy in sparsely pruned networks for data of the same\nor related languages [23, 24]. Each subnetwork's performance on\nunrelated languages has not been extensively studied prior to\nthe work presented here. For this study we chose to evaluate XLS-R\n[1], XLS-R is an open-source SSL ASR model based on the same\narchitecture as wav2vec 2.0 [5]. We evaluate the 300 million pa-\nrameter version that was pretrained through SSL on multilingual\ndata from 128 different languages. This paper will utilise LTH\nin order to identify language-specific weights and subnetworks\nwithin XLS-R and evaluate the extent to which language balance\nin the pretraining data affects bias towards or against performance\non various languages in downstream fine-tuning tasks."}, {"title": "2. BACKGROUND", "content": "Among highly cited open source SSL ASR models, wav2vec 2.0\nand HuBERT were trained on Librispeech [14] and Libri-Light\n[15], which only include English language data [5, 6]. XLS-R [1],\nhowever, is a large SSL ASR model pretrained on 128 different\nlanguages. It is built on the same architecture as wav2vec 2.0 but\nexpanded to 24 transformer layers in the encoder.\nLike other multilingual SSL ASR models, such as its prede-\ncessor XLSR-53 [16], the pretraining data contains more English\ndata than any other language, due to the largest datasets included\nin the XLS-R pretraining data [17, 25]. English language speech\ndata accounts for approximately 15.9% of the total training data.\nAdditionally, the first 24 languages account for 98% of the total\ndata XLS-R was trained on [1]. We chose XLS-R for this study\ndue to its variety of languages and also the imbalance in the num-\nber of hours for each language in the pretraining data. The number\nof hours and percentage total data for each language included in\nthis study are outlined in Table 1, a breakdown of the number of\nhours per language can be found in the original XLS-R paper [1]."}, {"title": "2.2. Model Compression and Network Pruning", "content": "Model compression is an area of study that aims to reduce the size\nof large pretrained models while preserving their accuracy. All of\nthese techniques take advantage of low-level changes in weights\nover the course of training. So, for our purposes, we can adapt\nthem as tools to monitor how weights behave across the model\nwhen subjected to a variety of data. There are multiple active ar-\neas of research when it comes to compressing large deep-learning\nmodels such as model distillation [18], low-rank adaptation [19]\nand model pruning [21].\nModel pruning allows for the removal or zeroing of certain\nweights across the model if they are not deemed necessary for\ndownstream tasks [20]. The Lottery Ticket Hypothesis (LTH) [21]\nstates that unstructured pruning of weights within a pretrained net-\nwork can uncover \"winning tickets\". These are subnetworks of\nweights that contribute more to learning downstream tasks than\nother groups. For our purposes, analysing these subnetworks can\ngive insight into which weights are most active when different lan-\nguage data stimuli are introduced to a model."}, {"title": "2.3. Language-Specific Subnetworks", "content": "Several studies have shown that language-specific subnetworks\ncan be obtained when pruning large multilingual ASR models. Lu\net al. [23] test joint training with multiple language-specific sub-\nnetworks. They find this approach has less degradation on the ini-\ntial high-resource language performance than when pruning with\na language-agnostic approach. Similarly, in 2023, Yang et al. [24]\nshow that language-specific subnetworks have better performance\nthan language-agnostic approaches.\nIn 2021, Lai et al. [22] studied language-specific networks\nin wav2vec 2.0 [5] and XLSR-53 [16]. They use the Intersec-\ntion Over Union (IOU) to show the overlap of weights in different\nlanguage-specific subnetworks, finding high overlap between dif-\nferent subnetworks, other than those that are randomly generated.\nThey also test the performance of language-specific subnetworks\non various different languages. They find that there is a large vari-\nability in performance depending on which language-specific sub-\nnetwork is used for which downstream language. However, this\nstudy does not explore an English subnetwork and concentrates\non the language-agnostic benefits of their own pruning algorithm.\nIn this paper, we explore how pretraining data affects language-\nspecific subnetworks and then assess their performance on a wide\narray of languages. Approaching language-specific subnetworks\nwith this method will give us insight into how different pretrain-\ning languages impact multilingual SSL ASR performance."}, {"title": "3. METHOD", "content": "XLS-R [1] is an ASR model pretrained through self-supervised\nlearning on 128 separate languages and has high performance on\nmany languages when fine-tuned. The smallest iteration of XLS-R\nwith 300M parameters was selected for this paper."}, {"title": "3.2. Data", "content": "The FLEURS: FEW-Shot Learning Evaluation of Universal Rep-\nresentations of Speech dataset [26] contains data from 101 lan-\nguages. FLEURS contains approximately 12 hours of data per\nlanguage. For our upstream languages, we selected English, Ger-\nman, French, Spanish and Polish. These languages are all Indo-\nEuropean languages from three sub-groups: Germanic, Latin and\nSlavic [27]. They are all high-data languages within the XLS-R\npretraining data, as seen in Table 1. All of these languages were\ntested downstream alongside Asturian and Xhosa. Asturian and\nXhosa are languages unseen by XLS-R in its pretraining [1]. As-\nturian is a language spoken in Northern Spain and like Spanish is\na Latin language [28]. Xhosa is a language spoken in South Africa\nwith no linguistic relationship to the other languages in these ex-\nperiments [27]. Finally, Catalan is a low-data language in XLS-R\npretraining data and is a Latin language from Spain."}, {"title": "3.3. Pruning", "content": "In all of these experiments, we apply L1-norm unstructured one-\nshot global weight pruning to the encoder of XLS-R. Global prun-\ning was used across the transformer-based encoder to target more\nvaluable connections that may exist across layers and not force the\npruning of a single layer more than is necessary [21]."}, {"title": "3.4. Training Strategies", "content": "To obtain subnetworks for each language tested, we must first train\nXLS-R on our upstream language. We use the same hyperparame-\nters for XLS-R as Rouditchenko et al. [29] but we differ by train-\ning on 8 2080ti GPUs. First, we train XLS-R for 100 epochs and\nselect the point in training with the lowest CTC loss on the vali-\ndation set for the final upstream model. Then to create our down-\nstream model we prune the upstream model to the target sparsity\nand train it on either the same or a new language.\nWhen the downstream language is the same as the upstream\nlanguage (i.e. English further trained on English), we train the\ndownstream model for 10 epochs at sparsities increasing in steps\nof 10%. For downstream languages that do not match the up-\nstream language (i.e. an English model trained on downstream\nSpanish), we freeze the encoder for one epoch of training before\nunfreezing and continuing to train for the 10 epochs. We evalu-\nate the final models' performance based on Character Error Rate\n(CER) as in [29]. All training pathways can be seen in Figure 1."}, {"title": "4. RESULTS", "content": "Figure 2 shows XLS-R trained on English and pruned to sparsities\nstarting at 0% up to 90% sparsity in increments of 10%. Figure 2\ncorroborates findings in previous papers [30, 31, 32] that show that\nsparse networks can achieve minimal degradation to the baseline\nunpruned accuracy of a model up to 50% or 60%. The impact of\nlanguage-specific subnetworks is more pronounced in sparsities\nat or above 70%. As such our findings throughout the rest of this\npaper will concentrate on 70%, 80% and 90% sparsities."}, {"title": "4.1. Language Specific Subnetworks", "content": "Figure 2, Figure 3 and Figure 4 show the same experiments\nperformed with different upstream languages. Figure 2 shows up-\nstream English as a base for the language-specific subnetworks,\nFigure 3 shows upstream Spanish and Figure 4 shows upstream\nPolish, three languages from separate Indo-European language\nfamilies [27] and all high-data within the XLS-R pretraining data.\nAcross the three figures, we can observe a clear trend of English\nas an outlier to the other languages-specific subnetworks. Figure\n2 has the lowest CER across all languages, even when the down-\nstream language matches the upstream language as with Spanish\nin Figure 3 and Polish in Figure 4. Figure 3 and Figure 4 both\nshow substantial increases in CER on English at 80% and 90%\nsparsity when compared to the other language-specific subnet-\nworks tested. This may suggest that the English language sub-\nnetwork is more effective for downstream training. Adverse to the\nfindings in [22] Figure 3 and Figure 4 suggest that not training on\nan English subnetwork is in fact detrimental to downstream fine-\ntuning, even on languages unrelated to English.\nWe further corroborate these findings in Figure 5 which shows\nthe average CER for each upstream model. To gauge how well\nthese models generalise, we exclude the language the model was\ntrained on for the results in Figure 5, i.e. the average perfor-\nmance for the English language-specific subnetworks fine-tuned\nto French, German, Polish and Spanish at each sparsity. We see\nin Figure 5 that the CER at 80% and 90% sparsity is the low-\nest among all subnetworks with an absolute difference of 26.92%\nCER at 90% sparsity between the highest and lowest error and\n21.46% CER at 80%. While the English CER at 70% is not the\nlowest among subnetworks, it is within 1% error from the low-\nest of the French subnetwork and 15.09% less than the highest\nCER at 70% sparsity for the Spanish subnetwork. This shows that\nacross all other languages, English language-specific subnetworks\nperform as well or better than the other subnetworks."}, {"title": "4.2. Evaluation of the English Subnetwork", "content": "In Table 3 we compare the results of the English language-specific\nsubnetwork to the Spanish language-specific subnetwork by fine-\ntuning to Asturian and Xhosa as well as English and Spanish.\nWe compare the performance from the English subnetwork to the\nSpanish subnetwork at 70% and 90% sparsity. We also test how\nwell Spanish and English fine-tuned models perform when pruned\nwith subnetworks generated from each of the other languages. En\n/ Es is the English fine-tuned model pruned to the Spanish subnet-\nwork and Es / En is the Spanish fine-tuned model pruned to the\nEnglish subnetwork. In both cases, the English upstream model\npruned with the English subnetwork has the lowest average CER.\nFinally, in Table 2 we show that combining all surviving\nweights from both English and Spanish subnetworks, noted in Ta-\nble 2 as subnetwork En / Es, does not decrease CER more than\nthe English single-language subnetwork alone. While the error on\nEnglish is reduced on the Spanish upstream model, it is still 3%\nhigher in CER than the upstream English model with the English\nsubnetwork."}, {"title": "4.3. Intersection Over Union", "content": "The results in this section so far have shown that fine-tuning on the\nEnglish subnetwork achieves lower error at high sparsities than\nany other subnetwork. To explore further how this behaviour is\noccurring we can use the Intersection Over Union (IOU) equa-\ntion from [22]. IOU is a measure of how many weights overlap\nbetween two subnetworks. Figure 7 shows the overlap of saved\nweights between two subnetworks when a model is pruned to\n90%. We test IOUs between English, Polish and Spanish subnet-\nworks. Figure 7 shows the lowest IOU at 80.67% when comparing\nthe English and Spanish subnetworks. This corroborates the find-\nings in [22] that discovered high overall overlap between different\nlanguage-specific subnetworks. However, given our findings in\nSection 4 we can surmise that the remaining weights not overlap-\nping have significant influence over downstream training. We also\nsee in Figure 7 that the English language subnetwork has the most\nsaved weights in common with the base XLS-R model. Adversely\nthe Spanish subnetwork has the least in common with the base\nXLS-R weights. This may imply that fine-tuning XLS-R to En-\nglish requires the least change in value for weights from the base\nmodel, prior to fine-tuning, when compared to other languages."}, {"title": "5. DISCUSSION", "content": "Throughout Section 4 we see the English subnetworks in XLS-\nR producing lower Character Error Rates (CER) when fine-tuned\nacross all languages when compared to other language-specific\nsubnetworks. Previous studies of language-specific subnetworks\nmake the assumption that a subnetwork generated for a language\nwill be the most efficient to further train on data from the same\nlanguage or a linguistically related language [23, 24]. However,\nour experiments show that when pretraining data is imbalanced,\nthis is not the case. The results in this paper instead show that re-\ngardless of linguistic content lower error will always be achieved\nby training on weights generated for the language with the highest\ndata in pretraining, in this case English. The English dominance of\nXLS-R's performance is true for Germanic languages, such as En-\nglish and German, but also languages from other Indo-European\nlanguage families and one South African language.\nThrough the use of the Intersection Over Union (IOU) we next\nanalyse where overlap between the weights of different language-\nspecific subnetworks occurs. We find the minimum overlap to\nbe 78.75%, which broadly corroborates previous results showing\nhigh overall overlap [22]. However, given our previous findings,\nthe remaining weights that are specific to one language do appear\nto impact downstream accuracy significantly. We compare each\nsubnetwork with the base weights of XLS-R after pretraining but\nbefore fine-tuning. This shows us that the English subnetwork\nhas the least deviation from the base model. This implies that\nthe reliance on English has been taught to the model at the pre-\ntraining stage and not during downstream fine-tuning. Addition-\nally, we also measure the overlap between subnetworks generated\nfrom two languages from Spain, Catalan and Asturian, to the En-\nglish and Spanish subnetworks. We find that both Catalan and As-\nturian language-specific subnetworks have more overlap in saved\nweights with the English language-specific subnetwork. This rein-\nforces our findings that when fine-tuning, XLS-R weights learned\nin pretraining from English language data are more impactful to\ntraining. This is regardless of the linguistic relation to English the\ndownstream training data has.\nThis paper shows that Self-Supervised Learning (SSL) in Au-\ntomatic Speech Recognition (ASR) can bias fine-tuning tasks to-\nwards weights learned from the data domains most present in\nits pretraining data. Open-source SSL ASR pretraining data is\nhighly imbalanced towards a small number of languages. These\nmodels are over-reliant on the features learned from those lan-\nguages which leads them to ignore linguistic relationships when\nfine-tuning to unseen or low-data languages. Previous research\n[13] has shown that downstream tasks can benefit from linguisti-\ncally guided pretraining so fine-tuning on linguistically unrelated\nfeatures is both unintuitive and inefficient. Given this, we rec-\nommend balancing the pretraining data by language and linguistic\nrelationships. When deploying pretrained open-source models re-\nsearchers must carefully analyse the data the model has been pre-\ntrained with. Imbalanced pretraining data that is abundant in just\none or a few languages, but limited across many others, is not best\nsuited for efficient multilingual SSL ASR."}]}