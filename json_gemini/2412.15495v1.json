{"title": "TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use", "authors": ["Junjie Ye", "Yilong Wu", "Sixian Li", "Yuming Yang", "Tao Gui", "Qi Zhang", "Xuanjing Huang", "Peng Wang", "Zhongchao Shi", "Jianping Fan", "Zhengyin Du"], "abstract": "Large language models (LLMs) achieve remarkable advancements by leveraging tools to interact with external environments, a critical step toward generalized AI. However, the standard supervised fine-tuning (SFT) approach, which relies on large-scale datasets, often overlooks task-specific characteristics in tool use, leading to performance bottlenecks. To address this issue, we analyze three existing LLMs and uncover key insights: training data can inadvertently impede tool-use behavior, token importance is distributed unevenly, and errors in tool calls fall into a small set of distinct categories. Building on these findings, we propose TL-Training, a task-feature-based framework that mitigates the effects of suboptimal training data, dynamically adjusts token weights to prioritize key tokens during SFT, and incorporates a robust reward mechanism tailored to error categories, optimized through proximal policy optimization. We validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four diverse open-source test sets. Our results demonstrate that the LLM trained by our method matches or surpasses both open- and closed-source LLMs in tool-use performance using only 1,217 training data points. Additionally, our method enhances robustness in noisy environments and improves general task performance, offering a scalable and efficient paradigm for tool-use training in LLMs. The code and data are available at https://github.com/Junjie-Ye/TL-Training.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) (OpenAI, 2023; Touvron et al., 2023b; Bai et al., 2023) excel in natural language understanding and reasoning due to pre-training on extensive datasets (Chen et al., 2023; Ye et al., 2023). By incorporating tool-use capabilities, LLMs can extend beyond text generation to interact with the external environment, enabling tasks such as web searches and email management (Tang et al., 2023; Ye et al., 2024a). Furthermore, these capabilities are essential for addressing real-world user needs and advancing the development of general-purpose AI (Xi et al., 2023).\nCurrent approaches to training LLMs for tool use rely heavily on large-scale datasets generated from trajectories of interactions with tools (Qin et al., 2024; Zhuang et al., 2023). Standard supervised fine-tuning (SFT) is then applied to pre-trained models (Touvron et al., 2023a;b). While effective in some cases, these methods overlook key task-specific characteristics, leading to performance bottlenecks. For instance, ToolLLaMA-2-7B-v2 (Qin et al., 2024) achieves only 80% of GPT-4's performance on tool-use benchmarks (Ye et al., 2024a; Wu et al., 2024), indicating room for significant improvement.\nTo fill this gap, we conduct an in-depth analysis of three tool-using LLMs, uncovering several key phenomena. Notably, over 17% of the training data for ROTLLAMA contains tool-calling errors (Figure 1), primarily due to reliance on data from GPT series models, which are not entirely error-free with complex tools. Training on such flawed data can hinder model performance. Additionally, our tests of ToolLLaMA-2-7B-v2 and NexusRaven-13B-v2 (team, 2023) reveal that incorrect tool selections often share a common prefix with the correct ones (Table 1), and correcting initial erroneous tokens can lead to successful predictions. This suggests that certain tokens are more critical in tool selection. Moreover, the types of errors produced by tool calls are relatively limited (Figure 3), providing a foundation for targeted improvements across various error categories.\nBased on these insights, we propose TL-Training, a task-feature-based framework for training LLMs in tool use. TL-Training mitigates the negative impact of flawed training data by identifying erroneous interaction paths and excluding them from gradient updates. It prioritizes key tokens through adaptive weighting during SFT and incorporates tool feedback into a robust reward mechanism for reinforcement learning using the proximal policy optimization (PPO) (Schulman et al., 2017) algorithm.\nWe validate our approach by training CodeLLaMA-2-7B (Rozi\u00e8re et al., 2023) on a curated dataset of 1,217 tool-call trajectories generated with GPT-40\u00b9. Evaluations on four open-source test sets demonstrate that the model"}, {"title": "2. Preliminaries", "content": "Task Formulation Given a model M, a user query q, and a collection of tools T, the task of tool use requires M to iteratively select the appropriate tool $t_s \\in T$ at each step s, process its feedback $o_s$, and continue selecting subsequent tools $t_{s+1}$ until the query is resolved and a final answer is obtained. Formally, this can be represented as $t_{s+1} = M(\u00b7 | q, T, t_{0..s}, o_{0..s})$. This task is distinct from traditional natural language processing tasks, as it requires the model to invoke tools repeatedly and interpret their feedback dynamically. Despite its importance, to the best of our knowledge, there has been no systematic examination of the intrinsic properties of tool use. Thus, we aims to fill this gap by conducting an in-depth analysis focusing on both the training data and model performance.\nData Analysis For our analysis, we use the training set from RoTLLaMA, which includes 12,247 filtered multi-turn tool-call trajectories generated by GPT-4. Illustrated in Figure 1, 17% of these trajectories contain various errors in tool use, indicating that even advanced models like GPT-4 encounter challenges with complex tools. These erroneous trajectories pose a challenge for models trained through SFT, as they inherit these error patterns during learning. This predisposition to incorrect tool invocation highlights the need for more robust training methods to mitigate error propagation and improve overall model performance."}, {"title": "3. Approaches", "content": "Building on the analysis in Section 2, we propose TL-Training, a novel training paradigm for LLMs in tool use. As shown in Figure 2, this paradigm incorporates three core techniques: mitigating the adverse effects of suboptimal data by preventing its back-propagation (Section 3.1), prioritizing key tokens using adaptive weight adjustments (Section 3.2), and implementing a reward mechanism tailored to tool invocation error categories to enable effective reinforcement learning (Section 3.3).\n3.1. Mitigating Adverse Effects\nDuring the SFT stage, the objective is to align LLMs with the distribution of the training data. However, erroneous interaction paths in the data can negatively affect the model's decision-making, leading to an increased likelihood of incorrect tool calls. To address this, we design an automated process that identifies erroneous interaction paths and blocks their back-propagation, thereby reducing their harmful impact on the model.\nGiven a data sequence (q, to..s, 00..s), we seek to identify the erroneous tool call trajectory $T_e \\{t_0, t_1,...,t_s\\}$. Directly determining whether a specific ti is correct is challenging. However, the feedback or generated after each tool call contains structured error-reporting information, as summarized in Figure 3. We automate the identification of incorrect calls by sequentially analyzing or to extract $T_e$.\nOnce $T_e$ is identified, we mitigate the impact of these erroneous interactions by blocking their back-propagation during training. This is achieved by modifying the loss function as follows:\n$L_{MAE} = - \\sum_{D\\backslash t_s \\notin T_e} \\sum_{t_s} log p_M(t_s / q, T, t_{0..s-1},o_{0..s-1}),$\nwhere $D$ represents the entire training dataset.\n3.2. Prioritizing Key Tokens\nBased on the analysis in Section 2, and the insights from rows 1-2 of Table 1, we observe that the first token of a tool name, along with any subsequent token that shares a common prefix with other tool names, plays a more critical role in successful tool identification. As such, these tokens are more challenging for LLMs to generate correctly. However, standard SFT training maximizes the conditional probability of each token without distinction, treating all tokens as equally important. To address this limitation, we propose a scheme that adaptively adjusts the training weights of tokens according to their relative importance.\nGiven a data sequence (q,to..s, 00..s), where each tool $t_i = (t_i^1, t_i^2,...,t_i^{l_i})$ consists of $l_i$ tokens, we categorize the tokens into two sets:\n$K_i = \\{t_i^n \\in t_i | t_i^n is \\ a\\ key\\ token\\}$\n$NK_i = \\{t_i^n \\in t_i | t_i^n is \\ not\\ a\\ key\\ token\\}$\nWe then adjust the weights of $K_i$ and $NK_i$ based on their relative importance, allowing the model to focus more on the key tokens.\n$w_i^n = \\begin{cases} CLIP(\\frac{|NK_i|}{|K_i|}, 1, w_{max}) & if\\ t_i^n \\in K_i\\\\ 1 & otherwise \\end{cases}$\nHere, $w_{max}$ is the maximum adjustment multiplier, and CLIP(x, min, max) is used to constrain the adjustment factor to lie within the range [min, max]. The notation || represents the size of the set.\nWith these computed weights, we prioritize key tokens during training with the following objective:\n$L_{PKT} =  \\sum_{D\\backslash t_s} \\sum_{t_s} \\sum_{t_m} -w_i^m . log p_M(t_i^m | q, T, t_{0..s-1}, o_{0..s-1}, t_{i^1}...t_{i^{m-1}}).$\n3.3. Introducing a Reward Mechanism\nThe three stages of tool use by LLMs are interdependent, where an error in any stage can lead to the failure of the entire tool invocation. Fortunately, the types of errors that arise are limited, enabling us to introduce a reward mechanism based on these specific errors. This allows us to apply reinforcement learning algorithms that help align the model more closely with human intent and enhance its tool-use proficiency. To achieve this, we define a set of reward functions tailored to the tool use task and employ the PPO algorithm to optimize the model's performance.\nGiven an LLM-generated tool call prediction ti and its corresponding ground truth, we define the following reward function based on the quality of the LLM's tool use in various scenarios:\n$R(t_i) = \\begin{cases} -2  & if\\ t_i\\ cannot\\ be\\ parsed\\\\ -2  & if\\ t_i \\ contains\\ tool\\ hallucinations\\\\ -1.5 & if\\ t_i\\ calls\\ the\\ wrong\\ tool\\\\ R_p(t_i) & if\\ t_i\\ has\\ parameter\\ identification\\ issues\\\\ -0.25 & if\\ t_i \\ has\\ content\\ filling\\ issues\\\\ 1 & if\\ t_i\\ is\\ correct \\end{cases}$"}, {"title": "4. Experimental Setup", "content": "4.1. Dataset\nTo validate our approach, we construct a custom training set focused on multi-turn tool usage and evaluate it using four publicly available test sets. The corresponding statistics are provided in Table 2.\nTraining Data To train the LLMs using our method, we first construct a training dataset. Since ToolEyes provides a comprehensive set of invocable tools, we use it as a foundation to artificially create 1,217 relevant user requirements. GPT-40 is then employed to interact with these tools and generate the corresponding tool usage trajectories, which form our training set.\nWhile previous studies often construct over 100,000 data points for training (Qin et al., 2024), we deliberately limit our dataset size. Our main goal is to validate the effectiveness of our approach rather than to scale data volume. Surprisingly, the experimental results in Section 5 show that training on just 1,217 data points using our method matches or even exceeds the performance of leading LLMs.\nTest Sets To comprehensively evaluate LLM tool-use performance, we use four open-source tool usage test sets. ToolAlpaca (Tang et al., 2023), RoTBench (Ye et al., 2024c), and BFCL-v3 (Patil et al., 2023) are selected for single-turn tool use evaluations, while ToolEyes is used for multi-turn tool use assessment.\n4.2. Baselines\nIn this paper, we select nine existing LLMs from three different sources for a comprehensive comparison with our tool-use LLMs.\nTool-Use LLMS ToolLLaMA-2-7B and NexusRaven-2-13B are prominent tool-use LLMs, built on LLaMA-2-7B and CodeLLaMA-2-13B, respectively. These models are trained on a large volume of tool-use data, enhancing their ability to interact with tools. For example, ToolLLaMA-2-7B is trained on over 120,000 data points covering more than 16,000 tools using standard SFT, significantly boosting its tool-use capabilities.\nOpen-Source LLMs Among the existing open-source, general-purpose LLMs, ChatGLM-4-chat-9B (Zeng et al., 2024), Qwen-2-Instruct-7B (Yang et al., 2024a), LLaMA-3.1-Instruct-8B (Team, 2024a) and Qwen-2.5-Instruct-7B (Team, 2024b) have been specifically optimized for tool use, enabling them to interact with various tools to fulfill user needs.\nClosed-Source LLMs The GPT family represents some of the most advanced LLMs, demonstrating strong performance not only in general-purpose tasks but also in tool use, with notable generalization capabilities. For this study, we select GPT-3.5-turbo, GPT-40, and GPT-4-turbo as leading representatives of the GPT series for comparison.\nOur Model We apply the TL-Training paradigm to CodeLLaMA-2-7B, using the custom dataset of 1,217 examples, to develop TL-CodeLLaMA-2, a specialized tool-use LLM. Compared to the other models in this study, TL-CodeLLaMA-2 is the smallest and trained on the least amount of data."}, {"title": "4.3. Metrics", "content": "Given the distinctions between single-turn and multi-turn tool use, we design comprehensive metrics to evaluate the tool-use capabilities of LLMs.\nSingle-Turn Evaluation For the evaluation of single-turn tool calls, where the original dataset provides a standard answer, we follow Ye et al. (2024c) and assess the model's performance across three key areas:\n\u2022 Tool Selection (TS): Measures the model's accuracy in selecting the tool specified by the standard answer.\n\u2022 Parameter Identification (PI): Evaluates the model's ability to correctly select the tool and identify the relevant parameters required for invocation.\n\u2022 Content Filling (CF): Assesses the model's capacity to complete the single-turn tool invocation, including selecting the correct tool, identifying relevant parame-ters, and filling in the appropriate values.\nMulti-Turn Evaluation For multi-turn tool use, where no standardized interaction path exists, we adapt the methods of Qin et al. (2024) and Ye et al. (2024a), and assess performance based on following metrics:\n\u2022 Documentation Understanding Error (DE): Rep-resents the percentage of errors resulting from the model's failure to interpret the tool documentation, encompassing tool hallucinations, parameter hallucina-tions, and missing necessary parameters.\n\u2022 Tool Call Error (CE): Denotes the proportion of errors arising from incorrect tool invocation, covering all error types except those classified as DE.\n\u2022 Valid Answers (VA): Evaluates the percentage of instances where the model delivers valid responses within nine turns, reflecting its ability to meet user needs effectively."}, {"title": "4.4. Implementation Details", "content": "Training In the SFT stage, we use 1,217 constructed data samples, applying both the MAE and PKT strategies. We employ the AdamW optimizer (Loshchilov & Hutter, 2019) with cosine scheduling, setting the learning rate to le-6, a warmup rate of 0.01, and a batch size of 4, training for a total of 1 epoch. For the PKT strategy, $w_{max}$ is set to 9.\nIn the RL stage, we filter 1,194 entries from the constructed data and apply PPO with the reward function described in Section 3.3. The actor learning rate is set to 2e-6, the critic learning rate to le-6, and the batch size to 8, training for a total of 3 epochs.\nTesting For testing, we use the official prompt template for tool invocation, and apply greedy search during inference to maximize each model's tool-use capabilities."}, {"title": "5. Experiments", "content": "We compare the performance of nine LLMs, representing three different types, with our model across four tool-use evaluation sets (Section 5.1) and conduct detailed ablation experiments to validate the effectiveness of each component of our approach (Section 5.2).\n5.1. Main Results\nWe evaluate the performance of various LLMs on three single-turn tool-use test sets and one multi-turn tool-use test set, with the results summarized in Table 3 and Table 4. Despite using the smallest model size and the least amount of training data, our approach achieves results comparable to the best-performing models. These findings demonstrate the potential for smaller, efficient models to excel in the tool use task, making advanced capabilities more accessible for resource-constrained environments.\nSingle-Turn Evaluation Results from three single-turn tool-use test sets demonstrate that TL-CodeLLaMA-2, with only 7B parameters and 1,217 training examples, surpasses all other open-source LLMs in overall task completion (i.e., CF). Remarkably, on the ToolAlpaca and BFCL-v3 datasets, TL-CodeLLaMA-2 outperforms GPT-4-turbo, the top-performing GPT family model, by an impressive 15.78% and 12.08%, respectively. Most notably, TL-CodeLLaMA-2 is the only model\u2014among both open-and closed-source LLMs\u2014that consistently exceeds the average performance across all three aspects of every dataset evaluated, highlighting the effectiveness of our approach in enhancing single-turn tool usage capabilities.\nMulti-Turn Evaluation In the multi-turn test set, our approach significantly enhances the ability of LLMs to handle the tool use task. TL-CodeLLaMA-2 achieves an total error rate of just 5.64% on the test set, second only to GPT-40, which had the lowest error rate at 4.76%, and ahead of Qwen-2-Instruct at 7.49%. Additionally, TL-CodeLLaMA-2 maintaines a low error rate while achieving a high effective response rate, outperforming all other open-source models. In contrast, LLaMA-3.1-Instruct-8B frequently fails to provide a valid direct answer, effectively rendering it incapable of completing the task. These results highlight that our trained model effectively uses tools in multi-turn settings to solve complex user queries.\n5.2. Ablation Studies\nTo assess the individual contributions of the three compo-nents in our design that enhance LLMs' tool-use capabilities, we conduct ablation studies, comparing model performance"}, {"title": "6. Further Studies", "content": "To further demonstrate the strengths of our approach, we conduct additional analysis focusing on both robustness (Section 6.1) and general performance (Section 6.2).\n6.1. Robustness Improvement\nIn real-world environments, tools often contain various types of noise, and LLMs must be robust in their tool use to effectively meet user needs across different situations. RoTBench provides five tool-use test environments with varying noise levels, designed to evaluate whether LLMs can accurately understand the functions and properties of different tools and execute effective invocations. We compare the performance of TL-CodeLLaMA-2 and RoTL-LaMA across these five noisy environments, as shown in Figure 4. While ROTLLaMA has been optimized for such environments through targeted noise augmentation, TL-CodeLLaMA-2, without specific optimizations, matches or exceeds RoTLLaMA's performance in all aspects. This suggests that our approach allows the model to focus on the core functionality of external tools without being hindered by noise, making it more adaptable to real-world scenarios.\n6.2. General Performance\nThe strong performance of LLMs is largely attributed to their extensive world knowledge and generalizability acquired during pre-training (Ye et al., 2023). However, fine-tuning on domain-specific tasks can sometimes compromise this generalizability (Yang et al., 2024b; Ghosh et al., 2024). To assess whether TL-CodeLLaMA-2's general-purpose capabilities are affected by its exclusive training on tool-use data, we evaluate its performance on three general test sets: MMLU (knowledge) (Hendrycks et al., 2021), GSM8K (math) (Cobbe et al., 2021), and HumanEval (code) (Chen et al., 2021), comparing it to CodeLLaMA-2-7B. As shown in Figure 5, despite being fine-tuned solely for the tool-use task, TL-CodeLLaMA-2 retains its original knowledge and task performance and even shows slight improvements in math and coding abilities. This may be because our method requires only a small amount of training data, resulting in minimal changes to the model's original parameters, thus preserving its knowledge base (Ren et al., 2024; Wang et al., 2024; Ye et al., 2024d). Furthermore, the enhancement in tool-use ability appears to improve the model's reasoning capacity, contributing to better performance in math and coding tasks. These findings further underscore the broad applicability of our approach."}, {"title": "7. Related Works", "content": "Training LLMs for Tool Use LLMs capable of utilizing external tools significantly enhance their ability to interact with dynamic environments and address diverse user needs (Qin et al., 2023). However, the diversity and complexity of real-world tools present significant challenges in training such models. Existing methods, such as SFT, rely on the generation of extensive datasets of tool interactions (Song et al., 2023; Tang et al., 2023), enabling models to learn tool functionalities, invoke appropriate tools, and process feedback effectively. While effective, these methods are resource-intensive due to the large-scale data construction involved. To overcome these challenges, some studies have proposed encoding tool names as special tokens directly integrated into model training, embedding tool-specific knowledge into the model's intrinsic capabilities (Hao et al., 2023). This approach has shown promise for existing tools but remains limited in its ability to adapt to newly introduced tools. Building on these findings, our work introduces a novel training paradigm for tool-use LLMs, addressing both efficiency and adaptability. By leveraging a compact dataset of 1,217 data points and incorporating three task-specific components, our approach achieves state-of-the-art performance while significantly reducing data requirements.\nEvaluating LLMs in Tool Use Evaluating LLMs' tool-use capabilities is essential for understanding their ef-fectiveness in diverse scenarios. A common evaluation method involves comparing predicted outputs with standard answers from a single turn of tool usage (Chen et al., 2024). However, in multi-turn interactions, the variability in invocation processes complicates the definition of a single standard path. To address this, evaluations increasingly consider multiple dimensions of tool-use processes and outcomes (Ye et al., 2024a). Beyond tool-use performance, researchers have also investigated robustness and safety in practical scenarios. For example, robustness benchmarks like RoTBench (Ye et al., 2024c) and safety-focused studies such as ToolSword (Ye et al., 2024b) provide insights into how LLMs manage edge cases and avoid harmful outputs. In this paper, we evaluate LLMs across single-turn and multi-turn tool use to provide a more comprehensive assessment. Additionally, we analyze robustness to further demonstrate the superiority of our approach under diverse conditions."}, {"title": "8. Conclusion", "content": "In this paper, we introduce TL-Training, a novel paradigm for training LLMs specifically for tool use. Our approach mitigates the impact of erroneous interaction data, adaptively adjusts token weights, and introduces a reward mechanism tailed for tool use to facilitate PPO-based reinforcement learning. This methodology not only enhances LLMs' tool-use capabilities but also improves their robustness in noisy environments, all while preserving strong general performance across a range of tasks. Our findings demonstrate the effectiveness of TL-Training in addressing real-world challenges in tool use, offering a promising direction for future research in improving LLM interaction capabilities and adaptability."}, {"title": "Limitations", "content": "While we propose a novel paradigm for training LLMs in tool use, our work still has a few limitations. First, we do not construct large-scale training data. However, despite using only 1,217 data samples, our results show that we match or even surpass the best current tool-use performance, highlighting the strengths of our approach. Second, we design a reward function based on tool feedback for tool use directly, without training a separate reward model. Nonetheless, we experimentally demonstrate the effectiveness of our reward function. In future work, we plan to explore training a reward model specifically for tool learning to further improve model performance."}, {"title": "A. Theorems and Proofs", "content": "In this paper, we propose $L_{MAE}$ and $L_{PKT}$, aimed at enhancing the model's ability to utilize tools during the SFT stage. Additionally, we provide theoretical explanations of the effectiveness of these strategies.\nTheorem 1. During the SFT stage for LLM in tool use, gradient updates resulting from incorrect interaction paths in the training data can adversely impact the model's ability to choose the appropriate tool.\nProof. Let M be an LLM that interacts with a set of tools T to answer the user query q. At each step s, the model selects a tool $t_s \\in T$ based on the query and the history of tool selections and feedback:\n$t_{s+1} = M(\u00b7 | q, T, t_{0..s}, o_{0..s}),$\nwhere os is the feedback received after calling tool ts.\nConsider a dataset D comprising interaction sequences (q, to..s, 00..s), which includes both correct and erroneous tool calls. Let $T_e \\{t_0, t_1, ..., t_s\\}$ denote the set of erroneous tool calls identified via analysis of feedback 05.\nThe standard loss function during SFT aims to maximize the likelihood of the model's tool selections over the entire dataset:\n$L = -\\sum_{D\\backslash t_s} log p_M(t_s | q, T, t_{0..s-1}, o_{0..s-1}).$\nThe gradient of this loss with respect to the model parameters \u03b8 is:\n$\\nabla_{\\theta}L = - \\sum_{D\\backslash t_s} \\nabla_{\\theta} log p_M(t_s | q, T, t_{0..s-1}, o_{0..s-1}).$\nThis gradient comprises contributions from both correct and erroneous tool calls. The gradient component arising from erroneous tool calls is:\n$G_{error} = - \\sum_{D \\backslash t_s \\in T_e} \\sum_{\\nabla_{\\theta}} log p_M(t_s | q, T, t_{0..s-1}, o_{0..s-1}).$\nThese gradients encourage the model to replicate erroneous tool selections, thereby misguiding its learning process. Specifically, they can increase the likelihood of the model making incorrect tool calls in future interactions, which negatively impacts its performance. By including erroneous tool calls in the gradient updates, the model parameters @ are adjusted in directions that do not align with optimal decision-making. This is detrimental because it interferes with the model's ability to learn the correct sequence of tool selections that effectively resolve user queries. Therefore, errors in the training data introduce gradient updates that adversely affect the model's performance.\nTo mitigate this effect, we propose to modify the loss function to exclude erroneous tool calls from back-propagation:\n$L_{MAE} = - \\sum_{D\\backslash t_s \\notin T_e} \\sum_{t_s} log p_M(t_s | q, T, t_{0..s-1}, o_{0..s-1}).$\nBy omitting the erroneous tool calls from the loss computation, their associated gradients are not used to update the model parameters. This reduces the harmful impact of errors in the training data on the model's performance.\nTheorem 2. During the gradient update process in SFT, assigning higher weights to key tokens prioritizes their contribution to the loss function, enabling the model to focus more on these tokens and fit them better.\nProof. Let M be an LLM interacting with a set of tools T to answer the user query q. Each tool ts used at step s is a sequence of tokens:\n$t_s = (t_i^1, t_i^2,...,t_i^{l_i}),$\nwhere ls is the length of the token sequence for tool ts.\nFor each tool ts, we categorize its tokens into two sets:\n$K_s = \\{t_i^n \\in t_s | t_i^n is \\ a\\ key\\ token\\},$"}, {"title": null, "content": "$NK_i = \\{t_i^n \\in t_s | t_i^n is \\ not\\ a\\ key\\ token\\}.$\nWe assign weights $w_i^n$ to each token $t_i^m$ as follows:\n$w_i^n = \\begin{cases} CLIP(\\frac{|NK_i|}{|K_i|}, 1, w_{max}) & if\\ t_i^n \\in K_s,\\\\ 1 & if\\ t_i^n \\in NK_s, \\end{cases}$\nwhere | Ks and |NK| denote the number of key and non-key tokens in ts, respectively, wmax is the maximum adjustment multiplier, and CLIP(x, min, max) constrains x to the interval [min, max].\nThe modified loss function that prioritizes key tokens is:\n$L_{PKT} = - \\sum_{D\\backslash t_s} \\sum_{t_s} \\sum_{t_m} -w_i^m . log p_M(t_i^m | q, T, t_{0..s-1}, o_{0..s-1}, t_{i^1}...t_{i^{m-1}}).$\nwhere PM is the probability assigned by the model to token tm, given the context.\nThe gradient of the loss with respect to the model parameters 0 is:\n$\\nabla_{\\theta}L_{PKT} = -\\sum_{D\\backslash t_s} \\sum_{t_s} \\sum_{t_m} w_i^m. \\nabla_{\\theta} log p_M(t_i^m | q, T, t_{0..s-1}, o_{0..s-1}, t_{i^1}...t_{i^{m-1}}).$\nTokens with higher weights wr contribute more to the gradient:\n$||w_i^m . \\nabla_{\\theta} log p_M(t_i^m | q, T, t_{0..s-1}, o_{0..s-1}, t_{i^1}...t_{i^m}) || \\propto w_i^m$.\nAssuming $w_1^m > w_2^n$ for key token $t_i^m$ and non-key token $t_i^n$, the gradient contribution from $t_i^m$ is larger:\n$||w_1^m .\\nabla_{\\theta} log p_M(t_i^m | \u06f0\u06f0\u06f0 ) || > ||w_2^n . \\nabla_{\\theta} log p_M(t_i^n | \u06f0\u06f0\u06f0 ) ||.$\nDuring gradient descent, the parameter updates prioritize reducing the loss associated with higher-weighted (key) tokens:\n$\\theta' = \\theta \u2013 \\eta \\nabla_{\\theta}L_{PKT}.$\nAs a result, the model adjusts its parameters more significantly to fit the key tokens, improving its ability to generate them correctly. This leads to better fitting of tokens with higher weights. Therefore, assigning higher weights to key tokens during gradient updates enhances the model's performance on these tokens."}, {"title": "B. Prompt Template", "content": "We use the official prompt of each LLM for tool use, with corresponding examples provided from Table 6 to Table 13."}, {"title": "C. Examples of Tool Call and Feedback", "content": "In Table 14, we present examples of potential scenarios that may arise during tool calls to provide readers with a clearer and more visual understanding."}]}