{"title": "Predicting Trust Dynamics with Dynamic SEM in Human-AI Cooperation", "authors": ["Sota Kaneko", "Seiji Yamada"], "abstract": "Humans' trust in AI constitutes a pivotal element in fostering a synergistic relationship between humans and AI. This is particularly significant in the context of systems that leverage AI technology, such as autonomous driving systems and human-robot interaction. Trust facilitates appropriate utilization of these systems, thereby optimizing their potential benefits. If humans over-trust or under-trust an AI, serious problems such as misuse and accidents occur. To prevent over/under-trust, it is necessary to predict trust dynamics. However, trust is an internal state of humans and hard to directly observe. Therefore, we propose a prediction model for trust dynamics using dynamic structure equation modeling, which extends SEM that can handle time-series data. A path diagram, which shows causalities between variables, is developed in an exploratory way and the resultant path diagram is optimized for effective path structures. Over/under-trust was predicted with 90% accuracy in a drone simulator task,, and it was predicted with 99% accuracy in an autonomous driving task. These results show that our proposed method outperformed the conventional method including an auto regression family.", "sections": [{"title": "I. INTRODUCTION", "content": "AI technologies have been developed in various fields, and its use in everyday situations, such as autonomous driving, autonomous flying drones, and autonomous mobile robots, is rapidly advancing. The development of such technology allows people to delegate tasks to them, reducing their workload. While there are cases where all operations are simply left to autonomous driving or autonomous mobile robots, in many cases, they work together with humans in the same space. In this way, appropriate cooperation between humans and AI is indispensable in the use and development of AI technology, and what becomes important here is trust of humans in AI [1], [2].\nWhen humans overestimate the performance of AI beyond its actual capabilities, there is a risk of misuse, such as delegating tasks in situations where they should not be delegated. For example, in the case of autonomous driving, continuing to drive automatically despite a decrease in AI performance due to worsening weather conditions can lead to accidents. This overestimation of Als performance is referred to as over-trust[3]. On the other hand, underestimating the performance of AI excessively compared with its original capabilities can result in humans performing tasks that AI can carry out, preventing AI from demonstrating its actual performance. This under-trust, or excessive underestimation,"}, {"title": "II. RELATED WORK", "content": "A. Trust in HRI\nIn research on trust formation in human-robot interaction (HRI), the factors that influence trust are classified as follows [5], [6]: factors related to the robot (agent), factors related to the task and environment, and factors related to humans. The impact on trust formation is greatest in the order of factors related to the robot, factors related to the task and environment, and factors related to humans. Factors related to the performance of the robot include the reliability of the robot, the timing and frequency of task failures, the transparency of the system, etc., which are considered to determine the quality of the robots operation. Factors related to the task and environment include rationality, the danger that the task poses to humans, the load and complexity of the task, etc. Factors related to humans include personality, knowledge about the system, past experiences with robots, etc. Also, in HAI, trust formation can be considered in the same way by replacing factors related to the robot with factors related to the agent, but it is necessary to consider the difference resulting from the presence or absence of a physical entity.\nB. Trust Dynamics\nIn interactions between humans and agents, trust changes over time and with repeated interactions. This changing trust is referred to as trust dynamics. By capturing trust dynamics, it is also possible to more accurately understand the major factors that influence trust formation.\nIn Luo's study on trust dynamics in interactions with au- tonomous vehicles, it is shown that changes in performance due to internal system factors have a greater impact on trust than changes in performance due to external system factors [7]. Performance degradation due to internal system factors is caused by things like sensor failures, while per- formance degradation caused by external factors includes detours due to road construction and increased travel time due to traffic congestion.\nFurthermore, it has been shown that by incorporating the mechanism of trust, it is possible to make more accurate trust predictions than models that do not consider these factors [8]. In addition, there are efforts to develop new modeling methods that use trust dynamics, such as improving the accuracy of trust prediction by clustering based on trust dynamics and forming a suitable trust prediction model for each cluster [9].\nThus, constructing a model that takes into account trust dynamics not only enables accurate prediction of trust, but also enables accurate understanding of the factors that influence trust.\nC. Determination of Over/Under-trust by Equation of Inequality\nIn Lee study on designing reliance, over-trust is poor calibration in which trust exceeds system capability, and under-trust is trust falls of the system capability [10].\nOkamura [1], [2] proposed a framework for determining over/under-trust from a reliance equation involving the rela- tionship between AI performance and human performance. In human-AI cooperative decision-making task that involv- ing image recognition on a drone simulator, over/under- trust is determined from actually monitoring human rational decision-making behaviors based-on the reliance equation. The reliance equation is described like $T_H >=< T_A$, where $T_H$ and $T_A$ are AI's task success probabilities estimated by a human and the true value.\nIn this formulation, since over/under-trust can be detected only when a sequence of human decision-making behaviors by over/under-trust, it is hard to predict over/under-trust and prevent it. In contrast, our proposed method can predict over/under-trust and prevent it in advance.\nD. Trust Prediction\nFukuchi used a Transformer to predict reliance and performed reliance calibration using a reliance calibration"}, {"title": "III. METHOD", "content": "queue [11]. It should be noted that the reliance used here has a different definition from the trust we use.\nXu used a dynamic Bayesian network to predict trust dynamics [12]. A cooperative decision-making task was performed using a drone simulator, where humans intervene in the automatic control of the drone, and trust dynamics were predicted in situations where the environment changed over time. The Bayesian network was constructed using six variables: trust, AI performance, presence or absence of user intervention, changes in external factors, changes in trust, and feedback on trust. Trust at time $T = k$ is predicted from observations at times $T = k, k - 1$.\nA. Algorithm for Dynamic Path Diagrams\nTo predict and construct an explainable model of trust dynamics, which is an internal state that cannot be directly observed from the outside, we use DSEM, which extends structural equation modeling (SEM) to time series data. SEM is a model that estimates causal relationships by perform- ing path analysis between variables, dealing with observed variables that can be directly observed and measured, and latent variables that cannot be measured [13]. This SEM is extended along the time axis for the purpose of handling time series data and predicting the value at the next time point, which is DSEM [14].\nBy using DSEM as a prediction model, there are mainly three advantages:\n\u2022 It is possible to handle the concept of trust, which is defined as the value of Als performance estimated by humans and is an internal state of humans, as a variable.\n\u2022 It is possible to handle time series data and predict trust at the next time step.\n\u2022 the edges (paths) spanning between nodes are given top- down on the basis of prior research, so the model has high interpretability.\nOur proposed method of constructing a prediction model of trust dynamics can be summarized in the following steps.\n1) Exploratory design of path diagrams: A human designs an initial static path diagram for SEM based on domain knowledge containing insights from previous work and designer knowledge, and it is improved until the accuracy reaches a threshold $T$. This is done with human-in-the-loop procedures.\n2) Optimization of time-series structure: Dynamic path diagrams based on the static path diagram (Step 1) are automatically optimized with edges manually added between the path diagrams with different time steps. Optimization can be done by using a constrained-brute- force search algorithm which searches for all candi- dates of partial sequences within a constrained time range $n$. The objective function is time-series rolling- origin cross-validation [15]. Since the computational complexity of this search is $O(2^n)$, the exponential order of time-series length n is extremely large, so we introduced a hyper parameter, that it, the constrained time range $\\eta$ which can be heuristically set."}, {"title": "IV. EXPERIMENTS", "content": "A. Exp-1 Predicting Trust in Object Recognition with Drone Simulator\nThe path coefficients estimated for this experiment are the values in brackets in Fig. 1. This model is the result of applying Step 2 optimization of III A with $\\eta$ = 15 (the maximum length of the time-series data). The edges shown in solid lines represent causal relationships that have a positive effect, while the edges shown in dash-dotted lines represent causal relationships that have a negative effect.\nThe combination of E(AIP) input to the model as a past time series was selected to be E(AIP)(t,t\u22121) as a result of the model estimation for each combination of all subsets of time T. The combination of E(AIP) corresponding to the combination of time T was selected to be t, t - 1, which is the combination that makes the AIC the smallest among all combinations, after estimating the path coefficients of the model corresponding to all subsets of time T.\nFor the estimation of the model, we used the experimental data from a cooperative decision-making task involving im- age recognition on a drone simulator by [1]. These data were acquired from crowd sourcing-based online experiments with 194 participants, in which humans and AI cooperatively rec- ognize pothole on roads at 30 checkpoints. The snapshot of the simulator is shown in Fig. 2 and these date are completely discrete. The data is consisting two phases: high-performance AI of object recognition in the first 15 checkpoints and low- performance AI in the remaining 15 checkpoints to cause over-trust. Also, adaptive trust calibration was employed and trust calibration cues were expressed to a human when the over-trust was detected. As a result, the data of 96 participants include trust calibration with cues and that of other 96 participants do not include them. For more detail information on the data, see [1].\nThe estimated model allows us to infer the following qualitative causal relationship from coefficients of each edge:\n\u2022 Cognitive trust has positive causality from Al perfor- mance.\n\u2022 Over/under trust has negative causality from Al perfor- mance and positive causality from human performance and cognitive trust.\n\u2022 Cognitive trust has positive causality from calibration cue, while over/under trust has negative causality from calibration cue.\n\u2022 Reliance has positive causality from cognitive trust.\n\u2022 Over/under trust and reliance have positive causality.\n1) Results of Predicting Over-trust: The results of over- trust prediction at the next time step using the trust prediction model with our proposed method (PM) are shown below. These results are from an analysis of over-trust prediction at the next time step predicted from the current time observation value by each model. To verify the prediction results, we used the experimental data by [1] used for model estimation."}, {"title": "B. Exp-2 Predicting on Autonomous Driving Simulator", "content": "Next, we predicted over/under-trust using an autonomous driving simulator as a more continuous-time task in contrast of the previous discrete-time drone simulator. The task involved playing video clips from a car-mounted camera on the web and having a human intervene during playback. The video played as an onboard video of an autonomous vehicle was from the BBD100K driving dataset [16]. The users were told that the video being played was filmed by an autonomous vehicle. The users played the video on a web browser and indicated their intention to intervene by pressing the space bar on the keyboard when they felt danger while driving. During the experiment, users could continue to monitor the video being played and could intervene as necessary when they felt danger.\nThe video was played in 22 scenes, with the AI driving at high performance in the first seven scenes, the performance dropping in the next nine, and the AI's performance increas- ing again in the final six scenes. In the middle nine scenes, if no intervention was made when the Al's performance was lower than that of a human, it was considered to indicate over-trust. Interventions are recorded as one step within a 10-second window, and a total of four steps are recorded within one scene."}, {"title": "V. DISCUSSIONS", "content": "A. Algorithm for Preventing Over/Under-trust\nFirst, in this study, we focused on over-trust, which poses a bigger problem in particular environments when fallen into than distrust, and made predictions of over-trust. However, of course, the model we built is adaptable to both over-trust and under-trust predictions.\nWe are currently developing an algorithm for prevention over/under trust with our proposed trust dynamics prediction in this work. The algorithm's basic policy is very simple, that is, to express trust calibration cues to a human just when the over/under-trust is predicted to occur in the next time step.\nHowever, if a human does not react to these cues, what should the AI do? Repeatedly express the cues until the human executes trust calibration? This is not a simple problem so we need to develop an algorithm for preventing over/under-trust through the design of calibration cues. For effective cues, we should carefully design promising cues and conduct experiments to evaluate them. This is our future work."}, {"title": "VI. CONCLUSION", "content": "In this paper, we proposed a novel method for constructing a prediction model of trust dynamics toward AI in human-AI cooperative decision-making. In our method, first exploratory design is done, and a static path diagram is obtained; then optimization is applied to time-series path diagrams. In this framework, directly predict over/under trust without monitoring the execution of human rational behaviors is quite original and important for preventing over/under-trust. Another advantage of our proposed method is its high explainability due to the path structure. We applied this proposed method to two different task domains involving human-AI cooperative object recognition and autonomous driving. In both domains, we confirmed that our proposed method could outperform conventional methods including AR, ARMA and Seasonal ARMA."}, {"title": "B. Comparison with DNN-based Approaches", "content": "In the experimental comparison with conventional and baseline systems, we did not include deep neural net- work (DNN)-based time-series prediction including Trans- former [18] and LSTM [19] as state-of-the-arts methods. Our reasons for not utilizing them are because the task properties like snapshots (captured images) of a task simulator, levels of task difficulty, error significance etc., are hard to described and introduced to a SEM framework as observed variables of high-dimensional vectors. In contrast, DNN-based predic- tion can easily and fully utilize such task properties with embedded vectors as input.\nThus, it is difficult to prepare the same input for both our proposed method and DNN-based time-series prediction in a fair way. However, we are trying to develop both DNN- based prediction without task description as its input and our proposed methods with task descriptions using high dimensional vectors as observed variables."}, {"title": "C. Explainability, Interpretability, and Utility of Our Proposed Method", "content": "We think that the explainability and ease of interpretabil- ity [4] of our proposed method can be guaranteed because the prediction models can be described with path diagrams as directed graphs. However, explainability and interpretability were not confirmed in the experiments. Thus, we need to conduct experiments with participants to confirm them. This is also our future work.\nWe can utilize the same (static) path diagrams in both experiments in human-AI cooperative object recognition and driving. However, the design of path diagrams is basically dependent on the task domains. Clarifying general and com- mon path diagrams in various task domains, and the coverage of the proposed method are also open problems."}, {"title": "D. Limitation and Coverage of Our Proposed Method", "content": "Our proposed method has significant limitations. First, the SEM-based approach needs human knowledge because it utilizes an exploratory method with human intuition. This might be a hard limitation depending on the task domain.\nFurthermore, there is no guarantee that precise prediction models will be constructed. Last, our method basically in- cludes a brute-force search with a high computational cost for optimal partial path diagrams. In practice, we can restrict the search space with \u0442. We are investigating more sophisticated combinatorial optimization algorithms.\nWe need to discuss the coverage of our proposed approach with DSEM to apply it to other domains. Basically, we consider our approach to be applicable to any domain in which designers have rich knowledge on factors influencing target variables regardless of prediction accuracy.\nThus, we plan to apply this approach to human-robot interaction and trustworthy AI including the prevention of human abuse of robots [20] and robotic trust repair [21]. In particular, for trust repair, we will develop special trust repair cues [21]."}]}