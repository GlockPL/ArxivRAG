{"title": "Maybe you are looking for CroQS\nCross-modal Query Suggestion for Text-to-Image\nRetrieval", "authors": ["Giacomo Pacini", "Fabio Carrara", "Nicola Messina", "Nicola Tonellotto", "Giuseppe Amato", "Fabrizio Falchi"], "abstract": "Abstract. Query suggestion, a technique widely adopted in information\nretrieval, enhances system interactivity and the browsing experience of\ndocument collections. In cross-modal retrieval, many works have focused\non retrieving relevant items from natural language queries, while few\nhave explored query suggestion solutions. In this work, we address query\nsuggestion in cross-modal retrieval, introducing a novel task that focuses\non suggesting minimal textual modifications needed to explore visually\nconsistent subsets of the collection, following the premise of \"Maybe you\nare looking for\". To facilitate the evaluation and development of methods,\nwe present a tailored benchmark named CroQS. This dataset comprises\ninitial queries, grouped result sets, and human-defined suggested queries\nfor each group. We establish dedicated metrics to rigorously evaluate the\nperformance of various methods on this task, measuring representative-\nness, cluster specificity, and similarity of the suggested queries to the\noriginal ones. Baseline methods from related fields, such as image cap-\ntioning and content summarization, are adapted for this task to provide\nreference performance scores. Although relatively far from human per-\nformance, our experiments reveal that both LLM-based and captioning-\nbased methods achieve competitive results on CroQS, improving the re-\ncall on cluster specificity by more than 115% and representativeness mAP\nby more than 52% with respect to the initial query. The dataset, the im-\nplementation of the baseline methods and the notebooks containing our\nexperiments are available here: paciosoft.com/CroQS-benchmark/\nKeywords: Text-To-Image Retrieval Query Suggestion Cross-modal\nRetrieval Image Group Captioning Cross-modal Query Suggestion.", "sections": [{"title": "1 Introduction", "content": "Query formulation is a central challenge in Information Retrieval (IR), as it\nconsists of aligning user queries with their actual information needs. Query ex-\npansion and query suggestion are two families of query reformulation approaches\nthat aim to better align the search results with the user's information need [19]."}, {"title": "2 Related Works", "content": "Cross-modal query suggestion is a task spanning different research domains.\nBesides classical IR query suggestion, this task borrows from methods extracting"}, {"title": "2.1 Query Suggestion", "content": "In dense retrieval [9], query reformulation has a lower impact in improving re-\ntrieval metrics, while it can still be exploited to improve retrieval systems' inter-\nactivity and explorability. Query suggestion provides to the user several queries\nthat the system deems related to the user's interest according to assumptions\nmade by the retrieval system [19]. Various works proposed solutions to build\nquery suggestions based on large-scale graphs of queries and their click-through\nrates [5,14]. More recent approaches for solving this task in the textual domain\nemployed RNN [28] and transformer networks [18] or leveraged different learn-\ning schemas such as reinforcement learning [4]. Nowadays, the interest is shifting\ntowards the use of Large Language Models (LLMs) [3,2], which show remarkable\nabilities in suggesting queries given the user intention or the search history as a\nprompt. The work mostly resembling our objective is the one in Zha et al. [29],\nwhich proposed Visual Query Suggestion, a system for collections of captioned\nimages that groups the result set of an initial query and suggests a reformulated\nquery for each group. However, they built the queries from the texts associated\nwith the images leveraging a keyword selection algorithm, whereas our scenario\noperates under the assumption that no textual metadata is available for the\nimages."}, {"title": "2.2 Image Group Captioning", "content": "The choice of image captioning as a foundational field is motivated by several\nfactors. In cross-modal query suggestion, the goal is to generate a suggested\nquery that effectively represents a set of images while maintaining alignment\nwith the initial query. Similarly, image captioning methods aim to generate a\ndescriptive sentence for a given image. Although these two tasks are not identical,\nthey share important commonalities in their input-output structures. Both tasks\nstart from visual inputs \u2014 images and generate textual outputs. This overlap\nsuggests that techniques developed for image captioning may be adapted to\nhandle cross-modal query suggestion. The advent of CLIP [21] introduced the\npossibility of representing texts and images in a cross-modal space and paved the\nway for various works in the image captioning field exploiting it. Mokady et al.\nintroduced ClipCap [17], an architecture that, given a CLIP image embedding,\ngenerates a caption for it through the use of a mapping network and a large\nlanguage model. In 2023, Li et al. [10] introduced DeCap, which has the same\ngoal but leverages a simpler architecture and achieves higher captioning results.\nWang et al. proposed a method to build image captions optimizing the dis-\ntinctive aspects of the input image with respect to the characteristics of a group"}, {"title": "3 Cross-modal query suggestion", "content": ""}, {"title": "3.1 Problem definition", "content": "Let qo \u2208 T be the initial query prompted by the user in natural language (being\nT the set of text strings) and R(qo, I) C I be the result set obtained by searching\nI with qo, where I = {I}_{i=1}^N is the image collection. Indicating with 2^X the\npower set of X (the set of subsets of X), we define a cross-modal query suggestion\nsystem F: T \u00d7 2^I \u2192 2^T as\nQ = F(qo, R(qo, I)),\n(1)\nwhere Q = {\u011di}_{i=1}^M C T is the set of suggested queries, the output of the system.\nAll the suggested queries \u011di should be variations of the same initial query qo,\noriented to disambiguate and better explain the initial result set R(qo, I)."}, {"title": "3.2 Proposed Framework", "content": "The core of the cross-modal query suggestion task revolves around formulating\nsuggested queries based on the original query and the main concepts represented\nin the original result set. We assume that an initial result set R(qo, I) contains\none or more disjoint groups C = {C_i}_{i=1}^M \u2286 R(qo, I), Ci \u2229 Cj = \u00d8 Vi \u2260 j where\neach Ci comprises images sharing semantic content with a higher specificity (e.g.,\n\"a bike race\" or \"a skiing race\") than the original query qo (e.g., \"a sport race\").\nIn the proposed framework, the process begins with the user submitting an\noriginal query qo, then the result set R(qo, I) provided by the retrieval system is\npartitioned into M distinct semantic groups Ci by a clustering algorithm. At this\npoint, the cross-modal query suggestion system is tasked to generate a suggested\nquery qi for each group Ci. The suggestion system runs M times, receiving as\ninput qo and one semantic group at a time. Finally, a user interface presents the\nset of suggested queries {\u011d_i}_{i=1}^M to the user for further exploration.\nUnder this assumption, we simplify the formulation as:\n\u00cei = F(qo, Ci),\n(2)"}, {"title": "4 Evaluation", "content": ""}, {"title": "4.1 Dataset", "content": "Defining an objective way to compare different cross-modal query suggestion\nmethods is not trivial. One of the most critical variables for a fair comparison\nis the definition of the clusters C for the given result set of images R(90, I).\nTwo systems could come up with completely different and perhaps equally valid\nclusters, leading to difficult comparability of the results.\nConsequently, through manual work of collection exploration and query def-\ninition, we built a benchmark for cross-modal query suggestion, named CroQS,\nto factor out cluster variability in evaluating the generation of suggested queries.\nCroQS comprises 50 initial textual queries. For each query, we manually vali-\ndated the images in the result set and partitioned them into a varying number\nof semantic clusters of images (min 2, max 10, 5.9 on average), totaling 295\nclusters. Each cluster is associated with a human-annotated suggestion for the\ntarget refined query. We built CroQS on top of the train split of COCO [12], a\ndataset of more than 118,000 captioned images. The image captions provided by\nCOCO only assisted us in defining the human-annotated suggestions and were\notherwise discarded for our purposes. Employing CroQS, it is possible to compare different methods only by means\nof the suggested queries they generate. In the following, we present the properties\nto be evaluated and a set of metrics that we can exploit to measure them."}, {"title": "4.2 Evaluation Metrics", "content": "Due to the novelty of the cross-modal query suggestion task, defining the prop-\nerties to be pursued is also necessary. We distinguished three main desired prop-\nerties of the generated suggested queries: Cluster Specificity, Representa-\ntiveness, and Similarity to Original Query.\nCluster Specificity measures how the suggested query \u011di describes the images\nbelonging to its cluster Ci. In particular, it must highlight the properties that\ndistinguish the images in Ci from those in other clusters Cj, i \u2260 j. To measure"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Baseline Methods", "content": "We propose two baseline methods to tackle the cross-modal query suggestion\ntask inspired by related fields image captioning and content summarization."}, {"title": "5.2 Quantitative Results", "content": "This subsection reports the results obtained over our benchmark dataset by the\nadapted baseline methods. For reference, the scores obtained by the human-\nannotated query suggestions are also reported. Each score is computed as the\nMacro Average of the scores obtained on each cluster, for each initial query qo.\nRepresentativeness scores are calculated based on the top 100 documents of the\nresult sets for the evaluated suggestions, and NDCG is assessed at rank 10. Table 1 reports the metrics obtained by the best configuration of each archi-\ntecture. Human-annotated suggestions are getting the best results in all fields,\nexcept for similarity to the initial query, where the GroupCap model gets a higher\naverage score. The methods based on DeCap and ClipCap are biased towards the\nCluster Specificity metric, where those are getting the highest scores, while they\nrank last on the other metrics. This can be due to the original application field\nof these models, the captioning task, where the outputs need to be very specific\nand descriptive for the given image. This enabled them to effectively identify\nthe characteristics that distinguish one cluster from the others for the same ini-\ntial query, leading to high scores in Cluster Specificity. On the other hand, the"}, {"title": "5.3 Ablation Study", "content": "Prototype Selection We experimented with two ways to select the prototype\np of the cluster C in CLIP space as input for the prototype captioning methods:\na) selecting the centroid of the image cluster\np = \\frac{1}{|C|} \u2211_{IC \u2208 C}CLIP(I),\n(5)"}, {"title": "6 Conclusions", "content": "In this paper, we formalized the cross-modal query suggestion task, introducing\na set of suitable properties and metrics for quantitatively evaluating different"}]}