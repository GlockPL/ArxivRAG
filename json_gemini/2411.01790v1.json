{"title": "THINKING FORWARD AND BACKWARD: EFFECTIVE BACKWARD PLANNING WITH LARGE LANGUAGE MODELS", "authors": ["Allen Z. Ren", "Brian Ichter", "Anirudha Majumdar"], "abstract": "Large language models (LLMs) have exhibited remarkable reasoning and planning capabilities. Most prior work in this area has used LLMs to reason through steps from an initial to a goal state or criterion, thereby effectively reasoning in a forward direction. Nonetheless, many planning problems exhibit an inherent asymmetry such that planning backward from the goal is significantly easier \u2013 for example, if there are bottlenecks close to the goal. We take inspiration from this observation and demonstrate that this bias holds for LLM planning as well: planning performance in one direction correlates with the planning complexity of the problem in that direction. However, our experiments also reveal systematic biases which lead to poor planning in the backward direction. With this knowledge, we propose a backward planning algorithm for LLMs that first flips the problem and then plans forward in the flipped problem. This helps avoid the backward bias, generate more diverse candidate plans, and exploit asymmetries between the forward and backward directions in planning problems \u2013 we find that combining planning in both directions with self-verification improves the overall planning success rates by 4-24% in three planning domains. Code: github.com/irom-princeton/llm-backward.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) are increasingly capable in reasoning tasks - they can perform commonsense reasoning in a broad set of contexts (Kojima et al., 2022), reason about abstract patterns in data (Mirchandani et al., 2023), and learn to reason from human feedback (Liang et al., 2024). Such capabilities also open up the possibility of LLMs performing long-horizon planning (Valmeekam et al., 2024), where the model needs to reason about how the initial state and final goal of the problem can be connected through intermediate steps. Most existing work has explored such problems by asking the model to reason in the forward direction, i.e., generating intermediate steps from the initial state to the final goal. However, in many planning problems, there is an inherent asymmetry: generating the correct last steps leading to the goal can be much easier than generating the correct steps from the beginning. This leads to the question: can LLMs plan better if they also reason in the backward direction?\nAs an example, consider a robot navigating in a room (Fig. 1): if the final goal is the bedroom at the end of a long and narrow hallway, it is natural to connect the bedroom with the beginning of the hallway first in the plan, and then search for the path that connects the hallway to the initial state. In this example, there is a \u201cbottleneck\" that causes the asymmetry: the number of possibilities when planning backward from the goal is constrained by the bottleneck (hallway), while the possibilities fan out quickly when planning from the start. Such bottlenecks are ubiquitous in planning problems; for example, in proving mathematical theorems (Loveland, 2016), there may be many possible steps to start a proof of a theorem, but the final steps can be much more closely related to the theorem statement and thus easier to choose. In our experiments, we quantify this bottleneck effect by comparing the exact number of search steps used by common forward-search algorithm (e.g.,"}, {"title": "2 FORMULATION AND PLANNING DOMAINS", "content": "We define a (text-based) planning problem as $P = (S, A_s, s_0, g, T_{\\text{max}}, f)$, where $S$ is the space of possible states, $A_s = \\{A_s\\}_{s \\in S}$ includes the space of possible actions (planned steps) at each state $s \\in S$, $s_0 \\in S$ is the initial state, $g \\in S$ is the goal, $T_{\\text{max}}$ is the maximum allowable steps, and $f$ is a ground-truth solution verifier that determines if a plan solves $P$, i.e., whether a plan connects $s_0$ to $g$, and possibly, is of optimal length. The generated plan $A = (a_0, \\dots, a_T)_{T \\leq (T_{\\text{max}}-1)}$ is verified by $f$ based on these rules. Each action $a_t$ converts state $s_t$ to $s_{t+1}$, and we denote this as $s_t \\xrightarrow{a_t} s_{t+1}$.\nWe denote pre-trained LLMs with parameters $\\theta$ as $p_\\theta$. Instead of the usual token-level gen- eration, for convenience we consider LLMs generating the next step in a plan by sampling $a_t \\sim p_\\theta(a_t | s_0, g, \\{a_0, ..., a_{t-1}\\}, q)$, where $q$ denotes the rest of the text prompt (e.g., few-shot exemplars and instructions). For convenience, from now on we also assume that $q$ includes text de- scriptions of $s_0$, $g$, and $(a_0, ..., a_{t-1})$. During planning, LLMs may also generate other intermediaries such as the estimated states along the plan $(\\hat{s}_1, ..., \\hat{s}_t)$, which might help the LLM reason about the next step given the current estimated state."}, {"title": "3 LLMS CANNOT PLAN BACKWARD AS EFFECTIVELY AS FORWARD", "content": "As described in Section 1, many planning problems have an asymmetry that makes a given direction (forward vs. backward) easier for classical planning algorithms such as breadth-first search (BFS). Here, we examine whether the planning performance of LLMs is similarly impacted, and if the LLM's performance in a given planning direction can be predicted by the number of search steps required by BFS in that direction. If this is the case, it opens up the possibility of improving LLM-based planning by planing backward when the backward direction is favorable. Below we first introduce the backward planning algorithm we use for LLMs before showing the experimental results."}, {"title": "3.1 BACKWARD PLANNING", "content": "During backward planning, the LLM is given the same initial and goal states, but the prompt $q$ asks it to generate the plan $A$ in the reversed order. For example, consider an in- stance of GRAPH PLANNING where the shortest path from Node 1 to Node 3 is (1,5, 2, 3); then, the LLM should first output the backward plan as (3, 2, 5, 1) and reverse the order be- fore it proposes the plan to the verifier.\nAs described in Section 1, backward planning can be useful when the search space from the goal is smaller than from the initial state. We can quantify the asymmetry between forward and backward planning by computing the number of search steps used by algorithms such as BFS in either direction."}, {"title": "3.2 ALGORITHM: SAMPLE FORWARD (BACKWARD) THEN SELF-VERIFY", "content": "While the basic version of LLM planning involves asking it to generate a single plan, we consider a more robust version where the LLM first samples multiple candidate solutions in one direction, and then self-verifies them to choose the final solution; then the verifier $f$ checks the final plan. In both stages, the LLM is shown few-shot exemplars to familiarize itself with the problem and to learn to verify the candidate plans step by step. Given the maximum number of attempts M, the sampling temperature of the LLM is set to be 0 for the first attempt, and nonzero for the later ones. Self-verification works differently depending on the domain: in GRAPH PLANNING, since the solution needs to be optimal, we save all the unique candidate plans $\\{A_i\\}$ after all M attempts, and then present all of them to the LLM and ask it to verify them as well as to choose the optimal one; in ARRAY TRANSFORMATION and BLOCKSWORLD, since the solution does not need to be optimal, the LLM verifies each candidate plan as soon as it is sampled, and either stops when it deems one plan correct or after all attempts. For all the experiments, we use the GPT-4o model from OpenAI unless noted otherwise."}, {"title": "3.3 RESULTS", "content": "LLM plans better in the easier direction. Here we run experiments with both directed and undirected graphs in GRAPH PLANNING, and ask the LLM to plan either forward or backward. We also compute the number of search steps used by BFS in both directions. shows the planning success rates achieved by forward and backward planning at different levels of forward/backward difficulty (as quantified by BFS computations). For either direction, the success rate is generally higher when the number of computations is lower in that direction. This finding suggests that LLM planning is akin to a forward-search algorithm such as BFS in terms of the difficulty of planning in a given direction, and thus can be potentially improved by planning backward when it is easier. We also find similar results with ARRAY TRANSFORMATION shown in Appendix B.\nLLM plans worse in the backward direction.\nNext, we study the effectiveness of backward plan- ning by examining whether the LLM can achieve the same level of planning success in the backward direction as compared to forward. We find that this is not the case. shows that the backward suc- cess rate is consistently lower than forward. We also calculate the average planning success rates for all four settings (from experiments in Section 5) as shown in Table 1, the LLM consistently plans worse in the backward direction. We conjecture that this bias may be attributed to the forward (i.e., left to right) autoregressive nature of LLM output generation, as well as biases from the training dataset. Next we propose a solution to such backward bias and allow LLMs to plan effectively in the backward direction (Flip in)."}, {"title": "4 PROPOSED SOLUTION: PLAN BACKWARD BY FLIPPING THE PROBLEM", "content": "If the LLM cannot plan backward as well as forward, how can it effectively exploit the backward direction in planning? We propose a simple solution: flip the problem such that the original goal becomes the new initial state and vice versa. The LLM can then plan in the forward direction for the flipped problem, which corresponds to the backward direction of the original problem. This avoids the bias of weak LLM planning in the backward direction. However, there are a few subtleties that must be taken care of when flipping the planning problem.\nChange of the state-dependent action space. In some cases, $A_s$ needs to be adjusted for the flipped problem. For example, with a directed graph, an edge from Node 1 to Node 3 in the original problem corresponds to an edge from Node 3 to Node 1 in the flipped problem \u2013 Node 3 might not be reachable from Node 1 in the flipped problem. In contrast, there is no change needed for undirected graphs, ARRAY TRANSFORMATION, and BLOCKSWORLD. With directed graphs, we prompt the LLM to first generate the new text representation of the graph (Fig. 4 right), and then generate a plan for the flipped problem.\nFlipping back the plan. After the plan $A'$ for the flipped problem is generated, steps within it need to be reversed in order and also often \"flipped back\" to generate a forward plan for the original problem:\nFlip back the plan: $A' = \\{a'_0, ..., a'_T\\} \\rightarrow A = \\{a_T, ..., a_0\\}$, where $s_t \\xrightarrow{a'_t} s_{t+1}, s_{t+1} \\xrightarrow{a_t} s_t$. (1)\nWe assume that each action $a \\in A_s$ can be inverted, i.e., if $s \\xrightarrow{a} s'$, there exists $a' \\in A_s$, such that $s' \\xrightarrow{a'} s$. In practice, we prompt the LLM to generate the corresponding $a_t$ after it generates $a'_t$. shows an example for BLOCKSWORLD: the action \"put yellow on red\" is flipped back to \"unstack yellow from red\", after the order of the plan is flipped."}, {"title": "5 EXPERIMENTS WITH FLIPPING THE PROBLEM", "content": "With experiments in the three planning domains introduced in Section 2, we investigate the following questions in the corresponding subsections below:"}, {"title": "5.1 BACKWARD PLANNING WITH THE FLIPPED PROBLEM HELPS IMPROVE SUCCESS RATE", "content": "In order to test whether flipping the problem can help planning success rate, we run extensive experiments with the three planning domains and compare Fwd, Back, Flip, Fwd-Back, and Fwd-Flip. We use a maximum $M = 6$ attempts for all experiments. Table 2 shows each result averaged over 200 trials. Flip outperforms Back and matches Fwd performance; we also see the similar trends in . In addition, Fwd-Flip consistently leads to the highest planning success rate, improving by 4-24% over Fwd, except for one setting where all baselines achieve close to 100% success. These results corroborate that flipping the problem mitigates the backward bias.\nFwd-Flip exploits asymmetries in the problems. One of the main motivations of planning backward is that many planning problems have a bottleneck structure: there exists a less connected part of the state space near the initial state or the goal \u2013 this makes it easier to plan from the end closer to the bottleneck. In , we calculate the difference between forward vs. backward computations for BFS in GRAPH PLANNING, bin them, and find the success rate of each bin for Fwd and Fwd-Flip \u2013 the plot shows the average success rate over the three directed graph settings (shades show the minimum and maximum over the three). We find that Fwd tends to perform worse when the forward computations needed are higher, meaning that it cannot plan as effectively when the forward direction is more difficult. In contrast, Fwd-Flip maintains a similar level of success regardless of forward vs. backward planning difficulty.\nFlipping the problem generates more diverse candidate solutions. The improved performance of Fwd-Flip over Fwd can also be partly attributed to the generation of more diverse solutions. Fig. 6"}, {"title": "5.2 IMPROVEMENT FROM FLIPPING THE PROBLEM VARIES", "content": "Effect of backward success rate without flipping. While in Section 5.1 we have shown that flipping the problem helps the LLM exploit asymmetries between forward and backward planning, we tend to find that the performance difference between Fwd-Flip and Fwd-Back varies substan- tially in ARRAY TRANSFORMATION (Table 2). When func- tions repeat and cut are used, Back does not perform significantly worse than Fwd \u2013 we believe this is due to the backward step being easily recognizable by Back ( when the initial and goal array sizes differ (since repeat and cut change them). In this case, Fwd- Flip does not improve much over Fwd-Back. With func- tions shift_left|right, reverse, swap that do not change array sizes, we see a much more significant back- ward bias (53.5% vs. 36% between Fwd and Back) and there is a bigger margin between Fwd-Flip and Fwd-Back (56% vs. 46%).\nEffect of LLM capability. We also hypothesize that the effect of planning in the flipped problem also depends on the inherent capability of the LLM. First, regardless of the choice of the LLM, Fwd-Flip should consistently improve the success rate. In we run GPT-3.5-turbo and GPT-4-turbo besides GPT-4o with a directed graph setting and in BLOCKSWORLD, and we see that Fwd-Flip consistently outperforms Fwd. We also find that the design choices of the algorithm can affect the performance, and we highlight two aspects below:\n\u2022 Reliability of self-verification: while we allow the LLM to generate multiple candidates (in either direction), it can only present a single solution after it self-verifies the candidates. Hence, the LLM needs to reliably self-verify the candidate plans to achieve a high success rate. In addition to the success rate, shows the self-verification error rate. We find that the self-verification error reduces as the LLM becomes more capable. In BLOCKSWORLD, we find that GPT-3.5-turbo always self-verifies its sampled plan to be correct, leading to close to zero success rate."}, {"title": "5.3 LLM CAN REASON WHEN TO FLIP IN CERTAIN SETTINGS", "content": "With Fwd-Flip we randomly choose between the original and the flipped problem for each candidate solution. We now explore whether the LLM can also reason when to flip the problem \u2013 we"}, {"title": "6 RELATED WORK", "content": "Bidirectional planning. It is well known in classical planning that searching and planning from the backward direction can often reduce the computations needed. Bi-directional search has been incorporated into popular sampling-based planners (e.g., rapidly-exploring random trees) (Jordan & Perez, 2013) and heuristics-based techniques (e.g., A*) (Kuroiwa & Fukunaga, 2020) to improve efficiency.\nPlanning with LLMs. LLMs have been widely applied in different planning problems such as text games (Yao et al., 2024; Wang et al., 2023), robot planning (Huang et al., 2022; Ahn et al., 2022; Ren et al., 2023), scientific experimentation (Wang et al., 2022a), and web navigation (Deng et al., 2024) their strong planning capabilities originate from pre-training with enormous amounts of text data, which elicits reasoning capabilities (Wei et al., 2022). Beyond simple zero- or few-shot learning, LLMs have also been combined with classical planners (Liu et al., 2023; Silver et al., 2024) and external tools (Zeng et al., 2022) to further boost performance. However, no"}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this work, we investigate how to enable LLMs to effectively plan in the backward direction from the desired goal to improve the overall planning success rate. First, our experiments reveal consistently worse performance of LLMs when planning backward. To address this, we propose instructing the LLM to flip the problem first and then plan forward in the flipped problem. Combined with self-verification, we find that generating candidate solutions from both directions improves planning success rates by 4-24% over forward-only planning in three different domains.\nOne immediate future direction is to better teach LLMs to reason and plan backward, e.g., by fine-tuning with correct forward and backward reasoning traces . We also believe that our framework of combining forward and backward reasoning can be extended to general reasoning problems, e.g., allowing LLMs to generate more diverse reasoning traces from the backward direction or, enforcing reasoning self-consistency from both forward and backward directions"}, {"title": "A APPENDIX - PLANNING DOMAINS", "content": "We provide all codes needed to replicate the experiments in the attached supplementary materials."}, {"title": "A.1 GRAPH PLANNING", "content": "Configurations. To generate the desired graphs, we use gnp_random_graph function from the networkx package, with which we specify N, the number of nodes, p, the probability of two nodes are connected with an edge, and whether the edges are directed or not. We also apply rejection sampling to ensure the shortest path involves a total of five nodes. The the initial and goal nodes are sampled randomly from the graph.\nPrompts. For both the prompts for sampling the candidate plans and self-verifying them, we randomly generate three similar graphs as few-shot examples (listing 1, listing 2). With directed graph, we also prompt the model to re-order the flipped edges such that the graph still starts with \"Node 0 points to ...\" instead of \"Node 7, 9 points to 0\", if the original problem has \"Node 0 points to 7, 9.\" (listing 3), which we find improve the planning success with the flipped problem. listing 4 shows the prompt used for eliciting LLM's preference over the planning directions in zero-shot. We use temperature T = 0 for re-ordering, self-verifying, eliciting preference, and the first planning attempt, and T = 0.5 for the rest of planning attempts."}, {"title": "A.2 ARRAY TRANSFORMATION", "content": "Configurations. There are six possible functions used: repeat, cut, shift_left, shift_right, reverse, and swap. Depending on the set of functions used (e.g., {repeat, cut, shift_left, shift_right}), we first sample an random array of size 4 as the initial array, sample a random set of three functions, and then apply these functions to the initial array to get the goal one \u2013 if cut is sampled, we then first invert all the functions, reverse the order, apply the functions, and then reverse the initial and goal arrays. We limit that repeat can appear only once among the three to ensure the goal array is not too long.\nPrompts. The prompts used for sampling candidate solutions and self-verifying them are shown in listing 5, listing 6, and listing 7. listing 8 shows the prompt used for eliciting LLM's preference"}, {"title": "A.3 BLOCKSWORLD", "content": "Configurations. We use the problems from the task_1_plan_generation task (validity) in the PlanBench benchmark without modifying them.\nPrompts. listing 9 shows the original prompt from the benchmark. However, we find LLM often struggle to plan by following the examples in the original prompt, often mistaking the correct initial state. Instead, we use a two-step approach where LLM first summarizes the initial conditions into short format (e.g., \"yellow on red; blue; orange\") (listing 10), and then plan - during planning, LLM generates intermediate states in short form to help it reason the next steps (listing 11). listing 12 shows the prompt used for eliciting LLM's preference over the planning directions in zero-shot. We use temperature T = 0 for self-verifying, eliciting preference, and the first planning attempt, and T = 0.4 for the rest of planning attempts."}, {"title": "B APPENDIX - ADDITIONAL RESULTS", "content": ""}]}