{"title": "EMBRACING DIALECTIC INTERSUBJECTIVITY: COORDINATION\nOF DIFFERENT PERSPECTIVES IN CONTENT ANALYSIS WITH\nLLM PERSONA SIMULATION", "authors": ["Taewoo Kang", "Kjerstin Thorson", "Tai-Quan Peng", "Dan Hiaeshutter-Rice", "Sanguk Lee", "Stuart Soroka"], "abstract": "This study attempts to advancing content analysis methodology from consensus-oriented to\ncoordination-oriented practices, thereby embracing diverse coding outputs and exploring the dynam-\nics among differential perspectives. As an exploratory investigation of this approach, we evaluate\nsix GPT-40 configurations to analyze sentiment in Fox News and MSNBC transcripts on Biden and\nTrump during the 2020 U.S. presidential campaign, examining patterns across these models. By\nassessing each model's alignment with ideological perspectives, we explore how partisan selective\nprocessing could be identified in LLM-Assisted Content Analysis (LACA). Findings reveal that parti-\nsan persona LLMs exhibit stronger ideological biases when processing politically congruent content.\nAdditionally, intercoder reliability is higher among same-partisan personas compared to cross-partisan\npairs. This approach enhances the nuanced understanding of LLM outputs and advances the integrity\nof AI-driven social science research, enabling simulations of real-world implications.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs), such as ChatGPT, allow researchers to analyze extensive text corpora with efficiency\n(Bail, 2024). However, human and algorithmic biases can influence their outputs, reflecting ideological or cultural\nskewness in the training data (Bender et al., 2021; Kroon et al., 2023). With the growing adoption of LLM-Assisted\nContent Analysis (LACA)\u2014a method replacing manual coding with LLM-generated datasets (Chew et al., 2023)\u2014the\nscientific community must ensure these tools enhance understanding rather than reinforce existing biases (Messeri and\nCrockett, 2024).\nHuman bias is a long-standing issue in content analysis, which traditionally depends on a consensus-oriented research\npractice. Intercoder agreement is used to ensure reliability, aiming for shared interpretations of text that can be\nstatistically validated (Krippendorff, 1999). However, this emphasis on consensus may inadvertently neglect diverse\nperspectives, favoring uniform interpretations that risk simplifying the sociocultural complexities embedded in textual\nconnotations. This is particularly true in studies focused on ideologically polarized content, as social groups with"}, {"title": "Establishing Reliability in Content Analysis: Pros and Cons", "content": "Content analysis is \u201cthe backbone of communication research\u201d (Krippendorff, 1999). Identifying the properties of\nmessages is crucial for examining their effects. As a methodology for capturing the message states, replicability is\nthe scientific underpinning of content analysis, with the reliability of measurement determining the overall quality of\nthe research (Kolbe and Burnett, 1991). However, while reliability is necessary, it is not sufficient (Neuendorf, 2011).\nKrippendorff (1999) clarifies in his textbook of content analysis that while reliability ensures a study is replicable, it\ndoes not necessarily ensure that the study accurately reflects reality: \u201c[V]alidity concerns truths,\u201d whereas \u201creliability\ndoes not guarantee validity\u201d (pp. 212-213, italic in original). This common issue in any scientific research is particularly\nrelevant in content analysis when determining which aspects of a message are being measured.\nHistorically, systematic content analysis, developed after World War II, emerged from a need to understand how\nmessages from key press or political elites influence the masses (Berelson, 1952). In this context, content analysis has\nevolved to primarily seek to grasp a common ground of a text, which, in turn, \u201cprivileges content analysts' accounts\nover the readings by audience members\u201d (Krippendorff, 1999, p. 20). This approach is consistent with the traditional\ndefinition of content analysis, which involves drawing (statistical) \u201cinferences about the states or properties of the\nsources of the analyzed texts", "[r]eading is fundamentally a qualitative\nprocess\" (pp. 19-20). He further argues that \u201cif content analysts were not allowed to read texts in ways that differ from\nother readers, content analysis would be pointless\" (p. 23). This underscores the idea that texts often contain multiple\ndimensions, making it unlikely for a single reliable measure to capture their full complexity. Even with reliability and\nvalidity, such measures often reflect only one aspect of a text, potentially overlooking other latent dimensions.\nFor instance, content analysis research shows how media portray politicians differently depending on the ideological\norientation of the outlet. Conservative media tend to present conservative candidates in a positive light and liberal\ncandidates negatively, whereas liberal media typically do the reverse (Benoit, 2014; D'Alessio and Allen, 2000;\nNeuendorf, 2004; Schiffer, 2006). While a content analysis of such media biases offers a reliable inference about\nhow news producers encode specific properties (i.e., news framing), it does not necessarily provide an equitable\nunderstanding of how audiences decode these properties. Conservative or liberal voters may interpret political framing\"\n    },\n    {\n      \"title\": \"Persistent Indirectness of Validity in Content Analysis\",\n      \"content\": \"The multifaceted nature of communication processes prevents \u201cformal assessment of the validity of content analysis\nmeasures": "Neuendorf, 2011). It is unlikely that a content analyst can directly validate how communicators have\nencoded or decoded specific properties within a text (Holsti, 1969). Any validity measures thus are indirect and often\ntend to converge on face validity (Janis, 1965; see Krippendorff, 1980 for a review).\nScholars are aware of these challenges, yet it is uncommon to find methodological solutions aimed at addressing them\n(Matthes and Kohring, 2008). Instead, it is often recommended that the findings of content analysis be regarded as\npreliminary either serving as a groundwork for further research or complementing other methodologies. As such,\nKrippendorff (1999) advocates for an ethnographic approach, which connects content analysis with direct research on\ncommunicators (see also Altheide, 1987). Similarly, Neuendorf (2004) distinguishes content analysis from discourse\nanalysis, which emphasizes reflexivity, cognitive processes, and critical reflection, noting that the former focuses on\nproduction, outputs, and broad generalizations\u2014an approach described as industrial. This distinction often serves as a\nrationale for the necessity of subsequent discourse analysis, as content analysis tends to stimulate qualitative inquiry.\nConsequently, challenges in establishing validity often led to an emphasis on directly quantifiable reliability measures\nas the primary criterion for evaluating content analysis. However, as noted, the reliability of a measure for capturing\none facet does not guarantee its applicability to other aspects. This is evident in the recommendation that researchers\nvalidate content analysis within relatively homogeneous populations, thereby acknowledging the constrained empirical\ndomain of common ground (Krippendorff, 1980; Neuendorf, 2011)."}, {"title": "Intersubjectivity as Agreement", "content": "To capture the heterogeneity of a text's connotation, each latent dimension requires the identification of its own\nhomogeneous common ground. Although this conclusion is logical, the field of content analysis has struggled to\nimplement it due to its sole conceptualization of intersubjectivity. In content analysis research, intersubjectivity\ngenerally refers to the shared understanding throughout the research process (Neuendorf, 2004). It is positioned as a\nsubstitute for the unattainable ideal of absolute objectivity, aiming to establish a consensus among researchers (Babbie,\n2015). As a result, intercoder agreement is suggested as the mutual understanding of text properties that coders or raters\ncommonly identify, even while relying on their own mental schemas (Potter and Levine-Donnerstein, 1999).\nTaken for granted, there is nothing wrong with conceptualizing intersubjectivity as \u201csharing\u201d or \u201chaving in common\"\n(Matusov, 1996, p. 26). However, this conceptualization restricts content analysis to being a type of consensus-oriented\nactivity, treating disagreements as nuisances that should be eliminated. If separate analyses do not show increased\noverlap, it is seen as evidence of a failure in intersubjectivity G\u00f6nc\u00fc (1993), leading the research to focus on the process\nby which coders' subjectivities are unified Smolka et al. (1995). Yet, the benefits of unification come with hidden costs:\nexcluding not shared perspectives.\nThe meanings invoked by texts need not be shared. Although intersubjective agreement as to what an author meant to\nsay or what a given text means would simplify a content analysis tremendously, such consensus rarely exists in fact.\nDemanding that analysts find a \u201ccommon ground\u201d would restrict the empirical domain of content analysis to the most\ntrivial or \u201cmanifest aspects of communications,\u201d on which Berelson's definition relies, or it would restrict the use of\ncontent analysis to a small community of message producers, recipients, and analysts who happen to see the world from\nthe same perspective. (Krippendorff, 1999, p. 23)\nIn this regard, we propose that the evolution of content analysis beyond Berelson's paradigm requires a confrontation\nwith the validity challenge-particularly around notions of \"common ground\" that, while recognized, remain unresolved.\nBy reconceptualizing the inertia surrounding intersubjectivity, may we establish a basis for the advancement of content\nanalysis.\""}, {"title": "Intersubjectivity with and without Agreement", "content": "The traditional concept of intersubjectivity, defined as a state of shared individual understandings (Rommetveit, 1979),\nhas long faced criticism; Instead, Matusov (1996), in a pivotal study, proposes re-envisioning intersubjectivity as a"}, {"title": "Application of Dialectic Intersubjectivity in Content Analysis", "content": "From Gillespie and Cornish (2010)'s approach, traditional content analysis centers on capturing first-level intersubjec-\ntivity through a consensus-oriented approach. Coders report their interpretations of a text based on researcher-provided\nguidelines. If significant disagreement emerges to a degree that threatens the measurement reliability (i.e., unacceptable\nintercoder agreement), researchers then systematically engage with second-level intersubjectivity. Here, although coders\nmay not directly know how their direct perspectives converge or diverge from others, the researcher performs a kind of\niterative meta-analysis to mediate understanding and misunderstanding among them (Riffe et al., 2023).\nMethodologically, traditional content analysis aims to enhance mutual understanding and reduce misunderstanding\nthrough refining coding instructions, coder retraining, and mediated discussions (Lombard et al., 2002; Neuendorf,\n2004, 2011, 2017). Therefore, after resolving non-consensus, the third-level intersubjective communication of realizing\nboth understanding and misunderstanding should not occur (Gillespie and Cornish, 2010). Of course, such an\nagreement-seeking approach is indeed effective in capturing properties of texts accepted within consensus-oriented\ndisciplines (Krippendorff, 1999), yet this imposes a paradox on analysts, requiring them to \u201chave their own subjective\ninterpretations", "read between the\nlines\" (Berelson, 1952, p. 18).\nConfronting this paradox, the dialectical approach diverges from traditional methods in the researcher's communicative\nrole at the second-level and in the presence of third-level communication. At the second-level, disagreement serves\nnot merely as a source of misunderstanding, but as a tentative signal that various interpretations of the text might\nexist. Instead of treating unalignment as marginal errors failing to integrate into intersubjectivity, the researcher\nviews these as potential generators of heterogeneous meanings. If misunderstandings are, in fact, identifiable as\ndistinct interpretations\u2014where they are reliably captured, respectively\u2014then the researcher may proceed to assess\nintersubjectivity at the third-level.\nAt the third-level, the researcher identifies latent patterns among those perspectives. One homogeneous perspective\nsharing a common understanding may establish specific relations with another distinct perspective. These relations\nmight be close vs. distant, similar vs. opposite, or reconciling or conflicting (Lotman, 1988) and so on. By identifying\nthose dialectic relations, the researcher coordinates individual perspectives while collectively contributing to the text's\nheterogeneous meanings. This coordination is mediated through the researcher's dialogical analysis, which posits that\na text can radiate multifaceted meanings within a sociocultural context that transcends the immediate situation of its\ngeneration. In essence, the researcher seeks to understand the meaning(s) of the text, not as independent entities, but as\ninterdependent relationships across various communities.\"\n    },\n    {\n      \"title\": \"Necessity of Dialectic Intersubjectivity in LLM-Assisted Content Analysis (LACA)\",\n      \"content\": \"We propose that embracing dialectical intersubjectivity is particularly relevant when using Large Language Models\n(LLMs) as instruments for content analysis. In LLM-based text analysis, the evaluation of coding performance typically\nrelies on human-annotated datasets. Human annotations, regardless of their proliferation under terms such as \u201cground\ntruth,\" \\\"golden standard,": "r \u201cbenchmark\u201d (Heseltine and Clemm von Hohenberg, 2024; Wen and Younes, 2024),\noften serve as an ultimate standard against which the utility of LLMs as tools for text analysis is judged. For instance,\nGilardi et al. (2023) claim that ChatGPT classifications exhibit higher intercoder reliability with labels from human\ncoders than those generated by two Mturk crowd workers, suggesting that \u201cChatGPT outperforms crowd workers for\ntext-annotation tasks.\u201d The underlying issue, however, is that this ground truth, derived from \u201ctwo research assistants,\u201d\nis neither unbiased nor error-free (Platt and Platt, 2023).\nIronically, this problem is compounded by the touted advantages of LLMs as a scientific tool: low cost, time efficiency,\nand scalability. If a single ground truth reflects only a specific, potentially biased perspective, then LACA's scientific\napplications risk rapidly replicating these biases at scale (i.e., \u201creliably wrong"}, {"title": "Algorithmic Bias in LACA", "content": "To this end, LACA must focus on two primary goals: (1) identifying perspectives an LLM tends to represent (e.g.,\nSanturkar et al., 2024) and (2) assessing its ability to reflect other perspectives (e.g., Argyle et al., 2023). These priorities\nalign with Bail (2024)'s review of two major pathways for generative AI's use in social sciences.\nFirst, social scientists are developing frameworks for LLM-based textual analysis, based on the idea that these models\ncould potentially mimic human intellectual processing Hancock et al. (2020); Mitchell (2019). This approach aims\nto harness LLMs in analyzing complex text (e.g., Chew et al., 2023; Heseltine and Clemm von Hohenberg, 2024;\nT\u00f6rnberg, 2023; Ziems et al., 2024). The second pathway involves using LLMs to support simulation-based research.\nHere, social scientists employ the agent-based modeling (ABM) paradigm, creating synthetic societies to observe\ninteractions among simulated LLM personas (Bail, 2024). Studies in this area explore the potential of these models to\npredict real-world dynamics (e.g., Park et al., 2023).\nBoth approaches emphasize the potential for automated social science (Demszky et al., 2023; Manning et al., 2024;\nStokel-Walker and Van Noorden, 2023), while also recognizing algorithmic biases as a critical limitation. If algorithmic\nbias is viewed as an inherent cost of automated social science, then a key task becomes identifying which social\ngroups' standpoints an LLM may overrepresent and determining the extent to which this tendency can be adjusted to\navoid misrepresenting other groups. Accordingly, for our purposes, it is crucial to examine how LLMs reflect specific\nperspectives and the extent to which they can simulate a diversity of standpoints across relevant social groups.\nA prerequisite for these efforts is the conceptualization of algorithmic bias. Argyle et al. (2023) argue that LLMs exhibit\nmultiple-interwoven biases rather than a single overarching bias, describing it as \u201ca complex reflection of the many\nvarious patterns of association between ideas, attitudes, and contexts present among humans,\u201d rather than \u201ca singular,\nmacro-level feature of the model.\u201d Their analysis shows that when GPT-3 is prompted with specific demographic\npersonas based on American National Election Study (ANES) data, its responses often mirror views typical of them, a\nphenomenon they term \u201calgorithmic fidelity\"-the degree to which LLMs mirror human ideas and attitudes across\nsocial contexts. They suggest that low algorithmic fidelity risks misrepresenting certain subgroups (pp. 337-338).\nSimilarly, Santurkar et al. (2024) find that conditioning LLMs on demographic profiles from Pew Research's American\nTrends Panel (ATP) data makes responses reflect the actual opinions of these profiled groups. Their findings also show\nthat default LLM responses tend to align more with liberal views, consistent with prior studies (e.g., Hartmann et al.,\n2023). Collectively, these studies envision integrating the two primary strands of LLM application in social scientific\nresearch, enabling us to observe how text-analyses might converge or diverge when LLMs simulate personas from\ndifferent sociocultural groups (see Giorgi et al., 2024).\""}, {"title": "Sentiment Analysis of Partisan Media with Persona-based LLMS", "content": "In this exploratory study, we explore how partisan dispositions may shape interpretations of partisan texts and contexts;\nthat is, we attempt to analyze not only how political content is encoded, but also how it is differentially decoded based\non divergent partisan standpoints (see Hall, 1980), and ultimately, how these partisan standpoints are situated within"}, {"title": "Data and Methods", "content": ""}, {"title": "Textual Data", "content": "We utilized television transcripts from the (now deprecated) LexisNexis Web Services Kit. These are full transcripts for\nshows airing on Fox News and MSNBC. Because these networks do not use traditional evening news broadcasts in the\nway that ABC or CBS might, the corpus is all content from these networks that are regular and recurring shows. These\ndata are described in more detail in (The author's citation is masked).\nFox News and MSNBC were selected as focal points for this analysis due to their well-documented contrast in political\nbias along the left-right spectrum (Media, 2024) and because the ANES 2020-2022 survey includes exclusive self-\nreported trust measures for these two networks. Since the data were used to fine-tune the default model of GPT-40,\nanalyzing content from these outlets aligns closely with the goal of enhancing algorithmic fidelity by reflecting political\nattitudes and opinions across U.S. demographics.\nFox News transcripts were available by broadcast unit, totaling 115,712 cases from 1998 to 2021, with an average\nword count of 2,798.7. During the campaign period from June to October 2020, 1,040 transcripts referenced both \"Joe\nBiden\" and \"Donald Trump,\u201d with an average word count of 7,294.4; MSNBC transcripts included 34,586 cases from\n1999 to 2021, with an average word count of 7,408.8. Of these, 795 episodes from June to October 2020 referenced\nboth candidates, averaging 8,103.4 words. Because LACA outcomes tend to vary with word count (Heseltine and\nClemm von Hohenberg, 2024), transcripts with word counts between 7,500 and 8,000 were sampled, spanning the total\naverage of 7,698.9 words across both sources. This yielded 242 MSNBC and 259 Fox News transcripts within the\ndesired range. To equalize sample sizes, a random subsample of 242 cases was drawn from the Fox News data."}, {"title": "Sentiment Analysis for Candidate Portrayals", "content": ""}, {"title": "Zero-shot Models", "content": "Using the OpenAI API, we first assigned the role of assistant to the default zero-shot model (DZ) and prompted it to\nperform numeric sentiment coding for both Joe Biden and Donald Trump, with scores ranging from -2 for very negative,\n-1 for negative, 0 for neutral, 1 for positive, to 2 for very positive. The responses were requested in numeric form only.\nGiven the relatively high word count per case, the maximum tokens per response were limited to 10, and each transcript\nwas split into chunks of 2,000 tokens. To minimize randomness in the results, the temperature setting was fixed at 0\n(Santurkar et al., 2024; T\u00f6rnberg, 2023). The same process was replicated using the fine-tuned zero-shot model (FZ)."}, {"title": "Persona-based Models", "content": "Along with the same prompt and temperature setting, two partisan persona simulations were applied to both the default\nand fine-tuned models (DD, DR, FD, and FR). These personas were crafted by combining demographic attributes to\nreflect typical partisan profiles based on Pew Research's recent reports (Center, 2024a,b). For Democrat persona, the\nprompt described: \u201ca U.S. citizen who is a woman in her 20s, Black, with a college degree, Democrat, and middle\nincome,", "described": "a U.S. citizen who is a man in his 50s", "income.": "ach model was then prompted to analyze sentiment in the texts for\neach candidate based on these simulated personas (see Supplemental Materials for code and data)."}, {"title": "Results", "content": "With respect to RQ1, which investigates how the six models evaluate sentiment toward Biden and Trump, our LACA\nsurfaced the typical partisan biases of the two media outlets. Overall, all six models suggested that Fox News presented\nBiden in a negative light and Trump more favorably, whereas MSNBC exhibited the reverse pattern. Yet, the\nDemocrat persona models (DD and FD) outputted negative portrayals of Trump on Fox News as well. Conversely,\nthe Republican persona fine-tuned model (FR) assessed Trump as being depicted even more favorably than Biden in\nMSNBC transcripts.\nOur analysis also explored the sentiment contrast between Biden and Trump across different models. We computed\nthis by subtracting Trump's sentiment score from Biden's, yielding a range from -4 to 4. Higher positive values\nsignify a stronger positivity toward Biden over Trump, while lower negative values indicate a stronger negativity\ntoward Biden compared to Trump. The one-way ANOVA results indicate that sentiment contrast differs significantly\nacross models (for Fox News: F(5, 1446) = 120.72, p < .001, \u03b7\u00b2 = 0.294, indicating a large effect; for MSNBC:\nF(5, 1446) = 70.45, p < .001, \u03b7\u00b2 = 0.197, indicating a moderate effect) (see Table S1 for detailed post-hoc Tukey's\nHSD analyses).\nTo address RQ2 of how agreement level differs among LLMs, intercoder reliability across fifteen pairs from six models\nwas calculated using a random sample of 224 chunk responses (112 for each Fox News and MSNBC sample). This\nsampling approach aligns with recommended practices for large datasets, ensuring a representative assessment of coder\nconsistency without requiring exhaustive computation of the entire dataset (Krippendorff, 2018; Lombard et al., 2002).\nAs shown in Table 2, intercoder reliability metrics of Krippendorff's \u03b1 vary by how models are paired. The highest\nagreement was observed between default zero-shot and fine-tuned zero-shot models (\u03b1 = .86), followed by the pair of\nDemocrat persona default and Democrat persona fine-tuned models (\u03b1 = .85). Notably, the pair of default zero-shot\nand Democrat persona default models showed acceptable intercoder reliability (\u03b1 = .73). This was even higher than\nthe intercoder reliability from the pair of Republican persona default and Republican fine-tuned models, while this"}, {"title": "Discussion", "content": "The utility of generative AI in social sciences is under scrutiny (Bail, 2024), with concerns around potential biases\nbecoming pronounced as applications like LLM-Assisted Content Analysis (LACA) expand. These biases pose\na challenge to the \"ground truth\" of human annotations, a persistent issue in traditional content analysis; that is,\nhuman biases could be amplified in LACA, further obscuring the complex meanings embedded in texts. To address\nthese challenges, this study advocates for a conceptual shift from a consensus-oriented to a coordination-oriented\nintersubjectivity framework. Building on Matusov's (1996) concept of dialectic intersubjectivity, we employ Gillespie\nand Cornish's (2010) three-level model, which reframes intersubjectivity beyond mere consensus to include divergent\nperspectives. This model allows researchers to account for both agreements and productive disagreements, thereby\ncapturing sociocultural nuances within a text.\nOur findings underscore how dialectic intersubjectivity can reliably diversify LACA outputs, particularly when\nexamining sentiment toward politically charged figures across partisan communities. First, sentiment analyses differ\nacross LLM configurations, with within-community pairs (i.e., same partisan persona groupings) exhibiting higher\nintercoder reliability than between-community pairs. This suggests that partisan texts possess layers of meaning that"}, {"title": "Supplemental Material", "content": "Our code and data are available at https://github.com/casllmproject/dialectic_intersubjectivity.\nA. Measurement scales for fine-tuning training data from the 2020-2022 ANES Social Media Study (American\nNational Election Studies, 2023)\nDemographics\nGender ('What is your gender?'):\n\u25cb 0: Unknown\n\u2022 1: Male\n\u2022 2: Female\nAge (\"What is your age?\"):\n\u2022 (numeric input)\nRace/Ethnicity (\u201cWhat is your race/ethnicity?\"):\n\u25cb 1: White, non-Hispanic\n\u2022 2: Black, non-Hispanic\n\u2022 3: Other, non-Hispanic (includes Asian, non-Hispanic)\n\u2022 4: Hispanic\nEducation Level (\"What is your highest level of education?\"):\n\u2022 1: Less than high school\n\u2022 2: High school graduate or equivalent\n\u25cb 3: Vocational/tech school/some college/associates\n\u2022 4: Bachelor's degree\n\u2022 5: Postgraduate study/professional degree\nIncome Level (\"What is your income level?\"):\n\u2022 1: Less than $5,000\n2: $5,000 to $9,999\n3: $10,000 to $14,999\n\u2022 4: $15,000 to $19,999\n5: $20,000 to $24,999\n\u2022 6: $25,000 to $29,999\n07: $30,000 to $34,999\n\u2022 8: $35,000 to $39,999\n09: $40,000 to $49,999\n\u25cb 10: $50,000 to $59,999\n\u25cb 11: $60,000 to $74,999\n\u2022 12: $75,000 to $84,999\n\u2022 13: $85,000 to $99,999\n\u25cb 14: $100,000 to $124,999\n\u2022 15: $125,000 to $149,999\n\u2022 16: $150,000 to $174,999\n\u2022 17: $175,000 to $199,999\n\u2022 18: $200,000 or more\nPolitical Ideology\nSelf-Identified Political Ideology (\"When it comes to politics, would you describe yourself as liberal, conser-\nvative, or neither liberal nor conservative?\"):\n0-7: No answer\n\u2022 -6: Unit non-response"}, {"title": "Fine-tuning Processes with ANES 2020-2022", "content": "The training data for fine-tuning the gpt-40-2024-08-06 base model consisted of the ANES 2020-2022 panel survey\n(n=5,750). The ten variables used for training included: gender, age, race/ethnicity, education level, income, self-\nidentified political ideology, party-specific political ideology (Democratic and Republican parties), and trust in media\n(Fox News and MSNBC) (ANES, 2023). On September 18, 2024, the training data were uploaded and validated\naccording to OpenAI's fine-tuning protocol (OpenAI, n.d.). For the fine-tuning, the model was set with three epochs\nand a batch size of 11. Responses across the ten ANES variables totaled 3,374,289 tokens for the 5,750 respondent\nsamples. The training process took approximately 2 hours and 58 minutes."}, {"title": "Sentiment contrast analysis", "content": "Post-hoc Tukey's HSD analyses show that persona prompting significantly intensifies partisan bias for both outlets.\nHowever, this increase in bias does not vary significantly between default and fine-tuned models. Notably, the Fox News\nanalysis showed no significant difference in sentiment contrast between the default and fine-tuned models. Similarly,\nfor MSNBC, the default model's sentiment contrast did not differ significantly from the fine-tuned zero-shot model\nor from either the default or fine-tuned Democrat persona models. In other words, the overall polarity of sentiment\nanalysis for Biden and Trump remains consistent across these Default-to-Democrat model variations."}]}