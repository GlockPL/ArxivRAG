{"title": "CAPEEN: Image Captioning with Early Exits and Knowledge Distillation", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "abstract": "Deep neural networks (DNNs) have made significant progress in recognizing visual el-ements and generating descriptive text in image-captioning tasks. However, their im-proved performance comes from increased computational burden and inference latency. Early Exit (EE) strategies can be used to en-hance their efficiency, but their adaptation presents challenges in image captioning as it requires varying levels of semantic informa-tion for accurate predictions. To overcome this, we introduce CAPEEN to improve the performance of EE strategies using knowl-edge distillation. Inference in CAPEEN is completed at intermediary layers if predic-tion confidence exceeds a predefined value learned from the training data. To account for real-world deployments, where target dis-tributions could drift from that of training sam-ples, we introduce a variant A-CAPEEN to adapt the thresholds on the fly using Multi-armed bandits framework. Experiments on the MS COCO and Flickr30k datasets show that CAPEEN gains speedup of 1.77\u00d7 while maintaining competitive performance com-pared to the final layer, and A-CAPEEN ad-ditionally offers robustness against distortions. The source code is available at https:// github.com/Div290/CapEEN", "sections": [{"title": "Introduction", "content": "Image captioning, a multifaceted challenge at the intersection of computer vision and natural lan-guage processing, has reaped the benefits of deep neural networks (DNNs), characterized by their increased scale and complexity. This task en-tails not only the identification of visual elements within an image but also the intricate interpreta-tion of their relationships. Notably, the encoder-decoder framework has made significant strides in sentence generation by anticipating the next word in a sequence (Anderson et al., 2018; Chen et al., 2021; Chen and Lawrence Zitnick, 2015; Fei, 2021; Huang et al., 2019; Vinyals et al., 2015; Xu et al., 2021, 2015; Yao et al., 2018; Zhang et al., 2021; Li et al., 2022, 2023). This predictive modeling considers both the image's content and the preceding partial sentence, resulting in sub-stantial progress in the field (Bai and An, 2018). However, their large size restricts deployment in resource-constrained scenarios requiring fast in-ference. Early Exit (EE) strategies have emerged as a strategic solution to overcome this challenge.\nIn EE strategies, classifiers are attached to the intermediary layers. Each sample can exit from one of them without requiring to pass through all layers (see Fig. 1). This brings down computa-tional requirements and improves latency (Teer-apittayanon et al., 2016; Xin et al., 2020). How-ever, this generic approach of attaching exits to the pre-trained backbone may not be suitable for im-age captioning (Fei et al., 2022)- in the layered hierarchy of representation in transformer-based models, the initial layers focus on extracting low-level features, while deeper layers delve into the complexities of semantic fusion relations (Cornia et al., 2020; Liu et al., 2021c). Consequently, even 'easy samples' require a certain level of high-level information present at deeper layers.\nMoreover, the decisions of early exit are based on the confidence at the intermediary layers being above a predefined threshold. The threshold used to compare the confidence levels significantly im-pacts the amount of latency and accuracy. These thresholds are learned during training and serve as a crucial reference point during inference.\nPost-deployment, it may be possible that the distribution of the target sample could drift away from that of training samples. This is often en-countered in real-world scenarios, e.g., blurred images due to the camera being out of focus (Dodge and Karam, 2016) during inference. Such drifts could affect the threshold choice and signifi-cantly lower the DNNs performance (see figure 2) prompting the question: How to adjust the thresh-old of deployed pre-trained models when the la-tent distribution of target samples differs from the training samples due to variation in distortion lev-els? Also, this adaptation has to be unsupervised, as the ground truth labels may not be available during inference. This motivates a method that 1) gives initial layers high-level information during training and 2) adjusts early exit thresholds dur-ing inference for efficiency and robustness against distortions.\nWe introduce a new approach that extends the idea of knowledge distillation in early ex-its (Phuong and Lampert, 2019; Zhu, 2021) to image captioning tasks named Image Captioning with Early Exits and Knowledge Distillation (CAPEEN) to improve the efficiency of EEs in image captioning tasks. By distilling the knowl-edge residing in deeper layers (teacher), CAPEEN empowers initial classifiers (students) to utilize the richness of deeper representations, which en-hances both the performance and speed of the EE model.\nTo circumvent the issue of threshold choice un-der distribution change due to distortion in in-coming samples during inference, we propose a novel online learning algorithm A-CAPEEN based on the Multi-Armed Bandits (MAB) frame-work (Auer et al., 2002) to learn the optimal threshold as per latent distributions of input sam-ples. A-CAPEEN is activated during the inference phase and adapts to various levels of distortions in test samples with minimal computational require-ments, making our method more suitable for real-world scenarios.\nOur experiments on the MS COCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015) dataset demonstrate that CAPEEN significantly increases speedup while maintaining competitive accuracy performance (1.77x speedup in runtime as compared to the final layer). A-CAPEEN fur-ther enhances efficiency by dynamically adjusting exit thresholds based on the latent distribution of the test dataset to make it robust to noise present in the datasets during inference.\nIn summary, our contributions are as follows: 1) We present a novel self-distillation framework named CAPEEN tailored for early exiting in im-age captioning. 2) We introduce an online learning algorithm, A-CAPEEN, designed to dynamically choose optimal thresholds as per the latent data distribution utilizing the MAB framework. The algorithm uses the confidence scores to learn the optimal threshold. It makes the method robust to different levels of distortion in the test sam-ples. 3) Comprehensive experiments conducted on the MS-COCO and Flickr30k dataset, reveal the superior performance of CAPEEN across all key metrics compared to previous methodolo-gies. Additionally, we validate the effectiveness of A-CAPEEN in consistently determining opti-mal threshold values for various data distributions."}, {"title": "Related works", "content": "Image Captioning: Recent research has wit-nessed a surge in exploring efficient text descrip-tion generation for input images (Bai and An, 2018). Encoder-decoder frameworks have gained prominence for their exceptional performance in text generation tasks, leveraging contextual infor-mation (Anderson et al., 2018; Chen et al., 2021; Chen and Lawrence Zitnick, 2015; Fei, 2021; Huang et al., 2019; Vinyals et al., 2015; Xu et al., 2021, 2015; Yao et al., 2018; Zhang et al., 2021; Li et al., 2022, 2023). With the advent of the atten-tion mechanism (Vaswani et al., 2017), the focus has shifted towards employing multiple layers of transformers as both encoders and decoders.\nEarly-exits. In recent years, the issue of infer-ence latency has gained substantial attention (Mat-subara et al., 2022; Guo et al., 2020). To ad-dress this issue, DNNs are implemented with in-ternal classifiers in the intermediate layers. No-tably, BranchyNet (Teerapittayanon et al., 2016) explores early classification at intermediate layers for images, while SPINN (Laskaridis et al., 2020) uses a mobile cloud setup to split the DNN. SEE (Wang et al., 2019b) performs the early exiting in a service outage scenario. Multiple early ex-iting frameworks have also been proposed such as (Huang et al., 2017; Yang et al., 2020; Han et al., 2023) for improving early exits for image tasks dynamically choosing the depth of network for different regions for an image. Other works like (Phuong and Lampert, 2019) have utilized knowl-edge distillation in early exit framework but not for image captioning. Also, it performs joint training of teacher and student that deteriorates the opti-mality of the backbone.\nNumerous early exiting frameworks have been devised for natural language processing tasks (Xin et al., 2020; Liu et al., 2021b, 2020; Wang et al., 2019a; Li et al., 2021; Zhou et al., 2020; Zhu, 2021; Ji et al., 2023; Balagansky and Gavrilov, 2022; Zhang et al., 2022), primarily based on the BERT backbone. DeeCap (Fei et al., 2022) in-troduces early exiting to image captioning, em-ploying an imitation network to replicate outputs from computationally intensive transformer layers within an encoder-decoder architecture. Similarly, MuE (Tang et al., 2023) applies early exits to OFA (Wang et al., 2022a), a unified vision language model designed for multi-modal applications.\nMulti-armed bandits in Early exits. Several works utilize the MAB framework to adapt to dif-ferent scenarios. Notable methods like LEE (Ju et al., 2021b), DEE (Ju et al., 2021a), and AdaEE (Pacheco et al., 2023) aim to learn the optimal exit points in scenarios like mobile devices with restricted computational resources. Additionally, EPNet (Dai et al., 2020) adopts an offline approach to learning when to exit based on considerations of computational overhead and accuracy. On the other hand, UEEUCB (Hanawal et al., 2022) em-ploys a Multi-Armed Bandit (MAB) framework to dynamically learn the optimal exit strategy in an online and unsupervised manner. UEEUCB relies on the assumption of strong dominance in neu-ral networks, wherein accuracy increases with the layer number in the neural network.\nThe key differences are: 1) to the best of our knowledge, improving the performance of early exits using knowledge distillation has not been studied for image captioning. 2) Our online al-gorithm based on the MAB setup overcomes the challenge of choosing the optimal threshold by adapting to the underlying latent distributions of the noise present in the test dataset.\nNote: We are the first to apply knowledge dis-tillation for image captioning. Previous methods have applied knowledge distillation for early exits in text and image classification which are much simpler tasks than image captioning and they simultaneously perform distillation in a single-stage training. This cannot be extended to image captioning as it requires high-quality knowledge transfer else it can lead to incorrect learning paths.\nGiven the complexity of image captioning, we perform a two-stage training that not only transfers high-quality knowledge but also maintains the op-timality of the backbone. This approach provides us with a state-of-the-art backbone and serves as a testbed for A-CAPEEN."}, {"title": "Methodology", "content": "In this section, we discuss our method of fusing knowledge distillation with early exit layers by treating the final layer classifier as the teacher and the early exit classifiers as the students."}, {"title": "Backbone", "content": "We use the encoder-decoder framework for build-ing our backbone network motivated by previous works (Liu et al., 2021a; Li et al., 2023). The encoder component comprises a pre-trained Swin-Transformer-base model (Liu et al., 2021c). What sets the Swin-Transformer backbone apart from other vision transformer models (Ranftl et al., 2021) is its Window and Shifted-Window Multi-head Self-Attention (SW-MSA) for the extraction of high-quality rich features. Swin's unique ap-proach has consistently delivered state-of-the-art performance in various vision-related tasks (Wang et al., 2022b). The encoder extracts rich features from the input image and enhances them by cap-turing their intra-relationships. The output of the Swin-Transformer encoder represents the image in a way that takes into account both local and global context. These features capture details, objects, and their spatial relationships within the image. On the other hand, the decoder component uses the pre-trained GPT-2 (Lagler et al., 2013) model to generate captions in an autoregressive manner, ef-fectively capturing the inter-relationships between words and image features."}, {"title": "Finetuning Backbone and Training Exits", "content": "CAPEEN requires two main training steps: (i) The backbone fine-tuning and (ii) Training of the at-tached exits using knowledge distillation."}, {"title": "CAPEEN backbone fine-tuning", "content": "We start with a pre-trained encoder and decoder. The grid features of the image from the encoder output are passed to the decoder for cross-attention computation. The encoder-decoder backbone is then updated using cross-entropy loss calculated between the predicted token $y_t$ and the ground-truth token $y_t^*$. The loss function for fine-tuning is formulated as:\n$L(\\theta; \\Theta) = - \\frac{1}{T} \\sum_{t=1}^T log P_N(y_t^* | y_{1:t-1}, I; \\Theta)$,\nwhere @ denotes the collection of all the param-eters, I denote the input image, T is the caption length, $Y_T$ denotes ground-truth caption, $P_N$ de-notes the probability score from the final layer, and N denotes the number of layers in the decoder. We define vocabulary V as the set of tokens. Once the fine-tuning is complete, we freeze all the backbone parameters. This maintains the optimal quality of the backbone after exits are attached."}, {"title": "CAPEEN Exits Training", "content": "After obtaining the fine-tuned backbone from the previous step, we attach task-specific exits at each decoder layer except the final layer. We use a student-teacher setup where the teacher is the final layer, and each intermediate classifier is treated as a student, as visualized in Fig. 3. The weights of the ith exit/student are trained using the loss:\n$L_i(\\Theta; \\Theta, \\Theta_e) = \\frac{1}{T} \\sum_{t=1}^T (log(P_i(y_t^* | y_{1:t-1}, I; \\Theta, \\Theta_e) + KL(p_i, p_T))$\nwhere $de$ are the learnable weights for the ex-its, $y_t^*$ is the tth ground-truth token. $p_i$ is the probability vector on the vocabulary for ith student. Its vth component is given by $p_i(v) = P_i(v | y_{1:t-1}, I; \\Theta, \\Theta_e)$ and $p_T$ is the probability vector of the teacher model where $p_T(v) = P_N(v | y_{1:t-1}, I; \\Theta)$. KL is the KL-Divergence function defined as $KL(p_i, p_T) = \\sum_{v \\in V} p_i(v) log(\\frac{p_i(v)}{p_T(v)})$. The early classifiers are then jointly trained using the loss function $\\sum_{i=1}^{N-1} L_i$. In this way, the learning is guided by hard and soft labels from the final layer."}, {"title": "CAPEEN inference", "content": "We predict the caption in an autoregressive man-ner. This entails making token-by-token predic-tions for a given image, where the layer at which a token is predicted is determined by the prediction confidence $C_i = max_{v \\in V} P_i(v | y_{1:t-1}, I; \\Theta, \\Theta_e)$. The input to the decoder is processed sequentially through the decoder layers until $C_i$ (the confidence value) is greater than a predefined threshold value a. This threshold is set using the validation data based on the required accuracy-efficiency trade-off. The pseudo-code for inference is given in Al-gorithm 1. In the algorithm < bos > and <eos > denote the beginning of sentence and end of sen-tence token, respectively. We rely on fixed con-fidence threshold values during inference that are tuned during training and applied uniformly across all exits to make decisions of early inference. We denote the prediction word by c and the predicted caption by C."}, {"title": "Learning of Thresholds", "content": "Recall the discussion from the introduction that a threshold set using the validation set may not re-sult in better performance when test data distribu-tion drifts from that of the train data distributions, especially when the images in the test data are dis-torted. As the threshold used in early exit deci-sions significantly impacts both computational re-quirements and accuracy, setting it appropriately as per the test data distribution is crucial for op-timal performance. We address this challenge by adjusting the threshold as per input data distribu-tion in an online fashion using the MAB frame-work (Auer et al., 2002).\nIn the MAB setup, a decision-maker repeatedly selects from a set of arms (or actions) while adapt-ing to the unknown environment. Each arm cor-responds to a specific choice or decision. The challenge lies in learning which arms yield the most favourable outcomes (highest reward) over time. This adaptation and learning process, cen-tral to MAB setups, aligns with the dynamic na-ture of online learning problems, where decisions are made sequentially based on incoming data. By leveraging the principles of exploration and exploitation, MAB frameworks facilitate learning the optimal action for the latent distribution.\nIn this context, we define the action set as k thresholds for each exit as $A = \\{a_1, a_2, ..., a_k\\}$. For exit i, we define the latency factor as the cost of processing the sample from the 1st exit to the ith exit and denote it as oi. Let [N] denote the set $\\{1, 2, . . ., N\\}$. A token will exit the backbone only if the confidence is above the chosen thresh-old. For a given threshold a where a \\in A, sup-pose that $C_j < a$ for $j \\in [i - 1]$ and $C_i >= a$ then it exits at ith exit, and the reward is defined as:\n$r(a) = (C_i - C_1) - \\mu o_i$ (1)\nwhere \u03bc is the scaling factor/conversion factor to bring the cost in terms of confidence. If the to-ken does not gain sufficient confidence till the fi-nal layer, then $C_j < a$ for all $j \\in [N \u2013 1]$, and the token will be inferred at the final layer. Then the reward is:\n$r(a) = (C_N - C_1) - \\mu o_N$ (2)\nThe reward could be interpreted as follows: if a sample exits at layer i, then the gain is the confi-dence gain in the inference at layer i compared to the inference at the first layer, and the cost incurred in processing the token from the first exit to the ith exit. The reward is the net gain, expressed as the difference between gain and cost. The objective is to maximize the expected reward as :\n$E[r(a)] = \\sum_{i=1}^{N-1} E[(C_i \u2013 C_1) \u2013 \\mu o_i | exit(i)] .P[exit(i)] + E[(C_N \u2013 C_1) \u2013 \\mu o_N | exit(N)] .P[exit(N)]$ (3)\nwhere exit(i) denotes the exit from ith layer. The objective is to find an action that maximizes the expected reward function. Note that the reward does not use any label information. The optimal arm is defined as $a^* = arg max_{a \\in A} r(a)$. If we consider a policy \u03c0 that selects threshold $a_t \\in A$ in round t based on past observations. The effi-ciency of the chosen policy can be expressed in terms of cumulative regret, defined as:\n$R(\\pi, T) = \\sum_{t=1}^T E[r(a^*) \u2013 r(a_t)]$ (4)"}, {"title": "Algorithm", "content": "We introduce an algorithm called Adaptive-CAPEEN (A-CAPEEN) to adaptively choose the threshold values, and its pseudocode is outlined in Algorithm 2. The input for this algorithm includes latency factors $o_i$ for each exit $i \\in [N]$, the ex-ploration parameter \u03b3. The expectation above is with respect to the randomness induced by latent sample distribution in the selection of actions. A policy denoted as $\u03c0^*$ is characterized as sub-linear when the average cumulative regret diminishes, that is, $R(\u03c0*, T)/T \u2192 0$. Our primary goal is to devise a learning algorithm that has a sub-linear regret guarantee.\nThe initialization of the algorithm involves en-suring that each action is played at least once. In the subsequent rounds, the algorithm selects the arm with the highest UCB index, denoted as $\u03b2_t$ (line 5). UCB indices comprise weighted av-erages of rewards $Q_t(a)$ and incorporate confi-dence bonuses with \u03b3 as the exploration parameter (weight). This confidence threshold is associated with each exit i.e., $\u03b2_t$ is the threshold chosen for all exits for the tth token. The token is then processed until the confidence is above $\u03b2_t$ (line 10) and if the token never gains sufficient confidence, it is inferred at the final layer (line 16). After the token exits from one of the layers, the average reward of the played arm is updated (line 14). The caption starts with a < bos > token and the next predicted token c is appended to the predicted caption set C until the < eos > token is predicted. Note that there are two counters, one is for the image, once the < eos > token is predicted the caption for the image is complete and the counter for the image is updated while the token counter is updated ev-ery time a token is passed through it and denotes the number of times rewards are updated. Hence, the algorithm learns faster as the thresholds (arms) are learnt on the number of tokens passed instead of the number of images.\nDrawing from the analysis of UCB1 (Auer et al., 2002), the regret of A-CAPEEN can be shown to be of the order $O(\\sum_{a \\in A\\a^*} log(n))$, where $\u0394_\u03b1 = r(a*) \u2013 r(a)$ denotes the sub-optimality gap. For completeness, we provide proof in the Appendix A.1."}, {"title": "Experiments", "content": "Dataset and Metric: We evaluate the perfor-mance of our method using the MS-COCO and Flickr30k datasets for image captioning. Our pri-mary objective is to produce coherent image cap-tions. To maintain consistency with prior studies, we preprocess all captions by converting them to lowercase. Additionally, we filter out words that occur fewer than 5 times in the dataset, ensuring robustness in our evaluation. We report key met-rics, including BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), ROUGE (Lin, 2004) and SPICE (Anderson et al., 2016) scores. To be consistent with previous methods, we report the speedup ratio as the measure of reduction in com-putational requirements. This metric can easily be converted to the expected time reduction rate.\n$\\frac{\\sum_{i=1}^{N} w_l \\times N}{\\sum_{i=1}^{N} w_l \\times l}$ (5)\nwhere N is the number of decoder layers and l is the number of layers after which the token exits the backbone during inference. $w_l^f$ is the number of words that exit at the lth decoder layer for an image I. This metric provides insights into our decoding process's resource utilization and effi-ciency, a critical aspect discussed in our work.\nTraining: The encoder-decoder backbone is ini-tially fine-tuned for 10 epochs with a starting learning rate of 1e-5, which decays by 0.5 every 3 epochs. Subsequently, self-critical training is em-ployed for 5 epochs with an initial learning rate of 7e-6, also decaying by 0.5 every 3 epochs. The backbone weights are frozen post-fine-tuning, and exits are added to the decoder, whose weights are further trained for 5 epochs. The Adam optimizer and a batch size of 8 are chosen, with a thresh-old of 0.6 chosen based on the accuracy-efficiency trade-off on the validation split. Inference is con-ducted on the Karpathy test split of the MS-COCO dataset with a batch size of 1, using NVIDIA RTX 2070 GPUs.\nAdaptive threshold learning (A-CAPEEN): We experiment with two types of noise, Gaussian noise and Gaussian blur. We mimic real-world scenarios by adding different levels of noise to the images of the test set and then proceed to learn the thresholds. In this, we adapt the thresholds based on the level of distortion present in an image using Algorithm 2. For this step, we choose the action set as A = {0.1, 0.2, ..., 1.0}. We add noise to the dataset's test split. Then we perform learning of threshold values, using A-CAPEEN for all the exits. Note that algorithm 2 has small computa-tional complexity and does not add upon latency in the inference process. It maximizes the rewards over a finite set which has negligible complexity.\nWe set \u03bc = 1/N. The latency cost $o_i$ could be understood as the cost of processing the samples from 1st exit to ith exit. Hence we set the latency cost as $o_i = \u03bbi$ where \u03bb is per layer computational cost. Since the value of threshold a is adaptively chosen in this case, A models the trade-off between accuracy and efficiency. A higher value of \u03bb will provide higher accuracy while a lower value will give high efficiency. We use the value of \u03bb = 1. A detailed ablation study of this hyperparameter can be found in the Appendix B.4.\nImpact of change in Distribution: We evalu-ate the impact of data distribution shifts on our model's performance (BLEU-1, BLEU-4) using image distortion (Figure 2). We train on undis-torted MS-COCO images and introduce Gaussian noise/blur to the test set, simulating real-world im-perfections. Noise levels are varied for Gaussian noise (\u03c3\u2208 {0.1, 0.2, 0.3, 0.4, 0.5}) and Gaussian blur (\u03c3 \u2208 {0.5, 1.0, 1.5, 2.0}) to assess robustness to context-driven data variations.\nBaselines: We establish baseline models for per-formance evaluation. To assess improvements in speedup as compared to the final decoding layer, we consider this setup as a baseline for our ap-proach (final exit). In this case, all the samples are inferred only at the final layer. We also directly re-duce the layer number to 9 in Decoder-9L and use only 9th layer to make an inference. This base-line serves as a lower bound for performance met-rics since it does not employ any technique. Since DeeBERT (Xin et al., 2020), ElasticBERT (Liu et al., 2021b), F-PABEE (Zhou et al., 2020), Fast-BERT (Liu et al., 2020) and LeeBERT (Zhu, 2021) were originally conducted on BERT, we imple-"}, {"title": "Results", "content": "In this section, We discuss the main results (me-dian of 5 independent runs) of our work. Details of the stability of our method are in table 3.\nCAPEEN. In the context of pristine (undis-torted) images, Table 1 presents performance re-sults of early exit models, showcasing our ap-proach's superiority over various baselines. Our method outperforms all previous baselines, in-cluding DeeCap and MuE. Unlike DeeCap, our approach does not rely on an imitation network, avoiding noise accumulation as samples exit the main backbone early. Additionally, our method consistently outperforms MuE by leveraging deep representations crucial for semantic correctness. Baselines like DeeBERT, ElasticBERT, and F-PABEE exhibit significant performance drops due to limited access to deep representations. While FastBERT and LeeBERT performs better than these baselines by accessing deep representations, they lacks the appropriate ground-truth informa-tion during weight learning for attached exits.\nCAPEEN enriches early classifiers with higher-level semantic information distilled from the final layer, leading to minimal performance drops and the highest speedup ratio across all metrics.\nAdaptive learning of the thresholds (A-CAPEEN). In Table 2 and 4, we highlight the impact of adapting threshold values based on changes in data distribution, comparing them with thresholds learned during training and fixed dur-ing inference. Our findings demonstrate that fixed thresholds significantly affect inference time and performance, highlighting the need for dynamic threshold learning to accommodate contextual in-formation and inherent image noise.\nFor the CIDEr metric, A-CAPEEN observes minimal occasional gains from the final decoder layer, attributed to overthinking (Zhu, 2021) dur-ing inference similar to overfitting during training. A-CAPEEN consistently outperforms CAPEEN as well as other baselines when the dataset distri-bution changes due to distortion in images. The gain in performance and speed is observed as A-CAPEEN finds the optimal threshold that opti-mally models the accuracy-efficiency trade-off.\nWe perform an ablation study and a case study in Appendix B to further prove the effectiveness of our method."}, {"title": "Analysis of threshold a", "content": "We present in the figure 4, the trade-off between accuracy and efficiency. To obtain higher accu-racy, we increase the value of a and the time re-duction decreases with higher accuracy. On the other hand, decreasing the threshold a will in turn increase the time reduction but with compromis-ing performance. CAPEEN performed better than other baselines due to the available knowledge from deeper exits. Observe that in figure 4, there is a very minimal drop in performance (mostly constant). This ability comes from the extra in-formation available at the intermediate exits due to knowledge distillation. However, the perfor-mance begins to drop when we try to reduce time by more than 50% since then the sample exits from the very initial layers which in turn affects the per-formance."}, {"title": "Conclusion", "content": "We introduced a new encoder-decoder back-bone for image captioning with early exits and knowledge distillation named CapEEN. Using the student-teacher model, we trained early exit classi-fiers using knowledge distillation to capture high-level semantic representations available at the deeper layers. We demonstrated that CapEEN of-fers a significant increase in speedup while main-taining a competitive performance guarantee. Fur-ther, we introduced A-CapEEN, where the thresh-old used for early exit decisions can be adaptively learned for distributions that differ from the train-ing data due to changes in the distortion levels."}, {"title": "Limitations", "content": "In our work, we have applied a uniform thresh-old across all exits during inference, simplifying the implementation process. However, extending this to individual thresholds for each exit could offer additional flexibility, albeit with increased complexity. Additionally, while early exit models effectively reduce latency during inference, they do incur higher computational costs during train-ing. This is due to the need for exit classifiers to learn additional weights after each layer, result-ing in increased complexity. Nonetheless, post-training, these models significantly enhance infer-ence speed, which becomes the primary consider-ation post-deployment."}, {"title": "Algorithm A.1 Regret Bound", "content": "Theorem A.1. For any $\u03b3 \u2265 1$, the regret of A-CAPEEN with K arms in the action set after T rounds is given as:\n$R(A \u2013 CAPEEN, T) \u2264 4 \\sum_{\\alpha\u2260\\alpha^*} \\frac{log(T)}{\u0394_\u03b1} + (\\frac{\\pi^2}{3} + 1) \\sum_{\\alpha\u2260\\alpha^*} \u0394_\u03b1$ (6)\nwhere $\u2206_a = r(a^*) \u2013 r(a)$.\nProof. The proof follows similar lines as given in the classical UCB (Auer et al., 2002). the instan-taneous regret in round t is given as :\n$R_t = r(a_t) - r(a^*)$"}, {"title": "Ablation study", "content": "In Figure 6a, we conduct an ablation study by learning the early exit weights using different loss combinations. First, we learn the early exit only using the cross-entropy loss where only ground-truth labels are used to train the weights of early exits. Then we consider only knowledge distilla-tion loss and only soft labels guide the early exits. We observe that if we do not use the combination of both soft as well as hard labels then early clas-sifiers get the highest hit in terms of performance as they need rich information from deeper layers. As we move deeper into the backbone all three combinations converge to similar CIDEr scores as deeper layers already have access to rich features.\nThese observations suggest the significance of the various components within the loss function, ultimately guiding us towards a more comprehen-sive understanding of the model's behaviour."}, {"title": "Case Study", "content": "We also provide some examples of how the dif-ferent proposed models annotate an image. We compare the captions generated by the two pro-posed methods with the captions provided by hu-mans (ground-truth). As we can observe from fig-ure 6c, CAPEEN performs well in explaining the content of the image. As compared to the ground-truth, CAPEEN provides more details as it also recognizes that the cake is large. A-CAPEEN gets a caption which is closer to the CAPEEN when there is no noise in the image. However, as we add noise to the image, then the results of A-CAPEEN are better than CAPEEN. Note that A-CAPEEN is tested on this example after it has seen a sufficient amount (around 100 images) of data with similar noise so that it adopts the distribution of the data. Also, observe that there is a slight variation in the captions and some information is lost when there is added noise in the dataset. Still, the prediction given by A-CAPEEN is better as it has added 'is cutting' in the sentence while CAPEEN just out-puts 'cuts' and CAPEEN observes that it is a large knife which is redundant information."}, {"title": "Baselines", "content": "DeeBERT: do a separate training of the backbone and the attached exits in a setup similar to ours. It trains the model by"}]}