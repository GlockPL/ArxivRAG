{"title": "Large Language Model Prompting With Episodic Memory", "authors": ["Dai Do", "Quan Tran", "Svetha Venkatesh", "Hung Le"], "abstract": "Prompt optimization is essential for enhancing the performance of Large Language Models (LLMs) in a range of Natural Language Processing (NLP) tasks, particularly in scenarios of few-shot learning where training examples are incorporated directly into the prompt. Despite the growing interest in optimizing prompts with few-shot examples, existing methods for prompt optimization are often resource-intensive or perform inadequately. In this work, we propose Prompting with Episodic Memory (POEM), a novel prompt optimization technique that is simple, efficient, and demonstrates strong generalization capabilities. We approach prompt optimization as a Reinforcement Learning (RL) challenge, using episodic memory to archive combinations of input data, permutations of few-shot examples, and the rewards observed during training. In the testing phase, we optimize the sequence of examples for each test query by selecting the sequence that yields the highest total rewards from the top-k most similar training examples in the episodic memory. Our results show that POEM outperforms recent techniques like TEMPERA and RLPrompt by over 5.3% in various text classification tasks. Furthermore, our approach adapts well to broader language understanding tasks, consistently outperforming conventional heuristic methods for ordering examples.", "sections": [{"title": "1 Introduction", "content": "The recent rapid advancements in Language Models (LMs) have underscored the increasing significance of utilizing pre-trained language models, especially when paired with appropriate prompts, as evidenced by seminal works [3, 4, 8]. As Language Models increase in parameter count, they unveil new capabilities such as In-Context Learning (ICL) [3], enabling LLMs to tackle tasks with just a few example demonstrations in the prompt. ICL offers a data-efficient approach for performing NLP tasks, achieving remarkable few-shot performances across many downstream tasks [18, 20, 21].\nHowever, the prompt content and ICL examples necessitate meticulous tuning to ensure consistent performance across various tasks. To optimize prompt contents, early attempts focus on tuning the embeddings via gradient descent (\"soft prompts\", [16, 21]). Unfortunately, soft prompts require gradients from LLMs to construct prompts and often face challenges with interpretability and quality [11, 16]. Additionally, they struggle to handle ICL examples within the prompt, and thus can only be used for zero-shot prompting. Consequently, the current state-of-the-art has shifted towards discrete prompt optimization [6, 39], which enhances interpretability and permits ICL optimization. Selecting the right in-context examples and their orders in prompts is crucial for ICL optimization [20]. This task is challenging due to the vast array of possible combinations and diverse instructions [23]. Furthermore, the arrangement of these examples may introduce biases, including majority, recency, and primacy biases [23, 41, 25]. Although reasonable example selection can be achieved through nearest neighbor retrieval [20], determining the optimal order of examples remains an unresolved research challenge.\nInitial efforts employed heuristic rules to rank examples in descending or ascending order based on their similarity to the test instance [20, 23]. More recent work attempts to use LLMs as black-box optimizer [33, 26], calibration [41] or applies RL-based method for generating prompts with ICL examples [39].\nDiscrete prompt optimization presents its own set of challenges. Heuristic methods lack optimization principles, leading to success in some cases but failure in others [20]. Black-box methods are gradient-free and query-agnostic, thus failing to incorporate any query-related context into the prompt, which can lead to downstream performance degradation. Moreover, they often require additional LLM computation for prompt generation, resulting in extensive resource usage [37, 27, 42]. Although using RL-based methods for prompt editing sequentially presents a potential solution that does not require extra LLM for prompt generation [39], their slow convergence and intrinsic complexity hinder effectiveness.\nIn this paper, we propose a novel and efficient memory-based approach to optimize the order of ICL examples within LLM prompts. Drawing inspiration from the rapid, model-free, and instance-based learning evident in the hippocampus region of the human brain [15], our method eliminates the necessity for complex reinforcement learning optimization while being more reliable than heuristic methods through performance-driven optimization. Leveraging episodic control mechanisms in reinforcement learning [2, 14, 12], we formulate each evaluation of training data as an episode and utilize an episodic memory to store the performance of any combination of the training data and ICL orderings. By sampling and evaluating certain training inputs and ICL order pairs, we avoid exhaustive searches across all data-ICL order combinations, which is particularly beneficial when LLM evaluation is costly. During testing, this memory serves as a non-parametric nearest-neighbors model, utilizing the recorded performance of similar training data to determine the optimal order for the testing data.\nTo ensure the robust generalization of our episodic memory, we devise specialized representations for both the text input and the ordering of ICL examples. Specifically, we encode the input using the last hidden states of a pre-trained language model, ensuring high-quality similarity-based retrieval during testing. Moreover, directly encoding the permutation as a sequence of ICL examples would result in a vast search space. Suppose there are M training samples"}, {"title": "2 Method", "content": ""}, {"title": "2.1 Problem Formulation", "content": "Few-shot text classification. We follow the standard few-shot setting for downstream tasks of language models [3]. In this setup, a pretrained language model L is paired with a task-specific dataset D, where Y represents the label space. Y may vary in form; it can be categorical, as in classification tasks, or sequential, as in question answering and commonsense reasoning tasks. For classification tasks, we randomly assemble a dataset of L = g * G samples, with g as the samples per label and G as the total labels. In cases without categorical labels, we randomly sample L instances from D. This forms the training dataset, denoted as $D_{train} = \\{x_i, Y_i\\}_{i=1}^{L}$, while a separate hold-out set Dtest is reserved for evaluation.\nIn-context Learning. Following GPT-3 paper [3], In-context Learning is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstration. Given a test sentence $X_{test}$, template-based construction \u03a8, along with an optional task description tdesc and a set of in-context examples $T = \\{x_i, Y_i\\}_{i=1}^{m}$, denoting \u0393 as the prompt construction function, we can formulate an input prompt p as follows:\np = F (tdesc, T, Xtest)\n= tdesc \u2295 \u03a8 (x1,Y1) \u2295 ...\u2295 \u03a8 (xm, Ym) + \u03a8 (Xtest, *)\nwhere \u2295 is the concatenate function and m is the number of in-context examples for each prompt. In line with the few-shot setting [39, 6], we consider an in-context set of M samples, denoted as $D_{ic} \u2208 D$, excluding the few-shot training data. The in-context examples will be sampled only from this set.\nIn-context learning facilitates the construction of the task's output distribution $P_{LM} (y|x, p)$, where x represents the input string and y represents the output string. This powerful approach allows in-context learning to play an active role in shaping the selection and arrangement of the demonstration set T within the input prompt p. By carefully curating and organizing these demonstrations, in-context learning can significantly enhance the model's ability to perform effective optimization, ultimately leading to more accurate and reliable task outcomes.\nReinforcement Learning Formulation. We formulate in-context prompt optimization as an RL problem where a state s = E(x) is the embedding of the input x, where E is a pre-trained encoder. During training, given a set of m in-context examples, the RL agent selects one of the possible permutations as its action a from the action space A. We then construct the prompt p using the default task description/instruction (if any), combined with the a-ordered in-context examples, and query it to the downstream LM to get the reward r. The goal of the agent is to learn a policy that maximizes the episodic return $R_t = \\sum_{h=0}^{H-t} r_{t+h}$, where H is the time step at which the episode ends. To simplify the formulation, our episode consists of only one step, during which the sole action is selecting the permutation of the in-context examples."}, {"title": "2.2 Prompting with Episodic Memory", "content": "In this section, we present the architecture of our episodic memory. The memory is structured as a dictionary, storing the embeddings of training sentences (states) as keys. Each key's value is another dictionary, mapping a permutation (action) to its respective reward. We denote a key by si, and for each key, aj represents the j-th permutation, while rij denotes the associated reward. Our episodic memory M can be represented by the following structure:\n$M = \\{s_i : \\{a_1 : r_{i1}, a_2:r_{i2},..., a_p:r_{ip}\\}\\}_{i=1}^{L}$\nHere, p signifies the total number of permutations available for the in-context examples (with m examples per prompt, p = m!). L is the total of the stored states in the episodic memory. An illustration of our memory architecture is given in Fig. 1. Below are the detailed components of the memory.\nState representation Obtaining accurate and meaningful text representations for the state is crucial for both memory storage and efficient retrieval. In our approach, we leverage the power of the encoder E, specifically utilizing the SentenceTransformers model [28]. This encoder is designed to generate high-quality sentence embeddings that can be effectively applied across a wide range of NLP tasks, ensuring that the textual data is both rich in information and can be used effectively in various NLP tasks.\nExample selection To simplify the optimization, we do not aim to optimize the example selection process. Therefore, following prior work [20], from the in-context dataset Dic, given input x, we select m in-context examples that are semantically closest to x. We measure the semantic similarity between in-context example $x_{ic}$ and the input query x using Cosine Similarity:\nCS (x,xic) = $\\frac{s \\cdot s_{ic}}{||s||||s_{ic}||}$\nIn the context of a few-shot classification task, where label biases can significantly influence the prompting outcome, it is crucial to maintain an equal number of examples for each label. To address this, we propose the following strategy: if there are G unique labels in D, and G < m, we select $[]$ closest samples from each label. Conversely, if G > m, we iteratively choose one sample for each label until we have sufficient in-context examples. We denote the above process as the function \u03a9 that retrieves context examples for each state s:\n$T_s = \u03a9 (s, D_{ic})$\nAction encoding An action refers to a specific arrangement of in-context examples within Ts. As mentioned in the introduction, a naive approach of encoding the action as a sequence of ICL examples will lead to a huge action space. Therefore, we propose to encode the action as a sequence of similarity ranks. Concretely, for each state s, we measure the semantic similarity between it and the states of the in-context examples using Eq. 3. Next, we rank each example according to its similarity to the input query and encode the ICL ordering as a sequence of these ranks.\nFor illustration, consider the action encoding process illustrated in Fig. 2. First, we compute the cosine similarity between each in-context example and the test query, which ranks these examples based on their distance. To encode an action, we create a permutation of these rankings, resulting in a unique action sequence. For instance, in Fig. 2, the action a = (m, 3, ..., 1) represents a specific permutation of the in-context examples. Here, the first example is the farthest from the test query, the second is the third closest, and so on, with the m-th position being the closest to the test query. Each action is thus a vector with m elements. This encoding captures the relational similarity among the in-context examples and the test query, with the action space size being m!.\nGiven a set of in-context examples Ts and an action a, we define a permutation function V, used to reorder the elements of Ts according to the sequence specified by a. The action a represents a specific ordering of indices that correspond to the elements in Ts. Formally, we have:\n$T^{a} = \u2207 (T_s, a)$"}, {"title": "2.3 Episodic Memory Operations", "content": "Our memory operation involves two main phases. Training: The RL agent interacts with the LLM to try actions and collect rewards for (s, a) pairs to fill the memory. Testing: The RL agent uses the memory to determine the ICL ordering for testing data.\nTraining In this phase, we collect and store the rewards defined above for state-action pairs (s, a). Given an input query $x \u2208 D_{train}$, we obtain its representation using the pre-trained encoder, s = E (x). The action at training time is selected via a linearly decaying \u03b5-greedy policy, defined as:\n$\u03b5_t = \u03b5_{initial} - \\frac{(\u03b5_{initial} - \u03b5_{final})}{N} * t $\nwhere \u03b5t is the value of \u03b5 at episode t, N is the total number of training iterations, \u03b5initial and \u03b5final are the initial and final values of \u03b5, respectively.\nGiven the a pair of state and action (s, a) and the reward r, following [2], our Episodic Memory M is updated using Memory writing as follows:\nM(s, a) \u2190 $\\begin{cases}\nr,\\qquad &\\text{if } (s, a) \u2209 M \\\\\nmax \\{M (s, a), r\\}, & \\text{otherwise}\n\\end{cases}$\nIt is noted that the stored rewards never decrease, indicating a focus on high-return actions. In our setting, we use the highest possible reward that the downstream LM can achieve for the pair (s, a) to estimate the value of using ICL ordering a for input x. The motivation behind this is to emulate the brain's specialized learning mechanisms that exploit predictable patterns in the environment, enabling rapid learning from high-return actions recorded in memory.\nWe limit the size of our memory to be equal to the size of the training dataset $D_{train}$. In the few-shot scenario where there are limited training samples, it is guaranteed that our memory will not be overflowed. However, in other settings where $D_{train}$ is too big, our memory can have a fixed smaller size, and when a new state-value pair has to be introduced, the least recently used state will be discarded.\nTesting In this phase, we select the optimal permutation for each testing query. In reality, it is common to receive novel states (states that are not seen in training). To handle this, we employ a nearest-neighbor estimator. In particular, we obtain the approximated value of a testing state st and a candidate action a using the following Memory Reading process [13]:\n$\\tilde{M}(s_t, a) = \\begin{cases}  \\frac{\\sum_{i=1}^{k} CS(x_t, x^i) * M (s^i,a)}{\\sum_{i=1}^{k} CS(x_t, x^i)} , &\\text{if } (s_t, a) \\notin M\\\\  M(s_t,a), & \\text{otherwise}\\end{cases}$$\nwhere $s^{i}$, i = 1,..., k are the k states with the highest similarity to the testing state st, CS (xt, xi) is the Cosine Similarity between the neighbor xi to the test query xt. The motivation behind the weighted sum is that we believe the semantic similarity between training data and the test query should correspondingly affect its weight in the ordering process. After having calculated $\\tilde{M}(s_t, a)$, we get the action at for st as follow:\n$a_t = arg \\max_{a} \\tilde{M} (s_t,a)$\nWe note that with enough training steps, the actions for each state in the memory are filled to assure Eq. 12 is valid. In addition, the nearest neighbor estimation here is a different process from the nearest neighbor retrieval in the example selection described in Section 2.2. Algo. 1 summarizes the procedure of our method."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Few-Shot Text Classification", "content": "Datasets. For classification tasks, we conduct experiments for sentiment analysis, Natural Language Understanding (NLU), topic classification, and natural language inference datasets. For sentiment analysis, we choose SST-2 [32], IMDB [24] and CR [9] as datasets. For NLU task, we select COLA [36]. For Reading Comprehension, we choose Boolq [5]. For topic classification, we use AG News [40]. For Natural Language Inference (NLI), we choose QNLI [35].\nBaselines. For few-shot classification tasks, we compare our work with previous continuous and discrete prompt optimization methods. Finetuning is the method that finetunes the entire language model with a classification head using the few-shot dataset. For Manual Prompt, we use the hand-written prompts from [31]. In-context Demonstration randomly selects one example and concatenates it with the test query. Black-box Tuning [33] combines a discrete component optimized through non-gradient methods and a soft component refined via gradient descent. RLPrompt [6] generates discrete prompt tokens using RL framework. TEMPERA [39] trains a RL agent to edit prompt sequentially. To ensure a fair comparison, we rerun these methods under our setting. Besides complicated baselines, we also investigate a simple heuristic baseline. Random Ordering adopts nearest neighbor example selection, randomly permutes the in-context examples, and concatenates with the test query.\nExperiment Setup. We use RoBERTa-large [22] as the downstream LM. To ensure a fair comparison, we follow the same setting from [6, 39] by testing all baselines on a few-shot text classification setting. Training dataset Dtrain has 16 examples per class, and we sample another 16 data points as in-context dataset Dic (M = 16). For reporting the testing results, we make use of the models having the highest performance on the validation set and do inference on the test set Dtest provided by the task. We use m = 4 examples for each prompt as in prior works [39]. For each run, we use the same training and in-context samples for all baselines. At test time, we select the number of nearest-neighbor k = 10 for POEM.\nResults. We present our few-shot text classification results over 4 runs in Tab. 1. We can see that on most tasks, POEM outperforms previous baselines by a large margin. For example, we have a 2.9% absolute gain on SST-2 task (over TEMPERA), and 5.6% on IMDB, and the performance is almost comparable to finetuning the LM on QNLI task. Especially, for NLI and NLU tasks, on harder datasets like Boolq and COLA, we have a significance gain of 11.2 % and 14.2 % respectively. We also see that POEM has a much smaller variance between different runs than all other baselines, which indicates that it is more stable across different datasets. Unlike TEMPERA, which requires sequential modification of in-context examples, our method, POEM, enables one-step optimization. Our approach not only simplifies the process but also overcomes TEMPERA's slow inference involving several numbers of edits for each prompt. POEM also outperforms Random Ordering significantly, by nearly 13% on average, highlighting the importance of ICL optimization."}, {"title": "3.2 General Language Understanding Tasks", "content": "We further extend experiments of our method to general language understanding tasks that require stronger downstream LLM to generate the answers.\nDatasets. We measure performance on two main tasks with four datasets, categorized as follows: Commonsense Reasoning: Hellaswag [38], PIQA [1]; Question Answering: TruthfulQA-mc1 [19], TriviaQA [10]. For more complex tasks such as Question Answering and Reading Comprehension, which involve multiple text fields, we employ a reranking approach based on the field containing the most relevant information. In cases like TruthfulQA, where certain fields like type and category lack semantic significance, we measure textual distances between examples using the question field, which typically encapsulates the query to be answered."}, {"title": "3.3 Ablation Studies", "content": ""}, {"title": "3.3.1 Analysis of Efficiency", "content": "We provide empirical evidence for our claim of POEM being fast and efficient. We compare performance and runtime on SST-2 dataset [32] between POEM, RLPrompt and TEMPERA. For a fair evaluation, all experiments were conducted with one identical GPU Nvidia A100. As shown in Tab. 6, the training time (until convergence) of POEM is approximately 150 times faster than that of TEMPERA and RLPrompt while achieving better accuracy."}, {"title": "3.3.2 Analysis of POEM's Components", "content": "Action encoding. We aim to investigate further how our action encoding contributes to our framework. To achieve this, we design a naive action encoding for a sequence of m examples. This approach results in an action space of $\\binom{M}{m}$ rather than m!, where M denotes the total number of in-context examples and m represents the number of examples per prompt. As demonstrated in Tab. 5, the absence of our similarity-ranked encoding leads to a noticeable decrease in performance across all classification tasks.\nImbalanced labels. For few-shot classification task, we aim to investigate how imbalanced in-context example labels can affect performance."}, {"title": "3.3.3 Hyperparameter Sensitivity", "content": "Number of iterations. We present results for different numbers of training iterations, specifically 10, 60 (the default setting), and 120. As shown in Tab. 7, increasing the number of iterations tends to improve Memory Writing by allowing POEM to discover more effective permutations. This indicates that more iterations can enhance the model's ability to refine its training. However, we have selected 60 iterations as the default to balance training time with convergence stability, ensuring that the model achieves robust performance without excessive training duration."}, {"title": "4 Related Works", "content": "Prompt Engineering. The traditional approach to using pre-trained LMs involves fine-tuning downstream datasets [7, 17], which involves extensive updates to model parameters. However, this method has shown limited success on downstream tasks. Another approach involves utilizing manual prompts to guide LLMs in performing NLP tasks without requiring additional training [3, 30, 31]. A different line of work in prompt engineering aims to develop instructional prompts, which offer task descriptions instead of fill-in-the-blank questions. In-context Learning [3, 20, 23] achieves impressive performance by incorporating in-context demonstrations. However, these prompt engineering approaches are time-consuming and require manual tuning, which is not always feasible.\nPrompt Optimization. Previous studies have also explored the application of RL for prompt optimization. [6] propose using RL to directly generate prompts agnostic to specific queries ; however, the generated prompts may lack meaningfulness. Another previous RL editing method [39] allowed editing task descriptions and in-context examples. Yet, editing descriptions seldom aids optimization, and swapping examples can be time-consuming, potentially leading back to the initial state. Moreover, the method suits categorical tasks like classification. In the realm of continuous embedding space, gradients derived from LMs are employed to directly facilitate prompt optimization, a method also referred to as soft prompt tuning [18]. However, due to their continuous nature, soft prompts pose challenges in comprehension [16] and lack reusability across diverse models because of disparities in latent embedding spaces. With the expansion of LLMs in terms of both capacity and capabilities, there has emerged a new line of research that employs LLMs as prompt generators, prompt editors, or prompt scorers. [37] propose a method utilizing LLMs as prompt optimizers, where the generated prompts rely on the prior knowledge encoded within the LLMs. [42] utilize two distinct LLMs, one as a zero-shot instruction generator and the other as a scorer to optimize instructions. However, these approaches share the drawback of relying heavily on the prior knowledge encoded within LLMs, necessitating high-quality LLMs, which can be resource-intensive. Furthermore, the variability in generated outputs by LLMs can be unpredictable and challenging to interpret.\nExamplars retrieval and ordering in In-context learning. Research has demonstrated the significant influence of in-context example selection and arrangement on the performance of LMs. For instance, [29] utilize a retriever to select in-context examples. Additionally, [20] propose a heuristic approach to ordering examples, ranking them based on the textual similarity between the test query and in-context examples. Recently, [39] have applied RL to enable the swapping of in-context examples within their action space. These studies underscore the impact of selecting and arranging in-context examples on downstream task performance. Compared to heuristics, RL solutions are theoretically guaranteed. Unfortunately, existing RL-based methods are complicated and slow during training. Our study is the first RL-based prompt optimization method that is both simple and efficient, demonstrating superior performance compared to existing counterparts."}, {"title": "5 Discussion", "content": "We have introduced POEM, a novel approach to prompt optimization within the RL paradigm. By strategically reordering few-shot examples using episodic memory, POEM significantly enhances the performance of LLMs across a wide range of NLP tasks. Our method consistently outperforms existing techniques in few-shot classification on various datasets and demonstrates clear advancements over heuristic baselines in general language understanding tasks. The synergy between NLP and RL in POEM underscores the potential for future innovations in test-time prompt optimization algorithms, which could prove pivotal for real-world applications."}]}