{"title": "Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning", "authors": ["Manh Luong", "Khai Nguyen", "Dinh Phung", "Gholamreza Haffari", "Lizhen Qu"], "abstract": "Teacher-forcing training for audio captioning usually leads to exposure bias due to training and inference mismatch. Prior works propose the contrastive method to deal with caption degeneration. However, the contrastive method ignores the temporal information when measuring similarity across acoustic and linguistic modalities, leading to inferior performance. In this work, we develop the temporal-similarity score by introducing the unbiased sliced Wasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to account for temporal information across modalities. In contrast to the conventional sliced Wasserstein RBF kernel, we can form an unbiased estimation of USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to stochastic gradient optimization algorithms, and its approximation error decreases at a parametric rate of O(L^{-1/2}) with L Monte Carlo samples. Additionally, we introduce an audio captioning framework based on the unbiased sliced Wasserstein kernel, incorporating stochastic decoding methods to mitigate caption degeneration during the generation process. We conduct extensive quantitative and qualitative experiments on two datasets, AudioCaps and Clotho, to illustrate the capability of generating high-quality audio captions. Experimental results show that our framework is able to increase caption length, lexical diversity, and text-to-audio self-retrieval accuracy.", "sections": [{"title": "1. Introduction", "content": "Audio captioning task (Drossos et al., 2017) strives to describe acoustic events and their temporal relationship in natural language. Compared to other audio-related tasks, audio captioning is a multimodal learning task which lies at the intersection of audio and natural language processing. The popular framework for audio captioning is to train audio captioning models by maximizing the likelihood of ground-truth captions during the training stage and then utilizing trained models to generate audio captions at the inference stage.\nAlthough audio captioning models trained with maximum likelihood procedures are capable of generating plausible audio captions, they still suffer from exposure bias due to training and inference mismatch. (Schmidt, 2019) conducted a comprehensive study regarding exposure bias and argues that exposure bias can be viewed as a generalization issue for language models trained by teacher forcing procedures. Therefore, regularization techniques (Shi et al., 2018; An et al., 2022) are proposed to alleviate exposure bias in language models. (An et al., 2022) proposed a contrastive loss regularization for conditional text generation. The contrastive loss is jointly optimized with likelihood loss to mitigate exposure bias for language models. Then, the prediction sequence is chosen by maximizing the likelihood and cosine similarity between a prefix-text and generated sequences. The contrastive method is efficient for conditional text generation, but it is not well-suited for the audio captioning task. The cosine similarity induced by contrastive loss is unable to consider temporal information between audio and caption sequences when measuring the similarity between them. Thus, the cosine similarity is inadequate to rerank candidate captions at the inference stage.\nDynamic Time Warping (DTW) (Sakoe & Chiba, 1978) and Soft Dynamic Time Warping (soft-DTW) (Cuturi & Blondel, 2017) are two widely adopted distances used to measure the discrepancy between two time series. They are capable of considering temporal information, however, the monotonic alignment imposed by DTW is too strict and might adversely affect the measurement of the discrepancy between audio and caption when local temporal distortion exists. (Su & Hua, 2017) proposed an order-preserving Wasserstein distance to deal with the shortcoming of DTW. Although the order-preserving Wasserstein distance can measure the discrepancy between two sequential data when temporal distortion exists, it is ineffective to measure the discrepancy between high-dimensional sequences due to the dimensionality curse of the Wasserstein distance."}, {"title": "Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning", "content": "To address all aforementioned issues, we propose the Audio Captioing with Unbiased sliced Wasserstein kernel (ACUS) framework to alleviate the caption degeneration for the audio captioning task and better measure cross-modal similarity. We develop the unbiased sliced Wasserstein RBF kernel (USW-RBF) for precisely measuring the similarity score between acoustic and linguistic modalities. The USW-RBF leverages the radial basis function (RBF) kernel, in which the sliced Wasserstein distance equipped with the rotary positional embedding is used as the distance. The proposed kernel is unbiased. Hence, it is highly compatible with stochastic gradient optimization algorithms, and its approximation error decreases at a parametric rate of O(L^{-1/2}). We also derive the proposed kernel and show that it is capable of measuring the similarity in terms of features and temporal information. Furthermore, (Arora et al., 2022a) provides an analysis of exposure bias through the lens of imitation learning and empirically shows that stochastic decoding methods are able to alleviate exposure bias for language models. According to this observation, we leverage the ACUS framework with stochastic decoding methods at the inference stage to rerank generated captions to choose the most suitable candidate caption. To sum up, our contributions can be summarized as follows:\n1. We propose the USW-RBF kernel to precisely measure the similarity between acoustic and linguistic modalities for encoder-decoder audio captioning models. Our kernel is able to deal with the dimensionality curse and temporal distortion by leveraging the sliced Wasserstein distance equipped with rotary positional embedding.\n2. We analyze the USW-RBF kernel and prove that it is an unbiased kernel. Thus, it is well-suited to stochastic gradient optimization algorithms, with its approximation error diminishing at a parametric rate of O(L^{-1/2}) with L Monte Carlo samples.\n3. We propose the ACUS framework which leverage stochastic decoding methods, such as nucleus and top-k samplings, at the inference stage to significantly alleviate exposure bias for the audio captioning task."}, {"title": "2. Background", "content": null}, {"title": "2.1. Encoder-Decoder Audio Captioning", "content": "An encoder-decoder audio captioning model, denoted as M = (f_\\theta, g_\\phi), is capable of generating captions \\hat{y} = {y_t}_{t=0}^N conditioning on a given audio x. Here, f_\\theta (\\theta \\in \\Theta) and g_\\phi (\\phi \\in \\Phi) are the encoder and decoder parameterized by \\theta and \\phi respectively. The encoder is designed to extract acoustic features from audio, while the decoder is able to decode extracted acoustic features to natural language. The audio captioning model is trained to maximize the likelihood of ground-truth captions when predicting the current word"}, {"title": "2.2. Contrastive Learning for Audio Captioning", "content": "To mitigate the exposure bias with likelihood training, contrastive learning for audio captioning (Chen et al., 2022a; Liu et al., 2021) introduces a contrastive objective which aims to maximize cosine similarity between audio and ground-truth caption. Negative examples are directly drawn from minibatch as follows SimCLR (Chen et al., 2020) to compute the infoNCE loss (Oord et al., 2018)"}, {"title": "3. Methodology", "content": "We first develop the unbiased sliced Wasserstein RBF kernel (USW-RBF) to deal with the dimensionality curse and strict monotonic alignment for measuring similarity across multimodalities. The USW-RBF is equipped with the rotary positional embedding to consider temporal information when measuring similarity across linguistic and acoustic modalities. Then, we propose the Audio Captioing with Unbiased sliced Wasserstein kernel (ACUS) framework to mitigate text degeneration for audio captioning. We leverage stochastic decoding methods with the USW-RBF as similarity score across modality to alleviate exposure bias at the inference stage. Our training and inference procedures are illustrated in Figure 1."}, {"title": "3.1. Unbiased Sliced Wasserstein Kernel", "content": "Wasserstein distance. Given p > 1, Wasserstein distance (Peyr\u00e9 et al., 2019) between \\mu and \\nu be two distributions belongs to P_p(\\mathbb{R}^d) is defined as:"}, {"title": "3.2. Audio captioning with the Unbiased SW-RBF kernel framework", "content": "Positional encoding for USW-RBF kernel. Given a pair of audio and ground-truth caption is denoted as (x, y), the hidden representation of audio, extracted from the penultimate layer of the audio encoder, is denoted as Z_x = [z_1,..., z_N], where z_i \\in \\mathbb{R}^d, and the hidden representation of ground-truth caption conditioning on the audio, extracted from the penultimate layer of the decoder, is denoted as Z_y = [z_1,..., z_M] where z_i \\in \\mathbb{R}^d. Although the USW-RBF is effective in measuring the similarity between two sets of vectors, the order of vectors within a set is not taken into account when computing the sliced Wasserstein distance. More importantly, the order of vectors within a set contains the temporal information between them, which is crucial for audio and language modality. To preserve the temporal information, we define the temporal-information preserving vector as follows"}, {"title": "4. Related Work", "content": "Audio captioning. The audio captioning task can be formulated as a conditional text generation task, therefore, the prior works utilize the maximum likelihood estimation method to train audio captioning models (Mei et al., 2021; 2024; Sun et al., 2023; Kim et al., 2022; Deshmukh et al., 2023). There are two popular architectures for audio captioning models: encoder-decoder architecture (Mei et al., 2024; Kim et al., 2024) and prefix-tuning architecture (Deshmukh et al., 2023; Kim et al., 2023). Although both architectures are effective in generating plausible captions, they suffer from the inherent weakness of the MLE training method: exposure bias. Some recent works deal with exposure bias by leveraging a regularization (Zhang et al., 2023; Deshmukh et al., 2024), contrastive loss. The contrastive regularization can slightly remedy the exposure bias issue for audio captioning models. Another technique to combat with exposure bias is to utilize stochastic decoding methods (Arora et al., 2022a). (Su et al., 2022) proposed a contrastive search framework with stochastic decoding methods to alleviate text degeneration for conditional text generation. The contrastive search framework is yet successful to deal with exposure bias for text generation, it can not be directly applied for audio captioning task. The reason is that the contrastive score is not able to take temporal information of acoustic and linguistic features into account. To deal with the shortcomings of the contrastive framework, we develop a new framework, called ACUS, which can handle the temporal information between acoustics and linguistic modalities when measuring the similarity score and alleviate exposure bias at the inference stage for audio captioning.\nWasserstein distance. Wasserstein distance is a metric to measure the discrepancy between two distributions. There are enormous applications of the Wasserstein distance for multimodal learning, such as audio-text retrieval (Luong et al., 2024), multimodal representation learning (Tsai et al., 2019), and multimodal alginment (Lee et al., 2019). The prior work (Su & Hua, 2017) proposed an order-preserving Wasserstein distance between sequences by incorporating a soft-monotonic alignment prior for optimal matching, however, it still suffers from dimensionality curse and a strict monotonic alignment across modalities. Although the Wasserstein distance is capable of measuring the cross-modality distance, it suffers from the dimensionality curse. In this work, we develop the USW-RBF kernel equipped with positional encoding to deal with the dimensionality curse and the strict monotonic alignment issue of measuring cross-modal similarity for audio captioning."}, {"title": "5. Experiments", "content": "We design experiments to demonstrate the effectiveness of our proposed method in mitigating exposure bias in the audio captioning task. We conduct quantitative experiments on two datasets: Audiocaps (Kim et al., 2019) and Clotho (Drossos et al., 2020) to answer the question of whether our proposed method is capable of alleviating exposure bias in the audio captioning task. We further conduct qualitative experiments on audio-text retrieval tasks and subjective evaluation to show the high-quality of generated captions. Finally, we perform ablation studies on the choice of similarity metric and positional embedding techniques. The ablation studies show that the proposed metric outperforms both Wasserstein distance, DTW, and soft-DTW in measuring the similarity between latent representation of audio and generated captions. These studies also show that rotary positional embedding is the most well-suited positional embedding technique for incorporating temporal information for audio captioning.\nEvaluation metrics. We evaluate baselines and two backbone models, Enclap and ACT, for our proposed framework by widely used evaluation metrics for audio captioning, including METEOR (Banerjee & Lavie, 2005), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2014), SPICE (Anderson et al., 2016), and SPIDEr (Liu et al., 2016). In addition, we evaluate the quality of generated audio captions by performing a text-to-audio retrieval task leveraging the pretrained CLAP (Wu et al., 2023) model. If a generated caption and a given audio are highly similar to each other, the CLAP model is able to retrieve the audio by using the generated caption. We further measure the lexical diversity and caption length in generated captions to measure the degeneration of captions. We also conduct a subjective evaluation to evaluate the quality of generated captions in terms of discretiveness, correctness, and fluency.\nBaselines. We compare against all state-of-the-art audio captioning models on Audiocaps and Clotho datasets. The ACT (Mei et al., 2021) audio captioning model leverages a vision transformer encoder pretrained on the AudioSet (Gemmeke et al., 2017) dataset for sound-event classification. LHDFF (Sun et al., 2023) utilizes residual the PANNs encoder to fuse low and high dimensional features in Mel-spectrogram. CNN14-GPT2 (Kim et al., 2023), Pengi (Deshmukh et al., 2023), and RECAP (Ghosh et al., 2023) apply prefix-tuning method for the pretrained GPT2 (Radford et al., 2019). AL-MixGen (Kim et al., 2022) leverages the ACT backbone trained using audio-language mixup augmentation and test-time augmentation at the inference phase. Wavcaps (Mei et al., 2024) is the HTSAT-BART model (Chen et al., 2022b) fine-tuned on numerous weakly-labeled data which is generated by using large language models. We choose a subset of models evaluated on the Clotho dataset without complex training methods, such as ensemble training, to ensure a fair comparison. The CLIP-AAC (Chen et al., 2022a), MAAC (Ye et al., 2021), P-LocalAFT(Xiao et al., 2022), and Graph-AC (Xiao et al., 2023) are the baselines evaluated on Clotho dataset."}, {"title": "5.1. Implementation details", "content": "Enclap backbone. We follow the original settings in (Kim et al., 2024) to train the large Enclap backbone for AudioCaps and Clotho dataset. The training objective is described in Eq. 13, in which the MLE and temporal-similarity are jointly optimized to train the Enclap model. The training coefficient \\alpha is set to 0.1 for both two datasets. The Adam optimizer with \\beta_1 = 0.9, \\beta_2 = 0.999, and a weight decay coefficient of 0.01 is used to train the model"}, {"title": "5.2. Quantitative Experiments", "content": "To assess the performance of our proposed method for audio captioning, we performed quantitative experiments on Audiocaps and Clotho. The experimental results are shown in the Table. 1. All baseline models utilize deterministic decoding methods, the beam search decoding, therefore their performance is not variant in each evaluation. On the other hand, the contrastive method and our framework utilize stochastic decoding methods, such as the nucleus and top-k samplings, thus its performance varies for each evaluation. To make a fair comparison, we evaluate both our framework and contrastive method 5 times and report the average performance and standard deviation. It is clear to see that our proposed method outperforms all baseline models in terms of automated metrics on the AudioCaps test set. Specifically, our proposed framework significantly improves the quality of generated captions for the Enclap backbone model. There is a significant improvement regarding the statistical metrics SPICE, METEOR, CIDEr, and ROUGE-L. These results prove that our proposed method is able to mitigate the exposure bias for audio captioning models during inference. Furthermore, there is a significant performance gain regarding the SPICE score, from 0.186 to 0.192."}, {"title": "5.3. Qualitative Experiments", "content": "We carry out qualitative experiments to examine the capability of alleviating exposure bias and caption degeneration of our proposed method. The pretrained CLAP (Wu et al., 2023) model is used for the text-to-audio self-retrieval experiments. As shown in Table 3, our method is able to enhance the caption length and lexical diversity of generated captions on both datasets compared to the contrastive learning method. Caption length and lexical diversity increase from 7.63 to 8.14 and from 7.21 to 7.52 on AudioCaps dataset, respectively. Furthermore, the caption to audio self-retrieval experiments show that our proposed method is able to generate high-quality captions which are beneficial to retrieving corresponding audio. These results show that the proposed framework can mitigate the exposure bias for audio captioning tasks and generate high-quality captions.\nHuman evaluation. We conduct a human evaluation to better assess the quality of generated captions. We randomly choose 50 audio from AudioCaps and Clotho test data. Captions are generated for each audio by using different methods: maximum likelihood estimation (MLE), contrastive framework, and the ACUS framework. The MLE method utilizes a deterministic decoding method, beam search with a beam size of 5, while contrastive learning and the proposed method utilize a stochastic decoding method, top-p sampling with p = 0.7 to generate 30 candidate captions. The most suitable caption is chosen based on Equation (5) for contrastive learning and Equation (14) for the proposed method. We recruit five annotators, who are asked to independently assess the quality of a given caption following a 5-point Likert scale for three aspects\nDescriptiveness: Whether the caption is descriptive enough, describe all audio events in the given audio and their temporal relationships.\nCorrectness: Whether the caption is correct, all audio events occur in the given audio.\nFluency: Whether the caption is fluent and easy to understand as human written."}, {"title": "5.4. Ablation Studies", "content": "Table 5 shows the ablation study on choosing similarity metrics for measuring audio and caption similarity. The DTW and soft-DTW are ineffective in measuring the similarity across acoustic and linguistic modality. Therefore, there is a decrease in performance compared with the baseline method with beam search decoding. The hypothesis is that the constraint for monotonic alignment between acoustic and linguistic embedding is too strict for measuring the distance between two modalities. Our score and the Wasserstein distance relax the monotonic alignment constraint when computing cross-modality similarity. Both our score and the Wasserstein distance are equipped with the positional embedding to consider temporal information when measuring similarity across modalities. Relaxing the monotonic alignment and incorporating positional embedding(PE) shows a significant performance gain regarding METEOR and SPICE metrics with the Wasserstein distance, 0.254 to 0.262 and 0.186 to 0.194, respectively. Although the Wasserstein distance with positional embedding is effective in measuring acoustic and linguistic similarity, it possesses a weakness: the dimensionality curse. Thus, there is still a gap in calculating similarity across acoustic and linguistic modalities. As mentioned in (Nguyen & Ho, 2022; Nietert et al., 2022; Nadjahi et al., 2020), the sliced Wasserstein does not suffer from the dimensionality curse. The performance of the USW-RBF score acquires a performance gain with all evaluation metrics, which reflects that the sliced Wasserstein with positional embedding is the most effective score for computing audio and caption similarity. The ablation study on the number of Monte Carlo samples L for estimating the USW-RBF is shown in Table 9 in Appendix A.3."}, {"title": "6. Conclusion", "content": "We introduce the ACUS framework for alleviating text degeneration for the audio captioning task. Furthermore, we develop the USW-RBF kernel equipped with the rotary positional embedding. The USW-RBF is an unbiased kernel, thus, it is compatible with stochastic gradient optimization algorithms, and its approximation error decreases at a parametric rate of O(L^{-1/2}). Our experiments demonstrate that our framework is able to mitigate the text degeneration issue for audio captioning models and outperforms baseline methods in terms of quantitative and qualitative evaluations. We further find that the nucleus sampling technique is the best decoding method to generate descriptive and correct captions from pretrained audio captioning models."}, {"title": "A. Appendix", "content": null}, {"title": "A.1. Proofs", "content": null}, {"title": "A.1.1. PROOF OF PROPOSITION 3.2", "content": "From Theorem 4 in (Kolouri et al., 2016), we have K_\\gamma(\\mu, \\nu) = exp (-\\gamma W^2(\\mu, \\nu)) is a positive definite kernel for \\mu and \\nu are two absolute continuous distribution in one-dimension. It means that for all n > 1 one-dimensional absolute continuous distributions \\mu_1,..., \\mu_n and c_1,..., c_n \\in \\mathbb{R}, we have:"}, {"title": "A.1.2. PROOF OF PROPOSITION 3.3", "content": "We first recall the definition of SW-RBF (Equation (8)) and the definition of USW-RBF (Definition 3.1."}, {"title": "A.1.3. PROOF OF PROPOSITION 3.4", "content": "(i) For the unbiasedness, we check:"}, {"title": "A.2. Implementation details", "content": "DTW and soft-DTW as dissimilarity metric.. DTW is a non-parametric distance which measures an optimal monotonic alignment between two time series of different lengths. The definition of DTW is defined as follows"}, {"title": "A.3. Ablation studies", "content": "The ablation study for the bandwidth parameter \\gamma is shown in the Table 8. To simplify the hyperparameter tuning, we perform beam search decoding to evaluate the performance of different values of the bandwidth parameter on two datasets. The optimal values for the bandwidth parameter are \\gamma = 2.5 and \\gamma = 1.5 on Audiocaps and Clotho datasets, respectively. Furthermore, ablation studies on choosing hyperparameters for stochastic decoding methods on Audiocaps dataset are demonstrated in the Figure 2. The SPIDEr metric is chosen as the criterion for hyperparameter selection for stochastic decoding methods, like nucleus, top-k, and temperature samplings. According to the experiments, nucleus sampling acquires the highest performance regarding the SPIDEr metric with p = 0.7. Therefore, we choose nucleus sampling with p = 0.7 to conduct experiments for our proposed framework."}, {"title": "A.4. Qualitative Examples", "content": "AudioCaps test set\nEnclap: Wind blows strongly\nEnclap with contrastive loss: A motor vehicle engine is running and accelerating\nEnclap with SW:Wind blowing hard with distant humming of engines\nReferences"}]}