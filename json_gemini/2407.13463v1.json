{"title": "End-To-End Clinical Trial Matching with Large Language Models", "authors": ["Dyke Ferber", "Lars Hilgers", "Isabella C. Wiest", "Marie-Elisabeth Le\u00dfmann", "Jan Clusmann", "Peter Neidlinger", "Jiefu Zhu", "Georg W\u00f6lflein", "Jacqueline Lammert", "Maximilian Tschochohei", "Heiko B\u00f6hme", "Dirk J\u00e4ger", "Mihaela Aldea", "Daniel Truhn", "Christiane H\u00f6per", "Jakob Nikolas Kather"], "abstract": "Identifying suitable clinical trials for cancer patients is crucial to advance treatment modalities and patient care. However, due to the inconsistent format of medical free text documents and the often highly complex logic in the trials eligibility criteria, this process is not only extremely challenging for medical doctors, but also time-consuming and prone to errors. This results in insufficient inclusion of oncology patients in clinical trials, especially in a timely manner. The recent advent of Large Language Models (LLMs) has demonstrated considerable potential for interpreting electronic health records (EHRs), suggesting that they hold great promise to facilitate accurate trial matching at scale.", "sections": [{"title": "Patients and Methods", "content": "We generated 51 realistic oncology-focused patient EHRs. For each, a database of all 105,600 oncology-related clinical trials worldwide from clinicaltrials.gov was accessed by GPT-40 to identify a pool of suitable trial candidates with minimal human supervision. Patient eligibility was then screened by the LLM on criterion-level across a selection of trials from the candidate trial pool and compared against a baseline defined by human experts. We then used criterion-level AI feedback to iterate over discrepant AI and human results, refining the human ground truth where necessary."}, {"title": "Results", "content": "Our approach successfully identified relevant, human preselected candidate trials in 93.3% of test cases from all trials available worldwide and achieved a preliminary accuracy of 88.0% (1,398/1,589) when matching patient-level information on a per-criterion-basis using the initial human evaluation as baseline. Utilizing LLM feedback to interactively re-evaluate human scores revealed that 39.3% of criteria that were initially considered incorrect according to the human baseline were either ambiguous or inaccurately annotated by humans, leading to a total model accuracy of 92.7% after refining the human ground truth eligibility definitions."}, {"title": "Conclusion", "content": "We present an end-to-end pipeline for clinical trial matching using LLMs, demonstrating high precision in screening for appropriate clinical trials at scale and matching selected candidate trials with high precision to individual patients, even outperforming the performance of qualified medical doctors. Additionally, our pipeline can operate both fully autonomously or with human supervision and is not intrinsically restricted to cancer, offering a scalable solution to enhance patient-trial matching for the real world."}, {"title": "Keywords", "content": "Clinical Trial Matching Oncology Trials Eligibility Criteria \u2022 Artificial Intelligence\n\u2022 Large Language Model \u2022 GPT-40"}, {"title": "Introduction", "content": "In oncology, clinical trials serve two purposes: they offer potential therapeutic benefits to cancer patients across all disease stages, ranging from early intervention to experimental treatments for those with limited or exhausted standard care options 1,2. They are also crucial to advance scientific research, as new treatments can only be approved through rigorous clinical testing. However, the practical realization of clinical trial enrollments still remains far from satisfactory, from both patient and clinician perspectives. For clinicians, identifying suitable trials is often time-consuming\u00b3 and complex due to patient-related factors such as performance status or comorbidities, logistical challenges like regional trial availability, systemic issues including lack of access to genomic testing, and the difficulty clinicians might face in locating available trials, all of which contribute to low enrollment rates of only 2-3% of potential trial candidates\u00b9.\nOverall, there are three primary reasons for this:"}, {"title": null, "content": "First, the sheer volume of data generated during oncologic treatments, including hospital stay records, as well as genomic and imaging data, often accumulate, drastically increasing the burden on physicians 5-7. These data are typically fragmented and unstructured, comprising free text, tabular records, and more. Secondly, the complexity and volume of clinical trials tailored to oncology further complicate the process. There are approximately 500,000 studies registered on Clinical Trials.gov, out of which 105,732 are dedicated to patients with cancer as of May 2024. Like patient records, trial information often includes unstructured information, such as plain text eligibility criteria and requires complex logical combinations of disease conditions, histologic subtypes, molecular markers and comorbidities. Thirdly, from a patient's perspective, due to the evolution of the disease and the need to avoid patient attrition from deteriorating clinical conditions, it is crucial that the time to inclusion and initiation of treatment in clinical trials is minimized to the shortest possible duration.\nFrom a practical perspective, addressing these challenges requires clinicians to follow a two-step process: first, they must screen for potential trial candidates based on key patient criteria such as tumor type, stage, mutations and availability within the patient's area of residence; then, they need to perform detailed one-on-one matches of all the patient's information with each candidate trial's eligibility criteria.\nSo far, computational support tools designed to simplify this process have focused on only one of these steps at a time. For the first step, systems have primarily used embedding techniques, where patient and trial text data are converted into a numerical representation space and matched based on approximate mathematical similarity10,11. For the second step, most tools focus on converting unstructured text from patient records and trial information into a tabular-like format. For instance, Criteria2Query uses machine learning and rule-based methods to parse inclusion and exclusion criteria into a structured format accessible via database queries12.\nOnly recently, with advances in generative AI, particularly Large Language Models (LLMs) like GPT-413, extracting and structuring information from medical documents has been drastically simplified\u00b94. The potential of LLMs has also been explored for matching patients to clinical trials based on comparing eligibility criteria to patient records 15. For instance, den Hamer et al.16 demonstrated that LLMs can accurately provide eligibility labels such as 'yes', 'no', or 'unknown' when given both trial information and patient data as input at the same time. In oncology, Wong et al. 17 extended this idea to account for complex logical conditions using a hierarchical matching procedure, showing that GPT-4 can excel at this task even without additional training. Fine-tuning LLMs on annotated trial data has markedly improved their performance even further. This approach has facilitated the development of a local, privacy-preserving model that closely rivals the capabilities of proprietary, large cloud-based LLMs18. Recently, the same research team created OncoLLM\u00b99, a new model that significantly reduces the performance gap with the current leading model, GPT-4.\nNevertheless, the aforementioned projects have several limitations:\nFirst, they tend to focus on either step one or step two of the process, rather than integrating both. Additionally, for step one, discrete criteria such as recruitment status or intervention type take on only discrete values (e.g. sex, location or recruitment status), which would more effectively be managed through direct selection or filtering rather than embedding-based approaches that rely on inexact similarity matches. Second, all current LLM-based methods heavily rely on narrowly engineered prompts,"}, {"title": null, "content": "which can be lengthy and cumbersome (Wong et al.17 report prompts of up to four pages). Third, due to the free-text nature in which eligibility criteria and patient information are processed by the model20, there is no guarantee that the responses will adhere strictly to the required criteria structure.\nWe herein present a fully end-to-end pipeline for clinical trial matching which we designed to overcome the aforementioned limitations. Our approach is based on two principles: using LLMs as central reasoning agents21 capable of taking actions and programmatically enforcing trial eligibility criteria as structured programming objects rather than plain free text, thereby ensuring the model consistently outputs validly annotated information 22.\nOur contributions are the following:\n1. To the best of our knowledge, we present the first truly end-to-end pipeline for clinical trial matching, starting with searching relevant trial candidates for a given patient from all cancer trials available world wide and ending with fully-annotated trial eligibility criteria for a relevant set of trials.\n2. We provide an extensive and profound benchmarking, encompassing 51 oncology cases and matching over 1,580 single trial criteria that have been annotated by five human experts. We provide evidence that our pipeline excels in both, reliably filtering relevant trials from tens of thousands and providing highly accurate one-on-one eligibility matches with criterion-level feedback and explanations for users.\n3. We demonstrate that LLMs can outperform medical doctors in clinical trial matching. Our findings reveal that nearly 40% of the initially contradictory answers between GPT-40 and physicians were accepted as valid responses upon refining the human baseline with criterion-level AI feedback, resulting in an overall criterion-level accuracy of 92.7% for our pipeline.\n4. By enforcing trial eligibility criteria as structured programming objects rather than relying on them as free-text inputs, we guarantee that the LLM always outputs precisely and validly annotated information."}, {"title": "Methods", "content": "Clinical Trial Composition\nData was sourced from Clinical Trials.gov on May 13, 2024, by filtering for the Condition/disease \"cancer\", yielding a total of 105,600 registered clinical trials, provided in a JavaScript-Object Notation (JSON) file. Subsequently, we programmatically filter each clinical trial by selecting relevant metadata, including fields like recruitment status, available centers (locations) or allowed disease conditions. Next, to allow the generation of vector embeddings from free text, we combine several metadata fields such as the brief and official titles, detailed trial descriptions and brief summaries into a structured plain text field.\nDatabase Generation\nWhen finding appropriate clinical trials for patients, physicians most often need to initially filter by specific, structured criteria like the locations of participating centers,"}, {"title": null, "content": "recruitment status or allowed disease conditions, while then also examining free text descriptions to precisely match patient conditions to exclusion and inclusion criteria. From a computational perspective, we are thus confronted with the fact that certain attributes, such as discrete metadata fields that have a set of discrete allowed options, require exact matches, whereas others need to be matched based on free text: This requires handling the issue of synonyms - such as recognizing that \"lung metastases\" and \"pulmonary metastases\" are equivalent where exact pattern matches are unsuitable.\nTo address these issues, we developed a hybrid database that effectively combines exact field matching with vector proximity search to find clinical trials that most closely correspond to patient descriptions in representation space. For the former, we employed a local instance of a No-SQL database23 (MongoDB), which offers several advantages in this context, including high scalability, a flexible schema design for sending nested requests, and robust performance when handling large datasets. Next, we generate vector embeddings - numerical representations - of the free, preprocessed text for each clinical trial using the \"BAAI/bge-large-en-v1.5\" embedding model locally, producing vector embeddings with a dimensionality of 768 from text with a maximum of up to 512 tokens each. As clinical trial information is most often considerably longer, we performed text splits, including a 50-character overlap to ensure comprehensive coverage and avoid information loss, such as by splitting text in the middle of a sentence. We store all text embeddings in a local collection of a vector database (ChromaDB24) for efficient similarity search, using cosine distance as the default search metric throughout our experiments.\nClinical Case Generation\nOur experiments are based on published synthetic cases by Benary et al. 25, which include ten fictional patient vignettes representing seven different tumor types, primarily lung adenocarcinoma (four cases), each annotated with various mutations (59 in total). To create a more realistic setting, we extended these cases to full medical EHR reports, using in-house original patient reports as templates, and including clinical descriptions of patient diagnoses, comorbidities, molecular information, short imaging descriptions from staging CT or MRI scans and patient history at different levels of detail. To ensure reliable matching of patients to existing clinical trials, we initially selected candidate trials through manual search or by utilizing those approved by a molecular tumor board from Lammert et al26. This led us to generating a total of 15 patient cases, which we refer to as base cases in the following. We then aligned the clinical case descriptions to either meet or contain conflicts with the respective trial eligibility criteria. This procedure was performed by first manually crafting patient reports based on medical expertise, then utilizing ChatGPT (GPT-4) for iterative refinement of style, grammar and language flow, leading to a total of 51 case vignettes. The final versions of these were evaluated for clinical realism, completeness and linguistic authenticity, and were approved by one physician with expertise in oncology before performing the experiments.\nTrial Matching Pipeline Specifications\nThe pipeline consists of two main components: the hybrid No-SQL-&-Vector database and an LLM that acts at its core to sequentially orchestrate database search, trial retrieval and finally trial matching with patient information. We utilized the \"GPT-40\" model through the OpenAI integration in Python. Model hyperparameters were kept at"}, {"title": null, "content": "their default settings. As the LLM operates programmatically to access the database, its outputs cannot be plain text, but need to be valid programmatic data types and occasionally also adhere to certain constraints, such as belonging to a fixed, discrete set of options (like current recruitment status of a trial). Otherwise, invalid requests would lead to failures in accessing the database. We therefore constrain model output types by setting type hints in pydantic27."}, {"title": "Results", "content": "Target Trial Identification Performance\nWe hypothesized that the process of filtering relevant trials on clinicaltrials.gov could be optimized using GPT-40 to write No-SQL database queries, thereby reducing the manual burden on physicians. We evaluate this idea on a subset of 15 base cases from our EHR collection, using either clinical trials from Lammert et al26 or potential target trials manually identified from clinicaltrials.gov. All prompts are provided in Supplementary Table 1.\nOur results indicate that using GPT-40 is sufficient to write a No-SQL query that filters all (15/15) potentially relevant trials - those that were preselected via manual search by humans for each patient base thus narrowing the initial pool of over 100,000 trials to a few hundred candidates (Figure 2, left). However, due to the variability in the number of trials for different conditions - such as rare mutations or tumor types yielding only a handful of trials, while others result in hundreds it is not always feasible to process all filtered trials directly through an LLM. We therefore employed vector similarity search to enrich trials with highest potential relevance by calculating the cosine distance between trial information and patient EHRs in a representation space. We selected the top k=50 trials with the lowest cosine distance. These trials were then processed by GPT-40, which was instructed to discard any irrelevant trials that falsely appeared relevant due to semantic overlap (Figure 2, right). As an example, consider a patient with \"non-small cell lung cancer\" and a clinical trial that is eligible only for \"small cell lung cancer.\" Despite the high semantic similarity (low cosine distance) between these terms, the patient would be ineligible for the trial. This discrepancy is accounted for by instructing GPT-40 to discard such trials, ensuring only relevant trials are selected.\nOur results demonstrate that this combined approach is highly effective, reducing the number of candidate trials from hundreds to 20-30. Notably, 14 out of the 15 target trials (93.3%) fall within the top 10 trial options, and 10 out of 15 are ranked within the top 5 trials (Figure 2, right). Additionally, we evaluated the potential benefits of incorporating reranking models. Although these models have shown promising results in optimizing text retrieval tasks and relevance sorting for efficiency 29, we did not observe significant improvements when applied to the full text of the trials using Cohere's rerank-english-v3.028. Therefore, we omitted reranking and considered the selected trials from the previous step as final.\nOur findings demonstrate the potential of combining No-SQL database and vector similarity search with GPT-40 to effectively reduce the number of trials to a few candidate options, ensuring that only the most relevant ones are prioritized for each patient."}, {"title": "Inclusion and Exclusion Criteria Accuracy", "content": "Next, we evaluated the criterion-level accuracy of GPT-40 across all 51 oncology EHRS for one target trial each, resulting in a total of 1,589 evaluable criteria, including both flat and nested ones. We show an example of how GPT-40 internally structures these eligibility criteria in Supplementary Table 2 and 3 and provide an example of GPT-40's full trial annotations including the unaltered eligibility criteria and criterion-level AI reasoning in Supplementary Table 4. For each criterion, the model was instructed to return one of three responses: True if the patient was eligible based on that criterion alone, False if the patient was not eligible, or \"unknown\" if the available data was inadequate to make a decision. In cases involving nested criteria, where the criterion \"header\" was not directly evaluable (e.g., \"All patients:\" or \"(At least) one of the following:\"), the model was additionally instructed to provide a global criterion result"}, {"title": null, "content": "that reflects the logical aggregation of the nested criteria. We use the majority answer from annotations, generated by five independent board-licensed physicians on all 1,589 criteria as a human baseline for comparing to the model's performance, which we highlight in Figure 3. Notably, as elaborated later, we do not speak of human annotations as ground truth. Our results demonstrate that GPT-40 achieves an overall - preliminary accuracy of 88.0% (calculated as the number of criteria where human and LLM decisions agree, divided by the total number of criteria, Figure 3), with similar performance when considering inclusion and exclusion criteria separately (87.5% and 88.6% respectively). All patient cases can be found in Supplementary Table 5.\nAdditionally, we find that GPT-40 achieved a 96.5% accuracy when focusing solely on True or False answers by either the model or human observers (\"True/False\"). The same observation is made when excluding model N/A answers only, which led to 96.5% of the answers being considered correct upon comparison to the human annotations (\"no AI N/A\"). We consider excluding model N/A outputs as an even better indicator of the model's performance as outputs that point out its inability to answer the criterion due to insufficient patient or trial information are less critical in real-world settings than incorrectly assigning ineligibility or eligibility.\nIn summary, we can show that besides finding relevant trial candidates, GPT-40 can next evaluate patient eligibility on these selected trials with very high criterion-level accuracy.\nRefining human baseline with AI feedback\nTo better understand the reasons behind differences in model versus human annotations at the criterion level, we re-evaluated all 191 cases where answers did not align. This process was performed by three of the original observers, who debated these discrepancies: We found that 39.3% (75 out of 191) of the initially conflicting answers were accepted after considering the model's reasoning and re-assessing the patient case and specific criteria. Following this refinement of human baseline, our trial matching pipeline showed a 4.7% improvement in performance, achieving an overall accuracy of 92.7%. Furthermore, our model consistently performed above the 97% accuracy threshold when focusing on True and False answers only (\"True/False\") or excluding model responses referring to missing information (\"no AI N/A\") from the measurement (Figure 3A).\nWe next investigated the types of refinements human annotators made upon reviewing model answers (Figure 3B): We found that a substantial number of corrections to human annotations were necessary when annotators initially considered a patient eligible or ineligible for a certain criterion, while relevant information to make a decision was absent in reality (74.7%, 56/75 and 8%, 6/75 respectively). More importantly however, we found scenarios in which human annotators corrected eligible to ineligible decisions (10.7%, 8/75) and ineligible to eligible (6.7%, 6/75), indicating instances where human annotators made substantial mistakes that could be corrected using AI feedback.\nIn summary, we herein show that GPT-40 can match, and likely even exceed the performance of qualified physicians in evaluating patient trial eligibility."}, {"title": "Discussion", "content": "In this work, we describe and validate a fully end-to-end approach for leveraging LLMs for clinical trial matching using oncology cases as an example. Overall, we demonstrate how GPT-40 can first effectively screen potential trial candidates from a collection of over one hundred thousand clinical trials registered on clinicaltrials.gov and secondly match selected candidates on a criterion-by-criterion basis to patient records."}, {"title": null, "content": "This has several real-world advantages: From a clinical perspective, physicians must filter out over 99.9% of irrelevant trials due to differing tumor types, disease stages, or distant locations. Additionally, they must consider what type of trials they are specifically looking for: Should they target a particular molecular alteration? Are they seeking trials for treatment-naive patients, or for those refractory to other therapies? Consequently, physicians are often forced to rely on ad hoc searches rather than structured methods to find suitable clinical trials. Given the inherent capabilities of state-of-the-art LLMs, we show that the process of filtering relevant trials by keyword-based search can be automated using GPT-40, which can itself write queries for a No-SQL database, guided without or with human supervision, such as \"Please find a Phase 1 trial for the patient in Germany\" or \"Could you please find a clinical trial for the patient's KRAS mutation (or all of the patient's mutations)?\"\nOur approach leverages the robust capabilities of GPT-40 in generating valid computer code, allowing programmatic access to trial databases with only optional human guidance. For instance, GPT-40 can request trials in specific locations or target particular mutations, or combinations of both if given as instruction from medical professionals. This makes our system highly scalable and flexible, extending its applicability beyond pre-selecting trials from a single center, as demonstrated by Gupta et al.19\nMoreover, we are convinced of the inherent reasoning capabilities of LLMs21, particularly looking toward future advancements, allowing them to handle complex logic internally. In contrast, the approach by Wong et al. 17 explicitly enforces LLMs to rewrite eligibility logic into structured Disjunctive Normal Form (DNF), imposing constraints on the model by limiting the combination of categories such as disease state, histology, and biomarkers through logical conditions (and, or, all, any, not etc.). This method also alters the original trial criteria, complicating human evaluation. Our approach ensures that the model's output can be mapped back to the original criteria on a one-by-one basis, with each criterion accompanied by a detailed chain of reasoning explaining the model's decision. This allows medical doctors to fact-check each decision of the model, ensuring explainability and trust. By understanding why the model reaches a particular conclusion and identifying potential errors we can better understand capabilities and limitations of current LLMs in managing EHR data.\nAdditionally, we demonstrate that our system's performance can match and under certain conditions even surpass that of human experts in criterion decision tasks. Although not directly comparable due to different trials and patient data (Gupta et al. utilize real-world de-identified EHR cases) and potential variations in how trial criteria are processed, our overall pipeline achieves an accuracy of 92.7% and 97.4% when excluding N/A samples. This exceeds the results others have previously achieved with GPT-4, reporting accuracies of 68% and 72%, respectively19.\nMoreover, our program-rather-than-prompt strategy ensures that responses consistently adhere to the required format, reducing the burden of finding optimal, often specific and narrow prompts. We therefore can guarantee that regardless of the length or complexity of the criteria, we receive validly annotated and unaltered criteria back from the LLM, which is not the case if criteria are handled as plain, free text.\nAlso, this approach allows the broad transferability of our system to other medical domains, with minimal need for adjustments, such as only the need for addressing domain-specific edge cases."}, {"title": null, "content": "Nevertheless, our study has several limitations: In real-world scenarios, under current regulatory restrictions, GPT-40 is not a suitable candidate due to its cloud-based nature, which necessitates transferring sensitive patient data to proprietary servers. Thus, we consider GPT-40 as a best-in-class model suitable for proof-of-concept purposes. We anticipate that local model solutions will catch up in performance in the near future, making them more suitable for clinical application. Additionally, real-world patient data will be required to fully validate applicability of our system, incorporating longer and even more diverse patient documents. For instance, laboratory values may be nested in spreadsheets, and imaging data might be separate, with all relevant patient information distributed across various documents. Moreover, we aim to evaluate the model's ability to accurately rank and prioritize the most relevant trials, enabling doctors to quickly identify the best options for their patients. Although our system currently provides scores based on the number and ratio of fulfilled eligibility criteria, we have not yet established a sophisticated measure for quantitative evaluation. We plan to develop and refine this using real-world data in the near future.\nDespite these challenges, our work demonstrates that an LLM can autonomously narrow down relevant trials from thousands to a manageable handful and accurately match these trials criterion by criterion. To our knowledge, our study is the closest in mirroring the real-world scenario of how medical doctors interact with clinical trial databases like clinicaltrials.gov. This evidence suggests significant potential for our approach, particularly as we show, for the first time, that AI feedback can enhance the performance of medical specialists in identifying suitable clinical trials for their patients."}]}