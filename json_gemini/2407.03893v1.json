{"title": "Do Generalised Classifiers really work on Human Drawn Sketches?", "authors": ["Hmrishav Bandyopadhyay", "Pinaki Nath Chowdhury", "Aneeshan Sain", "Subhadeep Koley", "Tao Xiang", "Ayan Kumar Bhunia", "Yi-Zhe Song"], "abstract": "This paper, for the first time, marries large foundation models with human sketch understanding. We demonstrate what this brings \u2013 a paradigm shift in terms of generalised sketch representation learning (e.g., classification). This generalisation happens on two fronts: (i) generalisation across unknown categories (i.e., open-set), and (ii) generalisation traversing abstraction levels (i.e., good and bad sketches), both being timely challenges that remain unsolved in the sketch literature. Our design is intuitive and centred around transferring the already stellar generalisation ability of CLIP to benefit generalised learning for sketches. We first \"condition\" the vanilla CLIP model by learning sketch-specific prompts using a novel auxiliary head of raster to vector sketch conversion. This importantly makes CLIP \"sketch-aware\". We then make CLIP acute to the inherently different sketch abstraction levels. This is achieved by learning a codebook of abstraction-specific prompt biases, a weighted combination of which facilitates the representation of sketches across abstraction levels - low abstract edge-maps, medium ab-", "sections": [{"title": "1 Introduction", "content": "The vision community is witnessing a paradigm shift in the face of large foundation models [51,54]. Instead of learning visual semantics from scratch, the rich semantics inherent to foundation models are explored to enrich visual learning, as in [3,24,39] for retrieval, and [14,43,73] for generation. The most salient advantage that foundation models bring is their generalisation ability [26,38,90,91], which made a significant impact on zero-shot and few-shot learning.\nIn this paper, we marry human sketches with foundation models to naturally tackle two of most significant bottlenecks in the sketch community, with little effort, piggybacking on generalisability of foundation models. First is the data scarcity problem of human drawn sketches - the largest sketch dataset (Quick-Draw [31]) contains 350 categories compared with the easily > 1000 categories for photos [18], which makes generalised learning for sketch even more pronounced a problem. The second is, although everyone can sketch, most people sketch differently as a result of varying drawing skills and diverse subjective interpretations - see Fig. 1 for distinctly different sketches of a \"bike\". These sketch-specific challenges call for a single model generalising along two axes - (i) across unseen categories for the first data scarcity challenge, and (ii) across abstraction levels for the second challenge of sketches exhibiting varying abstraction levels.\nSolving these challenges, we show that it all comes down to making CLIP [51] sketch-specific. For the former (data scarcity problem), we learn a set of continuous vectors (visual prompts) injected into CLIP's vision transformer en-coder. This enables CLIP (pre-trained on ~400M image-text pairs) to adapt to sketches while preserving its generalisation ability - resulting in a generalised sketch recogniser that works across unknown categories. More specifically, we first design two sets of visual prompts \u2013 shallow prompts injected into the initial layer of the CLIP transformer and deep prompts injected up to a depth of 9 layers. Keeping rest of CLIP frozen, we train these prompts on the extracted sketch and [category] embeddings (as class labels) from CLIP text encoder us-ing cross-entropy loss. Although shallow+deep prompts encourage CLIP to be-come \"sketch-aware\", they do not model any sketch-specific traits during training. Hence, we additionally use an auxiliary task of raster to vector sketch conversion that exploits the dual representation of sketch [7] to reinforce that awareness.\nThe latter challenge of dealing with sketch abstraction\u00b9 is less obvious and extends the status quo of what is possible with foundation models. While there is no consensus on what constitutes an abstract sketch [17], prior works fol-low: (i) number of strokes (more stroke \u2192 less abstract) [70], and (ii) sketch"}, {"title": "2 Related Works", "content": "Sketch for Visual Understanding: Sketches can depict visual concepts [33] using intuitive free-hand drawings, overcoming linguistic barriers often faced in text representations. Fine-grained in nature, a sketch is an attractive query medium for tasks like sketch-based image [15, 21,57,64] and 3D shape retrieval [77]. Creative sketches [27] encouraged sketch-based synthesis and editing of images [42,82,84], natural objects or scenes [13,25,72], faces [12], and animation frames [81]. Despite being expressed as monochrome lines on a 2D plane, sketches convey complex 3D structures and find use in 3D shape modelling [5, 30, 78, 89]. As an interactive medium a sketch is an important modality of input in vision tasks like sketch-based object detection [67], image-inpainting [82], representation learning [71], incremental learning [9], image-segmentation [34], etc. Beyond its discriminative [57] or representative [71] potential, sketch has also been employed for pictionary style gaming [8]. The success of sketch-based visual understanding leads us to propose a framework for recognising any (open-set [4]) free-hand drawing on unseen categories.\nSketch Classification: Early approaches to sketch understanding [63] extract hand-crafted features like shape primitives [48], bag-of-words [23], or Fischer Vec-tors [60] from raster (static pixel-map) sketches. Better representations were for-"}, {"title": "3 Background", "content": "CLIP: The generalisability of CLIP makes it a popular choice [55] for open-set vision-language tasks. Specifically, CLIP uses 2 independent encoders: (i) a ResNet [32] or Vision Transformer [20] image encoder, and (ii) a transformer-based [68] text encoder. The Vision Transformer image encoder F processes input images as r fixed-sized patches $I = {P_1,...,P_r}; P_j \\in R^{3 \\times h \\times w}$ that are embedded along with an extra learnable class token [19] $c_p$. These are then passed through transformer layers [68] with multi-head attention to obtain the visual features $f_p = F(I,c_p) \\in R^d$. The text input (say, n words) is pre-processed to word-embeddings $W_o = {w_i}_{i=1}^n$, from which the text transformer T extracts textual features as $t_o = T(W_o) \\in R^d$. Since CLIP maps cross-modal (image+text) fea-tures on the same embedding space, features of text-photo pairs have maximal similarity $sim(\\cdot)$ compared to features from unpaired (mismatched) samples after contrastive training. For zero-shot classification, textual prompts like 'a photo of a [category]' (from a list of K categories) are used to obtain category-specific textual features ${t_y}_{y=1}^K$. The probability of input photo I (with photo-feature $f_p$) belonging to yth category can be calculated as\n$P(y|I) = \\frac{exp (sim (f_p, t_y)/\\tau)}{\\sum_{j=1}^K exp (sim (f_p, t_j)/\\tau)}$                                                                                             (1)\nPrompt Learning: Prompt learning uses foundation models, like BERT [19] and CLIP [51], as knowledge bases from which useful information can be ex-tracted for downstream tasks [49]. Apart from using handcrafted prompts like 'a photo of a', recent methods like CoOp [91] and CoCoOp [90] learns n con-tinuous context vectors, ${v_1, ..., v_n}$, each having $v_j \\in R^{d_t}$ dimension word em-beddings. With base CLIP frozen, the continuous context vectors $v_j$ are learned by backpropagating gradients through the text T(\u00b7) encoder. Using word em-bedding for the k-th\u2018[category]', denoted as $w_k^o \\in R^{d_t}$, the prompt is con-structed as $[v_1,..., v_n, w_k^o]$, and passed to the transformer to obtain text feature $f_t = T([V_1,..., V_n, w_k^o])$. Recent works learn continuous context vectors (i.e., prompts) for text [91] and image [2] multimodal prompts [38]. Variations of prompt learning also include shallow vs. deep [74] prompts, depending on the layers where the context vectors are injected. We use deep prompt learning to adapt CLIP to design a generalised sketch classifier."}, {"title": "4 Proposed Methodology", "content": "In this paper, we build a generalised sketch classifier that works in an unseen setup. This \"unseen\u201d problem in sketch representation learning has two axes: (a) generalisation across unseen categories \u2013 train on 'cats' or 'dogs' (not on 'zebras') but evaluate on \u2018zebras'; and (b) generalisation across abstractions - a sketch can"}, {"title": "4.1 Generalisation Across Unseen Categories", "content": "Baseline Sketch Classifier: Sketch classification aims at predicting the cate-gory a given query-sketch belongs to. An input raster sketch $I_s \\in R^{3 \\times H \\times W}$ is encoded using a backbone feature extractor $f_s = F(I) \\in R^d$ like ResNet-101 [32] followed by mapping it to a K-dimensional vector $F_c : R^d \\rightarrow R^K$ that classifies $I_s$ into predefined K categories $f_c = F_c(f_s) \\in R^K$. Both backbone F(\u00b7) and classifier $F_c(.)$ are learned given ground-truth class $f_{c,k}$ as,\n$L_{CE} = -f_{c,k}log \\frac{exp(f_{c,k})}{\\sum_{j=1}^K exp(f_{c,j})}$                                                                                              (2)\nPrompt Learning to Adapt CLIP for Sketches: We use CLIP with ViT visual encoder [51] which extends the fixed set classifier $F_c$ in Eq. (2) into an open-set setup. We now learn J sketch prompts $\\nu_s = {\\nu_1,..., \\nu_{J-1}}$, where $\\nu_i \\in R^{5 \\times d_t}$. First, we divide the raster sketch into r fixed-sized patches $I_s = {s_1,..., s_r}$ where each patch $s_i \\in R^{3 \\times h \\times w}$ is embedded as $E_o = {e_i}_{i=1}^r$. Next, the learnable prompts are injected into each transformer block of CLIP vision transformer F(\u00b7) up to a specific depth J, as\n$[c, E_j, \\nu_j^v] = F_j ([c, E_{j-1}, \\nu_{j-1}^v])  \\vert_{j=1}^J$\n$[c, E_i, \\nu_i^v] = F_i ([c, E_{i-1}, \\nu_{i-1}^v]) \\vert_{i=J+1}$\n$f_s = ImageProj(c_o)$\n(3)\nwhere, $c_o$ is a pre-trained [CLS] token (see Sec. 3). To classify the visual feature $f_s \\in R^d$, we use handcrafted prompts like 'a photo of a [category]' that is encoded using CLIP text encoder T(\u00b7) into $f_t$ as in Eq. (1). However, (i) our input is 'sketch' not 'photo', and (ii) handcrafted prompts are sub-optimal compared to learnable prompts [91], $\\nu_t = {\\nu_1^t,..., \\nu_{J-1}^t }, \\nu_i \\in [][R^{5 \\times d_t}$. Hence, we inject prompts $v_t$ in T(.) up to depth J as,\n$[ \\_, w_i^t] = T_i ([\\nu_{i-1}^t, w_{i-1}^o])  \\vert_{i=1}^J$\n$[ \\nu_i^t, w_i^t] = T_i ([\\nu_{i-1}^t, w_{i-1}^o]) \\vert_{i=J+1}$\n$f_t = TextProj(w_j^t)$\n(4)\nwhere, $w_J^o$ is the word embedding of \u2018[category]'. Naively using learnable prompts $\\nu^t$ overfits to training/seen categories, lacking generalisation to un-seen categories [90]. Hence, we use a lightweight Meta-Net $\u03c0 = H(f_s)$ to"}, {"title": "4.2 Generalisation Across Sketch Abstractions", "content": "Pilot Study: Here we elaborate Fig. 1 (right) that examines generalisation of CLIP when abstractions vary from EM \u2192 TU\u2192 QD. We randomly select 40 classes common across QD, TU and EM - 20 seen classes to adapt CLIP via prompt learning (CoOp [91]), and 20 unseen classes for zero-shot evaluation. We observe: (i) training and evaluating on the same abstraction (QD, or TU, or EM) performs ~5.09% better, than training on one and jointly evaluating on QD + TU + EM. This drop signifies that a naive CLIP + prompt learning fails to generalise across abstractions. (ii) Jointly training on sketches from QD + TU + EM, only marginally improves accuracy by 2.30%. This highlights an even deeper problem - simply scaling the training data will likely not solve the varying sketch abstraction problem.\nOverview: Although we can easily sketch at varying abstraction levels, collect-ing precise abstraction annotation is difficult. Fig. 1 (left), shows that in general, QD doodles are more abstract than TU sketches, however precisely annotat-ing abstraction score from 0 \u2192 1 for every sample (e.g., \"bike\" in QD) is an"}, {"title": "4.3 Inference Pipleline", "content": "First, we use CLIP vision transformer F(\u00b7) and our learned sketch prompts $\\nu_s = {\\nu_1,..., \\nu_{J-1}}$, where $\\nu_i \\in R^{5 \\times d_t}$, to encode an input sketch $I_s$ into a visual feature $f_s = F(I_s;\\nu) \\in R^d$. Second, $f_s$ is simultaneously given to two modules: (i) a lightweight Meta-Net to predict instance-specific context, $\u03c0 = H(f_s)$, where $\u03c0 \\in R^{5 \\times d_t}$, and (ii) a codebook classifier $C_o : R^d \\rightarrow R^3$ to"}, {"title": "5 Experiments", "content": "Implementation Details: We use pre-trained CLIP with ViT-B/16 (vision transformer) as visual encoder F(.) and transformer-based text encoder T(\u00b7). For text encoder, we learn five 512-dimensional context vectors as prompt $v \\in R^{5 \\times 512}$. The class token for the k-th\u2018[category]' is given by $w_k^o \\in R^{512}$. We learn five 768-dimensional sketch prompts $v \\in R^{5 \\times 768}$. The learnable prompts are injected upto a depth J = 9, where ${\\nu_s^v, \\nu_s^t }$ are shallow prompts, while the deep prompts are ${\\nu_1^v, ..., \\nu_{J-1}^v} \\in R^{8 \\times 5 \\times 512}$ and ${\\nu_1^t,..., \\nu_{J-1}^t} \\in R^{8 \\times 5 \\times 512}$ for vision and text encoders, respectively. Although CLIP's weights are frozen during training, we fine-tune the layer-norm parameters for improved performance [55].\nOur method consisting of Meta-Net + Codebooks + layer-norm + vision prompts ($\u03bd_s^v$) + text prompts ($\u03bd_s^t$) is trained with Adam optimizer for 7 epochs with 1e-4 learning rate and 64 batch-size.\nDatasets: We use sketches from QuickDraw [31] and TU-Berlin [23] along with Edgemaps [11] of TU-Berlin Extended [86]. Ranked from highest to lowest ab-straction, QuickDraw has 50M sketches across 345 categories, TU-Berlin has 20K sketches from 250 categories, and the Edgemaps are generated using [11] from 204K images across 250 categories in TU-Berlin extended. For few-shot training, we randomly pick 10 sketches per class from a list of 125 classes common to all three datasets and reserve the remaining ones (220 for QuickDraw, 125 for TU-Berlin, and 125 for Edgemaps) for zero-shot inference. Generating Edgemaps from complex scene images (with noisy backgrounds) leads to noisy sketches. We filter images with high classification scores (higher score \u21d2 less noisier back-ground) using pre-trained zero-shot CLIP (details in supplementary).\nEvaluation Setup: We evaluate our algorithm on two fronts: (i) few-shot ac-curacy: where we train our model on 10 randomly sampled sketches from each of 125 common classes in all three datasets and evaluate them on previously unseen samples from the same class list. (ii) zero-shot accuracy: where we use our previously trained few-shot model and evaluate on unseen samples from new classes in these datasets. This difficult evaluation setup helps us understand (a) how well the model generalises to unseen classes i.e., how much did we adapt the generalisation potential of CLIP for sketch recognition, and (b) how well the model trained on seen categories, generalises across varying abstractions using codebook vectors and their mix-up. We also evaluate the adaptability of our network, by replacing CLIP-backbone with FLAVA [65]."}, {"title": "5.1 Sketch Recognition", "content": "We report few-shot and zero-shot recognition results of our algorithm on QD, TU, and EM in Tab. 1 using average accuracy across all datasets for reference.\nFew-Shot Recognition: We obtain a Top-1 accuracy of 66.72%, 76.96%, and 45.20% on EM, TU, and QD respectively with our algorithm, beating SOTA MaPLe by an average margin of 6.66%. Works on shallow language prompts like CoOp (45.59%) and CoCoOp (46.75%) and shallow visual prompts like VPT-"}, {"title": "5.2 Ablation", "content": "We ablate various components and hyperparameters in Tab. 2. (i) Varying prompt depth J affects recognition accuracy, where J = 1 (shallow prompts) drops it to 66.87% and 59.39% in seen and unseen classes, respectively. (ii) Varying length of language prompt ($\\nu \\in R^{5 \\times 512}$) from 5 to 2, 10 and 20 drops accuracy to 74.52%, 74.21% and 75.54% respectively. (ii) Removing the Meta-Net drops accuracy by 4.94%, particularly zero-shot accuracy by 6.32%. (iii) Removing $L_{s\\rightarrow v}$ drops accuracy by 3.26%, due to lack of sketch-specific traits.\n(iv) Removing codebook learning that models abstraction drops accuracy by 3.87%. (v) Removing codebook mixup that models continuous abstraction drops accuracy by 3.06%. (vi) Removing layer-norm drops accuracy by 2.03%."}, {"title": "5.3 Recognising the Degree of Abstraction", "content": "The codebook classifier predicts abstraction score as a softmax normalised proba-bility distribution on 3 coarse abstraction levels: $A_l$ (EM), $A_m$ (TU), $A_h$ (QD)."}, {"title": "5.4 Evaluating On Unseen Abstraction Levels", "content": "To judge our model's generalisability, we simulate unseen sketch-abstraction lev-els using CLIPasso [70]. Specifically, we test on unseen CLIPasso generated sketches having 12, 16, 24 and 32 strokes representing decreasing order of ab-straction.\nFig. 7 shows our predicted abstraction scores (a) to align with the notion of abstraction (c) in CLIPasso (more strokes \u21d2 less abstract). A high recognition accuracy verifies our generalisability not only for abstraction prediction, but also for classification of unseen sketches."}, {"title": "5.5 Interpreting Learned Abstraction Prompts", "content": "We aim to interpret the influence of abstraction prompt $\u03b7$ on learned text prompts (vt) to understand how it instils abstraction-aware knowledge into our sketch-classifier. Recent studies [91] reveal that learned context tokens (vt) usu-ally converge close to their initial embedding corresponding to the prompt of 'a sketch of a [category]'. Influencing $v^t$ with the codebook vector (\u03b7) how-ever, pushes the embedding towards somewhat different words in the euclidean space [38] which are sketch-specific in nature. For instance, when analysing sketches at lower abstraction levels, we found cases where the euclidean dis-tance from our prompt embedding to a word 'artistic' is equivalent to that from other irrelevant words like 'box', 'camera', etc. This confusion dismisses naive measures like Euclidean distance as a tool for interpreting the influence of ab-straction prompt \u03b7, necessitating further investigation for alternatives in future."}, {"title": "6 Conclusion", "content": "We extend the notion of a generalised classifier from photos to sketches. To-wards this goal, we adapt CLIP (with open-set generalisation) for sketches by learning prompts for both the vision and language branches. In addition, to learn sketch-specific traits, we employ an auxiliary raster \u2192 vector sketch recon-struction loss. Finally, we generalise CLIP across varying sketch abstractions.\nAs sketches lack precise abstraction annotation, we assign coarse-level scores to Edgemaps as low abstraction, TU-Berlin sketches as medium, and QuickDraw doodles as high abstraction. We employ codebook learning and mixup to learn"}]}