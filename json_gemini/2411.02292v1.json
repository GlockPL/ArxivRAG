{"title": "ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence", "authors": ["Wenjie Mei", "Dongzhe Zheng", "Shihua Li"], "abstract": "Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can process data without the limitation of time intervals. They have advantages in learning and understanding the evolution of complex real dynamics. Many previous works have focused on NODEs in concise forms, while numerous physical systems taking straightforward forms, in fact, belong to their more complex quasi-classes, thus appealing to a class of general NODEs with high scalability and flexibility to model those systems. This, however, may result in intricate nonlinear properties. In this paper, we introduce ControlSynth Neural ODEs (CSODEs). We show that despite their highly nonlinear nature, convergence can be guaranteed via tractable linear inequalities. In the composition of CSODEs, we introduce an extra control term for learning the potential simultaneous capture of dynamics at different scales, which could be particularly useful for partial differential equation-formulated systems. Finally, we compare several representative NNs with CSODEs on important physical dynamics under the inductive biases of CSODES, and illustrate that CSODEs have better learning and predictive abilities in these settings.", "sections": [{"title": "Introduction", "content": "Neural ODEs (NODEs) [4] were developed from the limiting cases of continuous recurrent networks and residual networks and exhibit non-negligible advantages in data incorporation and modeling unknown dynamics of complex systems, for instance. Their continuous nature makes them particularly beneficial to learning and predicting the dynamical behavior of complex physical systems, which are often difficult to realize due to sophisticated internal and external factors for the systems.\nStarting from the introduction of NODEs, many types of variants of NODEs have been studied (see, e.g., [6, 19, 17, 15]). Nevertheless, there are rare studies concerning highly scalable and flexible dynamics that also present complex nonlinear natures, bringing difficulties in their modeling and analyses. No in-detail research attention has been paid to the scalability of depth and structure in NODEs despite numerous physical systems in the real world having inscrutable dynamics and compositions. To fill in this gap, we propose ControlSynth Neural ODEs (CSODEs), in whose structure another sub-network is also incorporated for enlarging the dexterity of the composition and controlling the evolution of the state. Different from most of the existing methods and experiments, we focus on widely investigated physical models, with known state evolution under necessary conditions. Subsequently, we will show that the proposed NODEs are effective in learning and understanding those models.\nOur contributions are mainly composed of the novel structure of CSODEs, their convergence guarantees, and the comparative experiments among CSODEs, several representative NODEs, and their"}, {"title": "ControlSynth Neural Ordinary Differential Equations", "content": "We begin by introducing the form of CSODEs as follows:\n$\\dot{x}(t) = A_0 x(t) + \\sum_{j=1}^{M} A_j f_j(W_j x(t)) + g(u(t))$, (1)\nwhere $x_t := x(t) \\in \\mathbb{R}^n$ is the state vector; the matrices $A_*$ are with approximate dimensions; $W_*$ are weight matrices; the input $u_t := u(t) \\in \\mathbb{U} \\subset \\mathbb{R}^M$, $u \\in \\mathcal{L}_m$ (refer to Appendix C.1); $f_j = [f_j^1 \\dots f_j^{k_j}]^T$ ($f_j : \\mathbb{R}^{k_j} \\to \\mathbb{R}^{k_j}$) and $g : \\mathbb{U} \\to \\mathbb{R}^n$ are the functions ensuring the existence of the solutions of the neural network (NN) (1) at least locally in time, and $g = [g_1 \\dots g_n]^T$; w.l.o.g., the time $t$ is set as $t > 0$.\nCSODES extend the concept of Neural ODEs, which are typically expressed as $\\dot{x}(t) = f(x(t))$, where $f$ is a neural network. CSODEs incorporate control inputs $u(t)$ and create a combination of subnetworks $\\sum_{j=1}^{M} A_j f_j(W_j x(t))$. This formulation enhances ex-pressiveness and adaptability to complex systems with external inputs. CSODEs provide a more universal framework and improve upon Neural ODEs by offering greater flexibility, interpretability through separated linear and nonlinear terms, and natural integration with techniques in control theory.\nFor simplicity, in the experiments (see Section 6), we select two common NNs as the specific examples of the function $g$: Traditional Multilayer Perceptron (MLP) and two-layer Convolutional Neural Network (CNN), and tanh as the used activation functions: a particular subclass of the functions $f_j$. Note that in the case that $g(u)$ represents an MLP, there are many different types of neurons, for example, functional neurons [21], and in CSODEs (1) there may exist $f \\in L^1(\\mathbb{X}, \\mathcal{A}, \\mu)$, where $(\\mathbb{X}, \\mathcal{A}, \\mu)$ denotes a $\\sigma$-finite measure space."}, {"title": "Related Work", "content": "SONODES, ANODES, and NCDEs vs CSODES SONODEs [17] are particularly suitable for learning and predicting the second-order models, and ANODES [6] are useful for cases where the state evolution does not play an important role. In contrast, many real models of second-order or even higher orders can be transformed into the common first-order but emerge numerous nonlinearities that are difficult to accurately treat by NODEs and their variants. Despite the intricate structure of CSODES, they can be equipped with a significant convergence attribute. The experiments show that our CSODEs are more appropriate for model scaling and natural dynamics that exhibit complex nonlinear properties. Although both Neural CDEs (NCDEs) [12] and CSODEs extend NODES through control mechanisms, their architectural philosophies differ. NCDEs introduce control through path-valued data driving the vector field, following $\\dot{x}(t) = f(x(t))$. In contrast, CSODES propose a more sophisticated structure combining multiple subnetworks with a dedicated control term $g(u(t))$. This design not only provides theoretical convergence guarantees but also enables CSODES"}, {"title": "Theoretical Results: Convergence Analysis", "content": "Consider the CSODEs given in Equation (1), where $f_j : \\mathbb{R}^{k_j} \\to \\mathbb{R}^{k_j}$ are nonlinear activation functions. For them, an imposed condition on $f_j^i$ (the $i$-th element of the vector-valued $f_j$) is presented as follows:\nAssumption 1 For any $i \\in \\{1, ..., k_j\\}$ and $j \\in \\{1, ..., M\\}$, $sf_j^i(s) > 0$ for all $s \\in \\mathbb{R}\\{0\\}.\nRemark 1 Assumption 1 applies to many activation functions, such as tanh and parametric ReLU. It picks up the activation functions passing through the origin and the quadrants I and III. For more explanations, the reader is referred to Appendix B.1.\nIn this study, to analyze the convergence property of the NN (1), we first define the concept of convergence:\nDefinition 1 The model (1) is convergent if it admits a unique bounded solution for $t \\in \\mathbb{R}$ that is globally asymptotically stable (GAS).\nIn order to investigate the convergence, two properties have to be satisfied, that is, the boundedness and the GAS guarantees of the solution $x^*$ for (1). In this respect, two assumptions are given as follows.\nAssumption 2 Assume that the functions $f_j^i$ are continuous and strictly increasing for any $i \\in \\{1,...,k_j\\}$ and $j \\in \\{1, ..., M\\}.\nAssumption 2 aligns with CSODE's structure, reflecting continuity and monotonicity of activation functions. This relates to model dynamics and is satisfied by most common activations.\nIn the analysis of convergence, one needs to study two models in the same form but with different initial conditions and their contracting properties. To that end, along with (1), we consider the model $\\dot{y}(t) = A_0 y(t) + \\sum_{j=1}^{M} A_j f_j(W_j y(t)) + g(u(t))$ with the same input but different initial conditions $y(0) \\in \\mathbb{R}^n$. Let $\\xi := y - x$. Then the corresponding error system is\n$\\dot{\\xi} = A_0 \\xi + \\sum_{j=1}^{M} A_j p_j(x, \\xi)$, (2)"}, {"title": "Preliminary Experiments", "content": "Convergence and Stability Convergence, an important attribute showcasing a model's learning ability, refers to its capability to consistently approach the theoretical optimal solution throughout the learning process over iterations. To validate the convergence and stability of CSODEs, we design an experiment that involves learning simple spiral trajectories, chosen for their fundamental complexity which all models should ideally handle. In this experiment, we compare CSODEs and NODEs, both based on the Latent ODE structure [4]. This setup provides a fundamental baseline for assessing convergence and ensures a fair comparison, enabling each model to demonstrate its learning capabilities under comparable conditions without bias toward specific structural advantages. The Mean Absolute Error (MAE) loss function, which measures the average of the squares of the differences between the estimated trajectories and the true trajectories, is used as the indicator. We"}, {"title": "Complex Systems Time Series Prediction Experiments", "content": "In this section, we experimentally analyze the performance of CSODEs, based on NODEs, compared to other models based on NODEs and traditional time series forecasting methods in extrapolative"}, {"title": "Model Architectures for Experiments", "content": "To comprehensively evaluate the performance of different model architectures in dynamic system modeling, this study compares several representative NODEs-based models, including the traditional NODES [4], ANODEs [6], SONODEs [17], and our proposed CSODEs. Additionally, these models are compared with traditional MLPs [22] and Recurrent Neural Networks (RNNs) [7].\nNotable that while CSODE can be integrated with the Latent ODE framework, as demonstrated in our preliminary experiments (in Section 5), we opted to conduct our main experiments without this integration. This decision was driven by our observation that NODE itself effectively models long-sequence dynamical systems, allowing us to evaluate CSODE's performance more directly by eliminating potential influences from additional Latent ODE components like encoders and decoders.\nAll models within our experiment that are based on the NODE framework utilize a uniform MLP to parameterize the core dynamical system $\\dot{y} = f(y)$. These models employ the forward Euler method for integration, though other numerical solvers like Dopri5 are also viable options (detailed performance comparisons of different solvers can be found in Appendix I.1). The MLP serves as the update function, iterating to act on the evolution of dynamic system states. Specifically, the MLP receives the current state as input and outputs the next state, thereby modeling the change in state over time. Meanwhile, the RNN employs the same structure as the MLP, using current input and the previous timestep's hidden state to update the current timestep's hidden state. To ensure a fair comparison, we finely align the number of parameters across all models, including the parameterization of the core dynamical system and other components (such as the subnetwork in the CSODE model and the augmented dimensions in the ANODE model). All designs ensure that the total number of parameters remains consistent within \u00b11%.\nThe unique feature of CSODEs lies in the introduction of an additional subnetwork to model the control term $g(u(t))$, extending the original dynamical system. We employ a unified auxiliary NN to model the changing rate of $g(u(t))$, with the initial value $g(u_0)$ set to be the same as $y_0$. These subnetwork structures are similar to the MLP or use simplified single-layer networks. We also introduce the CSODE-Adapt variant, replacing the function representing the changing rate of the control term with a network consisting of two convolutional layers, to explore the scalability and flexibility of CSODEs. Notably, for fair comparison of fundamental architectures, we used the standard Adam optimizer in our main experiments, though we note that alternative optimizers like L-BFGS could further enhance CSODE's performance (see Appendix I.2 for more details).\nAt the current research forefront, some researchers have proposed integrating NODEs with Trans-former layers (TLODEs) [27]. TLODEs can be considered a special case of CSODEs, where the Transformer layers implement the function $f(\\cdot)$. Building on TLODEs, we introduce the control term $g(\\cdot)$, creating a more complete CSODEs structure with Transformer layers (CSTLODEs). Considering the widespread application of Transformers in time-series prediction, we conduct comparative experiments between the Transformer, TLODE, and CSTLODE models. However, due to significant architectural differences between Transformers and MLPs and RNNs, directly incorporating them into the main experiment might introduce additional confounding factors, deviating from the theoretical discussion of general NODEs. Therefore, to maintain the focus and clarity of the main text, we have placed the experimental results and discussions related to TLODEs and CSTLODEs in Appendix F."}, {"title": "Experimental Tasks and Datasets Description", "content": "In this subsection, we detail the experimental tasks and datasets used to explore the application of NNs in simulating various dynamic systems. For more comprehensive simulation and experimental details, see Appendix D.\nModeling Hindmarsh-Rose Neuron Dynamics In this task, we explore the application of NNs in simulating the Hindmarsh-Rose neuron model [10]. We validate their potential in simulating complex neuronal dynamics and assess their prospects in broader neuroscience and biophysical research. The"}, {"title": "Metrics for Assessing Prediction Accuracy", "content": "We employ the following metrics to compare time series predictions and ground truth:\nMean Squared Error (MSE): Calculates the mean of squared differences between predicted and actual values, sensitive to large errors. Used in all three tasks."}, {"title": "Experimental Results and Analysis", "content": "Experiments are conducted ten times for each task, and the average metrics are presented in Table 2, the reader is referred to Appendix E for more statistical details. NNs in Group B, based on NODE and its variants, outperform traditional models in Group A, such as MLP and RNN, in predicting complex physical dynamic systems. Traditional models struggle with complex time dependencies and nonlinear dynamics, while models based on differential equations demonstrate greater adaptability and accuracy.\nWithin Group B, the CSODE model surpasses other models due to its control elements that enhance precision and adaptability to changes in initial conditions and system parameters.\nMoreover, the CSODE-Adapt model (Group C) integrates convolutional layers, enhancing its appli-cability and effectiveness, particularly in dynamic systems with significant spatial features, such as Reaction-Diffusion systems. This model performs better than all others, highlighting the flexibility and highly customizable structure of the CSODEs and its advantages and potential in predicting complex physical dynamic systems."}, {"title": "Comparison with Observation-aware Baselines", "content": "We conducted supplementary experiments (CharacterTrajectories and PhysioNet Sepsis Prediction) with CSODE-Adapt following the experimental setup in [12], maintaining similar network structures, parameter counts, and optimization methods. Overall, CSODE performs slightly worse than Neural CDE in irregular observation experiments but better in other time-series-related tasks.\nAs shown in Table 3, for the Character Trajectories task with irregular observations, Neural CDE achieves the best performance across all missing data ratios, followed by CSODE and then ODE-RNN. In the PhysioNet Sepsis Prediction task, without considering observation intensity (OI), CSODE-Adapt achieves the highest AUC value of 0.871, while with OI consideration, Neural CDE performs best with an AUC of 0.885, followed closely by CSODE-Adapt and CSODE. For the Reaction-Diffusion modeling task, CSODE-Adapt demonstrates superior performance across all metrics (MSE, MAE, CD), while CSODE and Neural CDE show comparable performance, both outperforming ODE-RNN. Overall, while Neural CDE exhibits advantages in handling irregular observations, CSODE-Adapt shows competitive or superior performance in tasks requiring complex dynamic system modeling and clinical prediction, demonstrating its effectiveness as a general-purpose time series modeling tool."}, {"title": "Model Scaling Experiment", "content": "In the experiments above, CSODEs demonstrate significant superiority over traditional NODES and their variants, under the maintenance of the same number of parameters and architectural configuration. Based on these findings, our scaling experiments focus primarily on exploring the scalability and architectural robustness of CSODEs, without further comparison to other models.\nWe also observe changes in system performance after scaling CSODEs. To maintain consistency in the experiments, each sub-network is configured with two dense layers. We select the Reaction-Diffusion task in Section 6 as an example to explore the impact of increasing the number of sub-networks and the width of each sub-network on system performance. Specifically, the network widths, which refer to the number of hidden dimensions in the dense layers, are set at 128, 256, 512, 1024, and 2048. The number of sub-networks, equivalent to $M$ in NNs (1), is set at 1, 2, 3, 4, and 5.\nThe experimental design varies network width and number of sub-networks. We employ a learning rate formula: $\\text{learning rate} = \\frac{k}{W \\times \\sqrt{N}}$, where $k$ is a constant, $W$ is the network width, and $N$ is the number of sub-networks. This adjusts the learning rate based on width and moderates it for sub-network count to handle complexity. For instance, with a width of 1024 and three sub-networks, the learning rate is $\\frac{0.1}{1024 \\times \\sqrt{3}}$. We use the Adam optimizer for training.\nIn terms of overall performance, the heatmap in Figure 6 shows that under the CSODEs, increasing the network width and number of subnetworks results in stable and enhanced overall performance. Additionally, the scatter plot demonstrates that increasing the number of subnetworks significantly improves the model's generalization ability, with training and validation performance showing a"}, {"title": "Conclusion", "content": "In this work, we analyzed the learning and predicting abilities of Neural ODEs (NODEs). A class of new NODEs: ControlSynth Neural ODEs (CSODEs) was proposed, which has a complex structure but high scalability and dexterity. We started by investigating the convergence property of CSODES and comparing them with traditional NODEs in the context of generalization, extrapolation, and computational performance. We also used a variety of representative NODE models and CSODEs to model several important real physical dynamics and compared their prediction accuracy.\nWe presented that although the control subjects (NODEs, Augmented Neural ODEs (ANODEs), and Second Order NODEs (SONODEs)) do not have the physics information-based inductive biases specifically owned by our CSODEs, they can learn and understand complex dynamics in practice. In particular, SONODEs own inductive biases for second-order ODE-formulated dynamics, while the ones of CSODEs mainly are for first-order models with high nonlinear natures, scalability, and flexibility that belong to a broad class of real systems. The experimental results on dynamical systems governed by the Hindmarsh-Rose Model, Reaction-Diffusion Model, and Shallow Water Equations preliminarily demonstrate the superiority of our ControlSynth ODEs (CSODEs) in learning and predicting highly nonlinear dynamics, even when represented as partial differential equations.\nLimitations and Future Work The effectiveness of inductive biases of CSODES varies which, depending on the specific application scenarios, may not be preferable; in the evolution of partial differential equations, there often exists mutual interference between different scales (e.g., spatial and temporal scales), which, however, is approximately learned by CSODEs. We believe this work could provide promising avenues for future studies, including: For enlarging the use scope of inductive biases of CSODEs in complex dynamics and reflecting the mutual intervention between scales, one can consider a more general NODE: $\\dot{x} = f(x, u)$, with guaranteed stability and convergence. This allows a greater scope of dynamics and thus may prompt the improvement of the accuracy of modeling and predicting systems with more complex structures and behaviors. Furthermore, in practice, CSODES are more sensitive to learning rate selections due to their more complex architectures. Our preliminary investigation in Appendix I.3 reveals the significant impact of hyperparameter adjustments on model performance. Building upon these initial findings, future research will examine this sensitivity more thoroughly and consider methods like adaptive learning rate adjustment and model simplification to address it. Finally, this work maintained standard optimization settings for fair comparison, future research could explore specialized training algorithms that leverage CSODE's structural properties and theoretical foundations."}, {"title": "Broader Impact", "content": "The introduction of ControlSynth Neural ODEs (CSODEs) represents an advancement in the field of deep learning and its applications in physics, engineering, and robotics. CSODEs offer a highly scalable and flexible framework for modeling complex nonlinear dynamical systems, enabling the learning and prediction of intricate behaviors from data, which has implications across various domains.\nIn the realm of physics, CSODEs provide a powerful tool for studying complex physical processes in areas such as meteorology, fluid dynamics, and materials science. By leveraging their highly expandable network structure and control term, CSODEs may capture the dynamics of systems with elusive internal and external factors, overcoming the limitations of traditional mathematical modeling approaches. This opens up new avenues for understanding and predicting the behavior of these complex systems.\nMoreover, the theoretical convergence guarantees of CSODEs lay a solid foundation for applications of neural networks in physical modeling. Despite their high nonlinearity, the convergence of CSODEs can be ensured through tractable linear inequalities. This enhances the interpretability and reliability of neural network models, facilitating their deployment in engineering and safety-critical domains where provable convergence is crucial.\nThe control term introduced in CSODEs also enables the learning of multi-scale dynamics, which is particularly relevant for systems described by partial differential equations. By simultaneously capturing dynamics at different spatial and temporal scales, CSODEs expand the practicality of neural networks to multi-scale physical problems.\nThe performance of CSODEs in learning and predicting the dynamics of physical systems, as demonstrated through the comparative experiments, highlights their alignment with physical laws and their ability to effectively uncover underlying patterns in data. This motivates the incorporation of appropriate inductive biases when designing neural network architectures for scientific and engineering applications.\nIn the field of robotics, CSODEs have the potential to change the design and control of robotic systems. They can be applied to disturbance observation, pose estimation, and the design of control strategies. By learning the dynamics of disturbances, CSODEs enable robots to accurately estimate their states and make appropriate compensations in the face of environmental perturbations. For pose estimation, CSODEs can learn the pose dynamics directly from sensor data, establishing adaptive pose estimators that enhance accuracy and robustness in complex environments."}, {"title": "Further Technical Details for Section 4", "content": "Further Implications of Assumption 1\nWith a reordering of nonlinearities and their decomposition, there exists an index $w \\in \\{0, ..., M\\}$ such that for all $s \\in \\{1, ..., w\\}$ and $i \\in \\{1, ...,k_s\\}$, $\\lim_{v \\to \\pm \\infty} f_s^i(v) = \\pm \\infty$. Also, there exists $\\zeta \\in \\{\\omega, ..., M\\}$ such that for all $s \\in \\{1, ..., \\xi\\}$, $i \\in \\{1, ..., k_s\\}$, we have $\\lim_{v \\to \\pm \\infty} \\int_0^r f_s^i(r) dr = +\\infty$. Here, $w = 0$ implies that all nonlinearities are bounded. The sets with the upper bound smaller than the lower bound are regarded as empty sets, e.g., $s \\in \\{1,0\\} = (\\emptyset$ in the case that $w = 0$.\nAn Example of Activation Functions Satisfying Assumption 3\nIn practice, Assumption 3 can be verified based on chosen $f_j$ (e.g., ReLU, Sigmoid, tanh). For example, for the tanh function,\n$f_j(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$, $\\mid f(x)\\mid < 1$, $\\mid f_j(x) - f_j(y)\\mid < \\mid x - y\\mid$.\nIt is Lipschitz continuous with constant 1 and satisfies:\np_j (x, \\xi)^\\top p_j(x, \\xi) \\leq \\xi^\\top W_j^\\top W_j \\xi, f_j(W_j \\xi)^\\top f_j(W_j \\xi) \\leq \\xi^\\top W_j^\\top W_j \\xi\nIn this case, we can choose:\n$S = H_3 = I, S_1 = S_2 = S_3 = H_1 = H_2 = H_3 = 0$.\nIf $S_1, S_2, S_3, H_1, H_2, H_3 > 0$, the restrictions are relaxed, verifying that Assumption 3 is less strict than Lipschitz continuity.\nVerification Experiment on Assumption 3\nTo verify the actual existence of matrices in Assumption 3, we designed a supplementary experiment to validate the trained CSODE models, involving three dynamical system time series prediction tasks in the main"}, {"title": "Formulations of Matrix Inequalities in Theorem 1", "content": "The linear matrix inequalities in Theorem 1 are formulated as follows.\n$\\mathbb{P} + \\sum_{j=1}^{M} \\tilde{A}_j^\\top \\mathbb{P}_j \\tilde{A}_j > 0; \\quad \\mathbb{Q} = \\mathbb{Q}^\\top \\leq 0; \\quad \\Xi + \\frac{1}{\\varsigma} \\sum_{j=1}^M \\mathbb{\\Gamma}_j + \\sum_{s=1}^\\mathbb{S} + \\sum_{s=1}^{M-1} \\sum_{r=s+1}^M \\mathbb{Y}_{s,r} > 0$.\n$\\mathbb{P} + \\sum_{j=1}^{M} \\mathbb{\\tilde{A}_j^{\\top} \\Gamma_j \\tilde{A}_j} > 0; \\quad \\mathbb{Q} = \\mathbb{Q}^\\top \\leq 0; \\quad \\Xi_j + \\mathbb{\\Gamma}_j + \\Xi - \\mathbb{\\Lambda}^{\\top} \\mathbb{W}\\big( \\sum_{j=1}^\\mathbb{M} \\frac{1}{\\varsigma} \\mathbb{\\Lambda}^{\\top} \\mathbb{W} \\big) \\mathbb{W} \\mathbb{W}^\\top \\mathbb{W}\\frac{\\theta}{\\varsigma} \\le 0; \\quad \\mathbb{\\Gamma}_j - \\varsigma S - \\frac{\\theta}{\\varsigma} \\mathbb{H}_1 > 0; \\quad \\mathbb{\\Omega}_j - \\varsigma \\mathbb{H}_2 > 0; \\quad \\mathbb{\\hat{\\Omega}} - \\varsigma \\mathbb{H}_2 > 0;$\n$\\sum_{j=1}^{M} (\\mathbb{\\Gamma} - \\varsigma S - \\frac{\\theta}{\\varsigma} \\mathbb{H}) + (\\mathbb{\\hat{\\Gamma}} - \\varsigma S - \\frac{\\theta}{\\varsigma} \\mathbb{H}) > 0; \\qquad \\mathbb{\\tilde{A}_j} = \\hat{\\mathbb{A}_j} + \\sum_{j=1}^M \\frac{\\theta}{\\varsigma} \\mathbb{H}_1$ + (W_j(A_j + A_jT) + \\sum_{j=1}^M \\frac{\\theta}{\\varsigma} \\mathbb{H}_1$ + (W_j A_jT) \n$"}, {"title": "Convergence Study in ControlSynth Neural ODE Model Scaling", "content": "In the context of scaling the CSODE framework, the impact of network configurations such as subnetwork count and width on model performance is critical. Several key observations are performed:\nIncreasing Sub-network Count at Fixed Width: When the width is held constant, for instance at 512, an increase in the number of sub-networks initially raises the loss due to the augmented complexity of the network as the left figure in Figure 8 shown. However, as training advances, the model effectively adapts to this increased complexity, ultimately achieving a lower level of loss than configurations with fewer sub-networks. This indicates that a higher number of sub-networks can enhance the model's depth of learning and capability for feature extraction, leading to improved performance after extensive training.\nIncreasing Width at Fixed Sub-network Count: With the sub-network count fixed, such as at three, expanding the width shows a marked improvement in learning capabilities, as the right figure in Figure 8 shows. The additional parameters afforded by a broader network aid in capturing more complex features of the data, which accelerates the reduction in both training and validation losses. This setup not only speeds up the learning process but also tends to improve generalization across different tasks.\nOverall Convergence Trends: Whether increasing the sub-network count or the width, all configurations eventually exhibit a trend toward convergence in losses. This suggests that, with suitable adjustments in training duration and parameter settings, various network configuration designs of CSODES are capable of uncovering and effectively learning the intrinsic patterns hidden in the data.\nSummary: These observations align well with the goals of the CSODE framework, which is designed to enhance NODE system scalability while ensuring strong convergence and high performance."}, {"title": "Solver, Optimizers, and Hyperparameters", "content": "In the training and evaluation of neural differential equation models, the selection of solvers, optimizers, and hyperparameters plays a pivotal role in determining both the performance and efficiency of the models. This section delves into the theoretical underpinnings of these components and provides an in-depth analysis of their impacts through comprehensive experimental studies.\nImpact of Solver Choice\nNumerical solvers are essential for approximating solutions to differential equations, which form the backbone of Neural Ordinary Differential Equations (NODEs) and their variants. The choice of solver affects both the accuracy and computational efficiency of the model. Solvers can be broadly categorized into explicit and implicit methods, with further subdivisions based on their order of accuracy and adaptive step-size capabilities."}, {"title": "Ability to Adapt to Different Spatial Scales", "content": "In practice, CSODE demonstrates superior robustness when handling dynamical system datasets across different spatial scales, as evidenced by the stability of its performance metrics.\nWhile all models show comparable performance in the original scale setting (NODE: 5.3%, ANODE: 4.6%, CSODE: 5.3%), significant differences emerge when the spatial domain is expanded. At 5 times the original scale, NODE and ANODE show notable degradation in stability (7.8% and 7.1% respectively), while CSODE maintains a relatively stable performance (5.5%). This trend becomes even more pronounced at 10 times the original scale, where NODE and ANODE further deteriorate to 8.8% and 7.3%, while CSODE maintains its stability at 5.5%.\nParticularly noteworthy is the stability drop from the original to 10x scale: CSODE shows only a 0.2% increase in standard deviation, compared to significantly larger increases for NODE (3.5%) and ANODE (2.7%). This remarkable stability across different spatial scales suggests that CSODE's architecture, particularly its control term component, provides inherent advantages in handling spatial scale variations in dynamical systems.\nThese results indicate that CSODE not only performs well in standard settings but also maintains consistent performance across varying spatial scales, making it particularly suitable for applications where spatial scale invariance is crucial."}]}