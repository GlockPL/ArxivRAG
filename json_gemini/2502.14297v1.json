{"title": "An Evaluation of Sakana's AI Scientist for Autonomous Research: Wishful Thinking or an Emerging Reality Towards \u2018Artificial General Research Intelligence' (AGRI)?", "authors": ["JOERAN BEEL", "MIN-YEN KAN", "MORITZ BAUMGART"], "abstract": "A major milestone toward Artificial General Intelligence (AGI) and even Super Intelligence is Al's ability to autonomously conduct research-a concept we term Artificial General Research Intelligence (AGRI). If machines could generate hypotheses, design and execute experiments, analyze results, write research papers, and refine knowledge without human intervention, it would transform science. Recently, Sakana.ai introduced the AI Scientist, a system claiming to automate the entire research lifecycle, generating both excitement and skepticism.\nWe independently evaluated Sakana's AI Scientist\u00b9 and found it to be a milestone in AI-driven research. While it streamlines some aspects, it falls short of expectations. Literature reviews are weak, nearly half of the experiments failed, and manuscripts sometimes contain hallucinated results. Most notably, users must provide an experimental pipeline as input, limiting the AI Scientist's autonomy in designing and conducting research. Despite its limitations, the AI Scientist advances research automation. Many reviewers or instructors conducting only a superficial assessment may not recognize its output as AI-generated, instead attributing it to an unmotivated undergraduate. The system produces research papers with minimal human effort and low cost. Our analysis suggests generating a paper costs 615 with 3.5 hours of human involvement, making it significantly faster than humans. Compared to AI capabilities from a few years ago, this marks notable progress toward AGRI.\nThe rise of AI-driven research systems necessitates urgent discussion within Information Retrieval (IR) and broader scientific communities. Enhancing literature retrieval, citation validation, and evaluation benchmarks could improve Al-generated research reliability. We propose concrete steps, including AGRI-specific benchmarks, refined peer review mechanisms, and standardized attribution frameworks. Whether AGRI becomes a stepping stone to AGI depends on how the academic and Al communities shape its development and governance.", "sections": [{"title": "1 Introduction", "content": "In autumn 2024, Sakana.ai, a Tokyo-based start-up that has raised $200 million in funding\u00b2, announced the \u201cAI Scientist\u201d.\nWith the AI Scientist, Sakana boldly promised the \"beginning of a new era in scientific discovery\" [11]. The open-source\u00b3 AI Scientist is supposed to \"automate the entire research lifecycle\"; i.e. it generates research ideas, designs and conducts experiments, analyzes results, writes research papers, and finally even reviews them - essentially automating the daily work of millions of researchers worldwide. According to Sakana, the AI Scientist produces research papers for approximately $15 each. Sakana acknowledges \"occasional flaws\" and explains further limitations in a pre-print manuscript [7]. Yet, based on Sakana's released information, most readers will understand that the AI Scientist appears to be a fully-functional system. Especially considering Sakana's claims that \"It is worth noting that the [AI Scientist] can autonomously run the entire life cycle of machine learning research without any human intervention except for initial preparation\" [12] and that the peer review system works \u201cwith near-human accuracy\" [11].\nSakana's AI Scientist is not the only AI tool that aims to support or even replace significant aspects of scientific work. A rapidly expanding body of research explores Al's role in scientific discovery, with recent preprints examining its impact on literature retrieval, idea generation, and automated experimental design. Several studies indicate that large language models (LLMs) can already generate research ideas comparable to those of human scientists [14, 17, 18], with some findings suggesting AI may even surpass human creativity [10, 13]. Google has framed Al as ushering in a \"new era of scientific discovery\" [9], while specialized workshops such as \"Towards Agentic AI for Science\" at ICLR 2025 highlight the field's growing momentum.\nNotably, just one day the submission of our current manuscript for review to a major IR conference, and hours before uploading it to arXiv, Google announced the AI Co-Scientist, indicating the emerging interest in autonomous research agents, or, as we term it 'Artificial General Research Intelligence' (AGRI), i.e. artificial intelligence that has not yet reached the level of general intelligence, but intelligence that is capable of conducting research that is indistinguishable from human research. In this context it is also interesting to note that [6] submitted a paper on AI-driven idea generation to ICLR 2025, which was rejected after an intense review process involving 45 discussion posts - described by the conference chair as \u201cfilled with drama\u201d. This case exemplifies the unease among researchers and reviewers who face the prospect of AI encroaching on their roles, raising questions about the future of human-led scientific inquiry.\nDespite the large body of work, the AI Scientist is probably the tool that has made the boldest promises, yet. And, possibly because of this, it has gained the most attention, both within the academic community and among the broader Al enthusiast audience. As of early February 2025, Sakana's pre-print manuscript gained more than 100 citations7, their GitHub repository received 8.9k stars and it was forked over 1.3k times. Prominent AI content creators on YouTube and X, each with hundreds of thousands of followers [4, 15], enthusiastically discuss the AI Scientist.\nNotably, the most positive reviewers of the AI Scientist, e.g., [4], have not tested the system themselves but rely solely on Sakana's information. Meanwhile, users in online forums and GitHub issue trackers report difficulties in setting up the AI Scientist or encountering technical issues. Other discussions 10 use the AI Scientist's release as a"}, {"title": "Evaluation of Sakana's AI Scientist", "content": "broader reflection on Al's role in scientific discovery. Some reviews provide in-depth analyses\u00b9\u00b9, examining its source code and generated papers. However, to the best of our knowledge, a comprehensive, independent assessment based on direct experimentation has yet to be conducted.\nThe AI Scientist is a machine learning-driven system, making it highly relevant to the machine learning research community. However, its impact extends beyond ML, with profound implications for the Information Retrieval (IR) community, which motivates us to write this perspective paper. While a recent perspective paper [19] provided valuable insights into how LLMs are transforming traditional IR tasks, we must also consider a broader question: What happens when LLMs move beyond assisting IR research and begin to conduct it autonomously? This possibility is no longer theoretical \u2013 AI systems are increasingly capable of generating research ideas, designing experiments, and even evaluating scientific work.\nIn this paper, we take the next step in this discussion by providing an independent, systematic assessment of the AI Scientist's capabilities, limitations, and trajectory. Our goal is not only to evaluate its functionality but to highlight its potential role in reshaping scientific inquiry, particularly within information retrieval. By analyzing its performance across idea generation, experimentation, and manuscript production and review, we offer a comprehensive perspective on what Al-driven research tools can and cannot - achieve today and may achieve in the future.\nOur study focuses exclusively on Sakana's AI Scientist rather than other comparable tools due to its particularly bold claims and the significant attention it has garnered within the research community. While numerous AI-driven tools support various aspects of scientific research-including AI-assisted literature retrieval and summarization (Semantic Scholar, Elicit, Scite.ai), automated experiment design (IBM's Watson Discovery), and research paper generation - the AI Scientist stands out for its promise to fully automate the entire research lifecycle, from idea generation to peer review. This ambitious goal, combined with its open-source nature and extensive media coverage, makes it a uniquely compelling subject for independent evaluation. Other tools, while valuable in their respective domains, typically focus on specific tasks rather than claiming end-to-end automation of scientific inquiry. By concentrating on the AI Scientist, we aim to provide a detailed and critical analysis of its capabilities and limitations without diluting our focus across multiple tools with varying objectives and architectures.\nOur findings reveal a system that, while far from replacing human researchers, demonstrates an ability to automate significant parts of the research process. The AI Scientist does not yet fulfill its promises, struggling with methodological soundness, experimental execution, and literature retrieval. However, these limitations are technical hurdles rather than fundamental barriers - challenges that will likely be addressed as AI-driven research systems evolve.\nFor the IR community, the emergence of such tools presents both an opportunity and a challenge. On one hand, they offer new ways to automate literature retrieval, citation analysis, and experimental design, potentially advancing the field. On the other, they raise critical questions about the role of human researchers, the nature of scientific discovery, and the reliability and generalizability of AI-generated knowledge.\nThe time to engage with these technologies is now. Whether by integrating them into research workflows, critically evaluating their outputs, or contributing to their development, IR researchers must take an active role in shaping the future of AI-driven science. These systems are poised to transform how knowledge is produced and disseminated, and the IR community is uniquely positioned to lead this transformation."}, {"title": "2 Al Scientist: Functionality and Evaluation", "content": "This section includes an overview of the features and architecture of the AI Scientist, case study snippets, critiques and recommendations for use."}, {"title": "2.1 Setup and Installation", "content": "All coding and experiment execution were carried out by the third author, a third-year computer science Bachelor's student with basic machine learning knowledge but strong Python proficiency. A typical user of the AI Scientist is likely to have greater expertise, such as that of a trained researcher or machine learning engineer. Thus, our setup time and usability assessments should be considered lower bounds. We have made our experimental codebase that replicates our experimentation in this paper available on GitHub: https://code.isg.beel.org/ISG-Siegen/the-ai-scientist-reproduced.\nThe repository includes a Singularity container definition file to facilitate full reproducibility and modifications in our experiments.\nWe installed the AI Scientist on two computers and found the setup to be relatively straightforward, contrary to some reports on the Web 12. The entire process took approximately five hours, including minor troubleshooting. For more details, we refer the interested reader to our GitHub repository.\nWe first tested and prototyped the AI Scientist on a consumer laptop13. The main experiments were then conducted on our university's computing cluster14. Although the AI Scientist provides an experimental pipeline optimized for deep learning with PyTorch and an NVIDIA GPU (CUDA), our experiments used a simple recommendation algorithm, FunkSVD, that runs on a CPU.\nThe AI Scientist requires a foundation large language model (LLM) to function. At the time of our experiments, it supported OpenAI's gpt-40-2024-05-13, multiple models from Anthropic's Claude series, DeepSeek Coder V2, and Meta's Llama 3.1. We opted for gpt-40-2024-05-13. Since then, Sakana has expanded support to additional models."}, {"title": "2.2 Preparing for the experiments", "content": "We initially assumed the AI Scientist could autonomously conduct research based solely on a prompt. However, it requires a user-defined \"template,\" which significantly limits its autonomy. Such a template consist of several elements, described in the following."}, {"title": "2.3 Idea Generation", "content": "Once provided with seed ideas and an experimental template, the AI Scientist generated the following ten research ideas:"}, {"title": "2.4 Conducting Experiments", "content": "The AI Scientist executes research ideas by modifying the user-provided experimental pipeline using Aider17, an LLM-driven coding assistant that iterates on experiment.py up to five times. This reliance on Aider adds complexity, as errors in one system propagate to the other. While the AI Scientist does introduce variation, it remains highly dependent on the provided framework, reworking the predefined templates rather than devising novel methodologies.\nOur code analysis suggests minimal changes per iteration. Our template had 255 lines (6,260 characters), with Aider adding, on average, 529 characters (+8%) in the first iteration, followed by 118, 83, 66, and 21 characters in subsequent iterations .\nThe AI Scientist struggled with both implementation and methodological correctness. It failed to execute five of twelve ideas due to unresolved coding errors, often cycling through flawed versions of the same code. Even when producing executable code, outputs were frequently unreliable. For instance, in our e-fold cross-validation experiment, the AI Scientist intended to test $e = \\{2, 3, 4, 5\\}$ but erroneously kept e fixed at 2, invalidating results. It also failed to re-run the baseline, making its novel approach appear superior an impossible outcome because e-fold cross-validation by design cannot lead to better performance but only energy savings (with, ideally, comparable performance) [1, 3, 8].\nThe apparent performance gain may have been caused by statistical noise or incorrect baseline execution."}, {"title": "2.5 Manuscripts", "content": "The AI Scientist generated manuscripts for all seven successfully conducted experiments, with lengths ranging from six to eight pages (median: seven). Most manuscripts (6 of 7) included tables or charts, though quality varied. References were scarce, with a median of five citations per paper (range: 2-9), mostly outdated; only five of the total 34 citations were from 2020 or later. Foundational works, such as Goodfellow et al.'s Deep Learning textbook, were frequently cited, suggesting weak retrieval of relevant literature.\nStructural issues were common: four manuscripts contained missing or misplaced figures, incomplete sections (e.g., \"Conclusions Here\" placeholder), duplicates, or repeated figures. Related work sections were particularly poor, often with irrelevant citations. In our e-fold cross-validation experiment, the AI Scientist failed to cite existing papers on the topic that used exactly the same terminology (e-fold cross validation), despite their availability on Semantic Scholar18.\nResults sections were often inadequate or misleading. Metrics were presented in plain text or simple charts, with no confidence intervals or p-values. Four of seven manuscripts contained incorrect or hallucinated numerical results, with discrepancies in hyperparameters and performance metrics. In one case, an energy-efficient machine learning experiment claimed an RMSE improvement while reducing training time from 116 to 115 seconds but increasing memory usage, failing to justify how this aligned with energy efficiency. Similar logical inconsistencies appeared in other experiments.\nGiven these flaws, these manuscripts would likely be rejected at reputable conferences or journals. Even at lower-tier venues, the lack of proper citations and unreliable results would be problematic. However, an undergraduate submitting such work for a course project might still pass if the instructor does not scrutinize the code or experiments."}, {"title": "2.6 Review Functionality", "content": "The AI Scientist includes a reviewer agent for Al-assisted peer review, evaluated by us on both Al-generated and human-written manuscripts. To generate a review, the AI Scientist extracts text from a provided PDF but cannot interpret figures, tables, or supplementary material. It outputs a structured review (review.txt) containing a summary, identified strengths and weaknesses, improvement questions, numerical ratings for originality, clarity, significance, soundness, presentation, and contribution, plus a binary accept/reject recommendation.\nReviews of AI-Generated Manuscripts. For all seven manuscripts generated by the AI Scientist, its own review system recommended rejection. One common critique was that the research was based on a single, small dataset (MovieLens 100k). This criticism is justified, and the use of the single dataset not an intrinsic flaw of the AI Scientist itself, but rather a consequence of our experimental pipeline, which only included this dataset. Had our pipeline contained multiple datasets, the AI Scientist likely would have incorporated them. That being said, in an ideal scenario, the AI Scientist would be capable of retrieving and utilizing additional datasets autonomously, rather than being entirely dependent on user-supplied input.\nThe AI Scientist highlighted valid methodological concerns, such as weak theoretical justification and potential bi-ases, but overlooked critical flaws, including redundant text, formatting errors, missing sections, and flawed experimental results. While human reviews vary in quality - from superficial comments to detailed critiques \u2013 the AI Scientist's reviews were consistently structured, though lacking depth. Despite this, they appeared plausible enough that editors and conference chairs might not immediately recognize them as AI-generated. While less rigorous than strong human reviews, they were still more substantive than the brief, low-effort reviews that sometimes occur in the conference reviewing process.\nReviews of Human-Written Manuscripts. We tested19 the AI Scientist's reviewing capabilities with ten human-written papers on OpenReview.net20, five that were accepted and five rejected. Reviews created by the AI Scientist followed the same structured format as for AI-generated manuscripts, including summaries, critiques, improvement suggestions, and numerical ratings.\nWhile indicative, the results were striking: the AI Scientist rejected 9 out of 10 papers, including four that had been accepted by human reviewers, while recommending acceptance for only one, which had been rejected on OpenReview. This suggests a strong conservative bias and misalignment with human judgment. Despite this, its reviews raised valid concerns about methodology and biases but lacked depth, often missing broader contributions and argumentation.\nA key challenge is determining how to evaluate AI-generated reviews, given the inconsistencies of human peer review. Should human judgment be the benchmark? Peer review is known for subjectivity, reviewer disagreements, and biases. An Al system that diverges from human reviewers may still highlight valid weaknesses or apply overly rigid criteria.\nThis raises a fundamental question: should AI peer review aim to mimic human reviewers or provide a more systematic alternative? If the former, the AI Scientist falls short. If the latter, human evaluation alone may be an insufficient"}, {"title": "2.7 Costs and Effort", "content": "When assessing the AI Scientist's potential, cost is a crucial factor - and surprisingly, it remains low. The total cost for our main experiments, which included generating 10 ideas, running experiments for 7 ideas (from 12 total, including 2 seed ideas, of which 5 failed), and producing 7 manuscripts with reviews, amounted to only $42 USD an average of $6 per manuscript. Given that our experiments were less complex than typical recommender systems research or those conducted by Sakana, their claim that a full research paper can be produced for around $15 seems realistic.\nHowever, human labor is still required. Setting up the AI Scientist took approximately 5 hours, while implementing the experimental template required an additional 15 hours. It must be noted that if we were to re-implement similar pipelines, the required time would decrease due to the learning curve. If we were to implement more complex experiments, time would increase. Beyond this, we spent about 10 more hours brainstorming initial seed ideas, reviewing outputs, and refining results. Excluding the initial setup time, this totals 25 hours of human effort -approximately 3.5 hours per manuscript - mainly from an undergraduate student.\nIn summary, producing a research paper even of relatively low quality at an average cost of $6 to $15 and 3.5 hours of human effort, is remarkable. By comparison, we estimate that an undergraduate student would require at least 20 to 40 hours, while an experienced researcher - who would likely be reluctant to produce work of similar quality - might take 10 to 20 hours. Thus, the AI Scientist operates approximately 3 to 11 times faster than a human researcher at negligible costs."}, {"title": "2.8 Limitations", "content": "Our own evaluation had limitations: we used a single dataset (MovieLens-100k) and one research domain (Green Recommender Systems), relied on fixed seed ideas and user-defined templates, and required manual adaptations for hardware compatibility and troubleshooting. While these constraints ensured a controlled assessment, they may affect generalizability and reproducibility. Despite this, we believe our findings remain meaningful."}, {"title": "3 Future Work and Recommendations", "content": "We outline several actionable recommendations for the Information Retrieval (IR) community, emphasizing practical steps for researchers, conference organizers, journal editors, and institutions. While these recommendations are primarily targeted at IR, they also reflect broader best practices in academia."}, {"title": "3.1 Researchers should familiarize themselves with research-assistive Al tools", "content": "At present, research-assistive tools like the AI Scientist hold little practical value. In contrast, more general AI tools, such as ChatGPT for text generation and GitHub Copilot for programming, already provide substantial benefits in research, improving writing, coding, and brainstorming efficiency. Researchers who ignore these readily available AI-powered assistants are already at a disadvantage compared to their peers who integrate them into their workflows."}, {"title": "3.2 Updated Guidelines for Authors and Researchers on the Use of Al", "content": "The ACM and other publishers provide guidelines on using generative AI in manuscript preparation. Currently, ACM permits AI to assist in manuscript writing, including generating text, images, tables, and code21. However, the potential for Al to conduct research and autonomously generate entire manuscripts and experimental workflows has not yet been addressed by these policies.\nWe urge the IR community and publishers, such as ACM, to initiate discussions on how AI-driven research tools, like the AI Scientist, should best be integrated into scientific workflows. Specifically, the community should establish policies as to what extent AI-generated research can be submitted to conferences such as SIGIR.\nAt present, our assessment suggests that tools like the AI Scientist are not yet reliable enough to contribute meaningfully to research. Consequently, manuscripts generated entirely by AI should not be accepted for publication at this time. However, this stance will need to be revisited regularly as AI capabilities evolve rapidly.\nWe recommend that the academic community, particularly editors of journals and members of steering or ethics committees, take an active role in revising guidelines regarding the use of AI tools in research. These revisions should be developed in collaboration with experts who possess in-depth knowledge of the capabilities and limitations of Al systems, such as the AI Scientist. Additionally, input from stakeholders like peer reviewers, program chairs of conferences, and other members of the academic publishing ecosystem will be essential to ensure that these tools are integrated effectively while maintaining academic integrity and quality control."}, {"title": "3.3 Addressing the Risks of Al-Generated Assignments and Homeworks", "content": "The ability of AI tools such as the AI Scientist to generate well-structured academic papers that are hard to distinguish from student submissions poses an immediate challenge for universities. These systems can produce coherent, plausible reports that, especially under time constraints, may not be easily recognized as AI-generated by instructors. This presents a fundamental risk to academic integrity, as students can now generate entire assignments without engaging"}, {"title": "3.4 Benchmarking Al-Driven Research Agents in Information Retrieval", "content": "We propose developing structured benchmarks to evaluate AI-driven research agents, recognizing the need for broader community discussion to establish consensus on assessment criteria. Such benchmarks are essential to determine when AI agents achieve \"Scientific AGI,\" the point at which they can autonomously conduct research at human-level quality.\nA critical area is Al-powered peer review. These systems generate structured reviews, yet their reliability remains untested. Benchmarks could include expert annotations, inter-reviewer agreement analyses, and alignment with evaluation criteria to compare AI and human reviews.\nAl-driven idea generation also requires assessment. Evaluations could compare Al-generated ideas with expert proposals, analyzing acceptance rates and citation impact. Experimental AI agents should be benchmarked for their ability to implement methodologies, configure parameters, and ensure reproducibility. Failure analysis is crucial to detect methodological flaws.\nManuscript generation benchmarks should go beyond fluency, assessing argument structuring, literature integration, and empirical accuracy. Automated inconsistency detection, including hallucinated citations, is essential. Blinded peer review comparisons and authorship attribution frameworks may help distinguish Al contributions.\nOther AI-assisted tasks, including literature review and dataset selection, require evaluation. AI benchmarks could include retrieval relevance metrics, citation analysis, and bias detection to meet expert standards.\nDeveloping a comprehensive benchmarking suite requires collaboration across academia, industry, and publishers. Standardized datasets, evaluation protocols, and performance metrics would help ensure scientific rigor and meaningful Al integration into research. Establishing these benchmarks will provide clear indicators of Al progress toward autonomous, human-level scientific inquiry."}, {"title": "3.5 Release of Al Research Logs for Transparency", "content": "A key advantage of AI-generated research is its ability to systematically log every step of the process. Unlike human researchers, Al systems can capture all experimental iterations, hyperparameter adjustments, and decision rationales"}, {"title": "3.6 Pilot Projects and Competitions", "content": "The IR community can learn from initiatives such as ICLR 2025 and NeurIPS, which have explored generative Al's role in peer review and manuscript submission through pilot projects. Expanding on these efforts, targeted pilot projects can further investigate Al's impact on scientific research.\nOne promising initiative is a competition assessing AI-generated research. We propose integrating such a competition into a major conference like SIGIR, where Al-generated papers would be evaluated alongside human-authored research through a blind peer-review process. This would provide an unbiased comparison of Al's ability to conduct rigorous scientific inquiry.\nWhile current Al research tools remain limited, a competition of this nature could become viable within the next few years. Early planning and discussion within the community would ensure that appropriate evaluation frameworks and ethical considerations are established ahead of time, enabling a structured and fair assessment of AI-driven scientific contributions."}, {"title": "3.7 Al-Generated Content Tagging: Research Attribution Markup Language (RAML)", "content": "As AI-generated research becomes more prevalent, distinguishing between human-authored and AI-assisted content is critical for transparency, academic integrity, and reproducibility. Existing solutions for image watermarking and document metadata do not adequately address the need for systematic attribution in text-based research outputs. Therefore, we propose the Research Attribution Markup Language (RAML), a standardized method to tag AI-generated content within academic manuscripts.\nRAML is a JSON-based schema embedded within research papers, either as inline annotations or in a separate metadata file. It enables authors and reviewers to track which sections of a manuscript were generated, modified, or influenced by AI.\nRAML defines various levels of Al involvement: Generated, meaning the content was fully created by AI; Edited, where the content was generated by AI but reviewed and revised by a human; and Suggested, in which the content was suggested by AI but primarily authored by a human. RAML also includes metadata on the Al model used, such as GPT-4, Claude, or Gemini, along with version and parameters. Additionally, it supports version control by storing AI-generated drafts, change logs, and prompts used."}, {"title": "3.8 Research Process Markup Language (RPML)", "content": "The AI Scientist faces difficulties in tracking detailed experimental metadata. Modifications made by its LLM-based coding assistant (Aider) are often undocumented, leading to reproducibility issues and making it hard to trace experimental changes.\nWe propose developing a standardized Research Process Markup Language (RPML) that captures all aspects of the research workflow in a structured format. RPML-based on a schema such as JSON or XML-should record experiment setups, code versions, container images, dataset versions, hyperparameters, and citation contexts automatically, ensuring full traceability and reproducibility."}, {"title": "3.9 Standardized Dataset Annotation and Integration", "content": "Tools like the AI Scientist rely on user-supplied datasets that often lack detailed, standardized metadata, limiting its ability to autonomously select high-quality relevant datasets for experiments.\nWe propose to encourage dataset providers (e.g., Hugging Face, Kaggle, UCI Machine Learning Repository) to adopt a standardized annotation schema. This schema should include comprehensive metadata such as domain applicability, data quality scores, versioning, benchmark results, and recommended use cases. With uniform metadata, the AI Scientist can automatically query and integrate datasets that best fit the experimental requirements."}, {"title": "3.10 Code Repositories Integration", "content": "The AI Scientist lacks access to recent baseline implementations from the state-of-the-art literature, making it challenging to benchmark its innovations against current best practices.\nWe propose to establish direct integrations with platforms such as Papers WithCode, GitHub, and Code Ocean. Standardized APIs and metadata from these platforms can be used to retrieve relevant source code repositories and performance metrics, allowing the AI Scientist to automatically download, run, and compare these baselines against its own results."}, {"title": "3.11 Participatory Strategic Retreat", "content": "The ideas presented in our paper should be seen as an initial stimulus for discussion. To explore the future of generative AI in IR research, key stakeholders in the community must be involved-particularly leading experts in the IR field, as well as young researchers whose future will be significantly shaped by generative AI. We believe that a strategic, multi-day workshop or retreat such as ones held by the Dagstuhl or NII Shonan seminar series - would be the right setting for engendering an initial, larger-scale discussion where such in-depth discussions can inform the writing of"}, {"title": "4 Discussion and Conclusion", "content": "Our study confirms that the AI Scientist represents a significant step toward automating scientific research, yet our evaluation reveals that its current capabilities remain far from fulfilling its ambitious promises. While the AI Scientist can generate research ideas, it only sometimes successfully conducts experiments, and many of its generated ideas are not truly novel. It heavily depends on user input, struggles with methodological soundness, and lacks the ability to critically assess its own results. In its present form, the AI Scientist is, at best, an advanced research assistant that requires a lot of supervision, rather than an independent scientific agent.\nA key takeaway from our study is that we believe AI systems like the AI Scientist will meaningfully contribute to the scientific critique process in the near future, benefiting both authors and reviewers. Researchers could leverage AI-generated feedback to iteratively refine their work through adversarial loops in which an AI challenges their hypotheses, methodologies, and analyses. Similarly, peer reviewers could use AI-assisted prompts to enhance their critiques, ensuring consistency and depth. Some academic conferences, such as ICLR, have already incorporated AI-generated review suggestions into their workflows. However, our findings indicate that AI-generated reviews by the AI Scientist often focus on surface-level critiques, while failing to detect deeper methodological flaws. This may make the job of second-level reviewers (such as area chairs or meta-reviewers) more important, as Al-generated reviews may lack depth but be well-formatted and directly address the review form's criteria. Such consolidators will have to look beyond surface-level analyses to judge whether a work holds promise.\nAnother promising role for the AI Scientist lies in replication and validation. Reproducibility is a cornerstone of scientific credibility, as highlighted by recent initiatives in recommendation systems (e.g., the Best Paper Award at RecSys) and NLP (e.g., ReproHum initiatives). AI could be employed to establish proof of work, ensuring that findings are replicable and that experimental procedures are verifiable. To achieve this, AI-driven replication efforts must prioritize transparency through open-weight and open-data models. Additionally, AI can support automated metadata creation and tagging (e.g., Dublin Core, Open Data Initiative) and facilitate structured data deposits in version-controlled repositories, providing verifiable provenance for scientific claims. These mechanisms incentivize transparent and reproducible research, potentially leading to new prestige metrics or blockchain-based certification systems for replication integrity.\nDespite these potential benefits, integrating Al into scientific research presents ethical and practical challenges. Al models inherit biases from historical data and cannot independently distinguish between scientific quality and consensus. This limitation raises concerns that AI tools might reinforce outdated methodologies or amplify biases in peer review and publishing. Additionally, junior researchers may develop an overreliance on AI-generated suggestions, leading to automation bias that stifles innovation and independent thought. Research further indicates that AI can enhance scientific productivity, as shown in [16], yet its impact on researcher satisfaction remains complex. In one study, while AI-assisted workflows increased efficiency, 82% of scientists reported lower job satisfaction afterwards. This paradox exemplifies a broader dilemma-if AI can outperform humans in key research tasks, what remains for human scientists to do? Addressing these risks requires AI-assisted workflows that actively encourage users to critically reassess assumptions and explore alternative perspectives.\nOne pressing concern is the potential for mass AI-generated paper submissions. Tools like the AI Scientist could enable researchers to generate large volumes of seemingly novel but low-value papers by tweaking existing codebases"}]}