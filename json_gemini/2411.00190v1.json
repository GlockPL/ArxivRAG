{"title": "Monitoring fairness in machine learning models that predict patient mortality\nin the ICU", "authors": ["TEMPEST A. VAN SCHAIK", "XINGGANG LIU", "LOUIS ATALLAH", "OMAR BADAWI"], "abstract": "This work proposes a fairness monitoring approach for machine learning models that predict patient mortality in the ICU. We inves-\ntigate how well models perform for patient groups with different race, sex and medical diagnoses. We investigate Documentation bias\nin clinical measurement, showing how fairness analysis provides a more detailed and insightful comparison of model performance\nthan traditional accuracy metrics alone.", "sections": [{"title": "1 INTRODUCTION", "content": "Benchmarking against other Intensive Care Units (ICUs) can provide ICU staff and hospital managers with a broader\nview and clearer perspectives of targets for improvement. Benchmarking can include comparing an ICU's actual perfor-\nmance with predicted performance. The increased interoperability of medical devices, electronic health records (EHRs)\nand information systems has improved the acquisition and presentation of data to healthcare professionals. This data\nhas enabled the training of predictive models. However, this plethora of data sources has also introduced new risks\nthat societal bias will lead to machine learning systems with fairness issues for patient groups. In addition, when vari-\nations in data documentation are non-random, significant bias can be introduced, improving, or worsening measured\nperformance for an institution relative to peers. This work focuses on ICU mortality benchmarking. In particular, we\nanalyze the fairness of a model based on Generalised Additive Models (GAM) [3] that predicts mortality in the ICU.\nThis model is used to compare actual versus predicted outcomes to assess ICU performance."}, {"title": "2 FAIRNESS METRICS IN PRODUCTION", "content": "We developed a new model [5] based on GAM, and compare this to an older, traditional model used for predictive\nanalytics in the ICU. Both models are trained on the same clinical data [7] with the same features. However, for the\npurpose of fairness analysis, both models are a black box."}, {"title": "2.1 Fairness analysis of sex, race and diagnosis", "content": "In order to make use of the metrics available from Fairlearn, we designed a schema to create logical groupings of\nmetrics and to be dynamically sized based on any number of sensitive features and any number of levels (i.e. possible\nvalues). The full schema is shown in the Appendix. These categories (e.g. binary sex) arise from upstream data capture\nand formatting and represent how the data is made available to researchers.  The Metric column contains accuracy and fairness\nmetrics. Feature Level represents the different possible values within this Sensitive Feature. Threshold is the value used\nto convert a probability (mortality risk) between 0 and 1, into a binary value, for the metrics that need it. The overall\naccuracy, area under the Receiver Operating Characteristic curve (auROC), is 0.92331, while accuracy for Female and\nNon-Female patients is 0.92187 and 0.9245 respectively. This very small difference shows that the model behaves very\nsimilarly for Female and Non-Female patients. We chose auROC as an accuracy metric, but other accuracy metric(s)\ncould also be appropriate. The schema presents performance metrics like accuracy in the same place as fairness metrics,\nto give them equal importance when viewed.\nPerforming the fairness analysis for different race groups, with auROC ranging around 0.92 (lowest) to 0.93 (highest)\nfor different races.\nWe then extended our analysis to explore additional model features, like diagnosis group (\"dxGroup\"). This is the\ndiagnosis that the patient received on admission to the ICU. Unlike sex and race, diagnosis group has a large difference\nin auROC in Table 1, with 0.82 being the lowest and 0.96 being the highest.\nThis may be because patients who come into the ICU with certain diagnosis groups, such as Acute Respiratory\nDistress Syndrome (ARDS) have lower accuracy predictions. This could be due to ARDS typically occurring in people\nwho are already critically ill or who have significant injuries so it would be hard to predict their recovery patterns.\nIn contrast, recovery patterns are easier to predict for other patients, such as those with Diabetic Ketoacidosis (DKA).\nThese patients, who may be young and otherwise healthy can be having an issue managing their blood sugar. They\nare likely to make a more predictable recovery once they receive a treatment like insulin."}, {"title": "3 DOCUMENTATION BIAS", "content": "Next, we extend fairness analysis to address the issue of documentation bias. We define documentation bias as the\nmeasurement error in clinical features that are used to train a model. We focus on the Glasgow Coma Scale (GCS),\nwhich is a standard assessment of the level of consciousness, where points are summed for eye opening, verbal and\nmotor response. A patient can score a minimum of 3 points (completely unresponsive) and a maximum of 15 points\n(responsive) (see Appendix).\nGCS is an important feature for predicting mortality, as a low score indicates that a patient is very unwell. Some-\ntimes the GCS cannot be accurately determined, for example when a patient is heavily sedated. In this case, a healthcare\nprofessional should capture the null data as \"Unable to score due to medications\". However, there is a lack of clear guid-\nance for this situation [4], and sometimes it is captured as the minimum (GCS=3) or the maximum (GCS=15). A GCS=3\nscore indicates that the patient is in a coma, so using it when the patient is healthier than that creates measurement\nerror. Different ICUs may have different documentation norms, and they may change their documentation without\nnotice and without recording what has changed. This systemic bias in the capture of GCS decreases model accuracy,\nnegatively affecting the benchmark reports which are created for ICUs.\nA model should perform equally well for a patient whether they visit one ICU or another, regardless of the hospital's\ndocumentation norms. The prediction for a patient should not depend on how their healthcare professional captured\nGCS data, which we consider a fairness issue for patients. We compared robustness to documentation bias of the new\nGAM-based model to the old model. In order to do this, we categorized ICUs by how often GCS=3 is captured: high\namount of GCS=3 (top 5%), low amount of GCS=3 (bottom 5%), medium amount (5th to 95th percentile). Thus, each\npatient stay is in an ICU of one of these three categories. We then used this new feature as a sensitive feature to perform\nfairness analysis on. First, we see results for an old model in Table 2.\nResults for the old model shows a low equalized odds ratio of 0.432985 (ideally it would be 1). This is the highest\nratio between false positive rates per group, or true positive rates per group. This indicates a large discrepancy between\ngroups when it comes to false positive rate. This is apparent when we consider ICUs that have a high amount of GCS=3\n(\"highGCS3\") which have a false positive rate of 0.356013. The model is likely to make many mortality predictions for\nthis group given the poor GCS score. However, many of these predictions are false positives because patients are not\nreally in a coma, they were just unable to be assessed due to sedation medications.\nNext, we consider the GAM-based model, which has been designed to be more robust to measurement error and\nthe resulting documentation bias. The GAM-based model shows in Table 2 a lower false positive rate in ICUs that\nfrequently document GCS=3. The equalized odds ratio is now higher (0.55209). Now we see that false positive rate in\nthe \"highGCS3\" group is approximately halved (0.17805).\nThis is an encouraging result which suggests that the GAM-based model is not only more accurate overall (higher\nauROC) but also more robust to documentation bias (false positive rate for ICUs using GCS=3 for null data is halved).\nTraditionally we might have just compare two models by comparing their accuracy. However, the fairness analysis\nframework provides us with more informative, detailed and nuanced information for model comparison.\nOther clinical features can also contain documentation bias. For example, the clinical diagnosis of sepsis when\na patient arrives in ICU. There is some subjectivity in diagnosing sepsis [2] as the definition is broad and medical\nguidelines are evolving. Therefore, it is important to have predictive models that are robust to documentation bias,\nand our analysis shows how we can check them for disparities."}, {"title": "4 DISCUSSION", "content": "Responsible Al is a socio-technical challenge and we cannot guarantee that a machine learning system will behave\nfairly for all users. Using fairness metrics to analyze model predictions does not guarantee that a dataset is without\nflaws. But, if we calculate fairness metrics for groups of patients as part of routine system monitoring, it does give us\nvisibility into fairness issues.\nIf disparities between patient groups are found during fairness analysis, they can be discussed with domain experts\n(ICU clinicians) to understand the underlying cause, whether clinical or due to societal bias. If fairness issues are\nfound, then unfairness mitigation techniques could be explored (potentially with Fairlearn). Future improvements to\nthe fairness metrics schema could include showing a sample size for each fairness metric, and showing a zero or one\nas an ideal value, to make the metrics easier to interpret."}, {"title": "5 CONCLUSION", "content": "Fairness is especially important for machine learning models used in the medical domain. We explored the fairness of\nICU mortality models for different patient groups (sex, race, and diagnosis). Fairness analysis helped us address the doc-\numentation bias in clinical data that could lead to decreasing model accuracy and can negatively affect benchmarking\nbetween different ICU units/hospitals. We showed that fairness metrics provide additional insights when comparing\nmodel performance, and can use them to show the value of an important new model which will enhance benchmark\nreports."}]}