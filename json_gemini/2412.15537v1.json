{"title": "Enhancing Large-scale UAV Route Planing with Global and Local Features via Reinforcement Graph Fusion", "authors": ["Tao Zhou", "Kai Ye", "Zeyu Shi", "Jiajing Lin", "Dejun Xu", "Min Jiang"], "abstract": "Numerous remarkable advancements have been made in accuracy, speed, and parallelism for solving the Unmanned Aerial Vehicle Route Planing (UAVRP). However, existing UAVRP solvers face challenges when attempting to scale effectively and efficiently for larger instances. In this paper, we present a generalization framework that enables current UAVRP solvers to robustly extend their capabilities to larger instances, accommodating up to 10,000 points, using widely recognized test sets. The UAVRP under a large number of patrol points is a typical large-scale TSP problem.Our proposed framework comprises three distinct steps. Firstly, we employ Delaunay triangulation to extract subgraphs from large instances while preserving global features. Secondly, we utilize an embedded TSP solver to obtain sub-results, followed by graph fusion. Finally, we implement a decoding strategy customizable to the user's requirements, resulting in high-quality solutions, complemented by a warming-up process for the heatmap. To demonstrate the flexibility of our approach, we integrate two representative TSP solvers into our framework and conduct a comprehensive comparative analysis against existing algorithms using large TSP benchmark datasets. The results unequivocally demonstrate that our framework efficiently scales existing TSP solvers to handle large instances and consistently outperforms state-of-the-art (SOTA) methods. Furthermore, since our proposed framework does not necessitate additional training or fine-tuning, we believe that its generality can significantly advance research on end-to-end UAVRP solvers, enabling the application of a broader range of methods to real-world scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "With the development of Unmanned Aerial Vehicle (UAV), using UAV for patrolling has become an emerging and important security measure. Planning a reasonable patrol route is key to improving the efficiency and effectiveness of the patrol. Our approach is to design a path planning algorithm for UAV to enhance patrol efficiency. The Unmanned Aerial Vehicle Route Planing (UAVRP) is a optimization problem wherein a UAV must visit a specific set of sites exactly once and return to the starting point, aiming to minimize the overall travel distance. The primary hurdle in UAVRP's resolution lies in the extensive search space that emerges when dealing with a substantial number of sites, denoted as n. The UAVRP under a large number of patrol sites is a typical Large-Scale Travelling Salesman Problem (LSTSP), we define a TSP instance that requires visiting more than 500 sites as an LSTSP in this paper. Despite its inherent theoretical complexity, the LSTSP finds numerous practical applications across various domains such as drone delivery, transport, genome sequencing, and circuit board design [1]-[6].\nOver the years, the operations research community has proposed various methods to tackle the UAVRP. Among these approaches, Concorde [7] stands out as a relatively powerful and extensively utilized method in real-world scenarios. However, when confronted with UAVRP problems involving tens of thousands of dimensions, Concorde falls short in producing accurate results within a reasonable timeframe (within 8 CPU hours). To address this limitation, numerous heuristic algorithms have been developed, including LKH3 [8] and OR-Tools. Nevertheless, when dealing with the LSTSP, these algorithms suffer from two key drawbacks. Firstly, they are still time-consuming. Secondly, their iterative search procedures yield unstable results and necessitate the manual development of heuristic rules.\nTo tackle the challenges of time-consuming and unstable solutions, a series of learning-based methods have been proposed [9]\u2013[11]. Such methods are trained on huge volumes of"}, {"title": "II. RELATED WORKS", "content": "The TSP problem, being a long-standing NP-hard challenge, has garnered a substantial body of related research. Given the primary focus of our paper on designing a framework for learning-based approaches, we delve into the intricate details of these approaches in the subsequent sections, rather than attempting to encompass all domains. Moreover, the landscape boasts several exceptional algorithms in diverse fields, including evolutionary computation. Those interested in non-learning methods can delve deeper into these alternatives in [18]\u2013[21].\nA. Learning-based Methods\nWe classify learning-based strategies into one-stage and two-stage methods, contingent on the progression stages required to achieve a solution and the necessity for intermediate product generation.\n1) One-stage Methods: Point Networks [22], [23] pioneered an end-to-end TSP approach using neural attention to handle variable output sizes, selecting inputs as outputs. [10] improved this with an attention layer, outperforming pointer networks, and used REINFORCE for model training with a greedy rollout baseline. POMO [24] explored REINFORCE for guiding TSP solutions towards multiple optima [25]. [26] proposed a direct TSP solution approach, avoiding costly search processes, offering a fast solution but challenging to generalize to Large-Scale TSP without extensive fine-tuning or retraining.\n2) Two-stage Methods: [15] introduces a two-stage learning-based TSP solver: generating an intermediate heatmap followed by solution search. Using divide-and-conquer, it breaks down LSTSP into smaller instances, solves sub-heatmaps, and merges results, then applies MCTS for performance comparable to LKH3 on 10,000-point instances. The approach is adaptable and scalable despite the additional search phase.\nDIMES [27] follows this model, offering two-stage processing with heatmap generation and MCTS for improved accuracy. It also creates a continuous space for parameterizing solution distributions, which stabilizes REINFORCE training and enables parallel sampling for fine-tuning.\nIt is important to underscore that the aforementioned DIMES [27], H-TSP [26] and Att-GCN [15] demonstrate the capacity to generalize to LSTSP. However, the scope of generalization for these three solutions remains confined to their respective models and does not extend to other pre-existing TSP solvers."}, {"title": "III. METHODS", "content": "A. Preliminaries\n1) Problem Definition: Our proposed framework concentrates on the Two-Dimensional Euclidean Traveling Salesman Problem (2D Euclidean TSP). This problem can be represented as an undirected graph G(V, E), where V (|V| = n) signifies the set of vertices, referred to as cities in the context of TSP, and E represents the set of edges. For consistency with the majority of learning-based methodologies, we presume that all vertices in V are uniformly distributed within a unit square of side length 1. This translates to the coordinates (xi, Yi) of each vertex adhering to the condition xi \u2208 [0,1], Yi \u2208 [0,1]. The Euclidean distance between cities is denoted by dij, and an element pij within the heatmap P for graph G signifies the probability of edge (i, j) being present in the optimal solution.\n2) Delaunay Triangulated Graph: Delaunay Triangulation(DT) is the fundamental method of study in algebraic topology. In the case of a surface, we divide the surface into pieces that satisfy the following conditions:\n\u2022 Each piece is a curved triangle;\n\u2022 Any two such curved triangles on the surface either do not intersect or intersect on exactly one common side (not two or more sides at the same time).\nThe definition of DT requires it to have the following characteristics:\n\u2022 The fact that the outer circle of every Delaunay triangle contains no other point in its area is called the empty outer circle property of Delaunay triangles, and this property has been used as a criterion for creating Delaunay triangles.\n\u2022 Another property is the maximum minimum angle property: the minimum angle of the six interior angles of a convex quadrilateral formed by the diagonals of any two adjacent triangles does not increase when they are exchanged.\nSeveral heuristic TSP solvers [28]\u2013[30] have empirically demonstrated that the optimal solution of a TSP is highly likely to reside along the edges delimited by the DT. We incorporate this attribute into our learning-based methodology to uphold global features throughout the process of sub-graph extraction.\nB. The Proposed Framework\nThe DTTGF process encompasses 7 primary steps as depicted in Figure 1. Initially, the Delaunay Triangulation (DT) is executed on the instances, followed by the extraction of sub-graphs based on the DT outcomes. The derived sub-graphs are then addressed by the integrated TSP solver to yield sub-results, encompassing sub-heatmaps or sub-solutions. Subsequently, these sub-results amalgamate to construct a global heatmap, which is subsequently processed warming-up strategy to generate the ultimate heatmap. The final heatmap undergoes a search process utilizing the chosen search technique, culminating in the production of the definitive solution.\nIn this paper, as a framework-oriented study, we integrate the most distinct S+2-OPT (Sampling Decoder+2-OPT) [10], [31] and MCTS (Monte Carlo Tree Search) [15] approaches into the proposed DTTGF during the experimental phase to showcase the heatmap's performance."}, {"title": "C. Graph Sampling", "content": "For LSTSP, the divide-and-conquer strategy is common. [15] uses k-nearest neighbors for sub-graph decomposition to reduce complexity, but this risks losing global features. This is crucial in TSP, as sub-graph solutions must align with the overall solution. Our DT-centric approach ensures effective sub-graph extraction while preserving global properties.\nInstances are processed with DT before sub-graph extraction. The DT result is represented in an adjacency matrix D, where Dij = 1 for DT edges and Dij = 0 otherwise. To cover the entire graph evenly, DTTGF uses a matrix O of size n, incrementing Oi each time node i is selected for a sub-graph. The framework starts with the node i having the lowest Oi and explores adjacent nodes using a depth-first search, sorting them by distance dik.\nStarting with the first node k\u2081 from the sorted list, it is included in the sub-graph, and Ok1 is incremented. The process continues, adding connected nodes from D and selecting the closest to extend the sub-graph, until the node count is met. Iteration stops when sub-graphs meet the criteria."}, {"title": "D. Sub-graph Solving", "content": "The difference between one-stage and two-stage solvers is the generation of an intermediate heatmap. Learning-based approaches share fast search speeds through parallelism or strong forward processing, handling many small instances quickly. However, they struggle with varying instance sizes in real-world scenarios, where parallelism is less effective. Despite this, it forms the basis for our framework, leveraging fixed sub-graph sizes and fast search times to manage many sub-graphs efficiently, meeting real-world time needs. This paper includes representative methods in DTTGF for both stages. The one-stage solver provides sub-graph solutions, and the two-stage solver generates a heatmap, with the following section detailing how both are integrated into a global heatmap."}, {"title": "E. Graph Merging", "content": "1) One-stage Solver: For a one-stage solver, each sub-graph outputs a tour T, and the probability of each edge (i, j) appearing in the optimal solution is as follows\n$P_{ij} = \\frac{\\sum_{l=1}^{I} T_{l}(i, j)}{S_{ij}}$                                                                                                                                                    (1)\nwhere I represents the count of sub-graphs, while $T_{l}(i, j)$ indicates the presence of edge (i, j) within the solutions $T_{l}$ of sub-graph l. Meanwhile, $S_{ij}$ signifies the aggregate instances of edge (i, j) being chosen across all sub-graphs. As such, $P_{ij}$ denotes the actual frequency of edge (i, j) within the solutions of all sub-graphs, calculated by dividing the occurrences of its selection in a sub-graph.\n2) Two-stage Solver: The two-stage solver can also conduct dual search stages to obtain the final solution for the sub-graph and use equation (1) to compute the heatmap of the sub-graph, similar to the one-stage solver. However, the two-stage approach inherently benefits from the divide-and-conquer concept. This approach involves generating intermediate heatmaps itself, and if these intermediate heatmaps can be directly employed, the efficiency of producing global sub-graphs can be enhanced, mitigating the computational expense of the second search stage.\nThe corresponding heatmap for the two-stage sub-graphs is formulated as follows:\n$P_{ij} = \\frac{\\sum_{l=1}^{I} P_{l}(i, j)}{S_{ij}}$                                                                                                                                                  (2)\nwhere $P_{l}(i, j)$ denotes the value of edge (i,j) within sub-graph l on the sub-heatmap.\nThe noteworthy distinction from the one-stage solver lies in the fact that $T_{l}$ exclusively contains information about the sub-solutions, while $P_{l}$ provided by the two-stage solver encompasses the probabilities of all sub-graph edges appearing in the optimal solution. The proposed framework adeptly maps the outcomes of TSP solvers for sub-graphs onto the global heatmap for both scenarios.\nIt is crucial to note that techniques centered around sub-graph partitioning and fusion can potentially lead to the loss of global characteristics. Specifically, edges with elevated $P_{ij}$ values might perform well within individual sub-graphs, or they could be selected only a relatively limited number of times and incidentally emerge in the optimal solution of those sub-graphs. However, the latter case involves edges that are unlikely to be superior within the global graph. To address this, the paper introduces Delaunay Triangulation (DT) as a filter for the amalgamated heatmap. As mentioned earlier, prior research has illustrated a pronounced correlation between the optimal TSP solution and the DT delineation outcome, indicating that edges in the optimal solution tend to be encompassed by the DT result [32]. DTTGF capitalizes on this insight, and for the generated heatmap P, the P values of edges (i, j) not present in the DT result are set to 0. This adjustment is applied as follows:\n$P_{ij} = 0, \\forall(i, j) \\notin DT$                                                                                                                                        (3)\nAfter applying the filter, the proposed framework acquires the fused heatmap corresponding to the original instance."}, {"title": "F. The Warming-up Strategy", "content": "While DT as prior knowledge can eliminate certain non-potential edges, two issues still remain. Firstly, some potentially valuable edges that are not part of the DT result could be removed during the filtering. Secondly, even the edges within the DT result might appear promising on a local scale but mislead on a global level.\nParticularly, when integrating a one-stage solver, it characterizes only the edges of the optimal solution for each subgraph. This results in a sparse heatmap after fusion, where each edge's information is crucial. In contrast, a two-stage solver provides a subgraph heatmap for each subgraph, containing information about the majority of edges in the subgraph. The fused heatmap it produces is relatively dense and can partially mitigate this issue.\nTo address these challenges, we introduce a warm-up strategy based on pseudo-reinforcement learning, primarily tailored to one-stage solvers. During subgraph fusion, the heatmap is pruned to retain only edges with a potential to appear in the optimal solution. [33], [34] Two types of edges require special attention: those with longer distances dij and those with higher values of Pij. In TSP, the final performance often hinges on the selection of longer edges in the optimal solution, and edges with higher Pij values have a greater likelihood of being selected. Thus, we define fitness values as follows:\n$A_{ij} = P_{ij} X d_{ij}$                                                                                                                                                      (4)\nThe warm-up process involves iterations, following this sequence. The initial solution Tori is derived by rapidly solving the heatmap using a sampling decoder [10] followed by 2-OPT improvements (referred to as S+2-opt). Note the current solution Tori as the baseline, i.e. Tb. For each iteration, we select the edge (i,j) with the largest current Aij, modify its Pij to zero, and apply S+2-opt to obtain the improved solution Tdel. If Tdel outperforms the baseline To (obtained from Tori), P is updated to Pdel after deleting the edge, and back-propagation is performed. Let the back-propagation formula be\n$P_{ij} = P_{ij} + a \\times \\beta (e^{\\frac{D(T_b) - D(T_{del})}{D(T_b)} }- 1)$                                                                                                                   (5)\n$a = \\begin{cases}0, if (i,j) \\in (T_b \\cap T_{del})\\\\1, if (i,j) \\in T_b \\\\-1, if (i,j) \\in T_{del} \\end{cases}$\nwhere D(Tb) denotes the length of the tour consisting of solving T\u266d, D(Tdel) denotes the length of the tour consisting of solving Tdel, \u03b2 is the learning rate, and the value of a depends on whether the edge (i, j) belongs to To or Tdel or not. After back-propagation, if Tdel is better than To, To is updated to Tdel. Once the iteration requirements are met, the process concludes, and the final heatmap is output.\nThe proposed warm-up module selectively removes non-potential edges from focus in the heatmap. Moreover, potential edges not in the DT results but filtered during fusion can enhance their P-value via back-propagation, increasing their chances of selection in the subsequent search process."}, {"title": "IV. EXPERIMENTS", "content": "Existing TSPsolver for UAVRP under a large number of sites, which is modeled as LSTSP. To validate the efficacy of DTTGF, we conducted evaluations using three datasets generated by [35]. These datasets comprise 128 instances of 500 points, 128 instances of 1000 points, and 16 instances of 10000 points. This dataset selection ensures comparability with many current works on learning-based TSP solvers, which commonly use this dataset for training and validation. All experimental results were acquired using machines equipped with one RTX 1080 and an Intel(R) Xeon(R) Gold 5118 CPU @ 2.30GHz (featuring 8 cores), and the results align with those of [27].\nA. Baselines\nThe baselines employed in the experiments are of two types: the traditional methods and the learning-base methods.\nTraditional methods: we have Concorde [36], a classic TSP accurate solver, and Gurobi 1, a widely used industrial combinatorial optimization problem solver. Also included is LKH-3 [8], the classic heuristic TSP solver, and Farthest Insertion, a heuristic solver for most combinatorial optimization problems.\nLearning-based methods:EAN [37] (CPAIOR2018) is a re-engineered LSTM neural network framework that introduced 2-OPT assisted search. AM [10] (ICLR2019) brought Attention Mechanisms to TSP solving. GCN [38] focuses on constructing and solving TSP representations with graph neural networks. POMO+EAS [39] (ICLR2022) improved training speed and extended the existing TSP solver with a new combined search strategy. Att-GCN (AAAI2021) solves TSPs using supervised learning and reinforcement learning. DIMES (NeurIPS2022) provides a new spatial representation and meta-learning framework for solving TSP instances of different sizes. Lastly, H-TSP (AAAI2023) [40] jointly trains upper and lower layer solutions that can directly generate solutions for a given TSP instance without relying on any time-consuming search process.\nB. Comparative Study\nIn our framework-based study, Att-GCN and POMO were integrated as the experimental TSP solvers. Att-GCN requires a pre-trained GCN model for heatmap generation, representing a two-stage learning approach. POMO excels in smaller TSP instances and is included as a one-stage solver in our framework.\nTable I shows DTTGF's advantages over other learning-based methods in scheduling and timing, with all baseline results from [27] except Att-GCN's. Our framework is easily integrated by embedding a TSP solver without extra training. Using MCTS, DTTGF outperforms Att-GCN and DIMES. With S+2-OPT, a fast search method for real-time tasks, DTTGF's accuracy drops but time is reduced. DTTGF+S+2-OPT even surpasses MCTS-based methods in accuracy, including for 1000-point datasets.\nA key issue is POMO's scaling challenges with Large-Scale TSPs (LSTSPs). The POMO+EAS benchmark struggled with TSP-1000 and TSP-10000 within 8 hours. However, integrated"}, {"title": "C. Ablation Study", "content": "Ablation results in Table I show the proposed assumptions' validity against DT. Att-GCN+DTTGF (without WU) differs from Att-GCN in using DT-based sampling and fusion. DTTGF surpasses Att-GCN in accuracy with MCTS decoding, suggesting DT's enhancement potential.\nDTTGF vs. DTTGF(warm-up) shows warm-up boosts accuracy, especially for one-stage methods. With MCTS, DTTGF with POMO improves TSP-1000 by over 10%.\nWarm-up times for TSP-500, TSP-1000, and TSP-10000 are 1.22s, 7.23s, and 5.12 min, respectively.\nAM and GCN integrated into DTTGF (Table II) show significant LSTSP improvements in accuracy and efficiency. GCN, like POMO, reaches TSP-10000, a feat beyond the original approach, proving DTTGF's versatility in enhancing existing methods."}, {"title": "V. CONCLUSION", "content": "Introducing DTTGF, a novel framework upgrading TSP solvers for UAVRP with many sites. It uses Delaunay triangulation for graph decomposition and embeds current solvers. With a warm-up strategy, DTTGF extends solvers efficiently, enhancing generalizability and scalability. Embedding Att-GCN and POMO, it handles LSTSP, outperforming or matching SOTA on three datasets. Ablation studies show DT-based subgraph methods preserve features and boost accuracy. The warm-up strategy improves search accuracy within reasonable time. Future work could enhance the framework's effectiveness, especially for large TSP instances, and refine the warm-up strategy with adaptive hyperparameters. The framework may also apply to other combinatorial optimization problems."}]}