{"title": "LIME-M: Less Is More For Evaluation of MLLMS", "authors": ["Kang Zhu", "Qianbo Zang", "Shian Jia", "Siwei Wu", "Feiteng Fang", "Yizhi Li", "Shuyue Guo", "Tianyu Zheng", "Bo Li", "Haoning Wu", "Xingwei Qu", "Jian Yang", "Zachary Liu", "Xiang Yue", "J.H. Liu", "Chenghua Lin", "Min Yang", "Shiwen Ni", "Wenhao Huang", "Ge Zhang"], "abstract": "With the remarkable success achieved by Multimodal Large Language Models (MLLMs), numerous benchmarks have been designed to assess MLLMs' ability to guide their development in image perception tasks (e.g., image captioning and visual question answering). However, the existence of numerous benchmarks results in a substantial computational burden when evaluating model performance across all of them. Moreover, these benchmarks contain many overly simple problems or challenging samples, which do not effectively differentiate the capabilities among various MLLMs. To address these challenges, we propose a pipeline to process the existing benchmarks, which consists of two modules (i.e., Semi-Automated Screening Process and Eliminating Answer Leakage). The Semi-Automated Screening Process filters out samples that cannot distinguish the model's capabilities by synthesizing various MLLMs and manually evaluating them. The Eliminate Answer Leakage filters the samples whose answers can be inferred without images. Finally, we curate the LIME-M: Less Is More for Evaluation of Multimodal LLMs, which is a lightweight Multimodal benchmark and can more effectively evaluate the performance of different models. Our experiments demonstrate that 1. LIME-M can better distinguish the performance of different MLLMs within less sample numbers (24% of original) and time spent (23% of original); 2. LIME-M eliminates answer leakage and mainly focuses on the information within images; 3. The current automatic metric (i.e., CIDEr) is insufficient for evaluating MLLMS' capabilities in captioning. Moreover, we find that removing the caption task score when calculating the overall score demonstrates a more precise reflection of model performance differences. All our codes and data are released at https://github.com/kangreen0210/LIME-M", "sections": [{"title": "INTRODUCTION", "content": "In order to better understand the model's capabilities and guide to address the shortcomings of MLLMs, researchers have developed numerous benchmarks for various tasks (Antol et al., 2015; Wei et al., 2023; Fu et al., 2023; Yue et al., 2024; Wu et al., 2024a). These benchmarks thoroughly explore the capabilities of MLLMs in various tasks such as image captioning, image question answering, and Multimodal retrieving.\nHowever, existing MLLM benchmarks and unified evaluation frameworks cannot effectively and efficiently reflect the ability of MLLMs. Current benchmarks include numerous relatively simple samples (i.e., how many chairs are in the picture) and some incorrect questions caused by annotation"}, {"title": "METHOD", "content": "Most benchmarks contain low-quality, noisy data. Figure 2 shows the statistics of different subtasks within our LIME-M benchmark. It is worth mentioning that the proportion of easy and wrong samples exceeds 30%. Out of the 10 subtasks, 6 have proportions exceeding 50%. Notably, for the POPE dataset, 95% of the data can be classified as noisy or erroneous. This indicates that existing benchmarks are filled with a large amount of low-quality data, which does not accurately reflect the true capabilities of MLLMs.\nTo address those issues, we propose a general data curation pipeline, which consists of three main stages: (1) Open-source models as judges, (2) Semi-automated screening process, and (3) Eliminating Answer Leakage. Our approach aims to enhance existing benchmarks by removing inaccurate and oversimplified data."}, {"title": "OPEN-SOURCE MODELS AS JUDGES", "content": "To avoid potential biases that may exist in individual MLLMs, we select ten different types of open-source models as judges. To categorize the difficulty of each sample, we analyze the performance of all judge models on each question and label the difficulty based on the number of models that answered correctly. If N \u2265 6, the question is classified as the easy set. If 3 \u2264 N \u2264 5, it is classified as the middle set. Conversely, if N < 2, it is classified as the hard set."}, {"title": "SEMI-AUTOMATED SCREENING PROCESS", "content": "Easy samples do not effectively differentiate the capabilities of various models, as most models can answer them correctly. Therefore, we remove the easy samples to assess model performance better.\nFurthermore, we find that some questions are not correctly answered by any model, which can be due to potential errors in the question design. To mitigate these potential errors and filter out totally incorrect questions, we implement a semi-automated screening process, which consists of two stages. In the first stage, all questions with zero passes are reviewed by GPT-4V to assess their"}, {"title": "ELIMINATING ANSWER LEAKAGE", "content": "Although the previous two stages have filtered out potential errors and assessed the quality of the questions, we still need to address the potential issue of ANSWER LEAKAGE. Multimodal Answer Leakage can be summarized into two main categories: 1.Text Answerable Questions: The textual information contains all the necessary details to answer the question, making the corresponding visual information redundant. 2.Seen Questions: The model has encountered a specific question during training and has memorized the question along with its corresponding ground truth. As a result, some questions can be answered by pure text-based language models (LLMs) without relying on visual information. Therefore, we conduct a text-only check using pure text LLMs. Based on LLMs' responses, we will remove the samples that can be directly answered without using the image. After that, we proportionally sample 1200 samples from these categories based on their difficulty levels, For benchmarks with fewer than 1,200 entries, we adapt all samples."}, {"title": "LIME-M: A COMPREHENSIVE MLLMS BENCHMARK", "content": "In this section, we propose LIME-M, a comprehensive benchmark for Multimodal Large Language Models (MLLMs). LIME-M streamlines existing mainstream benchmarks. Table 1 shows the main datasets included in our benchmark, as well as the data scale after careful pruning. For each sub-dataset, we aim to keep the size around 1k samples."}, {"title": "TASK DEFINITION", "content": "We have categorized the existing mainstream tasks into six domains: Captioning, T/F Reasoning, Normal VQA, Infographic Understanding QA, Science QA, and OCR. Below are the task definitions for each domain\n\u2022 Image understanding and captioning: The Captioning task focuses on the fundamental image-text understanding ability, requiring MLLMs to accurately describe and understand the content of images. This ability is commonly learned by most multimodal models during the pre-training stage. For example, the CLIP model aligns image and text features through contrastive learning, making Captioning a measure of the basic capabilities of MLLMs.\n\u2022 T/F reasoning: T/F Reasoning requires the model to judge the truthfulness of textual statements based on the image content. This not only demands basic image understanding from the MLLMs but also requires a certain level of reasoning ability.\n\u2022 Normal VQA: Normal VQA, or Visual Question Answering, comprehensively evaluates the model's ability to answer questions based on visual input. MLLMs are required to select the most appropriate answer from specific options.\n\u2022 Infographic Understanding QA: This task differs from Normal VQA as it tests the model's ability to retrieve details from images. MLLMs need to find the most relevant information in the image related to the question and then provide a reasoned answer.\n\u2022 Science QA: Science QA includes questions and answers related to natural science knowledge. This requires the model to have domain-specific knowledge in natural sciences, mainly assessing the MLLMs' mastery of knowledge within a specific domain.\n\u2022 OCR: The OCR task requires the precise extraction of textual content from images."}, {"title": "DATA STATISTICS", "content": "LIME-M is composed of 10 open-source multimodal evaluation benchmarks, with scales ranging from 1,000 to 9,000. After our three-stage data curation, the data scale of each benchmark is significantly reduced. Figure 1 shows the number of samples removed at each stage compared to the original dataset. The amount of data removed varies at each stage, with the most being removed in the first stage, reflecting a large number of low-difficulty or data-leakage samples in the existing 9 MLLMs. Comparing the data volumes before and after the second stage of semi-automated screening, we can see that many datasets, such as OK-VQA and TextVQA, have a high rate of low-quality data leading to MLLMs' incorrect answers. Additionally, some datasets, such as ScienceQA and AI2D, have a significant amount of data removed after the third stage, indicating that many questions in these datasets can be answered correctly by MLLMs without relying on images. The statistics of the curated data are shown in 1. LIME-M includes all data after the third stage of screening, and to further streamline the dataset, we have selected a subset from each screened benchmark based on difficulty, with each subset maintaining a data volume of around 1k."}, {"title": "EXPERIMENT", "content": "To evaluate the quality of LIME-M, we conducted a series of experiments across various open-source and closed-source models. These experiments primarily encompass the following four settings:\n\u2022 Main experiment: To demonstrate the performance of LIME-M, mainstream open-source models are evaluated using a standardized process to reflect their overall performance differences.\n\u2022 Text only set: To prevent potential data leakage issues, we conducted validation experiments using text-only QA pairs. This was to verify whether LLMs could correctly answer questions based on text-only information.\n\u2022 Text-only question with Image Description set: Image Description (ID) refers to simple descriptions of images that represent superficial information contained within them. For most MLLMs, questions containing only superficial information are easy to answer; inversely, questions requiring complex visual inference are significantly more challenging. To further validate whether LIME-M can reflect the capabilities of MLLMs, we input text-only QA pairs combined with ID into LLMs and test their ability."}, {"title": "BASELINES", "content": "We select LLaVA-1.5 (Liu et al., 2023a;b), LLaVA-1.6 (Liu et al., 2024), Tinny-LLaVA (Zhou et al., 2024), MiniCPM (Hu et al., 2024), Idefics-21, Deepseek-VL(Lu et al., 2024), CogVLM (Wang"}, {"title": "RESULTS", "content": "As shown in Tab 2, we evaluate the mainstream MLLMs on our LIME-M benchmark. Overall, models with larger parameter sizes and newer versions tend to have higher overall scores. The InternVL-1.5, InternVL-2-Large (26B,40B) and LLaVA-OneVision-7B achieve the best overall performance, whose overall scores all surpass 60%. The performance of InternVL-2-Small(1B-8B), CogVLM series, and Cambrian series are second only to InternVL-2-Large and LLaVA-OneVision-7B, with their overall scores ranging from 45% to 60%. Comparing the overall score of LIME-M and Origin benchmarks, it can be observed that certain model series, such as Cambrian and LLaVA-1.5, their overall scores experience a decline. Conversely, the CogVLM and LLaVA-OneVision series' overall scores show an improvement. Specifically, that of CogVLM2 and XComposer-4KHD experience significant fluctuations, improved by 4% and 6% respectively\nAs for the OCR task, all those MLLMs performance are all below 35%, and the performance of most models falls between 10% and 30%. This indicates that OCR tasks remain challenging for current MLLMs. Regarding with POPE subtask and caption subtask, most models demonstrate good performance. These tasks involve generating or assessing descriptions of the content in images, which proves that current MLLMs have strong image content recognition capabilities. As for the VQA task, current MLLMs have a relative high performance on the TextVQA, ChatQA and ScienceQA, which design questions directly ask the facts in the picture. While their performances are relative less on OK-VQA, infoVQA and AI2D, which require to use extra commonsense knowledge or complex reasoning to answer question. This demonstrates that the current MLLMs exhibit significant image content recognition capabilities. However, their ability to perform complex reasoning by utilizing additional knowledge is not strong. We believe this limitation may be due to the constraints of the language model component of MLLMs."}, {"title": "CORRELATION ANALYSIS", "content": "Figure 4 illustrates the correlation between the various sub-tasks in LIME-M and WildVision Bench. Most tasks in LIME-M exhibit a strong positive correlation with WildVision Bench. There are six subtasks that have correlation scores exceeding 80%. Additionally, the overall score of LIME-M correlates at 91% with WV-Elo, which is higher than any individual sub-task and the original bench's correlates, demonstrating that the overall score of LIME-M provides a more comprehensive reflection of MLLMs capabilities.\nAutomated evaluation metrics (e.g., CIDEr) cannot effectively assess the performance of LVMs in captioning tasks. As an earliest foundational problem, the captioning task has been extensively"}, {"title": "EFFECTIVENESS OF LIME-M", "content": "LIME-M provides a more challenging evaluation for MLLMs. As shown in Tab 2, The MLLMs' performances on the LIME-M are less than that on Original Bench on most tasks. Compared to the Origin benchmark, different MLLMs show a larger score range on our LIME-M, in-"}, {"title": "LIME-M eliminates potential data leakage", "content": "For multimodal question answering tasks, visual information input is essential, and LLMs are unable to provide correct answers due to they cannot perceive the content within the image. However, as shown in Figure 6 (right), there are severe data leakage issues in the original Bench for the AI2D and ScienceQA tasks. The average score for AI2D is close to 55%, and for ScienceQA, it exceeds 60%, which shows that data from AI2D and ScienceQA in Original are highly likely to have been exposed to the training data of LLMs. In contrast, the LIME-M has eliminated this potential threat, achieving scores below 25% in AI2D and close to 40% in ScienceQA.\nAccording to the results in Figure 6 (left), LLMs tend to score higher on ScienceQA than on AI2D. This is because the former includes many commonsense text-only questions, such as \u201cWhat is the capital of Massachusetts?\u201d which are unrelated to the images."}, {"title": "THE IMPACT OF DETAIL IMAGE PERCEPTION", "content": "In our data cleaning process, we removed many questions that most models could answer, as well as a small number of questions that are difficult for both humans and GPT-4 to answer, in order to make the benchmark better highlight the differences in model capabilities. As shown in Tab 5, to investigate whether the remaining samples need to be answered by using textual and image information, we conducted experiments using LLMs to generate answers on both the Original Benchmark and MLLMs Benchmark under QVD (question + visual description) setting.\nLIME-M requires MLLMs to perceive deeper levels of image information. Especially in tasks such as AI2D, OCRBench, and TCaps, the scores of LLMs on LIME-M are significantly lower than on the Original Benchmark when provided with only the questions and simple image descriptions. This indicates that, after removing some of the simpler questions, LIME-M is better at testing the models' ability to perceive image details."}, {"title": "RELATED WORK", "content": "In recent years, there has been increasing attention on establishing evaluation benchmarks to assess the performance of MLLMs in different scenarios to guide the development of MLLMs. Early multimodal evaluation benchmarks primarily focused on single tasks, such as Visual Question Answering (VQA) (Antol et al., 2015; Goyal et al., 2017; Kafle & Kanan, 2017; Singh et al., 2019; Marino et al., 2019), Image Captioning (Agrawal et al., 2019), and Information Retrieval (Wei et al., 2023). As MLLMS develop, simple benchmarks are no longer sufficient to evaluate the versatile capabilities of these models comprehensively, since most MLLMs have exceptional ability on those benchmarks. Consequently, numerous more difficult and diverse evaluation benchmarks have emerged in recent years to assess the capabilities of MLLMs comprehensively. For instance, MMMU (Yue et al., 2024) and CMMMU (Zhang et al., 2024), are comprehensive benchmark tests for university-level multidisciplinary multimodal understanding and reasoning, spanning various disciplines and fields such as Arts and Design, Business, Science, Health Medicine, Humanities and Social Sciences, and Technology Engineering. MMBench (Liu et al., 2023c) has developed a comprehensive evaluation pipeline that offers fine-grained capability assessment and robust evaluation metrics. MMRA (Wu et al., 2024b) systematically establishes an association relation system among images to access the multi-image relation mining ability of MLLMs. However, those benchmarks cannot distinguish the performance gaps among different models excellently, due to they still have some too simple or difficult samples that most models have the same results. Furthermore, training datasets across different models may contain the samples of those benchmarks, which results in data leakage issues (Fu et al., 2023). Therefore, we propose LIME-M: a comprehensive MLLMs benchmark. We have carefully selected six types of tasks from the existing mainstream benchmarks and scaled down the existing benchmarks according to clear guidelines. It is a streamlined version that retains the essential elements of mainstream MLLMs benchmarks."}, {"title": "CONCLUSION", "content": "As MLLMs continue to advance, a notable absence of convenient and high-quality multimodal benchmarks has emerged. In response to this, we propose a pipeline aimed at semi-automatically refining existing benchmarks to enhance their quality, culminating in the development of LIME-M, which comprises 9,400 evaluation samples across 6 types of tasks and 10 different benchmark datasets. By refining the original benchmarks to filter question difficulty and eliminate potentially problematic items, LIME-M offers a more rigorous evaluation for MLLMs, necessitating a deeper understanding of image information. The outcomes of our evaluation experiments demonstrate the heightened challenge posed by LIME-M for MLLMs. We anticipate that our approach will contribute to the advancement of MLLM evaluation systems, and we are committed to continually enriching LIME-M with an expanded array of datasets through regular updates and expansions. Our ultimate goal is to provide the community with a simpler, more efficient, and more accurate evaluation method and suite for MLLMs."}, {"title": "A.2.2 METRICS", "content": "\u2022 Subtask metrics: As shown in the table 6, different metrics are used for different subtasks. It is important to note that, except for the CIDEr metric, all other metrics have a range between 0 and 1. The final score for each subtask is calculated by taking the average of these metrics."}, {"title": "A.2.3 DIFFICULTY CLASSIFICATION DETAILS", "content": "For subtasks using the accuracy (acc) metric, where the scores are binary, with only 1 or 0, other tasks may have various possible score distributions (e.g., COCO-Caption, OK-VQA). Therefore, we determine the threshold score based on the overall distribution of subtask scores, and choose the cutoff value that offers the greatest distinction, as shown in table 8, for the metrics ANLS, Relaxed Overall and Accuracy (Acc), the threshold is set to 1.0, for BLEU-4 (for the captioning task, we use the BLEU-4 metric to represent the score for each question), the threshold is set to 0.2, while for Match Score, it is set to 0.6. When the score is greater than the threshold, it is marked as correct; otherwise, it is marked as incorrect."}, {"title": "ALIGNMENT WITH REAL WORLD USER QUERIES", "content": "To bridge the gap between LIME-M and real-world users' query. We construct a similarity search system between the LIME-M and the real-world benchmarks(such as wildvision bench, and VibeEval) Different from single-modal similarity search systems, our system leverages multimodal information as query instructions, which contains text and image features information, to explore the effectiveness of the methods and construct the optimal similarity search system, we propose the following different approaches:"}, {"title": "\u2022 single modal query", "content": "For single-modal query, we use the corresponding modality information for retrieval, such as text-to-text and image-to-image. The CLIP model, which consists of a text encoder and an image encoder (Radford et al., 2021), has achieved excellent performance in representing multimodal information. As shown in Figure 9, We use CLIP's encoders to encode single-modal features and use cosine similarity as measure metrics. For each data entry in the real-world benchmark, we index the top-k most similar entries sequentially."}, {"title": "\u2022 multi-modal query", "content": "For multimodal queries, we combine image features and text features as the query. As shown in Figure 9, We adapt both the Clip and Blip models to combine these features. For the clip model, the text and image inputs are processed into separate vector features, which are then combined into a unified vector through a weighted sum. As to the blip model, image features are fused with text features through a cross-attention mechanism. We only keep the top-1 similar data entry based on similarity."}, {"title": "Why current multimodal benchmarks can't reflect real user queries in the wild real world?", "content": "Instructions in original benchmark can not address real-world problems. Although there are various multimodal evaluation datasets (TextVQA, infoVQA, ScienceQA, and MMMU), the instructions in these datasets differs from real-world user instructions. For the same image, the types of instructions can also vary significantly. As illustrated in Figure11, typical VQA tasks require perception and reasoning about detailed content within an image, such as asking 'the population older than 65 in Vietnam in 2020'. In contrast, real-world queries might ask 'how to reproduce the charts using Python'. This discrepancy leads to inherent differences in instructions, even when using retrieval algorithms to find the most relevant dataset questions."}]}