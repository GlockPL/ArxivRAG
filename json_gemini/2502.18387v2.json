{"title": "How Far are LLMs from Real Search? A Comprehensive Study on Efficiency, Completeness, and Inherent Capabilities", "authors": ["Minhua Lin", "Hui Liu", "Xianfeng Tang", "Jingying Zeng", "Zhenwei Dai", "Chen Luo", "Zheng Li", "Xiang Zhang", "Qi He", "Suhang Wang"], "abstract": "Search plays a fundamental role in problem-solving across various domains, with most real-world decision-making problems being solvable through systematic search. Drawing inspiration from recent discussions on search and Large Language Models (LLMs), we systematically explore the complementary relationship between search and Large Language Models (LLMs) from three perspectives. First, we analyze how learning can enhance search efficiency and propose Search via Learning (SEAL), a framework that leverages LLMs for effective and efficient search. Second, we further extend SEAL to SEAL-C to ensure rigorous completeness during search. Our evaluation across three real-world planning tasks demonstrates that SEAL achieves near-perfect accuracy while reducing search spaces by up to 99.1% compared to traditional approaches. Finally, we explore how far LLMS are from real search by investigating whether they can develop search capabilities independently. Our analysis reveals that while current LLMs struggle with efficient search in complex problems, incorporating systematic search strategies significantly enhances their problem-solving capabilities. These findings not only validate the effectiveness of our approach but also highlight the need for improving LLMs' search abilities for real-world applications.", "sections": [{"title": "1 Introduction", "content": "Search lies at the heart of problem-solving, offering a systematic approach to explore solution spaces and find optimal answers. From everyday choices to complex strategic planning, virtually all real-world decision-making processes can be formulated and solved through systematic search strategies. This insight aligns with the recent discussions on search and learning (Sutton, 2019; Snell et al., 2024), which emphasizes that the most effective problem-solving approaches combine systematic search with learning from experience.\nTraditional search methods, such as brute-force searches, while theoretically complete, face challenges in systematically exploring large and complex search spaces. They are designed to ensure that all possible states are considered, but the vastness of such spaces often necessitates exhaustive exploration, making systematic traversal impractical. Moreover, these methods lack the intuitive problem-solving abilities that humans naturally employ, such as recognizing promising solutions early or quickly abandoning unproductive paths.\nRecent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have opened new possibilities for more human-like search approaches. Some recent LRMs such as OpenAI 01 (OpenAI, 2024), QwQ-32B (Qwen, 2024b), and DeepSeek-R1 (Guo et al., 2025) have demonstrated remarkable performance by incorporating LLM-guided search strategies. These approaches leverage LLMs' extensive knowledge bases and reasoning capabilities to guide the search process (Wei et al., 2022; Wang et al., 2023; Snell et al., 2024; Yao et al., 2023a; Hao et al., 2023; Wang et al., 2024a), attempting to mirror human-like intuition in problem-solving.\nDespite the great success of these LLM-based simulated searches, a critical limitation remains: they rely heavily on models' intrinsic knowledge rather than combining it with systematic search strategies. When confronted with complex problems requiring multi-step reasoning or extensive exploration, these models often struggle to maintain consistent performance (Wang et al., 2024b,d; Snell et al., 2024). Their reasoning can become unstable or incomplete, particularly when solutions require carefully exploring multiple solution paths or backtracking from dead ends\u2014abilities that humans naturally employ in problem-solving.\nDraw inspiration from recent discussions on"}, {"title": "2 Related Works", "content": "LLM-based Search Methods. Recent advancements in test-time compute scaling have sparked growing interest in methods that enable LLMs to simulate search processes and \u201cthink longer\" instead of directly generating answers in one pass. Several works (Wang et al., 2023; Hao et al., 2023; Feng et al., 2023; Yao et al., 2023a; Zhao et al., 2024; Besta et al., 2024; Wang et al., 2024a; Snell et al., 2024) adopt such approaches to enhance LLMs' problem-solving capabilities. Despite their innovation, these methods rely solely on LLMs' intrinsic knowledge, often leading to unstable performance due to limitations in LLMs' reasoning capabilities. In contrast, our proposed SEAL integrates LLMs with traditional search strategies, ensuring both completeness and efficiency in solving decision-making tasks. More LLM-based search methods are reviewed in Appendix A.1."}, {"title": "3 How Learning can Benefit Search", "content": "Search and learning represent two fundamental approaches to problem-solving. Traditional search methods offer systematic exploration with guaranteed completeness, while learning-based approaches leverage pattern recognition to identify promising solutions quickly. In this section, we conduct a preliminary analysis to systematically explore existing search algorithms and the synergies between learning and search. Building on these insights, we introduce SEAL, a framework that integrates learning into search algorithms to reduce unnecessary exploration, prioritize promising paths, and ultimately maintain reliable and efficient performance as problems grow in complexity."}, {"title": "3.1 Experimental Setup", "content": "Task Setup. To investigate the impact of learning on search, we use the Game of 24 as a representative task. This task can be solved using traditional search algorithms and is also widely adopted for evaluating LLMs' planning abilities. Details about the task are provided in Appendix F.1. Following the setting in Yao et al. (2023a), we select 100 problems indexed as 900 \u2013 999 for our experiments.\nBaselines. For our preliminary analysis, we evaluate three LLM-based simulated search algorithms: MAJORITY VOTE (Wang et al., 2023), BEST-OF-N (Snell et al., 2024), and BEAM SEARCH (Yao et al., 2023a). Additionally, we include VANILLA COT (Chain-of-Thought) (Wei et al., 2022) as a reference. To evaluate search efficiency, we also consider two traditional brute-force search methods, Depth-First Search (DFS) and Breadth-First Search (BFS), along with their pruning variants, DFS-PRUNE and BFS-PRUNE. These variants improve efficiency by avoiding exploration of previously visited states. Finally, an EXHAUSTIVE"}, {"title": "3.2 Analysis: How Learning can Benefit Search", "content": "To better understand how learning can enhance search, we divide the 100 problems from the Game of 24 into three difficulty levels based on human success rates\u00b9. Other experimental settings follow those in Sec. 3.1. Our experimental results, presented in Fig. 1, reveal several key findings regarding the performance of existing search methods. Full analyses are in Appendix B.1.\nObs. 1: LLMs Perform Better for Simpler Problems. As in Fig. 1(a), VANILLA COT achieve pass rates of 18.2%, 12.1%, and 11.7% for problems with difficulty levels 1, 2, and 3, respectively. This pattern suggests that LLMs excel at direct problem-solving for simpler cases, indicating their potential for single-step solutions rather than requiring iterative approaches. More examples are in Fig. 5.\nObs. 2: Learning-Based Pruning Has Precision-Coverage Trade-offs. According to Fig. 1(a) and 1(b), BEAM SEARCH achieves the superior performance in pass rate than other LLM-based search methods, and significantly saves search steps than DFS. To deeply analyze this observation, an example of BEAM SEARCH in Fig. 6 shows that it only relies on LLMs to generate a few of possible next steps instead of decomposing all possible states in brute-force searches in each time of generating next intermediate steps. However, it is possible that some valid states are overlooked during the"}, {"title": "3.3 Analysis: Existing Search Algorithms", "content": "While existing search algorithms show significant promise, they still encounter notable challenges. To better illustrate their limitations, we conduct an analysis following the setup in Sec.3.1 by applying existing search algorithms to the Game of 24 task. The results are reported in Table 1, revealing the following insights: (i) Traditional brute-force algorithms achieve perfect 100% accuracy but require up to 3,429 search steps, demonstrating their inefficiency in searching for answers. (ii) While LLM-based methods significantly reduce the search space (at least 92.4% fewer steps than DFS), their pass rates peak at only 35%, highlighting their instability compared to traditional searches. Full analyses are in Appendix B.3\nThese findings underscore the necessity of a framework that combines the completeness of systematic search with the efficiency of learning-based approaches. This motivates the development of a more effective search-and-learning methodology to address these limitations."}, {"title": "3.4 SEAL: Search via Learning with LLMs", "content": "Notations. We focus on solving planning problem following the essence of solving real-world decision-making problems via search: starting from an initial state, envisioning possible actions, and systematically working toward a goal state. Specifically, a planning problem is formally defined as a tuple $\\mathcal{P} = (\\mathcal{S}, s_{init}, s_{goal}, \\mathcal{A}, f)$. Here, $\\mathcal{S}$ represents a finite and discrete set of states describing the world (i.e., state space). $s_{init}, s_{goal} \\in \\mathcal{S}$ denote the initial and goal world states, respectively. $\\mathcal{A} = \\{a_1, a_2,..., \\}$ represents the set of possible actions, and $f(s,a_i) = s'$ is a transition function mapping a state and action to a resulting state. A solution to problem $\\mathcal{P}$ is a sequence of actions $(a_1, a_2, . . .)$ that transforms $s_{init}$ into $s_{goal}$.\nBuilding upon the above insights, we introduce SEAL, a framework that systematically integrates learning capabilities into search processes to emulate human problem-solving strategies. The goal of SEAL is on enhancing this natural problem-solving process by combining the systematic nature of search with the learning capabilities of LLMs. SEAL consists of four components to enhance different aspects of search processes through learning:\nDirect Solution Generation. Motivated by the insight from Obs. 1 that learning excels at solving simpler problems directly, we begin each step by attempting a direct solution, akin to how humans first try to solve problems in one step:\n$r_{cur} = M(\\mathcal{P}_{solve}(s_{cur}))$ (1)\nwhere $M$ represents the backbone LLM and $\\mathcal{P}_{solve}$ denotes the solution generation prompt. A verifier"}, {"title": "4 Toward Complete Search via Learning", "content": "While SEAL effectively balances efficiency and effectiveness, our observation indicates potential compromises in solution accuracy. In high-stakes domains such as autonomous driving (Mao et al., 2023) and pandemic response planning (Du et al., 2024), ensuring completeness is fundamental to search algorithms, as overlooking any viable solution can have severe consequences. This motivates us to develop a search framework that rigorously ensures completeness. Next, we formalize search completeness and introduce SEAL-C, an enhanced variant of SEAL that integrates efficient learning-guided exploration with formal guarantees of completeness."}, {"title": "4.1 Formalizing Search Completeness", "content": "We begin by formalizing search completeness-the guarantee of finding a solution when one exists:\nDefinition 4.1 (Search Completeness). A search algorithm is complete if and only if for any initial state $s_{init} \\in \\mathcal{S}$ and goal state $s_{goal}$, whenever there exists a valid solution path $\\mathcal{P} = (s^0, ..., s^n)$ where $s^n \\in s_{goal}$, the algorithm is guaranteed to find it."}, {"title": "4.2 SEAL-C: Achieving Search Completeness", "content": "Building on this definition, we first analyze potential completeness compromises in SEAL, then present SEAL-C's mechanisms for ensuring rigorous completeness. The full algorithm of SEAL-C is shown in Alg. 2.\nHow Can Completeness Be Compromised? According to Sec.3.4, SEAL uses LLMs for state decomposition and validity checking, inspired by Obs. 2 and 3. However, our analysis in Sec.3.2 reveals that they may inadvertently ignore valid states during decomposition or prematurely terminate exploration of valid paths, compromising completeness."}, {"title": "Learning-Guided Complete State Decomposition", "content": "To ensure completeness while maintaining efficiency, SEAL-C employs a learning-guided complete state decomposition strategy by combining learning-based prioritization with a fallback mechanism for exhaustive state expansion:\n$S_{next} = M(\\mathcal{P}_{d}(s_{cur}))||(D(s_{cur}) \\backslash M(\\mathcal{P}_{d}(s_{cur}))) $ (5)\nwhere $||$ denotes ordered concatenation, ensuring LLM-generated states $M(\\mathcal{P}_{d}(s))$ are explored first. $D(s)$ is the complete state decomposition function from Eq. (2). This approach prioritizes exploration of likely valid states while ensuring no potential solution is overlooked, guaranteeing completeness while benefiting from learning-guided efficiency.\nTwo-phase Ranking. To further improve efficiency, SEAL-C introduces a two-phase ranking strategy for $S_{next}$. Instead of ranking all states at once, it first ranks and explores the LLM-generated states $M(\\mathcal{P}_{d}(s_{cur}))$, which are more likely to reach $s_{goal}$. Only when no solution is found does the algorithm proceed to rank and explore the remaining states from $D(s_{cur})$, which significantly reduces the search space by avoiding unnecessary ranking of supplementary states."}, {"title": "5 Can LLMs Learn to Search by Themselves?", "content": "Having integrated learning to enhance search, a reverse but natural question emerges: Can LLMs execute search autonomously to improve the reasoning capabilities? Recent advances in scaling test-time computation suggest that systematic exploration could enhance LLMs' problem-solving abilities. While models like QwQ-32B (Qwen, 2024b) show promising reasoning capabilities, they struggle with focused problem-solving, often producing unfocused, recursive outputs. This motivates us to explore how search benefits LLMs and whether LLMs possess the potential for self search. To systematically investigate LLMs' self-search capabilities, we consider two types of prompts: high-level self-search, which relies solely on LLMs' internal knowledge, and low-level self-search, which explicitly encodes SEAL's search strategies into the prompts. Further details and illustrative examples are in Appendix E, with additional discussions and analyses provided in Sec. 6.4."}, {"title": "6 Experiments", "content": "In this section, we conduct experiments to answer the following research questions: (RQ1) How ef-"}, {"title": "6.1 Experiment Settings", "content": "Tasks. To evaluate the effectiveness of SEAL, in addition to the Game of 24 used in Sec. 3.1, we select two widely adopted planning tasks for evaluating LLMs' planning abilities: Mini Crosswords (Yao et al., 2023a) and Blocksworld (Valmeekam et al., 2022). Further details on these tasks are provided in Appendix F.1.\nBaselines. Following the settings in Sec. 3.1, we consider four LLM-based searches, VANILLA COT, MAJORITY VOTE, BEST-OF-N, and BEAM SEARCH, and three traditional searches, DFS, BFS and EXHAUSTIVE SEARCH. We also involve a baseline BEAM SEARCH+RV that adds a rule-based verifier to the beam search method. Details on the baselines are provided in Appendix F.2.\nModels. Our experiments use both closed-source models (GPT-40-mini, GPT-40 (Hurst et al., 2024)) and open-source models (Qwen2.5-72B-Instruct (Qwen, 2024a), QwQ-32B-Preview (Qwen, 2024b), DeepSeek-R1 (Guo et al., 2025)). Note that GPT-40-mini is a small language model (SLM), and QwQ-32B-Preview and DeepSeek-R1 are the state-of-the-art LRMs. Model and implementation details are in Appendix F.3 and F.5.\nEvaluation Metrics. We follow Sec. 3.1 to use two metrics: (i) pass rates (PR) and (ii) search steps (SS). More details are in Appendix F.4."}, {"title": "6.2 RQ1: Effectiveness and Efficiency Evaluations", "content": "We conduct experiments on the three tasks using three different LLM backbones: GPT-40-mini, GPT-40, and Qwen2.5-72B-Instruct. Detailed task setups are in Appendix G. The results are reported in Table 2. The key observations are: (i) SEAL significantly reduces search steps compared to brute-force searches. Specifically, it reduces search steps by up to 99.1% compared to DFS and still achieves state-of-the-art search steps compared to other LLM-based methods. This validates SEAL'S efficiency in navigating the search space. (ii) SEAL achieves a near-perfect pass rate across all settings, outperforming other LLM-based methods. BEAM SEARCH+RV achieves better pass rates than BEAM SEARCH, indicating that integrating rule-based ver-"}, {"title": "6.3 RQ2: Impact of Problem Difficulty on Search Completeness", "content": "We evaluate SEAL-C's performance across problem difficulties in three tasks. For Game of 24, we use three difficulty levels (Sec. 3.2); for Blocksworld, difficulty scales with minimum required action steps (2\u201312). Other settings follow Sec. 6.1. The results using GPT-40-mini in the two tasks are reported in Fig. 3, which show that: (i) SEAL-C achieves 100% pass rates across all difficulty levels, outperforming baseline methods whose performance degrades progressively, confirming its completeness. (ii) As the difficulty increases, SEAL-C's search steps increase slightly but remain significantly lower than brute-force baselines. This efficiency is attributed to SEAL-C's learning-guided complete state decomposition and two-phase ranking, which prioritize promising states and effectively reduce the search space. Mini Crosswords results are in Appendix I."}, {"title": "6.4 RQ3: Potential of LLMs in Self-Search", "content": "We conduct an initial exploration to study whether LLMs can learn to self-search for problem-solving by testing two self-search prompts in Sec. 5 and"}, {"title": "6.5 Ablation Studies", "content": "Inspired by Snell et al. (2024), we conduct ablation studies to understand the impact of search budgets on SEAL's performance. Instead of terminating the search upon finding the final state, we introduced a pre-defined search step budget, where SEAL terminates early if the budget is reached. We vary the search step budgets as {10, 20, 30, 50, 100, 150, 200}, and compare various search methods using GPT-40-mini in Game of 24. The comparison results are reported in Fig. 4. Our analysis reveals three key findings: (i) SEAL consistently outperforms other methods across all search budgets, demonstrating its effectiveness in accurately solving problems even under constrained search budgets. (ii) Pass rates for all search methods generally improve as the search budget increases, aligning with expectations that scaling test-time computation enhances search performance. (iii) When the search budget is small (less than 50 steps), BEAM SEARCH performs"}, {"title": "7 Conclusion and Future Work", "content": "In this paper, inspired by the principles of search and learning (Sutton, 2019; Snell et al., 2024), we systematically investigate the integration of learning into search. We first explore how learning benefits the search process via LLMs, demonstrating that LLMs improve search efficiency by reducing search space. Then, building on these insights, we introduce a novel framework, SEAL, and its variant SEAL-C, designed to combine the reasoning capabilities of LLMs with search strategies to achieve efficient and accurate problem-solving. Extensive experiments conducted on three real-world planning tasks demonstrate that SEAL achieves near-perfect pass rates across various settings while significantly reducing search spaces, showcasing the effectiveness and efficiency of SEAL. Furthermore, we also explore how search can benefit LLMs, evaluating whether LLMs can develop self-search capabilities. We show that search significantly enhances their reasoning and learning performance. These findings highlight the bidirectional synergy between search and learning, emphasizing the po-"}, {"title": "8 Limitations", "content": "One potential limitation of this work is the necessity of encoding our search strategies into prompts to enable LLMs to perform self-search. In the future, it is worth exploring how to allow LLMs to autonomously conduct self-search during reasoning without explicit supervision within the prompts. Additionally, our work primarily focuses on LLMs; an important direction for future investigation is to assess the applicability of our search strategy to multi-modal LLMs."}, {"title": "9 Impact Statements", "content": "This paper introduces SEAL and SEAL-C, frameworks designed to enhance search processes by integrating the reasoning capabilities of LLMs with structured search strategies. The potential societal impacts of this research are broad but largely align with the established consequences of improving computational problem-solving techniques. Enhanced search efficiency and the ability to leverage LLMs for self-search could benefit applications in diverse fields such as healthcare, logistics, education, and robotics, where intelligent decision-making is crucial. However, as with all advancements in AI, there are ethical considerations. For example, the misuse of improved search strategies in domains such as automated surveillance or adversarial systems could lead to privacy or security concerns. We encourage researchers and practitioners to apply this work responsibly and ensure it aligns with ethical guidelines. We see no immediate risks or unintended negative consequences specific to this work that require urgent attention. This paper primarily contributes to foundational research in search and learning integration. Future exploration of self-search capabilities in LLMs will include careful assessment of ethical implications to ensure responsible development and deployment of these technologies."}]}