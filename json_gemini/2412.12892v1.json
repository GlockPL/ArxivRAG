{"title": "SAUGE: Taming SAM for Uncertainty-Aligned Multi-Granularity Edge Detection", "authors": ["Xing Liufu", "Chaolei Tan", "Xiaotong Lin", "Yonggang Qi", "Jinxuan Li", "Jian-Fang Hu"], "abstract": "Edge labels are typically at various granularity levels owing to the varying preferences of annotators, thus handling the subjectivity of per-pixel labels has been a focal point for edge detection. Previous methods often employ a simple voting strategy to diminish such label uncertainty or impose a strong assumption of labels with a pre-defined distribution, e.g., Gaussian. In this work, we unveil that the segment anything model (SAM) provides strong prior knowledge to model the uncertainty in edge labels. Our key insight is that the intermediate SAM features inherently correspond to object edges at various granularities, which reflects different edge options due to uncertainty. Therefore, we attempt to align uncertainty with granularity by regressing intermediate SAM features from different layers to object edges at multi-granularity levels. In doing so, the model can fully and explicitly explore diverse \"uncertainties\" in a data-driven fashion. Specifically, we inject a lightweight module (~ 1.5% additional parameters) into the frozen SAM to progressively fuse and adapt its intermediate features to estimate edges from coarse to fine. It is crucial to normalize the granularity level of human edge labels to match their innate uncertainty. For this, we simply perform linear blending to the real edge labels at hand to create pseudo labels with varying granularities. Consequently, our uncertainty-aligned edge detector can flexibly produce edges at any desired granularity (including an optimal one). Thanks to SAM, our model uniquely demonstrates strong generalizability for cross-dataset edge detection. Extensive experimental results on BSDS500, Muticue and NYUDv2 validate our model's superiority.", "sections": [{"title": "Introduction", "content": "Edge detection is a fundamental AI task in low-level vision that plays a crucial role in image understanding. It is of great value in supporting various high-level vision tasks, including semantic segmentation (Yu et al. 2021), image enhancement (Nazeri et al. 2019; Xu, Wang, and Lu 2023), object detection (Qin et al. 2019; Liu, Hou, and Cheng 2020; Yao and Wang 2023), image translation (Jiang et al. 2023), etc.\nHowever, due to the observers' diverse levels of visual perception (Zhou et al. 2023), there are often multiple ground truth edge maps for a given image. The inconsistency between different edge maps can cause uncertainty in decision-making, complicate the model training, and reduce the overall performance. The scarcity of human annotations may further exacerbate this issue.\nMost previous works (Liu et al. 2017; He et al. 2019; Pu et al. 2022) tend to overlook the uncertainty in annotations by treating each edge map as equally valid, or fuse variant edge maps into a unified version by a simple voting strategy. As a result, these methods typically fail to capture the inherent subjectivity in the annotations. They can only produce a single edge map for a given image, lacking the ability to control granularity. This limitation severely declines their applicability and scalability in real-world scenarios.\nTo cope with the uncertainty issue of edge labels is non-trivial and remains relatively under-explored. UAED (Zhou et al. 2023) handles edge labels from a probabilistic perspective. It is assumed that the uncertainty of edge labels could be modeled using a Gaussian distribution, allowing the generation of multiple edges by sampling from the distribution. However, the distribution assumption imposes strict constraints, yielding limited diversity and a lack of control over granularity. RankED (Cetinkaya, Kalkan, and Akbas 2024) approached to calculate the label certainty of each pixel among different annotators, then favor the pixels with higher confidence during training. Consequently, pixel annotations with high uncertainty are largely overlooked. More recently, MuGE (Zhou et al. 2024) devised a granularity-controllable edge detector, which remedies label uncertainty by producing edge maps at various granularity levels. They naively assign a binary granularity score to its most simple (0) and complex (1) edge maps for an image to train a binary classifier, which is then applied to assign granularity scores (0-1) to rest edge maps. Then the granularity score is explicitly embedded in the edge detector, making it measurable and controllable. Unfortunately, this may easily introduce bias as a complex edge map might be incorrectly marked as simple if all the annotations are complex, and vice versa.\nIn this work, following the core idea of MuGE which models uncertainty by generating multi-scale edge maps, we seek to develop an edge detector that can produce granularity-controllable outputs without requiring granularity labels for edge maps. This can be achieved by progressively fusing and projecting intermediate features of the segmentation foundation model SAM (Kirillov et al. 2023) to edge map variants with increasing granularities. We demonstrate that SAM is highly effective in edge detection in the multi-scale setting. Intuitively, SAM excels in locating object boundaries, thus potentially providing strong prior knowledge of edge maps. Besides, we find out that the features from intermediate layers of SAM naturally encode rich information about various granularities for object edges.\nHowever, unlocking the full potential of SAM for multi-granularity edge detection presents significant challenges. As aforementioned, SAM inclines to provide redundant details and coarse object boundaries rather than detailed internal edges, as shown in Figure 1 (d). To fill the gap and capture uncertainty through granularity modeling, we incorporate a lightweight feature transfer network into SAM that intermediate features are gradually fused and projected into side outputs of edges at increasingly complex levels. To supervise the transfer network, linear blending is performed to the real edge labels to synthesize pseudo edges at diverse granularities. This step crucially normalizes the granularity levels of all edge labels, easing the convergence of network training. Moreover, we develop a novel diversity loss to encourage the obtained side edge maps to be sufficiently diverse, further enhancing uncertainty modeling.\nOur main contributions can be summarized as follows:\n\u2022 We propose a novel edge detector, named SAUGE, which sidesteps the uncertainty difficulty by modeling edge detection in a multi-granularity setting based on SAM. The obtained edge detector enables edge detection at any desired granularity level.\n\u2022 A lightweight module is proposed to be injected into the frozen SAM to progressively fuse and transfer its intermediate features to generate edges from coarse to fine. Linear blending is performed to real edge labels to synthesize normalized supervision cues.\n\u2022 We conduct extensive experiments on the BSDS500, Multicue and NYUDv2 datasets, and the results demonstrate that our model achieves the new state-of-the-art and suggest a strong generalizability on unseen datasets."}, {"title": "Related Work", "content": "Edge detection. Edge detection has been a significant re-search area for a long time. Traditional techniques such as (Kittler 1983; Canny 1986) rely on gradient computation on low-level features which are easy to be affected by noise.\nOver the past decade, deep learning-based approaches, such as (Shen et al. 2015; Liu and Lew 2016; He et al. 2019; Su et al. 2021), have risen to prominence, focusing on designing network architectures that surpass human-level performance. The majority of these methods, including RCF (Liu et al. 2017), leverage pre-trained VGG16 (Simonyan and Zisserman 2014) as their backbone. More recently, advanced methods like EDTER (Pu et al. 2022) incorporated Transformers to enhance edge detection. Research based on uncertainty explores the uncertainty caused by multiple labels. UAED (Zhou et al. 2023) models edge maps as multivariate Gaussian distribution and use predicted variance to measures the uncertainty, RankED (Cetinkaya, Kalkan, and Akbas 2024) sorts pixels to balance edge and non edge pixels and promote higher label certainty for high confidence edges. MuGE (Zhou et al. 2024) integrates encoded edge granularity into feature maps to produce edge maps at multiple granularities for alleviating uncertainty. Recent image generation methods such as DiffusionEdge (Ye et al. 2024) applies diffusion models to generate crisp edge maps.\nMost of these methods do not utilize prior knowledge from advanced tasks. In contrast, we explore the priors of semantic-aware features in SAM and construct multiple granularity edge maps to address the uncertainty.\nExploring SAM for downstream tasks. The Segment Anything Model (SAM) (Kirillov et al. 2023) accepts intuitive prompts (points or bounding boxes), and has established a new benchmark in natural image segmentation. It has demonstrated impressive performance across various downstream tasks, including medical imaging (Ma et al. 2024; Gu et al. 2024), object segmentation in challenging conditions (Chen et al. 2024; Zhang et al. 2024; Ke et al. 2024) and image inpainting (Yu et al. 2023).\nThe work most closely related to ours is EdgeSAM (Yang et al. 2024), which introduces an adapter module to fine-tune SAM for edge detection. However, their approach uses SAM in a simplistic manner, without fully exploring its potential and modeling the uncertainty. In contrast, we leverage the granularity-aware prior knowledge embedded in SAM's features and attempt to align uncertainty with granularity."}, {"title": "Method", "content": "The overall framework of the proposed uncertainty-aligned edge detector (SAUGE) is presented in Figure 2. As shown, our SAUGE is built upon the pre-trained SAM, utilizing the proposed lightweight Side Transfer Network (STN) to explicitly explore the knowledge embedded in SAM for multiple granularity edge detection. STN gradually uses Feature Fuse Block (FFB) to fuse intermediate features extracted from different stages of SAM to supplement edge-aware details, thereby gradually constructing side outputs represent-ing edge at varied granularity levels. Finally, STN fuses all the features of side outputs to generate final output. To ensure the quality of side outputs, we construct pseudo labels corresponding to varied granularity levels to supervise and emphasize diversity of side outputs. We also propose to use the masks outputted by SAM to improve our edge learning."}, {"title": "Aligning uncertainty through granularity modeling", "content": "The proposed Side Transfer Network (STN) aims to fill the gap and capture uncertainty through granularity modeling. Intermediate features of SAM are gradually fused and projected into side outputs of edges at increasingly complex levels, allowing the model to fully and explicitly explore diverse uncertainties with granularity.\nSpecifically, we feed the image $X \\in \\mathbb{R}^{H \\times W \\times 3}$ into the frozen SAM with the prompt of 8 \u00d7 8 points grid, obtaining shallow feature maps $E_s$ (i.e. the output of the first block in the encoder), image embeddings $E_i$ from the SAM encoder, and mask embeddings $E_m$ from the decoder.\nFor features ${E_s, E_i}$, we transform them into edge-aware features ${E_s', E_i'} \\in \\mathbb{R}^{D \\times D \\times C_1}$ using learnable convolution layers, where $D \\times D$ represents the spatial resolution and $C_1$ is feature channels. For the mask embedding feature $E_m$, we first rescale the feature channel to $C_2$ using learnable convolutional layers, and then reshape the features into edge-aware features $E_m' \\in \\mathbb{R}^{D \\times D \\times (C_2 \\times (8 \\times 8))}$, ensuring all edge-aware details from the prompt is integrated. The detailed process is formulated as follows:\n$E'_i = W_i(E_i), E'_s = W_s(E_s), E'_m = R(W_m(E_m))$ (1)\nwhere $W_i(\\cdot), W_s(\\cdot), W_m(\\cdot)$ are convolution layers, $R(\\cdot)$ is the reshape operator.\nThe Feature Fusion Block (FFB) is developed inside the STN to gradually aggregate edge-aware features ${E'_i, E'_s, E'_m}$. Inspired by (Wu et al. 2023), we construct the FFB as a stack of cross-attention operations and Gated-Dconv Feed-Forward Network proposed by (Zamir et al. 2022). The process of constructing edge features at different granularity levels can be summarized as follows:\n$F_c = W_c^h(E_s'), E_m' = FFB_1(E_i'\\bullet W_c^l(F_c), E_s'),$\n$F_m = W_m^h(E_m'), E_f' = E_m'\\bullet W_m^l(F_m),$\n$F_f = W_f^h(E_f'), E_f' = FFB_2(E_f, E_m'),$ (2)\nwhere $W_c^h(\\cdot), W_m^h(\\cdot), W_f^h(\\cdot), W_c^l(\\cdot), W_m^l(\\cdot), W_f^l(\\cdot)$ are convolution layers, $FFB(\\cdot)$ is the Feature Fuse Block. Specifically, we directly generate the coarse-grained edge feature $F_c$ using the $E_s'$. Then, we use the first FFB to fuse $E_i'$ and $E_s'$ for including more details, which forms our medium-grained edge feature $F_m$. The features are further fused with $E_m'$ by another FFB, resulting our fine-grained edge feature $F_f$.\nWith the edge features ${F_c, F_m, F_f}$ prepared, we then employ a shared classification head $H$ to generate the side outputs ${\\hat{Y_c}, \\hat{Y_m}, \\hat{Y_f}}$, which represents edges at different granularity levels. The features of these side outputs are then concatenated to form the final output $\\hat{Y^u}$. The detailed process is elaborated as follows:\n$\\hat{Y_c} = H(F_c), \\tilde{F_m} = W_m^a([F_m, F_c]),$\n$\\hat{Y_m} = H(F_m), \\tilde{F_f} = W_f^a([F_f, \\tilde{F_m}]),$\n$\\hat{Y_f} = H(F_f), \\hat{Y^u} = H(W_u^a([F_f, \\tilde{F_m}, \\tilde{F_f}])),$ (3)\nwhere $W_m^a(\\cdot), W_f^a(\\cdot), W_u^a(\\cdot)$ are convolutions, $H$ is output head shared across varied side outputs, $[\\cdot]$ is concatenation."}, {"title": "Generation of edge at arbitrary granularity", "content": "Given side outputs at three granularity levels (coarse-grained $\\hat{Y_c}$, medium-grained $\\hat{Y_m}$, fine-grained $\\hat{Y_f}$), we propose to generate edge maps of arbitrary granularity $\\hat{Y^{\\alpha}}$ by a simple linear weighting strategy. Specifically, following the work of MuGE (Zhou et al. 2024), we measure the granularity level using $\\alpha \\in [0, 1]$, where 0 represents the coarsest and 1 represents the finest. Then, the process of generating arbitrary granularity edge graph $\\hat{Y^{\\alpha}}$ can be formulated as follows:\n$\\hat{Y^{\\alpha}} = \\begin{cases}\n\\frac{\\alpha}{0.5}\\hat{Y_m} + (1 - \\frac{\\alpha}{0.5})\\hat{Y_c}, & 0 \\leq \\alpha \\leq 0.5\\\\\n\\frac{\\alpha - 0.5}{0.5}\\hat{Y_f} + (1 - \\frac{\\alpha - 0.5}{0.5})\\hat{Y_m}, & 0.5 < \\alpha < 1\n\\end{cases}$ (4)"}, {"title": "Loss functions for model training", "content": "Three loss functions are formulated to train our framework with three side outputs and one final output for detecting edges at multiple granularity: the granularity-aligned loss function for supervising side outputs with pseudo labels, the diversity loss function for enhancing the difference between different side outputs, and the guide loss constraints the produced edges to be compatible with SAM masks.\nPseudo labels at varied granularities. Given the input image $X$ and its manual annotations ${Y_n}_{n=1}^N$, where $N$ is the number of annotations, and $Y_n \\in \\{0, 1\\}^{H\\times W}$ is the annotation given by the n-th annotator. We sort all annotations in ascending order based on the number of pixels labeled as edges. Denote the sorted annotations as ${Y_n^s}_{n=1}^N$, then the coarse-grained annotation $Y^c$, medium grained annotation $Y^m$, and fine-grained annotation $Y^f$ can be constructed as:\n$Y^c = Y_1^s, Y^m = Y_1^s \\vee Y_2^s, Y^f = Y_1^s \\vee Y_2^s \\vee Y_3^s,$ (5)\nwhere $\\vee$ is element-wise OR operation. The side outputs at different granularity levels ${\\hat{Y_c}, \\hat{Y_m}, \\hat{Y_f}}$ are supervised by the corresponding ${Y^c, Y^m, Y^f}$.\nIn order to comprehensively consider all annotations and catch the uncertainty caused by multi-label, inspired by (Zhou et al. 2023), the distribution of final label $Y^u$ is obtained by randomly sampling from a multivariate Gaussian distribution $\\mathcal{N}(\\mu_Y, \\sigma_Y)$ and then binarizing $Y^u$ to the final label $\\Upsilon^u$ using the threshold $\\zeta$, where the mean $\\mu_Y$ and variance $\\sigma_Y$ are pixel-wise calculated by the label set ${Y_n}_{n=1}^N$.\nGranularity-aligned loss for side outputs. Edge detection is a binary classification task where each pixel needs to be predicted as an edge (positive) or not (negative), hence binary cross-entropy(BCE) loss is widely used in this task. However, the edge pixels in an image are usually only a tiny fraction of all pixels. Following (Xie and Tu 2015), we adaptively weight each pixel based on the ratio of positive and negative samples in the ground truth. For the side outputs ${\\hat{Y_c}, \\hat{Y_m}, \\hat{Y_f}}$, we minimize the following BCE loss:\n$\\mathcal{L}_{odetect} = -\\frac{1}{H W} \\sum_{j=1}^{H W} (\\Upsilon_j^g\\beta_g log(\\hat{Y_j^g}) + (1 - \\Upsilon_j^g)(1 - \\beta_g)log(1 - \\hat{Y_j^g})),$ (6)\nwhere $\\beta$ is used to balance the contribution of varied pixels, $g \\in \\{c, m, f, u\\}$ represents the granularity level, and j represents the j-th pixel in the prediction and ground truth. In addition, $\\beta_g = \\frac{\\mid\\Upsilon^g\\mid}{(\\mid\\Upsilon^g\\mid + \\mid\\Upsilon^g\\mid)}$, $\\mid \\cdot \\mid$ represents the total number of pixels, and $\\Upsilon^g$ and $\\Upsilon_\\varnothing^g$ represents positive and negative samples in ground truth $\\Upsilon^g$.\nGiven the pseudo labels at varied granularities ${Y^c, Y^m, Y^f}$, the loss for supervising the side outputs to align granularity is defined as:\n$\\mathcal{L}_{side} = \\sum_{g\\in\\{c,m,f\\}} \\mathcal{L}_{odetect}^g$ (7)"}, {"title": "Emphasize the difference between side outputs", "content": "Different side outputs represent edge maps at different granularity levels, we use mean absolute error (MAE) to emphasize the diversity of side outputs:\n$\\mathcal{L}_{differ}^{(i,k)} = \\frac{1}{H W} \\sum_{j=1}^{H W} \\mid\\hat{Y_j^i} - \\hat{Y_j^k}\\mid \\otimes (\\Upsilon_j^i \\oplus \\Upsilon_j^k),$ (8)\nwhere (i, k) is about the combination of $\\{c,m, f\\}$ with a size of 2, $\\mid\\cdot\\mid$ represents taking the absolute value, $\\oplus$ represents element-wise XOR operation. In this way, different pixels with edge definitions can be located, and two different side outputs can have differences on these pixels. $\\mathcal{L}_{differ}$ is obtained by enumerating combinations (i, k) and summing $\\mathcal{L}_{differ}^{(i,k)}$ as:\n$\\mathcal{L}_{differ} = \\mathcal{L}_{differ}^{(c,m)} + \\mathcal{L}_{differ}^{(c,f)} + \\mathcal{L}_{differ}^{(m,f)}$ (9)\nThe use of $\\mathcal{L}_{differ}$ promotes the generation of rougher $\\hat{Y_c}$ , ensuring the difference between the side outputs.\nGuide loss for final output. Here, we further use the object masks outputted by SAM to guide our edge learning. Given the object mask M outputted by SAM, we use the Sobel (Kittler 1983) operator to extract the edges of each mask and denote it as $\\Upsilon_{mask}$. Meanwhile, we calculate maps $\\Psi_{mask} \\in \\mathbb{R}^{H \\times W}$ which indicate the frequency of the corresponding pixel is recognized as edge on each mask.\n$w_j = -(Y_{mask})_j$, (10)\n$\\mathcal{L}_{guide} = \\sum_{j=1}^{H W} \\psi_j w_j \\mathcal{L}_{odetect}^u$ (11)"}, {"title": "Experiments", "content": "We conduct experiments on three widely-used edge detection datasets: BSDS500 (Arbelaez et al. 2010), Multicue (M\u00e9ly et al. 2016) and NYUDv2 (Silberman et al. 2012). For data augmentation, we adopt the same strategy as UAED (Zhou et al. 2023) across both datasets.\nBSDS500 consists of 500 natural images, with 200 for training, 100 for validation, and the remaining for test. Each image has 4 to 9 manual annotations. Additionally, the PASCAL VOC set (Everingham et al. 2010) with 10,103 images is used as supplementary training data, with edge annotations derived from semantic masks using Laplacian detector.\nMulticue includes 100 images from complex natural scenes, each of which is annotated by multiple individuals. We randomly split these images into training and evaluation sets, with 80 images for training and 20 for testing. This process is repeated three times and average scores are reported.\nNYUDv2 is a dataset for indoor scene parsing and edge detection, containing 1,449 paired RGB-D images. Each image has a single ground-truth edge map, with the dataset split into 381 training, 414 validation, and 654 testing images.\nImplementation Details\nWe implement SAUGE based on PyTorch (Paszke et al. 2019), and use SAM pre-trained on the SA-1B dataset (Kir-"}, {"title": "Comparison with State-of-the-arts", "content": "Existing methods can be divided into two categories: single definite output and multi-granularity output. For single definite output, traditional methods include Canny (Canny 1986) and OEF (Hallman and Fowlkes 2015). CNN-based methods, such as DeepContour (Shen et al. 2015), DeepBoundary (Kokkinos 2015), HED (Xie and Tu 2015), RDS (Liu and Lew 2016), CED (Wang, Zhao, and Huang 2017), BDCN (He et al. 2019), DSCD (Deng and Liu 2020), and LDC (Deng et al. 2021), are widely adopted. Transformer-based methods involve EDTER (Pu et al. 2022), RankED (Cetinkaya, Kalkan, and Akbas 2024) and EdgeSAM (Yang et al. 2024). The diffusion-based method involves DiffusionEdge (Ye et al. 2024). Currently, only MuGE (Zhou et al. 2024) is developed for multi-granularity output.\nResults on BSDS500. Since SAUGE can generate both definite output $\\hat{Y^u}$ and multi-granularity outputs $\\hat{Y^{\\alpha}}$, we compare it with excellent representatives of these two types of methods respectively. The quantitative results are presented in Table 1 and Table 2, where M=3 represents $\\alpha$= {0, 0.5, 1}, and M=11 represents the 11 outputs generated from 0 to 1 at intervals of 0.1. Experiments were conducted under two settings: Single-Scale input (SS) and Single-Scale input with additional PASCAL VOC data for training (SS-VOC).\nIn Table 1, we report the performance of existing state-of-the-art methods for definite output. As shown, our SAUGE achieves the best performances in both ODS and OIS metrics under all experimental settings (SS and SS-VOC), which significantly outperforms EdgeSAM with the same SAM backbone. It is worth noting that our model only needs to tune a very small number of parameters--82% fewer compared to EdgeSAM. When using a larger SAM (ViT-L SAM) as the backbone (denoted as SAUGE-L), the performance improves by 0.009 in ODS and 0.016 in OIS under the SS setting, and by 0.011 in ODS and 0.014 in OIS under the SS-VOC setting. Qualitatively, Figure 3 compares our SAUGE with other methods, demonstrating that SAUGE can produce more detailed predictions. We note that the background edges missed by SAM can be successfully obtained by our model. The Precision-Recall curve is shown in Figure 4.\nFor multi-granularity output, Table 2 shows that SAUGE significantly outperforms MuGE in all settings. It can be found that increasing the number of candidates (M) leads to higher performance, demonstrating the advantages of multi-granularity edge modeling in addressing uncertainty.\nResults on Multicue. We also conduct experiments on Multicue edges, and the results are illustrated in Table 3. As can be seen, the proposed SAUGE outperforms the previous state-of-the-art in the ODS metric and achieves second-place performance in the OIS metric, with scores of 90.5% in ODS and 90.7% in the OIS metric. The relatively lower performance in the AP metric may be attributed to overfit-"}, {"title": "Ablation Study and Further Analysis", "content": "Effect of key components. The key designs of our SAUGE lie in the lightweight Side Transfer Network (STN), the constraints on the multi-granularity side outputs, and the mask-guided loss $\\mathcal{L}_{guide}$. To verify the effectiveness of these designs, we begin with SAM as the baseline, and then sequentially add the Side Transfer Network, the side output constraints (denoted as SOC), and the mask-guided loss. The results on the BSDS500 dataset are presented in Table 5. As shown, for both pre-trained ViT-B SAM and ViT-L SAM baseline, the inclusion of our lightweight STN module leads to a significant improvement across all metrics, by more than 9% in ODS, 8.7% in OIS, 10.8% in AP metrics. Also, with the constraints on side output (SOC), our framework can generate edges at arbitrary granularity and achieve a notable enhancement in the AP metric. We attribute this to the fact that SOC explicitly facilitates the model in capturing different granularity information, thus enabling better alignment with the uncertainty in edge decisions. Additionally, mask-guided loss $\\mathcal{L}_{guide}$ also contributes positively to all metrics.\nTo study the impact of $\\mathcal{L}_{differ}$, Figure 6 visualizes model outputs with and without it. As shown, the $\\mathcal{L}_{differ}$ amplifies the differences between outputs at various granularities, further enhancing alignment with edge decision uncertainty."}, {"title": "Conclusion and limitation", "content": "In this paper, we designed a novel uncertainty-aligned edge detector called SAUGE. In our SAUGE, we developed a lightweight Side Transfer Network (STN) to explicitly explore the knowledge embedded in SAM for multiple granularity edge detection. Extensive experiments on three edge sets are reported to demonstrate the superiority of SAUGE.\nLimitation. Our SAUGE is constructed based on the SAM backbone model. In the future, we would like to extend our framework for a more efficient backbone model."}]}