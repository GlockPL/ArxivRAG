{"title": "Large Language Model for Verilog Generation with Golden Code Feedback", "authors": ["Ning Wang", "Bingkun Yao", "Jie Zhou", "Xi Wang", "Zhe Jiang", "Nan Guan"], "abstract": "Recent advancements in large language models (LLMs) have catalyzed significant interest in the automatic generation of Register-Transfer Level (RTL) code, particularly Verilog, from natural language instructions. While commercial LLMs like ChatGPT have dominated this domain, open-source alternatives have lagged considerably in performance, limiting the flexibility and data privacy of this emerging technology. This study introduces a novel approach utilizing reinforcement learning with golden code feedback to enhance the performance of pre-trained models. Leveraging open-source data and base models, we have achieved state-of-the-art (SOTA) results with a substantial margin. Notably, our 6.7B parameter model VeriSeek demonstrates superior performance compared to current best-in-class 13B and 16B models. Furthermore, through a comprehensive analysis of the limitations in direct fine-tuning and the training dynamics of reinforcement learning, we posit that the development of comprehensive supervisory signals, which are align with the inherent parallel semantics of Verilog code, is critical to effective generation. The code and data associated with this research are publicly available at https://github.com/CatIIIIIIII/veriseek. The model weights can be accessed at https://huggingface.co/WANGNingroci/VeriSeek.", "sections": [{"title": "Introduction", "content": "In recent years, the field of natural language processing (NLP) has witnessed a paradigm shift with the advent of large language models (LLMs), exemplified by GPT [2]. These models have demonstrated unprecedented capabilities in various linguistic tasks. Inspired by this remarkable progress, researchers in the domain of hardware design have begun to explore the potential of LLMs in revolutionizing hardware development methodologies.\nAmong diverse applications, the automatic generation of RTL designs based on natural language specifications has emerged as a particularly promising and widely studied direction. This task aims to transform high-level functional descriptions in natural language into fully-fledged Hardware Description Language (HDL) code, such as Verilog, VHDL, ab initio. In contrast to the well-established predictive machine learning (ML) methodologies in EDA, these generative techniques offer a more direct and potentially renewal impact on the hardware design and optimization process.\nA subset of the current literature has primarily concentrated on the development and refinement of prompt engineering methodologies, leveraging commercial Large Language Models (LLMs) such as GPT. The utilization of commercial LLMs, while offering immediate access to powerful language processing capabilities, inherently constrains the degree of customization and adaptation possible for domain-specific tasks such as RTL generation. This approach, while expedient, may not fully address the unique challenges and nuances inherent in hardware description languages and digital design"}, {"title": "Method", "content": "2.1 Continual Pre-training Utilizing C-Language Integrated Dataset\nIn the process of continual pre-training, we utilized the publicly available VGen dataset, as developed by [15]. VGen aggregates Verilog repositories from GitHub, systematically filters out duplicates and excessively large files, and retains only those files containing module and endmodule statements. Additionally, it extracts text from 70 Verilog textbooks using pymuPDF, subsequently cleans the data, and retains only the verified Verilog syntax. Following this, we further refine the dataset to include only Verilog code, resulting in a final cleaned VGen dataset of approximately 200MB in size.\nDuring implementation, we discovered that C programs could assist the model in understanding and generating Verilog code. This intriguing phenomenon was also observed and utilized by [10], who treated the corresponding C code as a functional description. Consequently, we incorporated the CodeSearchNet dataset [5], which contains approximately 40MB function codes and their documentation.\n2.2 Reinforcement Learning with Golden Code Feedback\n2.2.1 Dataset\nWe gathered high-quality specification-code pairs from Opencores [3], a community aimed to developing digital open-source hardware using electronic design automation (EDA). We then filtered"}, {"title": "Proximal Policy Optimization", "content": "We propose to formulate Verilog program generation as a reinforcement learning (RL) problem, aiming to further align the pre-trained model \\(\\pi_{\\theta}\\) with golden code preferences. Following the Reinforcement Learning from Human Feedback (RLHF) procedure as proposed in [9; 18], we fine-tuned the pre-trained model on our environment using Proximal Policy Opzimization (PPO, [12]). The environment is a bandit environment which receives a designer specification and generate a response to the specification. Given the generated code and golden code, it produces a reward determined by the reward function and ends the episode. In addition, we add a per-token KL penalty from the pre-trained model at each token to mitigate over-optimization.\nConsider a large language model (LLM) as a policy \\(\\pi_{\\theta}(\\hat{y} | x)\\) parameterized by \\(\\theta\\). The policy \\(\\pi_{\\theta}\\) is designed to receive user an instruction \\(x \\in X\\) and generate a text response \\(\\hat{y} \\in Y\\). Here we only consider single-turn conversations to simplify notations and modelling. Given a specification x, the LLM \\(\\pi_{\\theta}\\) will generate a response \\(\\hat{y}\\) in an auto-regressive manner:\n\\[\\pi_{\\theta}(\\hat{y} | x) = \\prod_{t} \\pi_{\\theta}(\\hat{y}_t | x, \\hat{y}_{<t}),\\]\nwhere \\(\\hat{y}_t\\) is the t-th token in the response and \\(\\hat{y}_{<t}\\) represents the tokens in the response before \\(\\hat{y}_t\\). The objective function between the generated code and the golden code is:\n\\[J_r(\\pi_{\\theta}) = E_{x\\sim P_{data}, \\hat{y}\\sim \\pi_{\\theta}} \\left[ r(\\hat{y}, y) - \\beta \\log \\frac{\\pi_{\\theta}(\\hat{y} | x)}{\\pi_{ref}(\\hat{y} | x)} \\right],\\]\nwhere \\(r\\) represents the reward function that reflects golden code preferences. It takes a response \\(\\hat{y}\\) and the corresponding golden code \\(y\\) as input, producing a scalar value as output. The model \\(\\pi_{ref}\\) serves as the reference model used to regularize \\(\\pi_{\\theta}\\) through Kullback\u2013Leibler (KL) divergence. The constant \\(\\beta\\) is introduced to control the degree of regularization."}, {"title": "Defining Reward by Golden Code AST", "content": "Parsing code to AST. Unlike traditional language models that treat code as simple sequences of subword tokens, we leverages the Abstract Syntax Tree (AST) to gain deeper semantic insights. For the purpose of parsing, we assume the provided code is syntactically valid \u2014 a reasonable assumption for code understanding. We employ Pyverilog [14], which is a popular open-source toolkit for RTL design, to construct ASTs. In this structure, each subtree represents a consecutive span of subword tokens, while every leaf node corresponds to an individual token.\nReward definition. For each generated Verilog code segment sequence \\(\\hat{y}\\), the reward r is defined by calculating the similarity between the AST of the response and the AST of the label, provided the response is valid. The reward r is then given by:\n\\[r(\\hat{y}, y) =\\begin{cases}\n-10.0, & \\text{if } \\hat{y} \\text{ contains no valid Verilog code segment} \\\\\n-5.0, & \\text{if } \\hat{y} \\text{ cannot be parsed into an AST (i.e., no Verilog semantics)} \\\\\n10 * sim_{AST}(\\hat{y}, y), & \\text{if } \\hat{y} \\text{ can be parsed into an AST successfully}\n\\end{cases}\\]\nTo get the similarity score between abstract syntax trees (ASTs), we designed a simple yet effective algorithm. As shown in Algorithm 1, the comparison algorithm, sim\u0104ST, computes a similarity score between two normalized ASTs. If the trees are identical, the score is 1.0. If both trees are tuples and their root node types match, it recursively compares their children. The similarity score is the average of the scores of the children. If the number of children differs, the score is penalized by dividing by the maximum number of children. The score ranges from 0.0 (completely different) to 1.0 (identical)."}, {"title": "Experimental Setup and Performance Evaluation", "content": "3.1 Training Detail\nWe continually pre-train VeriSeek (6.7B) based on deepseek-coder-6.7b-base [4]. Our experiments are conducted on a server equipped with 8 A800-80G GPUs. All experiments utilize a cosine learning rate scheduler with a warmup phase comprising 10% of the total training steps, and an AdamW optimizer [7] with a weight decay of 0.05. Additionally, we employ deepspeed ZeRO-3 offload [11] for acceleration.\nDuring continual pre-training, we use a peak learning rate (LR) of le-4, a batch size of 32, and train for 1 epoch. For reinforcement learning, we adopt a peak learning rate (LR) of le-5, a batch size of 8, and train for 15 epochs. The duration of continual pre-training is approximately 1 hour, whereas the reinforcement learning task requires about 1 day to complete training.\n3.2 Metric and Benchmark\nMetric. We follow [16] and evaluate the models using the widely-adopted pass@k metric for code generation, which is the percentage of problems solved by using k generated programs per problem:\n\\[pass@k := E_{problems} \\left[ 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}} \\right],\\]\nwhere n \u2265 k samples are generated per problem and a problem is solved if any of the k samples passes the testbench. In our experiments, we sample n = 20 code completions per problem for benchmark and measuring pass@1 and pass@5."}, {"title": "Discussion", "content": "4.1 Failure of Instruction Fine-tuning\nInstruction finetuning is a process where large language models are further trained on datasets comprising instructions and corresponding responses [17]. This enhances their ability to follow human instructions accurately and generate relevant outputs. It is widely adopted due to its efficacy in improving model performance on diverse, real-world tasks and user interactions.\nFor decoder-only LLMs, they generate a sequence by continuously predicting the next token based on the already generated previous ones. We denote the probability of generating the t-th token \\(\\hat{y}_t\\) as \\(P_r(\\hat{y}_t | x, \\hat{y}_{<t})\\), and the log probability of generating the whole sequence could be written as \\(\\sum_{t=1}^{T} log P_r(\\hat{y}_t | x, \\hat{y}_{<t})\\). Instruction fine-tuning employ Maximum Likelihood Estimation (MLE) to find the best parameters. MLE is usually defined as below:\n\\[L_{mle} = - \\sum_{t=1}^{T} log P_{\\theta}(\\hat{y}_t | x, \\hat{y}_{<t})\\]\nAs mentioned in [6], there exists exposure bias [1] in auto-regressive sequence generation, where the model predicts the next token based on its own generated previous tokens rather than the reference tokens, leading to potential deviations from the reference code during generation. We confirm this problem by fine-tuning the pre-trained model directly on Opencores dataset."}, {"title": "Training Dynamics of Reinforcement Learning", "content": "To gain a deeper understanding of the impact of reinforcement learning on model performance, we meticulously recorded the reward at each training step as well as the functional pass@1 and pass@5 every five training steps.\nAs illustrated in Fig. 1, it is noteworthy that the optimal model solution emerges at the initial stages of the training process rather than at the point of convergence. Moreover, after a long time, the model would converge to the fine-tuned model. The training process can be divided into four distinct phases:\n\u2022 Warm-up (0-20 steps): During this initial stage, the model tries to escape from local optima. This phase is crucial for ensuring that the model does not become prematurely trapped in suboptimal solutions, thereby setting a foundation for more effective learning.\n\u2022 Learning (20-50 steps): In the subsequent phase, the model actively explores its representational capacity by maximizing the rewards. This exploration is aimed at enhancing the model's ability to generalize and adapt to diverse scenarios, thereby improving its overall performance.\n\u2022 Deviation (50-100 steps): During this phase, the model intentionally deviates from the optimal region. This deviation arises from the misalignment between the reward signal and our evaluation expectations: although AST serves as a global supervisory signal, it still does not align completely with the anticipated evaluation metric. Consequently, the model continues to explore a broader solution space, thereby moving away from the optimal region.\n\u2022 Convergence (100-150 steps): Finally, the model converges towards a fine-tuned state. In this phase, the model refines its parameters to achieve an optimal balance between exploration and exploitation, culminating in a well-tuned and stable policy.\nThis training process can be visualized through the learning dynamics illustrated in Fig. 2, which provides a comprehensive representation of the optimization landscape across various stages of our proposed methodology. The base model, denoted by a black dot, is positioned at the periphery of the contour plot, indicative of an initial low-accuracy state. The pre-trained model's trajectory, depicted in yellow, diverges from the base model, suggesting a preliminary enhancement in model performance. The fine-tuned model, represented in purple, continues this trajectory from the pre-trained state. The best and converged Proximal Policy Optimization (PPO) models are delineated in red, with their convoluted path demonstrating the PPO algorithm's optimization process, characterized by significant fluctuations and explorations within the loss landscape. The terminal point represents the ultimate convergence of the PPO model towards the fine-tuned state.\nIt is noteworthy that while fine-tuning can propel the model from a sub-optimal region, it tends to direct the model towards another sub-optimal area. This is attributable to the potency of simple"}, {"title": "Conclusion", "content": "In conclusion, this study has advanced the automatic generation of Verilog code from natural language instructions using reinforcement learning with golden code feedback. Our approach, implemented in a 6.7B parameter model, achieves state-of-the-art results, outperforming larger models. The research highlights the crucial role of comprehensive supervisory signals aligned with Verilog's parallel semantics. By offering an open-source alternative to commercial LLMs, this work enhances flexibility and data privacy in hardware design. Our findings underscore the importance of well-defined global supervisory signals and exploration-capable algorithms in optimizing model performance, paving the way for more efficient hardware development methodologies."}]}