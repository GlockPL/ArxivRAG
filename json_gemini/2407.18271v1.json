{"title": "Large Language Model for Verilog Generation with Golden Code Feedback", "authors": ["Ning Wang", "Bingkun Yao", "Jie Zhou", "Xi Wang", "Zhe Jiang", "Nan Guan"], "abstract": "Recent advancements in large language models (LLMs) have catalyzed significant\ninterest in the automatic generation of Register-Transfer Level (RTL) code, par-\nticularly Verilog, from natural language instructions. While commercial LLMs\nlike ChatGPT have dominated this domain, open-source alternatives have lagged\nconsiderably in performance, limiting the flexibility and data privacy of this emerg-\ning technology. This study introduces a novel approach utilizing reinforcement\nlearning with golden code feedback to enhance the performance of pre-trained\nmodels. Leveraging open-source data and base models, we have achieved state-\nof-the-art (SOTA) results with a substantial margin. Notably, our 6.7B parameter\nmodel VeriSeek demonstrates superior performance compared to current best-in-\nclass 13B and 16B models. Furthermore, through a comprehensive analysis of\nthe limitations in direct fine-tuning and the training dynamics of reinforcement\nlearning, we posit that the development of comprehensive supervisory signals,\nwhich are align with the inherent parallel semantics of Verilog code, is critical to\neffective generation. The code and data associated with this research are publicly\navailable at https://github.com/CatIIIIIIII/veriseek. The model weights\ncan be accessed at https://huggingface.co/WANGNingroci/VeriSeek.", "sections": [{"title": "1 Introduction", "content": "In recent years, the field of natural language processing (NLP) has witnessed a paradigm shift\nwith the advent of large language models (LLMs), exemplified by GPT [2]. These models have\ndemonstrated unprecedented capabilities in various linguistic tasks. Inspired by this remarkable\nprogress, researchers in the domain of hardware design have begun to explore the potential of LLMs\nin revolutionizing hardware development methodologies.\nAmong diverse applications, the automatic generation of RTL designs based on natural language\nspecifications has emerged as a particularly promising and widely studied direction. This task aims\nto transform high-level functional descriptions in natural language into fully-fledged Hardware\nDescription Language (HDL) code, such as Verilog, VHDL, ab initio. In contrast to the well-\nestablished predictive machine learning (ML) methodologies in EDA, these generative techniques\noffer a more direct and potentially renewal impact on the hardware design and optimization process.\nA subset of the current literature has primarily concentrated on the development and refinement of\nprompt engineering methodologies, leveraging commercial Large Language Models (LLMs) such as\nGPT. The utilization of commercial LLMs, while offering immediate access to powerful language\nprocessing capabilities, inherently constrains the degree of customization and adaptation possible for\ndomain-specific tasks such as RTL generation. This approach, while expedient, may not fully address\nthe unique challenges and nuances inherent in hardware description languages and digital design\nparadigms. Furthermore, the black-box nature of these commercial models precludes comprehensive\nanalysis of their internal mechanisms, limiting the potential for targeted improvements in the realm\nof hardware design automation.\nThe adoption of open-source Large Language Models (LLMs) for hardware design automation\npresents compelling advantages over closed-source commercial solutions, such as GPT, in both\nresearch and practical applications. From a research perspective, open-source LLMs facilitate\nunrestricted scientific inquiry, enabling in-depth studies and customizations of this emerging technique.\nIn practical applications, open-source solutions address critical data privacy concerns.\nThe development of high-performance open-source RTL generation models is currently hindered by\nthe scarcity of high-quality circuit design data for training. This challenge stems from the proprietary\nnature of organized design data held by semiconductor companies and the inadequacy of publicly\navailable data, which is often unstructured and of poor quality. Furthermore, existing open-source\nmodels rely on simplistic pre-training and fine-tuning approaches that are highly dependent on\ndataset quality. To address these limitations, there is an urgent need to explore more sophisticated\nmethodologies capable of effectively leveraging limited data resources, thereby advancing the field of\nopen-source RTL generation.\nOur work presents a novel approach for training language models to generate Verilog code using\nProximal Policy Optimization (PPO). The method introduces a reward function based on Abstract\nSyntax Tree (AST) similarity between generated and golden code. We use Pyverilog to parse code\ninto ASTs and design an algorithm to compute AST similarity scores. The reward function penalizes\ninvalid code and rewards syntactically correct code based on its AST similarity to the golden standard.\nThis approach allows for a more semantically meaningful evaluation of generated Verilog code,\nfocusing on structural similarities rather than token-level matches, potentially improving the quality\nand correctness of the generated code. The contributions of our work can be summarized as follows:\n\u2022 Leveraging entirely open-source models and data, VeriSeek achieves state-of-the-art results with\na significant margin. Notably, our 6.7B model outperforms the current best 13B and 16B models.\n\u2022 To the best of our knowledge, VeriSeek is the first to employ reinforcement learning to enhance\nmodel performance and validate its effectiveness. It also demonstrates the efficacy of using\nAbstract Syntax Trees as a supervisory signal.\n\u2022 Through an analysis of the training dynamics of reinforcement learning, we posit that for Verilog\ncode generation tasks, designing document-level training objectives can better align the model\nwith the parallel logic of hardware description languages, thereby achieving superior results."}, {"title": "2 Method", "content": "2.1 Continual Pre-training Utilizing C-Language Integrated Dataset\nIn the process of continual pre-training, we utilized the publicly available VGen dataset, as developed\nby [15]. VGen aggregates Verilog repositories from GitHub, systematically filters out duplicates\nand excessively large files, and retains only those files containing module and endmodule statements.\nAdditionally, it extracts text from 70 Verilog textbooks using pymuPDF, subsequently cleans the data,\nand retains only the verified Verilog syntax. Following this, we further refine the dataset to include\nonly Verilog code, resulting in a final cleaned VGen dataset of approximately 200MB in size.\nDuring implementation, we discovered that C programs could assist the model in understanding\nand generating Verilog code. This intriguing phenomenon was also observed and utilized by [10],\nwho treated the corresponding C code as a functional description. Consequently, we incorporated\nthe CodeSearchNet dataset [5], which contains approximately 40MB function codes and their\ndocumentation.\n2.2 Reinforcement Learning with Golden Code Feedback\n2.2.1 Dataset\nWe gathered high-quality specification-code pairs from Opencores [3], a community aimed to\ndeveloping digital open-source hardware using electronic design automation (EDA). We then filtered"}, {"title": "2.2.2 Proximal Policy Optimization", "content": "We propose to formulate Verilog program generation as a reinforcement learning (RL) problem,\naiming to further align the pre-trained model $\\pi_\\theta$ with golden code preferences. Following the\nReinforcement Learning from Human Feedback (RLHF) procedure as proposed in [9; 18], we\nfine-tuned the pre-trained model on our environment using Proximal Policy Opzimization (PPO,\n[12]). The environment is a bandit environment which receives a designer specification and generate\na response to the specification. Given the generated code and golden code, it produces a reward\ndetermined by the reward function and ends the episode. In addition, we add a per-token KL penalty\nfrom the pre-trained model at each token to mitigate over-optimization.\nConsider a large language model (LLM) as a policy $\\pi_\\theta(\\hat{y} | x)$ parameterized by $\\theta$. The policy $\\pi_\\theta$\nis designed to receive user an instruction $x \\in X$ and generate a text response $\\hat{y} \\in Y$. Here we only\nconsider single-turn conversations to simplify notations and modelling. Given a specification x, the\nLLM $\\pi_\\theta$ will generate a response $\\hat{y}$ in an auto-regressive manner:\n$\\pi_\\theta(\\hat{y} | x) = [ \\pi_\\theta(\\hat{y}_t | x, \\hat{y}_{<t}),$\n                                                                                       t\n(1)\nwhere $\\hat{y}_t$ is the t-th token in the response and $\\hat{y}_{<t}$ represents the tokens in the response before $\\hat{y}_t$.\nThe objective function between the generated code and the golden code is:\n$J_r(\\pi_\\theta) = E_{x~P_{data}, \\hat{y}~\\pi_o} [r(\\hat{y}, y) - \\beta log \\frac{\\pi_\\theta(\\hat{y} | x)}{\\pi_{ref}(x)}]$         (2)\nwhere $r$ represents the reward function that reflects golden code preferences. It takes a response $\\hat{y}$\nand the corresponding golden code $y$ as input, producing a scalar value as output. The model $\\pi_{ref}$\nserves as the reference model used to regularize $\\pi_\\theta$ through Kullback\u2013Leibler (KL) divergence. The\nconstant $\\beta$ is introduced to control the degree of regularization."}, {"title": "2.2.3 Defining Reward by Golden Code AST", "content": "Parsing code to AST. Unlike traditional language models that treat code as simple sequences of\nsubword tokens, we leverages the Abstract Syntax Tree (AST) to gain deeper semantic insights. For\nthe purpose of parsing, we assume the provided code is syntactically valid \u2014 a reasonable assumption\nfor code understanding. We employ Pyverilog [14], which is a popular open-source toolkit for RTL\ndesign, to construct ASTs. In this structure, each subtree represents a consecutive span of subword\ntokens, while every leaf node corresponds to an individual token.\nReward definition. For each generated Verilog code segment sequence $\\hat{y}$, the reward r is defined\nby calculating the similarity between the AST of the response and the AST of the label, provided the\nresponse is valid. The reward r is then given by:\nr(\\hat{y}, y) = \\begin{cases}\n-10.0,                                      & \\text{ if } \\hat{y} \\text{ contains no valid Verilog code segment}\n-5.0,                                       & \\text{ if } \\hat{y} \\text{ cannot be parsed into an AST (i.e., no Verilog semantics)}\n10 * sim_{AST}(\\hat{y}, y), & \\text{ if } \\hat{y} \\text{ can be parsed into an AST successfully}\n\\end{cases}\n(3)\nTo get the similarity score between abstract syntax trees (ASTs), we designed a simple yet effective\nalgorithm. As shown in Algorithm 1, the comparison algorithm, sim\u0104ST, computes a similarity score\nbetween two normalized ASTs. If the trees are identical, the score is 1.0. If both trees are tuples and\ntheir root node types match, it recursively compares their children. The similarity score is the average\nof the scores of the children. If the number of children differs, the score is penalized by dividing by\nthe maximum number of children. The score ranges from 0.0 (completely different) to 1.0 (identical)."}, {"title": "3 Experimental Setup and Performance Evaluation", "content": "3.1 Training Detail\nWe continually pre-train VeriSeek (6.7B) based on deepseek-coder-6.7b-base [4]. Our experiments\nare conducted on a server equipped with 8 A800-80G GPUs. All experiments utilize a cosine learning\nrate scheduler with a warmup phase comprising 10% of the total training steps, and an AdamW\noptimizer [7] with a weight decay of 0.05. Additionally, we employ deepspeed ZeRO-3 offload [11]\nfor acceleration.\nDuring continual pre-training, we use a peak learning rate (LR) of le-4, a batch size of 32, and train\nfor 1 epoch. For reinforcement learning, we adopt a peak learning rate (LR) of le-5, a batch size of 8,\nand train for 15 epochs. The duration of continual pre-training is approximately 1 hour, whereas the\nreinforcement learning task requires about 1 day to complete training.\n3.2 Metric and Benchmark\nMetric. We follow [16] and evaluate the models using the widely-adopted pass@k metric for code\ngeneration, which is the percentage of problems solved by using k generated programs per problem:\n$pass@k := E_{problems} [1 - \\binom{n-c}{k} / \\binom{n}{k}]$\\n(4)\nwhere n \u2265 k samples are generated per problem and a problem is solved if any of the k samples\npasses the testbench. In our experiments, we sample n = 20 code completions per problem for\nbenchmark and measuring pass@1 and pass@5.\nMoreover, to facilitate a more accurate comparison with the baseline model RTLCoder, we adopt the\n'pass@5' metric as described in [6]. This metric considers an experiment successful if any test in five\ntrials passes the testbench. To avoid confusion with the notation 'pass@k, k = 5', we have renamed\nthis metric to hit@5.\nBenchmark. For fair comparison of large language models, we evaluate their performance on\nRTLLM V1.1 benchmark. The updated version of the RTLLM V1.1 benchmark [8] includes 29 RTL\ndesign tasks at a larger design scale, addressing several issues present in the original RTLLM V1.0.\nWhile we primarily adhere to the testing methods outlined in the original paper, we evaluate syntax\nand functional pass rate, utilizing ModelSim [13]. Syntax correction only requires the design to\ncomply with Verilog syntax rules while function also mandates that the design interface corresponds\nto the testbench, ensuring the circuit can be simulated.\n3.3 Performance Evaluation\nIn the evaluation section, we provide a comprehensive comparison of the performance of various\nlarge language models for Verilog code generation, as detailed in Table 1. This table includes both\nclosed-source and open-source models, offering a broad perspective on the current state of the field.\nAmong these models, our designed model stands out, particularly the version that has been pre-trained\nwith C programs (PTwC) and further enhanced through reinforcement learning (PTwC+RL). This\ncombination of continual pre-training and reinforcement learning has enabled our model to achieve\nstate-of-the-art (SOTA) functional performance among open-source models, setting a new baseline.\nSpecifically, our PTWC+RL model demonstrates exceptional capabilities in both syntax and func-\ntional metrics. It achieves syntax pass@5 of 93.1%, which indicates its robust ability to generate\nsyntactically correct Verilog code, although it is slightly lower than RTLCoder's 96.6%. However, our\nmodel excels in functional performance, achieving a functional pass@5 of 55.2%, underscoring its\nsuperior performance in generating functionally accurate code, which is a critical aspect of practical\ncode generation tasks. These metrics, especially the functional pass rate, are the highest among all\nopen-source models listed in the table, highlighting the effectiveness of our approach."}, {"title": "4 Discussion", "content": "4.1 Failure of Instruction Fine-tuning\nInstruction finetuning is a process where large language models are further trained on datasets\ncomprising instructions and corresponding responses [17]. This enhances their ability to follow\nhuman instructions accurately and generate relevant outputs. It is widely adopted due to its efficacy\nin improving model performance on diverse, real-world tasks and user interactions.\nFor decoder-only LLMs, they generate a sequence by continuously predicting the next token based\non the already generated previous ones. We denote the probability of generating the t-th token $\\hat{y}_t$\nas $P_r(\\hat{y}_t | x, \\hat{y}_{<t})$, and the log probability of generating the whole sequence could be written as\n$\\Sigma_{t=1}^T log P_r(\\hat{y}_t | x, \\hat{y}_{<t})$. Instruction fine-tuning employ Maximum Likelihood Estimation (MLE)\nto find the best parameters. MLE is usually defined as below:\n$L_{mle} = - \\sum_{t=1}^T log P_{\\theta}(\\hat{y}_t | x, \\hat{y}_{<t})$(5)\nAs mentioned in [6], there exists exposure bias [1] in auto-regressive sequence generation, where the\nmodel predicts the next token based on its own generated previous tokens rather than the reference\ntokens, leading to potential deviations from the reference code during generation. We confirm this\nproblem by fine-tuning the pre-trained model directly on Opencores dataset.\nAs shown in Table 2, a comparative analysis of the performance between the directly fine-tuned\nmodel and other model variants reveals that the fine-tuned model demonstrates a slight improvement\nin the functional pass@1 metric, but exhibits suboptimal performance across other evaluation metrics.\nIn addition to exposure bias, there is another potential explanation for the inadequacies observed in\ndirect fine-tuning. The limitations imposed by the auto-regressive nature of current generative large\nlanguage models (LLMs) restrict new tokens to attend only to their prefixes, which is not aligned\nwith the inherently parallel semantics of Verilog. While direct fine-tuning can enhance the model's\nresponse to instructions, it may adversely affect the model's creative capabilities. This phenomenon\nwill be further examined in the subsequent subsection. We recommend that the research community\nconsider adopting parallel and global supervise signals instead of sequential ones for Verilog code\ngeneration.\n4.2 Training Dynamics of Reinforcement Learning\nTo gain a deeper understanding of the impact of reinforcement learning on model performance, we\nmeticulously recorded the reward at each training step as well as the functional pass@1 and pass@5\nevery five training steps.\nAs illustrated in Fig. 1, it is noteworthy that the optimal model solution emerges at the initial stages\nof the training process rather than at the point of convergence. Moreover, after a long time, the model\nwould converge to the fine-tuned model. The training process can be divided into four distinct phases:\n\u2022 Warm-up (0-20 steps): During this initial stage, the model tries to escape from local optima. This\nphase is crucial for ensuring that the model does not become prematurely trapped in suboptimal\nsolutions, thereby setting a foundation for more effective learning.\n\u2022 Learning (20-50 steps): In the subsequent phase, the model actively explores its representational\ncapacity by maximizing the rewards. This exploration is aimed at enhancing the model's ability\nto generalize and adapt to diverse scenarios, thereby improving its overall performance.\n\u2022 Deviation (50-100 steps): During this phase, the model intentionally deviates from the optimal\nregion. This deviation arises from the misalignment between the reward signal and our evaluation\nexpectations: although AST serves as a global supervisory signal, it still does not align completely\nwith the anticipated evaluation metric. Consequently, the model continues to explore a broader\nsolution space, thereby moving away from the optimal region.\n\u2022 Convergence (100-150 steps): Finally, the model converges towards a fine-tuned state. In this\nphase, the model refines its parameters to achieve an optimal balance between exploration and\nexploitation, culminating in a well-tuned and stable policy.\nThis training process can be visualized through the learning dynamics illustrated in Fig. 2, which\nprovides a comprehensive representation of the optimization landscape across various stages of our\nproposed methodology. The base model, denoted by a black dot, is positioned at the periphery of the\ncontour plot, indicative of an initial low-accuracy state. The pre-trained model's trajectory, depicted in\nyellow, diverges from the base model, suggesting a preliminary enhancement in model performance.\nThe fine-tuned model, represented in purple, continues this trajectory from the pre-trained state. The\nbest and converged Proximal Policy Optimization (PPO) models are delineated in red, with their\nconvoluted path demonstrating the PPO algorithm's optimization process, characterized by significant\nfluctuations and explorations within the loss landscape. The terminal point represents the ultimate\nconvergence of the PPO model towards the fine-tuned state.\nIt is noteworthy that while fine-tuning can propel the model from a sub-optimal region, it tends to\ndirect the model towards another sub-optimal area. This is attributable to the potency of simple\nauto-regressive training, which is not inherently aligned with the parallel semantics of Verilog code.\nDuring the reinforcement learning phase, the model demonstrates enhanced capability in exploring\nthe landscape to attain an optimal solution. Another critical factor contributing to this success is the\nutilization of the Abstract Syntax Tree (AST), which serves as a globally defined reward. Although\nthe AST, representing the program's structure, does not fully encapsulate the parallel nature of the\ncode, it nonetheless provides robust guidance for the learning process. It is important to acknowledge\nthat in the long term, reinforcement learning is expected to converge the model towards the fine-tuned\nstate, as the reward is directly generated from the golden reference code.\nIn conclusion, our findings underscore the critical importance of a well-defined global supervisory\nsignal in enabling the model to effectively acquire the capability for Verilog code generation. Fur-\nthermore, in scenarios where such a well-defined signal is not readily available, it is imperative\nto consider employing algorithms capable of exploring the optimization landscape to enhance the\nmodel's performance."}, {"title": "5 Conclusion", "content": "In conclusion, this study has advanced the automatic generation of Verilog code from natural language\ninstructions using reinforcement learning with golden code feedback. Our approach, implemented in\na 6.7B parameter model, achieves state-of-the-art results, outperforming larger models. The research\nhighlights the crucial role of comprehensive supervisory signals aligned with Verilog's parallel\nsemantics. By offering an open-source alternative to commercial LLMs, this work enhances flexibility\nand data privacy in hardware design. Our findings underscore the importance of well-defined global\nsupervisory signals and exploration-capable algorithms in optimizing model performance, paving the\nway for more efficient hardware development methodologies."}]}