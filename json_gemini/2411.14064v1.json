{"title": "Multi LoRA Meets Vision: Merging multiple adapters\nto create a multi task model", "authors": ["Ege Kesim", "Selahattin Serdar Helli"], "abstract": "Abstract Parameter efficient finetuning (PEFT) methods\nare widely used in LLMs and generative models in computer\nvision. Especially one can use multiple of these during inference\nto change the behavior of the base model. In this paper we\ninvestigated whether multiple LoRA adapters trained on\ncomputer vision tasks can be merged together and used during\ninference without loss in performance. By achieving this,\nmultitask models can be created just by merging different\nLoRAs. Merging these will reduce inference time and it will not\nrequire any additional retraining. We have trained adapters on\nsix different tasks and evaluated their performance when they\nare merged together. For comparison we used a model with a\nfrozen backbone and finetuned its head. Our results show that\neven with simple merging techniques creating a multitask model\nby merging adapters is achievable by slightly loosing\nperformance in some cases. In our experiments we merged up to\nthree adapters together. Depending on the task and the similarity\nof the data adapters were trained on, merges can outperform\nhead finetuning. We have observed that LoRAs trained with\ndissimilar datasets tend to perform better compared to model\ntrained on similar datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Finetuning large pretrained models on different tasks and\ndomains has been popular for a long time. However as these\npretrained models get larger and larger it becomes\ncomputationally much more expensive to finetune them.\nParameter efficient finetuning (PEFT) methods became popular\nrecently as they enable finetuning models with less resources.\nSome popular PEFT methods are LoRA [1], DoRA[2], 0-\nLORA[3], Q-LORA [4] and MultiLoRA [5].\nSome of these PEFT methods such as LoRA enable only\nstoring much smaller weight files to be added on top of the\nbase model. In a case where there are multiple finetuned\nversions of a large model, by using LoRA it is enough to only\nstore the small weight files for each task rather than the full\nmodel. Multiple LoRA adapters can also be merged in order to\nadd new task capabilities to a model without causing\ncatastrophic forgetting. Due to these reasons, LoRA finetuning\nis heavily used in in large language models (LLMs). They are\nespecially used in LLMs to add new tasks by merging with a\nLORA or efficiently finetune the model with new data to update\nthe knowledge. Using multiple LoRA adapters is much more\npopular in LLMs compared to computer vision [3,6,7,8].\nIn computer vision multiple LoRA merging is almost only\nused in generative tasks [9,10,11,12,13,14]. Each LoRA\nusually correspond to a style or character. By merging these,\nnew styles and characters can be added to base model. This\nway multiple concepts and styles can be used at once and\ncapabilities of the model can be extended without any\nadditional retraining for merging.\nOther than generative models LoRA finetuning is also used\nin computer vision for various tasks such as segmentation [16],\nclassification [15,17] and object detection [18]. However, as it\nwas mentioned earlier using multiple LoRAs is almost only\nused in generative tasks.\nUsing multiple LoRAs in vision has multiple benefits:\n1) Without any multitask training multiple single task\nmodels can be merged to create a multitask model.\n2) Computation cost can be reduced for multiple task\ncases since there will be a shared backbone which is the\nbottleneck of the network due to its size.\n3) A network can be updated any time. New tasks can be\nadded and old ones can be removed from the network just by\nremoving the LoRA.\n4) In the need of retraining a task, instead of retraining\nthe whole model for all tasks it will be enough to just train the\nLORA of that task.\nUsing multiple LoRA adapters requires each LoRA to\nwork without affecting each other. In this work we explored\nwhether multiple LoRA models sharing the same base model\ncan be merged together to create a multi task system. We\nselected four classification and two regression tasks on five\ndifferent datasets to explore how adapters of these models\ninteract. One of these datasets consists of satellite images of\nearth, another dataset consists of galaxy images and the rest of\nthem are made of face images. We have purposefully selected\nthese datasets to see how datasets with similar and different\ncontent will affect the models when they are merged. We have\nalso compared regression and classification tasks to see if the\ntask type is affecting due to the pretraining of the backbone\nmodel. To see the how effective to use LoRA we compared\nusing LoRA with different ranks and, finetuned model with a\nfrozen backbone as a baseline. By merging multiple heads with\nthe frozen backbone, a multitask model can be created easily\nwithout performance loss due to merging. However only\nfinetuning the head may not give enough performance for\nevery task due to low number of parameters that are trained.\nIn our experiments we observed that merging multiple\nLoRA adapters result in performance drop. The amount of the\ndrop is determined by the adapter combination and this\nselection can make a huge difference in the performance. We\nobserved that among all adapters we have trained, two of the\nadapters preserve performance when combined with almost"}, {"title": "II. DATASETS", "content": "In our experiments we have used five different datasets for\nsix different tasks. These datasets were chosen so that the data\nis not similar to the data that the base model was pretrained\nwith. We chose four face related datasets expecting the\nadapters to learn similar features and two dissimilar datasets. In\norder to have similar dataset sizes we down sampled each\ndataset to a size around 10-17k samples.\nA. FireRisk\nFireRisk [19] is a dataset used in remote sensing. It consists\nof satellite images each labeled among 7 classes assessing the\nfire risk. There are 91k images but the distribution of data\namong these classes is highly imbalanced. As it was mentioned\nbefore to get each dataset to similar size we have down\nsampled each dataset. In FireRisk we have reduced the number\nof classes to 3 classes: non-burnable, high, and low. To reduce\nthe classes, we combined water and non-burnable, high, very\nhigh and moderate and lastly low and very low together to 3\nbins. After combining these we randomly sampled data from\neach bin to reduce the amount of data.\nB. UTKFace\nUTKFace [20] consists of 23k images. Each image is\nannotated for age, gender and ethnicity. We used this dataset\nfor two different tasks to predict age which are classification\nand regression. For classification we divided the dataset to 6\nclasses. The classes are ages of 0-3, 4-16, 17-30, 31-45, 46-59\nand 60+. After appending each image to one of these bins, the\ndataset was down sampled to 11k images. Each class is down\nsampled to 2k images except 0-3 class due to low number of\nimages in that class. The same down sampled dataset was used\nfor regression tasks without any change. We have used Dlib\n[21] face detector to detect and crop face in images.\nC. Expw\nExpression in the Wild (Expw) [22,23] is a dataset\ncontaining 91k face images. Each image contains a person\nexpressing one of the 7 basic emotion categories. We have\nreduced the dataset to 12k images.\nD. WFLW\nWFLW [24] is a popular dataset used in facial landmark\ndetection. It consists of 7500 train and 2500 test images. In\neach image landmarks are represented with 98 2D points.\nCompared to the other datasets the diversity of images is high\nin terms of expression, poses etc. Also, the test set images are\nannotated for these six categories.\nE. Galaxy10 DECals\nGalaxy 10 DECals [25] is a dataset containing galaxy\nimages with 10 classes. The total number of images is around"}, {"title": "III. METHOD", "content": "We employ a pre-trained Vision Transformer (ViT) [30], as\nthe backbone of our model. Additionally, we compare the\nefficacy of using LoRA against a baseline of fine-tuning a\nmodel with a frozen backbone, and the multi-merged LoRA\nmodels. For merging multiple LoRA adapters, concatenation\nmerging method was used. Although we have not reported the\nresults we also ran experiments for linear merging and\nobserved similar results.\nWe utilize the ViT model as the backbone. The model's\npooler output provides a 768-dimensional feature vector.\nFollowing the backbone a two-layer Multi-Layer Perceptron\n(MLP) is applied to each task. These MLPs consist of a 512-\ndimensional hidden layer with ReLU activation, followed by\nan output layer sized according to the task-specific\nrequirements. LoRA is applied to the key and value vectors of\nthe backbone.\nFor classification tasks we employ the cross-entropy loss\nwhich are fire risk classification, galaxy classification, emotion\nrecognition and age group classification. For the task of face\nage regression and landmark detection we utilize L1 loss.\nFurthermore we report the Normalized Mean Error (NME)\nscore for facial landmark detection task. NME is a common\nmetric for evaluating the accuracy of landmark detection,\ncalculated as the mean error between the predicted and true\nlandmark positions, normalized by an inter-ocular distance to\naccount for scale variations across different faces. In our\nexperiments we used Adam optimizer.\nAll datasets were divided into training, test, and validation\nsets. The models were trained until they converged. We report\nthe performance of LoRA adapters on six different tasks: fire\nrisk classification, age classification (UTKFace), emotion\nrecognition (Expw), galaxy classification (Galaxy10 DECals),\nage regression (UTKFace), and landmark detection (WFLW).\nThese tasks are evaluated based on accuracy (Acc), macro F1\nscore, root mean square error (RMSE), and normalized mean\nerror (NME) metrics, providing a comprehensive\nunderstanding of each model's effectiveness.\nIn our experiments we compared 16 and 64 rank LoRA\nfinetuning and only finetuning the head of a model with frozen\nbackbone. Performance comparison of these models can be\nfound in Table 1. Although there is no conclusion in which\nranks is better compared to other, in all scenarios LoRA\nperforms better compared to the baseline. In landmark\ndetection and galaxy classification the baseline method\nperforms much worst meaning the backbone needs some level\nof finetuning. It seems that in most cases the baseline has\ncomparable performance with LoRAs. We believe that since"}, {"title": "A. Effect of merging on each task", "content": "In this section we examine how each adapters performance\nchange on their own task when merged with other adapters.\nFire risk and galaxy classification adapters are resilient to\nmerging except the merge with the landmark adapter. From\nTables 4, 7 it can be seen that these adapters have only a slight\ndecrease in their performance when merged with other models.\nHowever even with this slight decrease they still outperform\nthe baseline.\nLandmark adapter lose performance in each case. It works\nbest with galaxy and fire risk adapter which are non face\ncontaining tasks. The adapters that have dataset with similar\ncontent creates a performance drop compared to galaxy and\nfire risk adapters. However landmark model still outperforms\nbaseline as the baseline performs poorly on this task. Overall\nlandmark adapter lose performance with other merges however\nit still performs well.\nAge regression adapter acts similar to landmark adapter. It\nperforms best when merged with non-face related tasks. Rank\n64 merges with galaxy and fire risk adapters outperform the\nbaseline.\nAge classification adapter can only outperform the baseline\nwhen merged with fire and galaxy adapters. However, emotion\nadapter gives similar performance for rank 16. Interestingly the\nage regression adapter, which is trained on the same dataset but\non a different task, creates a huge performance drop on this\ntask. Lastly the emotion adapter outperforms the baseline in\nmost cases."}, {"title": "B. How does each adapter effect the others?", "content": "In this section we examine the effect of each adapter to the\nother. Galaxy and fire risk adapters enable other adapters to\npreserve their performance. In every case their merges with\nother adapters outperform the baseline. Also, when merged\nwith landmark adapter although their performance drops on\ntheir own tasks they preserve performance on the landmark\ntask. They both maintain the performance of the landmark\ndetection task.\nLandmark adapter heavily effects other adapters and drops\ntheir performance drastically. On all tasks every worst\nperforming merge combination includes landmark adapter.\nInterestingly it allows fire risk and galaxy adapters to preserve\nperformance up to some level, where as other classification\ntasks go even below random guess threshold. Landmark\nadapter creates a huge drop in performance in these face related\ntasks. It should be noted that the output size of landmark model\nis much greater than the others since it predicts 98 landmark\npoints represented with 2D coordinates. So, both the output\nsize and the task being a regression task can play role in\nadapter dominating the others this much.\nAge regression adapter doesn't create a huge performance\ndrop as landmark adapter however it drops performance more\nthan the others. Lastly emotion and age classification adapters\ndoesn't have any significant effect on others. Other than some\nexceptions they preserve performance of the other adapter\nalthough not in the level of galaxy or fire risk adapters.\nOverall, we observe that every merge decreases the\nperformance in varying levels. Adapters trained on datasets\nwith dissimilar content are resilient to both preserving their\nown performance and the performance of the other adapter.\nAdapters trained on similar content tend to lower each other's\nperformance in varying levels, with even going down the\nrandom guess threshold."}, {"title": "C. Going beyond two adapters", "content": "To see if more than two adapters can be merged we\nexperimented with merging three models. Since there are too\nmany model combinations we selected fire risk and galaxy\nclassification adapters as the first two adapters and checked\ntheir compatibility with other adapters. In Table 8. the\nperformance of three merged adapters are given, where in each\ncase galaxy and fire risk adapters are the two base adapters and\nthe third varies. Only the name of the third adapter is given to\nprevent confusion and redundancy along with the metric used\nto evaluate the performance. The results of three adapter\nmerging show that the merged model can still beat the baseline\nwith the exception of landmark adapter as it degrades the\nperformance of fire risk and galaxy adapters. We can see that\nthe overall performance has been slightly degraded compared\nto merging two adapters. However, as the merged model still\nbeats the baseline merging three models seems achievable."}, {"title": "IV. DISCUSSION", "content": "In this study our goal was to explore if LoRA adapter merging\ncould be used in computer vision tasks to create a multi task\nmodel. We trained multiple LoRA adapters on multiple tasks\nand investigated their performance when they are merged. In\nour experiments we reported our results by merging up to three\nmodels. Our results show that although merged models lose\nperformance it possible to merge multiple LoRAs to create a\nmultitask model. We have found that LoRAs trained on\ndissimilar content tend to work better with each other. Due to\nthis it is not guaranteed that every merge will preserve\nperformance of the original adapters. Another limitation is that\nit is more likely for LoRA models with similar content to get\nmerged compared to dissimilar tasks."}]}