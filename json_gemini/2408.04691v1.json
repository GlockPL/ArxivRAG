{"title": "Improving Relational Database Interactions with Large Language Models: Column Descriptions and Their Impact on Text-to-SQL Performance", "authors": ["Niklas Wretblad", "Oskar Holmstr\u00f6m", "Erik Larsson", "Axel Wiks\u00e4ter", "Oscar S\u00f6derlund", "Hjalmar \u00d6hman", "Ture Pont\u00e9n", "Martin Forsberg", "Martin S\u00f6rme", "Fredrik Heintz"], "abstract": "Relational databases often suffer from uninformative descriptors of table contents, such as ambiguous columns and hard-to-interpret values, impacting both human users and Text-to-SQL models. This paper explores the use of large language models (LLMs) to generate informative column descriptions as a semantic layer for relational databases. Using the BIRD-Bench development set, we created COLSQL, a dataset with gold-standard column descriptions generated and refined by LLMs and human annotators. We evaluated several instruction-tuned models, finding that GPT-40 and Command R+ excelled in generating high-quality descriptions. Additionally, we applied an LLM-as-a-judge to evaluate model performance. Although this method does not align well with human evaluations, we included it to explore its potential and to identify areas for improvement. More work is needed to improve the reliability of automatic evaluations for this task. We also find that detailed column descriptions significantly improve Text-to-SQL execution accuracy, especially when columns are uninformative. This study establishes LLMs as effective tools for generating detailed metadata, enhancing the usability of relational databases.", "sections": [{"title": "1 Introduction", "content": "Structured data in relational databases is crucial for modern information systems. It not only stores essential business data but also allows organizations to analyze trends, optimize operations, and make informed decisions, maintaining a competitive edge in today's fast-paced economy. In this setting, text-to-SQL, which converts natural language queries into SQL commands, enhances access to structured data by reducing the need for expert knowledge.\nHowever, working with these databases often presents challenges due to complex table schemas and vague descriptors used for table contents, such as ambiguous column names and hard-to-interpret values (Talaei et al., 2024). These ambiguities can significantly hinder the effectiveness of both human users and generative models employed in text-to-SQL tasks. A good illustration of such an ambiguity can be found in the district table in the financial database of the BIRD-Bench text-to-SQL benchmark (Li et al., 2023), as illustrated in Figure 1. The meaning of columns labeled A2 and A3, which represent the district name and its respective region, are not immediately clear from the names alone.\nIn response to this challenge, the introduction of descriptive column descriptions as a semantic layer over the existing database structures can dramatically improve their usability. However, manually crafting such column descriptions is a time-consuming and labor-intensive process. The large amount of relational databases in use across society today, underscores the need for automated systems that can generate natural language descriptions for SQL databases in order to enhance their interpretability for both human users and text-to-"}, {"title": "SQL models.", "content": "The recent advancements in LLMs have shown that they can produce coherent and contextually relevant text across various domains, making them potential tools for automating such metadata creation [ref]. By leveraging LLMs, the hypothesis is that the labor-intensive process of generating metadata for relational databases can be significantly simplified. Previous research has used generative LLMs to expand abbreviated column names in a SQL setting (Zhang et al., 2023), but to the best of our knowledge, no works exist for generating column descriptions in the general case across a wider array of column types and formats.\nIn this paper we will for this reason explore how well LLMs can generate column descriptions that contain detailed and relevant information. To enable this study, we first create a dataset for column description generation. We use the development set of BIRD-Bench (Li et al., 2023) and its databases as the starting point for our dataset. While BIRD-Bench contains some column descriptions, it does not have complete coverage over all columns, and the existing descriptions are often uninformative.\nWe first improve the existing column descriptions in BIRD-Bench using an LLM to generate more informative descriptions, demonstrating the usefulness of an LLM in a scenario where some initial metadata exists. Secondly, expert human annotators manually correct the generated descriptions to create a refined dataset of gold-standard references as well as annotating the difficulty of generating a column description for each specific column. This corrected dataset serves as a benchmark for evaluating models on column description generation.\nWe then apply several instruction-tuned models of different sizes and types to our dataset and perform an extensive human evaluation for all models. We find that both GPT-40 and Command R+ frequently generate column descriptions that have perfect overlap with the gold standard.\nIn addition to human evaluations, we use an LLM-as-a-judge to evaluate model performance, studying how this automatic method correlates with human annotations and its viability for ranking models on this dataset. We find that the LLM judge does not align well with human evaluations and more work is needed to refine the LLM judge for this task."}, {"title": "Main Takeaway:", "content": "Our study demonstrates that LLMs can be a useful tool for generating column descriptions, and including detailed column descriptions leads to more accurate text-to-SQL query generation, particularly in scenarios where column information is limited or uninformative. Future work will focus on applying this methodology to generate other types of metadata, such as value descriptions, to improve the usability of relational databases for humans and generative LLMs."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Table Understanding and Metadata\nAugmentation Tasks", "content": "In recent years, significant effort has been devoted to simplifying the understanding of tabular data. Table-to-text (Zhao et al., 2023b,a; Kasner et al., 2023; Yang et al., 2022; Gong et al., 2019) and table question answering (Pal et al., 2023; Xie et al., 2022; Herzig et al., 2020) aim at developing models able to understand structured tabular data and natural language questions to perform reasoning and tasks across tables.\nMore relevant to our work are previous studies focusing on column type detection and column relationship identification for improving the understanding of tables. Column type detection involves classifying columns into predefined semantic categories, such as dates or currencies, to enhance data interpretation (Suhara et al., 2022; Zhang et al., 2020). Column relationship identification focuses on determining the structural relationships between columns, such as primary and foreign keys, to understand structured data in tables (Wang et al., 2021; Deng et al., 2022). Unlike these approaches, our method of generating column descriptions using large language models aims to produce natural"}, {"title": "2.2 Metadata Usage in Text-to-SQL", "content": "The shift from traditional query languages to natural language interfaces through text-to-SQL has been significantly advanced by current LLMs. However, the inherent ambiguities and missing information in database schemas necessitate the incorporation of external knowledge and metadata to improve conversion accuracy (Zhang et al., 2024). Previous studies have explored various approaches to address this challenge. Talaei et al. (2024) propose a pipeline that retrieves relevant data and context from a knowledge store to generate accurate SQL queries, leveraging a data store based on the original BIRD-Bench metadata files. Similarly, Dou et al. (2022) enhance the text-to-SQL process by using external domain-specific knowledge, such as mathematical formulas and publicly available resources.\nSun et al. (2024) introduce question-specific database content to improve tuning performance, while Hong et al. (2024) utilize a tailored data expert LLM to provide useful knowledge to the SQL-generating LLM. Petrovski et al. Petrovski et al. (2018) take a different approach by vectorizing entire data columns into embeddings, thus eliminating the need for traditional descriptions and metadata in the text-to-SQL process. These studies collectively validate that incorporating external knowledge and metadata can significantly enhance text-to-SQL systems.\nIn contrast to these approaches, our work focuses on generating comprehensive SQL column descriptions directly using LLMs. While prior research emphasizes leveraging external knowledge and complex data retrieval processes, we aim to enhance the interpretability and usability of SQL queries by creating detailed descriptions for SQL columns."}, {"title": "3 Column Description Dataset Creation", "content": "To enable our study on generating informative column descriptions, we create COLSQL, a new dataset based on the development set of the widely-used text-to-SQL benchmark, BIRD-Bench (Li et al., 2023)\u00b9. This dataset consists of 799 columns across 11 different databases and domains, which are presented in Table 1. We selected BIRD-Bench as the basis for our dataset because it allows us to test column description generation across a broad set of domains. Additionally, the natural language queries in BIRD-Bench were designed so that external knowledge (e.g., metadata) is necessary to perform well on the task. In contrast, the WikiSQL (Zhong et al., 2017) and SPIDER (Yu et al., 2018) text-to-SQL benchmarks primarily uses exact column names in its natural language queries, making them unsuitable for our use case.\nWhile BIRD-Bench already includes column descriptions, we discovered that many columns either lacked descriptions entirely or contained insuffi-"}, {"title": "3.1 What is a good column description?", "content": "Natural language descriptions are crucial for addressing the often subtle ambiguities and complexities found in column names. Previous works that relied on a single word or type to describe columns are insufficient for capturing the full context and meaning. A more detailed description ensures that the interpretation of the column is free of ambiguity, facilitating better understanding and more accurate querying.\nDefining exactly what constitutes a good description in a set of rules is a complex task, because of the complex nature of natural language and the complexities in SQL databases. However, we used the following criteria to define a good description:\n\u2022 It should eliminate ambiguity and enhance the interpretability of a column.\n\u2022 It should provide clear and precise information about the column, specifying the nature of the data it holds and its context within a table.\n\u2022 It should include information about the subject of the reference, e.g., instead of \"The name,\" use \"The name of the client that made the transaction\" to ensure clarity.\n\u2022 It must be a complete and grammatically correct sentence that explicitly describes what the column represents.\nWe do not require the inclusion of interpretations of specific values inside the database, leaving the interpretation and generation of metadata for database values for future work.\nGiven the numerous ambiguities and complexities in SQL databases due to omitted information, we invite the community to engage in discussions about what constitutes a good column description and the necessary metadata to include."}, {"title": "3.2 Improving Existing Column Descriptions\nwith LLMs", "content": "We aimed to improve existing column descriptions using LLMs for two reasons: as a preliminary step before human annotators create the gold standard dataset, and to evaluate the effectiveness of LLMs in improving existing column descriptions.\nTo improve column descriptions, we used GPT-40, providing it with the BIRD-Bench column descriptions, corresponding schema in CREATE_TABLE statements, example data rows from the corresponding table, and instructions to generate descriptions for given columns. The model was given clear guidelines and in-context examples of the desired description format. The full prompt template is in Appendix B.4.\nTwo human annotators who are authors of this paper, fluent in English, and experts in SQL, assessed the original BIRD-Bench descriptions and the LLM-generated descriptions, independently assigning each description one of the following labels:\n\u2022 Perfect: Contains enough information so that the interpretation of the column is completely free of ambiguity.\n\u2022 Somewhat Correct: Matching the perfect description but verbose with redundant information (although without any incorrect or misleading information).\n\u2022 Incorrect: Contains incorrect information.\n\u2022 No Description: Description is missing.\n\u2022 I can't tell: Annotator cannot determine the description's accuracy based on available data.\nThe full annotator guidelines and more detailed descriptions of the labels are presented in Apendix \u0392.1.2.\nAnnotators' assessments of the two datasets are shown in Table 2. There was a substantial level"}, {"title": "3.3 Creating the Gold Standard Dataset", "content": "Since not all LLM-generated improvements were perfect, manual corrections of the generations were necessary to achieve the high standard required for a gold standard dataset. First, disagreements in scores were addressed by reviewing each other's annotations and then through discussion to reach a consensus. Descriptions that did not achieve a \u201cPerfect\u201d quality score from both annotators after this phase were manually corrected until both annotators agreed on a \u201cPerfect\u201d rating. The final set of descriptions was then reviewed to ensure they provided clear, comprehensive, and contextually rich information for each column. Through this process, we believe we have created a dataset of high quality, serving as a reliable reference for evaluating the performance of models in generating column descriptions. The final composition of the dataset can be found in Table 1. Examples of gold descriptions can be found in Appendix B.3."}, {"title": "3.4 Annotating Column Difficulty", "content": "Following the finalization of the gold standard descriptions, each column was assessed on the difficulty in generating descriptions for it. Columns were categorized into four levels: \"Easy,\" \"Medium,\" \"Hard,\" and \"Very Hard.\" These levels reflect the annotators' assessment of the amount of available information necessary to generate an accurate description for a column. Examples of columns of each difficulty can be found in B.3.\nAt the \"Easy\" level, annotators could determine the column description by only looking at the schema of the relevant table. For example, in the Financial database, the Client table's birth_date column is self-explanatory.\nAt the \"Medium\" level, annotators could accurately determine the column description given example data in addition to the table schema from the database. This suggests that while some additional context is required, the column's role can still be deduced with a reasonable amount of in-formation. For instance, in the student_club database, the zip_code table contains a column named short_state. Given the data inside the database, it is possible to accurately determine that this column contains two-letter abbreviations of states.\nColumns categorized as \"Hard\" present greater challenges. Even with the schema and example data from the database, annotators were unsure about what the column description should be. This uncertainty arises from ambiguities or complexities that cannot be resolved with the given information, highlighting the need for further context or domain knowledge. For example, in the formula_1 database's results table, the rank column has data indicating rankings, but without context, it is hard to determine if it refers to race position, qualifying rank, or another metric. The description, \"The overall rank of the driver's fastest lap time in the race,\" clarifies its purpose.\nThe \"Very Hard\" level involved columns where accurate descriptions were impossible without additional documentation. An example is the A11 column in the Districts table of the Financial database, which contains only numbers and a completely uninformative name.\nThe assessment was performed by two independent annotators, distinct from those who created the gold standard descriptions, but with the same level of expertise. The inter-annotator agreement for difficulty level was fair, with a Cohen's kappa of 0.31.\nAfter completing the annotation process, the annotators used the same procedure as for the gold descriptions to resolve disagreements. It became clear during their discussion that one annotator had misunderstood the difference between the \"Easy\" and \"Medium\" criteria, causing the low agreement score. This annotator incorrectly thought \"Easy\" included only columns with unambiguous names, whereas our definition allows considering other column names in the table. All remaining disagreements were resolved through discussion, resulting in one final difficulty score for each column.\nThe distribution of columns across the four difficulty levels is presented in Table 1. We observed that a significant portion of columns fell into the \"Easy\" and \"Medium\u201d categories, with \u201cVery Hard\" columns primarily residing in the Financial domain, where tables often have completely uninformative column names such as \"A1\"."}, {"title": "4 Generating Column Descriptions", "content": "In this section, we assess LLMs' ability to generate detailed column descriptions using our dataset, using both human evaluations and the LLM-as-a-judge."}, {"title": "4.1 Models", "content": "To evaluate the effectiveness of LLMs in generating column descriptions, we used our dataset presented in Section 3.3. We evaluated the following models: GPT-40, Mixtral-8x22B-Instruct-v0.1 (Jiang et al., 2024), Command R+, Qwen2-72B-Instruct (Yang et al., 2024), Codestral-22B-v0.1, and Mistral-7B-Instruct-v0.3 (Jiang et al., 2023). These models were selected to provide a diverse range of perspectives, including a state-of-the-art closed-source model (GPT-40), several newly released high-performing open-source models (Qwen2-72B, Mixtral-8x22B, Codestral-22B, Command R+), and a smaller model (Mistral-7B).\nEach model was given the database schema, the specified column, example data rows from the corresponding table, and specific instructions for generating the column description. The exact prompt is presented in Appendix B.4.\nInference for the open-source models was performed on several A100 GPUs, using BitsandBytes (Dettmers et al., 2022) to load the models in 8-bit format. We set the model temperature to 0.7."}, {"title": "4.2 Human Evaluation", "content": "For each model's generated output, two human annotators independently assessed the quality of the column descriptions. As previously, the set of annotators was different from the ones that created the gold dataset, but with the same type of expertise. The annotations were also based on a slightly different set of criteria from what was used when rating the original and improved column descriptions, presented in Section 3.2, as we now have the gold column descriptions as a reference point.\nThe generated outputs were scored for their usefulness, using labels \u201cPerfect\u201d, \u201cAlmost Perfect\", \u201cSomewhat correct\u201d, \u201cIncorrect\u201d, and \u201cNo Description\u201d. For example, the main difference between \u201cPerfect\u201d and \u201cAlmost Perfect\u201d, is that the latter contains additional redundant information that is not incorrect. A \"Somewhat correct\" column description is correct, but there is information missing. The full instructions and definitions of the labels are presented in Appendix B.1.4.\nTo ensure the reliability of our evaluations, we measured the inter-annotator agreement for each model's output. Cohen's kappa for each model is presented in Table 4. The agreement among annotators was above 0.60 for all models, indicating a substantial agreement between the annotators.\nThe results of the model evaluations are presented in Figure 2 and Table 3. Our observations show that GPT-40 and Command R+ generated\""}, {"title": "4.3 Evaluating Models Using\nLLM-as-a-Judge", "content": "We also performed an evaluation using LLM-as-a-judge, with GPT-40 as the judge. This evaluation aimed to investigate whether there exists an automated metric that could effectively replace human evaluations.\nThe LLM judge was provided with a task description, the model-generated descriptions, the gold standard references, and the rating criteria. The full prompt is presented in Appendix B.4.\nWe compared the model rankings as judged by GPT-40 to those determined by human annotators to see if there was a consistent correlation. The results, as shown in Table 4, indicate low alignment between GPT-40 and human annotators, with Cohen's Kappa scores ranging from 0.12 to 0.25. Notably, the agreement between human annotators was significantly higher, with Kappa scores ranging from 0.60 to 0.71.\nAs a small side study, we also combined the \"Perfect\" and \"Almost\" Perfect' scores, resulting in a doubling of Kappa values. Although the scores remain modest, this suggests potential for refining the evaluation process to better align with human ratings."}, {"title": "5 Impact of Column Descriptions for\nText-to-SQL", "content": "To study the usefulness of column descriptions for text-to-SQL systems, we evaluated the best-performing model (GPT-40) on the development set of BIRD-Bench, which contains 1534 natural language questions and SQL query pairs. Our goal was to isolate and understand the effects of column descriptions on model performance.\nThe experimental setup involved four scenarios:\n1. Without any column descriptions: The model operates with only the database schema.\n2. With BIRD-Bench original column descriptions: The model uses the original descriptions provided by BIRD-Bench in addition to the schema.\n3. With generated column descriptions: The model uses descriptions generated by the best model from Section 4 in addition to the schema, simulating a realistic scenario.\n4. With gold standard descriptions: The model uses the manually refined gold standard descriptions in addition to the schema, representing the best-case scenario.\nIn all scenarios, we included only column descriptions as the available metadata alongside the database schema. The full prompt for the text-to-SQL generation is presented in Appendix B.4.\nThe full results are presented in Table 5. Our findings indicate that column descriptions improve model performance on text-to-SQL tasks. We also observe that the GPT-40 generated column descriptions, which have been generated using only the database schema and example data, is slightly better than the existing BIRD-Bench column descriptions, which were created by human annotators."}, {"title": "5.1 Analyzing the Effects of Difficult Columns", "content": "To better understand the usefulness of column descriptions in low-information scenarios, we replaced all the column names in each database with uninformative column names. The first table in database A where denoted by A1, the second by A2, and so on.\nWe chose to study this extreme naming scenario to highlight the crucial role of column descriptions when column names provide little to no context. Studying this extreme case is important because it helps us understand the boundaries of model performance and the essential need for descriptive metadata. While such uninformative column names occur in the BIRD-Bench dataset, most realistic scenarios likely fall somewhere between these uninformative names and more informative column names.\nThe results of this experiment are presented in Figure 3. The findings clearly show that column descriptions are highly useful when performing text-to-SQL in scenarios with low-information column names, as the model's performance significantly improves when detailed descriptions are provided.\nThrough this experiment, we illustrate that detailed column descriptions are essential for the robustness of text-to-SQL systems, especially in challenging scenarios where column names alone are insufficient."}, {"title": "6 Conclusions and Future Work", "content": "In this study, we have created COLSQL, a dataset with curated human annotations of column descriptions. This dataset serves as a reliable gold standard for evaluating the performance of models in generating accurate and contextually rich column descriptions.\nOur main findings demonstrate that LLMs can effectively generate high-quality column descriptions. Among the models evaluated, GPT-4o and Command R+ performed best, particularly in generating descriptions that meet our \"Perfect\" criteria where GPT-40 meets the criteria for 75% of the columns.\nThrough our evaluation process, supported by rigorous human annotations, we have highlighted both the strengths and limitations of various models in this task and shown that LLM-as-a-judge is not yet viable alternative to human annotations when ranking models for this task. We leave it to future work to find reliable automatic evaluations for this task.\nWe have also shown that detailed column descriptions significantly enhance text-to-SQL systems, especially for columns that are completely uninformative. For uninformative columns we see a 20% increase. This underscores the importance of metadata as a bridge between the database system and generative models interacting with that system.\nFuture work will explore the generation of other types of metadata, such as table, value, and relationship descriptions, and their impact on text-to-SQL systems. By enhancing metadata, we aim to further improve the accuracy and usability of text-to-SQL systems.\nIn conclusion, the creation and utilization of detailed metadata is essential for the development of text-to-SQL models and for enabling human users of database systems. This paper is a first step in demonstrating the role of LLMs as generators of such metadata."}, {"title": "Limitations", "content": "In this study, we restricted our evaluation to the development set of BIRD-Bench due to the labor costs associated with performing the multitude of human evaluations required. Although the full training set encompasses more domains and might offer a broader evaluation scope, we believe that the development set contains sufficient variety to ensure robust testing of the models. The chosen subset still represents a wide range of domains, allowing us to draw meaningful conclusions about the performance of large language models in generating column descriptions.\nAnother limitation is that our annotators were not experts in all the specific domains covered by the dataset, such as Thrombosis Prediction. Despite this, the annotators reported confidence in their ability to accurately label column descriptions across all domains, including these more challenging areas. Nevertheless, the lack of domain-specific expertise could introduce some bias or inaccuracies in the annotation process. Future work should consider incorporating domain experts for annotation, particularly for specialized fields, to further enhance the reliability of the dataset."}, {"title": "Ethical Statement", "content": "All annotators involved in this study were fully informed about the workload and had the option to decide whether they wanted to participate. Since all annotators were also authors of this paper, they were not compensated for their efforts. However, we ensured that the annotations were produced ethically, with voluntary participation and informed consent from all contributors.\nIt is important to note that the metadata generation models presented in this paper are not perfect. A model that achieves high scores on our dataset may not generalize well to scenarios outside of the domains we studied. Consequently, caution should be exercised when using these models to generate column descriptions in systems that will be used by humans or other AI systems, such as text-to-SQL models. Ensuring the reliability and accuracy of generated metadata in different contexts is crucial to prevent potential misinterpretations or errors in downstream applications."}, {"title": "A Datasheet for Datasets", "content": ""}, {"title": "A.1 Motivation", "content": "For what purpose was the dataset created? Relational databases often suffer from uninformative descriptors of table contents, such as columns and values, impacting both human users and text-to-SQL models. This dataset was created to explore and assess how large language models can generate detailed and meaningful column descriptions to be used as a semantic layer over databases, to enhance their usability."}, {"title": "A.2 Composition", "content": "What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? The dataset is made up of 11 SQLite databases, accompanied by a csv file which contains gold standard column descriptions and difficulty ratings for every column in the entire dataset. Each instance therefore represents a column and its accompanying gold description and difficulty rating.\nHow many instances are there in total (of each type, if appropriate)? There are 798 columns in total across the 11 databases in the dataset. A full breakdown of columns over the different domains can be found in Table 1.\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? The dataset is based on the BIRD-Bench development set and includes all databases, tables, and columns from the development set.\nWhat data does each instance consist of? Each instance contains a column, its corresponding table and database, and its accompanying gold description and difficulty rating.\nIs there a label or target associated with each instance? If so, please provide a description. Yes. Each column contains a corresponding gold column description, as well as an annotated difficulty rating (Easy, Medium, Hard or Very Hard). These levels reflect the annotators' assessment of the amount of available information necessary to generate an accurate description for a column. Examples of columns of each difficulty can be found in B.3.\nIs any information missing from individual instances? No. The dataset has full coverage over the entire set of columns from the included databases.\nAre there any errors, sources of noise, or redundancies in the dataset? The dataset was created using a combination of automated techniques and human annotators. Although the goal was to create a dataset free of noise, it could be that some noise exists inadvertently due to human error.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? The dataset is self-contained.\nDoes the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)? No.\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? No."}, {"title": "A.3 Collection", "content": "How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. The data was directly observable and derived from the BIRD-Bench development set. The column descriptions were generated using a large language model and then manually verified and corrected by human annotators. The difficulty ratings where annotated and verified by human annotators.\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. Finally, list when the dataset was first published. The data was collected and the column descriptions were generated and annotated over a period of three months. The dataset will be published after the anonymity period.\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? How were these mechanisms or procedures validated? The dataset was collected from the BIRD-Bench development set. LLMs were used to generate initial column descriptions, which were then manually curated and corrected by human annotators. The validation was performed by expert annotators through a consensus process.\nWhat was the resource cost of collecting the data? (e.g., what were the required computational resources, and the associated financial costs, and energy consumption - estimate the carbon footprint. The resource cost included the computational expense of running LLMs on multiple A100 GPUs and the human labor for manual annotation. The exact financial cost and carbon footprint would require further detailed calculations, but they are comparable to typical machine learning model training and annotation tasks.\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? The dataset is not a sample; it includes the complete BIRD-Bench development set.\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? The data collection and annotation were performed by the authors of the paper, who are researchers with expertise in SQL and natural language processing. There was no additional compensation as the work was part of their research activities.\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. No formal ethical review was conducted as the data does not involve personal or sensitive information.\nDoes the dataset relate to people? If not, you may skip the remainder of the questions in this section. No."}, {"title": "A.4 Preprocessing / Cleaning / Labeling", "content": "Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section. Yes, preprocessing included several steps:\n\u2022 Data cleaning steps were applied to the original BIRD-Bench databases and dataset:\n- Removed non \"utf-8\" tokens from the original BIRD-bench metadata CSV files.\n- Corrected spelling in the european_football_2 database in the \"Country\" description header from \"desription\" to \"description\".\n- Removed all columns with zero data and the name \"Unnamed\" due to (too many/missing) commas in the original BIRD-Bench metadata CSV files.\n- Changed the file name of \"ruling.csv\" to \"rulings.csv\" to match the original table name in the card_games domain.\n- Changed \"set_transactions.csv\" to \"set_translations.csv\" to match the database name in the card_games domain.\n- Removed all.DS_STORE files from the data directory.\n- Changed the names of the CSV files in the student_club database to match the original table names. Changed upper case to lower case on the first letter of all CSV files, and corrected the code in Zip_Code.\n- Removed the \"wins\" column from constructors.csv file as the column does not exist in the formula_1 database.\n\u2022 Generating initial column descriptions using LLMs, followed by manual correction and labeling of the descriptions.\n\u2022 Categorizing the columns based on the difficulty of generating their descriptions."}, {"title": "A.5 Uses", "content": "Has the dataset been used for any tasks already? If so, please provide a description. The dataset has been used to evaluate the performance of various large language models in generating column descriptions and to study the impact of these descriptions on text-to-SQL systems.\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. This will be updated after the anonymity period with the appropriate links.\nWhat (other) tasks could the dataset be used for? The dataset could be used for tasks such as evaluating and improving text-to-SQL models, studying the impact of metadata on database usability, and developing automated systems for generating database documentation.\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/-cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks)? If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms? The dataset is composed of data generated and annotated for research purposes and does not include personal or sensitive information. Future users should ensure that the generated metadata is appropriate for their specific use case to avoid misinterpretations or errors in applications.\nAre there tasks for which the dataset should not be used? If so, please provide a description. The dataset should not be used for applications involving sensitive or personal data without proper validation and consideration of privacy implications."}, {"title": "A.6 Distribution", "content": "Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description. Yes, the dataset will be made publicly available for research purposes.\nHow will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)? The dataset will be distributed via GitHub and will be assigned a DOI for reference.\nWhen will the dataset be distributed? The dataset will be distributed after the anonymity period.\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions. The"}]}