{"title": "Improving Relational Database Interactions with Large Language Models: Column Descriptions and Their Impact on Text-to-SQL Performance", "authors": ["Niklas Wretblad", "Oskar Holmstr\u00f6m", "Erik Larsson", "Axel Wiks\u00e4ter", "Oscar S\u00f6derlund", "Hjalmar \u00d6hman", "Ture Pont\u00e9n", "Martin Forsberg", "Martin S\u00f6rme", "Fredrik Heintz"], "abstract": "Relational databases often suffer from uninformative descriptors of table contents, such as ambiguous columns and hard-to-interpret values, impacting both human users and Text-to-SQL models. This paper explores the use of large language models (LLMs) to generate informative column descriptions as a semantic layer for relational databases. Using the BIRD-Bench development set, we created COLSQL, a dataset with gold-standard column descriptions generated and refined by LLMs and human annotators. We evaluated several instruction-tuned models, finding that GPT-40 and Command R+ excelled in generating high-quality descriptions. Additionally, we applied an LLM-as-a-judge to evaluate model performance. Although this method does not align well with human evaluations, we included it to explore its potential and to identify areas for improvement. More work is needed to improve the reliability of automatic evaluations for this task. We also find that detailed column descriptions significantly improve Text-to-SQL execution accuracy, especially when columns are uninformative. This study establishes LLMs as effective tools for generating detailed metadata, enhancing the usability of relational databases.", "sections": [{"title": "Introduction", "content": "Structured data in relational databases is crucial for modern information systems. It not only stores essential business data but also allows organizations to analyze trends, optimize operations, and make informed decisions, maintaining a competitive edge in today's fast-paced economy. In this setting, text-to-SQL, which converts natural language queries into SQL commands, enhances access to structured data by reducing the need for expert knowledge.\nHowever, working with these databases often presents challenges due to complex table schemas and vague descriptors used for table contents, such as ambiguous column names and hard-to-interpret values (Talaei et al., 2024). These ambiguities can significantly hinder the effectiveness of both human users and generative models employed in text-to-SQL tasks. A good illustration of such an ambiguity can be found in the district table in the financial database of the BIRD-Bench text-to-SQL benchmark (Li et al., 2023), as illustrated in Figure 1. The meaning of columns labeled A2 and A3, which represent the district name and its respective region, are not immediately clear from the names alone.\nIn response to this challenge, the introduction of descriptive column descriptions as a semantic layer over the existing database structures can dramatically improve their usability. However, manually crafting such column descriptions is a time-consuming and labor-intensive process. The large amount of relational databases in use across society today, underscores the need for automated systems that can generate natural language descriptions for SQL databases in order to enhance their interpretability for both human users and text-to-SQL models.\nThe recent advancements in LLMs have shown that they can produce coherent and contextually relevant text across various domains, making them potential tools for automating such metadata creation [ref]. By leveraging LLMs, the hypothesis is that the labor-intensive process of generating metadata for relational databases can be significantly simplified. Previous research has used generative LLMs to expand abbreviated column names in a SQL setting (Zhang et al., 2023), but to the best of our knowledge, no works exist for generating column descriptions in the general case across a wider array of column types and formats.\nIn this paper we will for this reason explore how well LLMs can generate column descriptions that contain detailed and relevant information. To enable this study, we first create a dataset for column description generation. We use the development set of BIRD-Bench (Li et al., 2023) and its databases as the starting point for our dataset. While BIRD-Bench contains some column descriptions, it does not have complete coverage over all columns, and the existing descriptions are often uninformative.\nWe first improve the existing column descriptions in BIRD-Bench using an LLM to generate more informative descriptions, demonstrating the usefulness of an LLM in a scenario where some initial metadata exists. Secondly, expert human annotators manually correct the generated descriptions to create a refined dataset of gold-standard references as well as annotating the difficulty of generating a column description for each specific column. This corrected dataset serves as a benchmark for evaluating models on column description generation.\nWe then apply several instruction-tuned models of different sizes and types to our dataset and perform an extensive human evaluation for all models. We find that both GPT-40 and Command R+ frequently generate column descriptions that have perfect overlap with the gold standard.\nIn addition to human evaluations, we use an LLM-as-a-judge to evaluate model performance, studying how this automatic method correlates with human annotations and its viability for ranking models on this dataset. We find that the LLM judge does not align well with human evaluations and more work is needed to refine the LLM judge for this task.\nTo assess the general utility of column descriptions, we apply a model to a text-to-SQL task on the BIRD-Bench development set, both with and without column descriptions. Our results demonstrate that column descriptions aid in more accurately generating SQL queries, underscoring their importance in such tasks, especially for columns identified as having limited information. To further verify the importance of column descriptions in scenarios with limited information, we create an alternative version of the BIRD-Bench development set featuring completely uninformative column names. In this extreme scenario, we find that the inclusion of column descriptions is crucial for text-to-SQL generation.\nMain Takeaway: Our study demonstrates that LLMs can be a useful tool for generating column descriptions, and including detailed column descriptions leads to more accurate text-to-SQL query generation, particularly in scenarios where column information is limited or uninformative. Future work will focus on applying this methodology to generate other types of metadata, such as value descriptions, to improve the usability of relational databases for humans and generative LLMs."}, {"title": "Related Work", "content": "In recent years, significant effort has been devoted to simplifying the understanding of tabular data. Table-to-text (Zhao et al., 2023b,a; Kasner et al., 2023; Yang et al., 2022; Gong et al., 2019) and table question answering (Pal et al., 2023; Xie et al., 2022; Herzig et al., 2020) aim at developing models able to understand structured tabular data and natural language questions to perform reasoning and tasks across tables.\nMore relevant to our work are previous studies focusing on column type detection and column relationship identification for improving the understanding of tables. Column type detection involves classifying columns into predefined semantic categories, such as dates or currencies, to enhance data interpretation (Suhara et al., 2022; Zhang et al., 2020). Column relationship identification focuses on determining the structural relationships between columns, such as primary and foreign keys, to understand structured data in tables (Wang et al., 2021; Deng et al., 2022). Unlike these approaches, our method of generating column descriptions using large language models aims to produce natural language explanations of column contents, offering a more comprehensive and contextually rich understanding of the data.\nWhile the work of Zhang et al. (2023) focuses on expanding abbreviated column names to improve database schema readability and table understanding tasks using LLMs, our work goes further by generating full descriptions for column contents and considering a much more diverse range of column names."}, {"title": "Metadata Usage in Text-to-SQL", "content": "The shift from traditional query languages to natural language interfaces through text-to-SQL has been significantly advanced by current LLMs. However, the inherent ambiguities and missing information in database schemas necessitate the incorporation of external knowledge and metadata to improve conversion accuracy (Zhang et al., 2024). Previous studies have explored various approaches to address this challenge. Talaei et al. (2024) propose a pipeline that retrieves relevant data and context from a knowledge store to generate accurate SQL queries, leveraging a data store based on the original BIRD-Bench metadata files. Similarly, Dou et al. (2022) enhance the text-to-SQL process by using external domain-specific knowledge, such as mathematical formulas and publicly available resources.\nSun et al. (2024) introduce question-specific database content to improve tuning performance, while Hong et al. (2024) utilize a tailored data expert LLM to provide useful knowledge to the SQL-generating LLM. Petrovski et al. Petrovski et al. (2018) take a different approach by vectorizing entire data columns into embeddings, thus eliminating the need for traditional descriptions and metadata in the text-to-SQL process. These studies collectively validate that incorporating external knowledge and metadata can significantly enhance text-to-SQL systems.\nIn contrast to these approaches, our work focuses on generating comprehensive SQL column descriptions directly using LLMs. While prior research emphasizes leveraging external knowledge and complex data retrieval processes, we aim to enhance the interpretability and usability of SQL queries by creating detailed descriptions for SQL columns."}, {"title": "Column Description Dataset Creation", "content": "To enable our study on generating informative column descriptions, we create COLSQL, a new dataset based on the development set of the widely-used text-to-SQL benchmark, BIRD-Bench (Li et al., 2023)\u00b9. This dataset consists of 799 columns across 11 different databases and domains, which are presented in Table 1. We selected BIRD-Bench as the basis for our dataset because it allows us to test column description generation across a broad set of domains. Additionally, the natural language queries in BIRD-Bench were designed so that external knowledge (e.g., metadata) is necessary to perform well on the task. In contrast, the WikiSQL (Zhong et al., 2017) and SPIDER (Yu et al., 2018) text-to-SQL benchmarks primarily uses exact column names in its natural language queries, making them unsuitable for our use case.\nWhile BIRD-Bench already includes column descriptions, we discovered that many columns either lacked descriptions entirely or contained insufficient or erroneous information. This necessitated improving the existing column descriptions to create an adequate gold standard for evaluating models.\nIn the following sections, we first define what constitutes a good column description. We then describe how we improved BIRD-Bench's descriptions using generative LLMs and human annotators. Additionally, we explain how we annotated the difficulty level for generating a description from each column."}, {"title": "What is a good column description?", "content": "Natural language descriptions are crucial for addressing the often subtle ambiguities and complexities found in column names. Previous works that relied on a single word or type to describe columns are insufficient for capturing the full context and meaning. A more detailed description ensures that the interpretation of the column is free of ambiguity, facilitating better understanding and more accurate querying.\nDefining exactly what constitutes a good description in a set of rules is a complex task, because of the complex nature of natural language and the complexities in SQL databases. However, we used the following criteria to define a good description:\n\u2022 It should eliminate ambiguity and enhance the interpretability of a column.\n\u2022 It should provide clear and precise information about the column, specifying the nature of the data it holds and its context within a table.\n\u2022 It should include information about the subject of the reference, e.g., instead of \"The name,\" use \"The name of the client that made the transaction\" to ensure clarity.\n\u2022 It must be a complete and grammatically correct sentence that explicitly describes what the column represents.\nWe do not require the inclusion of interpretations of specific values inside the database, leaving the interpretation and generation of metadata for database values for future work.\nGiven the numerous ambiguities and complexities in SQL databases due to omitted information, we invite the community to engage in discussions about what constitutes a good column description and the necessary metadata to include."}, {"title": "Improving Existing Column Descriptions with LLMs", "content": "We aimed to improve existing column descriptions using LLMs for two reasons: as a preliminary step before human annotators create the gold standard dataset, and to evaluate the effectiveness of LLMs in improving existing column descriptions.\nTo improve column descriptions, we used GPT-40, providing it with the BIRD-Bench column descriptions, corresponding schema in CREATE_TABLE statements, example data rows from the corresponding table, and instructions to generate descriptions for given columns. The model was given clear guidelines and in-context examples of the desired description format. The full prompt template is in Appendix B.4.\nTwo human annotators who are authors of this paper, fluent in English, and experts in SQL, assessed the original BIRD-Bench descriptions and the LLM-generated descriptions, independently assigning each description one of the following labels:\n\u2022 Perfect: Contains enough information so that the interpretation of the column is completely free of ambiguity.\n\u2022 Somewhat Correct: Matching the perfect description but verbose with redundant information (although without any incorrect or misleading information).\n\u2022 Incorrect: Contains incorrect information.\n\u2022 No Description: Description is missing.\n\u2022 I can't tell: Annotator cannot determine the description's accuracy based on available data.\nThe full annotator guidelines and more detailed descriptions of the labels are presented in Apendix B.1.2.\nAnnotators' assessments of the two datasets are shown in Table 2. There was a substantial level of agreement between annotators, with a Cohen's kappa of 0.68 on the original BIRD-Bench descriptions and 0.61 on the improved generated descriptions. The GPT-40 descriptions showed a significant improvement over the original BIRD-Bench descriptions. While not perfect, LLM-generated descriptions can be a useful tool for improving existing column metadata when there are already quality issues in the existing column metadata."}, {"title": "Creating the Gold Standard Dataset", "content": "Since not all LLM-generated improvements were perfect, manual corrections of the generations were necessary to achieve the high standard required for a gold standard dataset. First, disagreements in scores were addressed by reviewing each other's annotations and then through discussion to reach a consensus. Descriptions that did not achieve a \"Perfect\" quality score from both annotators after this phase were manually corrected until both annotators agreed on a \u201cPerfect\u201d rating. The final set of descriptions was then reviewed to ensure they provided clear, comprehensive, and contextually rich information for each column. Through this process, we believe we have created a dataset of high quality, serving as a reliable reference for evaluating the performance of models in generating column descriptions. The final composition of the dataset can be found in Table 1. Examples of gold descriptions can be found in Appendix B.3."}, {"title": "Annotating Column Difficulty", "content": "Following the finalization of the gold standard descriptions, each column was assessed on the difficulty in generating descriptions for it. Columns were categorized into four levels: \"Easy,\" \"Medium,\" \"Hard,\" and \"Very Hard.\" These levels reflect the annotators' assessment of the amount of available information necessary to generate an accurate description for a column. Examples of columns of each difficulty can be found in B.3.\nAt the \"Easy\" level, annotators could determine the column description by only looking at the schema of the relevant table. For example, in the Financial database, the Client table's birth_date column is self-explanatory.\nAt the \"Medium\" level, annotators could accurately determine the column description given example data in addition to the table schema from the database. This suggests that while some additional context is required, the column's role can still be deduced with a reasonable amount of information. For instance, in the student_club database, the zip_code table contains a column named short_state. Given the data inside the database, it is possible to accurately determine that this column contains two-letter abbreviations of states.\nColumns categorized as \"Hard\" present greater challenges. Even with the schema and example data from the database, annotators were unsure about what the column description should be. This uncertainty arises from ambiguities or complexities that cannot be resolved with the given information, highlighting the need for further context or domain knowledge. For example, in the formula_1 database's results table, the rank column has data indicating rankings, but without context, it is hard to determine if it refers to race position, qualifying rank, or another metric. The description, \"The overall rank of the driver's fastest lap time in the race,\" clarifies its purpose.\nThe \"Very Hard\" level involved columns where accurate descriptions were impossible without additional documentation. An example is the A11 column in the Districts table of the Financial database, which contains only numbers and a completely uninformative name.\nThe assessment was performed by two independent annotators, distinct from those who created the gold standard descriptions, but with the same level of expertise. The inter-annotator agreement for difficulty level was fair, with a Cohen's kappa of 0.31.\nAfter completing the annotation process, the annotators used the same procedure as for the gold descriptions to resolve disagreements. It became clear during their discussion that one annotator had misunderstood the difference between the \"Easy\" and \"Medium\" criteria, causing the low agreement score. This annotator incorrectly thought \"Easy\" included only columns with unambiguous names, whereas our definition allows considering other column names in the table. All remaining disagreements were resolved through discussion, resulting in one final difficulty score for each column.\nThe distribution of columns across the four difficulty levels is presented in Table 1. We observed that a significant portion of columns fell into the \"Easy\" and \"Medium\u201d categories, with \u201cVery Hard\" columns primarily residing in the Financial domain, where tables often have completely uninformative column names such as \"A1\"."}, {"title": "Generating Column Descriptions", "content": "In this section, we assess LLMs' ability to generate detailed column descriptions using our dataset, using both human evaluations and the LLM-as-a-judge."}, {"title": "Models", "content": "To evaluate the effectiveness of LLMs in generating column descriptions, we used our dataset presented in Section 3.3. We evaluated the following models: GPT-40, Mixtral-8x22B-Instruct-v0.1 (Jiang et al., 2024), Command R+, Qwen2-72B-Instruct (Yang et al., 2024), Codestral-22B-v0.1, and Mistral-7B-Instruct-v0.3 (Jiang et al., 2023). These models were selected to provide a diverse range of perspectives, including a state-of-the-art closed-source model (GPT-40), several newly released high-performing open-source models (Qwen2-72B, Mixtral-8x22B, Codestral-22B, Command R+), and a smaller model (Mistral-7B).\nEach model was given the database schema, the specified column, example data rows from the corresponding table, and specific instructions for generating the column description. The exact prompt is presented in Appendix B.4.\nInference for the open-source models was performed on several A100 GPUs, using BitsandBytes (Dettmers et al., 2022) to load the models in 8-bit format. We set the model temperature to 0.7."}, {"title": "Human Evaluation", "content": "For each model's generated output, two human annotators independently assessed the quality of the column descriptions. As previously, the set of annotators was different from the ones that created the gold dataset, but with the same type of expertise. The annotations were also based on a slightly different set of criteria from what was used when rating the original and improved column descriptions, presented in Section 3.2, as we now have the gold column descriptions as a reference point.\nThe generated outputs were scored for their usefulness, using labels \u201cPerfect\u201d, \u201cAlmost Perfect\", \u201cSomewhat correct\u201d, \u201cIncorrect\u201d, and \u201cNo Description\u201d. For example, the main difference between \u201cPerfect\u201d and \u201cAlmost Perfect\u201d, is that the latter contains additional redundant information that is not incorrect. A \"Somewhat correct\" column description is correct, but there is information missing. The full instructions and definitions of the labels are presented in Appendix B.1.4.\nTo ensure the reliability of our evaluations, we measured the inter-annotator agreement for each model's output. Cohen's kappa for each model is presented in Table 4. The agreement among annotators was above 0.60 for all models, indicating a substantial agreement between the annotators.\nThe results of the model evaluations are presented in Figure 2 and Table 3. Our observations show that GPT-40 and Command R+ generated the highest proportion of \"Perfect\" descriptions, indicating their ability to provide concise and accurate column descriptions. In contrast, Qwen 2 72B and Codestral 22B had a high number of \"Almost perfect\" descriptions, suggesting a tendency to be verbose and add unnecessary information. Incorrect generations were relatively uncommon for most models, which might be more important than the distinctions between different levels of correctness. However, the ratio of incorrect answers were higher for smaller models like Mistral 7B.\nWe believe that the difficulty ratings of columns will aid in understanding and improving the performance of models in generating column descriptions across varying levels of complexity."}, {"title": "Evaluating Models Using LLM-as-a-Judge", "content": "We also performed an evaluation using LLM-as-a-judge, with GPT-40 as the judge. This evaluation aimed to investigate whether there exists an automated metric that could effectively replace human evaluations.\nThe LLM judge was provided with a task description, the model-generated descriptions, the gold standard references, and the rating criteria. The full prompt is presented in Appendix B.4.\nWe compared the model rankings as judged by GPT-40 to those determined by human annotators to see if there was a consistent correlation. The results, as shown in Table 4, indicate low alignment between GPT-40 and human annotators, with Cohen's Kappa scores ranging from 0.12 to 0.25. Notably, the agreement between human annotators was significantly higher, with Kappa scores ranging from 0.60 to 0.71.\nAs a small side study, we also combined the \"Perfect\" and \"Almost Perfect\" scores, resulting in a doubling of Kappa values. Although the scores remain modest, this suggests potential for refining the evaluation process to better align with human ratings."}, {"title": "Impact of Column Descriptions for Text-to-SQL", "content": "To study the usefulness of column descriptions for text-to-SQL systems, we evaluated the best-performing model (GPT-40) on the development set of BIRD-Bench, which contains 1534 natural language questions and SQL query pairs. Our goal was to isolate and understand the effects of column descriptions on model performance.\nThe experimental setup involved four scenarios:\n1. Without any column descriptions: The model operates with only the database schema.\n2. With BIRD-Bench original column descriptions: The model uses the original descriptions provided by BIRD-Bench in addition to the schema.\n3. With generated column descriptions: The model uses descriptions generated by the best model from Section 4 in addition to the schema, simulating a realistic scenario.\n4. With gold standard descriptions: The model uses the manually refined gold standard descriptions in addition to the schema, representing the best-case scenario.\nIn all scenarios, we included only column descriptions as the available metadata alongside the database schema. The full prompt for the text-to-SQL generation is presented in Appendix B.4. The full results are presented in Table 5. Our findings indicate that column descriptions improve model performance on text-to-SQL tasks. We also observe that the GPT-40 generated column descriptions, which have been generated using only the database schema and example data, is slightly better than the existing BIRD-Bench column descriptions, which were created by human annotators."}, {"title": "Analyzing the Effects of Difficult Columns", "content": "To better understand the usefulness of column descriptions in low-information scenarios, we replaced all the column names in each database with uninformative column names. The first table in database A where denoted by A1, the second by A2, and so on.\nWe chose to study this extreme naming scenario to highlight the crucial role of column descriptions when column names provide little to no context. Studying this extreme case is important because it helps us understand the boundaries of model performance and the essential need for descriptive metadata. While such uninformative column names occur in the BIRD-Bench dataset, most realistic scenarios likely fall somewhere between these uninformative names and more informative column names.\nThe results of this experiment are presented in Figure 3. The findings clearly show that column descriptions are highly useful when performing text-to-SQL in scenarios with low-information column names, as the model's performance significantly improves when detailed descriptions are provided.\nThrough this experiment, we illustrate that detailed column descriptions are essential for the robustness of text-to-SQL systems, especially in challenging scenarios where column names alone are insufficient."}, {"title": "Conclusions and Future Work", "content": "In this study, we have created COLSQL, a dataset with curated human annotations of column descriptions. This dataset serves as a reliable gold standard for evaluating the performance of models in generating accurate and contextually rich column descriptions.\nOur main findings demonstrate that LLMs can effectively generate high-quality column descriptions. Among the models evaluated, GPT-4o and Command R+ performed best, particularly in generating descriptions that meet our \"Perfect\" criteria where GPT-40 meets the criteria for 75% of the columns.\nThrough our evaluation process, supported by rigorous human annotations, we have highlighted both the strengths and limitations of various models in this task and shown that LLM-as-a-judge is not yet viable alternative to human annotations when ranking models for this task. We leave it to future work to find reliable automatic evaluations for this task.\nWe have also shown that detailed column descriptions significantly enhance text-to-SQL systems, especially for columns that are completely uninformative. For uninformative columns we see a 20% increase. This underscores the importance of metadata as a bridge between the database system and generative models interacting with that system.\nFuture work will explore the generation of other types of metadata, such as table, value, and relationship descriptions, and their impact on text-to-SQL systems. By enhancing metadata, we aim to further improve the accuracy and usability of text-to-SQL systems.\nIn conclusion, the creation and utilization of detailed metadata is essential for the development of text-to-SQL models and for enabling human users of database systems. This paper is a first step in demonstrating the role of LLMs as generators of such metadata."}, {"title": "Limitations", "content": "In this study, we restricted our evaluation to the development set of BIRD-Bench due to the labor costs associated with performing the multitude of human evaluations required. Although the full training set encompasses more domains and might offer a broader evaluation scope, we believe that the development set contains sufficient variety to ensure robust testing of the models. The chosen subset still represents a wide range of domains, allowing us to draw meaningful conclusions about the performance of large language models in generating column descriptions.\nAnother limitation is that our annotators were not experts in all the specific domains covered by the dataset, such as Thrombosis Prediction. Despite this, the annotators reported confidence in their ability to accurately label column descriptions across all domains, including these more challenging areas. Nevertheless, the lack of domain-specific expertise could introduce some bias or inaccuracies in the annotation process. Future work should consider incorporating domain experts for annotation, particularly for specialized fields, to further enhance the reliability of the dataset."}, {"title": "Ethical Statement", "content": "All annotators involved in this study were fully informed about the workload and had the option to decide whether they wanted to participate. Since all annotators were also authors of this paper, they were not compensated for their efforts. However, we ensured that the annotations were produced ethically, with voluntary participation and informed consent from all contributors.\nIt is important to note that the metadata generation models presented in this paper are not perfect. A model that achieves high scores on our dataset may not generalize well to scenarios outside of the domains we studied. Consequently, caution should be exercised when using these models to generate column descriptions in systems that will be used by humans or other AI systems, such as text-to-SQL models. Ensuring the reliability and accuracy of generated metadata in different contexts is crucial to prevent potential misinterpretations or errors in downstream applications."}]}