{"title": "Multimodal Flare Forecasting with Deep Learning", "authors": ["G. Francisco", "S. Guastavino", "J. Fernandes", "T. Barata", "D. Del Moro"], "abstract": "Solar flare forecasting mainly relies on photospheric magnetograms and associated physical features to predict forthcoming flares. However, it is believed that flare initiation mechanisms often originate in the chromosphere and the lower corona. In this study, we employ deep learning as a purely data-driven approach to compare the predictive capabilities of chromospheric and coronal UV and EUV emissions across different wavelengths with those of photospheric line-of-sight magnetograms. Our findings indicate that individual EUV wavelengths can provide discriminatory power comparable or better to that of line-of-sight magnetograms. Moreover, we identify simple multimodal neural network architectures that consistently outperform single-input models, showing complementarity between the flare precursors that can be extracted from the distinct layers of the solar atmosphere. To mitigate potential biases from known misattributions in Active Region flare catalogs, our models are trained and evaluated using full-disk images and a comprehensive flare event catalog at the full-disk level. We introduce a deep-learning architecture suited for extracting temporal features from full-disk videos.", "sections": [{"title": "1. Introduction", "content": "Solar flares are sudden bursts of electromagnetic radiation and energetic particles that can pose significant threats to human health and technology. Their potential danger is typically assessed based on the intensity of the emitted soft Soft X-Rays (SXR) flux. Flares are classified into five categories: A, B, C, M, and X, with each class representing a peak SXR flux that is one order of magnitude higher than the previous class. The M-Class threshold marks the point from which flares are considered strong, while the X-Class threshold indicates flares that present the most serious threats to society. A significant research effort is given on identifying physical precursors of flares, often derived from photospheric magnetograms. For example, the FLARECAST project identified 209 potential flare precursors, with 94% of them related to Active Region (AR) magnetic field properties Another important trend, initiated by Huang et al. (2018), involves using deep learning as a purely data-driven approach, employing neural networks to magnetogram images. In contrast, the potential of chromospheric and coronal observations for flare prediction remains less explored despite the belief that flare-triggering mechanisms originate in these regions One barrier to investigating the upper atmospheric layers for flare precursors is the current observational limitations in deriving magnetograms and related physical quantities from these regions. However, this challenge may soon be addressed at the chromospheric level with upcoming projects like the Solar Activity Monitor Network (SAMNet) which aim to provide high temporal coverage of chromospheric magnetograms. Moreover, instruments like the Solar Dynamic Observatory (SDO) and its Atmospheric Imaging Assembly (AIA) already provide rich spectral data that may offer valuable thermal and morphological plasma features for both the chromosphere and the corona. In this context, Dissauer et al. (2023) introduced a dataset of AR AIA patches, from which Leka et al. (2023) derived chromospheric and coronal features based on moment analysis, achieving promising results. Notable precursors identified include total emission, steep brightness variations, and high-order moments of running differences, which suggest a tendency for short, small-scale brightening events in flare-imminent regions. Additionally, Sun et al. (2023) utilized Convolutional Neural Network (CNN)s with AIA coronal images to forecast flares above the M-class threshold, achieving state-of-the-art results. Both Leka et al. (2023) and Sun et al. (2023) identified the 94\u00c5line emission as particularly predictive of upcoming flares. Sun et al. (2023) further demonstrated that averaging predictions of single-wavelength models outperforms individual models' predictions. The performance improvement achieved by ensembling models, such as model bagging, can typically be attributed to the law of large numbers. Specifically, under the assumption of independent and identically distributed Gaussian errors, averaging models' outputs reduces error variance, as individual errors tend to cancel each other out. However, this method does not exploit potential dependencies between the distinct models' features, which might also enhance predictions.\nIn this study, we aim to extend the works of Leka et al. (2023) and Sun et al. (2023) by:\n1. Comparing the discriminative power of chromospheric and coronal features with those derived from line-of-sight magnetograms.\n2. Investigating potential linear and non-linear complementarities between features from different atmospheric layers using multimodal models.\n3. Exploring the discriminative power of long-term temporal features in addition to single-timestep features.\nWe utilize Deep Learning as a purely data-driven approach on full-disk images produced by the SDO. This full-disk approach complements the AR-level approach of Leka et al. (2023) and Sun et al. (2023), and reduces mislabels due to known misattributions in AR datasets. Such misattributions, as highlighted by van der Sande et al. (2022), can affect up to 8% of flares above the M-class threshold, and up to 20% of these events are simply missing in the standard GOES Flare catalog. Although the Heliophysics Event Knowledgebase (HEK) corrects many of these issues, some flares remain misattributed or unaccounted for, potentially compromising model training and evaluation in the already challenging context of imbalanced and scarce data.\nOur focus is on binary forecasts of the occurrence of at least one flare above the M-class threshold within 24-hour windows. This paper is organized as follows: Section 2 describes the data, Section 3 describes the models and training methods, Section 4 describes the results and discussions."}, {"title": "2. Data", "content": "We use the SDO-2H-ML image dataset introduced in Francisco et al. (2024). This is a 2 hour resolution datasett ranging from May 2010 to April 2023, for a total amount of about 54000 dates for which are associated the corresponding Line-Of-Sight (LOS)-magnetogram and AIA images in the wavelength 1600\u00c5, 304\u00c5, 171\u00c5, 193\u00c5, 211\u00c5 and 94\u00c5. The AIA images are preprocessed through alignment, exposure normalization, correction for instrument degradation, and are then downsampled to a resolution of 1024x1024 pixels. AIA and HMI pixel values undergo logarithmic scaling and are reduced to 8-bit depth, preserving the majority of the original pixel distribution with minimal information loss, while also reducing the dataset's size. The image are then compressed as JPEGs, resulting in another moderate loss of small-scale and high frequency details. In an ablation study, we found in Francisco et al. (2024), that such JPEG compression, as well as a further downsampling to 448x448 pixels, had no significant impact on the resulting model performances. We therefore work again with the 448x448 resolution. We use the same flare catalog as in Francisco et al. (2024), which is an extension of Plutino et al. (2023)'s catalog."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Models", "content": null}, {"title": "3.1.1. Single Timestep Models", "content": "To predict flares using single-wavelength images or magnetograms, we employ transfer learning from models pre-trained on the ImageNet dataset. The extensive sample and class diversity in ImageNet enable models to learn general hierarchical filters and features that transfer well to different problems and datasets. Research by Kornblith et al. (2019) indicates that for new problems with limited data, fine-tuning all layers of a pre-trained model on the new dataset yields optimal performance, typically proportional to the model's original performance on ImageNet. Consequently, we use the EfficientNetV2-S. As of now, EfficientNetV2 models are among the best-performing CNN architectures, significantly outperforming ResNet models. EfficientNetV2 incorporates advanced residual blocks, including Mobile Inverted Bottleneck Convolution (MBCConv) which captures complex features with fewer parameters by expanding and then reducing the feature number. Additionally, Fused-MBConv blocks, introduced for that model, optimizes MBConv by merging pointwise and depthwise convolutions into a single step. These innovations contribute to the model's parameter efficiency and performance. The EfficientNetV2 also appears more adapted to our problem than state-of-the-art Vision Transformers (ViTs), such as Swin and SwinV2, as they only provide slightly better performance, while being harder to fine-tune with small datasets. Indeed, ViTs generally require larger datasets, extensive data augmentation, and regularization techniques, which can also reduce the effectiveness of transfer learning. We choose the EfficientNetV2-S variant with 20 million parameters as it offers a good compromise between performance and ease of fine-tuning for our relatively small training dataset. We remove the top convolutional and prediction layers and replace them with a final convolutional layer with 16 filters, followed by batch normalization and Swish activation. This configuration is more suited to our binary classification problem and smaller dataset than the original 1280 filters. This final convolution block is followed by a global pooling layer, resulting in 16 features that are fed to a final dense layer with 2 neurons and a softmax activation, outputting the probabilities for the negative and positive classes.\nTo adapt grayscale images to the model, which is pre-trained on RGB images, we duplicate each grayscale image twice to create a 3-channel input. Each resulting model will be denoted by Efn-w, where w is the corresponding wavelength in \u00c5ngstroms, and Efn-BLOS for the magnetogram.\nFor single-timestep multimodal inputs, we compare the following approaches:\n1. Pretrained EfficientNetV2-S on RGB combinations of single inputs. This approach leverages the transfer learning capabilities of the model originally trained on RGB images. We compare two combinations: a full coronal combination of 193 \u00c5, 211 \u00c5, and 94 \u00c5 wavelengths (Efn-193x211x94), and a cross-atmospheric combination of the magnetogram, 304 \u00c5, and 94 \u00c5 wavelengths (Efn-BLos x304x94).\n2. Features fusion from single-wavelength models. In this approach, we combine features extracted from separately trained single-input models into a unified fully connected layer to investigate the linear complementarity between features from different inputs. Specifically, we perform this fusion for the cross-atmospheric combination of the magnetogram, 304 \u00c5, and 94 \u00c5 wavelengths. The resulting model is denoted EfnFuse-BLOSx304x94."}, {"title": "3.1.2. Video Models", "content": "To incorporate temporal dynamics in analyzing the best-performing inputs, we study models using videos composed of 13 frames spaced 2 hours apart, covering the 24 hours preceding the forecasting window. Previous studies have performed flare forecasts on magnetogram videos of ARs using combinations of 2D-CNN and Long Short-Term Memory (LSTM), while Sun et al. (2022) utilized 3D-Convolutions. Our study, however, focuses on full-disk videos rather than ARs and compares EUV multimodal inputs to magnetograms alone. Full-disk videos present unique challenges, and we propose a novel deep learning model designed to effectively infer local solar events from such videos. While 2D convolutions and associated downsampling techniques such as strides and max pooling-emphasize frame-dominant features, these features may originate from different ARs across distinct full-disk frames. This can complicate the learning of meaningful temporal patterns when using subsequent timeseries models like LSTMs, as the resulting time series of features may correspond to different ARs at different timesteps. In contrast, 3D convolutions better preserve the temporal coherence of features across frames. We propose a hybrid approach, Video Local Event Neural System (VideoLENS), for full-disk videos that maintains this temporal coherence and captures short-term patterns using 3D convolutions, while also leveraging long-term dependencies through LSTM cells and an attention mechanism. The VideoLENS architecture begins with an initial block of 3D convolutions (C3D Block) that scales the original full-disk input down to features with a spatial scale comparable to ARs. Specifically, the C3D Block is designed so that each pixel in the final output feature map has an original spatial receptive field slightly larger than the largest possible ARs, accounting for their rotation during the 24 hours covered by the 13 input frames. each final time series of features consequently contains information localized to the corresponding original receptive field, ensuring that the time series accurately represents the temporal evolution of the corresponding solar region. A Local-Timeseries Block is then applied to each of these feature timeseries to derive more complex temporal features then used for local predictions. This block starts with a multi-head attention layer that computes attention scores along the temporal dimension, ensuring that each timestep is contextually aware of the entire time series and emphasizing the most relevant parts. An LSTM layer is then used to learn long- and short-term features localized to ARs. Subsequently, a pixel-local prediction layer, consisting of a fully connected neuron with sigmoid activation, predicts the probability of a flare at each spatial region. The final full-disk prediction is derived as the maximum of these local predictions, obtained through a GlobalMaxPooling layer that aggregates the local predictions. The resulting model can therefore aslo be used as a semi-supervised framework able to learn spatial label while only receiving non spatial one at training. It thus extend the Patch-Distributed-CNN (P-CNN) to video input, while removing the limitations of the patches artificial boundaries of this former model. Similarly to the P-CNN, precise position estimation of the predicted events could then be calculated from gradient methods for each of the local prediction."}, {"title": "3.2. Training and hyperparameters", "content": "We use the same full disk Cross-Validation (CV) method as presented in Francisco et al. (2024). Specifically temporal buffers of 27-days are used to create independent temporal chunks that are selected by optimisation to build balanced training and validation folds. This result in a 5-fold CV done on the period ranging from May 2010 to December 2019, while the period from January 2020 to April 2023 is unaltered, meaning every sample and the natural distribution of the data is preserved and kept as a complementary test set. Our models are trained using the Adam optimizer. We conducted a Bayesian parameter search for the EfficientNetV2 model with magnetogram inputs (Efn-BLOS) to determine an optimal learning rate and the potential inclusion of a weight decay parameter. Although decoupled weight decay regularization is typically employed to reduce overfitting risk, we found it to have minimal impact on the fine-tuning of the pre-trained EfficientNetV2. This observation is consistent with the findings of which indicated that regularization methods may not always be useful in various transfer learning scenarios. Our results also showed that fine-tuning the EfficientNetV2 generally converges in fewer than 10 epochs across most parameter combinations tested. This rapid convergence aligns with who reported that fine-tuning pre-trained models typically requires significantly fewer epochs than training from scratch -approximately 17 times faster. Based on the parameter search results, we train all EfficientNetV2-based models with a learning rate of 3 \u00d7 10-4 for 15 epochs and without weight decay regularisation. For the VideoLENS models, a manual parameter search revealed that a slight weight decay regularization of 1 \u00d7 10-5 marginally enhances performance. We also found an optimal learning rate that remains the same as for the EfficientNetV2 models, at 3 \u00d7 10\u20134. Finally, we use weighted binary cross-entropy as the loss function to fully address class imbalance."}, {"title": "3.3. Evalutation", "content": "To compare the predictive power of the models, we focus on the Area Under the Curve (AUC) of the Receiver Operating Characteristic (ROC), the True Skill Statistic (TSS), the Heidke Skill Score (HSS) and the Matthews Correlation Coefficient (MCC). The ROC curve gives a model's achievable recall as a function of the False Positive Rate (FPR) over the different decision thresholds. It directly relates to the TSS which is one of the preferred metrics in flare forecasting studies, as the vertical distance between the diagonal and the ROC correspond to the TSS at the corresponding threshold. Optimal ROC AUC values therefore improve the likelihood of finding a threshold with good TSS values. The ROC AUC is an easily interpretable value of a classifier's discriminatory power as it corresponds to the probability that among two random positive and a negative samples, the classifier will assign a higher probability output to the positive one. The TSS, is defined as the difference between the recall and the FPR. Also known as the (bookmaker) informedness, Peirce's index or Younden's J index, it is also equal to the balanced accuracy rescaled between -1 and 1 :\n$TSS = \\frac{TP}{TP+FN} - \\frac{FP}{FP+TN} = TPR - FPR, $ (1)"}, {"title": "4. Results & Discussions", "content": "Figures 2, 4, 3 and 5 present the performance metrics of the models: ROC AUC, HSS, TSS and MCC respectively. The plots rank the models based on their performance, with a consistent ranking observed across all three metrics. Notably, while ROC AUC values are clustered within a narrow range (+16% from the worst to the best model), the MCC, HSS and TSS demonstrate more pronounced disparities in model performance (with maximum performance gaps of +54% and +51%, respectively). The results also show stability between validation and test sets, indicating good generalization capabilities. However, the model using the 1600\u00c5 wavelength exhibited a marked decline in HSS and TSS by 24% and 26%, respectively, on the test set compared to the validation set. This performance drop may be attributed to the learning of unrelevant features from the 1600\u00c5 wavelength that persist across the 27-day buffer period of the CV folds, leading to potential overfitting on the validation data. In addition, there might be some pattern specific to the solar cycle's 24th period which does not generalize to the next cycle. This finding underscore that even rigorous CV methods do not guarantee performance generalization in operational settings. Consequently, incorporating a separate operational test set, as advocated by Cinto appears essential for a more comprehensive evaluation of model performance."}, {"title": "4.1. Coronal and Chromospheric Features are Most Predictive of Flares", "content": "The 94\u00c5 wavelength provides the most discriminative features for flare prediction, consistent with findings from Leka et al. (2023) and Sun et al. (2023). Compared to the line-of-sight magnetograms, the Efn-94 model shows a significant improvement, with a 3.4% increase in TSS on the test set and a 10.4% increase on the validation set. Additionally, HSS values improve by 12% on the test set and 9% on the validation set. Although the 193\u00c5 and 211\u00c5 wavelengths demonstrate similar or lower predictive performance to the line-of-sight magnetograms, their combination with the 94\u00c5 wavelength in the Efn-193x211x94 model results in a further enhanced HSS, showing an overall increase of 18% on both the test and validation sets compared to the magnetograms alone. This suggests that the relative intensity variations between these three wavelengths may serve as effective precursors for flare events. Chromospheric observations also contribute valuable predictive features. While the lower chromospheric wavelength of 1600\u00c5 exhibits weak generalization performance on the test set, the upper chromospheric wavelength of 304\u00c5 emerges as the second most predictive single input, following the 94\u0102 wavelength."}, {"title": "4.2. Features from Different Atmospheric Layers Complement Each Other", "content": "Combining features from various solar atmospheric layers enhances overall performance compared to using the single best-performing layer. The model EfnFuse-BLos x304x94, which integrates features from Efn - BLOS, Efn - 304, and Efn \u2013 94, outperforms the Efn \u2013 94 model by 5.7% and 7.4% in HSS on the test and validation sets, respectively. While improvements in TSS and ROC AUC are more modest, the notable increase in HSS indicates enhanced precision, with improved performances in discriminating ambiguous cases. We found that, cross-atmospheric feature complementary is most effective through feature fusion after training individual models for each input (EfnFuse-BLOS x304x94). Training a single model directly on the cross-atmospheric channel combination with the pretrained EfficientNetV2 ((Efn-BLOS x304x94) does not surpass the performance of the model using the most predictive channel alone, conversely to the coronal wavelength combination (Efn-193x211x94). This result might arises from the 2D-CNN architecture, where combining multiple channels in early convolutional layers treats them as a distinct feature dimension."}, {"title": "4.3. Temporal Dynamics Enhance Predictive Performance Compared to Static Features", "content": "The VideoLENS-193x211x94 model outperforms all others, showing up to a 5.8% improvement in ROC AUC, a 14% increase in TSS, and a 23% enhancement in HSS over the Efn-BLOS model. When compared to the static single-frame model Efn-193x211x94, the VideoLENS-193x211x94 achieves improvements of up to 5.6% in TSS and 4.4% in HSS. Attempts to combine other wavelength pairs, such as the cross-atmospheric combination BLOS x304x94, using both parallel processing and an RGB-like processing approach similar to VideoLENS-193x211x94, did not yield models significantly surpassing the performance of the Efn-193x211x94 single-frame coronal model. These results underscore that the 193x211x94 wavelength combination provides some of the most predictive information for flare forecasting among the atmospheric layer combinations tested. The Efn-193x211x94 and VideoLENS-193x211x94 models achieve HSS scores of 0.46 and 0.48, respectively, significantly surpassing other known full-disk models for forecasting M+ flares within 24 hours. For context, Pandey et al. (2023) reports an HSS of 0.35 for a similar problem using full-disk line-of-sight magnetograms over a comparable validation period."}, {"title": "4.4. VideoLENS Ablation Study", "content": "The VideoLENS architecture was evaluated through an ablation study to determine the impact of various components on model performance. This study revealed that incorporating local-wise predictions and the time-series block offered slight improvements in ROC AUC compared to models using only C3D with global pooling. Increase in model size, through the number of filters or parameters of the attention and LSTM layers, did not result in significant performance gains, likely due to the modest size of the training dataset. At equal number of parameters, fewer C3D layers but more filters performed less effectively, indicating that simply increasing the number of filters is not beneficial without careful architectural tuning. More importantly, every C3D-based models consistently outperformed 2D-CNN-LSTM architectures, highlighting that the latter may not be as effective for analyzing full-disk videos. This is attributed to the tendency of the 2D-CNN component to capture features from various ARs across different frames, when multiple ARs coexist during the video."}, {"title": "4.5. The last and first flare challenge", "content": "An examination of model performance revealed that all models exhibited roughly null TSS and HSS scores when evaluated on the samples' subsets where the label of the predicted time window differed from the previous time window. This confirm that our previous observations from Francisco et al. (2024) generalise to a wide range a input modalities and to the use of low temporal resolution data to forecast flares. Specifically, the models struggle to forecast events that occurred in the next 24 hours following a period of inactivity, as well as to forecast a quiet period when flares occurred in the past 24 hours. This indicates that while the models can effectively predict flares when there is a persistent level of activity, they face substantial difficulties in scenarios involving activity changes, with performance akin to random guessing in these transitional cases. The current most predictive flare precursors might, therefore, be more indicative of the current activity level of an AR rather than being actually predictive."}, {"title": "5. Conclusion and Future Works", "content": "For years, studies on solar activity forecasting have been employing the photospheric AR complexity (computed either from continuum images or from magnetic field information) to predict the AR flare activity. However, it is widely accepted that the coronal magnetic topology and dynamics are crucial to driving and then triggering the release of solar eruptions.\nSince direct information on the coronal magnetic field is still not available for such studies, researchers are using as a proxy the EUV multiwavelength images to reveal the onset mechanism of solar flares in case studies and also for statistic approaches . Such uses have been recently extended to deep-learning approaches to automatically extract flare precursors from EUV images to build models for flare forecasting. Following this trend, we created a suitable dataset for DL approaches and investigated different 2D and 3D CNN models, exploiting the multiwavelength EUV images from the AIA instrument, the photospheric LoS magnetogram from the HMI instrument, and the temporal dimension.\nBelow is a summary of the main findings from this study:\n1. Enhanced Forecasting with Coronal Wavelengths: The use of coronal EUV wavelengths emissions notably enhances forecasting performance compared to relying on photospheric line-of-sight magnetograms. The combination of emission lines at 193 \u00c5, 211 \u00c5, and 94 \u00c5 proves particularly effective. This suggests that the relative intensity between EUV wavelengths may provide crucial information about flare precursors. Future research will focus on a detailed investigation of the Efn-193x211x94 model to identify specific precursors through explainability methods and gain further insights into flare mechanisms.\n2. VideoLENS Architecture and Temporal Dynamics: The VideoLENS architecture is an efficient way to forecast localized solar events from full-disk videos. Notably, the use of such model to incorporate temporal dynamics in flare predictive features yields superior performance compared to single-timestep input models. Specifically, the VideoLENS model using the 193 \u00c5, 211 \u00c5, and 94 \u00c5 wavelengths achieves an HSS of 0.50 and a TSS of 0.65, substantially outperforming typical full-disk models that forecast M-class flares within the next 24 hours.\n3. Challenges in Forecasting Activity Changes: All tested models exhibit limitations in forecasting transitions in activity levels. This suggests that the features derived from the studied inputs are more indicative of current activity levels rather than exclusive predictors of upcoming flares. While the inherent stochasticity of complex physical systems may partly explain these limitations in forecasting activity changes, further research could be of interest.\nFuture work will explore higher temporal resolution features than the 2 hours used in this work as well as different forecasting window sizes, and the addition of other data modalities such as SXR and Extreme Ultraviolet (EUV) flux timeseries."}, {"title": "6. List of Acronyms", "content": "AR\tActive Region\nCNN\tConvolutional Neural Network\nEUV\tExtreme Ultraviolet\nLOS\tLine-Of-Sight\nLSTM\tLong Short-Term Memory\nSDO\tSolar Dynamic Observatory\nTSS\tTrue Skill Statistic\nHSS\tHeidke Skill Score\nMCC\tMatthews Correlation Coefficient\nP-CNN\tPatch-Distributed-CNN\n\u0391\u0399\u0391\tAtmospheric Imaging Assembly\nSXR\tSoft X-Rays\nCV\tCross-Validation\nVideoLENS\tVideo Local Event Neural System\nAUC\tArea Under the Curve\nROC\tReceiver Operating Characteristic\nFPR\tFalse Positive Rate"}]}