{"title": "Adapting a Foundation Model for Space-based Tasks", "authors": ["Matthew Foutter", "Praneet Bhoj", "Rohan Sinha", "Amine Elhafsi", "Somrita Banerjee", "Christopher Agia", "Justin Kruger", "Tommaso Guffanti", "Daniele Gammelli", "Simone D'Amico", "Marco Pavone"], "abstract": "Foundation models, e.g., large language models, possess attributes of intelligence [23] which offer promise to endow a robot with the contextual understanding necessary to navigate complex, unstructured tasks in the wild. In the future of space robotics, we see three core challenges which motivate the use of a foundation model adapted to space-based applications: 1) Scalability of ground-in-the-loop operations; 2) Generalizing prior knowledge to novel environments; and 3) Multi-modality in tasks and sensor data. Therefore, as a first-step towards building a foundation model for space-based applications, we automatically label the AI4Mars dataset [22] to curate a language annotated dataset of visual-question-answer tuples. We fine-tune a pretrained LLaVA checkpoint on this dataset to endow a vision-language model with the ability to perform spatial reasoning and navigation on Mars' surface. In this work, we demonstrate that 1) existing vision-language models are deficient visual reasoners in space-based applications, and 2) fine-tuning a vision-language model on extraterrestrial data significantly improves the quality of responses even with a limited training dataset of only a few thousand samples.", "sections": [{"title": "I. INTRODUCTION", "content": "Advancements in the development of internet-scale machine learning models trained through self-supervision on a corpus of human knowledge, i.e., Foundation Models (FMs) [3], provide an opportunity to automate complex decision making and reasoning tasks transcribed through language, video, and speech. State-of-the-art (SoTA) large language models (LLMs) already display strong commonsense reasoning and understanding capabilities that, for example, enable them to score in the upper quartile on a variety of standardized exams [17]. These commonsense reasoning capabilities make the use of FMs attractive in space robotics, satellite operations, and other space-related domains, where they show potential to mitigate core challenges such as: 1) Scalability of ground-in-the-loop operations; 2) Generalizing prior knowledge to novel environments; and 3) Multi-modality in tasks and sensor data. Accordingly, in this paper, we conduct a preliminary investigation of the application of pretrained multi-modal FMs to the space domain. As a first step towards developing a space foundation model, we focus on a space robotics application in which a rover navigates a planetary environment (Fig. 1). We programmatically generate language annotations on the AI4Mars image dataset [22] to adapt and evaluate vision-language models (VLMs) across several spatial reasoning and navigation tasks. These tasks are inspired by the detailed sensory reasoning necessary to, e.g., identify sites of scientific interest or validate candidate motion plans. Our evaluations demonstrate that 1) existing VLMs are deficient visual reasoners in space-based applications, and 2) fine-tuning a VLM on our programmatically generated tasks significantly improves the"}, {"title": "II. RELATED WORK", "content": "Vision-Language Models: Recent advances in natural language and image processing have enabled the development of large-scale VLMs trained on internet-scale data. Early work develops an understanding of vision and language by using vision and text encoders [19], while VLMs build atop a language model to allow for open-ended visual reasoning such as Visual-Question-Answering (VQA) [11, 5, 14, 7]. In this work, we investigate LLaVA v1.5 [13] as the base model for fine-tuning given it is SoTA among open-source models on standard VQA benchmarks [1, 10].\nFoundation Models in Robotics: Prior work incorporates FMs at various levels of the autonomy stack, ranging from planning/decision making [24, 12] to semantic [8] and visual [21] reasoning. There is also emerging work on adapting FMs to space-based applications. SpaceTransformers [2] fine-tunes variations of BERT [6] on a corpora of systems engineering texts and an augmented mission standards dataset to recognize space mission requirements, while Rodriguez-Fernandez et al. [20] leverages GPT-3.5 [4] as the policy backbone for language-based autonomous satellite operations. We extend these works by incorporating both vision and language into a shared representation for enhanced reasoning.\nLarge-scale Dataset Curation: Related work in large-scale data collection includes Gao et al. [9] that develops a dataset of objects annotated with physical properties for image classification. We aim to extend this work by developing"}, {"title": "III. ARCHITECTING SPACE-LLAVA", "content": "In this work, we adapt a FM to two space-based applications using the AI4Mars dataset which encompasses 35k images with crowd-sourced semantic segmentation masks of Mars' terrain gathered from the Curiosity, Opportunity, and Spirit rovers. A representative example of raw terrain from the AI4Mars dataset and its associated semantic masks for each terrain class is provided in Fig. 2.\nGiven the AI4Mars dataset, we require a high-quality and scalable technique to generate QA pairs in natural language to endow an open-source VLM with the ability to perform spatial reasoning and high-level motion planning on withheld terrain. We ground a VLM in the visual and semantic features of Mars' terrain by fine-tuning LLaVA v1.5 13B [13] on our augmented dataset with the standard auto-regressive language modeling loss. Suppose we curate a dataset $\\mathcal{D} = \\{(\\bm{I}^{(i)}, \\bm{Q}^{(i)}, \\bm{A}^{(i)})\\}_{i=1}^n$ consisting of $n$ image $\\bm{I}^{(i)} \\in \\mathbb{R}^{h\\times w\\times 3}$, question $\\bm{Q}^{(i)} \\in \\mathbb{R}^{T_Q}$, and answer $\\bm{A}^{(i)} \\in \\mathbb{R}^{T_A}$ tuples where $T_Q$ and $T_A$ denote the maximum tokenized question and answer sequence length, respectively, with padding. We fine-tune LLaVA by freezing certain parameters in the model, e.g., only fine-tuning the language backbone, to optimize the objective\n$\\min_{\\Theta} L(\\Theta|\\mathcal{D}),$ (1)\nwhere we construct $L(\\Theta|\\mathcal{D})$ as the negative log-likelihood loss on token generation assuming samples are independent and identically distributed and using the chain rule factorization for auto-regressive generation. More formally, we define:\n$L(\\mathcal{A}|\\mathcal{D}) = - \\frac{1}{n} \\sum_{i=1}^n \\sum_{t=1}^{T} \\log p_{\\Theta}(\\bm{x}^{(i)}_t | \\bm{I}^{(i)}, \\bm{Q}^{(i)}, \\bm{A}_{0:t}^{(i)}),$ (2)\nwhere each term in the summation represents the log-likelihood, under the model's current weights $\\Theta$, to predict the ground-truth next text token in the answer sequence $\\bm{A}^{(i)} = \\{\\bm{x}_1^{(i)}, \\bm{x}_2^{(i)}, ..., \\bm{x}_{T}^{(i)}\\}$ conditioned on the sample's visual input, associated question and the full answer sequence preceding $\\bm{x}_{t-1}^{(i)}$. Here, $\\mathcal{A} \\subseteq \\Theta$ indicates that the unfrozen weights are a subset of the model's weights $\\Theta$. We evaluate the quality of the fine-tuned model's responses in comparison to a base model by prompting GPT-4 to choose the preferable response conditioned on the ground-truth answer for a particular question. A template of the prompt we provide to GPT-4 is provided in Appendix VI-A with further discussion on the prompt's construction.\nThat is, through fine-tuning a VLM on semantically annotated terrain from the AI4Mars dataset, we measure whether the fine-tuned model outperforms SoTA VLMs on the same task without adaptation, i.e., zero-shot."}, {"title": "IV. DATA GENERATION PIPELINE", "content": "In order to adapt a FM to the unique visual and semantic features on Mars' surface, we develop a language QA gener-ation pipeline on AI4Mars' semantic segmentation masks for spatial reasoning and high-level motion planning. Explicitly, we choose to curate a dataset of spatial reasoning and navigation samples since these two tasks together require semantic analysis on an extraterrestrial environment and leverage an existing, high-quality dataset. We first present our programmatic solution to curate spatial reasoning QA pairs after which we present a similar methodology to curate high-level motion planning QAs.\n### A. Spatial Reasoning Dataset\nWe translate semantic segmentation masks into QA pairs requiring spatial reasoning through two programmatic measures of position in an image. First, we process each terrain's segmentation mask using KMeans clustering with $K = 1$ to identify a surrogate for the centroid. While choosing $K = 1$ does represent a strong inductive bias in clustering, we have noticed that dominant collections of terrain tend to naturally cluster together on Mars' surface, i.e., rarely is it the case two large instances of soil or rocks are isolated on opposite ends of an image, in which case $K = 1$ represents a reasonable approximation of the terrain's general position. Further, we mitigate the impact of outliers in terrain classification, i.e., small patches of classified terrain distant from the dominant patch(s), through multiple random initializations for clustering and accept the cluster center with the lowest total variance across all runs. Also, we develop a second measure of each class' position by dividing the image into a 3x3 grid and measuring the population, or number of pixels, for each class in each grid. This simple measure of population helps formalize a notion of terrain density. A weakness of KMeans clustering is that we lose global information on the spread of terrain in an image, e.g., does 75% of soil or only 30% of soil in view exist in the bottom right corner of the image? Separately, we also enclose each semantic mask with bounding box annotations. Therefore, we supplement each cluster center with grid population counts and bounding boxes to provide a more holistic measure of terrain position. We use these three measures of a terrain's position to programmatically generate QA pairs to elicit spatial reasoning based on the AI4Mars dataset. That is, we develop a set of template prompts for seven styles of QA pairs: terrain description; terrain localization; multi-instance terrain localization; relative terrain localization; terrain coverage;"}, {"title": "B. High-level Motion Planning Dataset", "content": "In order to curate high-level motion plans that are cognizant of Mars' terrain, we curate a sequence of cardinal direction maneuvers, e.g., up, down, left, or right, connecting a start and goal point through a small grid over each AI4Mars sample. First, we discretize the image into a 5x5 grid and classify each grid into a terrain class according to majority representation. Then, we can mask out a particular class, e.g., large rocks, and if a feasible path to the goal exists, we use the A* search algorithm to plan a path from the start to the goal. We choose the starting point as the lowest point along the center column of the image which is not occupied by 1) the rover itself and 2) the class we choose to mask from the image. For each sample, assuming a feasible path exists, we choose multiple path endpoints among the unmasked grids.\nFor each image with an annotated mask to identify the rover, i.e., the MSL dataset within AI4Mars, we ask 2-3 question types for every start and end point pair: 1) whether a feasible path to the goal exists, 2) whether a candidate path overlaid on the image is both feasible and reaches the desired goal and 3) only if a feasible path exists, we request a path in natural language to the goal. An example for each question type is presented in Appendix VI-B."}, {"title": "V. EXPERIMENTS AND DISCUSSION", "content": "Having outlined our approach to data collection, we fine-tune LLaVA on our augmented planetary dataset, ablate different training configurations, and compare the model's performance to SoTA FMs applied zero-shot to space-based applications. Explicitly, we investigate whether adaptation is necessary for SoTA VLMs in an extraterrestrial environment, and if so, to what degree components of the model must be adapted. Then, we discuss future work to extend a space foundation model to applications in orbital space.\n### A. Experimental Setup\nRecall from Fig. 1 that the LLaVA model is comprised of three components: a vision encoder (VE), a multi-modal adapter (MMA), and a language model (LM). In this work, we experiment with training four combinations of LLaVA's components: (1) training the vision encoder and multi-modal adapter together; (2) training only the multi-modal adapter; (3) training only the language model backbone; and (4) training all components of the model. A complete description of the experimental details is available in Appendix VI-C.\n### B. Experimental Results\nThe results of fine-tuning each configuration using our augmented AI4Mars dataset in comparison to zero-shot LLaVA and GPT-40 are presented in Table I. Further, we provide an example generation from Space-LLaVA and a qualitative comparison to GPT-40 in Fig. 4. Based on these results, it is immediately apparent that SoTA VLMs out-of-the-box are ill-equipped to process the novel semantic features on Mars likely due to a visual domain gap. Indeed, SoTA VLMS produce inferior content relative to an adapted model in Table I. For example, in Fig. 4a, GPT-4o hallucinates a path of bedrock on the left and leads the rover into a smooth sand patch in pursuit of the goal; However, in Fig. 4b, Space-LLaVA suggests a more favorable plan which leads the rover along the bedrock in view and thereby reaches the goal without exposure to sand. We find that jointly training the language model and visual components provides the largest benefit for spatial reasoning tasks. As shown in Table I, fine-tuning the language and vision components in concert produces the best-performing model at just over 87% and 85% response preference to LLaVA zero-shot and GPT-40, respectively, as may be expected given"}, {"title": "C. Future Applications: Orbital Space", "content": "Extending the concept of a space foundation model to orbital operations presents additional challenges, because the characteristics of orbital scenarios and data sources are even further removed from typical terrestrial applications. Nevertheless, it remains desirable to leverage the advantages of FMs when pursuing complex in-orbit objectives such as resilient positioning, navigation and timing (PNT), space situational awareness (SSA), and collision avoidance.\nTo provide a holistic example, we propose a novel optical PNT/SSA framework named Fast Autonomous Lost-in-space Catalog-based Optical Navigation (FALCON). FALCON runs on board one or multiple observer spacecraft and uses bearing angles to visible Resident Space Objects (RSO) to: 1) determine the observer's orbit; 2) refine the orbits of tracked RSO targets; and 3) provide RSO collision avoidance alerts. FALCON oper-ates in a \"lost in space\" manner, i.e. without a-priori observer orbit knowledge. The key idea is to detect RSO in images from an on-board camera, match RSO to existing identities in an RSO catalog, and use the known positions of identified RSO as opti-cal beacons for positioning. Subsequently, RSO catalog data can be used to provide collision assessments and trigger collision avoidance maneuvers. Figure 5 provides a notional illustration. A key challenge presented by FALCON is its interaction with RSO catalog data. Existing catalogs feature tens of thousands of objects with significant orbit uncertainties, and the catalog identification, positioning, refinement, and collision detection tasks require, e.g., solving intensive geometric optimization problems or extensive orbit propagation. Therefore, we may be able to use an FM's generalist, semantic prior to improve flexibility or reduce computation costs on board. However, satellite data is even further out-of-distribution than planetary rover imagery and existing FMs will likely require extensive training and fine-tuning to bridge the domain gap.\nTo address this we propose a three-pronged approach. First, a pre-trained open-source FM is selected as the basis, motivated by desired modalities (e.g. image sequences, catalog data) and capabilities. Besides LLaVA, we consider models such as ViNT [21] (tuned towards solving large-scale navigation problems) or Tool-LLM [18] (to facilitate interaction with algorithmic tools). Second, additional training is performed using space-specific datasets: remote sensing data, satellite telemetry, space object catalogs, orbit trajectories, spacecraft hardware databases, among others, to improve generalization to spaceborne data. Third, the model is fine-tuned for tasks of interest using labeled datasets. For FALCON, this includes generating high-fidelity spaceborne images with accompanying ground-truth RSO labels (and/or labeling real spaceflight images), and generating propagated orbits and ground-truth collision estimates from space catalog data."}, {"title": "VI. CONCLUSION", "content": "In this paper, we argue that future challenges in space robotics motivate the development of a space foundation model. We introduce a first step towards a space foundation model by automatically labeling the AI4Mars dataset with QA pairs to adapt a VLM to terrain classification and navigation tasks for a planetary rover on Mars. We demonstrate that 1) existing VLMs are deficient visual reasoners in space-based applications, and 2) fine-tuning a VLM on automatically labeled in-situ extrater-restrial data significantly improves the quality of responses even with a limited training dataset of only a few thousand samples. We also propose new applications of foundation models to satellite scenarios focusing on highly complex PNT, SSA, and collision avoidance tasks, infeasible using traditional onboard algorithms. Future work in the development of a foundation model for space will incorporate: 1) collecting a sufficiently large and diverse space dataset, e.g., remote sensing data, spaceflight simulations, and space object catalogues, for space-related tasks and 2) developing data encoders to process the diverse modalities (LiDAR, GPS, etc) inherent to these data in order to create a meaningful representation for decision making."}]}