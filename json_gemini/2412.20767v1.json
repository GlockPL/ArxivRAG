{"title": "KeyGS: A Keyframe-Centric Gaussian Splatting Method for Monocular Image Sequences", "authors": ["Keng-Wei Chang", "Zi-Ming Wang", "Shang-Hong Lai"], "abstract": "Reconstructing high-quality 3D models from sparse 2D images has garnered significant attention in computer vision. Recently, 3D Gaussian Splatting (3DGS) has gained prominence due to its explicit representation with efficient training speed and real-time rendering capabilities. However, existing methods still heavily depend on accurate camera poses for reconstruction. Although some recent approaches attempt to train 3DGS models without the Structure-from-Motion (SfM) preprocessing from monocular video datasets, these methods suffer from prolonged training times, making them impractical for many applications.\nIn this paper, we present an efficient framework that operates without any depth or matching model. Our approach initially uses SfM to quickly obtain rough camera poses within seconds, and then refines these poses by leveraging the dense representation in 3DGS. This framework effectively addresses the issue of long training times. Additionally, we integrate the densification process with joint refinement and propose a coarse-to-fine frequency-aware densification to reconstruct different levels of details. This approach prevents camera pose estimation from being trapped in local minima or drifting due to high-frequency signals. Our method significantly reduces training time from hours to minutes while achieving more accurate novel view synthesis and camera pose estimation compared to previous methods.", "sections": [{"title": "Introduction", "content": "In recent years, 3D photorealistic reconstruction has gained popularity, especially with differential rendering techniques (Kerbl et al. 2023; Mildenhall et al. 2021; Edavamadathil Sivaram, Li, and Ramamoorthi 2024; Wang et al. 2021a; Xu et al. 2022). These methods use a novel approach, representing the 3D model as a differentiable volume field or a traditional representation, and optimize it through a differential rendering pipeline, leading to exceptionally high-quality reconstructions.\nNotable representations include Neural Radiance Fields (NeRF) (Mildenhall et al. 2021) and the recently popular 3D Gaussian Splatting (3DGS) (Kerbl et al. 2023). Both methods use volume rendering (Tagliasacchi and Mildenhall 2022), but they differ significantly in their approaches.\nNeRF employs ray-marching, which leads to slow inference due to the high computational demands of sampling along rays and feeding data to an MLP. In contrast, 3DGS uses differential rasterization without an MLP, enabling real-time inference speeds.\nIn many NeRF and 3DGS reconstruction pipelines, a common approach involves using software like COLMAP (Schonberger and Frahm 2016) for Structure from Motion (SfM) to estimate camera poses. SfM extracts SIFT features from images, applies the RANSAC algorithm for pose estimation, and performs bundle adjustment for refinement. However, this method often struggles under extreme conditions, such as noisy images, textureless regions, low resolution, or varying lighting, leading to inaccurate pose estimates. Moreover, SfM becomes computationally expensive with an increasing number of images due to the complexity of RANSAC and pairwise bundle adjustment. To overcome these challenges, some researchers focus on refining camera poses during 3D reconstruction or on methods that avoid SfM altogether.\nTo further refine camera pose estimates in SfM, various approaches (Lin et al. 2021; Chen, Chiu, and Liu 2024; Jeong et al. 2021; Fu et al. 2023; Liu et al. 2024; Heo et al. 2023; Park et al. 2023; Chen et al. 2023; Shi et al. 2022; Meuleman et al. 2023; Bian et al. 2023; Wang et al. 2021b; Lin et al. 2023; Sucar et al. 2021; Yan et al. 2024; Zhu et al. 2022) have been proposed to jointly refine camera poses, either starting from noisy initial poses or without any initial pose information. Bundle-Adjusting Neural Radiance Fields (BARF)(Lin et al. 2021) is the first NeRF method to use dense photometric signals for alignment. It also introduces a heuristic coarse-to-fine strategy that progressively increases the signal frequency to effectively refine camera poses. Joint TensoRF(Chen, Chiu, and Liu 2024) provides theoretical analysis indicating that image alignment may encounter gradient oscillation with high-frequency signals. To address this issue, it employs Gaussian filter to reduce frequency and utilizes a grid-based NeRF, TensoRF (Chen et al. 2022) as their representation. There are also more extreme approaches that completely avoid using any initial camera pose. Nope-NeRF(Bian et al. 2023) considers neighboring sequences and uses a monocular depth estimation model like DPT (Ranftl, Bochkovskiy, and Koltun 2021) to minimize reprojection error. Recently, with the emer-"}, {"title": "Related Work", "content": "Recent advancements in novel view synthesis have been notably propelled by the development of differential rendering. NeRF uses an MLP to model both the geometry and view-dependent appearance of scenes. By optimizing through ray marching with volume rendering (Tagliasacchi and Mildenhall 2022) techniques, NeRF achieves impressive rendering quality.\nHowever, NeRF can be very inefficient because it requires a large number of sample points to be fed into a deep MLP, using shared weights to represent the entire scene. To improve efficiency, some works (M\u00fcller et al. 2022; Chen et al. 2022; Yu et al. 2021; Sun, Sun, and Chen 2022) utilize spatial data structures, such as grids or octrees, to optimize specific learnable features stored in these structures within 3D space. Despite these enhancements, the computationally expensive process of ray marching remains a limitation for wider applications, as it prevents real-time rendering.\nTo overcome these challenges, 3DGS introduces a differential rasterization approach that represents 3D models as a set of Gaussian spheres and uses volume rendering to blend colors projected from these 3D Gaussians. This shift in the rendering pipeline results in higher inference speeds and a more explicit representation, garnering significant attention across various research domains.\nEven methods like NeRF and 3DGS typically require preprocessing to obtain accurate camera poses, as these recon-"}, {"title": "Joint Refinement", "content": "Although COLMAP can provide nearly accurate camera poses, it can still fail under certain conditions such as noisy images, a limited number of images, or reflective surfaces. To address these issues, several models have been developed to jointly refine camera poses and improve 3D reconstruction accuracy.\nBARF (Lin et al. 2021) is among the pioneering models that establish a link between camera pose estimation and 3D reconstruction. While NeRFmm (Wang et al. 2021b) refines intrinsic and extrinsic parameters using camera pose embeddings, NoPeNeRF (Bian et al. 2023) reconstructs image sequences by leveraging a depth estimator DPT (Ranftl, Bochkovskiy, and Koltun 2021) to obtain pseudo-depth information and jointly refines camera poses with reprojection error from neighboring images as regularization.\nSimilarly, CF3DGS (Fu et al. 2023) also utilizes a depth estimator DPT to initialize the 3D point cloud, which is crucial step in 3DGS. However, CF3DGS(Fu et al. 2023) fixes the camera poses once they are registered, which can lead to accumulated errors in pose estimation, as shown in Figure 1. Additionally, we observe that the error accumulation problem can cause CF3DGS(Fu et al. 2023) to excessively split the Gaussian spheres, resulting in increased memory usage and potential crashes during training due to memory limitations."}, {"title": "Coarse-to-Fine Strategy", "content": "BARF (Lin et al. 2021) found that naive joint refinement fails to recover accurate poses due to difficulties in alignment caused by high-frequency components. Instead, they employ a coarse-to-fine strategy to progressively reveal the frequency components of positional encoding in vanilla NeRF (Mildenhall et al. 2021), which enables effective joint refinement of camera poses and scene reconstruction.\nAlthough BARF (Lin et al. 2021) experimentally validates the effectiveness of the coarse-to-fine strategy, it is limited to vanilla NeRF (Mildenhall et al. 2021). Joint TensoRF (Chen, Chiu, and Liu 2024) is a NeRF model that focuses on joint refinement by utilizing grid-based NeRF (Chen et al. 2022) for reconstruction. Previous works (Heo et al. 2023; Liu et al. 2024) have shown that grid features can be unsuitable for joint refinement of camera poses due to discretization and gradient oscillations. However, Joint TensoRF (Chen, Chiu, and Liu 2024) addresses this issue by providing general theoretical analysis and solution of joint refinement. They use a 1D signal $f_{gt}$, as an example. To"}, {"title": "Proposed Method", "content": "Given a sequence of images ${I_i}_{i=1}^N$, our goal is to efficiently recover camera poses ${T_i}_{i=1}^N$ and generate a photorealistic 3D scene, simultaneously. We propose KeyGS, a framework designed to quickly refine initial rough camera poses. It then jointly reconstructs the scene and refines the camera poses using 3DGS along with our coarse-to-fine frequency-aware densification technique."}, {"title": "Preliminary: 3D Gaussian Splatting", "content": "3DGS (Kerbl et al. 2023) represents a scene using explicit 3D Gaussians, unlike NeRF's implicit representation. It uses rasterization to project the 3D Gaussians onto the image plane and applies volume rendering to blend colors. This approach is efficient for inference due to its active projection and the absence of MLP involvement.\n$G(x) = e^{-(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}$\nHowever, 3DGS requires appropriate initial conditions, often utilizing a point cloud estimated by SfM with position $\\mu \\in \\mathbb{R}^3$, color $c \\in \\mathbb{R}^3$ (parameterized by spherical harmonics), and initialized with low opacity $o \\in \\mathbb{R}^1$. In order to represent the 3D Gaussians in Equation 2, it employs KNN for estimating the covariance matrix $\\Sigma \\in \\mathbb{R}^{3\\times3}$. During optimization, $\\Sigma$ is decomposed into a rotation $R \\in \\mathbb{R}^{3\\times3}$ and a scale $S \\in \\mathbb{R}^{3\\times3}$ to ensure it remains positive semi-definite, as shown in Equation 3.\n$\\Sigma = R S S^T R^T$\nTo represent the scene using 3D Gaussians, rasterization is employed to render them as images. Given the camera"}, {"title": "KeyGS Framework", "content": "To address the efficiency issues related to the time cost of feature matching and bundle adjustment in the SfM problem, we use the sequential mode of COLMAP instead of the exhaustive mode to match features within a small neighborhood of images. This approach reduces the time complexity from $O(n^4)$ to $O(n^2)$, significantly lowering the computational cost of SfM. Furthermore, since the cost is highly dependent on the number of images, we leverage the sequential structure of the image set by uniformly subsampling of the images in the sequence and applying sequential SfM to"}, {"title": "Joint Refinement and Densification", "content": "To refine the camera pose $T_i \\in SE(3)$ for each image $I_i$, we train 3DGS with a learnable SE(3) transformation $\\Delta T_i$ to obtain the estimated pose $T_{pred} = \\Delta T_i T_i$, Specifically, we do not parameterize $\\Delta T_i$ using $se(3)$; instead, we use $so(3) \\times t(3)$ to reduce the influence between rotation and translation. The image is rendered using rasterization from 3DGS, which depends on the predicted camera pose $T_{pred}$ and the optimized Gaussians G. We minimize the photometric loss function $L_{rgb}$, which integrates $L_1$ and D-SSIM losses, balanced by a factor $\\lambda$, as detailed in Equation 7. The attributes of the Gaussians and the pose refinements $\\Delta T_i$ are updated through backpropagation.\n$L_{rgb} = (1 - \\lambda)L_1 + \\lambda L_{D-SSIM}$\nAnother important factor in joint refinement is the densification process. In 3DGS, Gaussians are split if the gradient\n$\\frac{\\partial L}{\\partial \\mu_{2D}}$, with respect to their position $\\mu_{2D}$, exceeds a certain threshold, and they are culled if their opacity, $o$, falls below a specified threshold. Both the refinement and densification processes are primarily influenced by the gradient term $\\frac{\\partial G}{\\partial \\frac{\\partial L}{\\partial \\mu_{2D}}}$, as described in Equation 8. Notably, only the 2D splat affects these processes.\n$\\frac{\\partial L}{\\partial \\Delta T} = \\frac{\\partial L}{\\partial G} \\frac{\\partial G}{\\partial \\mu_{2D}} \\frac{\\partial \\mu_{2D}}{\\partial \\Sigma_{2D}} \\frac{\\partial \\Sigma_{2D}}{\\partial \\Delta T}$\nAs shown in Figure 2 (c), high-frequency signals can lead to misplaced Gaussians, causing excessive splitting due to larger, oscillating gradients. Excessive splitting results in more misaligned Gaussians and an increases probability of trapping in local minima due to overfitting with incorrect poses, which complicate the optimization process and creating a vicious cycle. We depict an example for visualization in Figure 3."}, {"title": "Coarse To Fine Frequency-Aware Densificaion", "content": "To address gradient oscillation and excessive splitting issues, we propose a coarse-to-fine frequency-aware densification. First, we visualize the kernel $H(u, k)$, as mentioned in related work, with varying scales $\\sigma$ of Gaussian"}, {"title": "Regularization", "content": "In Figure 3, we observe that Gaussians tend to fit high-frequency signals, resulting in a spiky appearance. Similar to PhysGaussian (Xie et al. 2024), we apply anisotropy regularization, as described in Equation 11, to control the ratio between the maximum axis max(Si) and minimum axis min(S) of the Gaussians, preventing them from becoming spiky. Here, r represents the minimum ratio. Furthermore, to prevent Gaussians from being trapped in local minima due to gradient oscillations, we draw inspiration from AbsGS (Ye et al. 2024) and use the absolute gradient | $\\frac{\\partial L}{\\partial \\mu_{2D}}$ | to encourage Gaussian splitting, allowing them to search for a better"}, {"title": "Experiments", "content": "In this section, we compare our approach with existing joint refinement models: BARF (Lin et al. 2021), CF3DGS (Fu et al. 2023), Nope-NeRF (Bian et al. 2023), and SC-NeRF (Jeong et al. 2021), using the Tanks and Temples (Knapitsch et al. 2017) and CO3DV2 (Reizenstein et al. 2021) datasets. We also conduct an ablation study to highlight key components of our method. Moreover, we show that our method outperforms 3DGS (Kerbl et al. 2023), even when it uses camera pose estimates from COLMAP (Schonberger and Frahm 2016), which is often regarded as ground truth to evaluate the effectiveness of pose estimation. Additional experimental results are provided in the supplementary material."}, {"title": "Data Preprocessing", "content": "COLMAP is the most commonly used tool for registering camera poses with SfM. The major computational cost arises from bundle adjustment due to the large number of images and feature points, as described in (Schonberger and Frahm 2016). We analyze the average computation time for different keyframe subsampling intervals and various downsampling resolutions using all images, as shown in Figure 5. Both options offer significant speedups compared to"}, {"title": "Ablation Study", "content": "First, we highlight the most significant component of our strategy, as shown in Table 4. The results demonstrate that the coarse-to-fine frequency-densification is the core component of our method. Additionally, both the absolute gradient and anisotropy regularization further improve the performance of our approach."}, {"title": "Conclusion", "content": "In this paper, we presented KeyGS, an efficient framework for joint refinement of camera poses and novel view synthesis for monocular image sequences. We analyzed the relationship between densification and joint refinement and proposed the coarse-to-fine frequency-aware densification approach to address gradient oscillation from high-frequency signals. Our approach significantly outperforms previous methods with more accurate novel view synthesis and camera pose estimation as well as drastically reduced training times."}]}