{"title": "Ontological Relations from Word Embeddings", "authors": ["Mathieu d'Aquin", "Emmanuel Nauer"], "abstract": "It has been reliably shown that the similarity of word embeddings ob-tained from popular neural models such as BERT approximates effectivelya form of semantic similarity of the meaning of those words. It is there-fore natural to wonder if those embeddings contain enough information tobe able to connect those meanings through ontological relationships suchas the one of subsumption. If so, large knowledge models could be builtthat are capable of semantically relating terms based on the informationencapsulated in word embeddings produced by pre-trained models, withimplications not only for ontologies (ontology matching, ontology evolu-tion, etc.) but also on the ability to integrate ontological knowledge inneural models. In this paper, we test how embeddings produced by sev-eral pre-trained models can be used to predict relations existing betweenclasses and properties of popular upper-level and general ontologies. Weshow that even a simple feed-forward architecture on top of those em-beddings can achieve promising accuracies, with varying generalisationabilities depending on the input data. To achieve that, we produce adataset that can be used to further enhance those models, opening newpossibilities for applications integrating knowledge from web ontologies.", "sections": [{"title": "Introduction", "content": "Word embeddings [4] are vector representations of words in a text, used, in par-ticular, to perform learning tasks in natural language processing. Many of theembeddings used today are produced by neural (large) language models such asBERT [2]. These are neural network models trained on particular tasks (such asmasked-language modelling, i.e. predicting masked tokens in a given text) andfrom which selected hidden layers can be used as embeddings. Interesting prop-erties have been demonstrated for embeddings produced by different models,in particular, in representing the meaning of the words within the embeddingspace. For example, words that are semantically similar have been shown togenerally have similar embeddings [12]. The main question that is asked inthis paper is whether they encapsulate enough information to cover relations"}, {"title": "Related work", "content": "A lot of attention has been given recently to the whole area of knowledge graphembeddings (see, for example, [13]) in particular for their use in link predic-tion tasks (see, for example, [21]). Knowledge graph embeddings are methods(often based on neural models) to project the graph representation availablein knowledge graphs onto a vector space in a way that should align with thestructure and meaning of the graph. They are often used for link prediction, i.e.the task of predicting which entity might be related to which other in a graphsince, by encapsulating patterns in the existing graph, they should be able todiscover where missing relations might exist. These two categories of work areclosely related to the work presented in this paper since they relate to the pre-diction of relations between entities in knowledge structures through the useof embeddings. Here, however, we focus on ontological relations as predictablefrom word embeddings applied to the textual representation of the entities. Inother words, while knowledge graph embeddings, in a sense, distill the contentof a graph to find ways to complete it, we aim to exploit knowledge alreadycaptured by pre-trained language models through word embeddings to predictontological relations between classes and properties of web ontologies.\nFollowing the work on knowledge graph embeddings, ontology embeddingshave been proposed that focus on the OWL language [1] or on particular de-scription logics [8]. While ontological relations are considered, those works aimto create representations similar to knowledge graph embeddings but that arecapable of capturing the semantics of higher-level formalisms used for ontolo-gies. They therefore also accomplish a different task from the one endeavoured"}, {"title": "Overview", "content": "Figure 1 provides an overview of the approach taken to predict ontological rela-tions that might exist between two entities, represented as texts by their shortname (the last part of their IRI) and comments in English (if they have one).The texts of the short name and of the comment of each entity are first runthrough a language model (in Section 4.3, we describe the four we tested) toobtain word embedding vectors for each of them, i.e. a word embedding is com-"}, {"title": "Dataset generation", "content": "In this section, we detail how datasets are created for the training and validationof relation prediction models. We start by describing the ontologies used and theprocess of extracting relations from those ontologies. We then briefly introducethe language models used to extract embeddings of the textual representation ofentities, and finally what is included in the generated datasets. The generateddatasets and intermediary structures computed to construct them are availableon FigShare\u00b9 and the code to generate them from an N-Triples file containingan ontology is available on github.\u00b2"}, {"title": "Ontologies", "content": "As a basis for training and validation, we selected five ontologies. The reason forusing those five is that, considering that the language models used were trainedon open-domain texts, they should be better able to predict the relations existingin ontologies that are not specific to a particular domain. In other words,they rely, in the textual representation of the included entities, on a general"}, {"title": "Extracting relations from ontologies", "content": "The first part of the process of creating a dataset of relations between entitiescontained in the considered ontologies is to extract such relations, as well asthose that can be derived from them. Some of the ontologies considered arerelatively large (including a few thousand classes and properties), and relationsbetween potentially every pair of entities had to be considered. To minimise thetime to search the relations between two entities, each ontology is represented bya large nxn matrix where n is the number of entities, and each cell of the matrix"}, {"title": "Pre-trained Language Models for Word Embedding", "content": "In the process presented in Section 3, we rely on a selection of popular languagemodels to test which are more efficient in re-creating ontological relations. Allthose language models are pre-trained by their original authors and built ondifferent architectures, although they are all neural models based on transform-ers, as summarised below. There are a number of other language models thatcould have been used and could be added in the future, but those represent areasonable selection of what is openly available today.\nTo extract embedding vectors for textual representations of classes and prop-erties of ontologies with these models, we used the huggingface transformerslibrary in Python. In more detail, for every entity included in an index for anontology, we first run the tokenized shortname of the entity through the model,obtaining an \"in context\" embedding vector for each of the words in the short-name by extracting the activations of the last layer before output (last hiddenstates). We reduce this set of vectors to one by averaging them (the mean-pooling step in Figure 1). We apply the same process to the rdfs:comment ofthe entity if it has one, and average the name and comment vectors to obtain afinal embedding vector of the textual representation of the entity. As a result,our process leads to a dataset including an embedding vector from each of thefour language models for every class and property in each of the five ontologiesconsidered here."}, {"title": "Training", "content": "As shown in Figure 1, each model adds to the concatenation of the embeddingvectors produced for a pair of entities a few (one to three) fully connected hid-den layers with reLU activation and an output layer (of size 20) with sigmoidactivation. The decision on the number of hidden layers and their sizes is madefor each model empirically: Several values have been tried to identify some thatseem to consistently perform better than others. Other parameters, such as thenumber of epochs of training, the learning rate, or the batch size, are estab-lished by following the same approach. All the parameters used for trainingare recorded in our code repository on github. The results below were ob-tained using relatively small models on top of the embeddings used, the largest(Schema.org/Llama2) containing three hidden layers of sizes 100 each, and thesmallest (DBpedia/ROBERTA) containing only one layer of size 15.\nAll models were trained using the PyTorch Library with the Adam optimiserand the cross-entropy loss function applied to the 20-sized vector in output ofthe sigmoid layer, against the binary vector representing the actual relationsbetween the input pair of entities."}, {"title": "Learning individual ontologies", "content": "Table 2 provides the results in terms of overall precision, recall, and F-score foreach of the 20 trained models. To clarify here, those measures are consideredon a per-relation basis, that is, if a relation exists in the ontology between apair of entities and the model produces a number over 0.5 for the dimension ofthe output vector corresponding to that relation, then a true positive will becounted. If, however, the model outputs a number below the threshold of 0.5for that relation, then a false negative is counted (similarly for true negativesand false positives).\nThe firstconclusion which can be drawn is that the results confirm that, to an extent,the tested pre-trained language models include sufficient information in theirrepresentation of texts to be able to recognise ontological relations betweenclasses and properties, the best results obtained being Llama2 on DUL for anF-score of more than 88%. This is an interesting result in itself, as it showsthat, even without much effort in training, using only a few, small additionallayers on top of the produced embeddings, a fairly accurate reconstruction of asignificant part of some of the ontologies can be achieved."}, {"title": "Cross testing ontology models", "content": "In Figure 2, we present the results, in terms of precision, recall, and F-score, ofcross-validating models built on the training sets of each of the ontologies, onthe validation sets of each of the ontologies. Here, we rely on the five modelsbuilt using Llama2, as the best performing language model for our task. Thediagonals in the three matrices of Figure 2 therefore correspond to the resultsalready presented in the last column of Table 2.\nThe first conclusion here is that, once again without surprise, a model trainedon a part of an ontology is better able to predict another part of the sameontology, rather than a part of another ontology. Beyond this obvious statement,however, we can also see that the models based on the two upper-level ontologiesare not only the ones obtaining the best results on their ontology, they alsogeneralise fairly well to predicting each other. A more surprising result is that,however, even though it generally reaches very low performances, the modeltrained on OpenVocab is not much worse at predicting gUFO and DUL thanit is at predicting OpenVocab itself. A possible explanation for this is thatthe low quality of OpenVocab might not be as much the incorrectness of therelations it expresses as its incompleteness. Finally, another interesting aspect ofthese results is the observation that even though it achieves good performanceon schema.org itself, the model trained on this ontology performs extremely"}, {"title": "Building and testing a global model", "content": "As a last experiment, we trained a \"global\" model on a combined training setfrom the five ontologies and tested it using the five validation sets, again relyinghere on the Llama2 language model. The goal is to compare the performance ofsuch a general model, trained on a larger and more diverse set of relations, withthe results obtained above for more specific models, trained on smaller amountsof data. The results, in terms of precision, recall, and F-score, on the combinedvalidation set and on each validation set individually, are presented in Table 3."}, {"title": "Discussion: possible applications", "content": "The results presented in the previous section show a promising new way inwhich semantic web tools and applications could effectively exploit web knowl-"}, {"title": "Conclusion", "content": "In this paper, we report on experiments to build neural models to predict on-tological relations (direct or inferred) between classes and properties from wordembeddings. We showed that even very simple models built on top of suchembembeddings for the textual representation of those entities obtained promisingresults. We also showed that even if the results were often similar, the largerLlama2 model was consistently better as a source of embeddings in this taskthan other smaller models. We also discussed how the results were largely de-pendent on the quality of the ontology(ies) on which the model was trained, withcarefully designed, upper-level ontologies leading to excellent results where un-validated, community built ontologies led to disappointing model performances.\nBased on the promising results obtained, we discussed possible applications"}, {"title": "Supplementary material", "content": "At\nhttps://figshare.com/articles/dataset/Data_and_models_for_\nOntological_relations_from_word_embeddings_/25601010?file=\n45645084 is the FigShare data repository that includes the built modelsand the measures obtained from their validation. It also includes the generateddatasets used as input to the training and validation steps and the intermediarystructures built as part of constituting those datasets (indexes and matrices).\nThe ontologies themselves are not included, but a metadata file indicates fromwhere they were downloaded, at what time."}]}