{"title": "BIOLOGY INSTRUCTIONS: A DATASET AND BENCHMARK FOR MULTI-OMICS SEQUENCE UNDERSTANDING CAPABILITY OF LARGE LANGUAGE MODELS", "authors": ["Haonan He", "Yuchen Ren", "Yining Tang", "Ziyang Xu", "Junxian Li", "Minghao Yang", "Di Zhang", "Dong Yuan", "Tao Chen", "Shufei Zhang", "Yuqiang Li", "Nanqing Dong", "Wanli Ouyang", "Dongzhan Zhou", "Peng Ye"], "abstract": "Large language models have already demonstrated their formidable capabilities in general domains, ushering in a revolutionary transformation. However, exploring and exploiting the extensive knowledge of these models to comprehend multi-omics biology remains underexplored. To fill this research gap, we first introduce Biology-Instructions, the first large-scale multi-omics biological sequences-related instruction-tuning dataset including DNA, RNA, proteins, and multi-molecules, designed to bridge the gap between large language models (LLMs) and complex biological sequences-related tasks. This dataset can enhance the versatility of LLMs by integrating diverse biological sequenced-based prediction tasks with advanced reasoning capabilities, while maintaining conversational fluency. Additionally, we reveal significant performance limitations in even state-of-the-art LLMs on biological sequence-related multi-omics tasks without specialized pre-training and instruction-tuning. We further develop a strong baseline called ChatMultiOmics with a novel three-stage training pipeline, demonstrating the powerful ability to understand biology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics are publicly available and crucial resources for enabling more effective integration of LLMs with multi-omics sequence analysis.", "sections": [{"title": "INTRODUCTION", "content": "Understanding the complex activities across various omics in living organisms is of paramount importance. This includes studying DNA regulatory elements that control gene expression (Emilsson et al., 2008), RNA regulation (Mattick, 2004) that influences protein synthesis, and the functional properties of proteins themselves (Marcotte et al., 1999). These molecular processes critically affect the development of diseases and the synthesis of drugs within organisms. Recent BERT-like encoder-only models (Devlin, 2018) have achieved significant advances in natural language understanding tasks. When applied to genome or protein understanding tasks, these models (Zhou et al., 2023; Rives et al., 2021) are capable of capturing complex intrinsic relationships within biological sequences, achieving high accuracy in tasks such as promoter prediction. However, their reliance on specific classification or regression heads to predict a single task at a time limits their versatility, and their repeated fine-tuning sessions with different prediction heads to address multiple tasks further complicate the training, inference, and deployment process.\nIn contrast, powerful general-purpose large language models (LLMs) such as GPT-4 (Achiam et al., 2023) and Gemini (Achiam et al., 2023; Team et al., 2023) based on vast amounts of natural language tasks and data that encompass the general knowledge system of humanity, have shown substantial potential in domain-specific tasks. These decoder-only models approach every task as a completion task through next-token prediction, and offer an alternative by integrating various biological sequence-related tasks using natural language as an intermediary while retaining conversational capabilities. Therefore, utilizing LLMs combined with unified training and dataset construction techniques can make it possible to replace BERT-like models with the complicated fine-tuning pipeline."}, {"title": "RELATED WORKS", "content": "In recent years, LLMs have demonstrated significant advancements in the field of natural language processing (NLP). These models undergo self-supervised training on a substantial corpus of data in order to acquire knowledge. By means of fine-tuning the instructions, the capabilities of the model are enhanced, enabling it to respond to questions based on the specific prompt. Currently, numerous open-source models are available, including the Llama series (Dubey et al., 2024), Qwen series (Bai et al., 2023), GLM series (GLM et al., 2024), and numerous models fine-tuned based on Llama, such as Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023). Additionally, Galactica (Taylor et al., 2022) is a model that demonstrates exceptional performance in scientific domains and is trained on data from a multitude of scientific fields. Furthermore, there are closed-source SOTA models, such as GPT-40 and GPT-40-mini. However, these models are not pre-trained on specific biological data, and their capabilities are severely constrained, even Galactica."}, {"title": "BIOLOGY LARGE LANGUAGE MODELS", "content": "Researchers have concentrated on enhancing the capabilities of LLMs in the biology area. Instruct-Protein (Wang et al., 2023) aligns human and protein language through knowledge instructions. Another study (Fang et al., 2023) utilizes the protein part of a specially designed dataset called Mol-Instructions for instrution tuning with LLaMA-7B. ProLLaMA (Lv et al., 2024) is also a recent work focusing on multi protein tasks through a two-stage traing process from LLaMA-2. These methods can only deal with several protein tasks well, limited by fixed instruction tem-plates. BioMedGPT (Zhang et al., 2023) is equiped with special vision encoder, allowing the model to answer multi-modal biological questions. However, lack of specialized large-scale bi-ological instruction datasets, BioMedGPT cannot understand biological sequence languages very well. ChatNT (Richard et al., 2024) integrates a biological sequence encoder with a LLM, enabling effective handling of DNA-centric tasks using only an instruction-tuning dataset. However, it faces"}, {"title": "BIOLOGY-INSTRUCTIONS", "content": "To build a large-scale biology instruction-following dataset, we have gathered biology sequence data from a substantial aggregation of sources. This effort has resulted in a dataset encompassing 21 sub-tasks related to multi-omics fields. The Biology-Instructions exhibits the following characteristics:\nBiology-Instructions comprises 21 subtasks across three types of omics, including single-omics tasks and multi-omics interaction tasks. Joint training of different omics not only enhances efficiency by accomplishing multiple omics tasks with a single model but also improves the model's capability in a specific omics domain.\nWith over 3 million training samples, the Biology-Instructions dataset provides an ex-tensive foundation for biological sequences-related instruction data. This large-scale dataset enables models to better understand the traits and functions of biological sequences, leading to more accurate and comprehensive responses to given questions.\nTo ensure the quality of the dataset, we manually draft question and answer templates for each task type and expand the template pool using Cluade-3.5-sunnet and GPT-40. The resulting number of question-answer template pairs for each task range from 10,000 to 100,000, depending on the data magnitude of each task type. Throughout this process, we emphasize the importance of diversity in grammar and language style, ensuring that samples in the Biology-Instructions dataset have different question-answer style. For examples of question-answer template pairs, please refer to Table 10.\nAlthough previous studies (Richard et al., 2024; Liu et al., 2024b; Lv et al., 2024) have demonstrated large-scale primary instruction-following datasets can teach LLMs to answer bi-ological sequences-related questions, they often fail to fully harness the powerful language abilities of LLMs, as they focus primarily on basic language patterns. In other words, they failed to lever-age the powerful conversational abilities of these models to form natural and fluent dialogues, and further utilize reasoning to enhance the validity of the output results. To address this limitation, we design a prompt that requires powerful closed-source LLMs to reformulate answers for a subset of Biology-Instructions' validation set and provide polished answers ready for end-users to read and"}, {"title": "TASKS", "content": "As presented in Figure 2, the Biology-Instructions dataset comprises 21 tasks: 6 DNA tasks, 6 RNA tasks, 5 protein tasks, and 4 multi-molecule tasks. When considering the number of input sequences, there are 4 multi-molecule interaction tasks and 17 single-molecule tasks. Tasks were sourced from high-impact literature, journals, and competitions, ensuring coverage of biologically critical aspects in structure, function, and engineering across DNA, RNA, proteins, and their interactions. We fo-cus on predictive sequence-understanding tasks, leaving generative applications, such as sequence design, for future research. To the best of our knowledge, Biology-Instructions is the first instruc-tion dataset to include multi-omics tasks and multi-molecule interaction tasks. For detailed task definitions and distribution, please refer to Appendix A.2."}, {"title": "TEMPLATES", "content": "To convert the original classification and regression task dataset into an instruction tuning dataset, we employ question-answer templates to integrate the data. The primary objective of creating these templates is to teach the model how to follow biological instructions and complete tasks without overfitting to specific language patterns. To achieve this, we prioritize diversity in language styles, tones and lengths during the template construction process. We manually constructed 10 question templates and 10 answer templates for each task, covering various styles including, but not limited to, request, concise, informal, and academic styles. Then, we used GPT-40 and Claude-3.5-sunnet to expand the templates. Depending on the data volume for each task, we included 100 to 300 question templates and 100 to 300 answer templates. Ultimately, each task resulted in 10,000 to 100,000 question-answer template pairs. Since biological sequences are generally much lengthier than natural language prompts, we place the biological sequence at the very beginning of question templates for single biology sequence tasks for non-interaction tasks. This approach helps prevent the prompts from being overwhelmed by the lengthy biological sequences, ensuring that the model can accurately understand the question and complete the task. Figure 3 provides examples of the instruction prompts constructed for each type of omics, illustrating the diversity and structure of the templates used in the dataset."}, {"title": "REASONING DATA CONSTRUCTION", "content": "Similar to the data construction method used by LlaVA (Liu et al., 2024a). For a biology sequence X, and its related question Xq, simple answer Ys, we prompt GPT-40-Mini to construct optimized answer Yo base on the given information. Generally, the instruction data were transformed to the format USER: X5, X\u2084 ASSISTANT:Yo.\nIn the system prompt used for GPT-40-Mini, as shown in Figure 10, we emphasized the following key points to ensure the production of high-quality data: (1) first understand the provided biological sequence and the question; (2) analyze the biological sequence at the nucleotide or amino acid level, aiming to extract question-related information from the sequence; (3) refine the answer based on the previous analysis, including a rational explanation and a chain of thought approach, especially for complex questions; (4) list any relevant knowledge and information from reliable sources, and cite these sources appropriately; (5) return the polished answer in an end-to-end style, excluding any information from the standard answer and task hint. By following this approach, we gathered 8000 final AI-polished training data points without two multi-molecule tasks: antibody-antigen neutral-ization and RNA-protein interaction prediction to study transfer learning for reasoning capability. Figure 4 provides an overview of the complete construction process for Biology-Instructions, in-cluding the data collection, template construction, and reasoning data construction stages."}, {"title": "EVALUATION PIPELINE AND METRICS", "content": "Our evaluation framework is designed to assess the performance of each model's output across the diverse set of tasks included in Biology-Instructions in a robust approach. The task types, regardless of their respective omics, can be organized into single-label regression, multi-label regression, binary classification, multi-class classification, and multi-label classification, each requiring specialized evaluation metrics to capture model performance nuances. The evaluation pipeline involves pre-processing data from models' output, grouping entries by task, and then computing task-specific metrics. The metrics outcomes for reporting are all scaled by 100 and rounded to 2 decimals for enhanced readability. For detailed information on specific metrics, please refer to Appendix A.3."}, {"title": "MODEL", "content": "As shown in Figure 5, we train a model based on Llama3.1-8B-Instruct (Dubey et al., 2024) named ChatMultiOmics using multi-omics pre-training data and Biology-Instructions. In general, we per-form a three stages training paradigm to enhance the interactive biological sequence-related chat performance of the final biology assistant. For specific training details, please refer to Appendix B."}, {"title": "STAGE 1: BIOLOGICAL SEQUENCES CONTINUED PRE-TRAINING", "content": "Although the memory savings facilitated by LoRA (Devalal & Karthikeyan, 2018) are not that ob-vious when optimizer states are distributed across GPUs compared with training on single GPU, LORA can still significantly reduce training time by minimizing communication between data paral-lel ranks. However, directly applying LoRA to train a chat model on Biology-Instructions results in suboptimal performance on specific downstream tasks. Specifically, the model shows near-random performance in classification and regression tasks. As noted by (Ghosh et al.), LoRA supervised fine-tuning (SFT) primarily leverages pre-trained knowledge to generate well-formed answers based on the output format learned from SFT data. We suspect that large-scale LoRA instruction tuning on biological sequence-related data suffers due to the lack of pre-training on biological sequence data, which is evident from the baseline results. Therefore, continued pre-training of the model is essential for better performance. This involves teaching the model with biological sequences to enable it to understand the nature and functions of biological sequences. For this process, we utilized unlabeled human DNA data from the Genome Reference Consortium Human genome (GRCh) (Harrow et al., 2012), human non-coding RNA data from RNACentral (rna, 2019), and protein sequences from UniRef50 (Suzek et al., 2007) during the first phase of pre-training. This initial pre-training served as a foundational warm-up to improve the model's comprehension across multi-omics biological sequences.\nWe employed LoRA+ (Hayou et al., 2024) for all linear layers of our model, training on a con-tinued pre-training dataset. LoRA+ demonstrates superior convergence compared to vanilla LORA by increasing the learning rate of the zero-initialized weight B relative to the base learning rate for normal-initialized weight A and other trainable parameters. (Hayou et al., 2024) observed that setting the learning rate of weight B to 16 times that of weight A results in more effective model con-vergence. However, our experiments revealed that while LoRA+ indeed improves convergence rates, applying a large learning rate multiplier can lead to instability during the continued pre-training pro-cess for biological sequences. Based on this observation, we opted for a more conservative learning rate multiplier of 4. We trained the normalization layers of the model alongside LoRA parameters."}, {"title": "STAGE 2: MASSIVE INSTRUCTION TUNING", "content": "In Stage 2, we employ the Biology-Instructions dataset, excluding the reasoning sub-dataset. In the initial attempts of training, we find that the imbalance among tasks within the dataset can pose challenges for the model in distinguishing between different tasks. To mitigate this, we randomly select 30 percent of the training data and prepend a task label in the format"}, {"title": "STAGE 3: REASONING INSTRUCTION TUNING", "content": "In stage 3, we use reasoning sub-dataset from Biology-Instructions to fine-tune the model. To keep the classification and regression performance of the model, we additionally select 3000 samples from validation set composed of non-reasoning data to be trained simultaneously.\nTo better control the behavior of the model, a more detail system prompt Psd was used for rea-soning data: \"You are a highly knowledgeable AI assistant specializing in biology, particularly in sequence-related topics. Your primary task is to provide clear, accurate, and comprehensive answers to biology questions. When analyzing and interpreting sequences, ensure to provide step-by-step explanations to make your responses natural and easy to understand. Engage with the user by ask-ing clarifying questions if needed and offer detailed insights into the biological sequences.\" In this case, the format of training sample of reasoning data is transformed to SYSTEM:Psd USER:Xs, Xq ASSISTANT:Yo."}, {"title": "RESULTS", "content": "To evaluate the biological sequence understanding capabilities of current LLMs and determine if our method can enhance LLMs performance, we compare ChatMultiOmics with various open-source general-purpose LLMs: Llama3.1-8B-Instruct (Dubey et al., 2024), Llama2-7B-Chat (Touvron et al., 2023), Alpaca-7B (Taori et al., 2023), Vicuna-v1.5-7B (Chiang et al., 2023), Qwen2-7B (Bai et al., 2023), GLM4-9B-Chat (GLM et al., 2024), and Galactica-1.3b (Taylor et al., 2022). Additionally, we include comparisons with SOTA closed-source LLMs: GPT-40 and GPT-40-Mini We also evaluate biology-specialized LLMs: InstructProtein-1.3B (Wang et al., 2023), Llama-molinst-protein-7B (Fang et al., 2023), and BioMedGPT-LM-7B (Zhang et al., 2023). To ensure well-formed and quantifiable answers, we restrict the output format for all baselines and provide them with task information, enabling them to understand both what to output and how to format their output. The experimental results are visualized in Figure 6, showcasing the comparative performance of various LLMs across four types of datasets: DNA, RNA, protein, and multi-molecule interactions. For the full experimental results, please refer to Appendix C."}, {"title": "FINDINGS.1: GENERAL PURPOSE LLMS ARE NOT CAPABLE OF BIOLIGICAL SEQUENCES UNDERSTANDING", "content": "To assess whether LLMs can effectively tackle tasks related to biological sequences, we conducted comprehensive experiments using both open-source and closed-source general-purpose LLMs. For open-source LLMs, we selected models of comparable size to our model, ChatMultiOmics. For closed-source LLMs, we evaluated SOTA models such as GPT-40 and its streamlined version, GPT-40-mini.The results unequivocally demonstrate that all open-source LLMs of similar size to Chat-MultiOmics fail to surpass average performance levels. Similarly, the closed-source LLMs, GPT-40 and GPT-40-mini, exhibit performance on par with the open-source models.\nNotably, models within the same series but with different versions, such as Llama2-7B-Chat and Llama3.1-8B-Instruct, as well as models within the same series but of different sizes, like GPT-4o and GPT-40-mini, show comparable performance on tasks involving biological sequences.These findings suggest that the language capabilities of these models do not directly correlate with their performance in understanding biological sequences. This implies that natural language performance does not determine the effectiveness of these models in biological sequence understanding tasks, indicating a significant lack of pre-trained biological sequences knowledge. Despite LLMs possess-ing extensive text-based biological knowledge, they struggle to establish a connection between this knowledge and biological sequences, and they are unable to delve into the molecular level to analyze biological sequences effectively."}, {"title": "FINDINGS. 2: CURRENT BIOLOGY-SPECIFIED LLMS CAN NOT HANDLE MULTI-OMICS TASKS", "content": "Biology-specified LLMs have demonstrated remarkable performance on a variety of reported tasks. For instance, the Llama-molinst-protein-7B model excels in five key areas of protein understanding, including the prediction of catalytic activity, protein design, protein function prediction, and more. Despite these impressive achievements, these methods exhibit limitations. Notably, they lack trans-fer learning capabilities across multi-omics tasks and fail to outperform general-purpose baselines even in single-omics tasks and in some cases these models even can not follow the input instructions. This indicates that while specialized LLMs are highly effective within their specific domains, their applicability and efficiency in broader, more integrative biological studies remain constrained."}, {"title": "FINDINGS.3: CONTINUED PRE-TRAINED ON BIOLOGICAL SEQUENCES HELPS INSTRUCTION TUNING", "content": "Previous studies have utilized LoRA (Fang et al., 2023; Lv et al., 2024) for model training. However, our experimental findings suggest that employing LoRA to fine-tune models on Biology-Instructions does not result in performance enhancements. For LoRA fine-tuning, the quality and quantity of the pre-training on related knowledge appears to be a critical factor for achieving good results, as indirectly proved by the experimental setup in (Fang et al., 2023), where full fine-tuning was applied to protein-related tasks and LoRA fine-tuning was used for other tasks, alongside the near-random performance of the baselines on biological-sequences understanding tasks. After continued pre-"}, {"title": "FINDING.4: REASONING DATA BOOST OVERALL PERFORMANCE AND DEMONSTRATE TRANSFER LEARNING CAPCABILITY", "content": "We hypothesize that the model's performance can be enhanced by incorporating text-form task in-formation and reasoning steps, which can aid the model in better understanding the task and conse-quently lead to improved results. We tested the third-stage model using the system prompt Psc to facilitate results computation. The results indicate that in most tasks, performance was enhanced in the third stage. However, for some regression tasks, the performance was slightly adversely affected by the third-stage training.\nFurthermore, when the reasoning system prompt Psd was used, the model demonstrated excellent reasoning capabilities and extended its performance to untrained tasks, such as antibody-antigen neutralization and RNA-protein interaction prediction, as illustrated in Figure 1 (b)."}, {"title": "DISCUSSION", "content": "In this work, we present Biology-Instructions, the first large-scale, multi-omics bio-logical sequences-related instruction-tuning dataset. Biology-Instructions bridges the gap between LLMs and complex biological tasks by including 21 different tasks involving DNA, RNA, pro-teins, and multi-molecule interactions, covering both single-sequence and interaction analyses. By incorporating reasoning capabilities, Biology-Instructions make LLMs versatile in handling com-plex biological tasks while maintaining conversational fluency. Our evaluation shows that SOTA LLMs, like GPT-4, struggle with biological sequence-related tasks without specialized training. Us-ing Biology-Instructions for instruction tuning, we demonstrate significant improvements, proving its value in enhancing LLMs for multi-omics sequence analysis. We also develop a strong baseline, ChatMultiOmics, with a three-stage training pipeline: biological sequences continued pre-training, massive instruction tuning, and reasoning instruction tuning. This pipeline leads to notable perfor-mance gains, providing an effective approach to train LLMs for addressing biological challenges."}, {"title": "Limitations and Future Work", "content": "While Biology-Instructions is a significant advancement, it still has areas for improvement. The dataset covers primarily the predictive tasks. Future version should include generative tasks, such as designing novel protein sequences, which could greatly enhance its utility in protein engineering. ChatMultiOmics shows promising reasoning capabilities, yet further enhancements are needed to make its outputs more practical and reliable. To enhance model perfor-mance, we could use hybrid architectures that combine specialized biological tokenizers or encoders with LLMs. This could reduce information loss during the tokenization of biological sequences.\nIntegrating structural data, such as 3D molecular coordinates, could improve the model's ability to capture functional implications of molecular structures. Incorporating multi-hop data could be another potential enhancement for the model to reason over interconnected biological datasets and capture more intricate relationships across multiple omics layers. Future efforts should also expand evaluation metrics beyond accuracy to include interpretability, robustness, and computational"}, {"title": "DETAIL INFORMATION OF BIOLOGY-INSTRUCTIONS AND EVALUATION METRICS", "content": "The Biology-Instructions dataset addresses critical challenges in computational biology across mul-tiple omics domains. DNA instructions improve our understanding of regulatory elements in gene expression. RNA instructions tasks offer insights into transcriptomics and regulation at the RNA level. Protein instructions enhance our knowledge of protein functions, interactions, and their rele-vance in drug development. Multi-molecular instructions explore biomolecular interactions, such as RNA-protein and promoter-enhancer, revealing regulatory networks. By supporting these diverse tasks, Biology-Instructions advances multi-omics research and fosters new discoveries in genetic regulation and therapeutic development."}, {"title": "TASKS DEFINITION", "content": "As presented in Figure 2, the Biology-Instructions dataset comprises 21 tasks: 6 DNA tasks, 6 RNA tasks, 5 protein tasks, and 4 multi-molecule tasks. When considering the number of input sequences, there are 4 multi-molecule interaction tasks and 17 single-molecule tasks. Tasks were sourced from high-impact literature, journals, and competitions, ensuring coverage of biologically critical aspects in structure, function, and engineering across DNA, RNA, proteins, and their interactions. We fo-cus on predictive sequence-understanding tasks, leaving generative applications, such as sequence design, for future research. To the best of our knowledge, Biology-Instructions is the first instruc-tion dataset to include multi-omics tasks and multi-molecule interaction tasks. For detailed task definitions and distribution, please refer to Appendix A.2."}, {"title": "DNA TASKS", "content": "This is a binary classification task that predicts whether a DNA se-quence has chemical modifications affecting gene regulation without changing the DNA itself. Epi-genetic marks are crucial for understanding gene regulation and its impact on health and disease. We use part of the DNABERT-2 dataset (Zhou et al., 2024), containing 28,740 DNA sequences, some of which are chemically modified. Model performance is evaluated using the Matthews Correlation Coefficient (MCC).\nThis is a regression task that predicts the activity levels of enhancer regions in the DNA sequences. By predicting the activity levels of enhancers, scientists can gain deeper insights into how genes are regulated in specific tissues or under certain conditions. The target value are"}, {"title": "RNA TASKS", "content": "This is a regression task which predicts the usage of alternative polyadeny-lation (APA) isoforms by analyzing RNA sequences and outputting a proportion between 0 and 1 that represents the relative expression of each APA isoform. Accurate APA isoform prediction is critical for understanding the regulation of gene expression at the RNA level, which plays a funda-mental role in transcriptome diversity. For this task, we adopt APARENT's (Bogard et al., 2019) APA isoform prediction dataset, which consists of isoform usage data derived from synthetic and human 3'UTRs. The output represents the proportion of isoform usage, capturing the variability in polyadenylation signal processing. The performance of the prediction is evaluated using the Coeffi-cient of Determination (R2).\nThis is a multi-label classification task that predicts the functional class of non-coding RNA (ncRNA) sequences. The model outputs one or more class labels from a set of 13 possible ncRNA classes, such as 'tRNA', 'miRNA', and 'riboswitch'. Accu-rately classifying ncRNAs is essential for improving our understanding of their regulatory roles in gene expression, as well as their contributions to diverse biological processes and diseases. For this task, we adopt the nRC (non-coding RNA Classifier) dataset from (Fiannaca et al., 2017), which utilizes features derived from ncRNA secondary structures. The output assigns each RNA sequence to one or more functional classes, enabling a detailed examination of the functional diversity within ncRNAs. The performance of the model is evaluated using accuracy (Acc), reflecting the model's ability to correctly classify ncRNA functions across all categories.\nThis is a multi-label classification task that predicts post-transcriptional RNA modifications from RNA sequences. The model outputs one or more modification types from a set of 12 widely occurring RNA modifications, including 'm6A', 'm1A', and 'm5C'. Precise iden-tification of RNA modification sites is essential for understanding the regulatory mechanisms of RNA and their roles in various biological processes. For this task, we adopt the MultiRM dataset from (Song et al., 2021), which contains RNA sequences annotated with multiple modification types."}, {"title": "PROTEIN TASKS", "content": "Number Prediction. This is a multi-label classification task which predicts enzyme functions by annotating protein sequences with all corresponding EC numbers. We adopt DeepFRI's (Gligorijevi\u0107 et al., 2021) EC annotation dataset from PDB chains, whose binary"}, {"title": "MULTI-MOLECULE TASKS", "content": "This is a binary classification task, the objective of which is to identify interactions between non-coding RNAs (ncRNAs) and proteins, based on the sequences of the aforementioned ncRNAs and proteins. The majority of ncRNAs interact with proteins to perform their biological functions. Consequently, inferring the interactions between ncRNAs and proteins can facilitate the comprehension of the potential mechanisms underlying biological activities involving ncRNAs (Li et al., 2016). The dataset employed in this study was derived from (Han & Zhang, 2023), comprising 14,994 samples. The evaluation metric employed was MCC.\nThis is a binary classification task, which seeks to ascertain whether a corre-sponding interaction relationship exists based on the sequences of antibodies and antigens. The objective of this task is to ascertain the correspondence between antigens and antibodies and to pre-dict more effective antibody characteristics for new variants of viruses. The dataset was sourced"}, {"title": "EVALUATION METRICS", "content": "This type of task involves predicting one continuous numerical value. The evaluation process extracts the numeric values from model outputs using regular expressions, avoid-ing over- and underflow by limiting values to six significant digits. Metrics computed for regression tasks include:\nMeasures the monotonic relationship between predicted and true values according to their ranks. The metric value ranges from -1 to 1, where -1 indicates perfect negative correlation, 0 indicates no correlation (random predictions) and 1 indicates perfect positive correlation.\nObtained by squaring the Pearson correlation coefficient to reflect the proportion of variance in the dependent variable ex-plained by the independent variable. The metric value ranges from 0 to 1, where 1 indicates perfect prediction and 0 indicates predictions as good as the mean value (randomness).\nA custom metric (SAIS, 2020) balances regression error and classification accuracy by integrating F1 score (harmonic mean of precision and recall), Mean Absolute Error (MAE), and range-based MAE (MAE computed within a range threshold). Calculation details will be further explained in A.3.1.\nThis type of task involves predicting multiple continuous output for each input. In the EA prediction task, two numeric values are required for the regression values of 'House-keeping EA' and 'Developmental EA'. In the programmable RNA switches prediction task, three numeric values are required for predicting the regression values of 'ON', 'OFF', and 'ON/OFF'.\nAssesses the linear correlation between two sets of data. The metric value ranges from -1 to 1, where -1 indicates perfect negative linear correlation, O indi-cates no linear correlation (random predictions), and 1 indicates positive linear correlation.\nComputes individual R2 for each label and take the mean across labels to obtain an average R2 as the overall performance metric. The metrics values shares the same range and inter-pretations similar to the single-label R2.\nThis type of task asks the model to predict one of two possible classes. In our case, either positive or negative. The evaluation pipeline involves first classifying via keywords based on the presence of predefined positive or negative keywords. If keywords classification fails, the pre-trained sentiment analysis model Twitter-roBERTa-base will be utilized as fallback to determine the class based on the sentiment polarity assigned with a higher probability score.\nProvides a balanced measure for binary clas-sifications, even when classes are imbalanced. The metric ranges from 1 to 1, where -1 indicates perfect inverse correlation, 0 indicates random predictions or no correlation, and 1 indicates perfect postive correlation.\nCalculates the proportion of correct predictions out of all pre-dictions made. It ranges from 0 to 1, where 0 indicates no correct predictions, 1 indicates all correct predictions and 0.5 as random predictions.\nThis type of task asks the model to assign each input to one of sev-eral classes. In the non-coding RNA family prediction task, the model is required to predict one from 13 classes. Accuracy Score: Calculates the proportion of correct predictions out of all predic-tions made. It ranges from 0 to 1, where 0 indicates no correct predictions, 1 indicates all correct predictions and 0.5 as random predictions.\nThis type of task involves inputs that may belongs to multiple classes and asks the model to predict all of them. The evaluation process includes first extracting all relevant labels from the model outputs and converting them into binary multi-hot vectors representing the presence or absence of each class.\nMeasures the model's ability to distinguish between classes across all shredsholds. The metrics ranges from 0 to 1, where 1 indicates perfect ability to distinguish classes and 0.5 as random performance.\nRepresents the maximum F1 score over all possible thresholds, providing a balanced measure of precision and recall in multi-label settings. The metric ranges from 0 to 1, where 0 indicates worst balance of no correct predictions and 1 indicates perfect balance between precision and recall."}, {"title": "MIXED SCORE CALCULATION", "content": "The Mixed Score is a custom metric adopted from (SAIS", "components": "the F1 score", "below": "nThis measures the average magnitude of prediction errors across all samples", "as": "nMAE = 1/n \u03a3|yi-\u0177i|", "100": ".", "low remaining\" range is of significant importance in practical applications. Following (SAIS, 2020), we define this range as [0, 30": "."}, {"as": "nRange-MAE = 1/m \u03a3|yj \u2013\u0177j|", "100": ".", "30": "precision and recall are calculated for predictions falling within this interval"}, {"as": "nF1 = 2*Precision*Recall/ Precision + Recall\nThe final Mixed Score integrates these three components to provide a balanced assessment of re-gression and classification performance. The formula for the Mixed Score is:\nMixed Score = 50% \u00b7 (1 \u2013 MAE/100)"}]}