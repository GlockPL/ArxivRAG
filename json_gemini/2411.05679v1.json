{"title": "Tell What You Hear From What You See - Video to Audio Generation Through Text", "authors": ["Xiulong Liu", "Kun Su", "Eli Shlizerman"], "abstract": "The content of visual and audio scenes is multi-faceted such that a video stream can be paired with various audio streams and vice-versa. Thereby, in video-to-audio generation task, it is imperative to introduce steering approaches for controlling the generated audio. While Video-to-Audio generation is a well-established generative task, existing methods lack such controllability. In this work, we propose VATT, a multi-modal generative framework that takes a video and an optional text prompt as input, and generates audio and optional textual description (caption) of the audio. Such a framework has two unique advantages: i) Video-to-Audio generation process can be refined and controlled via text which complements the context of the visual information, and ii) The model can suggest what audio to generate for the video by generating audio captions. VATT consists of two key modules: VATT Converter, which is an LLM that has been fine-tuned for instructions and includes a projection layer that maps video features to the LLM vector space, and VATT Audio, a bi-directional transformer that generates audio tokens from visual frames and from optional text prompt using iterative parallel decoding. The audio tokens and the text prompt are used by a pretrained neural codec to convert them into a waveform. Our experiments show that when VATT is compared to existing video-to-audio generation methods in objective metrics, such as VGGSound audio-visual dataset, it achieves competitive performance when the audio caption is not provided. When the audio caption is provided as a prompt, VATT achieves even more refined performance (with lowest KLD score of 1.41). Furthermore, subjective studies asking participants to choose the most compatible generated audio for a given silent video, show that VATT Audio has been chosen on average as a preferred generated audio than the audio generated by existing methods. VATT enables controllable video-to-audio generation through text as well as suggesting text prompts for videos through audio captions, unlocking novel applications such as text-guided video-to-audio generation and video-to-audio captioning.", "sections": [{"title": "Introduction", "content": "The combination of human perception and cognition represents a \u201cmulti-modal\" way of processing and interpreting scenes. For example, when we are presented with a silent video of a fountain show attended by a crowd of people gathered around the spectacle our interpretation might translate the visual scene into an auditory experience, where the visuals are semantically processed and transformed into a corresponding sound narrative in our mind. Thus, we may associate audio that mixes sounds of splashing water accompanied by people talking and laughing with possibly background music in sync with the fountain.\nAs generative AI continues to progress, the incorporation of the aforementioned aspects into generative platforms presents itself as the future desirable capability. In particular, the goal of an ideal video-to-audio generative model would be to generate sounds that seamlessly match the video temporally and fully capture the semantics. Moreover, it is desirable to control such a generation process towards the themes and sounds that match user preference. Recent state-of-the-art approaches have adopted two types of generative modeling techniques: auto-regressive token-based modeling and diffusion-based modeling. These methods enable end-to-end video-to-audio generation and are applicable across a wide variety of video and audio categories. However, while these methods are capable of capturing the general semantics of sound sources in videos, they often overlook the subtleties of the context. For example, in a video depicting two cats in a territorial dispute, the model might produce a calm, amiable meowing sound, which contradicts the contentious nature of the scene. This discrepancy mainly stems from the limitations of the vision encoder, which struggles to distinguish between varying sound properties emitted by identical sound sources across different contexts, due to an incomplete understanding of the entire scene. Second, these methods lack controllability since the generation is conditioned only on visual frames, without taking into account the context and interpretation of the sounds. While text-to-audio models could explicitly control the context of the sounds, such models are based on text only without incorporating the rich and dynamic context of visuals, which could significantly inform video and audio alignment. Indeed, text only generative outcomes often result in unmatched audio with the visual (e.g., temporal misalignment or semantic loss).\nTo solve the above challenges, we propose a novel framework, Video-to-Audio Through Text (VATT), that is able to generate audio from both video frames and an optional text prompt describing the expected sounds. VATT consists of two modeling stages: i) Video-to-Caption stage, which converts video features into an audio caption through a pretrained large language model (LLM) with a learnable projection layer. Through this cross-modal conversion, visual features that are relevant to audio concepts are extracted. These features are closely connected to audio-related tasks such as audio captioning and audio generation. ii) Video + Text to Audio stage, that generates audio conditioned on the hidden states extracted from the LLM in the prior modeling stage. At its core, the proposed model in this stage is a bi-directional transformer decoder that generates audio using a token-based representation similar to [1, 2]. To obtain the conditioning on the hidden states of the preceding component, the projected video features along with the optional text prompts are concatenated together and fed into the LLM in stage i), with the hidden states from the last layer extracted and attached to the audio tokens for the decoder. The decoder is trained using masked token modeling, where the objective is to predict masked audio tokens from unmasked ones at varying masking ratios. During inference, starting from all tokens being masked, an efficient parallel decoding algorithm is implemented which gradually unmasks multiple tokens in parallel based on video and text inputs until a stop condition is met. Finally, the generated tokens are converted into audio waveforms through a neural audio codec decoder."}, {"title": "Related Works", "content": ""}, {"title": "Visual-to-Audio Generation", "content": "Visual-to-Audio Generation task has drawn significant attention since generative frameworks such as diffusion and transformer-based architectures have been developed. Existing Visual-to-Audio generation approaches can be divided into two branches of studies based on audio categories: visual-to-music generation and visual-to-natural sound generation. In visual-to-music generation domain, earlier studies explored Midi or spectrogram generation from human body movements by studying the temporal and semantics alignment [6\u201310]. More recently, diffusion-based methods have been proposed to generate music waveforms directly from videos [11]. In visual-to-natural sound generation, earlier efforts pioneered the generation of sounds linked to various objects and materials [12]. Later works proposed an audio generation approach based on SampleRNN [13, 14] that could generate several types of natural sounds from in-the-wild videos. While these approaches showcase promising results, they are often limited to specific audio categories. Neural codec [15\u201318] and autoregressive transformer architectures [19, 20] addressed these limitations and as they have evolved, generative models now effectively generalize across a broader range of sounds or music, leveraging compressed latent spaces [21\u201324]. Similar advances have been shown with diffusion techniques such as [25, 26]. However, these methods often lack detailed sound control and their inference time turns out to be consuming. Our work aims to address these limitations by introducing a text-guided framework to improve controllability and efficiency in video-to-audio generation. While there are several concurrent works that aim to achieve partially similar goals to our proposed method [27-29], our work is different since it is designed to achieve these capabilities within a single unified framework."}, {"title": "Text-to-Audio Generation", "content": "As an alternative to the generation of audio from video, text can be used as an input to guide audio generation. When text is the input, audio generation becomes more controllable semantically. Existing approaches such as Make-An-Audio [30], AudioLDM [31], AudioLDM-2 [32] and others [33, 34, 34, 35] enable general text-to-audio (or music) generation by adapting latent diffusion techniques, first developed in [36]. Concurrently, methods such as AudioGen [37], MusicGen [38], AudioLM [39], MusicLM [40], SoundStorm [2], VampNet [41] leverage transformer-based architectures and token-based modeling techniques to produce audio tokens, that are then decoded into waveforms using"}, {"title": "Multi-modal Large Language Models", "content": "Multi-modal Large Language Models (MLLMs), have been able to attain significant progress. With the advent of open source, pretrained and instruction-tuned LLMs such as LLama [43], Alpaca [44], Vicuna [45]. In particular, when extending these LLMs into MLLMs, a pretrained modality-specific encoder extracts the features and then a projection layer maps these features into vectors of the same dimension as text embeddings of the corresponding LLM. This approach led to developments in visual LLMs [46, 47], audio LLMs [5, 48], audio-visual LLMs [49] and showed improvement in multi-modal understanding tasks such as captioning [50] and question-answering [51, 52]. Recent efforts have also focused on tasks such as multi-modal retrieval [53], multi-modal embodied navigation [54, 55], leveraging LLM's strong reasoning capabilities to interpret or improve the results. In terms of generation, several works [56, 57] aimed at achieving any-to-any modality generation using LLMs as a central medium. While these methods have been successful in general modality-to-modality generation, they do not achieve particular end-to-end video-to-audio generation, with or without text guidance, which is the unique direction our work focuses on."}, {"title": "Methods", "content": "VATT is a flexible vision-to-audio generative framework that can process both visual and textual inputs and generate both audio waveforms and captions of audio. To achieve this, VATT consists of two modeling stages: i) Video-to-Caption : This stage utilizes a Large Language Model (LLM) with a learnable projection layer that converts video features into embeddings compatible with the LLM. The model receives an instruction to generate audio captions from video inputs. ii) Video + Text to Audio: This stage incorporates an encoder-decoder architecture. The encoder uses the finetuned LLM from Video-to-Caption stage with frozen weights. The decoder is a bi-directional transformer trained to generate audio tokens using masked token modeling techniques in training. The training pipeline of VATT system is shown in Figure 2. During inference, VATT generates audio tokens from video and optional text prompts through iterative parallel decoding. These tokens are then converted into audio waveforms using Encodec [17]."}, {"title": "Video-to-Caption Stage", "content": "VATT Converter is designed to integrate visual and textual prompts for audio generation as well as audio captioning. The core component, VATT Projector, is an embedding layer that maps video features into the text embedding space of the LLM. Given visual features extracted from frame-level vision encoders Vf = {v1, v2, ..., vT }, a Linear layer is applied to project each feature from its original dimension dv to the LLM text embedding dimension dlm, producing a sequence of transformed features Vim = VfWl + bl, where Wl and bl are learnable parameters of the linear projection."}, {"title": "V2A Instruction Tuning:", "content": "The key functionality of VATT Converter is to extract from visual stream semantic features relevant to audio. Drawn on the success of multi-modal LLMs, such as visual-LLM [46] and audio-LLM [5], we employ multi-modal instruction tuning to align the visual inputs of videos with the ground truth audio captioning of the same videos. Given a prompt instruction, Ti = {ti1, ti2, ..., tiK}, such as \u201cDescribe the audio that the video could generate:\" and the projected visual features Vim as inputs, we model conditional distribution of audio descriptions Ta = {ta1, ta2,..., taN }, as Po(Ta|Ti, Vim) by fine-tuning an instruction-tuned LLM, e.g., Vicuna-7B [45]. Unlike typical instruction-tuning that maps a signal into textual concepts within the same modality, our method bridges the concepts from visual to audio modality, unifying the representation for text-guided video-to-audio generation task that we describe in section 3.2. For training efficiency,\""}, {"title": "Video + Text to Audio Stage", "content": "Once the audio-related visual features are aligned with the text features in the LLM embedding space, the LLM effectively encodes multi-modal information that serves as a representation for text generation and audio generation. Indeed, in the second stage of VATT, there are two generation modes to generate audio: i) When no conditional text prompt is provided, the video features along with a standard template prompt (e.g., \u201cDescribe possible audio that the video could infer.\u201d) are fed as inputs to VATT Converter. ii) When an audio caption is provided as the text prompt, the video features and the audio caption are fed together into VATT Converter. In such a case, the provided audio caption helps guide the video-to-audio generation process and overrides the need for generated audio caption."}, {"title": "Audio Token Decoder", "content": "To generate audio, we design an audio token-based decoder, VATT Audio, conditioned on the encoded features from VATT Converter. In contrast to existing methods, which typically use auto-regressive token modeling [37, 38, 23] or latent diffusion techniques [31, 32], we adopt a novel token-based modeling technique based on masking tokens. The method, originally derived in image generation tasks [1] and recently adapted to text-to-audio generation [2, 41], is capable of achieving competitive generation quality while improving efficiency through an iterative parallel decoding algorithm during inference."}, {"title": "Token-based Representation for Audio", "content": "To represent audio waveforms using discrete tokens, we adopt a pretrained audio neural codec, Encodec [17], similarly to FoleyGen [23]. Encodec is a multi-level residual vector-quantized (RVQ) autoencoder trained with waveform reconstruction and"}, {"title": "Masked Audio Token Generative Modeling", "content": "We model the distribution of audio token matrix Atok \u2208 \u211dL\u00d7Tc by developing a token masking strategy which learns the joint distribution of the audio tokens in full parallelism. This is different than using \u201cdelayed patterns\u201d proposed in [38] which enables parallelism but only on the level of codebook dimension. At each time step of Atok, embedding vectors of L tokens are summed up to represent audio waveform at the corresponding segment. In order to perform masking operation at any position, we introduce an additional learnable <MASK> token in each codebook. By randomly replacing some of the tokens entries in the Atok with <MASK> at corresponding codebook we obtain the masked audio token matrix AMAtok \u2208 \u211dL\u00d7Tc. We obtain EM \u2208 \u211ddem\u00d7Tc by summation of the embedding vectors of each token in AMAtok along the level axis."}, {"title": "Conditional generative modeling", "content": "Conditional generative modeling is implemented as follows. We extract the hidden states of the last layer Hlm \u2208 \u211ddlm\u00d7Tlm (before the LLM prediction head) from VATT Converter as the conditional inputs into the audio token decoder. We use a linear layer to project Hlm to Elm \u2208 \u211ddem\u00d7Tlm with same feature dimension as the masked audio embeddings EM. A straightforward way to model the relationship between EM and Elm is to use an interleaving self-attention and cross-attention block as proposed in Vanilla Transformer architecture [59]. However, we find that such interleaved interaction between audio and multi-modal input condition does not capture the fine-grained correspondence between them. Therefore, we propose to use a bi-directional self-attention architecture to fuse the features.\nSpecifically, we concatenate Elm with EM along the temporal axis to obtain the fused features Emm = Concat([Elm, EM]). The decoder consists of Lmm layers of self-attention blocks, as shown in Fig. 3. The output hidden states in the last layer of the decoder, Hout = Dec(Emm), represent fused audio and conditions features. We only extract the part of the hidden states corresponding to the audio tokens, Hout \u2208 \u211ddmm\u00d7Tc, and pass it through L Linear layers in parallel to perform classification on masked tokens at each level of the codebooks. For each masked audio token in matrix AMatok, we calculate the cross-entropy loss between the predicted token \u0302atok and the ground truth token atok, formulated as\n LVATT = - \u2211atok\u2208AMAtok I(atok = <MASK>) log [P(\u0302atok = atok|AMAtok; Him)], (2)\nwhere \ud835\udf03 is the set of trainable parameters in the audio token decoder, and \ud835\udd40 is the indicator function."}, {"title": "Masking Design and Iterative Parallel Decoding", "content": ""}, {"title": "Masking Distribution Design", "content": "Inspired by [1, 2], we incorporate variable random masking. In particular, it was shown that masking ratio plays an important role in audio token decoder to generate meaningful signals. While in [1, 2] arc-cosine masking distributions is used by default, here we study several masking strategies that include distributions along with different hyper-parameters to find the strategy that reaches more optimal generation quality (see Appendix A for further details). Our study shows that normal distribution with a mean of 0.75 and standard deviation of 0.25, truncated from 0.5 to 1.0 is such optimal strategy. The general interpretation of this strategy is that a relatively high range masking ratio enables models to generate better initial tokens when most of the entries in the token matrix are masked. This is essential for future decoding steps to generate meaningful tokens."}, {"title": "Iterative Parallel Decoding", "content": "Scheduling of masking plays a key role as well. During inference, we follow the cosine scheduling scheme proposed in [1] to gradually resolve the audio tokens. The iterative sampling procedure starts with all <MASK> in the audio token matrix. At a step t, the model takes the audio token matrix At\u22121 from the previous step along with the conditions as inputs and samples a new audio token matrix \u00c2t in parallel with all tokens unmasked. Based on the confidence at each entry of \u00c2t only tokens with top-k confidence are kept while the remaining entries are re-filled with <MASK>, resulting in At. The cosine scheduling scheme determines the ratio of re-masked tokens by rt = cos (\u03c0 \u00b7 t/T). Notably, to resolve the confidence of each entry in the matrix, we adopt the \u201cgumbel-top-k trick\u201d [60] with temperature that varies, i.e., Ct = log(pi) + G, where G ~ Gumbel(0, 1) and pi denotes the output probability of the sampled token at the entry i. This is equivalent to sampling k values from multinomial distribution from the softmax probabilities without replacement. The temperature \u03c4 controls the degree of stochasticity. We use \u03c4 = \u03c40 \u00b7 (1 \u2212 t/T ) with linear decay during generation, where \u03c40 is the initial temperature. Similarly to [1, 2], our method achieves optimal quality and fast speed within a few decoding steps (typically 10 - 20)."}, {"title": "Experiments", "content": "Datasets: We use common benchmarks datasets VGGSound [3] and AudioSet-2M [4] for training and evaluation. VGGSound is a large-scale audio-visual dataset sourced from YouTube, containing 192k videos from 309 audio-visual categories, with 177k / 15k train-test video splits. AudioSet-2M is a larger audio-visual database with around 2M YouTube videos, with only 1.6M available online. In Stage 1, we train VATT Converter with both datasets and test on VGGSound only. In Stage 2, for fair comparison against existing video-to-audio generation methods, we train and evaluate on VGGSound dataset only.\nTo train VATT with text, we synthesize a large-scale audio caption dataset, \u201cV2A Instruction", "### Instruction: Close-ended question: Write an audio caption describing the sound. ### Response:\". For AudioSet [4": "and VGGSound [3] we generate a single audio caption per each video for a total of 1.77M videos.\nTo ensure the quality of captions, we first manually verified the validity of LTU-generated captions prior to using them as synthetic ground-truth (GT) and then performed an experiment to further evaluate captions quality. In particular, we randomly selected 100 videos from VGGSound test set with stratified sampling according to video categories to conduct a human study. We used 1-5 point MOS (Mean-Opinion-Score) scale (the higher the better) to measure correctness of the captions. We provide pairs of videos and the corresponding captions to the raters, asking \u201cHow accurately the provided caption reflects the sound events happening in the video? 1. Inaccurate and irrelevant. 2. Relevant but inaccurate with many mistakes. 3. Partially accurate but missing details and with mistakes. 4. Mostly accurate with some minor mistakes. 5. Accurate and complete.", "Details": "For visual inputs, we use eva-CLIP [61] image encoder to extract mean-pooled visual features from video frames at 5fps rate, which result in 50 \u00d7 768 visual sequence for a 10s video. To represent audio, we extract audio tokens from a pretrained Encodec-16kHz. For each 10s audio waveform, we represent it with Atok \u2208 \u21154\u00d7500 token matrix."}, {"title": "Evaluation Metrics:", "content": "To evaluate video-to-audio generation quality, we follow the method of [23], which proposed the metrics Kullback-Leibler-Divergence (KLD) with PassT [63], Fr\u00e9chet Audio Distance (FAD) [64] and Align Accuracy (Align Acc) [25]. KLD measures how closely the generated audio matches the GT through pairwise comparison, reflecting how well the audio captures the concepts in the video. FAD evaluates the overall distribution, indicating the overall quality of the audio. Align Acc assesses the relevance and temporal alignment of the audio and the video. Additionally, we incorporate generation speed (time taken per waveform sample) to measure efficiency. We also compute the CLAP score [65] to evaluate the adherence of generated audio to text prompts to compare our results with text-to-audio generation. Further details of these metrics are described in Appendix F.\nFor video-to-audio captioning, we use two types of metrics, natural language generation (NLG) metrics and audio-text relevance metric. NLG metrics evaluate the generated captions with respect to the ground truth audio captions using rule-based matching in terms of precision and recall. These metrics include BertScore [66], BLEU-4 [67], ROUGE-L [68] and CIDEr [69]. To assess the relevance of generated audio captions with the actual audio, we compute the CLAP-score [65] as cosine similarity between audio and text embeddings."}, {"title": "Quantitative Evaluation of Audio Generation:", "content": "We evaluate audio generation of VATT models on the VGGSound test split. For each of the 15,446 video samples, we generate a 10-second audio waveform. We compare VATT variants against existing video-to-audio generation methods as well as text-to-audio generation methods including AudioLDM-2 [32] and AudioGen [37] using different text prompts. The results on the metrics described above are summarized in Table 1 and Table 2. VATT models achieve best KLD score and Align Acc against other methods while maintaining competitive FAD (top 2). Notably, when guided by GT audio captions (VATT-LLama-T and VATT-Gemma-T; bottom) our models generate sounds that match the GT audio more accurately, as indicated by lowest KLD score of 1.41 and 1.66 for VATT models with two LLM backbones, surpassing both video-to-audio and text-to-audio methods. In comparison to text-to-audio methods, VATT models achieve competitive audio-text alignment in terms of CLAP score, demonstrating a strong capability to follow text prompts. Implementation details of these baselines are included in Appendix E."}, {"title": "Quantitative Evaluation of Video-to-Audio Captioning:", "content": "We evaluate video-to-audio captioning by prompting VATT Converter to generate audio captions. We use the prompt \u201cDescribe the possible audio for this video:\" to generate captions for all VGGSound test videos. For baselines, we prompt LLAVA-13B-v1.5 model in two zero-shot modes to generate visual and audio descriptions respectively. Since LLAVA can take a single image as an input only, we select the middle frame of videos. We\""}, {"title": "Conclusion", "content": "In this work, we propose a multi-modal generative framework that enables both text-guided video-to-audio generation and video-to-audio captioning. Experiments show that our method can generate high quality audio through text in both unconditional and conditional modes, as well as to generate reasonable audio captions from videos. One area for improvement is the diversity of the text generated by current audio LLMs. In cases where the user-provided text prompts significantly differ in style there is a possibility for a conflict of audio quality and adherence to the instructions. Future work could enhance the capability of the model to generalize across different text styles and to further develop capabilities for informative iterative conversation-like video-to-audio generation."}, {"title": "Broader Impact", "content": "VATT could augment existing audio-video creation tools for content creators by allowing generation of custom audio tracks for given visual content through user provided text prompts. Also, VATT has the ability to suggest potential sounds for a given video which can inspire creators by presenting audio options that may not have been considered otherwise. This feature can be useful for brainstorming of content creation, where audio choices can influence the style of the final product.\nFurther extensions of this work could involve conversational video-to-audio generation such that the audio content is iteratively being refined. By integrating a conversational interface, the users can engage in a dialogue with the system, making requests and receiving responses. This approach goes beyond static text inputs, offering a more accessible toolset that does nto require significant audio editing expertise. Moreover, the conversational system can seek clarifications or propose alternatives, functioning like an assistant to avoid misunderstandings and enhance audio quality. More broadly, the generative approach proposed here has the potential to adapt to other generative areas not limited to audio, video, but also potentially impact fields such as biochemistry, physics where a generative approach is utilized, e.g., generative modeling of high-energy particle events [70, 71]."}, {"title": "Appendix", "content": "In this Appendix, we provide:\n\u2022 Additional Ablation Studies and Comparisons, see Appendix A.\n\u2022 Qualitative Examples and Analysis, see Appendix B.\n\u2022 Details and Examples of our synthetic \"V2A Instruction\" Dataset, see Appendix C.\n\u2022 Additional Implementation Details of VATT, see Appendix D.\n\u2022 Additional Implementation Details of baselines, see Appendix E.\n\u2022 Details of Evaluation Metrics, see Appendix F\n\u2022 Human Evaluation Details, see Appendix G"}, {"title": "Additional Ablation Studies and Comparisons", "content": ""}, {"title": "Masking Ratio Distribution Ablation:", "content": "Designing an appropriate varying masking ratio distribution for training is essential to achieve high audio quality and relevance. We study several commonly used masking ratio distributions, including Uniform distribution, Gaussian distribution and arc cosine distribution. For Gaussian distribution, we experiment with distributions with 4 different mean values, 0.55, 0.75, 0.95, and moving mean following a sine schedule with respect to the training epoch (similar to curriculum learning), in the range of [0.25, 0.95]. The standard deviation is kept fixed at 0.25. We use VATT Gemma-2B model for the masking ratio ablation study. As shown in Table 5, the model performs better when the distribution has a higher mean in masking ratio, especially arc cosine distribution and Gaussian mean with 0.75. This is due to the fact that the initial steps during the sampling stage are important for future decoding steps. The initial steps correspond to high masking ratio cases. For later steps, new tokens are unmasked conditioned on more clues such that the masking ratio decreases and the generation becomes less challenging, thus making the learning at lower masking ratio easier during training."}, {"title": "Self-prompting Ablation:", "content": "When additional text prompts are provided as inputs, we could use the audio captions generated by the VATT Converter as the text prompt to generate the audio. In this self-prompting mode, the generated audio could be interpreted by the same model in terms of the caption. As shown in Table 6, when our model is fed with corresponding generated captions, the model performs slightly worse than the model without prompt input, showing the space for improvement in the quality of generated captions. Also, generation of audio with the captions from VATT-Converter-LLama outperforms captions from VATT-Converter-Gemma, in particular evident from the FAD and KLD scores. As the caption quality improves, the text-conditioned video-to-audio generation performance also improves. The GT audio captions generated by LTU obtains the highest CLAP score (measured with respect to the GT audio in original video) of 0.379, reflecting the best caption quality. Feeding such GT captions as input to the model also leads to the best audio generation results."}, {"title": "Comparison with Text-to-Audio generation methods on AudioCaps:", "content": "We use VGGSound as our main dataset and benchmark to evaluate since it is a large-scale audio-visual dataset with around 200K videos across many categories, and also the quality of audio-visual alignment is high. To further show the generalization capability of VATT, we experiment with AudioCaps dataset. Due to limited video samples in AudioCaps, we finetuned our VGGSound pretrained VATT model on AudioCaps dataset in two settings, with and without text prompts. To keep the comparison fair, we use the GT audio captions from AudioCaps as the text prompts. We use VATT-LLama and VATT-LLama-T to compare against AudioGen and AudioLDM-2. As shown in Table 7, VATT-LLama-T performs on a similar level to AudioGen in terms of FAD and KLD score, while falling behind AudioLDM-2. It is noteworthy that the audio decoder of both AudioGen and AudioLDM-2 are pretrained on much larger data scale (7000 hrs and 30000 hrs audio respectively) than ours (700 hrs audio). Despite this, VATT still performs reasonably well on this dataset."}, {"title": "Qualitative Examples and Analysis", "content": ""}, {"title": "Controllable audio generation through text prompt:", "content": "A unique advantage of our model lies in its capability to control the generated details of audio through text prompts. We show a few samples where different text prompts are applied to the same video to generate different variations of sounds. As shown in Figure 5, our model is able to generate reasonable sounds that are distinct in their semantic meaning but fit both the context of the video and are aligned with the text description. The text prompts shown are all human-written prompts rather than synthetic ones."}, {"title": "VATT without prompt v.s VATT with ground truth audio captions:", "content": "We also compare generation using our model with GT audio captions as prompt versus generation without prompt to understand why the KLD score with prompt outperforms the generation without prompt by a large margin. Upon inspection of generated samples, as shown in Figure 6, we find that GT audio captions could steer the model generation towards the GT audio in the test set. For example, the first video shows a man performing with a rope, so the rope tapping sounds (when hitting the ground) should be heard in the video. However, it occurs that the model without prompt fails to capture this important detail, but instead generates noises from the surrounding crowd. Similar cases apply to the other two examples shown in the figure. KLD measures the pairwise difference between generated sounds and GT sounds in the feature space. Therefore, a low score means that the model closely matches the semantic meaning of the GT audio in the original video, which indicates that the model is able to follow the text prompt instruction to generate the desired audio."}, {"title": "Video-to-Audio Captioning:", "content": "In addition to controllable video-to-audio generation through text, VATT is also able to generate audio captions from videos, providing textual suggestions interpreting what sounds could a given video make. As shown in Figure 7, VATT could produce reasonable audio captions for videos across a variety of audio-visual categories, showcasing the capability of VATT Converter in capturing the audio relevant features from the video."}, {"title": "Details and Examples of V2A Instruction Dataset", "content": "We describe the synthesis procedure of our V2A Instruction dataset. To obtain audio captions for training and evaluating VATT, we use existing an existing audio large language model, LTU [5], which is pretrained on a large-scale audio understanding OpenAQA dataset, including audio from VGGSound and AudioSet-2M. LTU demonstrates strong capability in audio captioning in a zero-shot prompting manner, accurately reflecting what happens in the audio.\nSpecifically, we adopt the prompt \u201cClose-ended question: Write an audio caption describing the sound.\" used during LTU training for audio captioning task, and feed 10-second audio from VG-GSound and AudioSet dataset as inputs into LTU-13B model (max length 108 version). In figure 8, we show 15 examples of synthesized captions from videos in VGGSound along with the correspond-ing video IDs and start-to-end time. The generated captions are clear natural language and faithfully describe the audio content in details, serving as a reliable dataset for training and evaluation.\""}, {"title": "Additional Implementation Details of VATT", "content": "Data Preprocessing: For visual inputs, we extract video frames at 5fps rate, resulting in 50 frames for each 10s video. For audio, we extract tokens from audio waveform using pretrained Encodec-16kHz, resulting in a 4 \u00d7 500 token matrix for each 10s audio. To extract visual features, we resize each video frame to 336 \u00d7 336 and normalize, and feed into the eva-CLIP-L [61] image encoder. By extracting the mean-pooled vector from the hidden states, we represent each video frame with a 768-dim vector. For textual inputs, we use template text prompts as input to instruct video-to-audio captioning, including 10 human-written prompts. For training the VATT Audio, two cases are considered. In unconditional generation case where no additional audio caption is provided, the video along with the one of the template text prompts (shown in Table 8) are fed as inputs to the model. In conditional generation case where the ground truth audio caption is provided, the caption replaces the template prompt as the textual input to the model. For both cases, the textual inputs are formatted using \"alpaca short\" instruction style, \u201c### Instruction: instruction ### Response:\"."}, {"title": "Implementation Details of video-to-audio generation baselines", "content": "SpecVQGAN [21"}]}