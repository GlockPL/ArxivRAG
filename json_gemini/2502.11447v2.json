{"title": "Does Editing Provide Evidence for Localization?", "authors": ["Zihao Wang", "Victor Veitch"], "abstract": "A basic aspiration for interpretability research in large language models\nis to \"localize\" semantically meaningful behaviors to particular components\nwithin the LLM. There are various heuristics for finding candidate locations\nwithin the LLM. Once a candidate localization is found, it can be assessed\nby editing the internal representations at the corresponding localization\nand checking whether this induces model behavior that is consistent with\nthe semantic interpretation of the localization. The question we address\nhere is: how strong is the evidence provided by such edits? To evaluate the\nlocalization claim, we want to assess the effect of the optimal intervention at\na particular location. The key new technical tool is a way of adapting LLM\nalignment techniques to find such optimal localized edits. With this tool\nin hand, we give an example where the edit-based evidence for localization\nappears strong, but where localization clearly fails. Indeed, we find that\noptimal edits at random localizations can be as effective as aligning the\nfull model. In aggregate, our results suggest that merely observing that\nlocalized edits induce targeted changes in behavior provides little to no\nevidence that these locations actually encode the target behavior.", "sections": [{"title": "Introduction", "content": "A basic goal of interpretability research for large language models is to map\nsemantically meaningful behavior to particular subcomponents of the model.\nSemantically meaningful encompasses a wide range of things, e.g., \"when asked\nfor directions to the Eiffel Tower, the model gives directions to Paris\", \"the\nmodel responds truthfully\", or \"the model will refuse to respond\". The aim is\nto find, e.g., neurons, circuits, or regions of representation space that control\nthese behaviors. If we could find such localizations, we could use them as\nbuilding blocks to understand complex model behaviors. Many interpretability\napproaches can be understood in terms of the following idealized template [e.g.,"}, {"title": "Background and results from ITI", "content": "We replicate the setup of ITI [Li+24].\nDataset and Model Architecture We use TruthfulQA [LHE21] as our\ndataset. It contains 817 questions that humans might answer incorrectly due\nto misconceptions. Each question contains an average of 3.2 truthful answers\nand 4.1 false answers. We use 60% of the questions for training, and the rest for\nvalidation and testing.\nWe use an Alpaca-7B [Tao+23] model that is finetuned from the Llama-7B\nbase model. The model consists of L = 32 layers, each consisting of a Multi-head\nAttention (MHA) layer, and a Multilayer Perceptron (MLP) layer. We focus\non the MHA layer, which has H = 32 attention heads, with each head having\ndimension H = 128 (the hidden dimension is $D_H$ = 4096).\nIgnoring MLP and layer normalization, the computation at layer I can be\nwritten as:\nLocalization and intervention using activation statistics To localize, we\ncollect representations for positive and negative examples, and use probing to\nfind where the truthfulness concept is represented. To intervene, we find the\ndirection best separating activations for positive and negative examples, and\napply this direction to the representation."}, {"title": "Editing Localized Heads Modifies the Output as Expected", "content": "In ITI, the authors find that editing on 16 localized heads (out of a total of 1024\nheads) successfully steers model generations to be more truthful while still being\ninformative. They also find intervening on all attention heads doesn't make\nmodel generations more truthful than intervening just at the localized heads."}, {"title": "Finding \"optimal\" interventions", "content": "To test whether a particular behavior is localized to specific location, we would\nlike to assess the effect of the optimal intervention at that location. In the case\nof our running example, we want the localized edit to the representation space\nthat does the best job of steering the model's generations to be more truthful\nwhile maintaining informativeness. Then, the questions are: what is the best\nwe could hope to achieve? (I.e., what is \"optimal\"?) And, (how) can we find a\nlocalized edit that achieves it?"}, {"title": "Fitting the alignment objective gives optimal interventions", "content": "The key observation is that the dataset used to construct positive and negative\nexamples can be restructured as paired \"preference\" data ${(x_i, y_i^+, y_i^-)}_i$, where\n$x_i$ is the question, $y_i^+$ is the truthful answer, and $y_i^-$ is the untruthful answer.\nSince the goal is to make model generations more truthful, we can directly\nadopt contrastive alignment methods for biasing the model towards the truthful\nanswers. In this case, we use the IPO [Aza+24] learning objective, where the\ngoal is to upweight probabilities for $y_i^+$ and downweight probabilities for $y_i^-$ (up\nto some threshold):\nTo test the effectiveness of IPO alignment, we finetune the weights for project-\n matrices $W^l$'s defined in (3) using (rank 1) LoRA [Hu+21]. The finetuned\nmodel gives nearly perfect trade-off between truthfulness and informativeness\nthan ITI, and contrasts with ITI results that\nintervening on all heads doesn't make model generations more truthful.\nNow we treat this result as the overall best performance that we can achieve\nwith interventions. We want to see if optimal interventions at localized heads\ncan achieve the same performance, and if random heads can achieve the same\nperformance."}, {"title": "Connect weight updates to representation editing", "content": "The connection to IPO lets us search for the best possible update to the model's\nweights. However, we are interested in localized edits to model representations.\nTo continue, we need to connect the weight editing to representation editing.\nRank-1 LORA Directly applying rank-1 LoRA to $W^l$, we can view the effect\nof adding in the modified LoRA weight matrix as an edit to the representation\nas follows:\nwhere $a^l$, $b^l$ are the LoRA weights to optimize. Comparing with (5), we see that\n$b^l$ plays the role of the added $\\Theta_\\text{edit}$, and $(a^l, o^l)$ is the intervention strength but\nis adapted to the representation $o_l$.\nThis formulation connects weight edits to representation edits. However, it\ndoesn't yet allow us to localize edits to specific heads  while $\\Theta$ can be read\nas concatenation of headwise intervention vectors, the projected $W^l \\Theta$ have no\ncorresponding interpretations. Therefore, we can't restrict the edits to specific\nheads by imposing structure on $b^l$'s.\nRank-1 LORA with reparameterization We can make more direct connec-\ntions by reparameterizing b with $W^l b$ (without changing expressiveness):\nHere $b_h$ plays the role of the intervention vector $\\Theta_h$, and $a$ decides the interven-\ntion strength adaptively.\nNow we have the algorithm to find the optimal interventions for the chosen\nset of heads:\n1. Finetune the model weights using reparameterized LoRA with the IPO\nobjective.\n2. And, restrict $b$ to be nonzero only for the chosen set of heads."}, {"title": "Optimal interventions at localized heads are nearly optimal, but so are random heads", "content": "Optimal Edits at Conjectured Localization We can now search for the\nbest possible interventions at the localized heads. Figure 2 shows the result.\nWe find that the optimal interventions strongly outperform the heuristic ITI"}, {"title": "Intervening a single head is just as effective", "content": "It is now clear that edit-based evidence does not provide strong evidence for\nlocalization in the 16 head setup. However, a possible way of saving localization\nwould be to argue that 16 heads is too many, giving too much leeway to induce"}, {"title": "Are the Probing-Localized Heads Anything Special?", "content": "So far what we mean by localization, is that we can change model generation\non target concept by an edit at this location. And our experiments show no\nevidence for this type of localization, and probing-localized heads play no special\nrole.\nSo, are the probing-localized heads anything special at all?\nProbing-localized heads seems special for MC scores We do observe\nthat these heads achieve slightly better Multiple-Choice (MC) scores compared\nto randomly selected heads (see Figure 5), although this advantage is not as\npronounced as with the ITI interventions (see Figure 1c). Thus, these heads may\nbe special in terms of changing model probabilities on the given fixed dataset,\nwhich is what MC measures.\nThe gap between what the model \"knows\" and what it generates\nIt's important to note that the model's probabilities for fixed responses, do not\ndirectly correspond to what the model actually generates. Even if the model\nassigns a higher probability to a truthful response than an untruthful one, it\nmay still not generate the truthful response if the fixed dataset is off-policy (i.e.\nboth probabilities are low). This highlights the well-known gap between what a\nmodel \"knows\" (which is the motivation behind probing) and what it ultimately\ngenerates [Jos+23; WLS20; Kad+22; Sau+22; Bur+22].\nImplications It's possible that while probing-localized heads are not special at\nall for controlling model generations, they are special in changing what the model"}, {"title": "Discussion", "content": "The main idea in this paper is that to assess the localization of a behavior we\nshould study the effect of the optimal intervention at the conjectured localization.\nThe main obstacle is that, in general, it is not clear how to define or find the\noptimal intervention. To overcome this, we map the problem of finding the\noptimal intervention to the problem of finding the optimal weight update, which\ncan be solved using existing LLM alignment methods.\nThe main result is an example where, naively, the evidence for localization\nappears strong, but when we use optimal interventions, the evidence disappears.\nThe particular example truthfulness and ITI-based evidence was selected\nsimply because the data used to define the heuristic happens to also allow us to\nset up a contrastive alignment problem. The most limited read of the results\nhere is that ITI interventions do not provide evidence for localization, and that\ntruthfulness does not appear to be localizable. However, the broader point is that\nby giving an example where editing-based evidence doesn't support localization,\nwe see that in general such edits by themselves cannot provide evidence for\nlocalization. This is true irrespective of the particular behavior or heuristic being\nevaluated.\nThus far, we've been a bit vague about what localization means. Editing\ndoes tautological evidence for localization in the sense of \"it's possible to modify\nmodel behavior on such-and-such a behavior by an edit at this location\". On the\nopposite end, the strongest possible standard would be to show that the location\nis unique, or at least necessary. This is the standard that would be required\nif our aim was, e.g., to establish that LLM truthfulness can be monitored by\nexamining a small set of heads. Potentially, there are interesting and useful\nnotions of localization in between these two extremes. However, we can see no\nuseful sense of localization that is consistent with the location being only as\ngood as a randomly selected alternative. As we have seen, heuristic edit-based\nevaluation cannot even rule out this case.\nOur findings add to a growing body of work that assesses the validity of\ninterpretability results. [Niu+24] argue that the Knowledge Neuron thesis, which\nsuggests that facts are stored in MLP weights, is an oversimplification and does\nnot adequately explain the process of factual expression in language models.\n[Mak+23] demonstrate that subspace activation patching can lead to an illusory\nsense of interpretability, as the effects may be achieved through dormant parallel\npathways rather than the hypothesized subspaces. Most relevant to our work,\n[Has+24] find that localization conclusions from causal tracing do not provide\ninsight into which model MLP layer would be best to edit to override an existing\nstored fact.\nOverall, the results here point to the need for precise statements of what\nthe objectives are in interpretability. With clear objectives, it may be possible\nto develop theoretically grounded methods for evaluation. Precise, falsifiable,\nstatements and clear standards of evidence would suffice to prevent the kind of\nfailure we observe in this paper."}, {"title": "Experiment Details", "content": "Dataset and Model Architecture We use the TruthfulQA dataset [LHE21]\nand the Alpaca-7B model [Tao+23] for our experiments. The dataset contains\n817 questions with truthful and untruthful answers. We turn them into pairs,\nand use 60% for training (6560 paired data) and the rest for validation and\ntesting. The model consists of 32 layers, each with 32 attention heads and a\nhidden dimension of 4096.\nTraining Details We use IPO objective [Aza+24] and use hyperparameter\n$\\tau = 0.1, 0.2, 0.3, 0.4, 0.5$. We train for two epochs with a cosine scheduler, with\na batch size of 4. We use \"paged_adamw_32bit\" optimizer. For training with\ndifferent numbers of heads, we find a smaller number of heads benefit from a\nhigher learning rate. For all-heads, we use a learning rate of $1 \\times 10^{-4}$, and for\n16 heads, we use $5 \\times 10^{-4}$. For single-head, we use $2 \\times 10^{-3}$.\nEvaluation Metrics We reuse code from ITI [Li+24] for evaluation when\npossible. For GPT-judge models, we follow [LHE21] and finetune on truthfulness\nand informativeness dataset using OpenAI API [Ope20]. Our finetuned model\nachieves similar validation error as in [LHE21]."}]}