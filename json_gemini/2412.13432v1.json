{"title": "Large Language Model Enhanced Recommender Systems: Taxonomy, Trend, Application and Future", "authors": ["Qidong Liu", "Xiangyu Zhao", "Yuhao Wang", "Yejing Wang", "Zijian Zhang", "Yuqi Sun", "Xiang Li", "Maolin Wang", "Pengyue Jia", "Chong Chen", "Wei Huang", "Feng Tian"], "abstract": "Large Language Model (LLM) has transformative potential in various domains, including recommender systems (RS). There have been a handful of research that focuses on empowering the RS by LLM. However, previous efforts mainly focus on LLM as RS, which may face the challenge of intolerant inference costs by LLM. Recently, the integration of LLM into RS, known as LLM-Enhanced Recommender Systems (LLMERS), has garnered significant interest due to its potential to address latency and memory constraints in real-world applications. This paper presents a comprehensive survey of the latest research efforts aimed at leveraging LLM to enhance RS capabilities. We identify a critical shift in the field with the move towards incorporating LLM into the online system, notably by avoiding their use during inference. Our survey categorizes the existing LLMERS approaches into three primary types based on the component of the RS model being augmented: Knowledge Enhancement, Interaction Enhancement, and Model Enhancement. We provide an in-depth analysis of each category, discussing the methodologies, challenges, and contributions of recent studies. Furthermore, we highlight several promising research directions that could further advance the field of LLMERS.", "sections": [{"title": "1 INTRODUCTION", "content": "The large language model has been proven to own unprecedented emergent capabilities in language understanding and reasoning [3, 69, 87]. In view of the only utilization of collaborative signals by conventional recommender systems [2, 65, 66], supplementing semantic information for RS by the LLM is attractive. Thus, many works have been proposed to fill the gap between natural language and recommendation, leading to a powerful RS.\nDespite a certain extent of success in adapting LLM to RS, one significant difference between the dialogue system and the recommender system lies in the inference latency. RS often requires low latency for a mass of requests, while the LLM, e.g., LLaMA-7B, can only achieve seconds latency for a response. However, many studies focus on using the LLM for recommendation directly at the early stage [13], which makes it difficult to meet the demands of real-world applications. Recently, more researchers have paid attention to such an issue and commenced diving into the LLM-enhanced RS for practice. Therefore, to boost such a direction, we write this survey to conclude the most up-to-date works.\nIn order to clarify the range of this survey, we first give out the definition of LLMERS: The conventional recommender systems are enhanced by LLM via assistance in training or supplementary for data, while no need for LLM inference during the service. Though there have been some surveys about LLM for RS, three critical differences exist. i) Most current surveys focus on how to use the LLM itself as a better RS, including generative recommendation [28, 31, 70] and discriminative recommendation [4, 6, 20, 33, 56, 89]. By comparison, our survey addresses the LLMERS especially. ii) LLM for RS is a cutting-edge direction, which develops rapidly. Some surveys [4, 33, 70, 89] do not include the most up-to-date papers. By comparison, more than 50 works in this survey were published after 2024. iii) A few surveys have referred to LLM-enhanced RS [4, 33], but they only focus on the enhancement from the aspect of feature engineering. In contrast, this"}, {"title": "1.1 Preliminary", "content": "Since LLM-enhanced RS is based on the conventional one, the introduction to its components and challenges is necessary to understand why and where the LLM is needed. As shown in Figure 1, the conventional RS often consists of the interaction data and recommendation model.\nInteraction Data. The conventional recommender system captures the collaborative signals from user-item records [26], so the interactions in data are necessary for training. Besides, many content-based models [43] extract the co-occurrence relationships contained in the features of users and items for recommendation. Therefore, Feature and Interaction are two necessities of data. However, two challenges of data hinder further advancements for conventional RS.\n\u2022 Challenge 1: For features, they are often converted into numerical or categorical values for utilization while lacking reasoning and understanding from the knowledge aspect.\n\u2022 Challenge 2: For interactions, data sparsity leads to insufficient training for RS models.\nRecommendation Model. With the wide application of deep learning techniques, the recommendation model conforms to an \"Embedding-Deep Network\" pattern. The embedding layer targets transforming the raw features into dense representations [88], and the deep network will then capture the user's interests [84]. However, they both face a unique challenge:\n\u2022 Challenge 3: For recommendation models, they can only capture the collaborative signals but are unable to leverage semantic information."}, {"title": "1.2 Taxonomy", "content": "LLMERS augments the conventional RS from its basic components, i.e., interaction data and recommendation model, so that only conventional RS models are conducted during serving. According to the challenge LLM will take effect, we categorize the LLM-enhanced RS into three lines, which are illustrated in Figure 1.\nKnowledge Enhancement. This line of work utilizes reasoning abilities and world knowledge of the LLM to derive textual descriptions for users or items. These descriptions will serve as extra features to supplement reasoning and understanding knowledge, and thus they can face the Challenge 1. (Section 2)\nInteraction Enhancement. To face the data sparsity issue, i.e., Challenge 2, some research studies adopt the LLM to derive new user-item interactions. (Section 3)\nModel Enhancement. The LLM can analyze the interactions from a semantic view, so some works have tried to make use of the LLM to assist the conventional recommendation models, addressing the Challenge 3. (Section 4)\nFor clarity, we show all the LLMERS papers according to the taxonomy in Figure 2."}, {"title": "2 KNOWLEDGE ENHANCEMENT", "content": "LLM owns extensive world knowledge and powerful reasoning abilities, which can supplement the RS with external knowledge. In this category, the LLMERS often utilize explicit knowledge to augment the recommender system. For example, as shown in Figure 1, we can prompt the LLM by textual user's interactions to output the \"preference\" of the user as an extra feature. Such a feature, containing the knowledge from the LLM, can enhance the current RS. Since the feature can be derived and saved in advance, the demand for LLM will be avoided during the service, leading to efficient inference. According to the type of the feature, we can further partition this line into unstructured Summary Text, structured Knowledge graph, and Combination of these two. The illustrations of these lines are shown in Figure 3."}, {"title": "2.1 Summary Text", "content": "This subcategory refers to utilizing LLM to summarize the characteristics of items or to reason for the preference of users. For example, the prompt for summarization can be \u201cGiven a user who has viewed <Browsing History>, please explain what he or she is interested in: \". The summary texts are then often encoded by a general language model, such as Bert [25]. Some works enhance the RS for either the user or item side, while others address both."}, {"title": "2.1.1 User Only and Item Only", "content": "For the user-only literature, LANE [85] and LLM-BRec [22] fabricate prompts to capture multi-preferences of users. For better utilization, LANE additionally designs an attention mechanism to align the preference representations of RS and LLM. On the other hand, as basic units in the RS, more works focus on the item-only aspect. As one of the pioneers in this field, ONCE [37] utilizes LLM as a content summarizer for news recommendation. Besides, it proposes to extend the news content by LLM before encoding, while Yada et al. [76] uses the LLM to generate category descriptions for augmenting the news content. Similarly, LAMAR [44] adopts LLM to output factual properties of movies. As for SeRALM [50], it additionally devises an alignment training pattern for better integration of LLM's knowledge. Recently, multimodal RS [38] has attracted much attention. MMRec [55] and X-Reflect [47] propose to use multimodal LLM to summarize the visual feature into textual description for use."}, {"title": "2.1.2 User & Item", "content": "Some research studies generate summary texts for both user and item to enhance RS, especially content-based RS [23]. KAR [71] is one of the pioneers in this line, which gets the reasoning knowledge of users and factual knowledge of items in textual form. The knowledge is then encoded by the designed multi-expert text encoders and integrated into the traditional CTR models, such as DeepFM [16] and DIN [92]. REKI [72], a successor of KAR, further refines the knowledge extraction module for various scales of recommendation scenarios. Different from these two works, GaCLLM [9] enhances the graph-based RS models by the knowledge of users and items. Specifically, it considers the"}, {"title": "2.2 Knowledge Graph", "content": "Summary text is a type of unstructured knowledge for enhancement, while the structured Knowledge Graph (KG) may drive better integration. Thus, many current works have explored how to apply the LLM to generate a KG (Generation) or augment an existing KG (Completion & Fusion) for enhancing traditional RS."}, {"title": "2.2.1 Generation", "content": "In this subcategory, the KG is generated initially and then used as supplementary features for enhancing RS. LLMRG [62] is the first to explore the KG generation for RS. It fabricates the reasoning prompt to derive possible interaction sequences from LLM and the verification prompt to filter out illogical sequences. The generated sequence forms an interaction graph, which is then used to enhance the user representation. Similarly, LLMHG [7] also prompts LLM to generate a KG based on the user's interaction records, but instead of an interaction graph, it derives user-interest hyper-graphs. Besides, SAGCN [35] imposes the LLM to distinguish the user-item interactions from various semantic aspects, generating multiple interaction graphs corresponding to these aspects. All of these works tend to generate KG at the interaction level. By comparison, HRGraph [67] utilizes LLM to conduct job entity extraction from job descriptions and curriculum vitae, producing the specified graph for job recommendation."}, {"title": "2.2.2 Completion & Fusion", "content": "Different from generation, this line of work often imposes the LLM on existing KG. For example, LLM-KERec [86] adopts the LLM to identify complementary relationships within an item KG. Then, the KG-based RS can derive better recommendations by enriched entity relationships. Besides, Yang et al. [78] find that the direct fusion of existing KG and LLM-generated entity relations is sub-optimal. To address such an issue, they propose an improved fusion method to better integrate the two sources."}, {"title": "2.3 Combination", "content": "In consideration of the effectiveness of these two types, some works also resort to combining both of them. KELLMRec [46] is one representative, which integrates the KG for constructing prompts to avoid the hallucination problem when getting the summary texts from LLM. Besides, SKarRec [29] also links the entity in KG to the textual prompts, allowing the LLM to generate KG-aware knowledge that augments the concept recommendations."}, {"title": "3 INTERACTION ENHANCEMENT", "content": "To address the sparsity problem in traditional recommender systems, data augmentation is a straightforward way [27], which targets generating new interactions. The augmented interactions are only supplemented for training traditional RS models, so no extra inference burden will be brought. Inspired by the semantic view of LLM, some research studies have explored the interaction generation by LLM. We categorize these approaches into two types based on the output of the LLM, i.e., Text-based and Scored-based, shown in Figure 4. The former gives out the names of pseudo-interacted items as the augmentation. By comparison, scored-based methods derive the logits of the possible interactions and then generate the augmented items by ranking."}, {"title": "3.1 Text-based", "content": "ONCE [37] is the first to investigate the data augmentation for RS using LLM. It prompts the LLM to recommend news based on a user's historical records. The generated news is added to the dataset as an augmented interaction for this user. However, the direct choice from a large item set is difficult for LLM. Thus, later works propose narrowing the selection to a smaller set of items for higher augmentation accuracy. For example, LLMRec [68] first adopts well-trained traditional RS to filter out a list of items and then prompt LLM to choose from them. Similarly, LlamaRec [45] imposes the LLM to generate interactions from a random-sampled item set. Furthermore, Wang et al. [60] proposes to utilize the LLM to judge which item a user prefers, generating pair-wise interactions for cold-start RS. In addition to generating new data, distinguishing noisy data is also a vital topic in data augmentation. LLMHD [52] addresses this issue by applying the LLM to distinguish hard or noisy samples in sequential recommendation."}, {"title": "3.2 Score-based", "content": "As mentioned in the last subsection, it is difficult for LLM to select from amounts of items. Besides, the textual output often faces the out-of-corpus problem [1], i.e., the generated item name cannot correspond to anyone in the item set. To avoid these two problems, some works explore the score-based method. One typical work of this line is LLM-InS [21]. It firstly imposes the LLM to derive the semantic embeddings of users and items, and then generate augmentations by calculating the similarity between them. By comparison, LLMDSR [58] considers the probability of word tokens as the confidence in deciding a noisy item and generates the new item by the similarity of embeddings."}, {"title": "4 MODEL ENHANCEMENT", "content": "Beyond data-level enhancement, the powerful abilities and semantics of LLM can also be injected into traditional recommendation models directly. RS models are generally composed of two main parts, i.e., embedding layer and hidden layers. The former often captures relationships between items, while the latter extracts more complex user preferences. According to the various parts, we categorize this line into Model Initialization, Model Distillation, Embedding Utilization and Embedding Guidance, as shown in Figure 5. The first two categories involve applying the LLM to the entire RS model, while embedding utilization and guidance only impose LLM on the embedding layer. It is worth noting that Model Initialization, Model Distillation, and Embedding Guidance only need the LLM to assist in training RS models, so LLM usage is eliminated during serving. Besides, Embedding Utilization can cache the LLM embeddings before inference. Thus, they all can meet the latency requirements of real-time service."}, {"title": "4.1 Model Initialization", "content": "Pretrain, as one of the initialization techniques, has shown potential in the recommendation field [36]. On the one hand, pretrained weights can help model convergence. On the other hand, the information learned from the pretrain stage will preserve and affect the downstream recommendation tasks. Thus, recent works have explored saving the semantics of LLM to the weights of RS models before training. Some approaches apply pretrained weights to the whole RS model, denoted as Whole, while the others are for the embedding layer, referred to as Embedding."}, {"title": "4.1.1 Whole", "content": "CTRL [30] and FLIP [59] are the representatives in this line. CTRL first considers the semantic representation extracted from LLM and collaborative representation derived from the traditional RS model as two distinct modalities [10-12]. It then aligns such two \"modalities\" by contrastive tasks. The parameters updated during the alignment process serve as the initialization of the whole traditional RS model. Building upon CTRL, FLIP [59] fabricates fine-grained masked tabular and language modeling alignment tasks for better model initialization."}, {"title": "4.1.2 Embedding", "content": "Embedding plays a key role in the traditional RS models [88], preliminarily capturing the relationships between items and features. The LLMERS in this sub-category all aim to extract semantic relationships between items by the derived embeddings from LLM (i.e., LLM embeddings). Harte et al. [17] initiates this line, who input the item title to a large text embedding model. Then, the derived LLM embeddings are used to initialize the embedding layer of a sequential recommendation model. Later, LEARN [24] proposes to adopt the hidden states of a frozen open-sourced LLM as the item embedding. However, the general-purpose LLM without fine-tuning may encounter incompatible challenges for the recommendation task. To address this, SAID [19] and TSLRec [34] specialize the item attribute word generation and information reconstruction tasks, respectively, to adapt the LLM to recommendation better. More recently, LLMEmb [39] integrates an attribute-level contrastive task to further extract fine-grained semantic relationships between items."}, {"title": "4.2 Model Distillation", "content": "Knowledge distillation (KD) is one of the compression techniques [15], which is also promising to bring the powerful abilities of LLM to small RS models. However, the gap between discriminative RS models and generative LLM may hinder the success of KD. Thus, existing studies design Feature-based and Response-based distillation ways to address such an issue."}, {"title": "4.2.1 Feature-based", "content": "Feature-based distillation transfers knowledge directly between the hidden states of models, regardless of the output type. LEADER [41] is the pioneer of this line, which distills the RS model using the hidden state from the final layer of the LLM with a trainable adapter. Notably, LEADER also fine-tunes the LLM to transform the LLM into a discriminative model. In contrast, SLM-Rec [74] maintains the basic structure of LLM, i.e., stack of several transformer layers, but reduces the number of layers to create a smaller-scale RS model. It then imposes knowledge distillation to several transformer layers between LLM and the RS model."}, {"title": "4.2.2 Response-based", "content": "Following the advancement in KD for recommendation, recent works have explored the use of LLM to generate a ranking list from a semantic view, followed by the ranking distillation loss [54] for the RS model. Specifically, DLLM2Rec [8] proposes to fine-tune the LLM to adapt the recommendation task and then distill the small RS model by the ranking list derived from the LLM."}, {"title": "4.3 Embedding Utilization", "content": "In general, the LLM generates natural language to complete various tasks. However, the textual outputs are often difficult and inefficient to be integrated into RS models directly. To avoid the deficiency and better utilize the semantics of LLM, many recent works utilize the embeddings derived from LLM, e.g., the hidden states of the last transformer layer of the LLM, to enhance traditional RS as a semantic supplementary. From the aspect that LLM embeddings address in the traditional RS, this line of works can be categorized into User Only, Item Only and User & Item."}, {"title": "4.3.1 User Only", "content": "In this cluster, the LLM is used to generate the representation of the user's preference directly. The LLM embeddings then will be fed into the traditional RS model for augmentation. A notable example in this line is LLM-CF [53]. It first designs a data mixture method to fine-tune the LLM for recommendation capability. Next, the well-trained LLM can generate Chain-of-Thought (CoT) reasoning to enhance user preferences."}, {"title": "4.3.2 Item Only", "content": "In some types of recommender systems, e.g., sequential recommendation, the item embeddings play a fundamental role. Thus, most existing embedding utilization focuses on the item-only enhancement. Many early studies in this sub-category directly adopt the frozen LLM without fine-tuning. For example, TedRec [73] apply LLaMA to encode the item texts and then propose a frequency-based fusion to combine the derived semantic embeddings and ID embeddings. LRD [77] adopts LLM embeddings in a manner akin to TedRec but introduces a novel self-supervised relation discovery task to optimize the utilization. However, these works may lose the informative semantics contained in LLM embeddings during training. To address such an issue, LLM-ESR [40] and AlphaRec [51] both devise a new pattern of frozen LLM embeddings with a trainable adapter to maintain the original semantic information."}, {"title": "4.3.3 User & Item", "content": "In this line, the current papers can be clustered into two categories. One is to utilize the user and item LLM embeddings for recommendation by embedding similarity function directly. For example, LoID [75] first aligns the LLM embeddings with collaborative ID information, then concatenates the user and item representations derived from the LLM for final recommendation. However, the user LLM embeddings are often sub-optimal because of over-length textual prompts of the user's interaction histories. To face this challenge, BAHE [14] proposes to partition an LLM into lower layers and higher layers. It uses the lower layers of the LLM to encode the item attributes to get the comic item representation. Then, the corresponding comic vectors of the user's interacted items are fed into the higher layer. By comparison, EmbSum [81] partitions the user's interaction records into several sessions and then uses the LLM to encode them separately. Except for the lengthy prompt issue, the general user LLM embedding also faces a lack of dynamics. To enhance dynamic graph recommendation, DynLLM [90] generates multi-faceted profiles for users.\nThe other line of work supplements the user and item LLM embeddings as extra features to augment the content-based RS models. For instance, Laser [32] utilizes the derived user and item LLM embeddings with an adapter for feature enhancement. As for LARR [57], it also adopts the LLM embeddings as extra features, but further highlights the importance of fine-tuning the LLM specialized for RS."}, {"title": "4.4 Embedding Guidance", "content": "Different from the direct embedding utilization, this line of papers only uses the LLM embeddings as the guidance for training or parameter synthesis. According to which aspect the LLM embeddings aim to enhance, we further categorize the current works into User Only and User & Item.\nDue to the lengthy prompt of user interactions, user LLM embeddings are often noisy. Thus, considering the LLM embeddings as guidance is a feasible way for user-only enhancement. LLM4SBR [48] enhances the short-term and long-term preferences for session-based recommendation models by identifying the representative items via LLM embeddings. By comparison, LLM-ESR [40] retrieves similar users by LLM embeddings to enhance the long-tail users during the training. These two works both aim to search for relevant users or items from a semantic view and enhance the training process. Differently, LLM4MSR [64] generates the user-specified parameters for multi-scenario recommendation models by LLM embeddings accompanied with meta networks. As for User & Item line, the research studies guide both the user and item representation learning. RLMRec [49] kicks off this line, which adds an extra loss to align the collaborative user and item embeddings with corresponding LLM embeddings during training. Based on the insights of RLMRec, DaRec [79] further proposes to disentangle the shared and specific parts for both LLM and collaborative embeddings. Only imposing the alignment on shared parts will lead to better performance."}, {"title": "5 TREND", "content": "To investigate the trend of the LLMERS, we visualize the current works based on the type of LLM and semantics in Figure 6. As mentioned before, the key to LLM enhancement lies in the semantics. At the early stage, many works [71, 86] prompt the LLM to derive the natural language, which contains knowledge and semantics for augmentation. Since they are readable and understandable, we denote them as explicit semantics. By comparison, the cluster of implicit semantics means that the mediation from LLM for enhancement is non-verbal, e.g., the hidden states of LLM. Though explicit knowledge should be preferable by humans due to its explainability, we find that more recent works have embraced implicit one. The reason lies in the better performance of implicit knowledge, while the information loss occurs in the extra encoding process for explicit knowledge.\nAs for the type of LLM, the figure reveals that more LLMERS works resort to the open-sourced LLM. One reason is that it can save money spent on calling the API. More importantly, the open-sourced LLM can be fine-tuned to suit the recommendation task [39]. Furthermore, observing the quadrants I and IV, the fine-tuned LLM has become prevalent recently, which indicates the significance of aligning the LLM with recommendation tasks. In Table 1, we also conclude the specific type of LLM used in LLMERS.\nOverall, we can sum up the trends of current LLMERS as follows:\n\u2022 The current LLMERS studies move from explicit semantics to implicit semantics.\n\u2022 For a better adaptation to the recommendation task, fine-tuned open-sourced LLM become more prevalent.\n\u2022 Model enhancement has gained more attention recently, because it can combine with implicit semantics and fine-tuned LLM better."}, {"title": "6 APPLICATIONS AND RESOURCES", "content": "Traditional recommender systems have been widely adopted in various applications, such as news recommendations. Though LLMERS is also based on traditional RS, it often requires abundant side information, especially informative texts, to make full use of the reasoning and understanding abilities of LLM. Thus, current LLMERS works often focus on applications that have one of two following characteristics: (i) it has a number of normal features that LLM can understand; (ii) it has a volume of texts that LLM can summarize. The E-commerce application belongs to the former one, where users have many profile features and commodities have attributes [40, 62]. Since the LLM owns open-world knowledge, these features can be understood by it and derive semantics for enhancement. By comparison, the news recommendation is the typical application that utilizes the LLM to summarize the abundant news texts [37]. To promising to replicate their success in some other recommendation tasks.\n\u2022 Multimodal RS. The multimodal RS [38] has become prevalent recently due to the emergence of more multimedia services. However, the existing multimodal RS often faces the challenge of extraction and fusion of features in different modalities. The adoption of multimodal LLM [80] may be a feasible way to augment existing multimodal RS and even eliminate the extraction and fusion procedures.\n\u2022 User-side Enhancement. There have been many efforts of LLMERS imposed on the item side, but few on the user side. The reason lies in that the user's historical interactions are often organized into texts orderly. However, such a prompt often faces the overlength problem [14]. Besides, the textual interactions are often difficult to understand for LLM. This problem thus hinders the user-side enhancement severely and needs to be addressed.\n\u2022 Explainability. Explainability is an important aspect of constructing a trustworthy RS. The traditional RS is often short in this aspect since they only adopt meaningless identities. The LLM enhances the RS by understanding users and items from a semantic view, which is promising to derive an explanation.\n\u2022 Benchmark. Since LLMERS is a newborn direction, there is no benchmark in this field. Therefore, developing a comprehensive and usable benchmark is an urgent need, because it can facilitate fresh researchers to contribute to this field and accelerate the advancement of this direction."}, {"title": "7 FUTURE DIRECTIONS", "content": "LLMERS is a cutting-edge direction, where many problems have not been addressed. Therefore, we give out several directions to inspire later research.\n\u2022 Exploration for More Recommendation Tasks. Existing LLMERS works have focused on revolutionizing several basic recommendation tasks, such as collaborative filtering [53] and sequential recommendation [40]. They have proven the effectiveness of bringing the semantics of LLM to traditional RS. Thus, it is also promising to replicate their success in some other recommendation tasks.\n\u2022 Multimodal RS. The multimodal RS [38] has become prevalent recently due to the emergence of more multimedia services. However, the existing multimodal RS often faces the challenge of extraction and fusion of features in different modalities. The adoption of multimodal LLM [80] may be a feasible way to augment existing multimodal RS and even eliminate the extraction and fusion procedures.\n\u2022 User-side Enhancement. There have been many efforts of LLMERS imposed on the item side, but few on the user side. The reason lies in that the user's historical interactions are often organized into texts orderly. However, such a prompt often faces the overlength problem [14]. Besides, the textual interactions are often difficult to understand for LLM. This problem thus hinders the user-side enhancement severely and needs to be addressed.\n\u2022 Explainability. Explainability is an important aspect of constructing a trustworthy RS. The traditional RS is often short in this aspect since they only adopt meaningless identities. The LLM enhances the RS by understanding users and items from a semantic view, which is promising to derive an explanation.\n\u2022 Benchmark. Since LLMERS is a newborn direction, there is no benchmark in this field. Therefore, developing a comprehensive and usable benchmark is an urgent need, because it can facilitate fresh researchers to contribute to this field and accelerate the advancement of this direction."}, {"title": "8 CONCLUSION", "content": "Large language model-enhanced recommender systems (LLMERS) have attracted much attention due to their effectiveness and practicability. In this paper, we conclude the most recent efforts in this field. According to the component of traditional RS that LLMERS targets, we categorize the papers into three lines, i.e., knowledge, interaction and model enhancement. Furthermore, to facilitate and inspire the researchers, we summarize the available resources and give out several future directions."}]}