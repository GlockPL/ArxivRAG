{"title": "Explainable Finite-Memory Policies\nfor Partially Observable Markov Decision Processes", "authors": ["Muqsit Azeem", "Debraj Chakraborty", "Sudeep Kanav", "Jan Kretinsky"], "abstract": "Partially Observable Markov Decision Processes (POMDPS)\nare a fundamental framework for decision-making under un-\ncertainty and partial observability. Since in general optimal\npolicies may require infinite memory, they are hard to im-\nplement and often render most problems undecidable. Con-\nsequently, finite-memory policies are mostly considered in-\nstead. However, the algorithms for computing them are typi-\ncally very complex, and so are the resulting policies. Facing\nthe need for their explainability, we provide a representation\nof such policies, both (i) in an interpretable formalism and (ii)\ntypically of smaller size, together yielding higher explainabil-\nity. To that end, we combine models of Mealy machines and\ndecision trees; the latter describing simple, stationary parts of\nthe policies and the former describing how to switch among\nthem. We design a translation for policies of the finite-state-\ncontroller (FSC) form from standard literature and show how\nour method smoothly generalizes to other variants of finite-\nmemory policies. Further, we identify specific properties of\nrecently used \u201cattractor-based\" policies, which allow us to\nconstruct yet simpler and smaller representations. Finally, we\nillustrate the higher explainability in a few case studies.", "sections": [{"title": "Introduction", "content": "POMDP and FSC. Partially observable Markov decision\nprocesses (POMDPs) are a fundamental framework for\ndecision-making under uncertainty, where the agent does\nnot have complete information of the current state. Unfortu-\nnately, decision-making in POMDPs is inherently very com-\nplex. In particular, infinite memory may be required for op-\ntimality for many objectives. Not only are such policies im-\npractical for real-world implementation, but this complex-\nity also makes finding the optimal policies particularly chal-\nlenging and, in most settings, undecidable (Madani, Hanks,\nand Condon 2003). Consequently, finite-memory policies\nare often considered. Besides rendering various problems\ndecidable (or at least allowing for efficient heuristics), they\nare clearly more amenable to implementation than general,\ninfinite-memory policies. Finite-memory policies are for-\nmalized as finite-state controllers (FSCs), essentially Mealy\nmachines (finite-state automata with both input and output)\nwhere each state corresponds to a stationary (a.k.a. memo-\nryless) policy and the automaton transitions describe how to\nswitch among them.\nExplainability. The automata representation is regarded as\ninterpretable and even explainable (Bork et al. 2024) due to\nits graphical structure and typically low number of states.\nHowever, the automaton drawing does not depict the whole\npolicy. Indeed, each state corresponds to a stationary pol-\nicy, which is a large table with an action to be played for\neach observation; due to its often enormous size it is hard\nto explain and thus does not expose the goal of that partic-\nular policy. Similarly, each transition between states is typ-\nically labeled by many observations, effectively hiding the\nreasons to switch to another policy. While both issues pre-\nvent explainability, the latter one is more severe due to two\nreasons. First, the size of the representation of transitions is\neven greater than that of the policies. Second, the former is-\nsue of representing a stationary strategy has been tackled in\nliterature, e.g. (Ashok et al. 2021), while the latter issue of\nrepresenting transitions not so.\nOur contribution. In this paper, we tackle these issues and\nobtain better overall explainability. In particular, we propose\na new data structure to represent FSC, merging the advan-\ntages of the Mealy machine and the interpretable decision\ntrees. The latter is used not only for representing the station-\nary policies similarly to literature, e.g. (Ashok et al. 2021),\nbut importantly also for the automata transitions. Besides,\nwe identify specific properties of \u201cattractor-based\" policies\n(Junges, Jansen, and Seshia 2021), which allow us to con-\nstruct yet simpler and smaller representations with more\ncompact and fewer transitions, by allowing skips.\nOur transformation procedure preserves the policies (and\nonly changes the representation) and thus, in particular, their\noptimality. Further, as it is modularly defined on parts of the\noriginal policy, it explains the parts separately, simplifying\nthe explanation compared to a larger monolyth."}, {"title": "Related work", "content": "Partial observable MDPS (POMDPs) can be solved using\nvarious techniques including value iteration (Smallwood and\nSondik 1973), point-based techniques (Pineau et al. 2003),\ndynamic programming (Geffner and Bonet 1998), Monte\nCarlo Tree Search (Silver and Veness 2010). Model check-\ners like STORM (Hensel et al. 2022) use belief-based analy-\nsis (\u00c5str\u00f6m 1965) to find an optimal policy. For almost sure"}, {"title": "Preliminaries", "content": "A probability distribution on a set $S$ is a function $d : S \\rightarrow\n[0,1]$ such that $\\sum_{s\\in S} d(S) = 1$. We denote the set of all\nprobability distributions on the set $S$ as Dist($S$).\nDefinition 1 (MDP). A Markov decision process (MDP) is\na tuple $M = (S, A, P, R, S_{init})$ where $S$ is a countable set\nof states, $A$ is a finite set of actions, $P : S \\times A \\rightarrow Dist(S)$\nis a partial transition function, $R: S\\times A \\rightarrow \\mathbb{R}$ is a reward\nfunction, and $S_{init} \\in S$ is the initial state.\nWe define the set of enabled actions in $s \\in S$ by $A(s) =$\n$\\lbrace a \\in A | P(s, a)$ is defined$\\rbrace$. A Markov chain (MC) is an\nMDP with $|A(s)| = 1$ for all $s \\in S$. For an MDP $M$, a\npath is a sequence of states and actions. For an MDP $M$, we\ndenote the set of all finite paths by FPathsM.\nDefinition 2 (POMDP). A partially observable MDP\n(POMDP) is a tuple $P = (M, Z,O)$ where $M =\n(S, A, P, S_{init})$ is the underlying MDP with finite number of\nstates, $Z$ is a finite set of observations, and $O: S \\rightarrow Z$ is an\nobservation function that maps each state to an observation.\nWe also assume that the observation space are factored,\ni.e. $Z = \\Pi_{i=1}^k Z_i$ where each component $Z_i$ represents\na distinct feature of the observation, meaning any obser-\nvation $o \\in Z$ can be represented as a tuple of features\n$(z_1, z_2,..., z_k)$, where each $z_i \\in Z_i$. Two paths $p =\ns_0A_0s_1A_1...s_k$ and $p' = s'_0a'_0s'_1a'_1...s'_k$ are called observation-\nequivalent if $O(s_i) = O(s'_i)$ for all i."}, {"title": "Policy Synthesis.", "content": "The policy synthesis problem usually\nconsists of finding a policy that satisfies a certain objective\n$\\varphi$. In this paper, we consider infinite-horizon reachability or\nexpected total reward objectives. Given a set of target states\n$T \\subseteq S$, we denote $\\mathbb{P}^{\\pi, s_0}(\\Diamond T)$ to be the probability of reach-\ning a state in $T$ following the policy $\\pi$. Also, for the set of\ntarget states $T \\subseteq S$, we denote $\\mathbb{E}^{\\pi, s_0}(reward(\\Diamond T))$ to be the\nexpected total reward accumulated till we reach the states in\n$T$ following the policy $\\pi$.\nIn the case of almost-sure reachability objective, a policy\nis winning from $s_0$ if $\\mathbb{P}^{\\pi, s_0}(\\Diamond T) = 1$. Finding such a win-\nning policy is EXPTIME-complete (Chatterjee, Doyen,\nand Henzinger 2010). Alternatively, we can look at quan-\ntitative objectives: given an objective $\\varphi$ and a threshold\n$\\lambda \\in (0, 1)$, a policy is winning if $\\mathbb{P}^{\\pi, s_0}(\\Diamond T) \\geq \\lambda$."}, {"title": "Finite State Controller.", "content": "A finite memory policy is often\nrepresented by a Finite state controller as follows:\nDefinition 4 (FSC). A finite state controller (FSC) is a tuple\n$F = (N, N_{init}, \\delta, \\gamma)$ where $N$ is a finite set of nodes, $n_{init} \\in$\n$N$ is the initial node, $\\delta: N\\times Z\\times Z \\rightarrow N$ is the transition\nfunction, $\\gamma: N \\times Z \\rightarrow A$ is the action mapping.\nIn state $s$ wth observation $z$, an agent following an FSC $F$\nexecutes the action $a = \\gamma(n, z)$ and then based on the cur-\nrent and the next (posterior) observation $z$ and $z' = O(s')$, the FSC evolves to node $n' = \\delta(n, z, z')$. A single-step exe-\ncution of an FSC is described in Algorithm 1."}, {"title": "An Illustrative Example Explaining Policy\nRepresentation", "content": "In this section, first, we use an example to illustrate how a\nnon-stationary policy in a POMDP can be represented as an\nFSC. We will then describe how we can use the DT-based\ntechniques to make this policy more explainable. As a run-\nning example, we use the cheese-maze model (Littman, Cas-\nsandra, and Kaelbling 1995) as detailed in Example 1.\nExample 1 (Maze). Consider the maze in Fig. 1 which can\nbe modeled as a POMDP. In the initial observation, a mouse\nis randomly put in the grid. The objective for the mouse is to\nreach the target (cheese) while avoiding traps placed in the\ngrid. Each cell in the grid represents a distinct state, and the\nmouse can move in one of the four possible cardinal direc-\ntions at each step unless there is a wall blocking it. However,\nthe mouse's observations are limited to observing the sur-\nrounding walls, preventing it from distinguishing between\ncells with identical wall configurations. Consequently, there\nare only seven distinct observations (with the target and traps\nconsidered as separate observations).\nFinite State Controller Representation. In Example 1,\nthe observation space of the model is structured with the\nfollowing Boolean variables: CanGoDown, CanGoLeft,\nCanGoRight, CanGoUp describing whether the mouse\ncan go down, left, right or up respectively. There are two"}, {"title": "Decision-Tree-Based Explainability.", "content": "Instead of using\nlarge tables in every node of the FSC to define both the sta-\ntionary policy and transitions, we represent them as decision\ntrees. These DTs can leverage the structure of observations\nto make the representation more interpretable. Notice that,\nin the DTs, the predicates did not need to consider the ob-\nservation CanGoUp.\nThe representation shown in Fig. 3 includes two nodes\nand a total of four DTs: two for action selection and two for\nthe transitions. Compared to a traditional policy table, these\ndecision trees provide a clearer explanation of the policy.\nInitially, the mouse will go up (if it is in the corridor with\nwalls on the left and right side), and then go left.\nBased on the DT describing the transition relation, the\nmouse will switch to another stationary policy when i) it can\nmove down but cannot move left, or ii) when it reaches a\ncell from which it can move down, left or right directions.\nAfter switching the policy, the mouse needs to follow the\nfollowing policy based on a set of decision rules: i) if the\nmouse cannot move left or down, it should move right, ii) if\nthe mouse can move down but cannot move left or right, it\nshould move down, iii) if the mouse is in the leftmost corner"}, {"title": "Augmenting FSCs with DTs", "content": "In this section, we formally outline how explainability can\nbe integrated into an FSC using decision trees (DTs). We in-\ntroduce a specific data structure called DT-FSC to facilitate\nthis. Next, we discuss a particular scenario where FSCs (and\ntheir corresponding DT-FSCs) can be built using an itera-\ntive approach, which simplifies the explanation process. For\nthis scenario, we also introduce a specialized data structure\nwhich provides decision-tree representations that are equally\nexplainable but more compact.\nDT-FSC For each node in the FSC $F = (N, n_{init}, \\delta, \\gamma)$,\nwe can extract the corresponding stationary policy and the\nnext transition function in tabular form. In other words, for\n$n\\in N$, we will extract two tables: i) $\\Upsilon_n: Z \\rightarrow A$ that\nmaps observation $z$ to the action $\\gamma(n, z)$, and ii) $\\Delta_n: Z \\times$\n$Z \\rightarrow N$ that maps a pair of observations $(z, z')$ to the node\n$\\delta(n, z, z')$. We learn decision trees $DT_{\\Upsilon,n}$ and $DT_{\\Delta,n}$ which\nrepresents these functions $\\Upsilon_n$ and $\\Delta_n$:\nDefinition 6 (DT-FSC). Given an FSC $F = (N, N_{init}, \\delta, \\gamma)$,\nwe define the DT-FSC representation of $F$ as follows:\n$F^{DT} = (N,N_{init}, \\{DT_{\\delta,n}\\}_{n\\in N}, \\{DT_{\\gamma,n}\\}_{n\\in N})$, where, for\n$n\\in N$, $DT_{\\delta,n}$ is the decision-tree representation of $\\delta_n$, and\n$DT_{\\gamma,n}$ is the decision-tree representation of $\\Upsilon_n$.\nAlgorithm 2 describes a single-step execution of a DT-\nFSC."}, {"title": "Representation using Iterative Approach", "content": "For the almost sure reachability objective, one can obtain\nFSCs using an SMT-based iterative approach as used for\ncomputing the winning region in (Junges, Jansen, and Seshia\n2021). In each iteration of this approach, an observation-\nbased stationary policy $\\sigma_i$ is obtained that either reaches the\ntarget directly or reaches an observation from which we have\nalready found a winning policy in a previous iteration. In the\nsecond case, the policy would switch to the previously cal-\nculated winning policy. This iterative procedure also helps\nus construct the FSC representing a winning policy. At it-\neration $i$, we add a new node $n_i$ to the FSC. The policy $\\sigma_i$\nfound in the iteration would describe the action mapping,\ni.e., $\\gamma(n,z) = \\sigma_i(z)$. If the policy requires switching to a\npreviously computed policy, this is captured by the transi-\ntion function $\\delta$. This can be conceptualized as follows:\nCase 1: Based on the current observation $z$, the policy can\nsuggest an action $a$ and does not switch to any other pol-\nicy. In that case, in the constructed FSC, $\\gamma(n_i,z) = a$\nand the node of the FSC would not change.\nCase 2: Upon encountering an observation $z'$, the policy\ncan switch to a previously computed policy $\\sigma_{j}$ (without\ntaking any action). In that case, in the constructed FSC,\nfor all $z \\in Z$, we would have $\\delta(n_i, z, z') = n_j$, where $n_j$\nis the node corresponding to the policy $\\sigma_j$ added in the\n$j^{th}$ iteration."}, {"title": "Finite State Controllers with Skip", "content": "We introduce an alternative representation for the FSCs pro-\nduced by the iterative approach, referred to as skip-FSCs\nwhere we exploit the structural characteristics as described\nin Observation 1. We define a skip-FSC as follows:\nDefinition 7 (skip-FSC). A skip-FSC is a tuple $F_{skip} =$\n$(N, n_{init}, \\delta_{skip}, \\gamma_{skip})$ where $N$ is a finite set of nodes, $n_{init} \\in$\n$N$ is the initial node, $\\delta_{skip}: N \\times Z\\times Z \\rightarrow N$ is a special\ntransition function, and $\\gamma_{skip}: N \\times Z \\rightarrow A \\cup \\{skip\\}$ is a\nspecial action labeling.\nIn state $s$, an agent receives observation $z = O(s)$. At\ncurrent node $n$, if $\\gamma_{skip}(n, z) = skip$, the agent transitions\nto the node $n' = \\delta_{skip}(n, z, z')$ and continues this process\nuntil reaching a node $n''$ with $\\gamma_{skip}(n'', z) \\neq skip$. The\nagent takes the action $a = \\gamma_{skip}(n'', z)$. The state of the\nMDP is then changed to $s'$ according to the probability dis-\ntribution. Then, based on the current and the next observa-\ntion $z$ and $z' = O(s')$, the FSC updates to the new node\n$\\delta_{skip}(n'', z, z')$. A single-step execution of this FSC is de-\nscribed in Algorithm 3."}, {"title": "DT-FSCs for the General Case", "content": "In general, policies may not be represented by finite mem-\nory controllers, e.g., when it is impossible to reach the target\nwith probability 1. However, one can find a policy maximiz-\ning the probability to reach the target. Unfortunately, in this\nscenario, the optimal policy to reach the target may need in-\nfinite memory (Madani, Hanks, and Condon 2003) and can-\nnot be extracted by the iterative approach described before.\nHowever, in practice, \"good enough\" finite memory con-\ntroller policies can be designed, as done in (Andriushchenko\net al. 2023; Bork et al. 2024). In the inductive synthesis ap-\nproach in (Andriushchenko et al. 2023), an FSC is computed\nin an anytime fashion: this fixes the size of the FSC and tries\nto find the optimal FSC of that fixed size. In further itera-\ntions, it increases the size and searches for a bigger but better\nFSC till it runs out of the given time budget."}, {"title": "Additional Details on Illustrative Example Section", "content": "We explain the policy for maze in detail here. The numbers\non the maze are to indicate different grid cells and the let-\nters are used for observations (e.g., observation b means that\nCanGoUp and CanGoDown are true). Having same letters\nfor cells implies that the mouse can not distinguish between\nthose cells: e.g., 2 and 4 have same observation y. Also, 6\ncells of the grid (6-11) have the same observation b. If the\nmouse observes b and can go down, it should go up to po-\ntentially avoid the traps. for the observations g, w in order\nto transition to the second policy (see Fig. 6). As soon as it\nsees the observation g, w, it should move to the next policy\nfrom where there is a memoryless winning policy to reach\nthe cheese. Observe that for observations b, y there are pol-\nicy defined in both the memory."}, {"title": "Proof of Correctness for FSC with Skip", "content": "For our new skip-FSC, we first recall the following observa-\ntion.\nObservation 2 (Restating Observation 1). For any winning\nobservation $z \\in Z$, there exists an iteration $i_z$, where we\nfind a policy that is winning from any state with observation\n$z$. Then in the FSC generated by the iterative approach, for\nany $j > i_z$, for any $z' \\in Z, \\delta(n_j, z', z) = n_{i_z}$.\nNext we recall Algorithm 5 and formally prove the cor-\nrectness of it."}]}