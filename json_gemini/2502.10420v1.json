{"title": "Position: Stop Acting Like Language Model Agents Are Normal Agents", "authors": ["Elija Perrier", "Michael Timothy Bennett"], "abstract": "Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.", "sections": [{"title": "1. Introduction", "content": "Language Model Agents (LMAs) (Xi et al., 2023) are agentic systems based upon large language models (LLMs).\nThey appear to be able to reason (Wei et al., 2023) and plan (Huang et al., 2023a; Momennejad et al., 2023) in natural language, which allows them to interact autonomously (Kinniment et al., 2023) with a wide range of human systems. This overcomes some persistent limitations of classical agents (Bennett & Maruyama, 2022b). LMAs are already being used in finance (Han et al., 2024; Zhou et al., 2024; Yu et al., 2023; Zhang et al., 2024b), politics (Yin et al., 2023), software engineering (Chowdhury et al., 2024; Jimenez et al., 2024), healthcare (Schmidgall et al., 2024; Mehandru et al., 2024; Tu et al., 2024), customer service, legal services and insurance for claims management. They are being talked about as the bedrock of new economies (Forum, 2024; Reuters, 2024) in which autonomous or semi-autonomous agents transact (Journal, 2024), negotiate, organise and act at scale and speed. As a result, frontier AI laboratories (Heaven, 2024) and technology companies (VentureBeat, 2024; Fortune, 2025) are becoming increasingly focused on the use of LMAs (Pit, 2024; PwC, 2024; Forbes, 2024a;b).\nLMAS are expected by some become the default interface or 'control layer' between humans and the cyberphysical systems with which we interact (Andreesen Horowitz, 2023; Ganapathy & Bennett, 2021; Bennett, 2025). Agency is a necessary step in the direction of artificial general intelligence (Goertzel, 2021; Thorisson, 2012; Wang, 2006). If an LMA acts like an agent, humans will tend to treat it as if it is (Urquiza-Haas & Kotrschal, 2015; Bennett, 2023c). Yet an LMA might only mimic human-like behaviour (Floridi & Chiriatti, 2020; Bennett & Maruyama, 2022a). It may not be agentic in the normal sense (Maes, 1994; 1995; Lieberman, 1997; Jennings et al., 1998; Johnson, 2011; Sutton & Barto, 2018; Russell & Norvig, 2021; Chan et al., 2023; Wu et al., 2023; OpenAI, 2018; Gabriel et al., 2024; Kolt, 2024; Lazar, 2024). We will argue the extent to which LMAs are truly agentic is a significant factor in how and where they are useful. We identify pathologies of agency intrinsic to LMAs, and argue these pathologies must be acknowledged and measured if they are to be mitigated. We conclude that LMAs are not agents in the normal sense, and by acting as if they are we limit their usefulness. Their uncanny nature need not be an impediment."}, {"title": "1.1. Differentiation from Related Work", "content": "Considerable attention has been paid to how LLMs hallucinate (Rawte et al., 2023), provide incorrect information (M\u00fcndler et al., 2023) or otherwise act inconsistently. Emerging research is starting to address reliability of LMA capabilities (Kapoor et al., 2024; Liu et al., 2023a;b; Mialon et al., 2023; Lu et al., 2024; Zhang et al., 2024a; Jimenez et al., 2024; Wu, 2024; Chowdhury et al., 2024;"}, {"title": "1.2. Position", "content": "Here we argue that LMAs face an identity crisis. This stems from the LLMs around which they are constructed. LMAs are not normal agents. They have intrinsic pathologies, which we enumerate here. Yet many LMA proposals treat LMAs as something like normal agents. This is both incorrect and normatively undesirable. It engenders a false sense of utility and trustworthiness in LMAs. We do not argue against the use of LMAS. Quite the contrary. Our position is that the identity crisis underpinning LMAs requires attention and, perhaps most importantly, means of scientifically evaluating the extent to which LMAs are ontologically robust and actually satisfy agentic criteria. The pathologies of LMAs should be acknowledged, measured and mitigated. We discuss these issues and sketch out our proposal for agentic evaluation below."}, {"title": "2. Agentic Identity", "content": "To govern an agent of any sort we need to specify what it is, and is not. We need identity criteria. Entities which satisfy identity criteria (in whole or part) are classified as agents. Theories of agency describe such criteria, providing an ontology of agents: what types of agent there are, the classification of their properties and how they may vary. While there is no universal, agreed upon definition of what constitutes an agent e.g. (Franklin & Graesser, 1997; Schlosser, 2019), there are common features. At minimum, an agent must be distinguished from its environment. It must be capable of action, pursuing goals and interacting with the environment responsively in accordance with plans, practical reasoning or intentional states (Russell & Norvig, 2010; Wooldridge & Jennings, 1995). A variety of detailed criteria also exist, such as functionalist, cognitive and legal where agency is a matter of degree rather than a binary property of systems. Different types of agentic systems satisfy agentic criteria to greater or lesser degrees. Our claim in this Position paper is that regardless of which criteria of agency is chosen, LMAs struggle to satisfy it. This is a somewhat contrarian view. LMAs appear on their face to satisfy these criteria in abundance. They seem to exhibit independence, autonomy, reactivity, reasoning capabilities and the capability to act in ways that far exceed traditional computational agents. We are able to easily and readily interact with these systems as if they were agents who can converse, generate plans and execute upon them. It is therefore understandable why the use of LLMs is often framed in terms of agentic concepts, and why it seems natural to do so."}, {"title": "2.2. Ontological Identity Conditions for Agents", "content": "However, as we argue in this Position paper, the appearance of LMAs satisfying criteria of agency is upon deeper investigation problematic. In both philosophical and computational treatments of agency, how we identify agents and their boundaries is critical (see (Olson, 2024) for a review). So too is the ability to demarcate and re-identify the same agent and its properties as persisting over time. Identification of agents is generally treated as prerequisite for attributing higher-level agentic properties to a system, such as the ability to reason, deliberate, plan and execute tasks. We denote the these ontological agent identity conditions as they consist of conditions upon the manner in which agents and their properties are identified."}, {"title": "2.2.1. IDENTIFIABILITY", "content": "The first condition is that LMAs be identifiable. We must be able to (synchronically) identify what an agent is and what it is not. Identifiability requires: (i) a legible and well-formed set of criteria by which to identity an agent (identity criteria); and (ii) a means of determining whether any system or phenomena satisfies this criteria (satisfiability criteria). Identifiability also implies that an agent must be causally distinguishable from its environment and other agents (Bennett, 2022). In other words an observer must be able to construct a means of discriminating between the environment, and those parts of that environment we call the agent: 'causal identities' classifying the agent's causal interventions, and the 1st order effects of agentic properties such as autonomy, planning, reasoning, perception (Bennett, 2023b; Bennett et al., 2025)."}, {"title": "2.2.2. CONTINUITY", "content": "LMAs should maintain their (diachronic) identity over time, even a very short time (Bratman, 2000), throughout their instantiation. In the extreme case, an agent that exhibits no continuity from one time step to another is impossible to identify. But an agent also ought to be adaptable. It"}, {"title": "2.2.3. PERSISTENCE", "content": "LMA ontology conditions also imply a degree of persistence (be it intentional (Dennett, 1971) or psychological (Parfit, 1984)). We distinguish persistence from continuity as the maintenance of an LMA's identity across different instantiations (distinct from a single session or instance). Persistence also implies that the properties of an agent and its identity ought not to be so sensitive to perturbations as to dissolve or radically alter. If LMAs do not exhibit persistence (e.g. where the same prompt sequencing and scaffolding lead to quite different outputs), it calls into question whether an instance of an agent was anything more than the stochastic output of the LLM itself. If we have persistence then an observer should, in theory, be able to construct a reasonably specific classifier or causal identity denoting the LMA across instantiations, based upon its behaviour\u00b9."}, {"title": "2.2.4. CONSISTENCY", "content": "Consistency refers to the coherence of an agent's description and actions according to which it is identified. An agent's state description - and that of any of its properties - should not be contradictorily described. It should not be described by a predicate and its negation, or by inconsistent outputs. An LMA prompt or trace riddled with contradictions would undermine most attempts to identify it as an agent. Consistency also has another sense in terms of consistency of objectives and actions with intentions.\nThe ontological identity conditions above are not unique to agents. Nor are they sufficient to constitute something as an agent. However, they are necessary preconditions of agency. For human agents, satisfaction of identity conditions is anchored in physical embodiment and cognitive unity(Parfit, 1984; Dennett, 1971). For artificial agents which lack physical embodiment, such as corporations (List & Pettit, 2011), it occurs by way of stable institutional practices, such as the law. For classical computational agents (Wooldridge, 2009), such as those based upon formal systems, it occurs via their relatively fixed ontologies of limited scope. But these necessary conditions are no longer guaranteed to be satisfied in case of LMAs. This in turn means that whether LMAs satisfy the properties that causes us to classify them as normal agents - such as reasoning, planning, autonomy, reactivity - may be called into question. This can be seen by considering how LMAs are constituted using LLMs and the structural scaffolding, such as memory, tool use and infrastructure, that supports them."}, {"title": "3. LLM Pathologies", "content": "The foundation of an LMA is an LLM that takes prompts and outputs text (or other data modalities) conditioned on those queries. This procedure is undertaken post-training, at inference stage. While LMAs can be described at different levels of abstraction from the micro (model) to macro (output) level - behaviour that appears recognisably agentic only arises at the macro scale. We cannot reliably identify agentic behaviour at the level of activations for example. We are therefore primarily reliant upon on macro-level outputs of LLMs and scaffolding around them to infer whether these systems meet agentic criteria and identity conditions.\nBecause of the way transformer models are designed (Radford et al., 2019), LLMs that form the bedrock of LMAs exhibit exhibit distinct characteristics of being stateless, stochastic, semantically sensitive and linguistically intermediated. These properties are integral to their computational power and versatility. But they also are the source of instability and uncertainty when it comes to LMA identity. For this reason and in the context of their effect on LMA identity, we denote them LLM pathologies."}, {"title": "3.1. Statelessness", "content": "First, LLMs are stateless. They do not store a persistent record of prompts and outputs from one interaction to the next (Merrill et al., 2024; Vaswani et al., 2017). A query and response exist in isolation unless additional context is explicitly included. Consequently, there is no classical concept of state transition within an LLM at inference. This is in sharp distinction to traditional agents whose evolution over time is represented via changes of state. LLMs are often discussed in terms of inhering world models (Hao et al., 2023; Nottingham et al., 2023; Wong et al., 2023; Brooks et al., 2024), but this framing is problematic. LLMs do allow construction of representations of worlds, but lack the usual concept of state integral to their continuity and persistence. This ephemeral nature of LLM facilitates broad reusability: the same model can handle a plethora if different queries without being restricted to a particular context. Yet this statelessness compromises persistence of identity or memory and can give rise to inconsistencies. If the user does not reinsert context, the LLM will not recall commitments or preferences from earlier exchanges (Merrill et al., 2024). This is in contrast to traditional agents which are stateful in some way (such as being described by states). Even formal computational agents usually have an identifiable system state at the level of computation (e.g. the configuration of a circuit at a point in time)."}, {"title": "3.2. Stochasticity", "content": "Second, LLMs are stochastic (Bender et al., 2021; Li et al., 2023; Cui & Yu, 2024). Repeating the same query can yield different or incorrect outputs (Ferrando et al., 2024). This unpredictability makes it difficult to distinguish stable traits that might serve as evidence of a coherent agent across time. Attempts to dampen this variability, such as adjusting temperature parameters, can have mixed results. LLMs generate output by sampling from probability distributions over tokens (Vaswani et al., 2017). This stochasticity facilitates creativity and divergent problem-solving, but it also means repeated prompts might yield contradictory outputs. While agentic theory expects a measure of autonomy, it typically presupposes consistency over identical conditions, which is difficult to guarantee when the system's next token is partially determined by random sampling."}, {"title": "3.3. Semantic Sensitivity", "content": "Third, LLMs exhibit high semantic sensitivity. Small perturbations in input wording can accumulate into significantly divergent outputs (Wang, 2023; Zhu et al., 2023). Saturating an LMA with different context can alter its core properties in ways that do not occur with normal agents. This phenomenon is seen in jailbreaking, adversarial attacks (McDermott et al., 2023; Moradi & Samwald, 2021; Wang et al., 2023b) or contradictory responses (Zhang et al., 2024c) where slight prompt modifications can produce unexpected or inconsistent behaviour despite safeguards (Mei, 2023). It is also discernible in what we can denote as context attrition (Shi et al., 2023; Leng et al., 2024), the fact that as we layer more and more context into query, the weight ascribed to features (such as agentic features) can diminish. Seemingly trivial changes in phrasing can swing an LLM's response dramatically (Moradi & Samwald, 2021; McDermott et al., 2023). An LLM's training on extensive text corpora can render it highly reactive to minor context shifts. Although this property can be harnessed for precise prompt engineering, it also endows LLMs with a sensitivity that can undermine the stability of its agentic properties, such as goals, reasoning, planning and execution which can all be modified by subtle textual cues."}, {"title": "3.4. Linguistic Intermediation", "content": "Fourth, LLMs are linguistically intermediated. Everything is filtered through to tokens of text and embedding-space representations (Shanahan, 2024): agent definitions, environment descriptions, actions, events, and prompts. This is a form of computational dualism, meaning interaction between the LLM and the LMA's environment is filtered through an 'abstraction layer' and subject to its interpretation (Bennett, 2024). In interactive settings this can undermine constraints upon or claims regarding behaviour. For example, the general reinforcement learning agent AIXI was initially thought to be Pareto optimal (Hutter, 2010). This claim was undermined when the agent's performance was shown to hinge upon a choice of universal Turing machine (Leike & Hutter, 2015). Likewise, natural language constitutes an additional abstraction layer that separates the software 'mind' of an LMA from the environment in which it pursues goals. All information is expressed in tokens. This is certainly nothing like how interaction occurs traditionally where agents perceive, adapt and act. Nor does it resemble how biological self-organising systems enact cognition within the world (Thompson, 2007; Ciaunica et al., 2023; Friston et al., 2023; Bennett, 2025). Rather than sensing objects or states, the LLM receives tokenized descriptions of them and responds in kind. Some information may be lost. Adaptation is separated from embodiment by an abstraction layer, which can potentially reduce efficiency (Bennett, 2024; 2025). Unintended ambiguities or adversarial phrasing thus can shift the LLM's perceived reality, undermining agentic boundaries and consistent situational awareness."}, {"title": "3.4.1. LLM-ONLY LMAS", "content": "We can see how LLM pathologies problematise LMAs by considering the simplest model of an LMA: an LLM-Only agent which operates purely via repeated calls to an underlying language model, with no cumulative context, nor external memory, nor tools. In realistic LMA scenarios, they are equipped by scaffolding, but this scenario is useful to illustrate our point. In a single prompt-response situation (with no memory and no caching as is common on LLM platforms like ChatGPT), there is minimal data to establish identity criteria upon which to identify an agent at all: only a solitary trace in the form of a query-output tuple. The boundaries of any purported agent are inseparable from this record. Individuating an agent using text data alone is difficult. It is therefore unclear how to distinguish such an agent from (i) the LLM itself, (ii) its user prompts, or (iii) the environment described by the text.\nBecause the LLM is stateless, the system lacks any built-in mechanism for maintaining continuity of decisions or output over time. Nor do we see meaningful guarantees of consistency through repeated interactions. An essential element of any identity criteria is that it provides a means of being able to identify the same thing by its repeated application. But repeating the same prompt may yield different outputs. This is due to the stateless and stochastic nature of LLMs in concert with linguistic intermediation. For traditional agents, consistent responses arise because of a state which is unaffected by the act of querying. This might be physical or ontological, in the sense of classes and rules which constrain the agent. This is not the case for LLMs where the query can instrumentally affect the ontology of the LMA in ways very different from normal agents.\nSemantically similar inputs (e.g. schemas for designating an LMA) which vary slightly may lead to large differences in output, upon which an LMA's properties are inferred. If two queries are constructed with the intention of referring to the same agent, there is no way to determine if they refer to the same underlying LMA, or two different instantiations. From an observer's perspective, there is no 'causal identity' denoting interventions by a particular agent (Bennett, 2023b; Pearl & Mackenzie, 2018). Multiple LMAs could be running on top of the same LLM, making it difficult to distinguish an LMA from the LLM, or different LMAs from each other. We might try to unify multiple outputs into a coherent narrative of a single agent. But the inherent stochasticity of LLMs hinders consistency across queries. These features of LLMs challenge the identifiability of LMAs. They make it uncertain whether we can confidently infer that the same agent is persisting from one interaction to the next."}, {"title": "3.4.2. LLMS WITH CONTEXT", "content": "Consider the next simplest model: an LLM-only LMA but where context is added. This is a common strategy in response for achieving the semblance of persistent agency and continuous interactivity. Context includes the history or summary of previous outputs (Zhang et al., 2019; Gekhman et al., 2023). Adding context does enhance the consistency of responses across multiple prompts and enable more coherent conversations. Yet the underlying LLM remains stateless. The prompts merely carry forward relevant text from earlier exchanges. It does not solve the deeper problems of stochasticity and semantic sensitivity. Outputs remain probabilistic. Small perturbations in context can produce large and unpredictable changes in output. Consequently, any apparent persistence of an LMA is affected by how a user curates, summarises, or appends prior outputs, rather than a property of the LLM itself. If the appended history is incomplete or semantically altered, previous decisions might be lost or reversed. Text-based context can be rearranged or truncated, making it difficult to track the same agent's boundaries across repeated interactions. Likewise, the distinguishability of LMAs is rendered uncertain when many agents share overlapping contexts. These problems undermine any strong notion of agentic continuity simply by adding context."}, {"title": "4. LMA Scaffolding", "content": "In an attempt to overcome these limitations, architectural scaffolding can be used:\n1. Memory (Wang et al., 2023a; Huang et al., 2023b; Zhang et al., 2024d), such as in the form of browser caching, databases or other information registers which enable the retention of information; and\n2. Tools (Schick et al., 2023b; Lu et al., 2024), which serve as extensions that enable the LMA to effectually act via interaction with other external systems, such as via executing code, or control physical devices.\n3. Planning (Huang et al., 2024b) as a separate and distinct module (planning is in practice manifest via a combination of memory, tool use and prompting).\n4. Infrastructure (Horowitz, 2023; Chan et al., 2025), this may include containerised instances of LMAs e.g. via Docker (Docker, 2023), or ecosystems such as cloud technology stacks, or distributed networks.\nThe purpose of scaffolding is twofold: to overcome underlying LLM pathologies and to provide LMAs with agentic capabilities, such as being able to use tools to perform tasks, or long-term memory for planning and reasoning. The modularity nature of scaffolding means there are numerous possible configurations of LMA architectures. But as we show below, despite considerable improvement in robustness, versatility and utility, scaffolding does not fully address the underlying effect of LLM pathologies in LMA identity."}, {"title": "4.1. Memory Mechanisms", "content": "Firstly, we examine the effect of memory scaffolding on LMA ontology. Consider an LMA constituted by an LLM with external memory modules or additional storage (Zhang et al., 2024d). This is a common approach in attempting to overcome the statelessness of LLMs. Memory may take the form of browser caches, external databases, or specialised vector stores that maintain relevant text, summaries of previous actions, and user interactions (Wang et al., 2023a; Zhong et al., 2024). Memory mechanisms aim to instantiate a degree of persistence, allowing the LMA to reference past states or decisions. This may be to enable the LMA to perform multi-turn tasks or maintain context across longer interactions, as is required for chain-of-thought reasoning (Wei et al., 2023). However, the LLM itself is never truly updated by these memory modules. Memory is just another form of context. The LLM simply ingests more data as part of each query. As a result, continuity and distinguishability of LMAs rely heavily on how that memory is orchestrated. A memory store might contain a detailed record of previous interactions, but any subsequent LLM-generated output can still deviate substantially from that record. With minor prompt alterations, context attrition in long-term memory (Dannenhauer, 2023) or noisy retrieval, LLM outputs may contradict prior statements (M\u00fcndler et al., 2023) or lose saliency, compromising the consistency of the LMA over time.\nExternal-memory also complicates LMA identifiability. Two or more LMAs can share the same LLM, but point to different (or partially overlapping) memory stores. In this case, it is unclear whether we have multiple distinct LMAs or just different views of the same underlying system. A single LMA can dynamically switch or shuffle memory modules depending on relevance. This can undermine its identity by causing it to become reconfigured in ways that break continuity. Memory scaffolding can improve user-facing coherence of LMAs, particularly for extended, multi-turn applications. But it falls short of guaranteeing the stable boundaries and consistent identity demanded by the traditional criteria of agency."}, {"title": "4.2. Tool Use and API Integration", "content": "The second cornerstone of LMA scaffolding is tool use and integration. When LMAs gain the ability to invoke external tools, they extend their reach into broader environments. Tools allow LLM textual outputs (e.g. code) to trigger real actions (Schick et al., 2023a; Bran et al., 2023; Ruan et al., 2023) and interact with the environment. Tool use can improve LMA performance (Gou et al., 2023). It typically requires adherence to schemas or formal inputs which can also enhance predictability. In many cases, tool use can be readily traced. If an LMA calls an API with specific parameters, we can record that event in more structured logs. This can yield a partial audit trail (Waiwitlikhit et al., 2024; M\u00f6kander et al., 2023), or 'trace' of actions. This is often how practical agentic applications frame LMA identification (Chase, 2022; Wu et al., 2023). LLMs can also autonomously accumulate tools when paired with actuating environments, like SDEs, for access to external services or code execution (Schick et al., 2023b; Lu et al., 2024). This can include creating full applications and orchestrations among multiple applications. The agent can call an API, parse the response, and incorporate the result into its output. This leaves a considerable depth of trace data which is often used to identify LMAs and often fosters the appearance of agentic autonomy.\nYet tool use (or creation by LLMs) remains linguistically intermediated and subject to stochastic generation and semantic fragility. An inadvertent or malicious prompt can steer the model to misuse tools or produce nonsensical commands. The model's environment is mediated entirely by language, leaving it susceptible to manipulations that exploit linguistic oversights. In any case, tool use does not solve the fundamental problems facing LMA identity. The LLM remains free to generate varying tools or contradictory tool-calling directives. Tool use may be of varying accuracy or quality (Lu et al., 2024; Furuta et al., 2023). The same textual instruction that guided a prior tool operation might in another context trigger some other action, eroding consistency of the LMA's actions over time. Multiple LMAs (e.g. within the same session or instantiated in a single prompt) might share tools, further blurring where one LMA ends and another begins. The policies and tools which are used by LMAs may also be ambiguous. The action space of an LMA may be difficult to discern. Complicated coding structures integrated within the LMA itself may make tools difficult to distinguish from the LMA itself. Semantic sensitivity can mean that tool use can be subject to prompt injection (Zhan et al., 2024) and adversarial attacks in unexpected ways. As such, sophisticated tool integration does not eliminate the deeper problems of a lack of LMA continuity or how to unify the LMA into a single identifiable agent with stable boundaries."}, {"title": "4.3. Cognitive Architectures and Planning Modules", "content": "More elaborate scaffolding frameworks introduce chain-of-thought prompting, hierarchical planning modules, or meta-level reflection (Wei et al., 2023; Valmeekam et al., 2023a). These can yield more systematic reasoning steps, reduce shallow guesswork, and encourage the LLM to \"explain\" intermediate decisions. Chains of thought are usually claimed to mirror an agent's deliberative processes (Wei et al., 2023), but this is known to be problematic (Turpin et al., 2024). Despite impressive gains in reliability, these methods still revolve around the LLM and remain exposed to its pathologies. A single contradictory token can unravel the entire plan. Where standard agents update an internal state following each step, an LLM may simply produce textual placeholders of state, which, if inconsistent or corrupted, can lead to inconsistent reasoning.\nThese characteristics of memory integration mean that proposed cognitive architectures such as COALA (Sumers et al., 2023) which promise more elaborate internal processes for reasoning and planning (such as short-term working memory, long-term semantic and episodic stores, and procedural modules) are problematised. LMAs have no mental states per se in any traditional sense of the word. Memory scaffolding doesn't change this. An LMA's mental state is just an inference made upon using the cumulative trace of its outputs, something we infer as resembling a state of mind."}, {"title": "5. Alternative Views", "content": "Our arguments above are premised upon a close analysis of how the features of LLMs that underpin LMAs give rise to pathological effects which propagate in ways that challenge the claim that LMAs are normal agents. Yet there are a range of alternative views and challenges to our claims, ranging from questioning whether the same criticisms might be levelled at traditional agents, to whether LLM properties such as statelessness and stochasticity may be present within normal agents also. We focus on two primary alternative views below."}, {"title": "5.1. LLM pathologies are not unique", "content": "An alternative view to the one we offer is that the problems of LMA identity are not unique to LMAs and could be said of almost any agent to some degree. For example, it may be objected that LLM weights preserve information in essentially the same way as memory and that querying via prompts approximates the process of memory itself. Responses to queries are not usually wildly chaotic. They might exhibit some deviations in edge cases, but they are reliably salient. The retention of information via weights is clearly true of LLMs. But it is currently difficult or impossible to discern at the activation exactly where or what memories reside. Information may be held in superpositions (Henighan et al., 2023; Elhage et al., 2022) that are difficult to disentangle and impose boundaries upon. And even then such information can still be unreliable or subject to hallucination (Ferrando et al., 2024). The reliable saliency of outputs can be relatively easily undermined. The same is true of representations of agency. LLM weights do not provide the sort of stable basis of memory we would expect of typical agents. While internal and external memory can and does improve planning abilities, LMAs can still exhibit inconsistency and unpredictability between plans and task execution (Mallen et al., 2023; Valmeekam et al., 2023a;b). Cognitive structuring such as COALA is therefore at base simply more intricate context architecture. It does not address the confounding of LMA identity and boundaries that arise from LLM pathologies."}, {"title": "5.2. LMA identity problems are inconsequential", "content": "A second objection to our position is that the LMA identity and ontology are largely irrelevant because all that matters is functionality. The extent to which LLM pathologies confound the identity conditions underpinning agency criteria is largely an empirical question. It may be that in the future models are developed to overcome such issues e.g. embedding statefulness in some way. In certain cases, these problems may be of limited consequence. But for complex planning tasks, especially where stakes are high, the consequences may be considerable. It may be objected that variation in how an agent is identified is inconsequential. After all, humans, corporations and other traditional agents exhibit variation in their attributes, yet their agency is not called into question. While it is true that traditional agents do indeed vary across their properties and states, the way in which they satisfy identity conditions is not grounded in anything like an LLM. A human's persistence is irrespective of how they are described. A corporation's persistence depends upon persistent reliable legal practices. A classical computational agent upon formal logically-instantiated code."}, {"title": "5.3. Scaffolding can mitigate but not cure the LMA identity crisis", "content": "Although scaffolding strategies partially mask or mitigate the pathologies, they cannot eliminate them without compromising the generative breadth that make LLMs so versatile in the first place. Memory modules, APIs, and multi-step reasoning each rely upon textual input and output; any change in assumptions encoded within scaffolding can expose underlying unpredictability. Consequently, LMAs remain constrained by the same design choices that underlie their flexible creativity. In our view, this actually highlights a feature that we conjecture is generic about LLMs and LMAs in general: that there is a necessary trade-off between the ontological stability of such systems on the one hand and their power on the other. The more ontologically rigid a system, by definition the less variance. In this sense, the conundrum of LMAs is in analogous spirit with no free lunch theorems or typical bias-variance trade-off where the more deterministic a system, the more identifiable, continuous and consistent it is, but at the cost of less expressivity, generalisability or versatility."}, {"title": "6. Consequences and Responses", "content": "There is usually little emphasis upon these foundational identity challenges faced by LMAs arising from their instantiation upon LLMs. Often uncertainty over LMAs is referred to in the small print or relegated to discussion of edge cases. The paradigm of LMAs for developers and consumers remains that of normal agents. But LMAs are not normal agents. And the effects of LLM pathologies are not edge cases. They are inherent to any LMA architecture and it is very unclear whether they can be remedied, because they stem from transformer models on which they are based."}, {"title": "6.1. Reliability and Predictability"}]}