{"title": "FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting", "authors": ["Wenzhen Yue", "Yong Liu", "Xianghua Ying", "Bowei Xing", "Ruohao Guo", "Ji Shi"], "abstract": "This paper presents FreEformer, a simple yet effective model that leverages a Frequency Enhanced Transformer for multivariate time series forecasting. Our work is based on the assumption that the frequency spectrum provides a global perspective on the composition of series across various frequencies and is highly suitable for robust representation learning. Specifically, we first convert time series into the complex frequency domain using the Discrete Fourier Transform (DFT). The Transformer architecture is then applied to the frequency spectra to capture cross-variate dependencies, with the real and imaginary parts processed independently. However, we observe that the vanilla attention matrix exhibits a low-rank characteristic, thus limiting representation diversity. This could be attributed to the inherent sparsity of the frequency domain and the strong-value-focused nature of Softmax in vanilla attention. To address this, we enhance the vanilla attention mechanism by introducing an additional learnable matrix to the original attention matrix, followed by row-wise L1 normalization. Theoretical analysis demonstrates that this enhanced attention mechanism improves both feature diversity and gradient flow. Extensive experiments demonstrate that FreEformer consistently outperforms state-of-the-art models on eighteen real-world benchmarks covering electricity, traffic, weather, healthcare and finance. Notably, the enhanced attention mechanism also consistently improves the performance of state-of-the-art Transformer-based forecasters.", "sections": [{"title": "1 Introduction", "content": "Multivariate time series forecasting holds significant importance in real-world domains such as weather [Wu et al., 2023b], energy [Zhou et al., 2021], transportation [He et al., 2022] and finance [Chen et al., 2023]. In recent years, various deep learning models have been proposed, significantly pushing the performance boundaries. Among these models, Recurrent Neural Networks (RNN) [Salinas et al., 2020], Convolutional Neural Networks (CNN) [Bai et al., 2018;\nWu et al., 2023a], LLM [Zhou et al., 2023; Jin et al., 2021], Multi-Layer Perceptrons (MLP) [Zeng et al., 2023; Xu et al., 2023], Transformers-based methods [Nie et al., 2023; Liu et al., 2024a; Wang et al., 2024c] have demonstrated great potential due to their strong representation capabilities.\nIn recent years, frequency-domain-based models have been proposed and have achieved great performance [Yi et al., 2024c; Xu et al., 2023], benefiting from the robust frequency domain modeling. As shown in Figure 1, frequency spectra exhibit strong consistency across different spans of the same series, making them suitable for forecasting. Most existing frequency-domain-based works [Yi et al., 2024a] rely on linear layers to learn frequency-domain representations, resulting in a performance gap. Frequency-domain Transformer-based models remain under-explored. Recently, Fredformer [Piao et al., 2024] applies the vanilla Transformer to patched frequency tokens to address the frequency bias issue. However, the patching technique introduces additional hyper-parameters and undermines the inherent global perspective [Yi et al., 2024c] of frequency-domain modeling."}, {"title": "2 Related Works", "content": "In this paper, we adopt a simple yet effective approach by applying the Transformer to frequency-domain variate tokens for representation learning. Specifically, we embed the entire frequency spectrum as variate tokens and capture cross-variate dependencies among them. This architecture offers three main advantages: 1) As shown in Section 3, simple frequency-domain operations can correspond to complex temporal operations [Yi et al., 2024c]; 2) Inter-variate correlations typically exists (e.g., Figure 1) [Liu et al., 2024a], and and learning these correlations could be beneficial for more robust frequency representation; 3) The permutation-invariant nature of the attention mechanism naturally aligns with the order-insensitivity of variates.\nFurthermore, we observe that for the frequency-domain representation, the attention matrix of vanilla attention often exhibits a low-rank characteristic, which reduces the diversity of representations. To address this issue, we propose a general solution: adding a learnable matrix to the original softmax attention matrix, followed by row-wise normalization. We term this approach enhanced attention and name the overall model FreEformer. Despite its simplicity, the enhanced attention mechanism is proven effective both theoretically and empirically. The main contributions of this work are summarized as follows:\n\u2022 This paper presents a simple yet effective model, named FreEformer, for multivariate time series forecasting. FreEformer achieves robust cross-variate representation learning using the enhanced attention mechanism.\n\u2022 Theoretical analysis and experimental results demonstrate that the enhanced attention mechanism increases the rank of the attention matrix and provides greater flexibility for gradient flow. As a plug-in module, it consistently enhances the performance of existing Transformer-based forecasters.\n\u2022 Empirically, FreEformer consistently achieves state-of-the-art forecasting performance across 18 real-world benchmarks spanning diverse domains such as electricity, transportation, weather, healthcare and finance."}, {"title": "2.1 Transformer-Based Forecasters", "content": "Classic works such as Autoformer [Wu et al., 2021], Informer [Zhou et al., 2021], Pyraformer [Liu et al., 2022b], FEDformer [Zhou et al., 2022b], and PatchTST [Nie et al., 2023] represent early Transformer-based time series forecasters. iTransformer [Liu et al., 2024a] introduces the inverted Transformer to capture multivariate dependencies, and achieves accurate forecasts. More recently, research has focused on jointly modeling cross-time and cross-variate dependencies [Zhang and Yan, 2023; Wang et al., 2024c; Han et al., 2024]. Leddam [Yu et al., 2024] uses a dual-attention module for decomposed seasonal components and linear layers for trend components. Unlike previous models in the time domain, we shift our focus to the frequency domain to explore dependencies among the frequency spectra of multiple variables for more robust representations."}, {"title": "2.2 Frequency-Domain Forecasters", "content": "Frequency analysis is an important tool in time series forecasting [Yi et al., 2023]. FEDformer [Zhou et al., 2022b] performs DFT and sampling prior to Transformer. DEPTS [Fan et al., 2022] uses the DFT to capture periodic patterns for better forecasts. FiLM [Zhou et al., 2022a] applies Fourier analysis to preserve historical information while mitigating noise. FreTS [Yi et al., 2024c] employs frequency-domain MLPs to model channel and temporal dependencies. FourierGNN [Yi et al., 2024b] transfers GNN operations from the time domain to the frequency domain. FITS [Xu et al., 2023] applies a low-pass filter and complex-valued linear projection in the frequency domain. DERITS [Fan et al., 2024] introduces a Fourier derivative operator to address non-stationarity. Fredformer [Piao et al., 2024] addresses frequency bias by dividing the frequency spectrum into patches. FAN [Ye et al., 2024] introduces frequency adaptive normalization for non-stationary data. In this work, we adopt a simple yet effective Transformer-based model to capture multivariate correlations in the frequency domain, outperforming existing methods."}, {"title": "2.3 Transformer Variants", "content": "Numerous variants of the vanilla Transformer have been developed to enhance efficiency and performance. Informer [Zhou et al., 2021] introduces a ProbSparse self-attention mechanism with O(NlogN) complexity. Flowformer [Wu et al., 2022] proposes Flow-Attention, achieving linear complexity based on flow network theory. Reformer [Kitaev et al., 2020] reduces complexity by replacing dot-product attention with locality-sensitive hashing. Linear Transformers, such as FLatten [Han et al., 2023] and LSoftmax [Yue et al., 2024], achieve linear complexity by precomputing KTV and designing various mapping functions. FlashAttention [Dao et al., 2022] accelerates computations by tiling to minimize GPU memory operations. LASER [Surya Duvvuri and Dhillon, 2024] mitigates the gradient vanishing issue using exponential transformations. In this work, we focus on the low-rank issue and adopt a simple yet effective strategy by adding a learnable matrix to the attention matrix. This improves both matrix rank and gradient flow with minimal modifications to the vanilla attention mechanism."}, {"title": "3 Preliminaries", "content": "The discrete Fourier transform (DFT) [Palani, 2022] converts a signal \\(x \\in \\mathbb{R}^{N}\\) into its frequency spectrum \\(F \\in \\mathbb{C}^{N}\\). For \\(k = 0,1,..., N - 1\\), we have\n\\begin{equation}\nF[k] = \\sum_{n=0}^{N-1} e^{-jnk}x[n].\n\\end{equation}\nHere, \\(j\\) denotes the imaginary unit. For a real-valued vector \\(x, F[k]\\) is complex-valued and satisfies the property of Hermitian symmetry [Palani, 2022]: \\(F[k] = (F[N - k])^*\\) for \\(k = 1, \\ldots, N - 1\\), where \\((\\cdot)^*\\) denotes the complex conjugate. The DFT is a linear and reversible transform, with the inverse discrete Fourier transform (IDFT) being:"}, {"title": "4 Method", "content": "In multivariate time series forecasting, we consider historical series within a lookback window of \\(T\\), each timestamp with \\(N\\) variates: \\(x = {X_1,\\ldots, X_T} \\in \\mathbb{R}^{N \\times T}\\). Our task is to predict future \\(\\tau\\) timestamps to closely approximate the ground truth \\(y = {X_{T+1},\\ldots, X_{T+\\tau}} \\in \\mathbb{R}^{N \\times \\tau}\\)."}, {"title": "4.1 Overall Architecture", "content": "As shown in Figure 2, FreEformer employs a simple architecture. First, an instance normalization layer, specifically RevIN [Kim et al., 2021], is used to normalize the input data and de-normalize the results at the final stage to mitigate non-stationarity. The constant mean component, represented by the zero-frequency point in the frequency domain, is set to zero during normalization. Subsequently, a dimension extension module is employed to enhance the model's representation capabilities. Specifically, the input \\(x\\) is expanded by a learnable weight vector \\(d_a \\in \\mathbb{R}^d\\), yielding higher-dimensional and more expressive series data: \\(x' = x \\times d_a \\in \\mathbb{R}^{N \\times d \\times T}\\). We refer to \\(d\\) as the embedding dimension.\nFrequency-Domain Operations Next, we apply the Discrete Fourier Transform (DFT) to convert the time series \\(x'\\) into its frequency spectrum along the temporal dimension:\n\\begin{equation}\nF = DFT(x') = Re(F) + j \\cdot Im(F) \\in \\mathbb{C}^{N \\times d \\times T},\n\\end{equation}"}, {"title": "4.2 Enhanced Attention", "content": "In the Transformer block, as shown in Figure 2(b), we first employ the attention mechanism to capture cross-variate dependencies. Then the LayerNorm and FFN are used to update frequency representations in a variate-independent manner. According to Theorem 1, the FFN corresponds to a series of convolution operations in the time domain for series representations. The vanilla attention mechanism is defined as:\n\\begin{equation}\nAttn(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{D}})V.\n\\end{equation}\nHere, \\(Q, K, V \\in \\mathbb{R}^{N \\times D}\\) are the query, key and value matrix, respectively, obtained through linear projections. We denote \\(D\\) as the feature dimension and refer to \\(Softmax(QK^T /\\sqrt{D})\\), represented as \\(A\\), as the attention matrix.\nHowever, as shown in Figure 3, compared to other state-of-the-art forecasters, FreEformer with the vanilla attention mechanism usually exhibits an attention matrix with a lower rank. This could arise from the inherent sparsity of the frequency spectrum [Palani, 2022] and the strong-value-focused properties of the vanilla attention mechanism [Surya Duvvuri and Dhillon, 2024; Xiong et al., 2021]. While patching adjacent frequency points can mitigate sparsity (as in Fredformer), we address the underlying low-rank issue within the attention mechanism itself, offering a more general solution."}, {"title": "5.1 Forecasting Performance", "content": "In this work, we adopt a straightforward yet effective solution: introducing a learnable matrix B to the attention matrix. The enhanced attention mechanism, denoted as EnhAttn (Q, K, V), is defined as 1:\n\\begin{equation}\nNorm \\left(Softmax(\\frac{QK^T}{\\sqrt{D}}) + Softplus(B)\\right)V,\n\\end{equation}\nwhere Norm() denotes row-wise L1 normalization. The Softplus() function ensures positive entries, thereby preventing potential division-by-zero errors in Norm(.)."}, {"title": "Theoretical Analysis", "content": "We choose 10 well-acknowledged deep forecasters as our baselines, including (1) Transformer-based models: Leddam [Yu et al., 2024], CARD [Wang et al., 2024c], Fredformer [Piao et al., 2024], iTransformer [Liu et al., 2024a], PatchTST [Nie et al., 2023], Crossformer [Zhang and Yan, 2023]; (2) Linear-based models: TimeMixer [Wang et al., 2024b], FreTS [Yi et al., 2024c] and DLinear [Zeng et al., 2023]; (3) TCN-based model: TimesNet [Wu et al., 2023a].\nComprehensive results for long- and short-term forecasting are presented in Tables 2 and 3, respectively, with the best results highlighted in bold and the second-best underlined. FreEformer consistently outperforms state-of-the-art models across various prediction lengths and real-world domains. Compared with sophisticated time-domain-based models, such as Leddam and CARD, FreEformer achieves superior performance with a simpler architecture, benefiting from the global-level property of the frequency domain. Furthermore, its performance advantage over Fredformer, another Transformer- and frequency-based model, suggests that the deliberate patching of band-limited frequency spectra may introduce unnecessary noise, hindering forecasting accuracy.\nNotably, in Table 4, we compare FreEformer with additional frequency-based models, where it also demonstrates a clear performance advantage. The visualization results of"}, {"title": "Feature Diversity", "content": "According to Equation (9), feature diversity is directly influenced by the rank of the final attention matrix Norm (A + B), where \\(B \\equiv Softplus(B)\\). Since row-wise L1 normalization does not alter the rank of a matrix, we have: rank (Norm (A + B)) = rank (A + B). For further analysis, we present the following theorem:\nTheorem 2. Let A and B be two matrices of the same size N \u00d7 N. The rank of their sum satisfies the following bounds:\n\\begin{equation}\n\\text{rank}(A) - \\text{rank}(B)| \\le \\text{rank}(A + B) \\le \\text{rank}(A) + \\text{rank}(B)\n\\end{equation}\nThe proof is provided in Section B of the appendix. As illustrated in Figure 4, the original attention matrix A often exhibits a low rank, whereas the learned matrix B is nearly full-rank. According to Theorem 2, the combined matrix A + B generally achieves a higher rank. This observation aligns with the results shown in Figure 4."}, {"title": "Gradient Flow", "content": "Let \\(a \\in \\mathbb{R}^N\\) denote a row in \\(QK^T/\\sqrt{D}\\). For vanilla attention, the transformation is \\(a' = Softmax(a)\\). Then the Jacobian matrix of \\(a'\\) regarding a can be derived as:\n\\begin{equation}\n\\frac{\\partial a'}{\\partial a} = \\text{Diag}(a') - a'a'^T,\n\\end{equation}\nwhere Diag(a') is a diagonal matrix with a' as its diagonal. For the enhanced attention, the transformation is given by:\n\\begin{equation}\nc = Norm (Softmax(a) + b),\n\\end{equation}\nwhere b represents a row of Softplus(B). The Jacobian matrices of c with respect to a and b can be derived as:\n\\begin{equation}\n\\frac{\\partial c}{\\partial a} = \\frac{1}{\\|b\\|_1} \\left(\\text{Diag}(a') - a'a'^T\\right),\n\\end{equation}\n\\begin{equation}\n\\frac{\\partial c}{\\partial b} = \\frac{1}{\\|b\\|_1} \\left(\\I - \\frac{b 1^T}{\\|b\\|_1}\\right)\n\\end{equation}"}, {"title": "Combination of MLP and Vanilla Attention", "content": "We now provide a new perspective on the enhanced attention. In Equation (9), the attention matrix is decomposed into two components: the input-independent, dataset-specific term B, and the input-dependent term A. If A is zero, the enhanced attention reduces to a linear transformation of V, effectively functioning as an MLP along the variate dimension. By jointly optimizing A and B, the enhanced attention can be interpreted as an adaptive combination of MLP and vanilla attention."}, {"title": "5 Experiments", "content": "We choose 10 well-acknowledged deep forecasters as our baselines, including (1) Transformer-based models: Leddam [Yu et al., 2024], CARD [Wang et al., 2024c], Fredformer [Piao et al., 2024], iTransformer [Liu et al., 2024a], PatchTST [Nie et al., 2023], Crossformer [Zhang and Yan, 2023]; (2) Linear-based models: TimeMixer [Wang et al., 2024b], FreTS [Yi et al., 2024c] and DLinear [Zeng et al., 2023]; (3) TCN-based model: TimesNet [Wu et al., 2023a]. Comprehensive results for long- and short-term forecasting are presented in Tables 2 and 3, respectively, with the best results highlighted in bold and the second-best underlined. FreEformer consistently outperforms state-of-the-art models across various prediction lengths and real-world domains. Compared with sophisticated time-domain-based models, such as Leddam and CARD, FreEformer achieves superior performance with a simpler architecture, benefiting from the global-level property of the frequency domain. Furthermore, its performance advantage over Fredformer, another Transformer- and frequency-based model, suggests that the deliberate patching of band-limited frequency spectra may introduce unnecessary noise, hindering forecasting accuracy. Notably, in Table 4, we compare FreEformer with additional frequency-based models, where it also demonstrates a clear performance advantage. The visualization results of"}, {"title": "5.2 Model Analysis", "content": "Architecture Ablations The FreEformer utilizes an enhanced Transformer architecture to capture cross-variate dependencies in the frequency domain. Table 5 presents a comparison of several FreEformer variants, evaluating the impact of linear and enhanced Transformer layers, different dimensional configurations, and patching along the frequency dimension. To ensure a fair comparison, the enhanced Transformer is used for all Transformer-based settings. The results indicate that: 1) Enhanced Transformer blocks outperform linear layers due to their superior representation capabilities; 2) Multivariate dependency learning generally outperforms inter-frequency learning, aligning with the claim in FreDF [Wang et al., 2024a] that correlations among frequency points are minimal; 3) Furthermore, patching does not improve FreEformer, likely because patching frequencies creates localized receptive fields, thereby limiting access to global information.\nFrequency-Domain vs. Temporal Representation To construct the time-domain variant of FreEformer, we remove the DFT and IDFT steps, as well as the imaginary branch."}, {"title": "5.3 Enhanced Attention Analysis", "content": "We compare the enhanced Transformer with vanilla Transformer, state-of-the-art Transformer variants and Mamba [Gu and Dao, 2023] in Table 8. The enhanced Transformer consistently outperforms other models, verifying the effectiveness of the enhanced attention mechanism."}, {"title": "6 Conclusion", "content": "In this work, we present a simple yet effective multivariate time series forecasting model based on a frequency-domain enhanced Transformer. The enhanced attention mechanism is demonstrated to be effective both theoretically and empirically. It can consistently bring performance improvements for state-of-the-art Transformer-based forecasters. We hope that FreEformer will serve as a strong baseline for the time series forecasting community."}, {"title": "A Theorem 1 and Its Proof", "content": "Theorem 3 (Frequency-domain linear projection and time-domain convolutions). Given the time series \\(x \\in \\mathbb{R}^{N}\\) and its corresponding frequency spectrum \\(F \\in \\mathbb{C}^{N}\\). Let \\(W \\in \\mathbb{C}^{N \\times N}\\) denote a weight matrix and \\(b \\in \\mathbb{C}^{N}\\) a bias vector. Under these definitions, the following DFT pair holds:\n\\begin{equation}\nF = WF + b \\leftrightarrow \\sum_{i=0}^{N-1} \\Omega_{i} \\otimes M_{i}(x) + IDFT(b),\n\\end{equation}\nwhere\n\\begin{equation}\nw_{i} = [diag(W, i), diag(W, i - N)] \\in \\mathbb{C}^{N},\n\\Omega_{i} = IDFT (w_{i}) \\in \\mathbb{C}^{N},\nM_{i}(x) = x \\otimes e^{\\left[-j \\frac{i k}{N}\\right]} \\in \\mathbb{C}^{N}.\n\\end{equation}\nHere, \\(j\\) is the imaginary unit, \\(\\leftrightarrow\\) denotes the DFT pair relationship, \\(\\otimes\\) represents the circular convolution, \\(\\circledR\\) indicates the Hadamard (element-wise) product, and [,] represents the concatenation of two vectors. \\(diag(W,i) \\in \\mathbb{C}^{N-|i|}\\) extracts the \\(i\\)-th diagonal of \\(W\\), and \\(diag(W, N)\\) is defined as \u00d8. If \\(i = 0\\), \\(diag(W, i)\\) corresponds to the main diagonal; if \\(i > 0\\) (or \\(i < 0\\), it corresponds to a diagonal above (or below) the main diagonal. \\(M_{i}(x)\\) represents the \\(i\\)-th modulated version of \\(x\\), with \\(M_{0}(x)\\) being \\(x\\) itself.\nProof. To prove Theorem 1, we first introduce two supporting lemmas.\n1) A circular shift in the frequency domain corresponds to a multiplication by a complex exponential in the time domain. This can be expressed as follows:\n\\begin{equation}\nRoll(F, -k) \\leftrightarrow x e^{\\left[-j \\frac{k n}{N}\\right]}, n=0,1,\\ldots,N-1,\n\\end{equation}\nwhere \\(Roll(F, -k)\\) denotes a circular shift of \\(F\\) to the left by \\(k\\) elements, e.g., \\(Roll(F, -1) = [f_2,\\ldots, f_N, f_1]^T\\).\nBy combining these results, \\(F = WF\\) can be reformulated as the sum of a series of element-wise vector products:\n\\begin{equation}\nWF = \\sum_{i=0}^{N-1} w_{i} \\odot Roll(F, -i).\n\\end{equation}\nGiven that \\(\\Omega_{i}\\) is defined as the IDFT of \\(w_{i}\\), we derive from Equations (2) and (3) that:\n\\begin{equation}\nWF \\leftrightarrow \\sum_{i=0}^{N-1} \\Omega_{i} \\otimes M_{i}(x).\n\\end{equation}\nBy further applying the linearity property of DFT, we obtain Equation (15), which concludes the proof. \u03a0\nTheorem 1 demonstrates that a linear transformation in the frequency domain is equivalent to a series of convolution operations applied to the time series and its modulated versions."}, {"title": "B Theorem 2 and Its Proof", "content": "Theorem 4 (Rank and condition number of matrix sums). Let A and B be two matrices of the same size N \u00d7 N. Let rank(\u00b7) and \\(\\kappa(\\cdot)\\) represent the rank and condition number of a matrix, respectively. The condition number \\(\\kappa(\\cdot)\\) is defined as \\(\\kappa(\\cdot) = \\frac{\\sigma_{1}(\\cdot)}{\\sigma_{N}(\\cdot)}\\), where \\(\\sigma_{1}(\\cdot)\\) and \\(\\sigma_{N}(\\cdot)\\) are the largest and smallest singular values of the matrix, respectively. The following bounds hold for rank(A + B) and \\(\\kappa(A + B)\\):\n\\begin{equation}\n|\\text{rank}(A) - \\text{rank}(B)| \\le \\text{rank}(A + B) \\le \\text{rank}(A) + \\text{rank}(B)\n\\end{equation}\nand\n\\begin{equation}\n\\frac{\\underset{i=1,\\ldots N}{\\max} |\\sigma_{i}(A) - \\sigma_{i}(B)|}{\\underset{i+j=N+1}{\\min} {\\sigma_{i}(A) + \\sigma_{j}(B)}} \\le \\kappa(A + B) < \\frac{\\sigma_{1}(A) + \\sigma_{1}(B)}{\\max {\\sigma_{N}(A) - \\sigma_{1}(B), \\sigma_{N}(B) - \\sigma_{1}(A), 0}}.\n\\end{equation}\nProof. We first prove the upper and lower bounds for rank(A + B). Upper bound of rank(A + B). Let Col(A) and Col(B) denote the column spaces of A and B, respectively. Naturally, the column space of A + B satisfies\n\\begin{equation}\nCol(A + B) \\subseteq Col(A) \\cup Col(B)\n\\end{equation}\nTherefore, we have\n\\begin{equation}\n\\text{rank}(A + B) = \\text{dim}(Col(A + B))\n\\le \\text{dim}(Col(A) \\cup Col(B))\n\\le \\text{dim}(Col(A)) + \\text{dim}(Col(B))\n= \\text{rank}(A) + \\text{rank}(B).\n\\end{equation}\nLower bound of rank(A + B). Let Null(A) and Null(B) denote the null spaces of A and B, respectively. For any vector \\(x \\in Col(A) \\cap Null(B)\\), we have"}, {"title": "C.1 Gradient (Jacobian) Matrix of Softmax", "content": "\\begin{equation}\n(A + B)x = Ax + Bx = Ax \\neq 0,\n\\end{equation}\ni.e., \\(x \\in Col(A + B)\\). Therefore, it holds that\n\\begin{equation}\nCol(A) \\cap Null(B) \\subseteq Col(A + B),\n\\end{equation}\nwhich implies that\n\\begin{equation}\n\\text{rank}(A + B) = \\text{dim}(Col(A + B))\n\\geq \\text{dim}(Col(A) \\cap Null(B))\n\\geq \\text{dim}(Col(A)) + \\text{dim}(Null(B)) - N\n= \\text{rank}(A) - \\text{rank}(B).\n\\end{equation}\nGiven the equivalence of A and B, we also obtain\n\\begin{equation}\n\\text{rank}(A + B) \\geq \\text{rank}(B) - \\text{rank}(A).\n\\end{equation}\nCombining these results, we have\n\\begin{equation}\n\\text{rank}(A + B) \\geq \\text{max} {(\\text{rank}(B) - \\text{rank}(A), \\text{rank}(A) - \\text{rank}(B)}\n= |\\text{rank}(A) - \\text{rank}(B)|.\n\\end{equation}\nThus, we complete the proof of Equation (23).\nTo analyze the upper and lower bounds of \\(\\kappa(A + B)\\), we first recall Weyl's inequality [Horn and Johnson, 2012], which provides an upper bound on the singular values of the sum of two matrices. Specifically, for the singular values of A, B, and A + B arranged in decreasing order, the inequality states that:\n\\begin{equation}\n\\sigma_{i+j-1}(A + B) \\le \\sigma_{i}(A) + \\sigma_{j}(B),\n\\end{equation}\nfor \\(1 \\le i, j \\le N\\) and \\(i + j \\le N + 1\\), where \\(\\sigma_{1}(\\cdot) \\geq \\sigma_{2}(\\cdot) \\geq \\cdots \\geq \\sigma_{N}(\\cdot)\\) denotes the singular values in non-increasing order. From Equation (32) (Weyl's inequality), we can deduce:\n\\begin{equation}\n\\sigma_{i}(A) \\le \\sigma_{1}(A + B) + \\sigma_{i}(-B),\n\\end{equation}\nwhich implies that:\n\\begin{equation}\n\\sigma_{i}(A + B) \\geq \\sigma_{i+j-1}(A) - \\sigma_{j}(-B)\n= \\sigma_{i+j-1}(A) - \\sigma_{j}(B).\n\\end{equation}"}, {"title": "C.2 Jacobian Matrix of Enhanced Attention", "content": "In the vanilla self-attention mechanism, the softmax is applied row-wise on \\(\\frac{QK^T}{\\sqrt{D}} \\in \\mathbb{R}^{N\\times N}\\). Without loss of generality, we consider a single row \\(a \\in \\mathbb{R}^N\\) in \\(\\frac{QK^T}{\\sqrt{D}}\\) to derive the gradient matrix. Let \\(a' = Softmax(a)\\). Then the \\(i\\)-th element of \\(a'\\) can be written as\n\\begin{equation}\na'_i = \\frac{e^{a_i}}{\\sum_{k=0}^{N-1} e^{a_k}}.\n\\end{equation}\nFrom this, we compute the partial derivative of \\(a'_i\\) with respect to a:\n\\begin{equation}\n\\frac{\\partial a'_i}{\\partial a_i} = \\frac{e^{a_i} \\sum_{k=0}^{N-1} e^{a_k} - e^{a_i} \\cdot e^{a_i}}{(\\sum_{k=0}^{N-1} e^{a_k})^2}\n= a'_i - a'_i a'_i.\n\\end{equation}\nFor \\(j \\neq i\\), the partial derivative is:\n\\begin{equation}\n\\frac{\\partial a'_i}{\\partial a_j} = \\frac{- e^{a_i} \\cdot e^{a_j}}{(\\sum_{k=0}^{N-1} e^{a_k})^2}\n= -a'_i a'_j.\n\\end{equation}\nCombining Equations (38) and (39), the Jacobian matrix can be expressed as:\n\\begin{equation}\n\\frac{\\partial a'}{\\partial a} = \\text{Diag}(a') - a'a'^T,\n\\end{equation}\nwhere Diag(a') is a diagonal matrix with a' as its diagonal, and all other elements set to zero.\nEquation (40) reveals that the off-diagonal elements of the Jacobian matrix are the pairwise products \\(a'_i a'_j\\). Since \\(a'\\) often contains small probabilities [Surya Duvvuri and Dhillon, 2024], these products become even smaller, resulting in a Jacobian matrix with near-zero entries. This behavior significantly diminishes the gradient flow during back-propagation, thereby hindering training efficiency and model optimization."}, {"title": "C.3 Variants of Enhanced Attention", "content": "In this paper", "as": "n\\begin{equation"}, "nc = Norm (Softmax(a) + b),\n\\end{equation}\nwhere Norm(\u00b7) = \\(\\frac{\\cdot}{\\|\\cdot\\|_1}\\) denotes the L1 normalization operator, and \\(b_i > 0\\) for \\(i = 0, \\ldots, N - 1\\). Define \\(b' = Softmax(a) + b\\). Since Softmax(a) is non-negative and \\(b_i > 0\\), it follows that \\(b'_i > 0\\) for all \\(i\\). Consequently, the entries of \\(c\\) can be written as:\n\\begin{equation}\nc_i = \\frac{b'_i}{\\sum_{k=0}^{N-1} b'_k}\n\\end{equation}\nThen, the partial derivative of \\(c_i\\) with respect to \\(b'_i\\) is:\n\\begin{equation}\n\\frac{\\partial c_i}{\\partial b'_i} = \\frac{1}{\\sum_{k=0}^{N-1} b'_k} - \\frac{b'_i}{(\\sum_{k=0}^{N-1} b'_k)^2}\n= \\frac{1}{\\|b'\\|_1} - \\frac{b'_i}{\\|b'\\|_1^2} = \\frac{1}{\\|b'\\|_1} \\left(1 - \\frac{b'_i}{\\|b'\\|_1}\\right).\n\\end{equation}\nFor \\(j \\"]}