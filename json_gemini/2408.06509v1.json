{"title": "Fooling SHAP with Output Shuffling Attacks", "authors": ["Jun Yuan", "Aritra Dasgupta"], "abstract": "Explainable AI (XAI) methods such as SHAP can help discover feature attributions in black-box models. If the method reveals a significant attribution from a \"protected feature\" (e.g., gender, race) on the model output, the model is considered unfair. However, adversarial attacks can subvert the detection of XAI methods. Previous approaches to constructing such an adversarial model require access to underlying data distribution, which may not be possible in many practical scenarios. We relax this constraint and propose a novel family of attacks, called shuffling attacks, that are data-agnostic. The proposed attack strategies can adapt any trained machine learning model to fool Shapley value-based explanations.\nWe prove that Shapley values cannot detect shuffling attacks. However, algorithms that estimate Shapley values, such as linear SHAP and SHAP, can detect these attacks with varying degrees of effectiveness. We demonstrate the efficacy of the attack strategies by comparing the performance of linear SHAP and SHAP using real-world datasets.", "sections": [{"title": "Introduction", "content": "Explainable AI (XAI) methods are increasingly being used to detect unfairness in black-box machine learning models (Arrieta et al. 2020; Das and Rad 2020). For example, suppose the XAI method detects that \u201cprotected features\" such as gender or race significantly contribute to a model's prediction. In that case, it may indicate that the model is unfair.\nExisting work shows adversarial classifiers can be constructed via scaffolding (Slack et al. 2020), which can fool explanations generated by LIME (Ribeiro, Singh, and Guestrin 2016) and SHAP (Lundberg and Lee 2017). Specifically, LIME and SHAP cannot detect that the classifier is making decisions heavily influenced by the \u201cprotected feature\". However, a scaffolding procedure assumes we have access to the underlying dataset, which can be used to train an additional classification model. Such an assumption might not hold good in real-world scenarios where data accessibility is restricted. For example, while the training data for a machine learning model remains confidential, the types of input data (including both protected and unprotected attributes) can often be deduced from the model's user instruction manual. This method of releasing models is frequently seen in large language models like Meta's Llama series.\nIn this work, we focus on scoring functions that assign probability or scores to data items, where the scores can either be used to assign items to different groups (in the case of a classifier) or put them in a particular order (in the case of an algorithmic ranker).\nScoring functions represent a more fine-grained model output than the model predictions themselves. We can turn a scoring function into a classifier by setting thresholds on the score output (e.g., if the score is above a threshold, it belongs to class A, otherwise class B.). Also, when the input of the scoring function is a group of items rather than a single item, the resulting score vector can be used to generate a ranking (indicating the relative preference for the items). Hence, the scoring function can also be considered an intermediate component of an \u201calgorithmic ranker\u201d (Yuan et al. 2023).\nOur contributions can be summarized as follows: 1) We propose and algorithmically demonstrate a number of model output shuffling attacks, 2) Theoretically, we prove that Shapely values cannot detect our proposed attacks. 3) Experimentally, we show the impact of the attack on various detection settings using multiple real-world datasets."}, {"title": "Related Work", "content": "The goal in this work is to modify a base function f without accessing the training data of f (and even the internal structure of f) and achieve adversarial function f' that modifies the outcome of input x \u2208 X. However, an XAI method g(), which requires f as one of the inputs, considers f' to behave similarly to f. In particular, the protected features' attribution is smaller than a certain threshold. Based on a recent survey paper (Baniecki and Biecek 2023) that provides a unified notation and taxonomy of adversarial attacks on XAI, our proposed method can be formulated as:\n$$f \\rightarrow f' \\implies  \\begin{cases} \\forall x \\in X \\qquad g(f,x) \\approx g(f', x)  \\\\ \\exists x \\in X \\qquad f(x) \\neq f'(x)  \\end{cases}$$\n(1)\nThe survey describes other adversarial works in a similar notation as Formula (Eq.1). However, no existing work can be described as formula (Eq.1) exactly. Hence, our method is unique from other works. The survey includes one work (Noppel, Peter, and Wressnegger 2023) that is most similar to our method (Eq.1) but relies on poisoning data used in training (e.g., in a scenario of outsourced training) to trigger wrong outputs for certain input. For the attack methods that construct models without changing data, (Dimanov et al. 2020) attempt to learn a new model that performs similarly to the original but has a much lower target (e.g., gender, race) feature attribution. The resulting new model's behavior is unclear whether improving or degrading the fairness of the original. They use common fairness metrics to conduct post-hoc evaluations to compare the fairness shifts between original and new models. Our method, however, constructs specific unfair behavior of the new model without any training. This allows the attack to be performed not only by the data owners themselves. Instead, the attack can be performed by model distributors or brokers. Other works ((Slack et al. 2020), (Laberge et al. 2023)) attempt to deceive explanation methods using deception mechanisms that exploit the SHAP's permutation strategy on data or data distribution. Our method does not require accessing data or data distribution, training a new model, or changing the data."}, {"title": "Attack Strategies", "content": "Existing works on adversarial attacks on XAI methods assume the adversary action is executed by the model developers (Slack et al. 2020) or the data owners (A\u00efvodji et al. 2022). We consider the case in which the adversary action is executed by the model distributors or the model brokers. Model users may only access the model via the API provided by the model brokers. We assume model brokers (e.g., companies hosting their models using cloud services) do not access or change the model architecture or the underlying training data but may gain input from the model users.\nIn this section, we introduce the Shapley values. Then, we provide the intuition of the shuffling attack and three examples of it. We discuss the connections among the attacks, the relative strength of the attack, and ways to modify the attack to bypass detection."}, {"title": "Shapley values", "content": "Shapley values originated from cooperative game theory. Intuitively, Shapley values attribute payouts to a game's players. Let X contain d random variables corresponding to features in set D. A coalition S is a subset of the total feature set D. The $X_S$ and $X_{\\overline{S}}$ denote in-coalition and out-of-coalition feature variables. Considering different combinations (i.e. coalition) of a team, the average change in the output of the model when a player (i.e. feature) joins the team is its payout or Shapley value:\n$$\\phi_j = \\sum_{S \\subseteq D \\setminus \\{j\\}} \\frac{|S|!(|D| - |S| - 1)!}{|D|!} (v(S \\cup \\{j\\}) - v(S)).$$\n(2)\nBoth S and $S \\cup \\{j\\}$ are subsets of the total feature set D. The difference between v(S) and $v(S \\cup \\{j\\})$ is the payout of individual feature j in one coalition.\nShapley value of feature j is a weighted sum of payout differences between coalition S and $S \\cup \\{j\\}$, for weight is the frequency of such coalition S in random selection. A payout is calculated by the value function v(\u00b7) on a certain coalition. Commonly defined value function may be the expectation with respect to a distribution R that can be inferred from data or prior knowledge.\nFor an observed $x_S$, the payout of coalition S is\n$$v(S) = E_R[f(x_S, X_{\\overline{S}})].$$\n(3)"}, {"title": "Adversarial shuffling", "content": "Shuffling attacks exploit the order-agnostic nature of the expectation calculation for value function v. For N samples of $y_i$ that\n$$y_i = f((x_S, X_{\\overline{S}})_i), i = 1, 2, \u2026\u2026\u2026, N.$$\nThe vector of $y_i$ is y. We rewrite the equation (3) as:\n$$v(S) = \\frac{1}{N} \\sum_{i=1}^N y_i.$$\n(4)\n(5)\nLet shuffle\\{j\\} denote an operator that transforms a vector y to a permutation y' and depends on feature j or the vector $X_j$ only. In other word, feature j determines the shuffle\\{j\\} operator. For example, between the original order of the output y = [Y1, Y2,\u00b7\u00b7\u00b7, Yn] and the permuted order of the output y' = [Y1, Y2,\u2026\u2026, yn], there is a one-to-one mapping between each element of y and y'.\nThe mapping is defined based on values in feature j,\n$X_j = [X_{1j}, X_{2j},.\u2026\u2026,X_{nj}]$. When all the elements of $X_j$ are the same, the shuffle operator is not executed. When the length of y is 1, n = 1, the shuffle operator is not executed. For simplicity, we assume feature j does not affect the calculation in equation (4).\nAssuming we want to calculate the attribution of feature j, since shuffle operator neither affects the total sum of y nor changes the expectation, we get\n$$v(S \\cup \\{j\\}) \u2013 v(S) = \\frac{1}{N} \\sum_{i=1}^N y_i' - \\frac{1}{N} \\sum_{i=1}^N y_i = 0$$\n(6)\nWe plug equation (6) in (2), and get attribution of feature j, $\\phi_j$, is always zero. Although neglected by Shapley values, the shuffling effect on y can reflect unfair treatment between groups. For example, it may describe an unfair allocation of bonuses, such as a manager swapping bonuses between male and female employees, although the females contributed more. Since the gender's attribution is non-detectable based on Shapley values, the manager gets away.\nTheoretically, we may state that shuffling will never be detected by Shapley values. However, common implementations of Shapley values, such as linearSHAP and SHAP, perform certain estimations that can detect non-zero attributions from shuffling features.\nWe experimented with different shuffling strategies on real-world datasets and different SHAP implementations. In general, Shapley estimators or implementations of Shapley values take inputs of a given instance ID i, a background data $X_D$, and return a row vector $\\Phi_i$ for the instance i.\nIn this work, we focus on the manipulation of the behavior of f. For example, the parameters (or code) of the function f can be random in each while-loop, or adapted to data $X_{DI}$ totally or partially."}, {"title": "Shuffling Attack", "content": "We introduce three types of shuffling attacks in pseudocode. We denote the protected features P (e.g. race, gender), and non-protected features D\\P. We define function f and function f', so an XAI method (e.g., SHAP) that can explain f can also explain f'.\n$$y = f(X_D) = f(X_{D\\P}, X_{P} = 0)$$\n(7)\n$$y' = f'(X_D) = h_{X_P}(f(X_{D\\P}))$$\n(8)\nf is a scoring function that does not use $X_P$. f can be a simple linear model or a complex neural network that takes input data and returns a score or vector of scores.\nFunction f may be used to assign probability or scores to data items. Scores can be used to i) assign items to different groups (e.g., a classifier) by setting thresholds or ii) order the items (e.g., an algorithmic ranker) using the score vector.\nFunction f' is a wrapper of f, and contains a shuffling function h. Both the f and f' functions take the entire data as input. However, the attack integrated in function f' does not require data other than access to the protected features. Function h first needs to determine the superior outcomes. For example, if f outputs the admission rate, a higher value is superior.\nWith such information, it can sort the vector y. The second step is determining the shuffling strategy to produce the y'. Due to the complexity and various choices involved in constructing function f', we first provide a pseudocode template (Algorithm 1).\nLine 5 calls an Attack function. We further provide three Attack functions (Algorithms 2, 3, 4). To ensure clarity and consistency of the codes, we define all attacks based on a two-class gender bias scenario. Each attack describes a decision-maker who prefers the male group over the female group.\nOur definition and codes can be generalized beyond the two-class gender scenario to race, age, or other protected features and their intersections.\nDominance. Algorithm 2 describes a biased decision-maker, who see a sorted list of female and male candidates, gives all the male candidates the high scores and female low scores (i.e. the last male is ranked one position higher than the first female), giving male candidates an unfair advantage."}, {"title": "Techniques to modify the attacks", "content": "We describe the techniques to modify the shuffling attacks regarding relaxing and data accessibility.\nRelaxing. We consider Mixing and Swapping algorithms as special cases and relatively relaxed versions of the Dominance algorithm. Although relaxed, the attacks are still detrimental to certain data instances or groups if bypass the detection. In practice, all three attacks can be further relaxed via certain restricting techniques including restricting the attack's frequency, count, region to decrease the shuffling attack's total occurrence for a given input.\nFor example, shuffling is only successful half the time, shuffling only happens a maximum amount of times, or shuffling is more active in the high-score region than the low-score region. If we remove line 4 in Algorithm 4, the female candidate in such case will be restricted to swapped only one time.\nData accessibility. Shuffling attacks do not require data accessibility other than the protected feature, but if the adversary can access the whole training data $X_D$ of the model, they can trigger attacks on in-distribution data and not on out-of-distribution data.\nFor example, (Slack et al. 2020) propose a \u201cscaffolding\u201d technique that classifies input data as in or out of distribution. The success of the attack depends on effectively training the \"scaffolding\" classifier. And the attack will relax if input data deviates from the training data. Or if the adversary can only access partial training data, effective scaffolding may not be achievable. However, given the same data accessibility, shuffling attacks are still achievable. For example, the adversary can model the correlation between $X_S$ and $X_P$ to overcome certain cases that protected features are inaccessible from data input. In deployed scenarios, the difference between shuffling attack and scaffolding is that the former is persistent and the latter may become non-existent after being exposed to new data."}, {"title": "Estimating SHAP's detection capability", "content": "In this section, we demonstrate that estimation algorithms of Shapley values may detect non-zero attributions of shuffling attacks. We consider a crude estimation scenario of Shapley values.\nFirst, we assume the features are independent. Second, we assume the features constructed a linear model. With those two assumptions, we can estimate the Shapley values with a special case of SHAP, linear SHAP. The SHAP values can be derived directly from the model's weight coefficients (Lundberg and Lee 2017).\nSHAP detection in linear model We prove that, for linear models, we can calculate the attribution of the shuffling features leveraging the additive constraint of Shapley values.\nThis allows us to estimate the effectiveness of the attacks without running the default time-consuming SHAP algo-rithms. Since the shuffling attack is constructed on a group of N input x. We use the matrix form of the formulas. For scoring function\n$$f(x) = \\sum_j \\beta_jX_j + 0 \\cdot X_P$$\n(9)\nthe contribution matrix $\\Psi$ given the input X are:\n$$\\Phi_j(f, X) = \\beta_jX_j \u2013 \\beta_j E[X_j], \\Phi_P(f, X) = 0$$\n(10)\nDue to the additive constraint of Shapley values, instance-wise feature contributions add up to the difference between the model output and the average model output,\n$$\\Phi_P(f, X) = 0 = f(X) \u2013 E[f(x)] \u2013 \\sum_j \\Phi_j(f, X)$$\n(11)\nWe apply the shuffling function $h_{X_P}$ on the output vector of f(X) to obtain f'(X). $h_{X_P} (\\cdot)$ can be omitted if all values in $X_P$ is the same:\n$$f'(x) = h_{X_P}(f(x)) = h_{X_P}( \\sum_j \\beta_jX_j)$$\n(12)"}, {"title": "Experiments", "content": "We analyze the capability of SHAP to detect the attacks on three real-world datasets. Our data and code can be found in this anonymized link: https://t.ly/UkIlu.\nIn these experiments, first, we compare linearSHAP and SHAP (the default option in the official SHAP python package) on attacks under different hyper-parameters. Then, we test combinations of shuffling attacks on multiple features. Finally, we demonstrate how to fool SHAP using the Dominance attack and achieve a similar effect in the prior work (Slack et al. 2020) by localizing the attack."}, {"title": "Datasets", "content": "We describe here the real datasets used in the experiments, varying across admission, medical prediction, and finance.\nGraduate Admission Data (Acharya, Armaan, and Antony 2019) includes 500 college applicant records.\nWe consider three scoring features GRE, TOEFL, University rating, and one protected feature Research.\nDiabetes Risk Data (Islam et al. 2020) includes 520 patient records with features such as Weakness, Itching, Irritability. We consider all features as scoring features, except the two protected features, Sex and Age.\nGerman Credit Data (Hofmann 1994) includes 1000 loan applicant records including financial and demographic information. We consider one scoring feature Loan Rate%Income and one protected feature Gender."}, {"title": "Graduate admissions prediction", "content": "We use the Graduate admission dataset to demonstrate the characteristics of the three attacks. The base model f is equal weighted summation between X1 (GRE) X2 (TOEFL), X3 (University rating). We normalize features' data and then feed them into function f or the adversarial function f'. We also feed the $X_P$ (Research) into the functions. But the f ignores it, while f' uses it to manipulate the outputted score vector.\nTo understand the characteristics of different attacks, we conduct experiments on the attacks under different hyper-parameters. The attacks are conducted on Research feature.\nFigure 2 shows the attribution detected by different explainers for all the features. Although our attacks are designed for Shapley values or Shapley estimators (e.g., SHAP, linearSHAP), the attacks are explainer-agnostic, and we add the LIME explainer for comparison. Each dot on the plot is the mean absolute attribution from the 100 students' explanation outcome.\nFigure 2(first row) is the attribution detected from repeated Swapping attacks using the starting at different quantiles of the input data. A quantile value indicates the Swapping attack starts at the corresponding list position and repeats at every list position until the top. Quantile 0 indicates attacking one time on the entire input list of scores, the default Swapping attack. Quantile 1 is equivalent to a Dominance attack."}, {"title": "Diabetes prediction", "content": "We use Diabetes prediction data to demonstrate when shuffling attacks are based on complicated conditions using multiple protected features (age, and sex). In this experiment, we obtain a non-linear scoring function f by training a logistic regression instead of defining it. The target label is positive or negative for diabetic diagnosis. We train the model on 80% training data and 20% testing data are used in the SHP detection experiments.\nf outputs the probability of diabetes. Then we use the protected features to construct adversarial scoring function f'. We set a probability threshold of 0.9 as a positive label for all functions' output. We construct hybrid attacks using combinations of Dominance, Mixing, and Swapping algorithms. For each list of scores, we break it into the top half and bottom half and apply different attacks or no attack on each half. We conduct SHAP detection on the adversarial scoring function. We evaluate all the adversarial scoring functions using various group fairness metrics and measured the fairness drop between f and f' for the gender feature (between male and female groups).\nFigure 3 shows in general, When the attack uses gender (i.e. male is the privileged group) only, the age feature's attribution is zero (Fig. 3i) and iii)). When the attack uses both gender and age (i.e. put younger people beyond elders), the gender's attribution decreases while age's attribution increases (Fig. 3ii) and iv)). Dominance attack on the top half and Mixing attack on the bottom half (Dom+Mix) results in the most significant feature attributions, in both rank and score (Fig. 3i)-iv) upper left).\nThe (Mix+Swap) and (Mix+None) result in the least significant feature attributions (Fig. 3i)-iv) lower right). Assuming a model auditor sets a threshold to reject models if protected features are in the top-10 ranks or higher than 0.05 in scores, only the (Dom+Mix) model is successfully detected and removed.\nWe also measure the group fairness drops in gender for all attack combinations. We use the following group fairness metrics that were implemented in IBM AI Fairness360 toolkit (Bellamy et al. 2018):\n\u2022 Demographic Parity: the predicted positive rates for both groups should be the same.\n\u2022 Equal Opportunity: the true positive rates for both groups should be the same.\n\u2022 Equal Odds: the true positive rates and the true negative rates for both groups should be the same.\n\u2022 Disparate Impact: the ratio of positive rate for the un-privileged group to the privileged group, e) Theil Index: between-group unfairness based on generalized entropy indices (Speicher et al. 2018)."}, {"title": "Credit prediction", "content": "Prior work demonstrates, using German credit data, scaffolding deception can fool SHAP. Without such deception, the protected features (Gender) have the top-1 attribution among all the data instances. With deception, the protected features drop from top-1 to top-2 or lower importance for most data instances.\nIn this experiment, we aimed to achieve the same using the Dominance attack on instances at the top-k percent region. We demonstrate that with small k, Dominance attack can bypass SHAP detection and achieve a similar result as scaffolding deception. Such an attack is effective for decisions that only take top instances into account, and the effectiveness is persistent even with data shifting.\nWe construct the scoring function f only based on Loan rate similar to the prior work, and use Gender feature to shuffle the output of f. Fig. 4 shows that when the attack region is the entire input data, Gender is the 1st important feature for 67.1% of data instances. This encourages model auditors to investigate further. However, when the attack region is constrained to the top 15% of the data instances, Gender is only at the first importance 1.7%, at second importance 25.8%, at third importance 13.7%. This gives auditors less incentive for investigation.\nAdversaries can engineer Dominance attacks to be more sophisticated than top-k constraints. In practice, if the Dominance attack only occurs 1 out of 1000 times on a target group of input, SHAP may be fooled."}, {"title": "Discussion", "content": "In this work, we propose three shuffling attacks that exploit the vulnerability of averaging processes in calculating Shapley values. We test two different implementations of Shapley values-based feature attribution methods, linear SHAP and SHAP, on real-world datasets to demonstrate the efficacy of the attacks. Our proposed data-agnostic attack strategies are more robust under data shifting than other data-dependent attack strategies. We theoretically prove that Shapley values are vulnerable to model output shuffling attacks.\nDue to the differences in the nature of their estimation, the implementations of Shapley values, such as SHAP can detect the shuffling attack. We notice the difference between linearSHAP and SHAP in certain experimental settings. Empirically, we observed SHAP detects the dominance attack better, but linearSHAP detects the swapping attack better. The detection success varies based on the attack, underlying data, and loss function of the XAI method.\nThis important finding demonstrates a counter-intuitive observation: the more accurate the Shapley value calculation algorithm is, the more vulnerable it is to the Swapping attack. For other Shapley values implementations that use different value functions, shuffling attacks can always be constructed by exploiting the process of averaging across a group of values so that the averaged outcome is invariant to shuffling.\nWe leverage the current XAI methods that use batch processing to obtain the vector of n score outcomes from a vector of n inputs. The attacks can be conducted in one function call. However, future XAI methods may call the function n times to obtain a score for each item in the input vector one by one and then create one attribution score. The attacks must be organized among multiple function calls in such cases. For example, the attack agents have to memorize previously called inputs and outputs to determine whether and how to adjust the current score.\nIf SHAP or other XAI methods employ consistency checks for the black-box model, the attack may be better detected and disabled. However, shuffling attacks can survive in many cases, such as algorithmic rankers or continuous machine learning, where consistency constraints are relaxed or removed. Another defense against shuffling attacks is to use incomplete coalitions while calculating SHAP or use other heuristic feature attribution methods such as LIME. We would like to emphasize that when the score difference due to Swapping is relatively small on the scale of the scores, it is still challenging. For example, in Figure 2, LIME detects near-zero attribution from the default swapping attack (quantile parameter is 0), and so does SHAP.\nA possible defense is to have another post-processing function that scales the score differences to avoid model output cluttered in a small score range (Yuan and Dasgupta 2023). But it is to be noted that such scaling is a distortion of the model behavior. Maintaining the usability of the explanation after distortion is part of our future work.\nOne limitation of the shuffling attack is its effect on the decision-maker's conclusion is not always guaranteed. For example, the conclusion will not change if the attack impacts the low-score items and the decision is based only on high-score items. Defense against the shuffling attacks can be facilitated by end-to-end communication between the developer and the user (who may use SHAP for auditing) regarding which outcome is superior. Specifically, the algorithm 1 requires such a parameter to perform the sorting in line 5. The cost of such defense is a topic beyond the scope of this work. We only demonstrate scenarios using protected features (e.g. gender) directly in the shuffling attack, future work may explore using proxy features (e.g. pregnancy status) that are correlated with the protected features.\nWe only tested a few Shapley estimators under the attacks (i.e. SHAP and linearSHAP), we may explore other improved Shapley estimators such as kernelSHAP (Covert and Lee 2020) and bayeSHAP (Slack et al. 2021) in future work. A recent work proves that the Generalized Linear Model's parameters can be fully recovered with n-Shapley value (Bordt and von Luxburg 2023). It may be fruitful to investigate the interplay between higher-order Shapley values and the shuffling attacks."}, {"title": "Conclusion and Future Work", "content": "Our work demonstrated that the shuffling attack is powerful enough to fool SHAP. It is still an open problem in XAI research to develop robust detection methods for more complicated shuffling attacks. For example, we only considered swapping between two nearby items. Still, it can be generalized to swapping between nearby items for which the behaviors are not explored. Generalization or derivatives of the shuffling attacks are easy to define by an attacker such as a model distributor or model broker without the need for model training. This relaxes the requirements of AI expertise in performing such attacks and increases the risks. We have not yet explored the cascading impact of our unfair scoring function. In the real world, it is common that a higher-scored candidate may receive additional advantages (e.g., getting the job offer) which leads to advantages in future scoring (e.g., approval of a bank loan). A small shuffling initially may result in huge future differences. Our work opens up an alternative way of explaining model unfair behaviors: one group's score is shuffled against another in a specific way. In the future, we will design XAI methods to generate such an explanation. We will develop a testing framework and user interface for interactively and collaboratively conducting attacks on current and future XAI methods using synthetic and real datasets to elicit the domain decision-makers' desiderata of decision-aid XAI systems, such as the stability and sensitivity of the XAI interpretation and decision outcome under attacks. We will consider shuffling scores to promote candidates from non-privileged groups and investigate these fairness-preserving interventions in conjunction with established ranking fairness metrics."}, {"title": "Ethical Statement", "content": "Our work aims to raise awareness about the risk of using Shapley value-based feature attribution explanations in auditing black box machine learning models. Through demonstrative experiments, we expose the potential negative societal impacts of relying on such explanations.\nGiven the nature of our proposed shuffling attacks, where the attacks do not require access to the underlying data distribution, We especially want to draw the community's attention to the severity of such attacks and propose a call to action to collaboratively find more robust ways to deploy and leverage explainable AI methods in socio-technical settings.\nWe acknowledge that malicious model distributors or brokers could use this attack to mislead end users or cheat during an audit. However, we believe this paper increases the vigilance of the community and fosters the development of trustworthy explanation methods.\nFurthermore, by showing how data-agnostic unfairness rules can be incorporated after the model is trained, this work contributes to the research on auditing or certifying the fairness of AI-based decision systems."}]}