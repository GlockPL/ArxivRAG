{"title": "Toward Time-Continuous Data Inference in Sparse Urban CrowdSensing", "authors": ["Ziyu Sun", "Haoyang Su", "Hanqi Sun", "En Wang", "Wenbin Liu"], "abstract": "Abstract-Mobile Crowd Sensing (MCS) is a promising paradigm that leverages mobile users and their smart portable devices to perform various real-world tasks. However, due to budget constraints and the inaccessibility of certain areas, Sparse MCS has emerged as a more practical alternative, collecting data from a limited number of target subareas and utilizing inference algorithms to complete the full sensing map. While existing approaches typically assume a time-discrete setting with data remaining constant within each sensing cycle, this simplification can introduce significant errors, especially when dealing with long cycles, as real-world sensing data often changes continuously. In this paper, we go from fine-grained completion, i.e., the subdivision of sensing cycles into minimal time units, towards a more accurate, time-continuous completion. We first introduce Deep Matrix Factorization (DMF) as a neural network- enabled framework and enhance it with a Recurrent Neural Network (RNN-DMF) to capture temporal correlations in these finer time slices. To further deal with the continuous data, we propose TIME-DMF, which captures temporal information across unequal intervals, enabling time-continuous completion. Additionally, we present the Query-Generate (Q-G) strategy within TIME-DMF to model the infinite states of continuous data. Extensive experiments across five types of sensing tasks demonstrate the effectiveness of our models and the advantages of time-continuous completion.", "sections": [{"title": "I. INTRODUCTION", "content": "With the evolution of information society and the increasing portability of wireless devices, Mobile CrowdSensing (MCS) [1], [2] has recently emerged as a promising paradigm of data collection. Typically, it recruits a large number of users equipped with mobile devices to collect data from specific sensing areas at particular time. Due to budget constraints and presence of unreachable sensing data, traditional MCS can only collect incomplete or even sparse data in most cases. To this end, a modified paradigm called Sparse MCS [3] is proposed, which introduces inference strategies to complete the full sensing data from the partial ob- servations. Sparse MCS has already shown great advantages in some practical applications, such as the air quality monitoring [4], [5], traffic control [6], [7] and urban sensing [8], [9]. In Sparse MCS, data inference is the most essential part and has therefore received considerable attention. To reduce cost and simplify inference process, most of existing works study the data inference problem from a time-discrete perspective [10]\u2013[15]. For example, in Fig. 1, a requester would like to analyze urban traffic for a period. Upon receiving the task, existing works typically discretize the time period into units and aggregate the sensed data within each unit. Then, by assuming that the sensed data remains constant within each time unit, they use the data inference methods, such as compressive sensing [12], [16] or matrix completion [17], [18] to infer the missing data. However, in practical scenario, the sensing data changes continuously over time. Previous time- discrete approaches may cause significant errors on practical applications that are sensitive to change. For example, temper- ature or wind speed may fluctuates greatly within a short time due to severe weathers and the rough time-discrete method may fail to capture this dramatic local changes. Therefore, time-continuous data inference has become a crucial issue that urgently needs addressing for Sparse MCS. In this paper, we adopt a time-continuous perspective, moving away from the traditional method of discretizing time into fixed units. This shift eliminates the assumption that data remains constant within a specific period, making it impossible to aggregate observed data within each time unit to reduce matrix sparsity. Consequently, our first challenge is to handle the extremely sparse data matrix. Additionally, in time- continuous scenarios, data is collected in real-time, leading to unequal lengths between sensed data intervals, which affects the relationships between consecutive data points. Thus, our second challenge is to model and utilize these unequal in- tervals effectively, maximizing the temporal information for accurate data inference. Finally, while time-discrete methods could represent the problem with a fixed-size matrix, they cannot infer the continuously changing data at every moment. This leads to our third challenge: how to complete the data from a continuous perspective, ensuring comprehensive data inference across all moments. To tackle the challenges inherent in time-continuous data completion, we introduce a compre- hensive approach that begins by reformulating the problem and progresses to designing a method that addresses these challenges effectively. We start with fine-grained completion, which serves as an intermediary between traditional time- discrete methods and our final goal of continuous comple- tion. Unlike traditional time-discrete completion, which uses predefined unit lengths to discretize the timeline, fine-grained inference divides the timeline based on the actual distribution of submissions, ensuring that each time unit contains only one submission. This adjustment eliminates the need to assume data remains constant within each time unit, thereby offering a more accurate model. However, this approach significantly reduces the data volume in each unit, leading to an extremely sparse spatiotemporal matrix (i.e., our first challenge). To address this, we introduce Recurrent Neural Network-enabled Deep Matrix Factorization (RNN-DMF), a neural network- based framework for data completion that also incorporates a temporal encoder to fully leverage previously hidden infor- mation, addressing the challenge of sparsity. Moving beyond fine-grained completion, we recognize that it still does not fully capture the continuous nature of time. In time-continuous scenarios, we avoid discretizing the timeline entirely and handle each submission directly, preserving the precise arrival times and intervals between submissions\u2014our second challenge. To leverage this additional temporal infor- mation, we introduce Time Gates-enhanced Deep Matrix Fac- torization (TIME-DMF), which further captures the temporal dynamics within intervals through the use of time gates and a more sophisticated propagation pattern. Finally, addressing the challenge of representing the infinite number of moments within a period, we propose the Query-Generate (Q-G) strat- egy, which works in conjunction with TIME-DMF to model any moment on the timeline, thus providing a comprehensive solution to time-continuous data completion. Our work has the following contributions: We reformulate the problem of data inference from a time-continuous perspective. It pays attention to the conti- nuity of data changes and is a much closer approximation of practical MCS problems."}, {"title": "II. RELATED WORK", "content": "Mobile CrowdSensing [1], [19] is an emerging paradigm that leverages mobile device users to collect data, enabling a wide range of services within the Internet of Things ecosystem [20]\u2013[22]. MCS has been widely used in domains such as traffic supervision [16], [23], pollution control [24], [25], and facility management [26], [27]. Initially, the mainstream algo- rithms for MCS were based on compressed sensing [16] and its various adaptations [12]. However, as these algorithms were implemented, it became apparent that data collection often exhibited sparsity [10] due to cost constraints and limitations of sensing devices. Consequently, algorithms used for inferring missing data [28] gained popularity. In 2016, Wang et al. [3] provided a comprehensive review of MCS methods based on sparse sensed data and systematically introduced frameworks [11], [13] for data collection and completion. Since then, Sparse MCS has emerged as an evolving paradigm, with many innovative algorithms being developed. Primary areas of work in this field include cell selection [29], data inference [17], [18] and user privacy protection [30]. In data inference, methods are generally categorized into two classes: dense-supervised [31], [32] and sparse-supervised [15], [17], [18]. Dense-supervised methods rely on large amounts of complete spatiotemporal data for training. Most Transformer models and their variants, which are powerful in handling time series, fall under this category. In contrast, sparse-supervised methods do not require complete spatiotem- poral data for training and instead rely on capturing correla- tions within sparsely observed data. Despite their differences, neither approach considers the continuous nature of time."}, {"title": "B. Spatiotemporal Granularity", "content": "The goal of sensing technology is to capture more fine- grained spatiotemporal information. Initially, this was achieved"}, {"title": "III. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "Fig. 1 and Fig. 2 showcase different methods for formulat- ing real-world spatiotemporal data. The majority of existing research has relied on the time-discrete approach depicted in the lower half of Fig. 1, which serves as a coarse-grained approximation of the real world. In this section, we will begin with the time-discrete formulation and subsequently introduce our novel time-continuous approach to provide a deeper understanding of our concept. In Sparse MCS tasks, our sensing map is divided into N sub- regions. At moment t, a worker submits data of the n th sub-region. We use a position vector $c(t) \\in R^N$ and a data vector $y(t) \\in R^N$ to represent the submission. The position vector $C(t) = [0,0,..., 1,0...0]$ indicates the index of the subarea submitted by the worker. If the submitted data is from the i-th sub-region, the i-th element of $c(t)$ is set to 1, and all other elements are set to 0. The data vector $y(t)$ indicates the value of the submitted subarea. The i-th element of $y(i)$ is the submitted value, and other elements of $y(i)$ are set to meaningless values like 0 or negative numbers. Assuming that there are M submissions on the entire timeline, the results of M submissions are organized into position matrix and data matrix, we have $C \\in R^{N \\times M}$ and $Y' \\in R^{N \\times M}$ by stacking all the submissions together:\n$C = [c_1^T, c_2^T, \\dots, c_M^T]$,\n$Y' = [y_1^T, y_2^T, \\dots, y_M^T]$.\nSimilarly, $Y = [y_1, y_2, \\dots, y_M]$ represents the real values of each sub-region at each submission time. So we have:\n$Y' = Y \\odot C$,\nwhere $\\odot$ represents the Hadamard product. In traditional Sparse MCS tasks, the timeline is evenly discretized into time units of predefined length, and we no more distinguish the difference of arriving time of submissions"}, {"title": "A. System Model", "content": "$\nc^{\\prime}(D) \\in \\mathbb{R}^{N \\times P}$ and $Y^{(D)^{\\prime}} \\in \\mathbb{R}^{N \\times P}$ by putting each submission into the time unit they locate in and merge all the submissions within the same unit.\n$C^{(D)} = [c_1^{(D)^T}, c_2^{(D)^T}, ..., c_P^{(D)^T}]$,\n$Y^{(D)^{\\prime}} = [y_1^{(D)^T}, y_2^{(D)^T}, ..., y_P^{(D)^T}]$,\n$C_p^{(D)}, y_p^{(D)} = merge(C_{p_0}, C_{p_1},..., Y_{p_0}, Y_{p_1},\u2026)$,\nwhere $C_{p_i}$ represents the i \u2013 th submission arrived within the p \u2013 th time unit and $merge(\\cdot)$ represents the merging oper- ation. Technically, for each subarea, if there is a submission within the given p-th time unit, the corresponding position in $C_p^{(D)}$ will be set to one and the submitted value will be filled in $y_p^{(D)}$. If there are multiple values observed for one location, the final value would be set to the average. By the merging operation, the sparsity of $C^{(D)}$ will be much less than that of matrix C. After the discretization and merging operation, we no longer keep the specific arrival time of all submissions and their intervals but use p-index to represent their arrival time. Then our task is to infer the missing values in $Y^{(D)^{\\prime}}$ to know the value of each location on the map at each time unit.\n$\\hat{Y} = f(Y^{(D)^{\\prime}})$.\nHowever, it is obvious that some temporal information has been ignored during the discretization and merging process such as the precise arrival time and the intervals. Inspired by this observation, we propose a new system model to reduce information loss during problem modeling phase. For the M submissions, we use an additional vector $T = [t(1),t(2),\u2026,t(M)]$ to represent the accurate arrival time of each submission and no longer discretize C and Y' to $C^{(D)}$ and $Y^{(D)^{\\prime}}$. What's more, since in time-continuous completion each column in the observed matrix represents a specific moment instead of a period, we not only want to complete the matrix Y', but also the infinitely many moments that do not exist in Y'. In order to achieve this, we divide the challenging time-continuous completion task into to two subtasks. Our first task is to achieve a mapping $f(.)$ only to complete the matrix Y' which fully utilizes the temporal information in T. This is similar to the time-discrete scenarios.\n$\\hat{Y} = f(Y^{\\prime})$.\nAfter that, we want to provide accurate inference for any moment on the time line. Obviously there are countless values distributed along the time line, and our solution is to find a model that can provide accurate responses $\\hat{y} \\in \\mathbb{R}^N$ for any given time moment t.\n$\\hat{y} = g(Y^{\\prime}, t), t \\in (t_0, t_M)$."}, {"title": "B. Problem Formulation", "content": "Problem [Completion and Generation on Continuous Time- line]: Given sparse sensed data Y' and time vector T, we aim to achieve the following two objectives:\n\u2022 Identify a mapping f(.) to complete all the unsensed data in the matrix Y'. The mapping f(\u00b7) should adequately consider the high sparsity of Y' and the temporal infor- mation in T.\n\u2022 Identify a model G(.) to accomplish the completion at any given time t'. y(t')' can be a column in Y' or not.\nIn this process, the mean square error is used to measure the quality of the completed and generated data. The following value should be minimized:\n$\\epsilon (Y,Y^{\\prime}) = \\sum_{i}^{N} \\sum_{j}^{M} |Y_{ij} - Y_{ij}^{\\prime}|$\nIV. FINE-GRAINED COMPLETION WITH RNN-DMF"}, {"title": "IV. FINE-GRAINED COMPLETION WITH RNN-DMF", "content": "Fine-grained completion is the first step towards time- continuous completion. With the insight that we can construct our spatiotemporal matrix by including each submission in a unique time unit, we could align with the previous time- discrete problem setting but eliminate the assumption that data stays constant within each time unit. We propose Deep Matrix Factorization (DMF) [17] as a foundational framework for the following works. Due to its neural network-enabled structure, it can be updated by adding modules of different functions. In fine-grained completion, there is one submission within each time unit so that we only know the information of one spatial location, leading to the great reduction of spatial information within a unit and the extreme sparsity of observation matrix. In order to solve this problem, we further propose Recurrent Neural Network-enabled Deep Matrix Factorization (RNN- DMF) by introducing a temporal encoder into DMF. The encoder is able to utilize temporal information to compensate for the loss of spatial information."}, {"title": "A. Deep Matrix Factorization (DMF)", "content": "Given a sparse matrix $Y_{mxn}$, the traditional method is to take full rank decomposition of the estimated matrix $Y_{mxn}$ by using the property that the real matrix $Y_{mxn}$ has the lower rank and can be decomposed. Assuming that rank(Y) = r, then the full rank decomposition can be expressed as:\n$Y_{mxn} = P_{mxr}Z_{rxn}$,\nwhere P is a full rank matrix and Z is a row full rank matrix. Therefore, any column $y_t$ of $Y_{mxn}$ can be expressed as $y_t = Pz_t$. In this formula, $z_t$ represents the low-rank vector that fully contains the information of the t-th column of $Y_{mxn}$, which is also the t-th moment in real scenarios. P denotes the projection from the low-rank vectors to the inferred matrix. Note that Eq. (11) assumes that the spatiotemporal correlations between data are linear. To model the widely existing nonlinear spatiotemporal correlations, DMF was proposed. Similarly, we use the function f(.) to represent the nonlinear correlations between space and time.\n$y_t = f(z_t),Y = f(Z)$.\nWe aim to obtain suitable z and f(.) by fitting them with a deep neural network (DNN). Specifically, assuming the neural network consists of K hidden layers, and their parameters are\n$W^* \\leq \\{W^{(1)}, W^{(2)},...,W^{(K)}, W^{(K+1)}\\}$,\n$b^* \\leq \\{b^{(1)}, b^{(2)}, ..., b^{(K)}, b^{(K+1)}\\}$.\nThe corresponding activation function set is\n$g^* \\leq \\{g^{(1)}(\\cdot), g^{(2)}(\\cdot), ..., g^{(K)} (\\cdot), g^{(K+1)}(\\cdot)\\}$.\nIn the set W*, b*, g*, the (K + 1)th term represents the parameter or the activation function from the hidden layer K to its output layer. The nonlinear function is expressed as:\n$f(z) =g^{(K+1)}(W^{(K+1)}g^{(K)} (W^{(K)},...,\ng^{(1)} (W^{(1)} z + b^{(1)}) ... + b^{(K)}) + b^{(K+1)})$.\nDifferent from traditional neural networks, the input z and the neural network parameters are both optimizable parameters in this context. The optimization object is as follows:\n$min ||(Y' \u2013 f(Z)) * C||$.\nIn this way, the inferred results can be obtained:\n$Y = f(Z)$."}, {"title": "B. Recurrent Neural Network-enabled Deep Matrix Factoriza- tion (RNN-DMF)", "content": "In each time step, the proposed DMF framework has low- rank vectors that can be concatenated to form the complete low-rank representation of spatiotemporal data. While in the training process, each low-rank vector is learned separately and is not visible to each other. This limits the ability of DMF to fully capture temporal information. However, capturing and utilizing temporal information is an urgent in fine-grained completion due to the extreme sparsity of matrices. For this reason, we further propose Recurrent Neural Network-enabled Deep Matrix Factorization (RNN-DMF). Fig. 6 shows that RNN-DMF is composed of two key modules: an upstream RNN-enabled encoder and a down- stream DMF-like decoder. Unlike DMF, which initializes low-rank representations randomly, RNN-DMF accounts for relationships between low-rank vectors. The RNN structure enables the encoder to generate low-rank representations by incorporating temporal correlations, sharing parameters U, W, and V across time steps. The hidden state St is generated based on the previous state St-1 and the primary low-rank vector Xt at each timestamp.\n$S_t = f(U \\cdot X_t + W \\cdot S_{t-1})$.\nAs DMF, $S_0$ and the primary low-rank vector of each step is randomly initialized and serves as optimizable parameters during training. The encoded low-rank representation is then generated by a final projection. This process is similar to what traditional RNN does.\n$Z_t = g(V \\cdot S_t)$.\nAt this step, we finally have our encoded low-rank vectors with temporal information integrated. They will then be con- catenated and decoded, serving as the final completion results. This is done by the downstream DMF-like decoder:\n$Z = [z_1^T, z_2^T,..., z_t^T]$,\n$Y = f(Z)$.\nIt is obvious that RNN-DMF performs strictly superior to DMF theoretically. This is because DMF generates its low- rank vectors randomly and independently without consider- ing temporal correlations. RNN-DMF considers the possible temporal correlations between low-rank vectors during the generating process, which is at least better than complete random. When sensed data is extremely sparse, there is very limited information for use within each time step, making it urgently necessary to share information between time steps. This explains why RNN-DMF performs significantly better than DMF, especially on extremely sparse matrices."}, {"title": "V. TIME-CONTINUOUS COMPLETION WITH TIME-DMF", "content": "In the previous fine-grained completion, we address the challenges of the reduction of spatial information and the extreme sparsity of observation matrices. Based on that, we can finally introduce the time-continuous completion. In traditional works and fine-grained completion, the intervals between submissions are ignored during the discretization stage. However, in real-life scenarios, users submit data at real time and the interval between submissions are of great impor- tance for inference. This oversimplification is also the reason why traditional time-discrete completion methods cannot fully capture temporal information. Intuitively, it is not the absolute arrival time of submissions but their intervals that matter. The sensing data with a long period of time interval will vary significantly but the sensing data at adjacent time may be quite similar. This is the intuition of Time Gates-enhanced Deep Matrix Factorization (TIME- DMF) for our introducing time gates and a more complex propagation pattern to model the influence of intervals on correlations between time steps. Furthermore, in fine-grained scenarios, period is sliced into a finite number of units and the observation matrix is of fixed size. But from a time- continuous perspective, data changes continuously and there are infinitely many states of sensed data in a given period. In order to characterize data of all states at an acceptable price, we propose Q-G strategy. It allows users to query data of any time and leverages the generative ability of TIME-DMF to dynamically respond. By combining TIME- DMF and the Q-G strategy, we achieve the ultimate time-continuous completion. In this section, we will introduce the details of TIME-DMF and the Q-G strategy. Finally, we will conclude the complete flow of TIME-DMF algorithm for time-continuous data completion tasks."}, {"title": "A. Time Gates in TIME-DMF", "content": "The inner structure of TIME-DMF is shown in Fig. 6. Inspired by [41], we design two types time gates to manage a more complex pattern of information propagation. The first"}, {"title": "Algorithm 1 Deep Matrix Factorization with Time Gates (TIME-DMF)", "content": "Require: $C^{N \\times M}, Y^{N \\times M}, t, T \\ni [t_1, t_2,\u2026, t_M], X \\ni [X_1,X_2,..., X_M]$\nEnsure: $Y^{N \\times M}$\n1: Random Init $x$;\n2: if $t \\notin T$ then\n3: $\tT \\leftarrow [t_1, t_2,...,t_k,t,t_{k+1},...,t_M]$;\n4: $\\X_T \\leftarrow [X_1,X_2,\u2026, X_k, X, X_{k+1},..., X_M]$;\n5: $\\CNxM \\leftarrow CNx(M+1), YNxM \\leftarrow YNx(M+1)$;\n6: end if\n7: while not convergent do\n8: for $x^{(1)}$ to $x^{(M)}$ do\n9: $\t_t, \\tC_{t}, C_{t} \\leftarrow encoder(x^{(t)}, \\tC_{t-1}, C_{t-1})$;\n10: end for\n11: $\\hat{Y} \\leftarrow decoder([\\tz_1, \\tz_2,\u2026\u2026, \\tz_t])$; \n12: calculate and reduce $||(Y' \u2013 Y) * C||$ ;\n13: end while\ntime gate controls global memory. All data within the observed time period should follow a certain underlying distribution, reflecting the overall characteristics of the dataset, such as the mean or periodicity in real-life scenarios. The second time gate controls local memory. The distribution of data at adjacent time shows the local increase/decrease or other short- term trends. We combine these two memories to ensure the completion result is rational both globally and locally.\nThe two kinds of memories are controlled by different parameters and are updated independently. Intuitively, intervals determine the correlations between time steps. The shorter the interval between two time steps, the more similar they are expected to be. So we utilize intervals to control the update of memories. If the memories are slightly updated between the two adjacent time steps, their completion results will be very similar. Conversely, if the memories have a substantial update, the following results will greatly defer from the previous ones. We use the interval as a parameter within each time gates.\n$\tT_{1m} = \\sigma(x_mW_{x1} + \\sigma{\\Delta_t}(A_{tm}W_{t1}) + b_1)$,\n$\tT_{2m} = \\sigma(x_mW_{x2} + \\sigma{\\Delta_t}(\\Delta_{tm}W_{t2}) +b_2)$.\nThe function of two memories is similar to that of the hidden state in RNN, which is shown in Eq. (19).\nIn TIME-DMF, the global memories and local memories operate in different manners. The local memory has a more direct impact on the completion of each step:\n$\ta_t = f(U \\cdot X_t + W \\cdot \\tC_{t-1})$.\nThe global memory flows more consistently and has a broad effect on the update of memories. We use the two time gates calculated formerly to control the update of the two memories. In this way we make use of the information within intervals.\n$\\tC_t  = f[C_t \\odot \\tT_{1m} + a_t \\odot (1 \u2013 \\tT_{1m})]$,\n$C_t = f[C_t \\odot T_{2m} + a_t \\odot (1 \u2013 T_{2m})]$.\nLastly, as the local memory can better assess the data status at the current moment, it is converted into an low-rank vector representing time t through the output gate.\n$\\tz_t = f(V \\cdot \\tC_t)$.\nAt this point, the upstream encoder has been obtained. The encoder is capable of doing time-continuous completion as it can deal with submissions at any given moments. We further use the DMF-like decoder to map the low-rank representation to yield the final completion results. Similar to RNN-DMF, TIME-DMF adopts a joint training method for the upstream and downstream networks."}, {"title": "B. Q-G Strategy", "content": "Given a sparse observation matrix, RNN-DMF or TIME- DMF can infer the missing data. However, unlike previous methods relying on discrete slicing, in time-continuous sce- narios, there are infinitely many possible moments on the timeline. Thus, it is impossible for the prior Submit-Complete strategy to infer data at all moments. To overcome this chal- lenge, we propose the Query-Generate strategy which allows users to query data for any moment within the given period and dynamically generate the results. To use TIME-DMF for data generation, we leverage its property of completing data by first generating low-rank representation. We can insert a randomly initialized low-rank vector into the previous low-rank repre- sentation and then proceed with the usual completion process. The only difference is that the inserted vector does not affect the backward gradient propagation process."}, {"title": "C. The Complete Flow of TIME-DMF", "content": "For a time query provided by the user, we first check if there is an existing column in the given sparse matrix. If not, we insert an empty column into matrix Y' and update the intervals of its both sides. Concurrently, we insert a column with all zeros into matrix C to ensure the query won't affect existing completion process. We then deploy our encoder and decoder sequentially to complete the data matrix Y' and minimize the loss between the completed data and known data at observation positions. In this process, the encoder, decoder and the completed results are continuously updated by calculating and reducing the optimization objective. The complete algorithm flow is shown in Alg. 1."}, {"title": "VI. EXPERIMENTAL VALIDATION", "content": "In this section, we first introduce the datasets and the comparison methods. Then we present performance evaluation of our proposed results. In particular, our experiments can be divided into the answers to the following research questions:\n\u2022 RQ1: Does our RNN structure offer better completion performance for extremely sparse matrices?\n\u2022 RQ2: Do our time gates truly leverage the time interval?\n\u2022 RQ3: In time-continuous, the model needs to have additional generative capabilities. How effective is the generative capability of TIME-DMF?\n\u2022 RQ4: Even our method achieves time-continuous com- pletion, is time-continuous completion truly more effec- tive than time-discrete completion?\n\u2022 RQ5: In the domain of spatiotemporal data, transformers seem to have become the mainstream approach. Why do we choose not to use transformer architectures?"}, {"title": "A. Datasets", "content": "\u2022 U-Air [42] is utilized to gather significant air quality data, specifically PM2.5 and PM10 levels, via monitoring stations located in Beijing, China.\n\u2022 Sensor-Scope [43] is employed to collect a diverse array of environmental readings through the deployment of numerous static sensors on the EPFL campus. A rep- resentative type of sensing, namely humidity, is selected for evaluation purposes.\n\u2022 TaxiSpeed [44] gathers traffic speed data pertaining to road segments in Beijing, China by utilizing GPS devices installed on taxis.\n\u2022 Highways England (HE) [45] serves as a resource in providing information pertaining to travel times, traffic flow rates, incidents, event data and camera imagery for England's major motorways. Due to its large space and time range, we manually selected spatiotemporal data from multiple adjacent regions and adjacent time periods, thus the data size is larger and more flexible."}, {"title": "B. Comparison Methods", "content": "1) Comparative models for completion task: Sparse- supervised methods which only rely on sparse observed data for training:\n\u2022 MC, a classic linear matrix completion method, assumes a linear relationship Y = PZ.\n\u2022 KNN-S, a variant of the K-Nearest Neighbors algorithm. KNN-S retrieves information from the K closest sub-regions to the region to be imputed and uses their average value as the imputation result.\n\u2022 GP algorithm, a method that assumes the spatial dis- tribution of data in the same cycle obeys the Gaussian distribution. The unknown data are inferred by calculating the expectation and variance of the known data.\n\u2022 DMF, which has been introduced previously and also serves as a basic component of our method.\n\u2022 STformer [46], a transformer-based model with multiple designed embedding and attention layers to capture spa- tiotemporal relationship. STformer is specially designed to be trained with only sparse observed data.\n\u2022 iTransformer [31], a variant which applies the attention and feed-forward network on the inverted dimensions. This is fine-tuned for completion tasks.\n\u2022 AutoFormer [32], another variant of transformer which entangles different blocks in the same layers during su- pernet training.It is also fine-tuned for completion tasks.\n2) Comparative Predictive models for generative task:\n\u2022 LINEAR, which applies the linear regression model to predict the full map of the future cycles. It assumes that the sensed data varies linearly over time.\n\u2022 WNN, which combines wavelet transform and neural network. WNN is good at extracting periodic features of time series for data prediction.\n\u2022 NAR, which uses a nonlinear autoregressive neural net- work to predict the near future. NAR considers the nonlinear temporal correlations within data."}, {"title": "C. Completion on Extremely Sparse Data (RQ1)", "content": "For most sparse data completion tasks, the sensing rate typically ranges from 20% to 80%, indicating that we can utilize abundant spatiotemporal information. However, in fine- grained and time-continuous completion, we have the matrices with sense ratio of 1/n-columns which traditional methods may have difficulty handling. To show the effectiveness of RNN-DMF on extremely sparse data, we compare it with other existing completion methods on multiple datasets. Note that the sense ratio is not limited to 1/n-columns to show the generalization performance of RNN-DMF. As problem setting, each column of the data matrix repre- sents a submission, and each row represents a subarea to sense. To build sparse datasets, we randomly mask the complete matrix and leave 1-5 data points unmasked in each column to represent the sensed data. This process is entirely random, as cell selection strategy is not our focus. For fair comparison, we use a small amount of complete data to train dense-supervised methods to ensure that models function properly."}, {"title": "D. Ablation Study on Time Gates (RQ2)", "content": "Based on RNN-DMF, we aim to further demonstrate the effectiveness of time gates which are designed to capture information within unequal intervals between submissions. We design an ablation experiment between TIME-DMF, which incorporates time gates, and RNN-DMF, which does not incorporate time gates, on multiple datasets. As most publicly available datasets consist of sensor data with equal time intervals, we artificially create datasets through random masking and deletion. In specific, for the complete data matrix, we randomly remove some columns from the matrix so that the left columns are of random intervals. For the columns left, we do random masking again like that in the last experiment. This preprocessing results in extremely sparse matrices where the intervals between columns vary in length. Because of the deletion, the datasets will be much smaller. So we need datasets of enough columns to make sure the experiments can have stable results. As Highways England is a dataset that collects road data per 15 minutes for years, it provides an ample number of columns for use. In this experiment we employ this typical dataset to test the effectiveness of time gates."}, {"title": "E. Demonstration of Generative Capability (RQ3)", "content": "Since we account for data continuity, we cannot provide states for all moments. Instead, we enable our model to dynamically generate data states based on user requests. In this experiment, we test the generative capability of TIME-DMF. Note that existing time-discrete models don't have generative needs, so we compare TIME-DMF with series of predictive models. In order to make the comparison fairer, we just predict one step ahead in predictive models. As we construct datasets via random masking and deletion, and each time we only generate data for a moment (a column in the matrix), the generation accuracy is highly unstable. For more persuasive results, we conduct extensive experiments on different randomly built datasets and depict the findings in a box plot. From the average height of the boxes in Fig. 9 we can tell that TIME-DMF provides superior overall generation accuracy."}, {"title": "F. Comparison between Time-discrete and Time-continuous completion (RQ4)", "content": "In time-discrete scenarios, we reduce matrix sparsity by sac- rificing accurate submission times, while in time-continuous scenarios, we use time intervals but face increased sparsity. After showing that our model can handle high sparsity and effectively use intervals, we need to prove that time-continuous completion truly outperforms time-discrete solutions. To test this, we compare TIME-DMF with time-discrete methods in their original problem setting. We simulate real MCS scenarios by constructing submissions with random masking and deletion from the original datasets. For time-discrete methods, we divide the timeline into equal slices and merge data within each slice, while TIME-DMF keeps submissions in their original state with recorded arrival times. We then apply various completion methods and compare their accuracy. After random deletion, the size of datasets will reduce a lot and completion methods may yield different results on datasets of varying sizes. To show the universality, we conduct the experiments on both smaller datasets and larger datasets. In Fig. 10 and Fig."}]}