{"title": "Toward Time-Continuous Data Inference in Sparse Urban CrowdSensing", "authors": ["Ziyu Sun", "Haoyang Su", "Hanqi Sun", "En Wang", "Wenbin Liu"], "abstract": "Abstract-Mobile Crowd Sensing (MCS) is a promising paradigm that leverages mobile users and their smart portable devices to perform various real-world tasks. However, due to budget constraints and the inaccessibility of certain areas, Sparse MCS has emerged as a more practical alternative, collecting data from a limited number of target subareas and utilizing inference algorithms to complete the full sensing map. While existing approaches typically assume a time-discrete setting with data remaining constant within each sensing cycle, this simplification can introduce significant errors, especially when dealing with long cycles, as real-world sensing data often changes continuously. In this paper, we go from fine-grained completion, i.e., the subdivision of sensing cycles into minimal time units, towards a more accurate, time-continuous completion. We first introduce Deep Matrix Factorization (DMF) as a neural network-enabled framework and enhance it with a Recurrent Neural Network (RNN-DMF) to capture temporal correlations in these finer time slices. To further deal with the continuous data, we propose TIME-DMF, which captures temporal information across unequal intervals, enabling time-continuous completion. Additionally, we present the Query-Generate (Q-G) strategy within TIME-DMF to model the infinite states of continuous data. Extensive experiments across five types of sensing tasks demonstrate the effectiveness of our models and the advantages of time-continuous completion.\nIndex Terms-Mobile CrowdSensing, data inference, fine- grained completion, continuous time.", "sections": [{"title": "I. INTRODUCTION", "content": "With the evolution of information society and the increasing portability of wireless devices, Mobile CrowdSensing (MCS) [1], [2] has recently emerged as a promising paradigm of data collection. Typically, it recruits a large number of users equipped with mobile devices to collect data from specific sensing areas at particular time. Due to budget constraints and presence of unreachable sensing data, traditional MCS can only collect incomplete or even sparse data in most cases. To this end, a modified paradigm called Sparse MCS [3] is proposed, which introduces inference strategies to complete the full sensing data from the partial ob- servations. Sparse MCS has already shown great advantages in some practical applications, such as the air quality monitoring [4], [5], traffic control [6], [7] and urban sensing [8], [9].\nIn Sparse MCS, data inference is the most essential part and has therefore received considerable attention. To reduce cost and simplify inference process, most of existing works study the data inference problem from a time-discrete perspective [10]\u2013[15]. For example, in Fig. 1, a requester would like to analyze urban traffic for a period. Upon receiving the task, existing works typically discretize the time period into units and aggregate the sensed data within each unit. Then, by assuming that the sensed data remains constant within each time unit, they use the data inference methods, such as compressive sensing [12], [16] or matrix completion [17], [18] to infer the missing data. However, in practical scenario, the sensing data changes continuously over time. Previous time- discrete approaches may cause significant errors on practical applications that are sensitive to change. For example, temper- ature or wind speed may fluctuates greatly within a short time due to severe weathers and the rough time-discrete method may fail to capture this dramatic local changes. Therefore, time-continuous data inference has become a crucial issue that urgently needs addressing for Sparse MCS.\nIn this paper, we adopt a time-continuous perspective, moving away from the traditional method of discretizing time into fixed units. This shift eliminates the assumption that data remains constant within a specific period, making it impossible to aggregate observed data within each time unit to reduce matrix sparsity. Consequently, our first challenge is to handle the extremely sparse data matrix. Additionally, in time- continuous scenarios, data is collected in real-time, leading to unequal lengths between sensed data intervals, which affects the relationships between consecutive data points. Thus, our second challenge is to model and utilize these unequal in- tervals effectively, maximizing the temporal information for accurate data inference. Finally, while time-discrete methods"}, {"title": "", "content": "could represent the problem with a fixed-size matrix, they cannot infer the continuously changing data at every moment. This leads to our third challenge: how to complete the data from a continuous perspective, ensuring comprehensive data inference across all moments. To tackle the challenges inherent in time-continuous data completion, we introduce a compre- hensive approach that begins by reformulating the problem and progresses to designing a method that addresses these challenges effectively. We start with fine-grained completion, which serves as an intermediary between traditional time- discrete methods and our final goal of continuous comple- tion. Unlike traditional time-discrete completion, which uses predefined unit lengths to discretize the timeline, fine-grained inference divides the timeline based on the actual distribution of submissions, ensuring that each time unit contains only one submission. This adjustment eliminates the need to assume data remains constant within each time unit, thereby offering a more accurate model. However, this approach significantly reduces the data volume in each unit, leading to an extremely sparse spatiotemporal matrix (i.e., our first challenge). To address this, we introduce Recurrent Neural Network-enabled Deep Matrix Factorization (RNN-DMF), a neural network- based framework for data completion that also incorporates a temporal encoder to fully leverage previously hidden infor- mation, addressing the challenge of sparsity.\nMoving beyond fine-grained completion, we recognize that it still does not fully capture the continuous nature of time. In time-continuous scenarios, we avoid discretizing the timeline entirely and handle each submission directly, preserving the precise arrival times and intervals between submissions\u2014our second challenge. To leverage this additional temporal infor- mation, we introduce Time Gates-enhanced Deep Matrix Fac- torization (TIME-DMF), which further captures the temporal dynamics within intervals through the use of time gates and a more sophisticated propagation pattern. Finally, addressing the challenge of representing the infinite number of moments within a period, we propose the Query-Generate (Q-G) strat- egy, which works in conjunction with TIME-DMF to model any moment on the timeline, thus providing a comprehensive solution to time-continuous data completion.\nOur work has the following contributions:"}, {"title": "", "content": "\u2022 We reformulate the problem of data inference from a time-continuous perspective. It pays attention to the conti- nuity of data changes and is a much closer approximation of practical MCS problems.\n\u2022 We introduce DMF which is neural network- enabled framework for data completion and extend it to RNN- DMF with a temporal encoder to handle the extreme matrix sparsity in fine-grained completion.\n\u2022 We propose TIME-DMF based on RNN-DMF with time gates to capture temporal information within intervals and its accompanying Q-G strategy which allows users to make queries and dynamically generate responses to achieve time-continuous completion.\n\u2022 Extensive experiments of five types are conducted step by step to validate the effectiveness of our methods.\nThe reminder of this paper is organized as follows. Section II reviews related works. Section III presents the system model and the problem formulation. In Section IV, we introduce the fine-grained completion, which comprises the DMF frame- work and RNN-DMF model. In Section V, TIME-DMF and its accompanying Q-G strategy for time-continuous completion are discussed. We evaluate the performance of our approaches through extensive experiments in Section VI, followed by the conclusion in Section VII."}, {"title": "II. RELATED WORK", "content": "Mobile CrowdSensing [1], [19] is an emerging paradigm that leverages mobile device users to collect data, enabling a wide range of services within the Internet of Things ecosystem [20]\u2013[22]. MCS has been widely used in domains such as traffic supervision [16], [23], pollution control [24], [25], and facility management [26], [27]. Initially, the mainstream algo- rithms for MCS were based on compressed sensing [16] and its various adaptations [12]. However, as these algorithms were implemented, it became apparent that data collection often exhibited sparsity [10] due to cost constraints and limitations of sensing devices. Consequently, algorithms used for inferring missing data [28] gained popularity. In 2016, Wang et al. [3] provided a comprehensive review of MCS methods based on sparse sensed data and systematically introduced frameworks [11], [13] for data collection and completion. Since then, Sparse MCS has emerged as an evolving paradigm, with many innovative algorithms being developed. Primary areas of work in this field include cell selection [29], data inference [17], [18] and user privacy protection [30].\nIn data inference, methods are generally categorized into two classes: dense-supervised [31], [32] and sparse-supervised [15], [17], [18]. Dense-supervised methods rely on large amounts of complete spatiotemporal data for training. Most Transformer models and their variants, which are powerful in handling time series, fall under this category. In contrast, sparse-supervised methods do not require complete spatiotem- poral data for training and instead rely on capturing correla- tions within sparsely observed data. Despite their differences, neither approach considers the continuous nature of time."}, {"title": "B. Spatiotemporal Granularity", "content": "The goal of sensing technology is to capture more fine- grained spatiotemporal information. Initially, this was achieved"}, {"title": "III. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "Fig. 1 and Fig. 2 showcase different methods for formulat- ing real-world spatiotemporal data. The majority of existing research has relied on the time-discrete approach depicted in the lower half of Fig. 1, which serves as a coarse-grained approximation of the real world. In this section, we will begin with the time-discrete formulation and subsequently introduce our novel time-continuous approach to provide a deeper understanding of our concept.\nIn Sparse MCS tasks, our sensing map is divided into N sub- regions. At moment t, a worker submits data of the \\(n^{th}\\) sub-region. We use a position vector \\(c(t) \\in R^N\\) and a data vector \\(y(t) \\in R^N\\) to represent the submission. The position vector \\(C(t) = [0,0,..., 1,0...0]\\) indicates the index of the subarea submitted by the worker. If the submitted data is from the i-th sub-region, the i-th element of c(t) is set to 1, and all other elements are set to 0. The data vector y(t) indicates the value of the submitted subarea. The i-th element of y(i) is the submitted value, and other elements of y(i) are set to meaningless values like 0 or negative numbers.\nAssuming that there are M submissions on the entire timeline, the results of M submissions are organized into position matrix and data matrix, we have \\(C \\in R^{N\\times M}\\) and \\(Y' \\in R^{N\\times M}\\) by stacking all the submissions together:\n\\[C = [c_1^T, c_2^T,\u2026\u2026, c_M^T],\\]\n\\[Y' = [y_1^T, y_2^T, \u2026\u2026\u2026, y_M^T].\\]\nSimilarly, \\(Y = [y_1, y_2,\u2026\u2026, y_M]\\) represents the real values of each sub-region at each submission time. So we have:\n\\[Y' = Y \\odot C,\\]\nwhere \\(\\odot\\) represents the Hadamard product.\nIn traditional Sparse MCS tasks, the timeline is evenly discretized into time units of predefined length, and we no more distinguish the difference of arriving time of submissions"}, {"title": "", "content": "within the same time unit. Suppose that we manually discretize the given timeline into P time units, we can have our new matrices \\(C^{(D)} \\in R^{N\\times P}\\) and \\(Y^{(D)'} \\in R^{N\\times P}\\) by putting each submission into the time unit they locate in and merge all the submissions within the same unit.\n\\[C^{(D)} = [c_1^{(D)T}, c_2^{(D)T},..., c_p^{(D)T}],\\]\n\\[Y^{(D)'} = [y_1^{(D)T}, y_2^{(D)T},..., y_p^{(D)T}],\\]\n\\[c^{(D)}, y^{(D)} = merge(c_{p0}, c_{p1},..., y_{p0}, y_{p1},\u2026),\\]\nwhere \\(c_{pi}\\) represents the i \u2013 th submission arrived within the p \u2013 th time unit and merge(\u00b7) represents the merging oper- ation. Technically, for each subarea, if there is a submission within the given p-th time unit, the corresponding position in \\(c_p^{(D)}\\) will be set to one and the submitted value will be filled in \\(y_p^{(D)'}\\). If there are multiple values observed for one location, the final value would be set to the average. By the merging operation, the sparsity of \\(C^{(D)}\\) will be much less than that of matrix C. After the discretization and merging operation, we no longer keep the specific arrival time of all submissions and their intervals but use p-index to represent their arrival time. Then our task is to infer the missing values in \\(Y^{(D)'}\\) to know the value of each location on the map at each time unit.\n\\[\\hat{Y} = f(Y^{(D)'}).\\]\nHowever, it is obvious that some temporal information has been ignored during the discretization and merging process such as the precise arrival time and the intervals. Inspired by this observation, we propose a new system model to reduce information loss during problem modeling phase. For the M submissions, we use an additional vector \\(T = [t^{(1)}, t^{(2)},\u2026\u2026, t^{(M)}]\\) to represent the accurate arrival time of each submission and no longer discretize C and Y' to \\(C^{(D)}\\) and \\(Y^{(D)'}\\). What's more, since in time-continuous completion each column in the observed matrix represents a specific moment instead of a period, we not only want to complete the matrix Y', but also the infinitely many moments that do not exist in Y'. In order to achieve this, we divide the challenging time-continuous completion task into to two subtasks.\nOur first task is to achieve a mapping f(.) only to complete the matrix Y' which fully utilizes the temporal information in"}, {"title": "", "content": "T. This is similar to the time-discrete scenarios.\n\\[\\hat{Y} = f(Y').\\]\nAfter that, we want to provide accurate inference for any moment on the time line. Obviously there are countless values distributed along the time line, and our solution is to find a model that can provide accurate responses \\(\\hat{y} \\in R^N\\) for any given time moment t.\n\\[\\hat{y} = g(Y',t), t \\in (t_0, t_M).\\]"}, {"title": "B. Problem Formulation", "content": "Problem [Completion and Generation on Continuous Time- line]: Given sparse sensed data Y' and time vector T, we aim to achieve the following two objectives:\n\u2022 Identify a mapping f(.) to complete all the unsensed data in the matrix Y'. The mapping f(\u00b7) should adequately consider the high sparsity of Y' and the temporal infor- mation in T.\n\u2022 Identify a model G(.) to accomplish the completion at any given time t'. y(t')' can be a column in Y' or not.\nIn this process, the mean square error is used to measure the quality of the completed and generated data. The following value should be minimized:\n\\[\\epsilon(Y,Y') = \\sum_i^N\\sum_j^M ||Y_{ij} - Y'_{ij}||\\]"}, {"title": "IV. FINE-GRAINED COMPLETION WITH RNN-DMF", "content": "Fine-grained completion is the first step towards time- continuous completion. With the insight that we can construct our spatiotemporal matrix by including each submission in a unique time unit, we could align with the previous time- discrete problem setting but eliminate the assumption that data stays constant within each time unit. We propose Deep Matrix Factorization (DMF) [17] as a foundational framework for the following works. Due to its neural network-enabled structure, it can be updated by adding modules of different functions. In fine-grained completion, there is one submission within each time unit so that we only know the information of one spatial location, leading to the great reduction of spatial information within a unit and the extreme sparsity of observation matrix. In order to solve this problem, we further propose Recurrent Neural Network-enabled Deep Matrix Factorization (RNN- DMF) by introducing a temporal encoder into DMF. The encoder is able to utilize temporal information to compensate for the loss of spatial information."}, {"title": "A. Deep Matrix Factorization (DMF)", "content": "Given a sparse matrix \\(Y_{mxn}\\), the traditional method is to take full rank decomposition of the estimated matrix \\(Y_{mxn}\\) by using the property that the real matrix \\(Y_{mxn}\\) has the lower rank and can be decomposed. Assuming that rank(Y) = r, then the full rank decomposition can be expressed as:\n\\[Y_{mxn} = P_{mxr}Z_{rxn},\\]\nwhere P is a full rank matrix and Z is a row full rank matrix. Therefore, any column \\(y_t\\) of \\(Y_{mxn}\\) can be expressed as \\(y_t = Pz_t\\). In this formula, \\(z_t\\) represents the low-rank vector that fully contains the information of the t-th column of \\(Y_{mxn}\\), which is also the t-th moment in real scenarios. P denotes the projection from the low-rank vectors to the inferred matrix. Note that Eq. (11) assumes that the spatiotemporal correlations between data are linear. To model the widely existing nonlinear spatiotemporal correlations, DMF was proposed. Similarly, we use the function f(.) to represent the nonlinear correlations between space and time.\n\\[y = f(z), Y = f(Z).\\]\nWe aim to obtain suitable z and f(.) by fitting them with a deep neural network (DNN). Specifically, assuming the neural network consists of K hidden layers, and their parameters are\n\\[W^* \\triangleq {W^{(1)}, W^{(2)},...,W^{(K)}, W^{(K+1)}},\\]\n\\[b^* \\triangleq {b^{(1)}, b^{(2)}, ..., b^{(K)}, b^{(K+1)}}.\\]\nThe corresponding activation function set is\n\\[g^* \\triangleq {g^{(1)}(\\cdot), g^{(2)}(\\cdot), ..., g^{(K)} (\\cdot), g^{(K+1)}(\\cdot)}.\\]\nIn the set \\(W^*, b^*, g^*\\), the (K + 1)th term represents the parameter or the activation function from the hidden layer K to its output layer. The nonlinear function is expressed as:\n\\[f(z) =g^{(K+1)}(W^{(K+1)}g^{(K)} (W^{(K)},...,\\]\n\\[g^{(1)} (W^{(1)} z + b^{(1)}) ... + b^{(K)}) + b^{(K+1)}).\\]\nDifferent from traditional neural networks, the input z and the neural network parameters are both optimizable parameters in this context. The optimization object is as follows:\n\\[min\\frac{1}{2}||(Y' - f(Z)) * C||.\\]\nIn this way, the inferred results can be obtained:\n\\[Y = f(Z).\\]\nThe detailed structure of DMF is shown in Fig. 4. DMF is a neural network framework that is already able to handle basic data completion tasks. However, it is not tailored for any specific scenarios. By inserting DNN modules of different functions into DMF, we can further enable DMF to deal with data of various specific properties."}, {"title": "B. Recurrent Neural Network-enabled Deep Matrix Factoriza- tion (RNN-DMF)", "content": "In each time step, the proposed DMF framework has low- rank vectors that can be concatenated to form the complete low-rank representation of spatiotemporal data. While in the"}, {"title": "V. TIME-CONTINUOUS COMPLETION WITH TIME-DMF", "content": "In the previous fine-grained completion, we address the challenges of the reduction of spatial information and the extreme sparsity of observation matrices. Based on that, we can finally introduce the time-continuous completion. In traditional works and fine-grained completion, the intervals between submissions are ignored during the discretization stage. However, in real-life scenarios, users submit data at real time and the interval between submissions are of great impor- tance for inference. This oversimplification is also the reason why traditional time-discrete completion methods cannot fully capture temporal information.\nIntuitively, it is not the absolute arrival time of submissions but their intervals that matter. The sensing data with a long period of time interval will vary significantly but the sensing data at adjacent time may be quite similar. This is the intuition of Time Gates-enhanced Deep Matrix Factorization (TIME- DMF) for our introducing time gates and a more complex propagation pattern to model the influence of intervals on correlations between time steps. Furthermore, in fine-grained scenarios, period is sliced into a finite number of units and the observation matrix is of fixed size. But from a time- continuous perspective, data changes continuously and there are infinitely many states of sensed data in a given period. In order to characterize data of all states at an acceptable price, we propose Q-G strategy. It allows users to query data of any time and leverages the generative ability of TIME-DMF to dynamically respond. By combining TIME- DMF and the Q-G strategy, we achieve the ultimate time-continuous completion. In this section, we will introduce the details of TIME-DMF and the Q-G strategy. Finally, we will conclude the complete flow of TIME-DMF algorithm for time-continuous data completion tasks."}, {"title": "A. Time Gates in TIME-DMF", "content": "The inner structure of TIME-DMF is shown in Fig. 6. Inspired by [41], we design two types time gates to manage a more complex pattern of information propagation. The first"}, {"title": "", "content": "training process, each low-rank vector is learned separately and is not visible to each other. This limits the ability of DMF to fully capture temporal information. However, capturing and utilizing temporal information is an urgent in fine-grained completion due to the extreme sparsity of matrices. For this reason, we further propose Recurrent Neural Network-enabled Deep Matrix Factorization (RNN-DMF).\nFig. 6 shows that RNN-DMF is composed of two key modules: an upstream RNN-enabled encoder and a down- stream DMF-like decoder. Unlike DMF, which initializes low-rank representations randomly, RNN-DMF accounts for relationships between low-rank vectors. The RNN structure enables the encoder to generate low-rank representations by incorporating temporal correlations, sharing parameters U, W, and V across time steps. The hidden state \\(S_t\\) is generated based on the previous state \\(S_{t-1}\\) and the primary low-rank vector \\(X_t\\) at each timestamp.\n\\[S_t = f(U \\cdot X_t + W \\cdot S_{t-1}).\\]\nAs DMF, \\(S_0\\) and the primary low-rank vector of each step is randomly initialized and serves as optimizable parameters during training. The encoded low-rank representation is then generated by a final projection. This process is similar to what traditional RNN does.\n\\[Z_t = g(V \\cdot S_t).\\]\nAt this step, we finally have our encoded low-rank vectors with temporal information integrated. They will then be con- catenated and decoded, serving as the final completion results. This is done by the downstream DMF-like decoder:\n\\[Z = [z_1^T, z_2^T,..., z_M^T],\\]\n\\[Y = f(Z).\\]\nIt is obvious that RNN-DMF performs strictly superior to DMF theoretically. This is because DMF generates its low- rank vectors randomly and independently without consider- ing temporal correlations. RNN-DMF considers the possible temporal correlations between low-rank vectors during the generating process, which is at least better than complete random. When sensed data is extremely sparse, there is very limited information for use within each time step, making it urgently necessary to share information between time steps."}, {"title": "B. Q-G Strategy", "content": "Given a sparse observation matrix, RNN-DMF or TIME- DMF can infer the missing data. However, unlike previous methods relying on discrete slicing, in time-continuous sce- narios, there are infinitely many possible moments on the timeline. Thus, it is impossible for the prior Submit-Complete strategy to infer data at all moments. To overcome this chal- lenge, we propose the Query-Generate strategy which allows users to query data for any moment within the given period and dynamically generate the results. To use TIME-DMF for data generation, we leverage its property of completing data by first generating low-rank representation. We can insert a randomly initialized low-rank vector into the previous low-rank repre- sentation and then proceed with the usual completion process. The only difference is that the inserted vector does not affect the backward gradient propagation process."}, {"title": "C. The Complete Flow of TIME-DMF", "content": "For a time query provided by the user, we first check if there is an existing column in the given sparse matrix. If not, we insert an empty column into matrix Y' and update the intervals of its both sides. Concurrently, we insert a column with all zeros into matrix C to ensure the query won't affect existing completion process. We then deploy our encoder and decoder sequentially to complete the data matrix Y' and minimize the loss between the completed data and known data at observation positions. In this process, the encoder, decoder and the completed results are continuously updated by calculating and reducing the optimization objective. The complete algorithm flow is shown in Alg. 1."}, {"title": "VI. EXPERIMENTAL VALIDATION", "content": "In this section, we first introduce the datasets and the comparison methods. Then we present performance evaluation of our proposed results. In particular, our experiments can be divided into the answers to the following research questions:\n\u2022 RQ1: Does our RNN structure offer better completion performance for extremely sparse matrices?\n\u2022 RQ2: Do our time gates truly leverage the time interval?\n\u2022 RQ3: In time-continuous, the model needs to have additional generative capabilities. How effective is the generative capability of TIME-DMF?\n\u2022 RQ4: Even our method achieves time-continuous com- pletion, is time-continuous completion truly more effec- tive than time-discrete completion?"}, {"title": "", "content": "Algorithm 1 Deep Matrix Factorization with Time Gates (TIME-DMF)\nRequire: \\(C_{N\\times M}\\), \\(Y_{N\\times M}\\), t, T = \\([t_1, t_2,\u2026, t_M]\\)\nEnsure: \\(\\hat{Y}\\)\n1: Random Init x;\n2: if t \\(\\notin T\\) then\n3: \\(T \\leftarrow [t_1, t_2,...,t_k,t,t_{k+1},...,t_M]\\);\n4: \\(x \\leftarrow [X_1,X_2,\u2026, X_k, X, X_{k+1},..., X_M]\\);\n5: \\(C_{N\\times M} \\leftarrow C_{N\\times (M+1)}, Y_{N\\times M} \\leftarrow Y_{N\\times (M+1)}\\);\n6: end if\n7: while not convergent do\n8: for \\(x^{(1)}\\) to \\(x^{(M)}\\) do\n9: \\(z_t, \\~{C}_t, C_t \\leftarrow encoder(x^{(t)}, \\~{C}_{t-1}, C_{t-1})\\);\n10: end for\n11: \\(\\hat{Y} \\leftarrow decoder([z_1^T, z_2^T,\u2026\u2026, z_M^T]);\\)\n12: calculate and reduce \\(\\|(Y' - \\hat{Y}) * C\\|;\\)\n13: end while"}, {"title": "", "content": "time gate controls global memory. All data within the observed time period should follow a certain underlying distribution, reflecting the overall characteristics of the dataset, such as the mean or periodicity in real-life scenarios. The second time gate controls local memory. The distribution of data at adjacent time shows the local increase/decrease or other short- term trends. We combine these two memories to ensure the completion result is rational both globally and locally.\nThe two kinds of memories are controlled by different parameters and are updated independently. Intuitively, intervals determine the correlations between time steps. The shorter the interval between two time steps, the more similar they are expected to be. So we utilize intervals to control the update of memories. If the memories are slightly updated between the two adjacent time steps, their completion results will be very similar. Conversely, if the memories have a substantial update, the following results will greatly defer from the previous ones. We use the interval as a parameter within each time gates.\n\\[T_{1m} = \\sigma(x_mW_{x1} + \\sigma_{\\Delta t}(\\Delta t_mW_{t1}) + b_1),\\]\n\\[T_{2m} = \\sigma(x_mW_{x2} + \\sigma_{\\Delta t}(\\Delta t_mW_{t2}) +b_2).\\]\nThe function of two memories is similar to that of the hidden state in RNN, which is shown in Eq. (19).\nIn TIME-DMF, the global memories and local memories operate in different manners. The local memory has a more direct impact on the completion of each step:\n\\[a_t = f(U \\cdot X_t + W \\cdot \\~{C}_{t-1}).\\]\nThe global memory flows more consistently and has a broad effect on the update of memories. We use the two time gates calculated formerly to control the update of the two memories. In this way we make use of the information within intervals.\n\\[\\~{C}_t = f[C_t \\odot T_{1m} + a_t \\odot (1 - T_{1m})],\\]\n\\[C_t = f[C_t \\odot T_{2m} + a_t \\odot (1 - T_{2m})].\\]\nLastly, as the local memory can better assess the data status at the current moment, it is converted into an low-rank vector"}, {"title": "", "content": "representing time t through the output gate.\n\\[Z_t = f(V \\cdot \\~{C}_t).\\]\nAt this point, the upstream encoder has been obtained. The encoder is capable of doing time-continuous completion as it can deal with submissions at any given moments. We further use the DMF-like decoder to map the low-rank representation to yield the final completion results. Similar to RNN-DMF, TIME-DMF adopts a joint training method for the upstream and downstream networks."}, {"title": "VII. CONCLUSION", "content": "In this paper, we propose a time-continuous completion method called TIME-DMF to challenge a widely existing assumption in Sparse MCS that data stays constant within peri- ods and finally increase data completion accuracy to a great ex- tent. TIME-DMF is based on DMF which is a neural network-enabled framework for traditional data completion. Based on that, TIME-DMF is further inserted with a temporal encoder that has the function of passing temporal information between time steps and utilizing the intervals of different lengths. To pass temporal information, we imitate the structure of RNN to generate appropriate embedding vectors. For utilizing the temporal information within intervals, we design the global memory to control the overall property of data distribution and the global memory to control local trend. The length of interval serves as a parameter to monitor the memory updating process. TIME-DMF is used in cooperation with the Q-G strategy which allows users to query and then dynamically generate responses. Finally, we do extensive experiments on real-world datasets to prove the effectiveness of our models."}]}