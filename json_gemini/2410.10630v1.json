{"title": "THINKING LLMS: GENERAL INSTRUCTION\nFOLLOWING WITH THOUGHT GENERATION", "authors": ["Tianhao Wu", "Janice Lan", "Weizhe Yuan", "Jiantao Jiao", "Jason Weston", "Sainbayar Sukhbaatar"], "abstract": "LLMs are typically trained to answer user questions or follow instructions simi-\nlarly to how human experts respond. However, in the standard alignment frame-\nwork they lack the basic ability of explicit thinking before answering. Thinking is\nimportant for complex questions that require reasoning and planning \u2013 but can\nbe applied to any task. We propose a training method for equipping existing\nLLMs with such thinking abilities for general instruction following without use\nof additional human data. We achieve this by an iterative search and optimiza-\ntion procedure that explores the space of possible thought generations, allowing\nthe model to learn how to think without direct supervision. For each instruction,\nthe thought candidates are scored using a judge model to evaluate their responses\nonly, and then optimized via preference optimization. We show that this procedure\nleads to superior performance on AlpacaEval and Arena-Hard, and shows gains\nfrom thinking on non-reasoning categories such as marketing, health and general\nknowledge, in addition to more traditional reasoning & problem-solving tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) are based on the Transformer architecture (Vaswani et al., 2017)\nthat predicts the next token at each step. Each token takes the same amount of compute, so when\nLLMs are prompted with a user instruction, they have a fixed compute budget to generate the first\nresponse token regardless of the instruction's complexity. One way to increase the compute budget\nfor harder instructions is to allow LLMs to think internally before outputting an response. This is\nsimilar to humans who will take more time and think before answering complex questions.\nOne approach is to generate thoughts as text, which takes advantage of the natural language capa-\nbilities of LLMs. LLMs are pre-trained on text containing human-written thoughts, which are hence\nencoded into the model. Chain-of-Thought (CoT) (Wei et al., 2022) is a widely used prompting\ntechnique that elicits such behavior by asking the model to write down its reasoning steps. However,\nthe usage of CoT has been mostly limited to math and reasoning tasks. Meta-analysis by Sprague\net al. (2024) found CoT methods to be unhelpful on tasks that do not involve math and logic.\nIn this paper, we focus on general instruction following instead of focusing on math or logic tasks.\nWe argue that \u201cthinking\u201d should have broad utility. For example, in a creative writing task, internal\nthoughts can be used to plan overall structure and characters. In other tasks, internal thoughts can be\nused for understanding the user instruction better. Of course, it is likely that less thinking is required\nfor simpler tasks, and more thinking for more complex ones. In general, we hypothesize that such\nThinking LLMs will have an advantage on all sufficiently complex tasks. The emergence of recent\ncommercial products like OpenAI-O1 (OpenAI) also support our motivation.\nHowever it is challenging to train a model to think due to the lack of supervised training data.\nAlthough pre-training data does contain valuable information, coverage can be limited in certain\ndomains as internal thoughts are often omitted in human writing. Existing post-training datasets\ntypically consist of human responses, or preferences over responses, with no information on thought\nprocesses. The same is true for existing reward models. Combined with the difficulty and cost\nconsiderations of collecting human thought data, these factors impose a barrier in training Thinking\nLLMs."}, {"title": "2 THOUGHT PREFERENCE OPTIMIZATION", "content": "We now describe our Thought Preference Optimization (TPO) method for teaching LLMs to think\nbefore responding, as depicted in Figure 1. We start with a typical instruction-tuned LLM that\noutputs a response directly after the user instruction. We assume that there is no provided labeled\nthought data that we can finetune on, which makes training much more challenging. Instead, as a\nstarting point to bootstrap our training process, for a given training user instruction, we prompt the\nmodel to generate its thought process followed by the response. Sampling multiple such outputs,\nwe then use preference optimization to improve the quality of thoughts (and paired responses) based\nsolely on the quality of the responses."}, {"title": "2.1 GENERATING THOUGHTS FROM THINKING LLMS", "content": "Ideally, thought generation should be simple and compatible with existing LLM infrastructures.\nHence, we keep the model architecture the same, as an autoregressive Transformer, although our\nmethod is potentially compatible with any model that outputs a sequence of tokens. At inference\ntime, the core process is that the output consists of two parts: a thought part followed by a response\npart, both of which are in natural language. After generation, instead of directly sending that entire"}, {"title": "2.2 OPTIMIZING THOUGHTS VIA PREFERENCE OPTIMIZATION", "content": "While our initial thought prompting generates thoughts via the instruction tuned model, they are\nnot optimized to be actually useful in making the response better. We find they typically underper-\nform thoughtless direct responses, which instruction-tuned LLMs have been heavily optimized for."}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 SETUP", "content": "We use Llama-3-8B-Instruct (Dubey et al., 2024) as a seed model in our training. As a judge model,\nwe consider two choices of model: Self-Taught Evaluator (STE) (Wang et al., 2024b) and ArmoRM\n(Wang et al., 2024a). STE is a LLM-as-a-Judge model based on Llama-3-70B-Instruct. Given two\nresponses, it outputs its preference in natural language after generating a CoT. ArmoRM is a 8B\nreward model that directly outputs a scalar score to a single response.\nFor initial experiments, we use the synthetic instructions from Yuan et al. (2024b) for training. These\ninstructions are generated from Llama-2-70B-Chat using 8-shot prompting consisting of random\nsamples from the Open Assistant dataset (K\u00f6pf et al., 2024). For later experiments, we switched to\nUltraFeedback (Cui et al., 2023), which contains actual human instructions. Each training iteration\nuses 5000 instructions that were not part of the previous iterations.\nWe generate $K = 8$ responses per prompt using temperature 0.8 and top-p of 0.95. We train for\n10 epochs in each iteration and select the best checkpoint using a validation set of 1500 prompts\nrandomly sampled from UltraFeedback. We perform up to 4 iterations. We usually set the length-\ncontrol parameter $\\rho\\in [0,0.5]$, with 0 equivalent to no length-control. Unless otherwise specified,\nwe use the specific thought prompt trained using the ArmoRM judge on UltraFeedback instructions\nas the default setup.\nAs a baseline, we train the same seed model that outputs responses directly without any thinking\n(note, this can still perform CoT as a part of the response due to its initial instruction training). We\ntrain this baseline in the exactly same way, using the same judge, data and loss. This allows us to\ndirectly measure the effect of the thoughts on response quality."}, {"title": "3.2 ALPACAEVAL RESULTS", "content": "The highest win rate our model TPO achieves is 52.5%, which is +4.1% better than the direct\nbaseline, as shown in Table 1. It is also a +27.6% increase over the seed model and puts our method\nin 3rd position on the leaderboard\u00b9, just after GPT-4 Omni and GPT-4 Turbo. This is an impressive\nresult given the small size (8B) of our model.\nAs of Sep. 27th 2024."}, {"title": "3.3 ARENA-HARD RESULTS", "content": "Results on the Arena-Hard benchmark are shown in Table 1 (right) and Figure 3 (right). They follow\na similar trend to the results from AlpacaEval. Thinking performance is poor with the initial seed\nmodel at the start of training, but with more training iterations it matches the direct baseline and starts\nto outperform it. TPO reaches a win rate of 37.3%, which is +4.3% better than the baseline. This\nmakes our model the best model on the leaderboard with a such small size\u00b2. It performs similarly\nto much larger models like GPT-4 (06/13) or Mistral Large (24/02). Detailed results comparing\ndifferent experimental setups are shown in Appendix Table 4."}, {"title": "3.4 FINE-GRAINED EVALUATION", "content": "While the above benchmarks evaluate overall performance, they lack granularity to inform which\ntypes of instructions benefit from thinking. To obtain a more fine-grained evaluation, we build our\nown evaluation using UltraFeedback. We take instructions not used in training, and assign them\nindividually to one of 20 categories until each category has 200 samples.\nTo measure the performance on this dataset, we compare responses generated from our TPO model\nagainst responses from the direct baseline model. We use the evaluation prompt from Arena-Hard\nand GPT4 (gpt-4-1106) as a judge. The ordering of responses is randomized to reduce position-\nbias. Figure 4 shows the win rates of TPO on all 20 categories. Surprisingly, we observe that non-\nreasoning categories obtain large gains through thinking. This includes language and translation,\nmarketing and health. We also see improvement in reasoning categories like research and analysis,\nmath and calculations. See Appendix A for more details and additional results."}, {"title": "3.5 ANALYSIS AND ABLATIONS", "content": "In this section, we present analysis and several ablation results to further understand TPO.\nQualitative analysis of thoughts In Figure 5, we present an example of a non-reasoning task to\nillustrate the broader utility of thinking. While writing a poem is not typically viewed as a reasoning\ntask, it can benefit from better planning and understanding of the instruction. This is also a good\nexample of when it makes sense to hide the thought process. Figure 6 shows the thinking of a TPO\nmodel trained with the specific thought prompt on a factoid question (which dog breed is smallest).\nWe see that the model thinks first about the question, then evaluates its draft response. We provide\nmore thought examples in Appendix C.\nThought Prompt Types Table 3 compares different thought prompt types in terms of different\nmetrics across iterations. As we have observed previously, the seed model performs poorly when it\nis asked to generate thoughts compared to directly generating a response, which is the result of the\nmodel being well optimized to do the latter. However, after one iteration of training, we see the gap\nis shrinking. Between the two thought prompt types we try, there is not much difference in terms of\nwin rate, but the thought and answer lengths vary greatly. Also, the specific thought prompt leads to\nlonger responses and have a higher average ArmoRM score (see Appendix Figure 21).\nThought lengths As shown in Table 3, the specific thought prompt has the longest thought gen-\nerations because it asks the model to generate a draft response and evaluate it within the thought.\nAlthough we did not directly supervise the thought process, we found the model learns to shorten\nand condense the thought throughout the training. Table 3 shows the length changes after 1 training\niteration, but more detailed statistics across training are given in Appendix Table 6. We find that"}, {"title": "4 RELATED WORK", "content": "Reasoning in Language vs. Vectors: In this work we focus on thinking that is explicitly in natural\nlanguage. Thinking in words takes advantage of the natural language understanding capability of\nLLMs. LLMs are trained on large pretraining corpora of human text, which contain human thoughts\nexpressed in natural language, and this thinking ability is hence encoded into the model. While\nthinking in continuous values might provide more bandwith, the Transformer architecture already\ncan compute continuous vectors as hidden states and feed them to the next layer. However, these\nhidden vectors do not feed back into the model at the next token, and thus are not accessible to the\nfuture lower layers (Fan et al., 2020). Word tokens on the other hand are fed back to the model\nimmediately, i.e. during inference the previous output token is fed as input to predict the next token\nmaking it possible to condition all future computations on them (Merrill & Sabharwal, 2024).\nAnother advantage of word tokens is that there exist simple sampling mechanisms which allow\nthoughts to take different paths each time (Wang et al., 2023), which can be used to improve results\ne.g. via majority vote.\nChain-of-Thought (CoT): CoT prompting (Wei et al., 2022) demonstrated that LLMs perform\nbetter at reasoning tasks when they are encouraged to write down intermediate reasoning steps.\nSince the type of thinking in CoT is dictated by the prompt instruction, there are now many different\nvariations of it facilitating different types of reasoning, such as decomposing into smaller problems\n(Zhou et al., 2023). It is now widely used for math and reasoning tasks, and most current LLMs are\nfinetuned to do CoT by default for those types of tasks (Dubey et al., 2024). Other works like Pfau\net al. (2024) show that the model equipped with CoT might be able to perform hidden thinking using\nfiller tokens. However, CoT usage has had more limited use in other types of tasks. Meta-analysis\nby Sprague et al. (2024) found that CoT techniques have little benefit outside of math and logic\nrelated tasks.\nTraining to Think: There have been other previous efforts to train LLMs to think. Nye et al.\n(2021) trained a model to write intermediate calculations into a scratchpad section before writing\nthe final answer, which improved performance in math and coding tasks. Similarly Lehnert et al.\n(2024) showed that Transformers can solve complex planning tasks if they are trained to write A*\nsearch traces before outputting the solution. However, these methods rely on supervised training\nso ground-truth thought data is required. STaR (Zelikman et al., 2022) removes this constraint by\ngenerating both thought and answer from a model using few-shot prompting. Then the generations\nare filtered by the correctness of the answer to be used for supervised finetuning. It also has an option\nto feed correct answers to the model to generate better thought candidates. It was applied to multi-\nchoice reasoning and math tasks where the correct answers were available. Its generalization Quiet-\nSTaR (Zelikman et al., 2024) aims to insert thought segments into unstructured text. This involves\nsampling a sequence of thought tokens after every input token, then training using a REINFORCE\nbased loss that optimizes the likelihood of subsequent input tokens. While it showed promising\nresults in multi-choice reasoning and math tasks, the training mechanism is complex and compute\nheavy. V-STaR (Hosseini et al., 2024) trained a DPO verifier on both correct and incorrect solutions\nand uses the verifier to select the response in inference time. IRPO (Pang et al., 2024) also trains a\nvariant of DPO on math and reasoning problems to learn CoTs, assuming access to gold labels on\nthe training set. Similarly, Self-Notes (Lanchantin et al., 2023) allows the model to deviate from the\ninput at any time to write its thoughts, but relied on supervised training data in symbolic tasks. None\nof these methods have been applied to general instruction following using LLMs.\nSystem 2 methods: Many system 2 methods emerged in recent years that add intermediate steps\nat inference time before producing a final answer. Those steps typically involve prompting the\nmodel with a certain goal, such as verification of the answer (Dhuliawala et al., 2024), rephrasing\nuser questions (Deng et al., 2023), selecting sentences to attend to (Weston & Sukhbaatar, 2023),"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduced Thinking LLMs, which think in natural language before writing a\nresponse for general instruction-following tasks. To train such models, we proposed a new train-\ning recipe called Thought Preference Optimization for teaching Thinking LLMs to improve their\nthoughts. Unlike prior methods (Snell et al., 2024; Kumar et al., 2024b), which directly supervise\nthe thought generation process through techniques like self-correction or self-refinement, we instead\nprovide incentives for the model to generate its own thoughts, without explicitly teaching it how to\nthink. In our experiments, we train and evaluate the models in the general instruction following\nsetup. The results on benchmarks show that the initial seed model and first iterations of training of\nthe Thinking LLM perform poorly compared to the typical direct response model. However, after\nmultiple iterations of training using TPO, our method outperforms the baseline. Further, fine-grained\nevaluations reveal that thinking helps in categories that are not usually associated with reasoning or\nchain-of-thought methods. This is an encouraging result and hopefully leads to wider adoption of\nThinking LLMs in non-reasoning domains."}, {"title": "6 LIMITATIONS", "content": "We experimented with two different thought prompts, and observed some performance differences\nbetween them. It is likely that certain thought types are suited for certain tasks, and direct responses\nwould even work better in certain situations. Therefore, training on a diverse set of thought prompts\nand allowing the model to switch between them could potentially lead to further improvements in\nperformance. This would allow the model to better search the space of possible thoughts in order to\nlearn to choose the most appropriate ones. However, we have not conducted these experiments.\nWhile we see improvement in overall performance with TPO, evaluation on GSM8K showed de-\ngraded math performance. As we discussed, this is likely due to our setup not being oriented toward\nsuch tasks. Incorporating more math instructions during training and having access to a judge capa-\nble of evaluating of their answers are likely solutions.\nIn the current version of the method, thought lengths are purely determined by model itself. There is\nno steerability in terms of changing the number of thought tokens. Adding such functionality could\nbe useful as longer thoughts increase computation and corresponding cost per user instruction. We\ncould use techniques like Yuan et al. (2024a) for this purpose.\nAll our experiments are based on 8B parameter sized models. However, it is worth investigating the\neffect of thinking on larger scale models. Given the compute requirements of such experiments, we\nleave that to future work."}]}