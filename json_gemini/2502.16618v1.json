{"title": "Can Large Vision-Language Models Detect Images Copyright Infringement from GenAI?", "authors": ["Qipan Xu", "Zhenting Wang", "Xiaoxiao He", "Ligong Han", "Ruixiang Tang"], "abstract": "Generative AI models, renowned for their ability to synthesize high-quality content, have sparked growing concerns over the improper generation of copyright-protected material. While recent studies have proposed various approaches to address copyright issues, the capability of large vision-language models (LVLMs) to detect copyright infringements remains largely unexplored. In this work, we focus on evaluating the copyright detection abilities of state-of-the-art LVLMs using a various set of image samples. Recognizing the absence of a comprehensive dataset that includes both IP-infringement samples and ambiguous non-infringement negative samples, we construct a benchmark dataset comprising positive samples that violate the copyright protection of well-known IP figures, as well as negative samples that resemble these figures but do not raise copyright concerns. This dataset is created using advanced prompt engineering techniques. We then evaluate leading LVLMs using our benchmark dataset. Our experimental results reveal that LVLMs are prone to overfitting, leading to the misclassification of some negative samples as IP-infringement cases. In the final section, we analyze these failure cases and propose potential solutions to mitigate the overfitting problem.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of generative artificial intelligence (GenAI) has ushered in a new era of content creation, enabling the synthesis of high-quality text, images, and multimedia content at an unprecedented scale. While these innovations have expanded creative possibilities and applications across industries, they have also raised significant ethical and legal concerns, particularly regarding intellectual property (IP) rights (Sag, 2023; Poland). One of the most pressing issues is the unauthorized reproduction of copyrighted material, where generative models may inadvertently produce outputs that closely resemble or replicate IP-protected content (Zirpoli, 2023; Dzuong et al., 2024; Sag, 2023; Poland; Wang et al., 2024). This issue has led to widespread debates among legal experts, policymakers, and AI researchers on the potential liabilities and regulatory measures required to address copyright infringement in AI-generated content.\nExisting efforts to mitigate copyright concerns in generative models have primarily focused on two key approaches: \u2460 reducing memorization during training using techniques such as differential privacy (Dwork et al., 2014), which limits the retention of specific data points to prevent models from reproducing protected content (Abadi et al., 2016; Chen et al., 2022; Dockhorn et al., 2022), and 2 employing prompt engineering strategies that discourage the generation of IP-infringing material through explicit negative prompts (Wang et al., 2024; He et al., 2024) or optimized safe prompt modifications (Chin et al., 2023; Rando et al., 2022). While these approaches offer some level of control over generative outputs, they do not directly address the challenge of detecting copyright infringement in already-generated content. As a result, there is an urgent need for robust evaluation methods and benchmarks to assess the ability of AI models-specifically large vision-language models (LVLMs)\u2014to identify potential instances of copyright violations.\nVision-language models (VLMs), which integrate both textual and visual data to enable cross-modal reasoning, have demonstrated remarkable capabilities in tasks such as image classification, visual question answering (VQA) (Antol et al., 2015), and multimodal understanding. Notable VLMs such as CLIP (Radford et al., 2021), large vision-language models (LVLMs) such as GPT-40 (GPT4V), Claude 3.5 (Claude3.5), VILA-2.7b (Lin et al., 2023), and Qwen-VL (Bai et al., 2023) have been trained to interpret and generate content based on textual and visual inputs, making them prime"}, {"title": "2 Related Work", "content": "candidates for assessing IP infringement detection. However, despite their extensive deployment in various applications, the effectiveness of LVLMs in identifying copyright-protected content remains largely untested. Given the increasing reliance on these models in content moderation, digital rights management, and automated compliance monitoring, it is crucial to evaluate their ability to detect copyright infringement.\nTo address this gap, our work presents a systematic evaluation of LVLMs for copyright detection by constructing a dedicated benchmark dataset. Recognizing the absence of a comprehensive dataset that includes both clear cases of IP infringement and ambiguous non-infringing samples, we create a dataset comprising:\n\u2022 Positive samples that contain well-known IP characters generated using different AI models with direct and descriptive prompts that replicate their distinctive features.\n\u2022 Negative samples that resemble IP characters in certain aspects but do not fully qualify as copyright violations. These images are generated using modified negative prompt engineering techniques.\nThese images are selected through rigorous human annotation after the generation.\nOur dataset focuses on five widely recognized fictional characters: Iron Man, Batman, Spider-Man, Superman, and Super Mario, ensuring a balanced representation of both positive and negative samples. To evaluate the effectiveness of VLMs, we conduct experiments using in-context learning (ICL) (Mann et al., 2020; Dong et al., 2022) and zero-shot learning (ZSL) (Wang et al., 2019) approaches, where models are tested on their ability to classify image samples accurately.\nOur findings indicate that while LVLMs exhibit strong recall in detecting potential copyright violations, they often suffer from overfitting and exhibit a tendency to classify ambiguous samples as infringing content, leading to a high rate of false positives. This issue highlights a fundamental challenge in using LVLMs for automated copyright detection-these models may prioritize superficial visual similarities rather than deeper conceptual understanding of IP infringement. To address this limitation, we propose a set of mitigation strategies, including contrastive learning techniques that refine the models' ability to differentiate between genuine IP violations and non-infringing variations.\nThe contributions of our work are fourfold:\n\u2022 Introduction of a novel benchmark dataset specifically designed to evaluate the copyright detection capabilities of LVLMs, incorporating both positive and negative samples.\n\u2022 Comprehensive analysis of leading LVLMs, including GPT-40, Claude 3.5, Vila 2.7b and Qwen-VL, across multiple experimental settings, assessing their strengths and weaknesses in copyright infringement detection.\n\u2022 Identification of failure cases and potential solutions, highlighting key challenges in current LVLM-based detection approaches and proposing improvements to enhance accuracy and robustness.\n\u2022 By systematically investigating the role of LVLMs in copyright detection, our study provides valuable insights into the potential and limitations of AI-driven content moderation tools. Our findings underscore the need for continued research in this space to develop more reliable, ethically responsible, and legally compliant AI models capable of safeguarding intellectual property rights in the digital age."}, {"title": "2.1 Vision Language Models", "content": "In recent years, Vision-Language Models (VLMs) have significantly advanced the integration of visual and textual data, leading to more sophisticated AI applications. A notable example is CLIP, which employs contrastive learning to align images and text in a shared latent space, enabling zero-shot image classification and cross-modal retrieval (Radford et al., 2021). Building upon such foundations, Large Vision Language Models (LVLMs) like GPT-4 have extended capabilities to process both textual and visual inputs, enhancing tasks such as image description and visual question answering (GPT4V). Similarly, Anthropic's Claude 3.5 has been developed to handle multimodal inputs, contributing to advancements in understanding and generating content across different modalities (Claude3.5). Further contributions include LLaVA, which integrates visual features into language models to improve visual reasoning (Liu"}, {"title": "2.2 Copyright Issues Related to Generative Models.", "content": "The rapid advancement of generative AI enables the creation of text and images that closely mimic human-authored works, leading to significant legal and ethical concerns regarding potential infringements of intellectual property rights (Zirpoli, 2023; ; Dzuong et al., 2024; Sag, 2023; Poland). A key contributing factor is that visual generative models may memorize portions of their training data, resulting in outputs that inadvertently reproduce IP-protected content (Carlini et al., 2023; Somepalli et al., 2023; Gu et al., 2023). To mitigate IP infringement, two primary approaches have emerged:\n\u2022 Reducing Memorization During Training: Implementing differential privacy (Dwork et al., 2014) techniques during the training of generative models can help minimize the retention of specific data points, thereby reducing the risk of reproducing protected content (Abadi et al., 2016; Chen et al., 2022; Dockhorn et al., 2022).\n\u2022 Prompt Engineering: Employing strategies such as negative prompts during the inference phase can exclude undesired concepts or elements from the generated output(Wang et al., 2024; He et al., 2024), or optimizing unsafe prompts (Chin et al., 2023; Rando et al., 2022), thereby avoiding the inclusion of IP-protected material.\nDespite the widespread copyright concerns surrounding generative AI and the numerous IP mitigation approaches recently proposed, the issue of benchmarking VLM IP infringement detection remains largely underexplored. As a result, a primary focus of our paper is to address the capabilities of VLMs in detecting and mitigating IP infringement."}, {"title": "2.3 In-context Learning", "content": "In-context learning, as discussed in (Mann et al., 2020; Dong et al., 2022), is a paradigm where large language models (LLMs) perform tasks by conditioning on a prompt that includes a few examples, enabling them to adapt to new tasks without explicit parameter updates. An in-context learning prompt generally includes two main parts: demonstrations and a new query. Demonstrations consist of several question-answer examples, each providing a full question along with its corresponding answer. The new query is a fresh question presented to the model for response. What's more, recent studies (Zhou et al., 2024; Monajatipoor et al., 2023; Li et al., 2024; Zhang et al., 2023) have demonstrated that vision-language models (VLMs) can also effectively facilitate in-context learning: Given a few images, or masks as examples, the VLMs could perform segmentation, classification, and visual question answering (VQA) (Antol et al., 2015) in effective ways."}, {"title": "3 Evaluation Benchmark Dataset", "content": "To evaluate the intellectual property (IP) infringement detection capabilities of LVLMs, the availability of comprehensive benchmark datasets is crucial. However, the lack of such datasets, especially those containing ambiguous negative IP infringement samples, poses a significant challenge for researchers aiming to thoroughly assess the detection abilities of different LVLMs.\nTo bridge this gap, we have introduced a benchmark dataset, which includes images of widely recognized IP characters to support VLM evaluations. Our dataset curation and selection adhere to the following principles: \u2460 IP Renown: To effectively evaluate the ability of LVLMs to infringe on intellectual property (IP), our dataset includes both positive and negative samples derived from well-known, IP-protected figures. \u2461 Diversity: A comprehensive assessment of LVLM performance requires diverse image generation. Thus, we ensure a wide range of IP characters are included, with each character type synthesized using multiple generation techniques. \u2462 Reproducibility: Our dataset is built using open-source generative models with well-defined prompts, allowing for easy replication by other entities.\nAs a result, we focused on five iconic figures: Iron-Man, Batman, Spider-Man, Superman, and Super Mario. For each character, we gathered 200 images, ensuring a balanced distribution of positive and negative samples, all of which were meticulously labeled through human evaluation (refer"}, {"title": "3.1 Collecting Images", "content": "In this section, we outline the principles behind generating and curating benchmark images using Stable Diffusion XL (SDXL) (Podell et al., 2023), Ideogram (ideogramAI), DALL-E (Betker et al., 2023), and Stable Diffusion XL-PerpNeg. Notably, SDXL-PerpNeg is an adaptation of the SDXL model (Podell et al., 2023) incorporating the Perp-Neg method (Armandpour et al., 2023), which effectively mitigates image-based IP infringement. A detailed discussion on the impact of SDXL-PerpNeg is provided in the appendix."}, {"title": "3.1.1 Collecting Positive Samples", "content": "In this section, we outline two methods for generating positive samples that infringe on copyright protection laws. For this purpose, we selected three widely-used generative AI models: Stable Diffusion XL (generated 40% of the image samples) (Podell et al., 2023), Ideogram (generated 40% of the image samples) (ideogramAI), and DALL-E (generated 20% of the image samples) (Betker et al., 2023).\nGenerate IP Characters with Direct Prompt. The simplest approach to generating positive samples involves using direct prompts with generative models, such as \"Generate an image of <a character>.\" This method typically produces images that closely resemble the IP-protected characters, as illustrated in Fig. 2. Using this technique, we generated 40 images for each character class.\nGenerate with Descriptive Prompt. Rewriting direct prompts that reference copyright-protected content into longer, more descriptive prompts, as explored by Wang et al. (2024) and He et al. (2024), can sometimes reduce the risk of IP infringement. However, this approach is not entirely effective in preventing outputs that closely resemble copyrighted characters (He et al., 2024), as rewritten prompts often retain a high degree of similarity to the original IP-associated names.\nTo enhance the diversity of our dataset, we first generate images using descriptive prompts and then apply a human evaluation process to filter out most positive samples, as detailed in Section 3.2. We use GPT-40 (GPT4V) here as its exceptional text generation capabilities. We construct descript prompt with the following guidance to GPT-40:\n\u2022 Creating a prompt that describes a character similar to <Target Character>. This prompt should enable text-to-image AI models to generate images without directly mentioning the name of the <Target Character>.\nFinally, we curate the selected positive images, as illustrated in Fig. 3."}, {"title": "3.1.2 Collecting Negative Samples", "content": "Generate with Plain Negative Prompts. Negative prompts are commonly utilized in deploying diffusion models to enable users to exclude unwanted concepts or elements from the generated output. Incorporating the negative prompt through classifier-free guidance (Ho and Salimans, 2022), the predicted noise gradient $o(z_t, t, c)$ between timestamp t and t-1 can be written as\n$\\begin{equation}\\check{\\epsilon}\\_{o}(z_{t}, t, c, d) = \\epsilon\\_{\\theta}(z_{t}, t, c)-w(\\epsilon\\_{\\theta}(z_{t}, t, d)-\\epsilon\\_{\\theta}(z_{t},t))\\end{equation}$$\nwhere $z_t$ is the noise at timestamp t, $\\epsilon_{\\theta}$ is the UNet of diffusion models, c is the non-negative text prompt, d is the negative prompt, and w is the scale parameter of the classifier-free guidance (we set w to default value 7.5). By combining negative prompts, such as IP character names or commercial brands, the generative model can be guided away from synthesizing features or contents that resemble the infringing characters, as shown in Fig. 4.\nGenerate with Non-overlapped Negative Prompts (NNP). When the input text prompt c and the negative prompt d have overlapping semantics, composing positive and negative prompts linearly could lead to undesired results, particularly in cases of concept negation. Perpendicular negative prompt, as discussed in (Armandpour et al., 2023), improved the overlap problem by applying the"}, {"title": "3.2 Labeling Images", "content": "After gathering the images, we employ human annotators to label the dataset samples. The process begins by selecting 50 images from the entire dataset that appear ambiguous or challenging to classify. We then ask 10 participants to determine whether the character in each image resembles the target character. These participants are pre-screened to ensure they are familiar with all the target characters. Once the responses are collected, we calculate an agreement score, with the majority opinion among the participants serving as the final label for each image."}, {"title": "4 VLM IP Infringement Detection Experiments", "content": "All model that boasts impressive benchmark scores. VILA-2.7b is a vision-language model designed to integrate visual and textual information for comprehensive understanding and generation tasks. InternVL2 is a pioneering open-source alternative to GPT-40. Qwen-VL and DeepSeek-VL2 are state-of-the-art multimodal models that rivals Claude Sonnet and GPT-40, with open weights. We compare and analyze the infringement detection capability of these models on our benchmark dataset using in-context learning and zero-shot VQA.\nEvaluation Metrics. To evaluate the IP infringement detection capabilities of LVLMs on our benchmark dataset, we employed a diverse set of evaluation metrics that capture different aspects of their performance. These metrics provide a well-rounded assessment of each model's strengths and weaknesses in identifying IP infringement cases.\nWe assess the LVLM detection performance using the following key metrics:\nPrecision: Measures the proportion percentage of"}, {"title": "4.1 Experimental Settings", "content": "Dataset. In our experiments, we use our benchmark dataset as discussed in Sec. 3, which contains famous IP characters: Iron-Man, Batman, Spider-Man, Superman, and Super Mario, with each class contains challenging hard negative samples.\nLarge Vision Language Models. We evaluate seven large vision-language models (LVLMs) for their intellectual property infringement detection capabilities: GPT-40 (GPT4V), GPT-4o mini (GPT4V), Claude 3.5 (Claude3.5), VILA-2.7b (Lin et al., 2023), Qwen-VL-7b (Bai et al., 2023), DeepSeek-VL2-1b (Wu et al., 2024) and Intern-VL2-2b (Chen et al., 2024). Developed by OpenAI, GPT-40 and GPT-40 mini are multilingual, multimodal generative pre-trained transformers capable of processing and generating text, images, and audio. Anthropic's Claude 3.5 Sonnet is an"}, {"title": "4.2 Evaluate with In-context Learning", "content": "To effectively utilize in-context learning for IP infringement detection, we begin by providing the vision-language model with a set of labeled examples, including both positive and negative instances of intellectual property (IP) content. These examples are accompanied by text prompts that clearly indicate whether they are positive samples (legally protected IP) or negative samples (content free from copyright concerns). This approach helps the model learn to distinguish between protected and non-protected contents within the given context. Subsequently, we give the LVLMs an image sample, and query whether the image infringes any existing IP-character. The output from the vision-language model serves as the final determination for identifying potential infringement in the given image, as shown in Fig. 6. This approach leverages the model's ability to analyze and interpret visual content in conjunction with textual information, enabling it to assess whether the image contains elements that may violate intellectual property rights. By evaluating the image through this model, it is possible to detect unauthorized use of copyrighted material or other forms of infringement."}, {"title": "4.3 Evaluate with Zero-shot VQA", "content": "We also investigate the IP infringement detection with zero-shot VQA. This is achieved by directly asking vision-language models whether the given image samples has copyright issues."}, {"title": "4.4 Experimental Results and Analysis", "content": "In Table 2, we evaluate the performance of LVLMs using in-context learning (ICL) and compare it with their performance with zero-shot learning across various intellectual property (IP) metrics from our benchmark dataset. The results demonstrate that IP infringement detection with in-context learning significantly outperforms detection with zero-shot learning, highlighting the effectiveness of integrating ICL with LVLMs for this task. Among the evaluated models, GPT-40 mini achieves the highest performance in IP infringement detection. Additionally, the results reveal that all models maintain a high recall score but exhibit relatively low precision. This suggests that LVLMs are more sensitive to positive models than negative models, and tend to classify image samples as positive cases more frequently, potentially leading to the misidentification of numerous negative samples as instances of IP infringement."}, {"title": "5 Failure Case Analysis", "content": "In this section, we analyze the failed detection cases produced by LVLMs. The results in Table 2 indicate that all LVLMs tend to exhibit high recall values while maintaining relatively low precision. This suggests that the primary source of misjudgment in LVLMs is false positive samples. Consequently, we delve deeper into false negative cases to understand their underlying causes and explore potential solutions.\nWe specifically examine false positive samples from the GPT-40 mini model combined with in-context learning and analyze the reasoning behind its classification of images as positive cases, as shown in Table 3. The results reveal that the GPT-40 mini model primarily focuses on specific features, such as the \"golden armor\" worn by Iron Man, the \"mustache\" of Super Mario, and the \"mus-"}, {"title": "6 Conclusion", "content": "In this paper, we present a novel benchmark dataset specifically designed to evaluate the copyright detection capabilities of Large Vision-Language Models (LVLMs). Our dataset incorporates both positive and negative samples, carefully engineered and manipulated through input text prompts to rigorously assess model performance. Furthermore, we conduct a comprehensive analysis of state-of-the-art LVLMs, including GPT-40, Claude 3.5, Vila 2.7b, Qwen-VL-7b, DeepSeek-VL2-1b, and Intern-VL2-2b across diverse experimental settings. Our evaluation of six cutting-edge LVLMs reveals significant shortcomings in identifying and detecting intellectual property (IP) infringement, particularly when faced with challenging negative samples. To further investigate these limitations, we analyze failure cases\u2014specifically, false positives\u2014by probing the reasoning behind LVLM misclassifications. Our findings suggest that while LVLMs focus on specific features of potentially infringing images, they lack the holistic judgment necessary for accurate copyright assessment. These insights highlight a pressing need for dedicated benchmarks to guide the development and validation of copyright-specific LVLMs, ensuring their effectiveness in real-world applications."}, {"title": "7 Limitations", "content": "Challenges in Dataset Annotations. While this paper introduces a dataset containing IP infringement samples to facilitate the evaluation of LVLMs, it does not fully account for the legal thresholds of copyright infringement, which vary across jurisdictions and involve complex interpretations beyond mere visual similarity. For instance, U.S. copyright"}, {"title": "A The Effectiveness of Non-overlappped Negative Prompt (NNP)", "content": "In this section, we compare the performance of plain negative prompts and non-overlapped negative prompts. To evaluate text-to-image alignment, we utilize the CLIP score (Rando et al., 2022). Additionally, we assess the IP infringement rate through human evaluation, where a panel of five inspectors determines whether the generated images resemble the target IP characters. The results are presented in Table 4."}, {"title": "B Dataset Images and Their Corresponding Prompts", "content": "In this section, we will display dataset images and their corresponding prompts."}, {"title": "B.1 Descriptive Prompt Generation", "content": "We generate the descriptive prompt by employing a large language model. And we use GPT-40 (GPT4V) here as its exceptional text generation capabilities. We construct descript prompt with the following guidance to GPT-40:\n\u2022 Creating a prompt that describes a character similar to <Target Character>. This prompt should enable text-to-image AI models to generate images without directly mentioning the name of the <Target Character>."}, {"title": "B.2 Visualization", "content": "In this section, we will display image samples with their corresponding classes, input prompts, and generative models which produced them, as shown in Table 4."}, {"title": "C Prompt for Failure Case Analysis", "content": "The following prompts are used to identify and analyze failed false positive cases, as discussed in Section 5:\n\u2022 Could you provide a plausible reason for identifying this image as a violation of the intellectual property rights of <Target IP Character>?"}, {"title": "D Possible Solutions for Improving IP Infringement Detection", "content": "Since IP figures often include background elements that may interfere with LVLMs in detecting potential IP infringements, one possible approach to enhancing detection is leveraging the Segment Anything Model (SAM)\u2014a state-of-the-art image segmentation model. Specifically, we apply SAM with text prompts corresponding to the IP figures to generate masks that isolate these figures within the images. We then follow the procedures outlined in Section 4 using images that contain only the masked IP figures.\nExperimental results in Table 6 indicate a slight improvement in precision and recall when combining SAM with LVLMs. However, the enhancement is marginal, and the approach lacks significant innovation and novelty. These findings suggest that achieving substantial advancements in IP infringement detection remains a challenging problem in this domain."}]}