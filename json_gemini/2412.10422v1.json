{"title": "AutoPrEP: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework", "authors": ["Meihao Fan", "Ju Fan", "Nan Tang", "Lei Cao", "Xiaoyong Du"], "abstract": "Answering natural language (NL) questions about tables, which is referred to as Tabular Question Answering (TQA), is important because it enables users to extract meaningful insights quickly and efficiently from structured data, bridging the gap between human language and machine-readable formats. Many of these tables originate from web sources or real-world scenarios, necessitating careful data preparation (or data prep for short) to ensure accurate answers. However, unlike traditional data prep, question-aware data prep introduces new requirements, which include tasks such as column augmentation and filtering for given questions, and question-aware value normalization or conversion. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose AUTOPREP, a large language model (LLM)-based multi-agent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AUTOPREP performs data prep through three key components. Planner: Determines a logical plan, outlining a sequence of high-level operations. Programmer: Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor: Iteratively executes and debugs the generated code to ensure correct outcomes. To support this multi-agent framework, we design a novel Chain-of-Clauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation. Extensive experiments on real-world TQA datasets demonstrate that AUTOPREP can significantly improve the SOTA TQA solutions through question-aware data preparation.", "sections": [{"title": "1 INTRODUCTION", "content": "Tabular Question Answering (TQA) refers to the tasks of answering natural language (NL) questions based on provided data tables [10, 11, 20, 25]. TQA empowers non-technical users such as domain scientists to easily analyze tabular data and thus has a wide range of applications, including table-based fact verification [3, 9] and table-based question answering [23, 26]. Because TQA requires NL understanding and reasoning over tables, state-of-the-art solutions [11, 36, 40\u201342] naturally rely on large language models (LLMs). These solutions either utilize LLMs as black-boxes or leverage them for code generation. However, question-aware data preparation is not explicitly explored in these approaches, which limits the effectiveness of LLM-based methods. Figure 1 presents the results of an error analysis of LLM-based code generation (using GPT-4) across two TQA tasks: table-based question answering on the WikiTQ dataset [26] and table-based fact verification on the TabFact dataset [9]. The analysis reveals that 84% and 76% of the errors stem from insufficient data prep at various granularities (i.e., table-level, column-level, and cell-level) in relation to the NL questions. A similar trend is observed when using LLMs as black-boxes, though those results are not displayed here due to space constraints. These findings underscore the necessity for meticulous data prep to ensure accurate answers. To better understand these cases, let's illustrate by examples."}, {"title": "2 PRELIMINARIES", "content": "This section first introduces Tabular Question Answering (TQA) in Section 2.1 and then formalizes the problem of question-aware data prep for TQA in Section 2.2. Then, we analyze the limitation of existing LLM-based methods in Section 2.3."}, {"title": "2.1 Tabular Question Answering", "content": "Let Q be a natural language (NL) question, and T a table consisting of m columns (i.e., attributes) A1, A2,..., Am and n rows r1, r2,..., rn, where $v_{ij}$ denotes the value in the i-th row and j-th column of the table. The problem of tabular question answering (TQA) is to generate an answer, denoted as A, in response to question Q based on the information in table T. By the purposes of the questions, there are two main types of TQA problems: (1) table-based fact verification [3, 9], which determines whether Q can be entailed or refuted by T, and (2) table-based question answering [23, 26], which extracts or reasons the answer to Q from T."}, {"title": "2.2 Data Prep for TQA", "content": "Unlike traditional data prep, for question-aware data prep for TQA, we need to tailor table T to the specific needs of question Q. Consider Example 2: careful data prep, such as extracting countries into a new column and then normalizing them as ISO codes, is crucial for generating an accurate answer. Moreover, it is important to note that different questions may require different data prep tasks, even for the same table. For instance, country extraction may not be necessary if the question is asking which cyclist, instead of country, won the most medals.\nData Prep Operations. To meet the new requirements of data prep for TQA, this paper defines high-level data prep operations (or operations for short) to formalize the question-aware data prep tasks, such as column augmentation, question-aware value normalization or conversion, and column filtering. Formally, an operation, denoted as o, encapsulates a specific question-aware data prep task that transforms table T into another table T', i.e., T' = o(T). Specifically, this paper considers the following three types of operations.\n\u2022 Augment: a data prep task that augments a new column for table T based on existing columns, to better answer Q. This task typically involves column combinations via arithmetic operations, value extraction and inference, etc.\n\u2022 Normalize: a data prep task that normalize types or formats of the values in a column of T based on the needs of Q. This task typically involves value representation or format normalization, type conversion, etc.\n\u2022 Filter: A data prep task that filters out columns in T that are not relevant to answering question Q. This is crucial for handling large tables to address the input token limitations and challenges in long-context understanding of LLMs [22].\nGiven a high-level operation $o_i$, we define $f_i$ as its low-level implementation, either by calling a well-established algorithm from"}, {"title": "2.3 Limitations of Existing LLM-based Solutions", "content": "Naturally, LLMs can be utilized to support question-aware data prep for TQA. A straightforward way is to prompt an LLM with a task description and several demonstrations (i.e., examples), and requires the LLM to generate data prep code, as shown in Figure 3. However, as shown in Figure 2, the diversity of NL questions necessitates distinct data prep operations. Next, let's see how a single LLM is used to solve this problem, by an example."}, {"title": "3 AN OVERVIEW OF AUTOPREP", "content": "To address the limitations, we propose AUTOPREP, a multi-agent LLM framework that automatically prepares tables for given NL questions. Figure 4 provides an overview of our framework. Given an NL question Q posed over table T, AUTOPREP decomposes the data prep process into three stages:\n(1) PLANNER Agent: the Planning stage. It guides the LLM to suggest high-level data prep operations O = {$o_1, o_2, ..., o_{|O|}$}, which are tailored to specific question Q,\n(2) Multiple PROGRAMMER Agents (i.e., AUGMENTER): the Programming stage. It directs the LLM to generate low-level implementation $f_i$ (e.g., Python code) for each operation of customized for the table T.\n(3) An EXECUTOR Agent: the Executing stage. It executes the given low-level code and debugs iteratively to improve the generated code quality."}, {"title": "4 THE PLANNER AGENT", "content": "Given a question Q posed over a table T, PLANNER aims to generate a logical plan that outlines a sequence of high-level operations needed to prepare the table for answering the question. In this section, we first introduce a direct prompting method in Section 4.1 that better demonstrates the key challenge in designing the PLANNER agent. Then, to address the challenge, we propose our Chain-of-Clauses (CoC) reasoning method in Section 4.2."}, {"title": "4.1 A Direct Prompting Method", "content": "The most common way to generate a logical plan is to directly prompt an LLM using a typical in-context learning approach, as shown in Algorithm 1. The inputs are a question Q, a table T, a set \u2211 of specifications for each operation type, and an LLM 0. Here, each specification \u03c3\u2208\u03a3 describes the purpose of an operation type, e.g., \"an Augment operation augments a new column for a table based on existing columns, in response to the specific needs of a question\u201d. The output of the algorithm is a set O of high-level operations like those shown in Figure 4, e.g., Normalize(\u201cCast to INT\u201d, Medal). Specifically, Algorithm 1 first prepares a set D of k exemplars {(Ti, Qi, \u03a3, Oi)}=1 that demonstrates to LLM 0 the goal of this logical plan generation task. Then, the algorithm prompts the LLM to infer the operations O for our input table T and question Q."}, {"title": "Key Challenge", "content": "The above example clearly demonstrates that the key challenge in designing the PLANNER agent, as discussed earlier, is to capture the relationships between different parts of question Q and the columns in table T. For instance, as shown in Figure 4, the Normalize operation on the Date column is generated by considering two key factors: (1) the question contains \"in February\", and (2) the values in the Date column are inconsistent."}, {"title": "4.2 A Chain-of-Clauses Method", "content": "To address this challenge, we propose a Chain-of-Clauses (CoC) reasoning method the decomposes the entire process of logical plan generation into two phrases, as shown in Figure 5(b).\n\u2022 The first phase leverages an LLM to generate an SQL-like Analysis Sketch, outlining how the table should be transformed to produce the answer.\n\u2022 The second phase iteratively examines different clauses in the Analysis Sketch, e.g., Date LIKE '02-%'. For each clause, we associate the corresponding data in T (e.g., values in column Date) with it to prompt the LLM for generating possible high-level operations (e.g., Normalize).\nCompared with existing CoT methods [36], which simply break down questions into sub-questions, our approach is more effective for logical plan generation due to the following reasons. First, our method decomposes the question into a set of analysis steps over the table, formalized as an SQL-like Analysis Sketch, which simplifies the task of logical plan generation. More importantly, our method jointly considers each analysis step along with the corresponding relevant data to prompt the LLM, effectively capturing the relationships between the question and the table.\nSpecifically, Algorithm 2 presents the pseudo-code of our CoC reasoning method, which takes as input a question Q, a table T, a specification set 2, and an LLM @ to produce an operation set O. To this end, we introduce a key concept of SQL-like Analysis Sketch, which is formally defined as follows.\nSQL-like Analysis Sketch. An Analysis Sketch S is an SQL-like statement, which can be represented as follows.\nSELECT A | agg(A) | f({A}) FROM T\nWHERE Pred(Ai) AND AND Pred(Aj)\nGROUP BY A ORDER BY A LIMIT n\nwhere agg(A) is an aggregation function (e.g., SUM and AVG) over column A, Pred(Ai) denotes a predicate (i.e., filtering condition) over column Ai, such as Date LIKE '02-%. Note that f({A}) is a user-defined function (UDF) that maps existing columns {Ai} in the table into a new column. Figure 5(b) shows an example Analysis Sketch with a UDF function that specifies a new column Country, i.e., f(T) AS Country. Obviously, this UDF is introduced because the Analysis Sketch contains a GROUP BY Country clause.\nGiven the key concept Analysis Sketch, Algorithm 2 produces a logical plan via two phases, as shown in the following example."}, {"title": "5 THE PROGRAMMER & EXECUTOR AGENTS", "content": "The PROGRAMMER agents translate a high-level logical plan into a physical plan by generating low-level code, which is then passed to an EXECUTOR agent for iterative code execution and debugging. A straightforward approach is to prompt an LLM with the logical plan using typical in-context learning with several exemplars, asking the LLM to generate code for each high-level operation in the plan. The code is then executed iteratively, and if any runtime errors occur, the DEBUGGER is prompted with the error messages for debugging. However, the above method may have limitations, as the generated code may be overly generic and unable to effectively address the heterogeneity challenge of the tables. Specifically, many tables originate from web sources or real-world scenarios, where values have diverse syntactic (e.g., \"19-Oct\u201d and \"9/14\u201d in T6) or semantic formats (e.g., \"ITA\" and \"Italia\" in T4), making it difficult to generate code customized to these tables. For example, consider table T4 in 2b. Given a Normalize operation to standardize country formats,"}, {"title": "5.1 Tool-Augmented Method: Key Idea", "content": "Figure 6 provides an overview of our tool-augmented method for physical plan generation and execution. Given a table T and a set O = $o_1,..., o_{|O|}$ of high-level operations, the method iteratively generates and executes low-level code for each individual operation $o_i$ \u2208 O, thus generating a sequence of intermediate tables $T_1$ (i.e., the input T), $T_2$, ... $T_{|O|+1}$. Specifically, in the i-th iteration, given a high-level operation oi (e.g., Augment) and an intermediate table Ti, the method generates low-level code and executes it to produce an updated table Ti+1 through the following two steps.\nFunction Selection and Argument Inference. The method first prompts the LLM to select a specific function from a function pool F corresponding to the operation type (e.g., FAug) and infer the arguments (e.g., regular expression) for the selected function. For instance, given the Augment operation over table Ti shown in Figure 6, the LLM selects a function from the pool Faug designed specifically for Augment, obtaining an extract function with two arguments: column and func. The LLM then generates preliminary code for these arguments, assigning Cyclist to the column argument and generating a lambda function with a specific regular expression for argument func."}, {"title": "5.2 Search Space of Function Pools", "content": "This section presents a search of function pools for the different types of high-level operations formalized in the current AUTOPREP.\nFunction pool for Augment. We define a specific PROGRAMMER agent, i.e., Augmenter, to generate low-level code for the operation type Augment from the following function pool Faug.\n\u2022 extract. This function extracts a substring from a column to generate a new column. It has two arguments: column, representing the name of the source column, and func, representing the substring extraction function. Using table T1 from Figure 2a as an example, to extract the country code, we would set column to Cyclist and func to the lambda function lambda x: re.search(r'(.*?)',x).group(1).\n\u2022 calculate. This function generates a new column through arithmetic operations of columns such as addition, subtraction, multiplication, or a combination of these. To use this operator, we need to provide two arguments: columns and func. The argument columns should be a list of source column names. Similar to extract, the func argument is typically a lambda function. However, for calculate, the input to the function should be a dictionary of values rather than a string, since we may perform calculations on multiple columns. For example, given a table with two columns A and B, representing the scores of team A and team B, we need to generate a new column ScoreDifference. In this case, we set columns to ['A', 'B'] and func to the specific lambda function lambda x: x['A']-x['B'].\n\u2022 map-to-boolean and concatenate. Both functions generate a new column from multiple columns. The difference is that one generates boolean values, while the other concatenates new strings. Both operators take two arguments, columns and func, similar to those in the calculate operator.\n\u2022 infer. This function directly uses LLMs to deduce values that could not be processed by the functions above. It takes a list of source columns and the failed target values as input, learning from demonstrations of successfully processed values to directly output the target values.\nFunction pool for Normalize. We define a specific PROGRAMMER agent, i.e., Normalizer, to generate low-level code for the operation type Normalize from the following function pool FNorm.\n\u2022 to-numerical. This function standardizes a column to a numerical type, such as int or float. To use it, two arguments need to be specified: column, representing the source column, and func, representing a lambda function. For example, given the table T5 in Figure 2b, we use this function to standardize the Females column to integers, with column set to Females and func as lambda x: int(x.replace(',',\")).\n\u2022 format-datetime. The function standardizes the format of columns with DateTime values. It has two arguments: column and format. The format argument specifies the desired format for standardization. Given the table T6 in Figure 2b, to make the values in the Date column comparable, we use the format-datetime function with column set to Date and format set to \"%m-%d\".\n\u2022 clean-string. To clean string values in tables, we define the function clean-string. It has two arguments: column and transdict. The transdict is a dictionary where each key k and its corresponding value v represent that k in column should be replaced with v. Given the original table T4 in Figure 2b, we set transdict to {'Italia': 'ITA'}.\n\u2022 infer. Similar to the one for the Augment operation, this function uses LLMs to deduce values that could not be processed by the functions above.\nFunction pool for Filter. We design the Normalizer agent to utilize a function pool FFilter with a single function, filter-columns. This function has one argument, rel_columns, which represents a list of question-related column names.\""}, {"title": "5.3 Code Generation and Execution", "content": "Algorithm 3 presents the pseudo-code of our tool-augmented approach for code generation and execution. The algorithm takes as input a question Q, a table T, a set O of high-level operations, three function pools, and an LLM, and produces a prepared table $T_{|O|+1}$.\nThe algorithm iteratively examines the operations in O. In each iteration, it first generates a function from the corresponding function pool and refines it through multiple rounds of debugging. The algorithm then applies the generated function to the corresponding intermediate table and proceeds to the next iterations."}, {"title": "6 EXPERIMENTS", "content": "We evaluate our proposed framework AUTOPREP using two well-adopted benchmarking datasets WikiTQ [26] and TabFact [9]. The statistics are shown in Table 1."}, {"title": "6.1 Experimental Setup", "content": "Datasets. We evaluate our proposed framework AUTOPREP using two well-adopted benchmarking datasets WikiTQ [26] and TabFact [9]. The statistics are shown in Table 1.\n\u2022 WikiTQ. WikiTableQuestions (WikiTQ) is a dataset designed for QA tasks, containing complex questions annotated by crowd workers based on diverse Wikipedia tables. WikiTQ comprises 17,689 question-answer pairs in the training set and 4,344 in the test set with significant variation in table size. The answers in WikiTQ can be categorized into three forms: (i) string from a cell in the table, (ii) string from analyzing, and (iii) list of cells in the table.\n\u2022 TabFact. Table-Fact-Checking (TabFact) dataset provides a collection of Wikipedia tables, along with manually annotated NL statements. One requires to deduce the relations between statements and tables as \u201ctrue\u201d (a statement is entailed by a table) or \u201cfalse\u201d (a statement if refuted by a table). To reduce the experimental cost without losing generality, we choose the small test set provided by [21] which contains 2024 table-statement pairs. The tables in TabFact are much smaller than those in WikiTQ. The answers are binary with nearly equal proportions.\nEvaluation Metrics. We adopt the evaluator in Binder [11] to avoid the situation where program executions are likely to be semantically correct but fail to match the gold answer exactly. For example, when the SQL outputs 1/0 for yes/no questions, the Binder evaluator will conduct a pre-matching check and translate it to boolean values first. It will avoid mismatches of correct prediction and golden answer pairs like 7.45 and 7.4499, 4 years and 4, etc. Afterwards, we adopt accuracy as our evaluation metric.\nBaselines. We evaluate AUTOPREP using two categories of methods: (a) TQA w/o Data Prep: Zero-Shot QA and Few-Shot QA [8], as well as Text-to-SQL [29]; (b) TQA with Data Prep: Binder [11] and ReAcTable [42]. We further describe them below.\n\u2022 Zero-Shot and Few-Shot QA [8]. Zero-Shot QA directly asks an LLM to generate the answer based on the serialized table and question. To improve the performance, Few-Shot QA further provides several demonstrations to enhance the In Context Learning ability of the LLM. We implement these two methods with the prompt provided by [11].\n\u2022 Text-to-SQL [29]. Text-to-SQL is a natural code generation method for the TQA tasks. Consider a serialized table and a question, it first generates the SQL program and then executes it to get the final answer from the table for the question.\n\u2022 Binder [11]. Binder further enhances the Text-to-SQL method by integrating LLMs into SQL programs. It uses LLMs to incorporate external knowledge bases and directly answer questions that are difficult to resolve using SQL alone. We utilize the code provided by [1].\n\u2022 ReAcTable [42]. ReAcTable uses the ReAct paradigm to extract relevant data from the table using Python or SQL code generated by LLMs. Once all relevant data is gathered, it asks the LLMs to predict the answer. We run the original code from [2] and kept all settings as default.\nImplementation Details. We mainly use DeepSeek [14] API2 model as the foundation model for each LLM-based agent. We also explore other proprietary LLMs, such as GPT-3.5 [5], using the GPT-3.5-turbo-0613 version, as adopted by [35]. To ensure reproducibility, we set the temperature parameter for all agents to 0.01."}, {"title": "6.2 Data Prep Method Comparison", "content": "Exp-1: Comparison of AutoPREP with other data prep methods for TQA. We conduct experiments to evaluate AUTOPREP against current state-of-the-art data prep methods for TQA on two datasets: WikiTQ and TabFact. Specifically, we compare AUTOPREP with Binder and ReAcTable, presenting the results in Table 2. Notably, due to the lack of public scripts for TabFact from ReAcTable [42], we report results only for DeepSeek on WikiTQ and use GPT-3.5 results from their original paper."}, {"title": "6.3 Improvement of Data Prep for TQA", "content": "Exp-2: Impact of data prep on TQA performance. We evaluate the performance of three baseline TQA methods, namely Zero-Shot QA, Few-Shot QA, and Text-to-SQL with and without the data prep provided by AUTOPREP. The results are detailed in Table 3.\nAs demonstrated, integrating AUTOPREP significantly improves the performance of all evaluated methods. Notably, Text-to-SQL"}, {"title": "6.4 Evaluation on Planner Agent", "content": "Exp-3: How does our proposed Analysis Sketch method in suggesting high-level operations compared with Direct Prompting method? We change the high-level operation suggestion methods in Planner and evaluate the framework variants on WikiTQ and TabFact dataset. The results are reported in Table 4.\nAs shown in the table, Analysis Sketch based method outperforms Direct Prompting method by 12.52% and 3.67%, which indicates the superiority of our proposed method in generating more accurate high-level operations. Moreover, we find that the performance improvement on WikiTQ is much more significant than that in TabFact. This is because the WikiTQ dataset has relatively large tables and complex questions, which could make the high-level operation suggestion problem more challenging to be solved using direct prompting. In Summary, the experimental results prove that our proposed Analysis Sketch method is more effective in suggesting high-level operations, especially for difficult TQA instances."}, {"title": "6.5 Evaluation on Programmer Agents", "content": "Exp-4: How does our proposed tool-augmented method in generating low-level operations compared with existing code generation methods? We change the low-level operation generation methods in the PROGRAMMER agents and keep other settings of AUTOPREP as default. We evaluate the performance on WikiTQ and TabFact and report the results in Table 4.\nAs illustrated in Table 4, our proposed tool-augmented method achieves better performance compared with the existing code generation method by an improvement of 4.19% and 5.42%. Considering the high-level operations input to the Programmer agents are the same, we can conclude that selecting function from a function pool and then completing its arguments can generate more accurate and high-quality programs to implement the high-level operations. Moreover, when using demonstrations constructed from same table-question pairs for the prompt of programmer agents, Tool-augmented method can save 18.07% input tokens for low-level operation generation. The above experimental results show the superiority of our method in both accuracy and cost.\nMoreover, to compare the debugging method of these two methods, we record the average debugging rounds in Table 5. Our tool-augmented method conduct debugging process with fewer rounds, implying the effectiveness of our proposed debugging method for tool-augmented method."}, {"title": "6.6 Ablation Study", "content": "Exp-5: What is the contribution of each agent in AUTOPREP? We conduct ablation experiments to evaluate the contribution of each agent in AUTOPREP. Specifically, we ablate each PROGRAMMER agent including FILTER, AUGMENTER and NORMALIZER and compare the performance with AUTOPREP."}, {"title": "6.7 Case Study", "content": "This section presents two illustrative examples selected from the WikiTQ dataset to qualitatively analyze effectiveness of AUTOPREP.\nAs shown in Figure 8a, to answer the question based on the original table, a Text-to-SQL method tries to filter all records with a score greater than 30. However, due to the limitation of SQL grammar, it can only use the operation Like to extract records where number 31, 32, or 39 is in values of column Score. This could miss out values with score greater than 40. Thus, it outputs error answer \u201c3\u201d. AUTOPREP improves the performance of TQA methods by solving question-related data prep issues of the table in two stages. Specifically, based on the question in Figure 8a, we first suggest two required high-level operations, i.e., a Filter to select the related column Score and a Augment to generate a new column from Score representing whether the team score is over 30. Then, we select appropriate functions from specific function pool and implement corresponding arguments to implement these high-level operations. Finally, we output a prepared table which only has columns related to the question. By passing the processed table and original question into the Text-to-SQL method, it generate a SQL that can easily extract the correct answer \u201c4\u201d.\nFigure 8b illustrates a TQA instance with redundancy and inconsistency issues. To answer the question, it requires to first extract records with AirDate previous to \u201cNovember 15, 2007\u201d and then select records with RankOverall between 30 to 39. However, the values in column AirDate can not be directly assessed for chronological order through string comparison and RankOverall values are not in numerical type. Thus, although the generate SQL query is logically correct, it produces an wrong answer. AUTOPREP suggests three high-level operations to solve this. Specifically, AUTOPREP adopts an Filter to extract all question-related columns. And utilize two Normalize operations to format values of AirDate into sortable strings and cast RankOverall column to the integer"}, {"title": "7 RELATED WORK", "content": "Tabular Question Answering. Most of the SOTA solutions for TQA rely on Large Language Models (LLMs) [5, 14], as TQA requires NL understanding and reasoning over tables. The existing solutions can be broadly divided into two categories: end-to-end reasoning [36] and code generation [11, 40\u201342]. End-to-end reasoning approaches, which prompt an LLM to directly generate answers through next token prediction [36], may struggle when processing large tables due to the input token limitation and long-context understanding challenges [22]. On the other hand, code generation approaches leverage LLMs to generate programming code (e.g., Python) that extracts answers from tables, providing better explainability and achieving higher accuracy on real-world TQA tasks [11, 42]. For example, Dater [41] proposes to decompose questions and use code to deconstruct table contexts, enhancing end-to-end table reasoning with decomposed sub-tables and sub-questions. ReAcTable [42] utilizes the ReAct paradigm [40] to generate codes which are executed step-by-step to extract intermediate question-related sub-tables for deducing the answer.\nHowever, current LLM-based TQA methods mainly focus on the reasoning process of answer extraction instead of thorough and comprehensive data prep. While Binder [11] proposes integrating SQL with an LLM-based API to incorporate external knowledge, which may partly address augmentation tasks, it does not account for a wide range of data prep tasks, such as normalization and column filtering. In this paper, we highlight the significance of question-aware data prep for TQA which could be a bottleneck restricting the performance. We also point out primary challenges in question-aware TQA data prep and propose an automatic multi-agent framework to solve them.\nTraditional Data Preparation. Data Prep techniques have been widely adapted to a wide range of tasks, such as machine learning (ML) and exploratory data analysis (EDA), as they turn raw data into a format that is ready for downstream tasks. A complete data"}, {"title": "8 CONCLUSION", "content": "In this paper, we have introduced AUTOPREP, an LLM-based multi-agent framework designed to support automatic data prep for TQA"}]}