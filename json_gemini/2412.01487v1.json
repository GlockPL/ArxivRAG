{"title": "FastRM: An efficient and automatic explainability framework for multimodal generative models", "authors": ["Gabriela Ben-Melech Stan", "Estelle Aflalo", "Man Luo", "Shachar Rosenman", "Tiep Le", "Sayak Paul", "Shao-Yen Tseng", "Vasudev Lal"], "abstract": "While Large Vision Language Models (LVLMs) have become masterly capable in reasoning over human prompts and visual inputs, they are still prone to producing responses that contain misinformation. Identifying incorrect responses that are not grounded in evidence has become a crucial task in building trustworthy AI. Explainability methods such as gradient-based relevancy maps on LVLM outputs can provide an insight on the decision process of models, however these methods are often computationally expensive and not suited for on-the-fly validation of outputs. In this work, we propose FastRM, an effective way for predicting the explainable Relevancy Maps of LVLM models. Experimental results show that employing FastRM leads to a 99.8% reduction in compute time for relevancy map generation and an 44.4% reduction in memory footprint for the evaluated LVLM, making explainable AI more efficient and practical, thereby facilitating its deployment in real-world applications.", "sections": [{"title": "Introduction", "content": "Large Vision Language Models (LVLMs) have emerged as the next family of powerful foundation models that further advances in AI applications. LVLMs augment LLMs and are capable of processing and integrating information from textual and visual inputs. LVLMs have shown promising performance enhancements in a wide range of applications such as healthcare (Huang et al. 2023; Mesk\u00f3 2023; Li et al. 2024), autonomous-driving systems (Liao et al. 2024; Cui et al. 2024), education (Bewersdorff et al. 2024), and virtual assistants (Team et al. 2023; OpenAI 2024).\nYet despite their impressive capabilities, LVLMs are often constrained due to the opaqueness of their decision-making process and susceptibility to generating hallucinations, describing cases when the model produces responses that are not supported by any information given in the input, but rather, is drawn from the language priors within the LLM (Li et al. 2023b; Liu et al. 2024; Bai et al. 2024). To address this challenge, many explainability methods have sought to provide insight into the reasoning behind responses as a means to validate model outputs and mitigate future hallucinations."}, {"title": "Related Work", "content": "Explainable methods In their study, (Ras et al. 2021) introduce three dimensions to classify explainable methods: model distillation, intrinsic methods and visualization methods. Model distillation such as (Ribeiro, Singh, and Guestrin 2016) offers a way to provide explanations alongside the model's outputs by mimicking the input/output behavior. In general, the distillation method is interpretable by nature. Intrinsinc methods (such as (Hendricks et al. 2016)) make use of a model that has been specifically created to render an explanation along with its output. An additional explanation \"task\" can be added to the original model and jointly trained along with the original task providing the expected explanation. Here, the explainable method is by itself a black box but helps to understand the model behaviors. Lastly, visualization methods identify input features causing a maximum response influencing the model's output. Methods include perturbation-based and gradient-based techniques ((Mahendran and Vedaldi 2016; Dabkowski and Gal 2017; Simonyan, Vedaldi, and Zisserman 2014)).Our work positions itself at the intersection of the 3 types of methods enunciated above. We propose to extend a black box LVLM to render a visual explanation by distilling from interpretable methods.\nRelevancy Map Based Interpretability For transformer-based architectures, relevance maps have proven to be effective tools for offering interpretable explanations of model predictions (Chefer, Gur, and Wolf 2021b; Zhang et al. 2024b; Stan et al. 2024b). These explainability techniques are instrumental in uncovering the reasoning behind model decisions, often revealing that models may overlook certain objects (Chefer, Gur, and Wolf 2021c). Chefer, Gur, and Wolf (2021a) method leverages the attention map from each attention layer and their gradients to update relevancy maps. Due to its effectiveness, Stan et al. (2024a) adapted this method for LLaVA and showed how to make use of these relevancy maps The implementation of this method need to store attentions and gradients cross all layers and heads, increasing the hardware memory usage and inference latency, which makes the method less ideal in scenarios where memory and latency are constrained.\nSeveral methods have been proposed to actively guide models by regularizing their attributions across various tasks, including classification (Ross, Hughes, and Doshi-Velez 2017; Gao et al. 2022a,b), visual question answering (Selvaraju et al. 2019; Teney, Abbasnedjad, and van den Hengel 2020), segmentation (Li et al. 2018), and knowledge distillation (Fernandes et al. 2022). These methods aim not only to enhance performance but also to guarantee that the model is 'right for the right reasons' (Ross, Hughes, and Doshi-Velez 2017). Optimization of relevance maps in Vision Transformers has been shown to enhance the robustness of the model. For example, Chefer, Schwartz, and Wolf (2022) introduced a relevancy-guided training approach that encourages models to prioritize foreground information over background noise. While these methods have demonstrated improvements in robustness, they typically entail significant computational and memory demands, limiting their practicality in real-time or resource-limited scenarios."}, {"title": "Method", "content": "Our method is broadly applicable and can be extended to any explainability technique that depends on saliency maps. The only requirement is that the training dataset must be generated accordingly. Specifically, FastRM features a lightweight relevancy proxy layer that mimics the overall behavior of the relevancy maps introduced by (Chefer, Gur, and Wolf 2021a). In this paper, we will show case the validity of our method based on LLavA v1.5-7b (later called FastRM-7) and LLavA v1.5-13b (FastRM-13) (Liu et al. 2023) types of architectures. Unlike the original computation of the relevancy map, which involves saving all attentions and computing their gradients with respect to the outputs, our model relies solely on the last hidden states, resulting in a reduced memory footprint and latency. For the remainder of the paper, we will focus on the visual aspects of the relevancy map; however, the same principles can be applied to the query input tokens as well."}, {"title": "Dataset collection", "content": "Dataset Creation To train our proxy model, we utilized the VQA dataset (Goyal et al. 2017). For every output token of each samples, we saved the relevancy maps as computed in (Chefer, Gur, and Wolf 2021a) as well as the final hidden states corresponding to those tokens. For instance, given a query such as What color is the man's shirt? , we generated the answer using a LVLM and calculated the relevancy maps for each output token (Yellow) using the equations described in (Chefer, Gur, and Wolf 2021a). Finally, we save samples, each of which contains the following information:\n{token: To, relevancy: R0, hidden_states: Ho},\n{token: T\u2081, relevancy: R\u2081, hidden_states: H1},\n,\n{token: TN, relevancy: RN, hidden_states: HN}\n}\nGiven that the input sentence consists of Nin tokens and the output sentence consists of Nout tokens. Output length is thus N = Nin + Nout\n\u2022 Ro is of shape (Nin, Nin) and Ho is of size (Nin, hidden_size).\n\u2022 Ri will be of shape (Nin + i, Nin + i) and Hi will be of size (Nin + i, hidden_size)\nGiven the large solution space (RN\u00d7N) and our focus on the visual relevancy map, we selected the last row of the relevancy map (which corresponds to the relevancy for the token of interest) and the patches associated with the vision modality. For LLaVA types of architecture, the number of patches, is 24x24=576. This results in an output of size (1,576).\nSince over 90% of image tokens have relevancy scores below 5% of the maximum, as shown in Figure 2(a), this means that fewer than 10% have significant scores. This highlights the need to focus only on the most relevant tokens, as lower-scored ones are often background noise. To address this, we trained a classification model, FastRM, described in the next section. We applied a threshold to binarize our input, aiming to prioritize the prediction of the image's relevant portions while minimizing emphasis on less significant areas.\nFor a relevancy map associated with token at position j in the input-output sentence R:\nRGTj =  {1 if R[\u22121,j]v \u2265 max(R[\u22121,j]v) \u00d7 thresholdotherwise(1)\nWhere R is the relevancy with respect to the visual tokens only and the variable threshold, \"labeling threshold\" is defined empirically and set to 0.3.\nOur training dataset contains 100,000 samples as explained above. It includes 10,000 queries from VQA, with each query averaging 10 tokens per response."}, {"title": "FastRM Model", "content": "We subsequently trained a lightweight model that takes a the last hidden states of all input+output tokens as input and generates the corresponding relevant tokens as a classification problem. Our model predicts how probable a patch is relevant to the model decision. Unlike the original computation of the relevancy map, which requires all attentions and their gradients, our approach depends solely on the representation of the final hidden state. We can thus save memory and only store the last hidden states (using hooks) instead of all of the attentions. We establish the relevancy proxy by performing layer normalization on the final hidden state of the model under consideration. Following this, we apply the concept of the self-attention mechanism, described in 2, restricted to a single query Q and a single key K followed by a sigmoid layer to map the values between 0 and 1. The difference between the FastRM-7 and FastRM-13 architectures lies in the hidden size for Q and V, which are 4096 and 5120, respectively. Both models were trained over 3,500 steps with a labeling threshold of 0.3.\nAttention(Q, K) = QKT\u221adk(2)\nwhere Q and K are respectively the queries and the keys and dk is their dimension.\nQ and K use the hidden states as inputs. The resulting attention is of size of interest: (N, N). We then compute RFastRM the same way as RGT, by only taking the last row and limiting to the visual tokens (1, Nv), where Nv is the number of image patches. RFastRM is then trained toward achieving the expected relevancy using cross entropy. A diagram of the model is shown in Figure 1."}, {"title": "Usage", "content": "Here after is a snippet of the simple code you will need in order to produce the relevancy maps at inference time.\n1 model = LVLM.from_pretrained()\n2 proxy = FastRM()\n3\n4 outputs = model.generate(inputs)\n5 for last_hidden_states in outputs:\n6 relevancy_maps = proxy(\n last_hidden_states)"}, {"title": "Results", "content": "Evaluating the method presents a significant challenge because the original relevancy maps, which we aim to compare our outputs against, are continuous, whereas our model was trained on their binarized versions. This discrepancy complicates direct comparisons and necessitates careful consideration of evaluation metrics and methodologies to ensure meaningful assessments of the model's performance.\nLike mentionned above, in the following section, experiments will be carried out for both LLaVA v1.5-7B and LLaVA v1.5-13B. We acknowledge that two models may not be sufficient to draw a definitive conclusion, and expanding to other models and datasets is a goal for future work.\nPerformance Evaluation We computed the accuracy and F1 score on the two FastRM (7 and 13) models. For evaluations, we use a subset of 10,000 samples from the VQA validation dataset. These metrics were evaluated by comparing the binarized labels used for training, with the predicted output obtained by applying different classification/decision thresholds. The labeling and classification thresholds are distinct and serve different purposes. The labeling threshold is applied during data preparation to define which data points are considered relevant, impacting the ground truth labels the model learns from. Changes in this threshold alter the training examples. In contrast, the classification threshold is set at inference time to determine how the model's output (a probability score) is converted into a final prediction to decide whether the instance belongs to the positive class (relevant token) or the negative class (irrelevant token). It controls how strict the model is in predicting positive cases. Changes in this threshold affect the performance metrics (especially recall and precision) of a trained model without altering the underlying model itself. While the labeling threshold is applied on raw relevancy scores and defines the ground truth, the classification threshold acts on the model's predictions."}, {"title": "Latency and Memory Efficiency", "content": "All the experiments described in this section were conducted on one NVIDIA A100. We evaluated the gain in latency and memory footprint on 1000 samples from VQA with a maximum of generated tokens of 100. We first measured memory footprint and latency using the original computation of the relevancy maps as stated in (Chefer, Gur, and Wolf 2021a) using LLavA-v1.5-7b and we compared it with FastRM-7.\nFigure 4(a) shows an exponential increase in latency as the number of output tokens increases while our method shows a minimal increase. Specifically, generating 10 and 100 output tokens takes respectively around 15 seconds and 14 minutes with the baseline method, while FastRM takes just 0.35 seconds for 100 output tokens. Considering that most queries for VQA require between 10 to 30 tokens to answer, as shown in Figure 3, FastRM significantly accelerates the process, making it a more efficient solution for typical output lengths. VQA involves short questions needing brief answers, but for benchmarks requiring longer answers, the computation time difference will be even greater. On average, the latency for computing the relevancy map using the original calculations is 620 times greater than with our method, representing a 99.8% reduction in time. Figure 4(b) also demonstrates a 44% memory reduction on average compared to the baseline during the entire generation phase and can prevent from out-of-memory issues in some cases"}, {"title": "Qualitative results", "content": "Since our model was trained on binary data and was not aware about the absolute relevancy values, we did not expect a correlation between how the model is confident about a token being relevant and the original raw values of the baseline. Figure 8 presents the output of the sigmoid of FastRM compared to the baseline and shows a correspondence."}, {"title": "Perturbation-based evaluation", "content": "To demonstrate that the output generated by our proxy layer is indeed correlated with the model's focus on the image, we conducted positive and negative-perturbation based experiments by masking image patches based on their relevancy values. The underlying intuition is that masking the most probable relevant image patches should significantly degrade performance, and masking the least probable relevant ones should have minimal impact.\nThis study was carried out for 10,000 randomly selected samples from the validation set of the VQA dataset. In order to produce short answers, VQA-like, we added to the prompt the following: Answer the question using up to two words. We used a temperature of 0, in order to reproduce the same answers. We then gradually removed the image patches and computed the VQA accuracy where an answer is considered accurate if at least 3 annotators provided that exact answer. In positive perturbation, image patches are removed in order of highest to lowest relevance, whereas in the negative version, they are removed from lowest to highest.\nWe compared our approach with three baselines: one that involved the random removal of image patches, the second using GradCAM (Selvaraju et al. 2017), the third using the method proposed by (Chefer, Gur, and Wolf 2021c). For random selection, we performed five experiments and averaged the scores. Figures 5(a) and 5(b) illustrate the results of masking different percentages of image patches. As we progressively mask image tokens selected by different methods, our method leads to a large degradation in performance for the positive perturbations. This indicates that our method more effectively selects the most relevant image patches. Our method consistently results in a greater performance drop than the random selection and GradCAM methods. We notice a sensitively slower performance drop than the baseline. This slightly lower performance is expected when dealing with model distillation and introduces a trade-off between memory/speed and how closely we match the baseline. Similarly, we also performed negative perturbation-based evaluations, in which masking the least relevant image patches is expected to have a minimal effect on accuracy. Figures 5(c) and 5(d) show a comparison between FastRM, GradCAM, baseline and random removal. We observe that as we mask the least probable relevant patches of the image, FastRM's accuracy decreases almost as slowly than that of the baseline and slower than random and for GradCAM"}, {"title": "Generalizability", "content": "To assess the generalizability of our proxy model, which was trained on the VQA training dataset, we expanded the evaluation to include two additional datasets: a 10,000-sample subset from the GQA validation dataset (Hudson and Manning 2019) and POPE (Li et al. 2023a) which contains 9,000 samples. Table 1 presents the accuracy and F1 score per model across benchmarks for a decision threshold of 0.5. Notably, the accuracy and F1 scores are higher for GQA and POPE compared to the in-domain dataset, VQA, on which FastRM was trained. This indicates strong generalizability and demonstrates that our model does not overfit but instead benefits from being trained on a more challenging dataset. Figure 7 presents the perturbation-based evaluations for FastRM-7 and shows similar behaviors than on VQA, fulfilling its role in identifying regions that are crucial for the model's answers."}, {"title": "Ablation studies", "content": "We conducted ablation studies to assess the impact of labeling threshold, dataset size and training steps for both FastRM-7 and FastRM-13 performance. We performed positive perturbation experiments based on their relevancy values probabilites and evaluated the VQA accuracy. We focused on the first 20% of token removal as this is where we want to observe the steeper drop and will only show results for FastRM-13, as FastRM-7 ablations show less significant differences especially for the labeling threshold ablations.\nFigure 6(a) illustrates how the choice of labeling threshold impacts the performance. We present ablation results for FastRM-13 (FastRM-7 exhibited similar trends across all thresholds). The figure indicates that the optimal performance is achieved with a labeling threshold of 0.3.\nWe evaluated FastRM-13 with different dataset sizes of 500, 5k, 50k, and 100k samples. 6(b) indicates that training on 100k outperforms the other setups (same behavior observed for FastRM-7).\nFinally, 6(c) shows that a longer training leads to better performance (same behavior for FastRM-7)."}, {"title": "Conclusion", "content": "In this work we distilled a lightweight model, FastRM, from an explainable metric with improved memory efficiency and reduced latency, making it suitable for on-the-fly interpretation of model decisions. Generating relevancy maps quickly is essential for real-time decision-making, especially in high-stakes or interactive scenarios (such as for the medical domain or for autonomous driving), where it makes explainable AI more practical and easier to deploy in these critical applications. Also, relevancy maps can be used to guide training by identifying important features or regions of input data. Minimizing latency ensures that these feedback mechanisms integrate seamlessly into the training loop, avoiding memory and/or compute bottlenecks when working with large models."}, {"title": "Future work", "content": "To further enhance FastRM's performance, future research could explore incorporating intermediate hidden states in addition to the final hidden states. Additionally, our methodology could be tested on architectures beyond LLaVA-type of models. Lastly, future work will focus on integrating FastRM during LVLM training to improve vision-language grounding."}]}