{"title": "Mirage: Evaluating and Explaining Inductive Reasoning Process in Language Models", "authors": ["Jiachun Li", "Pengfei Cao", "Zhuoran Jin", "Yubo Chen", "Kang Liu", "Jun Zhao"], "abstract": "Inductive reasoning is an essential capability for large language models (LLMs) to achieve higher intelligence, which requires the model to generalize rules from observed facts and then apply them to unseen examples. We present MIRAGE, a synthetic dataset that addresses the limitations of previous work, specifically the lack of comprehensive evaluation and flexible test data. In it, we evaluate LLMs' capabilities in both the inductive and deductive stages, allowing for flexible variation in input distribution, task scenario, and task difficulty to analyze the factors influencing LLMs' inductive reasoning. Based on these multi-faceted evaluations, we demonstrate that the LLM is a poor rule-based reasoner. In many cases, when conducting inductive reasoning, they do not rely on a correct rule to answer the unseen case. From the perspectives of different prompting methods, observation numbers, and task forms, models tend to consistently conduct correct deduction without correct inductive rules. Besides, we find that LLMs are good neighbor-based reasoners. In the inductive reasoning process, the model tends to focus on observed facts that are close to the current test example in feature space. By leveraging these similar examples, the model maintains strong inductive capabilities within a localized region, significantly improving its deductive performance.", "sections": [{"title": "1 INTRODUCTION", "content": "Inductive reasoning, known as the ability of an intelligent agent to infer abstract rules from limited observations and apply them to new examples, is crucial for large language model (LLMs) progressing toward artificial general intelligence (AGI) (Xu et al., 2024b; Sun et al., 2024; Wang et al., 2024b). As illustrated in Figure 1, given a set of observed facts, inductive reasoning process expect the model to generate abstract rules from the provided facts (i.e. [A,B,C] \u2192 [B+C,B+C,C] in the inductive task) and apply these rules to answer specific new questions (i.e. [3,4,7] \u2192 [11,11,7] in the deductive task). Despite its significant research value, it has been relatively neglected compared to other types of reasoning (e.g., math reasoning, multi-hop reasoning, etc.).\nRecently, some works have started to explore this problem. They primarily evaluate the model's inductive reasoning capabilities using various datasets (Shao et al., 2024; Cheng et al., 2024; Qiu et al., 2024; Jiang et al., 2024). Though they have made great progress, their works still have two main limitations: (1) Previous works lack comprehensive evaluation. Most works have only one evaluation task: the inductive task on collected rules (Yang et al., 2024b; Shao et al., 2024) or the deductive task on specific test samples (Chollet, 2019; Xu et al., 2024a; Qiu et al., 2024). Therefore, they can only evaluate the rule induction performance or final results of inductive reasoning, instead of comprehensively analyzing the whole process (i.e. inductive + deductive). (2) Previous works lack flexible test data. Most former datasets evaluate the overall performance of models by collecting observation and test examples under the same rules (Rule, 2020; Kim et al., 2022; Lake et al., 2019). However, due to the absence of transformation rules, it is impossible to extend these examples, resulting in a fixed test set. This limitation makes it challenging to assess the impact of factors such as distribution, quantity, and form of input examples on the model's inductive reasoning, thereby hindering a deeper analysis of the model's reasoning mechanisms."}, {"title": "2 DATA CONSTRUCTION", "content": "In this section, we describe the whole pipeline to build MIRAGE. We start by constructing rules based on five basic operations (\u00a72.1). Next, we substitute the instantiate vectors into the rules to generate facts (\u00a72.2) and apply filtering to them (\u00a72.3). Finally, we transform the facts into different scenarios, creating questions to evaluate the LLM's inductive reasoning performance (\u00a72.4)."}, {"title": "2.1 RULE GENERATION", "content": "According to previous work and relevant definitions (Huber, 2017; Han et al., 2024), in inductive reasoning, for each observed fact $X_k = (x, y)$, the input vector \u00e6 is transformed into the output vector y according to a certain rule f, i.e.:\n$$f(x) = y, \\forall (x, y) \\in X$$\nwhere X is the observed fact set under the rule f. We believe that f is the core of the problem, as it allows us to generate additional facts for X based on the rule automatically. Conversely, inferring f from X requires significantly more effort due to the vast range of possible rules. Therefore, we first consider automating these rules' large-scale synthesis.\nBased on previous representative datasets (Chollet, 2019; Rule, 2020; Xu et al., 2024a), we summarize the main types of rules, resulting in five atomic operations in this dataset:\n\u2022 Add: The operation adds certain components together. For example: [x, y, z] \u2192 [x, x + y, z].\n\u2022 Copy: The operation copies some components to others. For example: [x, y, z] \u2192 [x, x, z].\n\u2022 Map: The operation applies a linear transformation to some components. For example: [x, y, z] \u2192 [x, ky + b, z]. Here, to avoid the interference of complex mathematical calculations, we have k \u2208 [1, 9] and b \u2208 [0, 9].\n\u2022 Pad: The operation fills certain components with constant values. For example: [x, y, z] \u2192 [x, c, c], where c \u2208 [0, 9].\n\u2022 Swap: The operation swaps certain components. For example: [x, y, z] \u2192 [z, y, x].\nFor each operation O, we randomly initialize the set index vector d on which the operation applies and the index vector r where the result is output. Specifically, for x \u2208 x, y \u2208 y:\n$$Y_j =\n\\begin{cases}\n[O(x_d)]_i, &\\text{if } j \\in r\\\\\nX_j, &\\text{if } j \\notin r\n\\end{cases}$$\nwhere ri = j and [] represents the i-th component. Therefore, we can generate a meta-rule f = (0,d,r). Through sampling (O, d, r) randomly, we can construct a meta-rule library F."}, {"title": "2.2 FACT GENERATION", "content": "After generating the rule library, we can randomly initialize \u00e6x, and apply a specific rule f \u2208 F to get y. We repeat this process to generate the fact set X under the rule f. All the (x, y) \u2208 X are used for the LLM to induce the rule f. It is worth noting that we can control the inductive difficulty by adjusting two factors: the dimension D of x, y and the fact number N of X. As an example, in Figure 1, D is 3 and N is 5. Empirically, a higher D and a smaller N tend to increase the task difficulty. Additionally, to avoid the interference of complex mathematical calculations in evaluating inductive reasoning ability, we restrict the elements in each x to integers between 0 and 9.\u00b9 Since we can synthesize any D-dimensional vector \u00e6 to construct a fact, we can flexibly control the input distribution."}, {"title": "2.3 DATA FILTERING", "content": "To ensure the quality of the dataset and the effectiveness of the evaluation, we need to filter out some noisy data. The following filtering steps are applied: (1) Filtering out duplicate facts. For any two facts in X, if their input vectors \u00e6 are identical, one of them is removed and resampled. This ensures that for each rule, all observed facts are unique. (2) Filtering out duplicate rules. To ensure diversity in the evaluation, we also remove duplicate rules, which have the same (O, d, r). (3) Filtering out trivial facts. After random sampling, X may include some trivial facts that provide little value for model induction, such as facts like x = y, x = 0, or y = 0. We filter the data to ensure that each X contains at most one trivial fact, thereby limiting the noise that could affect the model's inductive reasoning process."}, {"title": "2.4 QUESTION GENERATION", "content": "So far, we have constructed all the metadata that we need to generate specific questions. It is worth noting that both F and X contain only abstract rules and facts, without any specific context. Therefore, they represent the fundamental inductive reasoning test data, which is why we refer to them as meta-rules and meta-facts. As shown in Figure 2, to evaluate the practical inductive reasoning capability of models, we apply these metadata to various scenarios to generate concrete problems. Specifically, we have:\n\u2022 List Transformation (LT): List transformation is the primary format used in previous inductive reasoning tasks (Rule, 2020; Xu et al., 2024a; Chollet, 2019), and here we adopt this approach as well. We transform all fact vectors into one-dimensional lists and require the model to inductively infer the transformation rules applied to these lists.\n\u2022 Real-world Problem (RP): Previous datasets lack tests for inductive reasoning capabilities in real-world scenarios (Rule, 2020; Xu et al., 2024a; Qiu et al., 2024).\u00b2 To mitigate this gap, we populate the metadata into different natural language templates across five real-life scenarios. The example in Figure 2 describes a trading scenario, where we use different items to represent different dimensions of the vector. All item transactions follow the same rule.\n\u2022 Code Generation (CG): For each fact, we use x as the input and y as the output of a function. The model is then tasked with predicting the corresponding Python function.\n\u2022 String Transformation (ST): The former three scenarios are related to numbers. Here, we replace the basic elements in the fact vectors with characters to conduct a new test. Notably, we modify the operations as follows: addition in the Add and Map operations is replaced with string concatenation, multiplication in Map is replaced with character replication, zero-padding in Pad becomes character deletion, and the numbers 0-9 are replaced with the characters a-j.\nFor each scenario, we design both inductive (Ind) and deductive (Ded) tasks. In the inductive task, given a set of observed facts X, we ask the model to generate the rule f in a specified format (e.g. a Python Function for CG) and then evaluate the accuracy of the generation. In the deductive task, using the same observed facts, we provide an unseen example input \u00e6t as input and measure the accuracy of the predicted yt. We provide all the prompts used for these tasks in Appendix A.3."}, {"title": "3 LANGUAGE MODELS ARE POOR RULE-BASED REASONERS", "content": "3.1 OVERALL PERFORMANCES ON MIRAGE\nSetup We first evaluate the overall performance of various LLMs on MIRAGE. Here, we select GPT-4 (OpenAI, 2023), GPT-40, Claude-3.5, Llama3-8B (Dubey et al., 2024), and Llama2-13B (Touvron et al., 2023) as representative models.\u00b3 For the first three models, given their strong instruction-following capabilities, we provide only the instruction and allow them to answer the questions in a zero-shot setting. For the latter two models, to improve the format accuracy of the response, we additionally provide five examples before they answer the questions. Unless otherwise specified, we continue to use this setup to prompt the model in the subsequent experiments. For the dataset setting, we fix the size N at 5 and measure performance across four scenarios when the dimension D = 3, 5, 8. We sample 500 questions for each test. More implementation details can be found in Appendix B.1."}, {"title": "3.2 PERFORMANCES OF ADVANCED METHODS", "content": "In \u00a73.1, we observe that LLMs perform poorly on our dataset, especially in inductive tasks. Considering previous work has proposed numerous methods to elicit the model's reasoning abilities (Wei et al., 2022; Wang et al., 2023b; Madaan et al., 2023), we wonder whether they can boost models' performance on MIRAGE.\nSetup Since we focus on exploring the model's intrinsic capabilities, we only consider methods that do not introduce any external tools or knowledge. Specifically, the methods are as follows: Input-Output (IO): We prompt models to generate answers directly under different shots. Inductive-Deductive (ID): We prompt models to generate rules for inductive tasks and apply them to answer questions in deductive tasks. Chain-of-Thought (CoT) (Wei et al., 2022): We prompt models to generate rationales and answers for the two tasks. Self-Consistency (SC) (Wang et al., 2023b): Based on CoT, we sample n rationales and use the major voting"}, {"title": "3.3 IMPACT OF INCREASING FACT SIZE", "content": "In the previous experiments, we consistently fix the observed fact numbers N. Therefore, as a supplement, we explore the impact of N on the model's inductive reasoning process in this section. Theoretically, as the number of observed facts increases, the scope of the candidate rules narrows, which can lead to the incorrect inductive process becoming correct. If the reasoning process is rule-based, the model is likely to generate the correct rule (inductive) before applying it correctly (deductive). In other words, the time when the LLM induces the correct rule is no later than the time it performs the correct deduction. Thus, the cumulative number of observations required for the inductive rule to change from incorrect to correct should not exceed the number required for the test case to become correct. We design this experiment to validate whether it holds on the LLM.\nSetup Given a fact set X* of size k, and a fixed test input \u00e6t, we define the inductive correction threshold (ICT) and deductive correction threshold (DCT) as follows:\n$$ICT = k \\Leftrightarrow \\forall i <k, M(X^i|I) \\neq f \\land M(X^k|I) = f$$\n$$DCT = k \\Leftrightarrow \\forall i <k, M(X^i, x_t|D) \\neq f(x_t) \\land M(X^k, x_t|D) = f(x_t)$$\nHere, M(\u00b7|I), M(\u00b7|D) are the model's outputs in inductive and deductive tasks. We set D = 5 and vary N, then analyze the distribution of these two thresholds across 100 samples, reporting the results in Figure 3."}, {"title": "3.4 TRANSFERABILITY TEST OF INDUCTIVE RULES", "content": "Finally, we investigate the impact of different scenarios on the inductive reasoning process. For rule-based reasoning, once a rule is formed through induction, it should be transferable. That is, a rule induced in one scenario should be applicable to another scenario with the same underlying transformation. We experiment to explore whether LLMs possess this ability when performing inductive reasoning.\nSetup Specifically, we exclude ST in this experiment since its basic transformations differ from the other three scenarios (see \u00a72.1). For the remaining three scenarios, we generate the observed facts in one scenario, and then transform the test case into another scenario. Since our dataset can generate questions in different scenarios based on the same meta-rule, we can easily ensure that they share the same underlying transformation.\nResults From the results shown in Figure 4, we can get that: (1) LLMs lack transferability in inductive reasoning. Across different cases, the highest performance occurs when the scenarios of the observed and test facts are consistent (i.e., the diagonal from the top left to the bottom right in the figure). (2) The inductive reasoning process of the LLM is form-related. Compared to the transfer between LT and RP (or CG and RP), the transfer between LT and CG demonstrates better performance. We infer that this is because the forms of LT and CG are more similar (see Figure 2). Based on the above two observations, we further confirm that LLMs do not rely on abstract rules when performing inductive reasoning. So, what is the underlying mechanism behind it? In the following section, we focus on addressing this question."}, {"title": "4 LANGUAGE MODELS ARE GOOD NEIGHBOR-BASED REASONERS", "content": "4.1 MOTIVATION\nFrom \u00a7 3.4, we know that closer forms between the observed facts and the test case can enable the model to perform inductive reasoning more effectively. However, is the positive impact brought by the similarity limited only to the form? The answer to this question is likely \"No\". Upon reviewing related works, we find that models tend to match various similar patterns in the context and use them to predict the next token (Olsson et al., 2022; Wang et al., 2023a; Hu et al., 2024b). Therefore, we aim to identify a metric to measure some other similarities between the observed facts and the test input. Since all of our facts are transformed from vectors, we associate this similarity with the distance between these facts in feature space.\nIn topology, if f : X \u2192 Y is a continuous function between two Euclidean spaces and N(x0, \u20ac) is a e-neighborhood of the point xo in X, then we have:\n$$\u039e\u03b7 > 0, s.t. \u2200x \u2208 N (x_0, \u20ac), f(x) \u2208 N (f(x_0), \u03b7)$$\nIn other words, continuous functions preserve the neighborhood property. If a fact input vector x closes to the test input xt, then their output vectors y and yt will remain close. Therefore, the close distance between yt and y may allow LLM to predict yt based on y in observed facts without the need for correct rule generation. In the following sections, we demonstrate through experiments that the model's inductive reasoning relies on this paradigm, which we refer to as neighbor-based reasoning."}, {"title": "4.2 NEIGHBOR FACTS IN INDUCTIVE REASONING", "content": "Before conducting the experiments, we first define some key concepts in our work: the distance d and neighborhood N. In our setup, the components at corresponding positions in the vectors follow the same transformation rules, while non-corresponding components may undergo different transformations (see Equation 2). Hence, we consider using the distance based on the corresponding components: Chebyshev distance. Given observed fact $X_i = (x_i, Y_i)$ and test input $x_t$, we have:\n$$d(X_i, x_t) = max_k (|X_{ik} - X_{tk}|)$$\nwhere xik and Itk are the k-th component of two input vectors. Then we can define the e-neighborhood of \u00e6t based on the distance:\n$$N(x_t, \u20ac) = {X_i | d(x_i - x_t) < \u20ac}$$\nSetup According above definitions, we can divide an observed fact Xk into three categories based on the distance between \u00e6 and the test input xt: (1) In-neighborhood Fact (IF): If \u216bk \u2208 N(xt, e), we call Xk is a in-neighborhood fact. (2) Cross-neighborhood Fact (CF): If Xk \u2209 N(xt, \u20ac), but \u2203i \u2208 [1, D], s.t. |xi - Xti \u2264 6, we consider it a suboptimal neighbor fact because some of its components can still contribute to the model's inductive reasoning process. In this case, we call Xk is a cross-neighborhood fact. (3) Out-neighborhood Fact (OF): If Vi \u2208 [1, D], Xi - Xti > e, we call Xk is an out-neighborhood fact. By substitution, we can make the fact set X contain only one type of the fact, while keeping the size N fixed. After constructing different fact sets, we compare the model's performance on deductive tasks under these settings. Besides, we use the performance under the default fact set as the baseline, where all facts are randomly sampled without any constraints."}, {"title": "4.3 UNIVERSALITY OF NEIGHBOR-BASED REASONING", "content": "We consider whether LLM's inductive reasoning universally relies on neighbor cases, hence, we set e to 1 and repeat the experiment under more different settings, where the baseline is the same as we set in \u00a7 4.2. The results on GPT-40 are reported in Table 4, from which we can prove that the neighbor-based paradigm is universal in LLMs' inductive reasoning process. Across different scenarios and fact numbers, IF consistently gets the highest accuracy, while OF gets the lowest accuracy. The reliance of LLMs' inductive reasoning on neighbor facts is independent of the specific task scenarios, models, or fact numbers."}, {"title": "4.4 EFFECTIVE SCOPE OF NEIGHBOR-BASED REASONING", "content": "In our experiment, we sample n test cases Xt (here n = 5) in each deductive task 7 and define the accuracy a for this particular task as:\n$$\u03b1 = \\frac{1}{n}\\sum_{x \\in X_t} I[M(X_\u03c4, x|D) = f(x)]$$\nHere X, is the observed fact set of the task. Let T denote the set of all deductive tasks (we set |T| = 100), we define deductive density Id as:\n$$I_d = \\frac{1}{|T|}\\sum_{T \\in T} \u03b1[M(X_T, x|D) = f(xt)]$$\n$$|T_C| = \\sum_{T \\in T}I[M(X_T, xt|D) = f(xt)]$$\nwhere xt is the origin test input in task 7. We use this metric to indicate the impact of a successful deduction (i.e. [3, 4, 7] in Figure 6) on reasoning over other examples in the test region N (xt, \u03b7). A high Ia indicates that the model performs well in most cases within this region, while a low Ia suggests that the model's reasoning is more localized or even individual. For localized scope, the"}, {"title": "5 LIMITATIONS AND DISCUSSIONS", "content": "Interpretation Methods. Most model interpretation studies delve into the internal of models (e.g. neurons, attention layers), providing a comprehensive explanation of the working mechanisms (Romera-Paredes"}, {"title": "6 RELATED WORK", "content": "Evaluating Inductive Reasoning Abilities of LLMs. Existing studies on evaluating LLM's inductive reasoning capabilities mainly use only a single task. On one hand, some works assess the model's rule induction ability by evaluating the accuracy on unseen examples (Moskvichev et al., 2023; Tang et al., 2023; Gendron et al., 2023; Xu et al., 2024b; Qiu et al., 2024). However, since the model's deduction does not always rely on inducing the correct rule, this indirect evaluation method can introduce some inaccuracies. On the other hand, some studies directly evaluate the correctness of the generated rules to assess inductive reasoning ability (Shao et al., 2024; Cheng et al., 2024; Yang et al., 2024b). These studies lack evaluation on test examples, making it difficult to confirm the model's mastery of the inductive rules. Our work evaluates both aspects, providing a comprehensive analysis of the model's inductive reasoning process.\nMechanism Analysis on LLM's Reasoning. A growing body of interpretability research has begun analyzing the reasoning mechanisms of LLMs, aiming to deepen our understanding of how these models function. Some studies explore the mechanisms behind mathematical reasoning (Zhang et al., 2024; Hu et al., 2024b; Romera-Paredes et al., 2024; Stolfo et al., 2023), some works investigate multi-hop reasoning (Wang et al., 2024a; Hou et al., 2023; Yang et al., 2024a; Biran et al., 2024), and some focus on other types of reasoning (Li et al., 2024; Hu et al., 2024a). However, there is currently a lack of analysis on the mechanisms of inductive reasoning. Our work mitigates this gap and uncovers the neighbor-based paradigms LLMs follow when performing inductive reasoning."}, {"title": "7 CONCLUSION", "content": "In this paper, we focus on evaluating and explaining the inductive reasoning process of LLMs. First, we construct a dataset MIRAGE, which provides both inductive and deductive evaluation tasks, with the flexibility to generate test examples in any distribution, different difficulties, and various forms. Based on it, we prove that LLM is a poor rule-based reasoner, it does not need to rely on inductive rules when performing deductive tasks. Compared to correct induction, the model can perform successful deduction with fewer observations, and this deduction is closely related to the form of the input. Furthermore, we identify a key paradigm of LLM"}, {"title": "C.1 PROOF OF CONTINUES FUNCTIONS", "content": "Here, we prove that the five basic vector operations in MIRAGE are all continuous functions:\nTheorem 1 (Add Operation Continuity). Let A = (a1, a2,..., an) \u2208 Rn. Define a mapping f : R\" \u2192 R\"\nsuch that for a fixed index k \u2208 {1, 2, ..., n} and a fixed subset I \u2286 {1, 2, ..., n}, we have\n$$f(A) = (a_1,..., a_{k-1}, \\sum_{i \\in I} a_i, a_{k+1},..., a_n),$$\nwhere k & I. Then f is a continuous function.\nProof. Consider two vectors A, B \u2208 R\":\n$$A = (a_1,a_2,..., a_n), B = (b_1, b_2,..., b_n).$$\nThe mapping f replaces the k-th element of the vector with the sum of elements indexed by the subset I. Thus,\n$$f(A) = (a_1,..., a_{\u03ba-1}, \\sum_{i \\in I} a_i, a_{k+1},..., a_n),$$\n$$f(B) = (b_1,..., b_{k-1}, \\sum_{i \\in I} b_i, b_{k+1},..., b_n).$$The distance between the images of A and B under f is\n$$\\||f(A) - f(B)\\|| = \\sqrt{\\sum_{j=1, j\\neq k}^{n} (a_j - b_j)^2 + (\\sum_{i \\in I}a_i - b_i)^2}.$$\nLet us focus on the term involving the sums:\n$$\\sum a_i-b_i = \\sum (a_i \u2013 b_i).$$\nBy the triangle inequality, we have\n$$| \\sum_{i \\in I} a_i-b_i | = \\sum_{i \\in I} |(a_i \u2013 b_i)|.$$\nTherefore,\n$$(\\sum_{i \\in I} | a_i-b_i |)^2 = (\\sum_{i \\in I} | a_i - b_i |)^2.$$\nUsing the Cauchy-Schwarz inequality, we get\n$$(\\sum_{i \\in I} (a_i - b_i))^2 \u2264 |I| \\sum_{i \\in I} (a_i \u2013 b_i)^2,$$\nwhere |I| is the cardinality of the set I.\nTherefore,\n$$\\||f(A) - f(B)\\|| \u2264 \\sqrt{\\sum_{j=1, j\\neq k}^{n} (a_j - b_j)^2 + |I|\\sum_{i \\in I} (a_i \u2013 b_i)^2}.$$\nThis can be bounded as\n$$\\||f(A) - f(B)\\|| \u2264 C\\||A \u2013 B\\||,$$\nwhere C is a constant depending on n and |I|.\nTherefore, for any \u20ac > 0, choose \u03b4 . If ||A \u2013 B|| < \u03b4, then\n$$\\||f(A) - f(B)\\|| < \u0421\u03b4 = \u0454.$$\nHence, f is continuous.\""}, {"title": "C.2 COMPARISON WITH OTHER DISTANCE METRIC", "content": "We aim to explore whether using different distance metrics to define neighbor facts would also influence the model's inductive reasoning. Therefore, we additionally introduce three other distance metrics: Euclidean distance deuc, Manhattan distance dman, and Minkowski distance dmin. Like Equation 6, we have:\n$$d_{euc}(X_i, x_t) = \\sqrt{\\sum_{k=1}^{D} (X_{ik} - X_{tk})^2}$$\n$$d_{man} (X_i, x_t) = \\sum_{k=1}^{D} |X_{ik} - X_{tk}|$$\n$$d_{min} (X_i, X_t) = ( \\sum_{k=1}^{D} |X_{ik} - X_{tk}|^p )^{\\frac{1}{p}}$$\nwhere we set p = 3. We can generate three distinct new neighborhoods N(xt, e) by incorporating these distances into Equation 7, thereby constructing three new kinds of OF. Therefore, we compare the model's performance on deductive tasks when using only these different OFs, and the results are shown in Figure 8. From the figure, we can see that our neighborhood construction outperforms those constructed using other distance metrics. The deductive performance of the other three OFs across different radii is similar to the baseline, indicating that removing neighbor facts constructed using these methods does not influence the model's inductive reasoning ability. In contrast, our constructed OF leads to a significant decline in accuracy, proving the validity of our neighborhood construction."}, {"title": "C.3 MORE EXPERIMENTS ON OTHER MODELS", "content": "We repeat the experiments in \u00a7 4.3 on Llama2-13B, Claude-3.5 and Llama3-8B. The results are shown in Table 9, 10, 11. Besides, we also repeat the experiments in \u00a7 4.4 on Claude-3.5 and report the results in Figure 9. The results of all these additional experiments are consistent with those in the main text."}, {"title": "C.4 SUPPLEMENTARY EXPERIMENT FOR MAIN EXPERIMENT", "content": "We observe that, in the experiment of \u00a7 4.2, though the performance of OF significantly decreases compared to the baseline, some models are still able to maintain around 40% accuracy, even with only distant observed facts. We infer that models are likely to conduct rule-based reasoning in these cases. Hence, we design an extra experiment for supplementary. In it, we prompt LLMs to induct rules and finish deductive tasks (i.e. ID in \u00a73.2) on these cases in Table 12. From the table, we can observe that the model's deductive accuracy using the rule exceeds 70% when there are fewer neighbor facts in the context. This demonstrates that the model tends to rely more on rule-based induction if there is less neighbor-based matching."}]}