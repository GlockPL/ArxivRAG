{"title": "GaussianPU: A Hybrid 2D-3D Upsampling Framework for Enhancing\nColor Point Clouds via 3D Gaussian Splatting", "authors": ["Zixuan Guo", "Yifan Xie", "Weijing Xie", "Peng Huang", "Fei Ma", "Fei Richard Yu"], "abstract": "Dense colored point clouds enhance visual per-\nception and are of significant value in various robotic applica-\ntions. However, existing learning-based point cloud upsampling\nmethods are constrained by computational resources and batch\nprocessing strategies, which often require subdividing point\nclouds into smaller patches, leading to distortions that degrade\nperceptual quality. To address this challenge, we propose a\nnovel 2D-3D hybrid colored point cloud upsampling framework\n(GaussianPU) based on 3D Gaussian Splatting (3DGS) for\nrobotic perception. This approach leverages 3DGS to bridge\n3D point clouds with their 2D rendered images in robot vision\nsystems. A dual scale rendered image restoration network\ntransforms sparse point cloud renderings into dense represen-\ntations, which are then input into 3DGS along with precise\nrobot camera poses and interpolated sparse point clouds to\nreconstruct dense 3D point clouds. We have made a series of\nenhancements to the vanilla 3DGS, enabling precise control over\nthe number of points and significantly boosting the quality of\nthe upsampled point cloud for robotic scene understanding. Our\nframework supports processing entire point clouds on a single\nconsumer-grade GPU, such as the NVIDIA GeForce RTX 3090,\neliminating the need for segmentation and thus producing high-\nquality, dense colored point clouds with millions of points for\nrobot navigation and manipulation tasks. Extensive experimen-\ntal results on generating million-level point cloud data validate\nthe effectiveness of our method, substantially improving the\nquality of colored point clouds and demonstrating significant\npotential for applications involving large-scale point clouds in\nautonomous robotics and human-robot interaction scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Colored point clouds augment traditional point clouds,\nwhich solely encompass geometric positional information,\nwith the addition of color attributes. This enhancement\nconsiderably expands the data's dimensionality, providing a\nricher visual perception for robotic systems across various\napplications [1], [2] such as autonomous navigation, object\nmanipulation, and human-robot interaction. The integration\nof color information enables robots to better interpret their\nenvironment, improving tasks like object recognition [3],\nscene understanding [4], and semantic mapping [5]. How-\never, the performance constraints of current robotic sensors\noften result in colored point clouds that are sparse and non-\nuniform, leading to an inevitable reduction in the robot's\nperception accuracy and decision-making capabilities. In\nresponse to this challenge, the application of upsampling\ntechniques to increase the density of point clouds is of\nparamount importance in robotics. The aim is to generate\ndenser, higher-quality colored point clouds that can sig-\nnificantly improve the robot's ability to interpret complex\nscenes, recognize objects with fine details, and interact more\nprecisely with its surroundings. This enhanced perception\ncapability is crucial for advancing robotic systems, enabling\nmore sophisticated and reliable autonomous operations in\napplications ranging from industrial automation to search and\nrescue missions.\nContemporary advanced point cloud upsampling tech-\nniques commonly employ deep learning architectures based\non Multi-Layer Perceptrons (MLPs) [6]\u2013[9]. Despite the\nsignificant strides in performance achieved by these archi-\ntectures, their considerable computational demands and the\nbatch processing methodology employed in training often\nnecessitate pre-processing of point clouds into patches. These\npatches are subsequently utilized as the basic input units\nfor the model, a process that is exemplified in Figure 1.\nUpsampling models that segment point clouds into patches\nhave the potential to create discontinuities between these\nsegments. These inconsistencies can impair the overall qual-\nity and the perceptual experience of the point cloud. The\nissue is particularly relevant for color point clouds with a\nlarge quantity of points, which are intended for human visual\nperception.\nIn the realm of 3D robotic perception, neural rendering\ntechniques based on point cloud data have seen signifi-"}, {"title": "II. PRELATED WORK", "content": "Early point cloud upsampling approaches primarily relied\non optimization methods to upsample point clouds [11],\n[12], which were heavily dependent on prior knowledge of\nthe point cloud. With the advancement of deep learning,\npoint cloud upsampling networks based on deep learning\nhave emerged and achieved impressive results. PU-Net [13]\nemploys PointNet++ [14] for hierarchical learning and multi-\nlevel feature aggregation to map the 3D coordinates of\npoints to a feature space. MPU [15] employs an end-to-end,\nmulti-step patch-based network to progressively upsample\nsparse 3D point sets. PU-GAN [16]introduces a GAN-based\nframework for point cloud upsampling that learns diverse\npoint distributions and ensures uniformity across upsam-\npled patches. PU-GCN [17], leveraging EdgeConv [7] as\nits GCN backbone, introduces NodeShuffle for upsampling\nand Inception DenseGCN for multi-scale feature extraction,\nsignificantly advancing the point cloud upsampling pipeline.\nGrad-PU [18] introduces a novel framework for point cloud\nupsampling that can handle arbitrary upsampling rates by\ndecomposing the problem into midpoint interpolation and\nrefinement via point-to-point distance minimization. The\nabove deep learning-based methods use multi-sampling MLP\nand EdgeConv as the basic modules of the model, which\nbrings significant performance overhead and is difficult to\ndirectly process large-scale point clouds. Therefore, most\nexisting solutions are based on less than 1,024 points. Point\ncloud patches are trained, which in turn makes it difficult\nto learn the relationship between patches. The point cloud\nupsampling method mentioned above mainly upsamples the\ngeometry of colorless point clouds and does not utilize the\ncolor information characteristics of colored point clouds.\nAliev et al. [19] delivered an early pioneering work\nwith Neural Point-Based Graphics, utilizing neural networks\nto enhance point cloud rendering, setting a precedent for\nsubsequent innovations in image detail and texture from\npoint data. With the proposal of Neural Radiance Fields\n(NeRF) [20], neural rendering has entered a stage of rapid\ndevelopment [21]. Yu et al. [22] further streamlined the\nrendering pipeline with a spatially coherent point-based\nmultiplane image method,i.e. Point-NeRF improving the effi-\nciency of synthesizing complex scenes. Recently, Schmitt et\nal.introduced 3D Gaussian Splatting (3DGS) [10], a real-\ntime point cloud rendering technique that projects 3D points\nonto a 2D view plane using adaptive Gaussian kernels. By\nincorporating innovations such as sparse matrix Gaussian\nfiltering and GPU optimization, 3DGS achieves interactive\nrendering frame rates while maintaining high quality, making\nit a promising approach for efficient point cloud visualiza-\ntion.\nCurrently, 3D Gaussian Splatting (3DGS) [10] has been\nsuccessfully applied to various domains [23], such as dy-\nnamic 3D scene modeling [24]\u2013[27], artificial intelligence-\ngenerated content (AIGC) [28]\u2013[31], and autonomous driv-\ning [32]\u2013[34], achieving remarkable results. While signifi-\ncant progress has been made in the aforementioned areas, the\nadvancements of 3DGS in low-level point cloud processing\nhave been relatively limited. To the best of our knowledge,\nthis paper is the first work that introduces the advantages of\n3DGS to the task of colored point cloud upsampling."}, {"title": "III. METHOD", "content": "We present an upsampling pipeline for color point clouds\nas depicted in Figure 2. The entire upsampling framework\ncan be divided into two modules: data preparation and 3DGS\nupsampling. The data preparation stage involves rendering\nthe sparse point cloud, reconstructing the rendered images,\nand interpolating the sparse point cloud. Subsequently, the re-\nconstructed rendered images and the interpolated point cloud,\nalong with the camera poses, are fed into the upsampling\nmodule to obtain the upsampled dense point cloud.\nConsider a point cloud P consisting of N points, denoted\nas P = {p_i \\in \\mathbb{R}^6}_{i=1}^N, where each point p_i is represented\nby a 6-dimensional vector. This vector encapsulates two\nfundamental properties: geometry and color. The geometric\nproperties are characterized by the XYZ coordinates, which\ndefine the spatial position of each point within the 3D space,\nwhile the color attributes are represented by the RGB values\nassociated with each point, providing the visual appearance\ninformation of the point cloud. Given a sparse point cloud\nP_{sp} and an upsampling factor R, our objective is to generate\na dense point cloud containing R \u00d7 N points, represented as\nP_{ds} = {p_i \\in \\mathbb{R}^6}_{i=1}^{R \\times N}. Moreover, we aim for P_{ds} to exhibit\nhigher perceptual and geometric quality.\nWe encapsulate the rendering process through the following\nmathematical formulation:\n$I = \\text{Render}(P, PS, IR, T)$,\nwhere $PS$ represents the point size, $IR$ denotes the resolution\nof the rendered image, and $K$ is the camera's extrinsic\nparameters, which are expressed by the following formula:\n$K = \\begin{bmatrix}\n\\frac{f_x}{Z_x} & 0 & x_y \\\\\n0 & \\frac{f_y}{Z_y} & y_c \\\\\n0 & 0 & 1\n\\end{bmatrix}$,\nwhere $f_x$ and $f_y$ represent the camera's focal lengths in\npixel units along the horizontal and vertical axes of the\nimage sensor, respectively. The terms $Z_x$ and $Z_y$ denote the\ncoordinates of the principal point, The symbol $T \\in \\mathbb{R}^{4\\times 4}$\ndenotes the camera extrinsic parameters.\nIn the setting of the point size PS, we have discovered\nthat both large and small sizes of point renderings come\nwith distinct advantages and disadvantages. As illustrated\nin Figure 3, when the PS is set to 1, it retains more of\nthe original information from the point cloud. However, this\nsetting also allows noise points from the back of the point\ncloud to become visible during the rendering of sparse point\nclouds, which poses challenges for the image restoration\nmodel in discerning the front-to-back relationship and conse-\nquently impacts the performance of the restoration network.\nConversely, a larger PS can enhance the distinction between\nthe foreground and background of the point cloud, but it also\nincreases the likelihood of points overlapping, leading to a\nloss of point information. Therefore, we have rendered the\npoint cloud with two different point sizes to address these\nconcerns during restoration and set the PS to 1 and the\nupsampling factor of R, respectively.\nAfter\nobtaining the rendered images of the sparse point cloud,\nwe employ an image restoration network to restore the\nsparse point cloud renderings, with the goal of achieving\nrendering results comparable to those of a dense point\ncloud. With lightweight considerations in mind, we have\nchosen BRNet [36] as our backbone for image restoration.\nBRNet enhances FFDNet [37] by incorporating a mean shift\nmodule to normalize the input image, thereby achieving\nbetter performance. To combine the advantages and mitigate\nthe shortcomings of point cloud renderings with large and\nsmall PS, we expanded the input channels of FFDNet to\nsix channels and obtained a dual scale restoration network.\nBy concatenating the two point cloud renderings of different\npoint sizes, we input them together into the point cloud\nrestoration network to obtain a dense point cloud rendering\nwith a small size. For more details on the network architec-\nture, please refer to BRNet [36]. During network training,\nconsidering that there are certain areas of blank background\nin the point cloud rendering, we assign different loss weights\nto the foreground and background regions of the point cloud"}, {"title": "C. Gaussian Interpolation", "content": "To better control the desired number of points achieved\nduring upsampling, we perform Gaussian interpolation on\nthe input point cloud before feeding the sparse point cloud\nP_{sp} into the 3DGS upsampling module. To facilitate parallel\ncomputation, we employ the reparameterization sampling\ntechnique to perform Gaussian interpolation on the point\ncloud. The specific sampling formula is as follows:\n$\\hat{x} = \\mu + \\sigma \\epsilon, \\epsilon ~ N(\\epsilon; 0, 1)$\nIn the equation, $\\mu$ represents either the geometric or color\nattributes of the point and $\\sigma$ denotes the variance of the\nGaussian distribution. $N$ represents the standard normal\ndistribution. For the point cloud geometry, we set $\\sigma$ to\n0.25 times the point cloud step size, while for the point\ncloud color, we set $\\sigma$ to 0. By performing R iterations of\nreparameterization on each attribute of every point in the\nsparse point cloud, we obtain the Gaussian interpolated point\ncloud $P_{gi}$ with R times the number of $P_{sp}$.\nAfter obtaining the restored multi-view images, we utilize\nthem for 3DGS upsampling. We can obtain the point cloud\ngeometry and color information based on the center x of"}, {"title": "D. 3DGS Point Cloud Upsampling Module", "content": "the 3D Gaussians and the spherical harmonic coefficients c\nafter optimizing 3DGS [10", "10": "is able to learn\na 3D scene representation in the form of a set of 3D\nGaussians. This allows for the rendering of a new image\nfrom any desired viewpoint. To better adapt 3DGS for the\ntask of colored point cloud upsampling and achieve improved\nperceptual quality", "framework": "nTo precisely achieve the expected number of points after\nRx upsampling, we disable the cloning, splitting, and prun-\ning operations in the vanilla 3DGS during the optimization\nprocess. This fixes the number of 3D Gaussians, ensuring that\nthe number of points in the point cloud remains consistent\nwith the input $P_{gi}$ fed into the 3DGS upsampling module.\nLeveraging the prior knowledge that each point in the point\ncloud is rendered with the same size, we introduce a scale\nconstraint to ensure consistency across the entire point cloud.\nSpecifically, at each optimization iteration, we compute the\nmean scale value across all points in the point cloud, consid-"}]}