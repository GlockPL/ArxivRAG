{"title": "Promptable Closed-loop Traffic Simulation", "authors": ["Shuhan Tan", "Boris Ivanovic", "Yuxiao Chen", "Boyi Li", "Yulong Cao", "Philipp Kr\u00e4henb\u00fchl", "Marco Pavone", "Xinshuo Weng"], "abstract": "Simulation stands as a cornerstone for safe and efficient autonomous driving development. At its core a simulation system ought to produce realistic, reactive, and controllable traffic patterns. In this paper, we propose ProSim, a multimodal promptable closed-loop traffic simulation framework. ProSim allows the user to give a complex set of numerical, categorical or textual prompts to instruct each agent's behavior and intention. ProSim then rolls out a traffic scenario in a closed-loop manner, modeling each agent's interaction with other traffic participants. Our experiments show that ProSim achieves high prompt controllability given different user prompts, while reaching competitive performance on the Waymo Sim Agents Challenge when no prompt is given. To support research on promptable traffic simulation, we create ProSim-Instruct-520k, a multimodal prompt-scenario paired driving dataset with over 10M text prompts for over 520k real-world driving scenarios. We will release code of ProSim as well as data and labeling tools of ProSim-Instruct-520k at https://ariostgx.github. io/ProSim.", "sections": [{"title": "1 Introduction", "content": "Simulation provides an efficient way to evaluate AV systems in realistic environments. It avoids the high costs and risks associated with real-world testing, and forms the first step of any deployment cycle of an autonomous driving policy. A simulator is only able to fulfill its role if it is capable of mimicking real-world driving conditions. Chief among these conditions is highly-realistic simulation of interactive traffic agents and their behavior patterns. Further, the simulation must be controllable to enable the creation of interesting traffic scenarios by editing and customizing each agent's behaviors and intentions via user-friendly inputs or prompts.\nTo capture these needs, we propose a new task: promptable closed-loop traffic simulation (Section 3). Aside from simulating realistic traffic agent interactions in closed-loop, traffic models should also generate agent motions that satisfy a complex set of user-specified prompts. Our baseline algorithm ProSim (Figure 1) takes a scene initialization and multimodal prompts as inputs, and produces a set of agent policies that yield closed-loop, reactive scenario rollouts. As we show in Section 6, ProSim achieves high realism and controllability with a variety complex user prompts. ProSim even achieves competitive performance on the Waymo Open Sim Agent Challenge [1] without prompts.\nTo further enable research into promptable closed-loop traffic simulation task, we created ProSim-Instruct-520k, a multimodal paired prompt-scenario dataset (Section 5). Leveraging the real-world AV dataset [2], we provide a rich set of prompts for all agents. Specifically, for each scenario we label agents' Goal Points (their destinations), Action Tag (e.g., Accelerate, LeftTurn), Route Sketch (a noisy sketch of an agent's route), and Text instructions (e.g., \"Instruct <A0> to decelerate before turning right.\"), visualized in Figure 1. These prompts simulate some of the diverse ways users may wish to prompt scenarios. We ensure the labeled prompts accurately depict the agent's ground-truth (GT) behavior with careful human quality assurance stages. ProSim-Instruct-520k"}, {"title": "2 Related Work", "content": "Unconditional closed-loop traffic simulation. Recent advancements in generative models for closed-loop traffic simulation have garnered significant attention. These works aim to learn realistic multi-agent interactions from real-world data. Early work TrafficSim [3] learns multi-agent interac-tions with GNNs. More recent works like BITS [4] and TrafficBots [5] learn agent behaviors with a bi-level imitation learning design, while RTR [6] combines reinforcement learning and imitation learning objectives. Most recently, works like Trajeglish [7] and SMART [8] showed the effective-ness of formulating the task as a next-token prediction problem. For this research direction, Waymo Open Sim Agent Challenge [1] provides a common evaluation methodology and benchmark. The objective of these works is to replicate real-world agent interactions without much control over the simulation process. In contrast, ProSim focuses on providing users with different ways to prompt the agents, allowing efficient creation of interesting traffic scenarios.\nControllable open-loop traffic generation. Another line of work focuses on generating traffic sce-narios with user-input controls. Early works like CTG [9] and MotionDiffuser [10] generate agent trajectories with diffusion models that imposes control signals with cost functions. CTG++[11] re-places the gradient guidance process in CTG with cost functions generated by an LLM, supporting text conditions as input. Different from these works that assume existing agent locations as input, SceneGen [12] and TrafficGen [13] automatically initialize agent positions conditioned on an empty map. LCTGen [14] further supports both agent placement and motion generation with text condi-tions as input. RealGen [15] incorporates retrievals of examples to generate desired scenarios given the query. While these approaches can generate realistic scenarios controlled by user inputs, they only produce fixed-length agent trajectories in open loop. These trajectory outputs do not allow agents to react to each other or the tested driving policy in a simulator, limiting their usefulness in"}, {"title": "3 Problem Formulation", "content": "In the task of traffic modeling, we are given an initial traffic scene \\(\\sigma = (M, A)\\) comprised of map elements \\(M = {v_1, ..., v_S}\\) and the initial states of N agents \\(A = {a_1,..., a_N}\\) (i.e., their sizes, types, and past trajectory). Each map element v is a polyline segmentation of a lane into its centerline and edges. Agent states \\(s_t = {s_t^1, ..., s_t^N}\\) are agent positions and headings at time t.\nTo simulate a traffic scenario for T steps, the task of traffic modeling is to obtain the distribution \\(p(S_{1:T} | \\sigma)\\). We denote rollout \\(\\tau \\sim p(s_{1:T} | \\sigma)\\) as a sample from this distribution. Interactive traffic modeling [1] factorizes the rollout distribution autoregressively, \\(p(S_{1:T} | \\sigma) = \\prod_{t=1}^T P(S_t | S_{1:t-1}, \\sigma)\\), independently for each agent, \\(p(s_t | S_{1:t-1}, \\sigma) = \\prod_{i=1}^N P(s_t^i | S_{1:t-1}, \\sigma)\\). Thus, the task of uncondi-tional traffic simulation [1] reduces to predicting \\(p(s_{1:t} | \\sigma) = \\prod_{i=1}^N \\prod_1^T P(s_t^i | S_{1:t-1}, \\sigma)\\).\nIn this work, we aim to incorporate user-specified prompts \\(\\rho = {\\rho_1,\\dots, \\rho_N, L}\\), which is a set of prompts for each agent in the scenario as well as an optional natural language prompt L. For agent i, the prompt \\(\\rho_i\\) contains its static properties \\(a_s[i]\\) and optionally a set of user prompts \\(\\{(X_1, W_1), ..., (X_{|\\rho_i|},W_{|\\rho_i|})\\}\\), where x is the prompt input and w is the prompt type (e.g., goal point, action). The text prompt L can include multiple sentences or be empty. Our goal is to generate scenarios following user prompts, yielding the promptable closed-loop traffic simulation task:\n\\[p(S_{1:T}| \\sigma, \\rho) = \\prod_{t=1}^T \\prod_{i=1}^N p(s_t^i | S_{1:t-1}, \\sigma, \\rho).\\]\nWhen T is large, modeling the above equation becomes computationally hard. We thus dis-cretize T into smaller k-step chunks, and factorize the probability accordingly: \\(p(S_{1:T} | \\sigma, \\rho) = \\prod_{t=1}^{T'} \\prod_{i=1}^N P(S_{t\\cdot k-k+k} | S_{1:t\\cdot k-k-1}, \\sigma, \\rho)\\), where \\(T' = \\frac{T}{k}\\). In this way, we balance computation cost and simulation accuracy by selecting different values for k."}, {"title": "4 ProSim", "content": "ProSim(\\(\\tau|\\sigma, \\rho)\\) is a prompt-conditioned model that produces interactive traffic rollouts \\(\\tau\\) from a scene initialization \\(\\sigma\\) and a set of user prompts \\(\\rho\\). ProSim has three main components: first, a scene Encoder (Section 4.1) efficiently encodes an scene initialization \\(\\sigma\\) to a shared set of scene tokens \\(F = Encoder(\\sigma)\\). Secondly, a policy token Generator (Section 4.2) takes the scene tokens and agent prompts to generate policy tokens for all agents \\(\\{\\pi_1, ..., \\pi_N\\} = Generator(F, \\rho)\\). Thirdly, for agent i at timestep t, a Policy network (Section 4.3) takes the policy token \\(\\pi_i\\), agent states \\(S_{1:t-1}\\), and current observation F to predict the next k agent states \\(s_{t:t+k}^i = Policy(\\pi_i, S_{1:t-1}, F)\\). Finally, we obtain the sample rollout \\(\\tau \\sim ProSim(\\sigma, \\rho)\\) by iterating between sampling \\(s_{t:t+k}^i\\) from Policy for each agent and updating the observation \\(s_t\\) from t = 1 to T with k-step strides. We train ProSim with efficient closed-loop training (Section 4.4) on ProSim-Instruct-520k.\nProSim prioritizes three key properties. First, it is highly controllable. ProSim allows users to give a flexible combination of numerical, categorical, and textual prompts for each agent, or no prompt for unconditional driving. Users may also mix and match different prompting strategies. Second, ProSim allows interactive closed-loop traffic simulation. For each agent, the Generator outputs a policy token instead of the full trajectory. This allows each agent to interact with the environment and other agents, simulating real-world reactive behaviors. Third, ProSim allows efficient closed-loop training. During rollout, ProSim only runs the heavy Encoder and Generator once, while the Policy runs for all agents in parallel at each step. This design enables us to train ProSim closed-loop with full rollout of all the agents and get a complete training signal through all simulation steps."}, {"title": "4.1 Encoder", "content": "The Encoder translates the initial scene \\(\\sigma = (M, A)\\) into a set of scene tokens \\(F = [F_m, F_a]\\), where \\(F_m = {f_m[1], \\dots, f_m[S]}\\) is the set of S map features and \\(F_a = {f_a[1], \\dots, f_a[N]}\\) is the set of N agent features. To enable closed-loop inference for all agents in parallel, we design F to be symmetric across all agents and directly usable by the Generator and Policy modules for any agent.\nWe follow Shi et al. [16] and use a symmetric Encoder design. We normalize the map M and actors A to their local coordinate systems, and use global positions to reason about positional relation-ships between features with a position-aware attention module. The output features are then both symmetric and position-aware.\nSymmetric Feature Encoding. Each map element \\(v_i\\) is a set of polyline vertices representing a lane segment. These points are originally in a global coordinate system. To transform these points into an element-centered local coordinate system, we compute the geometric center and tangent direction of \\(v_i\\) in global coordinates as \\(p_m[i]\\) and \\(h_m[i]\\) and transform all points in \\(v_i\\) to the local coordinate system with \\(\\Gamma(v_i, p_m[i], h_m[i])\\), where \\(\\Gamma\\) is the coordinate transform function. Similarly, for each historical agent state \\(a_h[i]\\) in A, we obtain its last-step position \\(p_a[i]\\) and heading \\(h_a[i]\\) in the global frame. We then transform all historical states in \\(a_h[i]\\) to this local coordinate system with \\(\\Gamma(a_h[i], p_a[i], h_a[i])\\). These scene features are encoded with a PointNet-like encoder [17]:\\(f_m[i] = \\phi(MLP(\\Gamma(v_i)))\\), \\(f_a[i] = \\phi(MLP(\\Gamma(a_h[i]))) \\), where \\(\\phi\\) denotes max-pooling and \\(f_m[i], f_a[i] \\in \\mathbb{R}^D\\) are the agent-symmetric input tokens for \\(v_i\\) and \\(a_i\\), respectively, with feature dimension D. All map and agent input tokens are concatenated together to obtain input scene tokens \\(F^{ma}\\).\nPosition-aware Attention. Since each token in \\(F^{ma}\\) is normalized to its local coordinate system, vanilla MHSA cannot infer the relative positional relationship between tokens. Instead, we explic-itly model the relative positions between tokens with a position-aware attention mechanism. The resulting tokens we denote as F. Please refer to the Appendix A.1 for additional details."}, {"title": "4.2 Generator", "content": "The policy token Generator takes scene tokens F and user prompts \\(\\rho\\) and outputs policy tokens for all the agents \\(\\{\\pi_1,...,\\pi_N\\}\\). \\(\\rho\\) contains multiple layers of information: agent static properties \\(a_s[i]\\), multimodal agent-centric prompts \\(\\rho_i\\) and a scene-level text prompt L. Therefore, we generate the policy token \\(\\pi_i\\) with three steps. We first obtain agent policy query \\(q_a[i]\\) with \\(a_s[i]\\) and F. We then condition \\(q_a[i]\\) with agent-centric prompts \\(\\rho_i\\) to obtain prompt-conditioned policy query \\(q_{\\rho[i]}\\). Finally, we embed \\(q_{\\rho[i]}\\) and L together into text space and query an LLM to output the final policy token \\(\\pi_i\\). We explain these steps in details below.\nPolicy Query from Scenes. We use an MLP to encode the agent static properties \\(a_s[i]\\) into an agent policy query \\(q_i \\in \\mathbb{R}^D\\). To model agent-agent and agent-scene interactions, we use \\(Q = {q_1, ..., q_N}\\) and scene tokens F as inputs and pass them through multiple transformer layers. Each layer follows \\(Q' = MHCA'(MHSA'(Q), F))\\), where MHCA', MHSA' denote position-aware attention modules as in Section 4.1. We take last-layer outputs as the policy queries \\(Q_u = {q_a[1], \\dots, q_a[N]}\\).\nMultimodal Agent-centric Prompting. For each agent, its user-input prompt contains \\(|\\rho_i|\\) multimodal prompts \\(\\{(x_1, w_1), ..., (x_{|\\rho_i|},W_{|\\rho_i|})\\}\\), where x is the prompt raw input and w is the prompt type. We encode each prompt in \\(\\rho_i\\) into a D-dimensional feature \\(e_j = Enc_{w_j}(x_j)\\), where \\(Enc_{w_j}\\) is the feature encoder for prompt type \\(w_j\\). We then aggregate all the prompts of the same agent to a com-pound prompt condition feature: \\(q_{\\rho[i]} = \\phi'({e_1, e_2, ..., e_k})\\), where \\(\\phi'\\) can be any feature aggrega-tion function (max pooling, average pooling or a learned self-attention module). We add this prompt condition feature into policy queries to get prompt-conditioned policy queries \\(q_{\\rho[i]} = q_a[i] + q_{\\rho[i]}\\).\nText Prompting with LLMs. For each scene, we have an optional user-input text prompt L that con-tains multiple sentences that could describe agent behaviors, interactions, and scenario properties. To process the scene-level text prompt L and condition all the agents, we use an LLM to comprehend"}, {"title": "4.3 Policy", "content": "For rollout, at each timestep t each agent i independently gets its next k states from a Policy net-work: \\(s_{t:t+k}^i = Policy(\\pi_i, S_{1:t-1}, F)\\). Policy first updates the dynamic tokens in F with the new state observations \\(S_{1:t-1}\\), then it outputs agent actions for the next k steps conditioned the policy token \\(\\pi_i\\). We describe these two steps below.\nDynamic Observation Update. Recall that \\(F = [F_m, F_a]\\), where \\(F_m\\) is the map feature token set and \\(F_a\\) is the agent history token set. At timestep t > 0, \\(F_m\\) is unchanged while the agent trajectory feature \\(F_a\\) obtained at t = 0 is outdated. To update agent tokens with new observations, we encode \\(S_{1:t-1}\\) with symmetric feature encoding described in Section 4.1. Specifically, for each agent i in \\(S_{1:t-1}\\), we have its history states \\(s_{1:t-1}^i\\), last-step position \\(p_{i-1}\\) and heading \\(h_{i-1}\\) in the global coordinate system. We encode it with: \\(f_{i:t-1} = \\phi(MLP(\\Gamma(s_{i:t-1}, p_{i-1}, h_{i-1})))\\), where \\(\\Gamma\\) and \\(\\phi\\) are the coordinate transform and feature pooling functions defined in Section 4.1. With this operation we obtain the new agent token set \\(F_{a,t} = {f_{1:t-1}, ..., f_{i:t-1}}\\) with symmetric features.\nPrompt-conditioned Action Prediction. For each agent policy token \\(\\pi_i\\), we let it attend to all the map and agent history tokens through multiple cross-attention layers. At each layer we have \\(\\pi_i^* = MHCA'([\\pi_i], {F_m, F_{a,t}})\\), where MHCA is the position-aware multi-head cross attention module mentioned in Section 4.1. We extract the last-layer output \\(\\pi_t^*\\) as the action prediction feature for agent i at step t. Then, we use an MLP to predict the state changes for the next k steps \\(\\{s_{t'}{t+k}\\}\\) = MLP(\\(\\pi_t^*\\)). Finally, for each timestep \\(t' \\in [t, t + k]\\), we simply add the state changes to the previous step state: \\(s_{t'} = \\Delta s_{t'} + s_{t'-1}\\). This leads to the next k-step agent states \\(s_{t:t+k}^i = {s_t^i, ..., s_{t+k}^i}\\). We run this process for all the agents independently in parallel, supporting efficient scenario rollout."}, {"title": "4.4 Training", "content": "We train ProSim in a closed-loop manner. In this section we first describe the closed-loop rollout process, and then introduce the training objectives of ProSim. We further discuss the training details of the Generator LLM in Appendix A.3.\nClosed-loop Rollout. Training a reactive traffic simulator requires strong signals of 1) how an agent's current action influences its future; 2) how each agent's action influences other agent's ac-tions in real time. This leads us to training ProSim in a closed-loop manner. Specifically, given input scene and prompt (\\(\\sigma, \\rho)\\), we run Encoder and Generator once to get the policy tokens for all the agents. Then, at each timestep, we sample new states from Policy for all the agents indepen-dently. We update all the agent's states with states generated by Policy and feed them to Policy in the next iteration as new observations. Finally, when t = T we obtain the full rollout \\(\\tau\\) and compute the loss with the full trajectories of all agents.\nThe fully differentiable design of ProSim makes this theoretically possible as we can directly opti-mize the loss with back-propagation through time in simulation. We also design ProSim to make it practically feasible by only running the heavy Encoder and Generator once in the beginning, while running the light-weight Policy autoregressively for all agents in parallel through time.\nLearning Objective. We supervise the output \\(\\tau = ProSim(\\sigma, \\rho)\\) against GT scenario \\(\\tau\\) with imita-tion learning. Specifically, an output contains N agent trajectories through T steps \\(\\tau = {s_1, ..., s_N}\\). We compute the imitation learning loss with \\(L_{IL} = \\frac{1}{N\\cdot T} \\sum_{i=1}^N \\sum_{t=1}^T d(\\hat{s}_t^i, s_t^i)\\), where d is a distance function (e.g., Huber) and \\(\\hat{s}\\) is the GT state from \\(\\tau\\). In addition to \\(L_{IL}\\), we also add a collision loss \\(L_{coll}\\) and offroad loss \\(L_{off}\\) to encourage agents to avoid collisions and stay on drivable areas. For \\(L_{coll}\\), we compute the intersection area between each pair of agents' bounding boxes across time"}, {"title": "5 ProSim-Instruct-520k", "content": "To provide realistic and diverse agent motion data with multimodal prompts for promptable closed-loop traffic simulation, we propose ProSim-Instruct-520k: a high-quality paired prompt-scenario dataset with 520K real-world driving scenarios from the Waymo Open Motion Dataset (WOMD) [20]. It includes multimodal prompts for more than 10M unique agents, representing over 575 hours of driving data. For each real-world scenario, we label a comprehensive and diverse set of prompts (action tags, text descriptions, etc.) for all agents. We ensure that these descrip-tions accurately correspond to agent motions in the scenario with carefully-designed labeling tools and meticulous quality assurance by human labelers. Further, the labeling tools we developed can be directly transferred to other driving data, making it easy to expand the scale of our dataset. In the remainder of this section, we describe our labeling process and metrics to evaluate promptable closed-loop traffic simulation. Details about quality assurance can be found in Appendix B.5."}, {"title": "5.1 Multimodal Prompt Labeling", "content": "Given a GT scenario (\\(\\sigma, \\tau)\\) containing N agents, we aim to obtain a set of multi-modal prompts for each agent \\(\\{p_1, ..., p_N\\}\\) as well as a natural language prompt L for the whole scenario. For each type of prompt, we carefully design a labeling tool, explained below with further details in Appendix B.\nGoal Point is a 3-dimensional prompt. It allows the user to prompt an agent to reach a target location with a single click on the map. For an agent in the scenario, its goal point (x, y, t) represents a 2D location (x, y) to which this agent intends to reach at time t. We extract Goal Point from GT rollouts.\nRoute Sketch is a point-set prompt. It allows the user to draw a sketch on the map with the mouse as a route that they instruct an agent to follow. Route Sketches enable the users to have fine yet convenient control of the agents' routes. We randomly subsample and add Gaussian noise to the GT trajectories and view them as Route Sketch.\nAction Tag is a categorical prompt. It allows the user to give high-level semantic behavior instruc-tion to an agent by choosing from a set of action tags. Each action tag consists of an action type and its temporal duration \\([t_s, t_e]\\). We define 8 different action types: Speed (Accelerate, Decelerate, KeepSpeed, Stopping, Parked) and Turning (LeftTurn, RightTurn, Straight). For each action type, we carefully design an automatic labeling function that outputs the duration of an agent satisfying this action in the scene given GT trajectories. After obtaining tags from all action types, we conduct a postprocessing step to smooth temporal labels and remove conflicts.\nText is a natural language prompt. It allows the user to describe multiple agents' behaviors and the scenario properties with free-form natural language. To ensure that we have diverse language expressions while keeping the motion descriptions accurate, we prompt the Llama3-70B [18] model to generate diverse sentences given structured agent behavior descriptions (Action Tags). Please refer to Appendix B.3 for details. In total, the text prompts contain around 10M sentences."}, {"title": "5.2 Metrics", "content": "We measure the performance of promptable closed-loop traffic simulation via realism and control-lability. Here we implement metrics for them. Given GT data (\\(\\tau, \\sigma, \\rho)\\), we evaluate a model that outputs a rollout given initial scene and prompts: \\(\\tau = Model(\\sigma, \\rho)\\). We present a formal formulation for the metrics in Appendix B.4."}, {"title": "6 Experiments", "content": "Dataset. We train ProSim on the training split of ProSim-Instruct-520k, which contains around 480K paired prompt-scenario data. Each scenario contains at most 128 agents, including 1.1s history and 8s future with FPS = 10 (T = 80). We use k = 10 as the steps of each policy action segment length. We use the remaining 48K scenes in ProSim-Instruct-520k as the testing set.\nTraining. We train ProSim in two stages. In the first stage, we train ProSim without any prompt for 10 epoches with a batch size of 64. We use the AdamW optimizer with an initial learning rate of 1e-3 and a CosineAnnealing scheduler with a 2500-step warm start. In the second stage, we fine-tune ProSim using all types of prompts. For each scenario, to ensure ProSim retains unconditional rollout capacity, we randomly mask out 50% of the prompts. In this stage, we fine-tune full ProSim for 5 epoches with a batch size of 16 and a learning rate of 1e-4. For LLM, we fine-tune Llama3-8B [18] using a LoRA module with R = 16, \\(\\alpha\\) = 0.1. For the loss function, we set \\(\\lambda_1\\) = 50.0, \\(\\lambda_2\\) = 5.0."}, {"title": "6.2 Promptable Closed-loop Traffic Simulation", "content": "Realism and Controllability. Table 1 evaluates the realism and controllability of ProSim when different types of prompts are given separately, and when all types of prompts are given together, during evaluation. To simulate real-world user inputs, for each scenario, we provide 50% of all possible prompts as model input.\nAs can be seen, ProSim achieves high realism and controllability under all types of prompt. For example, given Goal Point as input, ProSim achieves an ADE of only 0.39m over an 8-second tra-jectory, which is 59.12% better than without prompt conditioning. Even the most high-level and abstract Text prompt leads ProSim to obtain a 26.96% Gain over the unconditional rollout. These results indicate that ProSim creates realistic traffic rollouts while closely following user prompts. Further, passing all prompt types as input achieves the best result of 0.28m ADE (69.7% better than without prompts), showing that ProSim efficiently combines and follows complex sets of multi-modal prompts."}, {"title": "6.3 Unconditional Rollout Evaluation", "content": "As ProSim can run without any prompts, we can compare its performance on unconditional traffic simulation on the Waymo Sim Agent Challange [1]. In Table 2, we show the result of ProSim on the 2024 challenge. We compare with the SOTA method and several representative methods on the leaderboard. Overall, ProSim shows competitive performance. In particular, ProSim has a high Interactive score, showing the effectiveness of ProSim's closed-loop training. On the other hand, ProSim has a relatively low Kinematic score (measures the diversity of agents' speed and acceleration), indicating that we can further improve ProSim's output diversity during unconditional rollouts. As the focus of this work is to make ProSim support promptable and closed-loop traffic simulation, we leave output diversity as future research."}, {"title": "7 Conclusion", "content": "We present ProSim, a multimodal promptable closed-loop traffic simulation framework. Given com-plex sets of multimodal prompts from the users, ProSim simulates a traffic scenario in a closed-loop"}, {"title": "Limitations", "content": "ProSim does not yet support arbitrary prompts. Complex agent interactions (e.g., \"<A0> overtakes <A1> from the left lane\") or more complex modalities (e.g., prompt <A0> with its front-view image) are left as future work."}, {"title": "A ProSim", "content": "In the appendix, we provide implementation and experiment details of our method ProSim, our dataset ProSim-Instruct-520k as well as additional experiment results. In Section A, we present details of Encoder and Generator as well as the training process. In Section B, we present details of labeling, metric and quality assurance processes of our dataset ProSim-Instruct-520k. Finally, in Section C, we show additional quantitative and qualitative results of ProSim."}, {"title": "A.1 Encoder: Position-aware Attention Details", "content": "To model relationships between scene tokens and aggregate features, we pass \\(F^{ma}\\) through multiple Transformer layers. In most existing methods, each layer follows \\(F_l^{ma} = MHSA(F_{l-1}^{ma})\\), where MHSA denotes multi-head self-attention and l is the layer index. However, since each token in \\(F^{ma}\\) is normalized to its local coordinate system, basic MHSA cannot infer the relative positional relationship between tokens. Instead, we explicitly model the relative positions between tokens with a position-aware attention mechanism. For scene token i, we compute its relative positional relationship with scene token j with:\n\\[p_{ma[i,j]} = Rot(p_{ma[j]} \u2013 p_{ma[i]}, -h_{ma[i]}), h_{ma[i,j]} = h_{ma[j]} - h_{ma[i]},\\]\nwhere \\(p_{ma[i,j]} \\in \\mathbb{R}^2\\) and \\(h_{ma[i,j]} \\in \\mathbb{R}\\) are the relative position and heading of token j in token i's coordinate system, Rot(\\(\\cdot\\)) is the vector rotation function. We denote this paired relative position as \\(r_{ma[i,j]} = [p_{ma[i,j]}, h_{ma[i,j]]}\\). Then, we perform position-aware attention for token i with:\n\\[f_{ma[i]} = MHSA(Q: [f_{ma[i]}, PE(r_{ma[i,i]})],\\newline K: {\\{f_{ma[j]}, PE(r_{ma[i,j]})\\}}_{j \\in \\Omega(i)},\\newline V: {\\{f_{ma[j]} + PE(r_{ma[i,j]})\\}}_{j \\in \\Omega(i)}),\\]\nwhere PE denotes positional encoding and \\(\\Omega(i)\\) is the scene token index of the neighboring tokens of i. In our experiments, we set \\(\\Omega(i)\\) to contain the nearest 32 tokens of i according to their positions. Note that the above result remains the same regardless of which global coordinate system we use for the scene input \\(\\sigma = (M, A)\\). With this formulation, we model the relative position relationship between different scene tokens symmetrically. At each layer, we apply Equation 3 to all scene tokens in parallel. We denote this position-aware attention module as:\n\\[F_l^{ma} = MHSA'(F_{l-1}^{ma}, P_{ma}, H_{ma})\\]\nNote that this position-aware modification can be similarly applied to multi-headed cross-attention MHCA', which we will use later in the Generator and Policy modules. Finally, we obtain the last-layer token features as scene tokens \\(F = [F_m, F_a]\\)."}, {"title": "A.2 Generator: Language Prompting Details", "content": "For each scene, we have an optional user-input text prompt L that contains multiple sentences that could describe agent behaviors, interactions and scenario properties. To make it easy to refer to different agents in the scene, we ask the user to use a specific format \"<A[i]>\" when mentioning the i-th agent. For example, to instruct agent a\u2081 to stop, the user would say \"let <A1> stop\".\nTo process the scene-level text prompt L and condition all the agents, we use an LLM to comprehend the natural language prompt and policy features, and generate language-conditioned policy features for all agents. To do this, we use a LLaMA3-8B model finetuned with LoRA as backbone, as well as two adaptors to bridge the latent spaces of LLM and policy tokens."}, {"title": "A.3 Training", "content": "As described above, we train the LLM in Generator with LoRA to compre-hend and generate policy token features. However, we found that directly training the LLM with the closed-loop imitation loss leads to inferior text-prompting performance. We conjecture this is because at the early training stage the LLM is not prepared to interact with the policy tokens. Meanwhile, the rollout loss can be decently optimized without using the LLM's output, giving little signal for LLM to learn.\nTo deal with this issue, we propose to pretrain the LLM to have the capacity to interact with policy token features. To this end, we first train a ProSim model without using text prompts and the LLM with the rollout loss L. Next, we add a simple MLP layer after the Generator to predict the goal point of each agent given its policy token \\(\\pi\\). We supervise this task with an MSE loss \\(L_{goal}\\) using GT goal points. This task is much simpler than the full task while enforces the LLM to interact with policy tokens. To pretrain the LLM, we fix all other modules and only train the LLM and this new MLP with \\(L_{"}]}