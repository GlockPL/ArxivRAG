{"title": "DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image Captioning", "authors": ["Kazuki Matsuda", "Yuiga Wada", "Komei Sugiura"], "abstract": "In this work, we address the challenge of developing automatic evaluation metrics for image captioning, with a particular focus on robustness against hallucinations. Existing metrics are often inadequate for handling hallucinations, primarily due to their limited ability to compare candidate captions with multifaceted reference captions. To address this shortcoming, we propose DENEB, a novel supervised automatic evaluation metric specifically robust against hallucinations. DENEB incorporates the Sim-Vec Transformer, a mechanism that processes multiple references simultaneously, thereby efficiently capturing the similarity between an image, a candidate caption, and reference captions. To train DENEB, we construct the diverse and balanced Nebula dataset comprising 32,978 images, paired with human judgments provided by 805 annotators. We demonstrated that DENEB achieves state-of-the-art performance among existing LLM-free metrics on the FOIL, Composite, Flickr8K-Expert, Flickr8K-CF, Nebula, and PASCAL-50S datasets, validating its effectiveness and robustness against hallucinations. Project page at https://deneb-project-page-nc03k.kinsta.page/.", "sections": [{"title": "1 Introduction", "content": "Image captioning has been extensively researched and applied in various social applications, such as the assistance of visually impaired individuals, the analysis of medical images, and the generation of explanations in robotics [3, 5, 17, 18, 23, 39, 42]. In scenarios where 'AI safety' is paramount, generating appropriate and reliable captions is crucial to avoiding the misrepresentation of the content of images. In particular, erroneous captions that include words not depicted in the image, commonly referred to as 'hallucinations', are a prevalent issue in image captioning [45, 48]. However, a significant issue exists in current image captioning research: models failing to address hallucinations are often wrongfully overrated, as existing evaluation metrics predominantly focus only on correlation with human judgment, and overlook critical flaws. This misalignment is especially problematic in the context of social applications where reliability is essential. Despite their importance, most existing metrics inadequately address the issue of hallucinations. In fact, some studies have demonstrated that while most data-driven metrics [19,24,26,64,65] correlate well with human judgments, they are less effective in addressing hallucinations [19, 46, 53, 55]."}, {"title": "2 Related Work", "content": "Image captioning has been widely applied in various areas of society, including the assistance of visually impaired individuals [3, 13, 17, 18], medical image analysis [5, 42], and the generation of explanations in robotics [23,39]. Several comprehensive surveys [17,21,50] have provided an exhaustive overview of representative models, standard datasets, and metrics, including those specifically designed for evaluating hallucinations, such as CHAIR [45]. Notably, [17] offers a detailed summary of how different approaches, including convolutional methods, attention mechanisms, and generative adversarial networks, contribute to reducing hallucinations in image captioning.\nHallucinations. Hallucinations pose significant challenges not only in LLMs but also in image captioning models [7, 25, 31, 37, 38, 48, 52]. In image captioning, hallucinations are instances in which models generate captions with words not corresponding to any of the elements in the input image. As noted in [17], this issue is particularly critical in social applications where the correctness of captions is prioritized over content coverage. Such hallucinations substantially affect the reliability of these models.\nA notable metric developed to evaluate this issue is CHAIR [45], which assesses hallucinations by calculating the proportion of generated words that accurately reflect objects in the input image. However, CHAIR's approach is rule-based and operates within a closed vocabulary, limiting its applicability and generalizability. The development of CHAIR underscores the need for more comprehensive benchmarks to evaluate the robustness of automatic evaluation metrics against hallucinations.\nIn response to the challenge of hallucinations, [48] introduced the FOIL benchmark to evaluate the robustness of metrics against hallucinations. The FOIL dataset, derived from the COCO dataset [33], consists of approximately 200,000 image-caption pairs, featuring a mix of both correct and hallucinated captions. Hallucinated captions are generated by altering a single word in a correct caption to create a similar but inaccurate version for instance, replacing \"motorcycle\" with \"bicycle\".\nMoreover, the study in [19] presented a widely-accepted methodology for using the FOIL dataset to evaluate metrics. As detailed in [19], this methodology entails the following steps: For each of the 32K test images in the dataset, a pair comprising a hallucinated caption and its correct counterpart is sampled. The robustness of each evaluation metric against hallucinations is then evaluated based on its capability to consistently assign higher scores to the correct captions than to their hallucinated versions.\nImage Captioning Metrics. Standard metrics for image captioning include BLEU [41], METEOR [6], ROUGE [32], CIDEr [40,53], and SPICE [4,54]. These classic metrics, primarily based on n-grams and/or scene graphs, have been extensively used but often exhibit low correlation with human judgments. This discrepancy has led recent studies to shift their focus toward data-driven metrics, such as BERTScore [64], CLIP-S [19], MID [24], and Polos [55]. BERTScore and MoverScore, for instance, leverage a pre-trained BERT encoder to compare word token embeddings in both candidate captions and references. However, their lack of image incorporation can limit their effectiveness in evaluating image captioning models.\nSeveral metrics adopt strategies that leverage both visual and language embeddings from pre-trained vision-and-language models, such as CLIP [43], ViL-BERT [35], and UNITER [9]. A notable example is CLIP-S [19], which employs an unsupervised approach to evaluate captions by measuring their similarity to embeddings generated by CLIP encoders. The distinct feature of CLIP-S is its ability to evaluate captions in contexts both with and without reference images. In [46], the authors introduced PAC-S, a variant of CLIP-S. In this variant, the"}, {"title": "3 Methods", "content": "In this study, we propose a novel automatic evaluation metric DENEB, which is specifically robust to hallucinations. Fig. 2 provides an overview of the proposed metric DENEB. Unlike pseudo-multifaceted metrics, DENEB is a multifaceted metric and can therefore effectively compare multifaceted descriptions of\n3.1 Feature Extraction\nGiven an image img, a candidate xcand, and N references $\\{x^{(i)}_{ref}\\}_{i=1}^N$, the automatic evaluation metrics for image captioning models output a score \u0177 that captures the appropriateness of xcand in the context of ximg and $\\{x^{(i)}_{ref}\\}_{i=1}^N$. First, the input x to our metric is defined as follows:\n$x= [x_{img}, x_{cand},\\{x^{(i)}_{ref}\\}_{i=1}^N ]$ (1)\nwhere $x_{img} \\in R^{3\\times H \\times W}$, $\\{x^{(i)}_{ref}\\}_{i=1}^N \\in \\{0,1\\}^{N\\times V\\times L}$, and $x_{cand} \\in \\{0,1\\}^{V\\times L}$ represent an image, N references, and a candidate, respectively. Furthermore, H, W, N, V, L denote the height and width of the image, the number of references, the vocabulary size, and the number of tokens, respectively.\nIn this study, we extract features from using CLIP [43] and RoBERTa [34]. We utilize the CLIP encoder (CLIP ViT-B/32) pre-trained with [46] and\n3.2 Sim-Vec Transformer\nSim-Vec Extraction. In the context of metrics for image captioning, accurately capturing the similarity between the image, the candidate, and the multiple references is crucial. Several studies have incorporated mechanisms to extract these similarities, thereby achieving high performance [36, 44, 49, 55]. According to the observations in [36,44,49,55], the exclusion of the candidate and the reference leads to a decline in performance. This suggests that these metrics do not fully leverage the extracted vector-form similarities, partially because the latter part of them, specifically responsible for processing vector-form similarities, is just a poorly performing MLP. Considering these mechanisms effectively extract similarities [36, 44, 49, 55], the inclusion of raw features of candidates and references could even hinder effective learning from vector-form similarities. Therefore, in this study, we introduce the Sim-Vec Extraction (SVE) module by extending them to enhance the focus on effectively leveraging these extracted similarities. Our transformer-based metric has greater potential in capturing vector-form similarities more effectively than the classic MLP approach.\nThe SVE module employs a Hadamard product and element-wise differences to extract features that capture the similarity from the following vectors:\n$S_{in} = \\{c_{clip}, \\{r^{(i)}_{clip}\\}^N_{i=1}, v, c_{rb}, \\{r^{(i)}_{rb}\\}^N_{i=1}\\}$ (2)\nWe compute hclip, hrb, dclip, drb as follows:\n$h_{clip} = \\{c_{clip} \\odot v, \\{c_{clip} \\odot r^{(i)}_{clip}\\}_{i=1}^N\\}$ (3)\n$d_{clip} = \\{\\|c_{clip} - v\\|, \\{\\|c_{clip} - r^{(i)}_{clip}\\|\\}_{i=1}^N\\}$ (4)\n$h_{rb} = \\{c_{rb} \\odot r^{(i)}_{rb}\\}_{i=1}^N$ (5)\n$d_{rb} = \\{\\|c_{rb} - r^{(i)}_{rb}\\|\\}_{i=1}^N$ (6)\nIt is important to note that, unlike RUSE [49] and COMET [44], we do not combine the candidate features c and the reference features r here to allow our"}, {"title": "4 Experiments and Results", "content": "4.1 Experimental Setup\nNebula dataset. The development of supervised metrics for image captioning requires a large-scale, diverse dataset, but there are limited available datasets for\neffectively training these models. Moreover, the largest dataset in this field [55] has a notable issue: a significant imbalance, characterized by a limited variety of images relative to the number of captions. Specifically, there is a discrepancy where the number of images is approximately only one-tenth of the total number of samples. This imbalance could potentially lead to suboptimal evaluation of various types of images. To mitigate this imbalance, we constructed the Nebula dataset by extending the Polaris dataset to have approximately three times the number of images.\nFollowing the standard procedure, human judgments were adopted on a five-point scale to assess the appropriateness of a candidate for a given image and references. The annotation process was carried out through a crowdsourcing service. Following previous studies [27, 54, 55], we instructed the annotators to assess the quality of the candidates from the perspectives of fluency, relevance, and descriptiveness. To ensure the reliability of our data, we excluded data from evaluators who exhibited suspicious behavior, such as extremely short response times or consistently providing identical values. In addition, the human judgments, given on a five-point scale, were normalized to the range [0, 1].\nThe Nebula dataset comprises 32,978 images and 32,978 human judgments collected from 805 annotators, and contains approximately three times more images than the Polaris dataset. The total number of references is 183,472, with a vocabulary size of 32,870, a total word count of 1,945,956, and an average sentence length of 10.61 words. The total number of candidates is 32,978, with a vocabulary size of 3,695, a total word count of 288,922, and an average sentence length of 8.76 words. All sentences are in English. For a comprehensive description of the dataset, please refer to the Appendix.\nImplementation Details. Our model had approximately 133 million trainable parameters. We trained our model on a Tesla A100 GPU and measured the inference time on the GeForce RTX 3090 with 24 GB of memory and an Intel Core i9 12900K with 64 GB of memory. The training phase was completed in approximately 2 hours, and the inference time per sample on GeForce RTX 3090 was approximately 22 ms. Further details are explained in Appendix.\n4.2 Comparison to State-of-the-Art\nWe evaluated the automatic evaluation metrics based on the accuracy they achieved when applied to the FOIL benchmark and PASCAL-50S, as well as their correlation coefficients when applied to Composite, Flickr8K-Expert, Flickr8K-CF, and Nebula. Given the importance of efficiency in practical applications, we also conducted experiments to measure the inference times of these metrics. This enables a comprehensive evaluation of both the performance effectiveness and practical usability of these metrics.\nWe adopted BLEU [41], ROUGE [32], METEOR [6], CIDEr [53], and SPICE [4] as they are standard metrics for image captioning tasks. Additionally, we included MoverScore [65], BERTScore [64], BARTScore [61], TIGEr [22], LEIC [12], ViLBERTScore [26], UMIC [27], MID [24], CLIP-S [19], PAC-S [46], Po-"}, {"title": "5 Limitations and Discussion", "content": "While our metric has clearly been shown to provide impressive robustness against hallucinations, it is not without its limitations. The primary limitation is the occurrence of errors stemming from differences in the areas of focus. Moreover, the metric tends to erroneously overevaluate candidates that only describe prominent objects in an image while neglecting finer details. These limitations indicate that the proposed metric may not effectively capture the relationship between Icand and the local regions of Ximg. Consequently, in future work, we plan to extend DENEB by introducing a mechanism to extract the relationship between features in local image regions and language features, as suggested in [66]. For further error analysis, see Appendix."}, {"title": "6 Conclusions", "content": "In this study, we have proposed the novel automatic evaluation metric DENEB, which is specifically robust to hallucinations. Our key contributions are as follows: (1) We have introduced the Sim-Vec Transformer, which handles the similarity between an image, a candidate caption, and reference captions. (2) We proposed the Sim-Vec Extraction (SVE), which utilizes a Hadamard product and element-wise difference to extract features beneficial for automatic evaluation. (3) We constructed the diverse and balanced Nebula dataset comprising 32,978 images, paired with human judgments provided by 805 annotators. (4) We achieved state-of-the-art performance on FOIL, Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, and the Nebula dataset."}]}