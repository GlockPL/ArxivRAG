{"title": "UNCOVERING POLITICAL BIAS IN EMOTION INFERENCE\nMODELS: IMPLICATIONS FOR SENTIMENT ANALYSIS IN SOCIAL\nSCIENCE RESEARCH", "authors": ["Hubert Plisiecki", "Maria Flakus", "Pawe\u0142 Lenartowicz", "Artur Pokropek"], "abstract": "This paper investigates the presence of political bias in emotion inference models used for\nsentiment analysis (SA) in social science research. Machine learning models often reflect\nbiases in their training data, impacting the validity of their outcomes. While previous\nresearch has highlighted gender and race biases, our study focuses on political bias-an\nunderexplored yet pervasive issue that can skew the interpretation of text data across a\nwide array of studies. We conducted a bias audit on a Polish sentiment analysis model\ndeveloped in our lab. By analyzing valence predictions for names and sentences involving\nPolish politicians, we uncovered systematic differences influenced by political affiliations.\nOur findings indicate that annotations by human raters propagate political biases into the\nmodel's predictions. To mitigate this, we pruned the training dataset of texts mentioning\nthese politicians and observed a reduction in bias, though not its complete elimination. Given\nthe significant implications of political bias in SA, our study emphasizes caution in employing\nthese models for social science research. We recommend a critical examination of SA results\nand propose using lexicon-based systems as a more ideologically neutral alternative. This\npaper underscores the necessity for ongoing scrutiny and methodological adjustments to\nensure the reliability and impartiality of the use of machine learning in academic and applied\ncontexts.", "sections": [{"title": "Introduction", "content": "\"The bias I'm most nervous about is the bias of the human feedback raters.\u201d\n\nSam Altman, OpenAI CEO\nIt is a well-documented fact that machine learning models are prone to being biased by their training data.\nStudies have repeatedly shown the presence of biases against various social groups, including gender and race\nbiases, in machine learning based sentiment analysis (SA) systems (i.e., systems that predict the positivity of\ntext snippets (Diaz et al., 2018; Kiritchenko & Mohammad, 2018; Ungless et al., 2023). These types of biases"}, {"title": "1.1 Emotion and Sentiment Analysis in Social Sciences", "content": "In recent years, social scientists have increasingly recognized the profound influence of emotions across a\nbroad range of disciplines. This interdisciplinary approach has illuminated the significant role emotions play\nin shaping human behavior and societal dynamics in fields such as political science (Mintz et al., 2022),\nsociology (Bericat, 2016; Turner & Stets, 2006), economics (Loewenstein, 2000), anthropology (Lutz &\nWhite, 1986), and organizational research (Diener et al., 2020), among others. The proliferation of text data\nsources including social media, computer-based survey responses, political speeches, newspapers, online\nforums, customer reviews, blogs, and e-books-has provided unprecedented opportunities to examine emotions\noutside traditional psychological laboratory settings. Consequently, various tools have been developed to\ndetect emotions (Mohammad, 2016; \u00dcveges & Ring, 2023). As a result, research in this area has expanded\nrapidly.\nTo provide specific examples, in previous research SA been employed to predict election results (Ramteke et\nal., 2016), gauge public sentiment toward pressing social issues (Kim et al., 2021), and compare the emotional\ncontent between news sources from different ends of the political spectrum (Rozado et al., 2022). During the\nCOVID-19 pandemic, numerous studies analyzed public sentiment based on online data (Alamoodi et al.,\n2021; Wang et al., 2022), leading to conclusions such as describing the crisis communication styles on Twitter\nof different Indian political leaders (Kaur et al., 2021). Similar research examined the emotional tone of the\nAustrian 2016 presidential election candidates (Ku\u0161en & Strembeck, 2018). From a psychological perspective,\nSA and emotion prediction have been used to assess suicide risk (Glenn et al., 2020), automate feedback in\nonline cognitive behavioral therapy (Provoost et al., 2019), predict the subjective well-being of social media\nusers (Chen et al., 2017), and analyze the subjective well-being of people over the past centuries (Hills et al.,\n2019). All of these studies relied on sentiment analysis written text to reach scientific conclusions, showcasing\nthe importance of this technique in current social research. However, the exact implementation of SA can\nvary from study to study.\nOverall, there exist three main categories of sentiment analysis (SA) systems: A) dictionary-based approaches,\nB) large language model (LLM) approaches, and C) predictive model approaches. Dictionary-based approaches\n(A), also known as lexicon-based methods, rely on predefined lists of words associated with specific sentiments.\nThese dictionaries, such as the AFINN, SentiWordNet, and LIWC (Baccianella et al., 2010; Boyd et al., 2022;\nNielsen, 2017), assign sentiment scores to words and phrases within a text to determine its overall sentiment.\nThis method is straightforward and interpretable, but it can be limited by the coverage and accuracy of the\ndictionary, as well as by the inability to capture contextual information.\nIn contrast, large language model (LLM) approaches (B) leverage advanced neural networks trained on vast\namounts of text data. Models such as GPT-4, LLAMA, and their derivatives can capture nuanced sentiment\nby understanding the context and relationships between words in a sentence. However, their performance\nin emotion detection specifically falls short of the state-of-the-art (SOTA) predictive model approaches (C)\n(Koco\u0144 et al., 2023).\nThe predictive model approach (C) involves training machine learning-based classifiers or regressions on\nlabeled datasets. Techniques such as support vector machines, random forests, and deep learning models\nare used to predict sentiment based on features extracted from the text. These approaches are currently\nconsidered the best for analyzing emotion according to robust tests of prediction accuracy on political text"}, {"title": "1.2 Bias in Predictive Models", "content": "Bias in predictive models originates from the training data, which in the case of sentiment analysis (SA),\nconsists of annotated text datasets. These datasets are the result of the laborious work of annotators who\nread through provided materials and assign emotional labels. Annotators can differ on many accounts,\nincluding age, gender, socio-economic status, psychological individual differences, and political orientation.\nAll these differences can impact the annotation process. Studies such as Milkowski et al. (2021) have shown\nthat individual differences among annotators can significantly affect emotion annotations in text. These\nindividual differences introduce subjectivity into data assumed to be objective, leading to inconsistencies that\ncan skew the training and evaluation of models designed to predict emotional reactions from text. Moreover,\nannotation bias can result from a mismatch between authors' and annotators' linguistic and social norms, as\nnoted by Sap et al. (2019). This mismatch often reflects broader social and demographic differences that can\nmanifest in critical research areas like hate speech and abuse detection. For instance, studies by Gordon et al.\n(2022), Larimore et al. (2021), and Waseem (2016) show that the race and gender of annotators influence not\nonly the annotation process but also the performance of NLP models, further compounding biases.\nParticularly concerning is the influence of annotators' political and ideological biases. This type of bias not\nonly includes biases against specific social groups reported in earlier studies, but its generality makes the\nspecific extent of its influence on SA models difficult to determine, although we expect it to be significant\n(Diaz et al., 2018; Kiritchenko & Mohammad, 2018; Ungless et al., 2023). Ennser-Jedenastik and Meyer\n(2018) report that coders of political texts often incorporate their prior beliefs about political parties into their\ncoding decisions. For example, annotators are more likely to perceive a sentence as supporting immigration\nif they believe it comes from a left-wing party, regardless of the actual content. Experimental studies by\nvan der Velden (2023) show that personal characteristics of annotators, like political ideology or knowledge,\ninterfere with their judgment of political stances. It's important to note that this interference might not\nbe fully realized by the annotator, as previous psychological studies have shown the influence of political\norientation on implicit judgments (Carraro et al., 2014; Jost, 2019). Here of significant importance are the\nfindings that show that people of different political orientations differ significantly in many annotation tasks\nrelated to political science, including emotion annotation of images (Webb Williams et al., 2023). This means\nthat constructing an annotation strategy that eliminates the propagation of individual bias to SA models\nmight be problematic. This problem parallels many similar ones in algorithm creation, where the human\nbehavior information, on which the model is trained, falls short of the aim of the engineered algorithm. In\nsuch cases, Morewedge and associates (2023) recommend auditing the models under suspicion by testing\nthem for the presence of bias directly."}, {"title": "1.3 Current Study", "content": "In this study, we conduct a bias audit of an existing Polish sentiment analysis model developed by our lab\n(Plisiecki et al., 2024) to determine whether its predicted valence readings show systematic differences based\non the party affiliation of a diverse group of politicians from different political parties. We predict the valence\nof the names of the politicians, as well as sentences in which their names are embedded to vary based on\ntheir political affiliation (the latter were included to analyze both the direct valence towards the politicians\nas well as take into account the usual settings in which such a model would be used, where the name of the\npoliticians would be a part of a specific sentence.) We regress the political affiliation of the politicians onto\nthe sentiment readings of the model to see how much variance it can explain. To pinpoint the source of the\nbias, we prune the training set of any mentions of the aforementioned politicians, train a second model, and\nrepeat the analysis. To explore the hypothesis that the model's bias is linked to the political orientation of\nthe annotators, we administer a political orientation questionnaire to our annotators pool."}, {"title": "2 Methods", "content": null}, {"title": "2.1 The Prediction Model", "content": null}, {"title": "2.1.1 Model Training Data", "content": "The model has been trained on a training set sampled from a comprehensive database of Polish political\ntexts from social media profiles (i.e., YouTube, Twitter, Facebook) of 25 journalists, 25 politicians, and\n19 non-governmental organizations (NGOs). The complete list of the profiles is available in the Appendix.\nFor each profile, all available posts from each platform were scraped (going back to the beginning of 2019).\nIn addition, we also used corpora, which consists of texts written by \"typical\" social media users, i.e.,\nnon-professional commentators of social affairs. Our data consists of 1246337 text snippets (Twitter: 789490\ntweets; YouTube: 42252 comments; Facebook: 414595 posts).\nAs transformer models have certain limits, i.e., their use imposes limits on length, we implemented two types\nof modification within the initial dataset. First, since texts retrieved from Facebook were longer than the\nothers, we have split them into sentences. Second, we deleted all texts that were longer than 280 characters.\nThe texts were further cleaned from social media artifacts, such as dates scrapped alongside the texts. Next,\nthe langdetect (Danilak, 2021) software was used to filter out text snippets that were not written in Polish.\nAlso, all online links and usernames in the texts were replaced with \" link \" and \"_user_\", respectively, so\nthat the model does not overfit the sources of information nor specific social media users.\nBecause most texts in the initial dataset were emotionally neutral, we filtered out the neutral texts and\nincluded only those that had higher emotional content in the final dataset. Accordingly, the texts were\nstemmed and subjected to a lexicon analysis (Imbir, 2016) using lexical norms for valence, arousal, and\ndominance the three basic components of emotions. The words in each text were summed up in terms of\ntheir emotional content extracted from the lexical database and averaged to create separate metrics for the\nthree emotional dimensions. These metrics were then summed up and used as weights to choose 8000 texts\nfor the final training dataset. Additionally, 2000 texts were selected without weights to ensure the resulting\nmodel could process both neutral and emotional texts. The proportions of the texts coming from different\nsocial media platforms reflected the initial proportions of these texts, resulting in 496 YouTube texts, 6105\nTwitter texts, and 3399 Facebook texts, totaling 10,000 texts."}, {"title": "2.1.2 Annotators and Political Orientation Questionnaire", "content": "The final dataset consisting of 10,000 texts was annotated by 20 expert annotators (age: M = 23.89, SD =\n4.10; gender: 80% female) with regards to six emotions: happiness, sadness, disgust, fear, anger, and pride,\nas well as to two-dimensional emotional metrics of valence and arousal, using a 5-point Likert scale. All\nannotators were well-versed in Polish political discourse and were students of Psychology (70% of them were\ngraduate students, which in the case of Polish academic education denotes people studying 4th and 5th year).\nThus, they underwent at least elementary training in psychology.\nSince valence and arousal might not have been familiar to annotators, before the formal annotation process\nbegan, all annotators were informed about the characteristics of valence and arousal. General annotation\nguidelines were provided to ensure consistency and minimize subjectivity. For the purpose of annotating\nvalence of texts, the annotators were given the following instruction:\nEnglish translation (An in-depth description of the annotation process is available in the Appendix):\n\"Go back to the text you just read. Now think about the sign of emotion (positive / negative) and the arousal\nyou read in a given text (no arousal / extreme arousal). Rate the text on these emotional dimensions.\u201d\nFive months after the annotation process took place, the annotators received a political orientation ques-\ntionnaire in Polish, which consisted of two items: \"The government should take action to reduce income\ndifferences\", and \"People with a homosexual orientation, gays and lesbians, should have the freedom to\narrange their lives according to their own beliefs\u201d, to which they had to respond with whether they \u201cDefinitely\nagree", "Agree\u201d, \u201cNeither agree nor disagree\u201d, \u201cDisagree\u201d, or \u201cDefinitely disagree": "ith them. These questions\nwere sourced from the European Social Survey (European Social Survey, 2020) and were meant to capture\nas much variance in the political views of the annotators as possible, while at the same time not requiring\nthem to complete a long and tedious questionnaire. The questionnaire was anonymous and not obligatory, as\nit was no longer a part of the annotation process for which they were hired. 15 responses total have been\nreceived, which constitutes 75% of the original annotation team."}, {"title": "2.1.3 Model Training", "content": "For model training, we have considered two alternative base models: the Trelbert transformer model developed\nby a team at DeepSense (Szmyd et al., 2023), and the Polish Roberta model (Dadas, 2020). The encoders\nof both models were each equipped with an additional regression layer with a sigmoid activation function.\nThe models have been trained to predict each of the six emotion intensities, as well as valence, and arousal.\nThe maximum number of epochs in each training run was set to 100. At each step, we computed the mean\ncorrelation of the predicted metrics with their actual values on the evaluation batch, and the models with\nthe highest correlations on the evaluation batch were saved to avoid overfitting. We used the MSE criterion\nto compute the loss alongside the AdamW optimizer with default hyperparameter values. Both of the base\nmodels were then subjected to a Bayesian grid search using the WandB platform (Weights & Biases, n.d.)\nwith the following values: dropout - 0; 0.2, 0.4, 0.6; learning rate - 5e-3, 5e-4, 5e-5; weight decay - 0.1, 0.3,\n0.5; warmup steps - 300, 600, 900. The model which obtained the highest correlation relied on the Roberta\ntransformer model and had the following hyperparameters: dropout = 0.6; learning rate = 5e-5; weight_decay\n= 0.3. Its average accuracy on the test set is r = 0.80, and r = 0.87 valence, which is the main metric\nanalyzed in the current study as it shows the estimated general positivity of the analyzed text."}, {"title": "2.2 Bias Testing", "content": null}, {"title": "2.2.1 Stimuli", "content": "As stimuli for testing the bias hypothesis, to limit our arbitrary choice of stimuli, we used the names of 24\nwell-known Polish politicians who appeared in the November and October 2023 trust polls (CBOS, 2023b,\nCBOS, 2023c, IBRIS, 2023). The politicians were assigned to 5 political parties/coalitions on the basis of their\naffiliation or because they were candidates of that party/coalition. These parties/coalitions are Zjednoczona\nPrawica, which is right-wing and was the ruling coalition, Trzecia Droga, Koalicja Obywatelska, Nowa Lewica,\nwhich were centre-right, centre and left opposition respectively. The fifth party was Konfederacja, which\nwas a right to far right opposition. These coalitions cover 96.25% of the total votes in the November 2023\nparliamentary elections (PKW, 2023).\nIn addition to predicting the valence of politicians' names themselves, we also embedded them in neutral\nsentences and sentences with political context to estimate how much their presence changes the valence\npredicted by the model. Both the politicians' names and the sentences are available in the appendix."}, {"title": "2.2.2 Corpus Modification", "content": "To identify the potential source of the model's bias, we locate the texts in the training set that contain the\nsurnames of these politicians. We then manually review these texts to see if they are referring to a particular\npolitician. There are 459 of these texts in total, with a range of 71 to 0 and a median of 8.5 per politician.\nWe then prune the training set of these texts and train a second model with the same training parameters to\nestimate the degree to which their presence influences the model's bias. The training set contained 7999 texts\nbefore pruning, which means that the pruned texts constitute below 6% of its size."}, {"title": "2.3 Statistical Analyses", "content": "To test for the presence of bias, we examine where there are noticeable differences in the valence of politicians'\nnames and where they can be explained by the politicians' political affiliations. For this purpose, we build\nseveral regression models. As dependent variables, we use the valence score from the original model, the same\nscore from the modified model (trained on the pruned corpus), and the differences in valence between the\nfinal and the modified model. The models return the valence score as continuous variables ranging from 0 to\n1, which we chose to then recalculate on a 0-100 scale for better readability.\nAs independent variables we use the politicians' affiliation, and potential confounders: their gender, trust\ntowards them (from the same trust surveys as the names of politicians) and mean annotated valence of texts\nin which these politicians appear, recalculated to 0-100. The trust surveys were decoded as 5-point Likert\nscales (IBRIS, 2023) or 3-point Likert scales (CBOS, 2023b, CBOS, 2023c). Responses \"I don't know\" and"}, {"title": "3 Results", "content": null}, {"title": "3.1 Regression Models", "content": "The predictive model returns the valence metric as a continuous score ranging from 0 to 100. When applied to\nthe 24 names of politicians selected for analysis, the valence scores ranged from 42.3 to 56.6, with an average\n(not weighted) (M) of 49.5 and a standard deviation (not weighted) (SD) of 3.17. To examine potential bias\nin more natural contexts, we estimated valence for names embedded in both neutral and politically charged\nsentences. The mean valence was higher in neutral sentences (M = 54.4) compared to raw names (M =\n49.5) and lower in politically charged sentences (M = 45.7). Interestingly, the differences in valence among\npoliticians (measured by the standard deviation of valence) were larger for neutral sentences (SD = 4.35)\ncompared to raw names (SD = 3.17), and smaller for politically charged sentences (SD = 1.29).\nAlong with the visualization, we regressed the valence of politicians' names, as well as the aforementioned\nsentences, as predicted by the model, onto the independent variables of interest (Table 1.). The fitted models\nwith politicians' affiliation and gender (the reasoning for the inclusion of the gender confounder is driven by\nanalyses explained in the later section Confounds) seem to describe the data well, and explain 66.5%, 52%\nand 66.2% of variance (R2). All of the coefficients have the same direction and similar magnitudes in all of\nthe three models (Model 1, 2, and 3). The hypothesis of exchangeability of scores (Manly, 1997) could be\nrejected due to low p-values: p = .008, p = .049 and p = .018, which implies that the differences in valence\nare not random."}, {"title": "3.2 Confounds", "content": "The association of political affiliation and valence was significantly stronger than between valence and the\nconfounders. This is evidenced by comparing R\u00b2 of regression on valence in raw names (Model 1, Table 2.)\nand political affiliation (R2 = .49), and models with only confounds as independent variables. The model\nincluding only gender reared R2 = .109 (statistics that relate to gender should be interpreted carefully since\nthere are only 2 women in our sample), trust towards politician achieved R2 = .195, and the mean valence\nof mentions in which a given politician appeared resulted in R2 = .175. (These models are available in the\nAppendix)\nTo find the model that best describes the data, we compare adjusted R2 with different sets of potential\nconfounds. Since the model with affiliation and gender as independent variables has the highest adjusted R2,\nthis set of independent variables is used in other models. (See Table 2.)"}, {"title": "3.3 The Modified Model", "content": "In the model modified (See Table 3.) by pruning texts containing mentions of our set of politicians, the\nrelationships between affiliation and valence decreased significantly, but bias was still present in the model\nwith raw names (Model 1., Table 3.) It should be noted that not all mentions affecting the model could be\npruned, for example, the most mentioned politician is Jaros\u0142aw Kaczy\u0144ski, but in the dataset there are tweets\nmentioning his twin brother, Lech Kaczy\u0144ski, the former president and member of the same party."}, {"title": "3.4 Questionnaire Results", "content": "For the \"The government should take action to reduce income differences\" item, 4 people (26.7%) responded\nthat they \"Definitely agree\"; \"Agree\" - 1 (6.7%); \"Neither agree nor disagree\" - 5 (33.3%); \u201cDisagree\" - 5\n(33.3%). Nobody chose the option \u201cDefinitely disagree\". For the second item,", "Definitely agree\" (86.7%), and 2 people that they \"Agree\" (13.3%), these were the\nonly two options picked by the annotators.\"\n    },\n    {\n      \"title\"": "4 Discussion"}, {"content": "In the current study we have shown that a supervised model trained on annotations created by expert\nannotators in their domain shows signs of political bias with regards to well-known politicians. While the\ndegree of bias can be considered small in some research settings (differences in valence between politicians\nare up to 6 on a scale of 0-100, or around 0.5 Cohen's d, when considering the effect on sentences), it is not\ninconsequential. Such bias, when spread throughout the dataset can rear small yet significant effects which\ncan be mistaken for real findings or obscure them.\nThe modified model, trained on a dataset pruned of texts containing politicians' names, exhibited significantly\nlower bias than the primary model. This suggests that at least a substantial part of the bias can be attributed\nto the annotations made by the annotation team. This, however, does not indicate that pruning the names\nof the politicians eradicates all kinds of biases that political orientation might result in. Furthermore, the\ninstructions given to the annotators, which prompted them to estimate the \"positivity/negativity that they\nread in each text\" rather than their emotional reactions to it, bias still propagated into the annotated dataset\nin an implicit manner. Instances of such implicit propagation of political orientation have been documented\nin previous psychological research (Carraro et al., 2014; Jost, 2019).\nConsidering the political questionnaire results, there is some evidence indicating that the political beliefs\nof the annotators may correlate with the model's bias, though this correlation is not definitive. We know\nthat annotators have varying opinions on economic issues and tend to be progressive on social issues. If"}, {"title": "4.1 Online Supplementary Material", "content": "The code used in the current study is available at the github repository https://github.com/hplisiecki/\npolitical-model-bias and the Open Science Framework (OSF) repository https://osf.io/q8bes/?view_\nonly=6f246610bc0b43cc9e98d7c978f2f6fa. The base model used for the current study is available at\nhttps://huggingface.co/hplisiecki/polemo_intensity, while the modified model is available at the\naforementioned OSF repository."}, {"title": "4.2 Funding", "content": "This research is funded by a grant from the National Science Centre (NCN) 'Research Laboratory for Digital\nSocial Sciences' (SONATA BIS-10, No. UMO-020/38/E/HS6/00302)."}]}