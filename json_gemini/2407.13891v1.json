{"title": "UNCOVERING POLITICAL BIAS IN EMOTION INFERENCE MODELS: IMPLICATIONS FOR SENTIMENT ANALYSIS IN SOCIAL SCIENCE RESEARCH", "authors": ["Hubert Plisiecki", "Pawe\u0142 Lenartowicz", "Maria Flakus", "Artur Pokropek"], "abstract": "This paper investigates the presence of political bias in emotion inference models used for sentiment analysis (SA) in social science research. Machine learning models often reflect biases in their training data, impacting the validity of their outcomes. While previous research has highlighted gender and race biases, our study focuses on political bias-an underexplored yet pervasive issue that can skew the interpretation of text data across a wide array of studies. We conducted a bias audit on a Polish sentiment analysis model developed in our lab. By analyzing valence predictions for names and sentences involving Polish politicians, we uncovered systematic differences influenced by political affiliations. Our findings indicate that annotations by human raters propagate political biases into the model's predictions. To mitigate this, we pruned the training dataset of texts mentioning these politicians and observed a reduction in bias, though not its complete elimination. Given the significant implications of political bias in SA, our study emphasizes caution in employing these models for social science research. We recommend a critical examination of SA results and propose using lexicon-based systems as a more ideologically neutral alternative. This paper underscores the necessity for ongoing scrutiny and methodological adjustments to ensure the reliability and impartiality of the use of machine learning in academic and applied contexts.", "sections": [{"title": "Introduction", "content": "It is a well-documented fact that machine learning models are prone to being biased by their training data. Studies have repeatedly shown the presence of biases against various social groups, including gender and race biases, in machine learning based sentiment analysis (SA) systems (i.e., systems that predict the positivity of text snippets (Diaz et al., 2018; Kiritchenko & Mohammad, 2018; Ungless et al., 2023). These types of biases are significant from a social justice perspective, as they can exacerbate the reporting of spurious differences between social groups. While these biases present problems for research (e.g., by reducing external validity), due to the specificity of the groups that were analyzed, they only do so for specific research results that involve these social groups. Thus, the biases reported so far do not pose a significant problem for the use of such SA systems as a whole, but rather limit their specific applications. In this paper we argue that the potential bias of machine learning based SA systems is more substantial and pervasive.\nThe aim of this research is to show that there exists one type of SA bias that not only includes the biases reported so far but also extends beyond them, rendering many research conclusions less reliable. This bias is political bias itself, or the propagation of the political orientation of the annotators, through the medium of the annotated data, to the predictions of the SA model. Unlike gender or race biases, which typically directly affect only those specific demographic groups, political bias has the potential to skew the interpretation of data across a much broader spectrum of studies, affecting societal perceptions and policymaking at a systemic level. Given that nearly every text contains some level of political nuance (Fairclough, 2013), this kind of bias can potentially influence almost every study that employs SA (especially in social sciences), making it a pervasive and critical issue to address."}, {"title": "Emotion and Sentiment Analysis in Social Sciences", "content": "In recent years, social scientists have increasingly recognized the profound influence of emotions across a broad range of disciplines. This interdisciplinary approach has illuminated the significant role emotions play in shaping human behavior and societal dynamics in fields such as political science (Mintz et al., 2022), sociology (Bericat, 2016; Turner & Stets, 2006), economics (Loewenstein, 2000), anthropology (Lutz &\nWhite, 1986), and organizational research (Diener et al., 2020), among others. The proliferation of text data sources including social media, computer-based survey responses, political speeches, newspapers, online forums, customer reviews, blogs, and e-books-has provided unprecedented opportunities to examine emotions outside traditional psychological laboratory settings. Consequently, various tools have been developed to detect emotions (Mohammad, 2016; \u00dcveges & Ring, 2023). As a result, research in this area has expanded rapidly.\nTo provide specific examples, in previous research SA been employed to predict election results (Ramteke et al., 2016), gauge public sentiment toward pressing social issues (Kim et al., 2021), and compare the emotional content between news sources from different ends of the political spectrum (Rozado et al., 2022). During the COVID-19 pandemic, numerous studies analyzed public sentiment based on online data (Alamoodi et al., 2021; Wang et al., 2022), leading to conclusions such as describing the crisis communication styles on Twitter of different Indian political leaders (Kaur et al., 2021). Similar research examined the emotional tone of the Austrian 2016 presidential election candidates (Ku\u0161en & Strembeck, 2018). From a psychological perspective, SA and emotion prediction have been used to assess suicide risk (Glenn et al., 2020), automate feedback in online cognitive behavioral therapy (Provoost et al., 2019), predict the subjective well-being of social media users (Chen et al., 2017), and analyze the subjective well-being of people over the past centuries (Hills et al., 2019). All of these studies relied on sentiment analysis written text to reach scientific conclusions, showcasing the importance of this technique in current social research. However, the exact implementation of SA can vary from study to study.\nOverall, there exist three main categories of sentiment analysis (SA) systems: A) dictionary-based approaches, B) large language model (LLM) approaches, and C) predictive model approaches. Dictionary-based approaches\n(A), also known as lexicon-based methods, rely on predefined lists of words associated with specific sentiments. These dictionaries, such as the AFINN, SentiWordNet, and LIWC (Baccianella et al., 2010; Boyd et al., 2022;\nNielsen, 2017), assign sentiment scores to words and phrases within a text to determine its overall sentiment. This method is straightforward and interpretable, but it can be limited by the coverage and accuracy of the dictionary, as well as by the inability to capture contextual information.\nIn contrast, large language model (LLM) approaches (B) leverage advanced neural networks trained on vast amounts of text data. Models such as GPT-4, LLAMA, and their derivatives can capture nuanced sentiment by understanding the context and relationships between words in a sentence. However, their performance in emotion detection specifically falls short of the state-of-the-art (SOTA) predictive model approaches (C) (Koco\u0144 et al., 2023).\nThe predictive model approach (C) involves training machine learning-based classifiers or regressions on labeled datasets. Techniques such as support vector machines, random forests, and deep learning models are used to predict sentiment based on features extracted from the text. These approaches are currently considered the best for analyzing emotion according to robust tests of prediction accuracy on political text"}, {"title": "Bias in Predictive Models", "content": "Bias in predictive models originates from the training data, which in the case of sentiment analysis (SA), consists of annotated text datasets. These datasets are the result of the laborious work of annotators who read through provided materials and assign emotional labels. Annotators can differ on many accounts, including age, gender, socio-economic status, psychological individual differences, and political orientation. All these differences can impact the annotation process. Studies such as Milkowski et al. (2021) have shown that individual differences among annotators can significantly affect emotion annotations in text. These individual differences introduce subjectivity into data assumed to be objective, leading to inconsistencies that can skew the training and evaluation of models designed to predict emotional reactions from text. Moreover, annotation bias can result from a mismatch between authors' and annotators' linguistic and social norms, as noted by Sap et al. (2019). This mismatch often reflects broader social and demographic differences that can manifest in critical research areas like hate speech and abuse detection. For instance, studies by Gordon et al.\n(2022), Larimore et al. (2021), and Waseem (2016) show that the race and gender of annotators influence not only the annotation process but also the performance of NLP models, further compounding biases.\nParticularly concerning is the influence of annotators' political and ideological biases. This type of bias not only includes biases against specific social groups reported in earlier studies, but its generality makes the specific extent of its influence on SA models difficult to determine, although we expect it to be significant (Diaz et al., 2018; Kiritchenko & Mohammad, 2018; Ungless et al., 2023). Ennser-Jedenastik and Meyer (2018) report that coders of political texts often incorporate their prior beliefs about political parties into their coding decisions. For example, annotators are more likely to perceive a sentence as supporting immigration if they believe it comes from a left-wing party, regardless of the actual content. Experimental studies by van der Velden (2023) show that personal characteristics of annotators, like political ideology or knowledge, interfere with their judgment of political stances. It's important to note that this interference might not be fully realized by the annotator, as previous psychological studies have shown the influence of political orientation on implicit judgments (Carraro et al., 2014; Jost, 2019). Here of significant importance are the findings that show that people of different political orientations differ significantly in many annotation tasks related to political science, including emotion annotation of images (Webb Williams et al., 2023). This means that constructing an annotation strategy that eliminates the propagation of individual bias to SA models might be problematic. This problem parallels many similar ones in algorithm creation, where the human behavior information, on which the model is trained, falls short of the aim of the engineered algorithm. In such cases, Morewedge and associates (2023) recommend auditing the models under suspicion by testing them for the presence of bias directly."}, {"title": "Current Study", "content": "In this study, we conduct a bias audit of an existing Polish sentiment analysis model developed by our lab (Plisiecki et al., 2024) to determine whether its predicted valence readings show systematic differences based on the party affiliation of a diverse group of politicians from different political parties. We predict the valence of the names of the politicians, as well as sentences in which their names are embedded to vary based on their political affiliation (the latter were included to analyze both the direct valence towards the politicians as well as take into account the usual settings in which such a model would be used, where the name of the politicians would be a part of a specific sentence.) We regress the political affiliation of the politicians onto the sentiment readings of the model to see how much variance it can explain. To pinpoint the source of the bias, we prune the training set of any mentions of the aforementioned politicians, train a second model, and repeat the analysis. To explore the hypothesis that the model's bias is linked to the political orientation of the annotators, we administer a political orientation questionnaire to our annotators pool."}, {"title": "Methods", "content": null}, {"title": "The Prediction Model", "content": null}, {"title": "Model Training Data", "content": "The model has been trained on a training set sampled from a comprehensive database of Polish political texts from social media profiles (i.e., YouTube, Twitter, Facebook) of 25 journalists, 25 politicians, and 19 non-governmental organizations (NGOs). The complete list of the profiles is available in the Appendix. For each profile, all available posts from each platform were scraped (going back to the beginning of 2019). In addition, we also used corpora, which consists of texts written by \"typical\" social media users, i.e., non-professional commentators of social affairs. Our data consists of 1246337 text snippets (Twitter: 789490 tweets; YouTube: 42252 comments; Facebook: 414595 posts).\nAs transformer models have certain limits, i.e., their use imposes limits on length, we implemented two types of modification within the initial dataset. First, since texts retrieved from Facebook were longer than the others, we have split them into sentences. Second, we deleted all texts that were longer than 280 characters.\nThe texts were further cleaned from social media artifacts, such as dates scrapped alongside the texts. Next, the langdetect (Danilak, 2021) software was used to filter out text snippets that were not written in Polish. Also, all online links and usernames in the texts were replaced with \" link \" and \"_user_\", respectively, so that the model does not overfit the sources of information nor specific social media users.\nBecause most texts in the initial dataset were emotionally neutral, we filtered out the neutral texts and included only those that had higher emotional content in the final dataset. Accordingly, the texts were stemmed and subjected to a lexicon analysis (Imbir, 2016) using lexical norms for valence, arousal, and dominance the three basic components of emotions. The words in each text were summed up in terms of their emotional content extracted from the lexical database and averaged to create separate metrics for the three emotional dimensions. These metrics were then summed up and used as weights to choose 8000 texts for the final training dataset. Additionally, 2000 texts were selected without weights to ensure the resulting model could process both neutral and emotional texts. The proportions of the texts coming from different social media platforms reflected the initial proportions of these texts, resulting in 496 YouTube texts, 6105 Twitter texts, and 3399 Facebook texts, totaling 10,000 texts."}, {"title": "Annotators and Political Orientation Questionnaire", "content": "The final dataset consisting of 10,000 texts was annotated by 20 expert annotators (age: M = 23.89, SD = 4.10; gender: 80% female) with regards to six emotions: happiness, sadness, disgust, fear, anger, and pride, as well as to two-dimensional emotional metrics of valence and arousal, using a 5-point Likert scale. All annotators were well-versed in Polish political discourse and were students of Psychology (70% of them were graduate students, which in the case of Polish academic education denotes people studying 4th and 5th year). Thus, they underwent at least elementary training in psychology.\nSince valence and arousal might not have been familiar to annotators, before the formal annotation process began, all annotators were informed about the characteristics of valence and arousal. General annotation guidelines were provided to ensure consistency and minimize subjectivity. For the purpose of annotating valence of texts, the annotators were given the following instruction:\nEnglish translation (An in-depth description of the annotation process is available in the Appendix):\nFive months after the annotation process took place, the annotators received a political orientation ques- tionnaire in Polish, which consisted of two items: \"The government should take action to reduce income differences\", and \"People with a homosexual orientation, gays and lesbians, should have the freedom to arrange their lives according to their own beliefs\u201d, to which they had to respond with whether they \u201cDefinitely agree\", \"Agree\u201d, \u201cNeither agree nor disagree\u201d, \u201cDisagree\u201d, or \u201cDefinitely disagree\" with them. These questions were sourced from the European Social Survey (European Social Survey, 2020) and were meant to capture as much variance in the political views of the annotators as possible, while at the same time not requiring them to complete a long and tedious questionnaire. The questionnaire was anonymous and not obligatory, as it was no longer a part of the annotation process for which they were hired. 15 responses total have been received, which constitutes 75% of the original annotation team."}, {"title": "Model Training", "content": "For model training, we have considered two alternative base models: the Trelbert transformer model developed by a team at DeepSense (Szmyd et al., 2023), and the Polish Roberta model (Dadas, 2020). The encoders of both models were each equipped with an additional regression layer with a sigmoid activation function. The models have been trained to predict each of the six emotion intensities, as well as valence, and arousal. The maximum number of epochs in each training run was set to 100. At each step, we computed the mean correlation of the predicted metrics with their actual values on the evaluation batch, and the models with the highest correlations on the evaluation batch were saved to avoid overfitting. We used the MSE criterion to compute the loss alongside the AdamW optimizer with default hyperparameter values. Both of the base models were then subjected to a Bayesian grid search using the WandB platform (Weights & Biases, n.d.) with the following values: dropout - 0; 0.2, 0.4, 0.6; learning rate - 5e-3, 5e-4, 5e-5; weight decay - 0.1, 0.3, 0.5; warmup steps - 300, 600, 900. The model which obtained the highest correlation relied on the Roberta transformer model and had the following hyperparameters: dropout = 0.6; learning rate = 5e-5; weight_decay = 0.3. Its average accuracy on the test set is r = 0.80, and r = 0.87 valence, which is the main metric analyzed in the current study as it shows the estimated general positivity of the analyzed text."}, {"title": "Bias Testing", "content": null}, {"title": "Stimuli", "content": "As stimuli for testing the bias hypothesis, to limit our arbitrary choice of stimuli, we used the names of 24 well-known Polish politicians who appeared in the November and October 2023 trust polls (CBOS, 2023b, CBOS, 2023c, IBRIS, 2023). The politicians were assigned to 5 political parties/coalitions on the basis of their affiliation or because they were candidates of that party/coalition. These parties/coalitions are Zjednoczona Prawica, which is right-wing and was the ruling coalition, Trzecia Droga, Koalicja Obywatelska, Nowa Lewica, which were centre-right, centre and left opposition respectively. The fifth party was Konfederacja, which was a right to far right opposition. These coalitions cover 96.25% of the total votes in the November 2023 parliamentary elections (PKW, 2023).\nIn addition to predicting the valence of politicians' names themselves, we also embedded them in neutral sentences and sentences with political context to estimate how much their presence changes the valence predicted by the model. Both the politicians' names and the sentences are available in the appendix."}, {"title": "Corpus Modification", "content": "To identify the potential source of the model's bias, we locate the texts in the training set that contain the surnames of these politicians. We then manually review these texts to see if they are referring to a particular politician. There are 459 of these texts in total, with a range of 71 to 0 and a median of 8.5 per politician. We then prune the training set of these texts and train a second model with the same training parameters to estimate the degree to which their presence influences the model's bias. The training set contained 7999 texts before pruning, which means that the pruned texts constitute below 6% of its size."}, {"title": "Statistical Analyses", "content": "To test for the presence of bias, we examine where there are noticeable differences in the valence of politicians' names and where they can be explained by the politicians' political affiliations. For this purpose, we build several regression models. As dependent variables, we use the valence score from the original model, the same score from the modified model (trained on the pruned corpus), and the differences in valence between the final and the modified model. The models return the valence score as continuous variables ranging from 0 to 1, which we chose to then recalculate on a 0-100 scale for better readability.\nAs independent variables we use the politicians' affiliation, and potential confounders: their gender, trust towards them (from the same trust surveys as the names of politicians) and mean annotated valence of texts in which these politicians appear, recalculated to 0-100. The trust surveys were decoded as 5-point Likert scales (IBRIS, 2023) or 3-point Likert scales (CBOS, 2023b, CBOS, 2023c). Responses \"I don't know\" and \"difficult to say\" were recoded as neutral. For each survey, a normalized score was calculated, and the mean of these normalized scores was included in the analysis. Mean annotated valence scores were derived from texts that were later pruned, see 'Corpus Modification', and recalculated to 0-100 scale."}, {"title": "Results", "content": null}, {"title": "Regression Models", "content": "The predictive model returns the valence metric as a continuous score ranging from 0 to 100. When applied to the 24 names of politicians selected for analysis, the valence scores ranged from 42.3 to 56.6, with an average (not weighted) (M) of 49.5 and a standard deviation (not weighted) (SD) of 3.17. To examine potential bias in more natural contexts, we estimated valence for names embedded in both neutral and politically charged sentences. The mean valence was higher in neutral sentences (M = 54.4) compared to raw names (M =\n49.5) and lower in politically charged sentences (M = 45.7). Interestingly, the differences in valence among politicians (measured by the standard deviation of valence) were larger for neutral sentences (SD = 4.35) compared to raw names (SD = 3.17), and smaller for politically charged sentences (SD = 1.29).\nAlong with the visualization, we regressed the valence of politicians' names, as well as the aforementioned sentences, as predicted by the model, onto the independent variables of interest (Table 1.). The fitted models with politicians' affiliation and gender (the reasoning for the inclusion of the gender confounder is driven by analyses explained in the later section Confounds) seem to describe the data well, and explain 66.5%, 52% and 66.2% of variance (R2). All of the coefficients have the same direction and similar magnitudes in all of the three models (Model 1, 2, and 3). The hypothesis of exchangeability of scores (Manly, 1997) could be rejected due to low p-values: p = .008, p = .049 and p = .018, which implies that the differences in valence are not random."}, {"title": "Confounds", "content": "The association of political affiliation and valence was significantly stronger than between valence and the confounders. This is evidenced by comparing R\u00b2 of regression on valence in raw names (Model 1, Table 2.) and political affiliation (R2 = .49), and models with only confounds as independent variables. The model including only gender reared R2 = .109 (statistics that relate to gender should be interpreted carefully since there are only 2 women in our sample), trust towards politician achieved R2 = .195, and the mean valence of mentions in which a given politician appeared resulted in R2 = .175. (These models are available in the Appendix)\nTo find the model that best describes the data, we compare adjusted R2 with different sets of potential confounds. Since the model with affiliation and gender as independent variables has the highest adjusted R2, this set of independent variables is used in other models. (See Table 2.)"}, {"title": "The Modified Model", "content": "In the model modified (See Table 3.) by pruning texts containing mentions of our set of politicians, the relationships between affiliation and valence decreased significantly, but bias was still present in the model with raw names (Model 1., Table 3.) It should be noted that not all mentions affecting the model could be pruned, for example, the most mentioned politician is Jaros\u0142aw Kaczy\u0144ski, but in the dataset there are tweets mentioning his twin brother, Lech Kaczy\u0144ski, the former president and member of the same party."}, {"title": "Questionnaire Results", "content": "For the \"The government should take action to reduce income differences\" item, 4 people (26.7%) responded that they \"Definitely agree\"; \"Agree\" - 1 (6.7%); \"Neither agree nor disagree\" - 5 (33.3%); \u201cDisagree\" - 5 (33.3%). Nobody chose the option \u201cDefinitely disagree\". For the second item, \"People with a homosexual orientation, gays and lesbians, should have the freedom to arrange their lives according to their own beliefs\" 13 people responded with \u201cDefinitely agree\" (86.7%), and 2 people that they \"Agree\" (13.3%), these were the only two options picked by the annotators."}, {"title": "Discussion", "content": "In the current study we have shown that a supervised model trained on annotations created by expert annotators in their domain shows signs of political bias with regards to well-known politicians. While the degree of bias can be considered small in some research settings (differences in valence between politicians are up to 6 on a scale of 0-100, or around 0.5 Cohen's d, when considering the effect on sentences), it is not inconsequential. Such bias, when spread throughout the dataset can rear small yet significant effects which can be mistaken for real findings or obscure them.\nThe modified model, trained on a dataset pruned of texts containing politicians' names, exhibited significantly lower bias than the primary model. This suggests that at least a substantial part of the bias can be attributed to the annotations made by the annotation team. This, however, does not indicate that pruning the names of the politicians eradicates all kinds of biases that political orientation might result in. Furthermore, the instructions given to the annotators, which prompted them to estimate the \"positivity/negativity that they read in each text\" rather than their emotional reactions to it, bias still propagated into the annotated dataset in an implicit manner. Instances of such implicit propagation of political orientation have been documented in previous psychological research (Carraro et al., 2014; Jost, 2019).\nConsidering the political questionnaire results, there is some evidence indicating that the political beliefs of the annotators may correlate with the model's bias, though this correlation is not definitive. We know that annotators have varying opinions on economic issues and tend to be progressive on social issues. If these factors were the main sources of bias, the bias towards a progressive party (Nowa Lewica) should differ significantly from that towards the most conservative party (Konfederacja). However, this is not the case as the results consistently show a lower score for the ruling party (Zjednoczona Prawica) and a higher valence for all opposition parties, including Konfederacja. This could indicate an anti-government sentiment stronger than political leanings.\nAlternatively, existing evidence that people's political attitudes may not fully align with their supported parties' programs might prompt a different interpretation. Polish voter survey results (CBOS 2023a) reveal that opposition party voters' views are more similar to each other than to those of their party leaders and programs. This could explain the relative positivity of the Konfederacja members, despite the strong progressive leaning of the political questionnaire results. Furthermore, Zjednoczona Prawica's voters are shown to be more culturally conservative and economically leftist compared to voters of other parties and the implications of the party's program. The opposite goes for the questionnaire respondents, being strongly progressive, and inconsistent when it comes to economic issues. While the nature of the collected data does not permit drawing specific conclusions as to how exactly the political leanings of the annotators affect annotation results, previous studies on the manner support the general hypothesis of the propagation of the political orientation of annotators to the annotation results (Carraro et al., 2014; Jost, 2019; Webb Williams et al., 2023).\nThe existence of political bias in the model has thus been clearly documented, and so has its causal connection to the training data. This means not only that the annotations made by humans can lead to biased models, but also raises the very real possibility that their bias might have spread to more concepts in the dataset. If people implicitly propagate their political orientation towards social groups (Diaz et al., 2018; Kiritchenko &\nMohammad, 2018; Ungless et al., 2023) as well as specific politicians as proven by the current study, the only thing standing in the way of abstract concepts being affected by the same type of bias is the ability of the model to pick up on it.\nAs language models become more advanced, their understanding of language becomes gradually less reliant on specific entities which they pick up from the text as in the case of for example Naive Bayes algorithms (Webb et al., 2010), and more reliant on relations between abstract concepts. This is evidenced by the distributed nature of the information that large language models and other transformer-reliant architectures use, through the mechanisms of attention, to generate their outputs (Vaswani et al., 2017), as well as by the recent LLM interpretability research showcasing the crystallization of abstract concepts within the inner layers of these models (Templeton et al., 2024). This means that as models improve, the propensity of the models being biased towards specific abstract concepts such as for example anarchism, or democracy might increase, given that such bias will be present in the training data, which is likely.\nAt the same time auditing a model for these kinds of more abstract biases is way more difficult than doing so for specific entities, such as politicians or social groups. First, it is hard to create lexicons of words that are of the same valence, but of different ideological leaning. Take \"conservatism\" and \"progressivism\" - while the two are undoubtedly opposite to each other in terms of ideological meaning, it is not clear whether their base valence should be equal, even when assessed by an extreme centrist. Second, as the models get better at understanding human language, the identification of the sources of a specific bias in the training set would become more and more laborious, as in the case of abstract concepts, every sentence could relate to them in one way or another leading to the bias being spread across the whole dataset.\nGiven the biases that have been already uncovered in SA models, as well as those more abstract that can lurk in the shadows, yet unidentified, we discourage the use of such models for research, and advise caution in interpreting the results of those that have already used them. To stress the kinds of problems their use can lead to, let's go back to the examples of research performed with the use of SA systems. The analysis of the sentiment towards social issues might be biased towards the sentiment of the annotators team (Kim et al., 2021). Similarly, when comparing emotional content of news sources, the same propagation of bias can occur (Rozado et al., 2022), directly biasing the conclusions. This problem of propagation of bias directly biases studies that apply their SA systems to compare different groups of texts in terms of emotionality. When trying to predict something using SA scores, like in the case of predicting election results, assessing suicide risk, or subjective wellbeing, the effectiveness of the predictive model can be influenced by the beliefs of the annotator group, leading to replication issues (Chen et al., 2017; Glenn et al., 2020; Ramteke et al., 2016).\nAt the same time, when creating customer facing solutions such as automating feedback in online cognitive behavioral therapy one has to consider that annotator biases might lead to people with different political predispositions receiving different standards of care, however here the influence is not as clear cut as in earlier cases.\nHowever, not all of the studies mentioned in this paper are affected by this bias, as many of them have relied on lexicon-based SA systems, forgoing the increased accuracy of the predictive models. As these approaches depend on lists of emotionally loaded words which are not ideologically relevant, annotated separately, and without any contexts, they are significantly less susceptible to propagate the bias of their annotators. Given that reducing the bias in predictive models becomes gradually more complex when the bias does not relate to specific entities but rather to concepts, using lexicon models seems to be the only viable alternative. The higher accuracy of transformer-based, and other predictive models (Widmann & Wich, 2023) could be therefore traded in for the less accurate, but ideologically neutral lexicon-based systems. In the case where authors choose to use SA systems anyway, we recommend them to take the possibility of different types of potential biases into consideration when analyzing their results, and if possible, to corroborate their results using a lexicon-based system.\nThe alternative in the form of picking the annotation team so that it is balanced with regards to all of the individual differences such as political orientation and others that could influence their annotations is not viable as 1. it is as of yet not clear which differences could play a role in the annotation process 2. balancing a large number of them would require a very large annotation team which would be very resource intensive.\nIn conclusion, the current paper shows that supervised models trained on datasets annotated by humans are susceptible to showing the same biases as annotators, despite the annotation instructions being phrased in a way that should avoid the propagation of such bias. This result should be taken into consideration when conducting and interpreting sentiment analysis research in the political science sphere and beyond. We therefore recommend the research community to perceive machine learning based sentiment analysis models as biased until proven otherwise.\nThe main limitation of the current study is its focus on a single sentiment analysis model and a specific dataset largely composed of political texts in Polish. While these conditions are ideal for exploring political bias within the context of Polish politics, the generalizability of the findings cannot be stated with certainty, although should be taken into consideration. We recommend the researchers that are in doubt of whether our results extend to their models to replicate our findings before using them. Additionally, the sample size of politicians and the specific sentences used to assess bias were relatively small, which may limit the robustness of our regression analyses. The political orientation questionnaire administered to annotators also had a limited scope, potentially overlooking other ideological dimensions that could influence annotation behavior. Future research should aim to replicate these findings across diverse datasets, expand the number of annotators and the range of their political orientations, and explore the interaction between different types of bias in sentiment analysis models. This would not only help validate the current results but also enhance our understanding of how various biases can coexist and interact within AI models used in social science research."}, {"title": "Online Supplementary Material", "content": "The code used in the current study is available at the github repository https://github.com/hplisiecki/ political-model-bias and the Open Science Framework (OSF) repository https://osf.io/q8bes/?view_ only=6f246610bc0b43cc9e98d7c978f2f6fa. The base model used for the current study is available at https://huggingface.co/hplisiecki/polemo_intensity, while the modified model is available at the aforementioned OSF repository."}, {"title": "Funding", "content": "This research is funded by a grant from the National Science Centre (NCN) 'Research Laboratory for Digital Social Sciences' (SONATA BIS-10, No. UMO-020/38/E/HS6/00302)."}]}