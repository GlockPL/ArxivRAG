{"title": "Fair Summarization: Bridging Quality and Diversity in Extractive Summaries", "authors": ["Sina Bagheri Nezhad", "Sayan Bandyapadhyay", "Ameeta Agrawal"], "abstract": "Fairness in multi-document summarization of user-generated content remains a critical challenge in natural language processing (NLP). Existing summarization methods often fail to ensure equitable representation across different social groups, leading to biased outputs. In this paper, we introduce two novel methods for fair extractive summarization: FairExtract, a clustering-based approach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints. We evaluate these methods using Divsumm summarization dataset of White-aligned, Hispanic, and African-American dialect tweets and compare them against relevant baselines. The results obtained using a comprehensive set of summarization quality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well as a fairness metric F, demonstrate that FairExtract and FairGPT achieve superior fairness while maintaining competitive summarization quality. Additionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that integrate quality and fairness into a single evaluation framework, offering a more nuanced understanding of the trade-offs between these objectives. This work highlights the importance of fairness in summarization and sets a benchmark for future research in fairness-aware NLP models.", "sections": [{"title": "Introduction", "content": "Multi-document summarization, which condenses multiple documents into a concise summary, is a fundamental task in natural language processing (NLP). Summarization methods are typically either extractive, selecting the most important sentences, or abstractive, where the content is rephrased.\nEarly research focused on summarizing formal text sources such as news articles. However, with the rise of social media, attention has shifted to summarizing user-generated content, which is diverse in style and language. Social media platforms bring together users from varied backgrounds, introducing linguistic diversity through informal language, slang, and emojis. This diversity raises the challenge of ensuring fairness in summarization ensuring balanced representation of various social groups. In social media, where public opinion is shaped, fair summaries are essential to include different perspectives and avoid underrepresentation of one or more social groups.\nIn the context of social media, where millions of users contribute diverse perspectives, ensuring representation of this diversity in summaries becomes crucial. Social media platforms encompass a wide range of voices, including those from historically underrepresented or marginalized groups, making it essential that summarization methods capture this diversity fairly. Without proper representation, certain voices might be excluded or misrepresented, leading to biased summaries that skew public discourse (Binns, 2017; Hutchinson and Mitchell, 2018). The need for fairness in summarization is further heightened by the fact that user-generated content is often informal and marked by dialectal variations, requiring models to go beyond traditional summarization approaches (Pitsilis et al., 2018). Therefore, ensuring that all groups across race, gender, and linguistic diversity-are fairly represented is critical for generating balanced summaries that reflect the diversity of public opinion (Dash et al., 2018).\nDespite advancements, bias remains a concern in automated summarization (Dash et al., 2019; Jung et al., 2019; Keswani and Celis, 2021; Olabisi et al., 2022) as most existing summarization methods focus on quality but fall short in optimizing fairness. This gap leads to the key question: if a summarization method is optimized for fairness, how does it affect the overall summary quality?\nPrevious studies suggest a trade-off between fairness and quality (Jung et al., 2019). Improving fairness can sometimes lower quality. While existing algorithms have made strides in balancing the two, none achieve perfect fairness.\nIn this paper, we address two research questions:\n1.  How does achieving perfectly fair summaries affect overall quality?\n2.  How well do current methods perform when considering both fairness and quality?\nTo illustrate the performance of fairness-aware summarization models, we compare summaries generated by ChatGPT-EXT (Zhang et al., 2023) and our proposed FairGPT model on a sample instance from Divsumm dataset (Olabisi et al., 2022). As shown in Table 1, FairGPT ensures equal representation of tweets from different groups, while ChatGPT-EXT shows a slight imbalance.\nWe make the following contributions:\n\u2022 We propose FairExtract, a fair clustering-based extractive summarization method that achieves perfect fairness and is evaluated against baseline models using standard and composite quality-fairness metrics.\n\u2022 We develop FairGPT, a large language model-based extractive summarization method that enforces fairness through equal representation and accurate content extraction using the longest common subsequence.\n\u2022 We introduce composite metrics combining normalized quality scores with fairness, providing a comprehensive analysis of the quality-fairness trade-off in summarization models."}, {"title": "Related Work", "content": "The field of natural language processing (NLP) has increasingly focused on addressing bias and fairness, driven by the demand for equity in Al systems. Research has explored two key dimensions: intrinsic bias, stemming from text representations, and extrinsic bias, reflecting performance disparities across demographic groups (Han et al., 2023).\nEarly work on fairness in summarization (Shandilya et al., 2018; Dash et al., 2019) revealed that summaries often fail to represent source data fairly, even when source texts from different groups have similar quality. This led to the development of fairness-aware algorithms across various stages of summarization, including pre-processing, in-processing, and post-processing techniques. For example, Keswani and Celis (2021) proposed a post-processing method to mitigate dialect-based biases. Olabisi et al. (2022) introduced the DivSumm dataset, focusing on dialect diversity in summarization and evaluating algorithms on fairness.\nRecent work has explored bias related to the position of input data. Olabisi and Agrawal (2024) studied position bias in multi-document summarization, showing that the order of input texts affects fairness. Similarly, Huang et al. (2023) analyzed clustering-based summarization models, which may introduce political or opinion bias, emphasizing the need for fair representation.\nFair clustering, another key technique, has also seen significant research. Chierichetti et al. (2017) introduced the concept of fairlets-small, balanced clusters that ensure fair representation across protected groups. Building on this, Chen et al. (2019) proposed proportional centroid clustering to eliminate biases in cluster-based models.\nFurther advancements include scalable techniques for fair clustering, such as the fair k-median clustering method (Backurs et al., 2019), and approaches that generalize fairness constraints across multiple protected groups (Bera et al., 2019). Esmaeili et al. (2020) extended this work to probabilistic fair clustering, offering solutions for uncertain group memberships.\nIn the domain of clustering methodologies, Micha and Shah (2020) explored fairness in centroid clustering, while Li et al. (2020) proposed Deep Fair Clustering (DFC), which leverages deep learning to filter sensitive attributes, improving both fairness and performance. This underscores the growing importance of combining fairness with robust clustering methods in NLP tasks."}, {"title": "Task Formulation", "content": "In this work, we address the challenge of diversity-preserving multi-document extractive summarization. Given a collection of documents D = {d1,d2,...,dn} from two diverse social groups, G1 and G2, the goal is to produce an extractive summary S = {81, 82, ..., sk} C D of length k << n, ensuring balanced representation from both groups.\nIn this context, each document is a tweet from a specific dialect group, which serves as an indicator of its social group. Traditionally, various metrics like ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2019) have been used to evaluate summary quality. However, our primary focus is on balancing both quality and fairness, particularly in terms of representing different social groups equitably. To measure fairness, we use the Representation Gap (RG) metric, as proposed by Olabisi et al. (2022). This metric captures how well the summary reflects the proportions of the original groups. A lower RG score indicates better balance and thus a fairer summary.\nFor a summary S of length k, let N1(S) and N2(S) represent the number of documents from groups G1 and G2, respectively. The Representation Gap is defined as:\nRG(S) = \\frac{max{N_1(S), N_2(S)} \u2013 min{N_1(S), N_2(S)}}{k}\n(1)\nFor example, if k = 6, with 4 documents from G1 and 2 from G2, the RG is 0.333. When both groups are equally represented, the RG is 0, indicating a perfectly fair summary.\nOur analysis faces two key challenges: (1) While quality metrics improve with larger values, fairness improves with smaller Representation Gap (RG) values. (2) Quality and fairness metrics differ greatly in scale, making direct comparison difficult.\nTo address these issues, we introduce a new fairness metric, F, defined as:\nF(S) = 1 - RG(S)\n(2)"}, {"title": "Fair Extractive Summarizers", "content": "In this work, we introduce two novel methods for fair extractive summarization: FairExtract and FairGPT. FairExtract utilizes clustering techniques with fairlet decomposition to ensure diversity in summaries while maintaining high-quality representation across different groups. FairGPT, on the other hand, leverages large language models (LLMs) such as GPT-3.5, incorporating fairness constraints and the longest common subsequence (LCS) method to match and fairly select content from different groups. Both methods prioritize fairness and ensure equitable representation in the generated summaries."}, {"title": "FairExtract: A Clustering-based Fair Extractive Summarization Method", "content": "The task of clustering is central to the FairExtract process, which aims to generate diversity-preserving summaries. The method combines document embeddings, fairlet decomposition, and clustering techniques to ensure both fairness and quality. Below, we describe the steps involved in detail:\n1.  Embedding Documents: We begin by embedding each document (tweet) into a high-dimensional space (e.g., using a pretrained model such as BERT (Devlin et al., 2019)), capturing its semantic content in Euclidean space. This embedding enables us to compute meaningful distances between documents, which is crucial for clustering.\n2.  Fairlet Decomposition: To ensure fairness in the summarization process, we decompose the dataset into fairlets. A fairlet is the smallest set of documents that maintains proportional balance between two groups, G1 and G2 (Backurs et al., 2019). If the proportions of G1 and G2 are g1 and g2, respectively, where gcd(g1,92) = 1, a fairlet must contain exactly g1 documents from G1 and g2 documents from G2. This ensures that the composition of the fairlet reflects the required ratio between the two groups, maintaining fairness at the smallest possible scale. The decomposition aims to minimize the sum of Euclidean distances between documents within the same fairlet.\n3.  Finding the Fairlet Center: Once the dataset is divided into fairlets, we compute the center of each fairlet. The center is the document within the fairlet that minimizes the sum of distances to all other documents in the same fairlet. This document acts as the representative of the fairlet, summarizing the content while maintaining group balance.\n4.  k-Median Clustering on Fairlet Centers: After identifying the centers of all fairlets, we apply the k-median clustering algorithm to these centers. In the k-median problem, we are given a set of points P in a d-dimensional space, and we aim to partition them into k clusters II = {P1,..., Pk} that minimize the following cost:\nmin\\limits_{C \\subset P:|C|=k} \\sum\\limits_{c_i \\in C|1<i<k} \\sum\\limits_{p \\in P_i} |p-c_i|.\n(4)\nThe number of clusters k is selected such that k \u00d7 (g1+g2) equals the desired number of documents in the summary. This step ensures that the clusters formed are representative of both social groups.\n5.  Summary Construction: From each k-median cluster, we select the center fairlet and include all documents within that fairlet in the final summary. By selecting one fairlet from each cluster, we maintain both quality and fairness, as the summary reflects the balanced representation of both groups. The resulting extractive summary ensures that the most salient information is captured while maintaining equitable representation of the social groups."}, {"title": "FairGPT: An LLM-based Fair Extractive Summarization Method", "content": "FairGPT leverages GPT-3.5-turbo to generate fair extractive summaries by selecting an equal number of sentences from different social groups. It applies fairness checks and uses the longest common subsequence (LCS) to match generated summaries with the original tweets. Below are the detailed steps:\n1.  Input Preparation: The dataset is split into two groups (e.g., White-aligned and Hispanic dialects), and a document with sentences for each group is created as input for the summarization process.\n2.  Summarization using an LLM: We use an LLM (GPT-3.5-turbo) to generate a summary of length L, selecting L/2 sentences from each group to ensure balanced representation. The specific prompt used for this task is available in the Prompt 1.\n3.  Matching using Longest Common Subsequence (LCS): As GPT sometimes generates partial sentences, we apply LCS to match the generated summary with the closest original tweets. The full tweets corresponding to the longest common subsequences are added to the final summary.\n4.  Output Check: After generating the summary, we verify two key aspects. First, at least 50% of the content in each GPT-generated sentence must match the corresponding original tweet using the LCS. Second, we ensure that the summary is perfectly fair, with equal representation from each group.\nThis output check is crucial because large language models, such as GPT-3.5-turbo, sometimes generate unexpected outputs that do not align with the input instructions. To ensure the generated summaries meet both fairness and content similarity criteria, we repeat the process if either condition is not satisfied. In our tests of generating 75 summaries, the repetition process never exceeded 10 iterations, and the average number of repetitions across all tests was 1.6, indicating the efficiency and reliability of the output check mechanism.\n5.  Final Output: Once the summary satisfies both fairness and similarity requirements, it is saved as the final output."}, {"title": "Experimental Setup", "content": "Next, we describe the dataset, baseline methods, and evaluation metrics that are used to comprehensively assess the quality and fairness of the generated summaries."}, {"title": "Dataset", "content": "The dataset used in this study is DivSumm (Olabisi et al., 2022), consisting of tweets from three ethnic groups-White-aligned, Hispanic, and African-American-across 25 topics, with 30 tweets per group per topic, totaling 2,250 tweets. This diversity allows us to evaluate the fairness of our summarization methods across varied social and cultural contexts.\nOur model works with two groups at a time, so we explore three pairings: White-Hispanic, Hispanic-African American, and White-African American. Each pairing maintains proportional representation from both groups to ensure an equitable balance in the summarization process.\nFor our experiments, we formed 60 tweets per group pair (30 from each group) and generated a 6-tweet summary per pair, covering all 25 topics. This yielded 75 distinct summaries per model, allowing us to evaluate both fairness and quality comprehensively."}, {"title": "Baseline Methods", "content": "Here, we provide a detailed description of the baseline methods used in our comparative analysis:\nNaive: In the Naive baseline method, L tweets are randomly chosen from the input without any specific criteria. This approach represents a straightforward, non-strategic selection process and serves as a basic reference point for evaluating other methods.\nNaiveFair: The NaiveFair baseline method involves randomly selecting L/2 tweets from each social group. This method ensures equal representation from each group, providing a basic notion of fairness without any sophisticated processing.\nFor the Naive and NaiveFair methods, which involve randomness in selecting summaries, we conducted the experiment five times for each summary, resulting in 375 different summaries for each of these methods.\nTextRank: TextRank is an unsupervised graph-based ranking method used for extractive summarization (Mihalcea and Tarau, 2004). This standard vanilla baseline approach uses a single aggregated set of randomized documents from all groups as input for summarization, without any pre-processing.\nBERT-Ext: BERT-Ext is an extractive summarization model that utilizes pre-trained embeddings from BERT and k-means clustering to select sentences closest to the centroid as summaries (Miller, 2019). Similar to the TextRank baseline, we implemented BERT-Ext vanilla method.\nCluster-Heuristic (Cluster-H): This method first partitions the input documents into group-based subsets before generating separate group summaries of length. These group-level summaries are shuffled, combined and then used to generate a final, unified summary (Dash et al., 2019; Olabisi et al., 2022). As summarization models, we use TextRank and BERT-Ext.\nCluster-Automatic (Cluster-A): In this attribute-agnostic approach, documents are clustered automatically into m subsets, and corresponding summaries of length are generated. The summaries are concatenated and used to generate a final summary (Olabisi et al., 2022). As summarization models, we experiment with TextRank and BERT-Ext.\nChatGPT-EXT: This approach uses GPT-3.5 for extractive summarization by employing in-context learning and chain-of-thought reasoning to identify key sentences. It focuses on extracting salient content from documents to generate coherent summaries while maintaining the structure of the original text (Zhang et al., 2023)."}, {"title": "Evaluation Metrics", "content": "Below, we list the several reference-free metrics which do not rely on human-written reference text used for evaluation in this study.\n\u2022 SUPERT: SUPERT (Gao et al., 2020) evaluates the quality of a summary by measuring its semantic similarity with a pseudo reference summary. It employs contextualized embeddings and soft token alignment techniques, providing an in-depth analysis of the semantic fidelity of generated summaries.\n\u2022 BLANC: BLANC (Vasilyev et al., 2020) is a reference-less metric that measures the improvement in a pretrained language model's performance during language understanding tasks when given access to a summary.\n\u2022 SummaQA: SummaQA (Scialom et al., 2019) employs a question-answering model based on BERT to answer cloze-style questions using the system-generated summaries, providing insights into the summarization's factual accuracy and coherence.\n\u2022 BARTScore: BARTScore (Yuan et al., 2021) is a parameter- and data-efficient metric that supports the evaluation of generated text from multiple perspectives, including informativeness and coherence.\n\u2022 UniEval: UniEval (Zhong et al., 2022) is a unified multi-dimensional evaluator that reframes natural language generation evaluation as a Boolean Question Answering (QA) task, guiding the model with different questions to evaluate from multiple dimensions. It is reference-free in three dimensions (coherence, consistency, fluency), but not relevance. For our evaluation, we focused on the reference-free dimensions of UniEval and reported the overall average performance.\n\u2022 Fairness (F): To align fairness with the quality metrics, we define F = 1 RG, where larger values represent better fairness. The Representation Gap (RG) metric (Olabisi et al., 2022) assesses the fairness of summaries by measuring the balance in the representation of different groups.\n\u2022 Composite Metrics (Metric+F): For each quality metric (e.g., SUPERT, BLANC, SummaQA, BARTScore, and UniEval), we introduce a composite metric that combines the normalized quality score with the fairness score F. These composite metrics, such as SUPERT+F, BLANC+F, SummaQA+F, BARTScore+F, and UniEval+F, are computed by taking the average of the normalized quality metric and the fairness score F. A higher value of these composite metrics reflects a better balance between the summary's quality (as measured by the respective metric) and fairness."}, {"title": "Results and Discussion", "content": "In this section, we present the results of our evaluation, comparing the performance of various summarization models on both quality and fairness metrics."}, {"title": "Results of Quality and Fairness", "content": "The models were assessed using SUPERT, BLANC, SummaQA, BARTScore, UniEval, and the fairness metric F. Table 2 presents the results.\nNaive and NaiveFair Baselines: The Naive baseline, which randomly selects sentences without any fairness consideration, performs relatively poorly across most quality metrics, particularly on SummaQA and BARTScore, where it scores significantly lower. However, it achieves a reasonable fairness score (F = 0.732), despite its lack of sophisticated fairness mechanisms. The NaiveFair model, which ensures equal representation from both groups, shows a slight improvement in fairness, achieving the maximum F value of 1. However, this fairness comes at a slight cost to quality, as it falls behind on some metrics like UniEval.\nTextRank Models: The TextRank Vanilla method shows a balanced performance in terms of quality, with the highest SummaQA score (0.081), but suffers in BLANC and BARTScore. Variations of TextRank, such as Cluster-A and Cluster-H, show slight improvements in specific metrics like SUPERT and BLANC, but they still struggle in ensuring fairness, with scores in the range of F = 0.693 to F = 0.727.\nBERT-Ext Models: The BERT-EXT models generally outperform the TextRank methods in quality metrics. BERT-EXT Vanilla achieves higher SUPERT and BARTScore scores compared to TextRank, with BERT-EXT Cluster-A further improving on these metrics, particularly in SUPERT (0.553) and BLANC (0.138). However, the fairness scores for these models remain moderate, with F values ranging from 0.680 to 0.728, indicating room for improvement in terms of group representation balance.\nChatGPT-Ext: The ChatGPT-Ext method stands out as the top performer in terms of quality, achieving the highest scores in SUPERT (0.668), BLANC (0.140), BARTScore (-0.642), and UniEval (0.434). This demonstrates its effectiveness in producing semantically rich and coherent summaries. However, its fairness score of F = 0.698 indicates that while it excels in quality, there is still room for improvement in terms of group representation.\nFairExtract and FairGPT (Ours): Our proposed models, FairExtract and FairGPT, were designed with fairness as a core objective. Both models achieve perfect fairness, with F = 1, while still maintaining competitive quality. FairExtract performs comparably to TextRank in terms of quality metrics, excelling in BLANC (0.140) and achieving respectable scores in SUPERT and UniEval. FairGPT, leveraging the power of GPT-3.5, shows a strong balance between quality and fairness, with particularly high SUPERT (0.644) and BARTScore (-0.821) scores. These results suggest that our models successfully balance the trade-off between quality and fairness, making them robust options for fairness-aware summarization tasks.\nOverall, ChatGPT-Ext achieves the highest quality metrics, while FairExtract and FairGPT lead in fairness without compromising quality; notably, FairGPT emerges as the best model, striking an optimal balance between quality and diversity, underscoring the success of our proposed methods in achieving fair and high-quality summarizations."}, {"title": "Results Aggregating Quality and Fairness", "content": "The composite evaluation metrics are presented in Table 3. These metrics aggregate both quality and fairness, both receiving equal weight (50%) in the overall score. Our results show that FairExtract,"}, {"title": "Conclusion", "content": "In this paper, we introduced two novel methods, FairExtract and FairGPT, to address the critical challenge of fairness in multi-document extractive summarization. Both methods were designed to ensure equitable representation of social groups while maintaining competitive summarization quality. Our extensive experiments demonstrated that both FairExtract and FairGPT achieve perfect fairness without significantly compromising on standard quality metrics.\nWe also introduced new composite metrics (e.g., SUPERT+F, BLANC+F) that combine quality and fairness scores, offering a more nuanced evaluation of the trade-offs between these two dimensions. The results showed that our methods strike a strong balance between quality and fairness, with FairExtract performing exceptionally well in clustering-based approaches and FairGPT setting new benchmarks among LLM-based methods.\nThese findings highlight the importance and feasibility of integrating fairness into summarization tasks, where diverse representation is crucial. Future work can build on these models by extending them to abstractive summarization, exploring additional fairness constraints, and applying them to larger, more diverse datasets. Our work serves as a significant step toward building fair and inclusive summarization systems for real-world applications."}, {"title": "Limitations", "content": "While FairExtract and FairGPT show advances in ensuring fairness in multi-document summarization, several limitations remain.\nFirst, our methods focus on extractive summarization, which, while preserving input fidelity, may not capture the semantic richness of abstractive methods (Lebanoff et al., 2019). Extending our approach to abstractive models presents additional challenges, particularly in balancing fairness with coherence and fluency.\nSecond, the dataset consists of social media content, which may limit generalization to other domains like news or scientific articles. The informal nature of social media language introduces variability that might not translate to more formal text types.\nThird, our work focuses on monolingual inputs, specifically in English. Future research could extend these methods to multilingual inputs, where additional factors such as language diversity and cross-lingual transfer, as highlighted by Bagheri Nezhad and Agrawal (2024), would need to be addressed to ensure fairness across languages.\nAdditionally, while we employ standard quality and fairness metrics, they do not fully capture subjective factors such as readability or user trust. Human evaluation could provide deeper insights into the practical implications of fairness and quality.\nFinally, the computational complexity of fair clustering and large language models may limit scalability in real-time or resource-constrained environments.\nDespite these challenges, our work marks a significant step toward fairer summarization models, and addressing these limitations could enhance the robustness of fairness in NLP."}, {"title": "Appendix / supplemental material", "content": ""}, {"title": "Fair Extract Formal Algorithmic Processes", "content": "In this section, we provide a detailed breakdown of the formal procedures used in our proposed method, FairExtract. These algorithm ensure fairness and quality in extractive summarization, addressing the core objectives of balanced representation and high-quality content extraction from diverse groups.\nThe FairExtract algorithm utilizes clustering techniques combined with fairlet decomposition to ensure that summaries reflect an equitable representation of the input groups. This process involves embedding documents using BERT, dividing the dataset into fairlets, and applying k-median clustering to construct a diversity-preserving summary.\nThe formal descriptions of the algorithm are presented in Algorithm 2."}, {"title": "Sample of Dataset", "content": "Table 4 presents a sample of the dataset used in this study, containing tweets from different social groups about Chicago. Each entry indicates the group (White, African-American (AA), or Hispanic(Hisp)) and the corresponding tweet."}, {"title": "Impact of Varying Fairness Weight on Composite Metrics", "content": "In this section, we present the results of an experiment where we varied the weight assigned to fairness in the composite metric formula. Specifically, we explored the performance of FairExtract and FairGPT under different fairness weights to assess their robustness in balancing quality and fairness. Table 5 summarizes the results for the setting where the fairness weight a is reduced to 0.16, representing a 16% incentive toward fairness and an 84% incentive toward quality."}]}