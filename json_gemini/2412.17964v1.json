{"title": "Dynamic Multi-Agent Orchestration and Retrieval for Multi-Source Question-Answer Systems using Large Language Models", "authors": ["Antony Seabra", "Claudio Cavalcante", "Joao Nepomuceno", "Lucas Lago", "Nicolaas Ruberg", "Sergio Lifschitz"], "abstract": "We propose a methodology that combines several advanced techniques in Large Language Model (LLM) retrieval to support the development of robust, multi-source question-answer systems. This methodology is designed to integrate information from diverse data sources, including unstructured documents (PDFs) and structured databases, through a coordinated multi-agent orchestration and dynamic retrieval approach. Our methodology leverages specialized agents such as SQL agents, Retrieval-Augmented Generation (RAG) agents, and router agents that dynamically select the most appropriate retrieval strategy based on the nature of each query. To further improve accuracy and contextual relevance, we employ dynamic prompt engineering, which adapts in real time to query-specific contexts. The methodology's effectiveness is demonstrated within the domain of Contract Management, where complex queries often require seamless interaction between unstructured and structured data. Our results indicate that this approach enhances response accuracy and relevance, offering a versatile and scalable framework for developing question-answer systems that can operate across various domains and data sources.", "sections": [{"title": "1 Introduction", "content": "In recent years, the rapid evolution of Large Language Models (LLMs) has led to significant advancements in the fields of information retrieval and question-answer (Q&A) systems. These advanced models have proven capable of understanding and generating human-like text, offering new possibilities for retrieving precise and contextually relevant information from diverse sources. However, despite these advancements, challenges remain when integrating data from heterogeneous sources- es\u2014such as unstructured text documents, structured databases, and real-time APIs into a single system. Traditional systems often struggle to handle the complexity of retrieving and correlating information across different formats, leading to issues with accuracy and relevance in responses. This gap underscores the need for more sophisticated techniques that can dynamically orchestrate and retrieve information from multiple sources, while maintaining the high accuracy and contextual awareness that LLMs offer.\nIn many industries, professionals are required to navigate vast volumes of text-based documents while simultaneously accessing structured data from databases or other systems. This process is not only labor-intensive but also time-consuming, as locating specific pieces of information and correlating them across different sources can be very difficult. For instance, in domains like Contract Management, retrieving relevant details from both contract documents and database records can often require manually searching through hundreds of pages and cross-referencing these with structured metadata\u2014an arduous and error-prone task."}, {"title": "2 Background", "content": "To address these challenges, we propose a dynamic multi-agent orchestration and retrieval methodology aimed at improving the accuracy of multi-source Q&A systems using Large Language Models. By combining advanced retrieval-augmented generation (RAG), text-to-SQL techniques, and dynamic prompt engineering, we enable the system to handle complex queries across heterogeneous data sources, improving response precision without the need to retrain the model. At the heart of this approach lies an agent-based architecture that dynamically orchestrates different retrieval strategies based on the nature of the user query, ensuring optimal data retrieval from multiple sources.\nIn this paper, we evaluate our approach using the domain of contracts, incorporating qualitative feedback from users who tested the system. Contract Management systems often involve retrieving specific data from contract documents (e.g., penalties, SLAs, deadlines) as well as structured data from databases. While existing systems can handle basic information retrieval tasks, they typically struggle when required to provide detailed answers that integrate information from multiple sources. Our proposed system leverages specialized agents such as SQL agents, RAG agents, and router agents to route and execute the queries to the most appropriate source, thereby offering more comprehensive and context-aware responses.\nAdditionally, we introduce dynamic prompt engineering, which adapts the prompt' instructions in real-time, based on the context of the query, the type of data being retrieved, and the user's input. This ensures that the language model's responses are accurate, contextual, and optimized for each query's specific requirements, whether it's retrieving information from a structured database or extracting text from an unstructured document.\nThe paper is organized as follows: Section 2 provides technical background on agents orchestration and retrieval techniques for LLMs, like RAG, text-to-SQL, and Prompt Engineering. Section 3 discusses our methodology and the use of the presented techniques, while Section 4 describes how we evaluated the proposed methodology and the experimentation of the Q&A application. Finally, Section 5 concludes our study and proposes directions for future research in this field.\nTo build an effective multi-source question-answer system, it is essential to leverage several advanced techniques that address the complexities of retrieving and processing information from diverse sources, and orchestrate them using agents. This section explores the foundational technologies that enable the system's core functionality, including Large Language Models (LLMs), which provide the ability to understand and generate natural language; Prompt Engineering, a method used to optimize and guide the behavior of LLMs for specific tasks; Retrieval-Augmented Generation (RAG), which integrates external data into the LLM's context for more accurate and relevant answers; Text-to-SQL, a technique that translates natural language queries into database commands to retrieve structured data; and Agents, which dynamically orchestrate and route tasks to the most appropriate modules within the system [Mialon et al., 2023]. Together, these technologies form the backbone of our proposed multi-agent orchestration and retrieval methodology, enabling seamless integration of multiple data sources and improving the overall performance of question-answer systems."}, {"title": "2.1 Large Language Models", "content": "Based on the Transformer architecture [Vaswani et al., 2017], Large Language Models (LLMs) have transformed the field of natural language processing (NLP) by enabling"}, {"title": "2.2 Retrieval-Augmented Generation (RAG)", "content": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to enhance the performance of LLMs by incorporating external data into the generation process, enabling an extension of knowledge by accessing information that is not part of the LLM's internal knowledge base. While LLMs excel at generating text based on their training, they are limited by the information they contain, which becomes outdated or incomplete over time. RAG addresses this by retrieving relevant documents or data from external sources, such as databases or document repositories, and feeding this information to the LLM as context for generating responses. This ensures that the answers provided are up-to-date and rooted in real-world.\nThe RAG framework, as described by [Gao et al., 2023b] and [Feng et al., 2024], operates by embedding both the user's query and chunks of external information into high-dimensional vector spaces. These embeddings allow the system to compare and retrieve the most semantically relevant data from a vectorstore - a database optimized for high-dimensional vectors. Once retrieved, this relevant data is used as additional input to the LLM, ensuring that the generated answer is informed by the latest and most pertinent information available.\nA key advantage of RAG is its ability to provide answers that go beyond the internal knowledge of the LLM. This is particularly useful in dynamic environments where the information changes frequently or is too specialized to be captured fully by a pre-trained model. For example, in our multi-source question-answer system, RAG can enable the retrieval of information from both contract documents and databases, ensuring the responses are relevant to the specific user's question.\nThe chunking strategy used in RAG is critical to its success, as it determines how documents are divided into smaller pieces for embedding and retrieval. By effectively segmenting large documents, RAG ensures that only the most relevant sections are retrieved and fed into the LLM, preventing information overload and improving the precision of the answer. The choice of similarity metrics, such as Cosine or Euclidean distance, plays a significant role in determining which chunks are selected for retrieval [Gao et al., 2023b]. In RAG, the chunking strategy is important because it directly influences the quality of the retrieved information. A well-designed chunk generation ensures that the information is cohesive and semantically complete, capturing its essence. Several chunking options can be applied depending on the structure and type of data. For instance, one common approach is to divide text into chunks based on a specific number of tokens, often with an overlap parameter to ensure continuity between chunks. This overlap helps maintain context, especially in lengthy documents where relevant information may span across multiple chunks. Another approach, particularly suited for uniform documents, involves chunking based on specific sections or headers within the document, such as dividing contracts by clauses or legal sections. This ensures that each chunk represents a self-contained, semantically meaningful portion of the text. The choice of chunking method plays a crucial role in determining the precision of retrieval, as it helps balance the trade-off between capturing full context and maintaining relevance in the retrieved information.\nWhile RAG is highly effective in bridging the gap between static knowledge in LLMS and real-time data, it also presents challenges, particularly when the retrieved chunks are semantically similar but not relevant to the query. This issue often arises in scenarios involving structured documents, such as contracts, where different sections may contain similar language but vastly different meanings. The difference between similarity and relevance is one of the biggest challenges faced by retrieval systems, especially when working with large language models. While similarity measures help in identifying content that closely matches the query in terms of wording or context, it does not always guarantee that the retrieved information is truly relevant to the user's intent. This challenge often requires additional filtering or refinement techniques to ensure that retrieved results are not just similar but also meaningful and aligned with the specific needs of the query."}, {"title": "2.3 Text-to-SQL", "content": "Text-to-SQL is a powerful technique that bridges the gap between natural language queries and relational database systems by converting user inputs in plain text into executable SQL commands. This allows users to retrieve precise, structured information from databases without needing to understand SQL syntax [Liu et al., 2023]. By leveraging the capabilities of LLMs, Text-to-SQL systems can parse and interpret natural language questions and map them to the appropriate database schema, significantly improving the accessibility of data for non-expert users.\nA key advantage of Text-to-SQL systems is their ability to handle complex database queries while shielding users from the intricacies of database schemas and SQL commands. For instance, as discussed by [Pinheiro et al., 2023], LLMs can be used to construct natural language database (conversational) interfaces. They do this detecting entities, mapping them to corresponding tables and columns, and generating syntactically correct SQL queries based on the database structure. This approach is particularly useful in domains where the underlying data is stored in complex databases, such as contract management systems or healthcare databases, where queries may involve multiple tables and relationships.\nAccording to [Seabra et al., 2024], the main distinction between RAG and text-to-SQL techniques lies in their approach to retrieving information. RAG focuses on retrieving text segments from a vectorstore that are semantically similar to the user's question, and it uses these segments to generate a coherent and contextually appropriate answer. This approach is well-suited for questions where the answer can be synthesized from existing unstructured text. However, it may not always provide the precise information expected if the answer cannot be directly inferred from the retrieved text segments. On the other hand, Text-to-SQL translates natural language queries into SQL commands, as demonstrated in [Pinheiro et al., 2023], which are then executed against a structured database to return exact data matches. This ensures that when the text-to-SQL translation is accurate, the user receives a highly specific, structured answer derived directly from the relevant database fields.\nTherefore, while RAG operates on the principle of textual similarity and utilizes generative capabilities to synthesize responses from retrieved text, Text-to-SQL provides a more direct and precise mechanism for data retrieval. By translating natural language queries into executable SQL commands, Text-to-SQL allows for exact matches based on the user's intent, retrieving highly specific information directly from structured databases. This makes Text-to-SQL particularly effective for data investigations where precise, query-based access to relational data is crucial, such as financial reports, contract details, or inventory systems. Unlike RAG, which depends on finding semantically similar text, Text-to-SQL guarantees an exact match from database fields, ensuring that the user receives accurate, factual answers without ambiguity. As a result, it is a valuable tool in scenarios where precision and structure are paramount, complementing the generative and flexible nature of RAG for a more comprehensive information retrieval system."}, {"title": "2.4 Prompt Engineering", "content": "Prompt Engineering guides and optimizes the behavior of LLMs using direct instructions, ensuring that the generated responses align with the user's intent. By carefully crafting the input prompt, developers can influence not only the content of the response but also its tone, format, and level of detail [OpenAI, 2023b]. This technique becomes especially important in multi-source question-answer systems, where prompts must clearly define the task at hand, instruct the model on how to handle various data types, and help the model distinguish between relevant and irrelevant information.\nA well-constructed prompt can dramatically improve the accuracy and relevance of the answers generated by LLMs. Engineers can even outline the script for a response, specifying the desired style and format for the LLM response, as stated by [White et al., 2023] and [Giray, 2023]. For instance, when querying contract details from unstructured documents or structured databases, the prompt can explicitly instruct the model to only consider the relevant sections of the contract or to retrieve specific details, such as deadlines or penalties. By embedding instructions, context, and constraints into the prompt, it becomes possible to guide the LLM toward more focused and precise outputs. This is especially useful in domains where accuracy is critical, such as legal, healthcare, or finance, where responses must adhere to specific guidelines or regulatory frameworks.\nAccording to [Wang et al., 2023], prompts provide guidance to ensure that the model generates responses that are aligned with the user's intent. For example, the prompt can be designed to include contextual information that helps the LLM understand the role of the user or the nature of the query. In a contract management system, a prompt might instruct the model to retrieve details about penalty clauses or contractual obligations, with a directive such as: \u201cExtract and summarize any penalty-related clauses from the contract document, focusing on late delivery penalties.\" Additionally, the prompt might include a role-specific context like: \"You are a contract management assistant tasked with summarizing the key contractual obligations of the supplier.\" These instructions help the model generate responses that are not only factually accurate but also aligned with the user's expectations and the context in which the information is needed.\nMoreover, prompt engineering can address ambiguity and reduce the risk of factual hallucination, a common issue where LLMs generate responses that sound plausible but are not grounded in factual data. By explicitly defining the scope of the query in the prompt and instructing the model to refer only to external data sources or documents, the accuracy of responses is improved. For example, by using instructions like \"Do not use prior knowledge\", prompt engineering can restrict the LLM's answer to specific sources, thereby reducing the risk of factual hallucinations and ensuring that responses are grounded in the desired information.\nRecent studies have begun to explore the synergistic integration of these techniques with LLMs to create more sophisticated Q&A systems. For example, [Jeong, 2023] reinforces the importance of using Prompt Engineering with RAG to improve the retrieval of relevant documents, which are then used to generate both contextually relevant and information-rich answers. Similarly, [Gao et al., 2023a] explores the integration of Text-to-SQL with Prompt Engineering to enhance the model's ability to interact directly with relational databases, thereby expanding the scope of queries that can be answered accurately."}, {"title": "2.5 Agents", "content": "In the context of question-answer systems, agents play a pivotal role in orchestrating complex workflows and dynamically routing queries to the most appropriate processing path. Unlike traditional, linear retrieval systems, agent-based architectures introduce a level of flexibility and intelligence, enabling systems to handle multi-faceted queries with varying data sources and retrieval requirements. By incorporating agents, a question-answer system can dynamically adapt to the user's query, selecting different strategies based on the type of information being requested and the nature of the data sources.\nIn an agent-based framework, specialized agents can be designed to handle specific tasks, each tailored to different aspects of information retrieval. For example, a Router Agent can serve as the system's primary decision-maker, analyzing each query upon receipt and deciding on the optimal retrieval strategy. The router agent typically uses rules, such as regular expressions or other pattern-matching methods, to interpret the query structure and identify key indicators that suggest which retrieval path should be followed. For instance, if the query pertains to a specific clause or passage within a document, the router agent can direct the query to a Retrieval-Augmented Generation (RAG) Agent, which is optimized for unstructured text data. Alternatively, if the query requires precise, structured information, such as dates, financial figures, or other exact data, the router agent might direct it to a SQL Agent, which uses Text-to-SQL translation to interact with structured databases. These agents leverage recent advancements in AI, such as RAG and Text-to-SQL, to perform more complex and contextually aware tasks [Lewis et al., 2020].\nEach specialized agent in this framework brings unique capabilities that contribute to the overall effectiveness of the system. The RAG Agent operates by retrieving relevant text chunks from a vectorstore based on semantic similarity and then integrating these chunks into the language model's context. This allows the system to handle complex, interpretive questions that benefit from a nuanced understanding of unstructured text. Meanwhile, the SQL Agent translates natural language queries into SQL commands, enabling the system to retrieve precise, structured data directly from a database. This approach is particularly useful for answering fact-based questions that require a high degree of specificity, ensuring that the response reflects up-to-date information directly from the database. As outlined in [Singh et al., 2024], agent workflows allow LLMs to operate more dynamically by incorporating specialized agents that manage task routing, execution, and optimization. These agents serve as intelligent intermediaries, directing specific tasks such as data retrieval, reasoning, or response generation to the most suitable components within the system. One of the most important ones in place are the Router Agents, as they are the decision-makers of the system. When a user poses a query, the router agent analyzes the input and decides the best path forward.\nThrough the use of such agents, the question-answer system gains the ability to make intelligent decisions about query handling and data retrieval. This agent-based orchestration allows the system to seamlessly blend information from both structured and unstructured sources, improving the relevance and accuracy of the answers provided. Moreover, agents allow for modular expansion, meaning that new agents can be added to handle specific types of data or tasks, enhancing the system's scalability and adaptability to diverse domains. According to [Jin et al., 2024], applying LLMs to text-to-database management and query optimization is also a novel research direction in natural language to code generation task.\nIn sum, agents empower question-answer systems with the capability to route queries intelligently, select the best processing pathway, and ensure that the response leverages the most suitable data sources. This orchestration enables the system to handle complex, multi-source queries more effectively, providing responses that are both contextually rich and highly relevant to the user's intent."}, {"title": "3 Our Methodology", "content": "In designing our multi-source question-answer methodology, we employ a combination of advanced techniques to access diverse data sources and provide accurate responses tailored to the query and the specific source of information, integrating Retrieval-Augmented Generation (RAG), Text-to-SQL, Dynamic Prompt Engineering, and Agent-based orchestration to effectively manage the complexities of interacting with both structured and unstructured data sources. Each component plays a critical role in handling various aspects of information retrieval, ensuring that the system can dynamically adapt to the requirements of each query.\nRAG enables the retrieval of relevant information from large volumes of unstructured text, while Text-to-SQL facilitates precise access to structured data within relational databases. Dynamic Prompt Engineering customizes the query context, ensuring that responses are tailored to user intent, and Agent-based orchestration coordinates these techniques, directing queries to the appropriate modules and managing workflows seamlessly."}, {"title": "3.1 Applying RAG", "content": "According to [Seabra et al., 2024], the first step when applying RAG involves (1) reading the textual content of the PDF documents into manageable (chunks), which are then (2) transformed into high-dimensional vectors (embedding). The text in vector format captures the semantic properties of the text, a format that can have 1536 dimensions or more. These embeddings (vectors) are stored in a vectorstore (3), a database specialized in high-dimensional vectors. The vector store allows efficient querying of vectors through their similarities, using the distance for comparison (whether Manhatan, Euclidean or cosine). Once the similarity metric is established, the query is embedded in the same vector space (4); this allows a direct comparison between the vectorized query and the vectors of the stored chunks, retrieving the most similar chunks (5), which are then transparently integrated into the LLM context to generate a prompt (6). The prompt is then composed of the question, the texts retrieved from the vectorstore, the specific instructions and, optionally, the chat history, all sent to the LLM which generates the final response (7).\nChunking strategy One of the first decisions to be made when applying RAG is to choose the best strategy to segment the document, that is, how to perform the chunking of the PDF files. A common chunking strategy involves segmenting documents based on a specific number of tokens and an overlap (overlap). This is useful when dealing with sequential texts where it is important to maintain the continuity of the context between the chunks.\nThere is a common type of document with well-defined sections; contracts are a prime example. The have a standardized textual structure, organized into contractual sections. Therefore, sections with the same numbering or in the same vicinity describe the same contractual aspect, that is, they have similar semantics. For example, in the first section of contract documents, we always find the object of the contract. In this scenario, we can assume that the best chunking strategy is to separate the chunks by section of the document. In this case, the overlap between the chunks occurs by section, since the questions will be answered by information contained in the section itself or in previous or subsequent sections. For the contract page in the example in Figure ??, we would have a chunk for the section on the object of the contract, another chunk for the section on the term of the contract, that is, a chunk for each clause of the contract and its surroundings. This approach ensures that each snippet represents a semantic unit, making retrievals more accurate and aligned with queries.\nUsing predefined sections as the boundaries for chunks enhances the relevance of responses within a single contract. However, this approach presents two main challenges: (1) within a single document, when a term appears repeatedly, it can be difficult to identify the specific chunk that answers a question; and (2) as the number of documents increases, accurately selecting the correct document to address becomes more challenging for the system. In the Contract Management domain, consider a scenario where the user asks, \"Who is the contract manager of contract number 123/2024?\u201d. This query is intended to retrieve the specific name of the contract manager for the given contract. However, the term \"contract manager\" can appear in various clauses of the contract document, often in sections that do not contain the name of the actual manager but refer to responsibilities or general rules related to contract management. For instance, multiple clauses across different sections of the contract might mention the term \"contract manager\" in contexts like assigning responsibilities, explaining the duties of a manager, or defining roles in contract supervision. Even though these clauses contain the term \"contract manager,\" they do not answer the user's question, which is specifically asking for the name of the contract manager for contract 123/2024.\nDue to the similarity between the query and these irrelevant sections, the Retrieval-Augmented Generation (RAG) system may retrieve a chunk from one of these irrelevant clauses that does not actually contain the required name. For example, instead of retrieving the clause that explicitly names the contract manager, the system might retrieve a clause that discusses the general duties of a contract manager. This happens because the chunk embedding for a clause about the role or responsibilities of the manager may be semantically similar to the query, even though it lacks the specific information requested. In this case, the chunk retrieved is related to the term \"contract manager\" but does not include the answer the user expects. As a result, the system could return an incorrect response, such as a general description of the role of a contract manager, rather than identifying the actual manager for contract 123/2024. This illustrates the challenge of relying solely on textual similarity in chunk retrieval, as it can lead to the retrieval of information that is similar to the query in wording but not relevant to the specific context of the user's question. To mitigate this, additional filtering mechanisms, such as metadata checks or contract-specific identifiers, are required to ensure that the system retrieves the most contextually appropriate information from the correct contract section.\nTo overcome this issue, several strategies can be applied. One approach is to add metadata to the chunks and, when accessing the vectorstore, use this metadata to filter the information returned. This method improves the relevance of the retrieved texts by narrowing the search to only those chunks that match specific metadata criteria. Figure ?? displays the most relevant metadata attributes for the contracts: source, contract, and clause. Here, source represents the name of the contract's PDF file, contract refers to the contract number, and clause indicates the section title. For instance, when querying, \"Who is the contract manager of contract 123/2024?\" the system first filters for chunks that belong to contract number 123/2024 and clauses related to the contract manager. Once these chunks are filtered, a similarity calculation is applied to identify the most relevant text segments, which are then sent to the LLM to generate the final response.\""}, {"title": "Embeddings models", "content": "Embedding models are a cornerstone of modern NLP tasks and plays in importante role in our methodology. These models transform words, sentences, or even entire documents into high-dimensional vectors, or embeddings, and the key advantage of embeddings is that they enable more nuanced and semantically aware operations on text data, such as similarity comparisons and clustering. By embedding both the query and the text chunks in the same vector space, the system can measure how close they are to each other in meaning, ensuring that relevant information is retrieved even when it is not an exact keyword match.\nSelecting the right embedding model depends on several factors related to the specific needs of a task, including the type of data, the complexity of the queries, and the computational resources available. Pretrained Models, such as BERT or GPT, are trained on vast amounts of general-purpose text data and are ideal for general tasks where the text spans multiple domains or where high-quality embeddings are required without the need for domain-specific customization. By contrast, custom models work better in specialized fields like legal or medical domains, as they can be beneficial to train an embedding model on a domain-specific corpus. This can help the model better capture the unique terminology and context of that field.\nWith respect to the vectors' dimensionality, embedding vectors can range in dimensionality depending on the model and the task. For instance, models like GloVe or Word2Vec often produce lower-dimensional embeddings (e.g., 300 dimensions), whereas modern transformer-based models like BERT and GPT can produce embeddings with 768 or more dimensions. Higher-dimensional embeddings typically capture more information and are better for complex tasks like Q&A systems or semantic search, but they also require more computational resources and storage. Lower-dimensional embeddings are computationally cheaper and faster but may not capture as much nuance, making them better suited for simpler tasks like keyword matching. If precision and detailed contextual understanding are important, high-dimensional embeddings are the better choice. For simpler or resource-constrained tasks, lower-dimensional embeddings may suffice.\nIn designing our multi-source Q&A methodology, we carefully evaluated various options for embedding models and vector dimensionality to optimize the system's performance. After considering several alternatives, we selected text-davinci-002, a model from OpenAI's GPT-3.5 family, along with embeddings with 1536 dimensions to strike a balance between accuracy, context understanding, and computational efficiency. One of the main advantages of text-davinci-002 is its ability to handle long sequences of text while maintaining a clear understanding of the context. This is essential when dealing with lengthy documents where information can be dispersed across various sections. The model can track the user's query context and dynamically retrieve or generate responses that are coherent and relevant to the query. With 1536 dimensions, the embeddings can better represent the complex relationships between terms in the text, especially in documents where meaning often depends on subtle distinctions in wording. This is particularly useful in distinguishing between similar but contextually different terms, such as contract manager vs. contract supervisor, ensuring that the system retrieves the most relevant chunks."}, {"title": "Vectorstore", "content": "The need to store and query high-dimensional vectors efficiently has led to the development of specialized vector databases, also known as vectorstores. These databases allow for the storage and retrieval of vector embeddings, making it possible to perform similarity searches a key operation in tasks such as Retrieval-Augmented Generation (RAG) and semantic search. Unlike traditional databases that are optimized for structured, tabular data, vector databases are designed to handle embeddings generated by models like text-davinci-002, which represent semantic relationships in high-dimensional space.\nWhen choosing the right vector database for a project, several factors come into play, including scalability, ease of use, latency, and integration with machine learning models. In our work, we evaluated three popular vector databases: Pinecone, Weaviate, and ChromaDB. Pinecone is a cloud-native vector database that excels in providing a fully managed service for high-performance similarity search. Weaviate is an open-source vector database that provides a highly flexible, schema-based approach to storing and querying vectors alongside structured metadata. ChromaDB is an open-source, lightweight vector database that focuses on simplicity and tight integration with machine learning workflows, making it ideal for embedding-based retrieval tasks in research and smaller projects. Our choice was the last one, specially because ChromaDB is easy to set up and integrate into a project without requiring extensive configuration or overhead. Given that our system is heavily Python-based, ChromaDB's Python-first design allowed us to quickly embed it into our machine learning pipelines. This streamlined our development process, enabling rapid iteration and testing, which was especially important in the early stages of system design. Also, by using ChromaDB, we can directly connect our text-davinci-002 embeddings with the vectorstore, enabling efficient similarity searches and accurate retrieval of contextually relevant information."}, {"title": "Similarity searches", "content": "Similarity search is a fundamental operation in tasks that involve comparing vector embeddings to find data points that are semantically or contextually similar. This technique is widely used in fields such as information retrieval, recommendation systems, question-answering systems, and semantic search. The core of similarity search lies in the ability to measure how \"close\" two vectors are to each other in a high-dimensional space. Several distance metrics are commonly used to quantify this similarity, each with its own strengths and weaknesses depending on the nature of the data and the task. Three of the most popular algorithms for similarity searches include Cosine similarity, Euclidean distance, and Manhattan distance. Each method has a unique approach to measuring how similar two vectors are, and the choice of algorithm can significantly impact the performance and accuracy of a similarity-based system.\nCosine similarity measures the cosine of the angle between two vectors in a multi-dimensional space. It evaluates how \"aligned\" the two vectors are rather than how far apart they are. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are perfectly aligned (very similar), 0 means that the vectors are orthogonal (completely dissimilar), and -1 indicates that the vectors point in opposite directions. Cosine similarity is often used in text-based applications, where the magnitude of the vector is not as important as the direction. Euclidean distance is the most common metric for measuring the straight-line distance between two points (or vectors) in a multi-dimensional space. It calculates the \"as-the-crow-flies\" distance between two vectors, treating each dimension as an axis in a Cartesian plane. Euclidean distance is widely used in geometric tasks or where the actual distance between points matters. Manhattan distance, also known as L1 distance or taxicab distance, measures the sum of the absolute differences between the corresponding coordinates of two vectors. Instead of measuring the direct straight-line distance (as in Euclidean), Manhattan distance measures how far one would have to travel along the axes of the space.\nIn our work, we chose cosine similarity for its ability to prioritize semantic alignment between query embeddings and document embeddings. Its strength in handling high-dimensional data, minimizing the influence of vector magnitude, and focusing on the directionality of vectors makes it the ideal choice for our Q&A system methodology. Cosine similarity is widely recognized as one of the best similarity measures for text-based applications, especially when using vector embeddings generated from NLP models like text-davinci-002. Since our system heavily relies on textual data, cosine similarity was the natural choice for ensuring that user queries are matched with the most relevant sections of the text, even if the exact phrasing differs. Whether we are retrieving specific sections in documents or providing general answers based on lenghty documents, cosine similarity ensures that the system is aligned with the semantic intent of the query."}, {"title": "3.2 Using structured data", "content": "In order to improve our question-answer system methodology, we explored two distinct approaches to integrate data from structured databases effectively. The first approach involved extracting data directly from the database, transforming it into text, and embedding this text into vector representations stored in the same vectorstore as our document-based embeddings. This method allowed us to convert structured data into a more flexible, text-based format, enabling semantic similarity searches alongside the unstructured text from contract documents. By embedding database information in this way, we created a unified search space where both structured and unstructured data could be queried with the same similarity-based techniques. This approach offered the advantage of simplicity, as it enabled direct integration of database information into our existing RAG framework, ensuring that queries could retrieve relevant data without needing to connect to the database during runtime.\nThe second approach we implemented involved a Text-to-SQL method, where natural language questions are dynamically translated into SQL queries. In this setup, the system interprets the user's query, converts it into a structured SQL command, and then submits it to the database for execution. The Text-to-SQL approach allows for precise data retrieval by directly querying the database, which is particularly beneficial for questions requiring exact, up-to-date values, such as specific dates, contract numbers, or quantitative information. Unlike the first approach, this method does not rely on pre-embedded representations; instead, it provides real-time access to structured data, ensuring that answers are accurate and reflect the current database state.\nEach approach has its advantages. Embedding database data alongside unstructured text provides a unified search experience and reduces dependence on real-time database access. In contrast, the Text-to-SQL approach supports direct and precise querying, making it ideal for cases where exact values are necessary. Together, these approaches allow the system to leverage the strengths of both pre-embedded and dynamic querying, enhancing its versatility in handling a wide range of user queries."}, {"title": "3.3 Agents", "content": "Agents are central to the functionality and adaptability of our multi-source question-answer system, enabling it to handle diverse query types efficiently. By leveraging specialized agents, the system dynamically routes each query to the most suitable processing pathway, ensuring that user questions are handled with precision and contextual relevance. In our architecture, the Router Agent serves as the primary decision-maker, evaluating each incoming query and directing it to the appropriate agent based on predefined criteria.\nThe Router Agent uses regular expressions to identify keywords, patterns, or structures within the query. If the query is specific to a clause within a contract, the Router Agent recognizes this pattern and assigns the query to the RAG Agent. The RAG Agent is optimized for handling unstructured text data, retrieving relevant text chunks from the vectorstore. By focusing on textual similarity, the RAG Agent retrieves semantically aligned information and generates responses that incorporate precise, contextually relevant excerpts from the documents, addressing the specifics of the the user's question.\nConversely, if the Router Agent detects that the question involves broader contract information, such as dates, financial details, or other exact values, it directs the query to the SQL Agent. The SQL Agent translates the natural language question into a structured SQL query, which is then executed against the database to retrieve exact data. This approach is particularly effective for queries requiring precise, structured responses, ensuring that the system provides accurate and up-to-date information directly from the database.\nThis dynamic agent-based architecture enables our system to handle both unstructured and structured data seamlessly. The Router Agent's decision-making process allows the system to optimize query processing based on the context and specific needs of each query. By directing contract-specific questions to the RAG Agent and structured data queries to"}]}