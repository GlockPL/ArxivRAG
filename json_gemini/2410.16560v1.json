{"title": "Raising the Stakes: Performance Pressure Improves AI-Assisted Decision Making", "authors": ["NIKITA HADUONG", "NOAH A. SMITH"], "abstract": "Al systems are used in many domains to assist with decision making, and although the potential for Al systems to assist with decision making is much discussed, human-AI collaboration often underperforms. Investigation into why the performance potential is not realized has revealed many factors, including (mis)trust in the Al system and mental models of AI capabilities on subjective tasks. Performance pressure is known to influence human decision making behavior, yet how it interacts with human-Al decision making is understudied. In this work, we show the effects of performance pressure on AI advice reliance when laypeople (Amazon Mechanical Turk crowdworkers) complete a common Al-assisted task (fake review detection) and thus have inherently low performance pressure. We manipulate performance pressure by leveraging people's loss aversion towards potential monetary gains when completing a task. We find that when the stakes are high, people use AI advice more appropriately than when stakes are lower, regardless of the presence of an Al explanation. Furthermore, when the Al system gives incorrect advice, people correctly discount the poor advice more often when the stakes are higher than when they are lower. We conclude by discussing the implications of how performance pressure influences Al-assisted decision making and encourage future research to incorporate performance pressure analysis.", "sections": [{"title": "1 Introduction", "content": "Al decision aids are available across many industries (e.g., medical diagnoses [13, 15], financial management [52], and criminal recidivism risk [31]), and when used in a complementary way with humans, have the potential to outperform either the human or Al working alone. The potential is not necessarily realized, however, because of several challenges: debates on ethical resposibility of decisions [8, 26, 44], the human ability to recognize when AI advice should be taken [43], mental models (biases) regarding Al performance and ability [12, 27] to perform well on subjective tasks, and effects of how the AI advice is delivered [46]. Many research directions thus aim to resolve these barriers to complementarity in human-AI performance, including examining the effects of having AI systems explain predictions [4] using explainable AI (XAI) methods, introducing cognitive forcing functions when presenting AI advice [6], adjusting AI advice interactions/presentation methods [40], and adjusting task framing to account for mental models about the types of tasks AI can work with [9].\nIn AI-assisted decision making, the human makes the final decision, bearing full responsibility for its consequences. Performance pressure from responsibility can influence decision making behavior [2]. The bulk of research working towards complementary human-Al performance isolates human behavior away from the effects of performance pressure because the field is rapidly evolving its understanding of how humans perceive and work with AI tools. Intrinsically high and low stakes tasks are used in these experiments, but the stakes have little tangible effect or implication for evaluators. Hence, we observe a gap in the literature of how people rely on AI assistants under performance pressure, or when stakes matter personally.\nIn this work, we seek to understand how performance pressure affects Al advice usage when Al advice is provided as a second opinion. We induce performance pressure through a pay-by-performance scheme framed as a loss. Participants are"}, {"title": "2 Background", "content": "The high sensitivity of advice taking makes it challenging to design systems that can assist with decision making. People are influenced by many factors when taking advice, such as their personal expertise [42], the advisor's reputation [51], or the style of advice delivery (e.g., inviting or broadcasting) [11, 33], resulting in inconsistent advice taking behavior that can be challenging to predict. For example, even if advice is objectively high quality (e.g., advice based on fact), it may still be discounted, i.e., not incorporated in the final decision [50]. AI advisors are expected to complement human decision making and result in higher collaborative performance, compared to individual performance, but recent studies have observed that over- and under-reliance on AI advice result in suboptimal collaboration [7, 24].\nToward appropriate Al advice use. Algorithmic aversion has been shown to be task-dependent, in line with ideas on biases about how well AI can perform on subjective tasks. When the task is subjective, e.g., predicting speed dating results, Castelo et al. [9] found increased algorithmic aversion, as opposed to an objective task, e.g., predicting financial outcomes. Hypothesizing that people discount AI advice because they do not trust the AI, researchers have used explainable AI (XAI) methods and shown the explanations alongside the AI. Many studies have observed XAI improving appropriate AI reliance (e.g., Ben David et al. [5], Lee et al. [29], Panigutti et al. [38]). In contrast, Flei\u00df et al. [17] observed the opposite: when decisions were about quantifiable skills, rather than soft skills, adding explanations did not significantly increase AI advice reliance. Jiang et al. [25] similarly observed XAI failing when the user is too uncertain. Another set of methods to mitigate poor Al reliance is through cognitive forcing functions-interventions that cause a decision maker to engage in analytical thinking [28]. Rastogi et al. [40] successfully employ a cognitive forcing function to reduce anchoring bias-a bias where people weight earlier information higher-by adding a time delay before showing AI advice.\nDecision making under pressure. An important environmental factor to consider is the influence of stressors on the human decision maker. Decision making often occurs under different stressors such as time stress or decision responsibility. Different stressors can influence decision making in different ways [22], and when multiple stressors are present, their compound effect can present itself in additive, synergistic, and antagonistic ways [20]. The influence of stress on Al-assisted decision making is an understudied factor, although recent works have begun studying the effect"}, {"title": "3 Experiment 1: How does performance pressure affect Al advice taking?", "content": "To understand how performance pressure manipulation influences AI advice-taking, we recruit Amazon Mechanical Turk crowdworkers (Turkers) and task them with judging whether a hotel review is genuine or fake (i.e., written with the intent to deceive; not to be confused with the orthogonal question of whether the review is human- or AI-generated). They are provided AI advice in the form of a prediction for the review classification. Our performance pressure manipulation involves manipulating the payment condition for participants.\nOur hypotheses are as follows:\nH1: We can influence AI advice reliance by applying performance pressure through a monetary bonus framed as a loss. Increased pressure will result in more appropriate AI advice reliance.\nH2. We can predict the influence of the pressure manipulation through measuring the risk aversion level of participants. Participants with higher risk aversion will be more careful in their decision making.\nH3. Participants will spend more time considering the AI advice under higher pressure, regardless of whether they change their decision, because they want to be more careful about their response."}, {"title": "3.1 Dataset", "content": "We use a binary classification task where users judge whether a review is genuine or written with the intent to deceive. We draw data from the Deceptive Opinion Spam Corpus [36, 37], which contains a balanced set of deceptive and genuine hotel reviews in both positive and negative polarities. The genuine hotel reviews were taken from travel websites"}, {"title": "3.2 Model", "content": "We train an SVM classifier with TF-IDF features. Our model achieves 86% accuracy on the test set using 5-fold cross-fold validation, in line with the SVM used by Schemmer et al. [43]. We do not disclose the model performance in our study to avoid user bias about objective performance metrics of the advice."}, {"title": "3.3 User Interaction", "content": "To measure the influence of Al advice under different stakes conditions, we require a sequential decision making setup. For this reason, we use the judge-advisor system (JAS) [45]. Under JAS, a user will first make a judgment alone, then receive advice, and finally make a second judgment (either confirming or adjusting their initial judgment). The sequential nature allows us to measure influence by comparing the final judgment with initial pre-advice judgments. Our interface design is heavily inspired by [43] in order to compare our baseline conditions with previous work."}, {"title": "3.4 Experiment Setup", "content": "A total of 161 participants were recruited on Amazon Mechanical Turk. The Turk recruitment conditions were 95% HIT acceptance rate, and limited to U.S. workers. We included an attention check question after 8 reviews were judged, not including the two practice reviews. Only data from participants who passed the attention check were considered. After subjects accepted the HIT, they were directed to a consent form, then received instructions for the task. They completed two practice tasks without receiving AI advice to ensure they understood the JAS setup. During each task, reviewers decided whether a review was fake or genuine and rated their decision confidence on a scale of 0-100 using a range slider, received Al advice, then confirmed their decision and confidence level. Participants did not receive feedback on the correctness of their judgments, to prevent the influence of trust in the Al system. After two practice tasks, which are excluded from analysis, subjects judged 16 reviews, then completed a postsurvey. The postsurvey contained demographics questions and the Holt-Laury Risk Assessment (HL) [23] (Appendix A.3), which is a survey to determine the risk aversion preferences of users. The HL survey is a list of gambles where participants choose between \"safe\" and \"risky\" choices. Users are incentivized to answer truthfully on the survey because they can then earn a bonus of up to 3.85USD. The study was approved by our institution's IRB, and participants were guaranteed a wage of 20USD/hr. Overall the study took about 30 minutes per participant. The participants received 5USD base pay for completing the task and were aware of the bonus from the HL survey. To ensure the wage rate, we gave all participants a flat 5USD bonus, for participants to receive a total pay of 10USD."}, {"title": "Conditions", "content": "We use a between-subjects design varyiing payment conditions, and a within-subjects design varying the amount of pressure. In the baseline condition, full-payment (FP), participants are informed that they will receive a fixed payment (5USD + survey bonus) for the task. In the performance pressure condition, pay-by-performance (PbP), participants are informed they can earn bonuses, in addition to the 5USD base pay, based on their performance. We also use a within-subjects design to study effects of different amounts of pressure. The tasks were evenly split into two groups, \"high\" and \"low\" stakes groups, and the tasks within each group were randomized. The group order was also randomized. In pay-by-performance, the groups were labeled with different losses: in the \u201chigh\u201d stakes group, every incorrect answer incurs a loss of 0.8USD from a potential bonus of 2USD; in the \"low stakes\" condition, every incorrect answer incurs a loss of 0.1USD from a potential bonus of 1USD. In full-payment, the items in the groups are all treated the same as there are no \"stakes\" for any items."}, {"title": "Dependent Variables", "content": "The following measures are our dependent variables:\n\u2022 Relative positive AI reliance (RAIR; Appendix A Equation 1): the number of cases where the human correctly changes their mind to follow the AI advice, divided by the number of cases where the human could have done so [43]\n\u2022 Relative positive self-reliance (RSR; Appendix A Equation 2): the number of cases where the human correctly maintains their judgment, disregarding the incorrect AI advice, divided by the number of cases where they could have done so\n\u2022 Time spent considering the AI advice\n\u2022 Total accuracy of the final answers given by the human"}, {"title": "Analysis", "content": "We conduct significance testing between conditions on dependent variables using the Mann-Whitney U statistic and rank-biserial correlation (rrb) to determine effect size."}, {"title": "3.5 Results", "content": "After filtering out poor data (e.g., failed attention checks, using Al in the postsurvey, or multiple task durations > 4 standard deviations from the mean), we collected responses from 85 Turkers: 40 in FP, and 45 in PbP conditions. Participants' demographics reported were: 53% female, 46% male, and 1% declined to say, and age 18-24 (3%), 25-31 (49%), 32-38 (21%), and 39+ (27%).\nThe risk aversion score is based on the quantity of safe choices made in the HL survey. 59% of participants had high risk aversion (>= 7), 33% had moderate risk aversion (4-7), and 8% had low risk aversion (0-4).\nIn the full-payment (FP) condition, human accuracy was 48% overall, and 11.6% of decisions changed after seeing AI advice. In the pay-by-performance (PbP) condition, human accuracy was 49% overall (48% for high stakes task set and 49% for low stakes task set), and 16.3% of decisions changed after seeing AI advice. We observe no significant difference in performance between FP and PbP.\nIn the following discussion, we discuss significant results of analyzed variables in the context of our hypotheses.\nPerformance pressure via monetary loss aversion leads to overall more appropriate reliance on AI advice (H1). Figure 2 summarizes the difference in relative positive AI reliance (RAIR, left) and relative positive self-reliance (RSR), right across conditions. For both high and low stakes pay-by-performance, RAIR improves over the baseline full-payment condition; performance pressure increases the rate at which humans correctly change their minds given good AI advice. The high stakes pay-by-performance condition shows significantly higher RAIR than the full-payment condition (p = 0.008, rrb= 0.023). The low stakes pay-by-performance improvement in RAIR over the full-payment condition was not significant (p = 0.530). The results are more nuanced when it comes to the rate at which humans correctly \"stand their ground\u201d and resist taking bad AI advice. For low-stakes items in the pay-by-performance condition, RSR is significantly worse than the baseline full-payment condition (p = 0.013, rrb= -.021), but high-stakes items do not show a significant drop in RSR (p = 0.995). Note that in the low stakes case, participants would still receive a bonus even if"}, {"title": "4 Experiment 2: How does XAI hold up under pressure?", "content": "We explore how the inclusion of AI explanations affects AI reliance under pressure. In this experiment, subjects received an Al explanation simultaneously alongside the AI prediction. Our two treatment groups are full-payment XA\u0399 (FP-\u03a7\u0391\u0399) and pay-by-performance XAI (PbP-XAI). We use LIME feature importance explanations [41], which is a model-agnostic XAI model that aims to explain the importance of independent features-words, in our setting. A common explainable text visualization technique is to highlight words that indicate their importance to a model decision. Figure 3 shows an example of the PbP-XAI condition in the \"high stakes\" task set. Figure 3b shows an example of how text is highlighted to be associated with either a genuine or a fake prediction using the LIME method. We hypothesize:\nH4. The positive effects from XAI will hold under pressure, potentially further increasing appropriate reliance of AI advice.\nThe experiment setup is otherwise identical to that of Experiment 1."}, {"title": "4.1 Results", "content": "We collected data from 76 Turkers: 34 in FP-XAI and 42 in PbP-XAI. Participants' demographics reported were: 41% female and 58% male, and age 18-24 (12%), 25-31 (42%), 32-38 (29%), and 39+ (17%). Due to rejecting the hypothesis that risk aversion score can predict AI reliance, we drop this variable in Experiment 2.\nIn the full-payment (FP) condition, human accuracy was 49% overall, and 12.6% of decisions changed after seeing AI advice. In the pay-by-performance (PbP) condition, human accuracy was 49% overall (48% for high stakes task"}, {"title": "4.2 Does XAI complement performance pressure?", "content": "XAI had a mixed effect on RAIR and RSR. We still observed improved RAIR in the pay-by-performance condition over the full-payment baseline, but the significant difference diminished. In contrast, XAI had a detrimental effect on RSR. We build multiple linear regression models with pay-by-performance condition and XAI presence as the independent variables to investigate whether there is a predictable effect from XAI. Our models do not find XAI to be a significant predictor for RAIR and RSR. These mixed results open up more questions that are outside of the scope of this study. For instance, how do different XAI methods compare under performance pressure? Since our task has no time constraint, perhaps providing many different XAI methods simultaneously to give subjects a more comprehensive overview of possible reasoning could be more effective. Subjects with more concern about performance pressure may desire as much information as possible to make the most informed decision. Perhaps the quantity of XAI methods should scale with the performance pressure in terms of informativeness and diversity.\nOur regression results point towards the challenge in predicting how users will perform under performance pressure. We are able to capture significant AI advice behavior changes through Mann-Whitney U-tests, yet our predictive models are unsuccessful. The ability to predict how behavior changes under pressure will be important for informing the design of adaptive environments and XAI presence for AI assisted decision. We discuss further implications and potential directions for future work to build predictive models in Section 5."}, {"title": "4.3 How did participants judge reviews?", "content": "Our postsurvey included questions asking how participants used AI predictions and how they judged reviews.\nQ. Describe why you did or did not use the AI advice. Several users admitted to guessing because they could not determine any relevant features indicating review quality (fake or genuine). Five users stated they find AI trustworthy, especially because \"in todays world Ai is very smart and helpful\". Note that the AI model accuracy (86%) was not disclosed, and random chance performance on our task is 50%. This illustrates how loaded the term \"AI\" is, as it can prime users with high association bias. Two users did not trust the AI, particularly when the XAI condition highlighted"}, {"title": "5 Discussion", "content": "This work investigated how changing the payment method for crowdworkers influenced their Al advice taking behavior in an inherently low stakes task. A large body of literature has focused on improving Al reliance isolated from environmental factors such as pressure, but AI assisted decision making often occurs under pressure. Fortunately, simulating pressure effects in a lab environment can be a simple matter, and we demonstrate one such form of pressure, performance pressure through payment method, in this work."}, {"title": "5.1 Al advice reliance under pressure", "content": "There are many types of environmental pressures that an AI-assisted decision maker can be under, and interaction effects can vary widely. When looking at performance pressure, our findings show a subtle effect of how Al advice reliance changes under different levels of pressure. Higher performance pressure reduced overreliance on AI both with and without XAI. Unfortunately self-reliance suffered significantly when XAI was present. RSR in the pay-by-performance condition for both high and low stakes was lower than in the full-payment baseline. We suspect the XAI was too persuasive, overpowering the influential benefits of performance pressure acting as a cognitive forcing function. The task was challenging: human performance was roughly random chance, so there could be an authority bias at play, where participants believe AI to be an authority figure. The XAI explanation method also may have been confusing such that it created an illusion of expertise. It associated words with a prediction that may not have made sense (e.g., the linking verb \u201cis\u201d is associated with being genuine), but users reported associating emotional content with their prediction in the postsurvey."}, {"title": "5.2 Human Performance under Pressure", "content": "Human behavior changes under pressure, and we observe a positive influence on AI reliance with minimal cost in time, i.e., the pay-by-performance condition did not have a significant effect on task duration, yet AI reliance improved."}, {"title": "5.3 Adaptive Environments for Al Assisted Decision making", "content": "Our work has illustrated the challenges in predicting how environmental stressors influence AI assisted decision making. The mixed effectiveness of incorporating XAI under pressures reinforces the findings of where XAI is only effective for some people and some environments. This calls for building adaptive environments that can be personalized for the user and task to provide appropriate XAI methods, cognitive forcing functions, and other AI advice interventions. However, in order to develop these environments, we need the ability to predict ideal combinations of people, users, and intervention methods. Our limited success building predictive models demonstrates how the task will be challenging, but not impossible."}, {"title": "6 Limitations", "content": "Our experiments in simulating performance pressure through loss aversion used an inherently low-stakes task with crowdworkers. Varying the task to one with inherently high stakes, and comparing pressure influence between crowdworkers and experts, would be important to investigate. Varying the expertise of the subjects should also be done in conjunction with varying the XAI methods, because expertise can influence information seeking behavior [10]. We recruited participants from Amazon Mechanical Turk, one of many popular crowdsourcing platforms in the research community for recruiting laypeople. We rejected about 75% of submissions due to failing attention checks, being distracted, and using Al in open response questions."}, {"title": "7 Conclusion", "content": "AI assistance is already used in the real world for low and high risk tasks, and designing adaptive AI assistants that are domain-specific requires understanding the different factors influencing how humans use Al advice. In this paper, we investigated how performance pressure framed as financial responsibility influences the use of AI advice. We used a low-stakes task setting, fake review detection, and laypeople with little intrinsic motivation or expertise requirements, and found that our modest financial framing can effectively influence AI advice usage. When stakes were higher, users more successfully relied on AI appropriately, despite not spending significantly more time on the task. Our results contribute to the body of literature investigating how pressure influences AI assisted decision making. Our work aimed to create a more realistic setting to investigate AI advice reliance, and we found nuanced interaction effects between established Al advice reliance techniques and performance pressure. We believe studying AI advice reliance under stress factors will be an important research direction for the community and encourage future research to incorporate pressure analysis in their work to interpret results in a more realistic context."}, {"title": "A.1 RAIR and RSR", "content": "Calculating RAIR and RSR require four values:\n(1) positive_ai_reliance: when the initial judgment disagrees with the ground truth, the AI advice is correct, and the final judgment agrees with the AI advice\n(2) negative_ai_reliance: when the initial judgment agrees with the ground truth, the AI advice is incorrect, and the final judgment agrees with the AI advice\n(3) positive_self_reliance: when initial judgment agrees with ground truth, the AI advice is incorrect, and the initial judgment disagrees with Al advice\n(4) negative_self_reliance: when initial judgment disagrees with the ground truth, AI advice is correct, and final judgment disagrees with Al advice\n$RAIR = \\frac{positive\\_ai\\_reliance}{positive\\_ai\\_reliance + negative\\_self\\_reliance}$ (1)\n$RSR = \\frac{postive\\_self\\_reliance}{negative\\_ai\\_reliance + positive\\_self\\_reliance}$ (2)"}, {"title": "A.2 Confidence change", "content": "We asked subjects to rate their confidence in their judgement for every decision. We expected confidence to change in the final judgment after considering the AI judgment. However, few subjects adjusted their confidence level, even if they changed their decision. We suspect this is due to the effort required to self-evaluate confidence level. As a result of little confidence change, we drop this measure in our analyses."}, {"title": "A.3 Holt-Laury Survey", "content": "The Holt-Laury survey measures how risk averse a respondent is by asking them to make a decision between pairs of gambles. The total number of safe choices then buckets users into their risk aversion score."}, {"title": "A.4 Postsurvey", "content": "All postsurvey questions.\n(1) Indicate your age range (multiple choice)\n(2) Indicate your gender (multiple choice)\n(3) What is your highest level of education (or equivalent) completed? (multiple choice)\n(4) What is the area of study of your highest level of education completed?\n(5) If you are currently in an education program, what is the level of education? (multiple choice)\n(6) What is the area of study of your current education program?\n(7) Race (Select all that apply)\n(8) What's your native language?\n(9) Describe any proficiency in non-native languages\n(10) How familiar are you with the task? (Likert scale 1-5)"}, {"title": "B.1 Experiment 1", "content": ""}, {"title": "B.2 Experiment 2", "content": ""}, {"title": "B.3 Comparison between Experiment 2 and Experiment 1 (XAI vs no-XAI)", "content": ""}]}