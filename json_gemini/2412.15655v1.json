{"title": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical Speech-to-Formula", "authors": ["Sieun Hyeon", "Kyudan Jung", "Jaehee Won", "Nam-Joon Kim", "Hyun Gon Ryu", "Hyuk-Jae Lee", "Jaeyoung Do"], "abstract": "In various academic and professional settings, such as mathematics lectures or research presentations, it is often necessary to convey mathematical expressions orally. However, reading mathematical expressions aloud without accompanying visuals can significantly hinder comprehension, especially for those who are hearing-impaired or rely on subtitles due to language barriers. For instance, when a presenter reads Euler's Formula, current Automatic Speech Recognition (ASR) models often produce a verbose and error-prone textual description (e.g., e to the power of i x equals cosine of x plus i side of x), instead of the concise ATEX format (i.e., $e^{ix} = cos(x) + i sin(x)$), which hampers clear understanding and communication. To address this issue, we introduce MathSpeech, a novel pipeline that integrates ASR models with small Language Models (sLMs) to correct errors in mathematical expressions and accurately convert spoken expressions into structured ATEX representations. Evaluated on a new dataset derived from lecture recordings, MathSpeech demonstrates ATEX generation capabilities comparable to leading commercial Large Language Models (LLMs), while leveraging fine-tuned small language models of only 120M parameters. Specifically, in terms of CER, BLEU, and ROUGE scores for ATEX translation, MathSpeech demonstrated significantly superior capabilities compared to GPT-40. We observed a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores compared to GPT-40.", "sections": [{"title": "Introduction", "content": "Taking lectures that cover mathematical content through videos and watching academic presentations via video recordings or online streaming is no longer something out of the ordinary. Online video platforms have revolutionized mathematical education by making high-quality lectures and resources accessible to a global audience, breaking down geographical and financial barriers. However, these platforms face a critical challenge: accurately generating subtitles for mathematics lectures. Although platforms like YouTube offer automatic subtitle services, their performance deteriorates markedly when handling mathematical content, particularly equations and formulas. As a result, some providers are compelled to manually create subtitles, but this requires a tremendous amount of effort and human labeling resources.\nThe crux of this issue lies in the limitations of current Automated Speech Recognition (ASR) models when confronted with mathematical expressions. Despite advancements in ASR technology, these models significantly underperform in recognizing and transcribing mathematical speech. This shortcoming became apparent during our initial investigations, highlighting the absence of a benchmark dataset specifically for evaluating ASR models' proficiency in mathematical contexts.\nTo address this gap, we first developed a novel benchmark dataset comprising 1,101 audio samples from real mathematics lectures available on YouTube. This dataset serves as a crucial tool for assessing the capabilities of various ASR models in mathematical speech recognition. Our evaluations using this dataset revealed not only poor performance in equation transcription but also a critical lack of LATEX generation ability, which is the standard for typesetting mathematical equations, in existing ASR models. This limitation significantly impedes learners' comprehension, especially when dealing with complex mathematical expressions.\nTo address these challenges, we introduce MathSpeech, an innovative ASR pipeline specifically designed to transcribe mathematical speech directly into ATEX code instead of plain text. This approach enhances the learning experience by enabling the accurate rendering of mathematical expressions in subtitles, thereby supporting learners in their math education. Rather than incurring the significant costs associated with fine-tuning ASR models on domain-specific mathematical speech data-especially given the lack of publicly available datasets-we developed a novel method that converts mathematical speech into ATEX using small Language Models (LMs). Our methodology corrects ASR outputs containing mathematical expressions (even if they contain errors) and transforms the corrected output into ATEX using fine-tuned small LMs. By employing effective fine-tuning techniques that account for ASR errors and the nuances of spoken English in mathematical contexts, our pipeline has demonstrated superior ATEX generation capabilities compared to commercial large language models like GPT-40 and Gemini-Pro, despite using a relatively small language model with only 120M parameters.\nIn summary, our contributions are as follows:\n\u2022 We constructed and released the first benchmark dataset for evaluating ASR models' ability to transcribe mathematical equations.\n\u2022 We identified and demonstrated the poor performance of existing ASR models in reading mathematical equations.\n\u2022 We proposed a pipeline that corrects ASR errors and converts the output into $LATEX$\n\u2022 We confirmed that our pipeline, despite being significantly smaller (120M parameters) than commercial LLMs, outperformed GPT-40 and Gemini-Pro."}, {"title": "Related Works", "content": "As ASR systems have advanced, the need to correct ASR errors has become increasingly important. To enhance the quality of ASR output, there has been a significant amount of prior research focused on post-processing using language models. Since ASR outputs are in text form, many studies have employed sequence-to-sequence techniques.\nIn the past, statistical machine translation was used for this purpose. With the development of neural network-based language models, autoregressive sequence-to-sequence models are used for error correction, like neural machine translation. Moreover, with the advancement of attention mechanisms, research utilizing the Transformer architecture for error correction has demonstrated strong performance. Additionally, research on ASR error correction has been conducted using various language models, such as BERT, BART, ELECTRA, and T5.\nMoreover, with the emergence of various Large Language Models (LLMs), which have shown remarkable performance across diverse domains, research on post-processing for ASR correction has also been actively conducted. Several studies have shown that LLMs can be effectively used for ASR correction. However, LLMs have certain drawbacks, such as their large size and slow inference speed.\nWhen using LLMs for ASR error correction, some research has demonstrated that utilizing multiple candidates generated during beam search can result in a voting effect, leading to improved performance. This method, known as N-best, allows the N candidates obtained from the ASR output to provide clues to the language model regarding potential errors. Many studies on ASR correction have adopted this N-best approach."}, {"title": "LATEX translation and generation", "content": "Research related to LATEX has mainly focused on converting formula images to LATEX using Optical Character Recognition(OCR). Recently, studies have also been conducted on generating ATEX from spoken English that describes formulas. However, this research has not explored correcting ASR results obtained from actual speech before translating them into LATEX. The focus of this study has been on translating clean, error-free spoken English into LATEX, without considering ASR errors\nAdditionally, research has been conducted on using Large Language Models (LLMs) to enhance efficiency and quality in popular academic writing tools like Overleaf, which includes generating ATEX with LLMs. In fact, well-known state-of-the-art commercial LLMs such as GPT series, Gemini-Pro have demonstrated remarkable abilities in generating ATEX. However, the previous study indicates that comparing the LATEX generation and translation capabilities of different LLMs presents a significant challenge, as there is no suitable metric to measure LATEX generation performance."}, {"title": "Motivation", "content": "Subtitle services are often used when individuals watch academic videos or lectures. For the general public, subtitles serve as an auxiliary tool to help them understand video content. However, for individuals with hearing impairments or students who speak a different language than the lecturer, subtitles are essential. Inaccurate subtitle services can severely hinder content comprehension, leading to a significant decrease in learning effectiveness. With recent advancements in Automatic Speech Recognition (ASR) models, the ability to convert speech into text has become highly accurate, greatly benefiting these users. However, the accuracy of ASR models remains significantly lower for academic videos in fields such as mathematics and physics than for other subjects."}, {"title": "Methodology", "content": "Our goal is to implement a lightweight, fast, and high-performance pipeline that outputs IATEX. We determined that the most effective approach is fine-tuning small LMs, which involves collecting appropriate datasets. To achieve this, we collected two types of data.\n(1) (Spoken English, ATEX) pairs\nAs mentioned earlier, ASR models convert spoken mathematical expressions into plain English text. Since our goal is to translate such Spoken English(SE) into ATEX, we decided to use a dataset of (Spoken English, LATEX) pairs. We were able to obtain a publicly available dataset on HuggingFace and used it in our work. This data was collected by web crawling and OCR, extracting only the LATEX portions from arxiv papers and textbooks. This is a large dataset containing 23 million (SE, LATEX) data pairs. We fine-tuned the T5-small with this dataset to convert Spoken English(SE) to ATEX.\n(2) ASR Error Correction Dataset\nIn our initial experiments, we attempted to fine-tune T5 only using the (Spoken English(SE), LATEX) pair dataset and connect it to the ASR as a post-LM. However, the performance was not satisfactory because the ASR model itself made significant errors when converting spoken mathematical expressions into plain text."}, {"title": "Models", "content": "Our MathSpeech pipeline can be seen in Figure 3. We used two models configured in two stages.\n(1) Error Corrector\nThe purpose of the Error Corrector is to fix the errors that occur in the ASR model. In other words, the goal of the Error Corrector is to make the ASR error results similar to the input data of the LATEX Translator, which is called Spoken English. In previous studies, T5 has demonstrated good performance in ASR error correction. To minimize the model size, we used T5-small for the Error Corrector.\nAdditionally, recent research has shown that inputting multiple candidates generated by ASR beam search can yield good performance in various ASR Corrector models due to the voting effect. We have further developed this idea. Instead of using multiple candidates generated by ASR beam search, we use multiple candidates from the top-1 results of different models An advantage of this method is its versatility. We believe that training the Error Corrector with error information from various models will make it universally applicable to different ASR models. For example, if the corrector is trained with two ASR beam search results from Whisper-small, it may perform well on Whisper-small, but not on Whisper-base. However, if the corrector is trained with the ASR results from both Whisper-small and Whisper-base, we believe that it will effectively correct the errors for both models. Experimental results are presented in the following section.\n(2) LATEX Translator\nAccording to a previous study, the T5 model achieved the best performance for the LATEX translator. To minimize the model size, we used the smallest version, T5-small. We fine-tuned T5-small using the (SE, LATEX) pair dataset so that when Spoken English is input, LATEX is output."}, {"title": "Training", "content": "We implemented a pipeline connecting two T5-small models. Instead of simply chaining the fine-tuned Error Corrector and ATEX Translator, we performed end-to-end training. Considering the characteristics of ASR error results and LATEX, we constructed the loss function as follows.\nFor a given input audio $X_{tts}$, let the inference results of two ASR models be $Y_{asr1}$ and $Y_{asr2}$. Here, $Y_{asr1}$ and $Y_{asr2}$ are the outputs from the ASR models, which may contain errors. We then provide these two ASR outputs to the Error Corrector $F$. The resulting text can be denoted as $F(Y_{asr1}, Y_{asr2})$. Next, we take $\\hat{y}_{se} = F(Y_{asr1}, Y_{asr2})$ and feed it into the LATEX Translator $G$ to produce the corresponding ATEX, denoted as $\\hat{y}_{latex} = G(F(Y_{asr1}, Y_{asr2}))$. The loss function is then defined as follows:\n$L = \\lambda_{1}L_{se} + \\lambda_{2}L_{latex}$ \nAnd $L_{se}$ and $L_{latex}$ are calculated as cross-entropy losses of the tokenized outputs $\\hat{y}_{se}$ and $y_{se}$, and $\\hat{y}_{latex}$ and $y_{latex}$, respectively.\n$L_{se} = - \\sum_{t=1}^{T} \\sum_{i=1}^{V} y_{se_{ti}} log(\\hat{y}_{se_{ti}})$\n$L_{latex} = - \\sum_{t=1}^{T} \\sum_{i=1}^{V} y_{latex_{ti}} log(\\hat{y}_{latex_{ti}})$\nThe cross-entropy loss is calculated as a negative sum over all time steps $t$ from 1 to $T$ (the length of the sequence) and over all possible words $i$ in the vocabulary $V$. At each time step t, $y_{se}$ is a one-hot encoded vector over the vocabulary, where $y_{se_{ti}}$ is its $i$-th element. Specifically, $y_{se_{ti}} = 1$ if the correct ground-truth word at time step $t$ is the $i$-th word in the vocabulary, and 0 otherwise. Correspondingly, $\\hat{y}_{se_{t}}$ is the predicted probability distribution vector over the vocabulary at time $t$, and $\\hat{y}_{se_{ti}}$ is the predicted probability that the correct word at time $t$ is the $i$-th word in the vocabulary.\nWe calculated the final loss $L$ by assigning different weights to the two cross entropy loss functions. The weight assigned to the $\\hat{y}_{latex}$ was set higher than the weight for the loss on $\\hat{y}_{se}$ because there can be different SE for the same LATEX result."}, {"title": "Evaluation Metrics", "content": "In previous studies, when evaluating LATEX, metrics commonly used in translation tasks, such as ROUGE and BLEU, were employed. Based on this idea, we used ROUGE-1, BLEU, and ROUGE-L. Furthermore, we employed CER, a traditional ASR metric. However, WER was not measured in our experiments. This is because the spaces are often ignored in LATEX. For example, $A B$ and $AB$ result in the same LATEX compilation output. So the evaluation was conducted after removing all spaces."}, {"title": "Experiments", "content": "To evaluate the LATEX translation capabilities of our pipeline, we compared its performance against existing commercial large language models."}, {"title": "Setup", "content": "We fixed the same hyperparameters for the fine-tuned models. The maximum number of training epochs was set to 20, and the model with the lowest validation loss was selected. The learning rate was set to a maximum of 1e-4 and a minimum of 1e-6, adjusted using a linear learning rate scheduler. For the Error Corrector, which requires two ASR outputs as input, the maximum input sequence length was set to 540, with an output length of 275. For the LATEX translator, both input and output sequence lengths were set to 275. T5-small was trained with a batch size of 48 on an NVIDIA A100, and T5-base with a batch size of 84 on an NVIDIA H100.\nAs a comparison group for our pipeline, we selected GPT-3.5, GPT-40, and Gemini-Pro, using 1-shot prompting with one example for all. To observe ATEX translation results across various ASR models, we used five ASR models."}, {"title": "Result", "content": "The experimental results are presented in Tables 5 and 6, respectively. Table 5 lists the outcomes of inputting two ASR prediction candidates that require correction into the LM. In this regard, MathSpeech achieved the best scores for the CER, ROUGE-1, ROUGE-L, and BLEU. The LLM with the second-best performance was GPT-40, while GPT-3.5 and Gemini-Pro showed similar results.\nThe main factor that lowered the performance scores of the commercial large language models was hallucination. When the ASR results were unusual or ambiguous, the baseline LLMs would either output completely different LATEX formulas or produce non-IATEX texts (e.g., Sorry, I can't understand). Additionally, whisper-small and whisper-largeV2 outperformed whisper-base, whisper-largeV3, and canary-1b in overall metric scores. This result aligns with the WER measurements of Spoken English for whisper-small and whisper-largeV2, which were relatively better, as observed in Table 2. In other words, better ASR results lead to improved LATEX translation performance.\nTable 6 lists the outcomes when the top-1 results from different ASR models were used as inputs. The highest performance scores were achieved when the ASR results from Whisper-base & small were used as inputs. This can be attributed to the fact that our ASR error results dataset contains relatively more information from Whisper-base & small. The second highest performance was observed when using the ASR results from Whisper-small & largev2, which can be attributed to the relatively lower WER of these two models on our benchmark dataset. Furthermore, the strong performance on Whisper-largeV3, for which we did not collect ASR error results, demonstrates that MathSpeech can perform well even on ASR models that were not used during training. Moreover, our MathSpeech model, being a small-sized model with 120M parameters, has low latency. When inference latency was measured on an NVIDIA V100 GPU, it took 0.45 seconds to convert the ASR result of 5 seconds of speech into LATEX."}, {"title": "Ablation Study", "content": "To demonstrate the effectiveness of the MathSpeech structure, we conducted ablation studies (Table 7).\nTo show that correcting ASR outputs is crucial for LATEX translation, we removed the corrector and conducted an experiment in which ASR outputs were directly translated into LATEX. As a result, both T5-small and T5-base showed significant performance degradation.\nTo demonstrate that implementing the corrector and translator in a 2-stage structure is effective, we trained a single T5 model to perform both correction and translation in a 1-stage process and observed its performance. The experiment, where we trained the model to translate two ASR error outputs into LATEX with the same setup, showed lower performance compared to our pipeline.\nTo validate the effectiveness of our end-to-end training method, we compared it with a method where the corrector and translator were trained separately and simply concatenated. The results confirm that the proposed training method is more effective.\nSince a single T5-base (220M) is larger than two T5-small models (120M), we did not apply our end-to-end training method to the 2-stage T5-base pipeline. However, we can infer that if we were to apply our training method to the simple concatenation of two T5-base models, the performance could potentially improve further."}, {"title": "Future Works", "content": "(1) Defining a Metric to Solve the Label Ambiguity\nSince LATEX can represent the same formula in multiple ways, it is necessary to consider various possible cases when evaluating the performance of ATEX translation. LATEX is closer to a computer language, like SQL, than to natural language, so metrics like BLEU or CER are not perfect for evaluating LATEX. Therefore, it is necessary to implement a metric that is more suitable for evaluating LATEX.\n(2) Formula Detection in Practice and LATEX Conversion\nThis research focuses only on the ability of ASR models and LMs to generate LATEX. However, to apply this to actual subtitle services, it is essential to develop the ability to detect and separate formulaic parts from speech. This can likely be achieved by training the LM not only on LATEX but also on mixed general text. Additionally, in real-world situations, the speaker may not finish verbally expressing the entire formula. It must be possible to complete such interrupted formulas into full formulas. This could be implemented through the inference capabilities of large language models (LLMs)."}, {"title": "Conclusion", "content": "In this paper, we confirmed through a self-constructed benchmark dataset that existing ASR models lack the ability to read mathematical formulas and are unable to generate LATEX. To address this, we propose MathSpeech, a pipeline that connects ASR models with small LMs to generate LATEX. By effectively connecting two T5-small models and training them end-to-end, our approach demonstrated superior LATEX translation capabilities compared to existing commercial large language models. Our research opens up the possibility of more accurate subtitles in the field of math."}, {"title": "Appendix", "content": "The datasets used in this study\nThis section provides detailed information about the datasets used to train and test MathSpeech.\n(1) Test dataset\nIn our experiments, we used the MathSpeech benchmark dataset, which we created ourselves by extracting math lecture audio directly collected from MIT OpenCourseWare. The detailed information about the MathSpeech benchmark dataset is as follows."}, {"title": "Latency comparison between models", "content": "This section presents a latency comparison between Math-Speech and the baseline LLMs. Due to its compact size of 120M, our model achieves fast inference speeds even with relatively limited GPU resources. The table below demonstrates the efficiency of MathSpeech and highlights its potential for use in subtitle generation services."}]}