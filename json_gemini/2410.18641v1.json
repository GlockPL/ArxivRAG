{"title": "Smart ETL and LLM-based contents classification:\nthe European Smart Tourism Tools Observatory experience", "authors": ["Diogo Cosme", "Ant\u00f3nio Galv\u00e3o", "Fernando Brito e Abreu"], "abstract": "Purpose: Our research project focuses on improving the content update of the online\nEuropean Smart Tourism Tools (STTs) Observatory by incorporating and categorizing\nSTTs. The categorization is based on their taxonomy, and it facilitates the end user's search\nprocess. The use of a Smart ETL (Extract, Transform, and Load) process, where Smart\nindicates the use of Artificial Intelligence (AI), is central to this endeavor.\nMethods: The contents describing STTs are derived from PDF catalogs, where PDF-\nscraping techniques extract QR codes, images, links, and text information. Duplicate STTs\nbetween the catalogs are removed, and the remaining ones are classified based on their\ntext information using Large Language Models (LLMs). Finally, the data is transformed\nto comply with the Dublin Core metadata structure (the observatory's metadata structure),\nchosen for its wide acceptance and flexibility.\nResults: The Smart ETL process to import STTs to the observatory combines PDF-scraping\ntechniques with LLMs for text content-based classification. Our preliminary results have\ndemonstrated the potential of LLMs for text content-based classification.\nConclusion: The proposed approach's feasibility is a step towards efficient content-based\nclassification, not only in Smart Tourism but also adaptable to other fields. Future work\nwill mainly focus on refining this classification process.", "sections": [{"title": "1 Introduction", "content": "The research presented in this paper aligns with the broader goals of advancing knowledge\nsystems and enhancing information retrieval by leveraging large language models (LLMs)\nto classify Smart Tourism Tools (STTs) within an automated ETL process for extracting,\ntransforming, and classifying data from complex, unstructured sources like PDFs.\nThe use of LLMs in information retrieval, particularly for content classification, is a recent\nresearch topic where we could not find a systematic literature review (SLR), although many\nprimary studies have already been published. Due to its importance, we have produced such\nSLR on this topic ([4], supplemented by [3]). Therefore, instead of including a dedicated\nsection on related work in this article, we invite the reader to consult the aforementioned SLR\nfor a comprehensive review of this area.\nSmart Tourism (ST) is a usual designation for ICT-based innovation in the tourism sector.\nThe corresponding ICT tools are dubbed Smart Tourism Tools (STTs). A literature review on\nthis topic [7] concluded that \"Despite all the hype around ST, there is a lack of consensus on the\ndefinition of ST\" and proposed a definition of STTs, drawing from the insights of 330 worldwide\ntourism experts: \"seamlessly interconnected digital tools designed to benefit all stakeholders\nin the tourism industry, with a special focus on the tourist and the destination, that aim at\nsustainable development\". A taxonomy was also proposed there to aid in organizing these STTs,\nbased on products, services, and applications \u201cmade in Europe\". The taxonomy is divided\ninto three application domains: (Part of) the Touristic Offer, Marketing, and Management &\nOperations, each further subdivided into STT categories, and in some cases subcategories, as\nrepresented in Figure 1.\nThe work underlying this paper was developed in the framework of the RESETTING\nproject\u00b9, funded by the European Union's COSME program. Its main objective was to sup-\nport the transformation of European tourism businesses towards more resilient, circular, and\nsustainable operating models by testing and implementing cutting-edge, digitally-driven solu-\ntions that reduce unnecessary burdens, enhance the quality of the travel experience, support\nthe decarbonization of the tourism sector, and promote more equitable economic growth that\nbenefits both SMEs (Small and Medium Enterprises) and destination residents. RESETTING\naimed to help SMEs overcome the challenge of not having the resources and skills to keep up\nwith technological developments and, as a result, presented innovative solutions. Participat-\ning business associations, universities, and public sector organizations throughout the project\nsought to stimulate business innovation in European tourism SMEs.\nThe tourism industry's shift towards digitalization and innovation requires STTs. However,\nthere is a digital divide between large enterprises and SMEs in terms of their stake in the tourism\nindustry [11, 14], and their ability to capitalize on the opportunity of a digital transformation\nof their core business due to financial and technical limitations, as well as an unawareness\nof existing STT. The RESETTING project proposed to address the unawareness limitation\nby developing the European STT Observatory. It aimed to provide a comprehensive and up-\nto-date overview of STT provision in Europe. However, the goal goes beyond creating and\npopulating the Observatory with STTs. The aim is also to automatically categorize them based\non the STT taxonomy, making it easier for users to find exactly what they need."}, {"title": "2 Background", "content": "2.1 The observatory\nAs the backbone for our observatory, we chose the Omeka Net platform, a hosted version of\nthe open source Omeka platform\u00b2, because of its stability and widespread use. It also includes\nan API that allows us to automatically add STTs to the Observatory. It works through H\u03a4\u03a4\u03a1\nrequests with the data field in JSON format. Tasks related to inserting, updating and deleting\ncontent require an API key. Omeka allows us to define items, collections, and exhibits. Items\nare the most basic element and it is possible to upload files to them. In our case, an item\nrepresents an STT. A collection is like a folder that groups related items, and an exhibit is a\nway to combine items in a narrative text. While an item can be in multiple exhibits, it can only\nbe in one collection.\n2.2 STTs catalogs\nThe current observatory content is extracted from two types of catalogs:\nCatalogs of STTs: This category includes four catalogs, including the 2022 and 2023\nversions of the [16] catalog, produced by SEGITTUR - Sociedad Mercantil Estatal para la\nGesti\u00f3n de la Innovaci\u00f3n y las Tecnolog\u00edas Tur\u00edsticas in Spain. From now on, they will be\nidentified as SEGITTUR 2022 and SEGITTUR 2023, respectively. The other two catalogs\nare the first and second versions of the [2], produced by ADESTIC - Cl\u00faster de Empresas\nInnovadoras para el Turismo de la Comunitat Valenciana also in Spain, which will be\nreferred to as ADESTIC V1 and ADESTIC V2, respectively. These were used as data sources\nfor the STT classification.\nCatalogs of ST Practices: Rather than listing various STTs, these catalogs detail services\nand initiatives implemented using STTs, categorized as ST practices. The versions used\nwere 2022 and 2023 of the [15], commissioned by the European Commission, which will\nbe referred to as EU 2022 and EU 2023, respectively. Since these catalogs do not describe\nSTTs but rather their use, their content was not used in the classification task.\nThe SEGITTUR and ADESTIC catalogs contain only STTs developed by Spanish com-\npanies. In addition, the ADESTIC catalogs are in Spanish, so the extracted content must be\ntranslated into English. As for the European Commission catalogs, they only list services and\ninitiatives implemented in Europe that use STTs. Next, we will examine in detail the structure\nand types of content present in each catalog.\nThe SEGITTUR and ADESTIC catalogs share a similar structure. Both include an icono-\ngraphic glossary featuring various graphical icons, each associated with a specific label,\nrepresenting the types of solutions designated by the respective catalog producers. For each\nSTT, the associated icons are extracted and matched against those in the glossary to obtain the\ncorresponding label. SEGITTUR also offers an additional iconographic glossary illustrating\nthe types of destinations suitable for STTs (i.e., Culture and Urban, Nature and Sport, Niche,\nand Beach). Thus, a process similar to the one described previously is carried out. ADESTIC\nalso identifies the types of destinations suitable for STTs but uses a text label instead of an icon.\nThe relevant elements the Spanish catalogs have in common are presented in Table 1.\nIn addition to these elements, there are unique features. SEGITTUR catalogs identify the\nscope of the STT (e.g., governance, technology, sustainability, innovation, or accessibility)\nwith a text label. ADESTIC catalogs include the producer's phone number, email address,\nand QR codes pointing to videos and additional files. The phone prefix may also identify the\nproducer's regional location for landlines.\nConcerning European catalogs, the relevant elements are presented in Table 2\n2.3 Observatory ontology\nEach item published in the observatory must match the Dublin Core metadata structure to\ndescribe digital or physical resources. The Dublin Core, also known as the Dublin Core\nMetadata Element Set (DCMES), is a set of fifteen core metadata elements that have been\nstandardized as ISO 15836-1, IETF RFC 5013, and ANSI/NISO Z39.85. Each element has a\nUniform Resource Identifier (URI)3, and they have been assigned to the dc: namespace, i.e.,\nto ensure consistent identification and use across applications and systems, each URI has the\nprefix dc:. Table 3 provides descriptions of some of these elements, with the dc: namespace\nincluded in their URIs.\nThese core elements now belong to a wider set of metadata vocabularies and techni-\ncal specifications maintained by the Dublin Core Metadata Initiative (DCMI), named DCMI\nMetadata Terms (DCTERMS), and available on the dcterms: namespace. It was also stan-\ndardized as ISO 15836-2. When mentioning Dublin Core, elements refers to the DCMES\nproperties, while terms refers to the DCTERMS.\nCompared to the definition of elements, terms are more detailed and precise, including\nthe type of term, formal ranges, and classes. To maintain compatibility with existing RDF\n(Resource Description Framework)4 implementations of DCMES, fifteen terms with the same\nnames were created in DCTERMS and defined as sub-properties of the corresponding DCMES\nelements. This compatibility is essential for several reasons: it ensures interoperability, allow-\ning different systems and applications to work together seamlessly; it ensures data consistency,"}, {"title": "2.4 Topic overview", "content": "The main challenge of our research, which falls within the data integration subfield of the\ninformation retrieval domain [10], is to automate the process of ingesting and classifying STTs\naccording to their taxonomy into the observatory.\nData integration is the process of combining data from different sources into a unified\nview. It involves cleansing, transforming, and consolidating data from different databases,\napplications, systems, or services. Data integration aims to provide meaningful and valuable\ninformation that can be easily used for analytical, operational, or transactional purposes [6].\nIn addition to moving data from one place to another, data integration should ensure its\nconsistency, reliability, and quality. An interesting taxonomy of data integration functions can\nbe found online in [17].\nETL (Extract, Transform, and Load) explicitly focuses on the extraction, transformation,\nand loading phases of the data integration process. As such, ETL can be considered a subset\nof the data integration landscape. In data integration, ETL tools are critical for collecting data\nfrom disparate sources, transforming it into a consistent and usable format, and loading it into\na target database or data warehouse. These tools enable organizations to efficiently manage,\nconsolidate, and analyze data from disparate sources to provide a unified view for reporting\nand decision-making. ETL can be particularly challenging when extracting multimedia content\n(e.g., plain text, street addresses, URLs, logos, images, video) [9].\n2.4.1 Smart ETL\nOur research aims to extract data from unstructured STT catalogs in PDF format, organize\nthem in a human-readable and orderly manner, classify the STTs, and upload them to the\nobservatory. To emphasize the use of AI to automate the STT classification process, we propose\nthe term Smart ETL. Our approach to automating STT classification differs from traditional\nmachine learning (ML) classification methods in two main ways:\ni. The absence of a categorized dataset, typically with thousands of records, to be divided\ninto training and test sets.\nii. The dataset does not consist of multiple features with categorical, ordinal, or numeric\nvalues. The classification of STTs is based solely on their textual descriptions. In addition,\nthe model must be able to recognize implicit concepts. For example, if an STT is described\nas having sensors to measure crowding, this implies that it can monitor the flow of people\nand that the collected data can be used to adjust a tourist service. However, this information\nis not explicitly stated in the description; it should be inferred.\n2.4.2 Generative AI and large language models\nGenerative AI (GenAI) is a currently popular subset of AI that involves algorithms that\ngenerate new content based on their training data, including images, text, and audio. Large\nLanguage Models (LLMs) are a specific category of generative models explicitly designed\nto understand, generate, and manipulate human language. Among these, Transformer-based"}, {"title": "4 Smart transformation phase", "content": "The code used in this section is available on the RESETTING page on GitHub.\nTables 6 and 7 present some extracted elements that were incorporated into DCTERMS\nproperties. Also, a DCTERMS property may comprise more than one element.\nOmeka allows for the incorporation of the content of each item into HTML format. So,\nafter extracting the elements, we transform them into HTML format, which provides more\nflexibility in how the content is laid out. In addition, since ADESTIC's catalogs are in Spanish,\nthe deep-translator library was used to translate them into English.\nIn the Transformation phase, LLMs were used for removing duplicates (Section 4.2) and\nclassifying STTs (Section 4.3). Before diving into the tasks, we will justify the choice of the\nLLMs.\n4.1 Choice of LLMs\nDue to resource constraints, fine-tuning an LLM with satisfactory performance was not fea-\nsible. Hosting an LLM solely for inference was also not a viable option. Consequently,\nmodel selection should exclusively consider decoder-only models since they are the only type\naccessible online without needing local execution.\nHugging Face is a French-American company and an open-source community focusing\non NLP models and tools. They are renowned for their Transformers library, an open-source\nplatform that offers user-friendly interfaces to cutting-edge pre-trained NLP models. Besides\nthat, they have also developed HuggingChat, e.g., a free AI-powered conversational agent with\nthe latest NLP models. Hugchat is an unofficial Python API for HuggingChat, but notably,\nHugging Face's Chief Technology Officer (CTO) has expressed appreciation for the project\non their GitHub page.\nFor the duplicates removal task we used, through Hugchat, the mistralai/Mixtral-8x7B-\nInstruct-v0.1. At the time of execution, this model was Hugchat's default LLM and one of the\nbest LLMs available.\n4.2 Duplicates removal\nSome redundancies occurred since both ADESTIC and SEGITTUR catalogs focus on Spanish\nSTTs. However, when ADESTIC is translated into English, the STT names may not match\nthose in the SEGITTUR catalogs. Furthermore, solution types in different catalogs may not\nshare identical names, even if they represent the same concept, and a type in one catalog may\ncover several types in another. As a result, manual associations have been established between\nsolution types. However, some solution types remain unassociated because there is no possible\nassociation with the types in another catalog. The manual association is available here.\nTherefore, the duplicate removal process works as follows:\ni. The STT and producer names between two STTs are compared.\nii. If no match is found, the STTs associated with similar solution types are retrieved. For\ninstance, when searching in the ADESTIC V2 catalog for duplicates of a STT from the\nSEGITTUR 2022 catalog with the solution type \"Efficient Management: Energy\", we will\nbe looking for STTs having the solution type \"Efficient Management: water, air, energy\nor waste\" (Box 1).\nAnother limitation has been prevented; we will explain it through an example for under-\nstandability's sake. When searching within the SEGITTUR 2023 catalog for duplicates of\na STT from the ADESTIC V1 catalog with \"Accessibility\" as the solution type, and if the\nSEGITTUR 2023 catalog lacks that solution type (Box 2), all solution types without an\nassociation between these two catalogs are retrieved (Box 3). This process helps identify\nsolutions that are the same but were categorized differently by the catalog administrators.\nIn a real application, the \"{...}\" in Box 3 would be replaced by the remaining types,\nthat have no identified association.\niii. Subsequently, each STT will undergo comparison using the LLM. The prompt pro-\nvided in a zero-shot setup is available on the European STT Observatory GitHub page,\naccompanied by an application example.\n4.3 STTs classification\nAs noted above, the European Commission catalogs were not used as a data source for this\ntask because they do not specifically present STTs.\nWhile several LLMs were evaluated, only Microsoft Copilot in Precise Mode was employed\nto define the necessary prompts. This decision was influenced by the fact that, at that time,\nthe GPT-4 Turbo-powered Copilot Precise mode was considered one of the top-performing\nmodels."}, {"title": "4.3.1 Needle in a haystack challenge", "content": "Although the context length of Copilot is known, we conducted a test to determine the model's\nability to detect fine details within the provided context. This detection is essential for our\ntask, so we can know the model's limit to understand the taxonomy and the few-shot examples\nprovided. The test was a needle in a haystack challenge, where the haystack was the text of\nan extracted book and the needle was a short sentence out of the context. The needle was\nrandomly placed in the book text, and the LLM was asked to find it.\nHaystack: The Sonnets by William Shakespeare\nNeedle: \"Portugal's national team became European champions in 2016 against France.\nThe final was played in Paris and the final score was 1-0 after extra time.\"\nThe prompt template provided to the model is the one in Box 4.\nWe initiated the test with a prompt character limit of 10,000. If the LLM responded\ncorrectly, we increased the limit by 500 characters. However, if the model provided incorrect\nanswers 6 times consecutively, the test was concluded. In addition, we calculated the quartile\nin which the needle was randomly inserted in the text of the book so that we could get a better\nidea of the influence on the model's response.\nWe conducted a total of four tests. However, all tests were prematurely terminated due to\nreaching the Copilot online character limit. As a result, we were unable to accurately determine\nthe precise character limit at which the LLM can still successfully locate the needle. Despite\nthis limitation, we were able to derive several valuable insights:\nCopilot may be capable of finding the needle in the haystack when the character limit is\nabove 23,000.\nFigure 3 shows that Copilot performs better at detecting small details in the second half of\nthe prompt, as evidenced by the lower percentage of errors in the third and fourth quartiles.\nNote again that the needle was placed randomly, so n is not the same for all quartiles.\nThe requested JSON object was provided in 85% of the responses, for a total of 151\nresponses. This higher accuracy may confirm the previous point that the model is better at\ndetecting small details in the second part of the prompt, since the JSON request is the final\nstatement. However, for a comprehensive analysis, the JSON request should be placed in\ndifferent parts of the prompt for comparison, which was not done in this research as it was\nnot the primary focus.\nFigure 4 illustrates that the probability of errors increases as the maximum character limit\nincreases. The 19,500-23,000 range had almost as many errors as the 10,000-19,000 range,\nwith 22 errors compared to 21. It is important to note that the test was terminated after six\nerrors, not after the first error. This approach allows us to estimate the likelihood of Copilot\nmaking errors at each character limit."}, {"title": "4.3.2 Classification methodology", "content": "Based on these results, we decided on a maximum character limit of 15,000 characters for the\nSTT classification task. For that, we employed both training and testing datasets. However, due\nto the absence of a large categorized dataset, we manually categorized some STTs as follows:\nSTTs Selection: We randomly selected STTs from each Spanish catalog, since only those\ncatalogs contained STTs. We extracted an STT from one catalog and categorized it, then\nrepeated the process for the other catalogs.\nDataset Sizes: After reviewing all four catalogs, we returned to the first one and repeated the\nprocess until we created two datasets, each containing STTs that covered all the categories\nin the taxonomy. In one test, one dataset serves as the training set and the other as the\ntesting set. In the subsequent test, the roles are reversed, with the training dataset becoming\nthe testing set and vice versa. Since an STT can cover multiple categories, the size of the\ndatasets is smaller than the number of categories. Thus, Dataset A contains 8 STTs, and\nDataset B also contains 9 STTs.\nThe prompts for this task were created through an iterative approach, with changes or\nadditions made in each iteration. Three prompts were utilized: one to describe the STT\ntaxonomy, one for the few-shots, and another to introduce each STT to be classified. We\nverified the need to update the prompts through the answers given by the LLM about the STTs\ngiven to be classified. The answers let us ascertain when a definition was unclear, namely\nwhen the LLM confused two or more categories. When it was necessary to shorten or clarify\na sentence or paragraph in the prompts, we found it advantageous to use the LLM for this task\nsimply by asking the LLM to do so and providing the respective text to be updated.\nPrompt of Taxonomy Definition: the earlier versions of this proposal included an outdated\ntaxonomy. We recognized the need for an update in the \"(Part of) the tourist offer\" domain\nbecause the LLM struggled to fully understand it, and we concluded that people might\nface the same difficulty. A report on the iterations made while developing this prompt is\navailable online.\nFew-shot Prompt: we differentiated the few-shot examples from the STTs to be classified\nusing initial identifiers in the prompts: \"###EXAMPLE###\" or \"###Classification###\". A\nstructure identical to the one in Box 5 was used for each shot.\nAlthough it does not always work (i.e., the LLM did not comply with the request), the last\nstatement in the prompt is essential to prevent the LLM from giving long answers to the\nfew-shot examples in most cases. In addition, for the classification justification, we created\nan expression for each STT category so that we could repeat it in the justification every time\nthat category was selected, thus achieving consistency and repetition between the few-shot\nexamples, and trying to make the job of interpretation easier for the LLM. An example is:\nTourist Experience: \"It is \"Tourist Experience\" because it requires the tourist's active\nparticipation (i.e., . . . ) and it's not about the planning, organization, or execution of the\nactivity or future activities.\"\nThis sentence is repeated in the justifications each time this category is selected, with \"...\"\nreplaced by information that classifies the STT as \"Tourist Experience\". If the combination\nof two or more few-shots did not exceed the limit identified in the Needle in a Haystack test,\nthey were sent in the same prompt."}, {"title": "5 Load phase", "content": "This was the only phase where AI was not used. One of the reasons for choosing Omeka.net\nis that it includes an API that allows us to insert STTs into the observatory automatically. It\nworks via HTTP requests with the data field in JSON format, and for tasks related to inserting,\nupdating, and deleting content, it requires an API key that only the observatory managers\nhave. We created a Python class called OmekaAPI to facilitate the API processes. The class\ndoes not contain all the features that Omeka.net allows, but only the ones we need, and is\navailable here. To help sum up these last two sections, Figure 5 presents the BPMN process\nof the Smart Transformation Phase and the Load Phase applied in this research."}, {"title": "6 Results and discussion", "content": "Here, we will focus on presenting the results of the principal task of our Smart ETL process,\ni.e., STT classification. Each test corresponds to the classification of an STT, following a\nblack-box approach due to the LLMs' architecture, and its response is considered valid as long\nas the returned JSON contains the required keys and the STT categories identified as values.\nWe now discuss the results of the STTs classification for each LLM separately:\nNous Research/Nous-Hermes-2-Mixtral-8x7B-DPO: The LLM failed the tests due to its\ndifficulty in understanding the taxonomy and the classification task. This was evident after\ncompleting the first full set of tests with dataset A, as the results did not match the format\nof the expected results and showed a poor understanding of the taxonomy.\nmistralai/Mixtral-8x7B-Instruct-v0.1: We discarded the use of this LLM due to its recurrent\nhallucination, i.e., most of the classified STTs were assigned to non-taxonomy categories\nsuch as \"Tourist Information Systems\", \"Centralized Database & Distribution System\",\n\"Booking Systems\", or \"Appointment Manager\", among others. Running two full sets of\ntests, one with dataset A and the other with dataset B, was sufficient to conclude that the\nreasoning abilities of this model were inadequate for this task.\nmeta-llama/Meta-Llama-3.1-70B-Instruct: This LLM \u201cunderstood\" the required task.\nHowever, the output for the (Part of) the Touristic Offer domain was sometimes inaccurate.\nLike in the previous model, but less recurrently, it hallucinated, categorizing an STT with\nnon-taxonomy categories like \"Tourist Infrastructure\" or \"Parking Infrastructure\". At other\ntimes, it only returned the first-level category, such as \"Tourist Experience\" or \"Tourist Expe-\nrience Lifecycle Management\". Another minor issue was splitting categories like \"Tourist\nExperience Lifecycle Management, Building Block\" into separate categories. Despite this\nminor error, we considered the responses to be valid for the metrics, as it is possible to\ninfer implicitly the category assigned. Table 8 shows the average precision, recall and F1\nscores for each domain, together with the overall F1 score. Each dataset was tested twice\non different days and times to assess performance variability.\nMicrosoft Copilot: This model achieved the best results but also had the most hallucinations\n(being the more tested model may be a consequence). The (Part of) the Touristic Offer\ndomain continued to present difficulties. Common errors included:\nTreat the first-level categories (\"Tourist Experience\" or \"Tourist Experience Lifecycle\nManagement\") as domains. In the returned JSON, these categories were incorrectly\nplaced as keys with the second-level categories as their values. Despite this, these answers\nwere accepted because they were easily associated with the corresponding domain.\nIn some tests, there was repetition in the answers. Specifically, the LLM assigned the\nsame categorization to multiple STTs. As a result, these tests were considered invalid\nand excluded from the metric calculations. Since the LLM is available online, we chose\nnot to include these invalid tests in the metrics, as they reflect instances where the\nLLM hallucinated or repeatedly gave the same answer, potentially indicating times when\nCopilot was more overloaded and performing sub-optimally.\nOccasionally, the model used information from previously classified STTs within the\nsame context window to justify the classification of the current STT.\nAnother error, which occurred only once but highlighted the hallucination issues, involved\na test where instead of returning the requested JSON, the LLM returned a JSON with the\ncategories as keys and the STT name as a value for each key.\nTable 9 shows the average precision, recall and F1 scores for each domain, together with\nthe combined F1 score for valid tests. Each dataset was tested twice at different times/days\nto assess performance variability.\nOne of the limitations of this research is that the prompts were only refined using Microsoft\nCopilot and were not customized for each model used. As a result, the prompts may have\nbeen over-optimized for Microsoft Copilot, potentially leading to suboptimal performance in\nother models. In addition, because the models were used online rather than locally, we were\naffected by the variability of other users' interactions with the model. This left us uncertain\nabout the model's capabilities during testing, making it unclear whether occasional negative\nresults were due to potential model overload or our methodology."}, {"title": "7 Verification and validation", "content": "7.1 Demonstration\nFor online demonstration, we created a web site containing all the information related to\nthe observatory. This site includes a user manual detailing the Observatory's functionalities,\na video demo, and technical documentation summarizing the main technical aspects of the\nproject, such as Smart ETL, Large Language Models, the methodology used, as well as the\ntools and a link to the GitHub repository with the implemented code.\nSeveral onsite demonstrations also took place:\nAt the inauguration of the new \"Iscte - Knowledge and Innovation\" building in Lisbon\non November 20, 2023. This demonstration was attended by the then Prime Minister, the\nMinister for Science, Technology and Higher Education, the Minister for Cohesion, the\nMinister for Culture, two Secretaries of State, a large entourage representing the most\ndiverse sectors, and the media.\nAt the RESETTING Hackathon, held at Tecnoparc in Reus, Spain, on May 2024, which\nfocused on \"TourismTech solutions to address the challenges of European tourism\". During\nthis event, each tool developed in the RESETTING project, including the Observatory, was\npresented, accompanied by a poster. All posters are available here.\nAt the RESETTING Final Conference, held at Auditori Diputaci\u00f3 in Tarragona, Spain, also\non May 2024. During this event, the tools and posters were presented to the conference\nattendees, including some SME owners, our desired main target audience, who received\nfunding from the RESETTING project.\n7.2 Evaluation\n7.2.1 Continuity\nWe sent a survey to different smart tourism stakeholders to draw conclusions on the usability\nand usefulness of the European STTs Observatory and get insights on possible missing features.\nThe survey asked about the types of professionals for whom the Observatory could be helpful,\nhow they rated the demo, user manual, and technical documentation, how likely they would\nbe to use the Observatory in the future, and what additional features they would like to see\nimplemented.\nThe survey can be found at https://tinyurl.com/sttobservatory-survey, and at the time of\nwriting, it had already received 40 responses, most of them complete (not all questions were\nmandatory). Preliminary results show that researchers, producers of Smart Tourism Tools\n(STTs), and managers of tourism-related businesses are likely to find the Observatory most\nuseful (Figure 6). Tourists themselves and public service managers will also benefit. Figure\n8 shows the grades received for the demo, user manual, and technical documentation. We\ncan see that they all received high scores (7 or above) in most responses, reflecting good,\nthough not perfect, quality. In addition, most of the respondents said they were likely to use\nthe Observatory in the future (Figure 7).\nAmong the suggested functionalities, providing information on events and research pub-\nlications stands out as a feasible addition to the European STT Observatory. Among the\nremaining suggestions, the most interesting were:\nEasy way to submit STTs\nI recommend you include new technologies on your website, such as digital twins, cyber-\nsecurity, metaverse, generative AI, and quantum computing in smart tourist destinations.\nThese disruptive tools are being used in some STDs (Smart Tourism Destinations), and they\nwill change the new paradigm of travel and tourism activities in tourism cities and STDs.\"\n7.2.2 Monitoring\nThe Google Analytics platform is a crucial tool used to monitor access to the Observatory.\nThe snapshot below (Figure 9) corresponds to the last eight months (from January to August\n2024). This robust monitoring process ensures the Observatory's performance is meticulously\ntracked and any necessary adjustments can be made to maintain its effectiveness."}, {"title": "8 Conclusions", "content": "The initial objective of this paper was to create a smart ETL process, where \"smart\" means\nintegrating AI into a conventional ETL framework. The extraction phase was completed by\nextracting various types of content, including images, text, links, QR codes, phone numbers,\nemail addresses, and more, from STT catalogs in PDF format. The transformation phase\nfocused on developing an automatic classification system for STTs using AI, particularly\nLLMs. That classification (STT labeling) will facilitate tourism sector SMEs searching for\ninnovative and sustainable business solutions.\nIt is essential to strive for high accuracy in classification, but achieving a perfect one (100%)\nis unnecessary because eliminating all errors is often unfeasible. The results demonstrated that\nclassification is feasible but lacked the consistency required for large-scale implementation.\nThis could be due to either the use of potentially inappropriate few-shot examples or the\nvolatility of the LLM's reasoning capabilities, which, as an online model, can sometimes\nunderperform due to overload. There is also the problem of hallucinations in LLMs. In this\npaper, the model occasionally classified an STT as if it understood the problem but then\nreturned categories that were not part of the given STT taxonomy. In some cases, it even used\ninformation from previously classified STTs to justify the classification of the current STT. A\nrecent study, [1], examined the issue of hallucinations in LLMs and found that while there are\ntechniques to mitigate them, these hallucinations are unavoidable, and systems using LLMs\nmust be prepared to deal with them. The loading phase was completed by integrating the\ncontent into the European STT Observatory using the API of its hosting platform.\nAlthough the results were not as optimal as desired, the few successful tests conducted in\na volatile environment (online LLM) indicate that an acceptable classification accuracy could\nbe achieved in a controlled environment (local LLM)."}, {"title": "9 Future work", "content": "Given the promising initial results for the LLM-based classification", "follows": "nExtract and classify content from the STTs' URLs found in catalogs. The main challenge\nhere is that those pages do not have a standard layout as in the catalogs, so LLMs are also\nrequired for content extraction. If the content is then segmented, such as in catalogs, then\nthe technique proposed in this paper can be used for STT classification (labeling).\nIdentify new STTs by web searches automatically. The main challenge here is finding reliable\nSTT candidates. Analyzing the"}]}