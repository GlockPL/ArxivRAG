{"title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models", "authors": ["YANG FAN"], "abstract": "As Large Language Models (LLMs) are pretrained on massive-scale corpora, the issue of data contamination has become increasingly severe, leading to potential overestimation of model performance during evaluation. To address this, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data evaluation method aimed at mitigating the impact of data contamination on evaluation reliability. AdEval extracts key knowledge points and main ideas to align dynamically generated questions with static data's core concepts. It also leverages online search to provide detailed explanations of related knowledge points, thereby creating high-quality evaluation samples with robust knowledge support. Furthermore, AdEval incorporates mechanisms to control the number and complexity of questions, enabling dynamic alignment and flexible adjustment. This ensures that the generated questions align with the complexity of static data while supporting varied complexity levels. Based on Bloom's taxonomy, AdEval conducts a multi-dimensional evaluation of LLMs across six cognitive levels: remembering, understanding, applying, analyzing, evaluating, and creating. Experimental results on multiple datasets demonstrate that AdEval effectively reduces the impact of data contamination on evaluation outcomes, enhancing both the fairness and reliability of the evaluation process.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP) with their remarkable performance across a wide range of tasks [1], [2]. These models rely on pretraining over massive-scale internet corpora, while many widely-used benchmarks are also derived from online resources, inevitably leading to the issue of data contamination [3]. Recent studies indicate that data contamination is prevalent in LLM evaluations [1], [4], [5], undermining the credibility of evaluation results and hindering fair comparisons between models.\nTo address data contamination, two primary solutions have been proposed: data contamination detection and dynamic data evaluation [6]. Data contamination detection aims to identify overlaps between model outputs and training data [7]. However, these methods face limitations, particularly for proprietary models such as the GPT series, which often incorporate special filtering mechanisms during generation. Additionally, detection methods primarily focus on extreme memorization (i.e., direct reproduction of training data), making it difficult to capture more subtle forms of contamination. Moreover, the training datasets of closed-source models are often treated as trade secrets, leaving external communities with limited ability to intervene directly.\nCompared to static benchmarks, dynamic data evaluation effectively circumvents the problem of data contamination, avoiding the inherent limitations of detection methods. Existing dynamic evaluation approaches have achieved notable progress in various domains. For instance, DYVAL [4] introduces dynamic data generation tailored for mathematical tasks; KIEval [6] incorporates multi-turn dialogue-based evaluation; LatestEval [8] excels in generating contamination-free datasets; SciEval [9] emphasizes testing scientific research capabilities through the design of entirely new questions; and [10] propose a practical strategy for generating evaluation data using simple heuristics. These methods explore the potential of dynamic evaluation from different perspectives, offering diverse tools for assessing the capabilities of LLMs.\nTo address these challenges, this paper proposes AdEval (Alignment-based Dynamic Evaluation), a dynamic data evaluation method designed to reduce the impact of data contamination on LLMs evaluation results by dynamically generating aligned evaluation samples. The key innovations of AdEval are as follows:\n1) Dynamically generating aligned evaluation data: AdEval extracts key knowledge points and main ideas from static data and combines them with online search to provide detailed expansions of related content. This process generates dynamic data that aligns with the core concepts of the static data, ensuring both dynamism and consistency.\n2) Complexity control and dynamic alignment: AdEval incorporates a complexity regulation mechanism to dynamically adjust the difficulty of questions. This ensures that the generated data matches the difficulty level of static data while accommodating diverse complexity requirements.\n3) Multi-dimensional cognitive evaluation: Using Bloom's taxonomy, AdEval evaluates LLMs across six cognitive dimensions\u2014remembering, understanding, applying, analyzing, evaluating, and creating-offering a comprehensive assessment of their performance in various cognitive tasks."}, {"title": "II. RELATED WORK", "content": "To evaluate the performance of LLMs across multiple tasks, researchers have proposed various benchmarks. MMLU [11] assesses knowledge and reasoning capabilities across diverse disciplines. HELM [12] expands evaluation dimensions to include aspects such as fairness and efficiency. BIG-Bench [13] introduces 204 challenging tasks to explore the limits of model capabilities. AGIEval [14] and C-Eval [15] focus on standardized examinations and advanced knowledge evaluation in Chinese, respectively.\nWhile these benchmarks cover a wide range of tasks, they primarily rely on static datasets, making them susceptible to data contamination, which can result in an overestimation of model capabilities."}, {"title": "B. Data Contamination", "content": "Data contamination has emerged as a critical challenge in evaluating LLMs [3]. In traditional NLP and ML tasks, separating training and testing data is relatively straightforward. However, as LLMs grow in scale, overlaps between training data and evaluation benchmarks have become increasingly common. This contamination may lead to misunderstandings of model performance and, in some cases, misleading conclusions. Particularly, when evaluation benchmarks and training data are derived from similar online sources, the risk of contamination increases significantly, affecting the fairness and validity of model comparisons.\nSeveral studies have investigated the impact of data contamination. [16] and [17] explored how data contamination during pretraining affects language model performance, analyzing factors such as data volume and model size. [18] and [19] examined the influence of contamination on downstream tasks or zero- and few-shot evaluations. [20] demonstrated a technique called Evasion through Augmented Learning (EAL), an effective contamination method that bypasses most detection strategies.\nRecent data contamination detection methods include detecting data leakage through sample or question order manipulation (e.g., PAC [21], [22], [23]); using perplexity-based detection (e.g., [24], [25]); analyzing LLMs' internal state distributions or identifying low-probability outlier tokens (e.g., DICE [26], Min-K%++ [27], [28]); and identifying contamination by detecting and correcting output distributions (e.g., CDD and TED [29]). Other methods, such as DetCon and ComiEval [29], improve benchmark evaluation, while ConStat [30] identifies contamination by comparing benchmark performances. [31] combine guided prompts with classifiers for detection, [32] propose cross-lingual contamination detection, and [33] analyze membership characteristics using statistical inference. Despite their unique contributions, these methods often rely on intrinsic model information or complex analytical techniques, limiting their practical applicability.\nIn contrast to static benchmarks, dynamic evaluation overcomes data contamination by generating new datasets, providing a more reliable approach for evaluating LLMs. LatestEval [8] and [10] automatically generate test data from up-to-date texts, minimizing overlap with pretraining data. DYVAL [4] and DyVal 2 [34] dynamically produce samples with controllable complexity to evaluate reasoning capabilities and model improvements effectively. SciEval [9] combines static and dynamic data for multi-level evaluation of scientific tasks. [35] employs comprehensive tests to evaluate the subject-specific knowledge of Chinese LLMs.\nAdditionally, several methods mitigate data contamination and improve evaluation accuracy by generating variants, rephrasing questions, or removing similar samples. For instance, Clean-Eval [36] uses paraphrasing and semantic detection to optimize sample quality. [37] generate variants and rephrase questions for contamination detection, while [38] filters similar samples to reduce overfitting risks. DCQ [39] creates multiple-choice questions to detect data leakage, and KIEval [6] evaluates comprehension and knowledge application through multi-turn dialogues. These studies provide diverse strategies for dynamic evaluation, significantly enhancing fairness and reliability in LLM assessments."}, {"title": "III. METHODS", "content": "The proposed process for dynamically generating datasets is shown in Figure 1, consisting of the following steps: data preprocessing, extraction of knowledge points and main ideas, online search, question design, and complexity control, quality checking."}, {"title": "A. Prompt Optimization", "content": "Prompts significantly influence the output quality of large language models (LLMs), as their design directly determines the accuracy and alignment of the generated content with the anticipated answer. To improve output quality, this study adopts a multi-iteration prompt optimization strategy. Utilizing multiple LLMs, such as GPT-4, the prompts are continuously adjusted and refined to guide the model's outputs closer to the desired results.\nAs illustrated in Figure 2, the optimization process involves the following key steps:\n1) Example Selection: Select 1 to 5 representative examples from a predefined dataset as few-shot examples.\n2) Initial Prompt Design: Based on the examples, design the initial version of the prompt, embedding the examples and task requirements into the prompt to form a complete input.\n3) Model Generation and Comparison: Input the prompt along with the examples into the LLM to generate answers. Compare the outputs with the standard answers to analyze differences and shortcomings.\n4) Optimization Suggestion Generation: Input the initial prompt, the LLM outputs, and the standard answers into another LLM to generate optimization suggestions for the prompt.\n5) Prompt Adjustment: Refine and improve the prompt based on the optimization suggestions, guiding the LLM to produce outputs closer to expectations."}, {"title": "B. Data Preprocessing", "content": "Before integrating the data into the AdEval process, it must undergo formatting adjustments and sampling. Considering computational resource and time constraints, uniform sampling is performed on the dataset, selecting 302 samples. Among these, 300 samples are used for dynamically generating new data, while the remaining 2 samples serve as few-shot examples for the prompts.\nThe unified format of the adjusted dataset is as follows:\n\nUniform Sampling Formula:\n$i_k = \\lfloor \\frac{k \\cdot (N-1)}{S} \\rfloor, k = 0, 1, ..., S - 1$ (1)\nWhere $i_k$ is the k-th index of the sampled data, N is the total size of the dataset, S is the target sample size, and $\\lfloor x \\rfloor$ denotes the floor function. The term $\\frac{k \\cdot (N-1)}{S}$ maps k uniformly to the range [0, N-1], ensuring consistent intervals between sampling points. The floor function ensures valid integer indices."}, {"title": "C. Knowledge Point and Main Idea Extraction", "content": "In this section, the relevant knowledge points are extracted from the sampled dataset. During the extraction process, LLMs autonomously generate knowledge points, which need to meet the following requirements:\n1) The generated knowledge points must be relevant to the question.\n2) They should focus on summarizing broad concepts or categories of knowledge related to the problem rather than detailed explanations.\n3) The generated content must adhere to a predefined format. If it does not, the LLM is re-invoked to generate compliant results.\nFollowing the extraction of knowledge points, the main idea of the question is summarized. The LLM generates the main idea by summarizing the question, adhering to the following two requirements:\n1) The main idea must encompass the context of the problem and the core of the answer.\n2) The content must be concise and clear, avoiding redundant expressions.\nThe main idea plays a key role in the subsequent online search and question design stages, ensuring that the newly generated questions align with the core ideas of the static dataset questions.\nDuring the extraction of knowledge points and main ideas, a few-shot strategy is employed, providing examples tailored to different datasets to improve the quality of the LLM-generated content. Overall, the knowledge points depict various aspects of the current question, while the main idea focuses on its core concept. This design ensures consistency in the core ideas while expanding the content of new questions in different directions."}, {"title": "D. Online Search for Knowledge Point Explanations", "content": "In this section, we utilize the online search function of large language models to input the problem statement, related knowledge points, and corresponding main ideas of each question in the static dataset. The model then generates detailed explanations for the knowledge points. The generated content needs to meet the following three requirements:\n1) Focus on the Knowledge Points: The detailed explanation must be closely centered around the knowledge points provided, and can include background or supplementary information from the question or main ideas, but the focus must remain on the knowledge points.\n2) Concise Paragraphs: The output must be a clear and coherent paragraph, avoiding overly long or excessively short content, and must not include paragraph breaks or excessive embellishments.\n3) Straightforward: The output must be comprehensive, accurate, and logically clear, focusing on the main topic while avoiding irrelevant information or vague statements."}, {"title": "E. Question Design", "content": "This section aims to generate two types of new questions by embedding the aforementioned static dataset questions, knowledge points, main ideas, and knowledge point explanations into the prompts: general new questions and Bloom's Taxonomy-based [41], [42] new questions. The generated questions needs to meet the following requirements:\n1) High Relevance: The question, correct option, and answer must be directly based on the provided text and closely related to the main idea.\n2) Consistent/Greater/Lesser Difficulty: The complexity of the question should match the reference question's complexity, be slightly higher, or slightly lower, ensuring that the question requires deep understanding, analysis, and synthesis of details to arrive at the correct answer, rather than being a simple recall of facts.\n3) Reasonable and Discriminative Options: The options should be logically clear, avoiding obvious answers. At least one strong distractor should be included to assess the student's attention to detail and reasoning ability.\n4) Avoid External Dependencies: The question and options must be entirely based on the provided text and main idea. If external content is required, it should be embedded directly into the question, avoiding references such as \"According to the text\" or similar phrases. Questions should not rely on context that makes them difficult to understand in isolation.\n5) Unique Answer: Ensure only one option is correct, with other options being plausible but subtly incorrect or only partially matching the main idea and text content.\n6) Assess Deep Understanding: The designed questions can involve inference, comparison, method analysis, application of key concepts, etc., to increase the difficulty.\n7) Innovative Question Stem: The question format should not be too similar to the reference questions, demonstrating your ability to innovate, while maintaining rigor and professionalism in the question style.\n8) Different Emphasis: The generated questions should focus on different aspects compared to the reference questions. For example, if the reference question focuses on concept definitions, the generated question could focus on application of methods, comparison of key details, analysis of pros and cons, or practical scenario judgment from other angles.\n9) Focus on Knowledge Points: The question should be centered around the provided knowledge points, with the core content tightly aligned with the main idea.\nThe newly generated questions based on Bloom's taxonomy, in addition to meeting the requirements 1-5 and 9 listed above, must also satisfy the following: Level Coverage: Each question corresponds to a cognitive level, covering all six levels of Bloom's taxonomy (for specific details, see Appendix E). There should be one question for each level, totaling six questions."}, {"title": "F. Question Complexity Control", "content": "If no constraints are applied to the LLM's question generation process, the results tend to produce relatively simple datasets. To solve this issue, we designed a question difficulty adjustment process (as shown in Figure 3) to ensure that the generated dataset's complexity aligns with that of the static dataset.\nIn the initial phase of generating the dataset, we introduce a complexity alignment mechanism through few-shot prompting to ensure the complexity of the generated data matches expectations. However, in the initial phase, there is a large discrepancy in complexity, so further adjustments are necessary to control the complexity of the dataset within an acceptable error range.\nSpecifically, we indirectly measure the complexity of the dataset using the accuracy of the preset model (e.g., Qwen-7B-Chat). The newly generated dataset is tested with the preset model, and the accuracy of the model is calculated to derive the deviation in question difficulty:\n$\\Delta = S_{target} - S_{gen}$ (2)\nWhere $S_{target}$ represents the desired accuracy (for example, when aligning with the static dataset, it refers to the accuracy of the static dataset on the preset model), and $S_{gen}$ represents the accuracy of the newly generated dataset on the preset model. We aim for the deviation to satisfy $|\\Delta| < 5\\%$.\nAdjustment Strategy:\nIf $\\Delta > 5\\%$: The generated dataset's complexity is too high. We then select the questions that the model answered incorrectly from the generated dataset and reconstruct them to simplify the questions.\nIf $\\Delta < -5\\%$: The generated dataset's complexity is too low. We then select the questions that the model answered correctly and reconstruct them to increase the complexity.\nAfter each reconstruction, the newly generated dataset is re-tested, and the above steps are repeated until $\\Delta < 5\\%$. Through this process, we can effectively control the complexity of the generated dataset, aligning it with the static dataset or preset target."}, {"title": "G. Quality Control", "content": "To ensure that each step of the AdEval process meets quality requirements, we introduced a multi-model voting mechanism for quality assessment. Specifically, multiple LLMs (including Doubao-pro-32k, Qwen-plus, and GPT-40-mini) are used for quality evaluation. The voting rules are as follows:\n1) If at least 2 out of the 3 responses are judged as \"pass,\" the question is considered of acceptable quality.\n2) If the above condition is not met, the question is deemed to have quality issues and is regenerated."}, {"title": "IV. EXPERIMENT", "content": "Task and Complexity: The datasets we used include: MMLU [11] and ARC-Challenge [40], with 300 samples generated for each dataset. To reduce the interference of randomness on the evaluation results, each generated dataset was evaluated independently 5 times, while ensuring that the complexity of the generated data aligns with that of the static dataset.\nTo balance performance and computational costs, we selected Qwen-plus (API) as the primary tool for generating knowledge points, main ideas, and detailed explanations of knowledge points, and used Doubao-pro-32k (API) to align question complexity. The temperature parameter for the large models was set to 0 to avoid interference from randomness during generation."}, {"title": "B. Data Contamination Fine-Tuning Test", "content": "This experiment aims to explore the impact of data contamination on the fine-tuning effect of small models. Two models were compared: one is the clean model (CLN), and the other is the model contaminated by static data (CTM). The experimental results were compared and analyzed using different datasets, quantifying the effects of static data contamination and dynamic data generation methods on model performance.\nCLN: Clean model; CTM: Contaminated model with static data; $\\Delta$: The accuracy difference between CLN and CTM.  indicates $\\Delta > 10$, magenta indicates $\\Delta\\in [5, 10]$, and blue indicates $\\Delta\\in [0,5)$; Static: MMLU static data; DataGM (Data Generation Method): Dynamic data generation methods; AdEval: Data dynamically generated by the AdEval method; Auto-Dataset: Data dynamically generated by Auto-Dataset; LatestEval: Data dynamically generated by LatestEval\nIn Table II, the accuracy from static data on the clean model is used as a baseline, indicated with an underscore (e.g., 60.9\u00b10.8). If the accuracy of a dynamic data generation method for the clean or contaminated model differs from this baseline by less than 5%, it is also indicated with an underscore.\nThe following conclusions can be drawn from Table II:\n1) Significant Improvement in Model Performance with Static Data Contamination: Fine-tuning with static data contamination (Static) significantly improves the accuracy of all models, with $\\Delta$ values exceeding 10%, most exceeding 20%. This shows that static data contamination has a strong positive impact on model performance. For example, in the MMLU dataset, the $\\Delta$ values for static data contamination are 24.3%, 28.9%, and 25.6%, significantly higher than other dynamic data generation methods.\n2) Minimal Impact of Contaminated Data on Dynamic Data: Among the three dynamic data generation methods, AdEval and LatestEval maintain $\\Delta$ values within 5%, indicating that dynamic data is more stable under contamination. For example, in the MMLU dataset, AdEval's $\\Delta$ value is 4.2%, LatestEval's $\\Delta$ value is 2.4%; in the ARC dataset, AdEval's $\\Delta$ value is 0.3%, LatestEval's $\\Delta$ value is 4.2%. In contrast, Auto-Dataset shows higher $\\Delta$ values in some experiments, such as 8.1% in the MMLU dataset, indicating larger fluctuations.\n3) AdEval's Data Difficulty Consistent with Static Data: The data generated by AdEval is consistent with static data in terms of difficulty. Whether the model is contaminated or not, the accuracy of the data generated by AdEval differs by less than 5% from the baseline accuracy of static data on the clean model. For example, in the MMLU and ARC-Challenge datasets, the accuracy of the data generated by AdEval closely matches static data, with the difficulty significantly higher than that of LatestEval and Auto-Dataset. In contrast, the data generated by LatestEval and Auto-Dataset tends to be simpler in some experiments."}, {"title": "C. Data Reconstruction", "content": "This experiment demonstrates how AdEval controls data complexity through data reconstruction. We selected the data reconstruction process of AdEval on the MMLU dataset, using the Qwen2-7B-Chat model. AdEval progressively adjusts the complexity of the generated dataset through multiple rounds of data reconstruction to make it largely consistent with the complexity of static data. Figure 4 shows the accuracy changes of the clean model (CLN) and contaminated model (CTM), with the baseline accuracy of static data on the clean model shown by the red horizontal dashed line for comparison.\nAs shown in Figure 4, with each round of data reconstruction, the complexity of the data generated by AdEval gradually increases. This is reflected by the changes in model accuracy on the dataset (the lower the accuracy, the higher the complexity). The experimental results show that the accuracy of the data generated by AdEval gradually decreases from its initial higher value, indicating that AdEval effectively increases the complexity of the generated data through data reconstruction. After a certain number of rounds of reconstruction, both the clean model (CLN) and the contaminated model (CTM) show an accuracy that is nearly identical to the baseline accuracy of the static data (indicated by the red horizontal dashed line).\nIn conclusion, whether the model is contaminated or not, AdEval can effectively regulate the complexity of the data through multiple rounds of reconstruction, ultimately generating a dataset with a complexity level consistent with that of the static data."}, {"title": "D. Bloom's Cognitive Hierarchy", "content": "The radar chart (Figure 5) generated by AdEval reveals that the difficulty of dynamically generated questions in the MMLU dataset is significantly higher than that in the ARC-Challenge dataset. Despite the overall difficulty differences between the two datasets, the internal difficulty distribution within each dataset remains consistent. For example, questions in the \"Creating\" and \"Applying\" levels are more challenging in both datasets, while questions in the \"Understanding\" and \"Evaluating\" levels are less difficult. This demonstrates AdEval's stability in generating questions across different cognitive levels.\nAdditionally, when comparing the results of Qwen-7B-Chat-CLN (clean model) and Qwen-7B-Chat-CTM (contaminated model), although data contamination slightly affects certain cognitive levels, the overall impact on the model remains minor, and the difficulty stability is maintained."}, {"title": "E. Perplexity", "content": "This experiment evaluated the perplexity of the Qwen-7B-Chat clean model (CLN) and contaminated model (CTM) across multiple datasets. The results are shown in Table 3, with $\\Delta$ values highlighted in different colors: orange for $\\Delta > 0.2$, magenta for $\\Delta \\in [0.1,0.2]$, and blue for $\\Delta \\in [0, 0.1]$.\nFrom Table III, it can be observed that static data fine-tuning (data contamination) significantly reduces model perplexity, with $\\Delta$ values around 0.2. For example, in the MMLU dataset, the $\\Delta$ for static data fine-tuning is 0.17, while in the ARC-Challenge dataset, it is 0.23.\nAmong dynamic data generation methods, AdEval exhibits $\\Delta$ values below 0.1, indicating minimal and stable reductions. In contrast, LatestEval shows $\\Delta$ values between 0.1 and 0.2, while Auto-Dataset displays a wider range, with $\\Delta$ spanning from 0.05 to 0.2, showing more significant fluctuations.\nIt is noteworthy that both static and dynamic data fine-tuning reduce the model's perplexity. This is because all generated data belong to the same question types as static data. Although the specific questions differ, fine-tuning familiarizes the model with these question types, leading to an overall decrease in perplexity. Among all methods, AdEval demonstrates the smallest and most stable reduction in perplexity."}, {"title": "F. Performance Evaluation", "content": "We evaluated the LLMs listed in Table 1 using AdEval. This experiment tested various large models on dynamically generated data using different methods. A represents testing using only dynamic data questions, while B indicates adding the corresponding static data questions into the prompts of A. $\\Delta$ represents the difference between A and B, with the results color-coded as follows: orange for $\\Delta > 5$, magenta for $\\Delta \\in [2, 5]$, and blue for $\\Delta \\in [0, 2).\nNote: Due to stricter safety settings, the glm-4-flash model refused to answer approximately 5% of questions, causing some fluctuation in its results.\nFrom Table IV, it can be observed that:\n1) The impact of static data on dynamic datasets generated by the three methods: AdEval > Auto-Dataset | LatestEval.\n2) Difficulty of dynamic datasets generated by the three methods: AdEval < Auto-Dataset < LatestEval."}, {"title": "G. Quality Control", "content": "We conducted quality checks on questions and answers generated by AdEval, Auto-Dataset, and LatestEval. Three large language models (Doubao-pro-32k, Qwen-plus, and GPT-40-mini) voted to assess correctness. The rule was that if at least two of the three models judged an answer to be correct, the question was marked as correct; otherwise, it was marked as incorrect.\nThe table below shows the error rates for dynamic datasets generated by the three methods: AdEval Auto-Dataset LatestEval MMLU ARC-Challenge"}, {"title": "V. CONCLUSION AND DISCUSSION", "content": "The proposed AdEval method achieves remarkable results in addressing the data contamination problem in large language model (LLM) evaluation. By dynamically generating evaluation data consistent with static data, AdEval effectively mitigates the impact of data contamination while precisely controlling question complexity, ensuring the generated datasets share the same distribution as static datasets. Experimental results demonstrate that AdEval outperforms other dynamic data generation methods on the MMLU and ARC-Challenge datasets, particularly in controlling data complexity and maintaining quality stability.\nUnder the influence of data contamination, static data fine-tuning effectively enhances model performance, while dynamic data generation methods exhibit varying degrees of resilience to contamination. Compared to Auto-Dataset and LatestEval, AdEval better controls the complexity and quality of generated questions while minimizing the impact of data contamination on the model. Particularly in complexity control, AdEval shows significant potential in applications across different cognitive levels and data domains, providing a more stable and reliable solution for LLM evaluation.\nFuture research could explore the scalability and diversity of AdEval, especially by validating and applying it to more types of datasets. Key directions include enhancing the diversity and adaptability of generated data, optimizing generation algorithms, and reducing potential biases and errors during the generation process."}]}