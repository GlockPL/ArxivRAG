{"title": "Unveiling the Hidden:\nOnline Vectorized HD Map Construction with\nClip-Level Token Interaction and Propagation", "authors": ["Nayeon Kim*", "Hongje Seong*", "Daehyun Ji", "Sujin Jang"], "abstract": "Predicting and constructing road geometric information (e.g., lane lines, road\nmarkers) is a crucial task for safe autonomous driving, while such static map\nelements can be repeatedly occluded by various dynamic objects on the road. Recent\nstudies have shown significantly improved vectorized high-definition (HD) map\nconstruction performance, but there has been insufficient investigation of temporal\ninformation across adjacent input frames (i.e., clips), which may lead to inconsistent\nand suboptimal prediction results. To tackle this, we introduce a novel paradigm\nof clip-level vectorized HD map construction, MapUnveiler, which explicitly\nunveils the occluded map elements within a clip input by relating dense image\nrepresentations with efficient clip tokens. Additionally, MapUnveiler associates\ninter-clip information through clip token propagation, effectively utilizing long-\nterm temporal map information. MapUnveiler runs efficiently with the proposed\nclip-level pipeline by avoiding redundant computation with temporal stride while\nbuilding a global map relationship. Our extensive experiments demonstrate that\nMapUnveiler achieves state-of-the-art performance on both the nuScenes and\nArgoverse2 benchmark datasets. We also showcase that MapUnveiler significantly\noutperforms state-of-the-art approaches in a challenging setting, achieving +10.7%\nmAP improvement in heavily occluded driving road scenes. The project page can\nbe found at https://mapunveiler.github.io.", "sections": [{"title": "1 Introduction", "content": "Vectorized HD map construction (VHC) is a task of predicting instance-wise vectorized representa-\ntions of map elements (e.g., pedestrian crossings, lane dividers, road boundaries). Such static map ele-\nments are crucial information for self-driving vehicles, including applications like lane keeping [1, 6],\npath planning [29, 24, 15], and trajectory prediction [30, 40, 11]. Prior approaches to constructing\ndense and high-quality HD maps typically rely on SLAM-based offline methods (e.g., [48, 37, 38]).\nSuch an offline method generally involves a series of steps including feature extraction and selection\n(e.g., edge, plane), odometry estimation via feature matching, and mapping. However, these processes\ninvolve complicated and computationally burdensome tasks, limiting their use to offline applications.\nMore recently, camera-based multi-view VHC has been actively investigated as a cost-efficient and\nreal-time alternative to existing expensive offline approaches. Current works on camera-based VHC\ntypically aim to extract unified 3D Bird's Eye View (BEV) features that cover the surrounding\nenvironment of the ego-vehicle [20, 22, 23], relying on various Perspective View (PV) to BEV\ntransformation methods [50, 32, 21, 5]. Subsequently, a task-specific head follows to decode and"}, {"title": "2 Related Work", "content": "Multi-View HD Map Construction. SLAM (Simultaneous Localization and Mapping) [10] has\nbeen a central technique for constructing accurate HD maps [48, 37, 38]. However, these meth-\nods require memory-intensive, complex pipelines for global mapping of geometric features, and\nare therefore typically executed offline. Recently, deep learning approaches have emerged as an\nappealing alternative to those expensive offline methods, enabling online HD map construction\nusing cost-efficient multi-view camera sensors. The perspective-view (PV) to bird's-eye-view (BEV)\ntransformation methods [50, 32, 21, 5] enable the generation of 3D features from the surrounding\nenvironment of the ego-vehicles using camera sensors, even in the absence of precise spatial cues.\nBEVFormer [21] utilizes the deformable attention mechanism [52] to extract BEV features and\npredict rasterized semantic maps. However, it cannot generate instance-wise representation of map\nelements. To address this, HDMapNet [20] introduces a heuristic method to group pixel-level se-\nmantic maps into a vectorized representation. Similarly, VectorMapNet [26] proposes an end-to-end\nlearning approach to predicting vectorized map representations. Although such methods have demon-\nstrated notable prediction performance in single-frame inference, they do not consider the temporal\ninformation from multi-frame inputs. More recently, StreamMapNet [46] and SQD-MapNet [41]\nhave proposed a streaming feature paradigm [42], which aims to leverage temporal information for\nimproved temporal consistency across predictions. However, these methods propagate dense BEV\nfeatures directly, incorporating map information from previous frames that may have been occluded\nand undetected, resulting in the accumulation of noise, as shown in Fig. 1-(b). To address this issue,\nwe propose an end-to-end clip token learning approach that combines offline mapping techniques\nwith online strategies, aiming for high performance and computational efficiency.\nTemporal Token Learning. With the rapid development of transformers [39], there has been\nsignificant interest in efficient token learning alongside dense features, e.g., CNN representations.\nIn particular, temporal token learning has emerged as an attractive alternative to memory-intensive\nspatio-temporal dense CNN representations [45, 31]. VisTR [43] extends DETR [4] into the 3D\ndomain to extract spatio-temporal instance tokens that can be directly used for instance segmentation.\nIFC [17] proposes an efficient spatio-temporal token communication method, which replaces the\nheavy interactions within dense CNN features. VITA [13] learns efficient video tokens from frame-\nlevel instance tokens without dense CNN features. Cutie [7] updates CNN representations with\ntokens to avoid spatio-temporal dense matching. TTM [36] introduces an efficient long-term memory\nmechanism by summarizing tokens into memory rather than stacking [2, 18, 34] or recurrence [14, 8].\nWhile all the aforementioned approaches were designed to handle foreground instances, we discover\nthe potential of token learning to construct background maps. By learning compact tokens and\ninteracting with dense BEV features, we impose traditional mapping into online VHC model and\nenable online running."}, {"title": "3 Method", "content": "3.1 Overview\nWe present the overall architecture of MapUnveiler in Fig. 2. Given a set of synchronized multi-view\nimages (i.e., clip inputs), our model sequentially construct clip-level vectorized HD maps. We first\nextract frame-level BEV features and map queries, which are then used as inputs to MapUnveiler\nmodule. We employ memory tokens, which are written from the previous clip and facilitate the\nestablishment of long-term temporal relationships. From the memory tokens and map queries, we\ngenerate clip tokens that embed temporal map element cues in a compact feature space. This is the\nfirst step to understand clip-level map information. We then update BEV features with clip tokens,\nwhich is the core step of unveiling hidden maps. Using the updated (unveiled) BEV features, we\nextract map tokens and construct clip-level vectorized HD maps. After a clip-level inference, we\ngenerate new memory tokens using clip tokens, map tokens, and the current memory tokens. The new\nmemory tokens are used for providing temporal cues for the subsequent clip-level inference. Since\nwe opt for a clip-level pipeline, MapUnveiler efficiently infers with a temporal stride $S$, performing\nclip-level inference only $N_T/S$ times for a sequence of $N_T$ frames. In the following subsections, we\ndetail each module in the proposed framework.\n3.2 Frame-level MapNet\nWe adopt MapTRv2 [23] as our frame-level MapNet architecture to extract a clip set of BEV features\nand map queries from synchronized multi-view images. We extract perspective view (PV) image\nfeatures using a backbone network, then these PV image features are transformed into BEV features\nthrough a PV-to-BEV module. Following the setup of MapTRv2, we adopt ResNet50 [12] and Lift,\nsplat, shoot (LSS) [32]-based BEV feature pooling [16] for our backbone and PV-to-BEV module,\nrespectively. These BEV features are utilized for querying maps in the map decoder. With their\nBEV features, the map decoder outputs frame-level map queries which can be directly used for\nconstructing vectorized HD maps. Finally, the frame-level MapNet outputs BEV features and map\nqueries, which are the results from the PV-to-BEV module and the map decoder, respectively. BEV\nfeatures represent rasterized map features, whereas map queries embed vectorized map information;\nthus, we can directly construct vectorized HD maps using the map queries.\n3.3 MapUnveiler\nMapUnveiler is a novel framework designed to unveil invisible map information that cannot be\ncaptured by frame-level BEV features alone. To avoid heavy computations, we adopted a clip-level\ninference scheme with temporal window (clip length) $T$ and stride $S$. A detailed explanation of the\ninference scheme with the temporal window $T$ and stride $S$ is provided in Appendix (see Sec. A.1).\nOur MapUnveiler consists of two main components: (1) Intra-clip Unveiler and (2) Inter-clip Unveiler.\nFor each clip-level pipeline, our Intra-clip Unveiler generates vectorized maps for $T$ frames. The\nInter-clip Unveiler then writes memory tokens with the tokens generated in Intra-clip Unveiler to\nbuild global relationships."}, {"title": "3.3.1 Intra-clip Unveiler", "content": "Intra-clip Unveiler is composed of a sequence of $L$ layers. It initially takes a clip set of frame-level\nmap queries $Q^{map}$, BEV features $F^{BEV}$, and memory read $U^{Read}$ (read at Inter-clip Unveiler,\ndetailed in Sec. 3.3.2). In the first step, compact clip tokens are created by the clip token generator.\nThe BEV updater then unveils hidden maps in the BEV features with the clip tokens. Finally, map\ngenerator outputs clip-level map tokens from the updated BEV features. The map tokens are directly\nused for constructing vectorized HD maps with perception heads. We illustrate the Intra-clip Unveiler\nin Fig. 3. In the followings, we describe the detailed implementation of each module.\nClip Token Generator. Clip token generator yields clip tokens $U^{clip} \\in \\mathbb{R}^{N_c\\times C}$ from frame-level\nmap queries $Q^{map} \\in \\mathbb{R}^{T\\times N_i\\times N_p\\times C}$, where $N_c$, $N_i$, and $N_p$ denote the clip token size, number\nof predicted map element, and number of points per map element, respectively. To globally gather\nintra-clip map features, we opt for a naive cross-attention [39]. Through this step, we obtain compact\nclip-level map representations, enabling efficient intra-clip communication with small-scale features.\nBEV Updater. The second step is the BEV Updater, which updates bev features $F^{BEV} \\in$\n$\\mathbb{R}^{T\\times H\\times W\\times C}$ with the clip tokens $U^{clip}$ to unveil the hidden map element information. In cross-\nattention, query is derived from the bev features $F^{BEV}$, and the key and value are derived from the\nclip token $U^{clip}$. The output of this step is robustly updated bev features $U^{BEV} \\in \\mathbb{R}^{T\\times H\\times W\\times C}$\nenhanced via clip tokens for hidden areas relative to the original bev features. The main idea of BEV\nUpdater is to avoid heavy computation in spatio-temporal cross attention. To achieve this, we do not\ndirectly communicate intra-clip BEV features, but instead decouple the spatial BEV features and the\ntemporal clip tokens. We then update the spatial BEV features with compact temporal clip tokens,\neffectively communicating spatio-temporal information with reasonable computational costs. The\nupdated bev features $U^{BEV}$ are used as value features in the next step.\nMap Generator. The last step is the Map Generator, which generates map tokens $U^{map} \\in$\n$\\mathbb{R}^{T\\times N\\times N_p\\times C}$ using the updated BEV features $U^{BEV}$ created in the previous step. The objec-\ntive of this step is to generate a refined version of frame-level map queries. As illustrated in Fig. 3,\nthe map generator uses deformable attention [52] and decoupled self-attention [22] mechanisms,\nfollowing [22]. In deformable attention, query is derived from the map queries $Q^{map}$, and the value is\nderived from the updated BEV features $U^{BEV}$. Since the updated BEV features are spatio-temporally\ncommunicated, we directly extract map tokens. Each map token represents a vectorized map element\nthrough a 2-layer Multi-Layer Perceptron (MLP). The map tokens $U^{map}$ are written to the memory of\nthe Inter-clip Unveiler, and when the map tokens $U^{map}$ of the $L$-th layer pass through the prediction\nhead, vectorized maps are generated."}, {"title": "3.3.2 Inter-clip Unveiler", "content": "Inter-clip Unveiler propagates the tokens from previous clip input to the next one, thereby preserving\nthe dense temporal information from the prior frames. As shown in Fig. 2, Inter-clip Unveiler writes\nmap tokens $U^{map}$ and clip tokens $U^{clip}$ from the Intra-clip Unveiler to the memory. Here, we adopt\ntoken turning machine (TTM) [36] to efficiently manage the long-term map information. In the\nfollowings, we describe the detailed implementation of read and write.\nRead. We generate compact tokens that contain a global map information by reading from memory\ntokens and map queries. Following TTM [36], we read with the token summarizer [35] which\nefficiently selects informative tokens from inputs as follows:\n$U^{read}_{memory} = \\text{Read}(U^{-2S:t}_{L-1S},\\, Q^{map}) = \\text{SN}_{\\mathcal{N}}([U^{memory}_{L-1S} || Q^{map}]),$ (1)\nwhere $[U^{memory}_{L-1S} || Q^{map}]$ denotes the concatenation of two elements, and $U^{memory}_{L-1S}$ denotes mem-\nory tokens for a clip. We employ the location-based memory addressing used in [36] utilizing the\npositional embedding (detailed in Section 3.3.3). Note that the memory is not available in the first\nclip-level pipeline. Therefore, we initially write the memory token from learnable clip embeddings.\nWrite. We employ the write operation with the same token summarizer [35] that is used in [36].\nThe new memory $U^{memory} \\in \\mathbb{R}^{M\\times C}$ is generated by summarizing the clip tokens $U^{clip}$, map"}, {"title": "3.3.3 Positional Embedding", "content": "While the standard transformer structure is permutation-invariant, we require position information\nadded with temporal information to predict map elements at the clip-level. For the BEV features ($P_B$)"}]}