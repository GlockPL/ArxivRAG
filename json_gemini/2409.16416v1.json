{"title": "Selection of Prompt Engineering Techniques for Code Generation through Predicting Code Complexity", "authors": ["CHUNG-YU WANG", "ALIREZA DAGHIGHFARSOODEH", "HUNG VIET PHAM"], "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in software engineering tasks. However, improving their accuracy in generating correct and reliable code remains challenging. Numerous prompt engineering techniques (PETs) have been developed to address this, but no single approach is universally optimal. Selecting the right PET for each query is difficult for two primary reasons: (1) interactive prompting techniques may not consistently deliver the expected benefits, especially for simpler queries, and (2) current automated prompt engineering methods lack adaptability and fail to fully utilize multi-stage responses.\nTo overcome these challenges, we propose PET-Select, a PET-agnostic selection model that uses code complexity as a proxy to classify queries and select the most appropriate PET. By incorporating contrastive learning, PET-Select effectively distinguishes between simple and complex problems, allowing it to choose PETs that are best suited for each query's complexity level.\nOur evaluations on the MBPP and HumanEval benchmarks using GPT-3.5 Turbo and GPT-40 show up to a 1.9% improvement in pass@1 accuracy, along with a 74.8% reduction in token usage. Additionally, we provide both quantitative and qualitative results to demonstrate how PET-Select effectively selects the most appropriate techniques for each code generation query, further showcasing its efficiency in optimizing PET selection.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently Large Language Models (LLMs) have shown their promising performance in various software engineering tasks, such as unit test case generation [36\u201338], automated bug repair [15, 49], API specification [24]. Especially for code generation from natural language descriptions, LLMs demonstrate their impressive capability where code is generated with natural language descriptions [19, 39].\nGiven the state-of-the-art LLMs are all closed-source, the most popular way to enhance the LLM's ability to generate accurate and reliable code is to utilize various prompt engineering techniques"}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 Automated Prompt Engineering", "content": "Since Large Language Models (LLMs) are too large to fine-tune for every downstream task, prompt engineering has become a common approach to optimize performance across various tasks, including unseen ones. However, designing effective prompts for each task is a challenging process. Several studies have suggested reliable methods to improve language model performance, such as Chain-of-Thought and Self-correction prompting. Despite this, the question remains whether we can develop a system that automatically generates appropriate prompts for different queries. Previous studies [9, 58] proposed frameworks for automatic instruction generation and selection, where several candidate prompts are generated by LLMs, and the best prompt is chosen from these candidates. Another approach involves retrieving similar queries from a database and using them to create a more effective prompt [31]. However, these automatic prompt engineering methods primarily focus on crafting a single optimal prompt for a given problem. There is limited research on how to design multi-round prompting, where multiple interactions with language models are used to refine the response. Crafting prompts based on the model's responses is crucial, as many state-of-the-art prompting techniques rely on self-generated answers to achieve optimal performance. Whether used for correction or evaluation, iterative interactions with language models play a key role in helping them generate better responses.\nTechnically, prompting technique selection is also a form of automatic prompt engineering, as it involves choosing the relatively appropriate prompt automatically. Unlike previous approaches, prompting technique selection considers whether the prompt should be crafted for a single or multiple iterations, allowing for multiple rounds of interaction. A previous study [55] selects prompting techniques after each execution, which is costly and impractical in real-world applications, particularly when multiple techniques are considered as candidates. PET-Select is the first framework to select prompting techniques prior to execution. It employs a traditional deep learning model with contrastive learning to select the most suitable technique for each question, making it applicable and affordable even without the need to run language models."}, {"title": "2.2 Prompt Engineering Challenges", "content": "With the increasing number of prompting techniques being proposed and achieving state-of-the-art results on various benchmark datasets, a question arises: \u201cCan we apply the most advanced prompting techniques to every question?\u201d Unfortunately, the answer may be no. The first and most obvious issue is that using these advanced prompting techniques for every question is costly, as they often require multiple interactions with language models or involve crafting lengthy prompts with numerous examples. The second, and less well-known issue is that applying advanced prompting techniques to simpler questions can sometimes lead to incorrect answers. A recent study [7] experimented on a variant of GSM8K, where all the answers to the questions in the dataset were explicitly stated in the questions themselves and could be obtained without any calculations. Surprisingly, the accuracy improves when language models are restricted from performing any calculations or reasoning steps, compared to when no instructions are specified. This suggests that unnecessary calculations and over-reasoning can lead to incorrect answers. There is another study [16] suggests that language models are not able to self-correct themselves. Self-correction is defined as a scenario where the model attempts to correct its initial responses purely based on its capabilities, without relying on external feedback. Many advanced prompting techniques leverage the self-correction ability of language models, such as Progressive Hint [56] and Self-refine [30]. However, research has shown that accuracy decreases with each iterative round. This suggests that the model struggles to identify and correct the specific incorrect parts. When the initial answer is correct, the model often changes the correct portion to something incorrect, resulting in a wrong answer.\nPET-Select learns to determine whether a question is easy or difficult by predicting the code complexity of the ground-truth code. This allows PET-Select to choose the relatively appropriate prompting techniques for each query, applying simpler techniques to easy problems and more advanced ones to difficult problems. This approach helps prevent over-reasoning and redundant calculations for easy questions, while also avoiding situations where the model changes a correct answer to an incorrect one."}, {"title": "3 APPROACH", "content": "In this work, we propose PET-Select, a novel method to select suitable prompt engineering techniques (PETs) for each query.  PET-Select is a supervised learning approach and since no such record of execution is available for various prompt engineering techniques we start off by building the data in the Dataset Construction phase (Section 3.1). PET-Select's model consists of two main parts: the embedding layer (Section 3.2) and the classification layer (Section 3.3). Finally, we conduct a n-fold cross-validation evaluation to ensure that PET-Select is correctly evaluated."}, {"title": "3.1 Ranked PET Dataset Construction", "content": "PET-Select is designed to be prompt engineering technique (PET) agnostic, we decide that unsupervised learning is the most appropriate approach. To train PET-Select, we first need to conduct a study to collect the dataset of execution records of various representative PETs such as Zero-shot, and Few-shot and rank each PET given their performance and cost for each query. Since numerous PETs could be employed for the code generation task, we select the most representative ones by choosing at least one technique from each fundamental strategic design category, such as root techniques, refinement-based techniques, and others, as defined in a recent study [45]. Detailed descriptions and implementations of these prompting techniques are provided in Section 4.1."}, {"title": "3.1.1 Benchmark Dataset Construction", "content": "We choose the two most popular code generation datasets MBPP and HumanEval and benchmark the selected PETs on ChatGPT 3.5 and 40 (Step 1). The responses were recorded along with the cost of the query in terms of the number of input and output tokens. Along with the query cost, the generated code complexity measured by five metrics is also recorded, the weighted sum of which is used as the overall complexity score. Details of the metrics used are provided in Section 4.2."}, {"title": "3.1.2 PETs Ranking", "content": "Once every technique has been benchmarked, we select the most appropriate one for each query with the highest $R\\_Score_i$ (Step (2)). Where $R\\_Score_i$ for technique i is calculated as:\n$R\\_Score_i = log(max_{j=1}^{N}(T\\_tokens_j)) \\times pass_i \u2013 log(T\\_tokens_i)$\nHere, $T\\_tokens$ is the sum of the number of input and output tokens required by PET i, and the $max(Max_{j=1}^{N}(T\\_tokens_j))$ represents the highest number of required tokens across all prompting techniques for that query. The binary number $pass_i$ is 1 (i.e., the generated code passes all test cases) and 2 (i.e., at least one test case failed). Specifically, for techniques that fail to generate test passing code, the formula ensures that the score will be negative, and for successful techniques, the score will be positive. In all cases, the score is always inversely proportional to the number of required tokens. In the end, the technique that generates the correct code while requiring the fewest number of tokens will have the highest score. Since no technique uses the same number of tokens, there are no tied scores between the PETs, we can always choose the most appropriate one for each query.\nAfter this stage, we will obtain the Ranked PETs Dataset in which each entry includes the query string, the generated code, the number of tokens used, the complexity measures, and the most successful PET with the highest $R\\_score$ as the label."}, {"title": "3.2 Fine-tuning CodeBERT Embedding Model", "content": "Based on their design, some PETs are better with more complex queries than others [23]. Given this finding, we want to incorporate the generated code complexity into our model decision-making to achieve the best prediction result. We accomplish this by tuning CodeBERT [12] embedding model utilizing conservative learning [22]. Specifically, the tuning process reshapes the embedding space so that queries with similar generated code complexity will be closer while dissimilar queries are placed farther apart."}, {"title": "3.2.1 Query Triplets Construction", "content": "Contrastive learning performs optimization on query triplets each including an anchor query, a positive query, and a negative query [14, 48]. Specifically, anchor queries are the original natural language questions, positive queries are either semantically equivalent to or share the same answer as the anchor queries [22], while negative queries are unrelated to both the anchor and positive queries."}, {"title": "3.2.2 Contrastive Learning", "content": "Once the query triplets are constructed, we use them to fine-tune the CodeBERT sentence embedding model (Step (4). The objective of contrastive learning is to bring queries with similar features and complexity closer together while pushing unrelated queries with differing complexity further apart [32]. When constructing the query triplets, we designate an input query as the anchor, treating queries with similar code complexity scores as positive examples, while those with dissimilar scores are used as negative examples. This design allows the model to learn semantic representations by associating anchor queries with their positive counterparts, positioning them closer within the embedding vector space. Conversely, we expect the model to push unrelated queries further apart from the anchor queries."}, {"title": "3.3 Training Selection Model", "content": "Once the embedding is computed, it can be used to extract a sentence embedding for any given query. The embedding will be used as input to PET-Select's three fully connected layers of neural network with ReLU activation function (Step (5)). These layers are tasked with multi-class classification (i.e., PET selection). Specifically, the predicted technique is selected based on the highest probability according to the softmax function. These layers are trained normally using cross-entropy loss. It is important to note that the data used for training both the sentence embedding model and the selection model is within the training dataset and the model never sees the test set which is set aside to evaluate the model. For evaluation, we also record the probability of each class to calculate the MRR and nDCG metrics (described in Section 5) for the results (Step (6"}, {"title": "4 EXPERIMENTAL SETUP", "content": "In this section, we introduce the setup that we used to conduct our experiments. We first introduce the prompting techniques that are included in PET-Select selection pool, we then discuss the code complexity metrics, and finally, the experimental setting including the code generation datasets and the evaluation metrics."}, {"title": "4.1 Prompt Engineering Techniques (PETs) for code generation", "content": "provides a summary of the PETs used in our experiment. To ensure a broad exploration of techniques, we selected at least one from each category as stated in the recent work [45]. These prompting techniques are classified into five categories based on their core concepts: root techniques, refinement-based techniques, decomposition-based techniques, reasoning-based techniques, and priming techniques. The \u201cStrategic Category\u201d column indicates the categorization of each prompting technique, while the \u201cIteration\u201d column specifies whether the technique involves iterative interactions with the language models. The \u201cExamples\u201d column shows whether the technique includes examples in the prompt to guide the language models on how to answer the questions. The \"Template\u201d column demonstrates the prompting templates we used for each technique. For techniques with multiple iterations, we provided specific prompting templates for each stage. We briefly go through each PET and provide some pros and cons to emphasize that no one PET is optimal for all cases.\nRoot PETs: Zero-shot and Few-shot Root PETs directly query LLMs for answers. Zero-shot and Few-shot [3] are two examples of root PETs where Zero-shot provides no additional example and Few-shot includes several examples. While it is convenient and requires no domain-specific input, Zero-shot performance may be limited when the model encounters unfamiliar tasks. The added examples in Few-shot PET improve LLMs' ability to handle unseen tasks but are not trivial to craft [8, 28, 33] and can negatively impact the performance if given incorrectly [29, 35].\nReasoning PETs: Zero-shot/Few-shot Chain-of-Thought (CoT) are reasoning-based techniques that query LLMs to explain intermediate reasoning steps while generating answers [25, 47]. It enables LLMs to produce more coherent and accurate results. The zero-shot and few-shot CoT differ in the presence of examples: zero-shot CoT does not include examples while few-shot CoT offers additional reasoning examples in the query. Despite the performance improvements similar limitations persist: zero-shot CoT can yield unreliable results on unfamiliar tasks, and the need for carefully crafted prompts with examples remains a challenge with few-shot CoT.\nPriming PETs: Persona is a PET that LLM is guided to take on a specific identity or personality based on expertise, tone, or role. This \u201cpersona\u201d helps make the communication with LLMs consistent, but a too specific persona can lead to restrictive communication.\nDecomposition PETs: Self-planning involves having the LLMs create a mental blueprint or set of steps before answering a question. This is particularly useful for complex tasks that require a structured approach (e.g., solving math problems) [57]. On the one hand, this can provide structure to the solution but on the other, if the initial plan is incorrect, the entire response may be off track.\nRefinement PETs: Self-refine, Progressive Hint, and Self-debug take a different approach by having the LLM interact with its own response after generating it. Specifically, Self-refine [30], Progressive Hint [56], and Self-debug [6] ask the LLM to review its answers, use its answers as hints, and correct its output based on the execution result of test cases. While self-refine can sometimes correct itself, the errors might still pass notice. Progressive Hint also suffers from similar pitfalls where the first hint can be incorrect and create a domino effect. Finally, with the help of the external test cases, self-debug can sometimes correct itself, however, the debugging process is not perfect and sometimes LLM can over-correct itself thus generating the wrong answer."}, {"title": "4.2 Code complexity metrics", "content": "PET-Select utilize five popular code complexity metrics: Line of Code, Cyclomatic Complexity, Halstead Complexity, Cognitive Complexity, and Maintainability Index [21, 42, 54] to aid with the contrastive learning step: Line Complexity is also known as Lines of Code (LOC), which measures the number of lines in a codebase. In this study, Line Complexity is calculated using Physical Lines of Code (PLOC), which excludes comment lines and focuses solely on the program's source code. Cyclomatic Complexity [10] counts the number of independent paths through the code. Higher cyclomatic complexity indicates more potential paths, increasing the testing effort and potentially reducing maintainability. Halstead Complexity [13] evaluates code complexity from both linguistic and mathematical perspectives, based on the number of operators and operands. Cognitive Complexity [4] measures how difficult code is for a human to understand by considering factors like nesting depth and control structures such as if, switch, and for loops. Unlike cyclomatic complexity, it focuses on readability and the mental effort required to follow the code. Maintainability Index [50] is a composite metric that predicts the ease of maintaining a software system, combining factors like cyclomatic complexity, Halstead complexity, and lines of code. It ranges from 0 (difficult to maintain) to 100 (easy to maintain), with higher values indicating better maintainability. In this study, custom code was used to calculate LOC, the Radon package was used to calculate Cyclomatic Complexity, Halstead Complexity, and Maintainability Index, and Cognitive Complexity was computed with the cognitive-complexity Python package."}, {"title": "4.3 Experiment Settings", "content": "We used two of the most widely used code generation benchmark datasets to train the model and evaluate PET-Select's performance: HumanEval [5] and MBPP [1]. Both datasets provide test cases so that generated code can be functionally evaluated and the pass@k metric can be calculated for evaluation."}, {"title": "5 RESULT", "content": "In this section, we evaluate PET-Select and present the findings when exploring three research questions. RQ1 explores how various PETs perform on different types of code generation with different complexity (Section 5.1). In RQ2, we compare PET-Select performance against other baselines on two code generation benchmarks using two versions of GPT (Section 5.2). Finally, we analyze PET-Select's performance in quantitative and qualitative analysis (Section 5.3)."}, {"title": "5.1 RQ1. How do various PETs perform on different types of code generation with different complexity?", "content": "In this research question, we aim to explore the relationship between the code generation types and code complexity to inform our design decisions to incorporate query embedding and generated code complexity in PET-Select."}, {"title": "5.1.1 RQ1.1 Do different PETs excel at generating code for different types of tasks?", "content": "To explore the first part of the question, we first manually categorize questions from the MBPP and HumanEval datasets into six different types of tasks for which the generated code is responsible: Algorithm Design, String Manipulation, List Processing, Mathematical Computation, Parsing and Formatting, and Logical Conditions.\nSpecifically, we applied the following definition to perform the labeling:\n\u2022 Algorithm Design involves writing code to solve problems using specific approaches or procedures. Algorithm design includes tasks like designing search algorithms (e.g., binary search), sorting (e.g., quicksort), and dynamic programming. The focus is on the logic and structure required to solve problems efficiently.\n\u2022 String Manipulation deals with operations related to handling text data, such as modifying, concatenating, splitting, and searching within strings. Common tasks include pattern matching (using regular expressions), converting cases (e.g., uppercase to lowercase), and formatting strings for output.\n\u2022 List Processing involves handling collections or arrays of data. Operations include iterating through lists, filtering, mapping, sorting, and transforming data. Tasks like merging multiple lists or finding elements based on specific conditions also fall under this category.\n\u2022 Mathematical Computation covers tasks that involve performing mathematical operations, such as arithmetic, algebra, trigonometry, or calculus. Examples include calculating averages, finding prime numbers, performing matrix operations, or solving equations.\n\u2022 Parsing refers to interpreting structured data, such as converting a string into a number, extracting values from JSON or XML, or reading configuration files.\n\u2022 Formatting involves preparing data for output, such as formatting dates, numbers, or aligning text for display.\n\u2022 Logical Conditions involves decision-making in code, where you use conditions to control the flow of the program (e.g., if-else statements, switch cases). Logical conditions help programs execute different paths based on input or state, such as checking if a number is even or odd, or deciding which function to call based on user input."}, {"title": "5.1.2 RQ1.2 Do different PETs excel at generating code of different complexity?", "content": "Apart from task types, we also explored if code complexity can inform the correct PET. We hypothesized that simpler techniques might perform better on easier questions (i.e., requiring less complex code), while more complex techniques could be more effective on harder ones (i.e., requiring more complex code). To test this, we applied five code complexity metrics mentioned previously to the ground-truth code for each instance in the MBPP and HumanEval datasets. To account for multiple aspects of code complexity, we aggregate all the complexity scores into a single value called Combined Complexity, which serves as the final complexity score for each instance."}, {"title": "5.2 RQ2. How do PET-Select compare to single PETs and baselines?", "content": "In RQ2, we compare PET-Select with the individual PETs and our two selected baselines.  presents the pass@1 accuracy and token usage on the MBPP and HumanEval datasets for nine individual PETs, as well as various PET selection approaches, using GPT-3.5 Turbo and GPT-40. PETs marked with a star, such as Self-planning, indicate that these techniques require iterative rounds to arrive at the answer for each instance. The \u2018Random Selection' row represents a baseline approach where one of the nine PETs is randomly chosen as the most appropriate for each instance. The overall accuracy and token usage are then calculated based on the selected technique. As we mentioned in RQ1.1, \u2018Category Selection' is the baseline that randomly selects one of the nine techniques based on the probability of each technique being the most appropriate for a given task, as determined by the ranking score mentioned in Section 3.1. For example, if the probability of Zero-shot being the most appropriate technique for Algorithm Design is 60% (i.e., among all the questions correctly answered by the language model, Zero-shot is the most appropriate technique for 60% of them), then Zero-shot will have a 60% chance of being selected for questions categorized under Algorithm Design. For the \u2018PET-Select W/o CL\u02bc row, we train the selection model using the original CodeBERT without contrastive learning which does not incorporate the complexity measure. For the \u2018PET-Select' row, we present the results of selecting PETs based on the output of the selection model.\nOn the MBPP dataset, PET-Select achieves 65.6% accuracy with GPT-3.5 Turbo, which is 0.3% higher than the best accuracy achieved by Self-debug, a technique that applies the same method across all instances. Furthermore, PET-Select uses approximately 13% fewer tokens while achieving higher accuracy compared to Self-debug. This indicates that PET-Select can effectively identify instances that are simple enough for language models to generate correct code using basic techniques. A similar result is observed when running experiments with GPT-40, where PET-Select's accuracy is 0.6% higher than using only Self-debug, while also utilizing fewer tokens. On the HumanEval dataset, PET-Select achieves the same accuracy as Few-shot CoT but with 64.2% fewer tokens when using GPT-3.5 Turbo. With GPT-40, PET-Select achieves an accuracy of 85.4%, which is 1.9% higher than the best accuracy of the other techniques, while also saving up to 74.8% fewer tokens.\nAlthough the Category Selection method does not achieve the highest accuracy, it remains at least the third-best approach among all the baselines, with the lowest token usage when applied to the HumanEval dataset. This indicates that knowing the task category partially helps in selecting the optimal PET.\nThe original CodeBERT without contrastive learning incorporating problem complexity does not help the selection model consistently choose the appropriate techniques. Instead, it repeatedly selects Zero-shot, as it often appears to be the best technique among all the options. This result suggests that contrastive learning effectively clusters questions of similar complexity in the embedding space, and is essential in enabling the selection model to accurately choose the optimal PET.\nComplex PETs such as Self-debug, which require multiple rounds with language models, may not always be the best choice for all questions. For instance, aside from PET-Select, while Self-debug performs best on the MBPP dataset, it falls short on the HumanEval dataset, where simpler techniques like Few-shot CoT achieve the highest accuracy. This result provides more examples which support the claim that applying complex techniques to simpler questions can sometimes result in incorrect answers. With PET-Select, we can identify instances that are simple enough to not require complex techniques, while still generating the correct answers with fewer tokens."}, {"title": "5.3 RQ3. How is PET-Select able to select an appropriate technique for each query?", "content": "In this section, we perform quantitative and qualitative analyses to assess PET-Select's ability to select the most appropriate technique for each question."}, {"title": "5.3.1 Quantitative Analysis", "content": "As mentioned in Experimental Setup, we utilize two metrics, Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (nDCG), to evaluate PET-Select's recommendation ability. In  we present various selection methods' effectiveness measured by MRR and nDCG. Since we applied 5-fold cross-validation the MRR and nDCG values are the average results from the test set across five folds.\nWithout contrastive learning, PET-Select W/o CL achieves a high MRR value across all experiments. This occurs because the selection model consistently chooses Zero-shot as the appropriate technique. As a result, PET-Select W/o CL tends to perform well in MRR, since Zero-shot is often the most suitable technique for questions it answers correctly. However, a higher MRR score does not necessarily indicate that the best technique is selected for every instance. It simply means that for the instances where the selected technique provides a correct answer, the chosen method is likely one of the top-performing options. This is further demonstrated in , where PET-Select without contrastive learning does not achieve the highest accuracy but often uses fewer tokens than other techniques.\nOn the other hand, PET-Select consistently achieves the highest performance with respect to nDCG metric. This indicates that it can reliably select techniques that lead to correct answers. Although PET-Select falls short on the MRR metric, meaning it doesn't always choose the most appropriate technique for every instance, the selected PET still generates the correct code that passes all test cases. This is evidenced in , where PET-Select outperforms other approaches in terms of accuracy across all experiments. This result indicates that PET-Select is effective in selecting the correct technique that is capable of generating the correct code."}, {"title": "5.3.2 Qualitative Analysis", "content": "This section aims to provide some additional support for the experimental results by analyzing the queries that were only answered correctly by Zero-shot (our most simple PET) and successfully selected by PET-Select. Conversely, we also examined the queries that were only answered correctly by Self-debug (our most complex PET) and were likewise successfully selected by PET-Select. The purpose of these analyses is to provide additional examples that explain the reason why PET-Select is successful in selecting the correct PET in the previous experiments.\n lists some example instances in the MBPP dataset. For instance, questions containing the term 'nested' (numbers 1-3 in will likely require complex code as it will likely involves iterative loops. Complex PETs such as Self-debug are more likely to generate the correct answer. while basic techniques such as Zero-shot tend to answer incorrectly. PET-Select successfully selects the appropriate technique between Zero-shot and Self-debug, indicating that it learns to recognize such keywords in the queries. By placing sentences containing the word \u2018nested' closer together in the embedding space, PET-Select is able to classify them and select the correct PETs.\nIn contrast, sentences that do not contain specific keywords are pushed further away from those that do. As a result, PET-Select will select relatively basic techniques for those questions. For example, the queries 4-5 in  are also related to the List Processing tasks, they only require a single loop to solve. In this case, Zero-shot is a more appropriate PET while Self-debug is too complex and sub-optimal. Since those questions do not contain the specific keywords that indicate complex problems (e.g., nested), PET-Select selects Zero-shot instead of Self-debug as the appropriate technique. The above examples demonstrate that PET-Select can effectively select the appropriate technique based on code complexity predictions derived from keywords in the queries with the help of contrastive learning. By selecting simpler PET when appropriate, PET-Select not only performs well in all cases but also reduces the overall number of tokens required when compared to complex state-of-the-art PETs such as Self-debug."}, {"title": "6 RELATED WORK", "content": ""}, {"title": "6.1 Code Complexity Prediction", "content": "Code complexity prediction has emerged as a key area of focus in recent research, with various approaches leveraging machine learning and deep learning techniques. A notable advancement is the application of deep learning models, such as hierarchical Transformers, which process method-level code snippets and aggregate them into class-level embeddings [18]. These models excel in handling longer code sequences, surpassing previous methods through advanced multi-level pre-training objectives that enhance the model's understanding of complexity-related features.\nAdditionally, studies have explored the effectiveness of GPT-3-based models like GitHub Copilot, highlighting both their strengths and limitations in zero-shot complexity prediction [43]. While Copilot performs well with linear complexities, specialized deep learning models demonstrate superior overall accuracy."}, {"title": "6.2 Automated Prompt Engineering", "content": "Automated prompt engineering is an emerging method to adapt large language models (LLMs) for specific tasks by optimizing prompts without altering the model's core parameters. Techniques like AutoPrompt [41] use gradient-guided search to create prompts for tasks such as sentiment analysis and natural language inference, achieving results comparable to state-of-the-art models without additional fine-tuning. Methods such as prompt tuning [26] and prefix-tuning [27] further improve model efficiency by learning task-specific prompts while keeping the language model frozen, significantly reducing the number of tunable parameters. Additionally, approaches like Prompt-OIRL [44] optimize arithmetic reasoning through offline inverse reinforcement learning, offering cost-effective and scalable prompt recommendations. Although these automated prompt engineering approaches optimize the prompt without executing large language models (LLMs), they do not account for the iterative interaction with the models.\nHowever, these automated prompt engineering methods focus on optimizing a single prompt without accounting for iterative interactions with LLMs throughout the process. In contrast, PET-Select addresses this limitation by incorporating iterative interaction techniques to select the most suitable prompting strategies for code generation tasks."}, {"title": "7 THREATS TO VALIDITY", "content": ""}, {"title": "7.1 Internal validity", "content": "Our code has been thoroughly reviewed to ensure the implementation is correct, and we have confirmed that the questions in the testing dataset are not present in the question base. We also carefully craft our prompts for each prompting technique, adhering closely to the guidelines outlined in the original paper for each method. However, the way prompts and examples are crafted may influence the performance of each technique, which in turn can affect the results of PET-Select."}, {"title": "7.2 External validity", "content": "In our experiment, we use two of the most widely recognized benchmark datasets for code generation, MBPP and HumanEval, to demonstrate the effectiveness of PET-Select, which is primarily designed for Python programming. The performance of PET-Select can be different on prompting technique selection for other programming languages. In addition, we incorporate nine fundamental prompting techniques and five representative code complexity metrics across two datasets in our experiments. PET-Select may perform differently with additional techniques, metrics, and data points. Future work is needed to assess the performance of PET-Select using a broader range of techniques, metrics, and datasets."}, {"title": "7.3 Construct validity", "content": "We use MRR, nDCG, pass@k, and token usage calculated by the Tiktoken package to measure the performance of PET-Select. Our approach may have different performance under other metrics. In this work, we assume that code generation questions with similar code complexity scores are semantically equivalent when contrastively training our CodeBERT-based sentence embeddings. Future research is needed to validate this assumption using different metrics or features."}, {"title": "8 CONCLUSION", "content": "In this paper, we introduced PET-Select, a novel system designed to automatically select appropriate prompt engineering techniques (PETs) for code generation tasks based on code complexity predictions. By leveraging contrastive learning and a CodeBERT-based sentence embedding model, PET-Select effectively identifies simpler questions and applies suitable techniques, achieving comparable or higher accuracy with fewer tokens. Our evaluation of the MBPP and HumanEval datasets demonstrates that PET-Select not only enhances performance but also reduces computational costs. Future work will focus on refining the model and exploring its application to other domains."}, {"title": "9 DATA AVAILABILITY", "content": "We release our code and data through the following link:\nhttps://anonymous.4open.science/r/Prompt-Selection-B47F."}]}