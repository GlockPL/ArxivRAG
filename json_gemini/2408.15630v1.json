{"title": "CodeSift: An LLM-Based Reference-Less Framework for Automatic Code Validation", "authors": ["Pooja Aggarwal", "Oishik Chatterjee", "Ting Dai", "Prateeti Mohapatra", "Brent Paulovicks", "Brad Blancett", "Arthur De Magalhaes"], "abstract": "The advent of large language models (LLMs) has greatly facilitated code generation, but ensuring the functional correctness of generated code remains a challenge. Traditional validation methods are often time-consuming, error-prone, and impractical for large volumes of code. We introduce CodeSift, a novel framework that leverages LLMs as the first-line filter of code validation without the need for execution, reference code, or human feedback, thereby reducing the validation effort. We assess the effectiveness of our method across three diverse datasets encompassing two programming languages. Our results indicate that CodeSift outperforms state-of-the-art code evaluation methods. Internal testing conducted with subject matter experts reveals that the output generated by CodeSift is in line with human preference, reinforcing its effectiveness as a dependable automated code validation tool.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's software development landscape, the proliferation of large language models (LLMs) has vastly accelerated the pace and adoption of code generation. For instance, with the increase in IT deployments and cloud adoption, IT operations (ITOps) that are critical for maintaining reliable and resilient systems, can extend the use of AI and automation through code generation to reduce mean time to resolve incidents. When an incident occurs, Site Reliability Engineers (SREs) are tasked with diagnosing the fault and finding a resolution. Traditionally, SREs would typically write code scripts manually to carry out these resolutions. Recently, with the advent of LLMs, code generation capabilities are now used to assist in incident remediation and automation.\nThe widespread adoption of generated code brings a need for robust validation mechanisms to ensure its functional correctness before deployment in production systems. Deploying unvalidated code directly into these systems can lead to severe consequences, including performance degradation and critical system failures. While manual inspection is the gold standard solution, the sheer volume of generated code makes it labor-intensive and time-consuming.\nRecent approaches to evaluate generated code either use reference codes to compare with or are based on runtime execution correctness [1]\u2013[6]. However, these approaches come with the need for availability of reference codes or test cases and are limited by scalability constraints with increased workload.\nTo address these issues, we propose a novel framework, called CodeSift, to act as a first-line filter for generated code evaluation. This framework is designed to validate the functional correctness of generated code using LLMs as evaluators. By harnessing the power of advanced natural language understanding, semantic comprehension, and code analysis capabilities offered by LLMs, CodeSift offers an efficient solution to the challenge of code validation, significantly reducing the burden on human validators and streamlining the code validation process. Code snippets that pass the first-line validation can undergo further scrutiny by human experts or be tested in a controlled development environment, thus maximizing efficiency and minimizing the risk of deploying faulty code. Our contributions are:\n\u2022 We propose CodeSift, an automatic and effective validation framework for assessing the quality of generated code, eliminating the need for execution, reference codes, and test cases.\n\u2022 We contribute a novel Bash dataset consisting of 100 unique tasks with corresponding test cases and example ground truth code.\n\u2022 We showcase CodeSift's efficacy as a primary filter for automated script validation across various datasets including opensource Python dataset such as HumanEval [7] and MBPP [8] and our manually curated Bash dataset, through a comparative analysis with test case-based validation.\n\u2022 We present how CodeSift can serve as a valuable metric for ranking various code generation models, aiding in model selection and performance evaluation in the absence of ground truth reference codes or test cases.\n\u2022 Results from user studies indicate that CodeSift's evaluations closely align with human experts' judgments, demonstrating its reliability and effectiveness in assessing the functional correctness of generated code."}, {"title": "II. RELATED WORK", "content": "Many approaches for generated code evaluation have been proposed in the literature along four dimensions: (1) match-based (2) embedding-based, (3) execution-based, and (4) prompt-based.\nPrior work such as BLEU, ROGUE, and ChrF assessed the quality of the generated code relying on token matching with the reference code. BLEU score [9] and ROUGE score [10] calculate the precision and recall of word n-grams in the machine-generated code by comparing them to the reference code, respectively. ChrF [11] calculates the f-scores of the character n-grams between the generated code and the reference code. In programming languages, even unrelated pieces of code can share many common n-grams due to syntactic verbosity and coding conventions. Relying solely on n-gram matching fails to distinguish between similar code examples and those merely written using the same vocabulary. CrystalBLEU [1] addresses the issue of solely relying on n-gram matching by minimizing the noise caused by trivially shared n-grams, such as '(' and ','. CodeBLEU [2] assesses deep semantic similarities by incorporating weighted n-gram matching, syntactic AST matching, and semantic dataflow matching.\nEmbedding-based evaluation approaches for example, CodeBertScore [3] measure the similarity between generated and reference code by summing the cosine similarities between their token embeddings and by incorporating contextual information.\nBoth match-based and embedding-based approaches demonstrate a poor correlation with human judgment or runtime execution validation. Moreover, these approaches only work when reference code is available, hindering their practicability.\nExecution-based evaluation approaches evaluate code quality based on runtime execution correctness. SPoC [12] evaluates functional correctness using the pass@k metric. For each problem, k code samples are generated, and the problem is considered solved if any sample passes the unit tests. Codex [7] tackles the high variance issue in the pass@k metric by generating \\(n \\geq k\\) samples per task. It counts the number of correct samples \\(c < n\\) that pass unit tests and calculates the unbiased estimator. APPS [13] and CodeScore [4] evaluate functional correctness using the PassRatio metric, calculating the average fraction of test cases passed.\nExecution-based code evaluation approaches require running the generated code against a predefined set of test cases and comparing the output with expected results. While effective, they are inherently limited by scalability constraints. These limitations are mitigated by automated test case generation techniques [5], [6], [14]\u2013[17]. However, validating the generated test cases increases the workload, rendering the entire execution-based approach impractical. Moreover, running model-generated code carries security risks and requires execution within a secure sandbox, which introduce additional technical complexity.\nPrompt-based code evaluation approaches assessed the quality of generated code using LLMs with single answer grading, pairwise comparison, reference-guided grading, and chain-of-thoughts [18]\u2013[20]. They assign rating scores for evaluating the generated code by comparing it with the given task description or/and with the reference ground truth code. They conduct text-to-code or code-to-code comparisons. Inspired by those methods, in contrast to the above approaches, we use LLMs as a first-line filter to validate the functional correctness of the generated code by translating code into text dimension first and conducting the text-to-text comparison."}, {"title": "III. BASH DATASET", "content": "One of the emerging usages of CodeLLMs in AIOps is to automate incident remediation using automatic script generation for recommended actions. Bash is widely used by SREs for incident remediation scripting, primarily due to the prevalence of Linux-based systems. Hence it is crucial to evaluate the performance of CodeLLMs on the task of generating bash scripts. To evaluate LLMs on the task of code"}, {"title": "IV. CODESIFT FRAMEWORK", "content": "In this section, we outline the methodology of our code validation framework. Our objective is to check whether the generated code is correct and its functionally aligned with the desired behavior specified by the given task\u00b9. To accomplish this, CodeSift acts as a first-line filter to identify functionally incorrect code. This approach enables automated validation of code functionality without the need for execution or reference code, thereby reducing the validation effort and accelerating the adoption of generated code in real-world applications.\nThe process is illustrated in Figure 1. CodeSift consists of two primary evaluation components: (1) syntax correctness evaluation and (2) semantic correctness evaluation.\nIn the Syntax Correctness phase, we utilize pre-built syntax checkers such as ShellCheck [23] and PyLint [24] to detect any syntactic errors in the generated code. If an error is detected, the LLM model is prompted with both the error message and the previously generated code. Our observations indicate that while the majority of generated code is syntactically correct, in cases where errors arise, LLM models can often rectify them when provided with the error message.\nAfter successfully passing syntax evaluation, the code proceeds to undergo semantic correctness evaluation, which comprises of three main phases. In each of these phases, we utilize the same LLM. Below, we elaborate each phase of semantic validation.\n\u2022 Code-to-functionality: In this phase, the generated code is transformed into a text representation, referred to as code-func, which encapsulates the core functionality of the code. To accomplish this, we leverage a pre-trained LLM, capable of generating the primary purpose and operational logic\n1In this context, \"task\" refers to programming problems.\n\u2022 Similarity Analysis: Next, we utilize the same pre-trained LLM to assess the semantic similarity between the code-func and the task description. The LLM is prompted to determine whether the code-func can accomplish the intended behavior specified in the task. Prompt 2 in Figure 1 is utilized to assess the similarity between the task and code-func. If the similarity analysis indicates that both the code-func and the task achieve the same goal, then the code will be labeled as functionally correct in this phase.\n\u2022 Difference Analysis: In the third phase, we conduct a difference analysis by instructing the same LLM model to identify and examine any discrepancies between the code-func and the task description. Prompt 3, shown in Figure 1, is utilized to discern differences within both texts. This helps in identifying semantic variations between the two texts, enabling the detection of potential discrepancies or inconsistencies between the expected and generated output. If the difference analysis indicates that the code-func and task produce identical outputs and exhibit no discrepancies, the code will be labeled as functionally correct during this phase. This phase complements the similarity analysis phase by providing additional insights into the functional correctness of the code, ultimately contributing to a more comprehensive evaluation.\n\u2022 Ensemble Synthesis: The ensemble approach involves integrating the outputs of both similarity analysis and difference analysis to arrive at a more comprehensive evaluation of the generated code. The generated code is marked as correct by the CodeSift method only if the similarity analysis indicates that the code-func and task are similar, and the difference analysis finds no disparities between them. Consequently, we synthesize the results from both analyses to comprehensively evaluate the generated code, thereby determining its functional correctness. If either analysis indicates a deviation from the task, the code is labeled as functionally incorrect.\nIt's important to note that although the model receives identical inputs for both prompts, its focus varies depending on the objective. When assessing alignment with the task, the model emphasizes more on similarities and sometimes might disregard subtle differences. Conversely, when tasked with explicitly identifying differences, the model effectively does so. Therefore, capturing similarities and differences explicitly is crucial for accurately labeling the generated code, achieved by combining the outputs of both analyses."}, {"title": "V. EVALUATION", "content": "Here, we present the data for evaluation and present the performance of our evaluation framework using various models and baselines.\nExperimental Setup\n1) Dataset: We evaluate our framework on three datasets, including HumanEval [7], MBPP+ [25], and Bash (see Section III). The HumanEval dataset consists of 164 Python programming problems while MBPP+, which is a refined subset of the original MBPP dataset [8] by Evalplus [26], contains 399 python problems.\nWe analyze the performance of our framework on the code generated by four prominent models: Starcoder [27], Codellama_34B [28], ChatGPT [29], and Mistral_7B [30]. For the HumanEval dataset, we increase the size for evaluation by sampling 10 solutions for each problem from these models, using a temperature of 0.2 (except for ChatGPT for which the temperature was set to 0.8). This results in a total of 1640 task-code pairs for each model. Similarly, for Bash, we generate 10 scripts for 100 tasks to get a total of 1000. For the MBPP+ dataset, we use greedy decoding to generate 399 task-code pairs.\n2) Task: We evaluate CodeSift on the following two tasks: i) determining the functional correctness of the given code, ii) its utility as a filter in code generation pipeline to reduce incorrect code shown to the user. For the first task, we use accuracy as the metric to compare the performance of CodeSift with baseline methods. Establishing ground truth by marking task-code pairs as correct only if all associated test cases pass. We then calculate the accuracy by comparing the output of CodeSift with this ground truth. For the second task, we report the % of times correct code was shown to the user post-filtering by CodeSift and baseline methods.\n3) Models: We evaluate the performance of three models: Llama2-Chat_70B [31], Mistral_7B [30], and Mixtral_8x7B [32], in CodeSift's framework for functionality generation, similarity analysis and difference analysis. All the models use the sampling decoding method with 0.6 temperature and 1.2 repetition penalty.\n4) Baselines: We compare CodeSift with two baseline methods: ICE-Score and Reference Grading. ICE-Score determines code correctness by directly comparing it with the specified task using a Large Language Model [20]. Reference Grading (as described in the LLM-as-a-judge framework [18]) also considers a correct reference code in addition to the code and task to improve accuracy. For both methods, we use prompts specified in their respective frameworks. ICE-Score assigns scores from 0 to 4, where 4 indicates functional correctness, while Reference Grading assigns a score of 10 for perfect alignment with the task. We classify codes as correct based on these scoring criteria.\nResults\nThe results of our experiments are structured around several key research questions (RQ).\nRQ1: How effective is CodeSift at assessing the accuracy of generated code?\nCodeSift's effectiveness in assessing the accuracy of generated code is evident from the comparison presented in Table II. CodeSift provides insights into the fact that LLMs excel in text-to-text comparisons rather than text-to-code evaluations. When utilizing Mistral as the backbone, CodeSift outperforms"}, {"title": "RQ2: Is there alignment between the assessments made by CodeSift and human preference in evaluating the correctness of generated code?", "content": "To evaluate the practical utility of Code-Sift, it was deployed in an internal offline code generation pipeline for catalog creation of automation scripts for a widely-used AI observability platform. We conducted a user study involving 3 Subject Matter Experts (SMEs) who were tasked with generating and evaluating bash scripts using CodeSift. The experts were asked to assess the code functionality generated by the CodeSift along with the validation output. Feedback was received on 105 instances. The SMEs agreed with the code functionality output 78% of the time and agreed with the validation output 83% of the time. This study shows the effectiveness of CodeSift as a code evaluator in real-world applications but also helps us evaluate the quality of the different key components of CodeSift - functionality generation and functional validation, through user feedback."}, {"title": "RQ3: How does CodeSift's performance change with dif-", "content": "ferent LLMs as evaluators, and what factors influence performance variations among LLMs?\nOur experiment results reveal performance disparities among three LLMs as evaluators: Mistral, Mixtral, and Llama2-Chat, as demonstrated in Table II. Specifically, the Mistral model consistently outperforms the other models across the HumanEval and Bash datasets. However, in the MBPP dataset, Mixtral's performance is superior. Notably, when considering precision (in Table III), CodeSift with Mixtral ensures that users are presented with fewer incorrect cases, thereby enhancing the overall user experience. While reference grading exhibits high precision, it suffers from low recall, resulting in few cases being labeled as correct. In contrast, CodeSift (with Mixtral), compared to ICE-Score, which also does not consider reference code, demonstrates 5% to 12% better precision across most scenarios, except in the case of Starcoder-MBPP. Additionally, when considering the performance of the individual phases of CodeSift, namely similarity analysis and difference analysis, we observe similar performance between Mixtral and Mistral as shown in Table II. However, Mistral's similarity analysis can sometimes effectively highlight dissimilarities between the code functionality and task compared to other models. For instance, in the first example in Table IV, only similarity analysis using Mistral model accurately identified dissimilarity between the task and code-func whereas using other models for similarity analysis labeled it similar. We observed that the evaluator LLMs exhibit no bias, as evidenced by Mistral LLM outperforming others on codes generated by the Mistral model. This indicates the model's ability to detect errors in generated code, even when the code generation and evaluation models are the same. The accuracy of CodeSift with Llama2-Chat as the backbone model is generally lower than CodeSift with Mistral and Mixtral. However, in cases where code is generated by the GPT3.5 model, CodeSift-Llama's similarity analysis phase performs notably well due to the higher execution accuracy of the GPT3.5 model. Consequently, CodeSift-Llama tends to label most generated code as correct, potentially overlooking discrepancies between the code functionality and the task requirements. This may result in lower overall accuracy, particularly in detecting incorrect code instances."}, {"title": "RQ4: Can CodeSift effectively detect functional errors in the generated code, including logical errors, syntax errors, and missing functions?", "content": "One interesting finding of our analysis is that LLMs can sometimes detect issues with code validity that current validation schemes (such as unit tests) may fail to capture.This can be seen in the second example in Table IV. Here the task mentions \u201cdo not use recursion\". The test cases cannot capture this aspect and hence the generated code using recursion is deemed correct by the execution evaluation. However, Code-Sift accurately detects the use of recursion in the generated code, correctly determining that it does not fulfill the intended task by stating that \"No To efficiently compute the n-th element of the fib4 sequence without using recursion, one would need to implement an iterative solution instead.\u201d and suggests using \u201citerative solution instead\". For bash, in the fifth example of Table IV, CodeSift was able to recognise that the 5th row of df command's out represents used % rather than the actual value and was able to correctly mark it as incorrect.\nOne of the reasons for CodeSift's failure to evaluate the code correctness is due to incorrect functionality generation. For example, in the third entry of Table IV, CodeSift fails to provide the correct output due to a discrepancy between the code-func and the generated code. This leads to false negatives and hinders CodeSift's overall accuracy. Another source of inaccuracy occurs when the generated code is very close to the actual intended task with minor discrepencies. For example, in the sixth entry of table IV, the LLM is unable to determine that the task has asked to copy from dir1 to dir2 (both directories are in the same current directory) where as the code copies from dir1 to dir2 which results in it claiming that the code is correct whereas the execution fails due to incorrect directory.\""}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "We introduced a novel approach using LLMs for automatic code evaluation, demonstrating its usefulness in the absence of reference code and test case. Our experimental results demonstrate the effectiveness of CodeSift across various datasets and programming languages, outperforming baseline approaches such as ICE-Score and reference-based grading. Notably, CodeSift's ensemble approach, incorporating both similarity and difference analysis phases, yields the most reliable outcomes, providing a comprehensive assessment of code functionality. In the future, we plan to refine and expand the capabilities of CodeSift by exploring its performance across a broader range of programming languages. Additionally, we aim to enhance the interpretability and transparency of the framework's decision-making process. Explaining the rationale behind the framework's assessments in a more intuitive and human-understandable manner can increase trust in its recommendations, facilitating its adoption in real-world software development workflows. We'll also leverage explanations generated by the framework to offer feedback to the code generation model, ensuring better alignment with task specifications."}, {"title": "VII. LIMITATIONS", "content": "Our framework heavily relies on the functionality generated for a given code snippet. If this phase fails to capture the essential elements of the code, there is a higher likelihood of incorrectly labelling the generated code. Although Mistral and Mixtral models demonstrate proficiency in capturing essential functionality, instances of incorrect code functionality generation can still occur. Also since the outputs of the models are verbose, it is not always possible to automatically detect the correctness of the code from the similarity/difference analysis. Additionally, the explanations provided by the similarity and difference analysis phases may not always be entirely accurate upon manual inspection. Future improvements to these explanations could enhance the framework's efficacy and provide valuable feedback to the code generation model."}]}