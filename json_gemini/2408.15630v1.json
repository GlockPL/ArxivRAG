{"title": "CodeSift: An LLM-Based Reference-Less Framework for Automatic Code Validation", "authors": ["Pooja Aggarwal", "Oishik Chatterjee", "Ting Dai", "Prateeti Mohapatra", "Brent Paulovicks", "Brad Blancett", "Arthur De Magalhaes"], "abstract": "The advent of large language models (LLMs) has greatly facilitated code generation, but ensuring the functional correctness of generated code remains a challenge. Traditional validation methods are often time-consuming, error-prone, and impractical for large volumes of code. We introduce CodeSift, a novel framework that leverages LLMs as the first-line filter of code validation without the need for execution, reference code, or human feedback, thereby reducing the validation effort. We assess the effectiveness of our method across three diverse datasets encompassing two programming languages. Our results indicate that CodeSift outperforms state-of-the-art code evaluation methods. Internal testing conducted with subject matter experts reveals that the output generated by CodeSift is in line with human preference, reinforcing its effectiveness as a dependable automated code validation tool.", "sections": [{"title": "I. INTRODUCTION", "content": "In today's software development landscape, the proliferation of large language models (LLMs) has vastly accelerated the pace and adoption of code generation. For instance, with the increase in IT deployments and cloud adoption, IT operations (ITOps) that are critical for maintaining reliable and resilient systems, can extend the use of AI and automation through code generation to reduce mean time to resolve incidents. When an incident occurs, Site Reliability Engineers (SREs) are tasked with diagnosing the fault and finding a resolution. Traditionally, SREs would typically write code scripts manually to carry out these resolutions. Recently, with the advent of LLMs, code generation capabilities are now used to assist in incident remediation and automation.\nThe widespread adoption of generated code brings a need for robust validation mechanisms to ensure its functional correctness before deployment in production systems. Deploying unvalidated code directly into these systems can lead to severe consequences, including performance degradation and critical system failures. While manual inspection is the gold standard solution, the sheer volume of generated code makes it labor-intensive and time-consuming.\nRecent approaches to evaluate generated code either use reference codes to compare with or are based on runtime execution correctness [1]-[6]. However, these approaches come with the need for availability of reference codes or test cases and are limited by scalability constraints with increased workload.\nTo address these issues, we propose a novel framework, called CodeSift, to act as a first-line filter for generated code evaluation. This framework is designed to validate the functional correctness of generated code using LLMs as evaluators. By harnessing the power of advanced natural language understanding, semantic comprehension, and code analysis capabilities offered by LLMs, CodeSift offers an efficient solution to the challenge of code validation, significantly reducing the burden on human validators and streamlining the code validation process. Code snippets that pass the first-line validation can undergo further scrutiny by human experts or be tested in a controlled development environment, thus maximizing efficiency and minimizing the risk of deploying faulty code. Our contributions are:\n\u2022 We propose CodeSift, an automatic and effective validation framework for assessing the quality of generated code, eliminating the need for execution, reference codes, and test cases.\n\u2022 We contribute a novel Bash dataset consisting of 100 unique tasks with corresponding test cases and example ground truth code.\n\u2022 We showcase CodeSift's efficacy as a primary filter for automated script validation across various datasets including opensource Python dataset such as HumanEval [7] and MBPP [8] and our manually curated Bash dataset, through a comparative analysis with test case-based validation.\n\u2022 We present how CodeSift can serve as a valuable metric for ranking various code generation models, aiding in model selection and performance evaluation in the absence of ground truth reference codes or test cases.\n\u2022 Results from user studies indicate that CodeSift's evaluations closely align with human experts' judgments, demonstrating its reliability and effectiveness in assessing the functional correctness of generated code."}, {"title": "II. RELATED WORK", "content": "Many approaches for generated code evaluation have been proposed in the literature along four dimensions: (1) match-based (2) embedding-based, (3) execution-based, and (4) prompt-based.\nPrior work such as BLEU, ROGUE, and ChrF assessed the quality of the generated code relying on token matching with the reference code. BLEU score [9] and ROUGE score [10] calculate the precision and recall of word n-grams in the machine-generated code by comparing them to the reference code, respectively. ChrF [11] calculates the f-scores of the character n-grams between the generated code and the reference code. In programming languages, even unrelated pieces of code can share many common n-grams due to syntactic verbosity and coding conventions. Relying solely on n-gram matching fails to distinguish between similar code examples and those merely written using the same vocabulary. CrystalBLEU [1] addresses the issue of solely relying on n-gram matching by minimizing the noise caused by trivially shared n-grams, such as '(' and ','. CodeBLEU [2] assesses deep semantic similarities by incorporating weighted n-gram matching, syntactic AST matching, and semantic dataflow matching.\nEmbedding-based evaluation approaches for example, CodeBertScore [3] measure the similarity between generated and reference code by summing the cosine similarities between their token embeddings and by incorporating contextual information.\nBoth match-based and embedding-based approaches demonstrate a poor correlation with human judgment or runtime execution validation. Moreover, these approaches only work when reference code is available, hindering their practicability.\nExecution-based evaluation approaches evaluate code quality based on runtime execution correctness. SPoC [12] evaluates functional correctness using the pass@k metric. For each problem, k code samples are generated, and the problem is considered solved if any sample passes the unit tests.\nCodex [7] tackles the high variance issue in the pass@k metric by generating n \u2265 k samples per task. It counts the number of correct samples c < n that pass unit tests and calculates the unbiased estimator. APPS [13] and CodeScore [4] evaluate functional correctness using the PassRatio metric, calculating the average fraction of test cases passed.\nExecution-based code evaluation approaches require running the generated code against a predefined set of test cases and comparing the output with expected results. While effective, they are inherently limited by scalability constraints. These limitations are mitigated by automated test case generation techniques [5], [6], [14]\u2013[17]. However, validating the generated test cases increases the workload, rendering the entire execution-based approach impractical. Moreover, running model-generated code carries security risks and requires execution within a secure sandbox, which introduce additional technical complexity.\nPrompt-based code evaluation approaches assessed the quality of generated code using LLMs with single answer grading, pairwise comparison, reference-guided grading, and chain-of-thoughts [18]-[20]. They assign rating scores for evaluating the generated code by comparing it with the given task description or/and with the reference ground truth code. They conduct text-to-code or code-to-code comparisons. Inspired by those methods, in contrast to the above approaches, we use LLMs as a first-line filter to validate the functional correctness of the generated code by translating code into text dimension first and conducting the text-to-text comparison."}, {"title": "III. BASH DATASET", "content": "One of the emerging usages of CodeLLMs in AIOps is to automate incident remediation using automatic script generation for recommended actions. Bash is widely used by SREs for incident remediation scripting, primarily due to the prevalence of Linux-based systems. Hence it is crucial to evaluate the performance of CodeLLMs on the task of generating bash scripts. To evaluate LLMs on the task of code generation, people calculate execution accuracy (pass@k) [21] for benchmark datasets across different languages such as Python, Java, and Go [22]. These datasets contain the problem statement and associated test cases. When the given code passes all the test cases for the problem, it is marked as correct. Creating test cases for Bash scripts is challenging because Bash commands frequently alter the system. Therefore, verifying the code's success in completing the given task requires careful inspection of the system's state.\nWe create a new benchmark dataset consisting of 100 tasks for evaluating LLMs on generating bash scripts. The evaluation of bash scripts typically consists of the following steps: 1) Prologue: This step consists of creating a container along with the necessary prerequisites required to evaluate the code. For example, if the task is to copy a file from directory 1 to directory2, a container is created containing directory1 with the file and directory2. 2) Code Execution. 3) Epilogue: In this step, we check for any unnecessary system changes 4) Evaluation: We first check if the code was executed without any errors. Then we check if the code was able to fulfill the intended task. 5) Cleanup: The container is closed and the execution result is returned."}, {"title": "IV. CODESIFT FRAMEWORK", "content": "In this section, we outline the methodology of our code validation framework. Our objective is to check whether the generated code is correct and its functionally aligned with the desired behavior specified by the given task\u00b9. To accomplish this, CodeSift acts as a first-line filter to identify functionally incorrect code. This approach enables automated validation of code functionality without the need for execution or reference code, thereby reducing the validation effort and accelerating the adoption of generated code in real-world applications. The process is illustrated in Figure 1. CodeSift consists of two primary evaluation components: (1) syntax correctness evaluation and (2) semantic correctness evaluation.\nIn the Syntax Correctness phase, we utilize pre-built syntax checkers such as ShellCheck [23] and PyLint [24] to detect any syntactic errors in the generated code. If an error is detected, the LLM model is prompted with both the error message and the previously generated code. Our observations indicate that while the majority of generated code is syntactically correct, in cases where errors arise, LLM models can often rectify them when provided with the error message.\nAfter successfully passing syntax evaluation, the code proceeds to undergo semantic correctness evaluation, which comprises of three main phases. In each of these phases, we utilize the same LLM. Below, we elaborate each phase of semantic validation.\n\u2022 Code-to-functionality: In this phase, the generated code is transformed into a text representation, referred to as code-func, which encapsulates the core functionality of the code. To accomplish this, we leverage a pre-trained LLM, capable of generating the primary purpose and operational logic\n\u2022 Similarity Analysis: Next, we utilize the same pre-trained LLM to assess the semantic similarity between the code-func and the task description. The LLM is prompted to determine whether the code-func can accomplish the intended behavior specified in the task. Prompt 2 in Figure 1 is utilized to assess the similarity between the task and code-func. If the similarity analysis indicates that both the code-func and the task achieve the same goal, then the code will be labeled as functionally correct in this phase.\n\u2022 Difference Analysis: In the third phase, we conduct a difference analysis by instructing the same LLM model to identify and examine any discrepancies between the code-func and the task description. Prompt 3, shown in Figure 1, is utilized to discern differences within both texts. This helps in identifying semantic variations between the two texts, enabling the detection of potential discrepancies or inconsistencies between the expected and generated output. If the difference analysis indicates that the code-func and task produce identical outputs and exhibit no discrepancies, the code will be labeled as functionally correct during this phase. This phase complements the similarity analysis phase by providing additional insights into the functional correctness of the code, ultimately contributing to a more comprehensive evaluation.\n\u2022 Ensemble Synthesis: The ensemble approach involves integrating the outputs of both similarity analysis and difference analysis to arrive at a more comprehensive evaluation of the generated code. The generated code is marked as correct by the CodeSift method only if the similarity analysis indicates that the code-func and task are similar, and the difference analysis finds no disparities between them. Consequently, we synthesize the results from both analyses to comprehensively evaluate the generated code, thereby determining its functional correctness. If either analysis indicates a deviation from the task, the code is labeled as functionally incorrect.\nIt's important to note that although the model receives identical inputs for both prompts, its focus varies depending on the objective. When assessing alignment with the task, the model emphasizes more on similarities and sometimes might disregard subtle differences. Conversely, when tasked with explicitly identifying differences, the model effectively does so. Therefore, capturing similarities and differences explicitly is crucial for accurately labeling the generated code, achieved by combining the outputs of both analyses."}, {"title": "V. EVALUATION", "content": "Here, we present the data for evaluation and present the performance of our evaluation framework using various models and baselines.\n1) Dataset: We evaluate our framework on three datasets, including HumanEval [7], MBPP+ [25], and Bash (see Section III). The HumanEval dataset consists of 164 Python programming problems while MBPP+, which is a refined subset of the original MBPP dataset [8] by Evalplus [26], contains 399 python problems.\nWe analyze the performance of our framework on the code generated by four prominent models: Starcoder [27], Codellama_34B [28], ChatGPT [29], and Mistral_7B [30]. For the HumanEval dataset, we increase the size for evaluation by sampling 10 solutions for each problem from these models, using a temperature of 0.2 (except for ChatGPT for which the temperature was set to 0.8). This results in a total of 1640 task-code pairs for each model. Similarly, for Bash, we generate 10 scripts for 100 tasks to get a total of 1000. For the MBPP+ dataset, we use greedy decoding to generate 399 task-code pairs.\n2) Task: We evaluate CodeSift on the following two tasks: i) determining the functional correctness of the given code, ii) its utility as a filter in code generation pipeline to reduce incorrect code shown to the user. For the first task, we use accuracy as the metric to compare the performance of CodeSift with baseline methods. Establishing ground truth by marking task-code pairs as correct only if all associated test cases pass. We then calculate the accuracy by comparing the output of CodeSift with this ground truth.\n3) Models: We evaluate the performance of three models: Llama2-Chat_70B [31], Mistral_7B [30], and Mixtral_8x7B [32], in CodeSift's framework for functionality generation, similarity analysis and difference analysis.\n4) Baselines: We compare CodeSift with two baseline methods: ICE-Score and Reference Grading.\nB. Results\nThe results of our experiments are structured around several key research questions (RQ).\nCodeSift's effectiveness in assessing the accuracy of generated code is evident from the comparison presented in Table II. CodeSift provides insights into the fact that LLMs excel in text-to-text comparisons rather than text-to-code evaluations."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "We introduced a novel approach using LLMs for automatic code evaluation, demonstrating its usefulness in the absence of reference code and test case. Our experimental results demonstrate the effectiveness of CodeSift across various datasets and programming languages, outperforming baseline approaches such as ICE-Score and reference-based grading. Notably, CodeSift's ensemble approach, incorporating both similarity and difference analysis phases, yields the most reliable outcomes, providing a comprehensive assessment of code functionality. In the future, we plan to refine and expand the capabilities of CodeSift by exploring its performance across a broader range of programming languages. Additionally, we aim to enhance the interpretability and transparency of the framework's decision-making process. Explaining the rationale behind the framework's assessments in a more intuitive and human-understandable manner can increase trust in its recommendations, facilitating its adoption in real-world software development workflows. We'll also leverage explanations generated by the framework to offer feedback to the code generation model, ensuring better alignment with task specifications."}, {"title": "VII. LIMITATIONS", "content": "Our framework heavily relies on the functionality generated for a given code snippet. If this phase fails to capture the essential elements of the code, there is a higher likelihood of incorrectly labelling the generated code. Although Mistral and Mixtral models demonstrate proficiency in capturing essential functionality, instances of incorrect code functionality generation can still occur. Also since the outputs of the models are verbose, it is not always possible to automatically detect the correctness of the code from the similarity/difference analysis. Additionally, the explanations provided by the similarity and difference analysis phases may not always be entirely accurate upon manual inspection. Future improvements to these explanations could enhance the framework's efficacy and provide valuable feedback to the code generation model."}]}