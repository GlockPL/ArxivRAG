{"title": "TowerDebias: A Novel Debiasing Method based on the Tower Property", "authors": ["Norman Matloff", "Aditya Mittal", "Norman Matloff"], "abstract": "Decision-making processes have increasingly come to rely on sophisticated machine learning tools, raising concerns about the fairness of their predictions with respect to any sensitive groups. The widespread use of commercial black-box machine learning models necessitates careful consideration of their legal and ethical implications on consumers. In situations where users have access to these \"black-box\" models, a key question emerges: how can we mitigate or eliminate the influence of sensitive attributes, such as race or gender? We propose towerDebias (tDB), a novel approach designed to reduce the influence of sensitive variables in predictions made by black-box models. Using the Tower Property from probability theory, tDB aims to improve prediction fairness during the post-processing stage in a manner amenable to the Fairness-Utility Tradeoff. This method is highly flexible, requiring no prior knowledge of the original model's internal structure, and can be extended to a range of different applications. We provide a formal improvement theorem for tDB and demonstrate its effectiveness in both regression and classification tasks, underscoring its impact on the fairness-utility tradeoff.", "sections": [{"title": "1. Introduction", "content": "In recent years, the rapid development of machine learning algorithms and their extensive commercial applications have become increasingly relevant across domains like cybersecurity, healthcare, e-commerce, and more (Sarker, 2021). As these models increasingly guide"}, {"title": "1.1 The Setting", "content": "We consider prediction of a variable Y from a feature vector X and a vector of sensitive variables S. The target Y may be either numeric (in a regression setting) or dichotomous (in a two-class classification setting where Y = 1 or Y = 0). The m-class case can be handled using m dichotomous variables with regards to the material presented here.\nConsider an algorithm developed by a vendor, such as COMPAS, which was trained on data (Yi, Xi, Si), i = 1,...,n. This data can be assumed to originate from some specific data-generating process, with the algorithm's initial objective being to estimate E(Y|X, S). However, the client who purchased the algorithm wishes to exclude S and instead estimate E(Y|X).\nIn the regression setting, we assume squared prediction error loss to minimize error. In the classification setting, we define:\nE(Y|X, S) = P(Y = 1|X, S)\nwhere the predicted class label is given by:\narg max P(Y = i|X, S)\ni=0,1\nThis approach minimizes overall misclassification rate by selecting the class with the highest predicted probability.\nSeveral potential use cases for employing tDB can be identified:\n(a) The client has no knowledge of the internal structure of the black-box algorithm and lacks access to the original training data. Thus, the client will need to gather their own data from new cases that arise during the tool's initial use.\n(b) Client has no knowledge of the inner workings of the black-box algorithm but is given the training data.\n(c) User of the algorithm, potentially even the original developer, knows the black-box's details and possesses the training data. He/she is satisfied with the performance of the algorithm, but desires a quick, simple way to remove the influence of S in its predictions.\nIn each setting, the individual aims to predict new cases using only X. The client either does not know S or chooses to disregard it. In other words, although the algorithm provides estimates of E(Y|X, S), the goal is to use E(Y|X) instead as the prediction instead. In other words, even though the algorithm gives us an estimated E(Y|X, S), we wish to instead use estimated E(Y|X) as our prediction. In this paper, we present tDB as an innovative approach to modify the predictions of the original algorithm, bypassing the \"black box\" nature of the model."}, {"title": "1.2 Introducing the Fairness-Utility Tradeoff", "content": "Many commercial black-box models may include sensitive variables, which raises significant ethical and legal concerns. In the pursuit of fairness, a fundamental Fairness-Utility tradeoff emerges: a delicate balance between fairness and predictive accuracy (Gu et al., 2022; Sun et al., 2023). This tradeoff highlights that prioritizing fairness often leads to a reduction in accuracy. However, the extent of this tradeoff is influenced by the specific fairness metrics employed and their implementation details. This paper explores the impact of applying tDB on the fairness-utility tradeoff across different datasets, encompassing both regression and classification tasks."}, {"title": "1.3 Paper Outline", "content": "The paper is organized as follows: Section 2 reviews prior literature on fair machine learning, focusing on relevant methods and proposed fairness metrics; Section 3 introduces the towerDebias algorithm and provides supporting proofs demonstrating fairness improvements; Section 4 presents empirical results of tDB on multiple datasets; and Section 5 concludes with a discussion and future directions."}, {"title": "2. Related Work", "content": "A significant body of literature has been published in the field of algorithmic fairness, focusing on addressing critical social issues related to mitigating historical bias in data-driven applications. For example, Chouldechova (2017) examines the use of Recidivism Prediction Instruments in legal systems, highlighting the potential disparate impact on racial groups. Collaborative efforts from the Human Rights Data Analysis Group and Stanford University have developed additional frameworks for fair modeling (Lum and Johndrow, 2016; Johndrow and Lum, 2019). In particular, significant research has focused on fair binary classification, see: Barocas et al. (2023); Zafar et al. (2019). More recently, Denis et al. (2024) extends these concepts to multi-class classification from a fairness perspective.\nBroadly speaking, fairness criteria can generally be categorized into two measures: group fairness and individual fairness. The central idea behind individual fairness is that similar individuals should be treated similarly (Dwork et al., 2012), though the specific implementation details may vary. One approach to individual fairness is propensity score matching (Karimi et al., 2022). In contrast, group fairness requires predictions to be similar across different groups as defined by the sensitive attributes. Commonly used measures of group fairness include demographic parity and equality of opportunity (Hardt et al., 2016).\nMuch work has been proposed to integrate fairness constraints at various stages of the machine learning deployment pipeline (Kozodoi et al., 2022). Pre-processing involves removing bias from the original dataset before training the model, with several proposed methods outlined in Calmon et al. (2017); Zemel et al. (2013); Wi\u015bniewski and Biecek (2021); Madras et al. (2018). In-processing refers to incorporating fairness constraints during the model training process (Komiyama et al., 2018; Scutari, 2023; Agarwal et al., 2019). Post-processing involves adjusting predictions after the model has been trained (Hardt et al., 2016; Silvia et al., 2020). Notably, a considerable amount of research has focused on achieving fairness through ridge penalties in linear models (Scutari, 2023; Komiyama et al., 2018; Zafar et al., 2019; Matloff and Zhang, 2022). tDB employs a post-processing approach, as we are modifying the predictions of the original black-box models to achieve fairness."}, {"title": "2.1 Measuring Fairness", "content": "Already, much literature has been published on different fairness measures, many of which are complex and require substantial domain-specific knowledge. In this section, we discuss some widely accepted fairness measures:\n(a) Demographic Parity: This fairness criterion evaluates group fairness by requiring that individuals from both marginalized and non-marginalized groups have an equal probability of being assigned to the positive class, regardless of group membership. For a binary Y and S, it can be expressed as:\nP(\u0176 = 1|S = 0) = P(\u0176 = 1|S = 1)\nIn this equation, the probability of being assigned to the positive class (1) is the same for both Group S = 0 and Group S = 1, indicating group fairness. Thus, demographic"}, {"title": "(b) Equalized Odds", "content": "This criterion offers a more robust measure of group fairness compared to demographic parity. It requires that the predictor \u0176 satisfies equalized odds with respect to the protected attribute S and the outcome Y, if \u0176 and S are independent conditional on Y. For binary Y and S, this can be expressed as:\nP(\u0176 = 1|S = 0, Y = y) = P(\u0176 = 1|S = 1, Y = y), y\u2208 {0,1}\nFor the outcome y = 1, this constraint requires that \u0176 has equal true positive rates across the two demographics, S = 0 and S = 1. For y = 0, the constraint ensures equal false positive rates. In this way, equalized odds requires that the model maintains consistent accuracy across all demographic groups, penalizing models that perform well only on the majority group (Hardt et al., 2016)."}, {"title": "(c) Correlation p(\u0176, S)", "content": "Correlation-based measures are commonly used in fairness evaluation (Deho et al., 2022; Mary et al., 2019). Baharlouei et al. (2019) extends fairness measurement to continuous variables by applying the Renyi maximum correlation coefficient. Lee et al. (2022) introduces a maximal correlation framework for formulating fairness constraints. Roh et al. (2023) explores the concept of correlation shifts in the context of fair training. In general, correlations between predicted \u0176 and S can be expressed as:\n\u03c1(Y, S) = Correlation between predicted \u0176 and S\nIn this paper, we apply correlation-based measures to evaluate fairness between our predicted Y and the sensitive attribute S. Specifically, we use the Kendall's Tau correlation coefficient to examine the relationships between these variables. Kendall's Tau is a non-parametric measure that captures both the strength and direction of the association, making it well-suited for data on an ordinal or categorical scale. We choose Kendall's Tau over other correlation measures, such as Pearson's correlation, because it does not require the assumption of linearity, allowing us to assess relationships even when Y or S are categorical or binary (KENDALL, 1938).\nFor categorical S, we apply one-hot encoding to create dummy variables for each category, then compute Kendall's Tau for each category separately to assess the individual reduction in correlation. Lower values (closer to 0) of Kendall's Tau indicate reduced association between the predictions from the black-box model and S, which reflects higher fairness. Since interpreting the direction of association is not the primary focus of our fairness analysis, we use the absolute value of Kendall's Tau to evaluate the overall reduction in correlation between predictions and S."}, {"title": "2.2 Measuring Accuracy", "content": "Assessing accuracy is generally more straightforward than evaluating fairness. In regression tasks, we measure accuracy using the Mean Absolute Prediction Error (MAPE), which calculates the average absolute difference between the predicted and true values. For binary classification tasks, we measure accuracy using the overall misclassification rate to represent the proportion of incorrect predictions among all predictions made."}, {"title": "3. Introducing TowerDebias", "content": "The heart of this algorithm is based on a well-known theorem from measure-theoretic probability. In this section, we present the theorem, explain its intuitive meaning, and then demonstrate its application to our algorithm."}, {"title": "3.1 Tower Property of Conditional Expectation", "content": "The Tower Property of conditional expectation (Wolpert, 2009) states:\nE(Y|X) = E[E(Y|X, S)|X]\n(3.1)\nThe Tower Property of conditional expectation expresses that the conditional expectation of Y given X can be decomposed into the conditional expectation of Y given both X and a sensitive attribute S, then conditioned again on X. Note that conditional expectations themselves are random variables. For example, E(Y|X)\u2014as a function of the random variable X-is itself a random variable.\nTo illustrate this property intuitively, consider the following example using a dataset that will be referenced later in the paper. This dataset includes: Y, a score on the Law School Admissions Test; X, family income (five quintile levels); and S, race. What does equation (3.1) mean at the \"population\" level of the data generating process?\nFor instance, E(Y|4, Black) is the mean LSAT score for Black test-takers in the fourth income quintile. Now, consider the expression:\nE[E(Y|4, S)|X = 4]\nThis quantity averages the mean LSAT score for income level 4 over all races. (3.1) then states that this average will equal the overall mean score for income level 4, regardless of race. This aligns with our intuition: conditioning on income (level 4) and then averaging over all races gives the same result as directly conditioning on income alone. Note that the averaging is weighted by the probabilities of the various races."}, {"title": "3.2 Relation to towerDebias", "content": "In simple terms, our method can be summarized as follows:\nTo remove the impact of the sensitive variable S on predictions of Y from X, average the predictions over S."}, {"title": "3.3 Measuring Correlation Reduction\u2014Trivariate Normal Case", "content": "Suppose we are predicting Y using a single numeric feature X and a numeric S. We assume that (X,S,Y) follows a trivariate normal distribution. This implies that the joint distribution of (X, S, Y) is normal, and therefore, each individual variable X, S, and Y also has a marginal normal distribution (Johnson and Wichern, 1993).\nOur goal is to derive a closed-form expression for the reduction in Pearson correlation between the predicted Y and S when predicting Y from both (X, S) (Case I) versus predicting Y from just X (Case II). In this simulation, we are specifically interested in assessing the reduction in the Pearson correlation, as opposed to using Kendall's Tau correlation used in the tDB method.\n\u03c1(\u0176, S) = \\frac{cov(\u00dd, S)}{\u03c3(\u03a5)\u03c3(S)}\nPreduc(\u00dd, S) = p(\u01761, S) \u2013 \u03c1(\u01762, S)\nS = \u03b1\u03bf + \u03b1\u2081X + \u20ac\ncov(Y1, S) = \u03b2\u2081 cov(X, S) + \u03b22 \u03c3\u00b2(S)\ncov(X, S) = \u03b1\u03b9 \u03c3\u00b2 (\u03a7)\ncov(Y1, S) = \u03b2\u2081 \u03b11 \u03c3\u00b2(X) + \u03b22 \u03c3\u00b2(S)\n\u03c1(\u01761, S) = \\frac{\u03b2\u2081 \u03b11 \u03c3\u00b2 (\u03a7) + \u03b22 \u03c3\u00b2(S)}{\u03c3(1) \u03c3(S)}\np(\u01762, S) = \\frac{cov (Y2, S)}{\u03c3(2)\u03c3(S)}\ncov(\u01762, S) = \u03b4\u03b9 \u03b1\u03b91 \u03c3\u00b2 (\u03a7)\n\u03c1(\u01762, S) = \\frac{\u03b4\u03b9 \u03b1\u03b9 \u03c3\u00b2 (\u03a7)}{\u03c3(2) \u03c3(S)}\nPreduc (Y, S) = \\frac{\u03b2\u2081 \u03b11 \u03c3\u00b2(\u03a7) + \u03b22 \u03c3\u00b2 (S) - \u03b4\u03b9 \u03b1\u03b9 \u03c3\u00b2 (\u03a7)}{\u03c3(1) \u03c3(S) \u03c3(2) \u03c3(S)}\n(3.3)\nThis expression quantifies the reduction in Pearson correlation between \u0176 and S between Case I (with both X and S as predictors) and Case II (with only X)."}, {"title": "3.4 Measuring Correlation Reduction\u2014Generalized Case", "content": "Fairness improvements can be established more broadly, extending beyond the simulated trivariate normal case, through the use of L2 function spaces. This section will demonstrate the proof for this scenario."}, {"title": "4. Empirical Study", "content": "Our empirical analysis demonstrates the effectiveness of tDB on the fairness-utility tradeoff across several well-known datasets in fair machine learning, encompassing both regression and classification tasks. The analysis includes five datasets: SVCensus, Law School Admissions, COMPAS, Iranian Churn, and Dutch Census.\nTo establish baseline results for fairness and accuracy, we train several machine learning models to generate the initial predictions. tDB is then applied to each model's individual predictions to evaluate the improvement in fairness\u2014measured as a reduction in \u03c4(\u0176, S)\u2014 at some potential cost in predictive performance. Our study aims to explore how varying choices of the tradeoff parameter k, selected based on the specific algorithm and dataset, impacts the overall applicability of this method and the balance over the fairness-utility tradeoff.\nWe train several conventional machine learning models, including Linear & Logistic Regression, K-Nearest Neighbors, XGBoost, and Deep Neural Networks to establish baseline results. The Quick And Easy Machine Learning (qeML) package in R provides a user-friendly"}, {"title": "4.1 SVCensus", "content": "The SVCensus dataset is a subset of U.S. Census data from the early 2000s, focusing on income levels across six engineering occupations within Silicon Valley. Each person's record in the dataset includes attributes such as occupation, education level, number of weeks worked, and age. Our goal is to predict wage income (Y) with respect to gender as the sensitive attribute (S). Note that this is regression problem, and gender contains two categories: male and female."}, {"title": "4.2 Law School Admissions", "content": "The Law School Admissions dataset is a survey of law school students in the United States from 1991. It includes various demographic and academic details, such as age, grade decile score, undergraduate GPA, family income, gender, race, and more. The dataset is used to predict LSAT scores (Y), with race considered as the sensitive attribute (S). This is a regression task, where race is categorized into the following groups: Asian, Black, White, Other, and Hispanic. This setup broadens the scope of fairness measurement beyond binary sensitive attributes, such as gender, as shown in the previous example with the SVCensus data."}, {"title": "4.3 COMPAS", "content": "The COMPAS dataset, which contains data on criminal offenders screened in Florida during 2013-14, is being used to predict the likelihood that a defendant will recommit a crime in the near future (probability of recidivism\u2014denoted as Y). Race is treated as the sensitive variable (S), with the data pre-processed to include three racial categories: White, African American, and Hispanic. The task is now a binary classification problem where race is considered a categorical variable with three possible values."}, {"title": "4.4 Iranian Churn", "content": "The Iranian Churn dataset is used to predict customer churn, with \"Exited\" as the response variable and \"Gender\" and \"Age\" as the sensitive variables. In this example, we incorporate a continuous sensitive variable to extend the analysis beyond categorical or binary S."}, {"title": "4.5 Dutch Census", "content": "The Dutch Census dataset, collected by the Dutch Central Bureau for Statistics in 2001, is used to predict whether an individual holds a prestigious occupation or not (binary response variable), with \"gender\" as the sensitive attribute. Note: Zafar's Logistic Regression encountered errors in training and was excluded from our analysis. The following graphs illustrate the impact of tDB across several models.\nEmpirical results from the Dutch Census dataset demonstrate significant improvements in fairness with a controlled accuracy loss on classification tasks. The dataset includes 10 categorical predictors, and tDB applies one-hot encoding to convert these into dummy variables, increasing the dimensionality to 61 predictors. This example highlights tDB's effectiveness on high-dimensional, sparse datasets. Thus, this examples illustrates the effectiveness of tDB on high-dimensional, sparse datasets. In terms of fairness, Figure (9) shows considerable reductions in correlations when applying tDB to both traditional and fair machine learning models. For mid-to-high range of k values, the correlation reduction exceeds 50% compared to the baseline results. Regarding accuracy, Figure (10) hows that tDB leads to a 25% increase in the misclassification rate at mid-to-high range k values for traditional machine learning models, with similar trends observed in comparisons between fairML functions and tDB."}, {"title": "CHOOSING AN APPROPRIATE k", "content": "As observed in the tDB plots, selecting an appropriate value for k significantly impacts the fairness-utility tradeoff. General trends show a notable decrease in correlation at lower k values, with the reductions stabilizing once k reaches around 10. Thus, one could identify the \"elbow\" point, analogous to the choice of clusters in k-means clustering, as additional"}, {"title": "5. Discussion", "content": "Empirical evaluations of the towerDebias (tDB) method across conventional machine learning algorithms yield promising results. This approach achieves a significant reduction in p(Y, S) with only a modest utility loss. While results varied across datasets and methods, most cases showed a clear decrease in the correlation coefficient. The study highlights the"}]}