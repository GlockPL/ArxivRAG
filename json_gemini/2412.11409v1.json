{"title": "Multi-modal and Multi-scale Spatial Environment Understanding for\nImmersive Visual Text-to-Speech", "authors": ["Rui Liu", "Shuwei He", "Yifan Hu", "Haizhou Li"], "abstract": "Visual Text-to-Speech (VTTS) aims to take the environ-\nmental image as the prompt to synthesize the reverberant\nspeech for the spoken content. The challenge of this task\nlies in understanding the spatial environment from the im-\nage. Many attempts have been made to extract global spa-\ntial visual information from the RGB space of an spatial im-\nage. However, local and depth image information are cru-\ncial for understanding the spatial environment, which pre-\nvious works have ignored. To address the issues, we pro-\npose a novel multi-modal and multi-scale spatial environment\nunderstanding scheme to achieve immersive VTTS, termed\nM\u00b2SE-VTTS. The multi-modal aims to take both the RGB\nand Depth spaces of the spatial image to learn more com-\nprehensive spatial information, and the multi-scale seeks to\nmodel the local and global spatial knowledge simultaneously.\nSpecifically, we first split the RGB and Depth images into\npatchies and adopt the Gemini-generated environment cap-\ntions to guide the local spatial understanding. After that, the\nmulti-modal and multi-scale features are integrated by the\nlocal-aware global spatial understanding. In this way, M\u00b2SE-\nVTTS effectively models the interactions between local and\nglobal spatial contexts in the multi-modal spatial environ-\nment. Objective and subjective evaluations suggest that our\nmodel outperforms the advanced baselines in environmental\nspeech generation.", "sections": [{"title": "Introduction", "content": "Visual Text-to-Speech (VTTS) aims to leverage the envi-\ronmental image as the prompt to generate the reverberant\nspeech that corresponds to the spoken content. With the ad-\nvancement of human-computer interaction, VTTS has be-\ncome integral to intelligent systems and plays an important\nrole in fields such as augmented reality (AR) and virtual re-\nality (VR) (Liu et al. 2023b).\nUnlike acoustic matching tasks that transform input\nspeech to match the environmental conditions of a refer-\nence source (Chen et al. 2022; Liu et al. 2023a; Somayazulu,\nChen, and Grauman 2024; Im and Nam 2024), VTTS seeks\nto synthesize speech with the environmental characteristics\nof the reference based on given textual content (He, Liu, and\nLi 2024). For example, Lee et al. (2024) utilizes the pre-\ntrained CLAP model to map a textual or audio description\ninto an environmental feature vector that controls the re-\nverberation aspects of the generated audio. Tan, Zhang, and\nLee (2022) design an environment embedding extractor that\nlearns environmental features from the reference speech. In\nmore recent studies, Liu et al. (2023b) propose a visual-text\nencoder based on the transformer to learn global spatial vi-\nsual information from the RGB image. Building on these\nadvancements, this paper focuses on employing visual in-\nformation as the cue to generate reverberation audio for the\ntargeted scene.\nHowever, previous VTTS methods have not fully under-\nstood the spatial environment, due to the neglect of local and\ndepth image information. For example, the local elements in\nthe spatial environment can directly influence the reverber-\nation style. Specifically, the hard surfaces such as tables re-\nflect sound waves, while softer materials like carpets absorb\nthem, directly affecting the audio's authenticity and natural-\nness (Chen et al. 2023, 2022; Liu et al. 2023b). In addition,\nthe Depth space of the image contains positional relation-\nships within the spatial environment (Majumder et al. 2022;\nChen et al. 2023), such as the arrangement of objects, the\nposition of the speaker and the room geometry. Therefore, it\nis crucial for VTTS systems to accurately capture the local\nand depth spatial environment information simultaneously.\nTo address the issues, we propose a novel multi-modal\nand multi-scale spatial environment understanding scheme\nto achieve immersive VTTS, termed M2SE-VTTS. The\nmulti-modal aims to take both the RGB and Depth spaces\nof the spatial image to learn more comprehensive spatial in-\nformation, such as the speaker's location and the positions\nof key objects that influence sound absorption and reflec-\ntion. The multi-scale seeks to model the impact of local and\nglobal spatial knowledge on reverberation. Specifically, we\nfirst split the RGB and Depth images into patches follow-\ning the visual transformer strategy (Dosovitskiy et al. 2021).\nIn addition, we adopt the Gemini-generated (Team et al.\n2024) environment captions to guide the local spatial under-\nstanding based on an identification mechanism. After that,\nthe local-aware global spatial understanding takes the multi-\nmodal and multi-scale features as input and progressively in-\ntegrates spatial environment knowledge. In this way, M2SE-\nVTTS effectively models the interactions between local and"}, {"title": "Related Works", "content": "Spatial Environment Understanding plays a crucial role in\nspatial cognition, particularly in complex three-dimensional\nscenes where accurate spatial comprehension is essential for\napplications such as robotic navigation, augmented reality,\nand autonomous driving. In the visual domain, researchers\noften employ multi-modal and multi-scale approaches to\ncapture and analyze spatial information more comprehen-\nsively (Chen et al. 2020; Guo et al. 2022; Jain et al. 2023;\nJiang et al. 2024; Xu et al. 2024; Wang et al. 2024). For\ninstance, Cheng et al. (2024a) enhances Vision Language\nModels (VLMs) by introducing a data curation pipeline and\na plugin module that improves the understanding of 3D spa-\ntial relationships by integrating depth information and learn-ng regional representations from 3D scene graphs. Fu et al.\n(2024) further extends the capabilities of language models\nby incorporating 3D visual data, improving the embodied\nagents' reasoning and decision-making abilities in interac-\ntive 3D environments. This approach is particularly effec-\ntive in tasks such as dense annotation and interactive plan-\nning. Similarly, Cheng et al. (2024b) addresses the chal-\nlenge of depth estimation in autonomous driving by propos-\ning a method that fuses single-view and multi-view depth\nestimates, showing robust performance in scenarios with\nsparse textures and dynamic objects. These studies demon-\nstrate that multi-modal and multi-scale methods are pivotal\nin advancing the comprehensive understanding of spatial en-\nvironments, as they effectively integrate information from\nvarious sources to enhance spatial reasoning in complex en-\nvironments.\nWhile these works have made significant strides in im-\nproving spatial understanding in VLMs, they primarily fo-\ncus on extracting global spatial information, often overlook-\ning the importance of local and depth information. Our work\ndiffers from these approaches in several key aspects: (1) We\nfocus on the Visual VTTS task, rather than solely on visual\nspatial reasoning; (2) Unlike the previous works, we empha-\nsize the integration of both local and depth image informa-\ntion, in addition to global spatial data from RGB images,\nto achieve a more holistic understanding of the spatial en-\nvironment. These differences allow our approach to better\naddress the unique challenges of the VTTS task and excel in\ncomplex spatial environments."}, {"title": "LLM-based Image Understanding", "content": "In recent years, leveraging Large Language Models (LLMs)\nfor image understanding has emerged as a significant re-\nsearch focus in the fields of computer vision and natural lan-\nguage processing. By integrating the powerful natural lan-\nguage processing capabilities of LLMs with visual informa-\ntion, researchers have developed multi-modal large language\nmodels (MLLMs) to tackle complex tasks such as visual\nquestion answering, image captioning, and image compre-\nhension (Zhang et al. 2024; Zhu, Wei, and Lu 2024). For\nexample, Karthik et al. (2024) pioneered the combination\nof pre-trained vision encoders with language models, utiliz-\ning a Perceiver Resampler module to extract features from\nimages for generating textual descriptions, thus achieving\ncross-modal image-text alignment. Building on this, Swetha\net al. (2024) introduced a Querying Transformer (Q-Former)\nthat extracts the most relevant visual features through cross-\nmodal fusion, enhancing the model's visual understand-\ning capabilities. Chowdhury et al. (2024) further advanced\nthese efforts by incorporating spatial coordinate informa-\ntion, thereby improving multi-modal models' abilities in ob-\nject localization and visual reasoning. However, these ap-\nproaches largely focus on capturing global visual features,\nwhich, while effective in many cases, show limitations when\ndealing with tasks that require fine-grained visual under-\nstanding. To address this challenge, Swetha et al. (2024) pro-\nposed a novel approach that combines contrastive learning\n(CL) with masked image modeling (MIM), integrating fea-\ntures from both CLIP-ViT (Radford et al. 2021) and MAE-\nViT vision encoders. X-Former uses a dual cross-attention\nmechanism to align visual and language features, demon-\nstrating superior performance in fine-grained visual tasks,\nsuch as object counting and fine-grained category recogni-\ntion. In contrast, traditional multi-modal models often strug-\ngle with these tasks due to their limited ability to capture\nlocal details effectively.\nWhile these methods have significantly advanced the ca-\npabilities of multi-modal visual understanding, they still\nface limitations when applied to specific spatial environ-\nment perception tasks. Our work introduces a novel multi-\nmodal and multi-scale spatial environment understanding\nscheme, designed to overcome the shortcomings of existing\nmodels in capturing the local and depth information. Unlike\nprevious approaches, our method integrates both RGB and\ndepth information and utilizes Gemini-generated environ-\nmental captions to guide local spatial understanding. By fus-\ning multi-modal and multi-scale features, our method pro-\nvides a more comprehensive modeling of spatial environ-\nments, offering robust support for VTTS tasks, particularly\nin understanding the spatial layout and environmental char-\nacteristics of complex scenes."}, {"title": "Methodology", "content": "As shown in the pipeline of Fig. 1, the proposed M2SE-\nVTTS consists of four components: 1) Multi-modal Fea-\ntures Extraction; 2) Local Spatial Understanding; 3) Local-\naware Global Spatial Understanding and 4) Speech Genera-\ntion. As mentioned previously, multi-modal features, includ-\ning the RGB and Depth space representation of an image,\ncan provide more comprehensive information about the spa-\ntial environment. To understand the interactions between lo-\ncal and global spatial contexts, the multi-modal and multi-\nscale knowledge is integrated by local-aware global spatial\nunderstanding. The following subsections provide detailed\ndescriptions of the design and training processes for these\ncomponents."}, {"title": "Multi-modal Features Extraction", "content": "Given the RGB and Depth image pairs of the spatial envi-\ronment {VR, VD}, we first partition them into M patches.\nIn addition, we employ the image encoder of a pre-trained\nCLIP (Radford et al. 2021) model with frozen parameters\nto extract patch-level features as $F_B, F_D \\in \\mathbb{R}^{M \\times D}$ from\neach of VR and VD, where D denotes the dimensionality of\nthe features and M indicates the number of patches per im-\nage. As illustrated in Fig 1, a special [CLS] token is used at\nthe beginning of the first patch to represent the global-level\nfeatures as $F_B^c, F_D^c \\in \\mathbb{R}^{1 \\times D}$."}, {"title": "Local Spatial Understanding", "content": "As shown in the second panel of Fig. 1, Local Spatial Under-\nstanding consists of three parts: 1) LLM-based Spatial Se-\nmantic Understanding, leveraging Gemini's powerful multi-\nmodal understanding capabilities to accurately convert com-\nplex visual scenes into semantic information; 2) Topk RGB\nRegions Detector, guided by environmental captions to iden-\ntify crucial semantic information of the RGB space of the\nimage; and 3) Topk Depth Regions Selector, selecting im-\nportant semantic information of the Depth space of the im-\nage.\nLLM-based Spatial Semantic Understanding To cap-\nture rich spatial information, including the spatial positions\nof objects, their arrangement, and the overall scene struc-\nture, we utilize Gemini's advanced multi-modal understand-\ning capabilities to convert the complex visual data into the\nstructured caption. This approach enables us to accurately\nextract and represent the spatial semantics embedded within\nthe image.\nFirst of all, the spatial environment captions are gener-\nated using the Gemini Pro Vision, which is a multi-modal\nlarge language model configured with its default settings.\nThe prompt designed for Gemini is as follows: \u201cObserve this\npanoramic image and briefly describe its content. Identify\nthe objects in the image in one to two sentences, focusing\nonly on key information and avoiding descriptive words.\"\nAfter analysis by Gemini, the spatial environment in Fig. 1\nis described as follows: \"The image shows a spacious, cir-\ncular room with a blue and white color scheme. It features a\ndining table with chairs, a kitchenette, a bedroom area with a\nbed, and a person standing in the center of the room.\" In the\nend, the caption C is tokenized into N individual words, rep-\nresented as C = {$C_n$}$_{n=1}^{N}$. And each word cn is represented\nas a fixed-length vector using word embeddings, which are\ninput into the text encoder of a pre-trained CLIP model to\nobtain spatial semantic features $F_T^c$, where $F_T^c \\in \\mathbb{R}^{1\\times D}$. It\nis important to note that the [CLS] token is used to aggre-\ngate and represent the overall semantic information of the\nentire input text, with this embedding vector serving as the\nprimary representation of the text when aligning with image\nfeatures.\nTopk RGB Regions Detector Our goal is to identify and\nfocus on the image regions that significantly influence sound"}, {"title": "Local-aware RGB/Depth Attention", "content": "This section aims to\nunderstand how local spatial details, such as the position and\nmaterial of key objects, interact within the overall spatial\nlayout and to comprehend the spatial relationships across\ndifferent scales in the scene, thereby enabling the genera-\ntion of reverberation that more accurately reflects the actual\nphysical environment.\nFor the RGB image, given its $H_{Topk}^R$ and $F_B^c$, we per-\nform the Local-aware RGB Attention to model the interac-\ntions between the local and global spatial knowledge of the\nRGB space after using a linear projection layer, which is for-\nmulated as follows:\n$H_R^l$ = MultiHead($H_{Topk}^R, F_B^c, F_B^c$), (4)\nwhere $H_R^l \\in \\mathbb{R}^{Topk \\times D}$ is updated from $F_B^c$.\nFor the Depth image, giving its $H_{Topk}^D$ and $F_D^c$, the\nLocal-aware Depth Attention adopts a similar strategy,\nwhich is formulated as follows:\n$H_D^l$ = MultiHead($H_{Topk}^D, F_D^c, F_D^c$), (5)\nwhere $H_D^l \\in \\mathbb{R}^{Topk \\times D}$ is updated from $F_D^c$.\nSemantic-Guided RGB/Depth Attention To deepen our\nunderstanding of the complex relationships between spatial\ncontexts across different scales and to enhance the model's\nperformance in the multi-modal environment, we further\nemploy a semantic-guided attention mechanism to achieve\na more advanced fusion of local and global spatial features.\nFor the RGB image, given its $H_R^l$ and $F_T^c$, we adopt the\nSemantic-Guided RGB Attention to attain an advanced un-\nderstanding between the local and global spatial contexts\nfollowing a linear projection layer, which is formulated as\nfollows:\n$H_R^g$ = MultiHead($F_T^c, H_R^l, H_R^l$), (6)\nwhere $H_R^g \\in \\mathbb{R}^{1 \\times D}$ is updated from $F_B^c$.\nFor the Depth image, the Semantic-Guided Depth Atten-\ntion employs a similar method to learn an advanced under-\nstanding of the Depth space, which is formulated as follows:\n$H_D^g$ = MultiHead($F_T^c, H_D^l, H_D^l$) (7)\nEventually, we integrate the multi-modal and multi-scale\nfeatures to derive a comprehensive representation of the spa-\ntial environment, which is formulated as follows:\n$H_V = \\lambda_1 H_R^g + \\lambda_2 H_D^g$, (8)\nwhere the weights, $\\lambda_1$ and $\\lambda_2$, are both set to 0.5."}, {"title": "Speech Generation", "content": "As illustrated in Fig. 1, we adopt ViT-TTS as the back-\nbone for our TTS system. To begin with, the phoneme em-\nbeddings and visual features are converted into hidden se-\nquences. In addition, the variance adaptor predicts the du-\nration of each hidden sequence to regulate the length of\nthe hidden sequences to match that of speech frames. Af-\nter that, different variances like pitch and speaker embed-\nding are incorporated into hidden sequences following Ren\net al. (2021). Furthermore, the spectrogram denoiser itera-\ntively refines the length-regulated hidden states into mel-\nspectrograms. In the end, the BigVGAN (Lee et al. 2022)\ntransforms mel-spectrograms into waveform. For more de-\ntails, please refer to the ViT-TTS (Liu et al. 2023b)."}, {"title": "Experiments and Results", "content": "Dataset\nWe employ the SoundSpaces-Speech dataset (Chen et al.\n2023), which is developed on the SoundSpaces platform\nusing real-world 3D scans to simulate environmental au-\ndio. To enhance the dataset, we refine it following the ap-\nproach described in Chen et al. (2022); Liu et al. (2023b).\nSpecifically, we exclude out-of-view samples and divide\nthe remaining data into two subsets: test-unseen and test-\nseen. The test-unseen subset includes room acoustics de-\nrived from novel images, while the test-seen subset con-\ntains scenes previously observed during training. The dataset\nconsists of 28,853 training samples, 1,441 validation sam-\nples, and 1,489 testing samples. Each sample includes clean\ntext, reverberation audio, and panoramic camera RGB-D\nimages. To preprocess the text, we convert the sequences\ninto phoneme sequences using an open-source grapheme-to-phoneme tool 1.\nFollowing common practices (Ren et al. 2019; Huang\net al. 2022; Liu et al. 2024b,a), we preprocess the speech\ndata in three steps. First, we extract spectrograms with an\nFFT size of 1024, a hop size of 256, and a window size\nof 1024 samples. Next, we convert the spectrogram into a\nmel-spectrogram with 80 frequency bins. Finally, we extract\nthe FO (fundamental frequency) from the raw waveform us-\ning Parselmouth 2. These preprocessing steps ensure consis-\ntency with prior work and prepare the data for subsequent\nmodeling.\nImplementation Details\nFor the visual modality, we utilize the pre-trained CLIP-ViT-\nL/14 as the visual feature extractor. This model generates\n768-dimensional feature vectors at both global and patch\nlevels for each visual snippet. These visual features un-\ndergo a linear transformation and are subsequently aligned\nwith the 512-dimensional hidden space of the phoneme em-\nbeddings. The phoneme vocabulary consists of 74 distinct\nphonemes. The cross-modal fusion module employs two at-\ntention heads, while all other attention mechanisms use four\nheads each. The patch number, Topk, is set to 140. The con-\nfiguration of other encoder parameters follows the imple-\nmentation in ViT-TTS. In the denoiser module, we use five\ntransformer layers with a hidden size of 384 and 12 heads.\nEach transformer block functions as the identity, with T set\nto 100 and $\\beta$ values increasing linearly from $\\beta_1$ = $10^{-4}$\nto $\\beta_T$ = 0.06. This configuration facilitates effective noise\nreduction and enhances the quality of the generated outputs.\nThe training process consists of two stages. In the pre-\ntraining stage, we adopt the encoder pre-training strategy\nfrom ViT-TTS, training the encoder for 120k steps until con-\nvergence. In the main training stage, the M2SE-VTTS model\nis trained on a single NVIDIA A800 GPU with a batch\nsize of 48 sentences, extending over 160k steps until con-\nvergence. During inference, we use a pre-trained BigVGAN\nas the vocoder to transform the generated mel-spectrograms"}, {"title": "Evaluation Metrics", "content": "We measure the sample quality of the generated waveform\nusing both objective metrics and subjective indicators. The\nobjective metrics are designed to evaluate various aspects of\nwaveform quality by comparing the ground-truth audio with\nthe generated samples. Following the common practice of\nLiu et al. (2022); Huang et al. (2022), we randomly select\n50 samples from the test set for objective evaluation. We\nprovide three main metrics: (1) Perceptual Quality: This is\nassessed by human listeners using the Mean Opinion Score\n(MOS). A panel of listeners evaluates the audio's quality,\nnaturalness, and its congruence with the accompanying im-\nage. Ratings are assigned on a scale from 1 (poor) to 5 (ex-\ncellent). The final MOS is the average of these ratings. (2)\nRoom Acoustics (RT60 Error): RT60 measures the rever-\nberation time in seconds for an audio signal to decay by\n60 dB, which is a standard metric for characterizing room\nacoustics. To calculate the RT60 Error (RTE), we estimate\nthe RT60 values from the magnitude spectrograms of the\noutput audio, using a pre-trained RT60 estimator provided\nby Chen et al. (2022). (3) Mel Cepstral Distortion (MCD):\nMCD quantifies the spectral distance between the synthe-\nsized and reference mel-spectrogram features. It is widely\nused as an objective measure of audio quality, particularly\nin tasks involving speech synthesis. Lower MCD values in-\ndicate higher spectral similarity between the generated and\nground-truth audio.\nEach of these metrics provides a distinct perspective on\nthe quality of the generated waveform, allowing for a com-\nprehensive evaluation of the system's performance."}, {"title": "Baselines", "content": "To demonstrate the effectiveness of our M2SE-VTTS, we\ncompare it against five baseline systems:\n\u2022 ProDiff (Huang et al. 2022): This first baseline is a pro-\ngressive fast diffusion model designed for high-quality\nspeech synthesis, where the input is text and the model\ndirectly predicts clean mel-spectrograms, significantly\nreducing the required sampling iterations.\n\u2022 DiffSpeech (Liu et al. 2022): This method is a TTS\nmodel that employs a diffusion probabilistic approach,\nwhere the input is text and the model iteratively converts\nnoise into mel-spectrograms conditioned on the text.\n\u2022 VoiceLDM (Lee et al. 2024): The third system is a\nTTS model that uses text as its primary input, effec-\ntively capturing global environmental context from de-\nscriptive prompts to generate audio that aligns with both\nthe content and the overarching situational description.\nGiven the differences in environmental text descriptions\nbetween the training datasets-where the original dataset\nprimarily describes the type of environment, while ours\nemphasizes the specific components and their spatial re-\nlationships we choose to concentrate on the model's"}, {"title": "Main Results", "content": "As shown in Table 1, the performance of the M\u00b2SE-VTTS\nmodel on the test-unseen set is generally lower than that on\nthe test-seen set, largely due to the presence of scenarios not\nencountered during training. Nevertheless, our model con-\nsistently outperforms all baseline systems across both sets,\nachieving the best results in RTE (0.0744), MCD (4.4215),\nand MOS (3.849 \u00b1 0.025). These results demonstrate that\nour model is capable of synthesizing immersive reverber-\nant speech. In addition, our model outperformed TTS dif-\nfusion models, such as DiffSpeech and ProDiff, across all\nmetrics, notably in RTE. This indicates that traditional TTS\nmodels struggle to understand spatial environment informa-\ntion, focusing instead on audio content, pitch, and energy.\nTo address this limitation, our multi-modal scheme learns\nmore comprehensive spatial information. Furthermore, com-\nparison with voiceLDM highlights the advantages of the\nmulti-modal spatial cues and Gemini-based spatial envi-\nronment understanding. Although voiceLDM takes environ-\nmental context descriptions as prompts to synthesize envi-\nronmental audio, its choice of the spatial prompt and lack\nof a spatial semantic understanding strategy result in worse\nperformance in predicting the correct reverberation and syn-\nthesizing high-quality audio with perceptual accuracy. Fi-\nnally, ViT-TTS, which uses ResNet18 for global visual fea-\nture extraction, and ViT-TTS-CLIP, which employs CLIP-\nViT, both outperform other baseline models. However, com-\npared to our proposed model, both ViT-TTS and ViT-TTS-\nCLIP showed inferior performance in both test-unseen and\ntest-seen environments. This suggests that our accurate mod-"}, {"title": "Ablation Results", "content": "To evaluate the individual effects of several key techniques\non the Test-Unseen set in our model, including the RGB\nspace (RGB), the Depth space (Depth), the Gemini-based\nspatial semantic understanding (LLM), the local spatial un-\nderstanding (LSU), local-aware interactions (LGSU-L), and\nglobal knowledge interactions (LGSU-G), we remove these\ncomponents to build various systems. A series of ablation\nexperiments were conducted, and the subjective and objec-\ntive results are shown in Table 2.\nWe find that removing different types of modality infor-\nmation (w/o RGB and w/o Depth) in the Multi-modal Fea-\ntures Extraction led to a decrease in performance across\nmost objective metrics, and the subjective MOS scores also\ndropped. This suggests that our multi-modal strategy can\nlearn more comprehensive spatial information and enhance\nthe expressiveness of reverberation.\nIn addition, to validate the Gemini-based spatial seman-\ntic understanding (LLM), we remove this component (w/o\nLLM). As shown in Table 2, the removal of the semantic"}, {"title": "Topk Index Sharing Comparative Study", "content": "To evaluate the effectiveness of selecting depth features us-\ning shared Topk indices from the RGB image and to com-\npare this approach with independent semantic-guided meth-\nods, we focus on the efficacy of index-sharing and the im-\npact of different Topk values. Specifically, we design exper-\niments to compare two feature selection strategies: shared\nTopk indices and independent semantic-guided selection.\nIn the shared index strategy, Topk critical regions from\nRGB images guide depth feature selection. In contrast,\nthe semantic-guided strategy independently selects features\nfrom RGB and depth images based on their respective se-\nmantic information. Various Topk values (e.g., 20, 40,\n..., 240) are tested to observe their effects on performance, eval-\nuated using two objective metrics, with lower values indicat-\ning better performance.\nThe experimental results demonstrate that the shared\nTopk strategy consistently outperforms the independent\nsemantic-guided strategy for all tested values of Topk. This\napproach yields lower values for objective metrics, indicat-\ning more natural audio generation that better aligns with the\nenvironment. As the Topk value increases, performance im-"}, {"title": "Conclusion", "content": "This paper introduces M2SE-VTTS, an innovative multi-\nmodal and multi-scale approach for Visual Text-to-Speech\n(VTTS) synthesis. Our method addresses the limitations of\nprevious VTTS systems, such as their limited spatial under-\nstanding, by incorporating both RGB and depth images to\nachieve a comprehensive representation of the spatial envi-\nronment. This comprehensive spatial representation includes\nmodeling both local and global spatial contexts, which are\ncrucial for capturing the environmental nuances influenc-\ning speech reverberation. By combining local spatial under-\nstanding guided by the environment caption and local-aware\nglobal spatial modeling, M2SE-VTTS effectively captures\nthe interactions between different spatial scales, which are\nessential for accurate reverberation modeling. Evaluations\ndemonstrate that our model consistently outperforms state-\nof-the-art benchmarks in generating environmental speech,\nestablishing a new standard for environmental speech syn-\nthesis in VTTS.\nDespite its advances, the M2SE-VTTS framework has\nlimitations, such as increased computational complexity\nfrom multi-modal and multi-scale feature integration, po-\ntentially hindering real-time applications. Additionally, the\nmodel's performance is inconsistent in unseen environ-\nments, highlighting the need for improved generalization.\nFuture research should focus on optimizing computational\nefficiency and enhancing the model's adaptability to unseen\nspatial contexts."}]}