{"title": "Geometrically Inspired Kernel Machines for Collaborative Learning Beyond Gradient Descent", "authors": ["Mohit Kumar", "Alexander Valentinitsch", "Magdalena Fuchs", "Mathias Brucker", "Juliana Bowles", "Adnan Husakovic", "Ali Abbas", "Bernhard A. Moser"], "abstract": "This paper develops a novel mathematical framework for collaborative learning by means of geometrically inspired kernel machines which includes statements on the bounds of generalisation and approximation errors, and sample complexity. For classification problems, this approach allows us to learn bounded geometric structures around given data points and hence solve the global model learning problem in an efficient way by exploiting convexity properties of the related optimisation problem in a Reproducing Kernel Hilbert Space (RKHS). In this way, we can reduce classification problems to determining the closest bounded geometric structure from a given data point. Further advantages that come with our solution is that our approach does not require clients to perform multiple epochs of local optimisation using stochastic gradient descent, nor require rounds of communication between client/server for optimising the global model. We highlight that numerous experiments have shown that the proposed method is a competitive alternative to the state-of-the-art.", "sections": [{"title": "Introduction", "content": "In different domains of modern industry such as iron- or steelmaking, substantial amounts of data is generated and fused together to describe complex processes. These data are often distributed from multiple data vendors and clients, making collaborative data analysis and model training challenging. A potential bottleneck of using traditional centralized machine learning approaches is the overall data aggregation to a single location. The centralization of the data is not always feasible due to privacy concerns and logistical constraints.\nOur focus here is on collaborative learning from distributed and privately owned data. Federated learning is an increasingly popular approach to collaborative learning between multiple clients without the need to exchange raw training data. Given this advantage, federated learning can play a crucial role in process industry by leveraging distributed data to improve model performance while preserving data privacy. The classical federated learning approach (McMahan et al., 2017; Li et al., 2020; Wang et al., 2020) aims to train a common global model by repeating the following two operations: 1) training client local models using local data, and 2) aggregating these local models to update a global model. However, the sampling of distributed data from different local distributions makes it challenging to design and analyse efficient federated learning algorithms in practice (Ye et al., 2023; Vahidian et al., 2024). In general, essential requirements for federated learning include: 1) the capability of addressing heterogeneity among local data distributions, 2) the support for communication efficiency (allowing clients to transfer the required amount of parameters to the server under limited communication bandwidth), and 3) overall computational efficiency (for real-time operations) (Kairouz et al., 2021)."}, {"title": "1.1 Central Problem", "content": "In order to develop an accurate collaborative learning method that is efficient in both communication and computation, we formulate the following research questions:\nQ1: Can we build a theoretical analysis framework for collaborative learning from distributed and statistically heterogeneous data that, without making any assumptions on data distributions, allows us to calculate in practice: 1) the generalisation and approximation error bounds, and 2) the minimum number of training samples required to reduce the risk in approximating the target function below $\\epsilon$ (for any $\\epsilon > 0$) with probability at least $1 - \\delta$ (for any $\\delta \\in (0,1))$?\nQ2: Can we solve the global model optimisation problem in the federated setting without requiring multiple rounds of communication between clients and the server?\nQ3: Can kernel machine learning theory provide a competitive and computationally efficient alternative to stochastic gradient descent-based optimisation in a federated setting?"}, {"title": "1.2 The State of the Art", "content": "Addressing Data Heterogeneity. The issue of data heterogeneity in federated learning has been previously addressed by learning a personalised model for each client assuming that data features share a common global representation, while statistical heterogeneity across\nTheoretical Analysis Framework. In federated learning, the underlying models can be chosen from a reproducing kernel Hilbert space (Hong & Chae, 2022; Ghari & Shen, 2022) allowing for an application of the powerful kernel theory for design and analysis. Kernels have been applied in machine learning over the years (Hofmann et al., 2008; Rudi et al., 2017) and have recently gained renewed attention. In particular, the parallels between the properties of deep neural networks and kernel methods have been established to indicate that some key phenomena of deep learning are manifested similarly in kernel methods in the overfitted regime (Belkin, Ma, & Mandal, 2018), and deep kernel machines have been introduced (Wilson et al., 2016; Nikhitha et al., 2021). Kernel-based models are effective for learning representations (Gholami & Hajisami, 2016; Kampffmeyer et al., 2018; Laforgue et al., 2019) and facilitate analytical solutions for learning problems using a broad range of mathematical techniques. A convergence guarantee for federated learning can be established for strongly convex and smooth objective functions (Li et al., 2020; Khaled et al., 2020; Qu et al., 2023). For one-hidden layer neural network with ReLU activations, an analysis of federated learning can be provided (Li et al., 2021) by describing the training dynamics of federated learning by means of Neural Tangent Kernel (NTK) (Jacot et al., 2018). The gradient descent training dynamics of artificial neural networks follows that of the gradient descent of the functional cost with respect to a kernel: NTK (Jacot et al., 2018). A \u039d\u03a4\u039a based framework makes use of the theory on over-parameterised neural networks to provide proof of convergence of gradient descent and generalisation bound for over-parameterized ReLU neural networks in federated learning (Huang et al., 2021).\nVariational Optimisation as an Alternative to Gradient Descent. A kernel-based approach that does not rely on gradient descent-based learning and instead uses variational optimisation for deriving analytically the learning solutions, has been previously studied (Kumar & Freudenthaler, 2020; Kumar et al., 2021; Zhang et al., 2022; Kumar et al., 2021; Zhang et al., 2023; Kumar et al., 2021b, 2021a, 2023). This kernel-based variational optimisation approach was considered for privacy-preserving learning under the differential privacy framework (Kumar, 2023; Kumar et al., 2021, 2020) and fully homomorphic encryption (Kumar et al., 2023), and can potentially be explored for federated learning as well. So far there have been no attempts to extend the variational optimisation approach to federated learning in such a way that it can handle all our research questions Q1, Q2, and Q3."}, {"title": "Geometrically Inspired Kernel Approach as an Efficient Alternative", "content": "For collaborative learning in a federated setting, a geometrically inspired kernel approach has been introduced (Kumar, Moser, & Fischer, 2024, 2023). A recent paper (Kumar et al., 2024) introduced a so-called Kernel Affine Hull Machine (KAHM), wherein a representation of given data points is learned in RKHS to define a bounded geometric structure around data points within the affine hull of data points. The KAHM makes it possible to compute at any arbitrary point a measure of its distance from the data samples. The significance of this is that the KAHM's induced distance measure cannot only be used for classification, but for federated learning by aggregating locally trained KAHMs to build a global \u041a\u0410\u041d\u041c. Note that the crucial significance of KAHMs for learning from distributed data comes from the fact that a global model can be built by aggregating local models simply using a distance measure without requiring gradient-based learning of the global model parameters. This is best illustrated through an example as shown in Fig. 1. Consequently, a KAHM-based ap- proach is computationally more efficient and indeed promising for federated learning (Kumar et al., 2023, 2024). Moreover, KAHMs can be used to mitigate the accuracy-loss issue of differential privacy, where the post-processing property of differential privacy is leveraged for fabricating new data samples by means of a geometric model ensuring that the geometric modelling error of fabricated data samples is never larger than that of original data samples while simultaneously achieving the privacy-loss bound. Although a mathematical proof of fabricated data samples with modelling error less than that of original data samples is provided (Kumar et al., 2024), no generalisation error analysis and performance guarantees have been provided for the KAHM-based learning method. The federated learning solution, as suggested in (Kumar et al., 2024, 2023), has been introduced in a rather adhoc manner without providing a mathematical theory to justify the solution."}, {"title": "Remark 1 (Research Gap)", "content": "Existing studies on federated learning address data heterogeneity through a personalised or clustered approach, develop a theoretical analysis framework for gradient descent-based learning, and introduce a KAHM-based efficient solution without providing a mathematical theory as a suitable justification. However, these aspects have been studied separately and not together in a unified manner. Indeed, there is a lack of a uni- fied collaborative learning framework that is powerful enough to simultaneously address our formulated research questions Q1, Q2, and Q3."}, {"title": "1.3 Contributions", "content": "In this study, we give an affirmative answer to Q1, Q2, and Q3 by providing a novel approach (based on KAHMs) that harnesses the distributed computational power across clients. We provide a unified framework for the design and analysis of collaborative learning by means of geometrically inspired kernel machines such as KAHMs. Our contributions are summarised in the following four aspects:\nTheoretical Framework: We develop a framework to analyse KAHM-based collaborative learning in a federated setting. We introduce a novel kernel function defined by a global KAHM (that aggregates local KAHMs) such that the kernel function evaluates the degree of similarity between two data points in terms of their distance from training data samples. The hypothesis space for the learning is suitably (specifically, a convex hull) defined in the RKHS associated to the novel kernel function. An upper bound on the Rademacher complexity of the\nhypothesis space is provided (in Theorem 3) to derive a uniform bound on the generalisation error (in Theorem 4).\nBeyond Gradient Descent Learning Regime: Unlike most studies, we move beyond the gradient descent learning regime to derive a collaborative learning solution, that utilises the idea of KAHM's induced distance measure based aggregation of local geometrical models to build a global geometrical model. Our contribution lies in considering the global model learning problem (in Problem 1) and showing that under a realistic assumption, it is possible to derive analytically a learning solution (in Theorem 5) that does not require estimating global model parameters. The underlying assumption is that there is only a small error in the fitting of training data points by the KAHM. This assumption is realistic and is validated through various experiments as well. Since the learning solution does not require estimating the global model parameters, no rounds of communication between clients and server are required for optimising the global model, and the clients are also not required to perform multiple epochs of local optimisation using stochastic gradient descent. The advantage of our\napproach is hence that it leads to a communication and computationally efficient collaborative learning solution.\nPerformance Guarantees: The generalisation error bound allows us to derive an upper bound on the error in approximating the target function (in Theorem 7). Note that the target function approximation risk bound can be calculated in practice and decays as $O(1/\\sqrt{N})$, where $N$ is the total number of training data samples distributed across multiple clients. Remarkably, the risk bound depends only on $N$, and thus the sample complexity (i.e. the number of training samples needed for an arbitrarily small risk in approximating the target function) is calculated (in Lemma 3) and plotted (in Fig. 3). We additionally provide a deterministic analysis (in Theorem 8) that further justifies the proposed solution via an interpretation in-terms of distance from training data points.\nCompetitive Alternative: A KAHM-based learning approach provides a competitive alternative to the state of the art federated learning methods, and in fact outperforms tradi- tional solutions. Furthermore, the KAHM approach facilitates and enhances cross-domain knowledge transfer in federated settings. Experiments are used to show the improved per- formance of the proposed method when compared to the state of the art methods."}, {"title": "1.4 Proposed Approach, Novelty, and Significance", "content": "Our unified approach for the development of a collaborative learning framework for address- ing our research questions (Q1, Q2, and Q3) consists of the following 9 steps:\nStep 1: Define a KAHM induced kernel function. KAHMs let us define a novel kernel function that measures the similarity between two data points in terms of their distance from training samples of a class.\nStep 2: Define a data-dependent hypothesis space for learning. To predict the association between a class-label and a data point, the considered hypothesis space is defined by the given data samples and is in the form of a convex hull within the RKHS associated to the KAHM induced kernel function.\nStep 3: Calculate the upper bound of the Rademacher complexity of the hy- pothesis space. The Rademacher complexity of the considered hypothesis space has an upper bound such that this bound can be calculated in practice.\nStep 4: Derive the generalisation error bound for the hypothesis space. Follow- ing the standard approach, the Rademacher complexity can be used to derive a uniform bound on the generalisation error.\nStep 5: Formulate the global model learning problem. The global model learning problem can be formulated as an optimisation problem over a suitably chosen subset of the hypothesis space.\nStep 6: Exploit the convex hull form of the hypothesis space for deriving a learning solution analytically. The convex hull form of the hypothesis space can be leveraged together with a realistic assumption to derive a learning solution that does not require estimating any of the model parameters using gradient descent or any other numerical algorithm."}, {"title": "Step 7", "content": "Derive the upper bound of the error in approximating the target func- tion. The generalization error bound can be used to derive an upper bound of the error in approximating the target function."}, {"title": "Step 8", "content": "Calculate the sample complexity. The target function approximation risk bound can be used to calculate the sample complexity."}, {"title": "Step 9", "content": "Provide a deterministic analysis of the solution. The upper bound of the KAHM induced distance function can be used to analyse and interpret the solution in terms of the distance from training data points."}, {"title": "Remark 2 (Novelty)", "content": "The above 9 steps offer a novel approach to the development of a collaborative learning framework. In particular, the introduction of a KAHM-induced kernel function (step 1) and exploiting the convex hull form of the hypothesis space (step 2) for de- riving the learning solution analytically (step 6) are original. This is the first study applying geometrically inspired kernel machines (i.e., KAHMs) for a rigorous design and analysis of collaborative learning solutions."}, {"title": "Remark 3 (Significance)", "content": "The proposed approach has been carefully designed to address the formulated research questions. Q1 is addressed by steps 4, 7, and 8. Q2 and Q3 are addressed by step 6. A new deterministic way of studying and solving the learning problem is provided by step 9. The work provides a new theoretical analysis framework for learning beyond gradient descent."}, {"title": "1.5 Structure of the Paper", "content": "Section 2 presents the necessary mathematical background underlying our work. Section 3 develops the theory for KAHMs and includes steps 1-4 of the proposed approach outlined ear- lier. Section 4 continues with steps 5-9, thereby solving the collaborative learning problem. The experimental evaluation of our approach is given in Section 5, followed by concluding remarks in Section 6."}, {"title": "2. Mathematical Prerequisites and Notations", "content": "This section introduces the notation used throughout, presents the distributed data setting, and provides a review of the notion of KA\u041d\u041c."}, {"title": "2.1 Notation", "content": "In this paper, all matrices are denoted using boldface font. The following notation is used:\n\u2022 Let $n, p, c, N, Q, C' \\in \\mathbb{Z}^+$ be the positive integers.\n\u2022 For a set $\\{y^1,\\ldots, y^N\\} \\subset \\mathbb{R}^p$, its affine hull is denoted as $\\text{aff}(\\{y^1,\\ldots, y^N\\})$.\n\u2022 For a scalar $a \\in \\mathbb{R}$, $|a|$ denotes its absolute value. For a set $A$, $|A|$ denotes its cardi- nality. For a real matrix $Y$, $Y^T$ is the transpose of $Y$.\n\u2022 For a vector $y \\in \\mathbb{R}^p$, $||y||$ denotes the Euclidean norm and $y_j$ (and also $(y)_j$) denotes the $j^{th}$ element. For a matrix $Y \\in \\mathbb{R}^{N \\times p}$, $||Y||_2$ denotes the spectral norm, $||Y||_F$"}, {"title": "2.2 Distributed Data Setting", "content": "We consider the multi-class problem, where a label vector $z \\in \\{0,1\\}^C$ with $C > 2$ is assigned to a data point $y \\in \\mathbb{R}^p$ such that the $c$-th element of vector $z$, $z_c \\in \\{0,1\\}$, represents the association of $y$ with the $c$-th class. We consider the problem of collaborative learning from distributed data, where a number of clients $Q$ ($Q > 1$) participate in the learning. Let $q \\in \\{1,2,\\ldots,Q\\}$ be the client characterising variable. Let $D$ be a set consisting of $N$ number of samples drawn i.i.d. according to the distribution $P_{y,z,q}$:\n$D := \\{(y^i, z^i, q^i) \\in \\mathbb{R}^p \\times \\{0,1\\}^C \\times \\{1,2,\\ldots, Q\\} | i\\in \\{1,2,\\ldots,N\\}\\} \\sim (P_{y,z,q})^N$.                                                                                                                                                                                                                                                                                                                                                                                            (4)\nLet $\\mathcal{I}_{c,q}$ be the set of indices of those samples in the sequence $((y^i, z^i, q^i) \\in D)_{i=1}^N$ which are $c^{th}$ class labelled and owned by client $q$, i.e.,\n$\\mathcal{I}_{c,q} := \\{i \\in \\{1,2,\\ldots,N\\} | (z^i)_c = 1, q^i = q\\}$.                                                                                                                                                                                                                                                                                                                                                                                            (5)\nLet $(I_{1}^{c,q},\\ldots, I_{|\\mathcal{I}_{c,q}|}^{c,q})$ be the sequence of elements of $\\mathcal{I}_{c,q}$ in ascending order, i.e.,\n$\\begin{aligned} I_{1}^{c,q} &= \\min(\\mathcal{I}_{c,q}),\\\\I_{i}^{c,q} &= \\min(\\mathcal{I}_{c,q} \\setminus \\{I_{1},\\cdots, I_{i-1}\\})\\end{aligned}$.                                                                                                                                                                                                                                                                                                                                                                                            (6)\n(7)\nfor $i \\in \\{2,\\ldots, |\\mathcal{I}_{c,q}|\\}$. Let $Y^{c,q} \\in \\mathbb{R}^{|\\mathcal{I}_{c,q}| \\times p}$ be the matrix storing the $c^{th}$ class labelled and $q^{th}$ client owned samples, i.e.,\n$Y^{c,q} = [y^{I_{1}^{c,q}} \\ldots y^{I_{|\\mathcal{I}_{c,q}|}^{c,q}}]^T$                                                                                                                                                                                                                                                                                                                                                                                             (8)"}, {"title": "Remark 4 (Addressing Statistical Heterogeneity)", "content": "Our analysis takes into account the heterogeneity among client's data distributions by automatically considering, for arbitrary clients $q^i$ and $q^j$ with $i \\neq j$, the following cases as well:\n$\\begin{aligned} P_{y,z|q}(\\cdot,\\cdot|q = q^i) &\\neq P_{y,z|q}(\\cdot,\\cdot|q = q^j),\\\\P_{z|y,q}(\\cdot|y, q = q^i) &\\neq P_{z|y,q}(\\cdot|y,q = q^j).\n\\end{aligned}$                                                                                                                                                                                                                                                                                                                                                                                            (9)\n(10)\nHence, data shifts amongst clients are being considered."}, {"title": "2.3 A Review of Kernel Affine Hull Machines", "content": "We recall the notion of KAHM as originally defined in (Kumar et al., 2024).\nDefinition 1 (Kernel Affine Hull Machine (KAHM) (Kumar et al., 2024)). Given a finite number of samples: $Y = [y^1 \\ldots y^N]^T$ with $y^1,\\ldots,y^N \\in \\mathbb{R}^p$ and a subspace dimension $n \\leq p$; a kernel affine hull machine $A_{Y,n} : \\mathbb{R}^p \\rightarrow \\text{aff}(\\{y^1,\\ldots,y^N\\})$ maps an arbitrary point $y \\in \\mathbb{R}^p$ onto the affine hull of $\\{y^1,\\ldots,y^N\\}$.\nThe complete definition of $A_{Y,n}$ is provided in Appendix A. In addition, the subspace dimension $n < p$ is practically determined for the given samples using a procedure given in Appendix B.\nRemark 5 (KAHM for Automated Machine Learning (AutoML)). A KAHM $A_{Y,n}$, with the choice of subspace dimension $n$ as suggested in Appendix B, is completely defined by the data samples $Y$ without involving any free parameters to be tuned, allowing us to define an AutoML approach by means of KAHMs. Consequently, in what follows we use $A_Y$ to denote \u03b1 KAHM.\nTheorem 1 (KAHM as a Bounded Function (Kumar et al., 2024)). The KAHM $A_Y$, associated to $Y = [y^1 \\ldots y^N]^T$ with $y^1,\\ldots,y^N \\in \\mathbb{R}^p$, is a bounded function on $\\mathbb{R}^p$ such that for any $y \\in \\mathbb{R}^p$,\n$||A_Y(y)|| \\le ||Y||_2 \\Big(1 + \\frac{pN^2}{2||Y||_F^2}\\Big)$.                                                                                                                                                                                                                                                                                                                                                                                            (11)\nThus, the image of $A_Y$ is bounded such that\n$A_Y[\\mathbb{R}^p] \\subset \\{y \\in \\mathbb{R}^p | ||y|| \\le ||Y||_2 \\Big(1 + \\frac{pN^2}{2||Y||_F^2}\\Big)\\}$.                                                                                                                                                                                                                                                                                                                                                                                            (12)\nDefinition 2 (A Distance Function Induced by KAHM (Kumar et al., 2024)). Given a KAHM $A_Y$, the distance of an arbitrary point $y \\in \\mathbb{R}^p$ from its image under $A_Y$ is given as\n$\\Gamma_{A_Y}(y) := ||y - A_Y(y)||$.                                                                                                                                                                                                                                                                                                                                                                                             (13)\nTheorem 2 ($\\Gamma_{A_Y}(\\cdot)$ as a Measure of Distance from Data Points (Kumar et al., 2024)). The ratio of the distance of a point $y \\in \\mathbb{R}^p$ from its image under $A_Y$ to the distance of $y$ from $\\{y^1,\\ldots,y^N\\}$ evaluated as $||[ y - y^1 \\ldots y - y^N ]||_2$ remains upper bounded as\n$\\frac{\\Gamma_{A_Y}(y)}{||[ y - y^1 \\ldots y - y^N ]||_2} \\le 1 + \\frac{pN^2}{2||Y||_F^2}$.                                                                                                                                                                                                                                                                                                                                                                                            (14)\nTheorem 2 states that if a point $y$ is close to points $\\{y^1,\\ldots,y^N\\}$, then the value $\\Gamma_{A_Y}(y)$ cannot be large. Thus, a large value of the distance function at a point $y$ indicates that $y$ must be at a far distance from $\\{y^1,\\ldots,y^N\\}$."}, {"title": "2.4 Combining Locally Distributed KAHMs to build a Global KAHM using the Distance Function", "content": "We consider the scenario where class labelled data samples are distributed amongst Q different clients. Given Q different KAHMS $A_{Y_{c,1}},\\ldots, A_{Y_{c,Q}}$ built independently using data matrices $Y^{c,1},\\ldots, Y^{c,Q}$ respectively, a possible way to combine together the KAHMs is as follows:\n$\\begin{aligned}G_c(y) &= A_{Y_{c,\\hat{q}(y)}}(y) \\\\hat{q}(y) &= \\underset{q\\in\\{1,2,...,Q\\}}{\\text{argmin}} \\Gamma_{A_{Y_{c,q}}}(y),\n\\end{aligned}$                                                                                                                                                                                                                                                                                                                                                                                            (15)\n(16)\nwhere $G_c$ is the global KAHM (that combines together the individual KAHMs) and $\\Gamma_{A_{Y_{c,q}}}$ is the distance function induced by $A_{Y_{c,q}}$. A 2-dimensional data example where two dif- ferent KAHMs are combined to build a global KAHM is provided in Figure 2. Figure 2 shows the images of individual KAHMs (in Figure 2(a)) and the image of global KAHM (in Figure 2(b)).\nDefinition 3 (A Distance Function Induced by Global KAHM). Given a global KAHM ($G_c$) that combines together Q local KAHMs ($A_{Y_{c,1}},\\ldots, A_{Y_{c,Q}}$), the distance of an arbitrary point $y \\in \\mathbb{R}^p$ from its image under $G_c$ is given as\n$\\begin{aligned}\\Gamma_{G_c}(y) &:= ||y - G_c(y)||\n&= \\underset{q\\in\\{1,2,...,Q\\}}{\\text{min}} \\Gamma_{A_{Y_{c,q}}}(y).\n\\end{aligned}$                                                                                                                                                                                                                                                                                                                                                                                            (17)\n(18)"}, {"title": "3. Theory for Learning with Kernel Affine Hull Machines", "content": "This section develops a theory for learning by means of KAHMs. For this, a geometrically inspired kernel function and corresponding RKHS is presented in Section 3.1, followed by a definition of a hypothesis space in Section 3.2. Section 3.3 evaluates the Rademacher com- plexity of the hypothesis space, which is then used in Section 3.4 to derive the generalisation error bound for the hypothesis space."}, {"title": "3.1 A Novel Kernel Function Induced by the Global KAHM", "content": "A given global KAHM, $G_c$, induces a function, $K_c : \\mathbb{R}^p \\times \\mathbb{R}^p \\rightarrow [0, 1]$, defined as\n$K_c(y^1, y^2) := \\text{exp}\\Big(-\\frac{1}{p} \\Gamma_{G_c}(y^1)^2\\Big) \\text{exp}\\Big(-\\frac{1}{p} \\Gamma_{G_c}(y^2)^2\\Big)$.                                                                                                                                                                                                                                                                                                                                                                                             (19)\n$K_c$ is a positive definite kernel, since\n1. $K_c(y^1, y^2) = K_c(y^2, y^1)$, and\n2. for every $y^1,\\ldots, y^N \\in \\mathbb{R}^p$ and $\\alpha_1,\\ldots,\\alpha_N \\in \\mathbb{R}$,\n$\\sum_{i,j=1}^N \\alpha_i \\alpha_j K_c(y^i, y^j) \\ge 0$.                                                                                                                                                                                                                                                                                                                                                                                            (20)\nTo see (20), consider\n$\\begin{aligned} &\\sum_{i,j=1}^N \\alpha_i \\alpha_j K_c(y^i, y^j) \\\\\n&= \\sum_{i,j=1}^N \\alpha_i \\text{exp}\\Big(-\\frac{1}{p} \\Gamma_{G_c}(y^i)^2\\Big) \\alpha_j \\text{exp}\\Big(-\\frac{1}{p} \\Gamma_{G_c}(y^j)^2\\Big)\\\\\n&= \\sum_{i=1}^N \\Big(\\alpha_i \\text{exp}\\Big(-\\frac{1}{p} \\Gamma_{G_c}(y^i)^2\\Big)\\Big)^2 \\ge 0.\n\\end{aligned}$                                                                                                                                                                                                                                                                                                                                                                                            (21)\nRemark 6 (Justification and Interpretation of $K_c$). $K_c(y^1, y^2)$ will be high, only if both $y^1$ and $y^2$ lie close to the $c^{th}$ class labelled samples that may have been owned by any of the $Q$ clients. Thus, $K_c$ provides a measure of similarity between two data points in-terms of their association to the $c^{th}$ class. That is, $K_c(y^1, y^2)$ will be high even in the case when $y^1$ and $y^2$ are at far distance from each other but both $y^1$ and $y^2$ are close to some $c^{th}$ class labelled samples. This property of $K_c$ justifies its choice as kernel function for predicting the association of a point to $c$-th class.\nNow the RKHS associated to $K_c$ is given by:\n$\\begin{aligned} \\mathcal{H}_{K_c}(\\mathbb{R}^p) &= \\Big\\{f(\\cdot) = \\sum_{i=1}^L a_i K_c(\\cdot, y^i) | a_i \\in \\mathbb{R}, y^i \\in \\mathbb{R}^p, L < \\infty, ||f||_{\\mathcal{H}_{K_c}(\\mathbb{R}^p)} := \\sum_{i,j=1}^{L} a_i \\alpha_j K_c(y^i, y^j) < \\infty\\Big\\}                                                                                                                                                                                                                                                                                                                                                                                            (22)\\\\ \\end{aligned}\nwith inner product for any $f(.) = \\sum_{i=1}^{L} a_i K_c(\\cdot, s^i)$ (with $a^i \\in \\mathbb{R}, s^i \\in \\mathbb{R}^p$) and $g(.) = \\sum_{i=1}^{M} b_j K_c(\\cdot, t^j) \\in \\mathcal{H}_{K_c}(\\mathbb{R}^p)$ (with $b_j \\in \\mathbb{R}, t^j \\in \\mathbb{R}^p$) defined as\n$(f, g)_{\\mathcal{H}_{K_c}(\\mathbb{R}^p)} := \\sum_{i=1}^L \\sum_{j=1}^M a_i b_j K_c(s^i, t^j)$.                                                                                                                                                                                                                                                                                                                                                                                            (23)"}, {"title": "3.2 A Data-Dependent Hypothesis Space", "content": "Let $f_{y \\leftrightarrow z_c"}, "mathbb{R}^p \\rightarrow \\mathbb{R}$ be a function such that $f_{y \\leftrightarrow z_c}(y)$ serves as an approximation to $z_c$, i.e., $f_{y \\leftrightarrow z_c}(y)$ predicts the association of the data point $y$ to the $c^{th}$ class. Given the data set $D$, as defined in (4), the hypothesis space for predicting the association of a point to the $c^{th}$ class is defined as a convex hull within $\\mathcal{H}_{K_c}(\\mathbb{R}^p)$:\n$\\begin{aligned} &\\mathcal{M}_{D,c} := \\Big\\{f_{y \\leftrightarrow z_c} | f_{y \\leftrightarrow z_c} = \\sum_{i=1}^N \\alpha_{c,i} K_c(\\cdot, y^i), \\alpha_{c,i} \\in [0,1"]}