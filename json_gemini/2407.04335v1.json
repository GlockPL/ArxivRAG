[{"title": "Central Problem", "authors": ["Mohit Kumar", "Alexander Valentinitsch", "Magdalena Fuchs", "Mathias Brucker", "Juliana Bowles", "Adnan Husakovic", "Ali Abbas", "Bernhard A. Moser"], "abstract": "This paper develops a novel mathematical framework for collaborative learning by means of geometrically inspired kernel machines which includes statements on the bounds of generalisation and approximation errors, and sample complexity. For classification prob- lems, this approach allows us to learn bounded geometric structures around given data points and hence solve the global model learning problem in an efficient way by exploiting convexity properties of the related optimisation problem in a Reproducing Kernel Hilbert Space (RKHS). In this way, we can reduce classification problems to determining the clos- est bounded geometric structure from a given data point. Further advantages that come with our solution is that our approach does not require clients to perform multiple epochs of local optimisation using stochastic gradient descent, nor require rounds of communica- tion between client/server for optimising the global model. We highlight that numerous experiments have shown that the proposed method is a competitive alternative to the state-of-the-art.", "sections": [{"title": "Introduction", "content": "In different domains of modern industry such as iron- or steelmaking, substantial amounts of data is generated and fused together to describe complex processes. These data are often distributed from multiple data vendors and clients, making collaborative data analysis and model training challenging. A potential bottleneck of using traditional centralized machine learning approaches is the overall data aggregation to a single location. The centralization of the data is not always feasible due to privacy concerns and logistical constraints.\nOur focus here is on collaborative learning from distributed and privately owned data. Federated learning is an increasingly popular approach to collaborative learning between multiple clients without the need to exchange raw training data. Given this advantage, federated learning can play a crucial role in process industry by leveraging distributed data to improve model performance while preserving data privacy. The classical federated learning approach (McMahan et al., 2017; Li et al., 2020; Wang et al., 2020) aims to train a common global model by repeating the following two operations: 1) training client local models using local data, and 2) aggregating these local models to update a global model. However, the sampling of distributed data from different local distributions makes it challenging to design and analyse efficient federated learning algorithms in practice (Ye et al., 2023; Vahidian et al., 2024). In general, essential requirements for federated learning include: 1) the capability of addressing heterogeneity among local data distributions, 2) the support for communication efficiency (allowing clients to transfer the required amount of parameters to the server under limited communication bandwidth), and 3) overall computational efficiency (for real-time operations) (Kairouz et al., 2021)."}, {"title": "Central Problem", "content": "In order to develop an accurate collaborative learning method that is efficient in both com- munication and computation, we formulate the following research questions:\nQ1: Can we build a theoretical analysis framework for collaborative learning from distributed and statistically heterogeneous data that, without making any assumptions on data distributions, allows us to calculate in practice: 1) the generalisation and approximation error bounds, and 2) the minimum number of training samples required to reduce the risk in approximating the target function below $\\epsilon$ (for any $\\epsilon > 0$) with probability at least 1 \u2013 $\\delta$ (for any $\\delta \\in (0,1))$?\nQ2: Can we solve the global model optimisation problem in the federated setting without requiring multiple rounds of communication between clients and the server?\nQ3: Can kernel machine learning theory provide a competitive and computationally efficient alternative to stochastic gradient descent-based optimisation in a federated setting?"}, {"title": "The State of the Art", "content": "Addressing Data Heterogeneity. The issue of data heterogeneity in federated learning has been previously addressed by learning a personalised model for each client assuming that data features share a common global representation, while statistical heterogeneity across\nTheoretical Analysis Framework. In federated learning, the underlying models can be chosen from a reproducing kernel Hilbert space (Hong & Chae, 2022; Ghari & Shen, 2022) allowing for an application of the powerful kernel theory for design and analysis. Kernels have been applied in machine learning over the years (Hofmann et al., 2008; Rudi et al., 2017) and have recently gained renewed attention. In particular, the parallels between the properties of deep neural networks and kernel methods have been established to indicate that some key phenomena of deep learning are manifested similarly in kernel methods in the overfitted regime (Belkin, Ma, & Mandal, 2018), and deep kernel machines have been introduced (Wilson et al., 2016; Nikhitha et al., 2021). Kernel-based models are effective for learning representations (Gholami & Hajisami, 2016; Kampffmeyer et al., 2018; Laforgue et al., 2019) and facilitate analytical solutions for learning problems using a broad range of mathematical techniques. A convergence guarantee for federated learning can be established for strongly convex and smooth objective functions (Li et al., 2020; Khaled et al., 2020; Qu et al., 2023). For one-hidden layer neural network with ReLU activations, an analysis of federated learning can be provided (Li et al., 2021) by describing the training dynamics of federated learning by means of Neural Tangent Kernel (NTK) (Jacot et al., 2018). The gradient descent training dynamics of artificial neural networks follows that of the gradient descent of the functional cost with respect to a kernel: NTK (Jacot et al., 2018). A \u039d\u03a4\u039a based framework makes use of the theory on over-parameterised neural networks to provide proof of convergence of gradient descent and generalisation bound for over-parameterized ReLU neural networks in federated learning (Huang et al., 2021).\nVariational Optimisation as an Alternative to Gradient Descent. A kernel-based approach that does not rely on gradient descent-based learning and instead uses variational optimisation for deriving analytically the learning solutions, has been previously studied (Kumar & Freudenthaler, 2020; Kumar et al., 2021; Zhang et al., 2022; Kumar et al., 2021; Zhang et al., 2023; Kumar et al., 2021b, 2021a, 2023). This kernel-based variational optimisation approach was considered for privacy-preserving learning under the differential privacy framework (Kumar, 2023; Kumar et al., 2021, 2020) and fully homomorphic encryption (Ku- mar et al., 2023), and can potentially be explored for federated learning as well. So far there have been no attempts to extend the variational optimisation approach to federated learning in such a way that it can handle all our research questions Q1, Q2, and Q3."}, {"title": "Geometrically Inspired Kernel Approach as an Efficient Alternative", "content": "For collab- orative learning in a federated setting, a geometrically inspired kernel approach has been introduced (Kumar, Moser, & Fischer, 2024, 2023). A recent paper (Kumar et al., 2024) introduced a so-called Kernel Affine Hull Machine (KAHM), wherein a representation of given data points is learned in RKHS to define a bounded geometric structure around data points within the affine hull of data points. The KAHM makes it possible to compute at any arbitrary point a measure of its distance from the data samples. The significance of this is that the KAHM's induced distance measure cannot only be used for classification, but for federated learning by aggregating locally trained KAHMs to build a global \u041a\u0410\u041d\u041c. Note that the crucial significance of KAHMs for learning from distributed data comes from the fact that a global model can be built by aggregating local models simply using a distance measure without requiring gradient-based learning of the global model parameters. This is best illustrated through an example as shown in Fig. 1. Consequently, a KAHM-based ap- proach is computationally more efficient and indeed promising for federated learning (Kumar et al., 2023, 2024). Moreover, KAHMs can be used to mitigate the accuracy-loss issue of differential privacy, where the post-processing property of differential privacy is leveraged for fabricating new data samples by means of a geometric model ensuring that the geometric modelling error of fabricated data samples is never larger than that of original data sam- ples while simultaneously achieving the privacy-loss bound. Although a mathematical proof of fabricated data samples with modelling error less than that of original data samples is provided (Kumar et al., 2024), no generalisation error analysis and performance guarantees have been provided for the KAHM-based learning method. The federated learning solution, as suggested in (Kumar et al., 2024, 2023), has been introduced in a rather adhoc manner without providing a mathematical theory to justify the solution."}, {"title": "Contributions", "content": "In this study, we give an affirmative answer to Q1, Q2, and Q3 by providing a novel approach (based on KAHMs) that harnesses the distributed computational power across clients. We provide a unified framework for the design and analysis of collaborative learning by means of geometrically inspired kernel machines such as KAHMs. Our contributions are summarised in the following four aspects:\nTheoretical Framework: We develop a framework to analyse KAHM-based collaborative learning in a federated setting. We introduce a novel kernel function defined by a global KAHM (that aggregates local KAHMs) such that the kernel function evaluates the degree of similarity between two data points in terms of their distance from training data samples. The hypothesis space for the learning is suitably (specifically, a convex hull) defined in the RKHS associated to the novel kernel function. An upper bound on the Rademacher complexity of the\nhypothesis space is provided (in Theorem 3) to derive a uniform bound on the generalisation error (in Theorem 4).\nBeyond Gradient Descent Learning Regime: Unlike most studies, we move beyond the gradient descent learning regime to derive a collaborative learning solution, that utilises the idea of KAHM's induced distance measure based aggregation of local geometrical models to build a global geometrical model. Our contribution lies in considering the global model learning problem (in Problem 1) and showing that under a realistic assumption, it is possible to derive analytically a learning solution (in Theorem 5) that does not require estimating global model parameters. The underlying assumption is that there is only a small error in the fitting of training data points by the KAHM. This assumption is realistic and is validated through various experiments as well. Since the learning solution does not require estimating the global model parameters, no rounds of communication between clients and server are required for optimising the global model, and the clients are also not required to perform multiple epochs of local optimisation using stochastic gradient descent. The advantage of our\napproach is hence that it leads to a communication and computationally efficient collaborative learning solution.\nPerformance Guarantees: The generalisation error bound allows us to derive an upper bound on the error in approximating the target function (in Theorem 7). Note that the target function approximation risk bound can be calculated in practice and decays as O(1/\u221aN), where N is the total number of training data samples distributed across multiple clients. Remarkably, the risk bound depends only on N, and thus the sample complexity (i.e. the number of training samples needed for an arbitrarily small risk in approximating the target function) is calculated (in Lemma 3) and plotted (in Fig. 3). We additionally provide a deterministic analysis (in Theorem 8) that further justifies the proposed solution via an interpretation in-terms of distance from training data points.\nCompetitive Alternative: A KAHM-based learning approach provides a competitive alternative to the state of the art federated learning methods, and in fact outperforms tradi- tional solutions. Furthermore, the KAHM approach facilitates and enhances cross-domain knowledge transfer in federated settings. Experiments are used to show the improved per- formance of the proposed method when compared to the state of the art methods."}, {"title": "Proposed Approach, Novelty, and Significance", "content": "Our unified approach for the development of a collaborative learning framework for address- ing our research questions (Q1, Q2, and Q3) consists of the following 9 steps:\nStep 1: Define a KAHM induced kernel function. KAHMs let us define a novel kernel function that measures the similarity between two data points in terms of their distance from training samples of a class.\nStep 2: Define a data-dependent hypothesis space for learning. To predict the association between a class-label and a data point, the considered hypothesis space is defined by the given data samples and is in the form of a convex hull within the RKHS associated to the KAHM induced kernel function.\nStep 3: Calculate the upper bound of the Rademacher complexity of the hy- pothesis space. The Rademacher complexity of the considered hypothesis space has an upper bound such that this bound can be calculated in practice.\nStep 4: Derive the generalisation error bound for the hypothesis space. Follow- ing the standard approach, the Rademacher complexity can be used to derive a uniform bound on the generalisation error.\nStep 5: Formulate the global model learning problem. The global model learning problem can be formulated as an optimisation problem over a suitably chosen subset of the hypothesis space.\nStep 6: Exploit the convex hull form of the hypothesis space for deriving a learning solution analytically. The convex hull form of the hypothesis space can be leveraged together with a realistic assumption to derive a learning solution that does not require estimating any of the model parameters using gradient descent or any other numerical algorithm.\nStep 7: Derive the upper bound of the error in approximating the target func- tion. The generalization error bound can be used to derive an upper bound of the error in approximating the target function.\nStep 8: Calculate the sample complexity. The target function approximation risk bound can be used to calculate the sample complexity.\nStep 9. Provide a deterministic analysis of the solution. The upper bound of the KAHM induced distance function can be used to analyse and interpret the solution in terms of the distance from training data points.\nRemark 2 (Novelty). The above 9 steps offer a novel approach to the development of a collaborative learning framework. In particular, the introduction of a KAHM-induced kernel function (step 1) and exploiting the convex hull form of the hypothesis space (step 2) for de- riving the learning solution analytically (step 6) are original. This is the first study applying geometrically inspired kernel machines (i.e., KAHMs) for a rigorous design and analysis of collaborative learning solutions.\nRemark 3 (Significance). The proposed approach has been carefully designed to address the formulated research questions. Q1 is addressed by steps 4, 7, and 8. Q2 and Q3 are addressed by step 6. A new deterministic way of studying and solving the learning problem is provided by step 9. The work provides a new theoretical analysis framework for learning beyond gradient descent."}, {"title": "Structure of the Paper", "content": "Section 2 presents the necessary mathematical background underlying our work. Section 3 develops the theory for KAHMs and includes steps 1-4 of the proposed approach outlined ear- lier. Section 4 continues with steps 5-9, thereby solving the collaborative learning problem. The experimental evaluation of our approach is given in Section 5, followed by concluding remarks in Section 6."}, {"title": "Mathematical Prerequisites and Notations", "content": "This section introduces the notation used throughout, presents the distributed data setting, and provides a review of the notion of KA\u041d\u041c."}, {"title": "Notation", "content": "In this paper, all matrices are denoted using boldface font. The following notation is used:\n\u2022 Let $n, p, c, N, Q, C' \\in \\mathbb{Z}+$ be the positive integers.\n\u2022 For a set $\\{y^1,..., y^N\\} \\subset \\mathbb{R}^p$, its affine hull is denoted as $aff(\\{y^1,..., y^N\\})$.\n\u2022 For a scalar $a \\in \\mathbb{R}$, $|a|$ denotes its absolute value. For a set A, $|A|$ denotes its cardi- nality. For a real matrix Y, $Y^T$ is the transpose of Y.\n\u2022 For a vector $y \\in \\mathbb{R}^p$, $||y||$ denotes the Euclidean norm and $y_j$ (and also $(y)_j$) denotes the $j^{th}$ element. For a matrix $Y \\in \\mathbb{R}^{N \\times p}$, $||Y||_2$ denotes the spectral norm, $||Y||_F$"}, {"title": "Distributed Data Setting", "content": "We consider the multi-class problem, where a label vector $z \\in \\{0,1\\}^C$ with $C > 2$ is assigned to a data point $y \\in \\mathbb{R}^p$ such that the $c$-th element of vector $z$, $z_c \\in \\{0,1\\}$, represents the association of $y$ with the $c$-th class. We consider the problem of collaborative learning from distributed data, where a number of clients $Q$ ($Q > 1$) participate in the learning. Let $q \\in \\{1,2,...,Q\\}$ be the client characterising variable. Let $\\mathcal{D}$ be a set consisting of N number of samples drawn i.i.d. according to the distribution $P_{y,z,q}$:\n$\\mathcal{D} := \\{(y^i, z^i, q^i) \\in \\mathbb{R}^p \\times \\{0,1\\}^C \\times \\{1,2,..., Q\\} | i\\in \\{1,2,...,N\\}\\} \\sim (P_{y,z,q})^N$. (4)\nLet $\\mathcal{I}_c^q$ be the set of indices of those samples in the sequence $((y^i, z^i, q^i) \\in \\mathcal{D})_1^N$ which are $c^{th}$ class labelled and owned by client $q$, i.e.,\n$\\mathcal{I}_c^q := \\{i \\in \\{1,2,...,N\\} | (z^i)_c = 1, q^i = q\\} .$ (5)\nLet $(I_1^{c,q}, ..., I_{|\\mathcal{I}_c^q|}^{c,q})$ be the sequence of elements of $\\mathcal{I}_c^q$ in ascending order, i.e.,\n$\\begin{aligned}I_1^{c,q} &= \\min(\\mathcal{I}_c^q), \\\\I_i^{c,q} &= \\min(\\mathcal{I}_{c,q} \\setminus \\{I_1, ..., I_{i-1}\\}) ,\\end{aligned}$ (6)\nfor $i \\in \\{2,..., |\\mathcal{I}^{c,q}|\\}$. Let $Y^{c,q} \\in \\mathbb{R}^{|\\mathcal{I}^{c,q}|\\times p}$ be the matrix storing the cth class labelled and $q^{th}$ client owned samples, i.e.,\n$Y^{c,q} = \\begin{bmatrix}y^{I_1^{c,q}}\\  ... \\\\ y^{I_{|\\mathcal{I}^{c,q}|}^{c,q}}\\end{bmatrix}^T.$ (8)"}, {"title": "A Review of Kernel Affine Hull Machines", "content": "We recall the notion of KAHM as originally defined in (Kumar et al., 2024).\nGiven a finite number of samples: $Y = [y^1 ... y^N]^T$ with $y^1,...,y^N \\in \\mathbb{R}^p$ and a subspace dimension $n \\leq p$; a kernel affine hull machine $A_{Y,n} : \\mathbb{R}^p \\to aff(\\{y^1,...,y^N\\})$ maps an arbitrary point $y \\in \\mathbb{R}^p$ onto the affine hull of $\\{y^1,...,y^N\\}$.\nThe complete definition of $A_{Y,n}$ is provided in Appendix A. In addition, the subspace dimension $n < p$ is practically determined for the given samples using a procedure given in Appendix B.\nA KAHM $A_{Y,n}$, with the choice of subspace dimension $n$ as suggested in Appendix B, is completely defined by the data samples Y without involving any free parameters to be tuned, allowing us to define an AutoML approach by means of KAHMs. Consequently, in what follows we use $A_Y$ to denote a KAHM.\nThe KAHM $A_{Y}$, associated to $Y = [y^1 ... y^N]^T$ with $y^1,...,y^N \\in \\mathbb{R}^p$, is a bounded function on $\\mathbb{R}^p$ such that for any $y \\in \\mathbb{R}^p$,\n$||A_Y(y)|| < ||Y||_2 \\left(1 + \\frac{pN^2}{2||Y||_F^2}\\right).$ (11)\nThus, the image of $A_Y$ is bounded such that\n$A_Y[\\mathbb{R}^p] \\subset \\{y \\in \\mathbb{R}^p | ||y|| < ||Y||_2 \\left(1 + \\frac{pN^2}{2||Y||_F^2}\\right)\\} .$ (12)\nGiven a KAHM $A_Y$, the distance of an arbitrary point $y \\in \\mathbb{R}^p$ from its image under $A_Y$ is given as\n$\\Gamma_{A_Y}(y) := ||y \u2013 A_Y(y)|| .$ (13)\nThe ratio of the distance of a point $y \\in \\mathbb{R}^p$ from its image under $A_Y$ to the distance of $y$ from $\\{y^1,...,y^N\\}$ evaluated as $||[ y - y^1 ... y \u2013 y^N ]||_2$ remains upper bounded as\n$\\frac{\\Gamma_{A_Y}(y)}{||[ y - y^1 ... y \u2013 y^N ]||_2} < 1 + \\frac{pN^2}{2||Y||_F^2}$ (14)"}, {"title": "Combining Locally Distributed KAHMs to build a Global KAHM using the Distance Function", "content": "We consider the scenario where class labelled data samples are distributed amongst Q dif- ferent clients. Given Q different KAHMS $A_{Y^{c,1}}, ..., A_{Y^{c,Q}}$ built independently using data matrices $Y^{c,1},..., Y^{c,Q}$ respectively, a possible way to combine together the KAHMs is as follows:\n$\\begin{aligned}G_c(y) &= A_{Y^{c,\\hat{q}(Y)}}(Y) \\\\ \\hat{q}(y) &= \\underset{q\\in\\{1,2,...,Q\\}}{argmin} \\Gamma_{A_{Y^{c,q}}}(Y),\\end{aligned}$ (15)\nwhere $G_c$ is the global KAHM (that combines together the individual KAHMs) and $\\Gamma_{A_{Y^{c,q}}}$ is the distance function induced by $A_{Y^{c,q}}$.\nGiven a global KAHM $(G_c)$ that combines together Q local KAHMs $(A_{Y^{c,1}}, ..., A_{Y^{c,Q}})$, the distance of an arbitrary point $y \\in \\mathbb{R}^p$ from its image under $G_c$ is given as\n$\\begin{aligned}\\Gamma_{G_c}(y) &:= ||y \u2013 G_c(y)|| \\\\ &= \\underset{q\\in\\{1,2,...,Q\\}}{min} \\Gamma_{A_{Y^{c,q}}}(y).\n\\end{aligned}$ (17)"}, {"title": "Theory for Learning with Kernel Affine Hull Machines", "content": "This section develops a theory for learning by means of KAHMs. For this, a geometrically inspired kernel function and corresponding RKHS is presented in Section 3.1, followed by a definition of a hypothesis space in Section 3.2. Section 3.3 evaluates the Rademacher com- plexity of the hypothesis space, which is then used in Section 3.4 to derive the generalisation error bound for the hypothesis space."}, {"title": "A Novel Kernel Function Induced by the Global KAHM", "content": "A given global KAHM, $G_c$, induces a function, $K_c : \\mathbb{R}^p \\times \\mathbb{R}^p \\to [0, 1]$, defined as\n$K_c(y^1, y^2) := exp\\left(-\\frac{1}{p} \\Gamma_{G_c}(y^1)^2\\right) exp\\left(-\\frac{1}{p} \\Gamma_{G_c}(y^2)^2\\right)$ . (19)\nKe is a positive definite kernel, since\n1. $K_c(y^1, y^2) = K_c(y^2, y^1)$, and\n2. for every $y^1,..., y^N \\in \\mathbb{R}^p$ and $a_1,...,a_N \\in \\mathbb{R}$,\n$\\underset{i,j=1}{\\Sigma^N} a_i a_j K_c(y^i, y^j) \\geq 0.$ (20)\nTo see (20), consider\n$\\begin{aligned}\\underset{i,j=1}{\\Sigma^N} a_i a_j K_c(y^i, y^j) &= \\underset{i,j=1}{\\Sigma^N} a_i exp\\left(-\\frac{1}{p} \\Gamma_{G_c}(y^i)^2\\right) a_j exp\\left(-\\frac{1}{p} \\Gamma_{G_c}(y^j)^2\\right) \\\\&=\\underset{i=1}{\\Sigma^N} a_i  exp\\left(-\\frac{1}{p} \\Gamma_{G_c}(y^i)^2\\right)  ^2 \\geq 0.\n\\end{aligned}$ (21)\nNow the RKHS associated to Ke is given by:\n$H_{K_c}(\\mathbb{R}^p) : = \\left\\{ f(.) = \\underset{i=1}{\\Sigma^L} a_i K_c(., y^i) | a_i \\in \\mathbb{R}, y^i \\in \\mathbb{R}^p, ||f||_{H_{K_c}(\\mathbb{R}^p)} := \\underset{i,j=1}{\\Sigma^{L}} a_i a_j K_c(y^i, y^j) < \\infty \\right\\}$ (22)\nwith inner product for any $f(.) = \\underset{i=1}{\\Sigma^{L}} a_i K_c(., s^i)$ (with $a \\in \\mathbb{R}, s^i \\in \\mathbb{R}^p$) and $g(.) = \\underset{i=1}{\\Sigma^{M}} b_j K_c(., t^i) \\in H_{K_c}(\\mathbb{R}^p)$ (with $b_j \\in \\mathbb{R}, t^i \\in \\mathbb{R}^p$) defined as\n$(f, g)_{H_{K_c}(\\mathbb{R}^p)} := \\underset{i=1}{\\Sigma^{L}} \\underset{j=1}{\\Sigma^{M}} a_i b_j K_c(s^i, t^j) . $ (23)"}, {"title": "A Data-Dependent Hypothesis Space", "content": "Let $f_{y\\mapsto z_c}: \\mathbb{R}^p \\to \\mathbb{R}$ be a function such that $f_{y\\mapsto z_c}(y)$ serves as an approximation to $z_c$, i.e., $f_{y\\mapsto z_c}(y)$ predicts the association of the data point $y$ to the $c^{th}$ class. Given the data set $\\mathcal{D}$, as defined in (4), the hypothesis space for predicting the association of a point to the cth class is defined as a convex hull within $H_{K_c}(\\mathbb{R}^p)$:\n$\\mathcal{M}_{\\mathcal{D},c} := \\left\\{f_{y\\mapsto z_c} = \\underset{i=1}{\\Sigma^N} a_{c,i} K_c(., y^i) | a_{c,i} \\in [0,1], \\underset{i=1}{\\Sigma^N} a_{c,i} = 1, (y^i, z^i, q^i) \\in \\mathcal{D}\\right\\}.$ (24)\nIt is obvious that\n$\\mathcal{M}_{\\mathcal{D},c} \\subset H_{K_c}(\\mathbb{R}^p).$ (25)\nIt can be seen that for any $f_{y\\mapsto z_c} \\in \\mathcal{M}_{\\mathcal{D},c}$,\n$\\begin{aligned}|| f_{y\\mapsto z_c}||_{H_{K_c}(\\mathbb{R}^p)} &= ||\\underset{i=1}{\\Sigma^N} a_{c,i} K_c(., y^i)||_{H_{K_c}(\\mathbb{R}^p)} \\\\&= \\underset{i,j=1}{\\Sigma^N} a_{c,i} a_{c,i} K_c(y^i, y^j)  \\\\&< 1,\n\\end{aligned}$ (26)\nwhere (27) follows from $a_{c,i} \\in [0,1]$ and $\\underset{i=1}{\\Sigma^N} a_{c,i} = 1$. Thus,\n$\\underset{f_{y\\mapsto z_c}\\in\\mathcal{M}_{\\mathcal{D},c}}{sup} || f_{y\\mapsto z_c}||_{H_{K_c}(\\mathbb{R}^p)} \\leq 1.$ (28)"}, {"title": "Rademacher Complexity of the Hypothesis Space", "content": "To evaluate the Rademacher complexity of the hypothesis space, we introduce $\\sigma_1,...,\\sigma_N$ as the independent random variables drawn from the Rademacher distribution, and denote $\\sigma = (\\sigma_1,...,\\sigma_N)$. For a given data set $\\mathcal{D}$ (defined in (4)), the empirical Rademacher complexity of the hypothesis space $\\mathcal{M}_{\\mathcal{D},c}$ is given as\n$\\hat{R}_{\\mathcal{D}}(\\mathcal{M}_{\\mathcal{D},c}) = \\frac{1}{N}  \\underset{f_{y\\mapsto z_c}\\in\\mathcal{M}_{\\mathcal{D},c}}{sup}   \\underset{i=1}{\\Sigma^N} \\sigma_i f_{y\\mapsto z_c}(y^i).$ (29)\nGiven a dataset $\\mathcal{D} \\sim (P_{y,z,q})^N$, as defined in (4), we have\n$\\hat{R}_{\\mathcal{D}}(\\mathcal{M}_{\\mathcal{D},c}) \\leq \\frac{1}{\\sqrt{N}} $ (30)\nThus, the expected Rademacher complexity has an upper bound given by\n$\\mathbb{E}_{\\mathcal{D} \\sim (P_{y,z,q})^N}  [\\hat{R}_{\\mathcal{D}}(\\mathcal{M}_{\\mathcal{D},c})] \\leq \\frac{1}{\\sqrt{N}}$ (31)"}, {"title": "Generalization Error Bound", "content": "We consider the squared loss function and derive generalisation error bound for our hypoth- esis space. Given a hypothesis $f_{y\\mapsto z_c"}, "in \\mathcal{M}_{\\mathcal{D},c}$, let $l_{f_{y\\mapsto z_c}} : \\mathbb{R}^p \\times \\{0,1\\} \\to \\mathbb{R}$ be a loss function defined as\n$l_{f_{y\\mapsto z_c}} (y, z_c) := |z_c - f_{y\\mapsto z_c}(y)|^2.$ (32)\nConsider the following family of loss functions defined by the hypothesis space $\\mathcal{M}_{\\mathcal{D},c}$:\n$\\mathcal{L}_{\\mathcal{D},c} := \\{l_{f_{y\\mapsto z_c}} : (y, z_c) \\to |z_c-f_{y\\mapsto z_c}(y)|^2 | f_{y\\mapsto z_c} \\in \\mathcal{M}_{\\mathcal{D},c}\\} .$ (33)\nThe empirical Rademacher complexity of $\\mathcal{L}_{\\mathcal{D},c}$ is given as\n$\\hat{R}_{\\mathcal{D}}(\\mathcal{L}_{\\mathcal{D},c}) = \\mathbb{E}   \\underset{l_{f_{y\\mapsto z_c}} \\in \\mathcal{L}_{\\mathcal{D},c}}{sup}   \\frac{1}{N}  \\underset{i=1}{\\Sigma^N} \\sigma_i l_{f_{y\\mapsto z_c}}(y^i, (z^i)_c) $ (34)\nGiven a dataset $\\mathcal{D} \\sim (P_{y,z,q})^N$, as defined in (4), we have\n$\\begin{aligned}\\mathbb{E}_{\\mathcal{D} \\sim (P_{y,z,q})^N} [\\hat{R}_{\\mathcal{D}}(\\mathcal{L}_{\\mathcal{D},c})] &\\leq \\frac{2}{\\sqrt{N}}, \\\\ \\hat{R}_{\\mathcal{D}}(\\mathcal{L}_{\\mathcal{D},c}) &\\leq \\frac{2}{\\sqrt{N}} .\\end{aligned}$ (35)\nGiven a dataset $\\mathcal{D} \\sim (P_{y,z,q})^N$ (as defined in (4)), for any hypothesis $f_{y\\mapsto z_c} \\in \\mathcal{M}_{\\mathcal{D},c}$, the following holds with probability at least 1 \u03b4 for any d \u2208 (0,1):\n$\\mathbb{E}_{(y,z)\\sim P_{y,z}} [l_{f_{y\\mapsto z_c}} (y, z_c)] \\leq \\mathbb{E}_{\\mathcal{D}} [l_{f_{y\\mapsto z_c}}] + \\frac{4}{\\sqrt{N}} + \\sqrt{\\frac{log(1/\\delta)}{2N}}$ (37)\nwhere $l_{f_{y\\mapsto z_c}}$ is the loss function (32) and $\\mathbb{E}_{\\mathcal{D}}[l_{f_{y\\mapsto z_c}}]$ is the empirical averaged loss value given as\n$\\mathbb{E}_{\\mathcal{D}}[l_{f_{y\\mapsto z_c}}] = \\```json\n{\n  \"title\": \"Geometrically Inspired Kernel Machines for Collaborative Learning Beyond Gradient Descent\",\n  \"authors\": [\n    \"Mohit Kumar\",\n    \"Alexander Valentinitsch\",\n    \"Magdalena Fuchs\",\n    \"Mathias Brucker\",\n    \"Juliana Bowles\",\n    \"Adnan Husakovic\",\n    \"Ali Abbas\",\n    \"Bernhard A. Moser\"\n  ],\n  \"abstract\":", "This paper develops a novel mathematical framework for collaborative learning by means of geometrically inspired kernel machines which includes statements on the bounds of generalisation and approximation errors, and sample complexity. For classification prob- lems, this approach allows us to learn bounded geometric structures around given data points and hence solve the global model learning problem in an efficient way by exploiting convexity properties of the related optimisation problem in a Reproducing Kernel Hilbert Space (RKHS). In this way, we can reduce classification problems to determining the clos- est bounded geometric structure from a given data point. Further advantages that come with our solution is that our approach does not require clients to perform multiple epochs of local optimisation using stochastic gradient descent, nor require rounds of communica- tion between client/server for optimising the global model. We highlight that numerous experiments have shown that the proposed method is a competitive alternative to the state-of-the-art.", "sections\": [\n    {\n      \"title\": \"Introduction", "content\": \"In different domains of modern industry such as iron- or steelmaking, substantial amounts of data is generated and fused together to describe complex processes. These data are often distributed from multiple data vendors and clients, making collaborative data analysis and model training challenging. A potential bottleneck of using traditional centralized machine learning approaches is the overall data aggregation to a single location. The centralization of the data is not always feasible due to privacy concerns and logistical constraints.\nOur focus here is on collaborative learning from distributed and privately owned data. Federated learning is an increasingly popular approach to collaborative learning between multiple clients without the need to exchange raw training data. Given this advantage, federated learning can play a crucial role in process industry by leveraging distributed data to improve model performance while preserving data privacy. The classical federated learning approach (McMahan et al., 2017; Li et al., 2020; Wang et al., 2020) aims to train a common global model by repeating the following two operations: 1) training client local models using local data, and 2) aggregating these local models to update a global model. However, the sampling of distributed data from different local distributions makes it challenging to design and analyse efficient federated learning algorithms in practice (Ye et al., 2023; Vahidian et al., 2024). In general, essential requirements for federated learning include: 1) the capability of addressing heterogeneity among local data distributions, 2) the support for communication efficiency (allowing clients to transfer the required amount of parameters to the server under limited communication bandwidth), and 3) overall computational efficiency (for real-time operations) (Kairouz et al., 2021)."], "content": "In order to develop an accurate collaborative learning method that is efficient in both com- munication and computation, we formulate the following research questions:\nQ1: Can we build a theoretical analysis framework for collaborative learning from distributed and statistically heterogeneous data that, without making any assumptions on data distributions, allows us to calculate in practice: 1) the generalisation and approximation error bounds, and 2) the minimum number of training samples required to reduce the risk in approximating the target function below $\\epsilon$ (for any $\\epsilon > 0$) with probability at least 1 \u2013 $\\delta$ (for any $\\delta \\in (0,1))$?\nQ2: Can we solve the global model optimisation problem in the federated setting without requiring multiple rounds of communication between clients and the server?\nQ3: Can kernel machine learning theory provide a competitive and computationally efficient alternative to stochastic gradient descent-based optimisation in a federated setting?"}, {"title": "The State of the Art", "content": "Addressing Data Heterogeneity. The issue of data heterogeneity in federated learning has been previously addressed by learning a personalised model for each client assuming that data features share a common global representation, while statistical heterogeneity across\nTheoretical Analysis Framework. In federated learning, the underlying models can be chosen from a reproducing kernel Hilbert space (Hong & Chae, 2022; Ghari & Shen, 2022) allowing for an application of the powerful kernel theory for design and analysis. Kernels have been applied in machine learning over the years (Hofmann et al., 2008; Rudi et al., 2017) and have recently gained renewed attention. In particular, the parallels between the properties of deep neural networks and kernel methods have been established to indicate that some key phenomena of deep learning are manifested similarly in kernel methods in the overfitted regime (Belkin, Ma, & Mandal, 2018), and deep kernel machines have been introduced (Wilson et al., 2016; Nikhitha et al., 2021). Kernel-based models are effective for learning representations (Gholami & Hajisami, 2016; Kampffmeyer et al., 2018; Laforgue et al., 2019) and facilitate analytical solutions for learning problems using a broad range of mathematical techniques. A convergence guarantee for federated learning can be established for strongly convex and smooth objective functions (Li et al., 2020; Khaled et al., 2020; Qu et al., 2023). For one-hidden layer neural network with ReLU activations, an analysis of federated learning can be provided (Li et al., 2021) by describing the training dynamics of federated learning by means of Neural Tangent Kernel (NTK) (Jacot et al., 2018). The gradient descent training dynamics of artificial neural networks follows that of the gradient descent of the functional cost with respect to a kernel: NTK (Jacot et al., 2018). A \u039d\u03a4\u039a based framework makes use of the theory on over-parameterised neural networks to provide proof of convergence of gradient descent and generalisation bound for over-parameterized ReLU neural networks in federated learning (Huang et al., 2021).\nVariational Optimisation as an Alternative to Gradient Descent. A kernel-based approach that does not rely on gradient descent-based learning and instead uses variational optimisation for deriving analytically the learning solutions, has been previously studied (Kumar & Freudenthaler, 2020; Kumar et al., 2021; Zhang et al., 2022; Kumar et al., 2021; Zhang et al., 2023; Kumar et al., 2021b, 2021a, 2023). This kernel-based variational optimisation approach was considered for privacy-preserving learning under the differential privacy framework (Kumar, 2023; Kumar et al., 2021, 2020) and fully homomorphic encryption (Ku- mar et al., 2023), and can potentially be explored for federated learning as well. So far there have been no attempts to extend the variational optimisation approach to federated learning in such a way that it can handle all our research questions Q1, Q2, and Q3."}, {"title": "Geometrically Inspired Kernel Approach as an Efficient Alternative", "content": "For collab- orative learning in a federated setting, a geometrically inspired kernel approach has been introduced (Kumar, Moser, & Fischer, 2024, 2023). A recent paper (Kumar et al., 2024) introduced a so-called Kernel Affine Hull Machine (KAHM), wherein a representation of given data points is learned in RKHS to define a bounded geometric structure around data points within the affine hull of data points. The KAHM makes it possible to compute at any arbitrary point a measure of its distance from the data samples. The significance of this is that the KAHM's induced distance measure cannot only be used for classification, but for federated learning by aggregating locally trained KAHMs to build a global \u041a\u0410\u041d\u041c. Note that the crucial significance of KAHMs for learning from distributed data comes from the fact that a global model can be built by aggregating local models simply using a distance measure without requiring gradient-based learning of the global model parameters. This is best illustrated through an example as shown in Fig. 1. Consequently, a KAHM-based ap- proach is computationally more efficient and indeed promising for federated learning (Kumar et al., 2023, 2024). Moreover, KAHMs can be used to mitigate the accuracy-loss issue of differential privacy, where the post-processing property of differential privacy is leveraged for fabricating new data samples by means of a geometric model ensuring that the geometric modelling error of fabricated data samples is never larger than that of original data sam- ples while simultaneously achieving the privacy-loss bound. Although a mathematical proof of fabricated data samples with modelling error less than that of original data samples is provided (Kumar et al., 2024), no generalisation error analysis and performance guarantees have been provided for the KAHM-based learning method. The federated learning solution, as suggested in (Kumar et al., 2024, 2023), has been introduced in a rather adhoc manner without providing a mathematical theory to justify the solution."}, {"title": "Contributions", "content": "In this study, we give an affirmative answer to Q1, Q2, and Q3 by providing a novel approach (based on KAHMs) that harnesses the distributed computational power across clients. We provide a unified framework for the design and analysis of collaborative learning by means of geometrically inspired kernel machines such as KAHMs. Our contributions are summarised in the following four aspects:\nTheoretical Framework: We develop a framework to analyse KAHM-based collaborative learning in a federated setting. We introduce a novel kernel function defined by a global KAHM (that aggregates local KAHMs) such that the kernel function evaluates the degree of similarity between two data points in terms of their distance from training data samples. The hypothesis space for the learning is suitably (specifically, a convex hull) defined in the RKHS associated to the novel kernel function. An upper bound on the Rademacher complexity of the\nhypothesis space is provided (in Theorem 3) to derive a uniform bound on the generalisation error (in Theorem 4).\nBeyond Gradient Descent Learning Regime: Unlike most studies, we move beyond the gradient descent learning regime to derive a collaborative learning solution, that utilises the idea of KAHM's induced distance measure based aggregation of local geometrical models to build a global geometrical model. Our contribution lies in considering the global model learning problem (in Problem 1) and showing that under a realistic assumption, it is possible to derive analytically a learning solution (in Theorem 5) that does not require estimating global model parameters. The underlying assumption is that there is only a small error in the fitting of training data points by the KAHM. This assumption is realistic and is validated through various experiments as well. Since the learning solution does not require estimating the global model parameters, no rounds of communication between clients and server are required for optimising the global model, and the clients are also not required to perform multiple epochs of local optimisation using stochastic gradient descent. The advantage of our\napproach is hence that it leads to a communication and computationally efficient collaborative learning solution.\nPerformance Guarantees: The generalisation error bound allows us to derive an upper bound on the error in approximating the target function (in Theorem 7). Note that the target function approximation risk bound can be calculated in practice and decays as O(1/\u221aN), where N is the total number of training data samples distributed across multiple clients. Remarkably, the risk bound depends only on N, and thus the sample complexity (i.e. the number of training samples needed for an arbitrarily small risk in approximating the target function) is calculated (in Lemma 3) and plotted (in Fig. 3). We additionally provide a deterministic analysis (in Theorem 8) that further justifies the proposed solution via an interpretation in-terms of distance from training data points.\nCompetitive Alternative: A KAHM-based learning approach provides a competitive alternative to the state of the art federated learning methods, and in fact outperforms tradi- tional solutions. Furthermore, the KAHM approach facilitates and enhances cross-domain knowledge transfer in federated settings. Experiments are used to show the improved per- formance of the proposed method when compared to the state of the art methods."}, {"title": "Proposed Approach, Novelty, and Significance", "content": "Our unified approach for the development of a collaborative learning framework for address- ing our research questions (Q1, Q2, and Q3) consists of the following 9 steps:\nStep 1: Define a KAHM induced kernel function. KAHMs let us define a novel kernel function that measures the similarity between two data points in terms of their distance from training samples of a class.\nStep 2: Define a data-dependent hypothesis space for learning. To predict the association between a class-label and a data point, the considered hypothesis space is defined by the given data samples and is in the form of a convex hull within the RKHS associated to the KAHM induced kernel function.\nStep 3: Calculate the upper bound of the Rademacher complexity of the hy- pothesis space. The Rademacher complexity of the considered hypothesis space has an upper bound such that this bound can be calculated in practice.\nStep 4: Derive the generalisation error bound for the hypothesis space. Follow- ing the standard approach, the Rademacher complexity can be used to derive a uniform bound on the generalisation error.\nStep 5: Formulate the global model learning problem. The global model learning problem can be formulated as an optimisation problem over a suitably chosen subset of the hypothesis space.\nStep 6: Exploit the convex hull form of the hypothesis space for deriving a learning solution analytically. The convex hull form of the hypothesis space can be leveraged together with a realistic assumption to derive a learning solution that does not require estimating any of the model parameters using gradient descent or any other numerical algorithm.\nStep 7: Derive the upper bound of the error in approximating the target func- tion. The generalization error bound can be used to derive an upper bound of the error in approximating the target function.\nStep 8: Calculate the sample complexity. The target function approximation risk bound can be used to calculate the sample complexity.\nStep 9. Provide a deterministic analysis of the solution. The upper bound of the KAHM induced distance function can be used to analyse and interpret the solution in terms of the distance from training data points.\nRemark 2 (Novelty). The above 9 steps offer a novel approach to the development of a collaborative learning framework. In particular, the introduction of a KAHM-induced kernel function (step 1) and exploiting the convex hull form of the hypothesis space (step 2) for de- riving the learning solution analytically (step 6) are original. This is the first study applying geometrically inspired kernel machines (i.e., KAHMs) for a rigorous design and analysis of collaborative learning solutions.\nRemark 3 (Significance). The proposed approach has been carefully designed to address the formulated research questions. Q1 is addressed by steps 4, 7, and 8. Q2 and Q3 are addressed by step 6. A new deterministic way of studying and solving the learning problem is provided by step 9. The work provides a new theoretical analysis framework for learning beyond gradient descent."}, {"title": "Structure of the Paper", "content": "Section 2 presents the necessary mathematical background underlying our work. Section 3 develops the theory for KAHMs and includes steps 1-4 of the proposed approach outlined ear- lier. Section 4 continues with steps 5-9, thereby solving the collaborative learning problem. The experimental evaluation of our approach is given in Section 5, followed by concluding remarks in Section 6."}, {"title": "Mathematical Prerequisites and Notations", "content": "This section introduces the notation used throughout, presents the distributed data setting, and provides a review of the notion of KA\u041d\u041c."}, {"title": "Notation", "content": "In this paper, all matrices are denoted using boldface font. The following notation is used:\n\u2022 Let $n, p, c, N, Q, C' \\in \\mathbb{Z}+$ be the positive integers.\n\u2022 For a set $\\{y^1,..., y^N\\} \\subset \\mathbb{R}^p$, its affine hull is denoted as $aff(\\{y^1,..., y^N\\})$.\n\u2022 For a scalar $a \\in \\mathbb{R}$, $|a|$ denotes its absolute value. For a set A, $|A|$ denotes its cardi- nality. For a real matrix Y, $Y^T$ is the transpose of Y.\n\u2022 For a vector $y \\in \\mathbb{R}^p$, $||y||$ denotes the Euclidean norm and $y_j$ (and also $(y)_j$) denotes the $j^{th}$ element. For a matrix $Y \\in \\mathbb{R}^{N \\times p}$, $||Y||_2$ denotes the spectral norm, $||Y||_F$"}, {"title": "Distributed Data Setting", "content": "We consider the multi-class problem, where a label vector $z \\in \\{0,1\\}^C$ with $C > 2$ is assigned to a data point $y \\in \\mathbb{R}^p$ such that the $c$-th element of vector $z$, $z_c \\in \\{0,1\\}$, represents the association of $y$ with the $c$-th class. We consider the problem of collaborative learning from distributed data, where a number of clients $Q$ ($Q > 1$) participate in the learning. Let $q \\in \\{1,2,...,Q\\}$ be the client characterising variable. Let $\\mathcal{D}$ be a set consisting of N number of samples drawn i.i.d. according to the distribution $P_{y,z,q}$:\n$\\mathcal{D} := \\{(y^i, z^i, q^i) \\in \\mathbb{R}^p \\times \\{0,1\\}^C \\times \\{1,2,..., Q\\} | i\\in \\{1,2,...,N\\}\\} \\sim (P_{y,z,q})^N$. (4)\nLet $\\mathcal{I}_c^q$ be the set of indices of those samples in the sequence $((y^i, z^i, q^i) \\in \\mathcal{D})_1^N$ which are $c^{th}$ class labelled and owned by client $q$, i.e.,\n$\\mathcal{I}_c^q := \\{i \\in \\{1,2,...,N\\} | (z^i)_c = 1, q^i = q\\} .$ (5)\nLet $(I_1^{c,q}, ..., I_{|\\mathcal{I}_c^q|}^{c,q})$ be the sequence of elements of $\\mathcal{I}_c^q$ in ascending order, i.e.,\n$\\begin{aligned}I_1^{c,q} &= \\min(\\mathcal{I}_c^q), \\\\I_i^{c,q} &= \\min(\\mathcal{I}_{c,q} \\setminus \\{I_1, ..., I_{i-1}\\}) ,\\end{aligned}$ (6)\nfor $i \\in \\{2,..., |\\mathcal{I}^{c,q}|\\}$. Let $Y^{c,q} \\in \\mathbb{R}^{|\\mathcal{I}^{c,q}|\\times p}$ be the matrix storing the cth class labelled and $q^{th}$ client owned samples, i.e.,\n$Y^{c,q} = \\begin{bmatrix}y^{I_1^{c,q}}\\  ... \\\\ y^{I_{|\\mathcal{I}^{c,q}|}^{c,q}}\\end{bmatrix}^T.$ (8)"}, {"title": "A Review of Kernel Affine Hull Machines", "content": "We recall the notion of KAHM as originally defined in (Kumar et al., 2024).\nGiven a finite number of samples: $Y = [y^1 ... y^N]^T$ with $y^1,...,y^N \\in \\mathbb{R}^p$ and a subspace dimension $n \\leq p$; a kernel affine hull machine $A_{Y,n} : \\mathbb{R}^p \\to aff(\\{y^1,...,y^N\\})$ maps an arbitrary point $y \\in \\mathbb{R}^p$ onto the affine hull of $\\{y^1,...,y^N\\}$.\nThe complete definition of $A_{Y,n}$ is provided in Appendix A. In addition, the subspace dimension $n < p$ is practically determined for the given samples using a procedure given in Appendix B.\nA KAHM $A_{Y,n}$, with the choice of subspace dimension $n$ as suggested in Appendix B, is completely defined by the data samples Y without involving any free parameters to be tuned, allowing us to define an AutoML approach by means of KAHMs. Consequently, in what follows we use $A_Y$ to denote a KAHM.\nThe KAHM $A_{Y}$, associated to $Y = [y^1 ... y^N]^T$ with $y^1,...,y^N \\in \\mathbb{R}^p$, is a bounded function on $\\mathbb{R}^p$ such that for any $y \\in \\mathbb{R}^p$,\n$||A_Y(y)|| < ||Y||_2 \\left(1 + \\frac{pN^2}{2||Y||_F^2}\\right).$ (11)\nThus, the image of $A_Y$ is bounded such that\n$A_Y[\\mathbb{R}^p] \\subset \\{y \\in \\mathbb{R}^p | ||y|| < ||Y||_2 \\left(1 + \\frac{pN^2}{2||Y||_F^2}\\right)\\} .$ (12)\nGiven a KAHM $A_Y$, the distance of an arbitrary point $y \\in \\mathbb{R}^p$ from its image under $A_Y$ is given as\n$\\Gamma_{A_Y}(y) := ||y \u2013 A_Y(y)|| .$ (13)\nThe ratio of the distance of a point $y \\in \\mathbb{R}^p$ from its image under $A_Y$ to the distance of $y$ from $\\{y^1,...,y^N\\}$ evaluated as $||[ y - y^1 ... y \u2013 y^N ]||_2$ remains upper bounded as\n$\\frac{\\Gamma_{A_Y}(y)}{||[ y - y^1 ... y \u2013 y^N ]||_2} < 1 + \\frac{pN^2}{2||Y||_F^2}$ (14)"}, {"title": "Combining Locally Distributed KAHMs to build a Global KAHM using the Distance Function", "content": "We consider the scenario where class labelled data samples are distributed amongst Q dif- ferent clients. Given Q different KAHMS $A_{Y^{c,1}}, ..., A_{Y^{c,Q}}$ built independently using data matrices $Y^{c,1},..., Y^{c,Q}$ respectively, a possible way to combine together the KAHMs is as follows:\n$\\begin{aligned}G_c(y) &= A_{Y^{c,\\hat{q}(Y)}}(Y) \\\\ \\hat{q}(y) &= \\underset{q\\in\\{1,2,...,Q\\}}{argmin} \\Gamma_{A_{Y^{c,q}}}(Y),\\end{aligned}$ (15)\nwhere $G_c$ is the global KAHM (that combines together the individual KAHMs) and $\\Gamma_{A_{Y^{c,q}}}$ is the distance function induced by $A_{Y^{c,q}}$.\nGiven a global KAHM $(G_c)$ that combines together Q local KAHMs $(A_{Y^{c,1}}, ..., A_{Y^{c,Q}})$, the distance of an arbitrary point $y \\in \\mathbb{R}^p$ from its image under $G_c$ is given as\n$\\begin{aligned}\\Gamma_{G_c}(y) &:= ||y \u2013 G_c(y)|| \\\\ &= \\underset{q\\in\\{1,2,...,Q\\}}{min} \\Gamma_{A_{Y^{c,q}}}(y).\n\\end{aligned}$ (17)"}, {"title": "Theory for Learning with Kernel Affine Hull Machines", "content": "This section develops a theory for learning by means of KAHMs. For this, a geometrically inspired kernel function and corresponding RKHS is presented in Section 3.1, followed by a definition of a hypothesis space in Section 3.2. Section 3.3 evaluates the Rademacher com- plexity of the hypothesis space, which is then used in Section 3.4 to derive the generalisation error bound for the hypothesis space."}, {"title": "A Novel Kernel Function Induced by the Global KAHM", "content": "A given global KAHM, $G_c$, induces a function, $K_c : \\mathbb{R}^p \\times \\mathbb{R}^p \\to [0, 1]$, defined as\n$K_c(y^1, y^2) := exp\\left(-\\frac{1}{p} \\Gamma_{G_c}(y^1)^2\\right) exp\\left(-\\frac{1}{p} \\Gamma_{G_c}(y^2)^2\\right)$ . (19)\nKe is a positive definite kernel, since\n1. $K_c(y^1, y^2) = K_c(y^2, y^1)$, and\n2. for every $y^1,..., y^N \\in \\mathbb{R}^p$ and $a_1,...,a_N \\in \\mathbb{R}$,\n$\\underset{i,j=1}{\\Sigma^N} a_i a_j K_c(y^i, y^j) \\geq 0.$ (20)\nTo see (20), consider\n$\\begin{aligned}\\underset{i,j=1}{\\Sigma^N} a_i a_j K_c(y^i, y^j) &= \\underset{i,j=1}{\\Sigma^N} a_i exp\\left(-\\frac{1}{p} \\Gamma_{G_c}(y^i)^2\\right) a_j exp\\left(-\\frac{1}{p} \\Gamma_{G_c}(y^j)^2\\right) \\\\&=\\underset{i=1}{\\Sigma^N} a_i  exp\\left(-\\frac{1}{p} \\Gamma_{G_c}(y^i)^2\\right)  ^2 \\geq 0.\n\\end{aligned}$ (21)\nNow the RKHS associated to Ke is given by:\n$H_{K_c}(\\mathbb{R}^p) : = \\left\\{ f(.) = \\underset{i=1}{\\Sigma^L} a_i K_c(., y^i) | a_i \\in \\mathbb{R}, y^i \\in \\mathbb{R}^p, ||f||_{H_{K_c}(\\mathbb{R}^p)} := \\underset{i,j=1}{\\Sigma^{L}} a_i a_j K_c(y^i, y^j) < \\infty \\right\\}$ (22)\nwith inner product for any $f(.) = \\underset{i=1}{\\Sigma^{L}} a_i K_c(., s^i)$ (with $a \\in \\mathbb{R}, s^i \\in \\mathbb{R}^p$) and $g(.) = \\underset{i=1}{\\Sigma^{M}} b_j K_c(., t^i) \\in H_{K_c}(\\mathbb{R}^p)$ (with $b_j \\in \\mathbb{R}, t^i \\in \\mathbb{R}^p$) defined as\n$(f, g)_{H_{K_c}(\\mathbb{R}^p)} := \\underset{i=1}{\\Sigma^{L}} \\underset{j=1}{\\Sigma^{M}} a_i b_j K_c(s^i, t^j) . $ (23)"}, {"title": "A Data-Dependent Hypothesis Space", "content": "Let $f_{y\\mapsto z_c}: \\mathbb{R}^p \\to \\mathbb{R}$ be a function such that $f_{y\\mapsto z_c}(y)$ serves as an approximation to $z_c$, i.e., $f_{y\\mapsto z_c}(y)$ predicts the association of the data point $y$ to the $c^{th}$ class. Given the data set $\\mathcal{D}$, as defined in (4), the hypothesis space for predicting the association of a point to the cth class is defined as a convex hull within $H_{K_c}(\\mathbb{R}^p)$:\n$\\mathcal{M}_{\\mathcal{D},c} := \\left\\{f_{y\\mapsto z_c} = \\underset{i=1}{\\Sigma^N} a_{c,i} K_c(., y^i) | a_{c,i} \\in [0,1], \\underset{i=1}{\\Sigma^N} a_{c,i} = 1, (y^i, z^i, q^i) \\in \\mathcal{D}\\right\\}.$ (24)\nIt is obvious that\n$\\mathcal{M}_{\\mathcal{D},c} \\subset H_{K_c}(\\mathbb{R}^p).$ (25)\nIt can be seen that for any $f_{y\\mapsto z_c} \\in \\mathcal{M}_{\\mathcal{D},c}$,\n$\\begin{aligned}|| f_{y\\mapsto z_c}||_{H_{K_c}(\\mathbb{R}^p)} &= ||\\underset{i=1}{\\Sigma^N} a_{c,i} K_c(., y^i)||_{H_{K_c}(\\mathbb{R}^p)} \\\\&= \\underset{i,j=1}{\\Sigma^N} a_{c,i} a_{c,i} K_c(y^i, y^j)  \\\\&< 1,\n\\end{aligned}$ (26)\nwhere (27) follows from $a_{c,i} \\in [0,1]$ and $\\underset{i=1}{\\Sigma^N} a_{c,i} = 1$. Thus,\n$\\underset{f_{y\\mapsto z_c}\\in\\mathcal{M}_{\\mathcal{D},c}}{sup} || f_{y\\mapsto z_c}||_{H_{K_c}(\\mathbb{R}^p)} \\leq 1.$ (28)"}, {"title": "Rademacher Complexity of the Hypothesis Space", "content": "To evaluate the Rademacher complexity of the hypothesis space, we introduce $\\sigma_1,...,\\sigma_N$ as the independent random variables drawn from the Rademacher distribution, and denote $\\sigma = (\\sigma_1,...,\\sigma_N)$. For a given data set $\\mathcal{D}$ (defined in (4)), the empirical Rademacher complexity of the hypothesis space $\\mathcal{M}_{\\mathcal{D},c}$ is given as\n$\\hat{R}_{\\mathcal{D}}(\\mathcal{M}_{\\mathcal{D},c}) = \\frac{1}{N}  \\underset{f_{y\\mapsto z_c}\\in\\mathcal{M}_{\\mathcal{D},c}}{sup}   \\underset{i=1}{\\Sigma^N} \\sigma_i f_{y\\mapsto z_c}(y^i).$ (29)\nGiven a dataset $\\mathcal{D} \\sim (P_{y,z,q})^N$, as defined in (4), we have\n$\\hat{R}_{\\mathcal{D}}(\\mathcal{M}_{\\mathcal{D},c}) \\leq \\frac{1}{\\sqrt{N}} $ (30)\nThus, the expected Rademacher complexity has an upper bound given by\n$\\mathbb{E}_{\\mathcal{D} \\sim (P_{y,z,q})^N}  [\\hat{R}_{\\mathcal{D}}(\\mathcal{M}_{\\mathcal{D},c})] \\leq \\frac{1}{\\sqrt{N}}$ (31)"}, {"title": "Generalization Error Bound", "content": "We consider the squared loss function and derive generalisation error bound for our hypoth- esis space. Given a hypothesis $f_{y\\mapsto z_c"}, {"z_c}}": "mathbb{R"}, {"z_c)": "z_c - f_{y\\mapsto z_c"}, {"M}_{\\mathcal{D},c}$": "n$\\mathcal{L"}, {"mathcal{D},c}": {"z_c}}": "y", "0,1)": "n$\\mathbb{E}_{(y,z)\\sim P_{y,z}} [l_{f_{y\\mapsto z_c}} (y, z_c)] \\leq \\mathbb{E}_{\\mathcal{D}} [l_{f_{y\\mapsto z_c}}] + \\frac{4}{\\sqrt{N}} + \\sqrt{\\frac{log(1/\\delta)}{2N}}$ (37)\nwhere $l_{f_{y\\mapsto z_c}}$ is the loss function (32) and $\\mathbb{E}_{\\mathcal{D}}[l_{f_{y\\mapsto z_c}}]$ is the empirical averaged loss value given as\n$\\mathbb{E}_{\\mathcal{D}}[l_{f_{y\\mapsto z_c}}] = \\"}}]