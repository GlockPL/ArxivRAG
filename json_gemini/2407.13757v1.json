{"title": "Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models", "authors": ["Zhuo Chen", "Jiawei Liu*", "Haotan Liu", "Qikai Cheng", "Fan Zhang", "Wei Lu", "Xiaozhong Liu"], "abstract": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time\nconstraints of large language models, but it also induces vulnerabilities against retrieval corrup-\ntion attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-\ndomain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced\nGenerative (RAG) models when faced with black-box attacks for opinion manipulation. We\nexplore the impact of such attacks on user cognition and decision-making, providing new insight\nto enhance the reliability and security of RAG models. We manipulate the ranking results of the\nretrieval model in RAG with instruction and use these results as data to train a surrogate model.\nBy employing adversarial retrieval attack methods to the surrogate model, black-box transfer\nattacks on RAG are further realized. Experiments conducted on opinion datasets across multiple\ntopics show that the proposed attack strategy can significantly alter the opinion polarity of the\ncontent generated by RAG. This demonstrates the model's vulnerability and, more importantly,\nreveals the potential negative impact on user cognition and decision-making, making it easier to\nmislead users into accepting incorrect or biased information.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of artificial intelligence, large language models (LLMs) have demonstrated exceptional\ncapabilities in the field of natural language processing. However, constrained by their training data, these\nmodels have limited scope of knowledge and lack the most up-to-date information, which can lead to errors or\nhallucinations when tackling more complex or time-sensitive tasks. Retrieval-Augmented Generation (RAG)\ncombines information retrieval with the generative capabilities of large language models, enhancing the timeliness\nof knowledge acquisition and effectively mitigating the hallucination problem of these models. When given a\nquery, RAG retrieves the most relevant passages from a knowledge base to augment the input request for the LLM.\nFor example, the retrieved knowledge may consist of a series of text snippets that are semantically most similar to\nthe query. RAG has inspired many popular applications, such as Microsoft Bing Chat, ERNIE Bot, and KimiChat,"}, {"title": "2 Related Works", "content": "Research on the reliability of neural network models has long been established. In 2013, Szegedy et al. [18] found\nthat applying imperceptible perturbations to a neural network model during a classification task was sufficient to\ncause classification errors in CV. Later, scholars observed similar phenomenon in NLP. Robin et al. [11] found\nthat inserting perturbed text into original paragraphs significantly distracts computer systems without changing\nthe correct answer or misleading humans. It reflects the robustness of neural network models, i.e., the ability\nto output stable and correct predictions in tackling the imperceptible additive noises [20]. For large language\nmodels, Wang et al. [19] proposed a comprehensive trustworthiness evaluation framework for LLMs, assessing\ntheir reliability from various perspectives such as toxicity, adversarial robustness, stereotype bias, and fairness.\nWhile large language models have greater capabilities compared to general deep neural network models, they also\nraise more concerns regarding security and reliability.\nAs RAG is designed to overcome the hallucination problem in LLMs and enhance their generative capabilities,\nthe reliability of the content generated by RAG is also a major concern. Zhang et al. [25] attempted to explore the\nweaknesses of RAG by analyzing critical components in order to facilitate the injection of the attack sequence and\ncrafting the malicious document with a gradient-guided token mutation technique. Xiang et al. [22] designed an\nisolate-then-aggregate strategy, which gets responses of LLMs from each passage in isolation and then securely\naggregate these isolated responses, to construct the first defense framework against retrieval corruption attacks.\nThese studies are based on white-box scenarios and primarily focus on the robustness of RAG against corrupted\nand toxic content.\nThis paper intends to use adversarial retrieval attack strategies to perturb the ranking results of the retriever,\nensuring that opinion documents with a certain stance are ranked as high as possible, thereby guiding the generated\nresponses of the LLM to reflect that stance.\nThe adversarial retrieval attack strategy starts with manipulation at the word level. Under white-box setting,\nEbrahimi et al. [7] utilize an atomic flip operation, which swaps one token for an other, to generate adversarial\nexamples and the method, known as Hotflip. Hotflip gets rid of reliance on rules, but the adversarial text it\ngenerate usually has incomplete semantics and insufficient grammar fluency. While it can deceive the target\nmodel, it cannot evade perplexity-based defenses. Wu et al. [21] also proposed a word substitution ranking attack\nmethod called PRADA. To enhance the readability and effectiveness of the adversarial text, scholars further\ndesigned sentence-level ranking attack methods. Song et al. [17] propose an adversarial method under white-box\nsetting, named Collision, which uses gradient optimization and beam search to produce the adversarial text named\ncollision. The Collision method further imposes a soft constraint on collision generation by integrating a language\nmodel, reducing the perplexity of the collision. The method has shown promising[14] propose the Pairwise\nAnchor-based Trigger (PAT) method under black-box setting. Added the fluency constraint and the next sentence\nprediction constraint, the method generates adversarial text by optimizing the pairwise loss of top candidates and\ntarget candidates with adversarial text. Although the time complexity of PAT has increased compared to previous\nmethods, PAT takes ranking similarity and semantic consistency into account, so its manipulation effect on the\nretrieval ranking of target candidates is superior."}, {"title": "3 Method", "content": "This paper attempts to manipulate the opinions in the responses generated by black-box RAG models on con-\ntroversial topics, targeting both the retrieval model and the LLM which performs the integrated generation task.\nZhang et al. [25] tried to poison context documents to deceive the LLM into generating incorrect content, but this\nmethod requires extensive internal details of the LLM application, making it less feasible in real-world scenarios.\nFor black-box RAG, the manipulator has no knowledge of the internal information of the RAG, including model\narchitecture and score function, and can only access the inputs and outputs of the RAG. Specially, the manipulator\ncan only call the interface of the LLM in RAG instead of that of the retriever. Since the inputs consist of the"}, {"title": "4 Experiment and Analysis", "content": "After imitating the retrieval model of $RAG_{black}$ to obtain the surrogate model, this paper first compares the ranking\nability of the surrogate model $M_i$ and the target retrieval model $RM$, as well as the similarity of their ranking\nresults, as shown in Table 1, to ensure that the surrogate model has learned the capabilities of the black-box\nretrieval model.\nThis paper uses Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) to reflect\nthe ranking ability of the models themselves; higher values indicate stronger ranking ability in terms of relevance.\nInter Ranking Similarity (Inter) and Rank Biased Overlap (RBO) are used to measure the similarity between the\nranking results of the surrogate model and the target retrieval model; higher values indicate better performance of\nthe black-box imitation. The weight for RBO@10 is set to 0.7. In Table 1, \"-\" indicates that the metric is not\napplicable to the model.\nAs can be seen from Table 1, the surrogate model $M_i$ trained by black-box imitation is similar to the target retrieval\nmodel coCondenser in terms of relevance ranking performance and ranking results, validating the effectiveness of\nthe black-box imitation.\nAfter the black-box imitation training, the white-box surrogate model is conducted with opinion manipulation\nexperiments. Several controversial topics and their opinion text data under the four themes of \"Government\",\n\"Education\", \"Society\", and \"Health\" from the PROCON.ORG data are selected as the retrieval corpus. The\noriginal retrieval corpus is denoted as $Docs_{origin}$. Based on the surrogate model, we generate the corresponding $T_{pat}$\nfor the candidate items with the expected opinion $S_t$ on controversial topics, and then insert $T_{pat}$ at the beginning of\nthe target candidate items to obtain the perturbed retrieval corpus $Docs_{adv}$. Query $RAG_{black}$ twice on controversial\ntopics: once with $Docs_{origin}$ as the retrieval corpus, and once with $Docs_{adv}$ as the retrieval corpus, and obtain the\ntwo responses of RAG, representing the answers before and after opinion manipulation. The responses are then\nclassified into three categories based on their opinion on controversial topics: opposing, neutral, and supporting,\nrepresented by 0, 1, and 2, respectively, as the opinion scores of the generated responses. This study uses Average\nStance Variation (ASV) to represent the average increase of opinion scores of $RAG_{black}$ responses in the direction\nof the expected opinion $S_t$ before and after manipulation. A positive ASV indicates that the opinion manipulation\ntowards $S_t$ is effective, while a negative ASV indicates that the manipulation actually makes the opinions of RAG\nresponses deviate from $S_t$. The larger the ASV value, the more successful the opinion manipulation of RAG\nresponses. Additionally, this paper attempts to obtain the ranking results of the retriever coCondenser to evaluate\nthe effectiveness of the adversarial retrieval manipulation strategy at the ranking stage for dense retrieval. This\nevaluation is solely for assessment purposes and is not involved in manipulation, as no internal knowledge of\n$RAG_{black}$ was leaked during the manipulation process."}, {"title": "5 Conclusion", "content": "In this paper, we explore the vulnerability of retrieval-augmented generation (RAG) models to opinion manipulation\nagainst black-box attack in open-ended controversial topics, and delve into the potential impact of such attacks\non user cognition and decision-making. Through systematic experiments, we propose a novel adversarial\nattack strategy about retrieval ranking poisoning. This method significantly affects the polarity of the opinions\ngenerated by RAG by crafting adversarial samples, without requiring internal knowledge of the RAG model.\nThe experimental results indicate that the proposed attack strategy successfully alters the opinion of the content\ngenerated by the RAG model, revealing the vulnerability and unreliability of RAG when confronted with malicious\nretrieval corpus. More importantly, this opinion manipulation could have profound impacts on users' cognition and\ndecision-making processes, potentially leading users to accept incorrect or biased information, causing cognitive\nchanges and public opinion distortion. This phenomenon is particularly significant in open-ended and controversial\nissues.\nFuture research will expands the scale of the experiments by including more open-source and commercial RAG\nsystems to more comprehensively evaluate the reliability of viewpoint generation by RAG models. Given the\nvulnerabilities of RAG models, future work should focus on developing more robust defense strategies. These\nmay include improving the robustness of retrieval algorithms, enhancing the reliability of generation models, and\nintroducing multi-level input filtering mechanisms to counteract adversarial inputs, thereby achieving a balanced\noptimization of the understanding and reliability of RAG models."}, {"title": "6 Ethical Statement", "content": "This paper explores the feasibility of opinion manipulation on black-box RAG models in real-world scenarios.\nThe main goal is to assess the reliability of RAG technology in responding to ranking manipulation at the stage of\nretrieval, paving the way for future work to enhance the robustness and defense capabilities of RAG technology.\nThis study did not manipulate any commercial RAG systems or real-world data currently in use.."}]}