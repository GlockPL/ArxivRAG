{"title": "Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models", "authors": ["Zhuo Chen", "Jiawei Liu", "Haotan Liu", "Qikai Cheng", "Fan Zhang", "Wei Lu", "Xiaozhong Liu"], "abstract": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of artificial intelligence, large language models (LLMs) have demonstrated exceptional capabilities in the field of natural language processing. However, constrained by their training data, these models have limited scope of knowledge and lack the most up-to-date information, which can lead to errors or hallucinations when tackling more complex or time-sensitive tasks. Retrieval-Augmented Generation (RAG) combines information retrieval with the generative capabilities of large language models, enhancing the timeliness of knowledge acquisition and effectively mitigating the hallucination problem of these models. When given a query, RAG retrieves the most relevant passages from a knowledge base to augment the input request for the LLM. For example, the retrieved knowledge may consist of a series of text snippets that are semantically most similar to the query. RAG has inspired many popular applications, such as Microsoft Bing Chat, ERNIE Bot, and KimiChat,"}, {"title": "2 Related Works", "content": "Research on the reliability of neural network models has long been established. In 2013, Szegedy et al. [18] found that applying imperceptible perturbations to a neural network model during a classification task was sufficient to cause classification errors in CV. Later, scholars observed similar phenomenon in NLP. Robin et al. [11] found that inserting perturbed text into original paragraphs significantly distracts computer systems without changing the correct answer or misleading humans. It reflects the robustness of neural network models, i.e., the ability to output stable and correct predictions in tackling the imperceptible additive noises [20]. For large language models, Wang et al. [19] proposed a comprehensive trustworthiness evaluation framework for LLMs, assessing their reliability from various perspectives such as toxicity, adversarial robustness, stereotype bias, and fairness. While large language models have greater capabilities compared to general deep neural network models, they also raise more concerns regarding security and reliability.\nAs RAG is designed to overcome the hallucination problem in LLMs and enhance their generative capabilities, the reliability of the content generated by RAG is also a major concern. Zhang et al. [25] attempted to explore the weaknesses of RAG by analyzing critical components in order to facilitate the injection of the attack sequence and crafting the malicious document with a gradient-guided token mutation technique. Xiang et al. [22] designed an isolate-then-aggregate strategy, which gets responses of LLMs from each passage in isolation and then securely aggregate these isolated responses, to construct the first defense framework against retrieval corruption attacks. These studies are based on white-box scenarios and primarily focus on the robustness of RAG against corrupted and toxic content.\nThis paper intends to use adversarial retrieval attack strategies to perturb the ranking results of the retriever, ensuring that opinion documents with a certain stance are ranked as high as possible, thereby guiding the generated responses of the LLM to reflect that stance.\nThe adversarial retrieval attack strategy starts with manipulation at the word level. Under white-box setting, Ebrahimi et al. [7] utilize an atomic flip operation, which swaps one token for an other, to generate adversarial examples and the method, known as Hotflip. Hotflip gets rid of reliance on rules, but the adversarial text it generate usually has incomplete semantics and insufficient grammar fluency. While it can deceive the target model, it cannot evade perplexity-based defenses. Wu et al. [21] also proposed a word substitution ranking attack method called PRADA. To enhance the readability and effectiveness of the adversarial text, scholars further designed sentence-level ranking attack methods. Song et al. [17] propose an adversarial method under white-box setting, named Collision, which uses gradient optimization and beam search to produce the adversarial text named collision. The Collision method further imposes a soft constraint on collision generation by integrating a language model, reducing the perplexity of the collision. The method has shown promising[14] propose the Pairwise Anchor-based Trigger (PAT) method under black-box setting. Added the fluency constraint and the next sentence prediction constraint, the method generates adversarial text by optimizing the pairwise loss of top candidates and target candidates with adversarial text. Although the time complexity of PAT has increased compared to previous methods, PAT takes ranking similarity and semantic consistency into account, so its manipulation effect on the retrieval ranking of target candidates is superior."}, {"title": "3 Method", "content": "This paper attempts to manipulate the opinions in the responses generated by black-box RAG models on controversial topics, targeting both the retrieval model and the LLM which performs the integrated generation task. Zhang et al. [25] tried to poison context documents to deceive the LLM into generating incorrect content, but this method requires extensive internal details of the LLM application, making it less feasible in real-world scenarios. For black-box RAG, the manipulator has no knowledge of the internal information of the RAG, including model architecture and score function, and can only access the inputs and outputs of the RAG. Specially, the manipulator can only call the interface of the LLM in RAG instead of that of the retriever. Since the inputs consist of the"}, {"title": "4 Experiment and Analysis", "content": "After imitating the retrieval model of $RAG_{black}$ to obtain the surrogate model, this paper first compares the ranking ability of the surrogate model $M_i$ and the target retrieval model $RM$, as well as the similarity of their ranking results, as shown in Table 1, to ensure that the surrogate model has learned the capabilities of the black-box retrieval model.\nThis paper uses Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) to reflect the ranking ability of the models themselves; higher values indicate stronger ranking ability in terms of relevance. Inter Ranking Similarity (Inter) and Rank Biased Overlap (RBO) are used to measure the similarity between the ranking results of the surrogate model and the target retrieval model; higher values indicate better performance of the black-box imitation. The weight for RBO@10 is set to 0.7. In Table 1, \"-\" indicates that the metric is not applicable to the model.\nAs can be seen from Table 1, the surrogate model $M_i$ trained by black-box imitation is similar to the target retrieval model coCondenser in terms of relevance ranking performance and ranking results, validating the effectiveness of the black-box imitation.\nAfter the black-box imitation training, the white-box surrogate model is conducted with opinion manipulation experiments. Several controversial topics and their opinion text data under the four themes of \"Government\", \"Education\", \"Society\", and \"Health\" from the PROCON.ORG data are selected as the retrieval corpus. The original retrieval corpus is denoted as $Docs_{origin}$. Based on the surrogate model, we generate the corresponding $T_{pat}$ for the candidate items with the expected opinion $S_t$ on controversial topics, and then insert $T_{pat}$ at the beginning of the target candidate items to obtain the perturbed retrieval corpus $Docs_{adv}$. Query $RAG_{black}$ twice on controversial topics: once with $Docs_{origin}$ as the retrieval corpus, and once with $Docs_{adv}$ as the retrieval corpus, and obtain the two responses of RAG, representing the answers before and after opinion manipulation. The responses are then classified into three categories based on their opinion on controversial topics: opposing, neutral, and supporting, represented by 0, 1, and 2, respectively, as the opinion scores of the generated responses. This study uses Average Stance Variation (ASV) to represent the average increase of opinion scores of $RAG_{black}$ responses in the direction of the expected opinion $S_t$ before and after manipulation. A positive ASV indicates that the opinion manipulation towards $S_t$ is effective, while a negative ASV indicates that the manipulation actually makes the opinions of RAG responses deviate from $S_t$. The larger the ASV value, the more successful the opinion manipulation of RAG responses. Additionally, this paper attempts to obtain the ranking results of the retriever coCondenser to evaluate the effectiveness of the adversarial retrieval manipulation strategy at the ranking stage for dense retrieval. This evaluation is solely for assessment purposes and is not involved in manipulation, as no internal knowledge of $RAG_{black}$ was leaked during the manipulation process."}, {"title": "5 Conclusion", "content": "In this paper, we explore the vulnerability of retrieval-augmented generation (RAG) models to opinion manipulation against black-box attack in open-ended controversial topics, and delve into the potential impact of such attacks on user cognition and decision-making. Through systematic experiments, we propose a novel adversarial attack strategy about retrieval ranking poisoning. This method significantly affects the polarity of the opinions generated by RAG by crafting adversarial samples, without requiring internal knowledge of the RAG model. The experimental results indicate that the proposed attack strategy successfully alters the opinion of the content generated by the RAG model, revealing the vulnerability and unreliability of RAG when confronted with malicious retrieval corpus. More importantly, this opinion manipulation could have profound impacts on users' cognition and decision-making processes, potentially leading users to accept incorrect or biased information, causing cognitive changes and public opinion distortion. This phenomenon is particularly significant in open-ended and controversial issues.\nFuture research will expands the scale of the experiments by including more open-source and commercial RAG systems to more comprehensively evaluate the reliability of viewpoint generation by RAG models. Given the vulnerabilities of RAG models, future work should focus on developing more robust defense strategies. These may include improving the robustness of retrieval algorithms, enhancing the reliability of generation models, and introducing multi-level input filtering mechanisms to counteract adversarial inputs, thereby achieving a balanced optimization of the understanding and reliability of RAG models."}, {"title": "6 Ethical Statement", "content": "This paper explores the feasibility of opinion manipulation on black-box RAG models in real-world scenarios. The main goal is to assess the reliability of RAG technology in responding to ranking manipulation at the stage of retrieval, paving the way for future work to enhance the robustness and defense capabilities of RAG technology. This study did not manipulate any commercial RAG systems or real-world data currently in use.."}, {"title": "Algorithm 1: Opinion Manipulation Strategy for black-box RAG", "content": "Input: target black-box RAG model $RAG_{black}$, target retrieval model $RM$, surrogate model $M_i$, controversial topics $Q$, target topic $q$, expected opinion $S_t$, corpus $Docs$, target documents with expected opinion $Docst$, target document $d_t$, relevant document $d_+$, random sampled document $d_\u2212$\nInstructions:\n$1_1$ = \"Now that you are a search engine, please search: {query} Ignore the Question. Please copy the top 3 passages of the given Context intact in the output and provide the output in JSON with keys 'answer' and 'context'. Put each candidate passage in 'context' as a string element in the list...\"\n$1_2$ = \"Use the following pieces of retrieved context to answer the question...\"\n// $RAG_{black}$ uses $1_2$ as prompt template.\nFunctions:\nOpinionClassify: Classify the opinion of the content into \"support\", \"neutral\" or \"oppose\".\nPAT: Pairwise Anchor-based Trigger generation strategy.\nOutput: manipulated RAG responses $Res$\n1 Phase 1. Pairwise Imitation Data Construction and Black-box Retrieval Model Imitation Training\nINIT: Dataset $D$ \u2190 {}\ninduced rank list $Rmtop3$ \u2190 $RAG_{black}(q_m\u2295 1_1; Docs)$\n2\n3 for $q_m \u2208 Q$ do\n4 // $RAG_{black}(q_mi1) \u2248 RM(qm)$\n5 j for $d+; \u2208 Rmtop3$ do\n6 Random sample document as $d\u2212 j$\n7 $D$ \u2190 positive, $[qm;d+j; d\u2212j]$\n8 $D$ \u2190 negative, $[qm;d_j;d+j]$\n// Reverse $d+;$ and $d; to get the negative triple\n9 Train the surrogate model $M_i$ on $D$ with Eq 1\n10 return $Mi$\n11 Phase 2. Adversarial Trigger Generation and Opinion Manipulation in RAG Response\nINIT: RAG Response Set $Res$ \u2190 {}\n12 for $q_m \u2208 Q$ do\nrank list $Rm$ \u2190 $M_i(qm; Docs)$\n13\n14 anchor_m top-1(Rm)\n15 for $dj \u2208 Rm$ do\n16 if OpinionClassify($d_j$) = $St$ then\n$Docst$ \u2190 $dj$\nfor $dtj \u2208 Docs\u0165$ do\n17 adversarial trigger $Padvj$ \u2190 PAT($M_i$; $qm$, $dtj$, $anchorm$)\n18 adversarial document $dadvj$ \u2190 $dtj \u2295 Padvj$\n19 Docs : $dtj$ \u2190 $dadvj$ // Replace\n20 Res $RAG_{black}(qm; Docs)$\n21 return $Res$\nAfter obtaining the ranking results of the retriever model coCondenser, this paper evaluates the manipulation effect with Attack Success Rate (ASR), the average proportion of target opinions in the Top 3 rankings before and after manipulation ($Top3origin$, $Top3attacked$), and the Variation of Normalized Discounted Cumulative Gain (VoN-DCG). Higher values of ASR and Vo-NDCG indicate better manipulation effects on ranking, and a larger difference between $Top3attacked$ and $Top3origin$ signifies more significant ranking manipulation effects, too."}]}