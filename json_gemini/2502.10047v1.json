{"title": "Janus: Collaborative Vision Transformer Under Dynamic Network Environment", "authors": ["Linyi Jiang", "Silvery D. Fut", "Yifei Zhu", "Bo Li"], "abstract": "Vision Transformers (ViTs) have outperformed traditional Convolutional Neural Network architectures and achieved state-of-the-art results in various computer vision tasks. Since ViTs are computationally expensive, the models either have to be pruned to run on resource-limited edge devices only or have to be executed on remote cloud servers after receiving the raw data transmitted over fluctuating networks. The resulting degraded performance or high latency all hinder their widespread applications. In this paper, we present Janus, the first framework for low-latency cloud-device collaborative Vision Transformer inference over dynamic networks. Janus overcomes the intrinsic model limitations of ViTs and realizes collaboratively executing ViT models on both cloud and edge devices, achieving low latency, high accuracy, and low communication overhead. Specifically, Janus judiciously combines token pruning techniques with a carefully designed fine-to-coarse model splitting policy and non-static mixed pruning policy. It attains a balance between accuracy and latency by dynamically selecting the optimal pruning level and split point. Experimental results across various tasks demonstrate that Janus enhances throughput by up to 5.15\u00d7 and reduces latency violation ratios by up to 98.7% when compared with baseline approaches under various network environments.", "sections": [{"title": "I. INTRODUCTION", "content": "The ubiquitous deployment of cameras in various domains, from surveillance to autonomous vehicles [1]\u2013[3], has led to an exponential increase in the volume of visual data. This data needs to be processed and analyzed with low latency and high accuracy to meet the application-level performance needs consistently. Vision Transformers (ViTs) have emerged as a powerful alternative to traditional convolutional neural networks (CNNs) in this field, achieving state-of-the-art (SOTA) performances on a variety of computer vision tasks, such as image classification [4], object detection [5], semantic segmentation [6], and video understanding tasks [7].\nWhile ViTs offer unprecedented accuracy, they are computationally expensive, requiring millions of parameters and billions of floating-point operations (FLOPs) [8], [9], which makes it difficult for real-time analytics. For example, ResNet- 50 achieves 80.12% accuracy in ImageNet-1k classification while ViT-B achieves a higher accuracy 85.43% but with 8.02\u00d7 FLOPs [10].\nIn the deployment of computer vision models, a typical approach is on-device computing (Fig. 1 (a)), where computational resources on edge devices are often constrained (e.g., the local GPU of Jetson Orin Nano can only serve the ViT- L model for inference at a low-speed of 1.51 FPS). Thus, in this setup, on-device inference often involves the optimization of models. Techniques such as knowledge distillation [11], pruning [12], quantization [13], neural architecture search [14], and lightweight networks [15] are widely studied to offer competitive service with a smaller model footprint. However, the optimization of models for edge devices still inevitably compromises accuracy and is fundamentally limited by the scarce resources on the device side.\nAnother prevailing approach is to perform computer vision tasks in a distant cloud server [16]. In this setup (Fig. 1 (b)), data collected by edge devices are transmitted to a remote cloud server for inference utilizing more powerful accelerators. However, this approach highly depends on network conditions and introduces extra communication delay [17].\nTo address these limitations, a recent line of work [17]\u2013 [19] has proposed model splitting as a collaborative approach between device and cloud for low-latency inference. Such"}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "Despite the significant advantages demonstrated by collaborative inference in serving CNNs, it is non-trivial to apply the same paradigm to serve the emerging ViTs. Fundamentally, the key to such an effective collaboration lies in data reduction during the inference phase. Namely, by minimizing the volume of data transferred, we can reduce communication latency, ultimately leading to a reduction in E2E latency. While the down-sampling operations inherent in CNN structures naturally reduce input tensor size, minimizing the communication latency of transfer data, a vanilla ViT does not change the input tensor size at all. This naturally deprives ViTs of benefiting from the collaborative framework for further latency reduction.\nTo push the limit of low-latency ViT inference, in this paper, we introduce Janus\u00b9, a collaborative cloud-device inference system specifically designed for ViT models that achieves low- latency and high-accuracy inference over dynamic networks (Fig. 1 (c)). Token pruning in ViTs is an existing technique used to prune redundant image patches and basic input units in ViT models. Our key insight is that utilizing token pruning to reduce patches can create the opportunity for data reduction and enable potential synergy with model splitting for collaborative inference if carefully designed. Janus judiciously integrates the token pruning and the model splitting techniques, supported by carefully designed pruning and splitting policies to realize cloud-device collaboration. Specifically, the system includes a collaboration-aware token pruner and a fine-to- coarse model splitter. To determine the optimal pruning level and split point, we design a ViT-oriented latency profiler and a dynamic scheduler. These components empower Janus to make efficient configuration choices under dynamic network environments, addressing the unique challenges posed by ViTs. To the best of our knowledge, it is the first work that realizes low-latency collaborative ViTs inference. In summary, our contributions are provided as follows:\n\u2022 By analyzing the inference latency and structure of ViTs, we identify the opportunities and challenges of collaborative cloud-device inference for ViTs.\n\u2022 We introduce Janus, a novel collaborative cloud-device system designed for the low latency inference of the emerging ViTs over dynamic networks.\n\u2022 We propose a collaboration-aware token pruner that min- imizes accuracy degradation. We further design a fine-to- coarse model splitter that reduces the search space and system overheads."}, {"title": "A. Vision Transformers", "content": "ViT [4] is a groundbreaking model series that uses an encoder-only transformer architecture designed for computer vision tasks, without the traditional use of convolution operations. The core concept of ViTs is treating all the image patches as tokens and constructing multi-head self- attention among them. This self-attention mechanism computes a weighted sum of the input data, where the weights are computed based on the similarity between the patches in the image. This allows the model to discern the significance of different patches, which helps it capture more informative representations when making predictions.\nA typical ViT model comprises three key components:\nEmbedding: In the context of an input image with di- mensions \u0397 (height), W (width), and C (channels), the image is first split into HW/P\u00b2 patches, where P refers to the patch height and width. Each patch is then flattened to a vector of length CP2 and linearly projected to patch embeddings. Learnable position embeddings are then added to the patch embeddings to retain positional information to have the complete input embedding vector for all patches.\nTransformer Encoder: The multilayer transformer encoder then transforms input vectors into the same number of output vectors with the same length. The encoder includes a multi- head attention (MHA) layer, followed by a multilayer per- ception (MLP) which provides nonlinearity. The transformer encoder captures long-range dependencies and contextual in- formation in the input data.\nHeads: After being processed by the transformer encoder, the output vector is further transformed into the output label through the task-specific neural networks, referred to as heads, to provide predictions for a specific task. For example, ViT usually uses an MLP as the head for image recognition tasks.\nCompared to traditional CNNs, ViT and its subsequent mod- els gain advantages in capturing global relationships. Instead of focusing on local features in CNNs, such a global view allows ViT to understand complex visual patterns, making it a SOTA solution in various computer vision tasks [8]."}, {"title": "B. Cloud-only or Device-only: One Size Fits All?", "content": "The rapid inference capabilities of ViTs are especially cru- cial in low-latency applications. In the following subsections, we delve into the sources of ViT inference latency, providing insights into the challenges and opportunities for optimizing"}, {"title": "C. Challenges towards Collaborative ViT Inference", "content": "Inspired by our measurement insights in Section II-B, a nat- ural question arises: Is it possible to effectively leverage both the device and cloud resources to enhance the performance of ViTs inference?\nFor CNN-based vision models, model splitting [18], [24], [25] is a validated approach to address this challenge. The down-sampling operations in CNNs create opportunities for in- termediate data size reduction, consequently making it possible to reduce the data transmission latency during collaboration. For instance, when executing AlexNet, a representative model in CNNs, on the ImageNet-1k dataset, the data size after the execution of its Pooling Layer 5 is reduced from 147.88 KB to 26.02 KB, indicating a 95.68% reduction compared to the input data size of 602.53 KB. The significantly re- duced data size suggests a potential benefit of partitioning the CNN model into head and tail models, with the head model (e.g., from the initial layer to the Pooling Layer 5) being executed in the edge device and the tail model (e.g., the remaining layers in the previous example) being executed in the cloud server. Compared to the cloud-only approach, even though this approach may extend computation time when part of the workload runs on the edge device, it compensates by reducing data transmission and cloud-side computation latency, ultimately resulting in decreased total latency. On the other hand, compared to the device-only approach, despite this"}, {"title": "III. SYSTEM DESIGN", "content": "This section presents the system design of Janus. To address the challenges revealed by the motivation study, we introduce the first cloud-device collaborative ViT inference framework, named Janus. Janus builds upon the recent development of the token pruning technique and employs a carefully designed dynamic token pruning policy and a model splitting policy to facilitate adaptive and efficient collaborative ViT inference over dynamic networks.\nOur system, illustrated in Fig. 3, encompasses both of- fline and online phases. In the offline phase, we deploy a lightweight linear profiler (\u00a7III-C) to predict inference latency under various conditions. At run time, the dynamic scheduler (\u00a7III-D) operates in real-time, leveraging the profiled insights to determine optimal split points and pruning levels based on the target service-level agreement (SLA) for latency and the network environment. Guided by the dynamic scheduler, the collaboration-aware token pruner (\u00a7III-A) works with the fine-to-coarse model splitter (\u00a7III-B) to effectively prune and split the inference of a ViT model across device and cloud computing environments. The execution engine (\u00a7IV-A) then takes charge of coordinating the inference process and managing communication between partitions. The details are presented in the following."}, {"title": "A. Collaboration-aware Token Pruner with Mixed Pruning Policy", "content": "The first module in our system is a collaboration-aware token pruner that realizes dynamic token size reduction under the guidance of the scheduler.\nObservation. Token pruning is a novel model optimiza- tion technique in transformer-based models that reduces the number of tokens to be executed [26], [27]. Based on the importance or relevance of each token, a subset of tokens is pruned for removal at each transformer layer. While it is mostly studied to accelerate model inference, we observe that it has a huge potential synergy with model splitting to realize collaborative inference due to its data reduction outcome. For instance, we deploy the SOTA pruning approach, ToMe [27], which prunes a fixed number of tokens at each layer. Specifically, following the same setting as the original paper, we prune 23 image patches at each layer of the ViT-L@384 in our experiments and find out that 95.7% of image patches are pruned, demonstrating a pathway to substantial data reduction. However, how to incorporate it into the collaborative frame- work so that ViT can be executed in low latency over dynamic networks has not been explored before. Sticking to a fixed pruning level, as the current literature does, overlooks the intrinsic characteristics of the underlying computing infras- tructures, which leads to suboptimal performance. Considering the significant difference in computing capability between on-device and cloud computing, aggressive pruning can sig- nificantly reduce computing workload and latency on edge devices. Conversely, given the ample resources in the cloud, excessive pruning in the cloud can lead to reduced accuracy without significant gains in throughput.\nDesign. Therefore, we propose a novel mixed pruning policy specifically tailored for cloud-device collaboration. Our policy adopts mixed pruning levels among different layers in ViT. We use the term \u201cdeclining rate\" to measure the rate at which tokens are pruned in the model during inference and denote it as a. For the entire model, a higher declining rate a results in a larger cumulative reduction in tokens, leading to more loss in accuracy. For each layer of the model, the number of pruned tokens decreases with the increasing layer number. Specifically, we adopt an exponential form to control the extent of token pruning. The number of reduced tokens at each layer l is expressed as follows:\n$\\Delta \\alpha_{l} = \\begin{cases} [\\alpha_t 2^{\\alpha (N-(l-1))}] & \\text{if } \\alpha \\neq 0, \\\\ 0 & \\text{if } \\alpha = 0, \\end{cases}$ (1)\nThe declining rate a increases in increments of t within the specified range a \u2208 [0, \u03b1max]. When a = 0, no pruning occurs. The maximum value for \u03b1max is determined by the following constraint:\n$\\sum_{l=1}^{N} [2^{\\alpha_{\\text{max}}(N-(l-1))}] \\leq x_0 - 1$ (2)\nwhere xo is the initial number of tokens in the model. This ensures that the cumulative reduction in tokens does\""}, {"title": "B. Fine-to-Coarse Splitting Points Generation", "content": "After pruning, we design a model splitter to partition execution between a device and a server. As a model consisting of a sequence of layers, it allows us to split the model at the granularity of individual layers. For a ViT with N transformer layers, there are N+2 candidate split points within the model, including the point at the very beginning of the model, after each of the N transformer layers, and at the end of the model.\nObservation. As we mentioned before, the decrease in latency primarily comes from the reduction in transferred data. In the declining pruning policy, the front part experiences more data reduction, leading to a greater reduction in data transmission latency, which makes it easier to identify the highest-performing split point that brings the lowest latency. Conversely, the rear part experiences less data reduction, resulting in a smaller decrease in data transmission latency. Even if a split point with the lowest latency is identified in the rear part, its latency difference with the surrounding candidate split points is minimal during model inference. For example, in the ViT-L model with a pruning ratio a set at 0.25, the front part experiences approximately 90% data reduction, while the rear part contributes only 10% to the total data reduction. As the split point moves from the front to the rear, the benefits in transmission latency reduction gradually diminish. Based on this key observation, we decide to focus more on layers in the front portion where the potential for latency reduction is more significant, instead of considering splitting at each layer uniformly. This design allows us to reduce the search space of the partitioning points and overall system overheads.\nDesign. Consequently, we design a fine-to-coarse splitting points generating policy, aligning with the declining mixed pruning policy. In our fine-to-coarse splitting policy, more candidate points are set in the front part of the model, while fewer are in the rear part. Formally, the candidate split point set, denoted as C, is defined as the union of two sets:\n$C = \\{0, N + 1\\} \\cup \\{S_i | S_i = S_{i-1} + \\frac{k}{i} \\} (3)$\n$S_1 = 1, i \\geq 2 \\text{ and } S_i \\leq N\\}$"}, {"title": "C. Lightweight Linear Profiler", "content": "Given the varying trade-offs of different split points and de- clining rates, we need to consider latency constraints, network conditions, and expected latency to select the most suitable configuration. While the latency constraint is given by the user and the network condition estimation has been widely studied, it remains unknown how to determine the expected computation latency for each candidate point.\nObservation. We first conduct an experiment involving random pruning of ViT layers, observing the latency of each transformer layer. Experiments use different ViT models, in- cluding ViT-B and ViT-L@384. Fig. 5 illustrates the experimental results, showing the average layer inference latency for different numbers of input tokens per layer. As can be seen, for either the edge device or the cloud server, the inference latency of each layer exhibits a strong positive linear relationship with the number of its input tokens. Notably, in all cases, the correlation coefficient exceeds 0.85 and the P-value is very close to 0.\nDesign. Based on these observations, we propose a simple but effective linear model for each ViT as a profiler. The profiler adopts a linear function as the prediction model, which"}, {"title": "IV. JANUS RUNTIME", "content": "This section describes the Janus runtime and the implemen- tation of the prototype."}, {"title": "A. Execution Engine", "content": "We develop two customized modules as execution compo- nents of the Janus system. These modules are the Jdevice, which serves as the edge device infrastructure, and the Jcloud, which serves as the cloud server infrastructure.\nIn Jdevice, both the profiler and the dynamic scheduler are deployed on the device side. When an inference task arrives, the system reads the model type and latency requirements. The profiler gives corresponding latency prediction models and estimates the network conditions. Subsequently, the dynamic scheduler determines deployment parameters, including the split point and declining rate. The deployment parameters, along with the model type, are then transmitted to the Jcloud. Based on deployment parameters, the customized device-side"}, {"title": "Algorithm 1: Workflow of dynamic scheduler", "content": "Input:\nSpecific ViT model M; Number of layers in the ViT\nN; Layer in the ViT {l | l = 1\u00b7\u00b7\u00b7 N}; Prediction\nmodel fM (X\u03b9) that returns the latency of executing\nX\u03b9 tokens; Estimated current bandwidth B; Data size\nof each token DM; Latency requirement SLA\nOutput:\nSelection of declining rate and split point\nProcedure\nfor a \u2190 0 to \u03b1max do\nChoose a as the declining rate\n\u2192 {x\u03b9 | l = 1 N}: number of tokens at layer l\nfor l\u2190 1 to N + 1 do\nTdevicel \u2190 fdevicel (x\u03b9)\nTcloudl \u2190 fcloudl (x\u03b9)\nif l\u2208 C then\nTcomml \u2190 x1\u2217DMB\nend\nend\nLs,a = arg min (\u2211s\u03b9=1 Tdevicel + \u2211N\u03b9=s+1 Tcloudl + Tcomml )\nif Ls,a\u2264 SLA then\nreturn declining rate a and split point s\nend\nend\nreturn declining rate \u03b1max and split point s // cannot\nmeet the latency requirement\n\u03b1max and the split point s that offers the lowest latency among all configurations (line 17)."}, {"title": "C. Janus Performance", "content": "Overall Improvements: We first present the overall perfor- mance of Janus and baselines on different network conditions and tasks in Fig. 7. Janus successfully addresses the short- comings of the baselines, achieving the highest performance across diverse network conditions for both tasks. Specifically, compared to Device-only, Cloud-only, and Mixed approaches, Janus improves average throughput by a factor ranging from 1.23x to 3.04\u00d7, 1.20\u00d7 to 5.15\u00d7 and 1.00\u00d7 to 3.04\u00d7, respec- tively, while reducing violation ratios by a range of 89.4% to 98.7%, 49.8% to 98.3% and 49.8% to 98.3%, respectively. Janus achieves an average accuracy improvement ranging from 0.01% to 0.29% over the baselines, demonstrating that it provides higher throughput with a smaller reduction in accuracy compared to the baseline methods.\nUnderstanding Janus Improvements: To understand the reasons behind Janus yielding such benefits, we present a representative period extracted from the bandwidth trace for further illustrations. For brevity, we focus on the image recog- nition task under LTE network conditions.\nIn Fig. 8, the top figure illustrates bandwidth traces of 4G LTE (LTE Driving, Run 8 trace) networks over a specific duration. The bottom figure depicts the throughput of Cloud- Only, Device-Only, and Janus in the corresponding network environments. Yellow-shaded regions represent the application of model splitting to partition the model (excluding Cloud- Only or Device-Only inference), while blue-shaded regions indicate the application of token pruning to reduce tokens. Notably, due to the necessity of token pruning occurring before model splitting, the blue-shaded regions cover up the yellow- shaded regions in the figure."}, {"title": "D. Sensitivity Analysis", "content": "Sensitivity to Network Settings: We investigate the impact of varying bandwidth on both tasks. Fig. 9 (a) and Fig. 9 (c) show the variation in inference latency of different tasks under increasing bandwidth. Fig. 9 (b) and Fig. 9 (d) offer insights into the declining rate and split points selected by"}, {"title": "E. Overhead Analysis", "content": "We conducted an overhead analysis based on the prototype. The E2E latency of Janus mainly comes from four modules, including system overhead, data transmission, device com- puting, and cloud computing. The breakdown of their time consumption in real-world scenarios is illustrated in TABLE II. Due to limited space, we only present the detailed results for the image recognition task. In real-world deployments, we tested Janus across WiFi, 5G, and 4G networks, with average bandwidths of 29.3 Mbps, 17.8 Mbps, and 10.1 Mbps, respectively. To accommodate the network conditions, we set latency constraints of 500ms for image recognition tasks and 1500ms for video classification tasks. As can be seen, as network conditions degrade, the system overhead of Janus increases. This is primarily due to the dynamic scheduler needing to search for larger declining rate values to ensure the task meets latency constraints. The overall system overhead is less than 0.21%, indicating Janus is lightweight."}, {"title": "VI. RELATED WORK", "content": "Most efforts in accelerating ViT model inference are con- centrated on model optimization. Optimization methods, such as knowledge distillation [11], pruning [12], [35]\u2013[38], quan- tization [39], neural architecture search [14], and designing lightweight networks [40] have been developed to optimize the inference phase of ViT models. These methods aim to reduce the computational complexity and memory demands of the model, leading to faster inference. However, these models and techniques only work in local device environments. They cannot be directly applied to the collaborative inference setting and do not fully exploit the potential of resource-rich cloud servers to accelerate inference further.\nCloud-device collaborative inference enables the partitioned deployment of the model between the device and cloud in CNN architectures [17]\u2013[19], [41]\u2013[47]. Fundamentally, the benefit of cloud-device collaboration for inference acceleration relies on data reduction during the inference phase, leading to a shorter communication latency. Unlike the typical CNN architecture, the unique transformer architecture lacks this property, rendering the existing cloud-device collaboration efforts inapplicable. In our work, we introduce the first cloud- device collaborative inference framework for emerging ViTs."}, {"title": "VII. CONCLUSION", "content": "In this work, we present Janus, a cloud-device collaborative computing system for low-latency ViT inference over dynamic networks. Janus satisfies the stringent latency requirement and delivers high accuracy through innovative adaptation of token pruning techniques and meticulously designed pruning and model splitting policies. The design perfectly captures the characteristics of the underlying computing infrastructure and the intrinsic properties of ViT models. Furthermore, we develop a lightweight profiler to accurately forecast computing latency across various candidate points. Leveraging this insight alongside the tailored pruner and splitter, we propose an efficient scheduling policy to realize collaborative inference with low computing complexity. Extensive evaluations based on real-world devices and network scenarios demonstrate the effectiveness of Janus in achieving low latency and main- taining high accuracy. We believe that the development of Janus not only reveals new opportunities in model-aware video analytics systems but also significantly impacts the future serving stack of emerging transformer-based AI applications."}]}