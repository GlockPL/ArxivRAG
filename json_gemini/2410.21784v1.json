{"title": "MARCO: Multi-Agent Real-time Chat Orchestration", "authors": ["Anubhav Shrimal", "Stanley Kanagaraj", "Kriti Biswas", "Swarnalatha Raghuraman", "Anish Nediyanchath", "Yi Zhang", "Promod Yenigalla"], "abstract": "Large language model advancements have enabled the development of multi-agent frameworks to tackle complex, real-world problems such as to automate tasks that require interactions with diverse tools, reasoning, and human collaboration. We present MARCO, a Multi-Agent Real-time Chat Orchestration framework for automating tasks using LLMs. MARCO addresses key challenges in utilizing LLMs for complex, multi-step task execution. It incorporates robust guardrails to steer LLM behavior, validate outputs, and recover from errors that stem from inconsistent output formatting, function and parameter hallucination, and lack of domain knowledge. Through extensive experiments we demonstrate MARCO's superior performance with 94.48% and 92.74% accuracy on task execution for Digital Restaurant Service Platform conversations and Retail conversations datasets respectively along with 44.91% improved latency and 33.71% cost reduction. We also report effects of guardrails in performance gain along with comparisons of various LLM models, both open-source and proprietary. The modular and generic design of MARCO allows it to be adapted for automating tasks across domains and to execute complex use-cases through multi-turn interactions.", "sections": [{"title": "1 Introduction", "content": "Advancements in LLM technology has led to a lot of interest in applying Agents framework to realise solutions which require complex interactions with the environment including planning, tools usage, reasoning, interaction with humans. Recent works (Wang et al., 2024; Huang et al., 2024) demonstrate potential of LLMs for creating autonomous Agents while there are numerous challenges to overcome and provide a seamless experience for end users who interact with the system at a daily basis. LLMs are probabilistic next token prediction systems and by design, non-deterministic which can introduce inconsistencies in the output generation that can prove challenging for features like function calling, parameter value grounding, etc. There are also challenges of domain specific knowledge which can be an advantage and disadvantage at the same time. LLMs have biases inherent in them which can lead to hallucinations, at the same time it may not have the right internal domain specific context which needs to be provided to get the expected results from an LLM.\nWe present our work on building a real time conversational task automation assistant framework with the following emphasis, (1) Multi-turn Interface for, (a) User conversation to execute tasks (b) Executing tools with deterministic graphs providing status updates, intermediate results and requests to fetch additional inputs or clarify from user. (2) Controllable Agents using a symbolic plan expressed in natural language task execution procedure (TEP) to guide the agents through the conversation and steps required to solve the task (3) Shared Hybrid Memory structure, with Long term memory shared across agents which stores complete context information with Agent TEPs, tool updates, dynamic information and conversation turns. (4) Guardrails for ensuring correctness of tool invocations, recover for common LLM error conditions using reflection and to ensure general safety of the system. (5) Evaluation mechanism for different aspects and tasks of a multi-agent system.\nThis is demonstrated in the context of task automation assistant which supports adding usecase tasks to provide users a conversational interface where they can perform their intended actions, making it easier for them to refer to informational documents, interact with multiple tools, perform actions on them while unifying their interfaces. We provide detailed comparison across multiple foundational LLMs as backbone for our assistant tasks like Claude Family models (Anthropic, 2024), Mis-"}, {"title": "2 Related Work", "content": "Improvements to LLM technology through the release of foundational LLMs like GPT-4 (OpenAI et al., 2024), Claude (Anthropic, 2024) and Mistral (Jiang et al., 2024) has led to a flurry of research around autonomous agents and frameworks (Wang et al., 2024; Huang et al., 2024). Zero shot Chain-of-Thought (COT) reasoning (Kojima et al., 2023) allows LLMs to perform task reasoning by making it think step by step. LLMs can invoke external tools based on natural language instructions. HuggingGPT (Shen et al., 2023b) can coin series of model invocations to achieve complex tasks mentioned by the user. Toolformer (Schick et al., 2023) demonstrates how LLMs can be used as external tools through API invocations selecting the right arguments to be passed from few examples and textual instructions. Agents framework (Zhou et al., 2023) discuss using natural language symbolic plans called (Standard Operating Procedures) SOPs which define transition rules between states as the agent encounters different situations to provide more control over agent behavior along with memory to store relevant state information within the prompt (Fischer, 2023; Rana et al., 2023) or long term context externally (Zhu et al., 2023; Park et al., 2023). Amazon Bedrock Agents \u00b9 provide interface to quickly build, configure and deploy autonomous agents into business applications leveraging the strength of foundational models, while the framework abstracts the Agent prompt, memory, security and API invocations. LangGraph 2 is an extension of LangChain which facilitates the creation of stateful, multi-actor applications using large language models (LLMs) by adding cycles and persistence to LLM applications thus enhancing their Agentic behavior. It coordinates and checkpoints multiple chains (or actors) across cyclic computational steps. While these frameworks present novel ways for LLMs to act in a desired behaviour, they often have accuracy-latency trade-off where to improve on the accuracy the system latency increases due to multi-step planning and thinking (Yao et al., 2023; Wei et al., 2023). Our proposed solution, MARCO, not only interacts with user in a multi-turn fashion but also has multi-turn conversation with deterministic multi-step functions which comprises of pre-determined business logic or task execution procedure (TEP) requiring agents only at intelligent intervention related steps. Along with the usecase TEPs, multi-step functions and robust guardrails to steer LLM behaviour, MARCO is able to perform complex tasks with high accuracy in less time as detailed in subsequent sections."}, {"title": "3 MARCO: Multi-Agent Real-time Chat Orchestration", "content": "In this section, we discuss our approach for MARCO. Section 3.1 formulates the problem statement in terms of Task Automation via real-time chat, followed by components of MARCO in section 3.2 and the evaluation methods on performance and latency for MARCO in section 3.3."}, {"title": "3.1 Problem Statement", "content": "Given an user (Actor), who wishes to perform a task with intent $I \\in \\{OOD, Info, Action\\}$; where Out-Of-Domain (OOD) intent is defined as any user query which is not in scope of the system such as malicious query to jailbreak (Shen et al., 2023a; Rao et al., 2024) the system, foul language or unsupported requests, \u201cInfo\u201d intent is defined as getting information from predefined data-sources and indexed documents $(D_{index})$, and \u201cAction\u201d intent is defined as a performing a use-case related task $(u_x)$ which involves following a series of instructions/steps (Task Execution Procedure, TEP) defined for the usecase and accordingly invoking the right set of tools/functions $(F = \\{F_1,F_2, ..., F_N\\})$ with the identified required parameters $(P = \\{P_{F_1}, P_{F_2}, ..., P_{F_N}\\})$ for each function respectively. The objective for a task automation system is to, (1) interpret the user intent I for each query, (2) identify the relevant usecase $u_x$, (3) understand the steps mentioned in its TEP, (4) accordingly call the right sequence of tools $F$ with required parameters $P$, (5) correlate TEP, tool responses and requirements and conversation context to communicate back with the user, and (6) be fast and responsive for a real-time chat.\nAn example scenario is shown in figure 1 where User first asks \"The sale of certain item is going down in my restaurant. Can you please help me find out why?\", i.e. $I = Action$ for which"}, {"title": "3.2 MARCO \u2013 Components", "content": "MARCO built for task automation has 4 primary LLM components, (i) Intent Classifier, (ii) Retrieval Augmented Generation (RAG) to answer domain related informational queries, (iii) MARS for tasks orchestration and execution, and (iv) Guardrails. The sections below cover each of the component, except for RAG where the implementation details are out of scope for this paper."}, {"title": "3.2.1 Intent Classifier", "content": "Intent Classifier's (IC) primary role is to understand the intent behind an incoming user message considering the conversation context, and to seamlessly orchestrate between RAG for answering informational queries and Multi-Agents system (MARS) to execute supported tasks. IC also takes the role of first level guardrails to identify and gracefully reject queries to protect the underlying modules from harmful jailbreak instructions and Out-Of-Domain (OOD) queries. At a high level IC performs intent classification into one of the three supported classes {Info, Action, OOD}, leveraging language understanding capability of LLMs. Major challenges faced by intent classifier can be found in Appendix A.6."}, {"title": "3.2.2 Multi-Agent Reasoner and Orchestrator (MARS)", "content": "When a user query is classified as an $I = Action$ intent, the chat conversation history is redirected to MARS (Multi-Agent Reasoner and Orchestrator) module which is a Multi-Agent system responsible for (1) understanding the user's request and tool responses in the chat context (2) planning and reasoning for the next action according to the Task Execution Procedure (TEP) steps, (3) selecting relevant LLM Agent for the task, and (4) invoking the relevant tools/tasks with their required parameters. The key component of MARS are the LLM Agents, which we call Task Agents. These Task Agents comprise of their own TEP steps, tools/functions also known as Deterministic Tasks, Sub-Task-Agents (dependent Task Agents) and common instructions for reasoning and output formatting. We will explain each of these in detail:\nDeterministic Tasks: Task Execution Procedure (TEP) steps can be very complex with multiple instructions and steps to follow based on a given use-case scenario. While some of these steps require high judgement and reasoning (understanding natural language to parse required arguments, intents, performing checks defined in plain text without writing explicit code), most of the steps in the TEP are deterministic sequence of API calls, processing and propagating the output gathered from API1 to API2 and so on. Such sequence of deterministic steps can be encapsulated as a single tool to the LLM Agent, which when called performs the sequence of these deterministic steps and communicates with the agent intermittently with updates or any high judgement reasoning or inputs required by the underlying APIs (for example refer to Appendix A.9).\nTask Agents: A usecase TEP can be divided into multiple Sub-Tasks which are logical abstractions of complex steps inside the TEP. For example, if a usecase $u_x$ has sub-branches {a, b, c}, each with their own set of steps to follow, then each can be created as a Task Agent (A) where Agent $A_x$ has agents {$A_a, A_b, A_c$} as it's child Task-Agents. Each of these child Agents may further have their own children Agents based on their TEP complexity. A Task Agent has the steps comprised in its TEP along with the list of available determinisitic tasks/functions that the particular Agent can utilize, for e.g. The \u201cSales Drop Analysis"}, {"title": "3.2.3 Guardrails", "content": "LLMs exhibit stochastic behavior, generating varying outputs for the same input. They are susceptible to hallucination (Bang et al., 2023; Guerreiro et al., 2023), producing responses with fabricated or inaccurate information. It is crucial to establish mechanisms to steer LLMs in the desired direction for reliable systems. We introduce guardrails to identify issues and prompt the LLM-Agents to reflect on their mistakes, correcting their responses. Common issues and proposed guardrail solutions are: (1) Incorrect Output Formatting: Generating incorrect formats despite detailed instructions, causing parsing issues. If parsing fails, a reflection prompt is added to the Agent's chat history, and sent for a retry. (2) Function Hallucination: Hallucinating non-existent function names, even when prompted to use only existing tools. Our guardrails checks if the generated function name exists in the available tools and Sub-Agents. If not, reflection prompt is added. (3) Function Parameter Value Hallucination: When making function calls with required parameters, LLMs sometimes hallucinate parameter values instead of asking relevant questions to the user. This often occurs due to pre-trained dataset biases because they have seen this pattern frequently during pre-training, making it challenging to unlearn using prompting techniques. For each function parameter p, the module checks if p is part of the function schema; if not, p is removed (e.g., for get_low_sales_items(merchant_id, restaurant_name), the Agent also generated menu_item as a parameter). The parameter value for non-"}, {"title": "3.2.4 Context Sharing", "content": "As MARCO has multiple components (IC, MARS, RAG) and is a multi-turn multi-agent conversation system, it needs a mechanism to share the context amongst each other. Along with the usual roles of [[SYSTEM], [USER], [AGENT]] similar to Bedrock's Claude messages API format\u00b3, we introduce separate roles for function responses and guardrails, [FUNCTION_RESPONSE], [GUARDRAILS], respectively. This allows LLM-Agents to better differentiate each message in the chat history as the conversation is multi-turn from both Actor and Deterministic tasks (for example Figure 1 reason_for_low_sales() task communicates multiple times to MARCO), and it prevents jailbreaking by malicious Actors. When a Parent Agent (Agentp) loads its Child Agent (Agentc), the [SYSTEM] prompt is updated with Agentc details and a message is added to the chat history to capture that an agent switch has occurred. The common chat history thread is shared"}, {"title": "3.3 Evaluation Methods", "content": "A real-time task automation system should have highly accurate execution as well as fast turn-around time. Keeping these tenets in mind, we evaluate MARCO components on quality and accuracy of generated responses along with time taken to produce such outputs. For evaluating MARS we compare the expected function call and parameter $(F, P_F)$ with the generated function call and parameter $(\\hat{F}, \\hat{P}_F)$ whenever an action is expected in test data. We also implemented an LLM response evaluation prompt which takes in two response messages (m1, m2) and returns True if semantics of m\u2081 and m2 are the same else False. An manual audit based evaluation is also performed to validate the efficacy of our LLM response evaluation prompt (LLM evaluation prompt detailed in Appendix A.8). Both, the generated function call and response message semantics, should be evaluated as correct with the ground truth to mark the complete generated output as valid. We calculate the accuracy as the number of test cases where MARS's complete generated output is valid. For each component we also calculate and compare the latency and cost of response generation as it is a real-time chat system."}, {"title": "4 Experimental Setup", "content": "Dataset: For our experiments, we curated two conversational orchestration test datasets, Digital Restaurant Service Platform (DRSP-Conv) and Retail-Conv, each with 221 and 350 multi-turn conversations in the restaurant services and retail services domain respectively. These conversations are a mix of Out-Of-Domain (OOD), Action and Info queries with multi-turn interactions with both, User and Deterministic Tasks (an example conversation flow in the dataset is shown in figure 1 for DRSP-Conv). The dataset covers usecases along with their natural language Task Execution Procedure (TEP) steps, supported functions (deterministic tasks and utility tools \u2074) and sub-task agents. Each test conversation has multiple Assistant (Agent) messages (replying to the user, loading"}, {"title": "5 Experiments & Results", "content": "In this section we detail the various experiments to evaluate our proposed solution, MARCO, on task specific performance, operational performance (latency, run-time cost), scalability and ablations."}, {"title": "Conclusion", "content": "We presented MARCO, a multi-agent real-time chat orchestration framework for automating tasks using large language models (LLMs) addressing key challenges in utilizing LLMs for complex, multi-step task execution with high accuracy and low latency including reflection guardrail prompts for steering LLM behaviour and recover from errors leading to +30% accuracy improvement. We demonstrated MARCO's superior performance with up to +11.77% and +4.36% improved accuracy against single agent baseline for two datasets, DRSP-Conv and Retail-Conv, and improved latency by 44.91% and 33.71% cost reduction. The modular and generic design of MARCO allows it to be adapted for automating tasks across various domains wherever complex tasks need to be executed through multi-turn interactions using LLM-powered agents."}, {"title": "A Appendix", "content": "A.1 Discussion\nWhile as part of this work our experiments are focused towards Digital Restaurant Service and Retail task automations, the design for MARCO is generic LLM Agents based framework and can be adapted to any domain where the system is required to follow standard task execution steps to solve for a usecase while using set of available tools and interacting with an end user. Also, the guardrails and evaluation methods are generic for such a framework. As Intent Classifier, RAG and MARS are independent modules, we execute them in parallel to reduce the latency of our real-time chat system. The output from MARS or RAG is picked according to IC's classification."}, {"title": "A.2 Hyper-parameters:", "content": "For all experiments, unless specified otherwise, we used the underlying LLM as claude-3-sonnet with temperature=0, max_output_tokens=1000 and Top-P, Top-K values as defaults. We use LLM APIs provided by Amazon Bedrock dated July 1, 2024 for output generation. The maximum number of retires on any guardrail failure was set to 2, and if the issue still persisted, a constant \"Facing Technical Issue\" response was sent back. We ran each experiment five times and published the average and standard deviation for the results. We publish the cost calculation numbers with AWS Bedrock pricing \u2075 in this work."}, {"title": "A.3 Reflection Guardrails Ablation", "content": "Table 3 performs an ablation of each of the reflection prompts discussed in section 3.2.3. The results show that each reflection prompt contributes to the performance enhancement of MARCO without which the performance drops significantly on DRSP-Conv and Retail-Conv datasets. The latency also does not increase much due to re-trying with reflection with an average increase of only 1.54 and 1.24 seconds respectively when adding all guardrails to the system in claude-3-sonnet."}, {"title": "A.4 Effects of Temperature, Input & Output Token Lengths:", "content": "Effects of Temperature: We vary the temperature hyper-parameter at an increment of +0.2 from 0 to 1 and compare the performance accuracy of MARS using claude-3-sonnet and claude-v2.1. The results suggest that temperature=0 performs the best for MARCO.\nEffects of Input and Output Tokens on Latency: In figure 6 we plot the latency of MARS using claude-3-sonnet with respect to input tokens (x-axis). We further color code each instance on the plot based on the number of output tokens generated within a given range. The results show a correlation between the growing number of input tokens leading to an increase in the latency while also having large number of output tokens for similar input token length leading to further increase in the latency."}, {"title": "A.5 Cost Analysis", "content": "To calculate the cost of various LLM version we assume that the task automation system has on average:"}, {"title": "A.6 Intent Classifier prompting techniques", "content": "In this section we explain the various prompting techniques that we employed to improve the performance of Intent Classifier. The primary objective of the Intent Classifier is to classify between I=Info, I=Action intents, while also adeptly managing casual conversational contexts such as greetings, out-of-domain inquiries, and potential jailbreak attempts. Major challenges that we have addressed for IC are:"}, {"title": "A.7 LLM Agents Input Prompts & Output Formatting", "content": "In this section we go deeper into the details of how we prompt our Task-Agents (LLM Agents in MARS) to get desired reasoning and output.\nLLM Input Prompt: Below mention is a sample LLM Agent's prompt using which we intialise all our Task-Agents where details like agent_name, agent_purpose, agent_task_execution_steps, sub_task_agents, tools, history, user_message are dynamic variables replaced with the actual values on the fly using Agent's internal state. We employ techniques like Chain-of-thought reasoning, guiding LLM to complete the prefix string ([Agent]<thinking>) so that it steers in the required direction, output formatting instructions and XML tags to define segments in the prompts carefully.\n{{agent_name}}, {{agent_purpose}}\n<TEP_STEPS >\n{{agent_task_execution_steps}}\n</TEP_STEPS >\nSub-Tasks:\n<sub_tasks >\n{{sub_task_agents}}\n</sub_tasks >\nTools:\n<tools>\n{{agent_tools}}\n</tools>\nPlace to Add important instructions:\n<instructions >\n{{instructions}}\n</instructions >\nPlaceholder for chat history\n<history> {{history}} </history >\nLLM Output: We prompt the LLM to generate the following output format, which is then parsed to get relevant actions:\n<response >{\n\"content\": \"The message to be conveyed back to the user.\",\n\"function_call\": {\n\"name\": \"function name\",\n\"arguments\": \"{\\\"Arg1\\\": \\\" Arg1_value\\\"}\"\n}}\n</response >"}, {"title": "A.8 LLM Evaluation Prompt", "content": "In this section we detail the LLM based semantic similarity matching LLM prompt for evaluating MARS Agents' responses. While verifying the generated function call and corresponding parameters is easy as they can be matched after parsing from the string with the ground truth deterministically, it can be challenging to match whether the LLM generated response back to the Actor/User is same as the intended string in ground truth test set. Traditionally a manual audit is conducted to look at the generated string and ground truth string to identify if both have the same semantics or meaning. This can be a time taking and costly task depending on the size of your test dataset. We employ an LLM based task evaluation strategy where we prompt claude-instant-v1 to evaluate if two responses (sentence1 and sentence2) are semantically same or not. We conducted a manual audit as well and found a Cohen's Kappa score of 0.65 (96.66% agreement) between auditors and LLM generated evaluations establishing the effectiveness of our approach."}, {"title": "A.9 Digital Restaurant Service Platform Conversation Dataset", "content": "Each usecase has their own set of task execution procedure (TEP) steps in natural language, deterministic multi-step execution task and utility queries. Deterministic tasks (functions) are defined as JSONSchemas to the LLM prompt as input. A sample of TEP steps and a function JSONSchema is mentioned below:\nSample Function JSONSchema for Restaurant Menu Update:"}]}