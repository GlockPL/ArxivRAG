{"title": "FedHPD: Heterogeneous Federated Reinforcement Learning via Policy Distillation", "authors": ["Wenzheng Jiang", "Ji Wang", "Xiongtao Zhang", "Weidong Bao", "Cheston Tan", "Flint Xiaofeng Fan"], "abstract": "Federated Reinforcement Learning (FedRL) improves sample efficiency while preserving privacy; however, most existing studies assume homogeneous agents, limiting its applicability in real-world scenarios. This paper investigates FedRL in black-box settings with heterogeneous agents, where each agent employs distinct policy networks and training configurations without disclosing their internal details. Knowledge Distillation (KD) is a promising method for facilitating knowledge sharing among heterogeneous models, but it faces challenges related to the scarcity of public datasets and limitations in knowledge representation when applied to FedRL. To address these challenges, we propose Federated Heterogeneous Policy Distillation (FedHPD), which solves the problem of heterogeneous FedRL by utilizing action probability distributions as a medium for knowledge sharing. We provide a theoretical analysis of FedHPD's convergence under standard assumptions. Extensive experiments corroborate that FedHPD shows significant improvements across various reinforcement learning benchmark tasks, further validating our theoretical findings. Moreover, additional experiments demonstrate that FedHPD operates effectively without the need for an elaborate selection of public datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, Reinforcement Learning (RL) has achieved outstanding performance in various tasks [4, 13, 20, 45]. However, it still faces low sample efficiency [48] and privacy leakage [30]. Federated Learning (FL) [28] enables clients to collaboratively improve training efficiency while preserving data privacy, and mitigates instabilities associated with centralized model updates. The collaborative privacy-preserving characteristics of FL, combined with the need for sample-efficient and private training in RL, have led to the emergence of Federated Reinforcement Learning (FedRL). This fusion offers a promising approach for intelligent decision-making in distributed privacy-sensitive environments [6]. Recognizing its potential, the research community has extensively explored FedRL under various settings, such as Byzantine fault-tolerance [8, 12], environment heterogeneity [18, 27, 44, 50], and decentralization [19, 32]. Despite its promise, most FedRL frameworks [8, 10, 18, 50] operate under the assumption of agent homogeneity (i.e., identical policy networks and training configurations), which significantly limits FedRL's applicability in real-world scenarios. This limitation is particularly acute in resource-constrained environments, such as in edge environments, where agents have limited power and need to adapt network structures and training strategies based on their operational conditions to achieve effective training [47]. In addition, existing FedRL frameworks typically operate under a white-box paradigm, where models are openly shared among participants. However, in many business-oriented domains, such as healthcare and finance, the disclosure of internal details to server is often prohibited due to commercial sensitivities, intellectual property concerns, and regulatory compliance. These practical constraints lead us to formulate a more challenging question:\nHow can we effectively perform FedRL when each agent employs a unique model that remains a black box to all other participants?\nCompared with previous FedRL frameworks, the challenges faced by agent heterogeneity\u00b9 in black-box settings discussed in this research can be revealed as follows:\n\u2022 Knowledge Representation Difference. The lack of disclosure of agents' internal details results in a distributed black-box optimization problem [5]. Traditional federated aggregation methods (such as FedAvg [28]) are not applicable due to the agent heterogeneity. Therefore, novel approaches are required to convert and align different models to facilitate effective knowledge sharing. Knowledge Distillation (KD) [16] is regarded as an antidote to model heterogeneity [23, 24, 52]. However, existing FedRL methods related to KD are often accompanied by several limitations, such as model-based [35] and partial network sharing [26], which hinders their applications in our settings.\n\u2022 Inconsistent Learning Progress. The different configurations of agents lead to increased complexity in federated optimization.\n\u00b9In this paper, \"agent heterogeneity\" indicates that policy networks and training configurations of agents in FedRL are heterogeneous [9]."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Markov Decision Process", "content": "Almost all RL problems can be modeled as Markov Decision Processes (MDPs) [40]: \\(M = \\{S, A, P, R, \\gamma, p\\}\\), where \\(S\\) and \\(A\\) represent the state and action spaces, \\(P (s' | s, a)\\) denotes the transition probability from state \\(s\\) to state \\(s'\\) when taking action \\(a \\in A\\). \\(R (s, a): S \\times A \\rightarrow [0, R_{\\text{max}}]\\) is the reward function for taking action \\(a\\) in state \\(s\\), where \\(R_{\\text{max}} > 0\\). \\(\\gamma \\in (0,1)\\) and \\(p\\) respectively denote the discount factor and the initial state distribution.\nAn agent's actions are determined by a policy \\(\\pi\\), where \\(\\pi (a|s)\\) denotes the probability for state-action pair \\((s; a)\\). A trajectory \\(\\tau \\triangleq \\{s_0, a_0, s_1, a_1, ..., s_{H-1}, a_{H-1}, s_H\\}\\) is collected through the interaction with the environment, where \\(H\\) is the task horizon and \\(s_0 \\sim p\\). We define \\(R (\\tau) = \\sum_{t=0}^{H-1} \\gamma^t R (s_t, a_t)\\) as the cumulative reward of this trajectory."}, {"title": "2.2 Policy Gradient", "content": "Policy Gradient methods [36, 37, 41, 49] have demonstrated remarkable success in the model-free RL, particularly showcasing effectiveness in high-dimensional problems and offering the flexibility of stochasticity. In PG, we denote the policy parameterized by \\(\\theta\\) as \\(\\pi_{\\theta}\\), where \\(\\theta\\) represents the policy parameters, such as the weights of a neural network. The performance of a policy \\(\\pi_{\\theta}\\) can be quantified by the expected return \\(J(\\theta)\\), defined as: \\(J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} [R(\\tau)]\\).\nTo optimize the policy, we compute the gradient of \\(J(\\theta)\\) with respect to \\(\\theta\\):\n\\[\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^H \\nabla_{\\theta} \\log \\pi_{\\theta} (a_t|s_t) R(\\tau) \\right]. \\quad (1)\\]\nThen the policy parameters \\(\\theta\\) can be updated via gradient ascent. REINFORCE [40] is a fundamental on-policy RL algorithm that does not require trajectory sampling. The update of the REINFORCE is: \\(\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)\\), where \\(\\alpha\\) is the learning rate."}, {"title": "2.3 Knowledge Distillation", "content": "Knowledge Distillation (KD) [16] is a model compression technique that achieves model learning by minimizing the difference between the outputs of the student model and the teacher model. The Kullback-Leibler divergence (KL divergence), a common measure of the difference between two probability distributions, is used in KD to quantify the difference between the teacher and student model outputs' probability distributions:\n\\[\\min_{\\theta_s} \\mathbb{E}_{z \\sim D_p} [D_{KL}(p(z; \\theta_s)||p(z; \\theta_t))], \\quad (2)\\]\nwhere \\(D_p\\) is the proxy dataset, \\(p(z;\\theta_t)\\) and \\(p(z; \\theta_s)\\) denote the output of the teacher model and the student model, \\(\\theta_t\\) and \\(\\theta_s\\) are the parameters of the student and teacher models, respectively.\nKD has been extended to the domain of FL to address the model heterogeneity, by treating each client model as the teacher, whose information is aggregated into the student (global) model to improve its generalization performance:\n\\[\\min_{\\theta_s} \\mathbb{E}_{z \\sim D_p} \\left[D_{KL} \\left(p(z; \\theta_s)|| \\frac{1}{K} \\sum_{k=1}^K p(z; \\theta_k)\\right)\\right], \\quad (3)\\]\nwhere \\(\\theta_k\\) represents the parameters of the \\(k\\)-th client model."}, {"title": "3 FEDERATED HETEROGENEOUS POLICY GRADIENT", "content": ""}, {"title": "3.1 Problem Formulation", "content": "Assuming the system contains a set of \\(K\\) heterogeneous and mutually black-box agents, where agent \\(k\\) interacts in a separate copy of the MDP \\(M\\) according to policy \\(\\pi_k\\), generating their own local private data \\(D_k = \\{(s, a, s', r)_i\\}_{i \\sim D_k}\\). The policy \\(\\pi_k(a|\\phi_k(s; \\theta_k))\\) is composed of a nonlinear function \\(\\phi_k(s; \\theta_k)\\) that predicts the probability of taking action \\(a\\) given the state \\(s\\). The nonlinear function \\(\\Phi_k (s; \\theta_k)\\) is parameterized by a set of parameters \\(\\theta_k\\) and is learned using private experience data \\(D_k\\).\nThe nonlinear function is generally approximated using neural networks, and the agent heterogeneity is often manifested in the differences in neural network architectures and training configurations [9]. The key to addressing agent heterogeneity is to find effective methods to enable information sharing among agents. We aim to develop a new FedRL framework that allows heterogeneous agents to share their knowledge from local policies in a black-box manner, without being constrained by the server-side MDP process. The objective functions of the federated heterogeneous policy gradient can be formulated as follows:\nAssuming that all agents are trained synchronously, given the number of training rounds \\(T\\), let \\(\\Psi\\) and \\(\\psi_k\\) represent the training performance of agent \\(k\\) under the FedRL framework and its performance when trained independently, respectively. We have:\nUnder the same training rounds,\nfor system:\n\\[\\sum_{k=1}^K \\Psi = \\sum_{k=1}^K \\psi_k; \\quad (4)\\]\nfor agent \\(k\\), \\(k \\in \\{1, 2, ..., K\\}\\):\n\\[\\Psi \\ge \\psi_k. \\quad (5)\\]"}, {"title": "3.2 Technical Challenges", "content": "KD can effectively address the model heterogeneity in FL [23, 24, 52]. Inspired by this, we explore how to introduce it into FedRL to deal with the agent heterogeneity. Although KD has been previously applied in FedRL [26, 35], the methods used are not well-suited to address the heterogeneous FedRL problem. In this section, we discuss some unique challenges posed by introducing KD to address the problem:\n\u2022 Public Dataset Scarcity. KD in FL achieves knowledge sharing among clients via public dataset. Most FL works foucs on classification tasks in supervised learning, where training and testing datasets are available. However, in RL, training data is obtained through interaction with the environment. Therefore, acquiring public dataset is the critical issue. To tackle this issue, we generate an offline public state set as shared input data using a virtual agent. Consistent with the core intent of GANs [15], we aim to generate synthetic data using this virtual agent.\n\u2022 Knowledge Representation Limitation. In RL, especially when using policy gradient algorithms, we need to solve tasks in continuous action spaces. Traditional federated KD methods typically use logits as the form of knowledge representation, which is common in classification tasks but cannot be used in continuous action spaces tasks [26]. Thus, we use the action probability distribution output by the policy network as the form of knowledge representation. However, this also means that we need to consider the impact of distributional differences across different tasks, such as the discrete distribution derived from the softmax output and Gaussian distribution."}, {"title": "3.3 FedHPD", "content": "In practical applications, agents are typically trained in simulated environments before being deployed in real-world. Consequently, we set up a simulated environment on the server and train a virtual agent within it. Although optimal performance is not required, reasonable parameterization is needed to ensure stable operation in the environment. For example, in autonomous driving tasks, the simulated environment can be based on an existing simulation platform (such as CARLA [7]), where the virtual agent undergoes preliminary training to learn basic driving strategies. Next, we conduct multiple tests with different initial state inputs to simulate the diverse scenarios that the agent may encounter. In each test, the virtual agent generates a series of states, from which a subset of states is randomly selected to form the public state set \\(S_p\\).\nNotably, the synthetic state set \\(S_p\\) generated by the virtual agent is independent of local training and serves solely for KD. Of course, for tasks like autonomous driving, robot control, and games, there is an abundance of available experience data that can directly serve as the public state set \\(S_p\\). In conclusion, obtaining the public state set is reasonable and convenient in real-world. For communication considerations, \\(S_p\\) is distributed to each local agent before the start of training, so that the communication only involves the upload and distribution of the knowledge.\nTo handle tasks both in discrete and continuous action spaces, we use action probability distribution as a bridge for communication. Agent \\(k\\) generates its private experience data \\(D_k\\) by interacting with the environment for local training and REINFORCE is deployed locally in FedHPD. During the collaborative training process, knowledge (probability distribution) is distilled from the policies of heterogeneous agents using the public state set \\(S_p\\). Subsequently, agents digest knowledge by comparing the distributions output by themselves with the global average distributions (global consensus). In other words, local agents use public state set \\(S_p\\) and private data \\(D_k\\) to train and improve their policy \\(\\theta_k\\), surpassing individual efforts. Our framework is illustrated in Fig.1.\nThe pseudocode for FedHPD is described in Algorithm 1, which consists of two training stages:\n(1) Local Training (line 1-4): In each training round, all local agents must interact with the environment, using the generated data \\(D_k\\) to update their own policy parameters \\(\\theta_k\\), resulting in \\(\\theta_k^{i+1}\\). Note that no collaboration is required at this stage.\n(2) Collaborative Training (line 5-13): In the collaborative training phase (conducted every \\(d\\) training rounds), agents share their knowledge based on the output distributions under the public state set \\(S_p\\). The detailed steps are as follows:\n\u2022 Get Probability Distributions (line 5-7): Each local agent obtains the action distributions \\(P_k^{i+1}\\) under state set \\(S_p\\) based on its own parameterized policy \\(\\pi_{\\theta_k^{i+1}}\\), and uploads \\(P_k^{i+1}\\) to server;\n\u2022 Knowledge Aggregation (line 8-10): Server aggregates the uploaded probability distributions to obtain global consensus \\(P^{i+1}\\) and sends \\(P^{i+1}\\) to local agents;\n\u2022 Knowledge Digestion (line 11-13): Local agents calculate the KL divergence between their predicted distributions \\(P_k^{i+1}\\) and global distributions \\(P^{i+1}\\) to update their policy parameters \\(\\theta_k^{i+1}\\).\nIt is noteworthy that, to reduce communication overhead, we set a distillation interval \\(d\\), where agents engage in collaborative training after every \\(d\\) rounds of local training. A larger distillation interval implies fewer federated rounds are required. In summary, FedHPD achieves collaborative training of heterogeneous agents through policy distillation, enabling effective policy updates without additional sampling."}, {"title": "4 THEORETICAL ANALYSIS", "content": "In this section, we delve into the convergence of policy optimization in the context of KD. Specifically, we focus on two questions:\n\u2022 Does the introduction of KD affect the convergence of the original policy gradient algorithm? (Theorem 1)\n\u2022 Can FedHPD achieve fast convergence ? (Corollary 1)\nLet \\(J(\\pi_{\\theta_k})\\) be the objective function for the policy optimization of the \\(k\\)-th agent. Given the introduction of KD, the objective function can be reformulated as:\n\\[J' (\\theta_k) = J(\\theta_k) - \\lambda D_{KL}(\\pi_{\\theta_k} (a|S_p)||\\pi_{\\text{global}}(a|S_p)), \\quad (6)\\]\nwhere \\(\\lambda > 0\\) is the regularization coefficient. \\(\\pi_{\\theta_k} (a|S_p)\\) is the action probability distributions output by the policy \\(\\pi_{\\theta_k}\\) over the state set \\(S_p\\) and global action probability distributions \\(\\pi_{\\text{global}}(a|S_p) = \\frac{1}{K} \\sum_{k=1}^K \\pi_{\\theta_k} (a/S_p)\\).\nEq. (6) redefines the objective function, and by introducing the KL divergence term, we can reduce the difference between the local policies through multiple updates. Interestingly, the objective function of \\(J' (\\theta_k)\\) is similar to PPO with KL penalty [37]. The difference is that in our research, we aim to integrate global consensus through the KL divergence term.\nHere, we provide some assumptions required for theoretical analysis, all of which are common in the literature.\nAssumption 1 (Policy Parameterization Regularity). Let \\(\\pi_{\\theta}(a|s)\\) be the policy of an agent at state \\(s\\). For every \\(s, a \\in S \\times A\\), there exists constants \\(G, M > 0\\) such that the log-density of the policy function satisfies:\n\\[||\\nabla_{\\theta} \\log \\pi_{\\theta}(a|s)|| \\le G, \\quad ||\\nabla^2_{\\theta} \\log \\pi_{\\theta}(a|s)|| \\le M. \\quad (7)\\]\nAssumption 1 is aimed to ensure that the changes in the policy within the parameter space are controllable, providing a foundation for the smoothness assumption of the objective function [1, 33].\nProposition 1. Under Assumption 1, J(\\theta) is L-smooth. Therefore, for all \\(\\theta, \\theta' \\in \\mathbb{R}^d\\), there exist a constant \\(L_J > 0\\) satisfies:\n\\[||\\nabla_{\\theta}J(\\theta) - \\nabla_{\\theta}J(\\theta')|| \\le L_J||\\theta - \\theta' ||. \\quad (8)\\]\nProposition 1 is important for demonstrating convergence and its proof can be found in Xu et al. [46]. Additionally, Assumption 1 and Proposition 1 are widly used in policy gradient methods [8, 11, 31, 49].\nAssumption 2 (Softmax Function Smoothness). Let \\(\\pi_{\\theta}(a_i|S_p)\\) denote the probability distribution obtained from the softmax function:\n\\[\\pi_{\\theta}(a_i|S_p) = \\frac{e^{f(a_i, S_p)}}{\\sum_{j}e^{f(a_j,S_p)}}\\]\nAssume that the softmax function is L-smooth with respect to \\(\\theta\\). Specifically, this means:\n(i) The first-order derivatives of \\(\\pi_{\\theta}(a_i|S_p)\\) with respect to \\(\\theta\\) are continuous:\n\\[\\nabla_{\\theta}\\pi_{\\theta}(a_i|S_p) = \\pi_{\\theta}(a_i|S_p) (e_i - \\pi_{\\theta}(\\cdot|S_p)),\\]\nwhere \\(e_i\\) is the unit vector corresponding to \\(a_i\\).\n(ii) The second-order derivatives of \\(\\pi_{\\theta}(a_i|S_p)\\) with respect to \\(\\theta\\) are continuous:\n\\[\\nabla^2_{\\theta} \\pi_{\\theta}(a_i|S_p) = \\text{diag}(\\pi_{\\theta}(\\cdot|S_p)) - \\pi_{\\theta}(\\cdot|S_p)\\pi_{\\theta}(\\cdot|S_p)^T.\\]\nAssumption 3 (Gaussian Distribution Smoothness). Let \\(\\pi_{\\theta}(a|S_p)\\) denote the probability density function of a Gaussian distribution:\n\\[\\pi_{\\theta}(a|S_p) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(a-\\mu)^2}{2\\sigma^2}}\\]\nwhere \\(\\mu = \\mu(\\theta)\\) and \\(\\sigma^2 = \\sigma^2(\\theta)\\).\nAssume that \\(\\mu(\\theta)\\) and \\(\\sigma^2(\\theta)\\) are L-smooth functions of \\(\\theta\\). Specifically, this means:\n(i) The first-order derivatives of \\(\\mu(\\theta)\\) and \\(\\sigma^2(\\theta)\\) with respect to \\(\\theta\\) are continuous: \\(\\nabla_{\\theta}\\mu(\\theta)\\) and \\(\\nabla_{\\theta}\\sigma^2(\\theta)\\) are continuous.\n(ii) The second-order derivatives of \\(\\mu(\\theta)\\) and \\(\\sigma^2(\\theta)\\) with respect to \\(\\theta\\) are continuous: \\(\\nabla^2_{\\theta}\\mu(\\theta)\\) and \\(\\nabla^2_{\\theta}\\sigma^2(\\theta)\\) are continuous.\nAssumptions 2 and 3 establish the foundation for the L-smoothness of the KL divergence term, and it is reasonable to assume smoothness for distributions in machine learning [2, 25, 38, 40].\nLemma 1 (KL Divergence Smoothness). Under Assumptions 2, 3, the KL divergence term \\(D_{KL}(\\pi_{\\theta_k} (a|S_p)||\\pi_{\\text{global}}(a|S_p))\\) is L-smooth. Specifically, there exists a constant \\(L_{KL} > 0\\) such that for any two parameter sets \\(\\theta_k\\) and \\(\\theta\\), the following inequality holds:\n\\[||\\nabla_{\\theta_k}D_{KL}(\\pi_{\\theta_k}||\\pi_{\\text{global}}) - \\nabla_{\\theta'} D_{KL}(\\pi_{\\theta'}||\\pi_{\\text{global}})|| \\le L_{KL} ||\\theta_k - \\theta||, \\quad (9)\\]\nwhere \\(\\pi_{\\theta_k}\\) and \\(\\pi_{\\theta'}\\) are probability distributions parameterized by \\(\\theta_k\\) and \\(\\theta\\) respectively.\nLemma 1 is crucial for the convergence of the FedHPD. It is related to the L-smoothness of the overall objective function \\(J' (\\theta_k)\\) after introducing KD. Due to space constraints, the proof of Lemma 1 is deferred to Appendix A.1."}, {"title": "THEOREM 1 (CONVERGENCE OF REINFORCE WITH KNOWLEDGE DISTILLATION).", "content": "Under Assumptions 1, 2, 3, if we choose \\(L = L_J - \\lambda L_{KL} > 0\\) and learning rates \\(\\alpha^i\\) satisfy Robbins-Monro conditions, the objective function \\(J' (\\theta_k)\\) can converge to a stationary point as the number of iterations increases.\nThe proof of Theorem 1 is deferred to Appendix A.2. Theorem 1 analyzes the convergence conditions for REINFORCE with KD, but does not consider the effect of distillation interval. Building upon Theorem 1, we incorporate the distillation interval \\(d\\), leading to the analysis of FedHPD:\nCorollary 1 (Fast Convergence of FedHPD). In the setting of Theorem 1, if we choose \\(\\lambda = 1\\), FedHPD can achieve fast convergence by reducing variance, leading to:\n\\[\\mathcal{N}_{\\text{REINFORCEwithKD}} \\le \\mathcal{N}_{\\text{FedHPD}} \\le \\mathcal{N}_{\\text{REINFORCE}}, \\quad (10)\\]\nwhere \\(\\mathcal{N}\\) represents the sample size, and REINFORCE with KD refers to FedHPD without distillation intervals.\nProof. According to the description of FedHPD in Section 3.3, we introduce knowledge distillation with a period of \\(d\\).\nThe objective function of FedHPD can be formulated as:\n\\[J'' (\\theta_k) = \\begin{cases}J(\\theta_k), \\quad i \\% d \\neq 0. \\\\J' (\\theta_k), \\quad i \\% d = 0.\\end{cases} \\quad (11)\\]\nIn the setting of Theorem 1, \\(J(\\theta_k)\\) will progressively approach the global optimal solution, thus ensuring convergence [40, 43]. Theorem 1 demonstrates that \\(J' (\\theta_k)\\) is L-smooth and converges under the learning rates which satisfy Eq. (50).\nTherefore, the optimization process of \\(J'' (\\theta_k)\\) alternates between two sub-processes that have been proven to converge. As long as the learning rates \\(\\alpha^i\\) satisfy Eq. (50), the entire optimization process will still converge.\nNext, we discuss the fast convergence of FedHPD. For the vanilla REINFORCE, the sampled gradient for a single trajectory is:\n\\[\\hat{g}(\\theta) = \\sum_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t s_t)R(\\tau).\\]\nThe variance of the gradient can be formulated as:\n\\[\\text{Var}[\\nabla J(\\theta)] = \\mathbb{E} \\left[\\left(\\sum_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)R(\\tau)\\right)^2\\right] - \\mathbb{E} \\left[ \\sum_t \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t) R(\\tau) \\right]^2. \\quad (13)\\]\nTo prevent the KL divergence term from altering the overall gradient direction while reducing gradient variance, \\(\\lambda\\) should be a relatively small positive number. The variance of the new gradient can be formulated as:\n\\[\\text{Var}[\\nabla_{\\theta}J' (\\theta)] = \\text{Var} [\\nabla_{\\theta}J(\\theta)] + \\lambda^2 \\text{Var}[\\nabla_{\\theta}D_{KL}(\\pi_{\\theta}||\\pi_{\\text{global}})] - 2 \\lambda \\text{Cov}(\\nabla_{\\theta}J(\\theta), \\nabla_{\\theta}D_{KL}(\\pi_{\\theta}||\\pi_{\\text{global}})). \\quad (14)\\]\nThe selection on the value of \\(\\lambda\\) is deferred to Appendix A.3. In FedHPD, substituting \\(\\lambda = 1\\), we have:\n\\[\\text{Var}[\\nabla_{\\theta}J' (\\theta)] = \\text{Var} [\\nabla_{\\theta}J(\\theta)] + \\text{Var}[\\nabla_{\\theta}D_{KL}(\\pi_{\\theta}||\\pi_{\\text{global}})] - 2 \\text{Cov}(\\nabla_{\\theta}J(\\theta), \\nabla_{\\theta}D_{KL}(\\pi_{\\theta}||\\pi_{\\text{global}})). \\quad (15)\\]\nTherefore, the variance between the gradients of the two objective functions satisfies:\n\\[\\text{Var}[\\nabla_{\\theta}J' (\\theta)] < \\text{Var}[\\nabla J(\\theta)],\\]\nif \\(\\text{Var}[\\nabla_{\\theta}D_{KL}(\\pi_{\\theta}||\\pi_{\\text{global}})] < 2 \\text{Cov}(\\nabla_{\\theta}J(\\theta), \\nabla_{\\theta}D_{KL}(\\pi_{\\theta}||\\pi_{\\text{global}}))\\).\nThe above condition can be reformulated as:\n\\[\\cos(\\theta_{J&KL}) > \\frac{1}{2} \\frac{||\\nabla_{\\theta}D_{KL} (\\pi_{\\theta}|| \\pi_{\\text{global}})||}{||\\nabla_{\\theta}J(\\theta)||}, \\quad (17)\\]\nwhere \\(\\theta_{J&KL}\\) denotes the angle between \\(\\nabla_{\\theta}D_{KL}(\\pi_{\\theta}||\\pi_{\\text{global}})\\) and \\(\\nabla_{\\theta}J(\\theta)\\).\nEq. (17) indicates that the condition for variance reduction is that enhancing rewards and reducing the discrepancy with the global consensus must align to some extent. From the standpoint of this, a lower value of \\(\\frac{||\\nabla_{\\theta}D_{KL} (\\pi_{\\theta}|| \\pi_{\\text{global}})||}{||\\nabla_{\\theta}J(\\theta)||} \\) makes variance reduction easier to achieve. Actually, \\(||\\nabla_{\\theta}J(\\theta)||\\) typically tends to lower values to allow effective learning while maintaining stability [36, 37, 41, 42], especially in the early stages of the training. A smaller distillation interval results in smaller differences between policies of heterogeneous agents, thus making Eq. (17) easier to satisfy.\nCombined with Eq. (16), for FedHPD, we have:\n\\[\\text{Var}[\\nabla J' (\\theta)] \\le \\text{Var}[\\nabla_{\\theta}J'' (\\theta)] \\le \\text{Var}[\\nabla J(\\theta)], \\quad (18)\\]\nwhere the equality is obtained for \\(d = 1\\) and \\(d = \\infty\\).\nThe reduction in variance implies that, for a given precision \\(\\epsilon\\), fewer samples are required to achieve the same level of accuracy. According to concentration inequalities (e.g., Chebyshev's inequality):\n\\[P (|g - \\mathbb{E}[g]| \\ge \\epsilon) \\le \\frac{\\text{Var} [g]}{N \\epsilon^2}. \\quad (19)\\]\nTo ensure that this probability is below a threshold \\(\\delta\\), we require:\n\\[N > \\frac{\\text{Var} [g]}{\\delta \\epsilon^2}. \\quad (20)\\]\nCombined with Eq. (18), the proof can be completed."}, {"title": "REMARK.", "content": "Existing FedRL frameworks with theoretical convergence focus on homogeneous agents. We provide a preliminary theoretical analysis of the convergence conditions for heterogeneous agents in FedHPD and the fast convergence of FedHPD, offering guidance and reference for experimental validation."}, {"title": "5 EXPERIMENTS", "content": "We conduct experiments to validate the effectiveness of FedHPD using three representative tasks from OpenAI Gym [3]: Cartpole and LunarLander for discrete action spaces, and InvertedPendulum for continuous action spaces. Our experimental design aligns with the objective functions presented in Section 3.1, focusing on two critical aspects of improvement:\n\u2022 System-level Performance: We assess whether FedHPD achieves higher average rewards across all agents compared with independent training.\n\u2022 Individual Agent Performance: We evaluate whether each agent in the federated system demonstrates improved performance relative to its independently trained counterpart."}, {"title": "5.1 Experimental Settings", "content": "In our experiments, we set up 10 heterogeneous agents, each trained locally using the vanilla on-policy REINFORCE algorithm. Each agent differs in policy networks (model architecture and activation functions) and training configurations (learning rates). Tables 3, 4, 5, and 6 in Appendix B summarizes the variations among the agents for the three tasks, respectively.\nTo examine the effect of the distillation interval \\(d\\) on FedHPD during the collaborative training phase, we run FedHPD using various \\(d\\) values (\\(d = 5, 10, 20\\)) in validating the performance improvement. In all experimental results, we report the average rewards under different random seeds. Each experiment is independently run with five different random seeds (20, 25, 30, 35, 40)\u00b2."}, {"title": "5.2 System Performance Improvement", "content": "First, we evaluate the system-level performance improvement of FedHPD, as described in Eq. (4). For the entire system, we focus on the average performance of all agents, comparing the system performance under independent local training (NoFed) and FedHPD with different distillation intervals (\\(d = 5, 10, 20\\)).\nThe average rewards of the system during the entire training process are presented in Table 2. It is evident that, compared to NoFed, FedHPD achieves superior results across all tasks. The system performance comparison of the three tasks is shown in Fig. 2. For the Cartpole task, the system typically needs around 1200 local training rounds to achieve a reward above 300 under NoFed, whereas with FedHPD, it requires only about 700 training rounds on average to reach the same reward. For the InvertedPendulum task, with a limited number of training rounds, NoFed can only obtain a reward of approximately 400, but with FedHPD (regardless of the \\(d\\) value), the system consistently achieves rewards exceeding 600.\nAdditionally, it can be observed that the value of \\(d\\) has less impact on overall system performance in LunarLander and InvertedPendulum compared to Cartpole. We hypothesize that in simpler tasks, where policies converge faster, the distillation frequency significantly affects the speed and stability of policy optimization. In contrast, in more complex tasks, the effect of adjusting the distillation interval on performance is overshadowed by the complexity of the task and the randomness of the environment, making it less pronounced."}, {"title": "5.3 Individual Performance Improvement", "content": "Next, we evaluate the improvement effect of FedHPD on individual agents, corresponding to Eq. (5). We aim for FedHPD to enhance not\n\u00b2Our code is available at https://github.com/WinzhengKong/FedHPD."}, {"title": "5.4 Comparison with Related Work", "content": "According to the comparison in Table 1, most of the existing methods are incapable of addressing heterogeneous FedRL. Both FedHQL and DPA-FedRL take into account the issue of agent heterogeneity, but since FedHQL is a value-based method specifically designed for Q-learning, we select the"}]}