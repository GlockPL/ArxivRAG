{"title": "LAN: Learning to Adapt Noise for Image Denoising", "authors": ["Changjin Kim", "Tae Hyun Kim", "Sungyong Baik"], "abstract": "Removing noise from images, a.k.a image denoising, can be a very challenging task since the type and amount of noise can greatly vary for each image due to many factors including a camera model and capturing environments. While there have been striking improvements in image denoising with the emergence of advanced deep learning architectures and real-world datasets, recent denoising networks struggle to maintain performance on images with noise that has not been seen during training. One typical approach to address the challenge would be to adapt a denoising network to new noise distribution. Instead, in this work, we shift our focus to adapting the input noise itself, rather than adapting a network. Thus, we keep a pretrained network frozen, and adapt an input noise to capture the fine-grained deviations. As such, we propose a new denoising algorithm, dubbed Learning-to-Adapt-Noise (LAN), where a learnable noise offset is directly added to a given noisy image to bring a given input noise closer to-wards the noise distribution a denoising network is trained to handle. Consequently, the proposed framework exhibits performance improvement on images with unseen noise, displaying the potential of the proposed research direction.", "sections": [{"title": "1. Introduction", "content": "Noise, an unwanted byproduct during image processing, can cause severe degradation not only in image quality but also in high-level computer vision tasks. As such, noise has been a target to eliminate in the field of image denoising. One of the main challenges in image denoising is how to distinguish a noise from an original source without know-ing the noise distribution a priori.\nTo tackle such a challenging problem, a myriad of learning-based models have been proposed to learn to re-move noise that follows known distributions. To do so, models are trained on datasets composed of pairs of clean"}, {"title": "2. Related works", "content": "Single image denoising. Image denoising is a crucial area of computer vision research. Early approaches include total variation-based denoising [41], sparse coding-based denoising [34], and self-similarity-based denoising meth-ods [7, 12]. With the success of deep learning in com-puter vision, many deep learning-based denoising methods have been developed, starting with methods that combine sparse coding and MLP [47]. DnCNN [54] focuses on noise by residual structure. FFDNet [55] uses downsam-pling and non-uniform noise level maps to achieve a faster and more efficient performance. RIDNet [5] has further im-proved performance especially on real-world noisy datasets, such as SIDD [1], by incorporating a reinforcement atten-\ntion module that utilizes global information such as feature attention and local skip connection bases. In recent years, vision transformers [13] have brought significant advance-ments in image restoration. Several works have employed vision transformers to bring futher improvements to denois-ing [10, 31, 46, 53].\nBlind image denoising. While denoising neural networks achieve high performance, they require pairs of clean and noisy images. However, obtaining clean images can be challenging. Several blind denoising approaches [6, 40, 43, 45] have emerged to tackle unseen noise image denois-ing without the need for clean images to training network. Early strategy in blind denoising is based on an internal im-age prior [43]. Recently, blind denoising approaches based on self-supervised learning have endeavored to achieve per-formance levels comparable to supervised learning. Under the assumption that noise in each image is independent and has a zero-mean distribution, Noise2Noise [30] takes dif-ferent images of a scene that act as input and target im-ages for training a network, respectively. Noise2Self [6] is another self-supervised learning method that masks the noisy image at regular intervals, using the remaining pix-els as an input image and masked pixels as a target image. On the other hand, recent works have focused on creating a input-target pair by subsampling from a single noisy image. Notably, Neighbor2Neighbor [17] is uses a random neigh-bor sub-sampler to generate input-target pairs for training a network. Zero-Shot Noise2Noise [35] is another self-supervised learning approach that generates input-target pairs by applying a filter that computes the mean of diag-onal pixels. By test-time adaptation (TTA) [25, 44, 49] us-ing these self-supervised learning approaches, a pretrained denoising network can be either finetuned to handle unseen noise [16, 28]. However, the misalignment between new unseen noise and noise expected by a pretrained denois-ing network may lead to suboptimal performance even after adaptation. In this work, we approach the problem from a different perspective: adapt a new noisy image itself to re-duce the misalignment itself.\nDomain adaptive image translation. Adaptation of an in-put noisy image to handle the misalignment shares motiva-tions with image translation for domain adaptation. Image translation aims to learn to translate images from a source domain to a target domain, through the aid of generative models [15, 19, 22, 26, 32, 33, 56]. While our proposed framework shares some similarities with these works from the perspective of domain adaptation, domain adaptation approaches either require a large amount of target-domain images or require a large amount of images from various do-mains in order to train a large generative model for image translation. On the other hand, we do not need a separate"}, {"title": "3. Proposed method", "content": "Problem formulation. In this work, we aim to tackle a sce-nario, where a new input image contains noise that follows a different distribution from the distribution a denoising net-work is trained with. Formally, a denoising network $f_{\\theta}$ with parameters $\\theta$ is pretrained with pairs of clean images $x^s$ and their noisy counterparts $y^s$ that contain noise $e^s$ assumed to follow a certain distribution $D_s$, constructed as follows:\n$y^s = x^s + e^s$, where $e^s \\sim D_s$.\n(1)\nIn particular, a denoising network $f_{\\theta^*}$ is trained to map noisy images $y$ with certain noise $e^s$ to their clean counterparts $x^s$ via minimizing the empirical loss as follows:\n$\\theta^* = \\arg \\min_{\\theta} E [||f_{\\theta} (y) - x^s||_2^2]$.\n(2)\nDuring test time, we expect that a new input noisy image $y^u$ contains an unknown clean image $x^u$ with unseen noise $e^u$ that follows a distribution $D_u$ that is different from the distribution seen during training (i.e., $D_s$):\n$y^u = x^u + e^u$, where $e^u \\sim D_u$.\n(3)\nUnder such formulation, challenges arise due to the domain misalignment between a noise distribution a denoising net-work is trained on and a new noise distribution encountered during the test phase.\nSelf-supervised learning. One approach to handle such misalignment would be to adapt a pretrained network to given noisy image. However, only a noisy image $y^u$ is available while a clean image $x$ is unavailable, making it difficult to train or adapt a denoising network $f_{\\theta}$ via Equa-tion 2.\nTo extract an underlying clean image from a noisy im-age, a few recent works have introduced self-supervised learning approaches [6, 18, 30, 35], given the following as-sumption: clean image pixels and noise pixels exhibit dif-ferent attributes. Namely, clean image pixels are highly cor-related within local regions, whereas the noise pixels are in-dependent. Upon the assumption, two independent noisy images, $y_1^u$ and $y_2^u$, are created out of the same scene from"}, {"title": "3.2. Learning to adapt noise (LAN)", "content": "In this work, we aim to utilize a pretrained denoising net-work $f_{\\theta}$ to remove new unseen noise $e^u$ from a given noisy image $y^u$. However, when a denoising network $f_{\\theta^*}$ is trained with noise $e^s \\sim D_s$ via Equation 2 while a new noisy image contains noise $e^u \\sim D_u$, there arises misalign-ment between new input and what a pretrained network ex-pects. Such misalignment may worsen as the difference be-tween seen noise distribution $D_s$ and unseen noise distribu-tion $D_u$ is larger. The misalignment may lead to suboptimal performance, when directly finetuning a pretrained denois-ing network $f_{\\theta}$ to minimize self-supervised loss function (Equation 4) with new noisy images.\nTo resolve the noise misalignment issues, we shift the attention to the noise itself at the input level. In particular, we formulate a new unseen noise as deviation from the seen noise distribution:\n$e^u = e^s + \\epsilon^{s \\rightarrow u}$,\n(5)\nwhere $\\epsilon^{s \\rightarrow u}$ represents how much $e^u$ deviates from an arbi-trary noise $e^s$ sampled from $D_s$. Thus, a new noisy image $y^u$ can be seen as follows:\n$y^u = x^u + e^u$\n(6)\n$= x^u + e^s + \\epsilon^{s \\rightarrow u}$.\n(7)\nGiven the formulation, we observe that we can mitigate the misalignment issues if we can adapt a given noisy image $y^u$ to its translated counterpart noisy image $y^{u \\rightarrow s}$ with seen noise $e^s \\sim D_s$, by removing the deviations $\\epsilon^{s \\rightarrow u}$ as:\n$y^{u \\rightarrow s} := x^u + e^s$\n(8)\n$= x^u + e^s + \\epsilon^{s \\rightarrow u} - \\epsilon^{s \\rightarrow u}$\n(9)\n$= y^u - \\epsilon^{s \\rightarrow u}$.\n(10)\nThus, our goal becomes to find a deviation offset $-\\epsilon^{s \\rightarrow u}$ that we can add to a given noisy image $y^u$ to adapt un-derlying noise towards noise a pretrained network is more familiar with. To this end, we add a learnable parameter $\\phi$ to a given noisy image $y^u$ and then train $\\phi$ to approximate the deviation offset $-\\epsilon^{s \\rightarrow u}$, as also illustrated in Figure 1:\n$y^{u \\rightarrow s} \\approx y^u + \\phi$.\n(11)\nHowever, we do not have access to such deviation offset during the test phase, as new noise distribution is gener-ally unknown and it would be also difficult to explicitly model seen noise distribution. To approximate an unknown $-\\epsilon^{s \\rightarrow u}$ with $\\phi$, we train $\\phi$ to minimize a self-supervision loss function (Equation 4), under the assumption that loss function is minimized when an input noise becomes closer to seen noise distribution. The assumption is reasonable as a self-supervision loss function is a surrogate of Equation 2, which a pretrained network $f_{\\theta^*}$ is trained to minimize with seen noise distribution. Overall, our objective function to train $\\phi$ becomes:\n$\\phi^* = \\arg \\min_{\\phi} || f_{\\theta^*} (D_1(y^u + \\phi)) - D_2(y^u + \\phi)) ||_2^2$.\n(12)\nNote that we freeze the parameters of a pretrained denois-ing network $\\theta^*$ and only optimize $\\phi$. Then, finally, a clean image is estimated by\n$\\hat{x^u} = f_{\\theta^*} (y^u + \\phi^*)$.\n(13)"}, {"title": "4. Experiments", "content": "We perform experiments on scenarios of test noisy images with different noise from the training set, demonstrating the effectiveness of our LAN framework. The implementation details and experimental settings are in Section 4.1. Then, we present the results in Section 4.2. The discussions on zero-shot denoising, computational efficiency, and the ef-fects of noise adaptation are covered in Sections 4.3, 4.4, and 4.5, respectively."}, {"title": "4.1. Experimental setup", "content": "Dataset. For training denoising networks, we use one of widely used real-world noise dataset SIDD [1]. To simulate unseen new noise scenarios during the test phase, we utilize other real-world noise datasets with noise distribution dif-ferent from SIDD: in this work, PolyU [48] and Nam [37].\nWe split the original PolyU images into 512 \u00d7 512 patches and crop them into 256 \u00d7 256. For Nam, we use patches splited into 256 \u00d7 256.\nModels. We conduct experiments on prominent denoising networks, DnCNN [54], Restormer [53], and Uformer [46]. For Restormer and Uformer, we employ the parameters of SIDD-pretrained networks that are publicly available. On other hand, DnCNN does not have publicly available SIDD-pretrained network parameters. Thus, we train DnCNN from scratch using the SIDD dataset.\nEvaluation. We measure PSNR and SSIM to evalu-ate the performance of SIDD-pretrained networks and our LAN framework on noisy images from PolyU and Nam datasets, against four alternative adaptation methods: adapt-ing a whole network (\u2018full-trainable\u2019), adapting only the first layer (\u2018first-layer\u2019), adapting only the last layer (\u2018last-layer\u2019), and adapting a whole network via meta-learning using first-order MAML [14, 38] (\u2018meta-learning\u2019). For 'meta-learning', we initialize the network from SIDD pre-trained weights. Then we train with 200 samples and vali-"}, {"title": "4.2. Experimental results", "content": "Quantitative results. We test the adaptation methods in two scenarios: SIDD \u2192 PolyU and SIDD \u2192 Nam. SIDD \u2192 PolyU means a network is pretrained on SIDD dataset and evaluated on PolyU dataset, while SIDD \u2192 Nam rep-resents a scenario, where a network is pretrained on SIDD and evaluated on Nam dataset. To demonstrate the appli-cability of our framework, we also perform experiments with various network backbones: namely, DnCNN [54], Restormer [53], and Uformer [46] over different self-supervised losses such as ZS-N2N [35] and Nbr2Nbr [17]. The experimental results are displayed in Table 1.\nThe results demonstrate that LAN exhibits notable per-formance improvement even after performing adaptation for only 5 iterations. Furthermore, our framework LAN is shown to consistently outperform a 'full-trainable' method that adapts all parameters of a network to a given noisy image with the same self-supervision loss function (Equa-tion 4) across problem settings and network backbones. For instance, under the setting of Restormer with Nbr2Nbr self-supervised loss on SIDD \u2192 NAM, our LAN framework improves the pretraining performance by 0.35dB, while a 'full-trainable' method improves by only 0.05dB. As a mat-ter of fact, a 'full-trainable' method often results in perfor-mance degradation, let alone improve the performance of a pretrained network.\nOne may argue that fine-tuning all parameters of a net-work may lead to overfitting, hence the reason for the rela-tively low performance gain. It can also be argued that our method of directly adjusting the noise in the input image can prevent overfitting and have similar effects to finetuning only the first layer of a network. However, \u2018\u2018full-trainable\u2019 adaptation often leads to performance improvements. Fur-thermore, finetuning only a first layer (denoted as \u2018first-layer\u2019 in the table) results in inferior performance compared to \u2018full-trainable\u2019 and our method. The results suggest that our method does not have similar effects to finetuning only the first layer. In contrast to fine-tuning only the first layer, our method offers fine-grained noise adaptation on a per-pixel basis, rather than relying solely on convolution, which carries a significant inductive bias.\nOn the other hand, the \u2018full-trainable\u2019 adaptation can sometimes bring improvements even after 20 iterations, as it can be seen with Uformer network backbone adapted with ZS-N2N loss function on Nam dataset. Then, one may be curious as to whether \u2018full-trainable\u2019 adaptation can outper-form LAN if adaptation is performed for longer iterations.\nAs such, we plot the performance curve of \u2018full-trainable\u2019 and our LAN method as the number of iterations increases with Uformer network backbone adapted with ZS-N2N loss function on Nam dataset, as visualized in Figure 5. As shown in the figure, even after performing adaptation for longer iterations, \u2018full-trainable\u2019 method fails to achieve performance on par with our method and starts to worsen the performance after near 20 iterations.\nQualitative results. We display the qualitative results of a pretrained network, \u2018full-trainable\u2019 adaptation, and our LAN framework in Figure 3. The images are obtained with Uformer finetuned via ZS-N2N for 20 iterations on Nam dataset. We also visualize the adapted noisy image by our method. Interestingly, noise adaptation sometimes has bet-ter PSNR/SSIM than an original noisy image. Then, one may think that the performance of LAN may be because noise adaptation is introducing additional denoising pro-cess. However, the denoising performance improvement by noise adaptation is small, compared to pretrained net-work and our whole LAN framework. Furthermore, we ob-serve that noise adaptation itself does not always give better PSNR than an original image, as noted in last two rows of the figure. Notable performance improvement brought by LAN just with noise adaptation at input image suggests that noise adaptation is not just additional denoising process."}, {"title": "4.3. Zero-shot denoising", "content": "One may argue that the performance degradation is ex-pected with the full adaptation of a network when train noise distribution and new test noise distribution greatly differ. Another alternative to finetuning of an network would be to train a randomly initialized network on a new noisy image from scratch via self-supervision loss func-tions for blind denoising. In fact, ZS-N2N [35] is specif-ically designed for training a denoising network on a sin-gle noisy image with unknown noise. Thus, we demon-strate such zero-shot denoising performance with DnCNN, trained via ZS-N2N and Nbr2Nbr on each image in PolyU and Nam dataset, the results of which are displayed in Ta-ble 3. After training a network for more than 1K iterations as suggested in [35], the zero-shot training yields signif-icantly poor performance, compared to not just LAN but also SIDD-pretrained model and other alternative adapta-tion methods. Despite the deviations and misalignment in noise, the results suggest the benefits of exploiting the knowledge of denoising tasks from a large training set. This is similar to domain adaptation, supporting our formulation."}, {"title": "4.4. Computational efficiency", "content": "One may be concerned with the computational cost of our proposed LAN framework due to the introduction of a learnable parameter for each image pixel. However, our evaluations show that LAN is more efficient in memory and runtime including both adaptation and inference, es-pecially for large networks like Restormer and Uformer, as displayed in Table 2. This is because fewer parame-ters need updating compared to a \u2018full-trainable\u2019 adaptation method. A limitation is that the number of trainable param-eters depends on the input image size, which could reduce efficiency for large images. We plan to improve this in the future. Despite this, the notable performance improvement from our framework offers a promising research direction to explore for image denoising."}, {"title": "4.5. Effects of noise adaptation.", "content": "To better illustrate the effects of noise adaptation by LAN (Equation 11), we perform experiments with synthetic noises for clear visualization. Specifically, Gaussian $\\sigma =$ 50 or Gamma distribution $\\theta = 0.5, k = 9$ noise is added to the single training data from BSDS500 [36] dataset to create synthetic-noisy-clean image pairs for pretraining the DnCNN. Figure 6 shows a histogram of noise from a syn-thetic training set (denoted as \u2018Pretrained\u2019), a new noisy im-age (denoted as \u2018Input\u2019), and an adapted noisy image (de-noted as \u2018Adapted\u2019). The figure demonstrates that our algo-rithm brings a new noise closer towards a noise expected by a denoising network. We also visualize such noisy images in Figure 4, where a plain image is used for better visual-ization. LAN is shown to try to adapt a new noisy image to contain similar noise used during pretraining. In particu-lar, we observe newly added noise on top of adapted noisy image. The results illustrate that our noise adaptation is not just additional denoising."}, {"title": "5. Conclusion", "content": "In this work, we propose a new adaptation approach to handle unseen noise for image denoising. We focus on the fine-grained pixel-level misalignment issues between unseen noise in new noisy images and seen noise dur-ing the pretraining of a denoising network. In contrast to standard approaches of finetuning a model, we focus on adapting an input noisy image itself. To this end, we in-troduce a new denoising framework, named Learning-to-Adapt-Noise (LAN), that adds a new noisy image with a learnable offset that is trained to bring noise in a new noisy image closer to noise seen during the pretraining stage. The experimental results solidifies the motivation and effective-ness of noise adaptation by our proposed method. One limi-tation would be that the computation and resource complex-ities may grow with the size of input images, although LAN is more efficient in comparison to model adaptation for im-ages of typical size 256 \u00d7 256. Nevertheless, we believe that our work brings interesting results and research discussions, and suggests a new research direction."}]}