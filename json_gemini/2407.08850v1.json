{"title": "UICrit: Enhancing Automated Design Evaluation with a UI Critique Dataset", "authors": ["Peitong Duan", "Chin-yi Chen", "Gang Li", "Bjoern Hartmann", "Yang Li"], "abstract": "Automated UI evaluation can be beneficial for the design process; for example, to compare different UI designs, or conduct automated heuristic evaluation. LLM-based UI evaluation, in particular, holds the promise of generalizability to a wide variety of UI types and evaluation tasks. However, current LLM-based techniques do not yet match the performance of human evaluators. We hypothesize that automatic evaluation can be improved by collecting a targeted UI feedback dataset and then using this dataset to enhance the per-formance of general-purpose LLMs. We present a targeted dataset of 3,059 design critiques and quality ratings for 983 mobile UIs, col-lected from seven experienced designers. We carried out an in-depth analysis to characterize the dataset's features. We then applied this dataset to achieve a 55% performance gain in LLM-generated UI feedback via various few-shot and visual prompting techniques. We also discuss future applications of this dataset, including training a reward model for generative UI techniques, and fine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation.", "sections": [{"title": "1 INTRODUCTION", "content": "Feedback is essential for guiding designers towards improving their user interface design. However, human feedback is not always readily available. While automated UI evaluation methods are able to provide instantaneous feedback, they have other limitations. Non-LLM based methods have limited generalizability and require large amounts of training data in order to accomplish a specific task, such as predicting user task completion time [10]. While LLM-based methods are able to generalize, their performance for UI feedback generation has potential for improvement [9].\nSome of the shortcomings of prior LLM-based UI evaluation can be attributed to limitations of LLMs at the time, such as lack of multi-modal input and small context windows. However, other weaknesses may be due to more fundamental data gaps in the per-formance of LLMs for UI evaluation (pointed out by [9]), which include poor knowledge of popular design conventions and failure to properly prioritize in cases when design guidelines clash. These limitations could be potentially addressed by few-shot training or fine-tuning on a ground truth dataset of high quality design feedback provided by human experts, where the LLM could learn the specifics and nuances of UI evaluation. However, the research community currently lacks a large enough dataset to comprehen-sively capture the knowledge required to carry out high quality UI evaluation.\nTo advance the effort towards improving automated UI feedback, we introduce UICrit, the first targeted dataset of design critiques for 983 mobile UI screens, consisting of 3,059 natural language design critiques collected from seven experienced designers. To help contextualize the feedback, each critique contains a bounding box highlighting relevant regions in the UI screenshot, and the dataset also includes numerical ratings for the aesthetics, usability, and overall design quality for each UI screen, which were manually determined from its design critiques and a carefully constructed rubric. We then analyzed this dataset and categorized the topics covered by the design critiques into five broad categories (layout, color contrast, text readability, button usability, learnability), and also determined the types of Uls represented in this dataset and the distribution of critiques targeting individual UI elements, groups of elements, and the entire screen. We also identified underrepresented design issues and UI types in this dataset, so future work could address these gaps."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 UI Datasets", "content": "A myriad of UI-related datasets have been developed. There are UI datasets consisting of screenshots and XML representations [6, 35, 43], as well as datasets augmented with human usage data, such as gaze patterns [4, 17, 21, 39] and task traces [3, 23, 36]. The RICO dataset [6] was one of the first large-scale UI datasets, which contains over 66k mobile UI screenshots and view hierarchies from 9.3k Android apps. RICO also contains other metadata, such as the traces from random exploration of the apps and UI layout vectors. Deka et. al. collected the RICO dataset via a system that combined both crowdsourcing and an automated mining system to further explore UI states. There have been several studies that refined the data in RICO[20, 22]. Li et. al. created the CLAY dataset [22], which is a cleaned subset the RICO dataset, where Uls with invalid view hierarchy layouts were removed. The authors then used the CLAY dataset to train deep learning models to automatically denoise mobile UI layouts.\nThere are also UI datasets that are augmented with human usage data. For instance, Jiang et. al. collected the UIEyes dataset [17], which consists of gaze data (fixation points and scanpaths) from 62 participants on 1,980 UIs collected from a large eye-tracking study. The authors then analyzed the dataset to determine the effects of factors like location and color on gaze behavior. Burns et. al. created the MoTIF dataset [3], which contains descriptions of high level tasks to complete on the app, interaction traces of humans attempt-ing to complete the tasks, and feasibility annotations of whether or not the task could be completed. The dataset was collected through a multi-step process where human workers annotated potential tasks, attempted to complete the tasks, and marked which ones were feasible. The authors then trained a model to predict task fea-sibility given the task and the interaction trace of the task attempt. However, despite these numerous UI datasets with human data, there currently does not exist a dataset of UIs with expert-annotated design critiques and design quality ratings."}, {"title": "2.2 Automated UI Evaluation", "content": "Prior to LLMs, automated methods to evaluate UIs include metrics [18, 33, 41] and models that predict user behaviors, such as task completion time [10], gaze patterns [12, 18], and user engagement [44], which provide feedback that designers could use to revise their designs. Oulasvirta et. al. created the Aalto Interface Metrics [33], which is a set of 17 metrics collected from prior studies and includes metrics like visual search performance and visual clutter. They then built a website, where users could upload their designs to be evaluated by these metrics. Wu et. al. collected a large dataset of human annotated ratings for the engagment of UI animations, and used it to train a neural network to predict the engagement ratings of an input UI animation [44]. The authors also developed a web app that uses this model to predict user engagement, and if the predict user engagement was low, it returns a set of potential reasons from a pre-defined set. However, these pre-LLM automated methods are unable to generalize beyond the specific design aspect they were developed to evaluate. Furthermore, they also require significant effort to achieve high performance; analytical metrics"}, {"title": "2.3 Design Feedback Support: Frameworks and Guidelines", "content": "Accurate evaluation of a UI design requires a complex, multi-faceted approach [15]. For instance, Hartmann et. al. developed a frame-work for evaluating UI design quality that separated it into five criteria: usability, content, aesthetics, reputation, and customization. These individual criteria are still quite broad; for example, Norman et. al. defined usability by five quality components: learnability, efficiency, memorability, user errors, and satisfaction [30]. Further-more, guidelines and heuristics have been developed to assist in UI evaluation. These guidelines and heuristics contain specific rules that good design should follow, and they are used in methods like heuristic evaluation [31], where an evaluator identifies heuristic violations in a given design. A number of different sets of heuristics and guidelines have been developed, such as the Apple Human Interface Guidelines, which contain guidance and best practices for general UI design [1], as well as heuristics targeted to specific aspects of the design, such as Nielsen Norman's 10 Usability Heuris-tics [29], the CrowdCrit Visual Design Critiques [26], and a set of guidelines for designing semantically coherent UIs [8]. Our work utilizes design guidelines and evaluation frameworks during the data collection to ensure that design critiques are grounded in best design practices and that the ratings accurately reflect the design quality."}, {"title": "3 DATA COLLECTION", "content": "The goal of this data collection was to obtain a large dataset consist-ing of UI screenshots with corresponding design feedback, bound-ing boxes of screen regions being critiqued, and design quality ratings. We recruited seven annotators with prior professional de-sign experience from a contracting company. Table 1 details the areas of design expertise and number of years of professional design experience for each annotator. This section describes the annotation process."}, {"title": "3.1 Method", "content": "Figure 1 illustrates the annotation process. We implemented an interface on top of an existing internal crowdsourcing infrastruc-ture, which assigns annotation tasks to workers. Before starting the data collection, we held an orientation session with the participants where we went through instructions and expectations. To ensure workers provide high quality design comments that are grounded in existing best practices, we provided three well-established de-sign guidelines for them to reference during the annotation: the Nielsen Norman 10 Usability Heuristics [29], CrowdCrit Visual Design Critiques [26], and the Apple Human Interface Guidelines [1]. We instructed the workers to provide critiques based on these guidelines, as well as additional critiques drawn from their prior design experience, since UI design often goes beyond following guidelines. Furthermore, following [9], we asked workers to follow Sadler's [38] format for effective feedback and include these three things in each design critique: the expected standard (i.e. what good design should look like), the gap between the current design and the expected standard (i.e. the design issue), and how to close the gap (i.e. how to fix the current design).\nEach worker annotates critiques and ratings for a single UI screen at a time. The worker performs all the annotations for the UI on a single page, as shown in Figure 2. The annotation process is divided into four parts. Part 1 consists of inspecting the UI screen and writ-ing down the main task supported by the interface (e.g. \"register for app\" for the screen in Figure 2). In addition to contributing valuable metadata to the dataset, asking participants to record the task ensures they thoroughly understand the UI before carrying out the evaluation, and the task also provides helpful context for evaluating the UI's usability at a later part of the annotation. Part 2 involves workers providing their own design critiques and cor-responding bounding boxes. Part 3 entails filtering for valid LLM comments. To supplement the critiques provided by each worker, we also pre-generated LLM design comments for each UI screen. However, LLMs may generate invalid design feedback, as found by [9], so workers were instructed to read through each LLM-generated design comment and determine which ones are valid. Since LLMs have poor object localization [7], only the LLM's comment text is provided, and the worker would have to determine the appropriate bounding box for valid LLM comments. Worker would also make minor edits to the text of valid LLM comments, if necessary, and also mark LLM comments that overlapped with ones they provided in Part 1.\nFinally, in Part 4, workers provided the design quality ratings. To ensure accurate ratings, we designed a rubric that break downs the design quality evaluation into specific factors that are easier to assess. Figure 2 shows the entire rubric, which is part of the critique annotation interface. The rubric first breaks down design quality into aesthetics and usability, which is modeled from the framework developed by [15] that separates design evaluation into multiple criteria. We used the two criteria that are relevant to single screen UIs. To facilitate the rating process, we provide definitions for each criterion, factors to consider, and reminded participants to consider relevant critiques they provided, as illustrated in Figure 2. We further broke down usability into learnability and efficiency, which are two relevant factors to consider, according to [30]. Since learnability and efficency are more specific than other dimensions in the rubric, they were rated on a 5-point Likert scale. Other di-mensions (including the overall design quality) were rated on a 10-point scale. Participants were instructed to fill out the rubric and then come up with an overall rating of the design quality based on their ratings for each dimension.\nWe optimized for coverage, and only assigned one worker per UI to ensure we obtain annotations for a large set of UIs. We randomly selected a total of 1000 UIs to be annotated, taken from the CLAY dataset [22]. We used random selection to maximize the general-izability of this set of UIs. We then prompted Gemini Pro Vision [13] (zero-shot) with each UI screenshot and the text of the three"}, {"title": "3.2 Results", "content": "The data collection took around two weeks to complete, with all seven annotators working full-time. At the conclusion of the data collection, we obtained clean annotations (i.e. not missing any data) for 983 UI screens and collected a total of 3,059 critiques. The data for each UI screen includes the RICO ID, which can be used to access the screenshot, android view hierarchy, and other metadata from the original RICO dataset, the main task the UI screen is designed for, a set of design critiques with bounding box coordinates of corresponding screen regions, and numerical ratings along various dimensions including the aesthetics, usability, and overall design quality. Figure 3 illustrates examples of this data for a UI with low design quality, a UI with average quality, and a UI with high design quality. Section 4 contains additional details and analyses of this dataset."}, {"title": "4 DATASET ANALYSIS", "content": "To better understand the dataset, we first compute high-level quanti-tative metrics. As stated earlier, we collected a total of 3,059 critiques for 983 UIs, which means each UI has on average, 3 critiques. Out of the 3,059 critiques, 2283 (74.6 percent) came from human designers, 256 (8.3 percent) came from Gemini, and 520 (17.1 percent) were provided by both humans and Gemini. Furthermore, Gemini gener-ated a total of 5927 comments for this set of UIs, and only 776 were validated, which meant only 13.1 percent of the design comments generated by Gemini were valid. This indicates few-shot prompting or finetuning is needed for general purpose LLMs to perform the critique task effectively.\nFigure 4 contains histograms depicting the ratings count for aesthetics, usability, and the overall design quality with a line in-dicating the average rating. The ratings for all three dimensions follow a normal distribution, with the average rating being close to 6.0 for each. There are very few ratings that are lower than 4; this could be because these UI screens come from app published on the Google Play Store [22], which went through an approval process that filtered out poorly designed apps.\nWe next carry out more in-depth analyses to understand the topics covered by the critiques, tasks supported by the dataset UIs, and the proportion of comments targeting UI elements, groups, and the entire screen. An analysis on the correlation between usability and aesthetics ratings can be found in Section A of the Appendix."}, {"title": "4.1 Design Critique Topics", "content": "To qualitatively understand the dataset, we characterize the types design issues covered by the critiques. To determine the different categories of design issues, we carried out KMeans clustering of the semantically meaningful embeddings of the critique text generated by SentenceBERT [37]. We tuned the number of clusters using the Elbow method [40] to 5 and reduced the dimensions to 2 using t-SNE [42] for visualization.\nFigure 5 (left) shows the results of the clustering. Two of the authors qualitatively analyzed the critiques in each cluster using grounded theory coding [14] and thematic analysis [2] to deter-mine the main theme for each cluster. One author coded all the design comments in each cluster, while another coded a smaller randomized sample. Each author conducted three rounds of coding to determine higher-level themes and cluster topics, and the authors met after each round to establish consistency. The codebook gener-ated for this analysis can be found in Section C of the Appendix. The two authors arrived at the following themes for each cluster:\n\u2022 Layout (size: 696): focuses on the layout of the UI screen, and includes critiques regarding positioning and alignment, the visual hierarchy, the logical grouping of elements, and simplicity of the layout.\n\u2022 Color Contrast (size: 655): targets the color contrast of text, icons, buttons, etc. with the background color.\n\u2022 Text Readability (size: 591): contains critiques regarding the readability of text, based on font-size and weight.\n\u2022 Usability of Buttons (size: 601): examines the usability of buttons, and includes critiques regarding the visual design of buttons for better usability, the clarity of the button's purpose, and the addition of buttons to simplify tasks (e.g. for navigation)\n\u2022 Learnability (size: 601): contains critiques regarding the clarity or intuitiveness of the purpose of icons, other UI elements, regions of the UI screen, and the entire UI screen, as well as critiques covering the clarity of text labels and other text content\nThe clusters in Figure 5 (left) are labeled with their corresponding themes. We also compare these clusters with the guidelines from all three sets of heuristics used for the data collection, to see if any major types of design issues are missing. One design heuristic not covered by these clusters is error prevention, and also those not applicable to the evaluation of static single screen UIs, such as heuristics feedback to the user, consistency across screens in the app, direct manipulation of UI elements, and help and documenta-tion, which are reasonably left out of the dataset. There were two critiques in the dataset that cover visibility of system status (from Nielsen Norman's 10 Usability Heuristics), and since the sample size was so small, they were not reflected in the clusters. Furthermore, there were guidelines that were covered to a limited extent. Namely, most of the critiques related to user control and flexibility/efficiency of use involved the addition of buttons to simplify the process. The remaining heuristics were well-covered, such as metaphors, which was covered by the Learnability cluster, and all the visual design related heuristics, as four of the five clusters are relevant to visual design."}, {"title": "4.2 Tasks Supported by Dataset UIs", "content": "We carried out a similar analysis on the tasks annotated by workers to determine the types of tasks supported by the UIs in this dataset. This metadata is useful for selecting few-shot examples based on task similarity to the target UI, as it provides an overview of what types of tasks are represented in this dataset.\nFigure 5 (right) shows the results of this task clustering, and each cluster is labeled with its respective theme. An qualitative analysis revealed the following themes for each of the six clusters:\n\u2022 Media and content interaction (size: 122): includes tasks like playing music or video, and learning a new language\n\u2022 Exploration and Navigation (size: 175): contains tasks involving the exploration of apps such as browsing cate-gories, shopping options, etc, and navigating to another page through a process (e.g. onboarding).\n\u2022 Customization of app settings and preferences (size 198): includes tasks like adjusting the notification settings and setting an alarm, wallpaper, etc\n\u2022 Location and Travel (size: 131): contains tasks involv-ing navigation with maps, location tracking, and managing bookings (flights, hotels, etc) for travel\nComparing these categories of tasks with the categories of apps from RICO [6], a major category of tasks that is missing is commu-nication, such as instant messaging or sending an email.\nAt a higher level, we could directly plot the distribution of cate-gories for the apps that the UI screens in UICrit are from. Figure 6 shows a histogram of the screen counts in each app category (from RICO). This is different from the screen-level task analysis, as the screen's task may not reflect its app's category (e.g. a login screen for an Events app). According to the histogram, there are screens from all 27 apps, but the Beauty, Art & Design, and Events categories are underrepresented."}, {"title": "4.3 Proportion of Group, Element, and Screen-level Comments", "content": "Another interesting way to analyze the critiques is to break them down by ones that target individual UI elements, groups of elements, and the entire screen. To compute the proportion of each type of target, we obtained location and size data for each UI element from"}, {"title": "4.4 App-level Analyses", "content": null}, {"title": "4.4.1 Correlation Between UI Screen Ratings and App Ratings.", "content": "We computed the Pearson correlation coefficient between the screen-level ratings for aesthetics, usability, and overall design quality from UICrit and the app ratings on the Google Play Store (taken from RICO [6]), for the apps the screens are from. For apps with multiple screens in the dataset, we averaged the ratings for those screens.\nWe obtained the following correlation coefficients: 0.007 (Aes-thetics), 0.022 (Usability), and 0.023 (Overall Design Quality). Each type of rating had a weak positive correlation with the app rat-ing. While this result deviates from expectation, it could be due to fact that the UICrit evaluation ratings were based on one or a few screens from the app, which may not represent the entire app's de-sign. Additionally, app ratings are influenced by many other factors such as app latency, spam/ads, cost, and customer service [28]."}, {"title": "4.4.2 Consistency of Screen Ratings from the Same App.", "content": "We also measured the consistency of the UICrit ratings for screens from the same app. We found 95 apps with at least two screens in the dataset, with a total of 208 screens. To measure consistency, we computed the standard deviation of the ratings for UI screens within the same app and compared it with the standard deviation of the ratings for the entire dataset. The average standard deviation across these 95 apps for each rating type, along with the standard deviation of the entire dataset for that rating type, can be found in Table 2. The table shows that the average app-level standard deviation is around half that of the standard deviation for the entire dataset, for all rating types. This indicates some consistency among screen ratings within the same app."}, {"title": "5 APPLICATION OF DATASET FOR MODELING UI FEEDBACK GENERATION", "content": "The data available in UICrit can be applied to help automate two distinct feedback generation tasks that are potentially useful to de-signers: 1) generating feedback for a specific region in the UI screen that the designer is concerned with (Figure 8, left) and 2) producing feedback for the entire UI screen. The latter includes automatically marking the target region (bounding box) of each critique, and also generating design quality scores (Figure 8, right). We utilized this dataset to experiment with various few-shot training and visual prompting methods to tune Gemini for each task. We then ran a user study, where a different group of design experts (who did not participate in the critique data collection) compared the feedback generated by Gemini with our few shot and visual prompts against the feedback from general-purpose (zero-shot) Gemini and human annotated feedback from the dataset."}, {"title": "5.1 Generating Design Comments for a Region of Interest", "content": "This task entails generating design feedback specifically for a region in the UI (Figure 8), and addresses scenarios where a designer would want to obtain targeted feedback on a particular portion of the screen design."}, {"title": "5.1.1 Prompt Design.", "content": "The prompt design includes the UI screen-shot overlaid with a bounding box around the region of interest (ROI), and the instructions to provide UI feedback for the marked region and how to properly format the feedback, following [38]. We also provide the three sets guidelines that were used during the data collection and ask the LLM to find guideline violations to supplement its feedback."}, {"title": "5.1.2 Few Shot Method.", "content": "We tried three techniques to select few shot samples from UICrit: random sampling, sampling by visual similarity, and sampling by semantic similarity. Random sampling entails selecting a random UI and then a random bounding box from one of its critiques as the region of interest, with the corresponding critique as the ground truth output. Selecting a critique's bounding box ensures that the ROI corresponds to a meaningful region or element in the screenshot. Sampling by visual similarity involves selecting bounding box patches (cropped from the UI screenshot) that are the most visually similar to the input UI's region of interest. The intuition behind providing visually similar few shot examples is that UI regions that are visually similar may have similar critiques (especially regarding the visual design), which increases the chance of the LLM providing relevant and accurate feedback. We used root mean square difference to compute the visual similarity [27] between bounding box patches from the dataset and the ROI's patch. We then select the UIs, bounding boxes, and corresponding critiques from UICrit containing the most visually similar patches as few-shot examples. Sampling by semantic similarity entails applying CLIP [34] to embed the patch image in a shared image and text space. CLIP is optimized for finding the closest text description for an image, which means this CLIP embedding would capture the semantic details of the input patch, for instance, if it contains a login button. The intuition for semantic sampling is that it would"}, {"title": "5.1.3 Few Shot Results.", "content": "To evaluate each few shot method, we applied Gemini to generate UI feedback for bounding box patches corresponding to critiques using 2, 4 and 8 shots. Eight was the maximum number of shots for the prompt to consistently not exceed the context window limits of Gemini Pro Vision. We then devised a scoring method to evaluate the output, where a valid critique was assigned 1 point, a partially valid critique was assigned 0.5 points, and in invalid critique was assigned 0 points. A critique is considered valid if it is both accurate and helpful, following the rating criteria described in [9]. We recruited three annotators to manually score these design comments. Two of the annotators were authors with prior design experience, and the third annotator was P1 from Table X.\nTable 3 shows the scores for the top 5 performing few-shot configurations, applied to 6 distinct UI screens. The scores are nor-malized by the total number of comments (shown in a separate column) and averaged across all three annotators, allowing for com-parison across different few-shot methods. The three annotators had a Fleiss' Kappa [11] inter-rater agreement score of 0.30, which indicates fair agreement. Visual sampling with 8 shots had the best performance, and semantic and visual sampling outperformed random sampling, even with fewer shots, which illustrates the ef-fectiveness of these targeted sampling methods. One interesting observation to note from the table is that, when there are fewer shots, Gemini generates more comments for each patch, but they are of lower quality (on average). This case is more apparent for 0-shot, which had 45 comments total and a score of 0.24. The model likely learns to give comments of higher quality for the target patch from the few shot examples."}, {"title": "5.2 Generating Design Comments for Entire UI Screen with Score Prediction", "content": "Since modeling UI screen comments generation and UI screen com-ments and rating generation are the same, other than a slight mod-ification in the prompt, we focus on the second task. Automated critique and design rating generation is useful in cases when a designer would like feedback for the entire UI screen mockup, and automatic design prediction could be used to quantitatively com-pare different UI designs or to see if a revision improved the design."}, {"title": "5.2.1 Prompt Design.", "content": "Similar to the prompt for the previous task, the prompt for this task includes the UI screenshot (without any bounding boxes), the three sets of guidelines, and instructions on properly formatting the critique. Unlike the previous task, this prompt asks the LLM to provide critiques for the entire UI screen"}, {"title": "5.2.2 Few Shot Method.", "content": "We tried four different methods to select few shot samples from the dataset: random sampling, sampling based on visual similarity, sampling based on task similarity, and sampling based on visual and task similarity. Random and visual similarity sampling follow the same procedure as described in Sec-tion 5.1.2, except visual similarity comparison is now carried out on screenshots, and the UI screen and its entire set of critiques are selected, instead of those corresponding to a single region. Task sim-ilarity sampling selects UI screens with the most similar tasks, and the intuition is that Uls with similar tasks would likely have similar usability requirements and the critiques from few shot examples with similar tasks would be informative regarding the usability requirements of the input UI. Task similarity is computed by using SentenceBERT to embed the task description and then computing cosine similarity on the embeddings. Sampling based on task and visual similarity combines the two sampling methods into a more comprehensive sampling approach. We used CLIP to generate a joint embedding of the UI screenshot and task description and then apply cosine similarity on the embeddings to determine similarity. We did not use this joint task and visual similarity sampling method for generating design comments for an ROI because the task for the UI screen may not be semantically relevant to the target region in the UI screen (i.e. if it only contains an icon), which could introduce noise."}, {"title": "5.2.3 Few Shot Results.", "content": "We reused the evaluation method from 5.1.3 and the same three annotators to determine the best few-shot scoring method. For this evaluation, we also generate scores for the predicted aesthetics, usability, and design quality ratings based on the ground truth ratings from the dataset. We assign a score of 1 if the ratings match, a score of 0.5 if the predicted rating deviates by 1 point from the ground truth rating, and a score of 0 otherwise."}, {"title": "5.2.4 Visual Prompting Method.", "content": "Since LLMs have poor object lo-calization [7], we tried various \"visual prompting\" methods [25] to improve the accuracy of the corresponding bounding boxes gen-erated for each design critique. Our visual prompting approaches include adding coordinates on the edge of the UI screenshot to as-sist in the specification of bounding boxes coordinates, overlaying a grid over the screenshot, and overlaying a patch grid over the screenshot, where the LLM would just need to return the numbers of the patches corresponding to the bounding box, which was done by [25]. Figure 11 in the Appendix illustrates each visual prompting method."}, {"title": "5.2.5 Visual Prompting Results.", "content": "We first tried combining critique generation and bounding box detection with visual prompting into a single LLM call, but found that while the bounding boxes improved in accuracy with visual prompting, the design critiques decreased in quality. This is probably due to task overload on the LLM. We split critique generation and bounding box detection into two separate calls to preserve the critique quality, while improving bounding box accuracy. The second prompt takes the critique output from the previous LLM call and queries the LLM with the critiques, screen-shot (with various visual prompting methods), and instructions to output the corresponding bounding box in the screenshot for each critique. This chain of prompts, one for each major task, is illustrated in Figure 9 and follows the prompt design from [9]."}, {"title": "5.3 Validation of Performance Improvement", "content": "To validate that our few-shot prompt design with usage of UICrit actually results in improved performance, we ran a user study on the design critiques generated from Task 2 (generating UI comments and corresponding bounding boxes for the entire UI), as it is a broader use case compared to generating targeted feedback for a specified region in the UI (Task 1). We also leave out design quality scores from the evaluation, as we were able to compare it with the ground truth design quality ratings from the dataset."}, {"title": "5.3.1 Method.", "content": "We use the best performing configuration from Task 2: 8-shot with joint task and visual-similarity sampling for critique generation, and using visual prompting with coordinates to generate corresponding bounding boxes, and compare it with two baselines: zero-shot prompting for critique generation, followed by another LLM call to obtain corresponding bounding boxes without visual prompting and human generated feedback from the dataset. We make two LLM calls for the first baseline, following that finding from [9] that separating LLM calls for major tasks leads to better UI design feedback. We recruited 6 design experts from our institution for this study. Table X details the areas of design expertise and years of professional design experience for each participant. During the study, participants evaluated the design feedback and bounding boxes from all three cases for one UI at a time, for a total of 6 UIs. For each UI, they first scored each UI comment individually, following the scoring method detailed in 5.1.3. For the second part, they evaluated the set of comments from each case as a whole, and ranked the sets of comments based on their overall quality and comprehensiveness. Participants also provided explanations for their rankings. We provided a form for participants to record their scores, rankings, and ranking explanations. Before starting the study's tasks, we held a brief meeting with each participant to explain instructions and obtain consent."}, {"title": "5.3.2 Results.", "content": "To compare comment quality across the three cases, we again normalized the total comment quality score by the number of comments generated by each condition and then averaged this score across the 6 participants. Table 6 shows the average comment quality score for our setup (\"Few-shot with Visual Prompting\") and the two baselines, along with the total number of comments from each condition. These results align with expectation, where our setup outperformed the zero-shot baseline by 0.17 (p = 5e-4) in average quality score, which corresponds to a 55 percent increase. As expected, the critiques from humans had the highest average quality score. However, the quality score is less than 1, which is likely due to the fact that design evaluation is inherently subjective, leading to disagreement regarding design issues present in the UI. In fact, we computed the Fleiss Kappa inter-rater reliability score"}, {"title": "6 DISCUSSION", "content": "We discuss insights from our few-shot experiments and the user study on the utility of this dataset, potential broader applications of this dataset, and the scalability of our data collection method."}, {"title": "6.1 Utility of the Dataset", "content": "Results from the few-shot experiments show that a targeted few shot sampling approach performed better than random sampling. This dataset probably contributed to the strong performance of these targeted few shot sampling methods because it provided a large set of UIs to sample from, which likely resulted in few shot examples that are quite similar visually, semantically, or both to the input UI screen, which provided more informative critiques for its assessment. This implies that this dataset, combined with few shot sampling methods that fully utilize it, enables the generation of better UI feedback compared to methods that do not use this dataset.\nThe improvement in generated comment quality was confirmed via a user study with design experts, who scored the comments gen-erated by the best performing targeted few shot sampling and visual prompting setup 55 percent higher than the comments generated by zero-shot prompting, which corresponds to the condition of not utilizing dataset. Furthermore, the design experts also more often preferred the set of comments generated for a UI by the few-shot and visual prompting condition over the comments generated by the zero-shot condition. While this dataset enables better automated design feedback, the design feedback is still far from the quality of the design critiques from humans, which was rated and ranked considerably higher than the few-shot and visual prompting condi-tion. While there is still a significant performance gap between this 8-shot with visual prompting technique and experienced human designers, fine-tuning an LLM on all 938 UI examples in this dataset may potentially lessen this gap."}, {"title": "6.2 Potential Applications of Dataset", "content": "In addition to direct applications for few-shot training and fine-tuning LLMs and other models, this dataset has potential broader applications in the field of computational UI design."}, {"title": "6.2.1 Tool-agnostic UI Evaluation.", "content": "This dataset could be applied to fine-tune a multi-modal LLM to automatically generate design critiques and ratings given only the UI screenshot. This has strong implications for its flexible integration into design tools. Only the image of the UI mockup would be needed for the fine-tuned model to generate feedback, and this universal representation should be available in all design tools, which may have different internal rep-resentations of their mock-up, such as Figma's JSON representation based on the layers configuration. This implies that the fine-tuned model can be used to evaluate the mock-up of any design tool, independent of their internal mockup representation, and can be easily integrated into any design tool that support the addition of add-ons, such as Figma, which allows the creation of plugins. Other implications include the straightforward integration of this fine-tuned model as an automated mockup evaluation feature in the development of future design tools, and the use of this model to evaluate any arbitrary UI screenshot found on the web or a UI dataset. Furthermore, since the evaluation is automated, this model enables UI evaluation at a large scale."}, {"title": "6.2.2 Reward Signals for Improving UI Generators.", "content": "Recently, sig-nificant progress has been made on UI generation using diffusion models [5, 16] and LLMs[24]. However, the generated UIs often fall short on following detailed design principles, capturing latest style trends, or avoiding artifacts like misaligned elements. To enhance the generation quality, we can fine-tune existing models with our dataset. Specifically, recent studies [19, 45] have shown that nat-ural language feedback generated by LLMs, such as the design comments in our case, can serve as reward signals for fine-tuning. Moreover, these methods are general approaches to improve model performance, making this dataset valuable regardless of the type of UI generators."}, {"title": "6.3 Scalability of Data Collection System", "content": "The data collection system described in Section 3 is designed to be highly scalable and flexible, capable of accommodating any set of UI screenshots and design guidelines, given a sufficient group of human annotators with prior design experience. This flexibility implies that this data collection system could be applied to collect relevant critiques for new UI trends, given updated screenshots and guidelines, such as those from the release of a new, up-to-date UI dataset and current design guidelines. Running this annotation system regularly to integrate new UI trends would ensure that the dataset remains continually updated and relevant."}, {"title": "7 LIMITATIONS AND FUTURE WORK", "content": "We discuss some limitations of our work. Regarding the dataset, we had only seven participants recording design comments, and this small annotator pool may restrict the diversity of critiques in the dataset. This limitation is further evidenced by the underrep-resented design issues we found in Section 4.1. Another dataset-related limitation is that it contains critiques relevant to only single UI screens, which leaves out feedback applicable to the entire app or UX-related feedback for task flows, such as how the app handles user errors. Furthermore, to illustrate light-weight use cases for the dataset, we only applied the dataset for few-shot prompting and provided at most 8 UI samples from the dataset due to context win-dow limitations. This light-weight few shot pipeline still sometimes hallucinates, and we may miss out on a potentially larger perfor-mance gain from fine-tuning Gemini on the entire dataset. Finally, the critiques generated by our few shot and visual prompting setup were only evaluated by participants for validity. Participants in our study did not implement any of the critiques, which prevented as-sessment the generated critiques' helpfulness in practice and effect on design outcome.\nOpportunities for future work include using this dataset to fine-tune multi-modal LLMs, such as Gemini Pro Vision, and evaluating the resulting performance. In addition, various input modalities could be explored, such as screenshot-only input and supplementing the screenshot input with an XML representation, and comparing their performance. Furthermore, a study could be conducted to eval-uate the performance of our few-shot techniques across different multimodal LLMs (e.g., GPT-4V [32]). This would help determine if the performance gains we observed for Gemini generalize across various language models. The data collection could be expanded to include more UI screens to ensure representation of all major UI tasks, and new designers could be added to the dataset annotation pool, which may introduce more variety in the types of design issues covered in the dataset and alleviate missing or underrep-resented categories of design issues. The data collection system could be extended so that workers evaluate a series of UI screens corresponding to task traces and provide UX-related critiques and ratings on the design of the task flow. There are several datasets containing UI task traces, such as [36], that could supply task traces for this annotation. Finally, a model trained on this dataset (with only the UI screenshot as input) could be integrated as a plugin in existing design tools, such as a Figma, Sketch, and Adobe XD, which could then be used to carry out a study evaluating the gen-erated critiques helpfulness in design practice and effect on design outcome. Given the tool-agnostic nature of the fine-tuned model, this study could be carried out on multiple design tools to see if results vary by tool."}, {"title": "8 CONCLUSION", "content": "We collected a dataset of design critiques, corresponding bound-ing boxes, and design quality ratings from experienced designers for a set of 983 distinct UIs, through a carefully constructed pro-tocol. We then analyzed the dataset to characterize the types of design issues covered by the critiques, tasks supported by Uls in the dataset, and other informative features. We then applied this dataset to automate UI feedback generation, with a novel prompt design that includes targeted few-shot sampling from the dataset and visual prompting to determine corresponding bounding boxes for each critique. We verified that this method generates higher quality feedback compared to zero-shot prompting via a user study with design experts, which confirms that this dataset's utility in improving automated design feedback. In addition to this demon-strated contribution, UICrit has numerous potential applications,"}, {"title": "A CORRELATION BETWEEN USABILITY AND AESTHETICS RATINGS", "content": "To see if there is a correlation between the quality of usability and aesthetics for UIs in the dataset, we computed the Pearson correlation coefficient between the aesthetics and usability ratings. We obtained a correlation coefficient of 0.875, which indicates a very high positive correlation. This strong positive correlation could be attributed to the fact that poor aesthetics would lead to poor usability of the UI, and that designers who invest significant effort into aesthetics or usability would likely also put a lot of effort in the other, aiming to achieve an overall high quality UI design."}, {"title": "B VISUAL PROMPTING INPUT", "content": "Figure 11 illustrates the three visual prompting techniques used to improve the critique bounding box accuracy."}, {"title": "C DESIGN CRITIQUE TOPICS CODEBOOK", "content": "This section contains the codebook for the qualitative analysis described in Section 4.1. The codebook contains the codes agreed upon at the end of each round of coding for the first two rounds, as the cluster topic and definition for the final round are discussed in Section 4.1. Each code is followed by its definition."}, {"title": "C.1 Layout", "content": null}, {"title": "C.1.1 Round 1.", "content": "\u2022 Cluttered Layout: Overcrowded design elements causing visual confusion.\n\u2022 Alignment Issues: Misaligned text, images, or other ele-ments.\n\u2022 Visual Hierarchy Problems: Lack of differentiation in size, color, or spacing to indicate importance.\n\u2022 Margins and Spacing Issues: Inconsistent or insufficient spacing between elements.\n\u2022 Redundant or Unnecessary Elements: Elements that do not contribute to the design's purpose or user experience.\n\u2022 Poor Text Justification and Formatting: Poor text align-ment and justification\n\u2022 Lack of Visual Emphasis on Interactive Elements: In-teractive elements that do not stand out or are not clearly marked.\n\u2022 Visual Disorganization: Lack of a clear structure or logical arrangement of elements.\n\u2022 Redundant Information: Excessive or repetitive informa-tion overwhelming the user.\n\u2022 Ineffective Use of White Space: Poor management of white space leading to disjointed or cluttered appearance."}, {"title": "C.1.2 Round 2.", "content": "\u2022 Position and Alignment: Proper placement and alignment of design elements\n\u2022 Visual Hierarchy: Strategic use of size, color, spacing, and other visual features to establish a clear hierarchy of infor-mation.\n\u2022 Logical Grouping of Elements: The organization and logical arrangement of design elements into coherent/related groups."}, {"title": "C.2 Color Contrast", "content": null}, {"title": "C.2.1 Round 1.", "content": "\u2022 Poor Text Contrast: Insufficient contrast between text and background.\n\u2022 Color Scheme Issues: Inconsistent or poorly chosen color schemes leading to contrast issues.\n\u2022 Poor Background Contrast: Background not providing enough contrast with the text or other elements.\n\u2022 Background Clarity: Background being too bright or too dark, affecting the overall design.\n\u2022 Prominence of Elements: Elements not being visually prominent due to poor contrast.\n\u2022 Icon and Label Visibility: Icons or labels being hard to see or understand."}, {"title": "C.2.2 Round 2.", "content": "\u2022 Poor Text Contrast: Insufficient contrast between text and background.\n\u2022 Icon Contrast Issues: Icons being hard to see due to poor contrast."}, {"title": "C.3 Text Readability", "content": null}, {"title": "C.3.1 Round 1.", "content": "\u2022 Font Size: The font size is too small or difficult to read.\n\u2022 Font Weight: The need to increase or change the font weight for better readability.\n\u2022 Font Style: Changes to the font style to improve legibility.\n\u2022 Hierarchy and Emphasis: Lack of visual hierarchy or em-phasis in the text elements.\n\u2022 Consistency: Inconsistent font sizes, styles, or weights across the design.\n\u2022 Content Clarity: Need for clear and concise content to improve readability and user understanding.\n\u2022 Visual Prominence: Important text elements should be more visually prominent.\n\u2022 Element Overlap: Issues where text elements overlap with other UI elements.\n\u2022 Redundancy and Density: Text is too dense or contains redundant information that could be simplified."}, {"title": "C.3.2 Round 2.", "content": "\u2022 Font Size: The font size is too small or difficult to read.\n\u2022 Font Weight: The need to increase or change the font weight for better\n\u2022 Text Density: Text is too dense, which could cause overlaps with other elements, or contains redundant information that could be simplified."}, {"title": "C.4 Usability of Buttons", "content": null}, {"title": "C.4.1 Round 1.", "content": "\u2022 Button Placement: Confusion due to button location.\n\u2022 Button Visual Design: Need for visual differentiation using color, size, and style.\n\u2022 Spacing and Alignment: Uneven spacing between buttons.\n\u2022 Button Recognition: Issues with buttons not looking like interactive elements.\n\u2022 Button Size: Buttons being too small or too large.\n\u2022 Call-to-Action Clarity: Buttons or text labels not clearly indicating their function.\n\u2022 Button Style: Busy or outdated button styles.\n\u2022 Color Scheme: Mismatched button colors and overall de-sign.\n\u2022 User Feedback and Affordances: Lack of visual feedback on button presses.\n\u2022 Button and Text Alignment: Misaligned buttons and text fields.\n\u2022 Hierarchy and Flow: Misplaced primary action buttons disrupting visual flow.\n\u2022 Button Spacing: Buttons placed too close together.\n\u2022 Button Emphasis: Primary actions not visually empha-sized.\n\u2022 User Control Elements: Missing user control buttons.\n\u2022 Background Contrast: Low contrast between background and buttons.\n\u2022 Button Consistency: Inconsistent button designs.\n\u2022 Button Visual Hierarchy: Important buttons are not visu-ally prominent."}, {"title": "C.4.2 Round 2.", "content": "\u2022 Button Visual Design: All aspects of the visual design of the button or button group for improved usability.\n\u2022 Button Clarity: Clarity of the purpose of the button through text labels, etc\n\u2022 Addition of Buttons: The addition of buttons to simplify tasks and increase user control."}, {"title": "C.5 Learnability", "content": null}, {"title": "C.5.1 Round 1.", "content": "\u2022 Inappropriate Icon: The icon used does not match the intended message or function.\n\u2022 Unclear Icon Meaning: Icons do not clearly convey their meaning or function.\n\u2022 Missing Interactivity Indication: Elements that should indicate interactivity (e.g., checkboxes, buttons) are missing or unclear.\n\u2022 Missing Functionality: Functional elements (e.g., buttons) are missing, making the app less intuitive to use.\n\u2022 Unclear UI Region: Purpose of an element group or region in the UI is not clear.\n\u2022 Poor Visual Hierarchy: Elements lack a clear visual hier-archy, making the interface confusing.\n\u2022 Unrealistic Icon: Icons do not represent real-world objects or concepts.\n\u2022 Unclear UI Purpose: The purpose of the UI screen is not intuitive.\n\u2022 Missing Placeholder: Placeholder text is missing, making it hard for users to figure out what type of information the field asks for.\n\u2022 Unclear Element Functionality: Elements do not clearly indicate their functionality.\n\u2022 Missing Labels: Elements are missing labels, causing con-fusion."}, {"title": "C.5.2 Round 2.", "content": "\u2022 Unclear Functionality: Icons or other UI elements do not clearly indicate their functionality\n\u2022 Unclear Purpose: The purpose of regions in the UI or the entire UI screen is unclear\n\u2022 Unclear Labels: The text labels do not match the icon or UI element, or do not clearly explain the purpose of the UI region or screen."}]}