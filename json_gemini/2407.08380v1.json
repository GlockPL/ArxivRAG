{"title": "Digital twins to alleviate the need for real field data in vision-based vehicle speed detection systems", "authors": ["A. Hern\u00e1ndez Mart\u00ednez", "I. Garc\u00eda Daza", "C. Fern\u00e1ndez L\u00f3pez", "D. Fern\u00e1ndez Llorca"], "abstract": "Accurate vision-based speed estimation is much more cost-effective than traditional methods based on radar or LiDAR. However, it is also challenging due to the limitations of perspective projection on a discrete sensor, as well as the high sensitivity to calibration, lighting and weather conditions. Interestingly, deep learning approaches (which dominate the field of computer vision) are very limited in this context due to the lack of available data. Indeed, obtaining video sequences of real road traffic with accurate speed values associated with each vehicle is very complex and costly, and the number of available datasets is very limited. Recently, some approaches are focusing on the use of synthetic data. However, it is still unclear how models trained on synthetic data can be effectively applied to real world conditions. In this work, we propose the use of digital-twins using CARLA simulator to generate a large dataset representative of a specific real-world camera. The synthetic dataset contains a large variability of vehicle types, colours, speeds, lighting and weather conditions. A 3D CNN model is trained on the digital twin and tested on the real sequences. Unlike previous approaches that generate multi-camera sequences, we found that the gap between the the real and the virtual conditions is a key factor in obtaining low speed estimation errors. Even with a preliminary approach, the mean absolute error obtained remains below 3km/h.", "sections": [{"title": "I. INTRODUCTION", "content": "There is a clear causal link between speeding and safety risks, showing strong evidence that speed enforcement leads to a reduction of accidents [1]. Reducing speed limits in highways and rural roads improves the safety of all vehicle occupants, but in urban environments the positive impact also translates into a reduction in pedestrian fatalities [2]. Moreover, it has also proved to be beneficial for traffic noise, environment and health [3]. An increasing number of cities have implemented a 30km/h zone in the city centre.\nThe enforcement of speeds limits plays a key role. Most studies confirm its positive influence, with estimated crash reductions of between 5% and 69% [5]. Moreover, as the OECD stresses [6], drivers' expectations of being caught should extend beyond a few locations. The greater the number of enforcement points, the better the positive effects. However, as the requirements for accuracy and robustness in vehicle speed measurement for speed enforcement are very demanding [7], the technology for speed detection usually involves high-accuracy, high-cost range sensor (radar, LiDAR or inductive loops). For municipalities, cost-efficient speed enforcement solutions are a necessary condition.\nThe use of cameras and computer vision is beginning to be considered as a cost-effective alternative solution, with potentially enhanced functionality, for accurate speed estimation [8]. However, this a very challenging problem due to the discrete nature of video sensors, in which resolution decreases proportionally to the square of the distance [7] and the negative impact of adverse weather and lighting conditions. Unlike other problems where machine learning can be effectively applied, an added difficulty is the limited availability of data in real-world scenarios that allow the deployment of learning-based approaches. Data gathering in this area requires a complex and costly setup to capture images from cameras synchronised with some high precision speed sensor to obtain the ground truth values. The number of datasets with this type of information is still very limited, which implies that the use of data-driven strategies is far from being consolidated in this domain.\nIn our previous works [9], [10], we generated synthetic datasets from a driving simulator (CARLA [4]) and used them to preliminary evaluate their feasibility for training and test deep learning models to perform speed regression from simulated sequences. In this work, we extend and validate this concept by generating a digital twin from a real environment (the UTFPR dataset [11]). As showed in Fig. 1, the main idea is to create a synthetic replica of a real camera used for vehicle speed estimation by generating a sufficient number of synthetic sequences with known vehicle speeds and wide variability (speeds, lighting, weather, type of vehicles, etc.). Then we use this digital twin to train and validate a deep learning-based speed estimation model, which is finally applied the real environment. The results obtained are very promising, and allow us to envision a"}, {"title": "II. RELATED WORK", "content": "As established in [8], although we can find hundreds of works focusing on vision-based vehicle speed estimation, the topic is not sufficiently mature. A considerable number of works present sub-optimal camera pose and settings resulting in very high meter-to-pixel ratios that are unlikely to provide accurate measurements. Sensitivity to lighting and weather conditions, camera pose and settings are not sufficiently addressed. In addition, the number of learning-based ap- proaches, while dominating in other domains, is still very limited for speed detection. This can be explained by the lack of consolidated datasets to train and compare the different methods. In this section, we focus on recent learning-based methods for vision-based vehicle speed detection from the infrastructure, including available datasets. We refer to the survey presented in [8] for a complete overview of the state- of-the-art.\nAverage traffic speed estimation was posed as a video action recognition problem using 3D CNNs in [12], collating RGB and optical flow images. They emphasized that the main limitation was overfitting due to the lack of data. In [13] a Modular Neural Network (MNN) architecture was used to perform joint vehicle type classification and speed detection. In [14] camera calibration, scene geometry and traffic speed detection were jointly addressed by means of a transformer network trained with synthetic data, with the limited assumption that cars have similar and known 3D shapes with standardised dimensions. Although these three approaches are conceived for traffic speed detection, they can be adapted to perform single vehicle speed estimation.\nSome works have proposed the use of recurrent architec- tures [15], [9], [16]. However, there is preliminary evidence showing a worse performance than with non-recurrent meth- ods such as 1D CNN [17] or 3D CNNs [9]. In [18], after vehicle detection and tracking using YOLOv3 and Kalman filter, respectively, a linear regression model was used to estimate the vehicle speed. In [19], they used Faster R-CNN and DeepSORT methods to perform vehicle detection and tracking respectively. Next, they extracted dense optical flow using FlowNet2, and finally, used a modified VGG-16 deep network to perform speed regression. In [17], a YOLOv5 detector was used to generate a 1D-feature vector based on the changing bounding box area of the vehicle. Then, they trained a 1D-CNN to perform speed estimation.\nAs far as we know, only three datasets with real sequences and real speed values are publicly available so far. First, the BrnoCompSpeed dataset [20], which contains 21 sequences (~1 hour per sequence) with 1920 \u00d7 1080 pixels resolution images at 50 fps in highway scenarios. They obtained the actual speed values using a laser-based light barrier system.\nSecond, the UTFPR dataset [11], which includes 5 hours of sequences with 1920 \u00d7 1080 pixels resolution images at 30 fps in an urban scenario. They recorded the ground truth speeds using inductive loop detectors. Finally, a recent dataset combining video and audio data was presented in [21], including 400 annotated sequences. However, they obtained the actual speed values using the on-board cruise control systems, which are neither sufficiently accurate nor homogeneous across models and makers.\nThe limited number of available datasets with real field data is somewhat conditioning the use of synthetic datasets, which is becoming increasingly prevalent for this problem. For example, in [22] a CNN model to estimate the average speed of traffic from top-view images is trained using synthe- sized images, which are generated using a cycle-consistent adversarial network (Cycle-GAN). Synthetic scenes with a resolution of 1024 \u00d7 768 pixels covering multiple lanes with vehicles randomly placed on the road are used in [14] to train and test the method used to jointly deal with camera calibration and speed detection. In [9], a publicly available synthetic dataset was generated using CARLA simulator, using one fixed camera at 80 FPS with Full HD format (1920\u00d71080), with variability corresponding to multiple speeds, different vehicle types and colours, and lighting and weather conditions. This dataset was extended to include up to six different camera poses [10] and a total of almost 3.7K sequences. We also found in [19] a synthetic dataset using CARLA, including multiple cameras and generating more than 12K instances of vehicles speeds."}, {"title": "III. METHOD", "content": "This section describes the methodology used to generate a the digital twin based on the UTFPR dataset [11], and the 3D CNN model used to perform speed estimation. Our approach aims to train a system from the digital twin only and then apply the trained model to the real urban environment without the need of adaptation.\nThe proposed digital twin simulates traffic on an urban road where environmental elements such as trees, zebra crossings, traffic signs, etc., as well as lighting and weather conditions are fully controlled. Images are rendered from the perspective of a virtual camera and used to build a dataset of traffic scenarios. The vehicle speed is a design variable, so it is known for any instant of time. The digital twin is based on CARLA driving simulator [4], where the physics models of the actors are very accurate and depicts high-quality images in the rendering process due to using the Unreal-Engine graphic motor. The correct generation of the digital twin (see Fig. 2) has to consider two main factors. First, to replicate the intrinsic parameters of the virtual camera and the pose with respect to the road (i.e., the aspect ratio of a similar vehicle in real and simulation is maintained during the whole sequence). Second, to recreate with a certain degree of similarity the visual conditions of the simulation environment (it minimises possible system"}, {"title": "A. Digital twin generation", "content": "failures derived from components external to the studio's). Thus, the real system captures images of vehicles driving on the road with a fixed camera position: [x, y, z, \u03b1, \u03b2, \u03a9], according to a system coordinate origin placed on the road plane. The camera's theoretical focal distance and optical centre (intrinsic parameters) used in the UTFPR dataset can be obtained from the camera model provided. However, the camera pose is unknown, except for the height, which has a value of 5.5m. In addition, the authors also provide the size of the inductive loops, which are visible from the cameras (see Fig. 3) and where the world origin is placed, the upper left corner identified by the loop on the left, for pose camera evaluation. Therefore, optimisation techniques can be applied to estimate the camera pose.\nAs mentioned above, optimisation techniques are used to determine the camera pose since an analytical solution is complex to evaluate, the Stochastic Descent Gradient algorithm is used, and equation 1 defines the cost function.\n\nloss = \\sum||x_i - F_i||_1^2 + \\sum||d_i - \\hat{d_i}||^2\n\nIn the equation, x represents the coordinates of the corners of the [u,v] loop, while d indicates the distance between two adjacent loop corners. The optimised camera pose is [-3.613, 5.5, 19.567, 0.481, -0.059, -0.108] where the loss"}, {"title": "B. Speed estimation", "content": "function value is around 78 pixels, considering the optics distortion the consequence of this.\nSince these parameters are not given in the dataset, a first approach has been focused on a multipose dataset, in which the camera pitch is varied for each sequence, in a range of 5 degrees [-33\u00b0,-37\u00b0].\nWe also remark that the original sensor image used to record the dataset had a resolution of 2592x1944, but con- figured to record at 30FPS with a resolution of 1920x1080 (after cropping). We generate the images in CARLA with the same resolution and apply the same cropping.\nFor this study, we have selected the ResNet 3D (3D CNN) architecture to perform speed regression. The overall architecture is depicted in Fig. 5. This model has proven to be very effective in previous works [9], [10] for lane-based vehicle speed detection using synthetic datasets. However, in the case of multi-lane/vehicle sequences (the UTFPR dataset camera covers up to 3 lanes) the input must be adapted so that the entire input sequence only contains data from one vehicle. A lane-based masking pre-processing is applied so that the model only processes one lane and one vehicle at a time (see Fig. 6).\nIn order to evaluate the suitability of the proposed ar- chitecture to perform speed detection, we firstly performed a training, validation and test (60/20/20) using the real sequences and the ground truth speed values provided in"}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "the UTFPR dataset [11] (Real Environment). In addition, based on previous evidence [19], [10], we also studied the effect of using a multi-view synthetic dataset, generated with multiple virtual camera locations as in [10] (Multi Camera). Preliminary evidence suggested that a view-invariant speed detection system might be possible. However, as we will see, this hypothesis was not effective when applied to the real environment, as it generated a very large error.\nFurthermore, to explore the sensitivity to the initial po- sition of the vehicle within the sequence used to train the speed detection model, we also created a synthetic dataset with the calibrated camera pose, but with vehicles starting earlier, so that the first frames contain the vehicle entering the scene (Fixed Camera \u2013 Vehicle Earlier). As we will see, the method was also largely affected by the initial vehicle position. Finally, we trained the 3D CNN model with a synthetic dataset generated with the calibrated camera view, and the same starting position for all the vehicles. The first frame of the sequence approximately corresponds to the first complete view of the vehicle, including the license plate (the method used in [11] was based on the detection and tracking of the license plate). This model is what we call Digital Twin, as it minimises the conditions between the virtual and the real environment. We also trained this model with a smaller version of the dataset (Digital Twin Small)."}, {"title": "A. Training parameters", "content": "We used a learning rate of 3x10-4, a batch size of 5, the Adam optimizer, and the MSE as a loss function. The network is trained using early stopping to avoid overfitting. A normalization of the output speeds to the range [-1,1] has also been performed."}, {"title": "B. Dataset size", "content": "In all datasets, we used a 60/20/20 distribution (train- ing/validation/test). For the Real Environment analysis, we use all the sequences available in the UTFPR dataset (7958). The previous Multi Camera model was trained with a syn- thetic dataset containing 3660 sequences [10]. For the Fixed Camera - Vehicle Earlier and the Digital Twin the datasets"}, {"title": "C. Results", "content": "have a total of 6022 sequences. Finally, the Digital Twin (Small) was created using 1800 samples. The range of speeds covered 10-80 km/h, and the type of vehicles used included different types of cars and motorbikes.\nThe obtained results are summarized in Table I. As can be observed, the proposed methodology proves to be very effec- tive when training with real sequences, with a MAE of 0.53 km/h. This value clearly outperforms previous works testing on the UTFPR dataset [11], [19], which is a contribution in itself. However, in our case, this only proves that the 3D CNN architecture is robust enough to deal with vehicle speed detection in this scenario, but our main objective is to avoid dependence on real data.\nDespite preliminary evidence on the benefits of using synthetic sequences from multiple virtual cameras [19], [10], the results of the Multi Camera model provided a very large error when applied to the real scenario (MAE > 26 km/h). This suggests that the gap in the camera pose between the digital twin and the real-world scenario should be as low as possible. The sensitivity to the vehicle position within the input sequence is also a key factor, as the error obtained, even with the calibrated camera, when the position of the vehicles differ between the digital twin and the real environment is very large (MAE > 13 km/h).\nFinally, when the vehicle position does not change be- tween the synthetic and the real data, and the camera is properly calibrated, we obtained a reasonable error even with a small dataset (MAE of 3.03 km/h). This error is reduced"}, {"title": "V. CONCLUSIONS", "content": "As has been demonstrated in this study, synthetic datasets for vehicle speed detection can be used to train a neural network, and then be able to transfer this model to the real world, with satisfactory results, without the need to provide the network with images in the real environment. This can bring many benefits, reducing costs, and avoiding the need of real data with associated ground truth speeds to deploy deep learning-based solutions.\nThe sensitivity of the digital twin to the intrinsics and extrinsic parameters of the camera is considerable, so the gap between the virtual and real camera should be as low as possible. In addition, the position of the vehicles along"}]}