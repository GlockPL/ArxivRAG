{"title": "HSF: Defending against Jailbreak Attacks with Hidden State Filtering", "authors": ["Cheng Qian", "Hainan Zhang", "Lei Sha", "Zhiming Zheng"], "abstract": "With the growing deployment of LLMs in daily applications like chatbots and content generation, efforts to ensure outputs align with human values and avoid harmful content have intensified. However, increasingly sophisticated jailbreak attacks threaten this alignment, aiming to induce unsafe outputs. Current defense efforts either focus on prompt rewriting or detection, which are limited in effectiveness due to the various design of jailbreak prompts, or on output control and detection, which are computationally expensive as they require LLM inference. Therefore, designing a pre-inference defense method that resists diverse jailbreak prompts is crucial for preventing LLM jailbreak attacks. We observe that jailbreak attacks, safe queries, and harmful queries exhibit different clustering patterns within the LLM's hidden state representation space. This suggests that by leveraging the LLM's hidden state representational capabilities, we can analyze the LLM's forthcoming behavior and proactively intervene for defense. In this paper, we propose a jailbreak attack defense strategy based on a Hidden State Filter (HSF), a lossless architectural defense mechanism that enables the model to preemptively identify and reject adversarial inputs before the inference process begins. We activate its defensive potential through an additional plugin module, effectively framing the defense task as a classification problem. Experimental results on two benchmark datasets, utilizing three different LLMs, show that HSF significantly enhances resilience against six cutting-edge jailbreak attacks. It significantly reduces the success rate of jailbreak attacks while minimally impacting responses to benign user queries, with negligible inference overhead, and outperforming defense baselines.", "sections": [{"title": "1 Introduction", "content": "With the growing capabilities of models like ChatGPT (Achiam et al. 2023), Llama (Touvron et al. 2023), and Mistral (Jiang et al. 2023), there is increasing concern about the potential security risks associated with their powerful functionalities, such as biased reports (Ferrara 2023), inaccurate information (Ji et al. 2023), or harmful contents (Weidinger et al. 2021). Over the past few years, researchers make extensive efforts to align these models safely using techniques such as SFT (Ouyang et al. 2022) and RLHF(Bai et al. 2022), ensuring that the models adhere to human values and preferences for safety and reliability. Despite these alignment efforts providing models with the ability to judge the safety of inputs to some extent, models remain susceptible to adversarial attacks (Zou et al. 2023).\nRecent studies have highlighted the potential threat of \"jailbreak attacks\"(Liu et al. 2023; Wei, Haghtalab, and Steinhardt 2024), which can bypass existing alignments and induce models to output unsafe content. As shown in Table 1, different types of attacks vary greatly and can be introduced into the prompts in many ways. Current defense methods primarily focus on prompt's detection (Alon and Kamfonas 2023) and rewriting (Jain et al. 2023; Zheng et al. 2024) or output's detection (Xie et al. 2023) and control (Xu et al. 2024). The former defends against jailbreak attacks by using PPL (Alon and Kamfonas 2023), paraphrasing (Jain et al. 2023), or adding safer prompts (Zheng et al. 2024; Wei,"}, {"title": "2 Related Work", "content": "In the following sections, we provide an overview of the related work, beginning with a discussion of various approaches to jailbreak attacks, and then moving on to the defenses developed to counter these attacks."}, {"title": "2.1 Jailbreak Attacks", "content": "Current jailbreak attacks can be broadly categorized into two major types: template completion attacks and prompt rewriting attacks(Yi et al. 2024).\nIn template completion attacks, the attacker embeds harmful queries within a contextual template to generate jailbreak prompts. Liu et al. (2023) investigates adversarial examples against the GPT-3 model, where carefully crafted inputs induce the model to generate inappropriate content. Wei, Haghtalab, and Steinhardt (2024) identifies the root cause of LLMs' vulnerability to jailbreak attacks as conflicting objectives and generalization mismatches, noting that the model's safety capabilities do not match its complexity. Li et al. (2023) constructs a virtual, nested scenario to hypnotize large models into performing jailbreak attacks. Ding et al. (2024) utilizes prompt rewriting and scenario nesting within LLMs themselves to generate effective jailbreak prompts. Wei, Wang, and Wang (2023) leverages the model's contextual learning abilities to guide the LLM in generating unsafe outputs.\nPrompt rewriting attacks involve constructing jailbreak prompts through optimization techniques, with two main approaches: (1) Gradient-based methods, as Zou et al. (2023) uses gradient optimization to generate adversarial inputs; (2) Genetic algorithm-based methods, as Liu et al. (2024) and Yu, Lin, and Xing (2023) collect seeds from a curated library of handcrafted templates and continuously generate optimized adversarial prompts through mutation and crossover. Moreover, recent advancements in red-teaming approaches further extend the capabilities of jailbreak attacks. Hong et al. (2024) introduces a curiosity-driven red-teaming approach, which leverages curiosity-based reinforcement learning to explore a wide range of potential adversarial prompts. Samvelyan et al. (2024) cast adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Lee et al. (2024) uses GFlowNet fine-tuning, followed by a secondary smoothing phase, to train the attacker model to generate diverse and effective attack prompts."}, {"title": "2.2 Existing Defenses", "content": "We categorize existing defense measures into two main types: model-level defenses and prompt-level defenses."}, {"title": "3 Motivation", "content": "In this section, we explore the representation capabilities of LLMs in distinguishing harmful queries, benign queries, and harmful queries with adversarial prefixes."}, {"title": "3.1 Data Synthesis and Basement Model", "content": "If harmful and benign queries can be distinguished, we want this distinction to arise from their differences in harmfulness rather than other unrelated factors such as format or length. To address the impact of unrelated features, we used the commercial API of ChatGPT, gpt-3.5-turbo, to synthesize harmful and benign queries with fine control.\nTo ensure the diversity of the synthesized dataset, we sampled the top 100 data points from Advbench (Zou et al. 2023) as harmful queries and instructed gpt-3.5-turbo to generate benign versions of these queries while maintaining consistency in length as much as possible. Please refer to the appendix for the prompts we used to guide the data synthesis. We then applied additional manual checks to ensure effectiveness and quality. As a result, we collected 100 harmful and 100 benign queries, with an average length of 15.05 and 17.14 tokens, respectively (measured using the Llama2-7b-chat tokenizer).\nWe conduct experiments with three popular 7B chat LLMs available on HuggingFace, as well as an uncensored 30B LLM: Llama-2-chat (Touvron et al. 2023), Vicuna-v1.5 (Chiang et al. 2023), Mistral-instruct-v0.2 (Jiang et al. 2023), and WizardLM-30B-Uncensored (Computations 2024). Some of these models explicitly underwent extensive safety training (Llama2-7b-chat and Mistral-instruct-v0.2)."}, {"title": "3.2 Visualization Analysis", "content": "In this section, we first compare the performance of aligned and unaligned models in LLM's hidden state representations for harmful and benign queries. Then, we examine whether LLM's hidden state have the ability to distinguish between harmful, harmless, and jailbreaking."}, {"title": "Aligned Model vs. Unaligned Model", "content": "Following (Zheng et al. 2024), we used Principal Component Analysis (PCA) to visualize the hidden states of the models. We selected the hidden states of the last input token from the model's last Decoder Layer output, as these hidden states intuitively gather all the information about the model's understanding of the query and its response strategy. It should be noted that these hidden states are also projected through the language modeling head (a linear mapping) for the prediction of the next token, which means they present a linear structure in the corresponding representation space (as assumed by PCA). We use two sets of hidden states to compute the first two principal components for each model, which include both harmful and benign queries. By selecting these data points, we are able to extract the most significant features related to the harmfulness of the queries.\nWe find that the alignment models have a significantly better ability to distinguish between harmful and benign queries compared to unaligned models. From the top half of Figure 1, it can be seen that models trained with safety measures, such as Llama2-7B-chat, Vicuna-v1.5 and Mistral-instruct-v0.2, can naturally distinguish harmful queries from benign ones, with their boundaries (black dashed lines) easily fitted by logistic regression using the harmfulness of the queries as labels. However, the unaligned model WizardLM-30B-Uncensored lack this capability."}, {"title": "Distinguishing Ability for Jailbreak Attack", "content": "Inspired by this finding, we construct six different jailbreak attacks based on the easyjailbreak dataset and the 50 harmful queries from the aforementioned synthesized dataset. The attack methods include AutoDAN (Liu et al. 2024), DeepInception (Li et al. 2023), GPTFuzz(Yu, Lin, and Xing 2023), ICA(Wei, Wang, and Wang 2023), ReNellm(Ding et al. 2024), and gcg(Zou et al. 2023), each aiming to bypass the model's safety mechanisms through different approaches. For Llama2-7B-chat, we use three sets of hidden states to compute the first two principal components, which included benign queries, harmful queries, and jailbreak attacks.\nWe observe that models trained with safety protocols can not only distinguish between harmful and benign queries but also differentiate jailbreak attacks. The boundaries between these categories (indicated by black dashed lines) can be easily fitted using an SVM, with the harmfulness of the queries still serving as the labels. More reuslts can be found in AppendixB.2. Based on this observation, our insights for developing a lightweight classification model include: (i) employing more robust methods for sampling the classification criteria, and (ii) more accurately fitting the boundaries between benign queries, harmful queries, and jailbreak attacks within the model's representation space."}, {"title": "4 Hidden State Filter", "content": "In this section, we present an overview of the HSF, followed by a detailed description of its design."}, {"title": "4.1 Overview of Hidden State Filter", "content": "Our HSF consists of two stages, as illustrated in Figure3. The first stage is the training phase, where a weak classifier is constructed to classify user queries. This weak classifier is trained using hidden vectors extracted from the last Decoder Layer during the forward pass of a specified LLM. In the second inference stage, the user's query undergoes a forward pass through the LLM to obtain the hidden vectors from the last Decoder Layer. HSF then classifies the last k tokens of these hidden vectors to determine whether to block the current inference. The remainder of this section details each step. It is important to note that the HSF does not generalize across different models; therefore, for clarity, we will illustrate the process using Llama2-7B-chat and Mistral-instruct-v0.2 as the example model."}, {"title": "4.2 Training Phase: A Weak Classifier", "content": "To construct the expert model, we first collect 3,000 samples each from the UltraSafety (Guo et al. 2024) and PKU-SafeRLHF-prompt (Ji et al. 2024b) datasets as the harmful query dataset. These queries are expected to be rejected by any LLM aligned with human values. We also collect 6,000 samples from the databricks-dolly-15k (Conover et al. 2023) dataset as the benign query dataset. For the poorly aligned mistral-instruct-v0.2 model, we additionally sample 750 queries from both the harmful and benign datasets we construct. These samples are used to create a positive and negative dataset for training by adding template completion jailbreak attack templates.\nStep 1: To construct the training samples, we input these queries into the model and perform a forward pass, extracting the vectors corresponding to the last k tokens from the hidden state of the last Decoder Layer, denoted as $t_k \\in \\mathbb{R}^{n \\times k}$. Since the default chat template is used when inputting queries into the LLM, the token length is at least 8 for the Llama2-7B-chat tokenizer. Therefore, we set the search space for the last k tokens $t_k$ as $k \\in \\{1, 2, ..., 8\\}$.\nStep 2: Concatenation of k vectors. The vectors corresponding to the last k tokens are concatenated in their original order, with zeros padded between every two tokens to soft-distinguish different tokens and optimize the classifier's recognition capability. The specific formula is as follows:\nLet $t_1, t_2, ..., t_k$ be the vectors corresponding to the last k tokens, where $t_i \\in \\mathbb{R}^n$. We define the vector $T_k$ as:\n$T_k = [t_k, 0, t_2, 0, ..., 0, t_1]$, (1)\nwhere $0 \\in \\mathbb{R}^n$ denotes a zero vector of length n.\nStep 3: Constructing the weak classifier. Considering the model alignment capability extension and inference time cost control, we use a simple Multilayer Perceptron (MLP) as the classifier. We fit a logistic regression model using the empirical rejection probability of $T_k$, applying dropout regularization to prevent overfitting:\n$f_k:\\mathbb{R}^{n \\times k} \\rightarrow \\mathbb{R}, \\quad f_k(x) = w_k T_k + b_k$, (2)\nwhere $w_k \\in \\mathbb{R}^{n \\times k}$ and $b_k \\in \\mathbb{R}$ are the parameters to be fitted by the logistic regression model.\nStep 4: Model training. We train the model by minimizing the binary cross-entropy loss function. The specific training steps include forward propagation to compute the loss and backpropagation to update the parameters.\nFor a given query sample $T_k$, we define the following binary cross-entropy loss function:\n$L_k(\\theta) = -l \\log(\\sigma(f_k(T_k))) - (1 - l) \\log(1 - \\sigma(f_k(T_k)))$, (3)\nwhere: $l \\in \\{0,1\\}$ is the binary label indicating whether the query is safe, $\\sigma$ is the sigmoid function.\nTo optimize the model parameters $\\theta$, we minimize the above loss function $L_r(\\theta)$, ensuring that harmful queries (l = 1) receive a higher harmfulness score and benign queries (l = 0) receive a lower harmfulness score."}, {"title": "4.3 Defense Phase: Implementing Jailbreak Attack Defense by Classifying User Queries", "content": "In the defense phase, the trained weak classifier is inserted into LLM as a plug-in module. The process is as follows:\n1. Receive the concatenated vector $T_k$ of the selected last k tokens as input.\n2. Input the concatenated vector $T_k$ into the classifier to calculate a harmfulness score $\\alpha$ (ranging from 0 to 1)."}, {"title": "5 Experiments", "content": "In this section, we evaluate the defense performance of the HSF, focusing on two key metrics: Attack Success Rate (ASR) and Area Under the Curve (AUC)."}, {"title": "5.1 Experimental Setup", "content": "Basement Models. We deploy HSF on three open-source LLMs: Vicuna-v1.5 (Chiang et al. 2023), Llama2-7b-chat (Touvron et al. 2023), Mistral-instruct-v0.2 (Jiang et al. 2023)to evaluate its effectiveness.\nAttack Methods. We consider six state-of-the-art jailbreak attacks, each covering different categories, implemented through easyjailbreak (Zhou et al. 2024). Specifically, GCG (Zou et al. 2023) represents gradient-based attacks, AutoDAN (Liu et al. 2024) is based on genetic algorithms, and ICA (Wei, Wang, and Wang 2023)represents context-learning-based attacks. We also included ReNellm (Ding et al. 2024), DeepInception (Li et al. 2023), and GPTFuzz (Yu, Lin, and Xing 2023) as representative empirical jailbreak attacks. To evaluate the defense performance against naive attackers directly inputting harmful queries, we used two harmful query benchmark datasets: Advbench (Zou et al. 2023) and MaliciousInstruct (Huang et al. 2024). Detailed settings for these attack methods and harmful query datasets can be found in the AppendixA.1.\nBaselines. We consider six state-of-the-art defense mechanisms as baselines. PPL(Alon and Kamfonas 2023) and Self-Examination (Phute et al. 2023)) are input-output detection-based methods, while Paraphrase (Jain et al. 2023), Retokenization (Jain et al. 2023), and Self-Reminder (Xie et al. 2023) are mitigation-based methods. DRO(Zheng et al."}, {"title": "5.2 Experimental Results", "content": "HSF Enhances LLM Safety. Table 2 presents a comparison of the ASR for the Mistral-instruct-v0.2 and Llama2-7B-chat models when employing HSF and various baseline defenses against six different jailbreak attacks. The results indicate that for models with weaker safety alignment, such as Mistral-instruct-v0.2, HSF markedly decreases the ASR, outperforming almost all baseline defenses. Notably, while most other defenses were ineffective against AutoDAN, HSF successfully defends against this attack, achieving an ASR of 0. For models with strong alignment, such as Llama2-7B-chat, HSF consistently reduces the ASR across all attacks to nearly 0. Additional results for HSF applied to Vicuna-v1.5 can be found in AppendixB.2.\nMoreover, HSF has a high AUC score. Table 3 shows the AUC score of HSF on safe datasets."}, {"title": "5.3 Ablation Analysis", "content": "In this section, we conduct an ablation analysis on the hyperparameter k in HSF, tested on Mistral-instruct-v0.2 and Llama2-7B-chat models. Table 4 show that HSFr exhibits significant robustness against prompt rewriting attacks, maintaining an extremely low ASR regardless of the k value. However, for more complex template completion attacks, such as ReNellm and DeepInception, the defense performance of the model shows a clear dependency on the hyperparameter settings. This suggests that to maintain high classification performance in advanced attack scenarios, HSF requires appropriate hyperparameter tuning.\nMoreover, we find that due to the identical chat template used, the last 4 tokens of the model input remained consistent. As a result, HSF exhibites a lower ASR at k = 4, demonstrating better defense effectiveness. Additionally, for GCG jailbreak attacks, when k > 1, HSF effectively disrupts its robustness, leading to a significant drop in ASR."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a novel model architecture module, HSF, designed to defend against jailbreak attacks on LLMs by leveraging the model's alignment capabilities reflected in its hidden states. We observe that aligned models can internally distinguish between jailbreak attacks, harmful queries, and benign queries. Inspired by this observation, we develope a plug-in weak classifier module that uses the last k tokens from the LLM's DecoderLayer to maximize the utilization of the model's complex alignment capabilities as a safety measure. This ability allows HSF to identify the harmfulness of queries before inference begins. Our results demonstrate that HSF can effectively defend against state-of-the-art jailbreak attacks while being both efficient and beneficial. In future, we will explore how to leverage LLM hidden states to build more flexible harmful-behavior detectors and enhance jailbreak attack prevention through Retrieval-Augmented Generation methods."}, {"title": "7 Limitations", "content": "Limited Generalizability: The HSF is specifically tailored to work with certain LLMs, necessitating retraining when applied to other models. This dependency on the alignment capabilities of the underlying model restricts its generalizability to models that have undergone analogous alignment training. However, while retraining is required for each new model, the process incurs relatively low computational cost and does not alter the model's architecture."}, {"title": "A Appendix A: Detailed Experimental Setups", "content": "A.1 Attack Setup\nFor GCG (Zou et al. 2023), AutoDAN (Liu et al. 2024), GPTFuzz (Yu, Lin, and Xing 2023), DeepInception (Li et al. 2023), and ICA (Wei, Wang, and Wang 2023), we use the easyjailbreak (Zhou et al. 2024) framework to construct attacks. We utilized 100 different representative harmful queries from Advbench (Zou et al. 2023). For ReNellm (Ding et al. 2024), we rewrite these 100 representative harmful queries collected from Advbench using easyjailbreak and filled them into templates to create 100 jailbreak prompts.\nA.2 Baseline Setup\n1. PPL (Alon and Kamfonas 2023): PPL is an input detection mechanism that calculates the perplexity of a given input to determine whether the user's request should be accepted or rejected.\nThe definition of perplexity (P) is as follows:\n$Perplexity (P) = exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(W_i | W_{1:i-1})\\right)$ (4)\nFollowing (Alon and Kamfonas 2023), we use Llama2-7b-chat to calculate perplexity. As per (Xu et al. 2024), we set the PPL threshold to the highest perplexity value of harmful queries in Advbench. This ensures that queries from Advbench do not trigger the detector.\n2. Paraphrase (Jain et al. 2023): We follow (Jain et al. 2023) and used GPT-40 by default to paraphrase user queries. The prompt is as follows:\nThe paraphrased output is then used as the input to the target language model.\n3. Retokenization (Jain et al. 2023): This method splits tokens and represents them using multiple smaller tokens. For example, the token representing the word \"breaking\" is split into two tokens representing \u201cbreak\u201d and \u201cing.\u201d We use BPE-dropout (Provilkov, Emelianenko, and Voita 2020), which randomly drops p% of BPE merges during the tokenization process. We set p = 0.2 following (Jain et al. 2023).\n4. Self-Examination (Phute et al. 2023): This is an output detection method that uses the language model itself to distinguish whether harmful content is being generated. We use the prompt suggested by (Phute et al. 2023):\n5. Self-Reminder(Xie et al. 2023): Self-Reminder appends prompts to input prompts to remind the language model to respond responsibly.\n6. DRO (Zheng et al. 2024): This is an automated optimization method for generating safe prompts. Following previous"}, {"title": "B Appendix B: More Results", "content": "B.1 PCA visualization in More Models\nWe also conducted PCA visualizations on two additional models in Figure4 and Figure5.\nB.2 Hidden State Filter in More Models\nWe demonstrate HSF when applied in Vicuna-v1.5 in Table 5. Our observations reveal that, although jailbreak attacks on Vicuna-v1.5 yield high ASR and harmful scores, HSF can significantly mitigate its effectiveness.\nB.3 Jailbreak attack templates is essential\nIncorporating adversarial attack templates into the training set has proven to be a highly effective strategy for enhancing the robustness of lightweight classification models against adversarial inputs (Wallace et al. 2019). By systematically exposing the model to a diverse array of adversarial prompts during training, we empower the model to more effectively identify and counteract such attacks during inference. This proactive exposure helps establish more resilient decision boundaries within the model's representational space, thereby significantly reducing the success rate of jailbreak attempts.\nTo demonstrate the effectiveness of this approach, we present a comparative analysis of models trained with and without the inclusion of jailbreak attack templates in Table 6. The results clearly show that models trained with these templates achieve a substantially lower Attack Success Rate (ASR) across various jailbreak methods, highlighting the critical importance of this training enhancement in improving model robustness.\nThis table underscores the substantial reduction in ASR when jailbreak templates are included in the training set, thereby highlighting the importance of such templates in building more secure and reliable LLMs."}, {"title": "C Appendix C: Example Demonstrations", "content": "We present the following examples illustrating HSF across different models. For clarity, attack prompts are highlighted in red.\nThis example shows HSF is applied in Llama2-7b-chat to defend against AutoDAN(Liu et al. 2024)\nThis example shows HSF is applied in Mistral-instruct-v0.2 to defend against Deepinception(Li et al. 2023)\nThis example shows HSF is applied in Vicuna-v1.5 to defend against ReNellm(Ding et al. 2024)"}, {"title": "8 Reproducibility Checklist", "content": "This paper:\n\u2022 Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes)\n\u2022 Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes)\n\u2022 Provides well marked pedagogical references for less-familiare readers to gain background necessary to replicate the paper (yes)\nDoes this paper make theoretical contributions? (no)\nDoes this paper rely on one or more datasets? (yes)\nIf yes, please complete the list below.\n\u2022 A motivation is given for why the experiments are conducted on the selected datasets (yes)\n\u2022 All novel datasets introduced in this paper are included in a data appendix. (NA)\n\u2022 All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (NA)\n\u2022 All datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations. (yes)\n\u2022 All datasets drawn from the existing literature (potentially including authors' own previously published work) are publicly available. (yes)\n\u2022 All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (yes)\nDoes this paper include computational experiments? (yes) If yes, please complete the list below.\n\u2022 Any code required for pre-processing data is included in the appendix. (yes).\n\u2022 All source code required for conducting and analyzing the experiments is included in a code appendix. (yes)\n\u2022 All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes)\n\u2022 All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes)\n\u2022 If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (yes)\n\u2022 This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (no)\n\u2022 This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (yes)\n\u2022 This paper states the number of algorithm runs used to compute each reported result. (yes)\n\u2022 The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (yes)\n\u2022 This paper lists all final (hyper-)parameters used for each model/algorithm in the paper's experiments. (no)\n\u2022 This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. (yes)"}]}