{"title": "FACING THE MUSIC: TACKLING SINGING VOICE SEPARATION IN CINEMATIC AUDIO SOURCE SEPARATION", "authors": ["Karn N. Watcharasupat", "Chih-Wei Wu", "Iroro Orife"], "abstract": "Cinematic audio source separation (CASS) is a fairly new subtask of audio source separation. A typical setup of CASS is a three-stem problem, with the aim of separating the mixture into the dialogue stem (DX), music stem (MX), and effects stem (FX). In practice, however, several edge cases exist as some sound sources do not fit neatly in either of these three stems, necessitating the use of additional auxiliary stems in production. One very common edge case is the singing voice in film audio, which may belong in either the DX or MX, depending heavily on the cinematic context. In this work, we demonstrate a very straightforward extension of the dedicated-decoder Bandit and query-based single-decoder Banquet models to a four-stem problem, treating non-musical dialogue, instrumental music, singing voice, and effects as separate stems. Interestingly, the query-based Banquet model outperformed the dedicated-decoder Bandit model. We hypothesized that this is due to a better feature alignment at the bottleneck as enforced by the band-agnostic FiLM layer.", "sections": [{"title": "1. INTRODUCTION", "content": "Cinematic audio source separation (CASS) is a relatively new subtask of audio source separation. To the best of our knowledge, most CASS works thus far have relied on the three-stem setup introduced in [1], with the goal of separating the mixture into the dialogue (DX), music (MX), and effects (FX) stem. Although this is already a very useful setup for many downstream tasks, certain cinematic audio production workflow requires more granular controls over the distribution of sound sources into stems, with a significant number of edge cases and contextual nuances that evade putting all cinematic sound sources into three clear-cut boxes. As a result, additional auxiliary stems \u00b9 are often needed in some content, to account for sound events such as walla, archival materials, and singing voice."}, {"title": "2. DATA", "content": "Most CASS works so far relied on the Divide and Remaster (DnR) v2 [1] dataset, which is a three-stem English-language dataset with the music stem drawn from the Free Music Archive (FMA) dataset [6]. The recently released multilingual rework (v3) of DnR [5] also drew the music stem from FMA. Since FMA does not provide clean isolated vocal and instrumental stems, it is not possible to cleanly obtain isolated vocal and instrumental ground truths; there is also no way of distinguishing between types of vocalization (e.g. speech vs singing) in FMA.\nAs a result, the dataset used in this work is an adaptation of DnR v3, with the music stems drawing from the music source separation (MSS) datasets MUSDB18-HQ [7] and MoisesDB [8] instead. We also removed all tracks and/or stems from these MSS datasets that have been indicated to contain bleed. The rest of the generation process is similar to that in [5], with the MX-I and MX-V stems temporally aligned."}, {"title": "3. SYSTEM", "content": "The systems used in this work are 4-stem variants of the 64-band musical Bandit model [3] and its query-based"}, {"title": "4. RESULTS", "content": "To evaluate the performance of the models, we compute full-track signal to noise ratio (SNR) and scale-invariant SNR (SI-SNR) [10] on each of the stems. The results indicated that the performance of\nthe model is within a similar range to the benchmark reported in [5]. Moreover, the model interestingly did not exhibit any overfitting behavior during training, despite MUSDB18-HQ and MoisesDB combined being significantly smaller than the subset of FMA used in [5].\nAcross the board, the Banquet model outperformed the Bandit model by 0.3 dB on FX and between 0.5 dB to 0.7 dB for other stems. Paired-sampled t-test all yielded $p < 10^{-4}$, indicating the statistical significance of these results. Paired-sample Cohen's $d$ values indicated that the\neffect sizes are small on the FX stems, small-to-moderate on the MX-I stem, and moderate-to-large on the MX-V and DX stems.\nAlthough both models consist of a single encoder responsible for computing the mixture embedding Y, Bandit has a dedicated decoder for each stem while Banquet has a single shared decoder. As a result, in Bandit, the \"separation\" happens in the band-wise decoding block that maps Ab to Mi,b, where i is the stem index and b is the band index. This map is nonlinear, likely allowing the representation Y to remain semantically entangled. In Banquet, the \"separation\" occurs in the band-agnostic FiLM layer. Given that FiLM is equivalent to an affine operation whose linear map is constrained to a diagonal matrix, Banquet likely encourages independence across features or groups of features. This hypothesis is partially supported by the cluster map of \u03b3 for each stem, z-normalized across stem for each feature, shown in Fig.2. Although the clustering is not fully obvious, it can be seen that most features are only activated for one or two of the stems, likely indicating that each abstract feature is responsible for a semantic concept specific to only one or two stems."}, {"title": "5. CONCLUSION", "content": "In this work, we demonstrate a straightforward extension of the Bandit and Banquet models for cinematic audio source separation that distinguishes between singing voice, dialogues, and instrumental music. Experimental results indicated that the query-based Banquet model performed significantly better despite only requiring half the parameters. We surmise that this may be due to better feature alignment. Additional investigation is required."}]}