{"title": "FACING THE MUSIC: TACKLING SINGING VOICE SEPARATION\nIN CINEMATIC AUDIO SOURCE SEPARATION", "authors": ["Karn N. Watcharasupat", "Chih-Wei Wu", "Iroro Orife"], "abstract": "Cinematic audio source separation (CASS) is a fairly new\nsubtask of audio source separation. A typical setup of\nCASS is a three-stem problem, with the aim of separat-\ning the mixture into the dialogue stem (DX), music stem\n(MX), and effects stem (FX). In practice, however, several\nedge cases exist as some sound sources do not fit neatly\nin either of these three stems, necessitating the use of ad-\nditional auxiliary stems in production. One very common\nedge case is the singing voice in film audio, which may\nbelong in either the DX or MX, depending heavily on the\ncinematic context. In this work, we demonstrate a very\nstraightforward extension of the dedicated-decoder Bandit\nand query-based single-decoder Banquet models to a four-\nstem problem, treating non-musical dialogue, instrumental\nmusic, singing voice, and effects as separate stems. In-\nterestingly, the query-based Banquet model outperformed\nthe dedicated-decoder Bandit model. We hypothesized that\nthis is due to a better feature alignment at the bottleneck as\nenforced by the band-agnostic FiLM layer.", "sections": [{"title": "1. INTRODUCTION", "content": "Cinematic audio source separation (CASS) is a relatively\nnew subtask of audio source separation. To the best of our\nknowledge, most CASS works thus far have relied on the\nthree-stem setup introduced in [1], with the goal of separat-\ning the mixture into the dialogue (DX), music (MX), and\neffects (FX) stem. Although this is already a very useful\nsetup for many downstream tasks, certain cinematic audio\nproduction workflow requires more granular controls over\nthe distribution of sound sources into stems, with a sig-\nnificant number of edge cases and contextual nuances that\nevade putting all cinematic sound sources into three clear-\ncut boxes. As a result, additional auxiliary stems \u00b9 are of-\nten needed in some content, to account for sound events\nsuch as walla, archival materials, and singing voice."}, {"title": "2. DATA", "content": "Most CASS works so far relied on the Divide and Remas-\nter (DnR) v2 [1] dataset, which is a three-stem English-\nlanguage dataset with the music stem drawn from the Free\nMusic Archive (FMA) dataset [6]. The recently released\nmultilingual rework (v3) of DnR [5] also drew the mu-\nsic stem from FMA. Since FMA does not provide clean\nisolated vocal and instrumental stems, it is not possible\nto cleanly obtain isolated vocal and instrumental ground\ntruths; there is also no way of distinguishing between types\nof vocalization (e.g. speech vs singing) in FMA.\nAs a result, the dataset used in this work is an adaptation\nof DnR v3, with the music stems drawing from the music\nsource separation (MSS) datasets MUSDB18-HQ [7] and\nMoisesDB [8] instead. We also removed all tracks and/or\nstems from these MSS datasets that have been indicated to\ncontain bleed. The rest of the generation process is similar\nto that in [5], with the MX-I and MX-V stems temporally\naligned."}, {"title": "3. SYSTEM", "content": "The systems used in this work are 4-stem variants of the\n64-band musical Bandit model [3] and its query-based"}, {"title": "4. RESULTS", "content": "To evaluate the performance of the models, we compute\nfull-track signal-to-noise ratio (SNR) and scale-invariant\nSNR (SI-SNR) [10] on each of the stems. The results\nare shown in Table 1. Although the MX stem contents\nare different, the results indicated that the performance of\nthe model is within a similar range to the benchmark re-\nported in [5]. Moreover, the model interestingly did not\nexhibit any overfitting behavior during training, despite\nMUSDB18-HQ and MoisesDB combined being signifi-\ncantly smaller than the subset of FMA used in [5].\nAcross the board, the Banquet model outperformed the\nBandit model by 0.3 dB on FX and between 0.5 dB to\n0.7 dB for other stems. Paired-sampled t-test all yielded\n$p < 10^{-4}$, indicating the statistical significance of these\nresults. Paired-sample Cohen's $d$ values indicated that the\neffect sizes are small on the FX stems, small-to-moderate\non the MX-I stem, and moderate-to-large on the MX-V and\nDX stems.\nAlthough both models consist of a single encoder re-\nsponsible for computing the mixture embedding Y, Bandit\nhas a dedicated decoder for each stem while Banquet has a\nsingle shared decoder. As a result, in Bandit, the \"separa-\ntion\" happens in the band-wise decoding block that maps\n$A_b$ to $M_{i,b}$, where $i$ is the stem index and $b$ is the band\nindex. This map is nonlinear, likely allowing the repre-\nsentation Y to remain semantically entangled. In Banquet,\nthe \"separation\" occurs in the band-agnostic FiLM layer.\nGiven that FiLM is equivalent to an affine operation whose\nlinear map is constrained to a diagonal matrix, Banquet\nlikely encourages independence across features or groups\nof features. This hypothesis is partially supported by the\ncluster map of $y\u00bf$ for each stem, z-normalized across stem\nfor each feature, shown in Fig.2. Although the clustering is\nnot fully obvious, it can be seen that most features are only\nactivated for one or two of the stems, likely indicating that\neach abstract feature is responsible for a semantic concept\nspecific to only one or two stems."}, {"title": "5. CONCLUSION", "content": "In this work, we demonstrate a straightforward extension\nof the Bandit and Banquet models for cinematic audio\nsource separation that distinguishes between singing voice,\ndialogues, and instrumental music. Experimental results\nindicated that the query-based Banquet model performed\nsignificantly better despite only requiring half the param-\neters. We surmise that this may be due to better feature\nalignment. Additional investigation is required."}]}