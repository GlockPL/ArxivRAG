{"title": "Human Bias in the Face of AI: The Role of Human Judgement in AI Generated Text Evaluation", "authors": ["Tiffany Zhu", "Iain Weissburg", "Kexun Zhang", "William Yang Wang"], "abstract": "As AI advances in text generation, human trust in AI generated content remains constrained by biases that go beyond concerns of accuracy. This study explores how bias shapes the perception of AI versus human generated content. Through three experiments involving text rephrasing, news article summarization, and persuasive writing, we investigated how human raters respond to labeled and unlabeled content. While the raters could not differentiate the two types of texts in the blind test, they overwhelmingly favored content labeled as \u201cHuman Generated,\" over those labeled \u201cAI Generated,\" by a preference score of over 30%. We observed the same pattern even when the labels were deliberately swapped. This human bias against AI has broader societal and cognitive implications, as it undervalues AI performance. This study highlights the limitations of human judgment in interacting with AI and offers a foundation for improving human-AI collaboration, especially in creative fields.", "sections": [{"title": "Introduction", "content": "With the increasing accessibility of large language models (LLMs), like OpenAI's ChatGPT [13], Meta's LLaMA [20], and Anthropic's Claude [1], generative artificial intelligence's capabilities are being utilized in expansive tasks. Their sophisticated abilities in text generation raise both excitement and concern regarding the potential for these systems to displace human workers [21]."}, {"title": "Methods", "content": "Our data collection included three datasets available through Hugging Face [8]. The first dataset [16] is for the text rephrasing scenario. It has human written texts from the introductory sections of Wikipedia articles. The AI models were provided with the human written introductions and asked to rephrase them. The dataset for the summarization scenario [18, 7] contains full-length news articles from news sites like CNN and Daily Mail, alongside 2-3 sentence long human written highlights. Provided with the news articles, the Al models were instructed to write summaries. For the persuasion scenario, the dataset [3] contains various controversial topics, including sustainability policies, ethical dilemmas, etc., such as \"Banning gas car sales too soon is unrealistic,\" and \"Social media should verify user identities.\" Each topic has a corresponding human written persuasive paragraph, and the AI models were instructed to write persuasive articles with logical reasoning.\nFrom each dataset, we selected 200 entries uniformly at random. We then sent each entry to three major LLMs ChatGPT-4 (ChatGPT), Claude 2 (Claude), and Llama 3.1 405B Instruct (Llama) to collect their responses. Using multiple models introduced a broader range of AI-generated outputs while reducing the peculiarities of each model. In total, for each dataset, we collected 600 responses from the three AI models. To limit discrepancy, the LLMs were instructed to generate content with lengths close to the average word counts of human written content."}, {"title": "Experiment Design", "content": "We used Amazon Mechanical Turk (MTurk) [22] to gather the preferences of people from predominantly English speaking nations. MTurk was suitable because the demographic and socioeconomic profiles of workers closely mirror the general public, and workers find MTurk tasks low stress [11]. For our experiment, we had 600 entries (called tasks) from each of the three scenarios for different workers (called taskers) to cast their preferences.\nWe asked a different guiding question pertaining to each scenario. For the text rephrasing scenario, taskers were given a human written Wikipedia introductory paragraph and an AI rephrased version. The guiding question was: \"Which paragraph is better in terms of readability?\" For the summarization scenario, taskers were provided with a full article and two summaries generated by human and AI respectively. The guiding question was: \u201cWhich summary of the provided article presents the most relevant information in a clear manner?\" For the persuasion scenario, taskers were presented with an argument and two persuasive texts, one generated by human and the other by AI. The guiding question was: \"Which persuasive paragraph supports the argument more?\u201d\nOur goal was to assess whether knowing the source of the generated texts influenced taskers' preferences when the content remained the same. We conducted two tests to identify people's preferences: a blind test and a manipulated test.\nIn the blind test, taskers were given the texts with randomly assigned labels \u201c1\u201d and \u201c2.\u201d They were first asked to guess which text is more likely to be generated by human, and then choose their preferences. We call this experiment Blind-Labeled or \"No Label\" and used it to check if taskers can tell apart AI and human generated texts. If they cannot differentiate them, we then use the manipulated test to uncover their biases.\nThe manipulated test contains two experiments: \u201cCorrectly Labeled\u201d and \u201cWrongly Labeled'.' In correctly labeled, the texts were explicitly and correctly labeled with \u201cHuman Generated\" or \"AI Generated.\" The taskers were asked to choose their preference with the labels given. In wrongly labeled, taskers were given swapped labels. Unknown to the taskers, the AI generated text was labeled as \u201cHuman Generated,\" and vice versa. They were then asked to choose their preferences.\nEach experiment had 600 tasks to evaluate. Each task was assigned three taskers to work on, so we collected three \"hits\" per task. Overall, the total number of hits we collected was 3 \u00d7 3 \u00d7 600 x 3 = 16200. Each tasker could evaluate up to three tasks in an experiment. To avoid seeing duplicate content, a tasker was blocked from working in more than one experiment for any scenario."}, {"title": "Data Analysis", "content": "The survey result data was grouped by scenario, experiment, and LLM. We first assigned scores to rater choices for each task. Each rater choice is worth 1 point. For example, if two of the three taskers choose \"Human Generated,\" then for that task, \"Human Generated\" label gets a score 2 and \"AI Generated\" gets a score 1. We then summed up the scores for each label across all 600 tasks in an experiment and used the average as the score for that label in the experiment. A score of about 1.5 for a label means that the taskers could not differentiate the texts from the two labels.\nOur main goal was to study user preference change when presented texts with different labels. Hence, our findings reported in the next section will be centered around the labeling conditions. To study if the preference changes across labeling conditions were significant, we also report p-values from the Brunner-Munzel test [17]."}, {"title": "Results", "content": "Taskers fail to differentiate between human and AI generated texts. In the blind-labeled experiments, given a pair, taskers were asked to guess which text was written by human. Averaging the 600 tasks, we found that, for the text rephrasing scenario, 49.93% taskers responded incorrectly, i.e. they believed the AI rephrased texts were human generated. For the summarization scenario, the average was slightly lower at 43.1%, but in the persuasion scenario, we again got a near-equal split of 50.06%.\nIn blind tests, taskers slightly prefer AI texts. Across all three scenarios, the taskers preferred AI generated text slightly more than the human generated text in the blind-labeled experiments. The AI text had a score of 1.537 for the rephrasing scenario, 1.650 for summarization, and 1.534 for persuasion, all greater than the 1.50 halfway point.\nTaskers prefer text labeled \u201cHuman Generated\" even when wrongly classified. Compared to the blind-labeled experiments, the score for human generated texts in correctly labeled experiments increased by 32.9% to 1.945 in the text rephrasing scenario, 35.1% in the summarization scenario, and 26.1% in the persuasion scenario, showing that the taskers strongly prefer \"Human Generated\u201d texts. On the other hand, for the wrongly labeled experiments, the scores for the texts labeled \"Human Generated,\" but are actually \"AI Generated,\u201d also increased significantly compared to the blind-labeled cases. For the text rephrasing scenario, the score for AI texts increased from 1.537 to 2.052 when they were labeled as \"Human Generated.\u201d Consequently, the score for human texts dropped by 35.2% from 1.463 to 0.948. Similar results happened for the summarization and persuasion scenarios as well with drops of 47.8% and 36.8%, respectively.\nWe conducted the Brunner-Munzel test [17] to compare distributions between pairs of labeling conditions in each scenario. Our null hypothesis stated that the distributions of the two groups being compared are identical. In all cases, the p-value was less than 0.05, indicating we could reject our null hypothesis and the distributions between pairs of groups differ significantly."}, {"title": "Discussion and Conclusion", "content": "The fact that taskers had an almost equal split in labeling AI generated text as human and vice versa shows that, with LLMs so advanced, humans cannot distinguish between human written and AI generated text. This is consistent with prior research, in which one study found that linguists could only correctly identify AI generated academic writing 38.9% of the time [2]. Our results extend these findings to three other scenarios as well, suggesting that the challenge of differentiation remains true for all types of writing.\nOur study also found that when explicitly labeled, people prefer \"Human Generated\u201d texts much more than \"AI Generated.\" One reason for the preference change is that people may mistrust LLMs due to their tendency to \u201challucinate,\" outputting inaccurate or misleading information. While this might be the case for the persuasion scenario, where the LLMs have more creative freedom for argumentative writing, hallucinations are unlikely to happen in the rephrasing and summarization scenarios where potential for creativity is more limited. For the two scenarios, we manually examined over 20 output samples from each LLM and did not notice hallucinations. We therefore conclude, supported by the significant p-values, that \u201chuman bias\" is playing a role against AI generated contents even when the content is fact-based.\nThe biases not only hinder widespread AI deployments and applications but could also impact AI alignment in systems that use Reinforcement Learning from Human Feedback (RLHF). RHLF uses human feedback to refine models like InstructGPT [14], but if humans favor content perceived as human generated, AI systems may be trained to produce content that aligns with these already biased expectations.\nWe propose a few solutions to increase human trust in AI. First, we believe developing transparent and explainable AI is essential to gaining human's trust. With better understanding of how LLMs produces their output, as we do with mathematical calculations by computers, humans will be more likely to accept LLMs as an indispensable assistant for creative works. Second, Al's role should be delineated as an assistant and collaborator, not a replacement or competitor. People may resist AI if it is perceived as a threat to expertise. Lastly, AI should be deployed gradually and steadily. We believe AI systems should be initially introduced to low-stakes environments where the perceived threat is minimal. Over time, as people see AI perform well in these contexts, they will be more comfortable with its use in critical situations.\nWhile our research identified human bias as a major factor against AI content, we still need to pinpoint the source of human bias. We suspect \"human ego\" is playing a role. Humans are not ready to give up creativity and intellectual superiority to computers and AI, and therefore do not view AI content favorably. This hypothesis will require further investigation."}, {"title": "Limitations", "content": "One limitation in this study is that the data is collected from MTurk workers. Although the demographic is similar to the English-speaking public, specific biases may be inherent in this group, like their motivation for completing the hits.\nA second limitation was that only three scenarios were selected. Other creative domains, like story writing and problem solving, can be further explored to gain a fuller understanding of the role of human bias in people's acceptance of AI."}, {"title": "Ethical Considerations", "content": "This study qualifies as Exempt from the Institutional Review Board under category 3: Benign Behavioral Interventions. MTurk taskers received compensation for their hits above the federal minimum wage.\nThe \"Wrongly Labeled\u201d condition, where AI generated content was labeled as \u201cHuman Generated\" and vice versa, does not pose ethical concerns for several reasons. First, this type of intervention is considered minimal risk in social science research since it does not expose participants to any physical, psychological, or emotional harm. The intervention is brief and necessary to understand how labeling conditions affects perceptions of text quality. This could not be accurately measured if participants were aware of the text origins.\nFurthermore, the potential benefits of this research outweigh the minimal risks. By uncovering biases in how people perceive AI generated content, this study contributes valuable insights for the development and integration of AI into various fields. Understanding these biases is important for addressing potential barriers to AI adoption. Additionally, it could help develop strategies to mitigate unwarranted prejudices against AI generated content. This study's findings can lead to more effective human-AI collaboration and better decision-making processes in contexts where AI-generated content is increasingly prevalent. Therefore, the temporary and harmless intervention used in this study is justified by the significant potential benefits to the advancement of our understanding of human-AI interactions."}]}