{"title": "On the inductive bias of infinite-depth ResNets and the bottleneck rank", "authors": ["Enric Boix-Adsera"], "abstract": "We compute the minimum-norm weights of a deep linear ResNet, and find that the inductive bias\nof this architecture lies between minimizing nuclear norm and rank. This implies that, with appropriate\nhyperparameters, deep nonlinear ResNets have an inductive bias towards minimizing bottleneck rank.", "sections": [{"title": "Introduction", "content": "There are three key elements that affect the performance of a trained neural network: the data, the optimizer,\nand the architecture. This work focuses on the architecture specifically, the deep ResNet architecture\n[HZRS16], which is a core component of modern neural networks, including transformers [VSP+17]. We\nanalyze its inductive bias, which we define as follows. Given a neural network architecture $f_\\theta : \\mathcal{X} \\rightarrow \\mathcal{Y}$\nparametrized by weights $\\theta$, the cost of representing a function $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ is the minimum norm of the\nweights required to represent it:\n\n$\\text{cost}(f) = \\min{\\{\\Vert \\theta \\Vert \\} \\text{ such that } f_\\theta = f\\}} .$\n\nWhen the network is trained with weight decay or $L_2$ weight regularization, the training procedure is biased\ntowards outputting functions with lower cost. This inductive bias is determined by the neural network\nparametrization $f_\\theta$ (i.e., its architecture). For instance, deep linear networks favor learning low-rank linear\ntransformations [GWB+17, ACHL19], linear convolutional networks favor Fourier-sparse solutions [GLSS18],\nand diagonal linear networks favor sparse solutions [GLSS18].\n The result of [Jac22] on the inductive bias of deep nonlinear networks is particularly relevant to this paper,\nand forms the basis of our investigations. [Jac22] defines the \"bottleneck rank\", a notion of rank for nonlinear\nfunctions. Roughly speaking, the bottleneck rank of a function $f: \\mathbb{R}^{n_1} \\rightarrow \\mathbb{R}^{n_2}$ is the smallest dimension $k$\nsuch that can be written as $f = g \\circ h$ for two functions $g : \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{n_2}$ and $h : \\mathbb{R}^{n_1} \\rightarrow \\mathbb{R}^{k}$ (where $g, h$ satisfy\nregularity conditions). The papers [Jac22, Jac23] show that deep networks with nonlinear fully-connected\nlayers have an inductive bias towards learning functions with a low bottleneck rank. This result has been\nextended to leaky-ResNets [JK24] and to convolutional networks [WJ24] (but the latter requires a modified\nnotion of bottleneck rank).\n The arguments for the bottleneck rank are based on the observation that the minimum-cost network\nrepresenting function $f$ has the first layers represent $h$, the last layers represent $g$, and the vast majority of\nthe layers in the middle represent the identity map on a subspace of dimension $k$. When composed together,\nthese layers yield the function $f$. Since most layers are dedicated to representing the identity map, this\ndominates the cost when the architecture does not have skip connections or only has leaky skip connections.\nIt follows that fully-connected networks and leaky-ResNets have an inductive bias for low bottleneck rank.\nIn contrast, ResNets explicitly incorporate skip connections that trivialize representing the identity func-\ntion, raising the question of whether the bottleneck rank framework is still relevant to ResNet-based models\nwhich are used in practice:"}, {"title": "The inductive bias of linear residual networks lies between mini- mizing nuclear norm and minimizing rank", "content": "In this section, we consider linear residual networks with $L$ layers, as well as embedding and unembedding\nmatrices $W_e \\in \\mathbb{R}^{n \\times d_{in}}$ and $W_u \\in \\mathbb{R}^{d_{out} \\times n}$. These correspond to residual networks with MLPs at each layer,\nwith identity activation function. Because the activation function is linear, these networks can only represent\nlinear functions. For simplicity, we will suppose $n \\geq \\max(d_{in}, d_{out})$.\nArchitecture We study two variants of linear residual networks. In the first variant, the network has\ndepth-1 residual blocks $W_1, ..., W_L \\in \\mathbb{R}^{n \\times n}$.\n\n$f_{\\text{1-lin}}(W_u, W_e, \\{W_i\\}_{i \\in [L]}) = W_u (I + W_1) (I + W_2) ... (I + W_L) W_e \\in \\mathbb{R}^{d_{out} \\times d_{in}}.$\n\nIn the second variant, the network has depth-2 residual blocks $W_{1,1}, W_{1,2}, W_{2,1}, W_{2,2}, ..., W_{L,1}, W_{L,2} \\in \\mathbb{R}^{n \\times n}$,\nas is the case in the MLP layers in transformer architectures:\n\n$f_{\\text{2-lin}} (W_u, W_e, \\{W_{i,j}\\}_{i \\in [L], j \\in [2]}) = W_u (I + W_{1,2}W_{1,1}) (I + W_{2,2}W_{2,1}) ... (I + W_{L,2}W_{L,1}) W_e \\in \\mathbb{R}^{d_{out} \\times d_{in}}.$\n\nCost of network We define the cost for a certain choice of parameters to be the sum of the squared\nFrobenius norms of the weights, since this is what is penalized when training with weight-decay or explicit\n$L_2$ regularization. For any matrix $A \\in \\mathbb{R}^{n \\times n}$ and a parameter $\\lambda \\in (0, \\infty)$ we define the cost of representing\na linear transformation $A \\in \\mathbb{R}^{d_{out} \\times d_{in}}$ with a network with depth-1 blocks by:\n\n$c_{L, n, \\lambda}^{\\text{1-lin}}(A) = \\min_{W_u, W_e, \\{W_i\\}_{i \\in [L]}:\\atop f_{\\text{1-lin}}(W_u, W_e, \\{W_i\\}) = A} \\{ \\frac{1}{L} \\Vert W_u \\Vert_F^2 + \\frac{1}{L} \\Vert W_e \\Vert_F^2 + \\frac{\\lambda}{L} \\sum_{i \\in [L]} \\Vert W_i \\Vert_F^2 \\},\\$\nand by a network with depth-2 blocks by:\n\n$c_{L, n, \\lambda}^{\\text{2-lin}}(A) = \\min_{W_u, W_e, \\{W_{i,j}\\}_{i \\in [L], j \\in [2]}:\\atop f_{\\text{2-lin}}(W_u, W_e, \\{W_{i,j}\\}) = A} \\{ \\frac{1}{L} \\Vert W_u \\Vert_F^2 + \\frac{1}{L} \\Vert W_e \\Vert_F^2 + \\frac{\\lambda}{L} \\sum_{i \\in [L], j \\in [2]} \\Vert W_{i,j} \\Vert_F^2 \\}.$\n\nThe parameter $\\lambda \\in (0, \\infty)$ allows us to interpolate between weighting the cost of the embedding and unem-\nbedding layers, versus the cost of the residual layers in the network."}, {"title": "Theorem 2.1 (Explicit formula for cost in linear case)", "content": "For any linear transformation $A \\in \\mathbb{R}^{d_{out} \\times d_{in}}$, any\nparameter $\\lambda \\in (0, \\infty)$, any depth $L \\geq 1$, and any width $n \\geq \\text{rank}(A)$, we have\n\n$c_{L, n, \\lambda}^{\\text{1-lin}}(A) = c_{L, n, \\lambda}^{\\text{2-lin}}(\\{ \\sigma_i(A) \\}) = \\sum_{i=1}^n \\min_{\\alpha \\in \\mathbb{R}_{>0}} \\{ \\frac{\\sigma_i(A)}{(1 + \\alpha/L)^L} + \\lambda \\alpha^2 \\}$     (2.1)\n\n$c_{L, n, \\lambda}^{\\text{1-lin}}(A) = \\sum_{i=1}^n \\Gamma_n^{\\text{1-lin}}(\\{ \\sigma_i(A) \\}) = \\sum_{i: \\sigma_i(A) < \\lambda} \\sigma_i(A) + \\sum_{i: \\sigma_i(A) > \\lambda} \\lambda \\big( (L + 1) ((\\sigma_i(A) / \\lambda)^{1/(L+1)} - 1) \\big).$    (2.2)\n\nThe proof is deferred to Section 2.1. As a corollary of this result, we derive the cost in the limit of infinite\ndepth.\nCorollary 2.2 (Cost of infinite-depth linear residual network). Under the conditions of Theorem 2.1,\n\n$\\begin{aligned} c_{\\lambda}^{\\text{1-lin}} &:= \\lim_{L \\rightarrow \\infty} c_{L, n, \\lambda}^{\\text{1-lin}}(A) = \\sum_{i=1}^n \\min_{\\alpha \\in \\mathbb{R}_{>0}} \\{ \\frac{\\sigma_i(A)}{e^\\alpha} + \\lambda \\alpha^2 \\} \\\\\nc_{\\lambda}^{\\text{2-lin}} &:= \\lim_{L \\rightarrow \\infty} c_{L, n, \\lambda}^{\\text{2-lin}}(A) = \\sum_{i: \\sigma_i(A) < \\lambda} \\sigma_i(A) + \\sum_{i: \\sigma_i(A) > \\lambda} \\lambda \\big( 1 + \\log(\\sigma_i(A) / \\lambda) \\big). \\end{aligned}$$\n\nOne can see that $\\alpha_L < C_{\\sigma, \\lambda}$ for a constant $C_{\\sigma, \\lambda}$. Therefore $\\lim_{L \\rightarrow \\infty} \\big\\vert e^{\\alpha_L} - (1 + \\alpha/L)^L \\big\\vert = 0$.\n\nFor $c_{\\lambda}^{\\text{2-lin}}(A)$, this is a direct calculation, using that $(L + 1) ((\\sigma/\\lambda)^{1/(L+1)} - 1) \\rightarrow \\log(\\sigma/\\lambda)$.\n These expressions are still fairly complicated, so let us analyze the asymptotics of these costs, as either\n$\\lambda \\rightarrow 0$ and the embedding/unembedding becomes the dominating term in the cost, or as $\\lambda \\rightarrow \\infty$ and the\nresidual part of the network becomes the dominating term in the cost.\nCorollary 2.3 (Cost of infinite-depth residual network interpolates between nuclear norm and rank). Let\n$A \\in \\mathbb{R}^{d_{out} \\times d_{in}}$, and $n \\geq \\text{rank}(A)$.\n\n*   Nuclear norm minimization for large $\\lambda$. Taking $\\lambda \\rightarrow \\infty$ recovers the nuclear norm $\\Vert A \\Vert_*$.\n    $\\lim_{\\lambda \\rightarrow \\infty} c_{\\lambda}^{\\text{1-lin}}(A) = \\lim_{\\lambda \\rightarrow \\infty} c_{\\lambda}^{\\text{2-lin}}(A) = \\Vert A \\Vert_*$         (2.3)\n*   Rank minimization for small $\\lambda$. Taking $\\lambda \\rightarrow 0$ leads to a cost that is, to first order, the rank of A.\n\n$\\lim_{\\lambda \\rightarrow 0} \\frac{c_{\\lambda}^{\\text{1-lin}}(A)}{\\lambda \\log(1/\\lambda)^2} = \\lim_{\\lambda \\rightarrow 0} \\frac{c_{\\lambda}^{\\text{2-lin}}(A)}{\\lambda \\log(1/\\lambda)} = \\text{rank}(A)$    (2.4)\n\nFor any $\\sigma$, we have $\\lim_{\\lambda \\rightarrow \\infty} (\\min_{\\alpha \\in \\mathbb{R}_{>0}} \\sigma / \\exp(\\alpha) + \\lambda \\alpha^2) = \\sigma$, proving\nthe formula for depth-1 blocks. For depth-2 blocks, we have $c_{\\lambda}^{\\text{2-lin}}(A) = \\sum_i \\sigma_i(A) = \\Vert A \\Vert_*$ for large enough\n$\\lambda$.\n\nNow consider the limit $\\lambda \\rightarrow 0$. For any $\\sigma$, we have\n\n$\\lim_{\\lambda \\rightarrow 0} \\frac{(\\min_{\\alpha \\in \\mathbb{R}_{>0}} \\frac{\\sigma}{\\exp(\\alpha)} + \\lambda \\alpha^2)}{\\lambda \\log(1/\\lambda)^2} = \\lim_{\\lambda \\rightarrow 0} \\frac{(\\min_{\\beta \\in \\mathbb{R}_{>0}} \\frac{\\lambda \\sigma}{\\exp(\\beta)} + (\\beta + \\log(1/\\lambda))^2)}{\\lambda \\log(1/\\lambda)^2} = \\begin{cases} 1, & \\text{if } \\sigma \\neq 0 \\\\ 0, & \\text{if } \\sigma = 0 \\end{cases}$\n\nTherefore, finding the smallest-cost linear residual network that fits the data corresponds to finding\nthe minimum rank linear transformation that fits the data (for very small $\\lambda < 1$), and to the minimum\nnuclear-norm linear transformation (for very large $\\lambda \\gg 1$)."}, {"title": "Proof of Theorem 2.1", "content": "Each of the equalities in Theorem 2.1 has two parts: an upper bound, and a lower bound. The \"easier\"\ndirection is the upper bound, which we will show via a direct construction of a neural network with the\ncorrect cost. The \"harder\" direction is the lower bound, and here the essential ingredient is the following\ninequality of [GN50] concerning the singular values of products of matrices.\nProposition 2.4 (Proved in [GN50]; see Corollary 2.4 of [LM99] or Theorem III.4.5 in [Bha96]). For any\nmatrices $A \\in \\mathbb{R}^{n_0 \\times n}$, $C \\in \\mathbb{R}^{n \\times n_1}$, $B \\in \\mathbb{R}^{n \\times n}$, and any $k \\in \\{1, ..., n\\}$ and any indices $1 < i_1 < ... i_k \\leq n$ such\nthat $\\sigma_{i_j}(B) \\neq 0$, we have\n\n$\\prod_{j=1}^k \\frac{\\sigma_{i_j}(ABC)}{\\sigma_{i_j}(B)} \\leq \\prod_{j=1}^k \\sigma_{i_j}(A), \\sigma_{i_j}(C).$\n\nThis, in turn, implies the following lemma.\nLemma 2.5. For any $A \\in \\mathbb{R}^{N_1 \\times n_2}$, $B \\in \\mathbb{R}^{N_2 \\times n_3}$, and convex non-decreasing $g : [0, \\infty) \\rightarrow \\mathbb{R}$, we have\n\n$\\sum_{i=1}^{\\text{rank}(AB)} g(\\sigma_i(A)) \\geq \\sum_{i=1}^{\\text{rank}(AB)} g(\\sigma_i(AB)/\\sigma_i(B))$\n\nLet $k = \\text{rank}(AB)$, and for any $i \\in [k]$ define $r_i = \\sigma_i(B)/\\sigma_i(AB)$. Order these ratios as $r_{[1]} \\geq r_{[2]} \\geq\n... \\geq r_{[k]}$. By Proposition 2.4, for any $1 \\leq j \\leq \\text{rank}(AB)$, we have $\\sum_{i=1}^j \\log(\\sigma_i(A)) \\geq \\sum_{i=1}^j \\log(r_{[i]})$. In\nother words, $\\log(\\sigma_i(A))$ weakly submajorizes $\\log(r_{[i]})$. Since the function $t \\rightarrow g(\\exp(t))$ is convex and non-\ndecreasing, by fact 3.C.1.b in [MOA11], we have $\\sum_{i=1}^k g(\\sigma_i(AB)) \\geq \\sum_{i=1}^k f(r_{[i]}) = \\sum_{i=1}^k g(\\sigma_i(AB)/\\sigma_i(B))$.\n This result allows us to determine the minimum cost of networks with depth-1 blocks, and any increasing,\nconvex costs on the weight matrices in the residual layers.\nLemma 2.6. For any $A \\in \\mathbb{R}^{d_{in} \\times d_{out}}$, any depth $L$, width $n \\geq \\text{rank}(A)$, and convex, increasing $f : [0, \\infty) \\rightarrow \\mathbb{R}$\nwith $f(0) = 0$, define\n\n$c_{L, n}^{\\text{c-lin}}(A; f) := \\min_{W_u, W_e, \\{W_i\\}_{i \\in [L]}:\\atop f_{\\text{1-lin}}(W_u, W_e, \\{W_i\\}) = A} \\{ \\frac{1}{L} \\Vert W_u \\Vert_F^2 + \\frac{1}{L} \\Vert W_e \\Vert_F^2 + \\frac{1}{L} \\sum_{i \\in [L]} \\sum_{j \\in [n]} f(\\sigma_j(W_i)) \\}.$\n\nThen the cost decomposes additively across the singular values:\n\n$c_{L, n}^{\\text{c-lin}}(A; f) := \\sum_{i=1}^{\\text{rank}(A)} c_{L, 1}^{\\text{c-lin}}(\\{\\sigma_i(A)\\} ; f) = \\sum_{i=1}^{\\text{rank}(A)} \\min_{\\alpha \\in \\mathbb{R}_{>0}} (\\frac{\\sigma_i(A)}{(1 + \\alpha)^L} + Lf(\\alpha))$\n\nFor the upper bound, let $r = \\min(d_{out}, d_{in})$, and write the SVD decomposition\n$A = U \\text{diag}(\\sigma(A)) V^T$ where $U \\in \\mathbb{R}^{d_{out} \\times r}$, $V \\in \\mathbb{R}^{r \\times d_{in}}$ are semi-orthogonal matrices. Then, for some\n$\\alpha \\in \\mathbb{R}_{>0}$, let\n\n$W_u = U \\begin{bmatrix} \\text{diag}(\\sqrt{\\frac{\\sigma(A)}{(1 + \\alpha)^L}}) & \\\\ 0 & \\end{bmatrix} \\in \\mathbb{R}^{d_{out} \\times n}, W_e = \\begin{bmatrix} \\text{diag}(\\sqrt{\\frac{\\sigma(A)}{(1 + \\alpha)^L}}) \\\\ 0 \\end{bmatrix} V^T \\in \\mathbb{R}^{n \\times d_{in}}, W_i = \\begin{bmatrix} \\text{diag}(\\alpha) & \\\\ 0 & \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}.$\n\nBy construction, for any choice of $\\alpha$ we have $f_{\\text{1-lin}}(W_u, W_e, \\{W_i\\}) = A$, and therefore\n\n$c_{L, n}^{\\text{c-lin}}(A; f) < \\min_{\\alpha \\in \\mathbb{R}_{>0}} \\sum_{i=1}^{\\text{rank}(A)} (\\frac{\\sigma_i(A)}{(1 + \\alpha)^L} + Lf(\\alpha)) = \\sum_{i=1}^{\\text{rank}(A)} \\min_{\\alpha \\in \\mathbb{R}_{>0}} (\\frac{\\sigma_i(A)}{(1 + \\alpha)^L} + Lf(\\alpha)).$\n\nIt remains to prove the lower bound, which is the \"harder\" direction of this lemma. Suppose\nthat $W_u, W_e, \\{W_i\\}$ achieve the minimum cost subject to $f_{\\text{1-lin}}(W_u, W_e, \\{W_i\\}) = A$. For convenience, define"}, {"title": "Lemma 2.5 with g(t) = f(t), we have", "content": "For any integer\n$m \\geq 1$, we will show how to simulate this in the first $mL_1$ layers of the ResNet. Let $S_0, ..., S_{L_1} \\subseteq [n]$ be\ndisjoint of size $|S| = n$. The embedding weights are $b_e = 0$ and $[W_e]_{S_0 \\times [d_{in}]} = \\sqrt{\\lambda}V_e$, and 0 everywhere else.\nFor any $i \\in [L_1], j \\in [m]$, let $[W_{(i-1)m+j}]_{S_i \\times S_{i-1}} = V_i/m$ and 0 everywhere else. Let $[b_{(i-1)m+j}]_{S_i} = a_i\\sqrt{\\lambda}$,\nand 0 everywhere else. Define\n\n$h_1 := f_{mL_1} \\circ f_{mL_1-1} \\circ \\cdots \\circ f_1 \\circ f_e$\n\nand let $T \\subset S_{L_1}$ be the first $k$ coordinates of $S_{L_1}$. This has the property that, for all $x \\in \\Omega$ we have\n\n$[h_1(x)] = \\sqrt{\\lambda}h_1(x)$.\nAnd for all $i \\notin S_0 \\cup ... \\cup S_{L_1}$,\n\n$[h_1(x)]_i = 0.$\nFurthermore the total cost of these first $mL_1$ layers and embedding layer is at most\n\n$\\frac{1}{2} [ \\Vert W_e \\Vert_F^2 ] + \\lambda L \\sum_{i=1}^{mL_1} ||W_i|| = ||V_e||^2 + \\lambda L \\sum_{i=1}^{mL_1} \\Vert V_{[i/m^2]} \\Vert_F^2 / m^2 < \\lambda C_1(1 + L/m),$\nIntermediate residual layers. Next, in the intermediate $L_{int}$ layers, for any scalar $\\tau > 1$ we can choose\nthe weights so that\n\n$h_{int} := f_{L_1+L_{int}} \\circ f_{L_1+L_{int}-1} \\circ \\cdots \\circ f_{L_1+1}$"}, {"title": "3 Nonlinear residual networks and the bottleneck rank", "content": "We now consider nonlinear residual networks with ReLU activations $\\sigma(t) = \\max(0,t)$. These architectures\nrepresent functions $f : \\mathbb{R}^{d_{in}} \\rightarrow \\mathbb{R}^{d_{out}}$, and are parametrized by $\\theta$. We again study both depth-1 and depth-2\nblock networks, since they are the most popular variants.\nFor depth-1 block residual networks, the network $f_{\\text{1-nonlin}}(\\cdot; \\theta)$ is parametrized by embedding/unembedding\nweights and biases are $W_u \\in \\mathbb{R}^{d_{out} \\times n}, W_e \\in [\\mathbb{R}^{n \\times d_{in}}, b_u \\in \\mathbb{R}^{n}, b_e \\in \\mathbb{R}^{d_{out}}$, and the residual weights and biases\nare $W_i \\in \\mathbb{R}^{n \\times n}, b_i \\in \\mathbb{R}^{n}$. The network is given by\n\n$f_{\\text{1-nonlin}}(x; \\theta) = (f_u \\circ (id + f_L) \\circ (id + f_{L-1}) \\circ \\cdots \\circ (id + f_1) \\circ f_e)(x),$\n\nwith unembedding $f_u(z; \\theta) = b_u + W_uz, embedding $f_e(z; \\theta) = b_e + W_ez,$\nand internal layers $f_i(z; \\theta) = \\sigma(W_iz + b_e)$.\nDepth-2 block residual networks are the same, except that the residual weights and biases are parametrized\nby $W_{i,j} \\in \\mathbb{R}^{n \\times n}, b_{i,j} \\in \\mathbb{R}^{n}$, and each of the internal layers is a depth-2 network:\n\n$f_{\\text{2-nonlin}}(x; \\theta) = (f_u \\circ (id + f_L) \\circ (id + f_{L-1}) \\circ \\cdots \\circ (id + f_1) \\circ f_e)(x),$\n\nwith unembedding $f_u(z; \\theta) = b_u + W_uz, embedding $f_e(z; \\theta) = b_e + W_ez,$\nand internal layers $f_i(z; \\theta) = W_{e,2} \\sigma(W_{e,1}z + b_{e,1}) + b_{e,2}$.\nCost of representing FPLFs Given a finite piecewise linear function (FPLF) $g : \\Omega \\rightarrow \\mathbb{R}^{n_1}$ on a bounded\nset $\\Omega \\subseteq \\mathbb{R}^{n_0}$, we define the minimum cost of representing this function with depth-1 blocks:\n\n$c_{L, n, \\lambda}^{\\text{c-nonlin}}(g; \\Omega) = \\inf_{\\theta} \\{ \\frac{1}{L} \\Vert W_u \\Vert_F^2 + \\frac{1}{L} \\Vert W_e \\Vert_F^2 + \\frac{\\lambda L}{2} \\sum_{i \\in [L]} \\Vert W_i \\Vert : f_{\\text{1-nonlin}}(x; \\theta) = g(x) \\text{ for all } x \\in \\Omega \\}.$\n\nAnd we define the cost with a network with depth 2-blocks:\n\n$c_{L, n, \\lambda}^{\\text{c-nonlin}}(g; \\Omega) = \\inf_{\\theta} \\{ \\frac{1}{L} \\Vert W_u \\Vert_F^2 + \\frac{1}{L} \\Vert W_e \\Vert_F^2 + \\frac{1}{2} \\sum_{i \\in [L], j \\in [2]} \\Vert W_{i,j} \\Vert : f_{\\text{2-nonlin}}(x; \\theta) = g(x) \\text{ for all } x \\in \\Omega \\}.$\n\nThe scaling of the cost with $L$ is taken to be the same as in the case of linear networks, so as to be in the\ncritical scaling with nontrivial behavior when we send $L \\rightarrow \\infty$. We do not penalize the biases for simplicity,\nas this does not qualitatively change the nature of the results."}, {"title": "Nonlinear rank as \u03bb \u2192 0", "content": "We study the minimum-norm solutions in the limit of taking the hyperparameter $\\lambda \\rightarrow 0$. We will show that\nthe bias of these networks is towards finding a solution that minimizes the nonlinear rank, similarly to what\nwas shown in [Jac22, Jac23] for deep fully-connected networks without residual connections. Let us recall\nthe notions of nonlinear rank defined in that paper.\nDefinition 3.1 (Jacobian rank; Definition 1 of [Jac22]). The Jacobian rank of an FPLF $g$ is given by\n$\\text{rank}_J(g; \\Omega) = \\max_{x \\in \\Omega} \\text{rank}(J_g(x))$, where the maximum is over points $x$ in the interior of $\\Omega$ where $g$ is\ndifferentiable.\nDefinition 3.2 (Bottleneck rank; Definition 2 of [Jac22]). The bottleneck rank, $\\text{rank}_{BN}(g; \\Omega)$, of an FPLF\n$g$ is the smallest integer $k$ such that $g = h_2 \\circ h_1$ for two FPLFs $h_2 : \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{n_1}$ and $h_1 : \\Omega \\rightarrow \\mathbb{R}^{k}$.\nThe following theorem should be compared to Theorem 1 of [Jac22], and the proof closely follows the proof\nin that work, except with the main difference that since we have residual connections the main bottleneck in\nthe cost is not to represent the identity matrix. Instead, the main bottleneck is representing a large scaling\nof the identity matrix."}, {"title": "Theorem 3.3 (Infinite-depth cost sandwiched by nonlinear ranks as", "content": "In the infinite-depth limit, and\nas the parameter $\\lambda \\rightarrow 0$, the cost becomes lower-bounded by the Jacobian rank:\n\n$\\liminf_{\\lambda \\rightarrow 0} \\frac{c_{L, n, \\lambda}^{\\text{c2-nonlin}}(g)}{\\lambda \\log(1/\\lambda)} \\geq \\text{rank}_J(x; \\Omega),$\nand $\\liminf_{\\lambda \\rightarrow 0} \\frac{c_{L, n, \\lambda}^{\\text{c1-nonlin}}(g)}{\\lambda \\log(1/\\lambda)^2} \\geq \\text{rank}_J(x; \\Omega)$,    (3.1)\nOn the other hand, in the infinite-depth limit, and as the parameter $\\lambda \\rightarrow 0$, the cost becomes upper-bounded\nby the bottleneck rank. For any large enough $n$ depending on $g, \\Omega$\n\n$\\limsup_{\\lambda \\rightarrow 0} \\frac{c_{L, n, \\lambda}^{\\text{c1-nonlin}}(g)}{\\lambda \\log(1/\\lambda)^2} \\leq \\text{rank}_{BN}(x; \\Omega),$\nand $\\limsup_{\\lambda \\rightarrow 0} \\frac{c_{L, n, \\lambda}^{\\text{c2-nonlin}}(g)}{\\lambda \\log(1/\\lambda)} \\leq \\text{rank}_{BN}(x; \\Omega)$.     (3.2)\n\nIt remains to prove the lower bound, which is the \"harder\" direction of this lemma. Suppose\nthat $W_u, W_e, \\{W_i\\}$ achieve the minimum cost subject to $f_{\\text{1-lin}}(W_u, W_e, \\{W_i\\}) = A$. For convenience, define"}, {"title": "Lemma 2.5 with g(t) = f(t), we have", "content": "For any integer\n$m \\geq 1$, we will show how to simulate this in the first $mL_1$ layers of the ResNet. Let $S_0, ..., S_{L_1} \\subseteq [n]$ be\ndisjoint of size $|S| = n$. The embedding weights are $b_e = 0$ and $[W_e]_{S_0 \\times [d_{in}]} = \\sqrt{\\lambda}V_e$, and 0 everywhere else.\nFor any $i \\in [L_1], j \\in [m]$, let $[W_{(i-1)m+j}]_{S_i \\times S_{i-1}} = V_i/m$ and 0 everywhere else. Let $[b_{(i-1)m+j}]_{S_i} = a_i\\sqrt{\\lambda}$,\nand 0 everywhere else. Define\n\n$h_1 := f_{mL_1} \\circ f_{mL_1-1} \\circ \\cdots \\circ f_1 \\circ f_e$\n\nand let $T \\subset S_{L_1}$ be the first $k$ coordinates of $S_{L_1}$. This has the property that, for all $x \\in \\Omega$ we have\n\n$[h_1(x)] = \\sqrt{\\lambda}h_1(x)$.\nAnd for all $i \\notin S_0 \\cup ... \\cup S_{L_1}$,\n\n$[h_1(x)]_i = 0.$\nFurthermore the total cost of these first $mL_1$ layers and embedding layer is at most\n\n$\\frac{1}{2} [ \\Vert W_e \\Vert_F^2 ] + \\lambda L \\sum_{i=1}^{mL_1} ||W_i|| = ||V_e||^2 + \\lambda L \\sum_{i=1}^{mL_1} \\Vert V_{[i/m^2]} \\Vert_F^2 / m^2 < \\lambda C_1(1 + L/m),$\nIntermediate residual layers. Next, in the intermediate $L_{int}$ layers, for any scalar $\\tau > 1$ we can choose\nthe weights so that\n\n$h_{int} := f_{L_1+L_{int}} \\circ f_{L_1+L_{int}-1} \\circ \\cdots \\circ f_{L_1+1}$"}]}