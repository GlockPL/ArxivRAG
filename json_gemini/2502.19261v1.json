{"title": "DROP-UPCYCLING: TRAINING SPARSE MIXTURE OF EXPERTS WITH PARTIAL RE-INITIALIZATION", "authors": ["Taishi Nakamura", "Takuya Akiba", "Kazuki Fujii", "Yusuke Oda", "Rio Yokota", "Jun Suzuki"], "abstract": "The Mixture of Experts (MoE) architecture reduces the training and inference cost significantly compared to a dense model of equivalent capacity. Upcycling is an approach that initializes and trains an MoE model using a pre-trained dense model. While upcycling leads to initial performance gains, the training progresses slower than when trained from scratch, leading to suboptimal performance in the long term. We propose Drop-Upcycling \u2013 a method that effectively addresses this prob-lem. Drop-Upcycling combines two seemingly contradictory approaches: utilizing the knowledge of pre-trained dense models while statistically re-initializing some parts of the weights. This approach strategically promotes expert specialization, significantly enhancing the MoE model's efficiency in knowledge acquisi-tion. Extensive large-scale experiments demonstrate that Drop-Upcycling signifi-cantly outperforms previous MoE construction methods in the long term, specif-ically when training on hundreds of billions of tokens or more. As a result, our MoE model with 5.9B active parameters achieves comparable performance to a 13B dense model in the same model family, while requiring approximately 1/4 of the training FLOPs. All experimental resources, including source code, training data, model checkpoints and logs, are publicly available to promote reproducibil-ity and future research on MoE.", "sections": [{"title": "INTRODUCTION", "content": "Large-scale language models (LLMs) have achieved remarkable results across various natural lan-guage processing applications (Brown et al., 2020; Wei et al., 2022; Ouyang et al., 2022; OpenAI, 2024). This success largely depends on scaling the number of model parameters, the amount of train-ing data, and computational resources (Kaplan et al., 2020; Hoffmann et al., 2022), which leads to substantial training and inference costs of LLMs. Building and deploying high-performance models also require enormous resources, posing a significant barrier for many researchers and practitioners.\nThe Mixture of Experts (MoE) architecture has emerged as a promising approach to address the escalating resource demands of LLMs. MoE introduces multiple experts into some parts of the network, but only a subset is activated at any given time, allowing the model to achieve superior performance with reduced training and inference costs (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2021). In fact, cutting-edge industry models like Gemini 1.5 (Team et al., 2024) and GPT-4 (based on unofficial reports) (OpenAI, 2024) have adopted MoE, suggesting its effectiveness.\nWe refer to transformer-based LLMs without MoE as dense models and those incorporating MoE as MoE models. Upcycling (Komatsuzaki et al., 2023) is an approach that initializes and trains an MoE model using a pre-trained dense model, which aims to transfer learned knowledge for better initial performance. However, na\u00efve Upcycling copies the feedforward network (FFN) layers during initialization, which makes it difficult to achieve expert specialization. This disadvantage prevents\neffective utilization of the MoE models' full capacity, resulting in slower convergence over long training periods. Thus, there exists a trade-off between the short-term cost savings from knowledge transfer and the long-term convergence efficiency through expert specialization.\nIn this paper, we propose Drop-Upcycling a method that effectively addresses this trade-off, as briefly illustrated in Figure 1. Drop-Upcycling works by selectively re-initializing the parameters of the expert FFNs when expanding a dense model into an MoE model. The method is carefully de-signed to promote expert specialization while preserving the knowledge of pre-trained dense models. Specifically, common indices are randomly sampled along the intermediate dimension of the FFNs, and the weights are dropped either column-wise or row-wise, depending on the weight matrix types. The dropped parameters are then re-initialized using the statistics of those weights.\nExtensive large-scale experiments demonstrate that Drop-Upcycling nearly resolves the trade-off between the two aforementioned challenges and significantly outperforms previous MoE model construction methods such as training from scratch and na\u00efve Upcycling. By leveraging pre-trained dense models, Drop-Upcycling can start training from a better initial state than training from scratch, reducing training costs. On the other hand, Drop-Upcycling avoids the convergence slowdowns ob-served with na\u00efve Upcycling. Specifically, in our extensive long-term training experiments, Drop-Upcycling maintained a learning curve slope similar to that of training from scratch, consistently staying ahead. This success is attributed to effective expert specialization. As a result, we con-structed an MoE model with 5.9B active parameters that performs on par with a 13B dense model from the same model family, while requiring only approximately 1/4 of the training FLOPs.\nThis research is fully open, transparent, and accessible to all. With over 200,000 GPU hours of ex-perimental results, conducted on NVIDIA H100 GPUs, all training data, source code, configuration files, model checkpoints, and training logs used in this study are publicly available. By providing this comprehensive resource, we aim to promote further advancements in this line of research.\nOur technical contributions are summarized as follows:\n\u2022 We propose Drop-Upcycling, a novel method for constructing MoE models that effectively balance knowledge transfer and expert specialization by selectively re-initializing parame-ters of expert FFNs when expanding a dense model into an MoE model.\n\u2022 Extensive large-scale experiments demonstrate that Drop-Upcycling consistently outper-forms previous MoE construction methods in long-term training scenarios.\n\u2022 All aspects of this research are publicly available. This includes the MoE model with 5.9B active parameters that performs comparably to a 13B dense model in the same model family while requiring only about 1/4 of the training FLOPs."}, {"title": "RELATED WORK", "content": "2.1 MIXTURE OF EXPERTS\nThe concept of Mixture of Experts (MoE) was introduced about three decades ago (Jacobs et al., 1991; Jordan & Jacobs, 1994). Since then, the idea of using sparsely-gated MoE as a building block within neural network layers (Eigen et al., 2014; Shazeer et al., 2017) has evolved and has been incorporated into transformer-based language models (Lepikhin et al., 2021; Fedus et al., 2021). For a detailed overview of MoE, please refer to recent survey papers (Cai et al., 2024). Sparsely-gated MoE is currently the most common approach for building large-scale sparsely-activated models. In this paper, we focus on sparsely-gated MoE (also referred to as sparse MoE or sparsely-activated MoE), and unless otherwise specified, the term MoE refers to it.\nThere are various designs of MoE layers and ways to integrate them into transformer-based LLMs. For example, in addition to the standard token-centric routing, expert-centric routing has also been proposed (Zhou et al., 2022). To incorporate common knowledge, it has been suggested to introduce shared experts that are always activated (Dai et al., 2024). To simplify the discussion, we assume the most standard top-k token choice routing as the MoE layer and a decoder-only transformer-based LLM that uses MoE layers only in the FFNs as the MoE model. These are common design choices for recent MoE-based LLMs, such as Mixtral (Jiang et al., 2024), Skywork-MoE (Wei et al., 2024), Phi-3.5-MoE (Abdin et al., 2024), and Grok-12. Specifically, these models use 8 experts (Mixtral and Grok-1) or 16 experts (Skywork and Phi-3.5-MoE), with the top-2 experts being activated per input token. Our experiments also use top-2 routing with 8 experts per layer, as this setup aligns with those practical configurations. These facts indicate that Drop-Upcycling can be applied to most variations of MoE models. See Section 3.1 for technical details of MoE.\n2.2 MOE MODEL INITIALIZATION\nAs with conventional neural networks, MoE models can be initialized randomly and trained from scratch. However, to reduce training costs, leveraging existing pre-trained dense models has become a standard approach. Below, we introduce a few methods for achieving this.\nUpcycling (Komatsuzaki et al., 2023) leverages the weights of a pre-trained dense model for initial-izing an MoE model by initializing the experts in the MoE layer as replicas of the FFN layers in the dense model. The main advantage of Upcycling is that it boosts the model's initial performance. However, as our experiments show, MoE models initialized with Upcycling tend to have a much slower convergence, leading to suboptimal performance when trained for longer durations.\nBranch-Train-MiX (BTX) (Sukhbaatar et al., 2024) is a technique where a pre-trained dense model is replicated and fine-tuned on different datasets to produce multiple distinct expert dense models. These experts are then integrated into an MoE model, followed by additional training to optimize the routers. While this method appears to ensure expert specialization by design, Jiang et al. (2024) has highlighted that the diversity achieved in this way differs from that required for MoE layer experts, leading to suboptimal performance as a result. Our experiments also show that BTX suffers from suboptimal convergence similar to those observed in Upcycling.\nConcurrent with our work, the Qwen2 technical report (Yang et al., 2024) briefly suggests the use of a methodology possibly related to Drop-Upcycling in training Qwen2-MoE. Due to the report's brevity and ambiguity, it is unclear if their method exactly matches ours. Our paper offers a valuable technical contribution even if the methods are similar. The potential application of Drop-Upcycling in an advanced, industry-developed model like Qwen2-MoE that underscores the importance of fur-ther open investigation into this approach. We acknowledge the Qwen2 authors for sharing insights through their technical report."}, {"title": "METHOD", "content": "In this section, we explain the Drop-Upcycling method. Drop-Upcycling initializes an MoE model by utilizing a pre-trained dense model and consists of three steps:"}, {"title": "SWIGLU AND MOE LAYERS", "content": "We provide a brief overview of the MoE architecture. First, we review the feedforward network (FFN) layer in transformers. The SwiGLU activation function (Shazeer, 2020), now standard in state-of-the-art LLMs like LLAMA (Touvron et al., 2023) and Mixtral (Jiang et al., 2024), will be used for explanation here. However, it should be noted that Drop-Upcycling can be applied to transformers with any activation function. The FFN layer with SwiGLU is defined as follows:\n$\\mathrm{SwiGLU}(x) = (\\mathrm{Swish}(x^T W_{gate}) \\odot x^T W_{up}) W_{down}$                                                     (1)\nHere, $x \\in \\mathbb{R}^{d_h}$ represents the input vector and $\\odot$ denotes the Hadamard product. Each FFN layer contains the following three weight matrices: $W_{gate}, W_{up} \\in \\mathbb{R}^{d_h \\times d_f}$, and $W_{down} \\in \\mathbb{R}^{d_f \\times d_h}$. The dimensions $d_h$ and $d_f$ are referred to as the hidden size and intermediate size, respectively.\nWhen MoE is introduced into a transformer, each FFN layer is replaced with an MoE layer, while the rest of the architecture remains unchanged. Let us assume we use n experts and Top-k gating. An MoE layer comprises a router and n expert FFNs. The router has a weight matrix $W_{router} \\in \\mathbb{R}^{d_h \\times n}$. The i-th expert FFN is denoted as SwiGLU(i)(x), which, like a standard FFN layer, consists of three weight matrices. These weights are denoted as $W_{gate}^{(i)}, W_{up}^{(i)},$ and $W_{down}^{(i)}$. The output y of the MoE layer is computed as follows:\n$y = \\sum_{i=1}^n g_i(x) \\cdot \\mathrm{SwiGLU}^{(i)}(x),$                                                   (2)\nwhere $g_i(x)$ is the i-th element of the output $g(x) \\in \\mathbb{R}^n$ of the Top-k routing function, defined as:\n$g(x) = \\mathrm{Softmax}(\\mathrm{Top-}k(x^T W_{router})).$                                                        (3)\nSince $k < n$ is typically the standard setting, only the top-k selected experts out of n are computed. Therefore, the MoE layer is sparsely activated, meaning that only a subset of the parameters is involved in the computation. The number of parameters engaged in the computation for a given input is referred to as the active parameters of the MoE model. This value is widely used as an approximation for the computational cost as it correlates well with the cost of both training and inference. For non-MoE models, the total number of parameters corresponds to the active parameters as all parameters are involved in every computation."}, {"title": "EXPERT REPLICATION", "content": "Following (Komatsuzaki et al., 2023), we first construct a Transformer with MoE layers by repli-cating the weights from a pre-trained Transformer with standard FFN layers. As explained earlier, the architecture remains identical except the FFN layers, so we simply copy the weights of all non-FFN components. Each FFN layer needs to be replaced with an MoE layer, and the new MoE layers are constructed as follows: The router weights $W_{router}$ are initialized randomly. For the n experts, the weights from the original FFN are copied, such that $W_{gate}^{(i)} = W_{gate}, W_{up}^{(i)} = W_{up},$ and $W_{down}^{(i)} = W_{down}."}, {"title": "DIVERSITY RE-INITIALIZATION", "content": "Diversity re-initialization is the key step in Drop-Upcycling. This process is carefully designed to balance between knowledge retention and expert diversification. In particular, it is crucial to drop original weights along the intermediate dimension of the FFN layer based on shared indices across all three weight matrices. Specifically, the following operation is applied to every expert FFN in every MoE layer.\nStep 1: Column-wise Sampling. We sample indices from the set of integers from 1 to interme-diate size $d_f$, namely, $I_{d_f} = \\{1,2,\\ldots,d_f\\}$, to create a set of partial indices $S$. A hyperparameter $r$ ($0 \\le r \\le 1$) controls the intensity of re-initialization, determining the proportion $r$ used for sampling. That is, $S \\subseteq I_{d_f}$ and $|S| = \\lfloor r d_f \\rfloor$.\nStep 2: Statistics Calculation. We calculate the mean and standard deviation of the matrices of the weights corresponding to the selected indices $S$. Specifically, we compute the mean and variance $(\\mu_{up}, \\sigma_{up}), (\\mu_{gate}, \\sigma_{gate})$, and $(\\mu_{down}, \\sigma_{down})$ from the values obtained only from the non-zero columns of $I_S$ in the products $I_S \\odot W_{gate}, I_S \\odot W_{up},$ and $I_S \\odot W_{down}$, respectively, where $I_S$ is the indicator matrix whose values are 1 in the i-th column for $i \\in S$ and 0 otherwise.\nStep 3: Partial Re-Initialization. Finally, using the calculated statistics, we perform partial re-initialization of the three weight matrices $W_{gate}, W_{up},$ and $W_{down}$, obtaining $\\tilde{W}_{gate}, \\tilde{W}_{up},$ and $\\tilde{W}_{down}$. For the selected indices, the weights are dropped and re-initialized randomly, while for the unselected indices, the original weights are retained.\nLet $R_{type}$ be a matrix whose values are sampled from the $\\mathcal{N}(\\mu_{type}, (\\sigma_{type})^2)$ distribution, where type is one of the gate, up, or down, i.e., type = {gate, up, down}. We then obtain $\\tilde{W}_{type}$ by using the following equation:\n$\\tilde{W}_{type} = I_S \\odot R_{type} + (1 - I_S) \\odot W_{type},$                                                     (4)\nwhere we consider that the matrices, $W_{type}, R_{type}, \\tilde{W}_{type}$ are all transposed if type = down."}, {"title": "THEORETICAL CHARACTERISTICS", "content": "Applying the re-initialization strategy explained above, the initial MoE model obtained by Drop-Upcycling has the following characteristics:\n1. Parameter sharing among experts: since each expert retains the original representations with a ratio (1-r), with Top-k routing where k experts are selected, approximately $(1-r)^k$ of representations are preserved."}, {"title": "Characteristics of initial feedforward layers", "content": "Consider the output of an MoE layer with parameter re-initialization ratio r:\n$y = \\mathrm{FFN}_{common}(x) + \\sum_{i=1}^N g_i(x) [\\mathrm{FFN}_{retained;i}(x) - \\mathrm{FFN}_{common}(x) + \\mathrm{FFN}_{diverse;i}(x)]$                                                         (5)\nwhere $\\mathrm{FFN}_{common}$ represents the output from parameters that are common to all selected k experts (the proportion of such parameters is approximately $(1 - r)^k$ due to each expert independently preserving a ratio $(1 - r)$ of original parameters), $\\mathrm{FFN}_{retained;i}$ is expert i's output using uniquely retained original parameters (ratio (1-r)), and $\\mathrm{FFN}_{diverse;i}$ is the out-put using reinitialized parameters (ratio r). The estimation error in the number of common parameters has magnitude $O(\\frac{1}{d_f})$. A detailed derivation is provided in Appendix C.5."}, {"title": "EXPERIMENTAL SETUP", "content": "We conducted experiments to demonstrate the effectiveness of Drop-Upcycling described in Sec-tion 3. To clarify our model configurations, we introduce a notation where, for example, \"8\u00d7152M\u201d denotes an MoE model with eight experts and whose base dense model size is 152M.\nWe selected the Llama (Touvron et al., 2023) and Mixtral (Jiang et al., 2024) architectures for dense and MoE models, respectively, for our experiments. We employed 8 experts and the dropless (Gale et al., 2023) token choice top-2 routing (Shazeer et al., 2017) for the MoE. Detailed descriptions of the model configurations are provided in Appendix A.3\nWe evaluated four different methods to build MoE models, namely, training from scratch, na\u00efve Upcycling (Komatsuzaki et al., 2023), Random Noise Upcycling (Komatsuzaki et al., 2023) and Branch-Train-MiX (Sukhbaatar et al., 2024) to compare the performance with Drop-Upcycling. Moreover, we also evaluated dense models to provide a reference of the typical performance of LLMs in our configuration and illustrate the performance gains of MoE models. We initialized all parameters of dense models using a Gaussian distribution $\\mathcal{N}(0,0.02)$. The dense models are also used as the seed models of MoE models, except when we train MoE models from scratch. When training MoE models from scratch, we used the same initialization method as the dense models, that is, $\\mathcal{N}(0,0.02)$. In Random Noise Upcycling, Drawing from (Muennighoff et al., 2024), we initialize by copying the dense model parameters and then add Gaussian noise $\\mathcal{N}(0,0.02)$ to 50% of the weights in each FFN layer. In Branch-Train-Mix, we first obtained three distinct expert dense models by further training a seed dense model with 100B extra tokens of either Japanese, English, or code. Then, we used the four dense models (the seed dense model and three expert dense models) to initialize the parameters of an MoE model. Specifically, we averaged all parameters in the four dense models except the FFN layers and duplicated the FFN layers in each model twice to build eight MoE experts. Note that this method involved extra training steps with 300B more tokens compared to the other MoE construction methods.\nUnless otherwise stated, dense models were trained on 1T tokens, and MoE models were trained on 500B tokens. Our training data was obtained from publicly available data. We describe the detailed statistics of the training datasets in Appendix B.1. We followed the typical training configurations used in Llama to train dense models and Mixtral for MoE models. Details of the hyper-parameters we used are described in Appendix A.4. Moreover, the implementation and the computational envi-ronment used in our experiments are described in Appendix A.2.\nWe conducted a comprehensive evaluation using a wide range of tasks in Japanese and English. We used 12 evaluation datasets that can be categorized into seven types. The details of the evaluation datasets and metrics are described in Appendix B.2."}, {"title": "RESULTS AND DISCUSSION", "content": "In this section, we address the following questions through experiments: Is Drop-Upcycling superior to existing MoE construction methods, and does Drop-Upcycling resolve the issue of slower con-vergence? (Section 5.1) Does it perform well even in large-scale settings? (Section 5.2) What is the impact of the re-initialization ratio r? (Section 5.3) How are the experts specialized? (Section 5.4)"}, {"title": "METHOD COMPARISON", "content": "First, we compare Drop-Upcycling with existing methods using small (8\u00d7152M) to medium (8\u00d71.5B) scale settings. The left two columns of Figure 3 illustrate the learning curves under these settings. The top and bottom rows illustrate the changes in training loss and downstream task scores during training, respectively. Note that in LLM pretraining, training loss serves as a reliable per-formance indicator since the risk of overfitting is low. The performance on downstream tasks is represented by the average score across 12 tasks, which is commonly used as the overall evaluation metric. A detailed breakdown will be discussed later in conjunction with Table 1.\nFigure 3 shows that Drop-Upcycling at r = 0.5 (green) is significantly more efficient compared to other methods. The top row shows the training loss, while the bottom row displays the evaluation scores using downstream tasks. In both metrics and for both model sizes, Drop-Upcycling becomes the clear winner after some training. Notably, the slope of the learning curve, which indicates con-vergence rate, is superior. Furthermore, it can be observed that the slope of the learning curve is consistent with the case of training from scratch, suggesting that Drop-Upcycling resolves the cru-cial challenge of balancing knowledge transfer and expert specialization in Upcycling. For further analysis on expert specialization, see Section 5.4.\nAmong existing methods, na\u00efve Upcycling exhibited the slowest loss reduction rate and improve-ment in task scores. Branch-Train-Mix, which starts MoE training after each expert has been trained for 100B steps on different domains such as Japanese, English, and code, initially shows an advan-tage over na\u00efve Upcycling due to this favorable initialization. However, its long-term learning pace is on par with na\u00efve Upcycling, and it is ultimately overtaken by Drop-Upcycling. As an ablation study, we evaluated setting r = 1.0 in Drop-Upcycling, in addition to the standard r = 0.5. This configuration involves random initialization of all FFNs while reusing weights for embeddings and self-attention layers. This configuration might seem inefficient at first glance. Nevertheless, our"}, {"title": "SCALING TO 8\u00d73.7B", "content": "To further evaluate the effectiveness of Drop-Upcycling in larger-scale settings and to build a prac-tical MoE model, we conducted experiments with an 8\u00d73.7B configuration. Due to computational resource constraints, experiments under the 8\u00d73.7B setting were limited to training from scratch and Drop-Upcycling with r = 0.5.\nThe rightmost column of Figure 3 illustrates the learning curves under this configuration. Similar to the 8\u00d7152M and 8\u00d71.5B settings, Drop-Upcycling significantly outperforms training from scratch. There is an initial gain in performance due to the improved initialization, and expert diversification allows the training to progress as efficiently as in the case of training from scratch, ensuring that Drop-Upcycling never gets overtaken."}, {"title": "ANALYSIS 1: RE-INITIALIZAITON RATIO", "content": "We conducted a study to investigate the impact of the re-initialization ratio r in Drop-Upcycling. Figure 4 illustrates the effects of different re-initialization rates 0.0 (na\u00efve Upcycling), 0.1, 0.25, 0.5, 0.75, and 1.0 on models of sizes 8\u00d7152M and 8\u00d71.5B. Each model was trained up to 150B tokens, during which we monitored the training loss and the progression of the average downstream task scores.\nThe experimental results revealed similar trends across both model sizes. In terms of long-term performance, a re-initialization ratio of 0.5 yielded the best results for both models, maintaining superiority in both training loss and average task scores. An interesting pattern emerged regarding the influence of the re-initialization ratio. With lower re-initialization rates, particularly at 0.0 (na\u00efve Upcycling), the models struggled to significantly improve beyond the performance of the original pre-trained models. While re-initialization rates of 0.1 and 0.25 showed promising performance in the early stages of training, they were eventually surpassed by the 0.5 re-initialization rate as training progressed. These observations suggest that increasing the re-initialization ratio helps the models escape local optima, enabling more effective learning. However, excessively high re-initialization rates of 0.75 or 1.0 appeared to hinder the effective knowledge transfer from the pre-trained dense models. This phenomenon highlights an important trade-off concerning the MoE initialization: a balance must be struck between knowledge transfer and effective expert specialization. Drop-Upcycling with r = 0.5 is a robust and practical method that ideally balances these two aspects."}, {"title": "ANALYSIS 2: EXPERT SPECIALIZATION", "content": "We analyze expert routing patterns to examine how Drop-Upcycling facilitates expert specialization. We apply the methodologies of Jiang et al. (2024) and Muennighoff et al. (2024) to 8\u00d71.5B MoE models trained with different methods. This analysis investigates how data from different domains is routed to various experts. As input data from different domains, we use the validation sets from"}, {"title": "CONCLUSION", "content": "In this paper, we introduced Drop-Upcycling, a novel method for efficiently constructing Mixture of Experts (MoE) models from pre-trained dense models. Selectively re-initializing parameters of expert feedforward networks, Drop-Upcycling effectively balances knowledge transfer and expert specialization, addressing the key challenges in MoE model development.\nOur extensive large-scale experiments demonstrated that Drop-Upcycling, significantly outperforms previous MoE construction methods. As a result, we achieved an MoE model with 5.9B active parameters that matches the performance of a 13B dense model from the same model family while requiring only about 1/4 of the training FLOPs.\nBy making all aspects of our research publicly available-including data, code, configurations, checkpoints, and logs\u2014we aim to promote transparency and facilitate further advancements in ef-ficient LLM training. We believe that Drop-Upcyclingoffers a practical solution to reduce resource barriers in deploying high-performance LLMs, contributing to broader accessibility and innovation in AI research."}]}