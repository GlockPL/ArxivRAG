{"title": "DERMARK: A Dynamic, Efficient and Robust Multi-bit Watermark for Large Language Models", "authors": ["Qihao Lin", "Chen Tang", "Lan zhang", "Junyang zhang", "Xiangyang Li"], "abstract": "Well-trained large language models (LLMs) present significant risks, including potential malicious use and copyright infringement. Current studies aim to trace the distribution of LLM-generated texts by implicitly embedding watermarks. Among these, the single-bit watermarking method can only determine whether a given text was generated by an LLM. In contrast, the multi-bit watermarking method embeds richer information into the generated text, which can identify which LLM generated and distributed a given text to which user. However, existing efforts embed the multi-bit watermark directly into the generated text without accounting for its watermarking capacity. This approach can result in embedding failures when the text's watermarking capacity is insufficient. In this paper, we derive the watermark embedding distribution based on the logits of LLMs and propose a formal inequality to segment the text optimally for watermark embedding. Building on this foundation, we propose DERMARK, a dynamic, efficient, and robust multi-bit watermarking method. DERMARK divides the text into segments of varying lengths for each bit embedding, adaptively matching the text's capacity. It achieves this with negligible overhead and robust performance against text editing by minimizing watermark extraction loss. Comprehensive experiments demonstrate that, compared to the SOTA method, our method reduces the number of tokens required for embedding each bit by 20%, reduces watermark embedding time by 50%, and is robust to text editing and watermark erasure attacks.", "sections": [{"title": "1 Introduction", "content": "In recent years, large language models (LLMs) such as GPT-4 [Achiam et al., 2023], LLaMA [GenAI, 2023], and OPT [Zhang et al., 2022] have achieved significant advancements, excelling in tasks such as instruction following and question answering. Training an LLM requires substantial hardware resources, vast amounts of training data, and specialized expert knowledge. Consequently, LLMs are considered valuable intellectual property (IP) of their respective owners.\nHowever, the increasing functionality of highly capable LLMs poses potential risks, including malicious uses such as spreading misinformation [Chen and Shu, 2024] and generating harmful content [Perkins, 2023], as well as copyright infringement, such as redistributing or re-selling purchased LLM services [Birch et al., 2023]. Given these malicious behaviors, there is an urgent need for mechanisms to trace the distribution of LLM-generated texts, safeguarding the IP rights of LLM owners.\nThe watermarking technique for LLMs has emerged as a promising solution [Liu et al., 2024b]. By implicitly embedding a watermark into the output text, LLM owners can effectively trace the distribution of their generated content. Currently, most watermarking techniques for LLMs focus on identifying whether a given text was generated by a specific LLM, a method commonly referred to as one-bit watermarking [Kirchenbauer et al., 2023]. However, these approaches are unable to embed unique watermarks into LLM-generated texts tailored to different users. As a result, multi-bit watermarking solutions have been proposed to embed rich watermark information (e.g., binary strings [Wang et al., 2024]) into LLM-generated texts by adding bias to the LLM logits, as shown in Fig.1. This approach enables the accurate identification of malicious use or copyright-infringing content, including determining which user generated it with which LLM.\nThe primary factor determining the successful embedding of a multi-bit watermark is the watermark capacity of the LLM-generated text, defined as the maximum number of bits that can be embedded. This capacity is directly influenced by the text's entropy. For example, when the tokens in a text are predicted by the LLM with relatively even probabilities (i.e., high entropy), the watermark capacity is high. However, existing methods [Wang et al., 2024] overlook the watermark capacity of LLM-generated text and instead divide the text equally into multiple segments based solely on the watermark length, embedding each bit of the watermark into each segment. This approach significantly degrades the semantics of short texts. Furthermore, when the text exhibits low entropy (e.g., code), the watermark frequently fails to embed successfully. Thus, the critical aspect of embedding a multi-bit watermark into LLM-generated text lies in dynamically segmenting the text for each bit embedding based on its entropy and then embedding each bit into the corresponding segment.\nDesigning such a multi-bit watermarking method is a non-trivial task, presenting three key challenges: (1) Lack of an effective method to segment the LLM-generated text. While entropy can provide a rough estimate of the text's watermark capacity, it remains unclear how much entropy is required to embed a single watermark bit. This knowledge is essential for dynamically assigning segments to each bit for embedding. Although setting an entropy threshold based on pre-experiments is a feasible approach, it is inherently a subjective estimation, leading to potential errors and embedding failures. Addressing this challenge requires constructing a one-to-one mapping between the watermark and the segmentation of LLM-generated text, a task that is highly complex and difficult to achieve. (2) Multi-bit watermarks are vulnerable to vanishing due to text editing. Since each bit of the watermark is embedded within a specific segment, watermark extraction becomes highly sensitive to changes in the segment's tokens. If the LLM-generated text undergoes post-processing (e.g., additions or deletions), resulting in significant alterations to the segment, the watermark may become undetectable. (3) Watermarks are vulnerable to erasure attacks. Since the LLM-generated text is accessible to users, attackers may attempt to erase the watermark for malicious purposes or copyright infringement. For instance, because watermarked text exhibits bias, an attacker could infer the watermark embedding strategy by analyzing the LLM-generated text returned over multiple iterations, thereby erasing the watermark. Alternatively, an attacker could remove the watermark by paraphrasing the text, ensuring that it no longer reflects the watermark embedding.\nIn this work, we theoretically demonstrate that the watermark embedding, which is achieved by modifying the LLM logits, follows a normal distribution based on these logits. Then, we derive the inequality that must be satisfied for watermark bit embedding based on this distribution. Based on this inequality, we can compute the required segment for each bit embedding and determine the watermark capacity of any LLM-generated text. Subsequently, we design DERMARK, a dynamic and efficient multi-bit watermark embedding method. During the watermark embedding phase, the required segment is dynamically computed using the inequality derived above, based on the LLM logits. Moreover, in DERMARK, text segmentation and watermark embedding are based on the LLM logits, introducing only a finite computational overhead, which is negligible compared to the LLM inference overhead. In the extraction phase, instead of directly extracting the watermark through the embedding method, we use dynamic programming to minimize the segmentation loss (the difference between the two sides of the derived inequality due to the current segmentation) and the color loss (the difference in the proportion of red and green tokens in each segment), thereby achieving robustness against text editing. Additionally, DERMARK is compatible with robustness-enhancing one-bit watermarking methods. We leverage the one-bit watermarking method to embed each bit of the multi-bit watermark into each segment, enabling us to achieve robustness against erasure attacks by directly utilizing their improved watermarking techniques.\nOur main contributions are threefold as follows: (1) Based on the LLM logits, we derive that watermark embedding follows a normal distribution. This distribution provides a basis for determining how many tokens in the generated text are required to embed each bit of the watermark. (2) We design DERMARK, a dynamic, efficient, and robust method for multi-bit watermark embedding. This innovative method performs segmentation based on the text's watermark capacity, and the additional overhead is negligible. Moreover, DERMARK is robust against text editing and various other attacks. (3) Comprehensive experiments show that DERMARK uses, on average, 2.26 fewer tokens than SOTA for watermark bit embedding, and the additional overhead is negligible compared to the LLM inference overhead. Furthermore, our method is robust to text addition, deletion, and watermark erasure attacks."}, {"title": "2 Related Work", "content": "Since 2023, a series of efforts have been dedicated to embedding watermarks into LLM-generated text [Liu et al., 2024b]. These works fall into two main categories: one-bit watermarking and multi-bit watermarking. One-bit watermarking is used to determine whether a given text has been generated by a specified LLM. However, they cannot be directly extended to multi-bit watermarking scenarios to meet the requirements of watermarking LLM-generated text from different LLMs for different users. To address the above problem, multi-bit watermarking methods have been proposed to embed 0/1 strings into LLM-generated text. Given the diversity of watermarks, these methods can watermark different LLM-generated texts. Among them, [Wang et al., 2024] proposes adding an additional watermark loss in the inference phase to embed each bit of the watermark into a fixed-length segment. [Zhang et al., 2024] proposes training a model to encode the watermark into the LLM-generated text. However, it is unreliable to embed the watermark directly into a fixed-length segment without considering its capacity. Intuitively, the watermark capacity varies from text to text. However, none of the current multi-bit watermarking methods account for the LLM-generated text's capacity. This can lead to embedding failure when the text has low watermark capacity.\nTherefore, we aim to dynamically assign different segments to each bit of the multi-bit watermark based on the watermark capacity of the LLM-generated text and embed each bit into the corresponding segment."}, {"title": "3 Theoretical Analysis", "content": "In this section, we first define the problem of multi-bit watermarking. Next, we provide the necessary notations and preliminaries to facilitate a better understanding of our design. Finally, we theoretically derive the method for dynamically assigning segments to each bit of the watermark."}, {"title": "3.1 Problem Statement", "content": "In multi-bit watermarking, the objective is to embed the multi-bit watermark into the LLM-generated text during the LLM inference phase. As mentioned previously, instead of directly embedding the watermark into the text, we assign different segments to each bit for embedding. Therefore, the embedding of multi-bit watermarks can be divided into two steps:\nStep 1. Segmentation: segment the LLM-generated text per bit for embedding;\nStep 2. Embedding: embed each bit of the watermark into the assigned segment.\nNote that what we are concerned about is step 1, segmentation, rather than embedding. After segmentation, we leverage the one-bit watermarking method [Kirchenbauer et al., 2023] to embed each bit into the assigned segment."}, {"title": "3.2 Notations and Preliminaries", "content": "To better understand the design of our method, we provide a brief description of the processes and notations involved in LLM inference, watermark embedding, and watermark extraction.\nIn the LLM inference phase, we denote \\(x_p\\) as the prefix prompt, \\(s = \\{s^{(0)}, s^{(1)}, ...\\}\\) as the LLM-generated text, \\(V = \\{s_1,..., s_{|v|}\\}\\) as the vocabulary, where \\(s^{(t)}\\) represents the t-th token of the LLM-generated text, and \\(s_i\\) represents the i-th token in the vocabulary. The entire input at the t-th step for LLM is the combination of \\(x_p\\) and the sequence of tokens \\(s_{:t-1} = \\{s^{(0)},..., s^{(t-1)}\\}\\), where \\(s_{:t-1}\\) represents tokens generated by the LLM in previous steps. The LLM takes these as input and outputs the logits \\(L(x_p, s_{:t-1}) = \\{l_1^{(t)},..., l_{|v|}^{(t)}\\}\\). Then the logits are processed by the softmax function to produce a probability distribution \\(P(x_p, s_{:t-1}) = \\{P_1^{(t)},...,P_{|v|}^{(t)}\\}\\), where \\(P_i^{(t)} = \\frac{e^{l_i^{(t)}}}{\\sum_{j=1}^{|v|}e^{l_j^{(t)}}}\\) denotes the probability of the i-th token in the vocabulary at the t-th step. Finally, the t-th token \\(s^{(t)}\\) is sampled based on \\(P(x_p, s_{:t-1})\\) through a specific sampling rule, such as probabilistic sampling.\nMulti-bit watermark embedding is achieved during the LLM inference phase. For each bit \\(m_k\\) of the multi-bit watermark \\(m \\in \\{0,1\\}^K\\), we assign a segment \\(S_k\\) of consecutive tokens from \\(s\\) for the \\(m_k\\). Subsequently, we adopt the one-bit watermarking strategy to embed \\(m_k\\) into \\(S_k\\). As shown in Fig.1, the watermarking process includes the following steps:\n1. Vocabulary Partitioning: A random seed is used to partition the vocabulary into a \"green list\" G and a \"red list\" R of equal size. Generally, we assume: G = \\{s_1,...,s_{|V|/2}\\}, R = \\{s_{|V|/2+1},...,s_{|V|}\\}.\n2. Logits Modification: If \\(m_k = 1\\), add a bias \\(\\delta\\) to logits of green tokens; If \\(m_k = 0\\), add the \\(\\delta\\) to logits of red tokens.\n3. Softmax: The modified logits \\(L'(x_p, s_{:t-1}) = \\{l_1'^{(t)},...,l_{|v|}'^{(t)}\\}\\) are processed by the softmax function to generate a watermarked probability distribution \\(P'(x_p, s_{:t-1})\\). Subsequently, \\(s^{(t)}\\) is sampled based on \\(P'(x_p, s_{:t-1})\\).\nEventually, once all tokens in \\(S_k\\) have been generated, \\(m_k\\) is embedded into \\(S_k\\) simultaneously.\nIn the watermark extraction phase, to extract the k-th bit of the watermark, denoted as \\(m'_k\\), we examine the tokens in \\(S_k\\) and calculate the proportion of tokens belonging to G or R. If the number of tokens belonging to G exceeds half of the total tokens in \\(S_k\\), then \\(m'_k = 1\\); otherwise, \\(m'_k = 0\\)."}, {"title": "3.3 Theoretical Derivation", "content": "Based on the above notations and processes, we are now ready to derive the mapping from the watermark to the text segmentation.\nSince the vocabulary V is partitioned into G and R, we first analyze the probability that the next token \\(s_t\\) generated belongs to G or R before watermarking. Let \\(P_G^{(t)}\\) and \\(P_R^{(t)}\\) respectively indicate that the next token \\(s^{(t)}\\) at the t-th step belongs to G or R, which can be calculated as:\n\\[P_G^{(t)} = \\frac{\\sum_{s_i \\in G} e^{l_i^{(t)}}}{\\sum_{s_i \\in V} e^{l_i^{(t)}}}\\]\n\\[P_R^{(t)} = \\frac{\\sum_{s_i \\in R} e^{l_i^{(t)}}}{\\sum_{s_i \\in V} e^{l_i^{(t)}}}\\]\nThen we analyze the probability that the next token \\(s^{(t)}\\) generated belongs to G or R after watermarking. Let \\(P_G'^{(t)}\\) and \\(P_R'^{(t)}\\) respectively denote the probability of the next token \\(s^{(t)}\\) at the t-th step belonging to G or R after watermarking. Then we have the following Lemma:\nLemma 1. If \\(m_k = 1\\), \\(P_G'^{(t)} = \\frac{e^{\\delta} \\cdot P_G^{(t)}}{e^{\\delta} \\cdot P_G^{(t)} + (1-P_G^{(t)})}\\); Similarly, if \\(m_k = 0\\), \\(P_R'^{(t)} = \\frac{e^{\\delta} \\cdot P_R^{(t)}}{e^{\\delta} \\cdot P_R^{(t)} + (1-P_R^{(t)})}\\)\nProof. Taking \\(m_k = 1\\) as an example.\nFor \\(s_i \\in G\\), the predicted probability of \\(s_i\\) is:\n\\[P_G^{(t)} (s_i | x_{prompt}, s_{:t-1}) = \\frac{e^{l_i^{(t)} + \\delta}}{\\sum_{s_i \\in G} e^{l_i^{(t)} + \\delta} + \\sum_{s_i \\in R} e^{l_i^{(t)}}}\\]\nFor \\(s_i \\in R\\), the predicted probability of \\(s_i\\) is:"}, {"title": "4 DERMARK", "content": "The workflow of our design is illustrated in Fig.2. It consists of two main phases: watermark embedding and watermark extraction. In the watermark embedding phase, different watermarks are dynamically segmented using Eq.(6) and embedded into the LLM-generated text for various users. In the watermark extraction phase, the LLM-generated text is segmented through dynamic programming, optimizing segmentation loss and color loss to effectively extract the embedded watermark."}, {"title": "4.1 Design Overview", "content": "Based on Theorem 1, we segment the LLM-generated text using Eq.(6) and embed each bit of the watermark into its corresponding segment. Specifically, during each token generation, as illustrated in the watermark embedding process in Fig.\\ref{fig:workflow}, a bias is added to the logits of tokens based on the bit currently being embedded. The modified logits are then processed to obtain the output probability for each token, after which a token is sampled. The sampled token is subsequently evaluated using Eq.(6) to determine whether the inequality holds. If the inequality holds, it indicates that adding this token to the current segment has provided sufficient capacity to embed one bit. In this case, the next generated token should be assigned to a new segment. Conversely, if the inequality does not hold, it means that the current segment still lacks the capacity to embed one bit, and the next generated token should remain in the current segment. Finally, after adding the sampled token, the LLM iteratively generates the next token. As the text is progressively generated, the multi-bit watermark is simultaneously embedded into the text.\nNevertheless, the workflow outlined above faces several challenges in real-world applications. To address these limitations and enhance its practicality, we propose the following improvements:"}, {"title": "4.2 Multi-bit Watermarking", "content": "distribution of red and green tokens observed within the current segment:\n\\[P_t(m_k = 1) = \\frac{G_{a:b} + \\lambda}{b-a + 2\\lambda}, P_t(m_k = 0) = \\frac{R_{a:b} + \\lambda}{b-a + 2\\lambda}\\]\nHere, a and b denote the positions of the first and current tokens in the segment, respectively. \\(G_{a:b}\\) and \\(R_{a:b}\\) represent the counts of green and red tokens in the segment, respectively. The hyperparameter \\(\\lambda\\) is introduced to adjust the estimation.\n(2) Redundant tokens. When the number of tokens in the LLM-generated text exceeds the requirement for embedding the multi-bit watermark, the surplus tokens are utilized for padding. Specifically, the last bit of the watermark is reversed and embedded into these redundant tokens. Since the distribution of redundant tokens differs significantly from that of the last segment, it becomes straightforward to distinguish tokens used for watermark embedding from those that are redundant."}, {"title": "4.3 Watermark Extraction", "content": "Since the segmentation inequality in Eq.(6) is highly sensitive to token changes, even minor text edits can lead to different segmentation outcomes, thereby making the watermark extraction process unreliable. To address this issue and ensure accurate watermark extraction while minimizing the impact of text editing, we focus on identifying an optimal segmentation strategy for watermark extraction.\nAssume a segmentation result is represented as Seg(S) = \\{\\dots, S(a;b), \\dots\\}. Intuitively, for any given segmentation result, each segment must satisfy the inequality in Eq.(6). Consequently, we define the segmentation loss as the difference between the two sides of the inequality in Eq.(6) for each segment:\n\\[L_s(a, b) = (f(E[P'(t)]) - (\\Phi^{-1}(\\alpha))^2 - \\epsilon_s)^2,\\]\n\\[f(E[P'(t)]) = \\frac{\\frac{1}{N} (\\sum_{i=a}^b E[P'(t)])}{(\\sum_{i=a}^b E[P'(t)])^2}\\]\nHere, a and b denote the start and end indices of the segment, respectively, and \\(\\epsilon_s\\) is a hyperparameter introduced to correct for bias deviations in the inequality.\nFurthermore, due to the embedded watermark, each segment will exhibit a noticeable imbalance, with significantly more red or green tokens than the other. To quantify this imbalance, we define the color loss within each segment as the difference in the number of tokens of each color, expressed as follows:\n\\[L_c(a, b) = \\left| \\frac{min(G_{a:b}, R_{a:b})}{b-a} - \\epsilon_d \\right|,\\]\nwhere \\(\\epsilon_d\\) is also a hyper-parameter used to correct for the bias deviation caused by the inequality.\nAs a result, the watermark extraction loss is the sum of the above two losses described above:\n\\[L(Seg) = \\sum_j L(j, i) = \\sum_j (\\beta \\cdot L_s(j, i) + L_c(j, i)),\\]\nwhere \\(\\beta\\) is an adjustable parameter. By minimizing this loss, we use dynamic programming to identify an optimal segmentation among multiple segmentation options with a time complexity of O(N2). The entire watermark extraction process is illustrated in Fig.2, which involves the following key steps:\n1. Color each token. First, we assign a color to each token, with the token color determined by hashing the previous token.\n2. Identify the padding. Next, we identify the padding by locating all tokens with the watermark embedded. Since padding embeds the inverse of the last bit of the multi-bit watermark, it can be recognized based on the distribution of tokens in different colors.\n3. Determine Segmentation. Next, we begin segmenting the text using dynamic programming. We initialize the values of \\(\\epsilon_e = 0\\) and \\(\\epsilon_d = 0\\) and define the total loss L[k][p], which represents the sum of losses for dividing the tokens s'p into k segments. Additionally, we maintain a predecessor array prev[t][b], which stores the starting position of the last segment ending at a for t segments. The optimization process follows the recurrence relation below:\nIf L[t-1][a] + cost[a][b] < L[t][b],\nthen L[t][b] = L[t - 1][a] + cost[a][b], prev[t][b] = a,\nwhere:\nL[t - 1][a] is the minimum loss for dividing the first a tokens into t \u2212 1 segments;\ncost[a][b] is the loss of a single segment spanning from token a to b;\nL[t][b] is the minimum loss for dividing the first b tokens into t segments.\nprev[t][b] is the starting position a of the segment ending at b in the t-th segment."}, {"title": "5 Experiments", "content": "In this section, we conduct extensive experiments to demonstrate the superior performance of DERMARK in terms of watermark capacity, efficiency, and robustness."}, {"title": "5.1 Experimental Setup", "content": "In all experiments, we use the OPT-1.3b [Zhang et al., 2022] and GPT-2 [Radford et al., 2019] models as the LLMs for text generation, and the news-like subset of the C4 dataset [Raffel et al., 2020] as the prompt for evaluation. During the LLM inference phase, texts are randomly sampled from this subset, truncated to 100 tokens as input prompts, and tokens are generated through multinomial sampling, with a repetition penalty of 1.5 applied to enhance text diversity. The hyperparameter \\(\\alpha\\) is set to [0.8, 0.99] to control watermark embedding, \\(\\beta\\) is set to 14, and \\(\\lambda\\) is defined as a * \\(\\Phi^{-1}(\\alpha))^2\\). The SOTA multi-bit watermarking method, Balance-Marking, is chosen as the baseline, with GPT-2 serving as its LM proxy. All experiments were conducted on a server running CentOS Linux. The hardware setup included an Intel Xeon E5-2650 v4 CPU, with an NVIDIA Tesla P40 GPU equipped with 22 GB of memory, running CUDA version 11.3. The experiments utilized Python 3.9.13 as the primary programming environment."}, {"title": "5.2 Watermark Capacity", "content": "To demonstrate the superior performance of DERMARK in watermark capacity, we first evaluate the performance difference between DERMARK and Balance-Marking on the evaluation dataset. We calculate the watermark capacity by adjusting the value of \\(\\alpha\\) in DERMARK. Specifically, for each value of \\(\\alpha\\), we use the LLM to generate 500 texts. Then, we extract watermarks from these texts to compute the average watermark detection rate and the average number of tokens per bit for embedding. Similarly, the results for the baseline are obtained by adjusting the segment length for each bit. The results are shown in Fig.3(a) and Fig.3(b). When achieving the same watermark detection rate, DERMARK uses, on average, 2 fewer tokens per bit for embedding on OPT-1.3B, and 1 fewer token per bit for embedding on GPT-2, compared to the baseline. To further demonstrate the superior performance of DERMARK, we construct the top 25% of the worst-performing datasets, referred to as the \"poor\" dataset, based on the watermark detection rate of the baseline in the evaluation dataset. We then evaluate the performance of DERMARK on these datasets. The results are shown in Fig.3(c) and Fig.3(d). On the poor dataset, DERMARK significantly outperforms the baseline. First, DERMARK uses at least 4 fewer tokens per bit to achieve the same watermark detection rate. Additionally, DERMARK produces more stable results, unlike the baseline, which exhibits numerous outliers. Finally, the baseline performance plateaus after reaching a certain accuracy on this dataset (0.88 for OPT-1.3B and 0.925 for GPT-2)."}, {"title": "5.3 Impact of Watermarking on Text Quality", "content": "We explored the impact of watermark strength on text quality by varying the value of \\(\\delta\\). A total of 500 prompts were randomly selected. For each value of \\(\\delta\\), we used both GPT-2 and OPT-1.3B to generate text for each prompt and evaluated the generated text using the Perplexity (PPL) metric with OPT-2.7B to assess text quality. Lower PPL values indicate higher text quality. Table 1 summarizes the text quality data for watermarked text generated by the two methods. The PPL results for both methods exhibit similar trends across varying \\(\\delta\\) values. As a result, the two approaches demonstrate comparable performance in terms of text quality, as measured by"}, {"title": "5.4 Efficiency", "content": "We evaluated the text generation and watermark extraction efficiency of both Balance-Marking and DERMARK. For Balance-Marking, the large language model used for text generation also served as the auxiliary model, with a fixed watermark embedding length of 10 tokens per bit. For DERMARK, we set the error rate to 10%. Both methods were tested with \\(\\delta = 1\\) and a fixed prompt length of 100 tokens.\nAs shown in Fig. 5, during the watermark embedding phase, the DERMARK watermarking method demonstrates almost identical time overhead compared to the raw outputs of large models, which is significantly lower than that of Balance-Marking. This indicates that DERMARK achieves substantial time optimization during the embedding phase. Although the time overhead of DERMARK during the watermark extraction phase is slightly higher than that of the baseline, DERMARK provides more robust watermark extraction (as described in the next subsection) with only a minimal additional time overhead, which is acceptable in practice. Overall, DERMARK incurs lower time costs in practical applications and is more suitable for large-scale text watermarking tasks."}, {"title": "5.5 Robustness", "content": "We demonstrate that DERMARK exhibits robustness both to text modifications and to erasure attacks.\n(1) Insertion Attacks. After inserting 5% and 10% of additional tokens into the generated text of OPT-1.3b (Fig. 4(a)) and GPT-2 (Fig. 4(b)), DERMARK demonstrates higher watermark detection rates while requiring fewer tokens per bit compared to Balance-Marking. The scatter points and fitted lines show that DERMARK's curve lies below that of Balance-Marking, highlighting its ability to achieve equivalent accuracy with reduced token overhead.\n(2) Deletion Attacks. After deleting 5% and 10% of tokens from the generated text of OPT-1.3b (Fig. 4(c)) and GPT-2 (Fig. 4(d)), DERMARK consistently outperforms Balance-Marking, exhibiting a lower token-per-bit ratio and higher watermark detection rate. This advantage becomes more pronounced at higher detection rates, where efficiency differences are critical. The key takeaway is that DERMARK's superior efficiency in token utilization enhances its robustness against adversarial attacks, ensuring reliable watermark detection with minimal overhead. This performance advantage highlights DERMARK's potential as a preferred method for practical applications requiring efficient and resilient text watermarking.\n(3) Erasure Attacks. Our method does not impose specific requirements on the division of red and green sets or the choice of hash functions, ensuring compatibility with existing semantically unbiased approaches. By adjusting the partitioning of the red and green subsets, we can achieve semantic neutrality in watermarked texts [Hu et al., 2023]. Similarly, our method is compatible with SIR, allowing us to leverage textual semantics to guide the hash function, thereby enhancing robustness against erasure attacks [Liu et al., 2024a]."}, {"title": "6 Conclusion", "content": "In this work, we explore how to assign a segment to each bit of the watermark for embedding and design DERMARK, which introduces negligible additional overhead while meeting the dynamic embedding requirements and ensuring robustness against text editing. Comprehensive experiments on watermark capacity, efficiency, and robustness demonstrate that our approach provides a practical solution for multi-bit watermark embedding."}]}