{"title": "Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues and Challenges", "authors": ["Shagun Sinha"], "abstract": "This thesis presents abstractive text summarization models for contemporary Sanskrit prose and the challenges faced in developing the models. Different methods for text summarization (TS) training have long existed. However, some of the prevalent deep learning (DL) methods that have impressively succeeded in English and other high-resource languages (HRLs) serve as the motivation to apply the models to Sanskrit TS.", "sections": [{"title": "Introduction", "content": "Natural language processing (NLP) is a field of covering computer science and linguistics which develops computational models for processing, generating and analyzing natural language inputs (Lusetti, 2018). TS is a field under NLP in which models are trained to summarize important information from source texts. Two approaches to TS are popular in literature, extractive TS (ETS) and abstractive TS (ATS).\nETS summarizes the key information by picking sentences which contain the important information of the source text (Barve et al., 2016). ATS, on the other hand, summarizes the key information using new words. As a result, ATS involves more linguistic analysis for better processing and hence, it is more human-like in its summarization process (Chen & Bansal, 2018; Mishra & Gayen, 2018).\nThe approach with which an NLP task is performed has evolved over time. In TS, the earlier works were based on surface and linguistic features like word frequency, cue phrases etc."}, {"title": "Motivation \u2013 Why This Research?", "content": "I envisage two probable objections to this research.\nThe first objection may concern the utility of this project. \u201cWhy should a summarizer for Sanskrit be built at all?\u201d some may seek to know. Three key facts answer that question and hence, serve as the motivation for this work.\nFirst, an ATS may automatically summarize the vast literature of Sanskrit and provide to the research community for better access. The abundant literature of Sanskrit, including a large body of manuscripts, may be automatically summarized if a researcher aims to get an overview of manuscripts before studying them. As Reddy et al. (2018) observe, with large-scale digitization efforts in Sanskrit like the G\u00f6ttingen Register of Electronic Texts in Indian Languages (GRETIL), the Digital Corpus of Sanskrit (DCS) (Krishna et al., 2017), Digital Corpus of Sanskrit and Sanskrit Library of India, many manuscripts and texts have reached the public domain. However, due to technological and processing challenges, they are mostly inaccessible (Reddy et al., 2018). \u0410 summarization system would enhance accessibility by summarizing the contents of a text thereby enabling a prospective reader to decide whether or not to read it.\nSecond, Sanskrit offers an interesting field of research in NLP in general. The inflectional property of Sanskrit is one characteristic to be studied for summarization challenges just like in NLP which is increasingly seeking to adapt to inflectional languages (Hellwig, 2015a).\nThe third and simplest reason is that no abstractive summarization system has yet been built in a particular setting of Sanskrit which is a low-resource language (LRL). Literature offers many examples of ATS built in low-resource settings like low-"}, {"title": "Research Question", "content": "Given the why of this research work above, the what of this thesis is focused on training neural models for SATS and the key research question I answer is \u2013 what are the key challenges faced when ATS is developed for Sanskrit?\nTo answer the primary research question given above, the following are the sub-research questions based on four different themes that I have posed throughout this work:\n1. About training method\n a. What are the ways of training an ATS model in the literature?\n b. Which methods can be used in this research work?\n c. Why did this research work choose a given method?\n2. About data domain\n a. What is the desired dataset format for training?\n b. What are the challenges of building such a dataset?\n c. Why contemporary Sanskrit prose?\n d. What issues does data preprocessing pose?\n3. About the training process\n a. How was a specific training method used in this research?\n b. What are the challenges of deploying the chosen training method?\n4. About generated summaries\n a. What are the properties of the system-generated summaries?\n b. How should the summaries be evaluated?\n c. Why did some systems perform better?\n d. How can the summary quality be improved in the future?\nThe four themes above form the basis for the four core chapters of this work.\nQuestions based on the first theme have been answered in the chapter, literature review, which surveys the key works in TS and also establishes the methodology used in this research. The literature review also answers one question from the second theme, question 2(c).\nQuestions other than 2(c) from the second theme have been answered in the chapter, data preparation, which describes the preparation and preprocessing challenges.\nLikewise, questions from the third theme are presented in the chapter on experiments and results which presents the steps for implementing the training and the resources use for it.\nFinally, questions from the fourth theme have been covered in the chapter on results evaluation and discussion which analyzes the system-generated summaries, presents the evaluation scheme, and suggests path for future work.\nThe research questions will be answered within the limits of the following:"}, {"title": "Conceptual Framework \u2013 The How of This Work", "content": "Text summaries are mainly of two types \u2013 extractive (ETS) and abstractive (ATS). The first attempt at TS was by Luhn (1958) in which summaries of scientific articles were generated using surface-level features like word frequency, ranking sentences based on significance, and selecting the top-n best ranked sentences as the summary. His methods were extractive. The first attempt at ATS, however, was made very late after Luhn (Jones, 1999; Torres-Moreno, 2014). As noted by van Yperen et al. (2021), the headline-generation approach to TS by (Banko et al., 2000) was the first ATS-like system. FRUMP system developed at Yale is also one of the earliest ATS systems developed although very few attempts were made initially (Torres-Moreno, 2014). ATS is considered a preferred way of summarization because of its coherent summaries and consequently better quality. However, the metrics for evaluating the quality of TS systems varied for a long time before a metric called Recall-Oriented Understudy for"}, {"title": "Shallow Features vs Semantic Representation", "content": "TS in the initial years was more extractive owing to the lack of resources needed for deeper analysis required in abstraction. Statistical NLP dominated TS research for a long time with most efforts being extractive in nature as well as using surface-level features to gauge importance of a sentence before extracting it (Ferreira et al., 2013; Luhn, 1958). As a result, initial TS works analyzed a source text for its important sentences and ranked them according to certain threshold. If a sentence crossed that threshold, it was considered in the summary (Luhn, 1958). The sentence ranking process was based on shallow surface features used to represent the text such as word frequency, presence of certain cue-phrases, keyword presence, etc. (Edmundson, 1969). Such features are surface-level linguistic features which rely on the presence of tokens or specific phrases in a sentence to assess its importance.\nOn the other hand, representing the meaning of source text through graphs like abstract meaning representation (AMR) graphs and other techniques is called semantic representation (SR) which helps in improving semantic understanding (Mishra & Gayen, 2018). It may at times include syntactic representation methods such as part of speech (POS)-tagging, graph representation, named entity recognition (NER)-based methods, etc. (Embar et al., 2013; Liu et al., 2018)."}, {"title": "Machine Learning", "content": "The growth of ML and DL paved way for networks that could generate deeper representations of the source text enabling better understanding of the source text by the machine. With rise in efficient computational resources, abstractive summarization also became more popular and implementable. Starting with vanilla feedforward neural network (FFNN), NLP gradually moved to recurrent neural network (RNN) and long-short term memory (LSTM), gated recurrent unit (GRU) networks to handle sequential data such as natural language text (Goldberg, 2017). Sequential data like natural language sentences have words with long-range dependencies, i.e., a word at the starting of a sequence may be important to the last or later word in a sentence. As Section 1.3.4. explains, mapping such related words is an important challenge in NLP which the DL models continue to solve. Thus, neural networks and different learning methods could dig deeper than linguistic features to study the semantic features of a given input in natural language.\nBoth ML and DL need a lot of data to train on (Han et al., 2021). They primarily offer three methods of learning patterns from data \u2013 supervised learning (SL), unsupervised learning (USL), and reinforcement learning (RL). SL methods involve large-scale parallel dataset to learn input-output labeling. SL methods are widely used in classification tasks. USL refers to learning on large-scale unlabeled data and is mostly used in clustering, grouping, etc. RL trains a model while constantly giving feedback to the system in terms of rewards for improving its performance (Narayan et al., 2018b)."}, {"title": "Representations of Text", "content": "In moving from the word-level and surface-level to deeper semantic representations, one important development came with vector-based representation. The vector-based representations of text made textual analyses more meaningful, especially after the introduction of distributional hypothesis leading to distributed representations. Distributional hypothesis is the idea that words that occur in related contexts tend to have the same meaning (Firth, 1957; Harris, 1954). This hypothesis paved way for distributed representation of words with the aim to make machines better understand the context of a word, understanding similar words based on their meaning, and differentiating between them based on their contexts and semantics (Goldberg, 2017). Algorithms for distributed representations learn word representations from large scale monolingual corpora through unsupervised learning. Word2Vec algorithm for word embeddings is an algorithm for distributed representations introduced by (Mikolov, Sutskever, et al., 2013; Mikolov, Yih, et al., 2013). These vector models made reasoning possible with words given their ability to capture underlying semantics (Hobson et al., 2017)."}, {"title": "Encoder-Decoder Structures", "content": "Despite enhancements in semantic representations and advances in neural network architectures which could process sequential inputs, certain sequence generation tasks such as machine translation and text summarization posed immense challenges (Sutskever et al., 2014). Sequence-to-Sequence (Seq2Seq) learning problems required mapping an input sequence to an output sequence. Issues in these were solved by encoder-decoder architectures made their entry proving to be extremely beneficial for the implementation of sequence-to-sequence tasks (Sutskever et al., 2014). Encoder portion converts the input sequence into a hidden vector which is used by decoder to sequentially decode and convert into a desirable output sequence."}, {"title": "Transformer Architecture", "content": "The authors note that deep neural networks (DNN) are useful for classification tasks where input data dimension is not significant. However, for sequence generation tasks, input dimensions must be considered. The method used by Sutskever et al. (2014) used LSTMs for encoding as well as decoding inputs sequences thereby permitting the use of sequences.\nHowever, LSTM- and RNN-based encoder decoder setups could not handle long-term dependencies. A solution to this problem was introduced in 2014 called attention mechanism. Attention was introduced by (Bahdanau et al., 2014) for better alignment in MT tasks. Attention is a context vector that is conveys which portions of the input are important (Bahdanau et al., 2014). However, attention also has some disadvantages for which coverage or distraction mechanism was introduced (Lin & Ng, 2019, p. 9819).\nWhile sequence-to-sequence models by Sutskever et al. (2014) and attention mechanism by Bahdanau et al. (2014) had led to great improvements in DL-based NLP, it was the work by Rush et al. (2015) that first used neural methods for a paraphrase generation task which is close to summarization. The authors used the encoder-decoder architectures have been a great boon in resolving many sequential language processing tasks. With neural methods advancing in TS, many other solutions have also developed mostly in the form of modifications to the encoder-decoder models. One such advancement has been the advent of Transformers which led to a great development in NLP and TS.\nVaswani et al. (2017) introduced Transformer architecture which is a special type of neural encoder-decoder model that uses self-attention and is aimed at improving the ability of the"}, {"title": "Distributed Representation and Transformers", "content": "model to capture important portions of the source text. Self-attention or intra-attention is based on Key (K), Query (Q) and Value (V) vectors which are added as per the formula in (Vaswani et al., 2017):\nAttention (Q,K,V) = softmax ( \\frac{QK}{\\sqrt{d_k}} )V \\qquad (1)\nTransformers enhanced sequence processing through self-attention. The next section elaborates on their use in the direction of enhanced language understanding.\nWord embeddings such as Word2Vec (Mikolov, Yih, et al., 2013) outperformed the earlier language models in capturing relevant semantic content. However, those embeddings were not contextualized \u2013 that is, for a given word, the embeddings remained the same across different contexts. With ELMo (Peters et al., 2018), contextualized embeddings became possible. Models in NLP and TS have moved from non-contextualized word embeddings like word2vec, to contextualized embeddings like ELMo (Peters et al., 2018), and then to large Transformer- based language models (LM) which are the current state-of-the-art (SOTA) (Devlin et al., 2019; Rothe et al., 2020). With BERT and Transformer-based models (Devlin et al., 2019), the contextualized LMs became huge enough to capture language generalities and began being fine-tuned on downstream tasks while achieving results better than ELMo (Martin et al., 2020; Nemeskey, 2020). Section 2.5 discusses Transformers at length.\nThe key terms in current NLP space thus include \u2013 DL, ML, Transformers, LM. The next chapter will explore these in detail."}, {"title": "Contribution of this Thesis", "content": "In addition to initiating the first work in SATS and improving our understanding of neural methods for Sanskrit processing, this thesis contributes the following resources:\n1. A cleaned dataset for Sanskrit language model.\n2. Transformer-based checkpoints for SATS.\n3. A pipeline for building future works in the direction."}, {"title": "Resources Acknowledgment", "content": "This work uses the following resources:\n1. Google Colab Pro+ services, TPU Research Cloud (TRC), and Google Cloud Platform (GCP) support systems for training environments.\n2. HuggingFace Platform that made some of the most popular research works and models accessible free of cost (Wolf et al., 2019).\n3. Methodology from Rothe et al. (2020) which form the crux of this work.\n4. Publicly available Sanskrit resources including open access journal Anantaa, translated version of the speeches of the Prime Minister of India, collectively titled, Mann ki Baat, shared by Dr. Baldevanand Sagar, and OSCAR data shared by the developers (Su\u00e1rez et al., 2019)."}]}