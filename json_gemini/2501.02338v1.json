{"title": "Evaluating the Code Generation Capabilities of ChatGPT 4: A Comparative Analysis Across 19 Programming Languages", "authors": ["Laurenz Gilbert"], "abstract": "This bachelor thesis examines the code generation capabilities of ChatGPT 4 across 19 programming languages. It explores solution rates across three difficulty levels, error types, and code quality in terms of runtime and memory efficiency through a quantitative experiment. Data was collected on 188 programming problems from the LeetCode platform, with ChatGPT 4 having three attempts per problem to generate a correct solution, aided by feedback. ChatGPT 4 successfully solved 39.67% of all tasks, with the success rate decreasing significantly as task difficulty increased. The model demonstrated higher proficiency in widely used languages, likely due to the greater quantity and quality of training data available for these languages. Solution rates also indicated a preference for languages with lower abstraction levels and static typing. The Wrong Answer error was most common in highly popular languages, while compiler and runtime errors predominated in less popular languages, indicating misunderstandings regarding specific structural features. Across all programming languages, ChatGPT 4 demonstrated above-average runtime efficiency and favored statically typed, low-abstraction languages. Memory efficiency varied considerably, with above-average values in 14 languages and below-average values in 5 languages. Future research should include a greater number of tasks, iterations, and less popular languages. Additionally, ChatGPT 4's capabilities in code interpretation and summarization, debugging, and the development of complex, context-driven code could be analyzed.", "sections": [{"title": "1 Einleitung", "content": "Mit der Ver\u00f6ffentlichung von ChatGPT 4 im M\u00e4rz 2023 hat OpenAI neue Ma\u00dfst\u00e4be in der Entwicklung k\u00fcnstlicher Intelligenz gesetzt (OpenAI et al., 2024). Im Vergleich zum Vorg\u00e4ngermodell ChatGPT 3.5 weist die neueste Version der GPT-Reihe signifikant erweiterte Kenntnisse in verschiedenen Bereichen auf, darunter Rechts- und Naturwissenschaften, Sprachen, Mathematik und Medizin. Diese Bachelorarbeit widmet sich einem spezifischen Bereich, in dem ChatGPT 4 erweiterte Kompetenzen demonstriert: der Generierung von Programmcode (OpenAI et al., 2024).\nIn aktuellen Benchmarks zur Leistungsf\u00e4higkeit von Large Language Models (LLMs) bez\u00fcglich der L\u00f6sung von Programmieraufgaben demonstrieren die GPT-4-Modelle nicht nur eine signifikant bessere Performance im Vergleich zu GPT-3.5, sondern erreichen auch im Vergleich zu anderen popul\u00e4ren LLMs Spitzenpositionen (Liu et al., 2023; EvalPlus Leaderboard, o. D.). Es f\u00e4llt jedoch auf, dass sich die g\u00e4ngigen Benchmarks in der Regel auf eine oder wenige Programmiersprachen beschr\u00e4nken und die Ergebnisse als Ma\u00df f\u00fcr die allgemeinen Code-Generierungsf\u00e4higkeiten der Large Language Models interpretiert werden (Chen et al., 2021; Liu et al., 2023; Yu et al., 2024; Lai et al., 2023). Ein umfassender Vergleich der Leistung von ChatGPT 4 zwischen vielen verschiedenen Programmiersprachen fehlt derzeit, wodurch potenzielle Unterschiede zwischen den Sprachen bislang unber\u00fccksichtigt bleiben. Die vorliegende Bachelorarbeit zielt darauf ab, zur Schlie\u00dfung dieser Forschungsl\u00fccke beizutragen, indem die Leistungsdifferenzen von ChatGPT 4 zwischen 19 ausgew\u00e4hlten Programmiersprachen bei der L\u00f6sung allgemeiner Programmierprobleme systematisch untersucht werden.\nDie Analyse umfasst die Betrachtung der L\u00f6sungsraten, differenziert nach Schwierigkeitsgrad der Aufgaben, eine detaillierte Analyse der aufgetretenen Fehler sowie eine Bewertung der Codequalit\u00e4t anhand der Laufzeit- und Speicherwerte im Vergleich zu den Werten der L\u00f6sungen anderer Nutzer. Zur Strukturierung dieser Untersuchung werden drei Forschungsfragen formuliert:\nForschungsfrage 1 (F1): Wie effektiv l\u00f6st ChatGPT 4 allgemeine Programmierprobleme unterschiedlicher Schwierigkeitsgrade in 19 verschiedenen Programmiersprachen?\nForschungsfrage 2 (F2): Welche Fehler treten in den von ChatGPT 4 in 19 verschiedenen Programmiersprachen generierten L\u00f6sungen auf?\nForschungsfrage 3 (F3): Wie laufzeit- und speichereffizient sind die von ChatGPT 4 in 19 verschiedenen Programmiersprachen generierten L\u00f6sungen?\nZur Beantwortung der Forschungsfragen wird ChatGPT 4 in 19 ausgew\u00e4hlten Programmiersprachen auf 188 Code-Herausforderungen der Coding-Interview-Plattform LeetCode getestet. Dabei erh\u00e4lt ChatGPT 4 drei Versuche, in denen der Code mithilfe von Feedback verbessert werden soll. Alle Daten werden sorgf\u00e4ltig in einem Git-Repository dokumentiert und anschlie\u00dfend einer deskriptiven statistischen Analyse unterzogen, um Trends und Cluster zu ermitteln, die zur Beantwortung der Forschungsfragen dienen. Um eine"}, {"title": "2 Theoretischer Hintergrund", "content": "Im folgenden Abschnitt werden einerseits zentrale Begriffe, die f\u00fcr das Verst\u00e4ndnis dieser Arbeit essenziell sind, definiert und andererseits der aktuelle Forschungsstand zu den Code-Generierungsf\u00e4higkeiten von ChatGPT 4 in verschiedenen Programmiersprachen er\u00f6rtert."}, {"title": "2.1 Begriffsdefinitionen", "content": "Die Begriffsdefinitionen umfassen erstens das in dieser Bachelorarbeit verwendete KI-Modell ChatGPT 4, zweitens eine Erl\u00e4uterung zu den sogenannten Large Language Models (LLMs) und drittens eine Beschreibung der Plattform LeetCode, welche die Quelle der in dieser Untersuchung verwendeten Aufgaben darstellt und auf der die L\u00f6sungen von ChatGPT 4 bewertet werden."}, {"title": "2.1.1 ChatGPT 4", "content": "ChatGPT 4 ist ein interaktiver Chatbot, der auf der im M\u00e4rz 2023 von OpenAI vorgestellten GPT-4-Architektur basiert und im Vergleich zum Vorg\u00e4ngermodell, ChatGPT 3.5, weitreichend \u00fcberlegene F\u00e4higkeiten aufweist, beispielsweise in der Generierung von Programmcode (OpenAI et al., 2024; OpenAI, o. D.-a). W\u00e4hrend des sogenannten Pre-Trainings wurde GPT-4 auf einem umfangreichen Datenkorpus, der sowohl \u00f6ffentlich zug\u00e4ngliche als auch von OpenAI lizenzierte Daten umfasst, trainiert und anschlie\u00dfend durch Reinforcement Learning from Human Feedback (RLHF) feinabgestimmt (OpenAI et al., 2024). Dar\u00fcber hinaus wird das Modell kontinuierlich durch Nutzerdaten, beispielsweise durch Interaktionen mit ChatGPT 4, weiter trainiert (OpenAI, o. D.-b). Dank dieses Trainingsprozesses ist das Modell in der Lage, das n\u00e4chste Wort eines Textes anhand von Gewichtungen, den sogenannten weights, pr\u00e4zise vorherzusagen, um auf der Eingabe des Nutzers basierende kontextbezogene und koh\u00e4rente Antworten zu generieren (OpenAI, o. D.-a). Aufgrund des umfangreichen Datenkorpus, der w\u00e4hrend des Trainingsprozesses verwendet wurde und auch widerspr\u00fcchliche sowie inkorrekte Informationen enth\u00e4lt, kann ChatGPT 4 mitunter \u00fcberzeugende, jedoch unzutreffende Informationen produzieren, sogenannte Halluzinationen. Daher ist es ratsam, die von ChatGPT 4 generierten Antworten kritisch zu pr\u00fcfen, insbesondere wenn eine hohe Zuverl\u00e4ssigkeit der Information erforderlich ist (OpenAI et al., 2024)."}, {"title": "2.1.2 Large Language Models", "content": "Large Language Models (LLMs) sind Modelle k\u00fcnstlicher Intelligenz, die auf einem umfangreichen Textkorpus trainiert werden und mittels Deep-Learning-Techniken dazu in der Lage sind, nat\u00fcrliche Sprache zu verstehen sowie Texte und andere Inhalte, wie Programmcode, zu generieren (IBM, o. D.). LLMs bestehen aus vielschichtigen neuronalen Netzen mit mehreren Milliarden Parametern und basieren typischerweise auf der Transformer-Architektur, welche von Forschern bei Google im Jahr 2017 vorgestellt wurde"}, {"title": "2.1.3 LeetCode", "content": "LeetCode ist eine Plattform, die Programmieraufgaben in unterschiedlichen Themenbereichen und Schwierigkeitsgraden (leicht, mittel, schwer) anbietet und dabei insgesamt 27 verschiedene Programmiersprachen, einschlie\u00dflich kompilierter und interpretierter Sprachen sowie Skript- und Datenbanksprachen, unterst\u00fctzt (LeetCode, 2024; LeetCode, o. D.-c). Das Ziel der Plattform ist es, die F\u00e4higkeiten der Nutzer zu erweitern und sie auf technische Interviews vorzubereiten (LeetCode, o. D.-b). Jede Aufgabe enth\u00e4lt eine detaillierte Beschreibung mit Beispielen und Einschr\u00e4nkungen sowie eine Codevorlage zur Implementierung. Mit dem integrierten Interpreter bzw. Compiler kann der Code direkt auf der Plattform ausgef\u00fchrt werden, wobei entweder die spezifische Fehlermeldung oder eine Best\u00e4tigung des erfolgreichen Bestehens aller Tests ausgegeben wird. Zus\u00e4tzlich erfasst die Plattform die Speicher- und Laufzeitwerte des akzeptierten Codes, vergleicht diese mit den Werten der L\u00f6sungen anderer Nutzer und erstellt daraus ein Ranking, das dem Nutzer eine Einsch\u00e4tzung im Vergleich zu anderen erm\u00f6glicht. Dar\u00fcber hinaus bietet die Plattform die M\u00f6glichkeit, sich innerhalb der Community \u00fcber L\u00f6sungen auszutauschen (LeetCode, o. D.-d)."}, {"title": "2.2 Aktueller Forschungsstand", "content": "Der nachfolgende Abschnitt untersucht die neuesten Erkenntnisse und Fortschritte in der Erforschung der Code-Generierungsf\u00e4higkeiten von ChatGPT 4 \u00fcber verschiedene Programmiersprachen hinweg. Im Zuge der Literaturrecherche wurde deutlich, dass sich die vorherrschende Forschung prim\u00e4r auf eine Leistungsanalyse von ChatGPT 4 in einer oder wenigen Programmiersprachen konzentriert. Hierbei wird die Leistung in einer spezifischen Sprache repr\u00e4sentativ f\u00fcr die generelle Code-Generierungsf\u00e4higkeit von ChatGPT 4 verwendet, wodurch jedoch potenzielle Unterschiede in der Effizienz des Modells \u00fcber verschiedene Sprachen hinweg vernachl\u00e4ssigt werden. Zudem wird h\u00e4ufig ein Vergleich zwischen ChatGPT 4 und anderen LLMs vorgenommen, wobei der Fokus weniger auf den spezifischen Programmiersprachen liegt als vielmehr auf einem Vergleich der Modelle untereinander. F\u00fcr diese Evaluierung werden in der Regel Benchmarks wie HumanEval, EvalPlus, CoderEval und DS-1000 herangezogen (Chen et al., 2021; Liu et al., 2023; Yu et al., 2024; Lai et al., 2023). W\u00e4hrend der Durchf\u00fchrung der Literaturrecherche wurde zudem festgestellt, dass sich der Gro\u00dfteil der Studien auf die Vorg\u00e4ngerversion 3.5 von ChatGPT bezieht, anstatt auf die in dieser Arbeit untersuchte aktuelle Version 4.\nDiese Tendenz unterstreicht die Herausforderung, spezifische Studien zu finden, die sich explizit mit der Code-Generierungsf\u00e4higkeit von ChatGPT 4 auseinandersetzen und dabei einen direkten Vergleich zwischen verschiedenen Programmiersprachen ziehen, was die Notwendigkeit f\u00fcr weiterf\u00fchrende Forschungen in diesem Bereich verdeutlicht. In Anbetracht der bestehenden Forschungsl\u00fccke richtet sich das Augenmerk auf ausgew\u00e4hlte Studien, die den gegenw\u00e4rtigen Stand der Forschung repr\u00e4sentieren. Zun\u00e4chst wird eine besonders relevante Untersuchung er\u00f6rtert, die eine komparative Analyse der Code-Generierungsf\u00e4higkeiten des Vorg\u00e4ngermodells ChatGPT 3.5 in verschiedenen Programmiersprachen durchf\u00fchrt. Darauf folgen Studien, die die F\u00e4higkeiten zur Code-Generierung von ChatGPT 4 in einer begrenzten Anzahl von Programmiersprachen evaluieren. Abschlie\u00dfend konzentriert sich die Betrachtung auf Forschungsarbeiten, die einen Leistungsvergleich zwischen ChatGPT 4 und anderen LLMs in einer einzelnen Programmiersprache vornehmen.\nEin wegweisendes Beispiel f\u00fcr diese Bachelorarbeit stellt die Studie von Buscemi aus dem Jahr 2023 dar, welche die F\u00e4higkeiten von ChatGPT zur Code-Generierung in zehn Programmiersprachen untersucht. Zu den untersuchten Sprachen z\u00e4hlen C, C++, Go, Julia, JavaScript, Python, Perl, Ruby, R und Smalltalk, wobei C, C++, Go, JavaScript, Python und Ruby auch Gegenstand der vorliegenden Arbeit sind. In der betrachteten Studie kommt die vorherige Version 3.5 von ChatGPT zum Einsatz, wobei in dieser Arbeit die aktuelle Version 4 verwendet wird. Die Studie von Buscemi (2023) bewertet die Korrektheit, die ben\u00f6tigte Zeit und die L\u00e4nge des generierten Codes. In der vorliegenden Arbeit werden die L\u00e4nge des Codes und die Generierungszeit nicht ber\u00fccksichtigt. Stattdessen erfolgt zus\u00e4tzlich zur Code-Korrektheit eine Analyse der Laufzeit- und Speichereffizienz sowie der aufgetretenen Fehler.\nBuscemi (2023) nutzte zur Durchf\u00fchrung der Studie einen Datensatz, der aus 40 Aufgaben aus den Bereichen Data Science, Algorithmen, Sicherheit und Spieleentwicklung bestand. Diese Aufgaben wurden von Universit\u00e4tswebseiten und Coding-Plattformen entnommen. Im Gegensatz dazu verwendet diese Arbeit ausschlie\u00dflich Herausforderungen der Plattform LeetCode und bezieht 188 allgemeine Programmierprobleme in die Analyse ein. Ein wesentlicher Kritikpunkt an der Studie von Buscemi (2023) besteht darin, dass nicht \u00fcberpr\u00fcft wurde, ob ChatGPT 3.5 m\u00f6glicherweise auf L\u00f6sungen der gestellten Probleme trainiert wurde. In der vorliegenden Arbeit wurde hingegen darauf geachtet, diese M\u00f6glichkeit auszuschlie\u00dfen.\nBez\u00fcglich der Korrektheit des generierten Codes zeigte sich, dass ChatGPT 3.5 \u00fcber alle Programmiersprachen hinweg in 45,8 % der F\u00e4lle korrekten Code erzeugen konnte. Dabei erzielte das Modell die h\u00f6chste Erfolgsrate in der Programmiersprache Julia mit 81,5 %, w\u00e4hrend die niedrigste L\u00f6sungsrate mit 7,3 % in C++ verzeichnet wurde. Insgesamt stellte sich jedoch heraus, dass ein Gro\u00dfteil des von ChatGPT 3.5 generierten Codes trotz der vergleichsweise einfachen Aufgabenstellung \u00fcberwiegend nicht ausf\u00fchrbar war und die Wahl der Programmiersprache das Verst\u00e4ndnis der Aufgabenanforderungen beeinflusste.\nIn Bezug auf die Themengebiete zeigte ChatGPT 3.5 ebenfalls spezifische St\u00e4rken und Schw\u00e4chen. Das Modell war in den Programmiersprachen C, C++ und Smalltalk besonders effektiv bei Sicherheitsaufgaben, w\u00e4hrend es bei algorithmenbasierten Aufgaben in Go, JavaScript, Perl, Python und Ruby die besten Ergebnisse erzielte. In den Programmiersprachen Julia und R wurden hohe Leistungen im Bereich Data Science festgestellt, w\u00e4hrend in der Spieleentwicklung \u00fcber alle Programmiersprachen hinweg niedrigere Erfolgsquoten beobachtet wurden.\nDie Antwortzeiten variierten ebenfalls erheblich. ChatGPT 3.5 ben\u00f6tigte f\u00fcr C im Durchschnitt 60 % mehr Zeit als f\u00fcr andere Programmiersprachen, w\u00e4hrend C++-Code in etwa der H\u00e4lfte der durchschnittlichen Zeit generiert wurde. Bei zus\u00e4tzlicher Betrachtung der Code-L\u00e4nge stellte Buscemi (2023) weiterhin keine Korrelation zwischen der L\u00e4nge des Codes und der Antwortzeit fest. Es wurde jedoch eine gr\u00f6\u00dfere Variabilit\u00e4t in der Code-L\u00e4nge im Vergleich zur Antwortzeit beobachtet. Daraus schlie\u00dft Buscemi (2023), dass der zeitliche Aufwand f\u00fcr das Verst\u00e4ndnis der Aufgabenstellung geringer ist als f\u00fcr die Konzeption und das Schreiben des Codes.\nBuscemi (2023) kam zu dem Schluss, dass die Effektivit\u00e4t von ChatGPT 3.5 bei der Code-Generierung stark von der verwendeten Programmiersprache abh\u00e4ngt. In dynamisch typisierten Sprachen hoher Abstraktionsebene erzielt ChatGPT 3.5 bessere Ergebnisse als in statisch typisierten Sprachen mit niedriger Abstraktionsebene. Das Modell erweist sich in popul\u00e4ren Programmiersprachen als effektiver, da f\u00fcr diese umfangreichere Datens\u00e4tze verf\u00fcgbar sind, was ein umfassenderes Training erm\u00f6glicht.\nIm folgenden Abschnitt werden zwei wissenschaftliche Studien untersucht, die die F\u00e4higkeiten von ChatGPT 4 zur Code-Generierung in einer begrenzten Anzahl von Programmiersprachen evaluieren.\nW\u00e4hrend die Studie von Buscemi (2023) ein breites Spektrum von Programmiersprachen abdeckt, fokussiert sich die neuere Untersuchung von Bucaioni et al. (2024) auf die Programmiersprachen C++ und Java. Dabei wurde ChatGPT 4 auf 120 Programmierprobleme aus neun Kategorien und drei Schwierigkeitsgraden (einfach, mittel und schwer) der Plattform LeetCode getestet. Wie die vorliegende Bachelorarbeit nutzten Bucaioni et al. (2024) dieselbe Quelle f\u00fcr Programmierprobleme, die aktuelle Version 4 von ChatGPT, eine umfangreiche Anzahl an Programmierproblemen sowie die Metriken der L\u00f6sungsrate und der Speicher- und Laufzeiteffizienz. Im Gegensatz dazu fokussierten sich die Autoren jedoch auf den Vergleich zwischen C++ und Java und zogen dabei einen Vergleich zwischen ChatGPT 4 und menschlichen Programmierern, was in der vorliegenden Bachelorarbeit nicht vorgenommen wird.\nWie in dieser Arbeit wurde es ChatGPT 4 \u00fcber drei Iterationen hinweg erm\u00f6glicht, die L\u00f6sung, basierend auf bereitgestelltem Feedback, zu verbessern. Dabei wurden die L\u00f6sungsraten, anders als in der vorliegenden Arbeit, nach Iterationen getrennt betrachtet. F\u00fcr C++ wurde festgestellt, dass bei einfachen bis mittelschweren Aufgaben 96,67 % der L\u00f6sungen die Testsuite von LeetCode bereits in der ersten Iteration erfolgreich durchliefen, was sich im zweiten Durchgang auf 100 % steigerte. Bei schwierigen Aufgaben erreichte ChatGPT 4 eine Erfolgsquote von 76,67 %, die in der zweiten Iteration auf 93,33 % anstieg und in der dritten Iteration konstant blieb. Die spezifischen Ergebnisse f\u00fcr Java wurden von den Autoren nicht offengelegt, jedoch erw\u00e4hnten sie \u00e4hnliche Beobachtungen hinsichtlich der L\u00f6sungsraten. Diese mangelnde Transparenz ist kritisch zu betrachten.\nBei der Laufzeiteffizienz erreichte ChatGPT 4 f\u00fcr C++-Code Perzentile von 58, 59 und 56 bei einfachen, mittelschweren und schweren Aufgaben. Dies bedeutet, dass der Anteil der menschlichen L\u00f6sungen, die eine l\u00e4ngere Laufzeit aufwiesen und somit qualitativ unterlegen waren, im Vergleich zu den von ChatGPT 4 generierten L\u00f6sungen 58 %, 59% beziehungsweise 56 % betrug. F\u00fcr Java-Code erreichte das Modell bei einfachen Aufgaben fast das 100. Perzentil, schnitt jedoch bei mittelschweren und schweren Aufgaben deutlich schlechter ab.\nHinsichtlich der Speichereffizienz rangierte der C++-Code im 58., 47. und 56. Perzentil bei einfachen, mittelschweren beziehungsweise schweren Problemen. Die durchschnittlichen Werte des Java-Codes wurden textuell nicht erl\u00e4utert, was eine bedeutende L\u00fccke darstellt.\nBez\u00fcglich dieser Metriken stellten Bucaioni et al. (2024) zudem fest, dass der Versuch, den Code bez\u00fcglich Laufzeit und Speichereffizienz durch Hinzuf\u00fcgen eines Vermerks zum Prompt zu optimieren, paradoxerweise zu einer Verschlechterung der Speicher- und Laufzeitwerte f\u00fchrte, w\u00e4hrend das Hinzuf\u00fcgen dieser Information die L\u00f6sungsrate verbesserte, was Bucaioni et al. (2024) zu der Vermutung veranlasste, dass hier ein Kompromiss zwischen L\u00f6sungsrate und Speicher- sowie Laufzeiteffizienz besteht.\nBucaioni et al. (2024) zogen das Fazit, dass ChatGPT 4 bei Aufgaben von einfacher bis mittlerer Komplexit\u00e4t in Java und C++ \u00fcberzeugende L\u00f6sungsraten erzielt, w\u00e4hrend bei anspruchsvolleren Aufgaben eine geringere Effektivit\u00e4t zu beobachten ist.\nSie empfehlen daher eine Verbesserung der Programmierf\u00e4higkeiten von ChatGPT, da dessen Leistungen bei komplexen Aufgaben noch begrenzt erscheinen. Obwohl dieser Schluss nachvollziehbar ist, sollte betont werden, dass eine L\u00f6sungsrate von 93,33 % bei schwierigen Aufgaben dennoch ein beachtliches Ergebnis darstellt. Eine m\u00f6gliche Erkl\u00e4rung f\u00fcr diesen hohen Wert k\u00f6nnte sein, dass in der Studie auch Aufgaben ber\u00fccksichtigt wurden, die vor dem letzten Trainingsstand von ChatGPT 4 auftraten (OpenAI, 2023b). Dies k\u00f6nnte die Aussagekraft der Ergebnisse verringern, da das Modell die L\u00f6sungen m\u00f6glicherweise im Voraus kannte.\nObgleich ChatGPT 4 beachtliche Ergebnisse hinsichtlich Laufzeit- und Speichereffizienz erzielen konnte, kommen Bucaioni et al. (2024) zu dem Schluss, dass die Leistungen des Modells hinsichtlich Laufzeit- und Speichereffizienz im Vergleich zu menschlichen Programmierern noch nicht optimal sind. Es ist jedoch zu beachten, dass die Annahme, hinter den L\u00f6sungen st\u00fcnden ausschlie\u00dflich menschliche Programmierer, m\u00f6glicherweise nicht zutrifft. LeetCode wird, wie in dieser Literaturrecherche gezeigt, zunehmend in wissenschaftlichen Untersuchungen zu LLMs verwendet, weshalb die Vergleichsdaten auch nichtmenschliche L\u00f6sungen umfassen. Abschlie\u00dfend legen die Autoren nahe, dass ChatGPT 4 in gegenw\u00e4rtiger Form vorzugsweise als ein unterst\u00fctzendes Werkzeug denn als vollst\u00e4ndiger Ersatz angesehen werden sollte.\nNach \u00e4hnlichem Prinzip untersucht die Studie von Zhang et al. (2023) die F\u00e4higkeiten der Modelle GPT-3.5 und GPT-4 zur Generierung von Python-Code, einer Programmiersprache, die in der vorliegenden Bachelorarbeit ebenfalls betrachtet wird. Die Untersuchung basiert auf der Bewertung von L\u00f6sungen zu Programmieraufgaben, sogenannten Katas, von der Plattform Codewars, welche ein \u00e4hnliches Konzept wie die in dieser Arbeit verwendete Plattform LeetCode verfolgt. Diese Aufgaben decken ein breites Spektrum an Schwierigkeitsgraden ab: von 8 kyu f\u00fcr die einfachsten bis hin zu 1 kyu f\u00fcr die anspruchsvollsten Aufgaben. Die Autoren klassifizieren die Niveaus acht bis f\u00fcnf als leicht, vier und drei als mittelschwer sowie zwei und eins als schwer. In \u00e4hnlicher Weise werden in der vorliegenden Arbeit die Aufgaben nach den Schwierigkeitsgraden leicht, mittel und schwer eingeteilt.\nObwohl der Fokus von Zhang et al. (2023) auf dem Vergleich der beiden Modelle untereinander und mit menschlichen Programmierern liegt \u2013 ein Aspekt, der in dieser Arbeit nicht ber\u00fccksichtigt wurde \u2013, liefert die Studie wichtige Einblicke in die Code-Generierungsf\u00e4higkeiten von GPT-4 in Python. Es ist wichtig hervorzuheben, dass die Untersuchung auf nur 24 Probleme beschr\u00e4nkt ist. Zudem f\u00fchren die Autoren die L\u00f6sung eines Problems auf 2 kyu-Level durch GPT-4 auf ein vorheriges Training des Modells zur\u00fcck, was darauf hindeutet, dass GPT-4 einige L\u00f6sungen potenziell bereits im Vorhinein kannte. Die Aussagekraft der Ergebnisse ist dahingehend eingeschr\u00e4nkt.\nDie Ergebnisse der Studie zeigten, dass beide Modelle Aufgaben bis zu einem Schwierigkeitsgrad von 4 kyu effektiv l\u00f6sen konnten. Bei h\u00f6heren Schwierigkeitsgraden ab 3 kyu stie\u00dfen beide Modelle jedoch an ihre Grenzen. GPT-4 war dabei zuverl\u00e4ssiger und konnte alle 15 Aufgaben von 8 kyu bis 4 kyu erfolgreich l\u00f6sen, wobei es f\u00fcr vier Aufgaben auf Feedback angewiesen war. Zudem gelang es GPT-4, ein Problem auf dem 2 kyu-Niveau zu l\u00f6sen. Obwohl GPT-3.5 die meisten Aufgaben bis zum 4 kyu-Level bew\u00e4ltigen konnte, traten mehr Schwierigkeiten auf, wobei das Modell f\u00fcr f\u00fcnf der gel\u00f6sten Aufgaben auf Feedback angewiesen war und vier der 15 Probleme bis zum 4 kyu-Level nicht l\u00f6sen konnte.\nBeide Modelle zeigten ein gutes Verst\u00e4ndnis f\u00fcr die Aufgaben und ihre Anforderungen, auch wenn die endg\u00fcltige L\u00f6sung nicht immer korrekt war, und konnten ihre L\u00f6sungsans\u00e4tze durch gegebenes Feedback verbessern. Allerdings f\u00fchrte Feedback \u00fcber f\u00fcnf Runden hinaus zu keinen weiteren Verbesserungen. Kritisch zu betrachten ist, dass das Feedback keinem einheitlichen Schema folgte, sondern vergleichsweise informell und willk\u00fcrlich gew\u00e4hlt wurde. Dies ist ein Unterschied zu der vorliegenden Arbeit, da hier die Bereitstellung von Feedback auf maximal drei Iterationen begrenzt wurde und es einem einheitlichen Schema folgt.\nZhang et al. (2023) schlie\u00dfen, dass GPT-4 das Niveau von Junior-Programmierern erreicht hat. Sie betonen zwar das Potenzial von GPT-4, merken jedoch an, dass das Modell derzeit noch nicht in der Lage ist, menschliche Programmierer vollst\u00e4ndig zu ersetzen. Gr\u00fcnde hierf\u00fcr sind unter anderem das Fehlen kreativen Denkverm\u00f6gens und die Unf\u00e4higkeit zur Selbstvalidierung der erzeugten L\u00f6sungen.\nIm letzten Abschnitt werden zwei weitere wissenschaftliche Arbeiten hervorgehoben, die einen Leistungsvergleich von ChatGPT 4 mit anderen LLMs in einer spezifischen Programmiersprache durchf\u00fchren.\nIn einer 2023 ver\u00f6ffentlichten Studie von Bubeck et al. wurden verschiedene LLMs, darunter GPT-4, text-davinci-003 (ChatGPT 3.5), Codex (code-davinci-002) und CODEGEN-16B, hinsichtlich der L\u00f6sung der 164 Python-Programmierprobleme des HumanEval-Benchmarks sowie auf 100 LeetCode-Probleme der Sprache Python untersucht und verglichen. Zus\u00e4tzlich wurden die Modelle GPT-3.5 und GPT-4 in realistischeren Anwendungsszenarien evaluiert. In der vorliegenden Bachelorarbeit wurde ein solcher Vergleich nicht durchgef\u00fchrt, da der Fokus auf dem Vergleich der Code-Generierungsf\u00e4higkeiten von ChatGPT 4 in verschiedenen Programmiersprachen liegt.\nDie Ergebnisse des ersten Benchmarks zeigten, dass GPT-4 mit einer Erfolgsquote von 82% bei der L\u00f6sung der HumanEval-Probleme deutlich vor text-davinci-003 mit 65%, Codex mit 39 % und CODEGEN-16B mit 30% lag, was eine \u00dcberlegenheit des Modells zeigte. Die Autoren \u00e4u\u00dferten jedoch die Vermutung, dass GPT-4 m\u00f6glicherweise speziell auf diese Art von Aufgaben trainiert worden sein k\u00f6nnte.\nUm eine potenzielle Verzerrung auszuschlie\u00dfen, wurden zus\u00e4tzlich 100 LeetCode-Probleme der Programmiersprache Python herangezogen, die nach dem 8. Oktober 2022 ver\u00f6ffentlicht wurden. Dieses Datum liegt nach dem Ende der Trainingsperiode von GPT-4 zum Zeitpunkt der Studienerstellung, was einen unvoreingenommenen Vergleich gew\u00e4hrleisten sollte. Ein \u00e4hnliches Vorgehen wurde auch in der vorliegenden Bachelorarbeit angewandt.\nF\u00fcr diesen zweiten Benchmark verwendeten Bubeck et al. (2023) die pass@k-Bewertungsmethode, bei der das Modell k L\u00f6sungen generierte und anschlie\u00dfend gepr\u00fcft wurde, ob darunter eine korrekte L\u00f6sung vorlag. Diese Methode unterscheidet sich von der in der vorliegenden Arbeit verwendeten, insbesondere darin, dass der Code in den Versuchen der Studie von Bubeck et al. (2023) nicht durch Feedback verbessert wurde, w\u00e4hrend dies in der Bachelorarbeit der Fall ist. Dar\u00fcber hinaus beschr\u00e4nkt sich die Bachelorarbeit auf drei Iterationen, w\u00e4hrend Bubeck et al. (2023) die Erfolgsrate sowohl nach einem als auch nach f\u00fcnf Versuchen ma\u00dfen.\nBei den einfachen Problemen erreichte GPT-4 eine Erfolgsquote von 68,2 % in einem und 86,4 % in f\u00fcnf Versuchen. F\u00fcr die mittelschweren Probleme lag die Erfolgsrate bei 40 % in einem Versuch und erh\u00f6hte sich auf 60% nach f\u00fcnf Versuchen. Bei den schweren Problemen erzielte GPT-4 eine Erfolgsrate von 10,7 % im ersten Versuch und 14,3% nach f\u00fcnf Versuchen, was in einer Gesamterfolgsrate von 38 % im ersten Versuch und 53 % in f\u00fcnf Versuchen resultierte. Im Vergleich dazu erreichte text-davinci-003 eine Gesamterfolgsrate von 19% im ersten Versuch und 36% nach f\u00fcnf Versuchen. Codex verzeichnete eine Gesamterfolgsrate von 13 % im ersten Versuch und 23 % nach f\u00fcnf Versuchen. Menschliche Programmierer, spezifisch LeetCode-Nutzer, erreichten eine Rate von 38,2 %. Bubeck et al. (2023) folgern, dass GPT-4 den anderen Modellen in dieser Hinsicht \u00fcberlegen ist und vergleichbare Ergebnisse wie menschliche Programmierer liefert. Es ist jedoch kritisch anzumerken, wie bereits bei der Studie von Bucaioni et al. (2024) angemerkt, dass es sich bei den LeetCode-Nutzern m\u00f6glicherweise nicht ausschlie\u00dflich um Menschen handelt.\nZus\u00e4tzlich f\u00fchrten die Autoren einen dritten Benchmark durch, um die F\u00e4higkeiten von GPT-4 im Vergleich zu GPT-3.5 bei realistischeren Programmierherausforderungen zu evaluieren. Dies stellt eine sinnvolle Erweiterung der Analyse dar, welche in der vorliegenden Bachelorarbeit nicht durchgef\u00fchrt wurde, da dies \u00fcber den vorgesehenen Umfang hinausgehen w\u00fcrde. Die untersuchten Aufgaben umfassen die Bereiche der Datenvisualisierung, LaTeX, Front-End-Entwicklung und Deep Learning.\nIm Bereich der Datenvisualisierung extrahierten beide Modelle erfolgreich Daten aus LaTeX-Code, um Diagramme in Python zu erstellen. GPT-4 \u00fcbertraf GPT-3.5 deutlich, indem es korrekte Diagramme erstellte und effektiv auf Anfragen zur weiteren Datenmanipulation reagierte, w\u00e4hrend GPT-3.5 bereits bei der Erstellung korrekter Diagramme scheiterte. In der Front-End- und Spieleentwicklung konnte GPT-4 funktionsf\u00e4higen Code f\u00fcr ein komplexes 3D-Spiel in HTML und JavaScript generieren, w\u00e4hrend GPT-3.5 sich weigerte, den angeforderten Code zu erstellen. Im Bereich Deep Learning sollten die Modelle ein benutzerdefiniertes Optimierungsmodul schreiben. Beide produzierten syntaktisch korrekten Code, aber nur GPT-4 erf\u00fcllte weitgehend die Instruktionen, w\u00e4hrend GPT-3.5 signifikante Fehler machte. Bei der Verarbeitung von LaTeX-Code konvertierte GPT-4 fehlerhaften Code erfolgreich in ausf\u00fchrbare LaTeX-Kommandos, w\u00e4hrend GPT-3.5 diese Aufgabe misslang.\nBubeck et al. (2023) kommen zu dem Schluss, dass GPT-4 im Vergleich zu den anderen untersuchten Modellen eine signifikante \u00dcberlegenheit aufweist, sowohl hinsichtlich des HumanEval-Datensatzes als auch bei der L\u00f6sung von 100 LeetCode-Problemen. Dar\u00fcber hinaus zeigt GPT-4 gegen\u00fcber GPT-3.5 \u00fcberlegene F\u00e4higkeiten bei realistischeren Programmieraufgaben, was die erweiterte Anwendbarkeit des Modells in verschiedenen Programmierkontexten hervorhebt.\nDie abschlie\u00dfende Arbeit dieser Literaturrecherche bildet die Studie von Coello et al. (2024). \u00c4hnlich wie in der Studie von Bubeck et al. (2023) werden in dieser Untersuchung die Code-Generierungsf\u00e4higkeiten verschiedener LLMs, darunter GPT-4, GPT-3.5, Anthropics Claude, das auf GPT-4 basierende Bing sowie Googles Bard, untersucht. Der Fokus liegt dabei auf der Leistung dieser Modelle beim L\u00f6sen von 460 Problemen aus dem Mostly Basic Python Problems (MBPP)-Datensatz, der von Google-Forschern entwickelt wurde und sich an Programmieranf\u00e4nger richtet. Dies stellt einen bedeutenden Unterschied zu dem in der vorliegenden Bachelorarbeit verwendeten Datensatz dar, da dieser auch Probleme h\u00f6herer Schwierigkeitsgrade umfasst, die f\u00fcr Anf\u00e4nger ungeeignet sind. Die Beschr\u00e4nkung auf einfache Programmierprobleme sollte beim Betrachten der Ergebnisse ber\u00fccksichtigt werden. Des Weiteren untersuchten Coello et al. (2024) die F\u00e4higkeit von GPT-4 und Bard, Feedback zu verstehen und ihre Code-L\u00f6sungen entsprechend zu verbessern.\nIm ersten Benchmark zeigten sich deutliche Leistungsunterschiede zwischen den Modellen. Claude erreichte mit einer L\u00f6sungsrate von 71,43 % das schlechteste Ergebnis, gefolgt von Google Bard mit 76,16 %. Bing, basierend auf GPT-4, erzielte eine L\u00f6sungsrate von 81,96 %, w\u00e4hrend GPT-3.5 mit einem Wert von 83,18% knapp dar\u00fcber lag. An der Spitze stand GPT-4 mit 87,51 %. Eine bemerkenswerte Beobachtung war, dass nicht auf der GPT-Architektur basierende Modelle tendenziell l\u00e4ngeren Code generierten, w\u00e4hrend GPT-basierte Modelle effizienteren, pr\u00e4ziseren und kompakteren Code erzeugten.\nIm zweiten Vergleich testeten Coello et al. (2024) die F\u00e4higkeit von GPT-4 und Bard, aus manuell bereitgestelltem Feedback zu lernen und ihre Code-L\u00f6sungen entsprechend anzupassen. Es ist hierbei kritisch anzumerken, dass die Beschaffenheit des Feedbacks nicht offengelegt wurde. W\u00e4hrend GPT-4 bei 16 nicht im ersten Anlauf gel\u00f6sten Aufgaben 14 davon im zweiten Versuch mit Hilfe von Feedback korrigieren konnte, gelang dies Bard nur bei f\u00fcnf von 16 Aufgaben.\nDie Autoren kommen zu dem Schluss, dass GPT-4 im Vergleich zu anderen LLMs eine \u00dcberlegenheit bei der L\u00f6sung von Python-Aufgaben zeigt. Dabei weisen die GPT-Modelle die h\u00f6chsten Kompetenzen auf und die Modelle Claude und Bard die niedrigsten. Zudem unterstreichen die Autoren die Effektivit\u00e4t von GPT-4 im Umgang mit Feedback und dessen potenzielle N\u00fctzlichkeit als Programmierassistent. Coello et al. (2024) betonen jedoch, dass GPT-4 und die anderen betrachteten LLMs die Notwendigkeit menschlicher \u00dcberwachung und Feedback nicht eliminieren und somit aktuell nicht als vollst\u00e4ndiger Ersatz f\u00fcr menschliche Programmierer betrachtet werden k\u00f6nnen.\nDie vorangegangenen Studien zeigen die Wichtigkeit einer vergleichenden Analyse der Code-Generierungsf\u00e4higkeiten von ChatGPT 4 \u00fcber verschiedene Programmiersprachen hinweg. Eine solche Untersuchung ist essenziell, um die bestehende wissenschaftliche Forschung zu erg\u00e4nzen und eine pr\u00e4zise Bewertung der aktuellen F\u00e4higkeiten von ChatGPT 4 in der Code-Generierung zu erm\u00f6glichen, die wiederum die Basis f\u00fcr m\u00f6gliche Modellverbesserungen darstellen k\u00f6nnte. Dabei wird sichergestellt, dass ChatGPT 4 nicht vorab auf spezifische L\u00f6sungen trainiert wurde. Die Forschung ber\u00fccksichtigt Aufgaben unterschiedlicher Komplexit\u00e4tsstufen und verwendet einen zuverl\u00e4ssigen und umfangreichen Datensatz, um fundierte Schlussfolgerungen zu erm\u00f6glichen. Zudem erh\u00e4lt ChatGPT 4 die M\u00f6glichkeit, den Code durch Feedback zu optimieren, das nach einem einheitlichen Schema strukturiert ist. Im Zentrum der Analyse stehen die L\u00f6sungsraten in verschiedenen Programmiersprachen und eine Fehleranalyse, die darauf abzielt, spezifische Schw\u00e4chen in den Programmiersprachen zu identifizieren. Die Untersuchung umfasst weiterhin eine Bewertung der Laufzeit- und Speichereffizienz im Vergleich zu L\u00f6sungen anderer Nutzer, um eine fundierte Beurteilung der Code-Qualit\u00e4t des von ChatGPT 4 generierten Codes zu erm\u00f6glichen."}, {"title": "3 Methodik", "content": "Das Ziel dieser Bachelorarbeit besteht darin", "angewendet": "eine Literaturrecherche und ein quantitatives Experiment. Dabei wird einem induktiven Forschungsansatz gefolgt, bei dem auf Basis der experimentell gewonnenen Erkenntnisse generalisierte Aussagen \u00fcber die Leistungsf\u00e4higkeit von ChatGPT 4 in den betrachteten Programmiersprachen abgeleitet werden.\nUm einen \u00dcberblick \u00fcber den aktuellen Forschungsstand bez\u00fcglich der Code-Generierungsf\u00e4higkeit von ChatGPT 4 zu erlangen und die aktuellen Erkenntnisse mit den Ergebnissen dieser Arbeit vergleichen zu k\u00f6nnen, wurde eine Literaturrecherche unter Verwendung des PRISMA-Flussdiagramms vollzogen (Page et al., 2021). Die Resultate dieser Recherche sind in Abschnitt 2.2 dargestellt. Diese Methode wurde gew\u00e4hlt, da sie die M\u00f6glichkeit bietet, einen umfassenden Einblick in die aktuellen Forschungsbefunde zu gew\u00e4hren und dabei einen konzeptuell fundierten \u00dcberblick \u00fcber die zentralen Fakten und Daten zu vermitteln.\nZur spezifischen Beantwortung der Forschungsfragen wird zudem ein quantitatives Experiment durchgef\u00fchrt. Dabei werden Daten zur L\u00f6sungsrate, zu aufgetretenen Fehlern sowie zur Laufzeit- und Speichereffizienz von L\u00f6sungen, die von ChatGPT 4 in den 19 Programmiersprachen generiert wurden, mit einem strukturierten Ansatz erhoben und anschlie\u00dfend einer deskriptiven statistischen Analyse unterzogen. Die verwendeten Programmierprobleme stammen von der Plattform LeetCode, die eine Vielzahl an Programmieraufgaben aus verschiedenen Themengebieten und den drei Schwierigkeitsgraden leicht, mittel und schwer anbietet (LeetCode, o. D.-c). Um ein m\u00f6glichst vielf\u00e4ltiges und breites Spektrum an Programmierproblemen abzudecken, werden alle zum Zeitpunkt der Datenerhebung verf\u00fcgbaren Probleme einbezogen, die nach dem letzten Wissensstand von ChatGPT 4 im April 2023 erstellt wurden (OpenAI, 2023b). Dieses Vorgehen gew\u00e4hrleistet, dass die vorhandene Datenbasis vollst\u00e4ndig ausgesch\u00f6pft wird und ChatGPT 4 nicht vorab auf diese Probleme trainiert worden sein kann. Die von ChatGPT 4 generierten Code-L\u00f6sungen werden im Anschluss, abh\u00e4ngig von der verwendeten Programmiersprache, entweder mittels des auf LeetCode integrierten Compilers kompiliert oder durch den Interpreter ausgef\u00fchrt und mehreren Testf\u00e4llen unterzogen. Nach Abschluss der Tests werden s\u00e4mtliche Daten gesammelt und in einem Git-Repository gespeichert. Sobald alle Aufgaben bearbeitet wurden, werden die gesammelten Informationen zusammengefasst und statistisch analysiert. Der gesamte Prozess wird dabei durch speziell f\u00fcr diese Bachelorarbeit entwickelte"}]}