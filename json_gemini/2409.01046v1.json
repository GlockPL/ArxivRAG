{"title": "Accelerated Multi-objective Task Learning using Modified Q-learning Algorithm", "authors": ["Varun Prakash Rajamohan", "Senthil Kumar Jagatheesaperumal"], "abstract": "Robots find extensive applications in industry. In recent years, the influence of robots has also increased rapidly in domestic scenarios. The Q-learning algorithm aims to maximise the reward for reaching the goal. This paper proposes a modified version of the Q-learning algorithm, known as Q-learning with scaled distance metric (Q \u2013 SD). This algorithm enhances task learning and makes task completion more meaningful. A robotic manipulator (agent) applies the Q - SD algorithm to the task of table cleaning. Using Q \u2013 SD, the agent acquires the sequence of steps necessary to accomplish the task while minimising the manipulator's movement distance. We partition the table into grids of different dimensions. The first has a grid count of 3times3, andthesecondhasagridcountof 4 times 4. Using the Q SD algorithm, the maximum success obtained in these two environments was 86% and 59% respectively. Moreover, Compared to the conventional Q-learning algorithm, the drop in average distance moved by the agent in these two environments using the Q - SD algorithm was 8.61% and 6.7% respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "Robots are extensively used for object manipulation in various applications, ranging from industry to domestic set-tings. As population density increases, there is a growing demand for task planning and cleaning robots [1]. These robots play a crucial role in both industrial and domestic environments. Regular cleaning is essential for maintaining the living standards and functionality of these buildings [2]. Therefore, the automation of cleaning processes in building infrastructures has become increasingly necessary. To achieve efficient automation, robotic agents need to be equipped with intelligent algorithms to perform tasks effectively. Robots installed in buildings to autonomously complete area cover-age tasks like cleaning, building inspection and so on [3]. Infants acquire object manipulation skills at an early stage through interaction. Similarly, Reinforcement Learning (RL) solves problems by making repeated attempts with appropriate rewards. A wide variety of robotic applications also make use of RL[4]. In each instance of interaction, RL performs an action that causes a change in the environment's state. The agent is rewarded accordingly based on the state change. Through repeated actions, the agent maximizes the cumulative reward collected [5]. Q-learning algorithm is a table-based method with better convergence. In this work, the Q-learning algorithm is modified to learn tasks with reduced distance moved. This algorithm is applied to the task of cleaning the table partitioned as grids.\nCristian et al. [6] proposed three different approaches to object handling by the manipulator. They are soft actor-critic-based Interactive RL, Robust RL, and Interactive Robust RL. These different techniques were implemented in a simula-tion environment of manipulator classifying objects. In this work, advice is given related to the task and dynamics of the environment. As a result, performance is better in IRL compared to the classic RL algorithm. The training episodes in IRL are reduced compared to classic RL. RRL has taken more episodes for training but attains good performance with external disturbances. IRRL is better than RRL in terms of training.\nCruz et al. [7] proposed a learning algorithm for object handling in simulated domestic scenarios. In this, the agent manipulates objects like obstacles and sponges to clean the table. This problem was solved using three different algorithms namely classic RL, RL with affordance, and Interactive RL. In classic RL success rate of 35% was obtained with training of thousand episodes. In the second and third approaches, the number of episodes for training was reduced to a hundred. In IRL even receiving small advice around 10% enables the robot to finish the cleaning task faster. This work has better convergence compared to [8]. However, in this work, the focus is on efficient task learning with no adequate attention towards the utility value. Moon et al. [9] applied proximal policy optimization(PPO) combined with RL for cleaning using a mobile robot. They obtained a better performance in comparison to conventional methods. In [10] Q-learning-based Coverage Path Planning (CPP) is proposed. Compared to the conventional Q learning algorithm uses a predator-prey method to reward allocation. Thereby it avoids the problem of local optima. Additionally, it decreases the repetition ratio and converts numbers in CPP. This can be improved even more by applying it to an environment involving dynamic obstacles. Knowledge about handling objects in one task is transferred to other tasks in [11]. Here probability-based policy reuse is combined with with Q-learning. Learning time for new tasks is reduced due to knowledge transfer. This algorithm is applied to basic object-handling mechanisms by the robot and can be improved towards in-task learning processes. Further, this work can be improved to handle high sensory input.\nHandling of objects in cluttered environments with multiple objects was discussed in [12] and [13]. Cheong et al. [12] proposed the manipulation of obstacles to find a collision-free path to the target object. Objects are arranged in a grid-based environment. Deep Q-network is the learning al-"}, {"title": "II. FRAMEWORK OF AGENT'S LEARNING ENVIRONMENT", "content": "In this work, the problem of table cleaning with objects at the centre is being considered. The objective is to clean the table area without objects. This work proposes a novel Q-SD algorithm to learn the task of cleaning the table with the minimum distance moved by the agent. The entire table area is discretized into G number of grids. Each grid is taken to be of identical size and the object solely occupies a single grid. The robotic arm is taken as capable of reaching any grid on the table."}, {"title": "A. Affordance Environment Definition - Table with Grid", "content": "Two different grid partitions were considered one is of size 3\u00d73 and the other is of size 4 \u00d7 4. In the grid of size, 3\u00d73 nine grids range from G1 to G9. The object is placed in G5 as depicted in figure 1. The size of the object is taken to be within one grid size. The agent is trained to clean the grid in the table that contains no object. The terminal state is all the grids without objects has been cleaned. The simulation scenario of the 3 \u00d7 3 grid is given in the figure. 2. The state table for 3 \u00d7 3 is described in table I. For this environment design it consists of 256 states. Similarly, for the grid of size 4\u00d74 sixteen grids range from G1 to G16. Among those grids, G6, G7, G10 and G11 are occupied with objects as described in figure 3. Here again, objects are taken to be occupying the centre part of the table. This is depicted in the simulation diagram depicted in the figure. 4.\nThe state S is a representation of an environment consisting of G grids, which includes objects as well. Hence, each state S is represented as an array of size G \u00d7 1. Here, the number of possible actions for the agent is equal to the count of the grids. An action is described as gn is used to direct the agent to clean the nth grid. For 3 \u00d7 3 grid the action array is {91, 92, 93...99} and for 4 \u00d7 4 grid the action array is {91, 92, 93....916}. Action performed by the agent causes state transition from the current state to the next state. For example the initial state in 3\u00d73 is {0, 0, 0, 0, X, 0, 0, 0, 0}. If the action selected is g1 then transition happens to the state mentioned as {0,0,0,0, X,0,0,0,1}. A state machine is developed to describe all possible state transitions of the 3 \u00d7 3 grid scenario is given in table I. Here, each state is described by the object position and cleanliness condition of each grid. The object position of the grid is mentioned as X. All the other grids can have a value of either 0 or 1. A Grid with a value of 0 means it is unclean. Grid with a value of 1 means it is clean. In this work, the position of the object is taken to be static. Therefore, the terminal state is the state with all the grid in a clean state except the grid with objects. Similarly initial state of 4 \u00d7 4 is {0,0,0,0,0, X, X, 0, 0, X, X, 0, 0, 0, 0, 0} upon receiving action g2 it transits to the state {0,0,0,0,0, X, X, 0, 0, X, X, 0,0,0,1,0}. The possible state transitions for 4 x 4 is given in the table II. Action selection is based on the e-greedy method. It chooses an action with the maximum possible reward from the current state. The terminal state is when all the grid on the table is cleaned except those that contain the object. The agent may reach the terminal state in any sequence with respect to the order of the grid chosen to clean the table.\nRewards are given, in such a way that the objective of cleaning all the grids on the table is done. The allotment of reward is given in equation 1. The action of cleaning from the appropriate grid is given a maximum positive reward. A maximum negative reward is given for the agent's attempt to clean a grid with objects. Where a small negative reward is allotted for cleaning an already clean grid to keep the agent from cleaning the same grid.\n$r(t) = {\\begin{cases} +1, & \\text{cleaning an unclean grid.} \\\\ -1, & \\text{cleaning grid with object.} \\\\ -0.01, & \\text{cleaning grid already clean} \\end{cases}}$       (1)\nThe table cleaning job is logically implemented using Python programming. The grids are cleaned in a way by one grid at a time. The robotic manipulator with a sponge in its end-effector is mentioned with a grid number. The manipulator cleans that particular grid and receives an appropriate reward as per equation 1. In this work, the focus is on learning the correct sequence of actions to be performed by the agent to reach the goal. The objective is to reach the goal with maximum success rate and reduced distance moved by the manipulator. Further actuating mechanism for the robotic"}, {"title": "III. Q-SD: Q LEARNING WITH SCALED DISTANCE METRIC ALGORITHM", "content": "Infants learn continuously about different objects in the environment by way of interacting with them. By interac-tion, different attributes such as colour, shape, and usage are learned. Learning in infants happens using instinct observing the action-effect relations. In the same way, RL learns by repeated interactions with the environment [22]. Q-learning is an offline policy algorithm which has better convergence than SARSA [23]. The objective of Q learning, as in any RL is to maximize the reward. Q-Learning is a table-based method with states as columns and actions as rows. The main focus is to update the state-action Q-table in each iteration. In this work a modified Q learning algorithm namely Q \u2013 SD is proposed. Q SD stands for Q-learning with scaled distance metric. The main advantage of Q - SD algorithm is it maximizes the reward and also helps agent the agent to attain the goal with the minimum distance moved.\n$Q(St, gn,t) <= Q(st, gn,t) + a[rt+1 + \\gamma maxQ(st+1,\\ \\gn',t+1) - Q(st, gn,t)]$\\        (2)\n$Q*(s,a) = max Q (s,a)$\\       (3)\n$\\pi$\nThe Q-value update equation for the Q-learning algorithm is given by equation 2. In equation 2, $g_{n,t}$ represents the current action chosen from a set of available actions G that can be performed by the agent on the environment. Similarly, $s_t$ is the current state from a set of states S representing the environment. Among all the states in set S, one state $S_T$ is called a terminal state, marking the end of the training episode. In this work, the terminal state is achieved when all the grid without objects is clean. $r_t$ represents the immediate reward received according to a predefined pattern. $\u03b1$ is the learning rate, which controls how much the Q-values are updated in each iteration. The discount factor $\u03b3$ determines the importance of future rewards in the learning process. With each action performed by the agent, there is a state transition in the environment. Consequently, a positive or negative reward is received as feedback for each action and corresponding state transition. Q-learning has a bootstrapping effect as it relates to two consequent states. SARSA also is having a bootstrapping impact but it waits for the next step to happen for the current Q value update. This makes SARSA algorithm convergence longer compared to Q-learning. The reward is maximized to learn optimum policy as given by the equation 3.\n$Q(St, gn,t) <= Q(st, gn,t) + a[rt+1 + \\gamma maxQ(st+1,\\ \\gn',t+1) - Q(st, gn,t)]\\   sx dmetric$         (4)\nIn each iteration, the agent is aware of the current state and chooses an action to be executed on the table. Here, the action is the choice grid number Gn where cleaning is to be performed. Action performed causes a state change and consequently, the Q-table Q[S, A] is updated. This updated"}, {"title": "IV. SIMULATIONS AND RESULTS", "content": "The Q SD technique was coded in Python and the Coppeliasim software [21] was used for the simulation. In the simulation, the UR10 model was utilised as the manipulator. In this section the results of applying the Q-SD algorithm to an agent cleaning a table which is partitioned as a grid were discussed. Two sets of grid size are taken as input one is 3 \u00d7 3 and the other is 4 \u00d7 4. Four different graphs were considered to analyze the agent's learning and distance moved. They are the average reward convergence graph, success percentage of the agent, average distance moved with varying weight to distance metric and the graph with normalized values of three different parameters. The first two graphs were used to imply the learning of the agent. The third graph highlights the reduced distance moved by the agent using Q-SD algorithm. The final graph with the normalized value of three different parameters is used to determine the right scale factor for the distance metric to get an optimum balance between success rate and reduced distance moved.\nThe agent performing a single action is taken as iteration. The group of iterations is taken as an episode. The episode ends when the terminal state is reached or after a certain amount of iterations. The collection of episodes is termed as a run, all the readings taken an average of 1000 such runs. Any table or comparison with a scale factor value of 0 denotes the use of a standard Q-learning algorithm described in equation 2."}, {"title": "A. Discussions on impact of Q - SD algorithm on agent's learning and distance moved", "content": "In RL the average reward graph illustrates how effectively an agent is learning over the course of time. The average reward graph is depicted in Figure 6. The average reward converged around 25 episodes in both cases of varying grid size. The convergence of average reward occurs even with variations in the scaling factor to the distance metric. However, it is observed that the increased scale factor to the distance metric impacts negatively the learning rate of the agent. This can be observed in the graph with a negative slope for the average reward with scaling rates of 0.2 and 0.24 with a grid size of 3 \u00d7 3. In 4 \u00d7 4 grid size as well the learning is dragged with scale factors of 0.10 and 0.12. Thus, it is observed that the agent's learning is hindered by the enormous magnitude of the scale factor. The high impact of the scaling factor pulls down the Q-value resulting in degraded performance.\nA similar pattern of impact of distance metric and scaling factor on success rate is observed in Figure 7. As the scaling"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "The current research introduces a novel algorithm called the Q SD algorithm. The method involves incorporating a scaled distance metric parameter into a typical Q-learning algorithm. The Q SD algorithm is utilised to control a robotic manipulator (agent) to clean a table that is divided into grids. Two configurations of grid partitions were evaluated: one with dimensions of 3 \u00d7 3 and another with dimensions of 4 \u00d7 4. Furthermore, the arrangement involved the presence of objects occupying a few grids in the central area. In addition to its task learning capabilities, the Q-SD algorithm also gives a utility value. The distance parameter enables the agent to clean the grid systematically, hence minimizing the distance travelled by the agent across several grids. The displacement is decreased by 8.61% and 6.7% for 3 \u00d7 3 and 4 x 4 grids, respectively. The weight of the scaling factor is selected appropriately to achieve a balance between the success rate and the distance moved. As observed in section IV the increase in grid count resulted in a decrease in success percentage due to environmental complexity. Therefore other algorithms like Deep-Q network can be employed to tackle highly complex environments. Further, mobile manipulators may be employed to achieve a larger coverage area. This work focused on addressing the presence of stationary barriers in the grid. However, there is potential for further enhancement by incorporating dynamic objects and addressing the cleaning of areas located beneath objects. The scaling factor is taken to have a value of s \u2265 0, and its impact on agent learning for negative values can be investigated further."}]}