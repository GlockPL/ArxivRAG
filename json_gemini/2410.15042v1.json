{"title": "Adversarial Training: A Survey", "authors": ["Mengnan Zhao", "Lihe Zhang", "Jingwen Ye", "Huchuan Lu", "Baocai Yin", "Xinchao Wang"], "abstract": "Adversarial training (AT) refers to integrating adversarial examples inputs altered with imperceptible perturbations that can significantly impact model predictions into the training process. Recent studies have demonstrated the effectiveness of AT in improving the robustness of deep neural networks against diverse adversarial attacks. However, a comprehensive overview of these developments is still missing. This survey addresses this gap by reviewing a broad range of recent and representative studies. Specifically, we first describe the implementation procedures and practical applications of AT, followed by a comprehensive review of AT techniques from three perspectives: data enhancement, network design, and training configurations. Lastly, we discuss common challenges in AT and propose several promising directions for future research.", "sections": [{"title": "I. INTRODUCTION", "content": "Adversarial training (AT) has attracted widespread attention for its efficacy in enhancing the network robustness against perturbations [1], [2], which has been successfully applied in various fields such as medical image segmentation [3], [4], autonomous driving [5], and anomaly detection [6], [7]. Specifically, AT is typically framed as a min-max optimization problem. The outer minimization adjusts model weights to correctly classify both clean and adversarial examples, while the inner maximization generates adversarial examples by perturbing clean inputs under the fixed model weights [8]. Depending on the accessibility of fixed models, such as model parameters and gradients, these adversarial examples can be crafted by white-box attacks [9] or black-box attacks [10].\nIn addition to generating adversarial data, both types of attacks can evaluate the adversarial robustness of trained models during inference [11]. Researchers also assess models with their prediction accuracy on clean samples. These evaluation metrics are affected by multiple factors such as attack intensity and model architecture. Based on these factors, we categorize popular AT approaches from three dimensions: data enhancement, network design, and training configurations.\nData Perspective: This aspect focuses on increasing data diversity through various data augmentation (DE) techniques, including source DE, generic DE, and adversarial DE.\nSource DE primarily employs two strategies: data collection and data generation. The former typically integrates additional unlabeled samples into the training set [12], while the latter incorporates data generated by methods like diffusion models [13], [14]. Unlike the pre-training augmentation of source DE, generic DE augments data during training. For instance, techniques such as Cutout [15] and CutMix [16] modify local image regions through simple replacement operations. Data reconstruction approaches, on the other hand, reconstruct distinct samples from input representations [17].\nFurthermore, adversarial DE utilizes various adversarial attacks to generate adversarial training data. For instance, conventional AT [18] employs multi-step attacks [19], while fast AT [20] uses single-step attacks [21], [22]. Beyond standard adversarial attacks, specialized strategies for AT have been developed, such as filtering out data with low attack levels [8] or low gradient values [23] by calculating adjacent loss differences or back-propagated gradient values. Additionally, incorporating different perturbation initialization methods such as random initialization [24], learnable initialization [25], and prior-guided initialization [26] - along with adjusting attack intensity [27], [28], can further improve data diversity.\nModel Perspective: This aspect describes diverse network architectures and components used in AT.\nAT is applicable to different deep learning architectures, such as Convolutional Neural Networks (CNNs) [29], Graph Neural Networks (GNNs) [30], Recurrent Neural Networks (RNNs) [31], Transformers [32], and diffusion models [33], as its framework is not limited to specific network designs [24]. Researchers also explore multi-model AT techniques [34], such as ensemble AT [35], federated AT [36], and robustness transfer via fine-tuning [37] or knowledge distillation [38]. Moreover, AT performance is highly influenced by various network components [39], [40], e.g., activation functions [41], batch normalization layers [42], dropout configurations [43], network depth [44], and Softmax temperature values [45].\nTraining Configurations Perspective: In addition to data enhancement and network design, specifying training configurations such as loss functions, labels, and weight settings is essential for AT, e.g., improving AT stability [46].\nSpecifically, the loss function should be adjusted according to the AT task. Popular AT tasks include conventional AT, fast AT, federated AT, virtual AT, fine-tuned AT, robust distillation, curriculum AT, and domain AT. Common loss functions used in these tasks include cross-entropy loss [47], norm-based feature regularization [48], gradient regularization [49], weight regularization [50], perceptual loss (LPIPS) [51], Kullback-"}, {"title": "II. ADVERSARIAL TRAINING: EXPRESSION AND APPLICATION", "content": "Recent AT techniques are commonly formulated as a min-max optimization issue [19], which minimizes the worst-case loss within a specified perturbation set for each training example, expressed as:\n$\\min_{\\theta} E_{(x,y)\\sim D} \\max_{\\delta \\in B(x,\\epsilon)} l(x + \\delta,y;\\theta)].$ (1)\nHere, $\\theta$ denotes the model parameters, and $(x, y)$ represents training data sampled from the data distribution $D$. $l(x + \\delta, y; \\theta)$ refers to the loss value (e.g., cross-entropy loss) calculated with adversarial examples $x+\\delta$ and their true labels $y$. $\\delta$ means adversarial perturbations that are imperceptible to humans but significantly degrade model performance. The allowed perturbation set $B(x, \\epsilon)$ is defined as:\n$B(x, \\epsilon) = {\\delta | x + \\delta \\in [0, 1], ||\\delta||_p \\leq \\epsilon},$ \nwhere $\\epsilon$ is the maximum perturbation magnitude. $|\\delta||_p$ quantifies the perturbation size using the p-norm. All pixels are normalized to the range of $[0, 1]$.\nThe inner maximization of Eq. (1) aims to identify $\\delta$ under fixed parameters $\\theta$. One representative method to achieve this is the Fast Gradient Sign Method (FGSM) [21], which is a single-step adversarial attack and formulated as follows:\n$\\delta = Proj_{B(x,\\epsilon)} (\\epsilon \\cdot sign(\\nabla_x l(x, y; \\theta)))$ \nwhere $Proj_{B(x,\\epsilon)}$ is the projection operator, ensuring that $\\delta$ remains within the allowed perturbation range. The term $sign()$ denotes the sign function. $\\nabla l$ represents the back-propagated gradient of the loss $l$ with respect to the input $x$. \nFurthermore, a widely-used multi-step maximization technique is Projected Gradient Descent (PGD) [19],\n$x^{t+1} = Proj_{B(x,\\epsilon)} (x_t + \\alpha sign (\\nabla_{x_t} l(x, y; \\theta))),$ \nwhere $x_t$ denotes the adversarial example at iteration $t$. The initial adversarial example is $x_0 = x + \\delta_0$, where $\\delta_0$ represents randomly initialized perturbations. $\\alpha$ is the step size for each attack iteration. The outer minimization of Eq. (1) optimizes $\\theta$ on generated adversarial examples, typically using gradient-based methods such as stochastic gradient descent [64]."}, {"title": "B. Applications of AT", "content": "AT has been successfully applied to various tasks.\n\u2022 Abnormal event detection in videos. To tackle the lack of abnormal samples in training data, Georgescu et al. [65] propose an adversarial learning strategy, which generates out-of-domain pseudo-abnormal samples using autoencoders and trains binary classifiers to distinguish between normal and abnormal latent features.\n\u2022 Biomedical image segmentation. By conducting experiments on various eye vasculature segmentation datasets, Javanmardi et al. [66] demonstrate the effectiveness of AT in mitigating the impact of inaccurately annotated segmentation targets on model performance.\n\u2022 Emitter identification. To enhance the discriminability of radio frequency signals while reducing sensitivity to noise, Xie et al. [67] design a semi-supervised AT algorithm for emitter identification based on bispectrum analysis.\n\u2022 Graph embedding learning. This transforms graph elements into representations, facilitating subsequent graph analytic tasks like node and link prediction. To improve graph embeddings, Pan et al. [68] enforce latent representations of adversarial data to align with a prior Gaussian distribution.\n\u2022 Healthcare debiasing. Biases present in healthcare data collection, stemming from variations in health conditions, can substantially affect model performance. Yang et al. [69]"}, {"title": "III. OVERVIEW OF ADVERSARIAL TRAINING", "content": "This section provides a comprehensive review of AT techniques. Instead of focusing on specific approaches, Algorithm 1 summarizes the general procedures for AT. Furthermore, Figure 2 presents a comprehensive overview of advancements in AT across three dimensions: data enhancement, network design, and training configurations."}, {"title": "A. Data Perspective", "content": "Insufficient training data can impact AT performance, such as the trade-off between classification accuracy on clean and adversarial examples [81]. Hence, researchers introduce various data enhancement (DE) techniques, primarily including source DE, generic DE and adversarial DE.\nSource DE: Source DE refers to expanding the dataset before training begins.\nExtra data collection. Raghunathan et.al [12] augment the training set with additional unlabeled data. Gowal et al. [82] indicate that employing a 3:7 ratio of unlabeled to labeled data enhances adversarial robustness. Wallace et al. [83] demonstrate that multiple rounds of dynamic adversarial data collection can improve AT performance.\nExtra data generation. Collecting large datasets is time-consuming and expensive. Consequently, Gowal et al. [84] utilize generative models, such as Denoising Diffusion Probabilistic Models (DDPM) [85], to generate extra training data. Additionally, Wang et al. [86] show that leveraging improved diffusion models [87] can further augment AT performance.\nGeneric DE: After constructing the source dataset, various generic DE techniques are applied during training.\nSimple replacement operation. This category typically generates variants of the input by applying straightforward"}, {"title": "Adversarial DE", "content": "Adversarial DE forms the foundation of AT techniques, focusing on designing various adversarial attacks to generate adversarial examples.\nPerturbation Initialization. Before applying attacks, incorporating initial perturbations to clean inputs can significantly improve the diversity of adversarial perturbations. For instance, FGSM-AT utilizes uniformly random noise as initial perturbations [21]. Jia et al. [100] propose a two-step perturbation initialization process, namely, refining the perturbations generated by FGSM with an initialization generator. Furthermore, they [26], [101] employ three types of initial perturbations to compute subsequent adversarial perturbations: perturbations from previous batches, perturbations from previous epochs, and perturbations based on momentum statistics. The adversarial attack in AT using the first type of initial perturbations is formulated as\n$\\delta_{b+1} = \\delta_b + \\alpha sign (\\nabla_{x_b} l(x_{b+1} + \\delta_b, y; \\theta)),$\nwhere $\\delta_b$ and $\\delta_{b+1}$ are adversarial perturbations for inputs $x_b$ and $x_{b+1}$, respectively. Similarly, for the second initialization type, the attack is expressed as\n$\\delta_{e+1} = \\delta_e + \\alpha \\cdot sign (\\nabla_{x_e} l(x + \\delta_e, y; \\theta)),$\nwhere $\\delta_e$ and $\\delta_{e+1}$ represent adversarial perturbations for the same input at epochs $e$ and $e + 1$, respectively. Additionally, the adversarial attack using momentum-based perturbation initialization is implemented as\n$g = sign(\\nabla_{\\eta_e} l(x + \\eta_e, y; \\theta)),$ \n$g_{e+1} = \\mu g_e + g,$\n$\\delta_{e+1} = Proj_{B(x,\\epsilon)} (\\eta_e + \\alpha \\cdot g),$ \n$\\eta_{e+1} = Proj_{B(x,\\epsilon)} (\\eta_e + \\alpha \\cdot sign(g_{e+1})),$ \nwhere $g$ denotes the signed gradient, $\\eta_e$ represents the initial perturbation for epoch $e+1$, $g_{e+1}$ means the gradient momentum, $\\mu$ signifies the decay factor, and $\\delta_{e+1}$ is the generated adversarial perturbation.\nVarious adversarial attacks. After generating the initial adversarial perturbations, we illustrate several types of adversarial attacks that further refine these perturbations.\nTesting attacks. Testing attacks are widely employed to assess the robustness of pre-trained models during the inference stage, including both white-box and black-box attacks. White-box attacks, such as FGSM and PGD, assume that attackers can access the model architecture, parameters, and gradients. Similar to PGD, the basic iterative method [9] applies FGSM iteratively with a small perturbation stride to increase"}, {"title": "AT for defending against various adversarial attacks.", "content": "AT for testing attacks. AT can defend against both standard attacks, such as PGD and AutoAttack, and specialized attacks across various tasks. For improving the model robustness against standard attacks, Dong et al. [121] replace convolution parameters with randomly sampled mapping parameters, which are varied during the attack and inference processes. Araujo et al. [122] demonstrate that AT with random noise embedding can resist adversarial attacks constrained by l\u221e-norm or 12-norm functions. Jiang et al. [123] further show that AT can defend against l\u2081-bounded attacks by mapping perturbed samples back to the l\u2081 boundary. Metzen et al. [124] combine meta-learning with AT to enhance the model robustness against generic patch attacks.\nFor defending task-specific adversarial attacks like discrete attacks, Ivgi et al. [125] introduce discrete AT, which leverages symbolic perturbations such as synonym replacements designed for linguistic inputs. Gosch et al. [126] show that AT effectively mitigates adversarial structural perturbations. Ruan et al. [127] propose a viewpoint-invariant AT, where viewpoint transformations are treated as adversarial attacks. Specifically, the inner maximization learns a Gaussian mixture distribution to generate diverse adversarial viewpoints, while the outer minimization trains a viewpoint-invariant classifier against the worst-case viewpoint distribution. Gao et al. [128] investigate the effectiveness of AT in countering backdoor attacks, showing that models trained with spatial adversarial examples can defend against patch-based backdoor attacks. They also indicate that a hybrid AT strategy can improve robustness against various backdoor attacks. Additionally, Zhang et al. [129] identify a limitation of AT: it primarily adapts to the training distribution and may not effectively improve robustness on samples that deviate from this distribution.\nAT for delusive attacks. To defend against delusive attacks, Tao et al. [130] minimize adversarial risk within an \u221e-Wasserstein ball, which can reduce the reliance of adversarially trained models on non-robust features. They [131] further reveal that AT with a defense budget of $\\epsilon$ is insufficient to guarantee robustness against $\\epsilon$-bounded delusive attacks, and propose increasing the perturbation budget in AT. Yu et al. [132] propose a coded Merkle tree as a hash accumulator to provide a constant-cost protection against data availability attacks. They [133] also show that targeted adversarial perturbations are almost linearly separable and thus can serve as shortcuts for delusive attacks in AT.\nAT for ensemble attacks. Conventional AT methods generally concentrate on a single perturbation type, such as adversarial perturbations constrained by a specific norm function (l0, l1, 12, or l\u221e). To address this limitation, Tramer et al. [11] propose an affine attack that linearly interpolates across various perturbation types. Furthermore, adversarial distributional training [134] designs diverse perturbation distribution functions, from which adversarial examples are sampled. Compositional AT [135] combines multiple perturbation types, including semantic perturbations (e.g., hue, saturation, contrast, brightness) and perturbations in lp-norm space. Pyramid-AT [43] generates multi-scale adversarial perturbations and aligns the outputs of dropout layers for clean and adversarial"}, {"title": "AT for text-related attacks.", "content": "Unlike continuous visual pixels, text is discrete, prompting researchers to apply adversarial perturbations to text embeddings [137]. Specifically, for graph networks like DeepWalk [138], Dai et al. [139] generate negative graphs based on node dependencies and modify graph embeddings with adversarial attacks. To reduce the discrepancy between generated adversarial text examples and the text encoder, Zhao et al. [140] employ a discriminator to refine text embedding layers and a generator to yield adversarial examples for masked texts."}, {"title": "Attack Intensity Adjustment.", "content": "Under the same attack, adjusting the attack intensity can greatly affect AT performance.\nAT with small perturbation degree. Liu et al. [141] observe that large adversarial budgets during training can hinder the escape from suboptimal perturbation initialization. To address this, they propose a periodic adversarial scheduling strategy that dynamically adjusts the perturbation budget, similar to learning rate warmup. To enhance the prediction performance of adversarially trained models on clean samples, Kim et al. [142] verify multiple perturbation budgets in each attack iteration, selecting the smallest budget that successfully misleads the target model. Yang et al. [143] reduce the perturbation budget when adversarial examples excessively cross the decision boundary. Friendly AT [144] terminates the attack iteration early to prevent generating overly strong perturbations. Furthermore, Wang et al. [145] indicate that using high-quality adversarial examples early in AT can degrade adversarial robustness. Shaeiri et al. [146] first train models with small perturbations, followed by fine-tuning models on larger ones. Curriculum AT [147] gradually increases the hardness of adversarial examples, which are sampled from time-varying distributions using a series prediction framework. Hybrid AT [148] starts with FGSM and shifts to stronger attacks like PGD to mitigate catastrophic overfitting.\nAT with large perturbation degree. Conversely, Gong et al. [149] propose MaxUp to enhance adversarial generalization by performing the outer minimization on the worst-case adversarial examples. Specifically, they generate multiple adversarial examples for each input and select the most aggressive one. Xu et al. [28] propose a parameterized automatic attacker to search for optimal adversarial perturbations. Liang et al. [150] estimate the worst-case reward for the reinforcement learning policy using lp-bounded attacks. De et al. [151] show that using stronger adversarial perturbations, such as removing the clipping operation, can mitigate catastrophic overfitting.\nAT with learnable perturbation degree. Xiong et al. [152] design an optimizer with recurrent neural networks to predict adaptive update directions and hyperparameters for the inner maximization. Jia et al. [25] introduce a learnable attack strategy that dynamically adjusts hyperparameters such as the number of attack iterations, the maximum perturbation size, and the attack stride. Ye et al. [153] introduce the Amata annealing mechanism, which gradually increases the number of attack steps in inner maximization."}, {"title": "IV. CHALLENGES AND POTENTIAL RESEARCH DIRECTIONS", "content": "This section further investigates the influence of various training configurations, such as loss functions, labels, and optimization algorithms, on AT performance."}, {"title": "Loss Functions:", "content": "Loss functions in conventional AT. Conventional AT methods [191], [192] such as PGD-AT, are known for their stability and effectiveness in improving model robustness, requiring multiple gradient descent steps to yield adversarial data. Specifically, the inner maximization often employs the cross-entropy loss $lce$. Regarding the outer minimization, TRADES [191] incorporates the KL divergence,\n$\\min_{\\theta} (L_{TRADES} = lce(x, y; \\theta) + \\beta \\cdot KL(f(x + \\delta; \\theta) || f (x; \\theta)),$\nwhere $x$ means clean samples. $\\beta$ controls the performance trade-off between clean and adversarial examples. $f(\\cdot; \\theta)$ denotes the trained model with weights $\\theta$. Nuclear AT [193] utilizes the nuclear norm function $|| \\cdot ||_*$ to constrain the prediction differences between the two types of examples,\n$\\min_{\\theta}[lce(x, y; \\theta) + || f(x + \\delta; \\theta) \u2212 f(x; \\theta)||_\u2217],$\nCui et al. [194] propose a learnable boundary-guided AT, which employs cross-entropy and regularization losses to supervise the model weights for classifying clean and adversarial examples, respectively,\n$\\min_{\\theta_{clean}, \\theta_{AT}} [lce (x, y; \\theta_{clean}) + || f(x + \\delta; \\theta_{AT}) \u2212 f(x; \\theta_{clean}) ||_p].$\nRLAT [195] replaces the KL divergence in (TRADES with LPIPS [51] to perceive image patch similarity,\n$L_{LPIPS}(x, x + \\delta) = \\sum_{l=1}^{L} \\alpha_l \\cdot ||f_l(x + \\delta) \u2212 f_l(x)||_2^2,$\nwhere $|| f_l(x + \\delta) \u2013 f_l(x)||$ denotes the squared Euclidean distance between the extracted features for $x + \\delta$ and $x$ at the layer $l$. $\\alpha_l$ represents the layer weighting factor.\nThe above methods often regularize the model predictions for $x + \\delta$ and $x$, which can affect AT performance when clean examples are misclassified. To tackle this, Dong et al. [196] craft adversarial perturbations $\\delta_{clean}$ for clean samples, ensuring all crafted samples $x + \\delta_{clean}$ can be correctly classified. Then, they regularize the predictions for $x + \\delta$ and $x + \\delta_{clean}$,\n$\\min_{\\theta}[lce(x, y; \\theta) + || f(x + \\delta; \\theta) \u2212 f(x + \\delta_{clean}; \\theta)||_p].$\nFan et al. [197] combine adversarial and abstraction losses to achieve provably robust AT.\nIn addition to using $lce$ in the inner maximization, Zhang et al. [198] maximize the feature distribution distance between clean and adversarial examples based on optimal transport metrics. Zou et al. [199] leverage AT to develop robust classifiers against unknown label noise. They investigate various loss functions in the inner maximization, such as binary cross-entropy and non-convex S-shaped losses, and indicate that"}, {"title": "B. Potential Research Directions", "content": "In the supplementary material, we summary the application of AT across 100 popular deep learning tasks. Among them, 18 areas remain unexplored regarding the feasibility of integrating AT, including image deblurring, object tracking, face shape modeling, optical flow, virtual reality, environmental sound classification, motion deblurring, audio retrieval, 3D instance segmentation, 3D super-resolution, 3D object tracking, 3D style transfer, speaking style conversion, audio denoising, audio restoration, scene graph generation, and personalized medicine. Researchers can investigate whether AT can enhance these fields by analyzing its potential benefits and challenges."}, {"title": "C. CONCLUSION", "content": "This paper presents a comprehensive review of recent advancements in AT. We begin by illustrating the implementation of AT techniques and their practical applications in real-world scenarios. Next, we construct a unified algorithmic framework for AT and categorize AT methods across three dimensions: data enhancement, network architecture, and training configurations. Additionally, we summarize common challenges in AT and identify several potential directions for future research."}]}