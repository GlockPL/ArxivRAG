{"title": "DeMo: Decoupled Momentum Optimization", "authors": ["Bowen Peng", "Jeffrey Quesnelle", "Diederik P. Kingma"], "abstract": "Training large neural networks typically requires sharing gradients between accelerators through specialized high-speed interconnects. Drawing from the signal processing principles of frequency decomposition and energy compaction, we demonstrate that synchronizing full optimizer states and model parameters during training is unnecessary. By decoupling momentum updates and allowing controlled divergence in optimizer states across accelerators, we achieve improved convergence compared to state-of-the-art optimizers. We introduce Decoupled Momentum (DeMo), a fused optimizer and data parallel algorithm that reduces inter-accelerator communication requirements by several orders of magnitude. This enables training of large neural networks even with limited network bandwidth and heterogeneous hardware. Our method is topology-agnostic and architecture-independent and supports scalable clock-synchronous distributed training with negligible compute and memory overhead. Empirical results show that models trained with DeMo match or exceed the performance of equivalent models trained with AdamW, while eliminating the need for high-speed interconnects when pre-training large scale foundation models. An open source reference PyTorch implementation is published on GitHub at https://github.com/bloc97/DeMo.", "sections": [{"title": "1 Introduction", "content": "Large-scale neural networks, particularly language models, are characterized by high parameter counts. In fact, it is not uncommon to talk about models with trillions of parameters. Training these models requires multiple accelerators (e.g. GPUs, TPUs) to achieve tractable training times. Common strategies for distributing training across accelerators include Distributed Data Parallelism [5] and Fully Sharded Data Parallelism [13]. These techniques work by having accelerators split the weights and synchronize the gradients (sometimes multiple times per step), with communication volumes proportional to the model size itself.\nThis gradient synchronization between accelerators traditionally requires specialized high-speed interconnects (e.g. Infiniband). Such interconnects represent expensive localized networking topologies, constraining all accelerators to be present in the same data center. However, if the volume of synchronized data could be substantially reduced, these hardware constraints could potentially be relaxed.\nIn this paper, we demonstrate that gradients and optimizer states during the training of large neural networks exhibit significant redundancy and are highly compressible. Building on this insight, we develop DeMo, an optimizer that takes advantage of this compressibility to reduce inter-accelerator communication needs by several orders of magnitude. We evaluated DeMo by training a standard"}, {"title": "2 Background and Related Work", "content": "Various strategies have been developed to mitigate communication overhead in distributed training. For centralized and clock-synchronous training, the most effective techniques can be categorized into three main approaches:\n\u2022 Quantization and sparsification of gradients.\n\u2022 Low-rank projection of gradients.\n\u2022 Federated averaging (also known as Local-SGD).\nWe focus exclusively on centralized, clock-synchronous methods, excluding asynchronous and de-centralized approaches. Although these latter methods represent important areas of research, they introduce significant analytical complexity, often lack generalizability, and depend on specific network topologies or architectures. Our study instead concentrates on developing a generalizable centralized clock-synchronous distributed optimizer."}, {"title": "2.1 Quantization and Sparsification", "content": "Previous work, such as [10], has primarily explored the compressibility of gradients through quantization and sparsification, assuming that gradient values are uncorrelated and tolerant of compression errors. However, quantization-based approaches face fundamental limits: a 16-bit gradient can be compressed to no fewer than one bit. Although sparsification offers theoretically unbounded compression, achieving high compression ratios without degrading training performance remains challenging, making it most suitable for fine-tuning rather than pre-training."}, {"title": "2.2 Low Rank Projection", "content": "Recent work [12] demonstrated that LLM gradients exhibit a very low rank structure during training. This enables the use of Singular Value Decomposition (SVD) to project gradients onto lower-dimensional spaces that preserve the most significant directions, substantially reducing storage and communication requirements. However, computing SVD for very large models is computationally expensive, and the projection matrices must be shared or recomputed across nodes. While this overhead can be reduced by computing SVDs less frequently, it remains a significant bottleneck that scales poorly with model size. Nevertheless, this approach's success in achieving convergence\u2074 parity with full-rank optimizers provides valuable insight: low-rank projection offers advantages over sparsification and warrants further investigation."}, {"title": "2.3 Federated averaging", "content": "Federated averaging [7] reduces communication by allowing nodes to train independently for multiple steps before synchronizing through weight averaging. Essentially, each accelerator node trains independently for a fixed number of steps, then synchronizes the accelerator nodes by averaging their weights. While this eliminates the need for per-step gradient communication, it still requires sharing full model parameters during synchronization, incurring bandwidth costs comparable to standard training. Moreover, increasing the steps between synchronizations creates a fundamental trade-off: more steps reduce communication but slow convergence. This results in either fast iterations with poor convergence or good convergence with prohibitively slow iterations. Finding optimal hyper-parameters becomes challenging as they depend heavily on system-specific variables (node count,"}, {"title": "3 Methodology", "content": "Rather than incrementally modifying existing optimization algorithms, we propose a novel decoupled momentum optimization algorithm that intentionally allows and leverages divergent optimizer states across accelerators."}, {"title": "3.1 Assumptions", "content": "Our method is built on three key conjectures that, while currently lacking formal proofs, are supported by empirical evidence from large-scale neural network training:\nConjecture 3.1 The fast-moving components of momentum exhibit high spatial auto-correlation, with most of their energy concentrated in a small number of principal components.\nConjecture 3.2 Fast-moving momentum components show low temporal variance and should be applied to parameter updates immediately, while slow-moving components exhibit high temporal variance and benefit from temporal smoothing over longer periods.\nConjecture 3.3 Slow-moving momentum components, despite their high variance, are crucial for long-term convergence and should be preserved rather than filtered out.\nWe leave formal proofs of these conjectures to a later work, but highlight that the optimizer we present was made with all of these assumptions in mind. We hope that by proposing this novel method, it can help develop these ideas further in future research. A large leap of faith was needed."}, {"title": "3.2 Algorithm", "content": "Starting from SGD with Momentum, we make two key modifications: first, we remove the all-reduce operation on gradients \u011fk, decoupling momentum m across the accelerators. Second, after updating the momentum, we extract and remove its fast components q, which can be efficiently synchronized with minimal communication. Algorithm 1 presents the complete method:"}, {"title": "3.2.1\nEfficient Extraction of Fast Moving Components", "content": "Our goal is to efficiently identify and extract the most significant momentum components while minimizing computational overhead. While optimal decomposition methods exist, we prioritize practical efficiency for large-scale training.\nFor our method to work, we must first decorrelate, separate, and extract the principal components from the momentum during training. Assuming Conjecture 3.1 holds, one approach would be to apply a spatial Kosambi\u2013Karhunen-Lo\u00e8ve Transform (KLT) to separate faster-moving components from slower ones. However, computing KLT on momentum tensors for neural networks with billions or trillions of parameters is computationally prohibitive.\nAlternatively, taking cues from signal processing work, the Discrete Cosine Transform (DCT) can act as an approximation of the KLT, if used for the purpose of energy compaction, as they are both"}, {"title": "3.2.2 Low Bandwidth Synchronization", "content": "After extracting $m t_{f r e q}, m t_{a m p l}$ from the momentum $m_t$, we perform an all-gather operation along the last dimension of the extracted tensors. This enables each accelerator to perform the same inverse DCT operation by scattering both frequency and amplitude tensors the same way as before, but this time we average the amplitude of any duplicate frequencies. When hyperparameters s and k are chosen appropriately, tensors $m t_{f r e q}, m t_{a m p l}$ can be orders of magnitude smaller than the model size, enabling efficient synchronization across accelerators with minimal communication overhead.\nGiven Conjecture 3.2 and 3.3, here we are effectively averaging all of the fast moving components of the momentum at each step, while letting the slow moving components be decoupled from each-other. If we assume that slow moving components in the gradient are high variance, they will be"}, {"title": "3.3 Signum", "content": "In order to improve convergence when training LLMs, a signum [1] variant of DeMo can be used instead, where the gradient descent step is replaced by:\n$0_{t+1}=0_{t}-\\eta \\operatorname{sign}(Q t)$\nSince the second moment is not computed here, this variant of DeMo uses less memory for optimizer states as compared to AdamW."}, {"title": "4 Experimental results", "content": "We evaluated the signum variant of DeMo using OLMo [4], a highly reproducible large language model pre-training framework. Adapting OLMo to use DeMo required only including the DeMo optimizer class and disabling gradient synchronization in PyTorch Distributed Data Parallelism [5]. We provide the modified OLMo code as well as the configuration files for all experiments in the supplementary material.\nOur experiments used the Dolma v1.55 dataset for pre-training. As a baseline we used the publicly released OLMO-1B6, a standard decoder-only Transformer model consisting of 1.18 billion parameters using the AdamW optimizer (\u03b2\u2081 = 0.9, \u03b22 = 0.95, weight decay = 0.1) as compared to using the DeMo optimizer (\u03b2 = 0.999). The learning rate and the AdamW hyperparameters were untouched and set with the suggested defaults.\nDue to computational constraints, we trained models for 100 billion total tokens rather than the full 3 trillion tokens in Dolma. For complete comparability, we re-trained OLMo-1B with these same 100 billion tokens and adjusted the learning rate schedule accordingly. We also repeated the experiments on a smaller 300M model identical to the 1B except halving the model's hidden size. All experiments were performed on 64 H100 GPUs with a global batch size of 2048 with a sequence length 2048 tokens, resulting in a per-GPU batch size of 32."}, {"title": "5 Conclusion", "content": "In conclusion, we have shown that our proposed DeMo optimization algorithm can act as a drop-in replacement to AdamW when training LLMs, with no noticeable slowdown in convergence while reducing communication requirements by several orders of magnitude. The signum variant of DeMo is more memory efficient than AdamW and has negligible compute overhead if we use small pre-computed DCT transition matrices. Finally, the LLMs pre-trained with DeMo have equivalent or better scores on multiple standard benchmarks compared to their equivalents trained with AdamW."}, {"title": "5.1 Reproducibility Statement", "content": "As described in Section 4 we chose the OLMo framework and references to maximize reproducibility and comparability of our experiments. We have provided as publicly available supplementary material a standalone PyTorch implementation of DeMo, as well as the minimal patch to OLMo and configuration files used for the experiments. We do this in hopes of encouraging independent reproduction and improvement of our method."}]}