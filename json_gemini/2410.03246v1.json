{"title": "Latent Action Priors From a Single Gait Cycle Demonstration for Online Imitation Learning", "authors": ["Oliver Hausd\u00f6rfer", "Alexander von Rohr", "\u00c9ric Lefort", "Angela P. Schoellig"], "abstract": "Abstract-Deep Reinforcement Learning (DRL) in simulation often results in brittle and unrealistic learning outcomes. To push the agent towards more desirable solutions, prior information can be injected in the learning process through, for instance, reward shaping, expert data, or motion primitives. We propose an additional inductive bias for robot learning: latent actions learned from expert demonstration as priors in the action space. We show that these action priors can be learned from only a single open-loop gait cycle using a simple autoencoder. Using these latent action priors combined with established style rewards for imitation in DRL achieves above expert demonstration level of performance and leads to more desirable gaits. Further, action priors substantially improve the performance on transfer tasks, even leading to gait transitions for higher target speeds. Videos and code are available at https://sites.google.com/view/latent-action-priors.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Reinforcement Learning (DRL) has successfully enabled robots to learn complex behaviors from interactions with their environment [2-4]. Examples of applications of DRL in robotics include quadrupeds [3], drones [4], and manipulators [5, 6]. Part of this success is attributed to the model-free nature of DRL methods, which enable learning without expert knowledge and potentially easy-to-use algorithms. However, in order to learn complex behaviors, these algorithms typically require vast amounts of data, which can be hard to obtain. Moreover, policies trained in simulation may not be directly transferable to hardware due to the widely common simulation-to-reality gap. One way to improve data efficiency and sim-to-real transfer is to introduce inductive biases\u00b9 towards desired solutions.\nImitation learning (IL) approaches such as behavior cloning (BC) [7], generative-adversarial IL (GAIL) [8], or style rewards [1] introduce such biases by guiding the learning process toward behaviors observed in expert demonstrations. However, such methods that directly learn from expert transitions are often limited by the quality of the expert data, which restricts the agent to replicate potentially suboptimal behaviors. Additionally, generative-adversarial-based methods often introduce difficult-to-tune hyperparameters.\nThis paper proposes a complementary approach to IL: using a prior on the DRL action space learned from expert demonstrations. We show that these action priors can be learned from a single open-loop gait cycle using a simple autoencoder (Fig. 1). It is typically difficult to perform imitation learning on such non-diverse data that cover only a small region of the state space. Our latent action space priors capture task-specific correlations between actions, and thereby compress the action space into a low-dimensional manifold. This representation is especially suitable if the behavior is to be imitated with IL. The DRL agent can freely combine the action priors, offering a flexible and efficient representation that guides learning. This way, the agent is not"}, {"title": "II. RELATED WORK", "content": "Imitation Learning (IL). IL describes learning behaviors from expert examples. In case an expert policy is available, it can be used to query state-action pairs during IL (DAgger) [12]. We, however, do not assume access to an expert policy but only the availability of expert data in the form of state-action pairs. The seminal approach to IL from such data is Behavioral Cloning (BC) [7], which requires large amounts of expert samples (10 rollouts or more for simple environments [8, 13-15]). More recent approaches use generative adversarial networks (GANs) to distinguish between expert transitions and learned transitions. These approaches achieve imitation based on one rollout of available expert data for simple tasks, but still require larger datasets for complex tasks such as humanoid walking (GAIL) [8]. We hypothesize that even for complex tasks one gait cycle of expert demonstration contains sufficient information to significantly improve DRL when using latent action priors. Lastly, when only state transitions are available, the agent can be encouraged to behave similarly to the expert using style rewards. Style rewards can either be manually designed [1] or obtained with GAN-based methods (AMP) [16]. GAN-based methods suffer from difficulties in hyperparameter tuning and long training times. In contrast, style rewards offer more stable training for low data availability and thus are predominantly used for robotic applications [17-19]. In this work, we combine manually designed style rewards for imitation learning with our latent action priors.\nAction representations. Many natural movements in humans and animals can be described using a lower dimensional space. In human grasping, for instance, 80% of the variance can be explained using only two principle components [9]. Such so-called motion synergies have been used to design for dexterous robotic manipulation [10, 11]. Whole-body muscle synergies for locomotion have also been exploited to enable low-rank DRL by using the value function to estimate the similarity between actuators [20]. In contrast to these prior works that manually design action representations or discover them during the reinforcement learning cycle, we show that for a locomotion task, a single gait cycle of demonstration is sufficient to obtain suitable action representations.\nAnother way to represent actions for robotic locomotion is via oscillators, where parameters such as amplitude, frequency and phase-offset are learned [3, 21]. Oscillators enable learning of desirable gaits and sim-to-real transfer but increase the action space size and require different tuning for stance and swing phases. Oscillators mostly provide a temporal prior on the locomotion by forcing desired frequencies, whereas we suggest to exploit spatial priors describing correlations between actions.\nPriors in Robot Learning. If domain-specific prior knowledge exists, it can be leveraged to improve sample efficiency and achieved reward of robotic learning algorithms. Imitation learning and action representations are priors that can be used in case of available expert data or knowledge about the target movement. Other priors include dynamic models [22], reward shaping, models of expected returns [23], structured policies [24], and behavior priors via motion primitives [25]. Shaped exploration also incorporates assumptions about learning processes [26]. Finally, any form of temporal control abstraction, e.g., through hierarchical control, can be seen as a prior [27]. A comprehensive overview of priors used in robotic learning is given in Chatzilygeroudis et al. [28]. We propose that latent actions learned from a single gait cycle of demonstration provide a strong and beneficial prior for reinforcement learning in locomotion tasks."}, {"title": "III. METHOD", "content": "For this work, we assume the availability of expert data in the form of state action pairs for only a single gait cycle of demonstration. We propose to extract a latent representation from the actions, that is subsequently used as a prior for DRL. We combine the latent action priors with style rewards based on the expert's states [1] for imitation learning (Fig. 1).\nLatent action priors. We show that the actions from the demonstration for our locomotion tasks can be described in a lower dimensional latent space (Fig. 3). We learn the latent information using a standard non-linear autoencoder. An additional loss term encourages the latent action values to stay in [-1,1], which makes them suitable to use with standard DRL frameworks. The loss function is\n$\\mathcal{L}(a,\\hat{a},a_l) = ||a - \\hat{a}||^2 + \\mathcal{L}_{norm}(a_l),$\nwith\n$\\mathcal{L}_{norm}(a_l) = \\begin{cases} 0 & \\text{if } ||a_l||_{\\infty} < 0.8 \\\\ e^{ (||a_l||_{\\infty} - 1.0)} & \\text{otherwise}, \\end{cases}$\nwhere $a$ denote the actions, $\\hat{a}$ the predictions from the autoencoder, and $a_l$ the actions in the latent space.\nFor the remainder of the paper, we parameterize the non-linear encoders and decoders with one hidden layer with size 2|$a$| and use tanh as activation function. We add a residual of the full action space to the decoded latent actions before the actions are applied as joint torque commands to the robot. The weighting factor for the full action residual is a hyperparameter of our approach (Tab. I).\nExpert demonstrations. In order to demonstrate learning with little data we use a single gait cycle from the locomotion as the expert demonstration, as determined by visual inspection. Refer to the accompanying website\u00b2 for the demonstrations. For the humanoid, for instance, this corresponds to one left and right footstep. The demonstrations contain the following number of data points (frames) for each environment: HalfCheetah (5), Ant (11), Humanoid (22), Unitree A1 (46), Unitree H1 (106). For Ant and HalfCheetah, we use experts from [26] that have been optimized using open-loop oscillators. For humanoid, we train our own expert using off-policy DRL [31]. For Unitree A1 and Unitree H1, we take experts from the loco-mujoco benchmark environments [30]. There is no restriction to use more expert demonstration data in our method.\nPolicies. We test our method on the on-policy algorithm PPO [32], as it is most widely used in robotics [3, 4]. We use the default hyperparameters for the PPO learner from stable-baselines3 [33], unless stated otherwise in Tab. I. The observations and actions correspond to the defaults in the respective environments. We normalize the observations using the running mean and standard deviation during training. As we train cyclic locomotion tasks and incorporate a style reward, we additionally provide a phase variable $\\phi$ as input to the policy, as described in the next section. Note that the phase variable is only required for the style reward, but not for our latent action priors.\nReward. We use the default rewards of the environments and call them task reward $r_{task}$. For the baselines with style rewards an additional style reward term $r_{style}$ is added. This style component encourages the agent to behave similarly to the expert. The reference style corresponds to the same gait cycle of expert demonstration used to obtain the latent action priors. The full reward then is\n$r = w_{task} r_{task} + w_{style} r_{style},$\nwhere $w_{task}$ and $w_{style}$ are scalar weighting factors. We follow Qian et al. [17] and set $w_{task} = 0.67$ and $w_{style} = 0.33$. For the experiments without style reward, we set $w_{task} = 1$ and $w_{style} = 0$.\nFor the style reward component, we adopt the major reward term from DeepMimic [1] as\n$r_{style} = \\exp\\Biggl(-\\alpha \\cdot \\sum_{j} (q_j^t - q_j^{exp})^2\\Biggr).$\nThis pose reward encourages the agent to match the style of the expert at each time step and is computed as the difference between the j joint positions of the currently trained agent $q^t$ and expert demonstration $q^{exp}$. In addition to the joint positions, q includes the orientation of the torso of the agent in world coordinates. Using only one style reward term instead of multiple terms avoids tedious reward tuning.\nAs we consider only one gait cycle of expert demonstration data but want to match the style throughout the entire simulation episode, we introduce a phase variable $\\Phi \\in [0... N]$ that replays throughout the episode and ensures a Markovian reward. N is the number of frames in the expert demonstration. $\\Phi$ increases by 1 at every time step and is reset to 0 once $\\Phi$ reaches N. This phase variable maps the desired expert style to each time step in the episode.\nIn contrast to DeepMimic [1], we do not provide any information about the agent's (x,y) positions in the world coordinate frame to the reward or the policy. This is because i) we only have one gait cycle of expert demonstration instead of a full episode, and ii) we want the policy to be independent of the absolute position for robotic applications."}, {"title": "IV. RESULTS", "content": "We evaluate our method on the benchmark environments from Towers et al. [29] and Al-Hafez et al. [30] shown in Fig. 2. Additionally, we create environments to test the transferability of the expert demonstrations to other tasks. In one of these environments, a pair of Unitree Al robots collaborate to solve a task, demonstrating the efficacy of our approach in complex scenarios (Fig. 2.f). In this 2x Unitree Al environment, the goal is to maneuver a 1m rod to a randomly sampled target location. This scenario is interesting from a robotics perspective, as optimization of a monolithic policy might perform better than developing separate controllers for each quadruped. To the best of our knowledge, a similar task has only been solved using an optimization-based approach [34].\nFirst, we show that for our locomotion tasks, the actions from the demonstrations can indeed be described using a low number of latent actions. For all our agents, a latent space size of half the full action space is sufficient to explain more than 97% of the variance in the expert's actions (Fig. 3). For all further experiments, we set the latent action space dimension $a_l$ to half the action space dimension $a_{full}$. We round up in case of an odd action space dimension.\nNext, we compare using the latent actions as prior in reinforcement learning (PPO+latent) to baseline PPO and PPO with style rewards stemming from the same expert gait cycle demonstration (PPO+style) (Fig. 4). For all environments, latent actions significantly improve the speed of learning and maximum achieved reward compared to baseline PPO. Specifically, PPO+latent achieves a task reward increase of > 50% in the mean. Notably, our approach achieves above expert level for HalfCheetah and Ant (Fig. 4). This is because the agent can freely learn to combine the latent actions prior and the residual of the full action space to maximize the task reward. As the goal for the loco-mujoco-based agents (i) and (j) is to follow the speed from the expert demonstration, we can not see above expert-level performance for these tasks.\nWe show the contributions of the decoded latent actions and the full action space to the overall applied actions for Unitree A1 in Fig. 9. The contribution of the decoded actions changes throughout the training process. Specifically, the agent disregards the latent action priors at the beginning of the training. We assume that the latent action priors are not useful when the agent first needs to learn stable standing but are used later on in the transition to walking.\nCombining latent actions with style rewards improves the results in all cases, except for the Humanoid environment. This demonstrates that the latent action prior is especially useful for imitation learning.\nFor the Humanoid, style rewards do not improve learning over the baseline. However, this might be due to the expert data that was not obtained from an open-loop controller but from a trained DRL policy. We observe that this expert demonstration exhibits brittle walking behavior compared to the open-loop baselines for HalfCheetah and Ant a typical pathology encountered in DRL policies. We assume that this is the reason why the expert's style is difficult to imitate for the on-policy learner. Although the style reward does not improve learning, the latent actions provide a beneficial prior.\nFor a qualitative evaluation of the learned gaits we visually inspected the gaits after learning (see accompanying website). Using PPO+latent actions already leads to a more natural gait. However, as the latent actions provide only a spatial prior on the gait, i.e., correlations between"}, {"title": "V. DISCUSSION", "content": "Latent action priors significantly improve the achieved reward and convergence speed for the on-policy reinforcement learning algorithm PPO. We also demonstrate improved performance on transfer tasks and for suboptimal demonstrations. This is an advantage to other approaches in IL that directly try to imitate expert transitions. Using style rewards in conjunction with our approach significantly improves performance and the gait's visual appearance.\nFurther, we are able to perform imitation learning using only one open-loop controlled gait cycle of expert demonstration. Compared to DRL-based policies, deployable open-loop oscillator-based controllers can easily be designed in the real world, which makes them especially suitable for IL in robotics. However, the lack of diversity in data generated by such controllers often makes imitation learning challenging.\nIn contrast to recent trends in imitation learning, we attempt to imitate based on a micro-dataset (5-106 samples). We believe that our method can be applied using an even lower number of expert data frames. However, latent actions and style rewards are not necessarily limited to the micro-data regime. Future work could investigate how these approaches benefit from larger datasets of expert demonstrations. An interesting line of work could be the integration of multiple gait styles in the latent action priors that are available in those datasets.\nAlthough in our paper, we focused on on-policy methods as they are most widely used in robotics applications [3, 4], we think that latent action priors are beneficial for off-policy IL methods as well. As latent action priors complement existing IL approaches, future work could combine latent action space priors with other SOTA methods such as GAIL [8] or IQ-Learn [13].\nA significant limitation of our work is that we require access to the ground-truth actions. A variety of work focuses on inferring actions from observation-only datasets [15, 35, 36]. An expansion of our work could investigate how to combine these approaches with ours and extract latent action spaces from observations.\nFinally, whereas we demonstrate our approach in simulation, a logical next step is the deployment of our approach on a real robot."}, {"title": "VI. CONCLUSION", "content": "Latent action priors learned from a single gait cycle of expert demonstration significantly improve the task-solving capabilities of reinforcement learning with PPO. We expect that our method also improves the outcomes of reinforcement learning in unsolved and complex tasks beyond the ones investigated in this paper. Combined with style rewards, latent action priors enable imitation learning from only a few data points that can be generated using easy-to-design open-loop controllers. This avoids the need for extensive datasets covering the majority of the state space for imitation learning. Latent action priors are even beneficial when the demonstrations are suboptimal, which is useful in case data for transfer tasks is difficult to obtain."}]}