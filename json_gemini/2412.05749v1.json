{"title": "A Comparative Study on Code Generation with Transformers", "authors": ["Namrata Das", "Rakshya Panta", "Neelam Karki", "Ruchi Manandhar", "Dinesh Baniya Kshatri"], "abstract": "In an era of widespread influence of Natural Language Processing (NLP), there have been multiple research efforts to supplant traditional manual coding techniques with automated systems capable of generating solutions autonomously. With rapid research for code generation and a sole focus on large language models, there emerges a need to compare and evaluate the performance of transformer architectures based on several complexities of the model. This paper introduces the concept of a \"A Comparative Study on Code Generation with Transformers,\" a model based on Transformer architecture, and NLP methodologies to automatically generate C++ source code for different varieties of problems. Here, a comparative study is performed to evaluate the robustness of transformer-based models on the basis of their architecture complexities and their capability to handle diverse problem sets, from basic arithmetic to complex computations.", "sections": [{"title": "I. INTRODUCTION", "content": "Computer programming has become one of the foundations of technological evolution. With the rapid progress in machine learning and natural language processing, computer program-ming has become way easier with the help of numerous models capable of generating functional and executable code on the fly. With this research, we attempt to compare the performances and analyze the code execution capability of the generated output of transformer-based code generation models with different architectural complexities. The main objectives of the research are to train a base transformer for generating C++ source code via pseudo-code of problems related to arithmetic, array, string, and sorting operations; to perform transfer learning on a pretrained transformer model for better performance on pseudo-code to C++ source code conversion; to compare the results obtained from the base transformer and pretrained transformer model and analyze the performance. It must be noted that the comparison of performance also centers on the computational resources required by different models with different architectural designs. Additionally, pre-vious works were primarily focused on converting one line of pseudocode into corresponding C++ code statements. The major issue they suffered was the initialization error. This problem was tackled in this project by using complete program pseudocode as input to the model."}, {"title": "II. RELATED WORKS", "content": "There have been numerous endeavors for automated source code generation. Before natural language processing and large language models, most code generation tasks were carried out using UML statecharts [1]\u2013[3]. The utilization of machine learning algorithms were limited to compiler heuristics optimization and parallelization [4], [5]. Initial implementations of natural language processing models for code generation can be seen in the work of Barone et al. where they attempted code generation using Python docstrings [6]. Similarly, HTML code generation using wireframes was conducted by A\u015firo\u011flu et al. [7]. Later on, transformer based models primarily dominated the code generation domain [8]. Different models were introduced which were customized for tasks such as code completion, code understanding, code generation and also to support multilingual tasks [9]\u2013[11]. Meanwhile, comparison studies were also conducted between different neural network architectures for code generation and filling mask tasks [11]. Alphacode is one of the popular examples of transformer based code generation model for competitive programming [12].\nWith the code generation, a number of metrics for code evaluation were also introduced, CodeBLEU being most popular among them [13]. It is a modified version of popular machine translation evaluation metrics BLEU score [14]. CodexGLUE, a benchmark for code generation was established by OpenAI to encourage research in multiple code related tasks including clone detection, defect detection, cloze test and code summarization [15], [16]. Also, MCoNala, a benchmark for code generation from multiple natural languages was introduced adding more diversity to the feat of automated source code generation [17]. Recently, numerous code generation researches has been carried out introducing extremely versatile and multitasking models like CodeBERT, CodeT5 and GPT-based models which are gaining popularity for implementation and integration in varied set of use cases of code generation, code completion, code correction, code summarization and so on [18]\u2013[20].\nThe SPOC dataset used in this project was introduced"}, {"title": "III. DATA EXPLORATION", "content": "SPOC (Search-based Pseudocode to Code) dataset with 18,356 C++ programs for 677 programming problems having human-authored programs, pseudocodes and test cases, has been used for this project [21]. Each problem has roughly 27 programs, which are likely to have similar semantics yet different code syntax. The variation in the problem set of the SPOC dataset is shown in Figure 1.\nIn the previous works, the approach taken was to convert one line of pseudocode into corresponding C++ code statement [21], [22]. This introduced initialization errors. To tackle this issue and to tap into the competency of transformer based models for large sequence of sentences, the dataset has been modified such that the all the code and pseudocode statements belonging to any one program are aggregated together to one input and one reference output to the model. Along with it, code for basic header file imports are also added such that the generated output can be directly used for code generation. The modification brought to the dataset is illustrated in Figure 2 and Figure 3."}, {"title": "IV. IMPLEMENTATION", "content": "This system implements a structured code generation work-flow. The user-entered pseudocode undergoes UTF-8 encoding and is tokenized using a dedicated tokenizer. The parsed and tokenized input text is fed into an encoder, where queries gain similarity by referencing previously stored memory. Em-ploying an attention-weighted mean, the weights are mapped into values, representing stored information. These values are passed through a decoder, detokenized, and then concatenated. The concatenated output tokens form the generated code, which is UTF-8 decoded and postprocessed for optimal UI display. The final result is a C++ output program derived from the user's original pseudocode input."}, {"title": "A. Transformer Model", "content": "A comprehensive process was undertaken to enable the translation of pseudocode to C++ code through a custom-designed Transformer model and a pretrained transformer model fine-tuned on a new dataset and integrated into a web application. For the base transformer model, the BERT tok-enizer was fine-tuned and customized to differentiate between reserved and input tokens [23]. This facilitated the conversion of sentences into token IDs, essential for subsequent model input. Upon analyzing the pseudocode and C++ token vocab-ularies, the study identified 2285 unique tokens for pseudocode and 1989 for C++ programs. Positional encoding vectors with dimensions of 512 were established for 2048 positions.\nFigure 5 depicts positional encoding vectors with a dimension of 512 for 2048 positions. Blue points indicate positive values, reaching a maximum of +1; white denotes zero; and red points depict negative values, reaching a minimum of -1.\nAdditionally, masking was employed to disregard padding and look-ahead tokens. Hyperparameters were optimized through 5 iterations of random search, exploring layer counts (4-6), dmodel values (128-256), and dropout rates (0.1-0.2), with selected values of dmodel 128, dropout rate 0.1, and 4 layers for subsequent model training. Upon conducting training for 50, 80, and 30 epochs consecutively, it was noted that the model exhibited signs of overfitting during the 50 and 80 epoch training runs. Consequently, a training duration of 30 epochs with a batch size of 16 was selected as the optimal choice.\nFigure 6 shows random search performed to optimize hyperparameters in the base transformer model, including the number of layers, dmodel, and dropout rate. Five iterations were conducted with varying values, and the lowest validation loss occurred with parameters (128, 0.1, 4), resulting in a loss of 2.403 that was selected for model training.\nComparing the base transformer model with pretrained counterparts, the SPoC dataset was fine-tuned on 6 layers of CodeT5-small, consisting of 60.5 million parameters, for pseudocode-to-C++ translation [19]. CodeT5 had been trained for code translation tasks between Java and Csharp. On the grounds of semantic and syntactic similarity between Java and C++, the model was fine-tuned on the SPOC dataset. This further expanded the scope of the evaluation of the code generation capabilities of large language models. Tokenization of text utilized RobertaTokenizer, producing 'input_ids' and 'attention_mask' [24]. Dataset preprocessing led to the creation of a data dictionary with batch size of 8 for training and 4 for validation and test sets respectively.\nTo optimize performance, a random search explored hyperparameters including learning rate (1e-3 to 5e-5) and warmup steps (500, 1000, 1500). Among five instances, the best results emerged at a learning rate of 8e-4 and 1000 warmup steps, with validation losses ranging from 0.0857 to 0.1116.\nFigure 7 shows the random search conducted to optimize hyperparameters in the CodeT5 transformer model, focusing on warmup steps, number of epochs, and learning rate. With the number of epochs fixed at one, five iterations were conducted with varying values of learning rate and"}, {"title": "V. RESULTS", "content": "In comparison, the results obtained from the CodeT5 small model showed better performance than the base transformer for complex problem sets. However, much difference was not seen for simpler arithmetic programs."}, {"title": "VI. DISCUSSION", "content": "From the study, it was found that, though the number of layers and training parameters heavily affect the code generation performance, acceptable results can be obtained by transformers with very few layers and training parameters. The model was able to overcome the initialization errors found in previous works of code generation with the SPoC dataset when a complete program pseudocode and its corresponding C++ source code were input as pairs in the model instead of statement-wise input [21], [22]. Also, the models trained on different programming languages can generate executable source codes for an unseen programming language due to syntactic similarities. Here, in the case of CodeT5, the model was originally trained for translation between Csharp and Java [19]. The model was fine-tuned on a dataset of pseudocodes and C++ code pairs, and the results obtained were directly executable in most cases.\nThe code generation ability of the base transformer model for complicated problems with multi-step logic execution was not found to be executable and had multiple syntactic errors. The reason can be considered to be the small size and low variation of code problems in the training dataset. The base transformer model's performance seemed to decline with an increase in the complexity of problems, whereas CodeT5's performance was found to be robust enough to handle a variety of problems. The reason can be attributed to the multi-tasking capability of the model with training on a huge dataset of CodeSearchNet [25] with multiple programming languages. It is worth mentioning that the performance of models built on transformer architecture when trained on a smaller dataset requires training for multiple epochs to obtain reasonable output. This results in overfitting of the model on the training set and degrades its ability to handle unseen data as inputs. Hence, dataset sizes can be seen as the bottleneck for transformer models. The model's performance can be improved if the dataset is increased in size and variations in the complexity of problems are introduced.\nIt was found that the robustness of multiple code generation models to generate code with few sentence commands is a result of their huge architectural design with multiple layers and huge training data corpus. Along with design, the resources required to train such a large language model, result in significant computational costs. The computational requirements are not limited to training and validation stages, but the operational costs of models during their inference stages are also high. The average time taken to generate code for any input varied from 5 to 15 seconds for CodeT5 and 15 to 60 seconds for the base transformer model, respectively. Time requirements were found to heavily depend on the device used for computation, with faster results on GPUs. For faster results, GPUs with higher computing capabilities are required, which again leads to increased costs. Hence, the integration of large language models into the deployed systems is an expensive approach to improve the systems."}, {"title": "VII. CONCLUSION", "content": "It can be concluded that the corpus on which language models are trained heavily affects their performance in gen-erative tasks. Also, with increasing number of layers and parameters, the language models become more robust to handle unseen problems. It was observed that the CodeT5 model, though trained for translation between Csharp and Java, handled code generation tasks for the C++ language quite well when fine-tuned on pseudocode to code translation. The performance was found to be better than the four-layered base transformer model specifically trained for code generation from pseudocodes. The robustness of CodeT5 can be attributed to its large architecture with a higher number of layers than the base transformer and the large corpus of data that it has been trained on to carry out a number of coding tasks. In contrast, the resources and time required to train a few layered base transformer model was quite low in comparison to the resources required for training a large language model with billions of parameters. Hence, cost and performance trade-offs can significantly affect training a large language model for specific tasks."}]}