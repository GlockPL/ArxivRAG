{"title": "ProtoSAM - One Shot Medical Image Segmentation With Foundational Models", "authors": ["Lev Ayzenberg", "Raja Giryes", "Hayit Greenspan"], "abstract": "This work introduces a new framework, ProtoSAM, for one- shot medical image segmentation. It combines the use of prototypical networks, known for few-shot segmentation, with SAM a natural im- age foundation model. The method proposed creates an initial coarse segmentation mask using the ALPnet prototypical network, augmented with a DINOv2 encoder. Following the extraction of an initial mask, prompts are extracted, such as points and bounding boxes, which are then input into the Segment Anything Model (SAM). State-of-the-art results are shown on several medical image datasets and demonstrate automated segmentation capabilities using a single image example (one shot) with no need for fine-tuning of the foundation model.", "sections": [{"title": "1 Introduction", "content": "Deep learning models are now the go-to method for segmenting medical images. However, using these models usually requires a large set of manually labeled data, which is expensive and time-consuming. Additionally, these models strug- gle with new categories they have not seen before, requiring more training and adjustments. Few-shot segmentation (FSS) has been introduced as a solution to these problems. FSS trains the model to learn from just a few labeled examples, reducing the need for large, manually labeled datasets. Prototypical Networks (PN) [23] are widely used for FSS. These networks use prototypes, which repre- sent the key features of different classes, to make predictions based on similarity. One notable method is ALPNet [17,18], which is the leading FSS approach for medical data. Its key innovations are the Adaptive Local Prototypes Pooling module (ALP), which helps to better capture fine details in medical images, and utilizing superpixels as labels for training.\nRecently, a segmentation foundational model, Segment Anything Model (SAM) [12], was introduced. This model was trained on a curated dataset of 1B masks and 11M natural images. In our work, we aim to implement SAM for medical image segmentation. We hypothesize that with appropriately designed prompts, SAM could serve as an effective one-shot segmentation model for medical im- ages, as we show in Section 5. We demonstrate the performance of ProtoSAM"}, {"title": "2 Related Work", "content": "Few Shot Segmentation. In the supervised setting of medical image segmen- tation, deep learning models are trained to predict labels for each pixel in the input images. This process typically starts from scratch for each new task, requir- ing extensive annotated datasets. Few-shot segmentation offers a more practical solution in this context, enabling models to perform well with limited annotated data, typically referred to as the support set [7,13]. Prototypical Networks (PN) [23] are a popular choice for addressing few-shot learning tasks. They focus on exploiting representation prototypes of semantic classes extracted from the sup- port. These prototypes are utilized to make similarity-based predictions.\nALPNet [17,18] is a novel PN approach that introduced the Adaptive Local Prototypes (ALP) module, which is capable of generating prototypes at both lo-"}, {"title": "3 Method", "content": "Problem Formulation. In one-shot segmentation, the goal is to train a func- tion, $f(x^{q}, S)$, capable of generating a binary mask for an unseen class $c$ when provided with a support set, $S$, and a query image, $x^{q}$. The support set $S$, comprises pairs of images and their annotated masks $(x^{s}(c),y^{s}(c))_{i=1}^{k}, c \\in C_{test}$. Since we are dealing with one-shot learning, we set $k = 1$. Here, $x^{s}$ represents the image in the support image set, and $y^{s}(c)$ represents the annotated mask of the support image corresponding to the unseen class $c$. For training, we split the data into a training dataset, $D_{train}$, and a testing dataset, $D_{test}$. These datasets are comprised of image-binary mask pairs, with $D_{train}$ annotations belonging to $C_{train}$ and $D_{test}$ annotations belonging to $C_{test}$. It is important to note that $C_{train} \\cap C_{test} = \\emptyset$.\nThe training process of FSS networks is comprised of episodes. An episode consists of a pair of a support set and a query image, $(S, x^{q})$, where $S$ is a subset of the training dataset $D_{train}$ ($S \\subset D_{train}$), and $x^{q}$ is a query image. It is crucial to note that the query mask $y^{q}(c)$ is utilized solely for training purposes as a ground-truth label. Each episode is randomly selected from $D_{train}$, with"}, {"title": "Initial Segmentation Stage", "content": "For the initial segmentation stage we employ the ALPNet [17] framework, but we replace the encoder with a DINOv2 [16] encoder. The purpose of this stage is to extract a coarse segmentation mask and logits from the query image, based on the support set. We encode the support and query image using the encoder, $f_{e}(.)$ and extract the support feature map, $f_{e}(x^{s}) \\in R^{D \\times H \\times W}$, and the query feature map $f_{e}(x^{q}) \\in R^{D \\times H \\times W}$, where $D$ is the number of channels, and $H, W$ are the feature map spatial dimensions. The ALP module extracts local prototypes from $f_{e}(x^{s})$ by average pooling features for the support feature map using a sliding window. We define a local prototype at location $(m,n)$ for class $c$ as:\n$\\qquad P(m, n; c) = \\frac{1}{L_{H}L_{W}} \\sum_{h w} f_{e}(x^{s}(c))(h, w),$\nwhere $(L_{H}, L_{W})$ are the sliding window dimensions and $mL_{H} \\leq h < (m + 1)L_{H}, nL_{W} \\leq w < (n + 1)L_{W}$.\nAdditionally, we extract a global prototype defined as:\n$ P^{g}(c) = \\frac{\\sum_{h,w} y^{s}(c)(h, w) f_{e}(x^{s}(c))(h, w)}{\\sum_{h,w} y^{s}(c)(h, w)}$\nFor cases where the foreground class may be much smaller than the pooling win- dow. For each class (foreground or background), the prototypes are aggregated into a set $P_{c} = \\{P^{i}(c)\\}$. A similarity map is generated between each prototype of class $c$ and the query feature map, $f_{e}(x^{q})$, using:\n$S^{i}(c)(h, w) = \\alpha \\cdot P_{i}(c) \\odot f_{e}(x^{q})(h, w)$\nWhere $\\odot$ denotes cosine similarity, and $i$, the index for the prototype. We set $\\alpha = 20$ as in [17]. These similarity maps are merged into a class-wise similarity map using:\n$S^{\\prime}(c)(h, w) = \\sum S_{i}(c)(h, w) \\cdot softmax[S_{i}(c)(h, w)]$\nWe normalize the similarity maps for the foreground and background class into probabilities using:\n$\\hat{y}^{q}(h, w) = softmax[S^{\\prime}(c)(h, w)]$\nIt is important to note that we do not perform any fine-tuning unless explicitly stated otherwise, and rely solely on the feature extraction capabilities of the DINOv2 encoder."}, {"title": "Prompt Extraction", "content": "This stage receives as input the initial prediction prob- abilities. From these, we extract prompts in the form of points and bounding boxes for SAM [12]. Using connected component analysis, we first extract the most confident connected component from the initial prediction using the fol- lowing formula to rank the prediction confidence of each connected component:\n$Confidence = \\frac{\\sum_{i} p_{i} \\cdot \\hat{y}_{i}}{\\sum \\hat{y}_{i}}$\nwhere $p_{i}$ is the probability that pixel i belongs to the foreground class, $\\hat{y}_{i}$ is the i-th pixel's predicted label. From this point on, we only consider the probabilities and mask corresponding to that connected component. We extract the following prompts from the predicted logits and mask: bounding box (Bbox), centroid points (Cent) and confidence-based points (Conf). We set the bounding box to be the bounding box of the connected component, the centroid point is selected to be the centroid point of the connected component, and for the confidence- based point, we choose the point corresponding to the highest probability of the foreground class inside the connected component."}, {"title": "Final Segmentation", "content": "The prompts of the previous stage are fed into the Seg- ment Anything Model (SAM) which produces the final finer segmentation map.\nOptional: Encoder fine-tuning (EFT). To enhance performance, it is possi- ble to fine-tune the DINOv2 encoder on the target data. For that, we employ the ALPNet framework [17]. We emulate real-life scenarios by constructing episodes, comprised of a support set and a query set. A preprocessing step is taken, where we generate superpixels using Felzenszwalb [6] for all the available slices. At each episode an image $x$ is chosen from the train-set, together with a random superpixel $y^{r}(c^{p})$, which is used as the pseudo-label. This forms the support set $S = (x, y^{r}(c^{p}))$, where $c^{p}$ denotes the superpixel class and $r$ is the index of the random superpixel. The query set is formed by augmenting the chosen image $x$, i.e, $Q_{i} = (T_{g}(T_{i}(x))$, where $T_{i}$ and $T_{g}$ are geometric and intensity transforms respectively. The loss is\n$L_{seg}(\\theta; S, Q) = -\\frac{1}{H W} \\sum_{h=1}^{H W} \\sum_{w=1} \\sum_{c \\in \\{c^{0}, c^{p}\\} } T_{g}(y^{r}(c))(h, w) log(\\hat{y}^{r}(c)(h,w)),$\nwhere $\\hat{y}^{r}(c^{p})$ is the prediction of the pseudolabel, $c^{0}$ is the background class and $\\theta$ - the model parameters. Also, as in [17], we incorporate the prototype alignment regularization [25], where the roles of the support label and the prediction are reversed. The prediction assumes the role of the support label, and our aim is to segment the original superpixel accordingly. The regularization loss is\n$L_{reg}(\\theta; S^{\\prime}, S) = -\\frac{1}{H W} \\sum_{h=1}^{H W} \\sum_{w=1} \\sum_{c \\in \\{c^{0}, c^{p}\\} } y^{r}(c)(h, w) log(\\hat{y}^{r}(c)(h, w)),$\nwhere $S^{\\prime} = (x, \\hat{y}^{r}(c^{p}))$, and $\\bar{y}^{r}(c^{p})$ is the prediction of the superpixel label $y^{r}(c^{p})$."}, {"title": "6 Limitations", "content": "The current method, selects prompts based on the most confident component of the initial segmentation phase, limiting it a single object segmentation, making it unsuitable for tasks requiring multiple object segmentation. Additionally, while ProtoSAM outperforms many existing methods, there are still cases where SAM provides better masks, indicating room for improvement in the prompt genera- tion process. Furthermore, there is a performance gap between fully-supervised models and ProtoSAM's one-shot performance, evident in the polyp segmenta- tion results, suggesting that further refinement of the method is needed to match SOTA supervised approaches. Lastly, the encoder fine-tuning (EFT) technique, which improved results for CT and MRI datasets, led to degradation of results for polyp segmentation when using Felzenszwalb generated superpixels, highlighting the need for a more robust fine-tuning strategy across diverse datasets."}, {"title": "7 Conclusion", "content": "In this work we introduce ProtoSAM, a novel automatic framework for one-shot image segmentation. The experimental results demonstrate that the untrained ProtoSAM achieves competitive performance across various organs and modal- ities, often surpassing trained methods. Encoder fine-tuning further boosts its performance, enabling it to outperform all previous methods, including the best masks generated by the original SAM model, in specific areas. Future work could explore the refinement of prompt generation, including prompt generation for MedSAM and extension of the framework to other segmentation tasks. To"}]}