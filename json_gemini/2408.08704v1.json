{"title": "Beyond the Hype: A dispassionate look at vision-language models in medical scenario", "authors": ["Yang Nan", "Huichi Zhou", "Xiaodan Xing", "Guang Yang"], "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across diverse tasks, garnering significant attention in AI communities. However, their performance and reliability in specialized domains such as medicine remain insufficiently assessed. In particular, most assessments over-concentrate in evaluating VLMs based on simple Visual Question Answering (VQA) on multi-modality data, while ignoring the in-depth characteristic of LVLMs. In this study, we introduce RadVUQA, a novel Radiological Visual Understanding and Question Answering benchmark, to comprehensively evaluate existing LVLMs. RadVUQA mainly validates LVLMs across five dimensions: 1) Anatomical understanding, assessing the models' ability to visually identify biological structures; 2) Multimodal comprehension, which involves the capability of interpreting linguistic and visual instructions to produce desired outcomes; 3) Quantitative and spatial reasoning, evaluating the models' spatial awareness and proficiency in combining quantitative analysis with visual and linguistic information; 4) Physiological knowledge, measuring the models' capability to comprehend functions and mechanisms of organs and systems; and 5) Robustness, which assesses the models' capabilities against unharmonised and synthetic data. The results indicate that both generalized LVLMs and medical-specific LVLMs have critical deficiencies with weak multimodal comprehension and quantitative reasoning capabilities. Our findings reveal the large gap between existing LVLMs and clinicians, highlighting the urgent need for more robust and intelligent LVLMs. The code and dataset will be available after the acceptance of this paper.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT advancements in foundation models have demonstrated significant capabilities across diverse tasks. Among various foundational models, Large Vision-Language Models (LVLMs), which integrate visual and linguistic knowledge (e.g., GPT-4, Gemini, and LLaVA), have demonstrated impressive performance in real-world applications. Building upon these achievements, an increasing number of medical LVLMs have been developed, boasting remarkable performance in image captioning, visual grounding, and visual question-answering (VQA). Despite the broad applicability of LVLMs, their performance and reliability in the medical domain remain insufficiently assessed. This gap in evaluation is particularly concerning given the high stakes and complexity inherent in medical applications.\nTo bridge these gaps, this study built a comprehensive dataset, RadVUQA, to evaluate LVLMs in various aspects. RadVUQA incorporates two modalities, including multi-anatomical Computed Tomography (CT) and Magnetic Res- onance Imaging (MR) datasets. Unlike existing studies that primarily focus on recognizing low-level characteristics such"}, {"title": "II. RELATED WORKS", "content": "Vision-language models have garnered significant attention in recent years due to their ability to understand and inte- grate vision and language instructions. By jointly training on a vast dataset of images paired with corresponding textual descriptions, CLIP [4] learns to map images and text into a shared embedding space. Different from this straightforward learning protocol, Flamingo [5] and BLIP2 [6] utilized frozen image encoders to obtain visual representations and integrated them with textual instructions to form the input. In particular, Flamingo achieved this by utilizing cross-attention layers to integrate visual and linguistic features. In contrast, BLIP2 introduced a Q-former to bridge the frozen image encoder with the frozen language model, ensuring that the most relevant visual features are transferred to the LLM decoder to produce the desired text. Additionally, Liu et al. [7] proposed an auto- matic scheme to construct 158K language-image instruction- following data with the LVLM LLaVa. To further improve the quality of visual representations, Chen et al. scaled up the vision encoder to 6 billion parameters, integrating it into the InternVL [8], aiming to align the representation of the enhanced vision encoder with the LLM.\nInspired by these achievements, attention has been shifted to the clinical domain. Building on OpenFlamingo-9B, Med- Flamingo [9] pre-trained using paired and interleaved medical image-text data collected from publications and online text- books. Similarly, LLaVA-Med [10] constructed a biomedical VQA dataset by sampling image-text pairs from PMC-15M [11] and utilized GPT4 to produce multi-round questions and answers. In addition to 2D image inputs, RadFM [12] introduced an architecture that supports visually conditioned generative pre-training. It integrated text input with 2D or 3D medical scans, enabling the generation of responses for diverse radiologic tasks. To alleviate the inherent data noise, Chen et al. [13] refined medical image-text pairs from PubMed and constructed PubMedVision. With this dataset, HuatuoGPT- Vision [13] was proposed by incorporating Qwen as vision encoder [14] and Yi [15] as the LLM part.\nDespite growing attention, there remains a lack of com- prehensive evaluation of generalised LVLMs in the medical domain, especially for commercial ones (e.g., GPT4o, Gemini, etc.). To address this gap, this study conducts a thorough evaluation of highly competitive LVLMs."}, {"title": "III. METHODS", "content": "This section introduces the details of data collection and question design of RadVUQA. Data collection demonstrates the data resources and properties, while question design refers to how we build different subsets for specific assessments.\n1) Data Resources: RadVUQA was developed using multi- source, multi-anatomical public datasets, resulting in the sub- sets RadVUQA-CT, RadVUQA-MRI, and RadVUQA-OOD. Specifically, 2D CT(MR) images were sampled to ensure a diverse representation of body parts, encompassing 117(56) classes such as the spleen, heart, and kidney for RadVUQA- CT (RadVUQA-MRI). The dataset also varied in terms of prompts, including purely visual prompts, visual prompts with text descriptions, and prompts incorporating both text and spatial instructions. Additionally, the question types were cat- egorized into open-ended and close-ended formats to provide a comprehensive evaluation framework.\nRadVUQA-CT was collected from TotalSegmentator, one of the largest publicly available full-body CT datasets [18]. The dataset comprises 1,204 3D CT scans, with annotations for 117 distinct anatomical human structures. Scans with an axial plane width or height smaller than 200 pixels were excluded. From each scan, we extracted fifteen 2D slices from the transverse, sagittal, and coronal planes, specifically at 25%, 40%, 55%, 70%, and 85% of the total number of layers along each axis. This extraction process resulted in a total of 11,448 2D CT images, each accompanied by corresponding mask labels.\nRadVUQA-MRI was derived from TotalSegmentator-MRI [19], which consists of a random sample of MRI scans performed at the University Hospital Basel PACS between 2011 and 2023. The original dataset comprises 298 3D MRI scans, each annotated with 56 anatomical human structures. Scans with an axial plane width or height smaller than 200 pixels were excluded. We extracted 1,021 2D axial slices and their corresponding masks, taken at 10%, 25%, 40%, 55%, 70%, and 85% of the total number of slices.\nRadVUQA-OOD was collected from multiple resources, in- cluding 250 synthetic 2D chest CT images from [20], 250 real 2D chest CT images from [21], and 63 2D animal CT scans from embodi3D\u00b9. Additionally, the real images (from [21]) were augmented to simulate unharmonised data by introducing motion-blur, window-shift, noisy, sharpness, and low-resolution variants, with 1250 images (250 for each) in total.\n2) Data Preprocessing: In TotalSegmentator, the raw se- mantic labels are instance-based, assigning distinct labels to different instances within the same semantic category (e.g., different ribs). However, this labelling criteria becomes over- detailed for designing VQA datasets, which is beyond the scope of the capabilities of existing LVLMs. Therefore, we assigned four types of labels to each instance. (1) spatial category label (e.g., left lung, right lung, etc.); (2) category label (lung, heart, gut, rib, etc.); (3) anatomical location (abdominal cavity, thoracic cavity, pelvis, etc.); and (4) general category (e.g., organ, gland, bone, muscle, etc.). Details of our mapping criteria can be found in the supplementary materials. These labels enable us to set up QA pairs based on prior knowledge instead of manual labelling."}, {"title": "B. Question Design", "content": "Unlike most VQA datasets that were typically organized based on classification data, RadVUQA was developed using pub- licly available segmentation datasets (Fig. 1). This ensures that the QA pairs in RadVUQA can be more accurately generated and have richer semantic meaning than existing counterparts.\n1) RadVUQA-CT and RadVUQA-MRI: Each question in RadVUQA comprised a context prompt and a specific query. Initially, we set up a basic or an advanced prompt for each context prompt of the given image. The basic prompt briefly introduces the context of the input data, while the advanced prompt gives more details by telling LVLMs about the existing structures. It is of note that in all the QA examples, the content within the '{}' represents the semantic labels specific to each image.\nFollowing the context prompt (basic or advanced), the specific query assesses the capabilities of LVLMs by introduc- ing open-ended questions (OEQs) and close-ended questions (CEQs) across the following aspects.\nIt could be found that OEQ1.1 has less prior knowledge than OEQ1.2, which was designed to test the effectiveness of Prompt CoT for different LVLMs.\nMulti-modality comprehension explores the capabilities of interpreting linguistic and visual instructions. We achieved this by providing prior knowledge of the RoI (Region of Interest). In particular, we labelled the Region of Interest (RoI) with a green bounding box in the input image and asked LVLMs to recognize its anatomical structure. This task requires the model to not only understand linguistic instructions but also accurately localize the prompt bounding box (the green one). Additionally, CEQ2 refers to multiple-choice questions, fur- ther testing the model's comprehension ability.\nQuantitative and spatial reasoning test LVLMs' capabili- ties in quantitative analysis and spatial perception. OEQ3.1 simultaneously investigates the model's quantitative capability and anatomical knowledge, requiring the model to identify the largest object and distinguish its anatomical labels.\nSubsequently, OEQ3.2 and OEQ3.3 gradually reduce the complexity of the spatial perception assessment, allowing us to evaluate the model's ability to understand and interpret spatial relationships with decreasing difficulty.\nPhysiological knowledge investigates the model's capability to know the mechanisms and comprehend the functional roles of organs or structures.\n2) RadVUQA-OOD: Although there have been several benchmark studies for medical LVLMs, most of them ignore the effects of imaging variety and overly focus on increasing the scale of evaluation sets rather than more insights. Here comes the question: how do the LVLMs (Large Vision- Language Models) perform across different imaging settings, quality and characteristics? Unfortunately, despite the few existing studies only assessing noisy interference [17], this question remains largely unexplored.\nInitially, CT and MRI heavily rely on image preprocessing techniques. For instance, the influence of acquisition protocols (such as different reconstruction kernels, normalisation strate- gies, and patient positioning) on model performance has not been thoroughly investigated. These factors are critical in real- world clinical environments where imaging conditions are far from standardized and harmonised. Guided by [22], the OOD subsets were designed to assess the models' capability against these different scenarios, including noise, diverse contrast, sharpness, motion blur, low-dose scanning, etc. Specifically, we assess LVLMs in the following aspects:\nRobustness simulate various scenarios including motion blur, biased imaging protocol (different reconstruction kernels with different contrast and sharpness), low-resolution scanning, and noisy data. For fair comparisons, the QA pair for the unharmonised data remains the same as those for OEQ1.1, OEQ1.2, and CEQ1.\nSafety capability estimates the capacity of LVLMs against adversarial attacks or malicious instructions. For instance, replacing the query image with synthetic scans or nonhuman scans. Therefore, we designed two QA pairs to test this phenomenon"}, {"title": "IV. EXPERIMENTAL SETTINGS AND METRICS", "content": "Experimental settings. Models were evaluated on two NVIDIA H100 NVL GPUs. The system architecture includes an AMD EPYC 7443P 24-Core Processor with 48 threads, operating at a maximum frequency of 4035.6440 MHz, and features 128 MiB of L3 cache.\nFor open-source general LVLMs, we included Llava [7]\u00b2, InternVL-Family [8]\u00b3, MiniCPM [23]\u2074, and BLIP2-OPT-6.7B [6]\u2075, all of which are supported by the vLLM engine [24]. For medical LVLMs, LLaVA-Med [10]\u2076 and HuatuoGPT-Vision (7B and 34B) [13]\u2077 were evaluated by using the inference code provided in their GitHub repositories. As for the close-source multimodal large language model, GPT-40, we utilize the inference framework supported by OpenAI \u2078.\nTo ensure diversified output despite the similarity of the prompt prefix and medical image context, we configured the hyperparameters as follows: temperature was set to 0.8, top-p to 0.95, and max-tokens to 1024. All other hyperparameters were kept at their default settings.\nEvaluation metrics. To reduce the time costs and mini- mize subjective biases inherent in manual assessment, this study utilizes commercial large language models as evalua- tors. Previous research has shown that large language models are effective in assessing open-ended questions [25], [26]. Consequently, we implemented Gemini-Pro and GPT-40 as evaluators instead of human assessment.\nThe evaluation criteria:"}, {"title": "V. RESULTS", "content": "RadVUQA-CT. Table II demonstrates the capabilities of dif- ferent LVLMs on CT data. Specifically, Gemini-Flash achieved the highest score in anatomic understanding, with a 3.13 Response Accuracy (RA) score and a 0.68 Multiple-Choice Accuracy (MCA) score. GPT-40 outperformed all comparisons in openly answering vision & language comprehension ques- tions, achieving a leading RA score of 1.79. For close-ended questions, Huatuo-34b outperformed GPT-40 with the MCA score of 0.55. In quantitative reasoning, GPT-40 obtained the top RA score of 2.54 and the best MCA score (0.60). Interestingly, a similar trend is witnessed in spatial reasoning, with GPT-4o achieving the best RA score (3.32) in open-ended QA and Claude-Sonnet's 0.48 MCA score in close-ended QA. As for physiological knowledge, Huatuo-34b led with an RA score of 2.08, outperforming other LVLMs.\nThe average hallucination scores were various for all com- parisons, ranging from Gemini's 0.7% to LLaVA's 26.38% (Table II). To gain a deeper understanding of the hallucination across various clinical aspects, we delve further into the details presented in Fig. 3. In particular, half of the LVLMs have the most intensive hallucination in anatomic understanding (MiniCPM, LLaVA, Huatuo-34b, GPT-40). Huatuo-7b pre- sented the most severe hallucination (86.40%) in quantita- tive reasoning (Fig. 3), while this number was dramatically\nRadVUQA-MRI. The performance of LVLMs on MRI data presents a similar trend as that on CT, with commercial models dominating the overall performance. GPT-40 achieved state- of-the-art performance with the best score in most evaluation aspects. For instance, it achieved the best score in both multi- modal comprehension (2.04 RA and 0.53 MCA) and quanti- tative reasoning (3.00 RA and 0.57 MCA) QAs. Interestingly, owing to its superior anatomic recognition capabilities on MRI images, GPT-4o achieves better performance in physiological knowledge than Huatuo-34b.\nRegarding hallucination rates, Gemini exhibited more hallu- cinations in MRI data (10.07%) compared to CT data (0.7%), whereas GPT-40 demonstrated a significantly lower rate of 0.32%. Notably, Huatuo-34b showed better stability than GPT- 40 and Gemini, with the 2nd least hallucination scores on both CT and MRI data, indicating its more specific training resources.\nRadVUQA-OOD. Table III presents the capabilities of LVLMs on the unharmonised data. In particular, GPT-40 achieved the highest RA scores in open-ended questions under all five scenarios, with 2.92, 2.96, 2.97, 3.04, and 3.02 on\nnoisy, contrast, sharpness, motion blur, and low-resolution data, respectively). Meanwhile, Gemini-Flash obtained the best MCA scores in all close-ended questions (with 0.743, 0.723, 0.738, 0.733, and 0.738, respectively). In addition to GPT-40 and Gemini, InternVL also achieved competitive performance with around 2.83 RA scores. Notably, GPT-40 and BLIP2 were observed with the lowest hallucination scores for the unharmonised data."}, {"title": "VI. DISCUSSION", "content": "Overall Performance. The overall performance of LVLMs remains stable on both CT and MRI datasets, with GPT, Gemini, Huatuo (7b and 34b), and Claude achieving the top-5 performance of all comparisons. Specifically, GPT-40 achieved the highest capabilities on both CT and MRI datasets. Huatuo- 34 also shows reliable performance in OEQ, making it a strong competitor across different modalities. While models like Gemini and Claude show more variability in their rankings, their presence in the top five for both datasets suggests their high effectiveness.\nThe result also suggests that commercial LVLMs continue to outperform their open-source counterparts. Although LLaVA- Med is purportedly better suited for the medical domain, its performance still falls short of generalized open-source LVLMs. Notably, Huatuo-34b demonstrates comparable per- formance, approaching that of GPT-40 on CT data, despite its smaller model scale. This highlights the potential for medical- specific LVLMs to surpass commercial models as they scale up.\nIs LVLM sensitive to CT windowing? The performance of LVLMs on CT data was widely assessed in existing benchmark studies. Unfortunately, none of them considers the windowing settings while barely collecting 2D CT images (those have been preprocessed) or simply applying min-max normaliza- tion. CT windowing is widely used to adjust the contrast and brightness of the images, enhancing the visibility of differ- ent tissues and structures within the body. By adjusting the windowing parameters (window level WL and window width WW), radiologists can better differentiate between normal and abnormal tissues, resulting in better diagnosis/screening efficiency.\nTo gain a deeper understanding of how window settings influence LVLMs, we conducted comparative studies on two CT datasets. The control group was derived from 3D CT scans"}, {"title": "VII. CONCLUSION", "content": "This paper presented a novel benchmark study, Radiological Vision Understanding, Questioning and Answering, for eval- uating large vision language models through in-depth clinical- specific perspectives. Based on our comprehensive assess- ments, we claim that\n\u2022 Large-scale medical LVLMs remain underdeveloped but hold significant potential to outperform commercial mod- els.\n\u2022 Clinical-specific techniques such as CT windowing could improve LVLMs' capabilities and alleviate hallucination issues.\n\u2022 Prompt-CoT is effective in enhancing model capabilities with fewer hallucinations and higher accuracy.\n\u2022 LVLMs are not robust to unharmonised data, even certain models have lower hallucinations on the low-quality data.\n\u2022 LVLMs are struggling to distinguish synthetic or non- human data, thus adversarial attacks could bring serious safety challenges.\nA startling finding is that even though some LVLMs claim to be capable of diagnostics and report generation, they lack a basic understanding of anatomical structures. This reveals that current LVLMs are neither generalizable nor ready for clinical application. Moreover, it highlights a critical gap in their foundational medical knowledge, underscoring the need for the development of more versatile and robust models. We believe our findings offer important insights that will help guide future research in advancing LVLMs for medical use."}]}