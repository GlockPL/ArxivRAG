{"title": "Beyond the Hype: A dispassionate look at vision-language models in medical scenario", "authors": ["Yang Nan", "Huichi Zhou", "Xiaodan Xing", "Guang Yang"], "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across diverse tasks, garnering significant attention in AI communities. However, their performance and reliability in specialized domains such as medicine remain insufficiently assessed. In particular, most assessments over-concentrate in evaluating VLMs based on simple Visual Question Answering (VQA) on multi-modality data, while ignoring the in-depth characteristic of LVLMs. In this study, we introduce RadVUQA, a novel Radiological Visual Understanding and Question Answering benchmark, to comprehensively evaluate existing LVLMs. RadVUQA mainly validates LVLMs across five dimensions: 1) Anatomical understanding, assessing the models' ability to visually identify biological structures; 2) Multimodal comprehension, which involves the capability of interpreting linguistic and visual instructions to produce desired outcomes; 3) Quantitative and spatial reasoning, evaluating the models' spatial awareness and proficiency in combining quantitative analysis with visual and linguistic information; 4) Physiological knowledge, measuring the models' capability to comprehend functions and mechanisms of organs and systems; and 5) Robustness, which assesses the models' capabilities against unharmonised and synthetic data. The results indicate that both generalized LVLMs and medical-specific LVLMs have critical deficiencies with weak multimodal comprehension and quantitative reasoning capabilities. Our findings reveal the large gap between existing LVLMs and clinicians, highlighting the urgent need for more robust and intelligent LVLMs. The code and dataset will be available after the acceptance of this paper.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT advancements in foundation models have demonstrated significant capabilities across diverse tasks. Among various foundational models, Large Vision-Language Models (LVLMs), which integrate visual and linguistic knowledge (e.g., GPT-4, Gemini, and LLaVA), have demonstrated impressive performance in real-world applications. Building upon these achievements, an increasing number of medical LVLMs have been developed, boasting remarkable performance in image captioning, visual grounding, and visual question-answering (VQA). Despite the broad applicability of LVLMs, their performance and reliability in the medical domain remain insufficiently assessed. This gap in evaluation is particularly concerning given the high stakes and complexity inherent in medical applications.\nTo bridge these gaps, this study built a comprehensive dataset, RadVUQA, to evaluate LVLMs in various aspects. RadVUQA incorporates two modalities, including multi-anatomical Computed Tomography (CT) and Magnetic Res-onance Imaging (MR) datasets. Unlike existing studies that primarily focus on recognizing low-level characteristics such"}, {"title": "II. RELATED WORKS", "content": "Vision-language models have garnered significant attention in recent years due to their ability to understand and inte-grate vision and language instructions. By jointly training on a vast dataset of images paired with corresponding textual descriptions, CLIP [4] learns to map images and text into a shared embedding space. Different from this straightforward learning protocol, Flamingo [5] and BLIP2 [6] utilized frozen image encoders to obtain visual representations and integrated them with textual instructions to form the input. In particular, Flamingo achieved this by utilizing cross-attention layers to integrate visual and linguistic features. In contrast, BLIP2 introduced a Q-former to bridge the frozen image encoder with the frozen language model, ensuring that the most relevant visual features are transferred to the LLM decoder to produce the desired text. Additionally, Liu et al. [7] proposed an auto-matic scheme to construct 158K language-image instruction-following data with the LVLM LLaVa. To further improve the quality of visual representations, Chen et al. scaled up the vision encoder to 6 billion parameters, integrating it into the InternVL [8], aiming to align the representation of the enhanced vision encoder with the LLM.\nInspired by these achievements, attention has been shifted to the clinical domain. Building on OpenFlamingo-9B, Med-Flamingo [9] pre-trained using paired and interleaved medical image-text data collected from publications and online text-books. Similarly, LLaVA-Med [10] constructed a biomedical VQA dataset by sampling image-text pairs from PMC-15M [11] and utilized GPT4 to produce multi-round questions and answers. In addition to 2D image inputs, RadFM [12] introduced an architecture that supports visually conditioned generative pre-training. It integrated text input with 2D or 3D medical scans, enabling the generation of responses for diverse radiologic tasks. To alleviate the inherent data noise, Chen et al. [13] refined medical image-text pairs from PubMed and constructed PubMedVision. With this dataset, HuatuoGPT-Vision [13] was proposed by incorporating Qwen as vision encoder [14] and Yi [15] as the LLM part.\nDespite growing attention, there remains a lack of com-prehensive evaluation of generalised LVLMs in the medical domain, especially for commercial ones (e.g., GPT4o, Gemini, etc.). To address this gap, this study conducts a thorough evaluation of highly competitive LVLMs."}, {"title": "III. METHODS", "content": "This section introduces the details of data collection and question design of RadVUQA. Data collection demonstrates the data resources and properties, while question design refers to how we build different subsets for specific assessments.\n1) Data Resources: RadVUQA was developed using multi-source, multi-anatomical public datasets, resulting in the sub-sets RadVUQA-CT, RadVUQA-MRI, and RadVUQA-OOD. Specifically, 2D CT(MR) images were sampled to ensure a diverse representation of body parts, encompassing 117(56) classes such as the spleen, heart, and kidney for RadVUQA-CT (RadVUQA-MRI). The dataset also varied in terms of prompts, including purely visual prompts, visual prompts with text descriptions, and prompts incorporating both text and spatial instructions. Additionally, the question types were cat-egorized into open-ended and close-ended formats to provide a comprehensive evaluation framework.\nRadVUQA-CT was collected from TotalSegmentator, one of the largest publicly available full-body CT datasets [18]. The dataset comprises 1,204 3D CT scans, with annotations for 117 distinct anatomical human structures. Scans with an axial plane width or height smaller than 200 pixels were excluded. From each scan, we extracted fifteen 2D slices from the transverse, sagittal, and coronal planes, specifically at 25%, 40%, 55%, 70%, and 85% of the total number of layers along each axis. This extraction process resulted in a total of 11,448 2D CT images, each accompanied by corresponding mask labels."}, {"title": "B. Question Design", "content": "Unlike most VQA datasets that were typically organized based on classification data, RadVUQA was developed using pub-licly available segmentation datasets (Fig. 1). This ensures that the QA pairs in RadVUQA can be more accurately generated and have richer semantic meaning than existing counterparts.\n1) RadVUQA-CT and RadVUQA-MRI: Each question in RadVUQA comprised a context prompt and a specific query. Initially, we set up a basic or an advanced prompt for each context prompt of the given image. The basic prompt briefly introduces the context of the input data, while the advanced prompt gives more details by telling LVLMs about the existing structures. It is of note that in all the QA examples, the content within the '{}' represents the semantic labels specific to each image.\nFollowing the context prompt (basic or advanced), the specific query assesses the capabilities of LVLMs by introduc-ing open-ended questions (OEQs) and close-ended questions (CEQs) across the following aspects.\nIt could be found that OEQ1.1 has less prior knowledge than OEQ1.2, which was designed to test the effectiveness of Prompt CoT for different LVLMs.\nMulti-modality comprehension explores the capabilities of interpreting linguistic and visual instructions. We achieved this by providing prior knowledge of the RoI (Region of Interest). In particular, we labelled the Region of Interest (RoI) with a green bounding box in the input image and asked LVLMs to recognize its anatomical structure. This task requires the model to not only understand linguistic instructions but also accurately localize the prompt bounding box (the green one). Additionally, CEQ2 refers to multiple-choice questions, fur-ther testing the model's comprehension ability.\nSubsequently, OEQ3.2 and OEQ3.3 gradually reduce the complexity of the spatial perception assessment, allowing us to evaluate the model's ability to understand and interpret spatial relationships with decreasing difficulty."}, {"title": "IV. EXPERIMENTAL SETTINGS AND METRICS", "content": "Experimental settings. Models were evaluated on two NVIDIA H100 NVL GPUs. The system architecture includes an AMD EPYC 7443P 24-Core Processor with 48 threads, operating at a maximum frequency of 4035.6440 MHz, and features 128 MiB of L3 cache.\nFor open-source general LVLMs, we included Llava [7]2, InternVL-Family [8]\u00b3, MiniCPM [23]4, and BLIP2-OPT-6.7B [6]5, all of which are supported by the vLLM engine [24]. For medical LVLMs, LLaVA-Med [10] and HuatuoGPT-Vision (7B and 34B) [13]7 were evaluated by using the inference code provided in their GitHub repositories. As for the close-source multimodal large language model, GPT-40, we utilize the inference framework supported by OpenAI 8.\nTo ensure diversified output despite the similarity of the prompt prefix and medical image context, we configured the hyperparameters as follows: temperature was set to 0.8, top-p to 0.95, and max-tokens to 1024. All other hyperparameters were kept at their default settings.\nEvaluation metrics. To reduce the time costs and mini-mize subjective biases inherent in manual assessment, this study utilizes commercial large language models as evalua-tors. Previous research has shown that large language models are effective in assessing open-ended questions [25], [26]. Consequently, we implemented Gemini-Pro and GPT-40 as evaluators instead of human assessment.\nThe evaluation criteria:"}, {"title": "V. RESULTS", "content": "RadVUQA-CT. Table II demonstrates the capabilities of dif-ferent LVLMs on CT data. Specifically, Gemini-Flash achieved the highest score in anatomic understanding, with a 3.13 Response Accuracy (RA) score and a 0.68 Multiple-Choice Accuracy (MCA) score. GPT-40 outperformed all comparisons in openly answering vision & language comprehension ques-tions, achieving a leading RA score of 1.79. For close-ended questions, Huatuo-34b outperformed GPT-40 with the MCA score of 0.55. In quantitative reasoning, GPT-40 obtained the top RA score of 2.54 and the best MCA score (0.60). Interestingly, a similar trend is witnessed in spatial reasoning, with GPT-4o achieving the best RA score (3.32) in open-ended QA and Claude-Sonnet's 0.48 MCA score in close-ended QA. As for physiological knowledge, Huatuo-34b led with an RA score of 2.08, outperforming other LVLMs.\nThe average hallucination scores were various for all com-parisons, ranging from Gemini's 0.7% to LLaVA's 26.38% (Table II). To gain a deeper understanding of the hallucination across various clinical aspects, we delve further into the details presented in Fig. 3. In particular, half of the LVLMs have the most intensive hallucination in anatomic understanding (MiniCPM, LLaVA, Huatuo-34b, GPT-40). Huatuo-7b pre-sented the most severe hallucination (86.40%) in quantita-tive reasoning (Fig. 3), while this number was dramatically"}, {"title": "VI. DISCUSSION", "content": "Overall Performance. The overall performance of LVLMs remains stable on both CT and MRI datasets, with GPT, Gemini, Huatuo (7b and 34b), and Claude achieving the top-5 performance of all comparisons. Specifically, GPT-40 achieved the highest capabilities on both CT and MRI datasets. Huatuo-34 also shows reliable performance in OEQ, making it a strong competitor across different modalities. While models like Gemini and Claude show more variability in their rankings, their presence in the top five for both datasets suggests their high effectiveness.\nThe result also suggests that commercial LVLMs continue to outperform their open-source counterparts. Although LLaVA-Med is purportedly better suited for the medical domain, its performance still falls short of generalized open-source LVLMs. Notably, Huatuo-34b demonstrates comparable per-formance, approaching that of GPT-40 on CT data, despite its smaller model scale. This highlights the potential for medical-specific LVLMs to surpass commercial models as they scale up.\n\nRegarding hallucination rates, Gemini exhibited more hallu-cinations in MRI data (10.07%) compared to CT data (0.7%), whereas GPT-40 demonstrated a significantly lower rate of 0.32%. Notably, Huatuo-34b showed better stability than GPT-40 and Gemini, with the 2nd least hallucination scores on both CT and MRI data, indicating its more specific training resources.\nRadVUQA-OOD. Table III presents the capabilities of LVLMs on the unharmonised data. In particular, GPT-40 achieved the highest RA scores in open-ended questions under all five scenarios, with 2.92, 2.96, 2.97, 3.04, and 3.02 on"}, {"title": "VII. CONCLUSION", "content": "This paper presented a novel benchmark study, Radiological Vision Understanding, Questioning and Answering, for eval-uating large vision language models through in-depth clinical-specific perspectives. Based on our comprehensive assess-ments, we claim that\n\nClinical-specific techniques such as CT windowing could improve LVLMs' capabilities and alleviate hallucination issues.\n\nPrompt-CoT is effective in enhancing model capabilities with fewer hallucinations and higher accuracy.\n\nLVLMs are not robust to unharmonised data, even certain models have lower hallucinations on the low-quality data.\n\nLVLMs are struggling to distinguish synthetic or non-human data, thus adversarial attacks could bring serious safety challenges.\nA startling finding is that even though some LVLMs claim to be capable of diagnostics and report generation, they lack a basic understanding of anatomical structures. This reveals that current LVLMs are neither generalizable nor ready for clinical application. Moreover, it highlights a critical gap in their foundational medical knowledge, underscoring the need for the development of more versatile and robust models. We believe our findings offer important insights that will help guide future research in advancing LVLMs for medical use."}]}