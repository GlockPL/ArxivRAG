{"title": "Multi-View Black-Box Physical Attacks on Infrared Pedestrian Detectors Using Adversarial Infrared Grid", "authors": ["Kalibinuer Tiliwalidi", "Chengyin Hu", "Weiwen Shi"], "abstract": "While extensive research exists on physical adversarial attacks within the visible spectrum, studies on such techniques in the infrared spectrum are limited. Infrared object detectors are vital in modern technological applications but are susceptible to adversarial attacks, posing significant security threats. Previous studies using physical perturbations like light bulb arrays and aerogels for white-box attacks, or hot and cold patches for black-box attacks, have proven impractical or limited in multi-view support. To address these issues, we propose the Adversarial Infrared Grid (AdvGrid), which models perturbations in a grid format and uses a genetic algorithm for black-box optimization. These perturbations are cyclically applied to various parts of a pedestrian's clothing to facilitate multi-view black-box physical attacks on infrared pedestrian detectors. Extensive experiments validate AdvGrid's effectiveness, stealthiness, and robustness. The method achieves attack success rates of 80.00% in digital environments and 91.86% in physical environments, outperforming baseline methods. Additionally, the average attack success rate exceeds 50% against mainstream detectors, demonstrating AdvGrid's robustness. Our analyses include ablation studies, transfer attacks, and adversarial defenses, confirming the method's superiority.", "sections": [{"title": "1. Introduction", "content": "Currently, deep neural networks (DNNs) have achieved remarkable success across various fields and have become a core technology in artificial intelligence. They are widely applied with significant effectiveness in areas such as image recognition He et al. (2016), speech recognition Radford et al. (2023), natural language processing Zaremba and Demir (2023), and object detection Farhadi and Redmon (2018). For instance, in image recognition, DNNs can accurately classify and identify various images; in speech recognition, they can convert speech into text, achieving high-precision speech-to-text conversion; in natural language processing, DNNs can understand and generate natural language, enabling functionalities such as automatic translation and question-answering systems; in object detection, DNNs can accurately identify and locate target objects in images and videos.\nRegarding object detection, many research efforts involve training detectors by collecting a large number of samples in visible light environments He et al. (2017); Ren et al. (2015). These detectors, carefully designed and trained, demonstrate strong robustness and high accuracy under standard lighting conditions. However, their performance is significantly affected in low-light or dim conditions. To overcome this limitation, some researchers Liu et al. (2022) have recently shifted their focus to the infrared domain. By collecting infrared samples, researchers can train robust detectors that perform well under various lighting conditions. These infrared detectors not only function normally during the day but also maintain high-efficiency target detection capabilities at night or in low-light environments. The success of this approach not only broadens the application scope of object detection technology but also provides new solutions for addressing illumination variations in practical applications.\nAs DNN-based tasks advance, their security performance has also garnered scholarly attention. Infrared target detectors, combining infrared imaging technology and deep neural networks, are shown to be vulnerable to adversarial attacks Li et al. (2023a); Duan et al. (2021a). In adversarial attacks, digital adversarial attacks Li et al. (2023b) involve adding imperceptible perturbations directly to digital images. In contrast, physical adversarial attacks Zheng et al. (2023); Doan et al. (2022) require adding noticeable perturbations to the target object, then using sensors to capture samples and input them into the target model to execute the attack. Most current physical adversarial attacks Wang et al. (2022, 2021) on target detectors are deployed in the visible light modality, with only a few studies Zhu et al. (2021, 2023) exploring physical attacks in the infrared modality, making the research on infrared physical adversarial attack techniques particularly urgent.\nExisting infrared physical adversarial attack techniques, as illustrated in Figure 1, encompass various approaches utilizing different physical perturbations to execute infrared attacks. These include using light bulb arrays Figure 1(a) Zhu et al. (2021), QR code clothing Figure 1(b) Zhu et al. (2022), aerogels Figure 1(c) Wei et al. (2023b), and electric heating films Figure 1(d) Zhu et al. (2023). These methods primarily target white-box physical attacks on infrared pedestrian detectors, operating under the assumption that the attacker has access to the target model's architecture and parameters. While this assumption holds in theoretical research, it is impractical in real-world scenarios, where attackers typically cannot obtain such detailed internal information. For black-box physical attacks on infrared pedestrian detectors Hu et al. (2024a,b); Wei et al. (2023a), researchers often employ hot and cold pastes as physical perturbations Figures 1(e), (f), (g). These materials are readily available and can be discreetly affixed inside a pedestrian's clothing, making them difficult for human observers to detect. Without infrared sensors, defenders struggle to identify the presence of these physical perturbations. However, these existing black-box attack methods have significant limitations, particularly in achieving multi-view adversarial effects. This means that the attack's effectiveness diminishes or fails entirely when the target is observed or detected from different angles, thus limiting the practical applicability and effectiveness of the attack.\nTo address the limitations of existing infrared physical adversarial attack methods, this study proposes the AdvGrid for multi-view black-box adversarial attacks on infrared pedestrian detectors. This method utilizes cold patches that can be affixed inside clothing as physical perturbations to enhance stealth. By simulating the grid model and optimizing its physical parameters using genetic algorithms Holland (1992), the most adversarial infrared grid parameters are obtained. The Expectation Over Transformation framework Athalye et al. (2018) is then employed for robust enhancement, further improving the adversarial effect. Additionally, Thin Plate Spline Bookstein (1989) is used for fold simulation to enhance the stability of physical perturbations. Finally, in the physical world, the optimized perturbations are cyclically attached to the inside of pedestrian clothing, achieving multi-view black-box attacks on infrared pedestrian detectors.\nTable 1 compares the proposed method with existing infrared physical attack methods. It is evident that besides AdvGrid, no other method simultaneously offers the advantages of black-box adversarial attacks, stealth, and multi-view capability. Additionally, the cost of deploying physical adversarial attacks using our method is inexpensive, with a total cost of less than $10, making it easier to implement. The contributions of our method are summarized as follows:"}, {"title": "2. Related works", "content": "Given that the proposed method targets object detectors, this section will focus on physical attacks against object detectors, detailing techniques in both visible and infrared modalities."}, {"title": "2.1. Physical attacks in the visible light field", "content": "Pedestrian object detection technology plays a key role in enhancing quality of life, urban management, and intelligent development. Its primary task is to accurately detect and locate pedestrians in images or videos.\nXu et al. proposed AdvTshirt Xu et al. (2020), which uses flexible transformation techniques to simulate clothing deformation for robust optimization, ensuring the robustness of physical perturbations on non-rigid objects. Employing minimax optimization, this work extends to integrated attacks against multiple object detectors simultaneously. Experimental results validate the robustness of AdvTshirt across different pedestrian object detectors.\nWu et al. introduced AdvCloak Wu et al. (2020), which trains perturbation patterns using standard datasets to suppress widely used object detectors, thereby enhancing adversarial attack transferability. Experimental results confirm its effectiveness in both white-box and black-box settings, as well as its transferability across datasets, object categories, and detectors.\nTan et al. proposed LAP Tan et al. (2021), aimed at deceiving both human vision and object detectors. It employs a dual-stage training strategy to generate perturbation patterns that appear plausible to humans while fooling object detectors. Experimental results demonstrate that LAP achieves significant adversarial effects and maintains a reasonable visual appearance.\nHu et al. introduced NP Hu et al. (2021), which focuses on the visual plausibility of perturbations to enhance attack stealth. This work utilizes a pretrained generative adversarial network to learn real-world images, sampling optimal images to produce natural adversarial patches under high adversarial conditions. Experimental results indicate superior adversarial effects, with independent subjective surveys showing that NP-generated adversarial samples appear more natural than baseline methods.\nTo address multi-view adversarial attacks, Hu et al. proposed AdvTT Hu et al. (2022), based on a scalable generative framework with circular cropping, creating adversarial textures with repetitive structures. These textures are deployed on all sides of clothing to achieve multi-view adversarial attacks. Experimental results demonstrate that clothing items like t-shirts and skirts with these textures effectively execute multi-view adversarial attacks against pedestrian object detectors.\nVehicle object detectors are crucial in computer vision, tasked with accurately detecting and locating vehicles in images or videos. Applications include traffic monitoring, autonomous driving, and security.\nZhang et al. introduced CAMOU Zhang et al. (2019), which trains a neural approximation function to simulate camouflaged vehicles, searching for optimal camouflage to minimize approximate detection scores. Experimental results show that this camouflage deceives advanced vehicle detectors and generalizes across different vehicles, environments, and detectors.\nHuang et al. proposed UPC Huang et al. (2020), which introduces a set of transformations to simulate deformable properties, making the generated camouflages effective for non-rigid or non-planar objects. Experimental results indicate advantages in both virtual and physical environments.\nWang et al. introduced DAS Wang et al. (2021), which simultaneously suppresses model and human attention, generating highly transferable and visually natural physical adversarial camouflages. Extensive experiments in digital and physical worlds against state-of-the-art models demonstrate superior performance over existing methods.\nWang et al. proposed FCA Wang et al. (2022), rendering non-planar camouflage textures over the entire vehicle surface, and introducing a transformation function to convert optimized vehicles into more realistic scenes. Experimental results show that full-coverage camouflage attacks outperform existing methods across various test cases and generalize to different environments, vehicles, and detectors.\nDuan et al. introduced CAC Duan et al. (2021b), which transforms camouflage based on object posture in 3D space, fixes the top n proposals of the region proposal network, and attacks all classifications within fixed dense proposals to output errors. Experiments demonstrate that CAC outperforms existing attack algorithms, showing superior performance in both virtual and real-world scenarios."}, {"title": "2.2. Physical attacks in the infrared field", "content": "Zhu et al. pioneered physical adversarial attacks in the infrared modality with BulbAttack Zhu et al. (2021), using a set of light bulbs as heat sources to create white perturbations under infrared imaging. This optimization deceives well-trained infrared detectors, as demonstrated by an individual holding the optimized bulb board to evade detection.\nSubsequently, Zhu et al. proposed QRattack Zhu et al. (2022), which uses aerogel as insulation to create black perturbations in thermal imaging. Optimized adversarial QR codes were manually simulated, and individuals wearing these QR code vests could avoid infrared detection.\nWei et al. advanced the field with HCB Wei et al. (2023a), utilizing heat and ice patches as perturbations optimized with evolutionary algorithms for black-box attacks on infrared detectors. These patches, hidden inside clothing, made it difficult for humans to detect.\nWei et al. also introduced AIP Wei et al. (2023b), using aerogel patches optimized through a polynomial regularization method. Experimentally validated, these patches adhered to the fronts of clothing and effectively evaded infrared detectors.\nZhu et al. developed AdvCloth Zhu et al. (2023), employing flexible carbon fiber heaters as perturbations, constrained by L_dist loss functions for deployment feasibility in the real world. Experimental results showed significant improvements in both digital and physical adversarial effects.\nHu et al. proposed AdvIC Hu et al. (2024b), using Bezier curves to model infrared perturbations, optimized with particle swarm algorithms. Effective real-world adversarial attacks were demonstrated using only two Bezier curves.\nHu et al. further developed AdvIB Hu et al. (2024a), modeling perturbations as discrete patches with hot and cold stickers, optimized with differential evolution algorithms. Extensive experiments validated the multi-distance and transfer attack effectiveness in the physical world."}, {"title": "3. Methodology", "content": null}, {"title": "3.1. Problem definition", "content": "Object detector: Consider an infrared pedestrian imaging dataset DS, where X denotes the clean infrared images and Y represents the corresponding correct labels for pedestrians. Let f denote the object detector. For each input image \\(X \\in DS\\) in the dataset, the pretrained model \\(f : X \\rightarrow Y\\) of the object detector analyzes the image and predicts a label y that aligns with the correct label Y. The predicted label y includes the following key components: the location information of the bounding box (\\(Y_{pos}\\)), the confidence or probability of the target (\\(Y_{obj}\\)), and the category information of the target (\\(y_{cls}\\)). Utilizing this information, the object detector can accurately identify and locate pedestrians, ensuring stable and reliable detection performance across various environments:\n\\[\ny:= [Y_{pos}, Y_{obj}, Y_{cls}] = f(X)\n\\]\nEOT Framework: The Expectation Over Transformation (EOT) framework Athalye et al. (2018) presents a systematic approach to generate resilient adversarial samples, with the aim of bolstering the efficacy of adversarial attacks across diverse environments and conditions. Originally introduced by Athalye et al. in 2018, this framework has found extensive application in the realm of adversarial attack research within physical contexts. Leveraging a series of stochastic transformations applied to input samples, the EOT framework ensures the resilience of generated adversarial samples across varying conditions. Central to its methodology is the optimization of adversarial samples to effectively subvert target models amidst multiple transformational scenarios, encompassing translations, rotations, scalings, and alterations in lighting conditions. While conventional adversarial attack methods typically operate under the assumption of static and unchanging input data, real-world scenarios often entail dynamic transformations. EOT, by accounting for these transformations, engenders adversarial samples of heightened robustness.\nTPS Framework: The Thin Plate Spline (TPS) framework Bookstein (1989) constitutes a widely employed deformation model prevalent in image processing, computer vision, and machine learning domains. Originally formulated by Fred L. Bookstein in 1989, it primarily addresses non-rigid deformations of images and shapes. The TPS model employs a smooth and differentiable function to effectuate the transformation from a source image or shape to a target counterpart. Its procedural steps entail the selection of control points, adoption of the TPS deformation model, solution of linear equations to ascertain deformation function parameters, pixel-wise mapping from the source to the target image, and eventual application of the deformation function to yield the deformed image."}, {"title": "3.2. Adversarial attack framework", "content": "Figure 2 illustrates the adversarial attack framework of this method, comprising seven key components: Simulation and Modeling: This initial phase involves modeling the infrared grid to ascertain its physical parameters, encompassing factors like location, dimensions, and width. These parameters serve as the foundation for subsequent optimization steps. Binary Encoding and Population Initialization: The physical parameters of the infrared grid are encoded into binary form, and an initial population is initialized. This step ensures diversity and a rational distribution within the population. Conversion to Digital Perturbations: Binary encoded parameters are transformed into digital perturbations, which are then combined with clean samples to generate digital representations simulating real-world physical perturbations. Evaluation and EOT Robustness Enhancement: The digital samples are input into DNNs for evaluation. Subsequently, EOT robustness enhancement techniques are applied to augment the stability and efficacy of the adversarial perturbations. Genetic Algorithm Optimization: A suitable fitness function is selected, guiding the selection of individuals within the population. Crossover and mutation operations are performed iteratively on the population using genetic algorithms, gradually refining parameters towards optimal solutions. Perturbation Splicing and TPS Robustness Enhancement: The most adversarial perturbations are selected, followed by random cropping of perturbation blocks. TPS robustness enhancement techniques are applied to enhance stability and resistance to deformation. Enhanced perturbations are then integrated into pedestrian images from various viewpoints, ensuring effective attacks from multiple perspectives. Physical Deployment: In the physical realm, optimized perturbation patterns are embedded within pedestrian clothing. This facilitates multi-view adversarial effects, rendering real pedestrians challenging to detect by infrared pedestrian detectors in practical scenarios, thereby achieving covert attack objectives."}, {"title": "3.3. Generate adversarial sample", "content": "In this work, infrared perturbations are simulated and modeled using a grid approach for multi-view optimization and deployment. As shown in Figure 3, each individual infrared grid block consists of four physical parameters: location, color, width, and dimension. The detailed definitions of each parameter are as follows:\nLocation (\\(P(i,j)\\)): In this study, \\(P(i, j)\\) represents the top-left coordinate of each independent grid, determining the specific location of the grid on the pedestrian's body. Each independent grid within the grid block is continuously simulated, meaning the grids are connected. Disconnected appearances in Figure 3 are due to transparent grid settings (i.e., no perturbation at that position). Thus, the position parameters are formalized as \\(P = (i_{11}, j_{11}), (i_{12}, j_{12}), ..., (i_{DD}, j_{DD})\\), where (\\(i_{11}, j_{11}\\)) represents the position of each independent subgrid in the simulated infrared grid block.\nColor (C): Parameter C represents the color of the infrared grid block. Since objects under infrared cameras typically appear in black, white, and gray, the color value of each subgrid perturbation in this study is fixed to either no color or black (C(0,0,0)). When the subgrid color is set to no color, the perturbation at that position becomes discontinuous, increasing the diversity of perturbations and enhancing the likelihood of successful adversarial attacks.\nWidth (W): Parameter W represents the width of the infrared grid. If the grid width is too large, it will exceed the pedestrian's boundary; if the width is too small, the subgrids will be too dense, which is not conducive to physical deployment. After repeated testing, setting the width to one-fifth of the height of the pedestrian's bounding box is found to be a reasonable configuration. This setting is advantageous because in digital images where only the left or right half of the pedestrian's body is captured, setting the width to two-thirds of the bounding box width results in a smaller perturbation area, and the generated perturbations do not achieve the expected effect.\nDimension (D): Parameter D represents the dimension of the infrared grid. After fixing the grid width, if the dimension is set too large, the grids will be too dense, making it difficult to deploy in the physical world. Conversely, if the dimension is set too small, it reduces the diversity of perturbations and weakens the effectiveness of digital adversarial attacks. A small dimension also causes large wrinkles during multi-view deployment, weakening the physical attack effect. In the experiment, the dimension is set to \\(D = 8\\) to ensure that the perturbations are diverse and can be effectively deployed in the physical world.\nWe use (P, C, W, D) (abbreviated as \\(\\theta\\)) to represent an infrared grid. To ensure that the perturbations appear in the pedestrian area, a mask M is used to limit the position area of the perturbations. Therefore, the process of generating adversarial samples can be represented as follows:\n\\[\nX_{adv} = S(X,\\theta, M)\n\\]\nIn this context, \\(X_{adv}\\) represents the adversarial sample, and S is a linear fusion function that combines the clean sample with the simulated infrared grid to create the adversarial sample.\nAfter generating the adversarial samples, it is important to address the challenges associated with transferring these samples from a static and fixed digital environment to the physical domain. This transfer can introduce issues such as position errors, lighting changes, and noise impacts. To mitigate these issues, the EOT framework Athalye et al. (2018) is employed to simulate various transformation conditions, thereby generating more robust and practical adversarial samples. The EOT framework uses a distribution of transformations T to model domain transfer, where each instance represents a combination of random image transformations, including perspective transformation, brightness adjustment, downsampling, and more. Consequently, the adversarial samples enhanced by the EOT framework can be expressed as follows:\n\\[\nX_{adv} = E_{t \\sim T}t(X_{adv})\n\\]\nFollowing robustness enhancement with the EOT framework, it is necessary to consider the deployment of perturbations inside pedestrian clothing. Given that clothing is a non-rigid object prone to deformation, the TPS architecture Bookstein (1989) is further employed to map the perturbations from the digital domain to the physical domain. This ensures that the enhanced adversarial samples can better adapt to changes in the real environment. Specifically, the enhanced adversarial samples can be represented as:\n\\[\nX_{adv} = TPS(E_{t \\sim T}t(X_{adv}))\n\\]"}, {"title": "3.4. Infrared grid adversarial attack", "content": "The proposed method aims to optimize the physical parameters of the infrared grid using a genetic algorithm to generate the most adversarial infrared grid, which is then fused with clean samples to obtain the most adversarial digital samples. This method targets more realistic scenarios for black-box attacks, where the attacker cannot access the internal information of the model such as its architecture and parameters, and can only obtain the model's output, such as the detected object's category and confidence. Therefore, the method takes the confidence of the target object as the adversarial loss and formalizes the objective function to minimize the confidence of the infrared detector for the target object:\n\\[\n\\underset{\\theta}{\\text{arg min}} TPS(E_{t \\sim T}(Y_{obj} \\leftarrow f(t(X_{adv}))))\n\\]\nThis method implements the global optimization shown in Equation 5 using a genetic algorithm to obtain the final optimal solution, which are the physical parameters of the most adversarial infrared grid. These physical perturbations are then deployed in the real world to deceive infrared target detectors. The process of optimizing the infrared grid using a genetic algorithm Holland (1992) is introduced below.\nRandom Initialization: During the optimization process of AdvGrid, a randomly initialized population of candidate solutions (POP) is first generated. Each solution in the population corresponds to a set of physical parameters \\(\\theta\\) that define the infrared grid used to generate adversarial samples. Random initialization helps ensure the diversity of the search space, increasing the likelihood of finding the optimal solution. This process is represented as:\n\\[\nPOP = [\\theta_1, \\theta_2, ..., \\theta_G]\n\\]\nwhere G represents the population size and \\(\\theta_g\\) represents a candidate solution in the population (g = 1, 2, ..., G).\nFitness Calculation: After initializing the population, each individual in the population generates adversarial samples. The confidence score of the model's output for the correct label on these adversarial samples is used as the fitness value for each individual. This way, the effectiveness of each individual in attacking the target model can be evaluated - the higher the fitness value, the more successful the adversarial sample is:\n\\[\nfit_g = 1 - (Y_{obj} \\leftarrow f(S(X,\\theta_g, M))) \\quad g = 1, 2, ..., G\n\\]\nwhere, \\(fit_g\\) represents the fitness value of the g - th individual.\nSelection Operation: Based on the confidence values of each individual in the population, a selection operation is performed. This work adopts a novel selection operation where individuals with confidence values greater than 80% (i.e., fitness less than 20%) are eliminated first, as these individuals perform the worst in the attack. New individuals are then randomly generated to replenish the population, maintaining diversity and population size. The selection operation is denoted by SL, and the new population after selection can be represented as:\n\\[\nPOP_{s+1} \\leftarrow SL(POP_s, fit_g) \\quad s = 1, 2, ..., S \\quad g = 1, 2, ..., G\n\\]\nwhere S represents the total number of generations, and \\(POP_s\\) represents the population in the s - th generation.\nCrossover Operation: First, genes in the population are randomly paired. Then, for each pair of genes, a random gene position is selected, and with a probability \\(P_c\\), the selected gene and the genes following it are exchanged, achieving the crossover operation. The crossover operation is denoted by CS, and the new gene pairs after crossover can be represented as:\n\\[\nPOP_{s+1} \\leftarrow CS(POP_s, P_c) \\quad s = 1, 2, ..., S\n\\]\nMutation Operation: After crossover, the population undergoes mutation. For each individual in the population, a gene is randomly selected, and with a probability \\(P_m\\), this gene is flipped, i.e., 0 is mutated to 1 and 1 is mutated to 0. The mutation operation is denoted by MT, and the mutated population can be represented as:\n\\[\nPOP_{s+1} \\leftarrow MT(POP_s, P_m) \\quad s = 1, 2, ..., S\n\\]\nAdvGrid, through S iterations of the genetic algorithm, obtains the physical parameters of the most adversarial infrared grid to generate adversarial samples that can deceive the infrared target detector. These samples are then robustly enhanced using the EOT framework and TPS. Algorithm 1 shows the pseudocode of the proposed AdvGrid, with clean samples X, target detector f, population size G, number of iterations S, crossover rate \\(P_c\\), and mutation rate \\(P_m\\) as inputs. The algorithm randomly initializes the population and performs fitness calculation, crossover, and mutation operations for each individual, repeating this loop continuously. The algorithm ultimately outputs the physical parameters \\(\\theta^*\\) for subsequent physical attack experiments."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Experimental setting", "content": "Dataset: In line with AdvIB Hu et al. (2024a), we use the FLIR dataset FLIR for training infrared pedestrian detectors and testing digital attacks. The FLIR dataset comprises a total of 10,228 infrared images, captured using a FLIR Tau2 thermal camera. A notable feature of this dataset is the meticulous manual annotation of all thermal images, accurately distinguishing four categories of target objects: people, bicycles, cars, and dogs. For model training, we implement a strict filtering process, retaining only pedestrian images with heights exceeding 120 pixels. This refinement yielded 1,011 highly relevant samples for training infrared pedestrian detectors. Subsequent adversarial attack experiments are conducted using the test set of this dataset, providing a basis for comprehensive digital attack simulations.\nObject Detectors: For the object detection task, this study adopts a series of advanced detection models previously utilized by AdvIB, including Yolo v3 Farhadi and Redmon (2018), DETR Carion et al. (2020), Mask R-CNN He et al. (2017), Faster R-CNN Ren et al. (2015), Libra R-CNN Pang et al. (2019), and RetinaNet Lin et al. (2017). These models undergo meticulous training on the specially curated dataset to optimize their target recognition capabilities in complex scenarios. After rigorous training processes, the models demonstrated excellent performance on their respective test sets, with the following average precision scores: Yolo v3 achieves 90.7%, DETR reaches 91.2%, Mask R-CNN records 89.5%, Faster R-CNN achieves 90.8%, Libra R-CNN obtaines 88.0%, and RetinaNet leads with a high precision of 93.0%. These results not only validate the robustness of the selected models but also lay a solid foundation for subsequent adversarial attack research and defense strategy development. In subsequent attack experiments, unless otherwise specified, all attacks are conducted against Yolo v3.\nExperimental Devices: During the physical attack experiments, our devices, as shown in Figure 4 (a), consist of a stable tripod, a high-performance infrared camera, and cold patches. The infrared camera used is the HIKMICRO FQ25, which boasts excellent imaging performance with the following specifications: a focal plane array (FPA) of 640\u00d7512 pixels and a noise equivalent temperature difference (NETD) of less than 25 mK, ensuring high detection sensitivity and image clarity. To verify the general applicability of the method, we test the AdvGrid technique on various infrared camera models. The results clearly indicated that the effectiveness of the technique is not dependent on the specific camera model, demonstrating good compatibility and versatility. The cold patch, capable of maintaining a stable temperature at 24\u00b0C for up to 10 hours, provides reliable temperature control for long-duration experiments. Figures 4 (b), (c), and (d) show the visible light images of the external and internal views of the infrared stealth clothing generated by our method, as well as the infrared image. It is evident that without the aid of a infrared camera, the perturbations are imperceptible to the naked eye. Figures 4 (e) and (f) respectively display the visible light and infrared images of a person wearing the stealth clothing.\nEvaluation Metrics: The goal of the proposed AdvGrid is to achieve evasion attacks. Therefore, the attack success rate is used to evaluate the effectiveness of the proposed method, the higher the attack success rate, the more adversarial the method, and vice versa. The formula for calculating the attack success rate is as follows:\n\\[\nASR(X) = 1 - \\frac{1}{N} \\sum_{n=1}^{N} F(Y_{obj})\n\\]\n\\[\nF(Y_{obj}) = \\begin{cases}\n0 & Y_{obj} < 0.5\\\\\n1 & \\text{otherwise}\n\\end{cases}\n\\]\nwhere N denotes the number of true positive labels in the dataset that the detector can identify in the absence of any attacks. Throughout all our attack experiments, a threshold of 0.5 is maintained. Specifically, an evasion attack is deemed successful if the confidence level of the detected target drops below this threshold.\nBaseline Methods: Since AdvGrid is a black-box attack, this method uses existing black-box attacks as baseline methods for comparison, including HCB, AdvIB, and AdvIC.\nOther Settings: In this method, the dimension of the infrared grid is set to D = 8, and the width is set to one-fifth of the height of the infrared pedestrian target detection box. The genetic algorithm parameters are set as follows: G = 50 (population size), S = 10 (number of iterations), \\(P_c\\) = 0.6 (crossover rate), and \\(P_m\\) = 0.1 (mutation rate). All adversarial attack experiments are run on an NVIDIA GeForce RTX 4090 GPU."}, {"title": "4.2. Evaluation of effectiveness", "content": "Digital Testing: To evaluate the effectiveness of our digital attacks, we carefully select specific data from the FLIR dataset to form our test set and implement our attack strategy based on this selection. Using a meticulously designed infrared grid attack configuration (with grid dimensions of 8 and a width equal to one-fifth of the detection box height), we achieve an 80% attack success rate with an average query count of only 117.53. These experimental results robustly demonstrate the effectiveness and efficiency of our proposed method in a digital environment. Figure 5 vividly illustrates the adversarial samples generated during this attack process. As shown, the infrared grid perturbations are seamlessly integrated into the pedestrian images, maintaining a degree of visual stealthiness while effectively evading infrared pedestrian detection systems. This underscores the potential and advantages of our method in practical applications.\nPhysical Testing: In the physical testing phase, we use carefully crafted infrared stealth clothing to conduct the attack tests. The testing process involved pedestrians wearing the stealth clothing and using an infrared camera to capture images from various angles and distances, ranging from 6 meters to 11 meters in 1-meter intervals, and covering a 360-degree field of view with 30-degree intervals at each stop. We meticulously analyze the attack success rates for each combination of distance and angle, compiling all data into Table 2. Key findings from this data include: 1) Overall Excellent Performance: Across all physical tests, we achieve an average attack success rate of 91.86%, strongly validating the applicability and effectiveness of our proposed method in real-world scenarios. 2) Multi-Angle Effectiveness: Regardless of distance or angle, our method consistently produces significant adversarial effects. Even under the most challenging conditions, we achieve at least a 49.26% attack success rate, demonstrating robustness in various settings. 3) View Angle Adaptability: Remarkably, nearly half (34 out of 72) of our test cases achieve a perfect 100% attack success rate, highlighting the method's powerful capability to handle multi-angle challenges and validating its reliability in multi-view adversarial attacks. To visually present these test results, Figure 6 showcases a collection of physical samples generated by our method, encompassing adversarial examples from various distances and angles. As illustrated, our method successfully produces effective adversarial samples across all viewpoints, further confirming its excellent performance in physical environments."}, {"title": "4.3. Evaluation of stealthiness", "content": "Observing subfigures (e) and (f) of Figure 4, we notice that, compared to methods such as BulbAttack, QRattack, and AIP, which place perturbations conspicuously on the exterior of the pedestrian's clothing, our method integrates perturbations subtly within the clothing. This design strategy ensures that, without specialized equipment like thermal imaging cameras, ordinary observers cannot easily detect the presence of perturbations, significantly enhancing the stealthiness of adversarial samples in real-world settings. Figure 7 further displays the physical adversarial samples generated by baseline methods. Notably, the physical samples generated by AdvGrid exhibit more dispersed perturbation effects, which not only enhance the samples' stealthiness but also give AdvGrid a distinct advantage in physical environments. In stark contrast, the physical samples from baseline methods often appear less covert due to overly concentrated or exposed perturbations."}, {"title": "4.4. Evaluation of robustness", "content": "Deploying AdvGrid to attack various infrared pedestrian target detectors: We apply AdvGrid to a series of advanced infrared object detectors, including DETR, Mask R-CNN, Faster R-CNN, Libra R-CNN, and RetinaNet. These detectors, after careful tuning, achieve average precision rates of 91.2%, 89.5%, 90.8%, 88.0%, and 93.0%, respectively, on a curated infrared dataset, demonstrating their outstanding performance in infrared pedestrian detection tasks. After deploying AdvGrid for the attack, we record the relevant experimental results in Table 3 and summarize them as follows: 1) Average attack success rate: AdvGrid achieves an average attack success rate of 59.66% across all tested detectors. This result not only validates AdvGrid's generalization ability against different infrared pedestrian target detectors but also demonstrates its potential as an effective adversarial strategy. 2) Efficiency analysis: In all conducted attack experiments, AdvGrid requires fewer than 500 queries to complete the attacks. This indicates that AdvGrid not only effectively launches attacks on detectors but also performs quite efficiently in terms of computational resource consumption, making it an ideal choice for deployment in practical scenarios. 3) Attack effects on DETR: It is worth noting that AdvGrid's effectiveness is slightly inferior when attacking DETR detectors based on Transformer architecture compared to other models. This finding suggests that Transformer-based detectors may exhibit stronger robustness and defense capabilities against specific types of adversarial attacks. In summary, AdvGrid demonstrates significant generalization performance and efficiency in adversarial attacks against infrared pedestrian target detectors, especially when targeting non-Transformer architecture detectors. However, for Transformer-based detectors like DETR, its attack effectiveness is slightly lower, indicating directions for future research to improve the universality and effectiveness of adversarial strategies.\nComparison of AdvGrid with baseline methods' experimental results: We conduct a comprehensive comparison between the proposed AdvGrid and baseline methods, as shown in Table 4. It can be observed that while the proposed method ranks second to AdvIC in digital attack tests, it outperforms all baseline methods in physical attack tests. Additionally, although AdvGrid's digital attack performance is not as good as AdvIC's, AdvIC lacks multi-view adversarial effects."}, {"title": "5. Discussion", "content": null}, {"title": "5.1. Ablation study", "content": "In this section, we conduct ablation experiments on the physical parameters involved in AdvGrid to study the impact of different physical parameters on the adversarial effectiveness of AdvGrid. These experiments include color ablation, dimension ablation, and width ablation, which will be detailed below.\nAblation of color: In infrared physical attacks, both the image and perturbation are grayscale. Therefore, this ablation experiment sets colors to various grayscale values: (0,0,0), (51,51,51), (102,102,102), (153,153,153), (204,204,204), and (255,255,255), to investigate their effects on AdvGrid's adversarial effectiveness. The experimental results summarized in Table 5 indicate that AdvGrid achieves its best adversarial effectiveness when the color is (0,0,0) (black), and its effectiveness is weakest when the color is (153,153,153). This can be explained by the fact that when the color is (0,0,0), the perturbation color is most distinct from the color of pedestrians in clean samples, resulting in the best adversarial effectiveness. Conversely, when the color is (153,153,153), the perturbation color is closer to the color of pedestrians in clean samples, resulting in weaker effectiveness.\nAblation of dimension: This experiment explores the impact of the infrared grid dimension on the adversarial effectiveness of AdvGrid. The dimension ranges from 2 to 14, increasing in increments of 2. Smaller dimensions lead to fewer grid cells, reduced perturbation within the grid, and larger blank areas, while larger dimensions have the opposite effect. The experimental results, summarized in Table 6, indicate that AdvGrid's attack success rate initially rises with increasing dimensions, peaking at a dimension of 4, after which it declines with further increases. In practical physical experiments, we opt for a dimension of 8 instead of 4, as a dimension of 4 results in larger blank areas within the grid, hindering effective side-view adversarial attacks due to significant perturbation deformation. Thus, a dimension of 8 is considered the optimal configuration for physical attacks in real-world scenarios.\nAblation of width: This experiment studies the effect of the overall width of the infrared grid on AdvGrid's adversarial effectiveness. The width is set to 1/3, 1/4, 1/5, 1/6, 1/7, and 1/8 of the height of the pedestrian detection box. A wider grid results in more perturbation and poorer stealthiness, and vice versa. The experimental results summarized in Table 7 show that AdvGrid's attack success rate increases with the width of the grid. In this experiment, selecting a grid width of 1/5 of the height of the pedestrian detection box is a reasonable choice. When the width continues to increase, the perturbation will exceed the pedestrian target area."}, {"title": "5.2. Deploying AdvGrid to attack infrared vehicle detectors", "content": "In addition to attacking infrared pedestrian detection systems with AdvGrid, we conduct experiments to validate its applicability for attacking infrared vehicle detection systems, further verifying the generalization performance of the proposed method. We fine-tune infrared vehicle detection detectors, including Yolo v3, DETR, Mask R-CNN, Faster R-CNN, Libra R-CNN, and RetinaNet, on the FLIR dataset, achieving average precision rates of 92.1%, 94.6%, 94.2%, 94.4%, 95.6%, and 95.5%, respectively, on the test set. We summarize the experimental results of attacking these vehicle detection detectors with AdvGrid in Table 8, from which the following conclusions can be drawn: 1) Overall, the experiment achieves an average attack success rate of 50.78% with an average query count of 260.91, confirming the method's generalization effectiveness across different tasks. 2) Similar to the results in Table 3, AdvGrid exhibits the weakest adversarial effectiveness when attacking DETR, once again confirming the robustness of Transformer-based object detectors. Figure 8 displays adversarial samples generated from this experiment, demonstrating that vehicles after perturbation cannot be recognized by the target detectors. This experiment provides further evidence of AdvGrid's versatility and effectiveness in attacking various types of object detection systems, contributing to a comprehensive understanding of its adversarial capabilities."}, {"title": "5.3. Optimizing AdvGrid with different optimization algorithms", "content": "In this section, we compare the effects of different adversarial optimization algorithms on AdvGrid by optimizing AdvGrid with these algorithms. The optimization algorithms used here include random optimization, genetic algorithm optimization Holland (1992), particle swarm optimization Kennedy and Eberhart (1995), and differential evolution algorithm Storn and Price (1997) optimization. We summarize the experimental results in Table 9, from which it can be observed that among all optimization algorithms, random optimization yields the weakest optimization effect, achieving only a 58.56% attack success rate. On the other hand, genetic algorithm demonstrated the most outstanding optimization effect in this experiment, achieving an 80.00% attack success rate."}, {"title": "5.4. Attack transferability of AdvGrid", "content": "To evaluate AdvGrid's transfer attack capability, we use adversarial samples generated by AdvGrid that successfully attacked Yolo v3 as a dataset and conduct transfer attacks against DETR, Mask R-CNN, Faster R-CNN, Libra R-CNN, and RetinaNet. These transfer attacks achieve success rates of 27.03%, 27.03%, 35.14%, 27.03%, and 18.92%, respectively. These results indicate that AdvGrid's transfer attacks are effective against most detectors in the digital environment, albeit relatively weaker against RetinaNet.\nIn physical transfer attack experiments, we utilize physical adversarial samples generated by AdvGrid that successfully attacked Yolo v3 as a dataset and conduct transfer attacks against DETR, Mask R-CNN, Faster R-CNN, Libra R-CNN, and RetinaNet. We summarize these experimental results in Tables 10, 11, 12, 13, 14, and draw the following conclusions: 1) Overall, AdvGrid achieves effective physical transfer attacks from Yolo v3 to other advanced infrared pedestrian detection detectors (except for RetinaNet). 2) Yolo v3 to DETR: While 27 out of 72 cases resulted in a 0% attack success rate, the best attack effectiveness is observed at a distance of 6 meters. 3) Yolo v3 to Mask R-CNN: Achieves a high success rate, with optimal effectiveness observed at a distance of 6 meters, with very few cases of attack failure. 4) Yolo v3 to Faster R-CNN: Optimal attack effectiveness is observed at shorter distances, weakening as the distance increased. 5) Yolo v3 to Libra R-CNN: Overall, exhibits good attack effectiveness, with better performance observed at closer distances. 6)Yolo v3 to RetinaNet: In most cases, effective attacks could not be achieved, with some effectiveness observed only at closer distances, and attacks almost entirely ineffective at longer distances. In summary, AdvGrid demonstrates its transfer attack capability in both digital and physical environments, especially against DETR, Mask R-CNN, Faster R-CNN, and Libra R-CNN, where effective transfer attacks can be achieved under specific conditions. However, for RetinaNet, its transfer attack effectiveness is unsatisfactory in both digital and physical environments, particularly ineffective at longer distances in the physical environment. These findings highlight AdvGrid's capability for transfer attacks across different detectors while also revealing its limitations against certain detectors (such as RetinaNet) and in physical environments. These insights are crucial for future improvements in enhancing AdvGrid's universality and optimizing its performance in complex environments."}, {"title": "5.5. Defense of AdvGrid", "content": "In this section, we explore the adversarial defense of AdvGrid to investigate its effectiveness against common adversarial defenses. We employ two common adversarial defense mechanisms: Adversarial Training (AT) Zhu et al. (2022) and Digital Watermarking (DW) Hayes (2018). For AT, we introduce adversarial samples generated by AdvGrid and clean samples to the adversarial training dataset at a ratio of 5:1 and train the Yolo v3 model. After adversarial training, Yolo v3 achieves an average precision of 90.00% on the test set. For DW, it contains two defense settings: blind image repair and non-blind image repair. In blind image repair, the defender cannot know the location information of the perturbation in advance and needs to detect the perturbation's position first, then perform image repair in the perturbed area. In non-blind image repair, the defender can obtain the location information of the adversarial perturbation and directly repair the image. In this experiment, we adopt non-blind image repair for adversarial defense.\nWe summarize the experimental results of adversarial defense in Table 15. It can be observed that both AT and DW effectively defended against AdvGrid, reducing its attack success rate by 57.60% and 48.42%, respectively. The experimental results also indicate that under defense mechanisms, the adversarial effectiveness of AdvGrid will be weakened. However, despite these excellent defense mechanisms being able to effectively defend against AdvGrid, they cannot provide complete defense."}, {"title": "6. Conclusion", "content": "In this study, we introduce AdvGrid, a multi-view black-box physical attack tailored for infrared pedestrian detectors, addressing the research gap in multi-view black-box attacks in the infrared modality. Our method employs a grid deployed repeatedly in the infrared spectrum, filling the inner side of pedestrian clothing to achieve multi-view adversarial attacks. For adversarial optimization, we utilize genetic algorithms to optimize the physical parameters of the infrared grid to obtain the most adversarial grid. For robust optimization, we employ EOT and TPS to robustly enhance perturbations, making them more adaptable to the transition from digital to physical environments. We conduct numerous experiments to validate the effectiveness, stealthiness, and robustness of AdvGrid. The method achieved attack success rates of 80.00% and 91.86% in digital and physical environments, respectively, validating its effectiveness. In terms of stealthiness, the method's stealthiness is demonstrated by comparing the physical adversarial samples generated by the baseline methods with those generated by AdvGrid. Regarding robustness, by deploying AdvGrid to attack various state-of-the-art object detectors, we achieve an average attack success rate of no less than 50%, with the method outperforming baseline methods in physical attacks, validating its robustness.\nIn the future, we will continue to explore the security issues of infrared computer vision systems and the security of computer vision systems in cross-modal scenarios. Infrared object detectors play crucial roles in security screening, autonomous driving, and other fields. Given the outstanding performance of the proposed method in experimental tests, we call for widespread attention to the proposed method and hope that comprehensive defense strategies can be developed against such attacks."}]}