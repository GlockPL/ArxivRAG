{"title": "Urban Waterlogging Detection: A Challenging Benchmark and Large-Small Model Co-Adapter", "authors": ["Suqi Song", "Chenxu Zhang", "Peng Zhang", "Pengkun Li", "Fenglong Song", "Lei Zhang"], "abstract": "Urban waterlogging poses a major risk to public safety and infrastructure. Conventional methods using water-level sensors need high-maintenance to hardly achieve full coverage. Recent advances employ surveillance camera imagery and deep learning for detection, yet these struggle amidst scarce data and adverse environmental conditions. In this paper, we establish a challenging Urban Waterlogging Benchmark (UW-Bench) under diverse adverse conditions to advance real-world applications. We propose a Large-Small Model co-adapter paradigm (LSM-adapter), which harnesses the substantial generic segmentation potential of large model and the specific task-directed guidance of small model. Specifically, a Triple-S Prompt Adapter module alongside a Dynamic Prompt Combiner are proposed to generate then merge multiple prompts for mask decoder adaptation. Meanwhile, a Histogram Equalization Adapter module is designed to infuse the image specific information for image encoder adaptation. Results and analysis show the challenge and superiority of our developed benchmark and algorithm.", "sections": [{"title": "Introduction", "content": "Road water accumulation is a hidden danger, which not only causes structural damage to the pavement such as cracks and depressions, but also obstruct traffic flow, posing risks of accidents and threats to public safety. Therefore, early identification of waterlogged areas on urban roads is critical and essential.\nTraditional urban waterlogging detection methods involve the installation of sensors on roadways to measure water levels, but challenges to maintain and hardly achieve full coverage [1,24]. Recently, deep learning approaches have been explored for flood detection [17,22,29,30], leveraging surveillance cameras. However, due to the lighting variability of water image reflection and the complexity of urban backgrounds, urban waterlogging detection faces several challenges: 1) Waterlogged areas vary in shape, size and depth, making it difficult to learn a uniform set of features; 2) The reflection on water surface along with shallow and clear standing water renders water texture information indistinct; 3) Under low-light conditions, the waterlogging features are not prominent, further intensifying the difficulty of detection. Owing to these challenges, existing methods struggle to detect waterlogging or provide accurate segmentation in real-world urban scenarios. Particularly, very limited scale and insufficient diversity of labeled data also diminish the generalizability of current methods, making urban waterlogging detection a hard nut to crack.\nRecently, Meta AI has released an innovative visual foundation model known as the Segment Anything model (SAM) [14]. By prompt engineering and training on a corpus of over 1 billion images, SAM exhibits formidable zero-shot capabilities and impressive segmentation performance in numerous application fields [39]. However, due to lacking task-specific knowledge and reliance on manual prompts, SAM shows sub-optimal outcomes in downstream tasks [38]. Thereby, parameter fine-tuning [18, 20], integrating learnable adapters [5, 25, 40] or devising automated prompting [2,8,23,31] appear to improve SAM in downstream tasks. But these techniques still have gaps with real-world waterlogging under adverse conditions.\nTo advance urban waterlogging detection, the first challenge is data scarcity. Existing datasets are of limited scale or lack diversity and comprise only samples that are easy to recognize [30]. Models trained on such data tend to exhibit poor generalizability, struggling to deploy in real-world application. To solve this practical issue, we firstly construct a challenging benchmark tailored for real-world urban waterlogging detection, including adverse conditions, such as low-light conditions, strong-light reflections and clear water, etc. A total of 7,677 waterlogging images are collected with manual labels, containing frames from surveillance cameras and handheld mobile devices. Fig. 1 shows the vision challenge of waterlogging images and the effectiveness of our approach.\nTo compromise the generalization of large model in diverse conditions and the specificity of small model in downstream task, we propose a SAM guided Large-Small Model co-adapter paradigm (LSM-adapter), exploring combined prompts tuning and adapting for efficient yet robust urban waterlogging detection. We design a Triple-S Prompt adapter (TSP-Adapt) comprising a small model based spatial prompter, a prototype-based semantic prompter and a spectrum-based style prompter, to generate prompts from small model, large model and raw input, respectively. The origin and function of these prompts are distinct, to give complementary and counterbalancing benefits, thereby furnishing the large model with a more comprehensive and diverse information. Meanwhile, we propose a Dynamic Prompt Combiner (DPC) composed of a set of learnable weights and an adaptive embedding to dynamically weigh and blend the above prompts for mask decoder. Given the features of waterlogging images are often not prominent, we design a Histogram Equalization adapter (HE-Adpat) to infuse the enhanced task-relevant information (e.g., texture and contrast) into the image encoder. The proposed LSM-adapter paradigm is overall elaborated in Fig. 2.\nIn summary, our main contributions are as follows:\nWe first construct a challenging real-world urban waterlogging benchmark (UW-Bench) under adverse conditions, advancing the field towards application deployment with large model.\nWe propose an innovative large-small model co-adapter paradigm (LSM-adapter), aiming at achieving win-win regime. In order to learn a robust prompter, a Triple-S prompt adapter (TSP-Adapt) with a dynamic prompt combiner is formulated, enabling a success on adaptation.\nWe pioneer the use of vision foundation model i.e., SAM for urban waterlogging detection, providing new insights for future research."}, {"title": "Related Work", "content": "Urban waterlogging detection is crucial for traffic management, urban planning, and disaster early warning systems. The early methods are based on water-level sensors [1,24], which detect water accumulation within a certain area through sensor devices placed at specific locations in a city. However, this approach is cost-ineffective to maintain and very limited in detection range. The remote sensing satellite imagery with a wide monitoring range are thus involved [15,27]. Due to the lack of local detail information in remote sensing-based methods, some studies have explored to utilize images or video data from surveillance cameras to detect waterlogging [7,13,21,34,37]. [21] combines local spatial-temporal features and brightness signals to detect water in videos by using decision forests. [7] estimates flood extent from crowdsourced images using brown color segmentation to identify flood water. More efforts were made to explore CNN based deep learning approaches [17, 22, 29,30], such as Mask R-CNN [10] and DeepLabv3+ [3], and improved the waterlogging detection performance. In this paper, we pioneer the use of vision foundation model (SAM) with innovative designs on a newly developed urban waterlogging benchmark to advance this field fundamentally.\nSAM [14] is composed of a vision transformer-based image encoder, a lightweight mask decoder and a flexible prompt encoder that processes diverse input such as points, bounding boxes, masks and text. Numerous SAM variants have emerged, aiming to explore its potential in various tasks such as medical image analysis [12, 18, 20, 40], camouflaged object detection [5,33] and mirror and transparent objects detection [9]. Adapting SAM to downstream tasks becomes a challenge. Early attempts involved directly fine-tuning a part of SAM (e.g., decoder) on downstream datasets [12,18,20]. As full fine-tuning of image encoder"}, {"title": "Method", "content": "The SAM based large model is the main part in the entire framework for predicting the final segmentation mask. We retain three core components of SAM: the image encoder frozen with pretrained parameters, the lightweight mask decoder and the prompt encoder. As previously mentioned, directly deploying SAM to downstream task produces unsatisfactory results due to the frozen image encoder [5]. To facilitate the image encoder adaptation, we design an histogram equalization adapter laterally connected with the image encoder.\nThe internal structure of the enhanced-image adapter module is presented in Fig. 3 (a), which mainly consists of a histogram equalization, a high-frequency filter and MLP blocks. Given that the features of water are not pronounced in most challenging scenarios, we first conduct histogram equalization operation to highlight the contrast and texture of input image. The enhanced image is then passed through a high-frequency filter to extract high-frequency information beneficial for segmentation, and converted into frequency patch embedding. The patch embedding of original input image is reduced in dimension by fully-connected layer (FC) and added to the frequency patch embedding. This fused feature is mapped by N individual MLP blocks and one parameter-shared MLP, and then merged with the original features of each transformer block in the SAM image encoder.\nWaterlogging possesses the reflectivity and transparency, making it easily camouflage itself due to lighting and complex environmental backgrounds. To this end, we adopt SiNet [6], that succeeds in camouflaged object detection, as our task-specific small model. To accommodate diverse requirements, the choice of the small model is flexible and can be substituted with any other network without necessitating alterations to the overarching framework.\nActing as a domain expert, small model interacts with the large model through spatial prompt, furnishing it with prior knowledge and directional task guidance. Given an input image, spatial prompt is generated by a spatial prompter built on the small model, which could be a predicted mask or a further processed version such as a bounding box or some points, encapsulating the spatial location information of the object to be detected (see Sec. 3.3 for details).\nThe Triple-S prompt adapter module (TSP-Adapt) consists of a spatial prompter, a semantic prompter and a style prompter.\nSAM originally considered two sets of prompts, sparse (boxes or points) and dense (masks), both of which can provide spatial location information for the object to be segmented. We propose to generate such prompts via a spatial prompter utilizing the outputs of the small model. The masks $M_{small}$ predicted by small model can be directly used as the dense prompts. Further processing of the masks $M_{small}$ yields either boxes or points as sparse prompts. For box prompts, we take the bounding boxes of the regions composed of all pixels predicted as the foreground in the masks, represented by the coordinates of the top-left and bottom-right corners of the boxes. For point prompts, we divide the masks into multiple grid regions. In each grid area $G_{g\\times g}$, all pixels are divided into a positive point set $I_P = \\{(i, j) \\mid M_{small}(i, j) \\geq \\tau\\}$ and a negative point set $I_N = \\{(i, j) \\mid M_{small}(i, j) < \\tau\\}$, where $\\tau$is the preset threshold. If the set $I_P$ is not empty, we select one point $p\\in I_P$ with the highest prediction confidence as the positive prompt and the prompt label is set to 1. Otherwise, we take one point $p\\in I_N$ with the lowest prediction confidence as the negative prompt and the prompt label is set to 0. All points from the grids together form the grid point prompts, represented by their coordinates and labels. Despite providing three different types of prompts, considering the redundancy of information, we only select one prompt as the final spatial prompt fed into the prompt encoder and obtain spatial prompt $e_{spa}$. It is noteworthy that the prompt encoder processes dense and sparse prompts differently. Dense prompts are embedded using convolution before adding to the image embedding and sparse prompts are encoded with positional encoding to generate the corresponding sparse embedding.\nThe image embedding of large model contains rich semantic information. Therefore, we propose a prototype learning-based semantic prompter, which leverages useful foreground features from large model to generate semantic prompts. The above process is detailed in Fig. 3 (b). A projector is first adopted to map the image embedding $Z\\in R^{H\\times W\\times D}$ into the projected embedding $\\hat{Z} \\in R^{H\\times W\\times D}$. Inspired by [42], we randomly initialize a group of $C \\cdot K$ prototypes, i.e., $\\{P_{c,k} \\in R^D\\}_{ck=1}^{DC,K}$ in the embedding space, where C is the number of categories and each class is represented by K prototypes. For each pixel sample $Z_{i,j} \\in R^D, i \\in \\{1\\ldots W\\}, j \\in \\{1\\ldots H\\}$ in the projected image embedding $\\hat{Z}$, we respectively calculate its cosine similarity with each prototype $P_{ck}$ to obtain a similarity vector $s_{i,j} \\in R^J$ located at the $(i, j)$ position in the similarity matrix $S \\in R^{H\\times W\\times J}$, where $J = C \\cdot K$. The category of the prototype corresponding to the maximum value in the similarity vector $s_{i,j}$ is assigned to the pixel sample $z_{i,j}$ as the pseudo label $c_{ij}$. The pseudo mask generation (PMG) process can be represented as follows:\n$M = \\{C_{i,j}^\\ast\\}_{i,j}^{H,W} \\text{ with } (j^\\ast, k^\\ast) = arg\\,max_{(c,k)}^{DC,K} \\{(Z_{i,j}, P_{c,k})\\}_{ck=1}^{DC,K}$ (1)\nwhere $(,)$ denotes the cosine similarity operator. The pseudo mask $M$ is then one-hot encoded and employed in conjunction with the original image embedding Z to compute the masked average pooling (MAP). This filters out irrelevant background features and preserves significant foreground features, and derives the semantic embedding as follows:\n$e_{sem} = Concat(e_{sem}^1, e_{sem}^2,\\ldots, e_{sem}^{DC})$ (2)\nwhere $Concat(\\cdot)$ denotes the concatenation operator and $e_{sem}^c$ represents the semantic embedding of class c ($c\\in \\{1\\cdots C\\'}$), computed as follows:\n$e_{sem}^c = \\frac{\\sum_{i,j} Z(i, j) \\odot M^c(i, j)}{\\Sigma_{i,j} M^c(i, j)}$ (3)\nwhere $\\odot$ denotes the Hadamard product. The prototype $P_{ck}$ is momentum-updated after each training iteration according to the center of the k-th subcluster of the training samples assigned to the c-th class via online clustering. Meanwhile, a prototype loss $L_{proto}$ in [42] is utilized to optimize the large model.\nWe introduce a spectrum-based style prompter that extracts image-specific style embedding from the input image as the third type of prompt. The style of an image refers to features such as color and texture. In the context of urban waterlogging detection, these features to some extent can reflect information about illumination and the scene, where illumination is a critical factor"}, {"title": "Dynamic Prompt Combiner", "content": "The dynamic prompt combiner (DPC) is designed to find the optimal combination of the above three types of prompts. DPC comprises three sets of dynamic weights $\\{w_1, w_2, w_3\\}$ assigned to spatial, semantic and style prompt, respectively, and an adaptive embedding $e_{Ada}$ learnable to improve potential bias. The dynamically weighted prompts and the adaptive prompt are then concatenated to generate the final prompt as described in Fig. 2, computed as follows:\n$e_p = Concat\\{w_1 e_{Spa}, w_2 e_{Sem}, w_3 e_{Sty}, e_{Ada}\\}$. (6)\nwhere $\\odot$ denotes element-wise product. During training, the weights are dynamically updated to encourage well-performing prompts while diminishing less-effective prompts. The motivation of the learnable embedding $e_{Ada}$ arises from two aspects. 1) The learnable embedding enables the attention blocks within the decoder to comprehend nonlinear combination among these embeddings, and improve the bias that a linear combination may neglect. 2) It has a flexible capability to capture some useful implicit prompt information."}, {"title": "Optimization", "content": "Two training strategies are proposed to explore suitable joint training of models with diverse architectures, as illustrated in Fig. 4.\nWe introduce a straightforward one-stage training strategy, as depicted in Fig. 4 (a). The image encoder of the large model is frozen and the remaining parts are optimized together. We employ a combination of the focal loss $L_{focal}$ [19], cross-entropy loss $L_{ce}$, IoU loss $L_{iou}$ and the prototype loss $L_{proto}$ [42] for the large model:\n$L_{large} = L_{focal} + L_{ce} + L_{iou} + L_{proto}$. (7)\nThe total loss is given as:\n$L_{total} = L_{large} + \\lambda L_{small}$, (8)\nwhere $L_{small}$ is the original loss of the small model (the loss function depends on specific model selection, and Mask-RCNN [10], U2Net [26] and SiNet [6] are tested in experiments) and $\\lambda$ is a hyper-parameter.\nA two-stage training strategy is provided to mitigate issues related to synchronization difficulties and gradient conflicts that may arise during the joint optimization of large-small models, as shown in Fig. 4 (b).\nIn the first stage, the triple-s prompt adapter module, the dynamic prompt combiner and the prompt encoder are not involved. The image encoder remains frozen, while the remaining modules of the large model and small model are independently optimized by their own loss functions, i.e., $L_{large}$ and $L_{small}$, respectively. The loss function of small model is the same as Eq. 8. The training loss of the large model for the first stage is defined as:\n$L_{large} = L_{focal} + L_{ce} + L_{iou}$. (9)\nIn the second stage, we load the parameters of the modules (small model, HE-adapt and mask decoder) trained in the first stage, while integrating all the modules that were not considered previously for training (TSP-adapt, DPC and prompt encoder). With the parameters of image encoder, HE-adapt and the small model fixed, the optimization objective for the second stage is as follows:\n$L_{total} = L_{focal} + L_{ce} + L_{iou} + L_{proto}$. (10)"}, {"title": "Experiments", "content": "For advancing urban waterlogging detection challenge, we develop a UW-Bench dataset containing a total of 7,677 images from various scenarios, such as waterlogging scenes, dry roads and hard cases, such as slippery roads and nighttime roads. Using keywords such as urban waterlogging, waterlogged roads, and monitoring viewpoints, we crawled and filtered relevant images from surveillance videos and handheld cameras. The training set includes 5,584 images, while the test set is provided by Huawei inc. and contains 2,093 images of urban scenes captured by surveillance cameras only. For the test set, we consider general-sample and hard-sample cases. Some examples from the training set and test set in our UW-Bench are described in Fig. 5, which indicates the difficulty of detecting waterloggings. In the labeling phase, we use EasyData to annotate the dataset with masks. The pixel-level annotation process can be divided into several stages: training, annotation, validation, and correction. We first create some annotation samples and train the annotators to understand the annotation standard. We also assign an inspector to verify the mask annotations. For failed annotations, the inspector gives an explanation and feedback to each annotator to further improve the annotation quality. The overall annotation process ensures the accuracy and reliability of masks in waterlogging regions.\nThe waterlogging detection can be viewed as a pixel-level binary classification task for segmentation, and the waterlogged region is our prospect of interest. Based on the ground truth masks as well as the predicted waterlogging masks, we exploit the commonly used segmentation metrics, such as Precision, Recall, F1-score, and Intersection over Union (IoU) to evaluate the detection performance."}, {"title": "Experimental Results", "content": "We evaluate the performance of the proposed LSM-Adapter on our developed UW-Bench under two types of test set: UW-all and UW-hard (a challenging subset of hard samples). We compare with some representative semantic segmentation models, including UNet [28], DeeplabV3 [4], SETR [41], Segformer [36], Mask R-CNN [10], U2Net [26], SINet [6] as well as SAM-Adapter [5], a large model based on SAM. In the experiments, we adopt a two-stage training strategy in our LSM-Adapter and select the mask as the output type of the spatial prompter (notably, experiments on different training strategies and spatial prompt types are discussed in Section 4.3). Additionally, SAM-Adapter utilizes a default prompt embedding as one of the dual inputs of the mask decoder and the prompt encoder was omitted. For a fair comparison, we also feed the prompts into SAM-Adapter based on the three small models, respectively, following the same setting.\nFrom the results, we witness our proposed method achieves state-of-the-art performance in both test sets, and significantly outperforms extant methodologies, particularly in Recall, F1 score and IoU under different small models. Specifically, LSM-Adapterm, LSM-Adapteru, and LSM-Adapters demonstrate increment by 6.8% to 18.28% in F1 score and 8.22% to 19.89% in IoU, compared with their respective small models. Particularly, LSM-Adapterm exhibits an increment of 19.89% over Mask R-CNN on IoU, indicating that small models with inferior standalone performance are capable of realizing more pronounced performance improvements if co-trained with large model. Compared to the large model i.e. SAM-Adapter, our approach is improved by 7.45% to 9.02% in F1 score and 6.57% to 8.53% in IoU. Moreover, the competitive small model, i.e., SINet, exhibits an even greater gain in overall performance when integrated with the large model.\nFor the purpose of qualitative analysis, we illustrate the waterlogging segmentation results on several general test samples and hard test samples in Fig. 1. Evidently, the predicted masks of LSM-Adapter better approach the ground truth, further demonstrating its superiority to other methods. We further exploit the precision-recall curves (PR) to compare different methods. Fig. 6 illustrates the PR curves of our methods and other existing methods. Each subplot corresponds to the use of different small models. In each subplot, the PR-cureve of our method is closer to the top-right corner and exhibits better performance than existing CNN based segmentation models and Transformer based SAM-Adapter."}, {"title": "Discussion on Training Strategies and Spatial Prompts", "content": "We explore the impact of employing different training strategies and spatial prompts on model performance.\nResults based on different training strateges. The proposed LSM-Adapter that employs the one-stage training strategy is significantly inferior to the two-stage training strategy by comparing their best performances. This demontrates the proposed two-stage training strategy is more stable in adaptation. We posit that the following factors may impede the effective implementation of the one-stage training strategy. During the early stages of training, the predicted output of the small model is characterized by low accuracy, and add complexity to the training of the large model results in subsequent slow convergence. Concurrently, the joint training of two networks with distinct architectures is highly contingent upon the selection of suitable hyper-parameters to achieve a synchronized optimization process. Otherwise, the optimization objectives may conflict each other, thereby hindering the joint model from attaining the optimal performance.\nResults based on different spatial prompts. Under identical conditions concerning the small model and training strategy, we compare three types of spatial prompts. Except LSM-Adapteru, the performance by using mask as spatial prompt consistently surpasses that of other two types of prompts (box and point) in all scenarios. Although the box prompt in LSM-Adapteru is the best, the performance gap from the mask prompt is very small, with a mere 0.03% gap for both F1-score and IoU when employing two-stage training strategy. Moreover, the performance of models employing mask prompts predominantly exceeds that of both large models and their respective small models. A possible explanation for this observation may be that, in comparison to the sparse prompts of boxes and points, mask prompts furnish a more abundant referential information."}, {"title": "Ablation Study", "content": "To verify the effectiveness of each module in our proposed framework, we adopt the two-stage training strategy, select mask as the spatial prompt and exploit LSMU2Net and LSMSINet to conduct the ablation experiments. SAM-Adapter is the baseline model, i.e., without adding any of our proposed modules. Experimental results are shown in Tab. 3. We can see that adding adaptive histogram equalization adapter (HE-adapt) achieves performance improvement in F1-score and IoU, which proves the effect of HE-adapt for image encoder adaptation. Additionally, with spatial prompt (SpaP), semantic prompt (SemP), and style prompt (StyP) are added gradually, the performance is also improved gradually, which proves the effectiveness of the Triple-S prompt adapter (TSP-adapt) for mask decoder adaptation. On this basis, by exploiting the dynamic prompt combiner (DPC), the optimal performance is achieved, which proves our proposed DPC can effectively combine different prompts via a dynamic optimization strategy to improve potential bias implied in an individual prompt."}, {"title": "Conclusion and Outlook", "content": "In this paper, we first pioneer the use of large vision model (i.e., SAM) for a challenging downstream task i.e. urban waterlogging detection and advancing its real-world applications. Considering the generic segmentation capability of large model and the task-specificity of small model, we propose a large-small model co-adapter paradigm following the win-win mechanism. To address the data scarcity of real-world waterlogging detection, we further contribute a large benchmark to advance this field fundamentally, on which more powerful algorithms can be developed. In experiments, we provide new perspectives on the training strategy of large-small model collaboration, due to their architectural differences. This paper sheds light on the possibility of large model in adapting to challenging downstream task with congenital data scarcity and adverse conditions.\nIn the future work, it is expected to further enrich the benchmark to facilitate the pre-train and fine-tune of large model. We hope our proposed large-small model paradigm and perspectives can inspire future work, particularly for downstream tasks with limited resources."}, {"title": "A. Additional details on algorithm.", "content": "In Algorithm 1, we summarize the detailed training process of our proposed LSM-Adapter using a two-stage training strategy."}, {"title": "B. Additional experimental results.", "content": "For the one-stage training, batch size is set to 2, and the large model and small model are optimized jointly using the AdamW optimizer. The learning rate of the large model is set to 0.0005. For three different small models, i.e., Mask R-CNN [10], U2Net [26], and SINet [6], the learning rate is set to 0.0005, 0.001, and 0.0005, respectively. Cosine annealing decay is applied, and the epoch is set to 40.\nFor the two-stage training, the large and small models are first optimized individually. The training settings for the large model are the same as in the one-stage training. Mask R-CNN [10] is trained using the SGD optimizer with a learning rate of 0.001, a batch size of 8, and an epoch of 40. U2Net [26] is trained using the Adam optimizer with a learning rate of 0.001, a batch size of 8, and an epoch of 40. And SINet [6] is trained using the Adam optimizer with a learning rate of 0.0001, a batch size of 16, and an epoch of 100. In the second stage, the epoch is set to 20, and the rest of the settings for the large model are the same as in the first stage.\nWe evaluate the efficiency of LSM-Adapters, including parameter amount and inference time of a single image in Tab. 4. Due to the introduction of the small model, LSM-Adapters has an increased overall parameter amount compared to SAM Adapter [5]. However, the trainable parameters in the second training stage have only increased by 1M, and the inference time per image is comparable to that of the SAM Adapters.\nTo investigate the impact of the scale of the annotated dataset, we randomly select training data with and without waterlogging according to the ratior and then merge them to form a subset for training. The ratior is set to 1, 0.5, and 0.25, respectively, where a value of 1 indicates training with the original fully annotated data. Tab. 5 shows that models trained with more annotated data perform better on downstream tasks, indicating that although foundation model possesses powerful generalization capabilities, additional annotated data is still needed to help the model adapt to downstream tasks.\nWe analyze the impact of two hyper-parameters on performance, the threshold $\\tau$ for spatial prompt of point type"}]}