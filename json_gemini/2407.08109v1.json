{"title": "Urban Waterlogging Detection: A Challenging Benchmark and Large-Small Model Co-Adapter", "authors": ["Suqi Song", "Chenxu Zhang", "Peng Zhang", "Pengkun Li", "Fenglong Song", "Lei Zhang"], "abstract": "Urban waterlogging poses a major risk to public safety and infrastructure. Conventional methods using water-level sensors need high-maintenance to hardly achieve full coverage. Recent advances employ surveillance camera imagery and deep learning for detection, yet these struggle amidst scarce data and adverse environmental conditions. In this paper, we establish a challenging Urban Waterlogging Benchmark (UW-Bench) under diverse adverse conditions to advance real-world applications. We propose a Large-Small Model co-adapter paradigm (LSM-adapter), which harnesses the substantial generic segmentation potential of large model and the specific task-directed guidance of small model. Specifically, a Triple-S Prompt Adapter module alongside a Dynamic Prompt Combiner are proposed to generate then merge multiple prompts for mask decoder adaptation. Meanwhile, a Histogram Equalization Adapter module is designed to infuse the image specific information for image encoder adaptation. Results and analysis show the challenge and superiority of our developed benchmark and algorithm.", "sections": [{"title": "1 Introduction", "content": "Road water accumulation is a hidden danger, which not only causes structural damage to the pavement such as cracks and depressions, but also obstruct traffic flow, posing risks of accidents and threats to public safety. Therefore, early identification of waterlogged areas on urban roads is critical and essential.\nTraditional urban waterlogging detection methods involve the installation of sensors on roadways to measure water levels, but challenges to maintain and hardly achieve full coverage [1,24]. Recently, deep learning approaches have been explored for flood detection [17,22,29,30], leveraging surveillance cameras. However, due to the lighting variability of water image reflection and the complexity of urban backgrounds, urban waterlogging detection faces several challenges: 1) Waterlogged areas vary in shape, size and depth, making it difficult to learn a uniform set of features; 2) The reflection on water surface along with shallow and clear standing water renders water texture information indistinct; 3) Under low-light conditions, the waterlogging features are not prominent, further intensifying the difficulty of detection. Owing to these challenges, existing methods struggle to detect waterlogging or provide accurate segmentation in real-world urban scenarios. Particularly, very limited scale and insufficient diversity of labeled data also diminish the generalizability of current methods, making urban waterlogging detection a hard nut to crack.\nRecently, Meta AI has released an innovative visual foundation model known as the Segment Anything model (SAM) [14]. By prompt engineering and training on a corpus of over 1 billion images, SAM exhibits formidable zero-shot capabilities and impressive segmentation performance in numerous application fields [39]. However, due to lacking task-specific knowledge and reliance on manual prompts, SAM shows sub-optimal outcomes in downstream tasks [38]. Thereby, parameter fine-tuning [18,20], integrating learnable adapters [5,25,40] or devising automated prompting [2,8,23,31] appear to improve SAM in downstream tasks. But these techniques still have gaps with real-world waterlogging under adverse conditions.\nTo advance urban waterlogging detection, the first challenge is data scarcity. Existing datasets are of limited scale or lack diversity and comprise only samples that are easy to recognize [30]. Models trained on such data tend to exhibit poor generalizability, struggling to deploy in real-world application. To solve this practical issue, we firstly construct a challenging benchmark tailored for real-world urban waterlogging detection, including adverse conditions, such as low-light conditions, strong-light reflections and clear water, etc. A total of 7,677 waterlogging images are collected with manual labels, containing frames from surveillance cameras and handheld mobile devices. Fig. 1 shows the vision challenge of waterlogging images and the effectiveness of our approach.\nTo compromise the generalization of large model in diverse conditions and the specificity of small model in downstream task, we propose a SAM guided Large-Small Model co-adapter paradigm (LSM-adapter), exploring combined prompts tuning and adapting for efficient yet robust urban waterlogging detection. We design a Triple-S Prompt adapter (TSP-Adapt) comprising a small model based spatial prompter, a prototype-based semantic prompter and a spectrum-based style prompter, to generate prompts from small model, large model and raw input, respectively. The origin and function of these prompts are distinct, to give complementary and counterbalancing benefits, thereby furnishing the large model with a more comprehensive and diverse information. Meanwhile, we propose a Dynamic Prompt Combiner (DPC) composed of a set of learnable weights and an adaptive embedding to dynamically weigh and blend the above prompts for mask decoder. Given the features of waterlogging images are often not prominent, we design a Histogram Equalization adapter (HE-Adpat) to infuse the enhanced task-relevant information (e.g., texture and contrast) into the image encoder. The proposed LSM-adapter paradigm is overall elaborated in Fig. 2.\nIn summary, our main contributions are as follows:\nWe first construct a challenging real-world urban waterlogging benchmark (UW-Bench) under adverse conditions, advancing the field towards application deployment with large model.\nWe propose an innovative large-small model co-adapter paradigm (LSM-adapter), aiming at achieving win-win regime. In order to learn a robust prompter, a Triple-S prompt adapter (TSP-Adapt) with a dynamic prompt combiner is formulated, enabling a success on adaptation.\nWe pioneer the use of vision foundation model i.e., SAM for urban waterlogging detection, providing new insights for future research."}, {"title": "2 Related Work", "content": "2.1 Urban Waterlogging Detection\nUrban waterlogging detection is crucial for traffic management, urban planning, and disaster early warning systems. The early methods are based on water-level sensors [1,24], which detect water accumulation within a certain area through sensor devices placed at specific locations in a city. However, this approach is cost-ineffective to maintain and very limited in detection range. The remote sensing satellite imagery with a wide monitoring range are thus involved [15,27]. Due to the lack of local detail information in remote sensing-based methods, some studies have explored to utilize images or video data from surveillance cameras to detect waterlogging [7,13,21,34,37]. [21] combines local spatial-temporal features and brightness signals to detect water in videos by using decision forests. [7] estimates flood extent from crowdsourced images using brown color segmentation to identify flood water. More efforts were made to explore CNN based deep learning approaches [17,22,29,30], such as Mask R-CNN [10] and DeepLabv3+ [3], and improved the waterlogging detection performance. In this paper, we pioneer the use of vision foundation model (SAM) with innovative designs on a newly developed urban waterlogging benchmark to advance this field fundamentally.\n2.2 SAM Adaptation\nSAM [14] is composed of a vision transformer-based image encoder, a lightweight mask decoder and a flexible prompt encoder that processes diverse input such as points, bounding boxes, masks and text. Numerous SAM variants have emerged, aiming to explore its potential in various tasks such as medical image analysis [12,18,20,40], camouflaged object detection [5,33] and mirror and transparent objects detection [9]. Adapting SAM to downstream tasks becomes a challenge. Early attempts involved directly fine-tuning a part of SAM (e.g., decoder) on downstream datasets [12,18,20]. As full fine-tuning of image encoder is computationally intensive, some methods are inspired by adapters in natural language processing (NLP) and insert adapter in SAM, achieving efficient fine-tuning by training the adapter only [5,25,40]. For example, SAM-Adapter [5] adds adapters between transformer blocks of the image encoder. SAMed [40] employs the LoRA [11] approach to approximate low-rank updates of the parameters in image encoder. Several studies accomplish adaptation from the perspective of generating automatic task-specific prompts [2,8,23,31]. For example, RSPrompter [2] generates appropriate prompts based on semantic information to yield semantically clear segmentation results for remote sensing images. In this paper, we consider dual adaptation in image- and prompt-level, and collaboration of large and small models."}, {"title": "3 Method", "content": "3.1 SAM-Based Task-Generalized Large Model Branch\nThe SAM based large model is the main part in the entire framework for predicting the final segmentation mask. We retain three core components of SAM: the image encoder frozen with pretrained parameters, the lightweight mask decoder and the prompt encoder. As previously mentioned, directly deploying SAM to downstream task produces unsatisfactory results due to the frozen image encoder [5]. To facilitate the image encoder adaptation, we design an histogram equalization adapter laterally connected with the image encoder.\nHistogram Equalization Adapter Module (HE-Adapt). The internal structure of the enhanced-image adapter module is presented in Fig. 3 (a), which mainly consists of a histogram equalization, a high-frequency filter and MLP blocks. Given that the features of water are not pronounced in most challenging scenarios, we first conduct histogram equalization operation to highlight the contrast and texture of input image. The enhanced image is then passed through a high-frequency filter to extract high-frequency information beneficial for segmentation, and converted into frequency patch embedding. The patch embedding of original input image is reduced in dimension by fully-connected layer (FC) and added to the frequency patch embedding. This fused feature is mapped by N individual MLP blocks and one parameter-shared MLP, and then merged with the original features of each transformer block in the SAM image encoder.\n3.2 CNN-Based Task-Specific Small Model Branch\nWaterlogging possesses the reflectivity and transparency, making it easily camouflage itself due to lighting and complex environmental backgrounds. To this end, we adopt SiNet [6], that succeeds in camouflaged object detection, as our task-specific small model. To accommodate diverse requirements, the choice of the small model is flexible and can be substituted with any other network without necessitating alterations to the overarching framework."}, {"title": "3.3 Triple-S Prompt Adapter Module", "content": "The Triple-S prompt adapter module (TSP-Adapt) consists of a spatial prompter, a semantic prompter and a style prompter.\nSpatial Prompter. SAM originally considered two sets of prompts, sparse (boxes or points) and dense (masks), both of which can provide spatial location information for the object to be segmented. We propose to generate such prompts via a spatial prompter utilizing the outputs of the small model. The masks \\(M_{small}\\) predicted by small model can be directly used as the dense prompts. Further processing of the masks \\(M_{small}\\) yields either boxes or points as sparse prompts. For box prompts, we take the bounding boxes of the regions composed of all pixels predicted as the foreground in the masks, represented by the coordinates of the top-left and bottom-right corners of the boxes. For point prompts, we divide the masks into multiple grid regions. In each grid area \\(G_{g\\times g}\\), all pixels are divided into a positive point set \\(I_P = \\{(i, j) | M_{small}(i, j) \\ge \\tau\\}\\) and a negative point set \\(I_N = \\{(i, j) | M_{small}(i, j) < \\tau\\}\\), where \\(\\tau\\)is the preset threshold. If the set \\(I_P\\) is not empty, we select one point \\(p \\in I_P\\) with the highest prediction confidence as the positive prompt and the prompt label is set to 1. Otherwise, we take one point \\(p \\in I_N\\) with the lowest prediction confidence as the negative prompt and the prompt label is set to 0. All points from the grids together form the grid point prompts, represented by their coordinates and labels. Despite providing three different types of prompts, considering the redundancy of information, we only select one prompt as the final spatial prompt fed into the prompt encoder and obtain spatial prompt \\(e_{Spa}\\). It is noteworthy that the prompt encoder processes dense and sparse prompts differently. Dense prompts are embedded using convolution before adding to the image embedding and sparse prompts are encoded with positional encoding to generate the corresponding sparse embedding.\nSemantic Prompter. The image embedding of large model contains rich semantic information. Therefore, we propose a prototype learning-based semantic prompter, which leverages useful foreground features from large model to generate semantic prompts. The above process is detailed in Fig. 3 (b). A projector is first adopted to map the image embedding \\(Z \\in R^{H\\times W\\times D}\\) into the projected embedding \\(Z \\in R^{H\\times W\\times D}\\). Inspired by [42], we randomly initialize a group of \\(C \\cdot K\\) prototypes, i.e., \\(\\{P_{c,k} \\in R^{D}\\}_{ck=1}^{DC,K}\\) in the embedding space, where \\(C\\) is the number of categories and each class is represented by \\(K\\) prototypes. For each pixel sample \\(Z_{i,j} \\in R^{D}\\), \\(i \\in \\{1 \\cdots W\\}\\), \\(j \\in \\{1\\cdots H\\}\\) in the projected image embedding \\(Z\\), we respectively calculate its cosine similarity with each prototype \\(P_{ck}\\) to obtain a similarity vector \\(s_{i,j} \\in R^{J}\\) located at the \\((i, j)\\) position in the similarity matrix \\(S \\in R^{H\\times W\\times J}\\), where \\(J = C \\cdot K\\). The category of the prototype corresponding to the maximum value in the similarity vector \\(s_{i,j}\\) is assigned to the pixel sample \\(z_{i,j}\\) as the pseudo label \\(c_k^*\\). The pseudo mask generation (PMG) process can be represented as follows:\n\\[M = \\{C_{i,j}^*\\}^{H,W}_{i,j=1} \\text{ with } (j^*, k^*) = arg max \\{\\langle Z_{i,j}, P_{c,k} \\rangle \\}_{c k=1}^{C,K},\\]\nwhere \\(\\langle , \\rangle\\) denotes the cosine similarity operator. The pseudo mask \\(M\\) is then one-hot encoded and employed in conjunction with the original image embedding \\(Z\\) to compute the masked average pooling (MAP). This filters out irrelevant background features and preserves significant foreground features, and derives the semantic embedding as follows:\n\\[e_{sem} = Concat(e_{sem}^1, e_{sem}^2, ..., e_{sem}^{C}),\\]\nwhere \\(Concat(\\cdot)\\) denotes the concatenation operator and \\(e_{sem}^c\\) represents the semantic embedding of class \\(c\\) \\((c\\in \\{1\\cdots C\\}\\)), computed as follows:\n\\[e_{sem}^c = \\frac{\\sum_{i,j} Z(i, j) \\odot M^c(i, j)}{\\Sigma_{i,j} M^c(i, j)},\\]\nwhere \\(\\odot\\) denotes the Hadamard product. The prototype \\(P_{ck}\\) is momentum-updated after each training iteration according to the center of the k-th subcluster of the training samples assigned to the c-th class via online clustering. Meanwhile, a prototype loss \\(L_{proto}\\) in [42] is utilized to optimize the large model.\nStyle Prompter. We introduce a spectrum-based style prompter that extracts image-specific style embedding from the input image as the third type of prompt. The style of an image refers to features such as color and texture. In the context of urban waterlogging detection, these features to some extent can reflect information about illumination and the scene, where illumination is a critical factor"}, {"title": "3.4 Dynamic Prompt Combiner", "content": "The dynamic prompt combiner (DPC) is designed to find the optimal combination of the above three types of prompts. DPC comprises three sets of dynamic weights \\(\\{w_1, w_2, w_3\\}\\) assigned to spatial, semantic and style prompt, respectively, and an adaptive embedding \\(e_{Ada}\\) learnable to improve potential bias. The dynamically weighted prompts and the adaptive prompt are then concatenated to generate the final prompt as described in Fig. 2, computed as follows:\n\\[e_p = Concat\\{w_1 e_{Spa}, w_2 e_{Sem}, w_3 e_{Sty}, e_{Ada}\\}.\\]\nwhere denotes element-wise product. During training, the weights are dynamically updated to encourage well-performing prompts while diminishing less-effective prompts. The motivation of the learnable embedding \\(e_{Ada}\\) arises from two aspects. 1) The learnable embedding enables the attention blocks within the decoder to comprehend nonlinear combination among these embeddings, and improve the bias that a linear combination may neglect. 2) It has a flexible capability to capture some useful implicit prompt information."}, {"title": "3.5 Optimization", "content": "Two training strategies are proposed to explore suitable joint training of models with diverse architectures, as illustrated in Fig. 4.\nOne-stage Training. We introduce a straightforward one-stage training strategy, as depicted in Fig. 4 (a). The image encoder of the large model is frozen and the remaining parts are optimized together. We employ a combination of the focal loss \\(L_{focal}\\) [19], cross-entropy loss \\(L_{ce}\\), IoU loss \\(L_{iou}\\) and the prototype loss \\(L_{proto}\\) [42] for the large model:\n\\[L_{large} = L_{focal} + L_{ce} + L_{iou} + L_{proto}.\\]\nThe total loss is given as:\n\\[L_{total} = L_{large} + \\lambda L_{small},\\]\nwhere \\(L_{small}\\) is the original loss of the small model (the loss function depends on specific model selection, and Mask-RCNN [10], U2Net [26] and SiNet [6] are tested in experiments) and \\(\\lambda\\) is a hyper-parameter.\nTwo-stage Training. A two-stage training strategy is provided to mitigate issues related to synchronization difficulties and gradient conflicts that may arise during the joint optimization of large-small models, as shown in Fig. 4 (b). In the first stage, the triple-s prompt adapter module, the dynamic prompt combiner and the prompt encoder are not involved. The image encoder remains frozen, while the remaining modules of the large model and small model are independently optimized by their own loss functions, i.e., \\(L_{large}\\) and \\(L_{small}\\), respectively. The loss function of small model is the same as Eq. 8. The training loss of the large model for the first stage is defined as:\n\\[L_{large} = L_{focal} + L_{ce} + L_{iou}.\\]\nIn the second stage, we load the parameters of the modules (small model, HE-adapt and mask decoder) trained in the first stage, while integrating all the modules that were not considered previously for training (TSP-adapt, DPC and prompt encoder). With the parameters of image encoder, HE-adapt and the small model fixed, the optimization objective for the second stage is as follows:\n\\[L_{total} = L_{focal} + L_{ce} + L_{iou} + L_{proto}.\\]"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets. For advancing urban waterlogging detection challenge, we develop a UW-Bench dataset containing a total of 7,677 images from various scenarios, such as waterlogging scenes, dry roads and hard cases, such as slippery roads and nighttime roads. Using keywords such as urban waterlogging, waterlogged roads, and monitoring viewpoints, we crawled and filtered relevant images from surveillance videos and handheld cameras. The training set includes 5,584 images, while the test set is provided by Huawei inc. and contains 2,093 images of urban scenes captured by surveillance cameras only. For the test set, we consider general-sample and hard-sample cases. Some examples from the training set and test set in our UW-Bench are described in Fig. 5, which indicates the difficulty of detecting waterloggings. In the labeling phase, we use EasyData to annotate the dataset with masks. The pixel-level annotation process can be divided into several stages: training, annotation, validation, and correction. We first create some annotation samples and train the annotators to understand the annotation standard. We also assign an inspector to verify the mask annotations. For failed annotations, the inspector gives an explanation and feedback to each annotator to further improve the annotation quality. The overall annotation process ensures the accuracy and reliability of masks in waterlogging regions.\nEvaluation Metrics. The waterlogging detection can be viewed as a pixel-level binary classification task for segmentation, and the waterlogged region is our prospect of interest. Based on the ground truth masks as well as the predicted waterlogging masks, we exploit the commonly used segmentation metrics, such as Precision, Recall, F1-score, and Intersection over Union (IoU) to evaluate the detection performance."}, {"title": "4.2 Experimental Results", "content": "We evaluate the performance of the proposed LSM-Adapter on our developed UW-Bench under two types of test set: UW-all and UW-hard (a challenging subset of hard samples). We compare with some representative semantic segmentation models, including UNet [28], DeeplabV3 [4], SETR [41], Segformer [36], Mask R-CNN [10], U2Net [26], SINet [6] as well as SAM-Adapter [5], a large model based on SAM. In the experiments, we adopt a two-stage training strategy in our LSM-Adapter and select the mask as the output type of the spatial prompter (notably, experiments on different training strategies and spatial prompt types are discussed in Section 4.3). Additionally, SAM-Adapter utilizes a default prompt embedding as one of the dual inputs of the mask decoder and the"}, {"title": "4.3 Discussion on Training Strategies and Spatial Prompts", "content": "We explore the impact of employing different training strategies and spatial prompts on model performance. The results are presented in Tab. 2.\nResults based on different training strateges. The proposed LSM-Adapter that employs the one-stage training strategy is significantly inferior to the two-"}, {"title": "4.4 Ablation Study", "content": "To verify the effectiveness of each module in our proposed framework, we adopt the two-stage training strategy, select mask as the spatial prompt and exploit \\(LSM_{U2Net}\\) and \\(LSM_{SINet}\\) to conduct the ablation experiments. SAM-Adapter is the baseline model, i.e., without adding any of our proposed modules. Experimental results are shown in Tab. 3. We can see that adding adaptive histogram equalization adapter (HE-adapt) achieves performance improvement in F1-score and IoU, which proves the effect of HE-adapt for image encoder adaptation. Additionally, with spatial prompt (SpaP), semantic prompt (SemP), and style prompt (StyP) are added gradually, the performance is also improved gradually, which proves the effectiveness of the Triple-S prompt adapter (TSP-adapt) for mask decoder adaptation. On this basis, by exploiting the dynamic prompt combiner (DPC), the optimal performance is achieved, which proves our proposed DPC can effectively combine different prompts via a dynamic optimization strategy to improve potential bias implied in an individual prompt."}, {"title": "5 Conclusion and Outlook", "content": "In this paper, we first pioneer the use of large vision model (i.e., SAM) for a challenging downstream task i.e. urban waterlogging detection and advancing its real-world applications. Considering the generic segmentation capability of large model and the task-specificity of small model, we propose a large-small model co-adapter paradigm following the win-win mechanism. To address the data scarcity of real-world waterlogging detection, we further contribute a large benchmark to advance this field fundamentally, on which more powerful algorithms can be developed. In experiments, we provide new perspectives on the training strategy of large-small model collaboration, due to their architectural differences. This paper sheds light on the possibility of large model in adapting to challenging downstream task with congenital data scarcity and adverse conditions.\nIn the future work, it is expected to further enrich the benchmark to facilitate the pre-train and fine-tune of large model. We hope our proposed large-small model paradigm and perspectives can inspire future work, particularly for downstream tasks with limited resources."}, {"title": "A. Additional details on algorithm.", "content": "In Algorithm 1, we summarize the detailed training process of our proposed LSM-Adapter using a two-stage training strategy."}, {"title": "B. Additional experimental results.", "content": "B.1 Additional implementation details.\nFor the one-stage training, batch size is set to 2, and the large model and small model are optimized jointly using the AdamW optimizer. The learning rate of the large model is set to 0.0005. For three different small models, i.e., Mask R-CNN [10], U2Net [26], and SINet [6], the learning rate is set to 0.0005, 0.001, and 0.0005, respectively. Cosine annealing decay is applied, and the epoch is set to 40.\nFor the two-stage training, the large and small models are first optimized individually. The training settings for the large model are the same as in the one-stage training. Mask R-CNN [10] is trained using the SGD optimizer with a learning rate of 0.001, a batch size of 8, and an epoch of 40. U2Net [26] is trained using the Adam optimizer with a learning rate of 0.001, a batch size of 8, and an epoch of 40. And SINet [6] is trained using the Adam optimizer with a learning rate of 0.0001, a batch size of 16, and an epoch of 100. In the second stage, the epoch is set to 20, and the rest of the settings for the large model are the same as in the first stage.\nB.2 Model efficiency.\nWe evaluate the efficiency of LSM-Adapters, including parameter amount and inference time of a single image in Tab. 4. Due to the introduction of the small model, LSM-Adapters has an increased overall parameter amount compared to SAM Adapter [5]. However, the trainable parameters in the second training stage have only increased by 1M, and the inference time per image is comparable to that of the SAM Adapters.\nB.3 Additional ablation study.\nEffect of the scale of dataset. To investigate the impact of the scale of the annotated dataset, we randomly select training data with and without waterlogging according to the ratio r and then merge them to form a subset for training. The ratio r is set to 1, 0.5, and 0.25, respectively, where a value of 1 indicates training with the original fully annotated data. Tab. 5 shows that models trained with more annotated data perform better on downstream tasks, indicating that although foundation model possesses powerful generalization capabilities, additional annotated data is still needed to help the model adapt to downstream tasks.\nEffect of Hyper-parameter selection. We analyze the impact of two hyper-parameters on performance, the threshold \\(\\tau\\) for spatial prompt of point type"}, {"title": "B.4 Experiments on additional datasets.", "content": "We use four additional datasets, CHAMELEON [32], CAMO [16], COD10K [6] for camouflaged object detection, and ISTD [35] for shadow detection to evaluate the proposed method on more downstream tasks. Tab. 8 shows that LSM-Adapters outperforms other methods on all datasets.\nB.5 Additional qualitative results.\nWe provide more qualitative results to further demonstrate the effectiveness and superiority of the proposed LSM-Adapter. Fig. 7 illustrates the visual comparison results between LSM-Adapter and other existing methods, including Mask R-CNN [10], U2Net [26], SINet [6], and SAM-Adapter [5]. LSM-Adapter selects masks from three different small models as spatial prompts and utilizes the two-stage training strategy. It is evident that LSM-Adapter has the prediction results closest to the ground truth, whether compared with the small or large models, even in the case of challenging samples, such as the first, second, eighth, and ninth rows. Meanwhile, we observe that spatial prompts derived from the small model can compensate for the knowledge gap of the large model when the prediction output of the small model is superior, resulting in LSM-Adapter\u2019s predictions that more closely align with the ground truth. Concurrently, even when the small model\u2019s inferior predictive masks are employed as prompts, LSM-Adapter remains relatively unaffected, maintaining a prediction quality comparable to, if not slightly superior to, that of the standalone large model."}]}