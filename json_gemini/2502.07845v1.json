{"title": "Spread them Apart: Towards Robust Watermarking of Generated Content", "authors": ["Mikhail Pautov", "Danil Ivanov", "Andrey V. Galichin", "Oleg Rogov", "Ivan Oseledets"], "abstract": "Generative models that can produce realistic images have improved significantly in recent years. The quality of the generated content has increased drastically, so sometimes it is very difficult to distinguish between the real images and the generated ones. Such an improvement comes at a price of ethical concerns about the usage of the generative models: the users of generative models can improperly claim ownership of the generated content protected by a license. In this paper, we propose an approach to embed watermarks into the generated content to allow future detection of the generated content and identification of the user who generated it. The watermark is embedded during the inference of the model, so the proposed approach does not require the retraining of the latter. We prove that watermarks embedded are guaranteed to be robust against additive perturbations of a bounded magnitude. We apply our method to watermark diffusion models and show that it matches state-of-the-art watermarking schemes in terms of robustness to different types of synthetic watermark removal attacks.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in generative models have brought the performance of image synthesis tasks to a whole new level. For example, the quality of the images generated by diffusion models (DMs, Croitoru et al. [2023], Rombach et al. [2022], Esser et al. [2024]) is now sometimes comparable to the one of the human-generated pictures or photographs. Compared to generative adversarial networks (GANs, Goodfellow et al. [2014], Brock et al. [2019], diffusion models allow the generation of high-resolution, naturally looking pictures and incorporate much more stable training, leading to a more diverse generation. More than that, the image generation process with diffusion models is more stable, controllable, and explainable. They are easy to use and are widely deployed as tools for data generation, image editing (Kawar et al. [2023], Yang et al. [2023]), music generation (Schneider et al. [2024]), text-to-image synthesis (Saharia et al. [2022], Zhang et al. [2023], Ruiz et al. [2023]) and in other multimodal settings.\nUnfortunately, several ethical and legal issues may arise from the usage of diffusion models. On the one hand, since diffusion models can be used to generate fake content, for example, deepfakes (Zhao et al. [2021], Narayan et al. [2023]), it is crucial to develop automatic tools to verify that a particular digital asset is artificially generated. On the other hand, a dishonest user of the model protected by a copyright license can query it, receive the result of generation, and later claim exclusive copyright. In this work, we focus on the detection of the content generated by a particular model and the identification of the end-user who queried the model to generate a particular content. We develop a technique to embed the digital watermark into the generated content during the inference of the generative model, so it does not require retraining or fine-tuning the generative model. The approach allows not only to verify that the content was generated by a source model but also to identify the user who sent a corresponding query to the generative model. We prove that the watermark embedded is robust against additive perturbations of the content of a bounded magnitude.\nOur contributions are threefold:\n\u2022 We propose Spread them Apart, the framework to embed digital watermarks into the generative content of continuous nature. Our method embeds the watermark during the process of content generation and, hence, does not require additional training of the generative model.\n\u2022 We apply the framework to watermark images gener-"}, {"title": "2 RELATED WORK", "content": "2.1 DIFFUSION MODEL\nInspired by non-equilibrium statistical physics, Sohl-Dickstein et al. [2015] introduced the diffusion model to fit complex probability distributions. Ho et al. [2020] introduced a new class of models called Denoising Diffusion Probabilistic Models (DDPM) by establishing a novel connection between the diffusion model and the denoising scoring matching. Later, the Latent Diffusion Model (LDM) was developed to improve efficiency and reduce computational complexity, with the diffusion process happening within a latent space Rombach et al. [2022]. During training, the LDM uses an encoder \\( \\mathcal{E} \\) to map an input image \\( x \\) to the latent space: \\( z = \\mathcal{E}(x) \\). For the reverse operation a decoder \\( \\mathcal{D} \\) is employed, so that \\( x = \\mathcal{D}(z) \\). During inference, the LDM starts with a noise vector \\( z \\sim \\mathcal{N}(0, I) \\) in the latent space and iteratively denoises it. The decoder then maps the final latent representation back to the image space.\n2.2 WATERMARKING OF DIGITAL CONTENT\nWatermarking has been recently adopted to protect the intellectual property of neural networks (Wu et al. [2020], Pautov et al. [2024]) and generated content (Kirchenbauer-"}, {"title": "2.3 ROBUSTNESS TO WATERMARK REMOVAL ATTACKS", "content": "Watermarking attacks are aimed at removing the watermark embedded into the model's weights or generated content. In the prior works on removing the watermarks from generated images (Li et al. [2019], Cao et al. [2019]), the at-"}, {"title": "3 PROBLEM STATEMENT", "content": "In this section, we formulate the problem statement and the research objectives. Note that we focus on the watermarking of images generated by diffusion models, but the formulation below is valid for watermarking of any generated content, for example, audio, video, or text.\n3.1 IMAGE WATERMARKING\nIn our approach, we focus on detection and attribution of the generated image simultaneously: while detection is aimed to verify whether a particular image is generated by a given model, attribution is aimed at determining the user who generated the image.\nSuppose that we are given the generative model \\( f \\) deployed in the black-box setting, i.e., as a service: in the generation phase a user \\( u_i \\in [U_1,..., U_m] \\) sends a query to the model and receives a generated image \\( x \\in \\mathbb{R}^d \\). If \\( x \\) is a watermarked image, the owner of model \\( f \\) should be able to identify that \\( x \\) is generated by user \\( u_i \\) by querying the model \\( f \\). In our method, the image is watermarked during the generation phase, not during the post-processing. We formulate the process of watermarking and attribution in the following way:\n1. When the user \\( u_i \\in [U_1,..., U_m] \\) registers in the service, it is assigned a pair of public and private keys, namely, the watermark \\( w(u_i) \\) and the secret \\( s(u_i) \\). Wa-"}, {"title": "3.2 THE PROBABILITY OF INCORRECT ATTRIBUTION", "content": "We assume that the watermark \\( w(u_i) \\) attributed to the user \\( u_i \\) is drawn randomly and uniformly from the set of all possible \\( n \\)-bit watermarks, \\( {0,1}^n \\). Following the prior works (Fernandez et al. [2023]), we formulate the detection problem as the hypothesis test. In case of a single user \\( U_i \\), we define the null hypothesis \\( \\mathcal{H}_0 \\) = \u201cthe object \\( x \\) is generated not by \\( u_i \\)\u201d and the alternative hypothesis \\( \\mathcal{H}_1 \\) = \u201cthe object \\( x \\) is generated by \\( u_i \\)\u201d. Additionally, under the null hypothesis, we assume that the j'th bit in the watermark \\( w(u_i|x) \\) extracted from \\( x \\) is the same as the j'th bit from \\( w(u_i) \\) with the probability \\( p_r \\).\nIn the case of a single user \\( u_i \\) and given the attribution rule from the Eq. 2, we compute the probability of the false attribution, namely,\n\\( FRP(1)|u_i = P [d(w', w(u_i)) \\in [0, \\tau_1] \\cup [\\tau_2, n]] = \\sum_{q\\in[0,\\tau_1]\\cup[\\tau_2,n]} {n \\choose q} (1-p_r)^{n-q} \\),\nwhere \\( w' = w(u_i|x) \\) is a random watermark uniformly sampled from \\( {0,1}^n \\), namely, \\( w' \\sim {0,1}^n \\), \\( w' \\neq w(u_i) \\). In case of \\( m \\) users, the probability \\( FPR(m) \\) of incorrect attribution of the non-watermarked image \\( x \\) to some other"}, {"title": "3.3 ROBUSTNESS TO WATERMARK REMOVAL ATTACKS", "content": "When the user \\( u_i \\) receives the watermarked image \\( x \\), it can post-process it to obtain the other image, \\( x' \\), which does retain the sufficient part of the watermark \\( w(u_i) \\). The transition from \\( x \\) to \\( x' \\) may be done by applying an image transformation, such as brightness or contrast adjustment, Gaussian blur, or additive noise. The other approach is to perform an adversarial attack on the generative model to erase the watermark (Jiang et al. [2024]). In our settings, we assume that the generative model is deployed as the black-box service with limited access to the API, so an adversary can not apply white-box adversarial attacks (Jiang et al. [2023])."}, {"title": "4 METHOD", "content": "In this section, we provide a detailed description of the proposed approach, its implementation details, and the robustness guarantee against additive watermarking removal attacks of bounded magnitude.\n4.1 EMBEDDING AND EXTRACTION OF THE WATERMARK\nSuppose that \\( f \\) is the generative model. Recall that the user \\( u_i \\in [U_1,..., U_m] \\) receives a pair \\( (w(u_i), s(u_i)) \\) after the registration in the service, where both the watermark and the secret are unknown to the user and are privately kept by the owner of \\( f \\). Let \\( x \\) be the generated image. Then, the watermark embedding process is described as follows:\n1. The secret \\( s(u_i) \\) is interpreted as two sequences of indices, \\( \\mathcal{A} = {1,...,a_n} \\) and \\( \\mathcal{B} = {b_1,..., b_n} \\). The watermark \\( w(u_i) = {w_1,..., w_n} \\) is the binary string that restricts the generated image \\( x \\) in the areas represented by the sets \\( \\mathcal{A} \\) and \\( \\mathcal{B} \\)."}, {"title": "4.2 IMPLEMENTATION DETAILS", "content": "In this subsection, we describe the watermarking procedure. First of all, we have to note that in the Stable Diffusion model, the latent vector \\( z \\) produced by the U-Net is then decoded back into the image space using a VAE decoder: \\( x = \\mathcal{D}(z) \\). To embed the watermark into an image, we optimize a special two-component loss function with respect to the latent vector \\( z \\). The overall loss is written as follows:\n\\( \\mathcal{L} = \\lambda_{wm} \\mathcal{L}_{wm} + \\lambda_{qual} \\mathcal{L}_{qual} \\),\nThe first term, \\( \\mathcal{L}_{wm} \\), defines how the image complies with the pixel difference imposed by the watermark \\( w(u_i) = {w_1,...,w_n} \\) and the secret \\( s(u_i) = {a_1,..., a_n, b_1,..., b_n} \\):"}, {"title": "4.3 ROBUSTNESS GUARANTEE", "content": "By construction, the watermark embedded by our method is robust against additive watermark removal attacks of a bounded magnitude. Namely, let the watermark \\( w(u_i|x) \\) be embedded in \\( x \\) with the use of the secret \\( s(u_i) = {a_1,..., a_n, b_1, ..., b_n} \\) of the user \\( u_i \\). Let\n\\( \\Delta_i = \\frac{x_{a_i} - x_{b_i}}{2} \\)\nThen, the following lemma holds.\nLemma 4.1. Let \\( \\epsilon \\in \\mathbb{R}^d \\) and \\( \\Delta_{i_1} < \\Delta_{i_2} \\le \\dots < \\Delta_{i_n} \\). Then, if \\( ||\\epsilon||_{\\infty} < \\Delta_{i_k} \\), then \\( d(w(u_i|x + \\epsilon), w(u_i|x)) < k \\).\nProof. Note that to change the j'th bit of watermark \\( w(u_i|x) \\), an adversary has to change the sign in expression \\( (x_{a_j} - x_{b_j}) \\). Without the loss of generality, let \\( x_{a_j} - x_{b_j} \\ge 0 \\)."}, {"title": "5 EXPERIMENTS", "content": "5.1 GENERAL SETUP\nWe use stable-diffusion-2-base model (Rombach et al. [2022]) with the epsilon prediction type and 50 steps of denoising for the experiments. The resolution of generated images is 512 \u00d7 512. The experiments were conducted on DiffusionDB dataset (Wang et al. [2022]). Specifically, we choose 1000 unique prompts and generate 1000 different images.\nThe public key for the user is sampled from the Bernoulli distribution with the parameter \\( p = 0.5 \\). The length of a key is set to be \\( n = 100 \\). The private key is generated by randomly picking 2n unique pairs of indices of the flattened image.\n5.2 ATTACK DETAILS\nWe evaluate the robustness of the watermarks embedded by our method against the following watermark removal attacks: brightness adjustment, contrast shift, gamma correction, image sharpening, hue adjustment, saturation adjustment, random additive noise, JPEG compression, and the white-box PGD adversarial attack (Madry et al. [2018]). In this section, we describe these attacks in detail.\nBrightness adjustment of an image \\( x \\) was performed by adding a constant value to each pixel: \\( X_{brightness} = X + b \\), where \\( b \\) was sampled from the uniform distribution \\( \\mathcal{U}[-20, 20] \\).\nContrast shift was done in two ways: positive and negative. The positive contrast shift implies the multiplication of each pixel of an image by a constant positive factor: \\( X_{contrast} = cx \\), where \\( c \\) was sampled from the uniform distribution, \\( c \\sim \\mathcal{U}[0.5, 2] \\)."}, {"title": "5.4 INCREASING THE ROBUSTNESS TO WATERMARK REMOVAL ATTACKS", "content": "It is noteworthy that the set of watermark removal attacks described in Section 5.2 does not include geometric transformations. The version of the proposed method described in Section 4 does not provide robustness against geometric"}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this paper, we propose Spread them Apart, the framework to watermark generated content of continuous nature and apply it to images generated by Stable Diffusion. We prove that the watermarks produced by our method are provably robust against additive watermark removal attacks of a bounded norm and are provably robust to multiplicative perturbations by design. Our approach can be used to both detect that the image is generated by a given model and to identify the end-user who generated it. Experimentally, we show that our method is comparable to the state-of-the-art watermarking methods in terms of the invisibility of watermark and the robustness to synthetic watermark removal attacks."}, {"title": "A IMPROVING THE ROBUSTNESS TO WATERMARK REMOVAL ATTACKS", "content": "Note that the version of the watermarking method proposed in this paper is based on the embedding of the watermark into the pixel domain of a generated image. Hence, the method can not yield watermarks provably robust against geometric transformations of the image, such as rotations and translations. In this section, we describe an extension of the method to guarantee robustness against such transformations. An extension is done by embedding the watermark simultaneously into the pixel domain and special representations in the frequency domain, which are invariant under rotations and translations.\nA.1 INVARIANTS IN THE FREQUENCY DOMAIN\nWe will call image transform \\( \\gamma : \\mathbb{R}^d \\to \\mathbb{R}^d \\) invariant under parametric perturbation \\( \\varphi : \\mathbb{R}^d \\times \\Theta \\to \\mathbb{R}^d \\) at point \\( x \\in \\mathbb{R}^d \\) if\n\\( \\gamma(x) = \\gamma(\\varphi(x, \\theta)) \\) for all \\( \\theta \\in \\Theta \\),\nwhere \\( \\Theta \\) is the set of parameters of perturbation \\( \\varphi \\). In this work, we use two invariants discussed in Lin and Brandt [1993], formulated as theorems below.\nTheorem A.1. Let \\( h(x, y) \\) be an integrable nonnegative function and its Fourier transform\n\\( \\mathcal{H}(\\omega_x, \\omega_y) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} h(x, y)e^{-i(x\\omega_x+y\\omega_y)} dxdy = A(\\omega_x, \\omega_y)e^{-i\\psi(\\omega_x,\\omega_y)} \\)\nbe twice differentiable. Then the function \\( A(\\omega_x,\\omega_y) \\) is invariant under translation.\nTheorem A.2. Let \\( h(r,t) = h(e^r \\cos t, e^r \\sin t) \\) be the logarithmic-polar representation of the image \\( h(x, y) \\). The Fourier-Mellin transform of \\( h(r, t) \\) is\n\\( \\tilde{H}(\\omega, k) = \\int_{-\\infty}^{\\infty} \\int_{0}^{2\\pi} h(r, t)e^{-i(kt+\\omega r)}dtdr = \\tilde{A}(\\omega, k)e^{-i\\psi(\\omega,k)} \\),\nwhere \\( \\tilde{A}(\\omega, k) \\) is the magnitude and \\( \\psi(\\omega,k) \\) is the phase.\nIf \\( h(r, t) \\) is an integrable nonnegative function and its Fourier-Mellin transform \\( \\tilde{H}(\\omega, k) \\) is twice differentiable, then the function \\( A(\\omega, k) \\) is invariant under rotation.\nRemark. For consistent notation, we assume that the image \\( h(x,y) \\) is a scalar function on a two-dimensional plane. Later, we refer to invariants from Theorems A.1-A.2 as to \\( \\gamma_t \\) and \\( \\gamma_r \\), respectively."}, {"title": "A.2 SPREAD THEM APART: THREE WATERMARKS INSTEAD OF ONE", "content": "Recall from Section 4 that the watermark embedding process in the pixel domain is done by optimizing the loss function \\( \\mathcal{L} \\) in the form from Eq. 10: \\( \\mathcal{L} = \\lambda_{wm} \\mathcal{L}_{wm} + \\lambda_{qual} \\mathcal{L}_{qual} \\), where\n\\( \\mathcal{L}_{wm} = \\sum_{i=1}^{n} \\min((-1)^{w_i} (x_{a_i} - x_{b_i}) + \\varepsilon,0), x = \\mathcal{D}(z) \\),\n\\( w(u_i) = {w_1,..., w_n} \\) is the watermark assigned to user \\( u_i \\) and \\( s(u_i) = {a_1,..., a_n, b_1,..., b_n} \\) is the secret of user \\( u_i \\). To ensure the robustness of the watermark to geometric transformations, we suggest embedding the watermark simultaneously in the pixel domain and in invariants \\( \\gamma_t \\) and \\( \\gamma_r \\). To do so, we optimize the loss function \\( \\tilde{\\mathcal{L}} \\) in the form below:\n\\( \\tilde{\\mathcal{L}} = \\lambda_{wm} \\mathcal{L}_{wm} + \\lambda_{qual} \\mathcal{L}_{qual} + \\lambda_t \\mathcal{L}_t + \\lambda_r \\mathcal{L}_r = \\mathcal{L} + \\lambda_t \\mathcal{L}_t + \\lambda_r \\mathcal{L}_r \\).\nIn Eq. 22, \\( \\lambda_t \\), \\( \\lambda_r \\) are positive constants and\n\\( \\mathcal{L}_t = \\sum_{i=1}^{n} \\min((-1)^{w_i} (\\gamma_t(x)_{a_i} - \\gamma_t(x)_{b_i}) + \\varepsilon, 0) \\)\n\\( \\mathcal{L}_r = \\sum_{i=1}^{n} \\min((-1)^{w_i} (\\gamma_r(x)_{a_i} - \\gamma_r(x)_{b_i}) + \\varepsilon, 0) \\)\nare the loss functions that control the embedding of the watermark into invariants \\( \\gamma_t(x) \\) and \\( \\gamma_r(x) \\), respectively."}, {"title": "A.3 EXTRACTIONS OF WATERMARKS", "content": "Given \\( m \\) as the number of users, the owner of the generative model extracts 3m watermarks from the given image \\( x \\). Namely, given the secret \\( s(u_i) \\) of the user \\( u_i \\), three watermarks, \\( w(u_i|x) \\), \\( w(u_i|\\gamma_r(x)) \\), \\( w(u_i|\\gamma_t(x)) \\) are restored:\n\\( \\begin{cases} x_{a_j} \\ge x_{b_j} \\Rightarrow w(u_i|x)_j = 0, \\\\ x_{a_j} < x_{b_j} \\Rightarrow w(u_i|x)_j = 1, \\end{cases} \\)\n\\( \\begin{cases} \\gamma_r(x)_{a_j} \\ge \\gamma_r(x)_{b_j} \\Rightarrow w(u_i|\\gamma_r(x))_j = 0, \\\\ \\gamma_r(x)_{a_j} < \\gamma_r(x)_{b_j} \\Rightarrow w(u_i|\\gamma_r(x))_j = 1, \\end{cases} \\)\n\\( \\begin{cases} \\gamma_t(x)_{a_j} \\ge \\gamma_t(x)_{b_j} \\Rightarrow w(u_i|\\gamma_t(x))_j = 0, \\\\ \\gamma_t(x)_{a_j} < \\gamma_t(x)_{b_j} \\Rightarrow w(u_i|\\gamma_t(x))_j = 1. \\end{cases} \\)\nTo assign the (possibly) watermarked image \\( x \\) to the user, the owner of the model determines three candidates:\n\\( u = \\arg \\min_{u_i\\in[U_1,...,U_m]:\\xi_i=1} d(w(u_i), w(u_i|x)) \\),\n\\( u_r = \\arg \\min_{u_i\\in[U_1,...,U_m]:\\xi_i=1} d(w(u_i), w(u_i|\\gamma_r(x))) \\),\n\\( u_t = \\arg \\min_{u_i\\in[U_1,...,U_m]:\\xi_i=1} d(w(u_i), w(u_i|\\gamma_t(x))) \\),\nwhere\n\\( \\xi = 1[d(w(u_i|\\varphi(x)), w(u_i)) \\in [0, \\tau_1] \\cup [\\tau_2, n]] \\)\nindicates which of the users' watermarks are within an appropriate distance from the corresponding extracted watermarks. Finally, the owner of the model assigns the image \\( x \\) to the user \\( \\tilde{u} \\) that corresponds to the minimum distance among all 3m pairs of watermarks:"}, {"title": "A.4 PROBABILITY OF INCORRECT ATTRIBUTION", "content": "Assume that the user \\( u_i \\) owns the watermarked image \\( x \\). Note that the attribution of the image to the user \\( u_i \\) is guaranteed to hold if\n\\( u = u_i, u_r = u_i, u_t = u_i \\).\nHence, the probability of incorrect attribution, \\( FPR_3(m) \\), is bounded from above by the sum\n\\( P(u\\neq u_i) + P(u_r \\neq u_i) + P(u_t \\neq u_i) \\),\nyielding\n\\( FPR_3(m) \\le 3p \\),\nwhere \\( p \\) is from Eq. 4."}]}