{"title": "Affective Computing Has Changed: The\nFoundation Model Disruption", "authors": ["Bj\u00f6rn Schuller", "Adria Mallol-Ragolta", "Alejandro Pe\u00f1a Almansa", "Iosif Tsangko", "Mostafa M. Amin", "Anastasia Semertzidou", "Lukas Christ", "Shahin Amiriparian"], "abstract": "The dawn of Foundation Models has on the one hand revolutionised a wide range\nof research problems, and, on the other hand, democratised the access and use\nof AI-based tools by the general public. We even observe an incursion of these\nmodels into disciplines related to human psychology, such as the Affective Com-\nputing domain, suggesting their affective, emerging capabilities. In this work, we\naim to raise awareness of the power of Foundation Models in the field of Affective\nComputing by synthetically generating and analysing multimodal affective data,\nfocusing on vision, linguistics, and speech (acoustics). We also discuss some fun-\ndamental problems, such as ethical issues and regulatory aspects, related to the\nuse of Foundation Models in this research area.", "sections": [{"title": "1 Introduction", "content": "\"The world of Affective Computing has changed. I see it in the vision modality. I read\nit in the linguistic modality. I hear it in the speech modality. Much that once was is\noutdated...\" This quote, which might sound slightly familiar to the J. R. R. Tolkien's\nreaders, aims to literary exemplify how the disruption of Foundation Models (FM)\nmight be impacting the Affective Computing research as we knew it. Before centring\nthe discussion on this topic, we summarise where we come from.\n\nThe Affective Computing research can be broadly clustered into three main cate-\ngories: recognition of affect, generation of affective content, and response to affect [1].\nThe development of systems with affective features opens a vast arsenal of use cases,\nranging from digital psychology recognising depression [2] to security applications\n(e. g., stress prediction in driving [3]). However, Affective Computing applications are\nmostly centred in Human-Computer and Human-Robot Interaction (HCI/HRI) [4-6],\nwhere the ability to interpret the human affect is not only desirable to improve the\ncommunication [7, 8], but a necessary capability to correctly understand the message.\nEarly work in the field of psychology indicated that human affect is communicated in\na multimodal manner through physical channels such as facial expressions, language,\nor voice [9]. Consequently, the Affective Computing community has paid significant\nattention to visual [10, 11], linguistic [12, 13], and speech (acoustic) [14, 15] data\nprocessing, including multimodal configurations [16, 17].\n\nOf the different dimensions of human affect, the emotional states have attracted\nmajor attention for its impact in a wide range of aspects of peoples' lives. Given\nsuch relevance, the Affective Computing community has been putting major efforts in\nthe development of automated systems for the recognition and understanding of the\nhuman feelings. Early research in the field of emotion recognition relied on conventional\nMachine Learning (ML) pipelines, in which expert-crafted features were first extracted\nfrom the raw data such as pixels, words, or an audio signal, and then processed utilising\ntraditional statistical methods; e. g., Support Vector Machines (SVM). The key to this\nconventional approach was to try to design a suitable set of features that captures\nemotional content; i. e., the hand-crafted features. In the visual domain, the emotional\ncontent was assumed to be mainly reflected in the facial expression information, from\nwhich the features were extracted; e. g., through Principal Component Analysis (PCA),\nor traditional Computer Vision (CV) techniques, such as Gabor wavelets or texture\nfilters. In addition, prior knowledge of the Action Units (AUs) defined in the Facial\nAction Coding System (FACS) [18] helped in the design of these features. In linguistics,\nfeature extraction typically relied on n-grams or bag-of-words \u2013 a vector representation\nof text by sparse vectors that represent some form of vocabulary and some form of\nfrequency of occurrence of the words in the current text-, after carefully preprocessing\nthe raw text (e. g., by stemming or stopping). In audio, several works noticed the rich\nemotional information reflected in the prosodic features [19], which in combination\nwith spectral features \u2013 e.g., the Mel Frequency Cepstral Coefficients (MFCC) \u2013,formed the basis for Speech Emotion Recognition (SER).\n\nThe success of Deep Learning (DL) in the early 2010's entailed a disruption to the\nentire field of Artificial Intelligence. The development of affective systems embraced the\nnew trend as well, which was mainly marked by the use of representational learning via"}, {"title": "2 Emergence in Foundation Models", "content": "One of the main characteristics of the Foundation Models (FMs) is that they are\ntrained on a broad range of data, so that the resulting models can be utilised in a wide\nrange of problems. In addition, they exploit large amounts of learning parameters.\nGiven sufficient learning material, from a certain number of such parameters hence well\ntrained, knowledge 'emerges' in the FMs, and they achieve competitive performances\nin tasks they have not been specifically trained for. This can, however, be difficult to\npredict [43]. In this paper, we aim to investigate the \u2018emergent' affective capabilities of\nFMs. Focusing on the vision (cf. Section 2.1), linguistics (cf. Section 2.2), and speech\n(acoustics) (cf. Section 2.3) modalities, we assess the capabilities of current FMs to i)\ngenerate synthetic affective samples, from which we infer the conveyed emotions with\npre-trained emotion recognition classifiers\u00b9, and ii) analyse well-established datasets\nin the field in a zero-shot manner. To favour the comparability among the different\nmodalities investigated, we focus on the 'Big Six' Ekman emotions [44] (i. e., fear,\nanger, happiness, sadness, disgust, and surprise), in addition to the neutral state."}, {"title": "2.1 The Vision Modality Has Changed", "content": "2.1.1 Generation\n\nIn the visual domain, data synthesis started to obtain pseudorealistic results in the last\ndecade thanks to the Generative Adversarial Network- (GAN) based models [25, 30].\nNowadays, a boost in the quality of the synthesised images has been achieved via\ntext prompt inputs-based models, due to i) the CLIP model [38] and ii) the Diffuser\nmodel [45]. The former was presented in conjunction with DALL-E [46], as a model to\npredict how well a given caption describes an image; i. e., a text-to-image alignment."}, {"title": "2.1.2 Analysis", "content": "In order to evaluate the affective analytical capabilities of FMs in the image domain,\nwe explore their performance in a zero-shot emotion recognition task under differ-\nent configurations. Our experiments are conducted on the considered validation set\nof AffectNet [33] (cf. Section 2.1.1), as it is balanced, in-the-wild, and manually anno-\ntated. We compare three different approaches relying on model-prompting. In the first\ntwo, we start by extracting AU-based features with OpenFace [49], and we provide\nthese features in a textual format as input to a FM. Recalling from Section 2.1.1,\nOpenFace predicts both the presence and the intensity of a subset of AUs; hence, twoO\napproaches can be derived from its predictions. On the other hand, the third approach\nconsists in directly feeding the images within the prompt of a FM. Note that the\nfirst two approaches can be addressed with a Language Foundation Model (LFM), for\nwhich we select the LLaMA2 7B model. We utilise a Multimodal Foundation Model\n(MFM) for the third scenario; specifically, the LLaVA1.5 7B model 7 [53, 54]. This is an\nopen-source MFM with visual capabilities trained by fine-tuning Vicunas, an already\nfine-tuned version of LLaMA, with GPT-4 generated data. The selected models allow\nthem having the same number of parameters.\n\nIn Table 7, we present the results of the aforementioned scenarios. The prompts\nutilised for each scenario are detailed in Table 6. We also include in Table 7 the results\nof the ViT - FER model on the considered validation set of AffectNet for comparability"}, {"title": "2.2 The Linguistic Modality Has Changed", "content": "2.2.1 Generation\n\nThe evolution of text generation experienced an important shift in the mid-2010s with\nthe development of neural models [56]. The introduction of the Transformer model [36]\nhas revolutionised the Natural Language Processing (NLP) field and marked the begin-\nning of the Large Language Models (LLM) era. Since then, this architecture has been"}, {"title": "2.2.2 Analysis", "content": "In this section, we analyse the zero-shot sentiment analysis capabilities of the follow-\ning LLMs: Mistral, Mixtral, and two versions of LLaMA2 (7 billion and 13 billion\nparameters). We assess their zero-shot capabilities on the test partition of the GoE-\nmotions dataset. We design a prompt that requires the selected LLMs to predict the\ncorresponding emotion, including the neutral state (cf. Table 11). Prompt engineering\nis crucial for influencing LLMs, as it enhances nuanced responses and ensures more\naccurate behaviour [71]. To minimise randomness and increase confidence in the pre-\ndictions, we reduce the temperature setting to 0.1. This lower temperature sharpens\nthe probability distribution, ensuring that the predicted classes reflect those that the\nLLMs are predicting with the highest probabilities [69]. However, the LLM outputs\nsometimes include irrelevant or multiple emotions. To address this without intrusively\naltering the model's outputs, we select the first listed emotion as the most reliable pre-\ndiction. This approach aligns with the operational principle of decoder-based models,\nwhere the first valid emotion is mathematically the one with the highest confidence\nscore, thus considered the valid class prediction.\n\nTable 12 summarises the comparative performance of the tested LLMs. We include\nthe performance of the RoBERTa baseline model trained on the GoEmotions dataset\n(cf. Section 2.2.1) for benchmarking purposes. The first observation from the obtained\nresults is that the UAR scores obtained by all the investigated LLMs surpass the chance\nlevel (14.3%), underscoring the emergent affective capabilities of the LLMs tested in\na zero-shot manner. Nevertheless, none of the LLMs outperforms the RoBERTa base-\nline model, fine-tuned on the GoEmotions dataset, which confirms the advantage of\nmodel-specific tuning. Nevertheless, it is worth highlighting that the difference in the\nUAR scores obtained by the best-performing GPT-3.5 and GPT-4 models in compar-\nison to the ROBERTa baseline model is around 15%. This is an interesting result, as\nthese models have not been trained on the GoEmotions dataset, but still obtained a"}, {"title": "2.3 The Speech Modality Has (Not Yet) Changed", "content": "2.3.1 Generation\n\nResearch on generating affective speech has been conducted for more than three\ndecades [72]. The first approaches were rule-based, while contemporary methods typ-\nically rely on deep learning. A detailed overview can be found in [72]. All of these\nmethods are explicitly engineered to produce emotional speech. Thus, this line of\nresearch can be dubbed as a subfield of 'traditional' Affective Computing, while tech-\nnically belonging to the Text-To-Speech (TTS) field. In recent years, similar to the\ndevelopments in Natural Language Processing (NLP) and Computer Vision (CV),\nresearch on TTS has heavily been influenced by the success of the Foundation Model\n(FM) paradigm [73, 74].\n\nUniAudio [73] is a Transformer-based general-purpose audio synthesis system. It\nis pretrained on seven generative audio tasks, including TTS. In its pretrained state,\nhowever, it does not support affective speech synthesis. The authors demonstrate that"}, {"title": "2.3.2 Analysis", "content": "A system capable of analysing arbitrary affective properties of speech data without\nany tuning must ingest both audio and text inputs. Several FM approaches fulfilling\nthis requirement have been proposed. However, the vast majority of them are not\npretrained on speech data at all.\n\nExamples include AnyMAL [77], X-InstructBLIP [78], and ModaVerse [79]. In\nonly a few models, speech is part of the pretraining data. QWEN-Audio's training\ndata comprises several labelled speech datasets, including emotionality already. Hence,\nQWEN-Audio [80] in the proposed form is not a candidate for exploring 'emergent'\naffective recognition capabilities. X-LLM [81] processes video, text, and audio inputs\nand is explicitly designed to process speech. The authors, however, do not report any\nexperiments related to predicting affect in speech. As of now, the pretrained X-LLM\nmodel is not publicly available, hence, unfortunately again, not allowing us to carry\nout experiments analogous to the vision and the linguistic ones.\n\nSimilar to the affective speech synthesis problem, near-future multimodal FMs can\nbe expected to be capable of analysing affective speech in a zero-shot manner, even if"}, {"title": "2.4 The Evaluation Is Changing", "content": "One of the reasons to understand the impressive performance of currently available\nFoundation Models (FM) is that they use massive amounts of data from \"the Internet\"\nfor training. Nevertheless, the indiscriminate use of data poses the following challenge\nto the scientific community: can we guarantee that the data we feed to these models for\ntesting or even for evaluation \u2013 has not been used for training? In case of a negative\nanswer, how fair and representative of the model capabilities can standard evaluation\nmetrics be? Although we do not have a concrete answer yet, we hope these challenges\nengage the research community into looking for methods and metrics that allow a\nproper scientific evaluation of these emerging FMs in the field of Affective Computing.\nAs is, the current state of such models in Affective Computing may resemble a shell\ngame: many different tools and approaches are shuffled and mixed until some partially\nless, partially more convincing performances are obtained. Especially because it is the\npopular 'Big Six' Ekman emotions we considered herein, chances are high that the\nmodels only reverberate with what they have already seen. Testing on more subtle\nmodels such as the dimensional approach or less considered affective states is therefore\nurgently needed."}, {"title": "3 Concerns and Regulations Have Changed", "content": "In April 2021, the European Commission presented the AI Act: the first-ever legal\nframework worldwide to regulate the use of AI-based technologies in the European\nterritory. The Act was endorsed by all Member States in February 2024 after being\napproved by the EU Council. The regulation follows a risk-based approach, so instead\nof regulating specific systems and applications, it defines measures and requirements\nbased on a classification system that encapsulates varying degrees of risks posed by\nthe AI systems. The proposed classification system spans four different levels of risk:\nunacceptable, high, limited, and minimal. According to the final version of the Act14,\nsystems that fall into the unacceptable risk category will be prohibited.\n\nThis regulation is of special interest for the Affective Computing community, since\nit singles out affective systems in several ways. The regulation defines an emotion\nrecognition system in its Article 3 (34) as an \u201cAI system for the purpose of identifying\nor inferring emotions or intentions of natural persons on the basis of their biometric\ndata\". Thus, any emotion recognition system using speech or facial data, as well as\nother physical signals such as the electroencephalogram, falls under this definition.\nArticle 5 enumerates different practices and systems considered as prohibited, includ-\ning the placing on the market or the use of AI systems to infer emotions of a natural\nperson in the workplace or in education institutions. Whilst this point only prohibits\nemotion recognition systems in two specific contexts (workplace and education), the\nlist of high-risk systems presented in Annex III includes \"AI systems intended to be\nused for emotion recognition\" in the category of biometric systems. Article 6 provides"}, {"title": "4 Outlook and Conclusions", "content": "In this paper, we analysed the affective capabilities of currently available Founda-\ntion Models (FM) exploiting the vision, the linguistics, and the speech (acoustic)\nmodalities. While the affective generation and analysis capabilities of the vision-\nand the linguistic-based FMs are plausible, the affective generation and analysis of\nspeech-based FMs is not yet mature enough. Nonetheless, it is reasonable to imag-\nine a not-too-distant future where this technology achieves similar results as with\nthe other two modalities. Despite not being currently available, we also envision\nphysiological-based FMs to be developed and explored in the near future.\n\nOne of the main outcomes of this work is the collection of two synthetically-\ngenerated affective corpora generated with FMs one containing facial images, the\nother textual sentences that will be publicly available. The models training and the\nanalyses reported herein were performed assuming that the synthetically generated\ninstances conveyed the prompted emotions. Nonetheless, we acknowledge this could\nnot always be the case. To overcome this limitation, we plan to run a data collec-\ntion with human annotators to annotate the generated samples, assessing the affective\ncapabilities of the selected FMs from a human perspective."}]}