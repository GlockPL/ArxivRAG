{"title": "Towards Evaluating Large Language Models on Sarcasm Understanding", "authors": ["Yazhou Zhang", "Chunwang Zou", "Zheng Lian", "Prayag Tiwari", "Jing Qin"], "abstract": "In the era of large language models (LLMs), the task of \"System I\" - the fast, unconscious, and intuitive tasks, e.g., sentiment analysis, text classification, etc., have been argued to be successfully solved. However, sarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions, involving a higher level of abstraction than sentiment analysis. There is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evalua- tions on six widely used benchmark datasets through different prompting approaches, i.e., zero-shot input/output (IO) prompting, few-shot IO prompting, chain of thought (CoT) prompting. Our results highlight three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0%\u2191. Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks.", "sections": [{"title": "1. Introduction", "content": "Recent large language models (LLMs) have demon- strated outstanding instruction-following and in context learning abilities across various natural language pro- cessing (NLP) tasks, such as question answering [1], sentiment analysis [2], text classification [3], etc. In the era of LLMs, it has been argued that \"System I\" tasks - the fast, unconscious, and intuitive tasks have been suc- cessfully solved [4]. Persistent focus and efforts from both the academic and industrial sectors have primar- ily concentrated on what is known as \"System II\" tasks. Such tasks demand slow, deliberate, and multiple-step cognitive processes, including logical, mathematical, and commonsense reasoning [5]. Sarcasm is a subtle linguistic phenomenon that em- ploys rhetorical devices like hyperbole and figuration to convey true sentiments and intentions that are oppo- site to the literal meanings of the words used [6]. Hu- mans might say something that sounds positive on the surface, but in reality, they are expressing a negative sentiment. For example, the sentence \u201cI like to be rep- rimanded.\u201d appears to convey a positive sentiment be- cause it includes the word \"like\", which typically signi- fies positive emotion. However, \u201creprimanded\" gener- ally implies criticism and negative feedback. Sarcasm detection aims to determine whether a given text is sar- castic or non-sarcastic by leveraging different types of information, such as linguistic features, contextual in- formation, etc [7]. Due to its inherently ambiguous and metaphorical nature, sarcasm detection has consis- tently presented significant challenges, evolving from the era of feature engineering to that of prompt engi- neering [8, 9]. There is growing concern that the argument about LLMs' success may not be fully tenable when con-"}, {"title": "2. Related Work", "content": "This section reviews two lines of research that form the basis of this work: CoT prompting and sarcasm de- tection."}, {"title": "2.1. Sarcasm Detection", "content": "Sarcasm detection is habitually treated as a text clas- sification task, where the target is to identify whether the given text is sarcastic or not [16]. It has evolved from early rule based and statistical learning based ap- proaches to traditional neural methods, such as CNN, RNN, and further advanced to modern neural methods epitomized by Transformer models. In early stage, the rule based approaches infer the overall sarcasm polar- ity based on the refined sarcasm rules, such as the oc- currence of the interjection word [9]. Statistical learn- ing based approaches mainly employ statistical learning techniques, e.g., SVM, RF, NB, etc., to extract patterns and relationships within the data [17]. As deep learning based architectures have shown the superiority over statistical learning, numerous base neu- ral networks, e.g., such as CNN [18], LSTM [19], GCN [20], etc., have been predominantly utilized dur- ing the middle stage of sarcasm detection research, aim- ing to learn and extract complex features in an end-to- end fashion. As the field of deep learning continues to evolve, sarcasm detection research has stepped into the era of pre-trained language models (PLMs). An increas- ing number of researchers are designing sophisticated PLM architectures to serve as encoders for obtaining effective text representations. For example, Liu et al."}, {"title": "2.2. Large Language Models", "content": "LLMs are advanced language models characterized by their substantial parameter sizes and exceptional learning capabilities [25]. In recent years, the com- munity of natural language processing (NLP) has wit- nessed substantial progress largely due to the develop- ment of large language models (LLMs). These mod- els excel in areas such as in-context learning, few-shot prompting, and following complex instructions. Ope- nAI has been at the forefront of this innovation, notably with the introduction of transformative models such as ChatGPT and GPT-4. Nevertheless, the exclusive nature of these technologies has led to the emergence of vari- ous LLM versions, which often incorporate tens or even hundreds of billions of parameters [26]. We categorize these LLMs into two groups based on their specializa- tion: general LLMs and specialized LLMs. General LLMs are designed for versatility across a wide spectrum of NLP tasks. Prominent examples of these models are GPT-4, ChatGLM 4, LLaMA 3, PanGu-\u03a3 [27], Baichuan 34, etc. Such LLMs often perform well across a range of tasks, but their poten- tials in specific domains await further explore. In con- trast, specialized LLMs are fine-tuned for specific tasks via task-specific architectures and knowledge, allow- ing them to achieve higher performance. For example, Zhang et al. proposed a fine-tuned context and emo- tion knowledge tuned LLM for emotion recognition in conversations [2]. They also presented RGPT, an adap- tive boosting framework tailored to produce a special- ized text classification LLM by recurrently ensembling"}, {"title": "3. The Proposed Approach", "content": ""}, {"title": "3.1. Task Definition", "content": "Sarcasm detection is transformed as a conditional generative task, where the output y will be the labels. Given a set of input texts X = {X1,X2,...,xN} where each document xi is augmented with a designed prompt Prompti \u2208 P that provides contextual guidance, i.e., Prompti = INS\u00a1 \u2295 xi, where INS\u00a1 represents the task instruction, P represents the prompt set. Our task is to let a LLM M map an input document to its target label: M(X,P,0) \u2192 Y, where Y = {y1, y2, ..., yn} denotes the label sequence generated by the LLM M We formu- late the problem as:\nM = arg max \u041f\u00a1 Prob (yi = c|xi, Prompti, 0)  (1)\nC"}, {"title": "3.2. Evaluation Approach", "content": "Fig. 2 shows the overview of our proposed evaluation framework. Initially, we take both sarcastic and non- sarcastic samples as inputs. Subsequently, we design prompts tailored specifically for sarcasm. This involves constructing task instructions and providing contextual demonstrations. Our evaluation utilizes three distinct prompting approaches: zero-shot IO prompting, few- shot IO prompting, and CoT prompting. These meth- ods are implemented to assess how effectively LLMs generate responses that are both valid and rigorously aligned with the subtleties of sarcastic expressions. Fi- nally, we analyze and compare the outputs generated by the LLMs against the ground truth to evaluate the mod- els' proficiency in understanding and detecting sarcasm."}, {"title": "3.3. Prompt Construction", "content": "Our prompt Prompt\u00a1 contains of three key compo- nents: (1) Task instruction. It clearly defines the specific sarcasm detection task the model needs to accomplish. In this work, the task instruction is given as follows: This is a sarcasm classification task. Determine whether the following input text expresses sarcasm,"}, {"title": "3.4. Three Prompting Approaches", "content": "Zero-shot Input-Output Prompting. Zero-shot IO prompting refers to the scenario where a language model (LM) generates the output y based purely on the input x, without any additional examples or guidance. Mathematically, this can be represented as:\ny ~ poly | x),\nwhere pe represents the pre-trained model, and x is di- rectly passed to the model to generate y. The model must rely solely on its prior knowledge and understand- ing of the task.\nFew-shot Input-Output Prompting. Few-shot IO prompting involves providing the language model with a limited number of examples or demonstrations of the"}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experiment Setups", "content": "Datasets. Six benchmarking datasets are selected as the experimental beds which encompass the most sig- nificant datasets used in sarcasm detection tasks. They are IAC-V1 [33], IAC-V2 [34], Ghosh [35], iSarcas- mEval [36], Riloff [37] and SemEval 2018 Task 3 [38]. IAC-V1 and IAC-V2 are from the Internet Argument Corpus (IAC) [39], specifically designed for the task of identifying and analyzing sarcastic remarks within on- line debates and discussions. It encompasses a balanced mixture of sarcastic and non-sarcastic comments."}, {"title": "4.2. Compared Baselines", "content": "A wide range of strong baselines are included for comparison including traditional deep learning mod- els (DLMs), pre-trained language models (PLMs) and LLMs. They are:\n\u2022 Random:\n(1) Random This baseline makes random predic- tions sampled uniformly across the test set.\n\u2022 DLMs based approaches:\n(2) TextCNN [40] designs a CNN [40] with three convolutional layers and a fully connected layer. It is trained on top of word embeddings for utterance- level classification.\n(3) LSTM takes word embeddings as input and take the hidden representation of each word for sar- casm classification.\n(4) bi-LSTM [41] implements an bidirectional LSTM to learn both forward and backward long- range dependency information.\n(5) AT-LSTM [42]: is an attention-based LSTM with aspect embedding. We obtain aspect embed- dings by averaging the vectors of words, and ap- pend it with each word embedding vector.\n\u2022 PLMs based approaches:"}, {"title": "4.3. Results and Analysis of Zero-Shot IO Prompting", "content": "We report all of the Accuracy, Precision, Recall and macro F1 scores for 21 models in Table 2. We highlight three key observations:\n(1) In the zero-shot IO setting, PLMs demon- strate outstanding performance, significantly sur- passing traditional deep learning methods. For in- stance, ROBERTa achieves an average F1 score of 71.3"}, {"title": "4.4. Few-Shot v/s Zero-Shot", "content": "Since the above experiments are mainly based on a zero-shot IO setting, we are curious of whether the con- clusions also apply in a few-shot scenario. Therefore, we perform few-shot experiments to evaluate whether the LLMs can perform better when a limited number of contextual examples are available. We show the main results in Table 3 and Fig. 5, we sample k = 2 examples (including one sarcastic example and one non-sarcastic exmaple) from the training set via KNN search. We can make two main observations. (1) In the few-shot IO prompting setting, GPT-4 consistently out- performs ROBERTa and DC-Net-RoBERTa on three datasets: IAC-V1, SemEval 2018 Task 3, and Ghosh. Specifically, GPT-4 achieves an F1 score of 79.6 on IAC-V1, which is significantly higher than RoBERTa's 69.9 and DC-Net-ROBERTa's 69.1. On SemEval 2018 Task 3, GPT-4 reaches an F1 score of 68.3, surpassing"}, {"title": "4.5. Impact of Number of Demonstrations", "content": "In few-shot settings, we investigate the impact of the number of demonstrations for the state-of-the-art LLM, namely whether GPT-4 could perform better or not if more contextual examples are provided. We design five k-shot settings: zero-shot, two-shot, four-shot, six-shot, eight-shot. For each setting, we selectly sample k = {0, 2, 4, 6, 8} examples that are similar to the test sample using kNN sampling algorithm. The results are shown in Fig. 4. As we can see, the number of demonstrations (k-shot) has a significant impact on the F1 scores for both Mis- tral and GPT-4 in the sarcasm classification task. For instance, Mistral shows an overall upward trend in F1 scores from 0-shot to 8-shot, increasing from 51.6 to 56.2, representing a total improvement of 8.7%. The most significant performance boost occurs at 2-shot, reaching 55.8, which is an 8.2% increase compared to O-shot. This shows that Mistral has greater sensitivity"}, {"title": "4.6. Chain of Thought Results", "content": "In few-shot settings, we investigate a widely used prompting approach, CoT. The experimental results are shown in Table 4 and Fig. 6. Despite that CoT prompting has significant advantage over standard IO"}, {"title": "4.7. Error Analysis", "content": "The detailed error analysis is also conducted via the confusion matrices that are shown in Fig. 8, Fig. 9 and Fig. 10. Each cell (i, j) represents the percentage of class i is classified to be class j. Upon reviewing the classification results produced by GPT-4 Turbo on six datasets with three prompting approaches, we discover that imbalanced categories and the sarcastic samples are the key factors contributing to misclassification. In zero-shot IO setting, GPT-4 Turbo performs well in identifying non-sarcastic instances, particularly on datasets like IAC-V2 and SemEval Task 3, where the true positive rates for non-sarcastic labels are very high. However, the model struggles more with distinguish- ing sarcastic instances, especially on datasets like IAC-"}, {"title": "4.8. Case Study", "content": "Table 5 presents several examples where three repre- sentative LLMs made predictions. We conduct a brief analysis of the reasons behind these prediction errors. (1) Contextual misunderstanding. GPT-4 struggled with texts where sarcasm was deeply tied to contextual or cultural nuances. For instance, in Example 3 (\u201cBe- ing half Spanish and not being able to speak Spanish is honestly so disappointing\u201d), the sarcasm lies in the speaker's ironic expression of disappointment. GPT-4 misclassified this as non-sarcastic, possibly due to its difficulty in detecting sarcasm embedded in personal or cultural identity issues. Similarly, in Example 4 (\u201cI lit- erally just ate 1/4 of a pan of brownies ... #damnit #no- tagain\u201d), the self-deprecating tone and hashtags signal sarcasm, which GPT-4 failed to recognize."}, {"title": "4.9. Evaluating LLMs on Multi-Modal Sarcasm Under- standing", "content": "Due to the rapid surge of multimodal data on social networks, leveraging multi-modal information to en- hance human language understanding and reasoning has become increasingly attractive and significant. Beyond text-based LLMs, multi-modal LLMs typically incor- porate modality encoders, connectors, and generators. Through modality-aligned pre-training, these models acquire the ability to process multimodal information. Therefore, in this section, we aim to investigate their capabilities for understanding multi-modal sarcasm. More specifically, we select one popular and pub- licly accessible multi-modal sarcasm detection dataset, namely, MMSD [48], as our experimental platform. We evaluate one state-of-the-art multi-modal LLM: LLaVA 1.5 [49]. Given that CoT prompting is less effective in addressing sarcasm understanding tasks, we employ only the zero-shot IO prompting method. Addition- ally, we present four state-of-the-art multi-modal sar- casm detection baselines for comparison. The results are shown in Table 6. We can notice that LLaVA 1.5 significantly outper- forms other models in multi-modal sarcasm detection across all metrics-accuracy, precision, recall, and F1 score. This superior performance underscores LLaVA 1.5's enhanced capability to integrate and interpret vi- sual and textual cues effectively. The comparative anal- ysis reveals that while traditional multi-modal models are effective, the advanced training methodologies and deeper contextual understanding in LLaVA 1.5 provide a more refined analysis of sarcasm."}, {"title": "5. Conclusion", "content": "In the era of large language models (LLMs), there is growing concern that LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs)"}]}