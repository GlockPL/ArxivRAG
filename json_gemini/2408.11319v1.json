{"title": "Towards Evaluating Large Language Models on Sarcasm Understanding", "authors": ["Yazhou Zhang", "Chunwang Zou", "Zheng Lian", "Prayag Tiwari", "Jing Qin"], "abstract": "In the era of large language models (LLMs), the task of \"System I\" - the fast, unconscious, and intuitive tasks, e.g., sentiment analysis, text classification, etc., have been argued to be successfully solved. However, sarcasm, as a subtle linguistic phenomenon, often employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions, involving a higher level of abstraction than sentiment analysis. There is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evaluations on six widely used benchmark datasets through different prompting approaches, i.e., zero-shot input/output (IO) prompting, few-shot IO prompting, chain of thought (CoT) prompting. Our results highlight three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0%\u2191. Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks.", "sections": [{"title": "1. Introduction", "content": "Recent large language models (LLMs) have demonstrated outstanding instruction-following and in context learning abilities across various natural language processing (NLP) tasks, such as question answering [1], sentiment analysis [2], text classification [3], etc. In the era of LLMs, it has been argued that \"System I\" tasks - the fast, unconscious, and intuitive tasks have been successfully solved [4]. Persistent focus and efforts from both the academic and industrial sectors have primarily concentrated on what is known as \"System II\" tasks. Such tasks demand slow, deliberate, and multiple-step cognitive processes, including logical, mathematical, and commonsense reasoning [5].\nSarcasm is a subtle linguistic phenomenon that employs rhetorical devices like hyperbole and figuration to convey true sentiments and intentions that are opposite to the literal meanings of the words used [6]. Humans might say something that sounds positive on the surface, but in reality, they are expressing a negative sentiment. For example, the sentence \u201cI like to be reprimanded.\" appears to convey a positive sentiment because it includes the word \"like\", which typically signifies positive emotion. However, \u201creprimanded\" generally implies criticism and negative feedback. Sarcasm detection aims to determine whether a given text is sarcastic or non-sarcastic by leveraging different types of information, such as linguistic features, contextual information, etc [7]. Due to its inherently ambiguous and metaphorical nature, sarcasm detection has consistently presented significant challenges, evolving from the era of feature engineering to that of prompt engineering [8, 9].\nThere is growing concern that the argument about LLMs' success may not be fully tenable when considering sarcasm understanding, as it involves a higher level of abstraction than sentiment analysis, as shown in Fig. 1. Hence, the main research question can be written as:\nRQ: Have LLMs really made significant progress in understanding sarcasm?\nTo answer this question, we propose an evaluation framework that consists of various combinations of LLMs and prompting approaches, systematically assessing the performance of LLMs on sarcasm detection. In particular, we select eleven SOTA LLMs, including ChatGPT, GPT-4 [10], Claude 3, Mistral [11], Baichuan 2/3 [12], ChatGLM 2/3 [13], LLaMA 2/3 [14], Qwen 1.5 [15], and eight PLMs to evaluate their performance on six benchmarks using three popular prompting methods: zero-shot IO prompting, few-shot IO prompting, and CoT prompting.\nThe experimental results highlight three key findings: (1) Current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 consistently and significantly outperforms other LLMs across various prompting methods, with an average improvement of 14.0%\u2191. Claude 3 and ChatGPT demonstrate the next best performance after GPT-4. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT, with an average improvement of 4.5%\u2191. The reason is that sarcasm detection, being a holistic, intuitive, and non-rational cognitive process, is argued not to adhere to step-by-step logical reasoning, making CoT less effective in understanding sarcasm compared to its effectiveness in mathematical reasoning tasks."}, {"title": "2. Related Work", "content": "This section reviews two lines of research that form the basis of this work: CoT prompting and sarcasm detection."}, {"title": "2.1. Sarcasm Detection", "content": "Sarcasm detection is habitually treated as a text classification task, where the target is to identify whether the given text is sarcastic or not [16]. It has evolved from early rule based and statistical learning based approaches to traditional neural methods, such as CNN, RNN, and further advanced to modern neural methods epitomized by Transformer models. In early stage, the rule based approaches infer the overall sarcasm polarity based on the refined sarcasm rules, such as the occurrence of the interjection word [9]. Statistical learning based approaches mainly employ statistical learning techniques, e.g., SVM, RF, NB, etc., to extract patterns and relationships within the data [17].\nAs deep learning based architectures have shown the superiority over statistical learning, numerous base neural networks, e.g., such as CNN [18], LSTM [19], GCN [20], etc., have been predominantly utilized during the middle stage of sarcasm detection research, aiming to learn and extract complex features in an end-to-end fashion. As the field of deep learning continues to evolve, sarcasm detection research has stepped into the era of pre-trained language models (PLMs). An increasing number of researchers are designing sophisticated PLM architectures to serve as encoders for obtaining effective text representations. For example, Liu et al."}, {"title": "2.2. Large Language Models", "content": "LLMs are advanced language models characterized by their substantial parameter sizes and exceptional learning capabilities [25]. In recent years, the community of natural language processing (NLP) has witnessed substantial progress largely due to the development of large language models (LLMs). These models excel in areas such as in-context learning, few-shot prompting, and following complex instructions. OpenAI has been at the forefront of this innovation, notably with the introduction of transformative models such as ChatGPT and GPT-4. Nevertheless, the exclusive nature of these technologies has led to the emergence of various LLM versions, which often incorporate tens or even hundreds of billions of parameters [26]. We categorize these LLMs into two groups based on their specialization: general LLMs and specialized LLMs.\nGeneral LLMs are designed for versatility across a wide spectrum of NLP tasks. Prominent examples of these models are GPT-4, ChatGLM 4, LLaMA 3, PanGu-\u03a3 [27], Baichuan 3, etc. Such LLMs often perform well across a range of tasks, but their potentials in specific domains await further explore. In contrast, specialized LLMs are fine-tuned for specific tasks via task-specific architectures and knowledge, allowing them to achieve higher performance. For example, Zhang et al. proposed a fine-tuned context and emotion knowledge tuned LLM for emotion recognition in conversations [2]. They also presented RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling"}, {"title": "3. The Proposed Approach", "content": ""}, {"title": "3.1. Task Definition", "content": "Sarcasm detection is transformed as a conditional generative task, where the output y will be the labels. Given a set of input texts X = {X1,X2,...,xN} where each document xi is augmented with a designed prompt Prompti \u2208 P that provides contextual guidance, i.e., Prompti = INS\u00a1 \u2295 xi, where INS\u00a1 represents the task instruction, P represents the prompt set. Our task is to let a LLM M map an input document to its target label: \u041c(\u0425,\u0420,0) \u2192 \u0423, where \u0423 = {y1, y2, ..., yn} denotes the label sequence generated by the LLM M We formulate the problem as:\nM = arg max \u041f\u00a1 Prob (yi = c|xi, Prompt, 0) (1)\nC"}, {"title": "3.2. Evaluation Approach", "content": "Fig. 2 shows the overview of our proposed evaluation framework. Initially, we take both sarcastic and non-sarcastic samples as inputs. Subsequently, we design prompts tailored specifically for sarcasm. This involves constructing task instructions and providing contextual demonstrations. Our evaluation utilizes three distinct prompting approaches: zero-shot IO prompting, few-shot IO prompting, and CoT prompting. These methods are implemented to assess how effectively LLMs generate responses that are both valid and rigorously aligned with the subtleties of sarcastic expressions. Finally, we analyze and compare the outputs generated by the LLMs against the ground truth to evaluate the models' proficiency in understanding and detecting sarcasm."}, {"title": "3.3. Prompt Construction", "content": "Our prompt Prompti contains of three key components:\n(1) Task instruction. It clearly defines the specific sarcasm detection task the model needs to accomplish. In this work, the task instruction is given as follows:\nThis is a sarcasm classification task. Determine whether the following input text expresses sarcasm, if it does, output 'sarcastic', otherwise, output 'non-sarcastic'. Return the label only without any other text.\n(2) Input. The input is every testing sample xi to classify.\n(3) Demonstration. It provides specific examples of how to analyze text for sarcasm, showing both sarcastic and non-sarcastic responses to similar situations. It will guide the LLMs in recognizing subtle cues and patterns that identiy sarcasm. In addition, it also provides an output format that LLM's outputs should to follow. Note that demonstrations are only needed for the few-shot setup, but not for the zero-shot setup.\nWe adopt KNN search [32] to sample examples that are similar to the test sequence. In this method, the text initially undergoes transformation into a vector via the encoding function f. This vector is then employed as a query to traverse the complete training dataset, aiming to identify the k text sequences closest to it. From these, we extract the k most similar data examples to use as demonstrations."}, {"title": "3.4. Three Prompting Approaches", "content": "Zero-shot Input-Output Prompting. Zero-shot IO prompting refers to the scenario where a language model (LM) generates the output y based purely on the input x, without any additional examples or guidance. Mathematically, this can be represented as:\ny ~ poly | x),\nwhere pe represents the pre-trained model, and x is directly passed to the model to generate y. The model must rely solely on its prior knowledge and understanding of the task.\nFew-shot Input-Output Prompting. Few-shot IO prompting involves providing the language model with a limited number of examples or demonstrations of the task before predicting the output for a new input x. This can be formalized as:\ny~ poly | prompt10(x)),\nwhere promptio(x) embeds input x within a context of task instructions and few-shot examples. In this approach, pe adapts based on the provided examples, which helps guide the model in generating more accurate outputs.\nChain-of-Thought Prompting. Few-shot COT prompting extends the idea of few-shot prompting by explicitly modeling intermediate reasoning steps between the input x and the final output y. The language model is guided through a sequence of logical steps Z1, Z2,..., Zn, leading to the final answer. This process is described as:\n[Z1, Z2,..., Zn, y] ~ PCoTo(Z1, Z2, ..., Zn, y | X),\nwhere each zi represents a coherent reasoning step. By incorporating these intermediate steps, the model is better able to handle complex tasks, such as solving multi-step problems or answering detailed questions."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experiment Setups", "content": "Datasets. Six benchmarking datasets are selected as the experimental beds which encompass the most significant datasets used in sarcasm detection tasks. They are IAC-V1 [33], IAC-V2 [34], Ghosh [35], iSarcasmEval [36], Riloff [37] and SemEval 2018 Task 3 [38]. IAC-V1 and IAC-V2 are from the Internet Argument Corpus (IAC) [39], specifically designed for the task of identifying and analyzing sarcastic remarks within online debates and discussions. It encompasses a balanced mixture of sarcastic and non-sarcastic comments.\nGhosh consists of 51,189 tweets (24,453 sarcastic tweets and 26,736 non-sarcastic tweets) in which sarcastic tweets are automatically collected from Twitter using user's self-declaration of sarcasm/irony with sarcastic and ironic hashtags (e.g. #irony, #sarcasm). In this work, we have conducted a thorough double-check of this dataset and successfully filtered out 7,804 noisy tweets.\niSarcasmEval is the first shared task to target intended sarcasm detection. Each sample in this dataset is provided and labelled by the authors of the texts themselves. For sarcastic texts, there is a rephrase that conveys the same message non-sarcastically. For English sarcastic texts, there is a label specifying the category of ironic speech that it reflects.\nRiloff collects a set of 1,600 tweets that contain #sarcasm or #sarcastic, and another 1,600 without these tags. It chooses to remove such tags from all tweets and present the tweets to a group of human annotators for final labelling.\nSemEval 2018 Task 3 is collected using irony-related hashtags (i.e. #irony, #sarcasm, #not) and are subsequently manually annotated to minimise the amount of noise in the corpuses. It emphasize the challenges inherent in identifying sarcasm within the constraints of MUSTARD's concise format, and highlight the importance of context and linguistic subtleties in recognizing sarcasm. The statistics for each dataset are shown in Table 1.\nEvaluation metrics. To make a fair comparison, we employ precision (P), recall (R), accuracy (Acc) and F1 as evaluation metrics. For each method, we run five random seeds and report the average result of the test sets. The metrics are written as:\nP = \\frac{TP}{TP + FP}\nR = \\frac{TP}{TP + FN}\nF1 = \\frac{2*P*R}{P+R}\nAcc = \\frac{TP + TN}{TP + FN + FP + TN} (2)\nwhere TP (True Positives) is the number of positive samples correctly identified, FP (False Positives) is the number of negative samples incorrectly labeled as positive, FN (False Negatives) is the number of positive samples incorrectly labeled as negative, TN (True Negatives) is the number of negative samples correctly identified.\nImplementation details. We use the official implementations and hyper-parameters for all DLMs and"}, {"title": "4.2. Compared Baselines", "content": "A wide range of strong baselines are included for comparison including traditional deep learning models (DLMs), pre-trained language models (PLMs) and LLMs. They are:\n\u2022 Random:\n(1) Random This baseline makes random predictions sampled uniformly across the test set.\n\u2022 DLMs based approaches:\n(2) TextCNN [40] designs a CNN [40] with three convolutional layers and a fully connected layer. It is trained on top of word embeddings for utterance-level classification.\n(3) LSTM takes word embeddings as input and take the hidden representation of each word for sarcasm classification.\n(4) bi-LSTM [41] implements an bidirectional LSTM to learn both forward and backward long-range dependency information.\n(5) AT-LSTM [42]: is an attention-based LSTM with aspect embedding. We obtain aspect embeddings by averaging the vectors of words, and append it with each word embedding vector.\n\u2022 PLMs based approaches:"}, {"title": "4.3. Results and Analysis of Zero-Shot IO Prompting", "content": "We report all of the Accuracy, Precision, Recall and macro F1 scores for 21 models in Table 2. We highlight three key observations:\n(1) In the zero-shot IO setting, PLMs demonstrate outstanding performance, significantly surpassing traditional deep learning methods. For instance, ROBERTa achieves an average F1 score of 71.3 across all datasets, while DC-Net-RoBERTa reaches 71.5. In contrast, traditional models like Bi-LSTM and AT-LSTM score lower, with F1 scores of 66.7 and 65.3, respectively. PLMs outperform traditional methods by approximately 7.8%\u2191. This superiority stems from the extensive pretraining that PLMs undergo on large-scale data, enabling them to better capture contextual information and adapt to various tasks. Although traditional deep learning models perform adequately on specific tasks, their lack of large-scale pretraining support limits their generalization capabilities, resulting in weaker performance.\n(2) Despite that current Large Language Models have not yet fully outperformed PLM-based supervised learning methods, their potential is evident in specific tasks. For example, GPT-4 Turbo excels in the IAC-V1 and SemEval Task 3 datasets, achieving F1 scores of 78.7 and 76.5, respectively, significantly outperforming RoBERTa's scores of 69.9 and 72.0, with the margin of 8.8% and 4.5%\u2191. However, LLMs fall short in datasets like iSarcasmEval and Riloff, where their performance does not meet expectations. The average F1 score shows that GPT-4 Turbo's difference from ROBERTa in the IAC-V2 and Ghosh datasets is relatively small. This performance variation may be due to LLMs' advantage in handling long texts and complex semantics, but their generalization capabilities are weaker when facing domain-specific or fine-grained tasks without specialized tuning. Therefore, while LLMs show strong potential in some tasks, their consistency and generalization across different tasks still need improvement.\n(3) Among all 11 large language models, GPT-4 Turbo demonstrates a comprehensive lead. It performs exceptionally well across multiple datasets, particularly in IAC-V1 and SemEval Task 3, where it achieves F1 scores of 78.7 and 76.5, significantly higher than Claude-3-haiku's 77.0 and 57.9. Additionally, in the Ghosh dataset, GPT-4 Turbo reaches an F1 score of 82.2, far surpassing other models like ChatGPT (71.4), Claude-3-haiku (71.8) and LLaMA 3-8B (66.8). A significant reason for GPT-4's superiority over other models is its larger parameter size, which enhances its human language comprehension and reasoning capabilities. In contrast, most other models are smaller, with around 7B/8B parameters, which limits their ability to handle complex tasks. Moreover, GPT-4's ability to adapt to various data types and text structures across different tasks further contributes to its outstanding performance. Thus, GPT-4's strong results underscore its leading position among LLMs and its powerful generalization capabilities."}, {"title": "4.4. Few-Shot v/s Zero-Shot", "content": "Since the above experiments are mainly based on a zero-shot IO setting, we are curious of whether the conclusions also apply in a few-shot scenario. Therefore, we perform few-shot experiments to evaluate whether the LLMs can perform better when a limited number of contextual examples are available. We show the main results in Table 3 and Fig. 5, we sample k = 2 examples (including one sarcastic example and one non-sarcastic exmaple) from the training set via KNN search.\nWe can make two main observations. (1) In the few-shot IO prompting setting, GPT-4 consistently outperforms ROBERTa and DC-Net-RoBERTa on three datasets: IAC-V1, SemEval 2018 Task 3, and Ghosh. Specifically, GPT-4 achieves an F1 score of 79.6 on IAC-V1, which is significantly higher than RoBERTa's 69.9 and DC-Net-RoBERTa's 69.1. On SemEval 2018 Task 3, GPT-4 reaches an F1 score of 68.3, surpassing ROBERTa's 64.0 and DC-Net-ROBERTa's 67.8. Compared to the zero-shot setting, GPT-4 shows improvement on the remaining three datasets, indicating that with the benefit of a few examples, GPT-4 significantly enhances its performance. For example, on the iSarcasmEval dataset, GPT-4's F1 score increases from 39.8 in zero-shot to 52.3 in few-shot, marking a 31.4% improvement.\n(2) A series of LLMs exhibit slight performance improvements compared to the zero-shot setting. For instance, ChatGPT's F1 score on the IAC-V1 dataset rises from 70.0 in zero-shot to 73.2 in few-shot, reflecting a 4.6% improvement. Similarly, ChatGLM 3-6B shows an increase in its F1 score on IAC-V1 from 52.0 in zero-shot to 52.7 in few-shot, representing a 1.3% gain. Mistral-7B displays a more notable improvement on the iSarcasmEval dataset, where its F1 score advances from 26.7 in zero-shot to 28.0 in few-shot, indicating a 2.9% enhancement. Even though the extent of enhancement"}, {"title": "4.5. Impact of Number of Demonstrations", "content": "In few-shot settings, we investigate the impact of the number of demonstrations for the state-of-the-art LLM, namely whether GPT-4 could perform better or not if more contextual examples are provided. We design five k-shot settings: zero-shot, two-shot, four-shot, six-shot, eight-shot. For each setting, we selectly sample k = {0, 2, 4, 6, 8} examples that are similar to the test sample using kNN sampling algorithm. The results are shown in Fig. 4.\nAs we can see, the number of demonstrations (k-shot) has a significant impact on the F1 scores for both Mistral and GPT-4 in the sarcasm classification task. For instance, Mistral shows an overall upward trend in F1 scores from 0-shot to 8-shot, increasing from 51.6 to 56.2, representing a total improvement of 8.7%. The most significant performance boost occurs at 2-shot, reaching 55.8, which is an 8.2% increase compared to O-shot. This shows that Mistral has greater sensitivity to the number of examples provided, benefiting more from additional examples.\nIn contrast, GPT-4 consistently outperforms Mistral across all shot numbers, with a baseline performance of 64.5 F1 score. GPT-4 achieves its best performance at 2-shot, with an F1 score of 67.5, showing a 4.6% improvement over 0-shot. However, it it experiences a slight decline in performance at 6-shot (63.4) and 8-shot (64.3). This indicates that GPT-4 may reach its peak performance with fewer examples and does not benefit as much from additional demonstrations.\nThese results suggest that both models reach their optimal performance with 2-4 samples, indicating that moderate few-shot learning may be more effective than using a large number of samples for sarcasm detection tasks."}, {"title": "4.6. Chain of Thought Results", "content": "In few-shot settings, we investigate a widely used prompting approach, CoT. The experimental results are shown in Table 4 and Fig. 6. Despite that CoT prompting has significant advantage over standard IO prompting across logical, mathematical, and commonsense reasoning tasks, it may be not sufficient in solving sarcasm understanding tasks. For example, GPT-4 sees a decline in its average F1 score from 68.4 with IO prompting to 64.7 with CoT prompting, a 5.5% decrease. Similarly, Mistral, which benefits from IO prompting with an F1 score of 55.8, drops to 47.5 with CoT prompting, reflecting a 14.88% reduction. These examples highlight that while CoT can improve reasoning, it may introduce complexity that hinders models' ability to effectively detect and interpret sarcasm. Qwen 2 sees a 25.1% reduction, from 53.1 to 39.7. These results indicate that CoT often complicates sarcasm understanding, negatively impacting performance.\nThe reason is that sarcasm detection is often considered a holistic and non-rational cognitive process that does not conform to step-by-step logical reasoning. Sarcasm expression does not strictly conform to formal logical structures, such as the law of hypothetical syllogism (i.e., if A \u21d2 B and B \u21d2 C, then A \u21d2 C). For example, \"Poor Alice has fallen for that stupid Bob; and that stupid Bob is head over heels for Claire; but don't assume for a second that Alice would like Claire\". Hence, CoT prompting is less effective for sarcasm classification compared to the more straightforward IO approach. There is an urgent need to develop sarcasm understanding prompting approaches."}, {"title": "4.7. Error Analysis", "content": "The detailed error analysis is also conducted via the confusion matrices that are shown in Fig. 8, Fig. 9 and Fig. 10. Each cell (i, j) represents the percentage of class i is classified to be class j. Upon reviewing the classification results produced by GPT-4 Turbo on six datasets with three prompting approaches, we discover that imbalanced categories and the sarcastic samples are the key factors contributing to misclassification.\nIn zero-shot IO setting, GPT-4 Turbo performs well in identifying non-sarcastic instances, particularly on datasets like IAC-V2 and SemEval Task 3, where the true positive rates for non-sarcastic labels are very high. However, the model struggles more with distinguishing sarcastic instances, especially on datasets like IAC-V1 and iSarcasmEval, where the true positive rates for sarcastic labels are lower (0.46 and 0.38, respectively). This indicates that without any prior examples, the model tends to lean towards non-sarcastic predictions, failing to capture the nuanced language that often accompanies sarcasm.\nIn few-shot IO setting, we observe the improvement in performance across the datasets. The model benefits from a few examples, particularly in non-sarcastic classifications. In datasets like IAC-V2, the model still struggles, with only 46% of sarcastic instances correctly identified. The model shows improvement but remains inconsistent in sarcastic classifications, indicating that while few-shot prompting provides some benefit, it may not be sufficient to fully capture sarcasm's complexity.\nIn few-shot CoT setting, GPT-4 Turbo also similar difficulties in distinguishing between sarcastic and non-sarcastic instances, particularly in datasets with more subtle or nuanced sarcastic content. The reason is that CoT is not suitable to solve human sarcasm understanding tasks.\nThe improvements from few-shot IO and CoT are present but limited, suggesting that sarcasm requires more sophisticated handling, potentially incorporating richer contextual understanding and more advanced prompting strategies."}, {"title": "4.8. Case Study", "content": "Table 5 presents several examples where three representative LLMs made predictions. We conduct a brief analysis of the reasons behind these prediction errors.\n(1) Contextual misunderstanding. GPT-4 struggled with texts where sarcasm was deeply tied to contextual or cultural nuances. For instance, in Example 3 (\"Being half Spanish and not being able to speak Spanish is honestly so disappointing\"), the sarcasm lies in the speaker's ironic expression of disappointment. GPT-4 misclassified this as non-sarcastic, possibly due to its difficulty in detecting sarcasm embedded in personal or cultural identity issues. Similarly, in Example 4 (\u201cI literally just ate 1/4 of a pan of brownies ... #damnit #notagain\u201d), the self-deprecating tone and hashtags signal sarcasm, which GPT-4 failed to recognize."}, {"title": "4.9. Evaluating LLMs on Multi-Modal Sarcasm Understanding", "content": "Due to the rapid surge of multimodal data on social networks, leveraging multi-modal information to enhance human language understanding and reasoning has become increasingly attractive and significant. Beyond text-based LLMs, multi-modal LLMs typically incorporate modality encoders, connectors, and generators. Through modality-aligned pre-training, these models acquire the ability to process multimodal information. Therefore, in this section, we aim to investigate their capabilities for understanding multi-modal sarcasm.\nMore specifically, we select one popular and publicly accessible multi-modal sarcasm detection dataset, namely, MMSD [48], as our experimental platform. We evaluate one state-of-the-art multi-modal LLM: LLaVA 1.5 [49]. Given that CoT prompting is less effective in addressing sarcasm understanding tasks, we employ only the zero-shot IO prompting method. Additionally, we present four state-of-the-art multi-modal sarcasm detection baselines for comparison. The results are shown in Table 6.\nWe can notice that LLaVA 1.5 significantly outperforms other models in multi-modal sarcasm detection across all metrics-accuracy, precision, recall, and F1 score. This superior performance underscores LLaVA 1.5's enhanced capability to integrate and interpret visual and textual cues effectively. The comparative analysis reveals that while traditional multi-modal models are effective, the advanced training methodologies and deeper contextual understanding in LLaVA 1.5 provide a more refined analysis of sarcasm."}, {"title": "5. Conclusion", "content": "In the era of large language models (LLMs), there is growing concern that LLMs' success may not be fully tenable when considering sarcasm understanding. To address this question, we select eleven SOTA LLMs and eight SOTA pre-trained language models (PLMs) and present comprehensive evaluations on six widely used benchmark datasets. The results show that current LLMs underperform supervised PLMs based sarcasm detection baselines across six sarcasm benchmarks. In addition, GPT-4 consistently and significantly outperforms other LLMs across various prompting methods.\nThis suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. In the future, we plan to improve the classification performance by integrating the predictions of multiple models using methods such as bagging or boosting.\nLimitations. Our research also has several limitations. (1) We only employed standard prompting methods and chain-of-thought techniques to guide large language models in detecting human sarcasm, without exploring more refined prompting methods such as tree-based or graphical approaches. These methods will be further investigated in our future work. (2) Our study did not consider the variation in model performance on sarcasm across different contexts. Future work could explore how to enhance the model's understanding of sarcastic language by refining context processing mechanisms."}]}