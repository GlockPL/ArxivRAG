{"title": "RNR: Teaching Large Language Models to Follow Roles and Rules", "authors": ["Kuan Wang", "Alexander Bukharin", "Haoming Jiang", "Qingyu Yin", "Zhengyang Wang", "Tuo Zhao", "Jingbo Shang", "Chao Zhang", "Bing Yin", "Xian Li", "Jianshu Chen", "Shiyang Li"], "abstract": "Instruction fine-tuning (IFT) elicits instruction\nfollowing capabilities and steers the behavior of\nlarge language models (LLMs) via supervised\nlearning. However, existing models trained on\nopen-source IFT datasets only have the ability to\nfollow instructions from users, and often fail to\nfollow complex role and rules specified by de-\nvelopers, a.k.a. system prompts. The ability to\nfollow these roles and rules is essential for deploy-\nment, as it ensures that the model safely interacts\nwith users within developer defined guidelines. To\nimprove such role and rule following ability, we\npropose RoleNRules, an automated data genera-\ntion pipeline that generates diverse roles and rules\nfrom existing IFT instructions, along with corre-\nsponding responses. This data can then be used to\ntrain models that follow complex system prompts.\nThe models are evaluated on our newly created\nbenchmarks for role and rule following ability,\nas well as standard instruction-following bench-\nmarks and general NLP tasks. Our pipeline signifi-\ncantly improves role and rule following capability\nin LLMs, as evidenced by over 25% increase in\npass-rate on rule adherence, i.e. following all re-\nquirements, in our experiments with the Alpaca\nand Ultrachat datasets. Moreover, our models\nachieves this increase without any regression on\npopular instruction following benchmarks.", "sections": [{"title": "1. Introduction", "content": "Instruction fine-tuning (IFT) (Mishra et al., 2021; Ouyang\net al., 2022; Muennighoff et al., 2022; Longpre et al., 2023;\nWang et al., 2023b; Zhang et al., 2023a) has emerged as\na powerful technique to elicit latent capabilities and steer\nthe behaviors of Large Language Models (LLMS) (Radford\net al., 2019; Raffel et al., 2020; Brown et al., 2020; Wei et al.,\n2022a; Scao et al., 2022; Qin et al., 2023; OpenAI, 2023;\nChowdhery et al., 2022). By fine-tuning with carefully\ncurated instruction-response pairs, IFT enables LLMs to\ngenerate desirable responses to user instructions (Bach et al.,\n2022; Wang et al., 2022b;a; Taori et al., 2023b). IFT has\nexploded in popularity due to its ease of use and low cost,\nallowing developers to build a wide variety of LLM-based\nproducts.\nCurrent IFT research is mainly focused on handling user\ninput instructions. However, this overlooks the need to\nenable developers to control LLM behavior via specific role\nand rule requirements. This enhanced level of controllability\nis necessary, as developers often need to specify the LLM's\nrole in their application (\u201cYou are an AI coding assistant\"\nor \"You are an AI tax advisor"}, {"title": "2. Method", "content": "We propose RoleNRules, a data generation pipeline for\nthe automatic generation of RNR system prompts and the\ncorresponding responses for IFT datasets. As illustrated\nin Figure 3, the RoleNRules pipeline augments an existing\nopen-source IFT dataset into our RNR dataset with complex\nsystem-prompt data. Below, we describe the main processes\nin our pipeline."}, {"title": "2.1. RoleNRules System Prompt", "content": "Our method for improving role and rules following gener-\nates diverse and high quality system prompts from existing\ninstruction tuning datasets. These system prompts are de-\nsigned as a combination of a role description and a set of\ndefined rules:\n1.  Role: The role description explicitly specifies the iden-\ntity or persona that the LLM should adopt during the\ninteraction. For example, this role could be a technical\nassistant, customer service representative, data analyst,\nor any other persona with specific characteristics and\nknowledge. By defining the role, we guide the model to\nadopt a particular set of behaviors and language styles\nappropriate for that persona.\n2.  Rules: Accompanying the role description are spe-\ncific rules that set the boundaries and objectives of the\ninteraction. These rules encompass operational guide-\nlines, ethical considerations, data handling procedures,\nand specific goals to be achieved. For example, a rule\nmight mandate prioritizing data privacy or adhering to\nindustry-specific regulations.\nBy integrating these two elements, we can simulate a wide\nvariety of possible developer requirements. In particular, the\nrole descriptions make the LLM's response style customiz-\nable, while the rules ensure that the model operates within\nspecific requirements. In addition, this approach enhances\nthe LLM's ability to understand and execute complex and\nspecialized tasks. And it also provides a pipeline for devel-\nopers to tailor the model's responses for various professional\nscenarios."}, {"title": "2.2. System Prompt Generation", "content": "To generate unique system prompts comprised of different\nrole and rules, we prompt powerful LLMs to generate role\nand rules based on an instruction from existing open-source\ndatasets. We generate role and rules based on existing IFT\ndatasets as existing IFT datasets contain instructions from a\ndiverse range of subjects and tasks, which will in turn make\nour generated role and rules diverse.\nTo generate a role and rules based system prompt from an\nexisting instruction, we sample a random variable K from\n0-8 and ask Claude-2 to generate a role and K rules based\non a set of guidelines. This can be seen in Figure 3. These\nguidelines ensure that the role and rules are reasonable for\na LLM to follow and that the role and rules are not overly\nspecific. In addition, we provide three expert-generated\ndemonstrations to help the LLM generate new roles and\nrules. Our complete prompt can be found in Appendix G. In\nour experiments we use Claude-2 to generate the roles and\nrules, but in practice other powerful LLMs can be used."}, {"title": "2.3. Response Generation", "content": "Although existing IFT datasets contain responses to the\ninstructions, likely, the instructions do not follow the gener-\nated role and rules. Therefore, once we generate a system\nprompt for a given instruction from the IFT dataset, we\ncombine it with the instruction to form a new prompt for\nresponse generation. This is demonstrated in Figure 2. No-\ntably, we only use the instructions from the IFT dataset\nwithout incorporating the original responses. This approach\nprevents overfitting to the original responses and preserves\ndiversity in the generated requirements."}, {"title": "2.4. Fine-tuning", "content": "Once we have generated a dataset of (system prompt, in-\nstruction, response) triplets, we finetune a pre-trained LLM\nto maximize the probability of the response given the com-\nbined system prompt and instruction. To combine the sys-\ntem prompt and instruction, we follow the same template as\nin Figure 2. This combined system prompt is used as the in-"}, {"title": "3. Experiments", "content": "3.1. Experimental Setup\nTraining Setup. We present our main results on the Alpaca\ndataset (Taori et al., 2023b) and the Ultrachat dataset (Ding\net al., 2023). For Alpaca, we generate (system prompt,\ninstruction, response) triplets on the whole dataset and for\nUltrachat, we sample a subset of size 50K to generate the\ntriplets. For our main experiments, we use Llama 2 7B\n(Touvron et al., 2023) as our base model and fine-tune each\nmodel for two epochs with a batch size of 128 using a\ncontext length of 2048 and a learning rate of 2 \u00d7 10-5.\nBaselines. To investigate the efficacy of RNR, we compare\nit with two IFT baselines. For both of these baselines we\ngenerate responses to the original training instructions with\nClaude-2, so that there is no discrepancy in the teacher\nmodel used for training. Once we generate these responses,\nwe train models using (a) a fixed system prompt (\"You are\na helpful, harmless, and honest AI assistant\") and (b) with\nno system prompt. These baselines are denoted as IFT:\nfixed-system and IFT: no-system, respectively."}, {"title": "3.2. Benchmarks", "content": "We design three benchmarks using the RNR format (sys-\ntem prompt + instruction) to evaluate the effectiveness of\nRoleNRules:\n1.  RNR-Expert benchmark: This is a small benchmark\ncomprised of 50 data points with RNR system prompts\nand instructions handcrafted by domain experts, who\nare LLM researchers and are familiar with prompt-\ning techniques. The roles and rules in these system\nprompts cover diverse domains including program-\nming, engineering, medicine, law, finance, movies,\nbook recommendations, travel advice, and cooking\nrecipes, etc. We remark that this dataset contains chal-\nlenging and high quality examples.\n2.  RNR-Awesome benchmark: This is benchmark has\n495 data points based on the instruction collected by"}, {"title": "3.3. Main Results", "content": "Our main results on the RNR benchmark can be found in\nTable 1. The results show that RNR can significantly im-\nprove system-prompt following ability compared to the two\nIFT baselines for both the Alpaca and Ultrachat datasets. In\nparticular, we find that RNR improves prompt-level system-\nprompt following ability by 37.4% on the Alpaca dataset\nand by 26.1% for Ultrachat compared to the strongest base-\nline. These results indicate that RNR is an effective way to\nimprove system-prompt following ability.\nThe results for IFEval can be found in Table 2. From Ta-\nble 2 we can see that RNR significantly boosts instruction\nfollowing ability, even when the instruction is inputted by\nthe user, indicating that improve system prompt following\ncapability can positively transferred to user inputs.\nFor RoleBench, we find that RNR improves the winning\nscore (defined as \n+ 1) by over 25% com-\npared to both IFT baselines on the Alpaca dataset. The\nresults can be found in Table 7. This indicates RNR signifi-\ncantly enhances role-playing ability.\nFor evaluation on Alpacafarm, we do a head to head com-\nparison of RNR:Alpaca and IFT:no-system. We find that the\nperformance is similar for IFT:no-system and RNR, with\nRNR outperforming the IFT model ~ 3.7%. (See Appendix\nC). Therefore RNR can boost complex system-prompt fol-\nlowing ability without hurting general instruction following\nperformance. Finally, we demonstrate the performance of\nRNR on various benchmarks in Table 3. As expected, the\nbenchmark performance is similar for all models, which\nshows that RNR retains general reasoning capabilities."}, {"title": "3.4. Human Evaluation", "content": "To validate the efficacy of RNR, we conducted a human\nevaluation experiment in addition to using public LLM APIs"}, {"title": "4. Conclusion", "content": "In conclusion, our study presents a new approach to instruc-\ntion tuning for Large Language Models (LLMs), focusing\non both user-centric instructions and developer-defined roles\nand rules. Our novel system prompt structure and automated\ndata generation pipeline contribute to significant improve-\nments in rule-following capabilities in LLMs, as evidenced\nby enhanced performance on our benchmarks using the Al-\npaca and Ultrachat datasets. Despite these advancements,\nour work has limitations, including the absence of a quality-\nbased filtering mechanism in our data generation and the\nframework's restriction to single-turn interactions. Future\nefforts will focus on addressing these issues, aiming to re-\nfine the model's effectiveness in more complex, multi-turn\nconversation settings."}, {"title": "A. Related Works", "content": "A.1. Instruction Fine-tuning\nInstruction Fine-tuning (IFT) has emerged as a pivotal technique in enhancing the capabilities and controllability of LLMs.\nThe core of IFT lies in the generation and selection of instruction data (Chen et al., 2024; Chung et al., 2022; Lee et al., 2023;\nHonovich et al., 2022; Wang et al., 2022a; Zhou et al., 2023a), which leverage existing LLMs like InstructGPT (Ouyang et al.,\n2022) and ChatGPT (OpenAI, 2022) to recursively generate instruction sets. Since the introduction of LLAMA (Touvron\net al., 2023), a surge in IFT research has led to notable models (Taori et al., 2023b; Chiang et al., 2023; Peng et al., 2023).\nOur approach in instruction fine-tuning (IFT) distinctly focuses on system prompts, which are commonly a fixed one in\nprevious works (e.g., \u201cYou are a helpful, respectful, and honest assistant.\u201d). Instead, we focus on developing LLMs, which\nenables model developers to have fine-grained controls of model behaviors via system prompts.\nA.2. LLM Role Playing\nLLMs have demonstrated the potential to act as advanced agents (Ouyang et al., 2022; Bubeck et al., 2023; Wei et al., 2022a;\nWeng, 2023; Sumers et al., 2023; Park et al., 2023; Liu et al., 2023; Lin et al., 2023; Xu et al., 2023a). Recent research\nfocuses on assigning specific roles to large language model agents to enhance their interactivity, personalization, and ability\nto handle complex or specific tasks (Zhang et al., 2023b; Wang et al., 2023c; Wei et al., 2023; Shanahan et al., 2023; Li\net al., 2023a; Salemi et al., 2023). However, these work are designed for simple role-playing scenarios with no or few rules.\nInstead, RoleNRules is design for model developers, who often require flexible and complex requirements for the simulated\nroles of their LLM products.\nA.3. LLM Evaluation\nThe evaluation of language models has evolved significantly, with a transition from focusing on perplexity and standard\nbenchmarks to more comprehensive assessments including aspects such as commonsense reasoning (Li et al., 2019; K\u0131c\u0131man\net al., 2023; Li et al., 2022), counterfactual reasoning (Qin et al., 2019; 2020; Frohberg & Binder, 2021; K\u0131c\u0131man et al., 2023),\nnumerical reasoning (Chen et al., 2022; Qian et al., 2023), interpretability (Elazar et al., 2021; Geiger et al., 2021; 2022),\nspurious correlation detection (Veitch et al., 2021; Eisenstein, 2022), and fairness (Kusner et al., 2017; Nabi & Shpitser,\n2018). Recent works also leverage the capabilities of powerful models like GPT-4 for evaluating weaker LLMs, albeit with\nnoted shortcomings (Xu et al., 2023c;b; Wang et al., 2023a). Moreover, efforts have been made towards automating the\ninstruction-following evaluation with minimum human intervention, with studies exploring the performance of models\nin instruction-only settings (Zhou et al., 2023b) and assessing the impact of negation (Jang et al., 2023) and verbalizer\nmanipulations (Li et al., 2023b). However, existing benchmarks fail to evaluate the ability of LLMs to follow complex role\nand rules. To bridge this gap, we design multiple benchmarks to evaluate role and rule follow capabilities and at the same\ntime, automatic LLM-based evaluation methods with high human agreement. To the best of our knowledge, we are the first\nto systematically study complex role and rule following capabilities of LLMs."}, {"title": "B. Analysis", "content": "Human Evaluation. In addition to using public LLM APIs to evaluate the responses generated by RNR models and\nbaselines, we also design a human evaluation experiment to validate the efficacy of RNR. For this experiment, we use the\nexpert-curated set of the RNR benchmark, and ask the expert human researchers to evaluate the prompt-level pass rate of the\nRNR model and the IFT: no-system model.\nWe have 3 participants for human evaluation, and each of them is provided with a system-prompt, instruction, and set of\nresponse pairs. These response pairs consist of an output from our model and a baseline model, without revealing the source\nof each response. We randomize the order to minimize the effect of evaluator bias. The evaluators are tasked with analyzing\nthe responses based on two criteria (1) whether the responses are in the style of the roles and (2) whether the responses\nfollow the rules in the system prompt. If both of these criteria are met, the response is given a passing score, otherwise it is\nassigned a failing score. We also tell the evaluators to ignore the minor factual or mathematical errors in the response, as we\nfocus on the system-prompt following ability of RNR.\nThe results of human evaluation can be seen in in Figure 4. As we can see from this figure, RNR has significantly better pass\npercentage, both when considering strict evaluation (All Pass) and non-strict evaluation (Majority Pass). We provide two"}, {"title": "F. Example Train Datapoints", "content": "Here we display 3 training examples (randomly picked) from our dataset.\n### System Prompt: You are an AI Math Assistant. Your goal is to help users with math calculations and problems.\nHere are 7 requirements for the interaction:\n(1) You MUST correctly perform the calculation specified in the instruction.\n(2) You MUST insert the numerical result in the sentence provided, in place of the blank underscore \"_\" characters.\n(3) The inserted result MUST contain appropriate formatting - no unnecessary leading zeros or trailing decimals.\n(4) You MUST verify the accuracy of the calculation before inserting the result.\n(5) If the instruction does not provide enough information to perform the calculation, you MUST ask clarifying ques\ntions.\n(6) If you are unable to perform the calculation for any reason, you MUST tell the user \"I'm sorry, I'm unable to p\nerform that calculation.\"\n(7) You MUST maintain a polite, helpful tone when interacting with users.\nWhen I write START you will enter this role.\nSTART\n### Instruction:\nPerform the calculation and insert the answer into the following sentence.\nInput: The product of 45 and 32 is ____\n### Response: The product of 45 and 32 is 1440.\n### System Prompt: You are an AI text classifier. Your goal is to categorize text into different classes.\nWhen I write START you will enter this role.\nSTART\n### Instruction:\nCategorize the following post as either a personal or professional post.\nInput: Just had the best dinner with my team from work!\n### Response: Personal post\n### System Prompt: You are an AI Journalist Assistant. Your goal is to help users create news articles by conductin\ng interviews and summarizing key information.\nHere are 7 requirements for conducting an interview and writing a news article summary:\n(1) Greet the interviewee politely and explain your role as an AI Journalist Assistant.\n(2) Ask open-ended questions about the interviewee's background, goals, challenges, successes, and vision for the f\ntuture. Take notes on key points.\n(3) Maintain a respectful and professional tone throughout the interview. Do not interrupt or cut off the interview\nee.\n(4) After the interview, organize your notes into a logical structure with an introduction, body paragraphs, and co\nnclusion.\n(5) Summarize the interviewee's key points in your own words. Do not directly quote long passages. Use paraphrasin\ng.\n(6) The article should be between 250-500 words. Include the most important and interesting information from the in\nterview.\n(7) Proofread the article before sending it to the user. Check for spelling, grammar, clarity, and conciseness.\nWhen I write START you will enter this role.\nSTART\n### Instruction:\nMeet with the director of a wildlife conservation center and create a short news article summarizing the experienc\ne.\n### Response: Good morning! My name is Claude and I'm an AI assistant here to help conduct an interview with you fo\nr a news article. Thank you for taking the time to speak with me today. To start, could you please state your full\nname and position?"}, {"title": "G. The Prompt to Generate the System Prompt", "content": "I am going to give you an instruction and your task is to generate an AI assistant role description followed with\nrequirements for the role according to the guidelines in  xml tags:\n(1) Make sure that a text language model without external tools, e.g. Internet Search, can play the AI assistant role you\ngenerate and can help to finish the instruction. This means that the AI assistant role cannot create any visual or audio\noutput.\n(2) The AI assistant role should be general and able to finish diverse instructions, not just the given instruction.\n(3) The requirements for the AI assistant role should be detailed and executable. Remember that a text language model\nwithout external tools, e.g. Internet Search, can play the AI assistant role you generate. Hence, do not include not\nexecutable reqirements, e.g. finding the most up-to-date information.\n(4) The requirements should be general for various instructions in the same class and not specific to the given instruction.\nMake sure that AI assistant with generated requirements can generalize to diverse instructions.\n(5) Requirements can include format requirements or specific responses for certain type of instructions by using \"If\"\nstatement.\n(6) Some requirements are more important than the others. In these cases, you can capitalize words, use \"MUST\" or\n\"VERY IMPORTANT\u201d, or other methods to highlight these requirements.\n(7) Do NOT give names for the AI assistant role and ONLY generate K requirements, where K is a variable I will provide\nto you.\n(8) If K is 0, describe the role and then put \\n .\n(9) Give me your answer only if you are confident; otherwise tell me \"I don't know\"."}, {"title": "H. Evaluation Prompt", "content": "You are provided with a role description with requirements, a user instruction and a response from an AI Assistant. Your task is\nto evaluate if the response follows ALL requirements for the role when replying user instruction according to the guidelines in\n xml tags:\n(1) A good response MUST follow ALL requirements for the role. For example, if a requirement is \"Your output format should\nbe bullet points\", a good response MUST follow this requirement.\n(2) Before giving your judgement, examine each requirement step by step first when evaluating the response. After examining\nALL requirements, give your judgement. If the response follow ALL of them, output \"pass\" in  xml tag;\notherwise, output \"fail\" in  xml tag."}]}