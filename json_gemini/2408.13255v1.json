{"title": "Ensemble Modeling of Multiple Physical Indicators to Dynamically Phenotype Autism Spectrum Disorder", "authors": ["Marie Huynh", "Aaron Kline", "Saimourya Surabhi", "Kaitlyn Dunlap", "Onur Cezmi Mutlu", "Mohammadmahdi Honarmand", "Parnian Azizian", "Peter Washington", "Dennis P. Wall"], "abstract": "Early detection of autism, a neurodevelopmental disorder marked by social communication challenges, is crucial for timely intervention. Recent advancements have utilized naturalistic home videos captured via the mobile application GuessWhat. Through interactive games played between children and their guardians, GuessWhat has amassed over 3,000 structured videos from 382 children, both diagnosed with and without Autism Spectrum Disorder (ASD). This collection provides a robust dataset for training computer vision models to detect ASD-related phenotypic markers, including variations in emotional expression, eye contact, and head movements. We have developed a protocol to curate high-quality videos from this dataset, forming a comprehensive training set. Utilizing this set, we trained individual LSTM-based models using eye gaze, head positions, and facial landmarks as input features, achieving test AUCs of 86%, 67%, and 78%, respectively. To boost diagnostic accuracy, we applied late fusion techniques to create ensemble models, improving the overall AUC to 90%. This approach also yielded more equitable results across different genders and age groups. Our methodology offers a significant step forward in the early detection of ASD by potentially reducing the reliance on subjective assessments and making early identification more accessibly and equitable.", "sections": [{"title": "1. Introduction", "content": "Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition that affects approximately 1 in 36 children across diverse ethnic, racial, and socioeconomic backgrounds, highlighting its widespread impact and the need for comprehensive strategies to address it [15]. Children with ASD face significant challenges in communication, social interactions, repetitive behaviors, and restricted interests, often leading to profound difficulties in everyday functioning and development [9]. The impact of ASD extends beyond individual health, imposing a substantial economic burden with an estimated lifetime social cost of approximately $3.6 million per affected individual [3].\nEarly interventions are crucial as the developmental gap between children with ASD and neurotypical (NT) children widens over time [4]. Early diagnosis can lead to improved health outcomes; however, despite the potential for reliable diagnosis as early as 16-24 months, the average age of diagnosis is 4.5 years [14,21]. Diagnostic processes typically involve long waitlists and assessments, resulting in an average delay of two years [6]. Current diagnoses rely on in-person behavioral assessments, which are costly and subjective, lacking definitive medical tests or biomarkers [5]. This subjective process introduces variability and the potential for misdiagnosis influenced by clinician experience, training, and social biases [16].\nDigital phenotyping using naturalistic home videos offers a promising approach for faster and more objective diagnosis of ASD and other developmental conditions."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Digital Phenotyping of Autism", "content": "Computer vision tools have shown promise in identifying multiple phenotypes such as emotion, eye movement, and posture tracking in children through video analysis [11, 20, 27]. However, these tools often lack structured data on children with autism, and predictive models using individual phenotypes have rarely been assessed for fairness or integrated into ensemble models.\nEye tracking has been studied extensively for diagnosing ASD in children, with recent machine learning algorithms analyzing eye gaze patterns to differentiate between ASD and neurotypical (NT) children [7, 12, 25]. In a meta-analysis of eye-tracking based ML models for distinguishing between ASD and NT individuals, Wei et al. [7]reported a pooled classification accuracy of 81%, specificity of 79%, and sensitivity of 84%. However, Takahashi and Hashiya [23] reported high degress of variation in eye-tracking data collection, emphasizing the need for additional data regularization methods as well as incorporation of other data modalities.\nOther modalities such as head movement [28], facial expression [2, 8], finger movement [22], and facial engagement [25] have shown valuable for identifying autism. Nonetheless, challenges related to data variability, generalizability across diverse populations, and integration with other diagnostic measures remain.\nBuilding upon this groundwork, we use deep time-series models such as LSTM and GRU to analyze eye gaze, face, and head features to predict ASD. Our approach will include an advanced feature engineering pipeline and explore fusion methods to enhance predictive power and model robustness."}, {"title": "2.2. Data Fusion For ASD Classification", "content": "Recent studies have focused on the potential of data fusion for improving ASD classification performance. Perochon et al. [19] recently evaluated an autism screening digital application administered during pediatric well-child visits for children aged 17\u201336 months. Their algorithm integrated multiple digital phenotypes, achieving an AUROC of 0.90, sensitivity of 87.8%, specificity of 80.8%, negative predictive value of 97.8%, and positive predictive value of 40.6%. These findings underscore the potential of combining data sources to improve diagnostic outcomes.\nFurther extending this line of work, Thompson et al. [24] investigated the integration of acoustic and linguistic markers with visual data, reporting enhancements in the system's ability to predict ASD traits in varied social contexts. The integration of physiological data has been further explored by Nakamura et al. [17], who demonstrated that combining heart rate variability and skin conductance with traditional behavioral assessments could offer a more comprehensive understanding of ASD.\nOur research builds on these foundations, focusing on mobile videos captured in a home environment during gameplay. This approach offers a scalable and open method for dynamic data collection, crucial for conditions like autism that are not static [26]. It enables continuous monitoring and authentic behavioral data, potentially facilitating earlier and more accurate diagnoses and personalized interventions. Moreover, our work emphasizes fusing multiple indicators such as eye gaze, head positions, and facial landmarks-extracted from these videos to construct robust diagnostic models. By analyzing video data in real-world settings, we aim to bridge the gap between clinical practice and everyday environments, offering a practical solution for widespread autism screening and diagnosis."}, {"title": "3. Dataset", "content": null}, {"title": "3.1. Data Source and Type", "content": "As a home-based digital therapeutic for children with developmental delays, the mobile application GuessWhat [10] provides a rich dataset of videos to train new models to phenotype autism digitally. To date, GuessWhat has amassed more than 3,000 highly structured videos of 382 children aged from 2 to 12 years old with and without an autism diagnosis."}, {"title": "3.2. Filtering Pipeline", "content": "The instability of home videos can create noise and data drifts that reduce the performance of the models. Furthermore, some videos may be feature-poor, or the child of interest may be too far away to extract features of interest, etc. Rigorous filtering and feature engineering are needed to account for these limitations and build a minimally viable training set for our task. Our filtering pipeline can be summarized in three steps as illustrated in Figure 2.\nThe criteria for the videos are as follows: (1) they must be of high quality, suitable for feature extraction, and (2) each video must focus on the child of interest. To ensure these criteria, we filtered our videos using the Amazon Rekognition Video API for face detection [1], which provides estimates for sharpness, brightness, head pose, facial landmarks, and face size for each video. For the first criterion, we selected videos that guaranteed clarity through sharpness and brightness metrics, featured a sufficiently large face for detecting eye-gazing features, had the eyes open for more than 70% of the video's duration, and maintained the face predominantly facing the camera (as indicated by the head pose). To meet the second criterion, we selected videos where the face was proportionally large enough for reliable feature extraction and where the presence of multiple faces was minimal, ensuring the focus remained on a single child. This filtered dataset, referred to as Dataset A, includes 2123 videos featuring 288 children, as shown in Table 1."}, {"title": "3.3. Bias and Imbalances", "content": "Our dataset contains a significant imbalance with respect to ASD and NT classes, as reflected in Table 1. Dataset A has an ASD to NT ratio of 17:1. Furthermore, some users in the ASD class are more represented than others due to more gameplay, as can be seen in Figure 1.\nSince we only have 116 videos for NT children (cf. Table 1), we manually inspected all the NT videos to filter out those of poor quality or that did not adhere to the specified constraints. During this review, 7 videos were identified as invalid for reasons such as the parent playing and the child holding the phone, siblings playing one after the other in the same video, etc.\nTo avoid any bias towards a particular child and mitigate the imbalance between ASD and NT, we under-sampled the ASD class by keeping at most two videos with the highest quality (mean of sharpness and brightness) for each child. We kept all of our data for NT children (since it is a minority class). We obtained dataset B, which contains 700 videos (cf. Figure 2)."}, {"title": "3.4. Feature Extraction", "content": "The input of our models consists of a sequence of k frames, where each frame contains a feature vector of size d ($d$ being the dimension of features for a given modality, $d \\in$ {2, 7, 60} for the eye gazing, head and face modality, respectively). Hence, our input is a multivariate time-series denoted as [$X_1$, ..., $X_k$] where $X^i$ = [$x_1$,...,$x_d$] and our output is a binary label Y, where Y $\\in$ {0,1} (0: NT, 1: ASD).\nGuess What videos have an average frame rate of 28 frames per second and average length of 90 seconds. We down-sampled the dataset B videos to 10 frames per second and utilized AWS Rekognition [1] to extract frame-level eye gaze, head pose, and facial features, as illustrated in Figure 3. Rekognition-provided confidence scores demonstrated a mean confidence of 79.4 for eye and 99.12 for face and head detection features for every frame with a face. After extraction, we dropped any video with less than 15 seconds of features extracted and obtained a high-quality Dataset C with 688 videos (cf. Figure 2). Per-video and per-child demographics of the final dataset are detailed in Table 2."}, {"title": "4. Experimental Setup", "content": null}, {"title": "4.1. Data Preprocessing", "content": "Mobile data is inherently noisy, requiring robust feature engineering to extract informative sequences for learning. Our feature engineering pipeline, depicted in Figure 4, addresses several challenges.\nAt the start and end of videos, our features of interest are often missing due to camera stabilization or game initiation with the child. To mitigate this, we truncate frames where no face is detected.\nWithin videos, we identify periods of missing data, illustrated in Figure 6.\nMissing eye gaze data occurs when the child's eyes are closed, not facing the camera, or when the camera angle is off. These instances can be informative, reflecting the child's interaction challenges or meaningful movements. Conversely, periods without face detection or poorly centered cameras provide no useful information. We exclude these uninformative windows and concatenate informative segments to enhance feature continuity.\nTo reduce input length, we averaged features every two frames, for an effective frame rate of 5 fps, as shown in Figure 4. We then normalized the features and represented missing frames as a vector of tokens (-1) as shown in Figure 5., in order to incorporate missingness as a feature in the temporal data structure."}, {"title": "4.2. Model Training", "content": "The resulting dataset was split at the child level into training, validation, and test sets to prevent data leakage, especially for children with multiple videos. Split details are summarized in Table 3. To ensure fairness and representativeness, we stratified the splits by age group (1-4, 5-8, 9-12) and gender (Male, Female, Other), summarized in Table 4. This approach may result in an unconventional data distribution but ensures consistent and accurate representation across these demographic factors."}, {"title": "5. Results and Discussion", "content": null}, {"title": "5.1. Effect of Feature Engineering", "content": "Table 5 shows the impact of feature engineering on the performance of three models (Eye, Head, Face) by comparing their AUC and F1-scores before and after applying the pipeline. The Eye model shows the most significant improvementthough its F1-score slightly decreases, indicating some trade-offs. The Face model exhibits minimal improvement, with its AUC modestly rising and the F1-score decreasing, showing limited effectiveness of the feature engineering."}, {"title": "5.2. Performance comparison of our models", "content": "The eye and head models achieved strong predictive power with test AUCs of 0.86 and 0.78, respectively (Figure 7d). The facial landmarks model had moderate predictive power with a test AUC of 0.67. Table 6 summarizes the performance metrics for each model.\nThe confidence intervals in Table 6 were obtained using bootstrapping. We resampled the test set 1000 times, calculating each sample's metrics. The 2.5th and 97.5th percentiles of these metrics provide a 95% confidence interval.\nThe eye model outperforms the facial landmarks and head pose models, with narrower confidence intervals indicating consistent performance. The eye model's high F1-score reflects its balanced precision and recall, demonstrating the best overall performance.\nCombining the three modalities enhanced predictive power. The late fusion models, particularly the averaging method (test AUC of 0.90) and the linear method (test AUC of 0.84), performed strongly. The intermediate fusion model performed poorly (test AUC of 0.55).\nWe also tested Two-by-two feature combinations. For late fusion by averaging, the Eye+Head model achieved an AUC of 0.87, Face+Head 0.78, and Eye+Face 0.87. For late fusion with a linear layer, Eye+Head achieved an AUC of 0.82, Face+Head 0.67, and Eye+Face 0.90. The Eye+Face combination excelled in both methods, leveraging eye gaze and facial features for robust predictions."}, {"title": "5.3. Fairness evaluation", "content": "We evaluated our models for age and gender sensitivity, excluding geographic locations due to insufficient demographic parity differences and equalized odds differences, where a lower parity difference indicates more evenly distributed positive outcomes across groups, while a lower equalized odds difference indicates more evenly distributed error rates.\nThe Eye Model performs well 8for age groups 1-4 and 9-12 but struggles with age group 5-8, showing moderate fairness issues (Demographic Parity Difference: 0.1732, Equalized Odds Difference: 0.1569). The Face and Head Models perform well for age group 1-4 but poorly for age group 9-12, with significant fairness challenges (Equalized Odds Difference: 0.7460). The Late Fusion (Avg) model shows improved performance and fairness across all age groups (Demographic Parity Difference: 0.1826, Equalized Odds Difference: 0.1250).\nRegarding gender, the Eye Model performs slightly better for females 9, with balanced fairness metrics (Demographic Parity Difference: 0.1078, Equalized Odds Difference: 0.0268). The Face and Head Models show more gender disparities, with lower performance for females and higher fairness differences (Demographic Parity Difference: 0.2888, Equalized Odds Difference: 0.4196). The Late Fusion (Avg) model improves gender performance and fairness (Demographic Parity Difference: 0.2071, Equalized Odds Difference: 0.0769), while the Late Fusion (Linear) model offers the best balance of performance and fairness (Demographic Parity Difference: 0.1979, Equalized Odds Difference: 0.0769). Given the importance of early diagnosis, the Late Fusion (Linear) model is the most balanced and fair option for both age and gender groups. Fairness mitigation techniques applied to the Head and Face Models for the age group 9-12 led to marginal improvements, so we focused on the fairer and more effective Late Fusion Methods."}, {"title": "5.4. Net Benefit Analysis", "content": "The Net Benefit Analysis curve (Figure 8) compares different models across various thresholds, illustrating their performance in terms of net benefit. High sensitivity ensures most children with ASD are correctly diagnosed for timely interventions, while high specificity prevents incorrect diagnoses of NT children, avoiding unnecessary interventions. This balance ensures a reliable and practical diagnostic process, efficiently allocating resources.\nThe Late Fusion (Avg) model consistently offers higher net benefits, with the Late Fusion (Linear) and Eye Model performing well at varying thresholds. The Face and Head Models show lower net benefits. The dataset's skewed ASD prevalence affects the generalizability of these findings."}, {"title": "6. Limitations and Future Directions", "content": "Our current models face several limitations. First, incorporating accelerometer data can mitigate data drifts, ensuring more reliable predictions. Secondly, developing an automated preprocessing pipeline is crucial for handling challenges such as detecting when a child is too far or close to the camera, managing multiple faces, and tracking different children in videos. As we continue to collect more data, an age-centric approach will allow us to tailor games and features to different age groups, thereby enhancing both engagement and the informativeness of the features extracted. Moreover, expanding our models to incorporate additional modalities, such as speech, will provide us with a richer understanding of subjects. Additionally, focusing on time-series pre-training and improving interpretability will enhance robustness and transparency, making models more reliable and understandable. To ensure fairness, it is crucial to collect skin color data and generalize across different skin tones, thereby promoting equity and reducing bias in our predictive outcomes."}]}