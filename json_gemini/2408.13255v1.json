{"title": "Ensemble Modeling of Multiple Physical Indicators to Dynamically Phenotype\nAutism Spectrum Disorder", "authors": ["Marie Huynh", "Aaron Kline", "Saimourya Surabhi", "Kaitlyn Dunlap", "Onur Cezmi Mutlu", "Mohammadmahdi Honarmand", "Parnian Azizian", "Peter Washington", "Dennis P. Wall"], "abstract": "Early detection of autism, a neurodevelopmental disor-\nder marked by social communication challenges, is cru-\ncial for timely intervention. Recent advancements have\nutilized naturalistic home videos captured via the mo-\nbile application GuessWhat. Through interactive games\nplayed between children and their guardians, GuessWhat\nhas amassed over 3,000 structured videos from 382 chil-\ndren, both diagnosed with and without Autism Spectrum\nDisorder (ASD). This collection provides a robust dataset\nfor training computer vision models to detect ASD-related\nphenotypic markers, including variations in emotional ex-\npression, eye contact, and head movements. We have de-\nveloped a protocol to curate high-quality videos from this\ndataset, forming a comprehensive training set. Utilizing\nthis set, we trained individual LSTM-based models using\neye gaze, head positions, and facial landmarks as input fea-\ntures, achieving test AUCs of 86%, 67%, and 78%, respec-\ntively. To boost diagnostic accuracy, we applied late fu-\nsion techniques to create ensemble models, improving the\noverall AUC to 90%. This approach also yielded more\nequitable results across different genders and age groups.\nOur methodology offers a significant step forward in the\nearly detection of ASD by potentially reducing the reliance\non subjective assessments and making early identification\nmore accessibly and equitable.", "sections": [{"title": "1. Introduction", "content": "Autism Spectrum Disorder (ASD) is a complex neurode-\nvelopmental condition that affects approximately 1 in 36\nchildren across diverse ethnic, racial, and socioeconomic\nbackgrounds, highlighting its widespread impact and the\nneed for comprehensive strategies to address it [15]. Chil-\ndren with ASD face significant challenges in communica-\ntion, social interactions, repetitive behaviors, and restricted\ninterests, often leading to profound difficulties in everyday\nfunctioning and development [9]. The impact of ASD ex-\ntends beyond individual health, imposing a substantial eco-\nnomic burden with an estimated lifetime social cost of ap-\nproximately $3.6 million per affected individual [3].\nEarly interventions are crucial as the developmental gap\nbetween children with ASD and neurotypical (NT) children\nwidens over time [4]. Early diagnosis can lead to improved\nhealth outcomes; however, despite the potential for reliable\ndiagnosis as early as 16-24 months, the average age of di-\nagnosis is 4.5 years [14,21]. Diagnostic processes typically\ninvolve long waitlists and assessments, resulting in an av-\nerage delay of two years [6]. Current diagnoses rely on in-\nperson behavioral assessments, which are costly and sub-\njective, lacking definitive medical tests or biomarkers [5].\nThis subjective process introduces variability and the po-\ntential for misdiagnosis influenced by clinician experience,\ntraining, and social biases [16].\nDigital phenotyping using naturalistic home videos of-\nfers a promising approach for faster and more objective di-\nagnosis of ASD and other developmental conditions."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Digital Phenotyping of Autism", "content": "Computer vision tools have shown promise in identify-\ning multiple phenotypes such as emotion, eye movement,\nand posture tracking in children through video analysis\n[11, 20, 27]. However, these tools often lack structured data\non children with autism, and predictive models using indi-\nvidual phenotypes have rarely been assessed for fairness or\nintegrated into ensemble models.\nEye tracking has been studied extensively for diagnos-\ning ASD in children, with recent machine learning algo-\nrithms analyzing eye gaze patterns to differentiate between\nASD and neurotypical (NT) children [7, 12, 25]. In a meta-\nanalysis of eye-tracking based ML models for distinguish-\ning between ASD and NT individuals, Wei et al. [7]reported\na pooled classification accuracy of 81%, specificity of 79%,\nand sensitivity of 84%. However, Takahashi and Hashiya\n[23] reported high degress of variation in eye-tracking data\ncollection, emphasizing the need for additional data regu-\nlarization methods as well as incorporation of other data\nmodalities.\nOther modalities such as head movement [28], facial\nexpression [2, 8], finger movement [22], and facial en-\ngagement [25] have shown valuable for identifying autism.\nNonetheless, challenges related to data variability, gener-\nalizability across diverse populations, and integration with\nother diagnostic measures remain.\nBuilding upon this groundwork, we use deep time-series\nmodels such as LSTM and GRU to analyze eye gaze, face,\nand head features to predict ASD. Our approach will in-\nclude an advanced feature engineering pipeline and explore\nfusion methods to enhance predictive power and model ro-\nbustness."}, {"title": "2.2. Data Fusion For ASD Classification", "content": "Recent studies have focused on the potential of data fu-\nsion for improving ASD classification performance. Pe-\nrochon et al. [19] recently evaluated an autism screening\ndigital application administered during pediatric well-child\nvisits for children aged 17\u201336 months. Their algorithm in-\ntegrated multiple digital phenotypes, achieving an AUROC\nof 0.90, sensitivity of 87.8%, specificity of 80.8%, negative\npredictive value of 97.8%, and positive predictive value of\n40.6%. These findings underscore the potential of combin-\ning data sources to improve diagnostic outcomes.\nFurther extending this line of work, Thompson et al. [24]\ninvestigated the integration of acoustic and linguistic mark-\ners with visual data, reporting enhancements in the system's\nability to predict ASD traits in varied social contexts. The\nintegration of physiological data has been further explored\nby Nakamura et al. [17], who demonstrated that combining\nheart rate variability and skin conductance with traditional\nbehavioral assessments could offer a more comprehensive\nunderstanding of ASD.\nOur research builds on these foundations, focusing on\nmobile videos captured in a home environment during\ngameplay. This approach offers a scalable and open method\nfor dynamic data collection, crucial for conditions like\nautism that are not static [26]. It enables continuous moni-\ntoring and authentic behavioral data, potentially facilitating\nearlier and more accurate diagnoses and personalized in-\nterventions. Moreover, our work emphasizes fusing multi-\nple indicators such as eye gaze, head positions, and facial\nlandmarks-extracted from these videos to construct robust\ndiagnostic models. By analyzing video data in real-world\nsettings, we aim to bridge the gap between clinical practice\nand everyday environments, offering a practical solution for\nwidespread autism screening and diagnosis."}, {"title": "3. Dataset", "content": null}, {"title": "3.1. Data Source and Type", "content": "As a home-based digital therapeutic for children with de-\nvelopmental delays, the mobile application Guess What [10]\nprovides a rich dataset of videos to train new models to phe-\nnotype autism digitally. To date, GuessWhat has amassed\nmore than 3,000 highly structured videos of 382 children\naged from 2 to 12 years old with and without an autism di-\nagnosis."}, {"title": "3.2. Filtering Pipeline", "content": "The instability of home videos can create noise and data\ndrifts that reduce the performance of the models. Further-\nmore, some videos may be feature-poor, or the child of in-\nterest may be too far away to extract features of interest,\netc. Rigorous filtering and feature engineering are needed\nto account for these limitations and build a minimally vi-\nable training set for our task. Our filtering pipeline can be\nsummarized in three steps as illustrated in Figure 2.\nThe criteria for the videos are as follows: (1) they must\nbe of high quality, suitable for feature extraction, and (2)\neach video must focus on the child of interest. To en-\nsure these criteria, we filtered our videos using the Ama-\nzon Rekognition Video API for face detection [1], which\nprovides estimates for sharpness, brightness, head pose,\nfacial landmarks, and face size for each video. For the\nfirst criterion, we selected videos that guaranteed clarity\nthrough sharpness and brightness metrics, featured a suf-\nficiently large face for detecting eye-gazing features, had\nthe eyes open for more than 70% of the video's duration,\nand maintained the face predominantly facing the camera\n(as indicated by the head pose). To meet the second crite-\nrion, we selected videos where the face was proportionally\nlarge enough for reliable feature extraction and where the\npresence of multiple faces was minimal, ensuring the focus\nremained on a single child. This filtered dataset, referred to\nas Dataset A, includes 2123 videos featuring 288 children,\nas shown in Table 1."}, {"title": "3.3. Bias and Imbalances", "content": "Our dataset contains a significant imbalance with respect\nto ASD and NT classes, as reflected in Table 1. Dataset A\nhas an ASD to NT ratio of 17:1. Furthermore, some users\nin the ASD class are more represented than others due to\nmore gameplay, as can be seen in Figure 1.\nSince we only have 116 videos for NT children (cf. Table\n1), we manually inspected all the NT videos to filter out\nthose of poor quality or that did not adhere to the specified\nconstraints. During this review, 7 videos were identified as\ninvalid for reasons such as the parent playing and the child\nholding the phone, siblings playing one after the other in\nthe same video, etc.\nTo avoid any bias towards a particular child and mitigate\nthe imbalance between ASD and NT, we under-sampled the\nASD class by keeping at most two videos with the highest\nquality (mean of sharpness and brightness) for each child.\nWe kept all of our data for NT children (since it is a minority\nclass). We obtained dataset B, which contains 700 videos\n(cf. Figure 2)."}, {"title": "3.4. Feature Extraction", "content": "The input of our models consists of a sequence of k\nframes, where each frame contains a feature vector of size"}, {"title": "4. Experimental Setup", "content": null}, {"title": "4.1. Data Preprocessing", "content": "Mobile data is inherently noisy, requiring robust feature\nengineering to extract informative sequences for learning.\nOur feature engineering pipeline, depicted in Figure 4, ad-\ndresses several challenges.\nAt the start and end of videos, our features of interest are\noften missing due to camera stabilization or game initiation\nwith the child. To mitigate this, we truncate frames where\nno face is detected."}, {"title": "4.2. Model Training", "content": "The resulting dataset was split at the child level into\ntraining, validation, and test sets to prevent data leakage,\nespecially for children with multiple videos. Split details\nare summarized in Table 3. To ensure fairness and repre-\nsentativeness, we stratified the splits by age group (1-4, 5-8,\n9-12) and gender (Male, Female, Other), summarized in Ta-\nble 4. This approach may result in an unconventional data\ndistribution but ensures consistent and accurate representa-\ntion across these demographic factors."}, {"title": "5. Results and Discussion", "content": null}, {"title": "5.1. Effect of Feature Engineering", "content": "Table 5 shows the impact of feature engineering on the\nperformance of three models (Eye, Head, Face) by com-\nparing their AUC and F1-scores before and after applying\nthe pipeline. The Eye model shows the most significant\nimprovementthough its F1-score slightly decreases, indicat-\ning some trade-offs. The Face model exhibits minimal im-\nprovement, with its AUC modestly rising and the F1-score\ndecreasing, showing limited effectiveness of the feature en-\ngineering."}, {"title": "5.2. Performance comparison of our models", "content": "The eye and head models achieved strong predictive\npower with test AUCs of 0.86 and 0.78, respectively (Fig-\nure 7d). The facial landmarks model had moderate predic-\ntive power with a test AUC of 0.67. Table 6 summarizes the\nperformance metrics for each model.\nThe confidence intervals in Table 6 were obtained using\nbootstrapping. We resampled the test set 1000 times, cal-\nculating each sample's metrics. The 2.5th and 97.5th per-\ncentiles of these metrics provide a 95% confidence interval.\nThe eye model outperforms the facial landmarks and\nhead pose models, with narrower confidence intervals in-\ndicating consistent performance. The eye model's high F1-\nscore reflects its balanced precision and recall, demonstrat-\ning the best overall performance.\nCombining the three modalities enhanced predictive\npower. The late fusion models, particularly the averag-\ning method (test AUC of 0.90) and the linear method (test\nAUC of 0.84), performed strongly. The intermediate fusion\nmodel performed poorly (test AUC of 0.55).\nWe also tested Two-by-two feature combinations. For\nlate fusion by averaging, the Eye+Head model achieved an\nAUC of 0.87, Face+Head 0.78, and Eye+Face 0.87. For late\nfusion with a linear layer, Eye+Head achieved an AUC of\n0.82, Face+Head 0.67, and Eye+Face 0.90. The Eye+Face\ncombination excelled in both methods, leveraging eye gaze\nand facial features for robust predictions."}, {"title": "5.3. Fairness evaluation", "content": "We evaluated our models for age and gender sensitivity,\nexcluding geographic locations due to insufficient demo-\ngraphic parity differences and equalized odds differences,\nwhere a lower parity difference indicates more evenly dis-\ntributed positive outcomes across groups, while a lower\nequalized odds difference indicates more evenly distributed\nerror rates.\nThe Eye Model performs well 8for age groups 1-4 and\n9-12 but struggles with age group 5-8, showing moderate\nfairness issues (Demographic Parity Difference: 0.1732,\nEqualized Odds Difference: 0.1569). The Face and Head\nModels perform well for age group 1-4 but poorly for age\ngroup 9-12, with significant fairness challenges (Equalized\nOdds Difference: 0.7460). The Late Fusion (Avg) model\nshows improved performance and fairness across all age\ngroups (Demographic Parity Difference: 0.1826, Equalized\nOdds Difference: 0.1250).\nRegarding gender, the Eye Model performs slightly bet-\nter for females 9, with balanced fairness metrics (Demo-\ngraphic Parity Difference: 0.1078, Equalized Odds Differ-\nence: 0.0268). The Face and Head Models show more\ngender disparities, with lower performance for females\nand higher fairness differences (Demographic Parity Dif-\nference: 0.2888, Equalized Odds Difference: 0.4196). The\nLate Fusion (Avg) model improves gender performance and\nfairness (Demographic Parity Difference: 0.2071, Equal-\nized Odds Difference: 0.0769), while the Late Fusion (Lin-\near) model offers the best balance of performance and fair-\nness (Demographic Parity Difference: 0.1979, Equalized\nOdds Difference: 0.0769). Given the importance of early\ndiagnosis, the Late Fusion (Linear) model is the most bal-\nanced and fair option for both age and gender groups. Fair-\nness mitigation techniques applied to the Head and Face\nModels for the age group 9-12 led to marginal improve-\nments, so we focused on the fairer and more effective Late\nFusion Methods."}, {"title": "5.4. Net Benefit Analysis", "content": "The Net Benefit Analysis curve (Figure 8) compares dif-\nferent models across various thresholds, illustrating their\nperformance in terms of net benefit. High sensitivity en-\nsures most children with ASD are correctly diagnosed for\ntimely interventions, while high specificity prevents incor-\nrect diagnoses of NT children, avoiding unnecessary inter-\nventions. This balance ensures a reliable and practical diag-\nnostic process, efficiently allocating resources.\nThe Late Fusion (Avg) model consistently offers higher\nnet benefits, with the Late Fusion (Linear) and Eye Model\nperforming well at varying thresholds. The Face and Head\nModels show lower net benefits. The dataset's skewed ASD\nprevalence affects the generalizability of these findings."}, {"title": "6. Limitations and Future Directions", "content": "Our current models face several limitations. First, incor-\nporating accelerometer data can mitigate data drifts, ensur-\ning more reliable predictions. Secondly, developing an au-\ntomated preprocessing pipeline is crucial for handling chal-\nlenges such as detecting when a child is too far or close to\nthe camera, managing multiple faces, and tracking different\nchildren in videos. As we continue to collect more data,\nan age-centric approach will allow us to tailor games and\nfeatures to different age groups, thereby enhancing both en-\ngagement and the informativeness of the features extracted.\nMoreover, expanding our models to incorporate additional\nmodalities, such as speech, will provide us with a richer\nunderstanding of subjects. Additionally, focusing on time-\nseries pre-training and improving interpretability will en-\nhance robustness and transparency, making models more\nreliable and understandable. To ensure fairness, it is cru-\ncial to collect skin color data and generalize across differ-\nent skin tones, thereby promoting equity and reducing bias\nin our predictive outcomes."}, {"title": "A. Appendix", "content": "All thresholds are summarized in Table 10"}]}