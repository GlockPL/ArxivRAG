{"title": "Leveraging Retrieval Augment Approach for Multimodal Emotion Recognition Under Missing Modalities", "authors": ["Qi Fan", "Hongyu Yuan", "Haolin Zuo", "Rui Liu", "Guanglai Gao"], "abstract": "Multimodal emotion recognition utilizes complete multimodal information and robust multimodal joint representation to gain high performance. However, the ideal condition of full modality integrity is often not applicable in reality and there always appears the situation that some modalities are missing. For example, video, audio, or text data is missing due to sensor failure or network bandwidth problems, which presents a great challenge to MER research. Traditional methods extract useful information from the complete modalities and reconstruct the missing modalities to learn robust multimodal joint representation. These methods have laid a solid foundation for research in this field, and to a certain extent, alleviated the difficulty of multimodal emotion recognition under missing modalities. However, relying solely on internal reconstruction and multimodal joint learning has its limitations, especially when the missing information is critical for emotion recognition. To address this challenge, we propose a novel framework of Retrieval Augment for Missing Modality Multimodal Emotion Recognition (RAMER), which introduces similar multimodal emotion data to enhance the performance of emotion recognition under missing modalities. By leveraging databases, that contain related multimodal emotion data, we can retrieve similar multimodal emotion information to fill in the gaps left by missing modalities. Various experimental results demonstrate that our framework is superior to existing state-of-the-art approaches in missing modality MER tasks. Our whole project is publicly available on GitHub.", "sections": [{"title": "1 Introduction", "content": "Multimodal Emotion Recognition (MER) aims to recognize and understand human emotions by integrating multiple modalities such as text, audio, and video. By fusing information from different modalities, MER provides a more comprehensive and accurate sentiment analysis, which plays a crucial role in applications like video conferencing, virtual assistants, and social media analysis, thereby enhancing the emotional understanding and human-computer interaction experience of systems (Moin et al., 2023; Chandrasekaran et al., 2021). However, in real-world scenarios, MER tasks frequently encounter situations where certain modalities are missing due to factors such as sensor failures or network bandwidth issues causing data corruption or loss. This significantly increases the difficulty of accurately understanding emotions (Hazarika et al., 2020; Zhao et al., 2021). Existing approaches to address this problem primarily focus on two strategies: (1) reconstructing the missing modality representations, and (2) learning robust multimodal joint representations using the available modalities. For instance, Zuo et al. (2023) employed autoencoders to reconstruct missing modality representations and learn robust joint representations based on modality-specific and modality-invariant features; Liu et al. (2024) utilized contrastive methods to extract modality-invariant features and reconstruct missing modalities under different missing conditions. Despite these efforts, the negative effects of missing modalities can still adversely affect the performance of multimodal learning, leading to incorrect predictions. This is because most existing methods either assume that the missing modalities can be accurately reconstructed from the available data or that the available modalities contain sufficient complementary information to compensate for the missing ones, effectively eliminating the noise introduced by missing data. However, these assumptions may not always hold, especially when the missing information is substantial or when the available modalities lack sufficient correlation with the missing ones. Consequently, the information"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 MER Under Missing Modalities", "content": "Existing methods for multimodal emotion recognition under missing modalities can mainly summarized into two kinds: generation or reconstruction of missing modalities; and learning robust multimodal joint representations. Notably, these approaches have been used together in recent studies."}, {"title": "2.1.1 Generation and reconstruction methods", "content": "This method concentrates on obtaining the feature of missing modalities through available modalities. Tran et al. (2017) proposed the Cascaded Residual Autoencoder (CRA) that leverages a residual mechanism within an autoencoder framework to effectively restore incomplete data from corrupted inputs. Cai et al. (2018) utilized an encoder-decoder structure, to generate missing modalities (positron emission tomography, PET) from existing modalities (magnetic resonance imaging, MRI). Zhao et al. (2021) combined the CRA with cycle consistency loss for cross-modal imputation and imagined the features of missing modalities. Fan et al. (2023) used a variational autoencoder (VAE) to reduce the effect of noise and generate multimodal joint representations. Liu et al. (2024) proposed a contrastive learning method to acquire modality-invariant features and used for missing modality imagination."}, {"title": "2.1.2 Multimodal joint representation learning", "content": "This approach leverages robust multimodal joint representation to mitigate the negative effects of missing modalities. Pham et al. (2019) took incomplete utterances as input and learned utterance-level representations through cyclic translation to ensure robustness to the missing modalities. Zuo et al. (2023) learned robust multimodal joint representation with modality-specific and modality-invariant features. Graph network was used to capture the speaker and time dependence, which improves the accuracy of conversation emotion recognition under the condition of missing modalities situations (Lian et al., 2023a)."}, {"title": "2.2 Retrieval augment methods", "content": "Retrieval-Augmented (RA) methods have received increasing attention in recent years, effectively addressing some of the limitations of generative models, such as knowledge updating and long-tailed data coverage, by introducing a retrieval mechanism and integrating external knowledge or context into the generation process (Chen et al., 2024; Lewis et al., 2020). Initially, RA methods were mainly applied to text generation tasks like question answering, dialogue systems, and summarization (Zhao et al., 2024). By incorporating retrieved relevant documents during the generation phase, these methods produce more accurate and informative content, especially in scenarios requiring large amounts of external knowledge. Beyond the textual domain, RA methods have also made significant progress in multimodal tasks involving audio and video. In audio tasks, the quality of audio generation is enhanced by retrieving relevant acoustic features or textual descriptions (Yuan et al., 2024). In video tasks, the quality of subtitle generation for first-person videos can be improved by retrieving relevant third-person videos (Xu et al., 2024). Previous research has shown that the retrieval"}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Data Preparation", "content": null}, {"title": "3.1.1 Dataset", "content": "We utilize the MER2024 multimodal emotion dataset (Lian et al., 2024) as the data source for our research, which is an extension of the MER2023 dataset (Lian et al., 2023b). MER2024 is an extensive Chinese multimodal emotion dataset comprising 6 emotion categories, 5,030 labeled samples, and 115,595 unlabeled data samples. includes a wide range of film and TV show clips, which have been carefully selected to enhance the diversity and representativeness of the data. Additionally, filtering steps are employed to reduce noise and eliminate irrelevant content, thereby improving data quality and the accuracy of emotional labeling. The emotion labels are assigned by multiple emotion recognition experts through a rigorous labeling and cross-verification process, ensuring the high quality and reliability of the dataset. We employ pretrained language/vision/audio models to obtain text/video/audio embeddings. The preprocessing of the dataset and feature extracting process are placed in the Appendix A. Finally, we obtain the embeddings with audio, visual, and text"}, {"title": "3.1.2 Dataset division", "content": "We divide the MER2024 dataset into four scales,. The inclusion relationships between different parts are exhibited in Fig. 2. \"MER_small\" contains all the labeled data (5,030 samples) in the MER2024 dataset. This part of the data can identify distinct emotional categories and is reliable (Lian et al., 2023b). \u201cMER_medium\" consists of about half of all the unlabeled samples (57,780/115,595), which are randomly selected. This scale balanced the size of the retrieval database and the retrieval time. \u201cMER_large\" includes all the unlabeled samples (115,595) in the MER2024 dataset. \"MER_turbo\u201d involves all the labeled and unlabeled data (120,625 samples)."}, {"title": "3.2 Retrieval Augment MER Under Missing Modalities", "content": "Our approach is based on the assumption that, in most cases, different modalities in multimodal emotion data express correlated and similar emotions rather than irrelevant ones. Under this circumstance, we employ the retrieval approach to introduce more emotional information to help emotion recognition and reduce the negative influence of information loss. The whole structure of our RAMER framework includes three stages: Full-modality pretraining, retrieval vector store construction, and missing modality training. In the full-modality pretraining stage, we employ the complete labeled data to train a base MER model, ensuring that the model captures comprehensive unimodal emotion features. In the second stage, we use the pretrained model saved in the first stage to reason about the entire dataset (including labeled and unlabeled data) and save the unimodal emotion hidden features before each unimodal classifier. Finally, we proceed to the missing-modalities training stage, where we train the model to handle scenarios with missing modalities."}, {"title": "3.2.1 Full-modality pretraining", "content": "We first employ three Modality Encoder Networks to capture unimodal emotion features for each modality under complete modalities. They share the same structure of a one-layer Transformer Encoder to encode the embedding, a linear layer to map features to certain dimensions, and a classifier. This stage is trained on the labeled data. After training, we save the best model weights of the epoch that performed best on the test set."}, {"title": "3.2.2 Retrieval database construction", "content": "We utilize the pretrained model saved in the first stage to infer the whole dataset and respectively save tri-modal emotion hidden features hs to three hidden feature databases $D_a$, $D_v$, and $D_t$. hs comes from the output of the last layer before the classifier. Note that the hidden features are"}, {"title": "3.2.3 Missing modality training", "content": "In the final stage, we train the model under various missing modalities. Previous research has proved that the addition of full-modality knowledge helps to improve performance in the absence of modalities. Therefore, we first load the pretrained model saved in stage one to leverage its knowledge and send the missing modality data $F_{miss}$ into the model and obtain the multimodal hidden feature under missing modalities $h_{miss}$. We take text and video modality as missing modalities and the audio modality as an available modality for an example. As Fig. 1 (c) shows, $F_a$, $F_{miss}^t$, and $F_{miss}^v$ are sent to the model and obtain $h_a$, $h_{miss}^t$, and $h_{miss}^v$. Then we use the available modality emotion hidden feature $h_a$ to retrieve the data in $D_a$ that we built in the second stage, where $D_a = \\{h_a^1,h_a^2,..., h_a^N\\}$, $h_a^i \\in R^d$, i = 1, 2, . . ., N. The cosine similarity is employed to retrieve the database:\n$Similarity(h_a, h_{similar}) = cos(\\theta) = \\frac{h_a \\cdot h_{similar}}{||h_a|| ||h_{similar} ||}$"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Baselines", "content": "To evaluate the performance of our proposed approach, we select the following state-of-the-art missing modality multimodal emotion recognition methods as the baselines. The implementation details are listed in Appendix B. AutoEncoder (AE) (Bengio et al., 2006) has been used widely in solving missing modality problems (Wong et al., 2014). It used a self-supervised approach to impute missing data from input incomplete data. Following previous work (Lian et al., 2023a), we optimize the reconstruction loss of the autoencoder and the classification loss jointly in our implementation. Cascaded Residual Autoencoder (CRA) (Tran et al., 2017) is a strong baseline that combines a series of residual autoencoders to restore missing data from corrupted inputs. MMIN (Zhao et al., 2021) performs well in missing modality problems, which combines CRA with cycle consistency loss to learn latent representations of missing modalities. IF-MMIN (Zuo et al., 2023) is an enhancement of MMIN. It utilizes modality-specific and modality-invariant features to extend the performance of the MMIN. CIF-MMIN (Liu et al., 2024) learns modality-invariant features through the contrastive learning method and imagines the missing modalities."}, {"title": "4.2 Ablation study", "content": "We design several ablation studies to prove the effectiveness of our framework For the experiment \"w/o retrieval\u201d, we remove the retrieval process in Fig. ??c and concatenate the $h_a$, $h_{miss}^t$, $h_{miss}^v$ directly. For the experiment \"Unimodal Fs\u201d, we select raw embedding $F_a$, $F_v$, $F_t$ instead of the emotion hidden feature $h_a$, $h_v$, $h_t$ as the source to construct the vector database. The experiment \"Multimodal h\u201d takes the multimodal fusion strategy in the pretrain stage instead of the unimodal pretraining for comparison and utilizes emotional hidden features for the database construction. For experiment \u201cEuclidean", "MER_turbo_top1\" only takes the top1 similar feature while not employing the top-K fusion method in Sec. 3.2.3. For the rest of the experiments we validate the generalization performance of our method through using various scales and scopes of vector databases, also various top-K value selections.\"\n    },\n    {\n      \"title\"": "4.3 Evaluation metrics"}, {"content": "For both datasets, we follow Liu et al. (2024) and other previous works to employ weighted accuracy (WA) (Baidari and Honnikoll, 2020) and unweighted accuracy (UA) (Gupta et al., 2020) to evaluate performance. Due to class imbalance in the dataset, we choose WA as the preferred metric."}, {"title": "5 Results", "content": null}, {"title": "5.1 Main results", "content": "Table 3 shows that our model achieves significant performance improvements under all missing modality conditions. The performance gains are particularly notable when the text modality is involved (l, al, vl), indicating that the text modality provides rich emotional information and enables the retrieval of more reliable emotional features. The audio modality follows in importance. For example, in the last two rows of Table 3, the model achieves the largest improvement, reaching 26.55%, when only the text modality is available. Even when only the audio or video modality is available, the model still shows over a 7% improvement compared to the baselines, demonstrating the"}, {"title": "5.2 Ablation results", "content": "From Table 4, experiment \"w/o retrieval\u201d exhibits the outstanding performance of our approach. On row 4 and 5, the utilizing of unimodal Fs and multimodal h, shows the significant influence of source feature when constructing the retrieval database. The results indicate that directly using raw embeddings to create a retrieval database performs poorly. This may be due to the fact that raw embeddings tend to capture shallow features such as semantics, rather than deeper features like emotions. To effectively capture emotional tendencies, it is necessary to train deep models and apply emotion-related constraints to the learned features. The experiments also reveal that multimodal emotional hidden features do not perform well as unimodal ones in retrieval tasks. This could be attributed to two main reasons: (1) Emotional features are relatively high-level and complex, and the multimodal hidden features learned by current pre-trained models are insufficient for retrieving similar emotions. (2) The modality gap between different types of data makes cross-modal alignment challenging. As a result, the incorporation of multimodal interactions during pre-training may cause emotional features to become confused in unimodal retrieving, leading to a decrease in retrieval accuracy. Row 6 highlights the effect of similarity algorithm selection. Similarity algorithms that are not suitable for data distribution and type may result in a significant decrease in retrieval accuracy. The experiment indicates that the top1 similar feature sometimes may also not share similar emotions with the query feature and lead to misjudgment. However, the fusion strategy mitigates this situation: one similar feature may deviate from the original emotion, but more similar features may bring the retrieval results back to the correct distribution. From row 8 to the end, our method demonstrates a fluctuation range of only 2.28% in WA across different sizes of retrieval databases, varying data scopes, and different top-k values (best performance: 85.13%, worst performance: 82.85%). This indicates that our method maintains a considerable level of generalization performance across various retrieval conditions."}, {"title": "5.3 Visualization", "content": "We randomly select 1,000 samples for each modality in the vector database and visualize their distribution utilizing the T-SNE algorithm. From we observe that audio and text modality features are approximately clustered into six groups based on emotion categories, with samples of the same category positioned closer together. This demonstrates that our method effectively learns and represents the emotional features of the data. The clustering effect of the video modality features is less distinct compared to the first two modalities. This could be because the visual differences between certain emotion categories are not pronounced, for example, surprise and happy, making it more difficult for the model to distinguish between them."}, {"title": "6 Conclusion", "content": "This paper introduces a novel method for solving multimodal emotion recognition under missing modalities. We construct multimodal emotional hidden feature databases on the basis of the full-modality pretraining and utilize the retrieval augment approach to fill up the missing modalities and alleviate the information loss. Various experimental results exhibit the advantages of our approach over previous works. We intend to consider learning more robust emotional features and achieve more reliable multimodal emotion recognition under missing modalities."}, {"title": "7 Limitation", "content": "Despite the promising results of our proposed method, there are two key limitations that need to be addressed in future work. First, the current approach does not take retrieval time into consideration. As the size of the database increases, the retrieval time also grows correspondingly, which may present challenges in practical applications. Second, the retrieved content still requires filtering. Current feature fusion strategy may inadvertently incorporate irrelevant emotional features, which can lead to a decline in overall performance. Developing a more accurate feature selection algorithm will be crucial for improving the performance."}, {"title": "A Feature extraction", "content": "We follow the procedure outlined in the MER2024 Baseline (Lian et al., 2024) and extract utterance-level features. Initially, we preprocess the data by cropping and aligning the facial regions of each frame using the OpenFace toolkit (Baltrusaitis et al., 2018). Next, we employ the CLIP model (Radford et al., 2021) to extract frame-level features for each face image. These visual features are aggregated using average pooling to generate video-level embeddings. For the audio processing, we use the FFmpeg toolkit to separate the audio from the video at a sampling rate of 16 kHz. We then utilize the Chinese-HuBERT-Large model (Hsu et al., 2021; Guo and Liu, 2022) to extract acoustic features, leveraging its superior performance on Chinese sentiment corpora. The final acoustic features are obtained by averaging the hidden representations from the last four layers of the model. Additionally, we transcribe the audio files into text using WeNet (Yao et al., 2021), an open-source automatic speech recognition toolkit. For textual feature extraction, we use the Baichuan2-13B-Base model (Baichuan, 2023), which has been pretrained on a large-scale corpus."}, {"title": "B Implement Details", "content": "We split labeled data into train/validation/test sets, and the ratio of data set segmentation is 8:1:1 and"}]}