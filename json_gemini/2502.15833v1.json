{"title": "ADVANCING OUT-OF-DISTRIBUTION DETECTION VIA LOCAL NEUROPLASTICITY", "authors": ["Alessandro Canevaro", "Julian Schmidt", "Mohammad Sajad Marvi", "Hang Yu", "Georg Martius", "Julian Jordan"], "abstract": "In the domain of machine learning, the assumption that training and test data share\nthe same distribution is often violated in real-world scenarios, requiring effective\nout-of-distribution (OOD) detection. This paper presents a novel OOD detection\nmethod that leverages the unique local neuroplasticity property of Kolmogorov-\nArnold Networks (KANs). Unlike traditional multilayer perceptrons, KANs ex-\nhibit local plasticity, allowing them to preserve learned information while adapt-\ning to new tasks. Our method compares the activation patterns of a trained KAN\nagainst its untrained counterpart to detect OOD samples. We validate our ap-\nproach on benchmarks from image and medical domains, demonstrating superior\nperformance and robustness compared to state-of-the-art techniques. These results\nunderscore the potential of KANs in enhancing the reliability of machine learning\nsystems in diverse environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Most machine learning algorithms operate under the assumption that training and test data share the\nsame distribution. However, this assumption frequently fails in real-world scenarios where models\nencounter out-of-distribution (OOD) data-samples that deviate from the training distribution, such\nas those belonging to novel categories. This mismatch can significantly impair a model's accuracy\nand reliability. As a result, OOD detection has become a critical area of research, aiming to discern\nwhen inputs fall outside the scope of the distribution used for training. Effective OOD detection not\nonly enhances a model's robustness by identifying and handling these anomalous inputs but also\nensures that the model maintains reliable performance on known, in-distribution data (Yang et al.,\n2022).\nOOD detection poses a significant challenge due to the diverse nature of OOD types. While many\nOOD detectors excel when tested against specific OOD datasets, they often struggle to maintain\nhigh performance across a broad range of OOD samples. As stated by Zhang et al. (2023a) there\nis no single winner that always outperforms others across multiple datasets. One reason for this\ninconsistency is that OOD instances can vary widely, from subtle variations near the distribution\nboundary to completely dissimilar and far-off examples. As a result, developing a universal OOD\ndetection method that performs robustly across multiple datasets, spanning near to far OOD samples,\nremains challenging.\nIn this paper, we present a novel OOD detection method using Kolmogorov-Arnold Networks\n(KANs) (Liu et al., 2024b). KANs are neural networks with a unique architecture that enhances in-\nterpretability, improves the accuracy-to-parameter ratio, and mitigates catastrophic forgetting com-\npared to multilayer perceptrons (MLPs). Our approach takes advantage of KANs' distinctive prop-\nerty of local neuroplasticity-a characteristic absent in traditional MLPs due to their reliance on\nshared, non-trainable activation functions. In contrast, KANs demonstrate local plasticity owing to\ntheir spline-based architecture. This characteristic ensures that learning a new task impacts only the\nregions of the network activated by the training data, thereby preserving the integrity of distant and\nunrelated regions."}, {"title": "2 KAN-BASED OOD DETECTION", "content": "This section begins by providing a short background on KANs and their working principle. Next,\nwe delve into the core concept underlying our proposed method for OOD detection. Finally, we\ndescribe the primary limitation of the KAN detector and propose a strategy to enable its deployment\nin complex real-world scenarios."}, {"title": "2.1 BACKGROUND", "content": "KANs are neural network architectures based on the Kolmogorov-Arnold representation theorem.\nThis theorem states that any continuous multivariate function can be represented as a sum of con-\ntinuous functions of a single variable. Hence, KANs approximate high-dimensional functions using\nsimpler, univariate components, effectively addressing the curse of dimensionality in machine learn-ing.\nIn practice, KANs decompose multivariate functions into univariate B-spline functions with learn-\nable coefficients. Let \\(x_p\\) be the p-th component (feature) of the input vector \\(x \\in \\mathbb{R}^{n_{in}}\\) and let \\(y_q\\)\nbe the q-th component (feature) of the output vector \\(y \\in \\mathbb{R}^{n_{out}}\\. A KAN layer transforms x into y\nusing a matrix of univariate functions \\(\\Phi = {\\phi_{p,q}}\\) where each \\(\\phi_{p,q}\\) is parameterized by a B-spline.\nEach B-spline consists of a linear combination of G + k B-spline basis functions with learnable\ncoefficients \\(c_{p,q,i}\\). The spline order is denoted as k (usually k = 3) and G is the grid size.\n\\[\ny_q = \\Sigma_p \\Phi_{p,q}(x_p) \\quad \\text{with:} \\quad \\Phi_{p,q}(x_p) = \\Sigma_{i=0}^{G+k} c_{p,q,i} B_i(x_p).\n\\]\nKAN layers can be stacked to construct deeper networks, allowing for complex transformations\nacross multiple stages. Performance is further enhanced by incorporating residual connections,\nwhich add flexibility to the spline functions through trainable weights and additional basis func-tions (Liu et al., 2024b).\nLocal neuroplasticity in KANs is facilitated by two key properties. First, each input feature \\(x_p\\)\nis processed independently by its own set of activation functions \\({\\phi_{p,q} | \\forall q}\\). Second, during"}, {"title": "2.2 OOD DETECTION WITH KANS", "content": "We propose leveraging the local plasticity of KANs for OOD detection. The InD data seen dur-\ning training only affects specific regions (spline grid coefficients) of the network. By determining\nwhether a region contains InD data and inspecting which regions are activated by each sample,\nthe KAN-based detector can distinguish between InD and OOD samples. This differentiation is\nachieved by comparing the output of the trained activation functions with their values prior to train-ing. The step-by-step procedure is as follows:\n\\begin{itemize}\n    \\item Setup: Initialize an untrained KAN and create a copy. Train one KAN with the InD dataset\n    while keeping the other untrained.\n    \\item Detection: Perform a forward pass on both networks with the given sample x, and save the\n    output of the activation functions:\n\\[\n\\Phi^{\\text{trained}}_{p,q}(x_p), \\quad \\Phi^{\\text{untrained}}_{p,q}(x_p)\n\\]\n    \\item Compute the difference between the responses:\n\\[\n\\Delta_{p,q}(x_p) = \\Phi^{\\text{trained}}_{p,q}(x_p) - \\Phi^{\\text{untrained}}_{p,q}(x_p).\n\\]\n    \\item Analyze the difference matrix \\(\\Delta\\). OOD samples will tend to have a higher ratio of the\n    entries in the \\(\\Delta\\) matrix close to zero. To obtain a scalar InD score S(x), we aggregate the\n    differences using a scoring function \\(F_{\\text{score}}\\) (detailed in Appendix A.1):\n\\[\nS(x) = F_{\\text{score}}(\\Delta(x)).\n\\]\n\\end{itemize}\nTo clarify our method's working principle, let us rewrite \\(\\Delta_{p,q}(x_p)\\) using Eq. 1:\n\\[\n\\Delta_{p,q}(x_p) = \\Sigma_i c^{\\text{trained}}_{p,q,i} - c^{\\text{untrained}}_{p,q,i} B_i(x_p).\n\\]\nThe terms \\(c^{\\text{trained}}_{p,q,i} - c^{\\text{untrained}}_{p,q,i}\\) define the locations within the network where InD information is\nstored, while \\(B_i(x_p)\\) serves as a mask and specify the regions activated by the sample x. Con-\nsequently, multiplying these two terms provides a quantitative measure of the overlap between InD\nregions and the given sample. This overlap is subsequently utilized to compute the InD score.\nOnce the InD score is obtained, it is thresholded to classify the sample as InD or OOD:\n\\[\nD(x) = \\begin{cases}\n    \\text{InD}, & \\text{if } S(x) \\geq \\lambda \\\\\n    \\text{OOD}, & \\text{if } S(x) < \\lambda,\n\\end{cases}\n\\]\nwhere \\(\\lambda\\) is a predefined threshold. A test sample with a InD score less than \\(\\lambda\\) is categorized as OOD.\nOtherwise, it is classified as InD."}, {"title": "2.3 CAPTURING THE JOINT FEATURE DISTRIBUTION", "content": "Like MLPS, KANs are capable of processing multivariate inputs and producing multivariate outputs.\nHowever, differently from MLPs where activation functions receive a weighted sum of all inputs,\nin KANs, each activation function receives only a single input. While this characteristic allows the\nKAN detector to effectively capture the marginal distributions of input features, it also constrains its\nability to model the joint distribution of features."}, {"title": "3 EXPERIMENTS", "content": "First, we describe the benchmarks, metrics, and implementation details used in our study. The re-sults demonstrate the superior performance of our method, highlighting its key advantages. Finally,a comprehensive ablation study analyzes each hyperparameter and component, elucidating their im-pact on performance."}, {"title": "3.1 EVALUATION PROTOCOL", "content": "Setup. The evaluation of the proposed method is performed on seven different benchmarks from\ntwo different domains: OOD detection in images and tabular medical data.\nFor OOD detection in images, the experimental setup adheres to the OpenOOD (Yang et al., 2022)\nbenchmark protocol. We evaluate the KAN detector on the CIFAR-10 benchmark, using CIFAR-\n10 (Krizhevsky et al., b) as the InD dataset. The OOD datasets are categorized into near OOD\ndatasets (CIFAR-100 (Krizhevsky et al., a) and Tiny ImageNet (TIN) (Le & Yang, 2015)) and far\nOOD datasets (MNIST (Deng, 2012), SVHN (Netzer et al., 2011), Textures (Cimpoi et al., 2014),\nand Places365 (Zhou et al., 2018)). The CIFAR-100 benchmark contains the same datasets as the\nCIFAR-10 benchmark except for the CIFAR-10 and CIFAR-100 datasets which have an inverted\nrole (CIFAR-100 as training data and CIFAR-10 as OOD dataset). To evaluate the scalability of\nour method, we also tested it on the ImageNet-200 FS and ImageNet-1K FS benchmarks. Com-\npared to CIFAR-10 and CIFAR-100, these benchmarks features five to twenty times more training\nimages, each with a size seven times larger. The full-spectrum version increases the detection chal-lenge and, at the same time, makes it closer to real-world applications by enriching the InD testset with covariate-shifted InD samples (Yang et al., 2023). The datasets used in this benchmark are:ImageNet-200 or ImageNet-1K (Deng et al., 2009) as training set, ImageNet-V2 (Recht et al., 2019),ImageNet-C (Hendrycks & Dietterich, 2019), ImageNet-R (Hendrycks et al., 2021) as covariate-shifted InD test set, SSB-hard (Vaze et al., 2022), NINCO (Bitterwolf et al., 2023) as near OOD,andiNaturalist (Van Horn et al., 2018), Textures, OpenImage-O (Wang et al., 2022) as far OOD.\nFor OOD detection in tabular medical data, we follow the benchmark proposed by Azizmalayeri et al. (2023). We consider the benchmarks derived from the eICU dataset (Pollard et al., 2018),which contains clinical data of tens of thousands of Intensive Care Unit (ICU) patients in severalhospitals. In the near OOD benchmarks, the eICU dataset is divided into InD and OOD according to some features such as ethnicity (Caucasian as InD) or age (older than 70 as InD). The featureused for splitting the dataset is then removed. In the synthetic OOD benchmark, the OOD data isgenerated by scaling a single feature from the InD set by a factor F. For each factor, the experimentis repeated 100 times with different features, to minimize the impact of the chosen feature. Byvarying the scaling factor, the generated samples range from near to far OOD.\nIn contrast to training-time regularization methods (e.g., MOS (Huang & Li, 2021), CIDER (Minget al., 2023)), our detector operates in a post-hoc manner and can be seamlessly integrated with anypre-trained classifier, regardless of model architecture, training procedures, or types of OOD data.The backbone is used to perform the classification or regression task and in the case of post-hocmethods it is trained independently from the detector. The OOD detector only uses the latent fea-tures of the backbone for InD/OOD classification. The considered OpenOOD benchmarks employa pre-trained ResNet backbone (He et al., 2015) for feature extraction, while the tabular medicalbenchmarks use an FT-Transformer backbone (Gorishniy et al., 2021)."}, {"title": "3.2 RESULTS", "content": "Table 1 presents the results of our experiments on the CIFAR-10 and CIFAR-100 benchmarks, com-paring the KAN detector with several SOTA OOD detection methods (see Appendix A.5 for a listof all the considered baselines). On top of the numerous baselines provided by the benchmark wealso compare our approach to the current best post-hoc method on the CIFAR leaderboard: the NAC(Liu et al., 2024a). The results show that the KAN detector outperforms all previous methods onboth benchmarks, demonstrating the effectiveness of leveraging spline-based local activation func-tions for OOD detection. In each column of this and the following tables, we highlight in boldthe best-performing method. Where multiple seeds of the backbone are available we also highlight"}, {"title": "3.3 ABLATION STUDY", "content": "Parameter analysis. The main hyperparameters that regulate the performance of the proposedmethod are the number of partitions P and the grid size G. Table 7 illustrates the variations inAUROC performance as a function of the number of partitions obtained through k-means clustering.Increasing P enhances the detector's ability to capture the joint distribution of features, resulting"}, {"title": "4 RELATED WORK", "content": "This section reviews recent advancements in OOD detection, provides an overview of the latest in-novations to enhance KAN performance, and explores the diverse sectors where KANs have demon-strated successful applications."}, {"title": "4.1 OUT-OF-DISTRIBUTION DETECTION", "content": "OOD detection focuses on identifying instances with semantic shifts, a special case of distribu-tional shift. OOD detection methods can be broadly classified into the following categories (Yanget al., 2024). Classification-based methods use the output of classification models, such as softmax"}, {"title": "4.2 KOLMOGOROV-ARNOLD NETWORKS", "content": "The recently introduced KAN (Hou & Zhang, 2024) represents a significant advancement in neuralnetwork architectures, offering a potential alternative to traditional MLPs by not only enhancingaccuracy but also leading to more interpretable models. As a result, numerous studies have tried toinnovate and refine KANs further. For example, many articles replace the spline architecture withmore efficient or accurate alternatives such as Chebyshev polynomials (SS et al., 2024), wavelet-based structures (Bozorgasl & Chen, 2024), sinusoidal functions (Reinhardt et al., 2024), and ra-dial basis functions (Li, 2024). Others try to replicate advanced neural network architectures usingKAN's characteristics. This includes convolutional neural networks (Bodner et al., 2024) and graphneural networks (Kiamari et al., 2024; Bresson et al., 2024; Zhang & Zhang, 2024), further demon-strating the versatility and potential of KANs. Applications of KANs have rapidly expanded acrossvarious domains, including time series analysis (Vaca-Rubio et al., 2024; Xu et al., 2024b), solvingordinary and partial differential equations (Koenig et al., 2024; Wang et al., 2024), hyperspectralimage classification (Seydi, 2024; Jamali et al., 2024), and computer vision (Azam & Akhtar, 2024;Li et al., 2024; Cheon, 2024). Additionally, KANs have recently been applied to fields similar toOOD detection, such as abnormality detection (Huang et al., 2024) and AI-generated image detec-tion (Anon & Emon, 2024). These studies leverage the superior accuracy and interpretability ofKANs (Liu et al., 2024b) to uncover more complex patterns in the data. While their work focuseson developing robust models that demonstrate KANs' capacity to generalize effectively to unseensamples, they do not address the detection of these samples. In contrast, we present a novel OODdetection method that leverages the unique local plasticity property of KANs, applicable to anybackbone architecture."}, {"title": "5 CONCLUSIONS", "content": "This paper introduces a novel approach to OOD detection using KANs, capitalizing on their unique\nlocal neuroplasticity property. Our method effectively differentiates between InD and OOD samples\nby comparing the activation patterns of a trained KAN against its untrained counterpart. The exper-\nimental results show that our KAN-based detector reaches SOTA performance across seven bench-\nmarks from two different domains. Importantly, our experiments show that the previous methods\nsuffer from a non-optimal InD dataset size, while our method is unaffected by these perturbations.\nThis makes the KAN detector a robust and versatile method that can maintain high performance\nacross diverse and unpredictable data distributions. Future work will further explore the effect of\ndifferent training tasks on detection performance."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 SCORING AND AGGREGATION FUNCTIONS", "content": "The trainable coefficients of the detector networks are initialized randomly. As a result, it may\noccasionally occur that some of these coefficients are initialized to the exact values they would attain\npost-training. Consequently, the training procedure does not modify these coefficients, as they are\nalready optimal. This phenomenon can lead to false positives in detection, as both the trained and\nuntrained networks might exhibit the same response to an InD sample. This issue can be mitigated\nusing a scoring function that is more robust to outliers, such as the median. This hypothesis is\nexperimentally validated in Table 8.\nThe use of the maximum function for aggregation allows us to select the detector closest to the\ntest sample, which intuitively possesses the best information for decision-making. This approach is\nexperimentally verified in Table 9."}, {"title": "A.2 DETECTION ON REGRESSION-BASED DATASETS", "content": "As the standard benchmarks used in OOD detection mainly focus on classification tasks, we test our\nmethod on three additional regression-based datasets: the California Housing dataset (Kelley Pace\n& Barry, 1997), the Wine Quality dataset (Cortez et al., 2009), and the Friedman synthetic dataset\n(Friedman, 1991). To generate the InD and OOD partitions and ensure that the OOD samples are\nsemantically different from the InD ones, we thresholded the regression (output) value. The KAN\nis then directly applied to the raw dataset features, highlighting that the method is not only effective\non regression-based tasks but also in the absence of a feature extractor backbone network. This is\nnot possible for other methods such as NAC that require the gradients of the backbone network for\ndetection. Thus, as a baseline detector method, we used KNN, which, according to the results in\nSection 3.2, is one of the best approaches across all benchmarks. Table 10 reports the detection\nresults in terms of AUROC on the three datasets, showing that the KAN detector outperforms the\nKNN baseline on all of them.\nOn the California Housing and Wine Quality datasets, we used only one partition (P) for the KAN\ndetector because a value greater than one did not improve performance. This indicates that either the\npartitioning method does not work well on regression-based datasets, possibly due to poor internal\nseparability of data clusters, or these datasets do not require the detector to capture the joint feature\ndistribution to effectively separate InD and OOD samples. To investigate this observation further,\nwe also tested our method on the Friedman dataset. Here, the regression output is generated by the\nfollowing non-linear function of the inputs:\n\\[\ny(x) = 10\\cdot \\sin(\\pi\\cdot x_0\\cdot x_1) + 20 \\cdot (x_2 - 0.5)^2 + 10\\cdot x_3 + 5\\cdot x_4 + N(0, \\sigma).\n\\]"}, {"title": "A.3 AVERAGE OVERALL METRIC", "content": "Many benchmarks (including the OpenOOD CIFAR-10 and CIFAR-100) assess OOD detectionperformance on multiple OOD datasets. However, they lack an overall average that gives a holisticoverview of the methods' performance. In our experiments, we additionally evaluate our method onthe following overall metric:\n\\[\n\\mu_{\\text{overall}} = \\frac{1}{N} \\Sigma_{i=1}^{N} \\mu_i,\\text{ overall} = \\frac{1}{N} \\Sigma_{i=1}^{N} \\sigma_i,\n\\]\nwhere \\(\\mu_i, \\sigma_i\\) are the mean and standard deviation of dataset i calculated over multiple seeds."}, {"title": "A.4 EFFECT OF KAN STOCHASTICITY", "content": "All benchmarks average results over multiple seeds to address the inherent randomness associated\nwith weight initialization in the backbone model. Our method introduces an additional layer of\nrandomness due to the KAN initialization process. To illustrate that the variability introduced by our\ndetector is significantly lower than that stemming from the backbone initialization, we conducted\nthe following experiment.\nWe repeated the CIFAR-10 benchmark using five distinct KAN initialization seeds (N = 5). For\neach KAN initialization seed i, we recorded the mean and standard deviation (\\(\\mu_i, \\sigma_i\\)) of the exper-\niment conducted on the three pre-trained backbones specified in the benchmark. The results are\nsummarized in Table 12.\nWe compute the overall standard deviation attributable to the backbone initialization (\\(\\sigma_T\\)) and that\nof our detector (\\(\\sigma_A\\)) as follows:\n\\[\n\\sigma_T = \\frac{1}{N} \\Sigma_{i=1}^{N} \\sigma_i = 0.53, \\sigma_A = \\sqrt{\\frac{1}{N} \\Sigma_{i=1}^{N} (\\mu_i - \\overline{\\mu})^2} = 0.05 \\text{ with } \\overline{\\mu} = \\frac{1}{N} \\Sigma_{i=1}^{N} \\mu_i = 94.10\n\\]\nThe calculated standard deviations \\(\\sigma_T\\) and \\(\\sigma_A\\) differ by approximately an order of magnitude, indi-\ncating that the randomness introduced by our detector has a negligible effect on the overall results."}, {"title": "A.5 BASELINES METHODS", "content": "The baselines used in the banchmarks are: OpenMax (Bendale & Boult, 2016), MSP (Hendrycks &\nGimpel, 2017), TempScale (Guo et al., 2017), ODIN (Liang et al., 2018), MDS (Lee et al., 2018),"}, {"title": "A.6 FULL BENCHMARK RESULTS", "content": "In Table 17, we present the AUROC results for all officially available baselines on the CIFAR-10 and CIFAR-100 benchmarks. Table 22 provides the results for the same set of baselines andbenchmarks using the FPR@95 metric. Similarly, Tables 18 and 23 report the AUROC and FPR@95results respectively for all the available baselines on the ImageNet-200 FS and ImageNet-1K FSbenchmarks. The same metrics are reported for all the available baselines for the tabular medicalbenchmarks in Tables 19, 20, 21 for AUROC and 24, 25, 26 for FPR@95."}, {"title": "A.7 INFLUENCE OF CLUSTERING METHOD", "content": "This experiment analyzes the effect of different clustering methods on detection performance. Weconsidered five popular clustering approaches as alternatives to k-means: spectral (Ng et al., 2001),agglomerative (Murtagh & Legendre, 2014), bisecting k-means (Rohilla et al., 2019), BIRCH(Zhang et al., 1996), and DBSCAN (Ester et al., 1996). Table 13 presents the experimental resultsfor the CIFAR-10 benchmark.\nThe results show that the choice of clustering method has a negligible impact on detection perfor-mance, except for DBSCAN, which yields an approximate 4% drop. One reason for this behavior isthat DBSCAN is the only algorithm among those considered that does not necessarily assign a clus-ter to all samples. In our implementation, these unclustered samples are grouped into an additionalcluster. However, the samples in this extra cluster do not share common characteristics and canbelong to different and distant regions of the input space. Although we are not focused on obtainingsemantically meaningful clusters, we aim to divide the InD samples into smaller regions that can beeffectively processed by a KAN. The leftover samples cluster in DBSCAN has a counterproductiveeffect, as it can span a wide region of the input space, making it difficult for the KAN to handleeffectively."}, {"title": "A.8 CAPTURING THE JOINT FEATURE DISTRIBUTION", "content": "An alternative approach to the partitioning method for capturing the joint feature distribution is toexpand the input features with additional values. We applied this technique to the 2D L-shaped toydataset, as illustrated in Figure 5. In this scenario, the two input features are concatenated withthe latent features derived from a variational autoencoder trained on the two original features, resultingin an augmented input space of size 2 + 64.\nThis method demonstrates promising results, comparable to those achieved with the partitioningmethod. However, its applicability to high-dimensional input spaces remains uncertain. We hypoth-esize that the number of required features would become excessively large, leading to computationalinefficiencies."}, {"title": "A.9 TRAINING PARAMETERS", "content": "Finding the optimal training hyperparameters for KANs can initially be challenging, as they may\nnot follow the same intuitions as MLPs and other networks (Liu et al., 2024b)."}, {"title": "A.10 SOFTWARE AND HARDWARE", "content": "All experiments are conducted on a single NVIDIA GeForce GTX 1080Ti GPU. For testing larger\nmodels and accelerating the hyperparameter optimization, we leveraged a cloud computing platform\nwith an NVIDIA A100 GPU.\nWe used Python version 3.10 together with PyTorch 2.3.1 as the deep learning framework and\nleveraged CUDA 11.8 for GPU acceleration."}, {"title": "A.11 INFERENCE TIME AND SCALABILITY ANALYSIS", "content": "Inference time. Table 14 reports the inference time of a single sample for various methods. The\nmeasurements are averaged over 1000 samples, using 100 extra samples as GPU warmup. The\nresults show a positive correlation between the inference time and the overall AUROC performance.\nAlthough the KAN method is currently the slowest among the tested ones, it is important to em-\nphasize that the KAN architecture has just been developed recently. In just a few months since its\nrelease, its performance has been steadily improving thanks to many architecture refinements (e.g.,\nreplacing splines with Gaussian radial basis functions improves forward speed by approximately a\nfactor of 3.3 (Li, 2024)). We believe that in the future, KANs will achieve efficiency comparable\nto MLPs. Furthermore, it is worth considering that inference time is not always a critical concern\nin various applications, particularly in medical contexts. In such scenarios, the enhanced detection\nperformance offered by our method positions it as a more advantageous choice compared to faster\nalternatives."}, {"title": "A.12 HYPERPARAMETERS", "content": "Table 16 reports all hyperparameters and settings for the five benchmarks. The search space of each\nhyperparameter is as follows: [10, 200] for the grid size, [1, 200] for the partitions, [0.0001, 0.1] for\nthe learning rate, [1, 100] for the epochs, and [1, 100] for the histogram bins."}]}