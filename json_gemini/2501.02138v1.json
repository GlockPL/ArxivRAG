{"title": "Effective LLM-Driven Code Generation with PYTHONESS", "authors": ["Kyla H. Levin", "Kyle Gwilt", "Emery D. Berger", "Stephen N. Freund"], "abstract": "Abstract-The advent of large language models (LLMs) has paved the way for a new era of programming tools with both significant capabilities and risks, as the generated code lacks guarantees of correctness and reliability. Developers using LLMs currently face the difficult task of optimizing, integrating, and maintaining code generated by AI. We propose an embedded domain-specific language (DSL), PYTHONESS, to address those challenges. In PYTHONESS, developers program with LLMs at a higher level of abstraction. Rather than interacting directly with generated code, developers using PYTHONESS operate at the level of behavioral specifications when writing functions, classes, or an entire program. These specifications can take the form of unit tests and property-based tests, which may be expressed formally or in natural language. Guided by these specifications, PYTHONESS generates code that both passes the tests and can be continuously checked during execution. We posit that the PYTHONESS approach lets developers harness the full potential of LLMs for code generation while substantially mitigating their inherent risks. We describe our current prototype implementation of PYTHONESS and demonstrate that it can successfully leverage a combination of tests and code generation to yield higher quality code than specifications alone.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have significantly impacted how developers write code. Tools like Copilot [1] present developers with plausible code completions and alternatives while programming. Even novice programmers can use conversational AI systems like ChatGPT [2] to create entire ap-plications from scratch, and professional software developers are able to author code much more rapidly than before [3]. However, LLMs do not necessarily reduce the total time spent programming [4], as developers redirect the time spent writing code toward guiding and verifying the LLM's output to ensure that it meets their requirements [5], [6], [7], [8]. This process can be tedious and error-prone, consisting of repeatedly testing and evaluating the code, reformulating the prompt to reflect the developer's observations, and regenerating the code. Even after this iterative development process, the code may still be considered low quality because it is: (1) too general or not generalized enough; (2) too slow or memory-intensive; (3) not syntactically correct; or (4) unable to pass integration tests. Thus, there are significant challenges facing developers wishing to use LLMs to assist in code generation.\nTo address these obstacles, we propose a domain-specific"}, {"title": "II. PYTHONESS", "content": "This section describes the current status of the implemen-tation of our PYTHONESS prototype."}, {"title": "A. Initial Prototype", "content": "The PYTHONESS prototype works by letting developers specify the intended behavior of a function via a decorator, which guides the generation of the function; Figure 3 shows an example. When a decorated target function is first invoked, the PYTHONESS system generates the code for the function's body using the description and tests embedded in the decoration. The most salient aspects of the decoration header and this process are the following.\n1) Specifying Behavior and Tests: A PYTHONESS header includes a description of the function's purpose and expected behavior in natural language. Additionally, the header typically includes a set of tests that the function should pass.\nPYTHONESS supports multiple types of tests:\n(1) Simple unit tests captured as one-line assertion statements e.g. assert f(5) == 2.\n(2) Collections of unit tests defined as a TestCase object from the built-in Python unittest library."}, {"title": "Validation:", "content": "Once PYTHONESS has a minimally-validated candidate for the function's code, it runs the function against the developer's provided tests. For the unit tests, it directly checks that the code meets the given assertions. For the property-based tests, PYTHONESS uses the Hypothesis module [14] to validate properties through fuzzing. If the code fails any of the tests or violates its invariants at any point, PYTHONESS iteratively attempts to repair the code until the maximum number of tries is exceeded or valid code is produced. If the code passes all tests, PYTHONESS uses that code for the function and caches validated code in a database on disk for use in future runs. If the developer modifies a function's specification, the LLM will regenerate its code from scratch. The code can also optionally be spliced into place in the source code and the PYTHONESS header removed entirely- the developer may use this later on to insert production-ready code."}, {"title": "B. Example", "content": "We demonstrate the capabilities of PYTHONESS by im-plementing a \"Maximum Increasing Subarrays\" function that takes in a list nums of integers and attempts to find the maximum length k such that nums contains two consecutive subarrays of length k with strictly increasing elements. This is Problem #3350 from LeetCode [10]. Instead of using a dataset like HumanEval [15] on which GPT-40 already performs with high accuracy [16] and suffers from data leakage [17], we evaluate PYTHONESS on a LeetCode problem to take advantage of the challenging problem set and private test suites.\nFigure 3 shows the signature and PYTHONESS header for the maxIncSubarrays function (the LeetCode problem description has been condensed for brevity of exposition). When we provided PYTHONESS with only the description and not the tests in the full specification, PYTHONESS produces the code in Figure 4a on its first attempt and immediately accepts it, only checking if the generated code compiles and runs. However, it is incorrect. Figure 4a highlights two errors in red: the code incorrectly searches for a single subarray of increasing values rather than two consecutive subarrays and contains an off-by-one error. This faulty code passes only 42% (469 of 1,111) of the tests in LeetCode's private test suite.\nWhen we provided PYTHONESS with the full header in Figure 3, including the unit tests, it produced the code in Figure 4b that passes all 1,111 private tests. Corrections from the previous attempt are highlighted in green. In response to failed tests, PYTHONESS iteratively refined its initial code twice to produce the validated version.\nThis function demonstrates how the addition of tests can greatly improve the quality of the generated code over using a natural language prompt alone, and how PYTHONESS can be used to validate the code produced by the LLM. In this case, the developer only provides a handful of unit tests, and PYTHONESS ensures that the generated code passes those tests, and the resulting code then passes over a thousand unseen tests. We anticipate that the use of property-based tests will further help to consistently yield a high level of quality."}, {"title": "C. Future Work", "content": "Our initial PYTHONESS prototype has demonstrated its potential to enhance the effectiveness of LLM-based code generation, and we are planning a variety of extensions to further improve its capabilities:\nGuided Code Generation: Based on past work on program sketching [18], [19], we plan to enable the programmer to"}, {"title": "Maintaining Correctness:", "content": "Programs often contain multiple functions with inter-dependent behaviors. We plan to extend PYTHONESS to support specifying, or potentially inferring, relationships between functions. This will help PYTHONESS ensure that modifications to one function's specifications are reflected in the code of all related functions so that their relationships are preserved."}, {"title": "III. CONCLUSION", "content": "In this paper, we propose PYTHONESS, a Python-embedded DSL that lets developers generate validated, high-quality code via an LLM assistant that uses tests and specifications to guide code generation. With this abstraction, developers can focus more on the behavior of their code rather than the implementation details and trust that the code produced by the LLM is robust, reliable, and efficient.\nThe PYTHONESS prototype is open-source and has been downloaded over 5,000 times. It is available at https://github.com/plasma-umass/pythoness."}]}