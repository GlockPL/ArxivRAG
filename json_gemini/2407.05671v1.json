{"title": "MSTF: Multiscale Transformer for Incomplete Trajectory Prediction", "authors": ["Zhanwen Liu", "Chao Li", "Nan Yang", "Yang Wang", "Jiaqi Ma", "Guangliang Cheng", "Xiangmo Zhao"], "abstract": "Motion forecasting plays a pivotal role in autonomous driving systems, enabling vehicles to execute collision warnings and rational local-path planning based on predictions of the surrounding vehicles. However, prevalent methods often assume complete observed trajectories, neglecting the potential impact of missing values induced by object occlusion, scope limitation, and sensor failures. Such oversights inevitably compromise the accuracy of trajectory predictions. To tackle this challenge, we propose an end-to-end framework, termed Multiscale Transformer (MSTF), meticulously crafted for incomplete trajectory prediction. MSTF integrates a Multiscale Attention Head (MAH) and an Information Increment-based Pattern Adaptive (IIPA) module. Specifically, the MAH component concurrently captures multiscale motion representation of trajectory sequence from various temporal granularities, utilizing a multi-head attention mechanism. This approach facilitates the modeling of global dependencies in motion across different scales, thereby mitigating the adverse effects of missing values. Additionally, the IIPA module adaptively extracts continuity representation of motion across time steps by analyzing missing patterns in the data. The continuity representation delineates motion trend at a higher level, guiding MSTF to generate predictions consistent with motion continuity. We evaluate our proposed MSTF model using two large-scale real-world datasets. Experimental results demonstrate that MSTF surpasses state-of-the-art (SOTA) models in the task of incomplete trajectory prediction, showcasing its efficacy in addressing the challenges posed by missing values in motion forecasting for autonomous driving systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Predicting the future trajectory of vehicles is an essential task for autonomous driving systems. Autonomous vehicles (AVs) are empowered to conduct more reasonable local-path planning and collision warning based on the trajectory predictions of surrounding vehicles, which greatly improves the efficiency and safety of AVs in complex dynamic traffic systems. Based on sensory information derived from roadside or onboard sensing systems, such as vehicle location and road topology [1], [2], [3], [4], [5], [6], [7], [8], [9], existing methods typically perform temporal inference of the future trajectory by various well-designed models [10], [11], [12], [13], [14], [15]. Traditional approaches involve rasterizing the traffic scene and employing RNN-based models to capture temporal dependencies, yielding promising results in simple highway scenarios [16], [17], [18]. However, the intricate road topology of urban traffic scenes poses inherent challenge to the rasterization paradigm. In response, graph-based models [19], [20] have been introduced for flexible prediction within non-Euclidean space, which notably out-performs RNN-based models, particularly in scenarios with complex road networks and dynamic urban traffic scenes. The emergence of Transformer [21] has further advanced trajectory prediction. Transformer-based models [22] establish direct links for inputs, allowing them to capture long-term dependencies in trajectory, and advancing the state of the art in long-term trajectory prediction.\nHowever, existing methods often assume that the observed trajectory of the vehicle is entirely complete while ignoring the potential for missing values caused by object occlusion, sensor failures, and sensing scope limitation. To elucidate this concern, we statistically analyze the missing values of the trajectory using the multi-object tracking dataset from KITTI [23]. In Fig. 1 (a), distribution of missing percentages in trajectories is presented, revealing that a mere 37.13% of vehicle trajectory samples exhibit completeness, while 62.87% of the samples lack trajectory values at specific intervals. Notably, the missing percentages are dispersed randomly across the entire range of (0%, 100%). Fig. 1 (b) provides an illustrative example of an occlusion case. The missing values disrupt temporal dependence of the trajectory sequence, and predicting the future trajectory of vehicles under such circumstance undoubtedly hinders the performance and negatively influences the behavior understanding of vehicles.\nAlthough various recent methods have been proposed to solve the problem of missing values by imputation [24], [25], [26], most are autoregressive models that impute current missing values based on previous time steps, making them highly susceptible to compounding errors, especially in long-term temporal modeling. Additionally, widely used benchmarks are not tailored for the precise demands of vehicle trajectory prediction [27], [28]. More importantly, the two-stage incomplete trajectory prediction scheme that incorporates an imputation task brings extra parameters and computation burden, which hinders the lightweight and timeliness of autonomous driving systems.\nIn this paper, we present an end-to-end framework for incomplete trajectory prediction, Multiscale Transformer (MSTF). Specifically, we design a novel Multiscale Attention Head (MAH) leveraging the padding mask mechanism in the vanilla Transformer, and MAH observes the incomplete trajectory from different temporal granularities parallelly to extract multiscale motion representation. Meanwhile, we propose an Information Increment-based Pattern Adaptive (IIPA) module, capable of adaptively computing the information increment of various time steps utilizing the trajectory missing pattern (number and location of missing values, etc.), and model the motion continuity representation across time steps based on the information increment. The critical idea behind our method is that the motion representation at different scales may skip certain missing values, and the negative impact of missing values can be alleviated by using the multi-scale motion representation to predict the current value from different temporal granularities. Furthermore, the continuity representation reflects the overall trend of motion and is insensitive to the missing patterns of trajectory. It sacrifices part of the detailed information but can guide MSTF to output predictions that are consistent with motion consistency. It loses part of the detailed information but can guide MSTF to output predictions that are consistent with motion consistency. The main contributions of our work can be summarized as follows:\n\u2022 We statistically analyze the problem of missing values of trajectory in real traffic scenarios and devise an end-to-end framework, MSFT, for incomplete trajectory prediction. The MAH is designed to capture multi-scale motion representations of vehicles from different temporal granularities, mitigating the negative impact of missing values on vehicle trajectory prediction.\n\u2022 We propose a novel IIPA module that is able to adaptively compute information increments at different time steps using trajectory missing patterns, and then model missing pattern insensitive continuity representation across time steps to guide MSTF to output prediction that is consistent with motion consistency.\n\u2022 Through comparative experiments on both highway and urban scene datasets, MSTF consistently demonstrates superior performance compared to the existing SOTA methods."}, {"title": "II. RELATED WORK", "content": "A. Trajectory Prediction\nThe objective of trajectory prediction is to predict the future positions of vehicles conditioned on their observations through various well-designed models. As a typical representative of RNN models, Social-LSTM [16] innovatively embeds vehicle features by rasterizing traffic scenes for interaction extraction, and then sequentially decodes future trajectory through the recursive work mechanism of LSTM. Following this, other LSTM-based methods have been proposed [17], [18]. For special traffic scenes such as roundabouts and intersections, graph-based methods are proposed to adapt to complex road topology, facilitating the vehicle trajectory prediction in non-Euclidean space [19], [29]. Recently, Transformer-based models [22] have been applied to this task to establish direct links for inputs via an attentional mechanism, allowing the models to capture long-term dependency of the trajectory. However, these methods assume that vehicle observations are entirely complete, which is too strong an assumption to satisfy in practice. Existing methods are not applicable to the prediction of in-complete trajectory whose temporal dependency is disrupted by missing values.\nB. Trajectory Imputation\nSome statistical imputation techniques substitute missing values with mean or median values [30]. Alternative meth-ods also adopt linear fitting [31], k-nearest neighbors [32], and expectation-maximization algorithm [33]. One of the inherent limitations of such methods is that they use rigid prior, which hinders the generalization ability. In contrast, deep learning-based frameworks perform imputation more flexibly. For instance, some RNN-based models [34] estimate missing values in sequences through deep autoregression, and generative models [35] reconstruct incomplete sequences through GANs or VAEs. Nevertheless, the two-stage in-complete trajectory prediction framework of imputation fol-lowed by prediction brings extra parameters and computation burden, which hinders the lightweight and timeliness of autonomous driving systems. Therefore, we designed a novel framework called MSTF based on Transformer [21], which enables end-to-end incomplete trajectory prediction by extracting multi-scale motion representation and continuity representation."}, {"title": "III. METHODS", "content": "A. Problem Definition\nDue to the manual annotation, trajectory data provided by the existing large public datasets [36], [37] is complete, and the incomplete trajectory is unavailable. To address this limitation, we generate incomplete trajectory by randomly concealing portions of the complete data. Specifically, consider a set of complete vehicle observations $X = \\{x_{t+1}, x_{t+2}, ..., x_{t+Th} \\}$ over time step t+1 to t+Th, which is provided by public dataset, where $x_t \\in R^2$ represents the 2D coordinates of vehicle at time step t. To model the missing of vehicle observations due to occlusion, sensor failure, etc., we define a sequence mask matrix $M_s = \\{m_{t+1}, m_{t+2}, ..., m_{t+Th} \\}$ valued in \\{0,1\\}. The variable $m_t$ is assigned a value of 0 if the observation is missing at time step t and 1 otherwise, and the quantity and positions of absent observations are generated in a fully random manner. Following this setting, the generated incomplete trajectory can be expressed as:\n$X_{miss} = X \\odot M_S$ (1)\nwhere $X_{miss}$ is the randomly masked incomplete trajectory, and the training, validation and testing of the model are performed based on the incomplete trajectory.\nThe goal of the incomplete trajectory prediction task is to predict the vehicle trajectory $\\hat{Y} = \\{\\hat{y}_{t+Th+1},\\hat{y}_{t+Th+2}, ..., \\hat{y}_{t+Th+Tf}\\}$ within the future time step $t + T_h + 1$ to $t + T_h + T_f$, conditioned on its incomplete observations over time step $t+1$ to $t+T_h$, where $T_h$ and $T_f$ are the observation and prediction horizons, respectively.\nB. Model Framework\nFig. 2 provides a high-level depiction of our proposed framework. Firstly, the sequence mask matrix is obtained by randomly generating the number and distribution positions of masks, which is used to mask the complete trajectory provided by the public dataset to obtain the incomplete trajectory. Then, the incomplete trajectory is repeated and fed to multiple attention heads with different temporal granularities to extract the multi-scale motion representation. Finally, based on the sequence mask matrix and predefined padding mask matrix, the information incremental analysis at different temporal scales is performed for the weighted aggregation of the multi-scale motion representation across time steps to obtain continuity representation. Combining the detailed motion information expressed in the multi-scale motion representation with the overall trend of the motion reflected in the continuity representation, the future trajectory decoder outputs prediction for incomplete trajectory.\nC. Multiscale Attention Head\nThe core of trajectory prediction lies in effectively model-ing the temporal dependency between historical trajectory points, while the presence of missing values disrupts the dependency between adjacent time steps. We argue that RNN encoders (e.g., LSTM-based or GRU-based encoders) that serially process data using a recursive mechanism will un-doubtedly rely more on local dependency between adjacent time steps, which makes their performance more susceptible to the negative impact of missing values. On the contrary, the Transformer processes the sequence of trajectory in parallel and is able to establish direct links for all values of the sequence with the help of an attention mechanism, so that each value in the sequence can directly aggregate information from all the remaining values to obtain global dependency, which alleviates the negative impact of some missing values to a certain extent. Consequently, designing the encoder based on Transformer in our work is a natural decision.\nSpecifically, we first compute the query vector $Q = \\{q_1, q_2, ..., q^n\\}$, the key vector $K = \\{k_1, k_2, ..., k_n\\}$, and the value vector $V = \\{v_1, v_2, ..., v_n\\}$ for then attention heads based on the incomplete input.\n$\\hat{X}_{em} = \\Beta (X_{miss}) + Pos$\n$q^i = \\varphi_Q (\\hat{X}_{em}, W_Q)$ (2)\n$k^i = \\varphi_K (\\hat{X}_{em}, W_K)$\n$v^i = \\varphi_V (\\hat{X}_{em}, W_V)$\nwhere $\\Beta$ is used to extend two-dimensional coordinates to higher dimension to improve feature representation, which is achieved through MLP in our work. Following Transformer [21], positional encoding $Pos$ is adopted to the model to distinguish the order of input sequence. $W_Q$, $W_K$, and $W_V$ are the learnable parameter matrices for corresponding transformation $\\varphi_Q$, $\\varphi_K$, and $\\varphi_V$.\nFor different attention heads, the padding mask matrix $M_p \\in R^{n\\times len \\times len}$ with different temporal granularities is designed:\n$M_p = \\{m_p^1, m_p^2, ..., m_p^n\\}$ (3)\nwhere $m_p^i \\in R^{len \\times len}$ is the padding mask matrix of the attention head i, n is the number of attention heads, and len represents the length of input sequence.\nThe value of element $m_p^{a,b}$ in row a and column b of matrix $m_p^i$ can be further formulated as:\n$m_p^{a,b} = \\begin{cases}\n1, a,b \\in Z \\\\\n0, Others\n\\end{cases} a,b \\in \\{1,2,...,len\\}$ (4)\nwhere $Z$ denotes the set of integers. For intuitive illustration, we visualize the padding mask matrix $m_p^i$ for i = 2 in Fig. 3.\nBased on the padding mask matrix, multiple attention heads extract the multi-scale motion representation $R_m = \\{r_m^1, r_m^2, ..., r_m^n\\}$ of the vehicle in parallel with different temporal granularities.\n$a^i = q^i (k^i)^T$\n$ScaleAtten(a^i, m_p^i) = softmax(\\frac{a^i \\odot m_p^i}{\\sqrt{d_k}})$ (5)\n$r_m^i = ScaleAtten(a^i, m_p^i) * (v^i)$\nwhere $r_m^i$ is the motion representation extracted by the attention head i. $\\Psi$ is a mapping function, which is used to map the value in $a^i$ at the position corresponding to the value 0 in $m_p^i$ to negative infinity. $d_k$ represents the dimension of key vector $k^i$, and the number of attention heads n = 5 in practice. The complete computation process of attention head i is shown in Fig. 3.\nD. Information Increment-based Pattern Adaptive Module\nThe absence of trajectory points hinders the model from adequately capturing the temporal dependency within the tra-jectory sequence. This challenge is particularly pronounced for RNN-based models, as they struggle to effectively cap-ture the local dependency between consecutive time steps. The randomly generated missing patterns (the number of missing values and their distributed locations) also make the encoded feature of the same trajectory sample vary randomly with the missing patterns, which poses a great challenge to accurately decode the future trajectory of the vehicle. We argue that humans are not constrained by the locality of the sequence when facing the problem of incomplete trajectory prediction. Instead, they analyze the continuity of motion from a higher-level perspective. The continuity representation cannot encapsulate the detailed information of vehicle motion, but it aptly reflects the overall trend of motion across time steps and is insensitive to the missing patterns of trajectory, which is conducive to constraining the model to output the prediction consistent with the motion trend. Given the aforementioned analysis, we propose an Information Increment-based Pattern Adaptive (IIPA) module to extract the continuity representation.\nFormally, based on the randomly generated sequence mask matrix $M_s$ and the predefined padding mask matrix $M_p = \\{m_p^1, m_p^2, ..., m_p^n\\}$, the observation matrix $M_{obs} = \\{m_{obs}^1, m_{obs}^2, ..., m_{obs}^n\\}$ is computed:\n$m_{obs}^i = A(M_s, m_p^i)$ (6)\nwhere sequence mask matrix $M_s$ represents the missing pattern of trajectory. The padding mask matrix $m_p^i \\in R^{len \\times len}$ and observation matrix $m_{obs}^i$ reflect the scale of the observation and the observable values when the temporal granularity is i, respectively. $A$ denotes that $M_s$ and $m_p^i$ are multiplied by their corresponding elements row by row.\nThen, based on the observation matrix $m_{obs}^i$, we sta-tistically analyze the increment of information $\\Omega^i = [\\sigma_1^i, \\sigma_2^i, ..., \\sigma_{len}^i]$ of the sequence at the temporal granularity i.\n$\\mu_{lj} = \\begin{cases}\n1, m_{obs}^{ij} > 0 \\\\\n0, m_{obs}^{ij} = 0\n\\end{cases}$ (7)\n$\\sigma_j^i = \\sum_{l=1}^{len} \\mu_{lj}$\nwhere $m_{obs}^{ij}$ is the value of the observation matrix $m_{obs}^i$ in row j and column l. $\\mu_{lj}$ = 0 indicates that l \u2013 th trajectory point is missing or not within the observational scope of j \u2013 th trajectory point at temporal granularity i, which renders the j \u2013 th trajectory point incapable of aggregating information from the l \u2013 th trajectory point through the attention mechanism; Otherwise, l \u2013 th trajectory point is available for j \u2013 th trajectory point. $\\sigma_j^i$ is the information increment of j-th trajectory point in the trajectory sequence when the temporal granularity is i.\nThe multiscale attention head establishes a direct link for each value in the input sequence with the help of attention mechanism, which enables them to directly aggregate global information. Consequently, the feature of each trajectory point in the multi-scale motion representation can reflect the overall trend of the motion to a certain extent, only that the trajectory points at different locations observe the motion trend from different perspectives. Therefore, multi-scale motion representation is aggregated across time steps to synthesize different perspectives to obtain robust continuity representation. Specifically, considering the different impact of missing values on trajectory points at different locations, we compute the attention weights across time steps based on the information increment and give greater weight to the features of the trajectory points that are less affected by miss-ing values, and finally obtain the continuity representation $R_c = \\{r_c^1, r_c^2, ..., r_c^n\\}$ that is insensitive to missing patterns.\n$\\alpha^i = \\frac{exp(\\sigma^i)}{\\sum_{i=n}^{i=n} exp(\\sigma^i)}$\nAcross Atten ($ \\Omega^i$) = $\\{\\alpha_1, \\alpha_2, ..., \\alpha_{len}\\}$ (8)\n$r_c^i = Across Atten ($ \\Omega^i$) $\\times (r_m^i)^T$\nwhere $r_c^i$ represents the continuity representation at temporal granularity i.\nFinally, based on the multi-scale motion representation $R_m$ and continuity representation $R_c$, the future trajectory decoder combines the motion detail information with the overall trend of the motion to output the future prediction.\n$R = AGG (R_m, R_c)$\n$\\hat{Y} = P (R)$ (9)\nwhere AGG stands for data fusion, which is realized by concatenation in our work. P is LSTM, which is used as the future trajectory decoder."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets\nConsidering the difference of vehicle behavior in highway traffic scenarios and urban traffic scenarios, we validate the validity of the proposed model in different traffic scenarios by using the HighD dataset [36] and the Argoverse dataset [37], respectively. The HighD dataset was collected from the German highway as shown in Fig. 4 (a), where vehicles in the traffic scenario travel faster, but with simple traffic behaviors such as acceleration, deceleration, and lane changing only. The data is recorded at 25Hz from six different locations on Germany highway from the aerial perspective using a drone. It is composed of 60 recordings over areas of 400 420 meters span, with a mileage of 45,000 km, and more than 110, 000 vehicles are contained.\nArgoverse is a motion prediction benchmark that collects more than 30K data based on the onboard sensing system in urban traffic scenarios as shown in Fig. 4 (b), where vehicles are slow but have complex traffic behaviors such as left or right turns. Each scenario is a 5-second sequence sampled at 10 Hz, and the task is to predict the position of the vehicle in the next 3 seconds based on its historical trajectory over 2 seconds. The sequences are split into training, validation, and test sets, which have 205942, 39472, and 78143 sequences respectively. In our work, we only use historical vehicle trajectory for prediction and do not use map data such as rasterized drivable area maps and ground height maps provided by the benchmark.\nB. Evaluation Metrics\nTo facilitate the performance comparison, we follow pre-vious works [16], [17], [19], [38] and use different evaluation metrics on HighD dataset and Argoverse dataset. In the comparison based on the HighD dataset, we use the root mean square error (RMSE) to evaluate the performance of the model at different prediction horizons. In the comparison based on the Argoverse dataset, the average displacement error (ADE) and final displacement error (FDE) are adopted to evaluate models. In order to make a fair comparison with our proposed model, we only use the single prediction of the existing models for the evaluation, although they give multiple possible predictions for the same sample.\n$RMSE = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{t=T_h+1}^{T_h+T_f} (\\hat{y_i^t} - y_i^t)^2}$ (10)\n$ADE = \\frac{1}{m T_f} \\sum_{i=1}^{m} \\sum_{t=T_h+1}^{T_h+T_f} |(\\hat{y_i^t} - y_i^t)^2|$\n$FDE = \\frac{1}{m} \\sum_{i=1}^{m} |(\\hat{y_i^{T_h+T_f}} - y_i^{T_h+T_f})^2|$\nwhere m is the number of samples. $\\hat{y}$ and y are the predicted and true positions of the sample i at time t, which are 2D coordinates.\nC. Implementation Details\nWe implement MSTF in PyTorch and train on 1 NVIDIA GeForce RTX 3090 with a batch size of 128. The MSTF has four layers, each consisting of five attention heads with different temporal scales, all possessing a hidden layer dimension of 128. We use Adam to train the model for 200 epochs, and set the initial learning rate to 1$\\times10^{-4}$. We keep the same setting for both datasets.\nD. Results\nTo assess the prediction performance of existing models on incomplete trajectory, we only consider SOTA models with available code for comparison. The parameter settings of the following comparison models are set to default values, and only the trajectory is randomly masked to get incomplete input.\n\u2022 Vanilla-Transformer (V-TF): The Vanilla-Transformer model with exactly the same structure as our proposed MSTF (number of attention heads, number of layers, dimensions of hidden states) is used as an ablation experiment to compare with the MSTF to demonstrate the validity of our proposed modules.\n\u2022 CS-LSTM [16]: The method introduces convolutional operation into the social pooling layer to capture the inter-vehicle interaction while retaining the spatial in-formation between vehicles. The output of CS-LSTM is a binary Gaussian distribution parameter.\n\u2022 PiP [17]: The model couples trajectory prediction as well as the planning of the target vehicle by condition-ing on multiple candidate trajectory of the target vehicle, and the facilitation between planning and prediction enables the model to achieve accurate predictions in highway traffic scenarios.\n\u2022 LaneGCN [19]:The method proposes a fusion network consisting of four types of interaction based on graph convolution to model actor-lane, lane-lane, lane-actor and actor-actor interaction, and achieves accurate mul-timodal trajectory prediction with the help of this struc-tured map representation and actor-map interaction.\n\u2022 HLS [38]: The method introduces a hierarchical latent structure into VAE-based forecasting model. Based on the assumption that the trajectory distribution can be approximated as a mixture of simple distributions (or modes), the method employs low-level and high-level latent variables to model each mode of the mixture and the weights for the modes, respectively, which achieves promising prediction performance in complex urban traffic scenarios.\nTo evaluate the performance of the models across vary-ing degrees of missing data, we delineate three distinct missing rate intervals, which are (0%, 30%], (30%, 60%], and (60%, 90%]. Within these three intervals, the number and locations of missing trajectory points are randomly generated.\nTable. I shows the comparison results of the models based on the HighD dataset. In general, our proposed MSTF achieves optimal prediction accuracy in all experiment set-tings. Through the comparison of V-TF with CS-LSTM and PiP, it can be seen that although the performance of V-TF is not the best among the three in short-term prediction (1 s), the average performance improvement of V-TF in long-term prediction (2s-4s) reaches 67.49%, 67.89% and 64.63% in the three missing rate intervals, respectively. This indicates that the missing data disrupts the local dependency of adjacent time steps, which makes the performance of PiP and CS-LSTM degrade significantly, while V-TF can model the global dependency using the attention mechanism, which enables it to maintain better prediction performance even when the missing rate becomes large. Furthermore, the average performance improvement of MSTF over V-TF is 20.23%, 12.86%, and 11.39% in the three missing rate intervals, respectively. Since the structure (number of attention heads, number of layers, dimension of hidden state) of V-TF and MSTF are identical, the comparison reveals that the performance improvement of MSTF can be attributed to the MAH and IIPA modules we proposed, rather than a mere expansion of model parameters.\nThe quantitative experimental results on Argoverse dataset are summarized in Table II. HLS significantly outperforms LaneGCN in complete trajectory prediction, but obtains the largest prediction error in comparison experiments for in-complete trajectory prediction, with even worse performance than V-TF, which illustrates that existing models designed for the complete trajectory prediction task cannot be flexibly transfered to incomplete trajectory prediction task. However, compared to LaneGCN, the MSTF designed for incomplete trajectory prediction only achieves 18.53% and 11.05% performance improvement for ADE and FDE when the missing interval is (60%, 90%], while the prediction performance is worse than LaneGCN in all other experimental settings. We argue that the complex road topology makes the vehicle trajectory in the Argoverse dataset exhibit a high degree of nonlinearity, which affects the extraction of continuity representation by IIPA and ultimately limits the prediction performance of MSTF. In contrast, LaneGCN fully utilizes the high-definition map information and achieves reconstruc-tion of missing information with the help of four well-designed interaction modules, which enables it to achieve excellent performance in the task of incomplete trajectories in complex scenes.\nTo visually show the prediction effect of MSTF, we visualize the prediction results of trajectory for three different maneuvers at different missing rate intervals. Compared with the lane-changing trajectory, the lane-keeping trajectory achieves the most accurate prediction re-sults, and the prediction results are insensitive to the missing rate due to its simple behavior. In the case of the left lane changing, the continuity representation extracted by IIPA guides the MSTF to predict a lane-keeping trajectory that is more consistent with the historical motion trend, since the vehicle changes lanes within the prediction time horizon and the historical trajectory does not show a trend of changing lanes. In the case of right lane changing, the model is able to accurately output the right lane-changing trajectory that is consistent with the motion trend when the missing rate is less than 60%. However, as the missing rate increases to the interval (60%, 90%], the MSTF cannot effectively extract detailed motion information, and the model tends to perform lane-keeping prediction. The visualization results show that our model can effectively perform reasonable prediction that consistent with motion consistency for the incomplete trajectory with the missing rate less than 60%."}, {"title": "V. CONCLUSIONS AND DISCUSSION", "content": "This paper presents a novel end-to-end framework named MSTF for the incomplete trajectory prediction task, which integrates the Multiscale Attention Head (MAH) and Infor-mation Increment-based Adaptive (IIPA) module. We uti-lize the padding mask matrix in the multi-head attention mechanism to construct the MAH for extracting multiscale motion representations with global dependency from dif-ferent temporal granularities, so as to alleviate the lack of local dependence caused by random missing values. IIPA analyzes the information increment of different trajectory points through the missing patterns of trajectory, and uses them as weights to aggregate multi-scale representations across time steps to obtain continuity representations. The continuity representation ignores individual missing values and describes the overall trend of motion from a high level so that the MSTF outputs predictions that are consistent with motion consistency.\nIn the future, we will continue to explore the positive role of HD maps in the task of incomplete trajectory prediction, and further strengthen the prediction performance for incom-plete trajectory through the scene constraints extracted from HD maps, so that the model can output predictions that are consistent with scenes in complex traffic scenarios such as those provided by Argoverse."}]}