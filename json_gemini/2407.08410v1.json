{"title": "SPECIALIST VISION-LANGUAGE MODELS\nFOR CLINICAL OPHTHALMOLOGY", "authors": ["Robbie Holland", "Thomas R. P. Taylor", "Christopher Holmes", "Sophie Riedl", "Julia Mai", "Maria Patsiamanidi", "DimitraMitsopoulou", "Paul Hager", "Philip M\u00fcller", "Hendrik P. N. Scholl", "Hrvoje Bogunovi\u0107", "Ursula Schmidt-Erfurth", "Daniel Rueckert", "Sobha Sivaprasad", "Andrew J. Lotery", "Martin J. Menten"], "abstract": "Clinicians spend a significant amount of time reviewing medical images and transcribing their\nfindings regarding patient diagnosis, referral and treatment in text form. Vision-language models\n(VLMs), which automatically interpret images and summarize their findings as text, have enormous\npotential to alleviate clinical workloads and increase patient access to high-quality medical care.\nWhile foundational models have stirred considerable interest in the medical community, it is unclear\nwhether their general capabilities translate to real-world clinical utility. In this work, we show that\nfoundation VLMs markedly underperform compared to practicing ophthalmologists on specialist\ntasks crucial to the care of patients with age-related macular degeneration (AMD). To address this,\nwe initially identified the essential capabilities required for image-based clinical decision-making,\nand then developed a curriculum to selectively train VLMs in these skills. The resulting model,\nRetinaVLM, can be instructed to write reports that significantly outperform those written by leading\nfoundation medical VLMs in disease staging (F1 score of 0.63 vs. 0.11) and patient referral (0.67 vs.\n0.39), and approaches the diagnostic performance of junior ophthalmologists (who achieve 0.77 and\n0.78 on the respective tasks). Furthermore, in a reader study involving two senior ophthalmologists\nwith up to 32 years of experience, RetinaVLM's reports were found to be similarly correct (78.6% vs.\n82.1%) and complete (both 78.6%) as reports written by junior ophthalmologists with up to 10 years\nof experience. These results demonstrate that our curriculum-based approach provides a blueprint for\nspecializing generalist foundation medical VLMs to handle real-world clinical tasks.", "sections": [{"title": "1 Introduction", "content": "Medical images are central to many clinical decisions regarding patient diagnosis, referral, and treatment. Clinicians\nspend a significant amount of time transcribing image-based decisions into text in order to store and communicate\ntheir findings [1,2]. Visual-language models (VLM), which automatically interpret medical images and generate\ndetailed textual descriptions, have enormous potential to alleviate clinical workloads and increase patient access to\nhigh-quality medical care [3,4]. To date, the majority of medical VLMs have been trained to output a finite set of\npre-determined textual responses [5,6,7,8]. Only recently, the combination of large language models (LLM) with"}, {"title": "2 Results", "content": ""}, {"title": "2.1 Retina VLM, a specialist vision-language model for retinal image analysis", "content": "RetinaVLM combines two main components: an ophthalmic vision encoder that processes input OCT images, and a\ngenerative LLM that handles textual instructions and outputs the corresponding responses (see Figure 1a). The vision\nencoder is based on our previous work, in which we found it to perform on par with RETFound [17], a large foundation\nmodel for retinal image analysis [18]. We use Meta's Llama 3 as generative LLM, the best performing model that is\nopenly available at the time of this study [19]. However, without additional training it lacks specialist knowledge related\nto the analysis of OCT images and the clinical management of AMD. Both these deep neural networks have already\nbeen pre-trained on large OCT and natural language datasets, respectively, and we only finetune them in the scope of\nthis study. Additional details about the network architecture and training are included in Section 5.2."}, {"title": "2.2 A curriculum to encode specialist ophthalmological knowledge in vision-language models", "content": "An intuitive strategy to specialize VLMs while preserving their ability to flexibly interact with text queries is to provide\nthem with a set of medical images and corresponding question-answer pairs. They are then optimized based on the\nsimilarity of their predicted answers to the ground truth. However, visual question-answering (VQA) datasets do not\nexist for most medical specializations, including ophthalmology.\nTogether with a large team of ophthalmologists, which are involved with the patient care and academic research of\nAMD, we defined a set of required capabilities for accurate image-based clinical management of AMD. They include\nthe identification of AMD biomarkers in OCT images, the linking of these to the AMD disease stage, and ultimately\ndeciding on the required referral and treatment of the patient. We then curated a training curriculum, which consists of\n41,926 OCT images, and 479,710 visual questions and answers to progressively specialize VLMs in these capabilities.\nCurriculum part 1: Introduction to retina The first part of the curriculum, named Introduction to retina, primarily\ncovers the appearance of the retina and AMD biomarkers in OCT images. Using automated data collection, we\nobtained tabular reports for 41,926 retrospectively collected OCT images of AMD patients (see Figure 2a). Each report\ndescribes the visible biomarkers, patient's diagnosis, visual acuity and demographic information in 34 data fields. A\nfull description of the OCT dataset can be found in Section 5.1.1, the list of all tabular data fields and example tabular\nreports in Figure 7, and the methodology for their automated procurement in Section 5.4.1.\nNext, we tasked an independent LLM to generate question-answer pairs based on these reports (see Figure 2c). The\nmodel processed the content of the tabular reports \u2013 but not the OCT images \u2013 to output a numbered list of question-\nanswer pairs. We generated an average of ten question-answer pairs per report that are mostly related to the presence or\nabsence of specific biomarkers (see Figure 2e). The LLM was instructed to create both closed-ended 'yes or no' style\nquestions, and simple open-ended questions. Detailed information on the LLM setup can be found in Section 5.4.1."}, {"title": "2.3 RetinaVLM-Specialist outperforms foundation models and approaches junior ophthalmologists in AMD\ndisease staging and report writing", "content": "Estimating the disease stage is crucial to patient management as it allows clinicians to monitor and treat patients using\nstandardized protocols. We assessed the ability of four different generative VLMs to determine the AMD disease\nstage from retinal OCT images. Specifically, we compared two medical foundation VLMs, Med-Flamingo [10] and\nLLaVA-Med [9], to our two specialist VLMs, RetinaVLM-Base and RetinaVLM-Specialist. Using a testing dataset of\npreviously unseen 276 OCT images, VLMs were tasked to write reports that describe the OCT image before classifying\nthe patient into one of six disease stages (see Figure 3a). The model predictions were compared to ground truth labels\nobtained from ophthalmologists. Each image was initially graded by two out of six junior ophthalmologists, whose\nexperience in the field ranges from 2 to 15 years. Inter-rater disagreements were resolved by a panel of two senior\nophthalmologists with 25 and 32 years of experience, respectively. For additional methodological details, including the\ninstruction given to the VLMs to generate these reports, see Section 5.8.1.\nWe found that our intermediate RetinaVLM-Base model already performs significantly better than both foundation\nVLMs, which lack the ophthalmological specialism to stage disease (see Figure 3b). The most performant foundation\nVLM, Med-Flamingo, achieved a F1 score of 0.11. This was markedly outperformed by our advanced RetinaVLM-\nSpecialist model, scoring at 0.63. This approached, but did not match, the accuracy of the junior ophthalmologists\nwho achieved an F1 score of 0.78. We analyze this discrepancy in detail in Section 3. Both foundation VLMs and\nRetinaVLM-Base returned a substantial number of invalid reports that did not conclude with one of the six disease\nstages (see Figure 3c). Conversely, all generated reports by RetinaVLM-Specialist were valid. Similar to human experts,\nRetinaVLM-Specialist struggled the most when diagnosing wet inactive AMD. We attribute this to the high number of\nshared imaging biomarkers that indicate either intermediate and late-wet forms of AMD, which sometimes leads to\nmisdiagnosis by both ophthalmologists and RetinaVLM-Specialist (see Figure 3d). This was despite the identification\nof emerging features related to inactive late wet AMD (a small amount of hyperreflective material, or scar tissue)\nby RetinaVLM. Four more examples of success and failure cases of RetinaVLM-Specialist are shown in Figure 11a.\nMoreover, full numerical results as well as the confusion matrix for Med-Flamingo are shown in Figure 12 and 13,\nrespectively.\nEighty-four of the generated reports were scored by the two senior ophthalmologists for their correctness, completeness,\nand conciseness. They were shown 28 reports written by LLaVA-Med, 28 by RetinaVLM-Specialist, and 28 by the\ntwo annotating junior ophthalmologists in random order without knowledge of the author. For each report, the senior"}, {"title": "2.4 RetinaVLM-Specialist surpasses opticians and approaches junior ophthalmologists in AMD patient\nscreening and referral", "content": "As the prevalence of AMD is expected to further increase in the upcoming decades [16], ocular screening programs are\nbeing introduced around the world. In the United Kingdom, some projects involve opticians and pharmacies that acquire\nand interpret OCT images. They may refer a patient to a specialist clinic, summarizing their findings and the estimated\nlevel of the patient's risk in a letter. In the United Kingdom, treatment guidelines for AMD mandate that patients with\nsigns of neovascularization are referred for immediate treatment within two weeks. However, non-specialists exhibit a\ntendency to over-diagnose these cases. An internal audit at Southampton Eye Unit found that 74.2% of the referrals\nmade to the clinic do not have any form of treatable AMD. The processing and assessment of these false positives\naffects the clinic's ability to care for the remaining patients with treatable forms of AMD.\nWe evaluated the ability of VLMs to assess the level of referral urgency from OCT image (see 5a). For each case, the\nVLMs were provided explicit referral guidelines, and asked to recommend which of three levels of referral urgency\nwas most appropriate for the patient: no referral for healthy patients, to be seen within 18 weeks (routine referral) for\npatients that are at risk of progressing to active late wet AMD but do not require treatment yet, and referral within two\nweeks for patients with any signs of neovascularization that should be urgently referred for antiangiogenic treatment.\nTwo junior ophthalmologists independently reviewed images of 95 patients that have previously been referred to the\nhospital for treatment of wet AMD. For each patient, they independently decided the most appropriate of the three\nlevels of referral urgency, and disagreements were arbitrated by the two senior ophthalmologists. In line with previous\naudits, they found the false discovery rate for urgent referrals was 69.5%. We then calculated F1 scores for the highest\nrisk patients in need of urgent referral between the VLM's predictions and the ground truth. The full referral protocol\nand report generation instructions given to the VLMs are provided in Section 5.8.3.\nWe found that both medical foundation VLMs and Retina-Base perform worse than opticians regarding their ability to\nrefer patients in need of urgent treatment (see Figure 5b). While Med-Flamingo failed to refer any of the 29 high-risk\npatients cases, LLaVA-Med and RetinaVLM-Base were ineffective for differentiating high-risk patients from low- to\nmoderate-risk patients (see Figure 5c). RetinaVLM-Specialist was able to detect 23 out of the 29 high-risk cases that\nrequire immediate treatment. At the same time, RetinaVLM's false discovery rate, defined as the ratio of the number of\nfalse positives over the number of predicted positives, of 42.5% is substantially lower than that of opticians at 69.5%.\nOwing to their ability to better differentiate moderate from high-risk cases, the human ophthalmologists had the lowest\nfalse discovery rate of 9.1%, although they simultaneously missed three more cases in urgent need for treatment.\nIn practice, referral letters should communicate the reason for referral by citing suspected abnormalities in the\nOCT image that can inform the ophthalmologist's initial diagnostic plan. As in the conciseness study in Figure 4,\nRetinaVLM-Specialist sometimes documents the presence of small biomarkers that cannot be found in the image.\nMore often, RetinaVLM-Specialist wrote an accurate imaging report but did not accurately follow the complex set\nof referral guidelines provided in the instruction. This led RetinaVLM-Specialist to incorrectly recommend that 17\nof the moderate-risk patients potentially require treatment. However, this occurred less for the 25 low-risk patients,\nwhere RetinaVLM-Specialist correctly identified patients with little or no abnormalities, which are often referred\nto the treatment clinic for a second opinion by non-specialists (samples 1 and 3 in Figure 5d). Crucially, we find"}, {"title": "2.5 RetinaVLM accurately detects imaging biomarkers to make recommendations", "content": "It is important that clinical decision makers can provide evidence for their recommendations. Disease staging reports\nand written referral recommendations commonly contain descriptions of the most salient biomarkers that were detected\nin the scan. We tested the ability of four VLMs to correctly identify the presence or absence of 10 different biomarkers\nrelated to AMD. To this end, all VLMs were tasked with writing reports for 396 OCT images that conclude by stating\nthe presence or absence of the biomarker in question (see Figure 6a). The VLMs predictions were compared against the\nground truth labels obtained from junior ophthalmologists. The instruction used to generate these biomarker focused\nreports is provided in Section 5.8.4.\nWe find that RetinaVLM-Specialist outperforms both LLaVA-Med and Med-Flamingo in the detection of seven out\nof the ten of main biomarkers related to AMD (see Figure 6b). Biomarkers that were more severe, larger, and more\nnumerous were detected with higher accuracy by RetinaVLM-Specialist than less advanced presentations (see Figure\n6c). Most of the smaller biomarkers, such as small amounts of intraretinal fluid, drusen and hyperreflective foci, which\ncan be as small as 30 \u00b5m in size [21], were detected with lower sensitivity. Overall, clinically important hallmarks of\nlate AMD were detected with a very high sensitivity. Large volumes of subretinal and intraretinal fluid were detected in\n80% and 78% of cases, respectively, and severe levels of hypertransmission in 84% of cases.\nFinally, to visualize the functioning of RetinaVLM-Specialist, we calculated saliency maps based on Grad-CAM [22].\nThese saliency maps highlight the image regions deemed most important by the model when writing specific passages\nof the report (see Figure 6d). We refer to Figure 14 for four additional image reports with corresponding saliency maps.\nWe qualitatively found that RetinaVLM-Specialist is influenced by different imaging biomarkers when writing different\npassages of the report, and in making its final recommendation. We observed the saliency maps were especially effective\nfor highlighting hyperreflective material, RPE irregularities and hypertransmission."}, {"title": "3 Discussion", "content": ""}, {"title": "Main findings of the study", "content": "In this study, we have presented RetinaVLM, the first specialist generative visual language model in medicine. Given\na retinal OCT image, RetinaVLM provides accurate, detailed textual responses related to disease staging, referral or\nbiomarker identification of AMD. While large foundation deep learning models have been employed for retinal image\nanalysis before [23,18], our generative VLM is the first model that can flexibly process varied textual queries related to\ncomplex ophthalmological decisions and return detailed written responses. Through the use of language as primary\ncommunication medium, artificial intelligence systems are able to dynamically perform new tasks and meet the evolving\nrequirements of image-based clinical decision makers.\nIn extensive experiments, RetinaVLM significantly outperformed state-of-the-art generative VLMs designed for medical\nuse. We found that these are unable to interpret OCT images, derive the AMD disease stage and follow standard referral\nguidelines for AMD. Specifically, we have shown that in disease staging, RetinaVLM surpasses LLaVA-Med, the most\nperformant open-source medical VLM, and is approaching the accuracy of junior ophthalmologists. When testing\nthe ability of VLMs to screen for high-risk patients, LLaVA-Med substantially underperforms compared to junior\nophthalmologists and even non-specialist opticians. In comparison, RetinaVLM-Specialist's reports reduced the number\nof incorrect urgent referrals by almost four times compared to opticians and had higher recall for urgent referrals than\njunior ophthalmologists. Finally, RetinaVLM is able to reinforce its decisions by citing observable biomarkers within\nthe written report, and highlighting their corresponding regions within the image.\nWe postulate that the poor performance of existing medical foundation models stems from their lack of detailed\nknowledge related retinal OCT and AMD. Current VLMs are trained on broad, unstructured datasets that are extracted\nfrom medical textbooks, scientific publications or social media posts of healthcare professional [9]. In the United\nKingdom and the United States, clinical trainees aspiring to become specialists must undergo up to ten years of post-\ngraduate training to obtain the grade of a board-certified consultant. We argue that training data of current foundation\nmedical VLMs lacks this specialist knowledge and experience, hindering their effective application to real-world clinical\ntasks.\nA core innovation of our work was the creation of a dedicated training curriculum that specializes VLMs in image-based\nclinical decision making. Analogously to current medical education, this curriculum deconstructs clinical problems\ninto sets of mandatory capabilities required for their resolution and selectively trains VLMs in these skills. To this end,\nwe obtained a large number of tabular reports by processing of retrospectively collected clinical data using advanced\nalgorithms. Additionally, we tasked ophthalmologists to produce a limited number of highly specific textual reports.\nIn total, our curriculum comprises 41,926 OCT images with 479,710 corresponding visual questions and answers.\nWhile still modest in size compared to substantially larger foundation datasets, we believe such curated needs-driven\napproaches are required to deploy language models specialist healthcare. In a similar vein, leading technology companies"}, {"title": "Limitations and future research directions", "content": "Naturally, the quality of our curriculum depends on the underlying reports, in particular that of the 330 collected\ndetailed textual reports. The majority of the reports used to create RetinaVLM-Specialist were written by a junior\nophthalmologist with three years experience. The remaining reports were written by an ophthalmologist with ten years\nof experience, and their reports were more comprehensive. To match or surpass the performance of the average junior\nophthalmologist, we seek to finetune this model further on a small number of reports written by intermediate and senior\nophthalmologists.\nAdditionally, the data may reflect local clinical definitions and workflows. While RetinaVLM-Specialist was trained on\nreports written by the two UK-based ophthalmologists, it was tested on labels derived from four separate ophthalmol-\nogists from different countries and hospitals. In our testing we identified differences in the staging definitions used\nby the UK-based and Austria-based ophthalmologists, especially with regard to identifying fibrovascular features that\ndifferentiate intermediate from inactive late wet AMD. This discrepancy resulted in a number of the misclassifications\nby RetinaVLM.\nSimilarly, we observed a discrepancy in image interpretation between junior and senior ophthalmologists. Junior\nophthalmologists did not recommend patients for referral if it was likely that the retinal fluid observed was caused\nby traction rather than neovascularization, as it is not treatable with antiangiogenic drugs. Conversely, the senior\nophthalmologist preferred that these patients be still referred for immediate assessment to rule out neovascularization.\nRetinaVLM was explicitly instructed to refer patients with any sign of fluid of any cause (see Section 5.8.3), and\ncorrectly referred more patients as a result.\nWe found that the LLM generating the question-answer pairs from the reports was sensitive to the specifics of the\ngeneration instruction. Extensive trial and error were required to arrive at several instructions, listed in Section A.2, that\nresulted in diverse sets of high quality question-answer pairs. We discern that all aspects of dataset creation - deciding\non the required capabilities, collecting specialized annotations and converting these to question-answer pairs - should\nbe formalized to systematically compare different approaches and ultimately scale dataset curation in the future.\nBeyond the formalization and extension of the creation of the curriculum, there are other potential technical improve-\nments to RetinaVLM. Currently, RetinaVLM processes a single two-dimensional OCT image from one type of OCT\nscanner. In ophthalmological practice, decisions are made based on three-dimensional images from multiple time\npoints, although many recent studies on the use of foundation models in ophthalmology also analyze two-dimensional\nimages [18]. We mitigated the impact of this discrepancy on our study by tasking ophthalmologists to select the most\nrelevant two-dimensional slice of the imaged volume before proceeding with the referral decision. In the future, a\nmore sophisticated vision encoder, which is able to handle three-dimensional data from diverse OCT imaging devices,\ncould be integrated with RetinaVLM. Similarly, one may opt to incorporate multimodal information, such as health\nquestionnaires, clinical tests or the patient's medical history, into the decision making process [26]. The fundamental\nmodel architecture and training would remain similar, but the level of reasoning required for differential diagnosis\nacross multiple scans would potentially increase.\nRetinaVLM also inherits some of the fundamental limitations of language models. LLMs are prone to confidently present\nfalse or fabricated information, termed hallucinations, which has been identified as problematic in medical contexts\n[13,14]. RetinaVLM occasionally hallucinates observations of retinal fluid and consequently diagnoses more advanced\nAMD stages than necessary. RetinaVLM's output was also sensitive to the wording of questions and instructions. While\nthis had little impact on our qualitative analysis, extensive trial and error was necessary to ensure that RetinaVLM\nresponded with one of the provided options in the quantitative analyses.\nIn this study, we exclusively trained RetinaVLM for the management of AMD from OCT images, ignoring other\nretinal pathologies, such as diabetic retinopathy or glaucoma, or imaging modalities, such as color fundus photography\n[27,28]. While this enabled us to explore the potential to encode advanced clinical levels of specialism into VLMs at\ndepth, it would limit the applicability of the current version of our models for ocular screening. We hypothesize that\nVLMs could be specialized on a wider range of ophthalmological tasks by incorporating additional VQA datasets and\ntraining VLMs on them. This requires costly and time-consuming curation of specialized training datasets by medical\nexperts. However, we believe that the involvement of medical experts is necessary as routine clinical skills and patient\nmanagement protocols are rarely documented in existing datasets used to train AI models."}, {"title": "Conclusion", "content": "Foundation vision-language models (VLMs) have the potential to revolutionize healthcare by automatically interpreting\nmedical images and communicating their findings in detailed written reports. Trained on large datasets containing\nmillions of medical images and textual annotations, foundation language models have stirred considerable interest for\ntheir expert-level performance on medical licensing exams and case studies. However, in this work we have shown that\nfoundation medical VLMs substantially underperform human experts on routine clinical tasks.\nWe hypothesize that the training data of foundation medical VLMs currently lacks specialist clinical knowledge and\nexperience. To address this, we developed a curriculum-based approach that integrates the expertise of domain specialists\ninto the training of medical VLMs. The resulting model, RetinaVLM, can produce detailed imaging reports that make\naccurate recommendations for the clinical management of AMD. It approaches and often matches the performance of\njunior ophthalmologists in disease staging, and outperforms non-specialist opticians in patient referral.\nThese results indicate that merely increasing the scale of training datasets is insufficient for the development of VLMs\nwith real-world clinical utility. Instead, medical VLMs require high-quality data directly related to the challenges\nfaced by clinicians in their daily practice. We believe our proposed curriculum-based approach provides a blueprint for\nspecializing VLMs that generate true value in healthcare."}, {"title": "5 Methods", "content": ""}, {"title": "5.1 Retinal image dataset curation", "content": "We use two retinal OCT dataset in this study. The first, described in Section 5.1.1, contains a cohort of patients with\nAMD collected retrospectively at the Southampton Eye Unit. The second dataset, described in Section 5.1.2, contains\nscans of the initial visits of patients referred, primarily by opticians, to the Southampton Eye Unit.\nAll data was collected in the scope of the PINNACLE study (ClinicalTrials.gov NCT04269304), which received approval\nfrom the East Midlands-Leicester Central Research Ethics Committee in the United Kingdom (ref. 19/EM/0163) and\nthe institutional review boards of all participating institutions. It complies with the principles of Good Clinical Practice\nand the Declaration of Helsinki. All images were captured using Topcon 3D OCT scanners (Topcon Corporation, Tokyo,\nJapan). Both datasets contain images of size 416 \u00d7 512 with a pixel size of 3.5\u00d711.7 \u00b5m2."}, {"title": "5.1.1 Retrospective cohort for training RetinaVLM and testing disease staging, report writing and biomarker\nanalysis", "content": "The retrospective dataset contains 45,379 OCT images from 6,152 eyes belonging to 3,468 patients, collected over\neight years, between 2012 and 2020, at the Southampton Eye Unit and aggregated by the PINNACLE consortium. For\neach OCT scan we use the mediolateral 2D slice centered at the fovea.\nWe designated 41,926 of the 45,379 OCT images from 5,547 eyes of 3,057 patients patients for training purposes.\nAdditionally, we reserved 2,311 images from 326 eyes of 187 patients for validation, and 396 images from 279 eyes\nof 224 patients for testing. We ensured that images from each patient do not appear in more than one of the training,\nvalidation or test sets.\nThe training set was used to create both curriculum parts 1 and 2, detailed in Sections 5.4.1 and 5.4.2. The test set was\nused to evaluate the resulting model in Sections 2.3 and 2.5. For each patient in the test set, two junior ophthalmologists\nindependently decided the disease stage, and disagreements were arbitrated by the two senior ophthalmologists. Inactive\nlate wet AMD was defined by the presence of any subretinal hyperreflective material or fibrosis. Active late wet AMD\nwas defined by presence of any fluid within the image and took precedence over the inactive classification."}, {"title": "5.1.2 External cohort of patients referred to Southampton AMD treatment clinic", "content": "We also collected an external dataset of 95 patients that were referred primarily by opticians to the Southampton Eye\nUnit between 02/2023 and 12/2023. None had yet received treatment for AMD, and mostly had no AMD, intermediate\nAMD or small features related to active wet AMD. This represents a distribution shift from the retrospective cohort,\nwhere many patients had already received treatment for AMD and were in the inactive late wet stage of AMD. As such,\nit enabled us to estimate the robustness of both variants of RetinaVLM to shifts in patient population. This dataset was\nnot used for model training and was reserved for testing VLMs on patient referral, detailed in Section 2.4.\nFor each patient we sourced scans of both their left and right eye that were acquired on their first visit to the clinic.\nWe also collected the originally issued letter of referral, as depicted in Figure 5d. Then, two junior ophthalmologists\nanalyzed the 3D OCT volumes of each eye to assess the patient's risk and recommend a level of referral urgency. They\nthen selected the image slice that most supported their assessment of the patient's risk. In healthy patients where both\nvolumes contained no pathological signs in any of the image slices, they were instructed to select the mediolateral\nfovea-centered 2D slice from one of the two volumes."}, {"title": "5.2 Vision-language model architecture", "content": "RetinaVLM consists of two main components: an ophthalmological vision encoder and a generative LLM (see Figure\n15). For the ophthalmological vision encoder, we adopt a Resnet50 convolutional neural network with over 23 million\nparameters which was previously pre-trained with self-supervised contrastive learning on the 41,926 OCT images from\nthe train set of the retrospective cohort. Specifically, it was trained with Bootstrap Your Own Latent (BYOL) [29] using\nthe same implementation details as the standard contrastive approaches used in [17], which consistently performed\non par with RETFound [18] specifically on data from the Southampton Eye Unit. This vision encoder projects each\n192 \u00d7 192 input image to a set of spatially arranged 6\u00d76 visual embeddings, which are extracted from the last layer\nbefore global average pooling. Each embedding has a dimension of himg = 2048. They also have a receptive field\nof size 336, so each embedding contains global knowledge of the image that is contextualized at its local position.\nFor the LLM, we employ the 8 billion parameter instruction-tuned Llama3 model by Meta [19,30] as the generative\nLLM, which was was the most performant openly available model at the time of our study. LLama3 uses an embedding\ndimension of hlang = 4096."}, {"title": "5.3 Foundation medical vision-language models", "content": "We used the two most widely adopted foundation vision-language models for medical applications at the time of this\nstudy [10,9]. They were both trained on large biomedical datasets sourced from the Internet, and have been applied in\nchest x-ray [32]. The first, Med-Flamingo [10], which was built on Flamingo [33] and finetuned on image and text data\nfrom medical textbooks and the PubMed Central Open Access (PMC-OA) dataset [34]. The second, LLaVA-Med [9],\ndeveloped by Microsoft, is a VLM built on LLaVA [35] and finetuned to follow textual instructions regarding a broad\nrange of biomedical images contained in PubMed Central 15M (PMC-15M) [36]. As they were trained as generalist\nmodels on various imaging modalities, they were both purportedly capable of interpreting retinal OCT images. Both\ncorrectly identified that the provided image was a retinal OCT scan when instructed to report the modality of the given\nimage.\nFor Med-Flamingo, we then provide instructions using the following template provided in their code, replacing\n{question} with the instruction text:\nYou are a helpful medical assistant. You are being provided with images, a\nquestion about the image and an answer. Follow the examples and answer the\nlast question. <image>Question: {question} Answer:\nSimilarly, for LLaVA-Med we use their following system prompt:\nYou are a helpful medical assistant. You are being provided with images, a\nquestion about the image and an answer. Follow the examples and answer the\nlast question. <image>Question: {question} Answer:"}, {"title": "5.4 Report curation and question-answer pair generation", "content": ""}, {"title": "5.4.1 Curriculum part 1: Introduction to retina", "content": "To create the tabular reports for the first part of the curriculum we used a cluster-based approach to efficiently label the\n41,926 training images with biomarker annotations [37]. Contrastive learning is used to extract self-supervised features\nfrom the dataset. The dataset is then partitioned into 40 clusters of images that share common features. Labels are then\nassigned to these clusters by senior ophthalmologists. To this end, 20 images from each cluster were reviewed by senior\nophthalmologists. If the majority of the images exhibited common features, such as 'large drusen' or 'subretinal fluid',\nthese labels were assigned to the entire cluster. These labels were used in in combination with the patient's age, sex and\ntheir functional visual acuity score (measured on a LogMAR chart and converted to Letter score) to create the tabular\nreports. Additionally, the reports list three biomarkers that are stated as not being present. These are drawn from a\ndistribution of all biomarkers, weighted by their prevalence in the dataset, that were not among the cluster labels for\nthat image. Counts of the prevalence of each tabular variable among the images are shown in Figure 7a, and a sample of\nfour tabular reports they result in are shown in Figure 7b.\nTo generate question-answer pairs from the large volume of tabular reports we used WizardLLM-70B, which was the\nmost capable freely-available LLM at the time of the creation of the first part of the curriculum. This resulted in a total\nof 408,505 question-answer pairs. Examples of the question-answers pairs generated by this approach are shown in the\n'curriculum part 1' section of Figure 10a and Figure 10b."}, {"title": "5.4.2 Curriculum part 2: Advanced retinal specialism", "content": "The second part of the curriculum used manually curated reports written by retinal specialists. Two junior ophthal-\nmologists were tasked with describing the main pathological biomarkers and diagnoses related to AMD, while also\nnoting any other observation regarding the retinal anatomy. This yielded high-quality textual reports that go beyond\nthe short notes that ophthalmologists typically write in clinical routine. The first junior ophthalmologist, with three\nyears of experience specializing in ophthalmology, wrote the majority of 244 reports (see Figure 8a). While these were\nhighly accurate, they were less comprehensive in their analysis than the remaining 86 reports written by the junior\nophthalmologist with 10 years of experience (see Figure 8b). In total, this process yielded the 330 specialist reports."}, {"title": "5.5 Vision-language model training and generation", "content": ""}, {"title": "5.5.1 Combined vision language training", "content": "When training on both curriculum part 1 and part 2, both the vision encoder and LLM used in RetinaVLM are kept\nfrozen, that is, they are not updated during the entire training process. This mitigates issues of catastrophic forgetting\n[38], a phenomenon in which foundation models lose their general capabilities during the finetuning process. This\nis key to ensuring RetinaVLM can respond to versatile questions and instructions that it was not exposed to during\ntraining. While the vision encoder and LLM are frozen, RetinaVLM gains all its specialized capabilities solely through\nthe training of the adapter.\nBefore training on the retrospective cohort we downsample each image by a factor of 2 from 416 \u00d7 512 to 208 \u00d7 256\npixels. We then augment each image using the protocol outlined in [17], which results in a randomly cropped OCT\nimage of size 192 \u00d7 192. During training, we set the Llama 3's system prompt to:\nYou are a helpful ophthalmological specialist chatbot capable of interpreting\nretinal OCT images.\nWe begin the instruction that RetinaVLM will be trained on with the following line:\nHere is an encoding of a retinal OCT image <Img><ImageHere></Img>\nThen, for each image in the batch, we randomly select a corresponding question-answer pair from the current curriculum\ndataset. We then add the text of the question to the instruction. We then populate LLama3's conversation template\nwith the full instruction and corresponding answer to form the full textual input. We project this textual input to the\nembedding space of the LLM using the pre-existing tokenizer and embedding layer of the LLM (see Figure 15).\nSimultaneously, we use the ophthalmic vision encoder E to extract the 6 \u00d7 6 vision embeddings from each image.\nAfter flattening these, we apply the adapter to each embedding separately to project them to the embedding space of\nthe language model. To create the final set of embeddings that are provided to the LLM, we replace the embeddings,\ncorresponding to the <ImageHere> phrase in the input with the 36 adapted vision embeddings.\nFinally, the resulting sequence of adapted visual embeddings and language embeddings are passed together through the\nfrozen LLM. This yields a list of predicted token logits with the same length as the input sequence. We then compute\nthe causal language modeling loss between these predicted answer logits and the ground truth answer tokens. We then\noptimize the adapter to minimize this loss.\nBeginning by randomly initializing the adapter, we train in this fashion for 100,000 steps on questions and answers\nregarding the 41,926 images in curriculum part 1 (introduction to retina) to obtain RetinaVLM-Base. The trained adapter\nis then further finetuned on the 71,165 questions and answers regarding the 330 images in in curriculum part 2 (advanced\nretinal specialism) for 100,000 steps, resulting in the final RetinaVLM-Specialist model. For both curriculum parts, we\nuse a batch size of 12 and the AdamW optimizer with \u03b21 0.9 and \u03b22 0.999."}, {"title": "5.6 Testing for catastrophic forgetting", "content": "As a quick test to indicate whether RetinaVLM-Base and RetinaVLM-Specialist incurred catastrophic forgetting during\ntraining, we provided them a retinal OCT image and tested them on the following instruction which does not feature in\ncurriculum parts 1 or 2:\nHere is an encoding of a retinal OCT image <Img><ImageHere></Img>\nWhat is the capital of England?\nPrevious iterations of RetinaVLM that used a more complex adapter design were susceptible to responding with answers\nregarding the retinal OCT image, rather than answering the provided question. The simplified architectural approach\nused in this study results in models that pass this test."}, {"title": "5.7 Using VLMs for inference and generating text", "content": "After VLMs have been trained on image and text datasets, they can be used to generate responses to new questions\nand images. To use the baseline foundation VLMs for testing we take the central crop in each 416 \u00d7 512 OCT image,\nresulting in an image of size 384 \u00d7 384. We then downsample the image to 224 \u00d7 224 and repeat it along the color\ndimension, resulting in the 3 \u00d7 224 \u00d7 224 images that are required by both foundation models. To provide the image to\nthe RetinaVLM variants during testing, we take a central crop of the 208 \u00d7 256 downsampled images, resulting in an\nimage of size 192 \u00d7 192 required of both RetinaVLM variants.\nWe then employ the same method with all VLMs for generating responses to instructions. Provided the image and\ntextual instruction, we build the output sequence of tokens by repeatedly appending the token to the output assigned\nthe highest probability by the VLM. This is equivalent to using a temperature parameter set to 0, and is the standard\napproach for generating the most accurate and least creative output from LLM-based models. This process is repeated\nuntil a stop token is generated, signaling the end of the VLM's response. The model's tokenizer is then used to convert\nthe numeric output tokens to the final free text output."}, {"title": "5.8 Experimental setup", "content": "Our entire evaluation was conducted in zero-shot, that is, after training on curriculum part 1 and part 2 RetinaVLM\nrequires no further finetuning in order to perform tasks related to disease staging, patient referral and biomarker analysis.\nInstead, for each test we designed a specific instruction that was provided to all VLMs to generate the application-\nspecific reports that were used in our analyses. These instructions were derived through experimentation with all VLMs\non the validation set, so while they are not designed for any one VLM in particular, they do contain information related\nto the task at hand."}, {"title": "5.8.1 Tests of disease staging", "content": "The following instruction was given to all VLMs to and obtain the reports of the 276 test images that were analyzed\nin Section 2.3. The instruction requests VLMs to begin by describing the image, and then deduce the most advanced\ndisease stage:\nDescribe the OCT image in detail and list any biomarkers or abnormalities,\nincluding the most likely AMD stage of the patient.\nThen, based on those observations, state if the patient's most advanced AMD\nstage is 'healthy', 'early', 'intermediate', 'late dry', 'late wet (inactive)'\nor 'late wet (active)'?\nAfter the VLM generated its report (using a maximum of 500 tokens), we appended the following text to its response.\nBased off the image and those findings, the patient's most advanced AMD stage is\nbefore continuing generation for another 300 tokens. From these tokens, we extracted the final disease staging prediction\nby searching for the first instance of any of the listed disease stages. This post-processing step is only necessary for\nquantitative tests of accuracy, as it enables the reliable extraction of the disease stage from the free text report. In\ncases where the VLM discusses multiple disease stages, such as in 'more advanced than early AMD, and is\nintermediate AMD as there is no evidence of late wet AMD', the disease stage was manually extracted.\nIn cases where no disease stage was provided or could be extracted manually, this counted as an 'Invalid response'."}, {"title": "5.8.2 Evaluations of correctness, completeness and conciseness by senior ophthalmologists", "content": "For the direct evaluation by the senior ophthalmologists, we randomly selected 28 of the test images from the\nretrospective dataset, and tasked the two junior ophthalmologists with annotating the images. We provided the following\ninstruction to RetinaVLM-Specialist and LLaVA-Med to generate their image reports:"}, {"title": "5.8.3 Tests of patient referral", "content": "The following instruction was given to all VLMs to generate reports that focus on patient referral recommendations,\nwhich are analyzed in Section 2.3. This instruction was run for the 95 referral images, introduced in Section 5.1.2. In\norder to accurately convey the specific requirements of the wet AMD treatment clinic, we provided the comprehensive\nreferral protocol used by the senior ophthalmologists in the instruction:\nWrite an extensive report describing the OCT image and listing any\npresent biomarkers or other observations. Do not provide a disease\nstage, or referral recommendation yet.\nBeing seen by a specialist at the Southampton clinic:\nA. The Southampton clinic requires that patients with any sign of\nintraretinal fluid, any sign of subretinal fluid, or any sign of cyst(s),\nMUST be seen by a specialist at the Southampton clinic within the next two\nweeks.\nB. The Southampton clinic requires that patients who do not have any sign of\nintraretinal fluid, any sign of subretinal fluid, or any sign of cyst(s),\nbut do have some biomarkers of early or intermediate AMD, should be seen by\na specialist at the Southampton clinic for routine referral.\nC. The Southampton clinic requires that patients who do not have any sign of\nintraretinal fluid, any sign of subretinal fluid, or any sign of cyst(s),\nbut do have medium to large drusen, drusenoid PED, hypertransmission or\natrophy, should be seen by a specialist at the Southampton clinic for\nroutine referral.\nD. The Southampton clinic does not need to see patients who have no biomarkers\nand healthy retinas at all.\nSouthampton specialist visit: Next, tell me if your initial report of the O\u0421\u0422\nimage indicates that the patient should be seen by a specialist at the\nSouthampton clinic \"within the next two weeks\", to be seen \"within 18 weeks\n(routine referral)\", or \"not be seen\" at all?\nAs before, after the VLM generated its report (using a maximum of 500 tokens), we added the following text to its\noutput:"}, {"title": "5.8.4 Tests of biomarker analysis", "content": "The following instruction was given to all VLMs to generate reports that conclude the presence of absence of the 10\ndifferent biomarkers, evaluated on the 396 test images in Section 2.5:\nDescribe the OCT image in detail and list all biomarkers or abnormalities.\nDetail if there are any signs indicating that {biomarker} might be present,\neven if there is only a small amount.\nFinally, conclude your findings by telling me if {biomarker} {article} \"not\npresent\", or if potentially any amount of {biomarker} {article} \"present\" in\nthe OCT image.\nFor each of the 10 biomarkers, the phrase {biomarker} was replaced by the actual biomarker name (such as 'subretinal\nfluid'), and the {article} replaced by is for singular biomarkers or are for plural biomarkers (such as drusen). After\nthe VLM generated its report (using a maximum of 500 tokens), we added the following text to its output.\nTo conclude these findings, in the OCT image {biomarker} {article}"}, {"title": "5.8.5 Computing language-based image saliency maps", "content": "We provide methodogolical details for the computation of the language-based saliency maps discussed in Section 2.5,\nand shown in Figure 6d and Figure 14. With saliency maps we aim to identify which regions of the image were most\nrelevant to certain passages, such as large subretinal fluid, of RetinaVLM-Specialist's responses. The most\ndirect way to generate these visualizations to use attention maps, but we found Llama3's pretrained attention maps did\nnot result in any meaningful saliency maps. To address this, we used Grad-CAM [22], a technique for highlighting the\nmost relevant image regions to the prediction of an image classifier. By defining the predicted class as the sum over the\ntokens in the output passage, which formulates the LLM as an image classifier, we were able to generate the saliency\nmaps shown in Figure 6. A code implementation can be found at the repository referenced in Section 7."}, {"title": "5.9 Measurements of performance and statistical analysis", "content": "To calculate the performance of each VLM in multiple-choice question answering, we used the micro F1 score. This\naggregates the total number of false positives (FP), false negatives (FN), true negatives (TN) and true positives (TP)\nover all classes before computing the F1 score using equation 1\nF1 = \\frac{2 \\cdot TP}{2 \\cdot TP+FP+ FN}\nIn cases where the VLM returned an 'Invalid response' this was counted as a false negative for the ground truth class.\nAfter calculating the F1 score, we determined the 95% confidence interval through bootstrapping N = 1000 times with\nreplacement.\nTests of significance (aggregated in Figure 12) were calculated using a two-sided McNemar's test [39]. This test assesses\nthe difference in the number of correctly versus incorrectly predicted samples, focusing on cases where the models\nagree or disagree on the labels. A significant p-value from the McNemar test allows us to reject the null hypothesis\nthat both models have identical classification performance. We then used the following notation to indicate levels of\nstatistical significance: *** for p < 0.001, ** for p < 0.01, and * for p < 0.05 and 'ns' (not significant) for p > 0.05."}, {"title": "5.10 Computing hardware and software", "content": "We use Python 3.12.2 to conduct all model question-answer generation, VLM training, and VLM evaluation. To generate\nthe question-answer pairs for curriculum part 1 we used 3 40GB NVIDIA A40 GPUs. For both training RetinaVLM\nand for evaluating all VLMs we use a single 80GB NVIDIA A100 GPU and PyTorch [40] version 2.1.2. Training\nRetinaVLM on takes 1 day on curriculum part 1, and another day on curriculum part 2. Llama3 was downloaded via\nHuggingface with model ID \u2018meta-llama/Meta-Llama-3-8B-Instruct'. The baseline VLM Med-Flamingo's code and\nmodel weights were installed following the instructions at https://github.com/fastscience-ai/MedFlamingo,\nand LLaVA-Med's from https://github.com/microsoft/LLaVA-Med.Confusion matrices and results calculations\nwere computed with scikit-learn version 1.4.1 and numpy version 1.26.4. Figures and tables were created in draw.io\nv24.4.0 using plots generated by matplotlib version 3.8.4 and seaborn version 0.13.1. Grad-CAM was computed using\ngrad-cam version 1.5.0. McNemar's tests of significance were calculated using statsmodels version 0.14.1."}, {"title": "6 Data availability", "content": "Both imaging datasets are currently being curated and maintained by the Vienna Reading Center on behalf of the\nPINNACLE consortium. The data will be made available to once the PINNACLE study concludes in 2026 [41]."}, {"title": "7 Code availability", "content": "The code used to create the question-answer pairs, train, and evaluate the models will be uploaded to\nhttps://github.com/RobbieHolland/SpecialistVLMs. The code may be used to develop the models can be repurposed for\nother medical specialties. The model weights for RetinaVLM-Base and RetinaVLM-Specialist are both available at\nhttps://huggingface.co/RobbieHolland/RetinaVLM. These are only intended for research purposes related to retinal\nOCT images."}, {"title": "A Supplementary material", "content": ""}, {"title": "A.1 Question-answer generation prompts (introduction to retina)", "content": "The limited scope of the first part of the tabular reports led us to use only one prompt (see Section A.1.1) to create the\n408,545 question-answers in curriculum part 1, 'introduction to retina'."}, {"title": "A.1.1 General question-answering module", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nThe model will access ONLY the image to deduce attributes. For context, the image in question has attributes as follows:\n<ReportText>\nBased on these attributes, generate a numbered list of diverse questions and answers.\nEnsure the format is:\n1. Q: [Question about an image attribute]\nA: [Specific answer deduced from the image]\nRules:\nQuestions should be crafted in a way that they don't explicitly state the attribute values, but the answers should be\nbased on them. All attributes can be determined from the image.\n- Incorporate both yes/no and open-ended styled questions, but always provide a definitive answer in the answer section.\nOccasionally touch on patient outcome/treatment."}, {"title": "A.2 Question-answer generation prompts (advanced retinal specialism)", "content": "Creating a large quantity and variety of question-answer pairs was important for preventing overfit when finetuning on\nthe 330 images that constitute curriculum part 2. The prompts each focus on a different aspect of question answering\nand report writing, but are expected to have some overlap in the question-answer pairs they create. The six modules\nused to generate up to 230 question-answers per image report, as shown in Figure 2, are covered by the following 10\nprompts:\n1. Advanced biomarkers (Up to 30 total question-answers)\n\u2022 Up to 30 question-answer pairs specific to biomarkers (see Section A.2.1)\n2. Disease staging definitions (Up to 50 total question-answers)\n\u2022 Up to 20 question-answers that use the observation and staging guidelines (see Section A.2.2)\n\u2022 Up to 30 question-answers that train the model to replicate any reasoning linking observations to the\ndisease stage that is present in the report (see Section A.2.3)\n3. Staging reasoning (Up to 60 total question-answers)\n\u2022 Up to 20 question-answers that use the observation and staging guidelines to train the model to explain\nthe diagnosed disease stage in terms of the visible biomarkers (see Section A.2.4)\n\u2022 Up to 40 question-answers that focus on training the model in differentiating active from inactive late wet\nAMD (see Section A.2.5)\n4. Referral reasoning (Up to 25 total question-answers)\n\u2022 Up to 25 question-answer pairs that focus on training the model to reason about different levels of urgency\nin patient referral (see Section A.2.6)\n5. General visual question-answering (Up to 30 total question-answers)\n\u2022 Up to 15 question-answers created directly from the report, but specific to one attribute (see Section\nA.2.7)\n\u2022 Up to 15 question-answers created directly from the report, but including longer and more open-ended\nquestions A.2.8)\n6. Report writing (Up to 35 total question-answers)"}, {"title": "A.2.1 Advanced biomarkers module", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nBelow are guidelines outlining what can appear in an OCT image:\n<ObservationGuidelines>\nHowever, the image the model is being asked about is characterised by the following description:\nDESCRIPTION OF IMAGE: \"<ReportText>\"\nTask: Write 30 varied questions and answers that ask the model about the image.\nEnsure the format is:\n1. Q: [Question or statement to describe the image]\nA: [Augmented version of actual image description]\nRules:\nAsk separately about the presence, amount, location and type of some of the biomarkers in the\nguidelines\nAsk about the presence or absence of the biomarkers in the guidelines\nRather than saying the image/description does not specify/mention a biomarker, instead say 'the\nimage does not show/display/exhibit evidence of' the biomarker UNLESS its presence is already\nimplied by another present biomarker. The model does not see the above description of the image,\nit's only given the original image when answering questions.\nTry not to give away too much information included in the image description in the question\ntext. The model must learn to use the image to determine the answer, and not make educated\nguesses based on the question alone.\nThe answer must be accurate and reflect the same information in the image description.\nWrite nothing except the questions and answers.\nTips:\nExample questions: \"Does this image show any subretinal fluid?\" \"Do you see any intraretinal\nfluid?\" \"Is there any hypertransmission? \"Does the image show a PED?\"\nAnswer style variation: Finally, do not start too many questions with \"No, ...\" or \"Yes, ...\".\nVary the answer style (the biomarker is 'not present', 'is shown', 'exhibits no', 'does contain'\netc...)\nPositive and negative balance: Try to include an even balance of questions with positive\nresponses (i.e. ask the model about each of the biomarkers that were reported in the description)\nand negative responses (i.e. that biomarker is not present)\nTo do this, if you create a question about a biomarker that isn't in the description, try to\ncreate a second, similar question about a biomarker that is observable in the image.\nIn order to make the question set not give too much away about the image, you can make paired\nquestions which have positive and negative responses.\nFor example, for an image with a PED but no subretinal fluid, if you ask f.e.\n\"Q: Is there a PED? If so, where? A: There is a PED present, it's in the center...\""}, {"title": "A.2.2 Disease staging definitions modules", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nBelow are guidelines outlining what can appear in an OCT image:\n<ObservationGuidelines>\n<DiseaseStagingGuidelines>\nHowever, the image the model is being asked about is characterised by the following description:\nDESCRIPTION OF IMAGE: \"<ReportText>\"\nTask: Write 20 varied questions and answers that ask the model about the image.\nEnsure the format is:\n1. Q: [Question or statement to describe the image]\nA: [Augmented version of actual image description]\nRules:\nInformation about the image should not be in the question.\nThe answer must be accurate and reflect the same information in the image description.\nWrite nothing except the questions and answers.\nTips:\nQuestions should be specific and ask about certain attributes, or sets of attributes. For\nexample \"Q: Is there any subretinal fluid in this image?\" or \"Q: Is the AMD stage intermediate,\nor is it more advanced?\".\nAsk separately about the presence, amount, location and type of some of the biomarkers. Try to\ncreate an even balance of 'yes and 'no' answers.\nRather than saying the image does not directly/explicitly specify/mention the presence of an\nattribute, instead say 'the image does not exhibit/show/display/evidence' the attribute, UNLESS\nits presence is already directly implied by the presence of another attribute. The model does not\nsee the above description of the image, it's only given the original image when answering\nquestions.\nSometimes the desired output format should be specific in the question (f.e. Answer with 'yes'\nor 'no', or answer by stating if the image 'does' or 'does not' contain the biomarker in\nquestion.)"}, {"title": "A.2.3 Disease staging from the report module", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nCommon disease stages for age-related macular degeneration are: healthy (no-AMD), non-AMD\npathology, early AMD, intermediate AMD, late dry AMD, late wet (inactive) AMD, late wet (active)\nAMD"}, {"title": "A.2.4 Disease staging reasoning (with guidelines)", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nBelow is a schema outlining what can appear in an OCT image:\n<ObservationGuidelines>\n<DiseaseStagingGuidelines>\nHowever, the image the model is being asked about is characterised by the following description:\nDESCRIPTION OF IMAGE: \"<ReportText>\"\nTask: Write 20 varied questions and answers that require the model to perform chain-of-thought\nreasoning.\nEnsure the format is:\n1. Q: [Question or statement to describe the image]\nA: [Augmented version of actual image description]\nRules:\nInformation about the image listed in the description should not be used in the question, only\nin the answer.\nNever use the word 'description' or 'mention' in the answer, the model does not see the\ndescription of the image, it only sees the original image itself.\nThe answer must be accurate and reflect the same information in the image description.\nWrite nothing except the questions and answers.\nTips:\nSome questions should ask the model to provide a long and detailed answer to questions like\n'Describe all the observable biomarkers in the image and then link these to the most likely\ndisease stage'.\nOne or two questions should ask the model to explain/deduce the highest precedent AMD stage\n(i.e. the overall AMD stage) by describing the biomarker(s) in the image which belong to the most\nadvanced disease stage, and linking them to the relevant disease stage.\nSome questions should ask the model to summarise any relevant biomarkers and, explaining its\nreasoning using the guidelines, conclude with the AMD stage. For example, a drusenoid PED\nsuggests intermediate AMD, but if coupled with subretinal fluid the overall AMD stage becomes\nactive late wet due to the fluid.\nSome questions should ask the model to fully describe and list all its image observations, and\nthen conclude the presence, absence, location or quantity of a specific biomarker (f.e. 'Describe\nthe OCT image in detail and note any abnormalities, and then tell me if the image contains\nsubretinal fluid.')"}, {"title": "A.2.5 Disease staging reasoning second module", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nBelow are guidelines outlining what can appear in an OCT image:\n<DiseaseStagingGuidelines>\nHowever, the image the model is being asked about is characterised by the following description:\nDESCRIPTION OF IMAGE: \"<ReportText>\"\nTask: Write 40 varied questions and answers that require the model to estimate the patient's\ndisease stage from the image.\nEnsure the format is enumerated:\n1. Q: [Question or statement to describe the image]\nA: [Augmented version of actual image description]\nRules that always apply:\nIf the estimated disease stage is not provided in the report, do not write ANY questions and\nanswers. Simply write \"No disease stage in report\".\nIf reasoning for the disease stage is provided in the report, you MUST include this\nlogic/nuance in the model's answers.\nYou MUST not give away information about the image description in the question text. The model\nmust learn to use the image to determine the answer, and not make educated guesses based on the\nquestion alone.\nThe answer must be accurate and reflect the same information in the image description.\nThe questions and answers must vary in their style, formulation and vocabulary.\nWrite nothing except the questions and answers.\nRules that are specific to differentiating cases of active, vs inactive, late wet AMD:\nIn cases with an active late wet diagnosis, you MUST make it clear that the PRESENCE OF FLUID\nis what differentiates active from inactive late wet AMD. Make this clear when explaining the\nreasoning behind active late wet diagnoses.\nSo do not imply that f.e. \"The disease stage is late wet AMD (active), as indicated by the\nsubretinal/intraretinal fluid, subretinal hyperreflective material, ....\nInstead, you MUST explain that f.e. \"The overall disease stage is late wet AMD, which is active\ndue to the detection/presence of subretinal/intraretinal fluid. Inactive late wet features\ninclude fibrovascular PED, .\"\nOr \"Late wet AMD best describes the AMD stage. The detection/presence of\nsubretinal/intraretinal fluid means this is active late wet AMD. Other late wet features\ninclude fibrovascular PED, ...\"\nSimilarly, in cases with an inactive late wet diagnosis, you MUST make it clear in the model's\nreasoning that the lack of fluid, in combination with the other biomarkers, is what resulted in\nthe inactive late wet diagnosis.\nFor example, \"The stage is late wet AMD according to the evidence/presence of subretinal\nhyperreflective material, but it is inactive as there is no detectable fluid of any kind in the\nimage\"\nThe exact formulation of this answer must vary according to the question. Do not copy the\nexamples too many times. Add a lot of diversity in the model's responses.\nTips:\nMost questions should ask the model to first estimate the disease stage\nSome questions should first ask the model for its disease stage prediction, and then ask it to\nexplain its reasoning by noting visible biomarkers that relate to that stage\nFor example, \"Decide the most advanced AMD stage supported by the image, and explain your\nreasoning by noting any biomarkers most relevant to that stage.\"\nThe explanation for the disease stage may already be given in the report, but you can also use\nthe guidelines provided to work out which biomarkers resulted in that disease stage.\nQuestions 31 to 40 questions should ask for the biomarkers and then the disease stage, such as\n\"Describe any relevant/notable/significant biomarkers, and link them to the most likely disease\nstage\" (which will be the one in the report)"}, {"title": "A.2.6 Patient referral reasoning module", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nBelow are guidelines outlining what can appear in an OCT image:\n<DiseaseStagingGuidelines>\n<PatientReferral Guidelines>\nHowever, the image the model is being asked about is characterised by the following description:\nDESCRIPTION OF IMAGE: \"<ReportText>\"\nTask: Write 25 varied questions and answers that require the model to perform explain its\nreasoning before making conclusions and recommendations.\nEnsure the format is:\n1. Q: [Question or statement to describe the image]\nA: [Augmented version of actual image description]\nRules:\nInformation about the image should not be in the question.\nNever use the word 'description' or 'mention' in the answer, the model does not see the\ndescription of the image, it only sees the original image itself.\nThe answer must be accurate and reflect the same information in the image description.\nWrite nothing except the questions and answers.\nThe model sees each questions separately so they will not be seen together.\nTips:\nSome questions should ask the model to list any relevant biomarkers and, based on these,\nrecommend if the patient should be referred or not.\nMany questions should make a series of requests, by asking the model to write a long and\ndetailed answer reporting all the observable biomarkers, linking those to the most likely disease\nstage and then summarising the report with a patient referral recommendation.\nFor example, 'Describe the all biomarkers in the image, and based off of your observations\nwhich AMD best describe the patient. Summarise your report with a referral recommendation that\nfollows the treamtent guidelines.'\nSome questions should ask the model to recommend if the patient does: not need referral, if\nthey need general attention by a specialist, or if they likely need treatment with anti-vegf\nbased off the models observations\nSome questions should ask the model to explain/deduce/estimate the patient's risk (i.e. the\nrecommended referral action) by first describing the most advanced or concerning biomarker(s)\n(i.e. the observable biomarker(s) which belong to the most severe disease stage)"}, {"title": "A.2.7 Specific report-based questions module", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nThe image in question is characterised by the following description:\nDESCRIPTION OF IMAGE: \"<ReportText>\"\nTask: Write 15 varied questions and answers that ask the model about the image. They should"}, {"title": "A.2.8 General report-based questions module", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nThe image in question is characterised by the following description:\nDESCRIPTION OF IMAGE: \"<ReportText>\"\nTask: Write 15 varied questions and answers that ask the model about the image.\nEnsure the format is:\n1. Q: [Question or statement to describe the image]\nA: [Augmented version of actual image description]\nRules:\nThe answer should be a modified and augmented version of the actual description.\nTry not to give away too much information included in the image description in the question\ntext. The model must learn to use the image to determine the answer, and not make educated\nguesses based on the question alone.\nThe answer must be accurate and contain the same information as the actual description.\nHowever, the order of the sentences as they are written must change and randomly vary.\nWrite nothing except the questions and answers.\nTips:\nSome question should be general and ask to describe the image.\nOther questions should be more specific such as \"Q: Is there any subretinal fluid in this\nimage?\" that have shorter answers.\nExample questions/statements might be \"Describe the OCT image in detail.\" or \"Can you give me a\nsummary of the image?\".A.2.9 Basic report writing module"}, {"title": "A.2.9 Basic report writing module", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nThe image in question is characterised by the following description.\nDESCRIPTION OF IMAGE: \"<ReportText>\"\nTask: Write 15 questions and answers that ask the model to describe the image in full. The first\nten questions should ask to describe the entire image (with jumbled/permuted/randomised answers),\nwhile the final five should be more specific or use segments of the description.\nEnsure the format is:\n1. Q: [Question or statement to describe the image]\nA: [Augmented version of actual image description]\nRules:\nThe answer should be a modified and augmented version of the actual description.\nThe answer should contain the same information as the actual description but the order of the\nsentences as they are written must change and randomly vary\nThe question must not contain any information about the image.\nWrite nothing except the questions and answers.\nTips:\nExample questions/statements might be \"Describe the OCT image in detail.\" or \"Can you give me a\nsummary of the image?\" or \"Write a report on this image to be given to an optometrist.\""}, {"title": "A.2.10 Advanced report writing module", "content": "I am constructing a dataset to train a model to answer questions based solely on OCT images.\nBelow are guidelines outlining what"}]}