{"title": "SPECIALIST VISION-LANGUAGE MODELS FOR CLINICAL OPHTHALMOLOGY", "authors": ["Robbie Holland", "Thomas R. P. Taylor", "Christopher Holmes", "Sophie Riedl", "Julia Mai", "Maria Patsiamanidi", "Dimitra Mitsopoulou", "Paul Hager", "Philip M\u00fcller", "Hendrik P. N. Scholl", "Hrvoje Bogunovi\u0107", "Ursula Schmidt-Erfurth", "Daniel Rueckert", "Sobha Sivaprasad", "Andrew J. Lotery", "Martin J. Menten"], "abstract": "Clinicians spend a significant amount of time reviewing medical images and transcribing their findings regarding patient diagnosis, referral and treatment in text form. Vision-language models (VLMs), which automatically interpret images and summarize their findings as text, have enormous potential to alleviate clinical workloads and increase patient access to high-quality medical care. While foundational models have stirred considerable interest in the medical community, it is unclear whether their general capabilities translate to real-world clinical utility. In this work, we show that foundation VLMs markedly underperform compared to practicing ophthalmologists on specialist tasks crucial to the care of patients with age-related macular degeneration (AMD). To address this, we initially identified the essential capabilities required for image-based clinical decision-making, and then developed a curriculum to selectively train VLMs in these skills. The resulting model, RetinaVLM, can be instructed to write reports that significantly outperform those written by leading foundation medical VLMs in disease staging (F1 score of 0.63 vs. 0.11) and patient referral (0.67 vs. 0.39), and approaches the diagnostic performance of junior ophthalmologists (who achieve 0.77 and 0.78 on the respective tasks). Furthermore, in a reader study involving two senior ophthalmologists with up to 32 years of experience, RetinaVLM's reports were found to be similarly correct (78.6% vs. 82.1%) and complete (both 78.6%) as reports written by junior ophthalmologists with up to 10 years of experience. These results demonstrate that our curriculum-based approach provides a blueprint for specializing generalist foundation medical VLMs to handle real-world clinical tasks.", "sections": [{"title": "1 Introduction", "content": "Medical images are central to many clinical decisions regarding patient diagnosis, referral, and treatment. Clinicians spend a significant amount of time transcribing image-based decisions into text in order to store and communicate their findings [1,2]. Visual-language models (VLM), which automatically interpret medical images and generate detailed textual descriptions, have enormous potential to alleviate clinical workloads and increase patient access to high-quality medical care [3,4]. To date, the majority of medical VLMs have been trained to output a finite set of pre-determined textual responses [5,6,7,8]. Only recently, the combination of large language models (LLM) with medical vision encoders has led to the development of more powerful and versatile generative VLMs that are able to write comprehensive text reports or answer complex questions [9,10,11].\nThis current generation of medical language models is fueled by vast amounts of unstructured training data that is extracted from medical textbooks, scientific publications or social media posts of healthcare professionals [9,6,10]. These foundation language models have stirred considerable interest among the medical community for their expert-level performance on standardized medical question-answering tasks, such as licensing exams and case studies [12,13]. However, it is unclear whether this general performance translates to clinical utility in specialist medical domains [14]. Despite its impressive scale, the training data of foundation language models has been collected agnostically towards their downstream application and inherently lacks specialist information related to the challenges faced by clinicians in their daily practice.\nIn this study, we identify this missing piece in foundation models with the aim of developing generative medical VLMs with real-world clinical utility. We propose to deconstruct clinical problems into sets of mandatory capabilities required for their resolution and selectively train VLMs in these skills. We demonstrate the feasibility of this approach in ophthalmology. To this end we introduce RetinaVLM, the first specialist medical generative VLM (see Figure 1a). RetinaVLM is trained using a dedicated curriculum that is specific to the clinical management of age-related macular degeneration (AMD), the leading cause of blindness in the elderly [15,16]. It is able to process optical coherence tomography (OCT) images of the retina and flexibly respond to instructions and questions (see Figure 1b). In extensive experiments, we evaluate RetinaVLM's utility and versatility regarding disease staging, patient referral and biomarker analysis in AMD (see Figure 1c)."}, {"title": "2 Results", "content": ""}, {"title": "2.1 Retina VLM, a specialist vision-language model for retinal image analysis", "content": "RetinaVLM combines two main components: an ophthalmic vision encoder that processes input OCT images, and a generative LLM that handles textual instructions and outputs the corresponding responses (see Figure 1a). The vision encoder is based on our previous work, in which we found it to perform on par with RETFound [17], a large foundation model for retinal image analysis [18]. We use Meta's Llama 3 as generative LLM, the best performing model that is openly available at the time of this study [19]. However, without additional training it lacks specialist knowledge related to the analysis of OCT images and the clinical management of AMD. Both these deep neural networks have already been pre-trained on large OCT and natural language datasets, respectively, and we only finetune them in the scope of this study. Additional details about the network architecture and training are included in Section 5.2."}, {"title": "2.2 A curriculum to encode specialist ophthalmological knowledge in vision-language models", "content": "An intuitive strategy to specialize VLMs while preserving their ability to flexibly interact with text queries is to provide them with a set of medical images and corresponding question-answer pairs. They are then optimized based on the similarity of their predicted answers to the ground truth. However, visual question-answering (VQA) datasets do not exist for most medical specializations, including ophthalmology.\nTogether with a large team of ophthalmologists, which are involved with the patient care and academic research of AMD, we defined a set of required capabilities for accurate image-based clinical management of AMD. They include the identification of AMD biomarkers in OCT images, the linking of these to the AMD disease stage, and ultimately deciding on the required referral and treatment of the patient. We then curated a training curriculum, which consists of 41,926 OCT images, and 479,710 visual questions and answers to progressively specialize VLMs in these capabilities.\nThe first part of the curriculum, named Introduction to retina, primarily covers the appearance of the retina and AMD biomarkers in OCT images. Using automated data collection, we obtained tabular reports for 41,926 retrospectively collected OCT images of AMD patients (see Figure 2a). Each report describes the visible biomarkers, patient's diagnosis, visual acuity and demographic information in 34 data fields. A full description of the OCT dataset can be found in Section 5.1.1, the list of all tabular data fields and example tabular reports in Figure 7, and the methodology for their automated procurement in Section 5.4.1.\nNext, we tasked an independent LLM to generate question-answer pairs based on these reports (see Figure 2c). The model processed the content of the tabular reports \u2013 but not the OCT images \u2013 to output a numbered list of question-answer pairs. We generated an average of ten question-answer pairs per report that are mostly related to the presence or absence of specific biomarkers (see Figure 2e). The LLM was instructed to create both closed-ended 'yes or no' style questions, and simple open-ended questions. Detailed information on the LLM setup can be found in Section 5.4.1."}, {"title": "Curriculum part 2: Advanced retinal specialism", "content": "The second part of the curriculum, named Advanced retinal specialism, builds on top of the first part to link imaging biomarkers to AMD stage and the recommended course of treatment. As this reasoning cannot be fully conveyed via tabular information, we tasked two ophthalmologists with 3 and 10 years of experience, respectively, to create comprehensive textual reports for a subset of 330 OCT images (see Figure 2b). The ophthalmologists were asked to primarily describe the main pathological biomarkers related to AMD while also noting any other observations regarding the retinal anatomy. This task yielded high-quality reports that go beyond the short notes that are typically written by ophthalmologists in their clinical routine. Instructions given to the ophthalmologists as well as a set of sample reports are provided in Section 5.4.2 and Figure 8, respectively.\nSimilar to before, an independent LLM was then employed to automatically generate question-answer pairs based on the reports (see Figure 2d). Due to the substantially increased depth and scope of the full-text reports compared to the tabular ones, we used several advanced LLM instructions to create 216 diverse question-answer pairs per image on average (see Figure 2f). These cover additional biomarkers and sub-categorize them based on their size, type, and location. Other question-answer pairs are related to the causal relationship between biomarkers and six AMD disease stages as well as three levels of patient referral urgency. Moreover, the question-answer pairs were more varied in their structure in order to preserve interactive capabilities of the foundation LLM. For example, some queries asked to summarize the existing reports or provide several answers in succession. An example interaction with the LLM to generate question-answer pairs with the LLM is shown in Figure 9. Furthermore, a list of all the LLM instructions is provided in Section A.2 and example question-answers yielded by this approach are shown in the 'part 2' section of Figure 10.\nThis resulted in a dataset of 71,165 advanced question-answer pairs. By further training RetinaVLM-Base on the second part of the curriculum, we obtained our most performant VLM for the clinical management of AMD, RetinaVLM-Specialist (see Figure 2h)."}, {"title": "2.3 RetinaVLM-Specialist outperforms foundation models and approaches junior ophthalmologists in AMD disease staging and report writing", "content": "Estimating the disease stage is crucial to patient management as it allows clinicians to monitor and treat patients using standardized protocols. We assessed the ability of four different generative VLMs to determine the AMD disease stage from retinal OCT images. Specifically, we compared two medical foundation VLMs, Med-Flamingo [10] and LLaVA-Med [9], to our two specialist VLMs, RetinaVLM-Base and RetinaVLM-Specialist. Using a testing dataset of previously unseen 276 OCT images, VLMs were tasked to write reports that describe the OCT image before classifying the patient into one of six disease stages (see Figure 3a). The model predictions were compared to ground truth labels obtained from ophthalmologists. Each image was initially graded by two out of six junior ophthalmologists, whose experience in the field ranges from 2 to 15 years. Inter-rater disagreements were resolved by a panel of two senior ophthalmologists with 25 and 32 years of experience, respectively. For additional methodological details, including the instruction given to the VLMs to generate these reports, see Section 5.8.1.\nWe found that our intermediate RetinaVLM-Base model already performs significantly better than both foundation VLMs, which lack the ophthalmological specialism to stage disease (see Figure 3b). The most performant foundation VLM, Med-Flamingo, achieved a F1 score of 0.11. This was markedly outperformed by our advanced RetinaVLM-Specialist model, scoring at 0.63. This approached, but did not match, the accuracy of the junior ophthalmologists, who achieved an F1 score of 0.78. We analyze this discrepancy in detail in Section 3. Both foundation VLMs and RetinaVLM-Base returned a substantial number of invalid reports that did not conclude with one of the six disease stages (see Figure 3c). Conversely, all generated reports by RetinaVLM-Specialist were valid. Similar to human experts, RetinaVLM-Specialist struggled the most when diagnosing wet inactive AMD. We attribute this to the high number of shared imaging biomarkers that indicate either intermediate and late-wet forms of AMD, which sometimes leads to misdiagnosis by both ophthalmologists and RetinaVLM-Specialist (see Figure 3d). This was despite the identification of emerging features related to inactive late wet AMD (a small amount of hyperreflective material, or scar tissue) by RetinaVLM. Four more examples of success and failure cases of RetinaVLM-Specialist are shown in Figure 11a. Moreover, full numerical results as well as the confusion matrix for Med-Flamingo are shown in Figure 12 and 13, respectively.\nEighty-four of the generated reports were scored by the two senior ophthalmologists for their correctness, completeness, and conciseness. They were shown 28 reports written by LLaVA-Med, 28 by RetinaVLM-Specialist, and 28 by the two annotating junior ophthalmologists in random order without knowledge of the author. For each report, the senior"}, {"title": "2.4 RetinaVLM-Specialist surpasses opticians and approaches junior ophthalmologists in AMD patient screening and referral", "content": "As the prevalence of AMD is expected to further increase in the upcoming decades [16], ocular screening programs are being introduced around the world. In the United Kingdom, some projects involve opticians and pharmacies that acquire and interpret OCT images. They may refer a patient to a specialist clinic, summarizing their findings and the estimated level of the patient's risk in a letter. In the United Kingdom, treatment guidelines for AMD mandate that patients with signs of neovascularization are referred for immediate treatment within two weeks. However, non-specialists exhibit a tendency to over-diagnose these cases. An internal audit at Southampton Eye Unit found that 74.2% of the referrals made to the clinic do not have any form of treatable AMD. The processing and assessment of these false positives affects the clinic's ability to care for the remaining patients with treatable forms of AMD.\nWe evaluated the ability of VLMs to assess the level of referral urgency from OCT image (see 5a). For each case, the VLMs were provided explicit referral guidelines, and asked to recommend which of three levels of referral urgency was most appropriate for the patient: no referral for healthy patients, to be seen within 18 weeks (routine referral) for patients that are at risk of progressing to active late wet AMD but do not require treatment yet, and referral within two weeks for patients with any signs of neovascularization that should be urgently referred for antiangiogenic treatment. Two junior ophthalmologists independently reviewed images of 95 patients that have previously been referred to the hospital for treatment of wet AMD. For each patient, they independently decided the most appropriate of the three levels of referral urgency, and disagreements were arbitrated by the two senior ophthalmologists. In line with previous audits, they found the false discovery rate for urgent referrals was 69.5%. We then calculated F1 scores for the highest risk patients in need of urgent referral between the VLM's predictions and the ground truth. The full referral protocol and report generation instructions given to the VLMs are provided in Section 5.8.3.\nWe found that both medical foundation VLMs and Retina-Base perform worse than opticians regarding their ability to refer patients in need of urgent treatment (see Figure 5b). While Med-Flamingo failed to refer any of the 29 high-risk patients cases, LLaVA-Med and RetinaVLM-Base were ineffective for differentiating high-risk patients from low- to moderate-risk patients (see Figure 5c). RetinaVLM-Specialist was able to detect 23 out of the 29 high-risk cases that require immediate treatment. At the same time, RetinaVLM's false discovery rate, defined as the ratio of the number of false positives over the number of predicted positives, of 42.5% is substantially lower than that of opticians at 69.5%. Owing to their ability to better differentiate moderate from high-risk cases, the human ophthalmologists had the lowest false discovery rate of 9.1%, although they simultaneously missed three more cases in urgent need for treatment.\nIn practice, referral letters should communicate the reason for referral by citing suspected abnormalities in the OCT image that can inform the ophthalmologist's initial diagnostic plan. As in the conciseness study in Figure 4, RetinaVLM-Specialist sometimes documents the presence of small biomarkers that cannot be found in the image. More often, RetinaVLM-Specialist wrote an accurate imaging report but did not accurately follow the complex set of referral guidelines provided in the instruction. This led RetinaVLM-Specialist to incorrectly recommend that 17 of the moderate-risk patients potentially require treatment. However, this occurred less for the 25 low-risk patients, where RetinaVLM-Specialist correctly identified patients with little or no abnormalities, which are often referred to the treatment clinic for a second opinion by non-specialists (samples 1 and 3 in Figure 5d). Crucially, we find"}, {"title": "2.5 RetinaVLM accurately detects imaging biomarkers to make recommendations", "content": "It is important that clinical decision makers can provide evidence for their recommendations. Disease staging reports and written referral recommendations commonly contain descriptions of the most salient biomarkers that were detected in the scan. We tested the ability of four VLMs to correctly identify the presence or absence of 10 different biomarkers related to AMD. To this end, all VLMs were tasked with writing reports for 396 OCT images that conclude by stating the presence or absence of the biomarker in question (see Figure 6a). The VLMs predictions were compared against the ground truth labels obtained from junior ophthalmologists. The instruction used to generate these biomarker focused reports is provided in Section 5.8.4.\nWe find that RetinaVLM-Specialist outperforms both LLaVA-Med and Med-Flamingo in the detection of seven out of the ten of main biomarkers related to AMD (see Figure 6b). Biomarkers that were more severe, larger, and more numerous were detected with higher accuracy by RetinaVLM-Specialist than less advanced presentations (see Figure 6c). Most of the smaller biomarkers, such as small amounts of intraretinal fluid, drusen and hyperreflective foci, which can be as small as 30 \u00b5m in size [21], were detected with lower sensitivity. Overall, clinically important hallmarks of late AMD were detected with a very high sensitivity. Large volumes of subretinal and intraretinal fluid were detected in 80% and 78% of cases, respectively, and severe levels of hypertransmission in 84% of cases.\nFinally, to visualize the functioning of RetinaVLM-Specialist, we calculated saliency maps based on Grad-CAM [22]. These saliency maps highlight the image regions deemed most important by the model when writing specific passages of the report (see Figure 6d). We refer to Figure 14 for four additional image reports with corresponding saliency maps. We qualitatively found that RetinaVLM-Specialist is influenced by different imaging biomarkers when writing different passages of the report, and in making its final recommendation. We observed the saliency maps were especially effective for highlighting hyperreflective material, RPE irregularities and hypertransmission."}, {"title": "3 Discussion", "content": ""}, {"title": "Main findings of the study", "content": "In this study, we have presented RetinaVLM, the first specialist generative visual language model in medicine. Given a retinal OCT image, RetinaVLM provides accurate, detailed textual responses related to disease staging, referral or biomarker identification of AMD. While large foundation deep learning models have been employed for retinal image analysis before [23,18], our generative VLM is the first model that can flexibly process varied textual queries related to complex ophthalmological decisions and return detailed written responses. Through the use of language as primary communication medium, artificial intelligence systems are able to dynamically perform new tasks and meet the evolving requirements of image-based clinical decision makers.\nIn extensive experiments, RetinaVLM significantly outperformed state-of-the-art generative VLMs designed for medical use. We found that these are unable to interpret OCT images, derive the AMD disease stage and follow standard referral guidelines for AMD. Specifically, we have shown that in disease staging, RetinaVLM surpasses LLaVA-Med, the most performant open-source medical VLM, and is approaching the accuracy of junior ophthalmologists. When testing the ability of VLMs to screen for high-risk patients, LLaVA-Med substantially underperforms compared to junior ophthalmologists and even non-specialist opticians. In comparison, RetinaVLM-Specialist's reports reduced the number of incorrect urgent referrals by almost four times compared to opticians and had higher recall for urgent referrals than junior ophthalmologists. Finally, RetinaVLM is able to reinforce its decisions by citing observable biomarkers within the written report, and highlighting their corresponding regions within the image.\nWe postulate that the poor performance of existing medical foundation models stems from their lack of detailed knowledge related retinal OCT and AMD. Current VLMs are trained on broad, unstructured datasets that are extracted from medical textbooks, scientific publications or social media posts of healthcare professional [9]. In the United Kingdom and the United States, clinical trainees aspiring to become specialists must undergo up to ten years of post-graduate training to obtain the grade of a board-certified consultant. We argue that training data of current foundation medical VLMs lacks this specialist knowledge and experience, hindering their effective application to real-world clinical tasks.\nA core innovation of our work was the creation of a dedicated training curriculum that specializes VLMs in image-based clinical decision making. Analogously to current medical education, this curriculum deconstructs clinical problems into sets of mandatory capabilities required for their resolution and selectively trains VLMs in these skills. To this end, we obtained a large number of tabular reports by processing of retrospectively collected clinical data using advanced algorithms. Additionally, we tasked ophthalmologists to produce a limited number of highly specific textual reports. In total, our curriculum comprises 41,926 OCT images with 479,710 corresponding visual questions and answers. While still modest in size compared to substantially larger foundation datasets, we believe such curated needs-driven approaches are required to deploy language models specialist healthcare. In a similar vein, leading technology companies"}, {"title": "Limitations and future research directions", "content": "Naturally, the quality of our curriculum depends on the underlying reports, in particular that of the 330 collected detailed textual reports. The majority of the reports used to create RetinaVLM-Specialist were written by a junior ophthalmologist with three years experience. The remaining reports were written by an ophthalmologist with ten years of experience, and their reports were more comprehensive. To match or surpass the performance of the average junior ophthalmologist, we seek to finetune this model further on a small number of reports written by intermediate and senior ophthalmologists.\nAdditionally, the data may reflect local clinical definitions and workflows. While RetinaVLM-Specialist was trained on reports written by the two UK-based ophthalmologists, it was tested on labels derived from four separate ophthalmologists from different countries and hospitals. In our testing we identified differences in the staging definitions used by the UK-based and Austria-based ophthalmologists, especially with regard to identifying fibrovascular features that differentiate intermediate from inactive late wet AMD. This discrepancy resulted in a number of the misclassifications by RetinaVLM.\nSimilarly, we observed a discrepancy in image interpretation between junior and senior ophthalmologists. Junior ophthalmologists did not recommend patients for referral if it was likely that the retinal fluid observed was caused by traction rather than neovascularization, as it is not treatable with antiangiogenic drugs. Conversely, the senior ophthalmologist preferred that these patients be still referred for immediate assessment to rule out neovascularization. RetinaVLM was explicitly instructed to refer patients with any sign of fluid of any cause (see Section 5.8.3), and correctly referred more patients as a result.\nWe found that the LLM generating the question-answer pairs from the reports was sensitive to the specifics of the generation instruction. Extensive trial and error were required to arrive at several instructions, listed in Section A.2, that resulted in diverse sets of high quality question-answer pairs. We discern that all aspects of dataset creation - deciding on the required capabilities, collecting specialized annotations and converting these to question-answer pairs - should be formalized to systematically compare different approaches and ultimately scale dataset curation in the future.\nBeyond the formalization and extension of the creation of the curriculum, there are other potential technical improvements to RetinaVLM. Currently, RetinaVLM processes a single two-dimensional OCT image from one type of OCT scanner. In ophthalmological practice, decisions are made based on three-dimensional images from multiple time points, although many recent studies on the use of foundation models in ophthalmology also analyze two-dimensional images [18]. We mitigated the impact of this discrepancy on our study by tasking ophthalmologists to select the most relevant two-dimensional slice of the imaged volume before proceeding with the referral decision. In the future, a more sophisticated vision encoder, which is able to handle three-dimensional data from diverse OCT imaging devices, could be integrated with RetinaVLM. Similarly, one may opt to incorporate multimodal information, such as health questionnaires, clinical tests or the patient's medical history, into the decision making process [26]. The fundamental model architecture and training would remain similar, but the level of reasoning required for differential diagnosis across multiple scans would potentially increase.\nRetinaVLM also inherits some of the fundamental limitations of language models. LLMs are prone to confidently present false or fabricated information, termed hallucinations, which has been identified as problematic in medical contexts [13,14]. RetinaVLM occasionally hallucinates observations of retinal fluid and consequently diagnoses more advanced AMD stages than necessary. RetinaVLM's output was also sensitive to the wording of questions and instructions. While this had little impact on our qualitative analysis, extensive trial and error was necessary to ensure that RetinaVLM responded with one of the provided options in the quantitative analyses.\nIn this study, we exclusively trained RetinaVLM for the management of AMD from OCT images, ignoring other retinal pathologies, such as diabetic retinopathy or glaucoma, or imaging modalities, such as color fundus photography [27,28]. While this enabled us to explore the potential to encode advanced clinical levels of specialism into VLMs at depth, it would limit the applicability of the current version of our models for ocular screening. We hypothesize that VLMs could be specialized on a wider range of ophthalmological tasks by incorporating additional VQA datasets and training VLMs on them. This requires costly and time-consuming curation of specialized training datasets by medical experts. However, we believe that the involvement of medical experts is necessary as routine clinical skills and patient management protocols are rarely documented in existing datasets used to train AI models."}, {"title": "Conclusion", "content": "Foundation vision-language models (VLMs) have the potential to revolutionize healthcare by automatically interpreting medical images and communicating their findings in detailed written reports. Trained on large datasets containing millions of medical images and textual annotations, foundation language models have stirred considerable interest for their expert-level performance on medical licensing exams and case studies. However, in this work we have shown that foundation medical VLMs substantially underperform human experts on routine clinical tasks.\nWe hypothesize that the training data of foundation medical VLMs currently lacks specialist clinical knowledge and experience. To address this, we developed a curriculum-based approach that integrates the expertise of domain specialists into the training of medical VLMs. The resulting model, RetinaVLM, can produce detailed imaging reports that make accurate recommendations for the clinical management of AMD. It approaches and often matches the performance of junior ophthalmologists in disease staging, and outperforms non-specialist opticians in patient referral.\nThese results indicate that merely increasing the scale of training datasets is insufficient for the development of VLMs with real-world clinical utility. Instead, medical VLMs require high-quality data directly related to the challenges faced by clinicians in their daily practice. We believe our proposed curriculum-based approach provides a blueprint for specializing VLMs that generate true value in healthcare."}, {"title": "5 Methods", "content": ""}, {"title": "5.1 Retinal image dataset curation", "content": "We use two retinal OCT dataset in this study. The first, described in Section 5.1.1, contains a cohort of patients with AMD collected retrospectively at the Southampton Eye Unit. The second dataset, described in Section 5.1.2, contains scans of the initial visits of patients referred, primarily by opticians, to the Southampton Eye Unit.\nAll data was collected in the scope of the PINNACLE study (ClinicalTrials.gov NCT04269304), which received approval from the East Midlands-Leicester Central Research Ethics Committee in the United Kingdom (ref. 19/EM/0163) and the institutional review boards of all participating institutions. It complies with the principles of Good Clinical Practice and the Declaration of Helsinki. All images were captured using Topcon 3D OCT scanners (Topcon Corporation, Tokyo, Japan). Both datasets contain images of size 416 \u00d7 512 with a pixel size of 3.5\u00d711.7 \u00b5m\u00b2."}, {"title": "5.1.1 Retrospective cohort for training RetinaVLM and testing disease staging, report writing and biomarker analysis", "content": "The retrospective dataset contains 45,379 OCT images from 6,152 eyes belonging to 3,468 patients, collected over eight years, between 2012 and 2020, at the Southampton Eye Unit and aggregated by the PINNACLE consortium. For each OCT scan we use the mediolateral 2D slice centered at the fovea.\nWe designated 41,926 of the 45,379 OCT images from 5,547 eyes of 3,057 patients patients for training purposes. Additionally, we reserved 2,311 images from 326 eyes of 187 patients for validation, and 396 images from 279 eyes of 224 patients for testing. We ensured that images from each patient do not appear in more than one of the training, validation or test sets.\nThe training set was used to create both curriculum parts 1 and 2, detailed in Sections 5.4.1 and 5.4.2. The test set was used to evaluate the resulting model in Sections 2.3 and 2.5. For each patient in the test set, two junior ophthalmologists independently decided the disease stage, and disagreements were arbitrated by the two senior ophthalmologists. Inactive late wet AMD was defined by the presence of any subretinal hyperreflective material or fibrosis. Active late wet AMD was defined by presence of any fluid within the image and took precedence over the inactive classification."}, {"title": "5.1.2 External cohort of patients referred to Southampton AMD treatment clinic", "content": "We also collected an external dataset of 95 patients that were referred primarily by opticians to the Southampton Eye Unit between 02/2023 and 12/2023. None had yet received treatment for AMD, and mostly had no AMD, intermediate AMD or small features related to active wet AMD. This represents a distribution shift from the retrospective cohort, where many patients had already received treatment for AMD and were in the inactive late wet stage of AMD. As such, it enabled us to estimate the robustness of both variants of RetinaVLM to shifts in patient population. This dataset was not used for model training and was reserved for testing VLMs on patient referral, detailed in Section 2.4.\nFor each patient we sourced scans of both their left and right eye that were acquired on their first visit to the clinic. We also collected the originally issued letter of referral, as depicted in Figure 5d. Then, two junior ophthalmologists analyzed the 3D OCT volumes of each eye to assess the patient's risk and recommend a level of referral urgency. They then selected the image slice that most supported their assessment of the patient's risk. In healthy patients where both volumes contained no pathological signs in any of the image slices, they were instructed to select the mediolateral fovea-centered 2D slice from one of the two volumes."}, {"title": "5.2 Vision-language model architecture", "content": "RetinaVLM consists of two main components: an ophthalmological vision encoder and a generative LLM (see Figure 15). For the ophthalmological vision encoder, we adopt a Resnet50 convolutional neural network with over 23 million parameters which was previously pre-trained with self-supervised contrastive learning on the 41,926 OCT images from the train set of the retrospective cohort. Specifically, it was trained with Bootstrap Your Own Latent (BYOL) [29] using the same implementation details as the standard contrastive approaches used in [17], which consistently performed on par with RETFound [18] specifically on data from the Southampton Eye Unit. This vision encoder projects each 192 \u00d7 192 input image to a set of spatially arranged 6\u00d76 visual embeddings, which are extracted from the last layer before global average pooling. Each embedding has a dimension of h_{img} = 2048. They also have a receptive field of size 336, so each embedding contains global knowledge of the image that is contextualized at its local position. For the LLM, we employ the 8 billion parameter instruction-tuned Llama3 model by Meta [19,30] as the generative LLM, which was was the most performant openly available model at the time of our study. LLama3 uses an embedding dimension of h_{lang} = 4096."}, {"title": "5.3 Foundation medical vision-language models", "content": "We used the two most widely adopted foundation vision-language models for medical applications at the time of this study [10,9]. They were both trained on large biomedical datasets sourced from the Internet, and have been applied in chest x-ray [32]. The first, Med-Flamingo [10], which was built on Flamingo [33] and finetuned on image and text data from medical textbooks and the PubMed Central Open Access (PMC-OA) dataset [34]. The second, LLaVA-Med [9], developed by Microsoft, is a VLM built on LLaVA [35] and finetuned to follow textual instructions regarding a broad range of biomedical images contained in PubMed Central 15M (PMC-15M) [36]. As they were trained as generalist models on various imaging modalities, they were both purportedly capable of interpreting retinal OCT images. Both correctly identified that the provided image was a retinal OCT scan when instructed to report the modality of the given image.\nFor Med-Flamingo, we then provide instructions using the following template provided in their code, replacing {question} with the instruction text:\nYou are a helpful medical assistant. You are being provided with images, a question about the image and an answer. Follow the examples and answer the last question. <image>Question: {question} Answer:\nSimilarly, for LLaVA-Med we use their following system prompt:\nYou are a helpful medical assistant. You are being provided with images, a question about the image and an answer. Follow the examples and answer the last question. <image>Question: {question} Answer:"}, {"title": "5.4 Report curation and question-answer pair generation", "content": ""}, {"title": "5.4.1 Curriculum part 1: Introduction to retina", "content": "To create the tabular reports for the first part of the curriculum we used a cluster-based approach to efficiently label the 41,926 training images with biomarker annotations [37", "large drusen": "r 'subretinal fluid', these labels were assigned to the entire cluster. These labels were used in in combination with the patient's age, sex and their functional visual acuity score (measured on a LogMAR chart and converted to Letter score) to create the tabular reports. Additionally, the reports list three biomarkers that are stated as not being present. These are drawn from a distribution of all biomarkers, weighted by their prevalence in the dataset, that were not among the cluster labels for that image. Counts of the prevalence of each tabular variable among the images are shown in Figure 7a, and a sample of four tabular reports they result in are shown in Figure 7b.\nTo generate question-answer pairs from the large volume of tabular reports we used WizardLLM-70B, which was the most capable freely-available LLM at"}]}