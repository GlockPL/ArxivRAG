{"title": "GETAE: Graph Information Enhanced Deep Neural Network Ensemble Architecture for Fake News Detection", "authors": ["Ciprian-Octavian Truic\u0103", "Elena-Simona Apostol", "Marius Marogela", "Adrian Paschke"], "abstract": "In today's digital age, fake news has become a major problem that has serious consequences, ranging from social unrest to political upheaval. To address this issue, new methods for detecting and mitigating fake news are required. In this work, we propose to incorporate contextual and network-aware features into the detection process. This involves analyzing not only the content of a news article but also the context in which it was shared and the network of users who shared it, i.e., the information diffusion. Thus, we propose GETAE, Graph Information Enhanced Deep Neural Network Ensemble Architecture for Fake News Detection, a novel ensemble architecture that uses textual content together with the social interactions to improve fake news detection. GETAE contains two Branches: the Text Branch and the Propagation Branch. The Text Branch uses Word and Transformer Embeddings and a Deep Neural Network based on feed-forward and bidirectional Recurrent Neural Networks ([B1]RNN) for learning novel contextual features and creating a novel Text Content Embedding. The Propagation Branch considers the information propagation within the graph network and proposes a Deep Learning architecture that employs Node Embeddings to create novel Propagation Embedding. GETAE Ensemble combines the two novel embeddings, i.e., Text Content Embedding and Propagation Embedding, to create a novel Propagation-Enhanced Content Embedding which is afterward used for classification. The experimental results obtained on two real-world publicly available datasets, i.e., Twitter15 and Twitter16, prove that using this approach improves fake news detection and outperforms state-of-the-art models.", "sections": [{"title": "1. Introduction", "content": "As mass media becomes more digitalized, new journalistic models for information dissemination have emerged. These models have significantly transformed how society consumes news and information. In the race to stay ahead of competitors, journalists sometimes sacrifice the standards of traditional journalism, prioritizing speed and aiming to \"go viral\" by quickly generating views aiming for an increase in likes, comments, and shares. This new approach focuses on catering to users' needs, behaviors, and interests. While digital media offers many benefits, it also heightens the risk of misinformation [1, 2], which can have harmful consequences for society by enabling the rapid spread of false information, such as fake news related to the Brexit referendum [3], the 2016 US presidential election [4], and COVID-19 vaccinations [5]."}, {"title": "2. Related Work", "content": "Textual data representation is a challenging task when it comes to generating linguistic features. Shu et al. [9] discuss the use of linguistic and semantic features such as n-grams, total word count, and punctuation that can provide meaningful representations for fake news detection. Ahmad et al. [10] applied ensemble learning methods like Random Forests, Boosting, Bagging, and Voting Classifiers on the LIWC (Linguistic Inquiry and Word Count) dataset [11], achieving higher accuracy than traditional models across four datasets. N-gram models for feature extraction also yielded strong results [12, 13]. These approaches use classical machine learning algorithms with TF-IDF (Term Frequency-Inverse Document Frequency) for n-gram extraction to train models.\nThe current literature on detecting harmful content primarily focuses on techniques such as word embeddings [14], character-level embeddings [12], transformer embeddings [15, 16], sentence transformers [17], and document embeddings [18]. Various deep learning architectures are used as classifiers for harmful content detection, with most state-of-the-art models relying on LSTMS (Long Short-Term Memory networks), GRUS (Gated Recurrent Units), and/or CNNs (Convolutional Neural Networks) as foundational components, as seen in studies such as [14, 16].\nOther effective linguistic approaches for extracting features for fake news detection include using contextual inputs such as the headline, first two sentences of a news article, and news content to generate representations [19]. These features used for training models that employ BiLSTM (Bidirectional Long Short-Term Memory) and BiGRU (Bidirectional Gated Recurrent Unit), manage to capture long-term dependencies in sequences. Other approaches use Hierarchical Recursive Neural Networks and transformer-based architecture to construct a linguistic tree of news content using an encoder-decoder model [20]. This linguistic tree is then processed by a series of BiGRU layers within the network.\nGraph Neural Networks (GNN) use node-oriented data to capture and encode information from networks, unlike classical deep learning models that operate on sequential data. Understanding local and global features in graphs can help solve a great number of problems in areas like biological or social networks. Propagation-oriented methods use the networks to understand how features change when information is spread into a network.\nLu et al. [21] propose the graph-aware co-attention network, GCAN, a model that takes the user features in a propagation system and uses a GCN (Graph Convolutional Network) to learn representations for rumor detection. Using a retweet order sorted based on the retweet time, they create GCN, GRU, and CNN embeddings for the tweet and social encodings, which are then passed through a dual co-attention mechanism. This approach uses features from patterns in response time, source tweet content, and user characteristics on Twitter15 and Twitter16 datasets.\nHuang et al. [22] use a deep structure learning method for rumor detection on the same Twitter-based datasets. The authors create encoders for user data and propagation data with layers of GCN and RNN (Recursive Neural Networks), which are fed into an integrator module that uses both encoders to create representations of both user and propagation-tree data. On rumor detection using semantic and propagation information, Ke et al. [23] propose KZWANG, a model that uses Multihead Attention mechanisms to generate a better representation for microblogs and GCN layers to fuse microblog features with propagation encoding. This approach was tested on three datasets, surpassing other state-of-the-art models in accuracy and F1-score for multi-class classification.\nTopological information of a graph can be stored in a vector space, in the same way word embeddings are represented. High-dimensional vector representations for nodes, edges, and whole graphs are noted as node, edge, or graph embeddings. Goyal et al. [24] and Cai et al. [25] use graph embeddings that encode node, edge, and whole graph information for solving different problems. Xie et al. [26] use the GraphSage model for node embeddings, the Multihead Attention mechanism for creating user-based representations, and CNNs for content representations. These layers are then concatenated for a binary classifier which gets good results on BuzzFeed and PolitiFact datasets, compared to other embedding techniques. Imaduwage et al. [27] propose a method for combining local and global graph-level embeddings. Each propagation tree contributes only partially with half priority with its local GraphSage embeddings to a global embedding mechanism. These are fed to an autoencoder-based network that generates whole propagation-tree embeddings used then in a classification model.\nWhen dealing with information diffusion on networks, the current literature proposes multiple solutions that analyze the spread of online content [28, 29, 30] for both detection and network immunization tasks, such as preemptive [31], community-based [32], or tree-based [33] strategies. Furthermore, the current literature also presents full solutions for real-time Fake News Detection. STOPHC [34] is a full solution that builds on top of the current literature and proposes a novel deep-learning architecture for harmful content detection and mitigation. CONTCOMMRTD [35] is an architecture that detects misinformation on social media posts during real-time disaster reporting. Petrescu et al. [36] proposes a mixture of models to deal with harmful content detection."}, {"title": "3. Methodology", "content": "3.1. Feature Representation\n3.1.1. Text Preprocessing and Word Embeddings\nTo preserve semantic relations while removing any elements that do not contribute to extracting the context (e.g., hyperlinks, utf8 characters, etc.), we process the textual data using a preprocessing pipeline. We apply this preprocessing pipeline to reduce the vocabulary size [37] before training while increasing the model's performance during inference [38]. By standardizing the textual data, we allow the model to focus on the core content and generalize better when unseen data is used for inference [39]. The preprocessing pipeline involves applying the following steps on the textual data [37, 40, 41]: (1) remove URL; (2) remove punctuation; (3) remove stopwords (only for the Word2Vec model); and (4) assign each token a unique integer identifier (id) starting with id 1.\nWe reconstruct the textual data using the tokens' unique identifiers and obtain a 2-dimensional document-to-token matrix \\(D \\in \\mathbb{N}^{n \\times k}\\), where n is the number of documents in the corpus and k is the maximum size of a document in the corpus. We use left zero padding for documents that are smaller than the maximum size k, i.e., we add 0 on the left side of the document to achieve the maximum length k.\nUsing the tokenized text, we create 3 word embeddings:\n(1) WORD2VEC SKIP-GRAM [42], a model that takes as input the word w and tries to detect the context (the words on the left and right sides of w);\n(2) BERT [43] is a deep bidirectional transformer architecture that employs transfer learning to generalize language understanding;"}, {"title": "3.1.2. Graph Representation and Node Embeddings", "content": "To construct a graph from a social network dataset, we use the users as nodes and the posting and user interaction with a post (i.e., re-sharing, linking, commenting, etc.) as edges. Using this graph, we extract the information diffusion propagation graph \\(G = (V, E)\\) of each node \\(u \\in V\\) to construct node embedding. The diffusion is given by the edges \\((u, v) \\in E\\) that connect 2 nodes \\(u, v \\in V\\). For building node embeddings, we use NODE2VEC [45] and DEEPWALK [46]. Both methods aim to capture the structural properties of the network and represent them in a low-dimensional space. NODE2VEC is an extension of DEEPWALK that uses a biased random walk to sample nodes in the network, which allows for the capture of both local and global network structures.\nSpecifically, Node2Vec uses a bias factor \\(a_{p,q}\\), and two parameters p (return parameter) and q (in-out parameter). Equation (1) presents the calculation of the bias coefficient \\(a_{p,q}\\) when traversing a graph with a second-order random walk for the current node \\(u \\in V\\) by traversing edge \\((u, v) \\in E\\), where \\(d_{u,v} \\in \\{0, 1, 2\\}\\) is the length of path of the edge \\((u, v) \\in E\\).\n\\[a_{p,q} = \\begin{cases} 1/p, & \\text{if } d_{u,v} = 0 \\\\ 1, & \\text{if } d_{u,v} = 1 \\\\ 1/q, & \\text{if } d_{u,v} = 2 \\end{cases}\\]\nThus, the control of parameters (p,q) influences the sampling decision of nodes that are more likely to produce similar embeddings. The NODE2VEC model uses a Skip-Gram with Negative Sampling (SGNS) approach to generate embeddings that are trained to maximize the likelihood L of predicting the neighborhood \\(p(N_s(u) | f(u)\\) of a node given its embedding (Equation (2)), where \\(N_s(u)\\) is the set of nodes sampled with strategy s in the neighborhood of \\(u \\in V\\) and \\(f(u)\\) is the mapping function from nodes to representations.\n\\[L = \\sum_{u\\in V} \\log p(N(u) | f(u)),\\]\nDEEPWALK uses a random walk to generate sequences of nodes and then applies the Skip-Gram model to learn node embeddings that capture the co-occurrence statistics of the"}, {"title": "3.2. GETAE Architecture", "content": "3.2.1. Text Branch\nGETAE's Text Branch contains 3 layers: 1) the input layer (i.e., Textual Content), 2) the Word Embedding layer, and 3) a hidden layer (i.e., [B1]RNN). The Textual Content layer requires 2 inputs: 1) the document-to-token matrix \\(D \\in \\mathbb{N}^{n \\times k}\\) and 2) the word embedding matrix \\(W \\in \\mathbb{R}^{m \\times s}\\). The Word Embedding layer is used to pair document tokens from D to their corresponding word embeddings from W. The hidden [BI]RNN layer employs either Unidirectional Recurrent Neural Network (RNN) units or Bidirectional Recurrent Network (BIRNN) units. In GETAE's implementation, the RNN is replaced by either Recurrent Neural Networks (RNN) units, Long Short-Term Memory (LSTM) units, or Gated Recurrent Units (GRU) for ablation testing. We use the [B1]RNN notation when discussing this layer, regardless if we use the RNN or the BIRNN layer. Thus, when using an RNN hidden layer, the actual layer units employed by GETAE are either standard RNN, LSTM, or GRU. When using a BIRNN hidden layer, the layer units are either standard BIRNN, BILSTM, or BIGRU. During ablation testing, we interchange unidirectional with bidirectional RNNs. Thus, we aim to determine the importance of preserving the information from inputs when the information passes through the hidden state in a feed-forward manner (the unidirectional case) versus when the information is passed in both directions using a backward and forward approach (the bidirectional case). The output of the [BI]RNN hidden layer is a new word representation, i.e., Text Content Embedding. The novel Text Content Embeddings combines multiple complex textual features such as lexical (e.g., character and word level features) and syntactic (e.g., sentence level features) used to improve prediction.\nAs we employ multiple units for the [BI]RNN hidden layer, the Text Content Embeddings manage to create different types of embeddings that also consider the input Word Embedding. Regardless of the word embedding used as input, we create 6 subtypes of Text Embeddings using [B1]RNN:\n(1) Text Content Embedding using RNN: the word embedding passes through an RNN layer to obtain the final text embedding.\n(2) Text Content Embedding using BiRNN: the word embedding passes through a BIRNN layer to obtain the final text embedding.\n(3) Text Content Embedding using GRU: the word embedding passes through a GRU layer to obtain the final text embedding.\n(4) Text Content Embedding using BiGRU: the word embedding passes through a BIGRU layer to obtain the final text embedding.\n(5) Text Content Embedding using LSTM: the word embedding passes through an LSTM layer to obtain the final text embedding.\n(6) Text Content Embedding using BiLSTM: the word embedding passes through a BILSTM layer to obtain the final text embedding."}, {"title": "3.2.2. Propagation Branch", "content": "GETAE's Propagation Branch contains 3 layers: 1) the input layer (i.e., Graph Information), 2) Node Embedding layer, and 3) a hidden layer (i.e., Dense). The Graph Information layer requires 2 inputs: 1) list of nodes V and 2) the node embedding matrix \\(N \\in \\mathbb{R}^{|V| \\times s}\\). The Node Embedding layer is used to pair nodes from V to their corresponding node embeddings from N. A hidden layer uses the pairs to obtain a novel Propagation Embedding. This hidden layer contains Dense units that employ the ReLU activation function to create a new Propagation Embedding. The novel Propagation Embedding is designed to encode the information diffusion in order to encode the spread of information from a node to its followers."}, {"title": "3.2.3. Ensemble", "content": "GETAE's Ensemble concatenates the output of the Text and Propagation Branches into one tensor, which is passed to a Dense layer used for classification. During the concatenation, we create a new Propagation-Enhanced Content Embedding that takes into account both the textual content and the information propagation within the social media graph, i.e., it combines the Text Content Embedding with the Propagation Embeddings. This novel Propagation-Enhanced Content Embedding is then used by a Dense layer that extracts hidden contextual features and creates a new vector representation by employing the ReLU activation function. The new vector representation is passed through the final Dense layer for classification which uses the softmax activation function to determine the veracity of a post."}, {"title": "3.2.4. Algorithm", "content": "Algortihm 1 presents the pseudo-code for GETAE. As input, GETAE takes the dataset \\(X = \\{< t_i, v_i, C_i > | i = 1, n\\}\\), the graph \\(G = (V, E)\\), the configuration for the Text Branch given as a pair < bidirection, reccurent >, the word embedding model for the Text Branch \\(we\\_model\\), and the node embedding model for the Propagation Branch \\(ne\\_model\\). The dataset X contains for each observation \\(i = 1,n\\) (n = |X|) a tuple < \\(t_i, v_i, C_i\\) >, where \\(t_i\\) is the textual content, \\(v_i\\) is the author of the content, i.e., node, and \\(c_i\\) is the class. The graph G contains the list of nodes V and the list of edges E used to create the propagation graph of each node. The configuration for the Text Branch determines the types of units used by the [BI]RNN and if it is unidirectional or bidirectional as follows: 1) bidirection is a boolean that indicates the use (True) or not (False) of a Bidirectional RNN, and 2) reccurent is a Layer object that indicates what units are used, i.e., standard RNN, LSTM, or GRU. The \\(we\\_model\\) determines which word embedding model is used, i.e., WORD2VEC, BERT, or BERTWEET. Finally, \\(ne\\_model\\) determines which node embedding model is used, i.e., NODE2VEC or DEEPWALK.\nIn Lines 1-7, we initialize and populate the T, V, and C lists where we store the text \\(t_i\\), the nodes \\(v_i\\), and the class \\(c_i\\) for each entry in the dataset X. After iterating through each record in X, the lists will look as follows: T = \\(\\{t_i|i = 1,n\\}\\), V = \\(\\{v_i|i = 1,n\\}\\), and C = \\(\\{c_i|i = 1, n\\}\\). The list T is used to obtain the document-to-token matrix D (Line 8) and the word embedding matrix W (Line 9). The function getWordEmbeding(T, we_model) uses the we_model parameter to determine which word embedding model to use. The list V is used to obtain the node embedding matrix N (Line 10). The function getNodeEmbeding(G, V, ne model) uses the Graph and the ne_model parameter to determine which node embedding model to use. We note that the architecture is not limited to using only the presented word and node embeddings, as this is a modular architecture, any word and node embedding can be used.\nThe Text Branch is configured using the configuration pair < bidirection, reccurent > (Lines 11-27), as explained in Subsection 3.2.1. The input layer (Line 11) receives the document-to-token matrix D and the word embedding matrix W. This layer together with the Word Embedding layer is added to the Text Branch (Lines 12-13). The [BI]RNN layer is constructed in Lines 14 to 27 Depending on parameter bidirection, the architecture uses either a feed-forward or a bidirectional RNN layer. The type of units used by GETAE's RNN layer is selected based on the reccurent parameter from the following types: standard RNN, LSTM, or GRU.\nThe Propagation Branch is created similarly to the Text Branch. Lines 28-31 present the layers of the Propagation Branch, as explained in Subsection 3.2.2. The input layer (Line 29) receives the nodes list V and the node embedding matrix N. This layer together with the Node Embedding layer is added to the Propagation Branch (Lines 29-30). A Dense layer with the ReLU activation function is added to the Propagation Branch (Line 31) to obtain the Propagation Embedding."}, {"title": "4. Experimental Results", "content": "4.1. Datasets\nWe use Twitter15 and Twitter16 datasets [47] for our experimental validation. Both datasets contain Twitter data separated into: 1) the source tweets with the tweet ID and its"}, {"title": "4.1.1. Textual Data Analysis", "content": "Both datasets are labeled using 4 classes: false, true, non-rumor, and unverified. Because we are building binary classification models, we only used the source tweets labeled as true and false for training. Figure 2 presents the class distribution for these two labels. We observe that both datasets are fairly balanced."}, {"title": "4.1.2. Network Data Analysis", "content": "For both Twitter15 and Twitter16 datasets, each propagation tree is encoded using a list of edges from parent nodes to child nodes. The relationship between a child and a parent node represents retweets and mentions of the source tweet, only for the neighbor of the root node. Equation (4) shows the edge information from the tree file:\n\\((parentid, tweetid, t_{tweet}) \\rightarrow (childid, replyid, t_{reply})\\)"}, {"title": "4.2. Experimental Setup and Implementation", "content": "We conducted experiments on Twitter15 and Twitter16 datasets creating word embeddings, node embeddings, and deep learning models that combine this data. The text preprocessing for training word embeddings consists of removing 1) URLs, 2) double spaces, and 3) punctuations. To vectorize the text, we train a Wor2Vec Skip-Gram model. For BERT and BERTweet, we use the pre-trained models from HuggingFace: 'ber-base-uncased' and 'vinai/bertweet- base'. For Transformer embeddings, we applied the tokenizer with parameters presented in Table 2. We pass the input_ids and attention_mask tensors to the BERT models to obtain the transformer embeddings. For BERTweet, the only difference was loading a different tokenizer and model from huggingface. Table 1 specifies the hyperparameters used for obtaining node embeddings with NODE2VEC and DEEPWALK. Table 3 presents GETAE's deep learning ensemble architecture hyperparameters.\nWe implemented the preprocessing elements of GETAE in Python version 3.10. For text preprocessing, we use the nltk [48] package. For training the WORD2VEC word embeddings, we use the gensim [49] package. For training the node embeddings, we use the node2vec and karateclub [50] packages for training the embeddings with NODE2VEC and DEEPWALK, respectively. We implemented the GETAE architecture and trained the models using TensorFlow and Keras. The code is freely available on GitHub at https://github.com/DS4AI-UPB/GETAE."}, {"title": "4.3. Classification and Ablation Results", "content": "For this set of experiments, we use an 80-20 train-test ratio. We trained the multiple models using the GETAE architecture for 30 epochs, except for the models that employ the WORD2VEC word embeddings which were trained for only 8 epochs to avoid overfitting. For training the different GETAE models, we used a learning rate of 0.001. The training process for the models was carried out on Google Colab, with no hardware acceleration.\nAll experiments use k-fold cross-validation with k = 10 with the ratio 80-20 for the train and test sets. The tables present the mean and standard deviation for each metric, using the individual results obtained at each fold during the k-fold cross-validation.\nTables 4 and 5 present the results obtained on the test set after training the GETAE models on Twitter15 and Twitter16, respectively. We use three types of word embeddings (i.e., WORD2VEC Skip-Gram, BERT, and BERTWEET) and two types of node embeddings (i.e., NODE2VEC and DEEPWALK). These tables also present two types of ablation testing for the GETAE architecture: 1) adding or removing the Propagation Branch, 2) changing the RNN Layer for the Text Branch. The Text Branch uses six different recurrent neural network layers for ablation testing: RNN, BIRNN, GRU, BIGRU, LSTM, and BILSTM.\nThe evaluation of the models obtained by training the GETAE architecture includes performance metrics such as Accuracy, Precision, Recall, and F1-Score, measured on both Twitter15 and Twitter16 datasets. The experiments are conducted with and without the Network Branch for ablation testing. The hyperparameters for NODE2VEC are: p = 1, q = 1, d = 100, where p is the return hyperparameter, q is in-out hyperparameter, and d is the embeddings' dimensions. For DEEPWALK node embeddings, the only hyperparameter tuned was the dimensionality of the embedding space, set to d = 100.\nFor the Twitter15 dataset, the best configuration for GETAE by metrics is a tie between configurations (BERT, NODE2VEC, BILSTM) and (BERTWEET, DEEPWALK, BIRNN). When comparing these two models with other state-of- the-art ones, we choose the (BERT, NODE2VEC, BILSTM) configuration due to a higher F1-Score. BERTWEET should also be mentioned as having promising results, especially when employing the DEEPWALK node embedding and BIRNN as the recurrent layer.\nFor the Twitter16, the best configuration for GETAE is (BERT, NODE2VEC, RNN) with 89.6% Accuracy, 90.1% Precision, 89.7% Recall, and 89.5% F1-Score. The second best configuration for GETAE is a tie between (BERT, NODE2VEC, BIRNN) and (BERT, NODE2VEC, BILSTM) with a difference in metrics smaller than 1%. We observe that for the Twitter16 dataset, the model that employs BERTWEET for the word embeddings only performs better when using node embeddings and a BIRNN layer."}, {"title": "4.4. Node Embeddings Hyperparameter tuning", "content": "For this set of tests, we use the same methodology when training the models using the GETAE as in Subsection 4.3.\n4.4.1. NODE2VEC\nTables 6 and 7 present the detection results when varying Node2Vec parameters p (return hyperparameter), q (in- out hyperparameter), and d (dimensions) for the GETAE architecture. The return parameter p controls the likelihood of immediately revisiting a node in the walk, while the in-out hyperparameter q allows the search to differentiate between inward and outward nodes [45]. For each of these configurations of parameters, we use the following configurations for GETAE: 1) (BERT, BILSTM, NODE2VEC) for the Twitter15 dataset, and 2) (BERT, RNN, NODE2VEC) for the Twitter16 dataset.\nFor the Twitter15 dataset (Table 6), we observe that the best results are obtained for hyperparameters' values p = 1, q = 1, and d = 100 with 82.7% Accuracy, 83.1% Precision, 82.7% Recall, and 82.5% F1-Score. With this GETAE configuration, the node embedding model uses a balanced strategy for random walks, considering both breadth-first and depth-first traversals. The second best model trained using the GETAE architecture obtained the best results when using p = 0.5, q = 0.5, and d = 100 as hyperparameter values. The difference between the best and second best performing models is around 3%. This hyperparameters' configuration is not leaning toward breadth-first or depth-first strategies. Thus, in the case of Twitter15, we notice that having a bias for the random walk does not matter that much, while d = 100 tells us that a fine-grained representation is needed for understanding more complex patterns.\nFor the Twitter16 dataset (Table 7), the best model trained using the GETAE architecture uses the following hyperparameters' values p = 1, q = 1, and d = 100. This model obtains 89.6% Accuracy, 90.1% Precision, 89.7% Recall, and 89.5% F1-Score. These hyperparameters offer a balanced random walk strategy. The second best model is trained using the following hyperparameters' values p = 2, q = 1, and d = 32. The differences between the best and second best-performing models are between 2-7% for each metric. The second best performing model is leaning towards a depth-first sampling strategy due to a higher likeliness of the returning hyperparameter p. Furthermore, the second-best results are obtained when the dimension is small, i.e., d = 32. Thus, we can conclude that a 32-dimensional representation is also not only enough, but better at capturing node information than many other strategies and dimensions."}, {"title": "4.4.2. DEEPWALK", "content": "Tables 8 and 9 present the results obtained by the models trained using the GETAE architecture when using DEEPWALK node embeddings [46]. For this set of experiments, we use BERT for embedding the textual data and we vary both the node embedding dimension d as well as the recurrent neural network layer of GETAE's Text Branch.\nFor the Twitter15 dataset (Table 8), the best results are obtained when using d = 100 for the node embedding and the BILSTM layer for the Text Branch. The model trained using this configuration obtains 81.4% Accuracy, 81.8% Precision, 81.5% Recall, and 81.2% F1-Score on the test set. The second best model obtains the best results when the hyperparameters configuration is d = 32 and a BILSTM The differences between the best and second best-performing models are between 1- 2% for each metric. We observe that the models trained using the GETAE architecture that employs d = 100 obtain better results than when using d = 32. We conclude that having a higher number of dimensions yields better results when training models with the GETAE architecture for this dataset. However, we observe that choosing to use a BILSTM layer for GETAE's Text Branch has a greater influence than an increase in dimensions of the node embedding for GETAE's Propagation Branch.\nFor the Twitter16 dataset (Table 9), we observe that smaller node embedding dimensions are better for training models with the GETAE architecture. The best model uses the BILSTM for GETAE's Text Branch and a 32-dimensional node embedding for GETAE's Propagation Branch. This model obtains 88.1% Accuracy, 88% Precision, 87.9% Recall, and 88% F1-Score on the test set. In comparison, the results obtained by this model are around 1% better than when using d = 100 for the node embedding. We can conclude that for this dataset, the difference between using a lower or higher dimension representation of the node embedding changes very little the models' overall performance."}, {"title": "4.5. Comparison with State-of-the-Art Models", "content": "To better determine the performance of GETAE, we compare the results obtained by our model with state-of-the-art models trained on the Twitter15 and Twitter16 datasets. In our comparison, we employ the following models also used by [21] in their comparison:\n\u2022 DTC [51] uses Decision Trees from user and tweet data.\n\u2022 SVM-TS [52] uses a Support Vector Machine with data from the source tweet and the retweets sequence.\n\u2022 mGUR [52] is a modified GRU neural network using the source tweet and the patterns in retweets.\n\u2022 RFC [53] is a Random Forest-based classifier that learns on the source tweet and user retweets.\n\u2022 tCNN [54] is a modified convolutional neural network that learns local patterns of user profile sequence and tweet data.\n\u2022 CRNN [55] is an RNN and CNN approach using the source tweet data.\n\u2022 CSI [56] uses the tweets and a scoring mechanism of the users that retweet them.\n\u2022 DEFEND [57] is a co-attention-based model that correlates the source tweet and the user profile.\n\u2022 DANES [8] is a model that uses a new node embedding from social and textual context data."}, {"title": "5. Discussion and Limitations", "content": "GETAE is a promising ensemble architecture for training fake news detection models that consider the information propagation of content. By considering how harmful nodes infect the network, the models trained outperform existing state-of-the-art models.\nOn Twitter15, in terms of Accuracy and F1-Score, the model trained using BILSTM for the [BI", "BI": "RNN layer, BERTWEET as the word embedding model, and DEEPWALK as the node embedding, we obtain a Precision score of 83.9% and an F1- Score of 82.7%. During hyperparameter tuning, when using the BILSTM for the [BI", "B1": "RNN layer. The best GETAE configuration that uses WORD2VEC as the word embedding mode employs BIRNN for the [BI"}, {"BI": "RNN layer.\nFor the Twitter16 dataset, the model trained using RNN for the [BI"}, {"BI": "RNN layer, BERTWEET as the word embedding model, and DEEPWALK as the node embedding, we obtain an Accuracy score of 82.9%, Precision score of 84.2%, Recall score of 83.1% and an F1-Score of 82.6%. During hyperparameter tuning, when using the RNN for the [BI"}]}