{"title": "A GEOMETRY-AWARE ALGORITHM TO LEARN HIERARCHICAL EMBEDDINGS IN HYPERBOLIC SPACE", "authors": ["Zhangyu Wang", "Lantian Xu", "Zhifeng Kong", "Weilong Wang", "Xuyu Peng", "Enyang Zheng"], "abstract": "Hyperbolic embeddings are a class of representation learning methods that offer competitive performances when data can be abstracted as a tree-like graph. However, in practice, learning hyperbolic embeddings of hierarchical data is difficult due to the different geometry between hyperbolic space and the Euclidean space. To address such difficulties, we first categorize three kinds of illness that harm the performance of the embeddings. Then, we develop a geometry-aware algorithm using a dilation operation and a transitive closure regularization to tackle these illnesses. We empirically validate these techniques and present a theoretical analysis of the mechanism behind the dilation operation. Experiments on synthetic and real-world datasets reveal superior performances of our algorithm.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning data representation is important in machine learning as it provides a metric space that reveals or preserves inherent data structure (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017; Hoff et al., 2002; Grover & Leskovec, 2016; Perozzi et al., 2014; Nickel et al., 2011; Bordes et al., 2013; Riedel et al., 2013). Hyperbolic embeddings, a class of hierarchy representation methods, have shown competitive performances when data can be abstracted as a graph (Chamberlain et al., 2017; Davidson et al., 2018; Ganea et al., 2018a; Gu et al., 2018; Tifrea et al., 2018).\nIn this work, we focus on the following embedding task. Let D be a dataset incorporated with a set of hierarchical relations represented as edges in a tree-like graph G. The goal is to learn an embedding in the hyperbolic space by drawing positive and negative samples of edges from the graph such that preserves the edge relationships, which are reflected by the order of similarity between data pairs. The formal problem statement is presented in Section 2.\nTheoretically, hyperbolic space, such as the Poincar\u00e9 Ball model, benefit from high representational power due to their negative curvatures (Nickel & Kiela, 2017; Sala et al., 2018). This observation has motivated research on solving real-world problems in hyperbolic space. For datasets with an observed structure, hyperbolic space can embed the data and preserve the structure with arbitrarily low distortion (Nickel & Kiela, 2017; Chamberlain et al., 2017; Nickel & Kiela, 2018; Ganea et al., 2018b; Chami et al., 2019c). For datasets with a latent structure, especially those obeying the power-law, hyperbolic space can provide a natural metric such that finer concepts are embedded into areas allowing more subtlety (Tifrea et al., 2018; Leimeister & Wilson, 2018; Le et al., 2019).\nDespite the theoretical advantages of hyperbolic embeddings, learning such representation in practice is difficult. Specifically, the following fundamental difficulties have not been well-studied in the literature. (1) Many properties of the Euclidean space do not transfer to hyperbolic space. For example, the latter generally do not have the scale or shift-invariance in the sense of preserving similarity orders. (2) Many nice properties exclusive to hyperbolic space may improve learning. However, it is unclear how to design algorithms to effectively incorporate these properties. (3) Optimization in hyperbolic space is (i) expensive due to a more sophisticated distance measure and (ii) unstable because gradient descent is performed on hyperbolic manifolds.\nIn this paper, we analyze these difficulties and provide a set of solutions to them. First, we define bad cases as improper relationship between nodes and edges. We then categorize them into capacity illness, inter-subtree illness, and intra-subtree illness. Formal definitions and intuitive visualizations are presented in Section 3. We present a theoretical analysis of local capacity, capacity illness, and their relationship in Section 4. We then develop an algorithm that reduces these illness in Section 5. The algorithm involves a dilation operation during the learning process, adding transitive closure edges of data to positive samples, and a re-weighting strategy. We conduct experiments on synthetic and real world datasets in Appendix 6. The results show that our algorithm achieves superior performances under various evaluation metrics."}, {"title": "2 PRELIMINARIES", "content": "A hyperbolic space \\(H^d\\) is a d-dimensional Riemannian manifold with a constant negative sectional curvature. In this paper, we focus on the Poincar\u00e9 ball model. Let \\(B = B^d\\) denote the d-dimensional Poincare ball. The distance between any two points \\(B_1, B_2 \\in B^d\\) is defined as\n\\(d(B_1, B_2) = arcosh \\left(1 + 2 \\frac{||u - v||^2}{(1 - ||u||^2)(1 - ||v||^2)}\\right),\\)\nwhere u and v are the Euclidean vectors of \\(B_1\\) and \\(B_2\\). In the rest of the paper, we denote the Poincar\u00e9 distance by \\(d(\\cdot, \\cdot)\\) and the Euclidean distance by \\(||\\cdot||)\\). Given a set of points \\(V = \\{x_i\\}_{i=1}^n\\) and the relation set \\(\\mathcal{E} \\subset [n]^2\\), the goal is to learn an embedding \\(f : V \\rightarrow B\\) that preserves the inherent structure. To achieve this goal, we define and minimize the following loss function \\(\\mathcal{L}\\). For \\((i, j) \\in \\mathcal{E}\\), define \\(\\mathcal{N}(x_i, x_j)\\) as the set of negative samples of \\((i, j)\\). Let \\(\\Theta = \\{\\theta_i\\}_{i=1}^n\\), where each \\(\\theta_i \\in B^d\\) is the embedding of \\(x_i\\). Define \\(d(x_i, x_j) = d(\\theta_i, \\theta_j)\\). Then, the loss function is defined as\n\\(L(\\Theta) = \\sum_{(i,j) \\in \\mathcal{E}} \\log \\frac{e^{-d(x_i, x_j)}}{\\sum_{x' \\in \\mathcal{N}(x_i, x_j) \\cup \\{x_j\\}} e^{-d(x_i, x')}} = \\sum_{(i,j) \\in \\mathcal{E}} L_{i,j}(\\Theta).\\)\nThis objective can be optimized via Riemannian gradient descent (Nickel & Kiela, 2017)."}, {"title": "3 ILLNESS", "content": "In this section, we formally define illness that harms the performance of hyperbolic embeddings and is hard to optimize. Let \\(AB\\) be a ground-truth edge in \\(G\\), and \\(AB'\\) be the inferred edge from the\nhyperbolic embeddings, where \\(B' \\neq B\\). We call this situation illness with respect to \\(A\\). Let \\(C\\) be the nearest common ancestor of \\(B\\) and \\(B'\\). We categorize three kinds of illness according to the pairwise relationships among \\(B\\), \\(B'\\), and \\(C\\). Formally, we define capacity illness, intra-subtree illness, and inter-subtree illness in Definition 1.\nDefinition 1 (Categories of Illness). We define the illness to be capacity illness if \\(B\\) is the parent of \\(B'\\). We define the illness to be an intra-subtree illness if \\(B\\) is the ancestor but not the parent of \\(B'\\). We define the illness to be an inter-subtree illness if \\(C \\neq B\\).\nIt is straightforward to see that the union of capacity illness and intra-subtree illness are exactly situations where \\(C = B\\). Therefore, the above three kinds of illness are a partition of all illness. We visualize these three kinds of illness in Figure 3 in the appendix."}, {"title": "4 LOCAL CAPACITY", "content": "We define local capacity below and theoretically relate it to capacity illness.\nDefinition 2 (Local Capacity). Given a geodesic space \\((X, d)\\) and a geodesic ball \\(S_r\\) centered at \\(A \\in X\\) with radius \\(r\\). The local capacity of \\((A, r)\\) is defined as\n\\(\\max \\{|C| : C \\in S_r; \\forall C_1, C_2 \\in C, C_1 \\neq C_2, d(C_1, C_2) > d(C_1, A) \\lor d(C_2, A) \\}.\\)\nGiven a geodesic space \\((X, d)\\) and a geodesic ball \\(S_r\\) centered at \\(A \\in X\\) with radius \\(r\\). The local capacity of \\((A, r)\\) is defined as\n\\(\\max \\{|C| : C \\in S_r; \\forall C_1, C_2 \\in C, C_1 \\neq C_2, d(C_1, C_2) > d(C_1, A) \\lor d(C_2, A) \\}.\\)\nFor not very small \\(r\\) and large \\(d\\) we have the following bounds:\n\\(2^{de} \\leq A(d, \\theta_r) \\leq \\sqrt{2 \\log \\frac{2 \\cdot d^2 \\cdot e^{1 - d} r}{\\sqrt{3}}},\\)\nwhere \\(\\theta_r = arcsin (1/(2 cosh(r/2)))\\). Full derivations are in Appendix B."}, {"title": "5 THE ALGORITHM", "content": "In this section, we build a geometry-aware algorithm (Algorithm 1) targeting the three categories of illness by proposing the dilation operation and the transitive closure regularization.\nDilation. We define a mapping \\(g: B \\rightarrow B\\) as a \\(k\\)-dilation if for any \\(A \\in B\\):\n\\(d(O, g(A)) = k \\cdot d(O, A).\\)\nNotably, \\(g\\) can be computed explicitly. For instance, a 2-dilation can be formulated as \\(g(A) = \\frac{2 A}{1 + ||A||^2}\\). The dilation operation rescales the embedded structure so that each point is pushed to a location with sufficient local capacity. Given \\(A \\in B\\) with degree \\(k\\), this operation helps increase the distance between \\(A\\) and its \\(k\\)-nearest neighbor(\\(r_A\\)), thus increase the local capacity of \\((A, r_A)\\).\nTransitive closure regularization. It contains the following two operations.\nAdding transitive closure edges. The transitive closure edges \\(E_{tc}\\) are edges between nodes and their non-parent ancestors. These edges are also considered as positive samples in addition to \\(\\mathcal{E}\\) in the objective in equation 2. The purpose of adding these auxiliary edges is to push the subtrees apart so they are less likely to overlap in the Poincar\u00e9 ball.\nRe-weighting. We modify the weights of transitive closure edges to prevent overfitting in early (the first \\(N_{tc}\\)) epochs. Let \\(\\eta_{tc}\\) be a real number between 0 and 1. Then, the objective becomes\n\\(L_{tc} (\\Theta) = L(\\Theta) + \\eta_{tc} \\sum_{(i,j) \\in E_{tc}} L_{i,j} (\\Theta).\\)\nIt is noteworthy that these operations are not admissible in the Euclidean space, where the local capacity of any \\((A, r) \\in \\mathbb{R}^d \\times \\mathbb{R}\\) is a constant with respect to \\(d\\)."}]}