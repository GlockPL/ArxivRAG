{"title": "A GEOMETRY-AWARE ALGORITHM TO LEARN HIERARCHICAL EMBEDDINGS IN HYPERBOLIC SPACE", "authors": ["Zhangyu Wang", "Lantian Xu", "Zhifeng Kong", "Weilong Wang", "Xuyu Peng", "Enyang Zheng"], "abstract": "Hyperbolic embeddings are a class of representation learning methods that offer competitive performances when data can be abstracted as a tree-like graph. However, in practice, learning hyperbolic embeddings of hierarchical data is difficult due to the different geometry between hyperbolic space and the Euclidean space. To address such difficulties, we first categorize three kinds of illness that harm the performance of the embeddings. Then, we develop a geometry-aware algorithm using a dilation operation and a transitive closure regularization to tackle these illnesses. We empirically validate these techniques and present a theoretical analysis of the mechanism behind the dilation operation. Experiments on synthetic and real-world datasets reveal superior performances of our algorithm.", "sections": [{"title": "INTRODUCTION", "content": "Learning data representation is important in machine learning as it provides a metric space that reveals or preserves inherent data structure (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017; Hoff et al., 2002; Grover & Leskovec, 2016; Perozzi et al., 2014; Nickel et al., 2011; Bordes et al., 2013; Riedel et al., 2013). Hyperbolic embeddings, a class of hierarchy representation methods, have shown competitive performances when data can be abstracted as a graph (Chamberlain et al., 2017; Davidson et al., 2018; Ganea et al., 2018a; Gu et al., 2018; Tifrea et al., 2018).\nIn this work, we focus on the following embedding task. Let D be a dataset incorporated with a set of hierarchical relations represented as edges in a tree-like graph G. The goal is to learn an embedding in the hyperbolic space by drawing positive and negative samples of edges from the graph such that preserves the edge relationships, which are reflected by the order of similarity between data pairs. The formal problem statement is presented in Section 2.\nTheoretically, hyperbolic space, such as the Poincar\u00e9 Ball model, benefit from high representational power due to their negative curvatures (Nickel & Kiela, 2017; Sala et al., 2018). This observation has motivated research on solving real-world problems in hyperbolic space. For datasets with an observed structure, hyperbolic space can embed the data and preserve the structure with arbitrarily low distortion (Nickel & Kiela, 2017; Chamberlain et al., 2017; Nickel & Kiela, 2018; Ganea et al., 2018b; Chami et al., 2019c). For datasets with a latent structure, especially those obeying the power-law, hyperbolic space can provide a natural metric such that finer concepts are embedded into areas allowing more subtlety (Tifrea et al., 2018; Leimeister & Wilson, 2018; Le et al., 2019).\nDespite the theoretical advantages of hyperbolic embeddings, learning such representation in practice is difficult. Specifically, the following fundamental difficulties have not been well-studied in the literature. (1) Many properties of the Euclidean space do not transfer to hyperbolic space. For example, the latter generally do not have the scale or shift-invariance in the sense of preserving similarity orders. (2) Many nice properties exclusive to hyperbolic space may improve learning. However, it is unclear how to design algorithms to effectively incorporate these properties. (3) Optimization in hyperbolic space is (i) expensive due to a more sophisticated distance measure and (ii) unstable because gradient descent is performed on hyperbolic manifolds.\nIn this paper, we analyze these difficulties and provide a set of solutions to them. First, we define bad cases as improper relationship between nodes and edges. We then categorize them into capacity illness, inter-subtree illness, and intra-subtree illness. Formal definitions and intuitive visualizations are presented in Section 3. We present a theoretical analysis of local capacity, capacity illness, and their relationship in Section 4. We then develop an algorithm that reduces these illness in Section 5. The algorithm involves a dilation operation during the learning process, adding transitive closure edges of data to positive samples, and a re-weighting strategy. We conduct experiments on synthetic and real world datasets in Appendix 6. The results show that our algorithm achieves superior performances under various evaluation metrics."}, {"title": "PRELIMINARIES", "content": "A hyperbolic space Hd is a d-dimensional Riemannian manifold with a constant negative sectional curvature. In this paper, we focus on the Poincar\u00e9 ball model. Let B = Bd denote the d-dimensional Poincare ball. The distance between any two points B1, B2 \u2208 Bd is defined as\n\n $$d(B_1, B_2) = \\operatorname{arcosh}\\left(1 + 2 \\frac{||u - v||^2}{(1 - ||u||^2)(1 - ||v||^2)}\\right)$$\n\nwhere u and v are the Euclidean vectors of B\u2081 and B2. In the rest of the paper, we denote the Poincar\u00e9 distance by d(,) and the Euclidean distance by ||\u00b7||. Given a set of points V = {xi}=1 and the relation set & C [n]\u00b2, the goal is to learn an embedding f : V \u2192 B that preserves the inherent structure. To achieve this goal, we define and minimize the following loss function L. For (i, j) \u2208 E, define N (xi, xj) as the set of negative samples of (i, j). Let \u04e8 = {0i}=1, where each 0i \u2208 Bd is the embedding of xi. Define d(xi, xj) = d(0i, 0j). Then, the loss function is defined as\n\n$$L(\\Theta) = - \\sum_{(i,j) \\in E} \\log \\frac{e^{-d(x_i,x_j)}}{\\sum_{x' \\in N(x_i, j) \\cup \\{x_j\\}} e^{-d(x_i,x')}} = \\sum_{(i,j) \\in E} L_{i,j}(\\Theta).$$\n\nThis objective can be optimized via Riemannian gradient descent (Nickel & Kiela, 2017)."}, {"title": "ILLNESS", "content": "In this section, we formally define illness that harms the performance of hyperbolic embeddings and is hard to optimize. Let AB be a ground-truth edge in G, and AB' be the inferred edge from the"}, {"title": "LOCAL CAPACITY", "content": "We define local capacity below and theoretically relate it to capacity illness.\nDefinition 2 (Local Capacity). Given a geodesic space (X, d) and a geodesic ball S\u2081 centered at A \u2208 X with radius r. The local capacity of (A, r) is defined as\n\n$$\\max \\{|C|: C \\in S_r; \\forall C_1, C_2 \\in C, C_1 \\neq C_2, d(C_1, C_2) > d(C_1, A) \\lor d(C_2, A)\\} .$$\n\nGiven a geodesic space (X, d) and a geodesic ball S\u2081 centered at A \u2208 X with radius r. The local capacity of (A, r) is defined as\n\n$$\\max \\{|C|: C \\in S_r; \\forall C_1, C_2 \\in C, C_1 \\neq C_2, d(C_1, C_2) > d(C_1, A) \\lor d(C_2, A)\\} .$$\n\nFor not very small r and large d we have the following bounds:\n\n$$2^{de} \\frac{A(d, \\theta_r)}{\\sqrt{2}} \\gtrsim \\sqrt{2 \\log \\frac{2.d}{A(d, \\theta_r)}} ,$$\n\nwhere 0r = arcsin (1/(2 cosh(r/2))). Full derivations are in Appendix B."}, {"title": "THE ALGORITHM", "content": "In this section, we build a geometry-aware algorithm (Algorithm 1) targeting the three categories of illness by proposing the dilation operation and the transitive closure regularization.\nDilation. We define a mapping g: BB as a k-dilation if for any A \u2208 B:\n\n$$d(O, g(A)) = k \\cdot d(O, A).$$\n\nNotably, g can be computed explicitly. For instance, a 2-dilation can be formulated as g(A) =\n\n$$\\frac{2}{1+||A||^2} A.$$\n\nThe dilation operation rescales the embedded structure so that each point is pushed to a location with sufficient local capacity. Given A \u2208 B with degree k, this operation helps increase the distance between A and its k-nearest neighbor(ra), thus increase the local capacity of (A, ra).\nTransitive closure regularization. It contains the following two operations.\nAdding transitive closure edges. The transitive closure edges Etc are edges between nodes and their non-parent ancestors. These edges are also considered as positive samples in addition to & in the objective in equation 2. The purpose of adding these auxiliary edges is to push the subtrees apart so they are less likely to overlap in the Poincar\u00e9 ball.\nRe-weighting. We modify the weights of transitive closure edges to prevent overfitting in early (the first Ntc) epochs. Let Ntc be a real number between 0 and 1. Then, the objective becomes\n\n$$L_{tc} (\\Theta) = L(\\Theta) + N_{tc} \\sum_{(i,j) \\in E_{tc}} L_{i,j} (\\Theta).$$\n\nIt is noteworthy that these operations are not admissible in the Euclidean space, where the local capacity of any (A, r) \u2208 Rd \u00d7 R is a constant with respect to d."}, {"title": "EXPERIMENTS", "content": "We compare our algorithm to the baseline model (Nickel & Kiela, 2017) on the synthetic dataset in Figure 1 and Figure 2. Our method not only achieves perfect MAP (0.998) but also yields better reconstructed geometry. We do extensive experiments on multiple real-world datasets of various scales and characteristics in Appendix D. Results show our algorithm consistently outperform the baseline algorithms (Nickel & Kiela, 2017; 2018), especially on extremely bushy datasets."}, {"title": "CONCLUSION", "content": "In this paper, we analyze three categories of illness and develop a geometry-aware algorithm that targets at reducing these illnesses and improving performance. Our algorithm shows superior performance over baseline models on both synthetic and real world datasets."}, {"title": "RELATED WORK", "content": "Learning in hyperbolic space is initially proposed by Nickel & Kiela (2017). It is the most related work to our paper. Their method outperforms the Euclidean counterpart in low dimensions as to the task of learning embeddings for edge reconstruction. However, since their algorithm is directly adapted from the Euclidean space, it does not naturally leverage potentially useful geometrical properties of hyperbolic space (see Section 2). As a consequence, there remain many bad cases even after convergence (see Figure 1a).\nA series of work directly incorporate properties of hyperbolic space via optimization (Wilson & Leimeister, 2018; Bonnabel, 2013; Absil et al., 2009; Afsari et al., 2013). Specifically, Nickel &\nKiela (2018) conduct training in the Lorentz space with a closed-form expression of the geodesics on the hyperbolic manifold. However, since the learning objective equation 2 is highly non-convex, obtaining more accurate gradients does not completely solve the problem.\nAnother group of work either implement hyperbolic versions of commonly used neural network modules (Ganea et al., 2018c; Gulcehre et al., 2018; Chami et al., 2019b), or design models specifically tailored for hyperbolic space (Vuli\u0107 & Mrk\u0161i\u0107, 2017; Le et al., 2019; Cho et al., 2019; Leimeister &\nWilson, 2018; Weber et al., 2020; Chami et al., 2019a). These methods are task-specific and thus expensive to deploy in downstream applications.\nApart from the above learning approaches, Sala et al. (2018) presents a combinatorial algorithm that achieves better performance than Nickel & Kiela (2017; 2018) with even lower dimensions. The core idea is to extend the 2-dimensional results of Sarkar (2011) to arbitrary dimensions. However, this algorithm suffers from three vital weaknesses: (1) it requires complete information of the graph; (2) it is sensitive to addition/removal of data; and (3) most critically, it involves discrete operations and thus does not have gradients. Therefore, in scenarios where complete information is unavailable, the graph dynamically changes, or joint learning is needed, this approach does not suffice.\nIn this paper, we endorse the importance of leveraging geometrical properties in learning unsupervised hyperbolic embeddings. Based on this intention, we develop a geometry-aware algorithm that improves embedding performances, which, to the best of our knowledge, is original."}, {"title": "ILLNESS", "content": null}, {"title": "LOCAL CAPACITY", "content": "According to Definition 2, for A \u2208 V and a radius r, if |{C : C is a child of A, d(A,C) < r}|\nexceeds the local capacity of (A, r), then capacity illness must exist. To obtain bounds on local"}, {"title": "EXPERIMENTAL SETTINGS AND OVERVIEW", "content": "We apply our algorithm to both a synthetic dataset and real-world datasets on the graph reconstruction task. We evaluate the performance by mean average precision (MAP) and Mean Rank (MR) defined below. For A \u2208 V with degree deg(A) and neighborhood Na = {B1,...Bdeg(A)}, let RA,Bi be the smallest subset of V containing Bi and all points closer to A than Bi. Then, the MAP is defined as\n\n$$MAP(f) = \\frac{1}{|v|} \\sum_{A \\in V} \\frac{1}{deg(A)} \\sum_{i=1}^{NA} \\frac{|RA,Bi|}{|RA,B_i|}$$\n\nand the MR is defined as\n\n$$MR(f) = \\frac{1}{|V|} \\sum_{A \\in V} \\sum_{i=1}^{NA} (|RA,B_i| - i).$$\n\nIn addition, we report the number of three kinds of illness defined in Definition 1 after the algorithm converges.\nWe run baseline algorithms (Nickel & Kiela, 2017; 2018) and our algorithm on a synthetic dataset, Yelp Challenge (Tree) (see Table 2), WordNet Verbs (Tree) (see Table 3), WordNet Nouns (Tree) (see Table 4), Commodity Catalog (Tree) (see Table 5) and WordNet Nouns (Closure) (see Table 6). We report the reconstruction Mean Rank, MAP, number of capacity errors, number of intra-subtree errors and number of inter-subtree errors respectively.\nA key point to notice is that we focus on tree datasets instead of general DAG or transitive closures of trees. In terms of the objective (reducing MAP and MR), learning a tree structure is much harder because the size of the neighborhood set is as few as one. Experiments validate this statement: we apply the baseline algorithm (Nickel & Kiela, 2017) to the same WordNet Noun Hierarchy dataset (Nickel & Kiela, 2017) where the transitive closure edges are removed. The performances in terms of MAP and MR significantly drop compared to the numbers reported in (Nickel & Kiela, 2017) (See Table 4)."}, {"title": "SYNTHETIC DATASET EXPERIMENTS", "content": "We synthesize a balanced tree T of 5 layers including the root node. Each non-leaf node in T has 5 children. The edges are directed, pointing from children to parents. We illustrate our algorithm on this synthetic dataset.\nWe compare our algorithm (Algorithm 1) to the baseline Poincar\u00e9 embedding algorithm (Nickel &\nKiela, 2017). In both algorithms, we set the dimension to be 2, learning rate to be 0.5, batch size to be 50, and the number of negative samples m to be 50.\nThe comparison between learning procedures is presented in Figure 2. As demonstrated, our algorithm learns visually more balanced embeddings with less illness. Quantitative results of the comparison, including the number of illness, MAP and MR, are presented in Figure 4 and Figure 5. We compare different Nte in Figure 5 and different te in Figure 4. Our algorithm produces less illness than the baseline algorithm and achieves the highest MAP and MR.\nFigure 4 shows our explorations on how to use transitive closure edges.They tend to increase the overall effective gradient magnitude and draw vertices of a same subtree tightly together. Therefore, this could help reduce capacity illness quickly (See Figure 4 (a)) and eliminate inter-subtree illness (See Figure 4 (c)). However, it might also confuse the ground-truth tree edges with the added ones, thus increasing intra-subtree illness (See Figure 4 (b)). When we assign weights to the transitive closure edges, this side effect is mitigated: Ntc = 0.2 yields the best results.\nFigure 5 shows our explorations on how to use dilation and reweighting. These two operations should be applied after certain epochs of training so that 1) the subtrees are pushed relatively far from each other \u00b9 to ensure the dilation operation will push vertices in the appropriate directions, and 2) before the vertices are already pushed to places with sufficient capacity. Empirically we find this threshold epoch number Ntc = 300 yields the best results."}, {"title": "REAL-WORLD DATASET EXPERIMENTS", "content": "Table 1 displays statistics of several datasets used in our experiments (including the Yelp Challenge Dataset 2, the Commodity Catalog Dataset, and the WordNet dataset (Miller, 1998)). We report the number of nodes |V|, the number of edges E, maximum degree, variance of all degrees, and tree depths. We make subjective remarks to their size and characteristics.\nThe results for Yelp Challenge (Tree) are illustrated in Table 2. In all algorithms, we set the learning rate to be 1.0, batch size to be 10, and the number of negative samples m to be 50.\nThe results for WordNet Verbs (Tree) are illustrated in Table 3. In all algorithms, we set the learning rate to be 1.0, batch size to be 10, and the number of negative samples m to be 50.\nThe results for WordNet Nouns (Tree) are illustrated in Table 4. In all algorithms, we set the learning rate to be 1.0, batch size to be 50, and the number of negative samples m to be 50.\nThe results for Commodity Catalog (Tree) are illustrated in Table 5. In all algorithms, we set the learning rate to be 1.0, batch size to be 10, and the number of negative samples m to be 50.\nThe Commodity Catalog (Tree) dataset is generated from real-world e-commerce data. It is extremely bushy. According to Sala et al. (2018) the hyperbolic space is capable to embed even extremely bushy trees. However, we find the baseline algorithm in Nickel & Kiela (2017) can not fully exert such capability. This is because a bushy tree requires large local capacity, but the baseline algorithm takes very long time to reach such capacity and instead easily gets stuck at local optimum. Experimentally, the baseline algorithm does not perform well on such bushy dataset, while our algorithm with simple dilation yields significantly better results 3.\nThe results for WordNet Nouns (Closure) are illustrated in Table 6. In all algorithms, we set the learning rate to be 1.0, batch size to be 50, and the number of negative samples m to be 50."}]}