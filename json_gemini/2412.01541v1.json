{"title": "Effectiveness of L2 Regularization in Privacy-Preserving Machine Learning*", "authors": ["Nikolaos Chandrinos", "Iliana Loi", "Panagiotis Zachos", "Ioannis Symeonidis", "Aristotelis Spiliotis", "Maria Panou", "Konstantinos MoustakasD"], "abstract": "Artificial intelligence, machine learning, and deep learning as a service have become the status quo for many industries, leading to the widespread deployment of models that handle sensitive data. Well-performing models, the industry seeks, usually rely on a large volume of training data. However, the use of such data raises serious privacy concerns due to the potential risks of leaks of highly sensitive information. One prominent threat is the Membership Inference Attack, where adversaries attempt to deduce whether a specific data point was used in a model's training process. An adversary's ability to determine an individual's presence represents a significant privacy threat, especially when related to a group of users sharing sensitive information. Hence, well-designed privacy-preserving machine learning solutions are critically needed in the industry. In this work, we compare the effectiveness of L2 regularization and differential privacy in mitigating Membership Inference Attack risks. Even though regularization techniques like L2 regularization are commonly employed to reduce overfitting, a condition that enhances the effectiveness of Membership Inference Attacks, their impact on mitigating these attacks has not been systematically explored.", "sections": [{"title": "1 Introduction", "content": "Privacy, a multifaceted concept encompassing freedom of thought, control over personal information, and protection from surveillance, has become increasingly"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Deep Neural Networks", "content": "Deep Neural Networks (DNNs) are composed of multiple layers of interconnected neurons, where each layer transforms its input data, often into a higher-dimensional representation [6,18]. The operation of a fully connected (dense) layer can be mathematically described by:\ny = $ (Wx + b),\nwhere: x \u2208 Rm in the input vector, W \u2208 Rnxm is the weight matrix, b \u2208 Rn is the bias vector, \u03c6(\u00b7) is a non linera activation function (e.g. ReLU, sigmoid), y \u2208 Rn is the output vector [6,18,20].\nThe strength of DNNs lies in their capacity to model complex nonlinear relationships throughout their architecture. This ability enables the network to learn hierarchical feature representations from the data [6]. A DNN with L layers can be represented as:\ny = f (x; 0) = f(L) (f(L-1) (... f(1)(x)),\nwhere: f(1)(.) denotes the function computed at layer 1, \u04e8 = {W,b}1 represents all the networks parameters [6].\nTraining a DNN involves finding the optimal set of parameters that minimize a predefined loss function L (y, \u0177), where \u0177 is the true label corresponding to the input sample x. The loss function measures the discrepancy between the predicted output y and the actual output \u0177. For multi-class classification problems, the cross-entropy loss function is frequently employed [6].\nThe optimization of the loss function is typically carried out using gradient-based methods. The most common approach is Stochastic Gradient Descent (SGD) and its variants, such as Adam and RMSprop. The gradients of the loss with respect to the network parameters are computed using the backpropagation algorithm. Backpropagation applies the chain rule from calculus to efficiently propagate errors backward through the network layers, enabling the adjustment of weights and biases to minimize the loss [6]."}, {"title": "2.2 L2 Regularization", "content": "L2 regularization, also known as Ridge regression, is a widely utilized technique in machine learning, particularly in scenarios where model complexity needs to be controlled without eliminating features. The core idea behind L2 regularization is to penalize large coefficients in the model, thereby reducing the risk of overfitting [19,4].\nIn machine learning, L2 regularization modifies the loss function by adding a penalty to the sum of the squared coefficients of the model parameters. The modified loss function can be expressed as:\nLoss = LosSorig + \u03a3\u03c9\nHere, Lossorig represents the original loss function (such as mean squared error for linear regression), \u5165 is the regularization parameter that controls the strength of the penalty, and wi are the coefficients of the model. The term 1 w is the L2 penalty, which increases as the magnitude of the coefficients increases [19].\nThe effect of this penalty is to shrink the coefficients towards zero, but not necessarily to zero. This shrinkage prevents any single feature from dominating the model, especially in the presence of multicollinearity, where features are highly correlated [5]. The regularization parameter A plays a crucial role; a higher A value imposes a stronger penalty, leading to greater shrinkage of the coefficients [19].\nIn neural networks, L2 regularization is incorporated into the training process through a technique known as weight decay. The loss function, often based on cross-entropy or mean squared error, is augmented with an L2 penalty on the network's weights. The modified loss function in this context can be written as:\nLoss = LosSorig + \u03bb \u03a3\u03a3\u03c9\u03ba\nWhere wjk represents the weights connecting the neurons between layers j and k, m is the number of layers, and n is the number of connections [19].\nThe inclusion of the L2 penalty ensures that the network learns smaller weight values, which helps in preventing the model from overfitting to the training data. During the training process, the gradient update rule for the weights is modified to include this penalty. Specifically, the weights are updated according to:\nWjk\n\u2190\nWjk - \u03b7\n(\nLosSorig\ndwjk\n+ 2xwjk\n)\nWhere n is the learning rate, and the term 2\u03bbwjk represents the derivative of the L2 penalty. This update rule gradually reduces the magnitude of the weights as the training progresses, leading to a simpler and more generalizable model [19].\nThe use of L2 regularization in neural networks improves the model's ability to generalize, reduces sensitivity to input noise, and enhances the stability of the learning process [19]. However, the choice of the regularization parameter A is critical; if set too high, it can lead to underfitting, where the model is overly simplistic and fails to capture the underlying data patterns [19,4]."}, {"title": "2.3 Memberships Inference Attack", "content": "MIAs originate from the observation that models usually perform differently on the data they are trained on than first-time-seen data. Generally, it refers to"}, {"title": "2.4 Differential Privacy", "content": "Differential privacy (DP) techniques apply arbitrary modifications to a dataset such that if an individual has access to the dataset's entries, they will not be"}, {"title": "3 Experimental Results", "content": "In this work, we experimentally evaluate the MIAs performance in two different datasets: (a) MNIST [14], (b) CIFAR10 [13], by applying two neural network architectures: (a) a fully connected neural network (FCNN), (b) a convolutional neural network (CNN) and (c) an augmented version of the Toxic Tweets Dataset for text classification [9], covering a wide range of possible scenarios with commonly used architecture designs. For each aforementioned scenario, we compared two training methods: (a) a normal (baseline) approach using a non-DP-trained model, and (b) a DP method involving a DP-trained model. Additionally, for the text classification task, we use a simple embedding-based neural network tailored for natural language data. To study how L2 regularization affects their privacy, we also evaluated both methods with L2 regularization. Finally, we extend our experimental evaluation to examine how the difference in accuracy between training and evaluation affects MIAs."}, {"title": "3.1 Image Classification Task", "content": "The MNIST [14] dataset consists of handwritten digits, including 60,000 samples in the training set and 10,000 in the evaluation set. The input flattened images (784 features) are fed to a fully connected network which consists of three layers with 10, 20, and 10 neurons, respectively. The non-DP models are optimized for 50 epochs using Adam optimizer [12] with a learning rate equal to 0.0001 and using the cross-entropy loss. Batches of 32 samples were used. A similar configuration was used for the DP models, but instead of Adam, we used DP-Adam of TensorFlow Privacy [2]."}, {"title": "3.2 Text Classification Task", "content": "The dataset employed for this task was an artificially enhanced version of the Toxic Tweets Dataset [9]. The original dataset consisted of 54,313 tweets, where each tweet is labeled as either toxic or non-toxic with each class containing 24153 and 32592 posts respectively. The dataset was enhanced by adding a new column,"}, {"title": "3.3 Overfit Evaluation", "content": "These results underscore the potential of L2 regularization to provide better privacy protections, as it both reduces the attacker advantage and sustains strong validation performance, in contrast to the DP-optimized model. Furthermore, the strength parameter A can be seen as a trade-off factor between model accuracy and privacy. Lower values of A tend to result in higher accuracy but with reduced privacy protection, as reflected in higher attacker advantage. Conversely, increasing A enhances privacy by lowering the attacker advantage, but at the cost of reduced model accuracy. This trade-off can be tuned, for the specific use case, to strike a balance between maintaining strong validation performance and minimizing the model's vulnerability to attacks.\nFinally, we analyzed the relationship between the difference in training and evaluation accuracy (accuracy difference) and the attacker advantage. To visualize this relationship, we plotted the accuracy difference on the y-axis and the attacker advantage on the x-axis. The resulting plot, presented in Figure 4, demonstrates that as the accuracy difference increases, the attacker's advantage also increases. Specifically, we observed a correlation coefficient of 0.93, indicating a strong positive correlation between these two metrics."}, {"title": "4 Conclusion", "content": "We have investigated the effectiveness of L2 regularization in enhancing privacy protection against MIAs in DL models. Our approach was evaluated on benchmark datasets, including MNIST [14], CIFAR-10 [13], and an augmented version of the Toxic Tweets Dataset [9], using various neural network architectures."}]}