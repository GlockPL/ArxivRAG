[{"title": "Related Work", "authors": ["Luca Marzari", "Francesco Leofante", "Ferdinando Cicalese", "Alessandro Farinelli"], "abstract": "We study the problem of assessing the robustness of counterfactual explanations for deep learning models. We focus on plausible model shifts altering model parameters and propose a novel framework to reason about the robustness property in this setting. To motivate our solution, we begin by showing for the first time that computing the robustness of counterfactuals with respect to plausible model shifts is NP-complete. As this (practically) rules out the existence of scalable algorithms for exactly computing robustness, we propose a novel probabilistic approach which is able to provide tight estimates of robustness with strong guarantees while preserving scalability. Remarkably, and differently from existing solutions targeting plausible model shifts, our approach does not impose requirements on the network to be analyzed, thus enabling robustness analysis on a wider range of architectures. Experiments on four binary classification datasets indicate that our method improves the state of the art in generating robust explanations, outperforming existing methods on a range of metrics.", "sections": [{"title": "Introduction", "content": "Understanding and interpreting the decisions of black-box deep learning models has become a dominant goal of Explainable AI (XAI). Several strategies have been proposed to this end. In this paper, we focus on counterfactual explanations (CFX) (see [30, 15] for recent surveys on the topic), which aim to demystify the decision-making of a Deep Neural Network (DNN) by showing how an input needs to be changed to yield a different, typically more desirable, decision. Consider the widely used example of a loan application, where a mortgage applicant represented by an input x with features unemployed status, 25 years of age, and low credit rating applies for a loan and is rejected by the bank's AI. A CFX for this decision could be a slightly modified input, where increasing credit rating to medium would result in the loan being granted.\nAs CFXs have the potential to influence decisions with strong societal implications, their reliability has become the subject of intensive study (see [13] for a survey). In particular, recent work has highlighted issues related to the robustness of CFXs against Plausible Model Shifts (PMS) [32, 12], showing that the validity of CFXs is likely to be compromised when bounded perturbations are applied to the parameters of a DNN, e.g., as a result of fine-tuning [32, 1, 22, 12, 10]. Consider the loan example: if retraining occurs while the applicant is working toward improving their credit rating, without robustness, their modified case may still result in a rejected application, leaving the bank liable due to their conflicting statements.\nIn this paper, we focus on this troubling phenomenon and advance the state of the art in CFX robustness research in several directions. More specifically, in \u00a7 4, we study the computational complexity of exactly determining whether a CFX is robust to PMS, formally showing for the first time that answering this question is NP-complete. As our result rules out the existence of practical algorithms to exactly compute the CFX robustness, we shift our attention toward methods for obtaining probabilistic guarantees on the robustness of CFX under model shifts. We, therefore, consider the work by Hamman et al. [10], where they propose a probabilistic approach to compute the robustness of CFX under Naturally-Occurring Model Shifts (NOMS). Even though both PMS and NOMS notions are commonly used in the literature, very little is known about their potential interplay, and the question of whether robustness to NOMS subsumes robustness to PMS is still unresolved. We settle this question in \u00a7 5, where we show that these two notions capture profoundly different scenarios, proving that robustness guarantees given for NOMS do not directly extend to PMS. Having settled this, in \u00a7 6 we present \u0391\u03a1AS, a novel sampling-based certification algorithm which allows to determine a provable probabilistic bound on the maximum shift a CFX can tolerate under PMS. Unlike existing solutions for robustness under PMS, our approach comes with significantly reduced computational requirements and does not make any assumption on the underlying DNN, thus making it applicable to a wider range of architectures. Finally, to assess the effectiveness of our proposed solution, in \u00a7 7, we study the general performance of our certification algorithm and provide a comprehensive comparison of the proposed approach against several state-of-the-art methodologies for CFX robustness certification. Crucially, we show that our approach can also be used to generate robust CFXs, outperforming existing methods on a number of metrics from the CFX literature."}, {"title": "Related Work", "content": "Various methods for generating CFXs for DNNs have been proposed. The seminal work of [36] framed the task of generating CFXs as a gradient-based optimization problem and proposed a loss that promotes CFX validity (i.e., the CFX successfully changes the classification outcome of the network) and proximity (i.e., the CFX is as close as possible to the original input for some distance metric). In addition to these metrics, other important properties have been highlighted as crucial for the practical applicability of CFXs. Prominent examples include plausiblity (i.e., the CFX must lie on the data manifold) [26, 23] and actionability (i.e., the changes suggested by the CFX must be achievable by the user in practice) [33]. Differently from these works, here we focus on the robustness property of CFXs.\nSeveral forms of CFX robustness have been studied in the literature [13]. Robustness to input changes is the focus of, e.g. [28, 3, 38, 18], where solutions are devised to ensure that explanation algorithms return similar CFXs for similar inputs. In another line of work, [9, 35, 17, 24] considered the problem of generating adversarially robust CFXs that preserve validity under imperfect (or noisy) execution. Robustness to model multiplicity is instead considered in, e.g. [23, 19, 14], where CFXs that preserve validity across sets of models are sought. However, the study of these forms of robustness is outside the scope of this paper as our focus is on model shifts. Robustness to model shift has been studied in, e.g. [32, 1, 22, 12, 10]. Of these, the approaches of [32] and [12] are the most closely related to our work. The former presents an approach to generate robust CFXs under PMS using techniques from continuous optimization, which is able to guarantee robustness in the average-case scenario. The latter instead solves the same problem using abstraction techniques and discrete optimization tools, obtaining robustness guarantees that hold under worst-case conditions. Given their relevance, both approaches will be considered for an extensive experimental comparison in \u00a7 7."}, {"title": "Background", "content": "(Neural) Classification model. Let \\(X \\subset \\mathbb{R}^d\\) denote the input space of a classifier \\(M_\\theta: X \\rightarrow [0,1]\\) mapping an input \\(x \\in X\\) to an output probability between 0 and 1. We consider classifiers implemented by feed-forward DNNs parameterized by a (parameter) vector \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}^k\\). Given two parameter vectors \\(\\theta, \\theta' \\in \\Theta\\), we refer to the corresponding classifiers \\(M_\\theta\\) and \\(M_{\\theta'}\\) as instantiations of the same parametric classifier \\(M_\\theta\\). We assume concrete valuations of \\(\\theta\\) are learned from a set of labeled inputs as customary in supervised learning settings [7]. Once \\(\\theta\\) has been learned, the classifier can be used for inference. Without any loss of generality, we focus on binary classification tasks, i.e., the classification decision produced by \\(M_\\theta\\) for an unlabeled input x is 1 if \\(M_\\theta(x) \\geq 0.5\\), and 0 otherwise.\nCounterfactual explanations. Existing methods in the literature define CFXs as follows.\nDefinition 1. Consider an input \\(x \\in X\\) and a classifier \\(M_\\theta\\) s.t. \\(M_\\theta(x) < 0.5\\). Given a distance metric \\(d: X \\times X \\rightarrow \\mathbb{R}^+\\), a (valid) counterfactual explanation is any \\(x'\\) such that:\n\\[\\begin{array}{c}\n\\arg \\min_{x' \\in X} d(x, x') \\\\\n\\text{subject to } M_\\theta(x') \\geq 0.5\n\\end{array}\\]\nIntuitively, given an input x for which the classifier produces a negative outcome, a counterfactual explanation is a new input x' which is similar to x, e.g., in terms of some specified distance between features values, and for which the classifier predicts a different outcome. Common choices for d include the \\(l_1\\) and \\(l_\\infty\\) norms [36], which will also be used in this work.\nRobustness to model shifts Among several notions of robustness, recent work has placed emphasis on generating CFXs that remain valid under (slight) shifts in the classifier they were generated for. While existing approaches rely on a diverse range of techniques to solve this problem, they all share a common understanding of what constitutes a model shift, which we present next.\nDefinition 2 (Jiang et al. [12]). Let \\(M_\\theta\\) and \\(M_{\\theta'}\\) be two instantiations of a parametric classifier \\(M_\\theta\\). For \\(0 < p < \\infty\\), the p-distance between \\(M_\\theta\\) and \\(M_{\\theta'}\\) is defined as \\(d_p(M_\\theta, M_{\\theta'}) = ||\\theta - \\theta' ||_p\\).\nDefinition 3 (Jiang et al. [12]). A model shift (w.r.t. a fixed p-distance) is a function S mapping a classifier \\(M_\\theta\\) into another classifier \\(M_1 = S(M_\\theta)\\) such that:\n\\begin{itemize}\n    \\item \\(M_\\theta\\) and \\(M_{\\theta'}\\) are instantiations of the same \\(M_\\theta\\);\n    \\item \\(d_p(M_\\theta, M_{\\theta'}) > 0\\).\n\\end{itemize}\nInformally, a model shift captures changes in the parameters of a DNN, but does not affect its architecture. Based on this definition, we can formalize the robustness property for a CFX as follows.\nDefinition 4. Consider an input \\(x \\in X\\) and a classifier \\(M_\\theta\\) s.t. \\(M_\\theta(x) < 0.5\\). Let x' be a counterfactual explanation computed for x s.t. \\(M_\\theta(x') \\geq 0.5\\). Given a set of model shifts \\(\\triangle\\), we say that the counterfactual x' is \\(\\triangle\\)-robust if \\(S(M_\\theta)(x') \\geq 0.5\\) for all \\(S \\in \\triangle\\).\nThe definition of a model shift can be specialized to better characterize how \\(\\theta\\) is allowed to change under S. In the following, we report two most commonly studied notions of model shifts: Naturally-Occurring Model Shifts and Plausible Model Shifts.\nDefinition 5 (Hamman et al. [10] (NOMS)). Consider a classifier \\(M_\\theta\\). A set of model shifts is said to be naturally occurring if for a (randomly) chosen model shift S from \\(\\triangle\\) and \\(M_{\\theta'} = S(M_\\theta)\\) being the new classifier obtained after applying S to \\(M_\\theta\\) the following hold:\n\\begin{itemize}\n    \\item \\(\\mathbb{E}[M_{\\theta'}(x)] = M_\\theta(x)\\); where the expectation is over the randomness of \\(M_{\\theta'}\\) given a fixed value of x;\n    \\item \\(\\text{Var}[M_{\\theta'}(x)] = v_x\\), where \\(v_x\\) represents the maximum variance of the prediction of \\(M_{\\theta'}(x)\\), and whenever x lies on the data manifold X, \\(v_x\\) is upper bounded by a small constant v;\n    \\item If \\(M_\\theta\\) is Lipschitz continuous for some \\(\\Upsilon_1\\), then \\(M_{\\theta'}(x)\\) is also Lipschitz continuous for some \\(\\Upsilon_2\\).\n\\end{itemize}\nBroadly speaking, a naturally-occurring model shift allows the application of arbitrary changes to \\(\\theta\\) as long as the overall behavior of the classifier is not affected. This is in contrast with the notion of plausible model shift [32, 12], which requires changes to be bounded.\nDefinition 6 (Jiang et al. [12] (PMS)). Consider a classifier \\(M_\\theta\\) and a new classifier \\(M_1 = S(M_\\theta)\\) obtained after applying a model shift S to \\(M_\\theta\\). Given some \\(\\delta \\in \\mathbb{R}_{>0}\\) and \\(0 < p < \\infty\\), S is said to be plausible (w.r.t. the choice of parameters \\(\\delta\\) and p) if \\(d_p(M_\\theta, S(M_\\theta)) < \\delta\\).\nGiven an upper bound \\(\\delta\\), a set of PMS \\(\\triangle\\) can then be obtained by considering all shifts S that satisfy Definition 6, i.e. \\(\\triangle = \\{S | d_p(M_\\theta, S(M_\\theta)) \\leq \\delta\\}\\). In the rest of this paper, we will sometimes use \\(\\triangle_\\delta\\) to make the upper bound \\(\\delta\\) explicit. Given a set \\(\\triangle\\), a realization of \\(\\triangle\\) can be defined as follows.\nDefinition 7. Given a classifier \\(M_\\theta\\), and a set of plausible model shifts, we say that a realization of \\(\\triangle\\) is a classifier \\(M_{\\theta'}\\) such that \\(\\theta' \\in [\\theta - \\delta, \\theta + \\delta]\\)"}, {"title": "Checking Robustness is Hard", "content": "In this section, we study the computational complexity of deciding whether a given counterfactual explanation is robust in the presence of model shifts. Our aim here is to better understand the computational challenges arising from this problem and to use these results to guide the development of novel, more efficient certification procedures. Without loss of generality, we consider PMS to encode the problem. Deciding whether a given CFX x' is robust to a set of PMS \\(\\triangle\\) requires to check whether \\(\\triangle\\) contains at least one realization which yields a classification outcome that is different from the intended CFX outcome. For instance, in the case of the loan example, this would correspond to a model rejecting the loan (\\(M_\\theta(x') < 0.5\\)) as opposed to accepting it as intended. This problem can be formulated as follows.\nDISTINCT-REALIZATIONS PROBLEM (DRP)\nInput: a classifier \\(M_\\theta\\), a set of PMS, an input x, and a threshold T.\nOutput: yes \\(\\exists M_{\\theta_1}, M_{\\theta_2} \\in \\triangle: M_{\\theta_1}(x) < T < M_{\\theta_2}(x)\\).\nTheorem 1. Deciding DRP is NP-complete.\nProof sketch. The inclusion of DRP in NP requires two forward propagations of \\(\\theta\\) through two concretizations (i.e., the certificates) checking if T is between the two computed outputs. This is clearly polynomial in the size of the classifier. Regarding the hardness, we can show that 3-SAT reduces to DRP. Given a formula \\(\\phi\\) we can produce a DNN classifier \\(M_\\theta\\), an input x and a \\(\\delta\\) (maximum shift on the edge weights), defining a \\(\\triangle\\) such that \\(\\phi\\) is satisfiable if and only if there exists another \\(M_{\\theta'}\\) which is also a realization of \\(\\triangle\\) such that \\(0.5 \\leq M_\\theta(x)\\) and \\(M_\\theta(x) < 0.5\\).\nWe build on the reduction of [16], which, given a formula \\(\\phi\\) produces a neural network such that satisfying assignments for \\(\\phi\\) are encoded into inputs to the network producing a desired output.\nA first observation is that if we start with the neural network \\(M_\\theta\\) produced by the reduction of [16], and replace any weight equal to 1 with the interval \\([1 - 2\\delta, 1]\\) and any weight -1 with the interval \\([-1 - 2\\delta, -1]\\), we obtain \\(\\triangle\\) such that for any \\(M_{\\theta'}\\) being a realization of \\(\\triangle\\) and any input x, \\(M_{\\theta'}(x) \\in [(1 - p_1(\\delta))M_\\theta(x), (1 + p_2(\\delta))M_\\theta(x)]\\), where \\(p_1, p_2\\) are some fixed polynomials defined by the number of layers in \\(M_\\theta\\). Therefore, we can extend the hardness of [16] to the case of plausible model shift, where instead of checking for the output to be t, we could use an additional layer to check whether the output is in the interval \\([(1 - p_1(\\delta))t, (1 + p_2(\\delta))t]\\).\nHowever, with respect to the hardness proof of [16], there is a substantial difference in our problem since, besides starting from a DNN with each weight in \\([\\theta_i - \\delta, \\theta_i + \\delta]\\), we want to map satisfying assignments to realizations of the \\(\\triangle\\), rather than inputs of the network. In fact, encoding assignments to choices of the weights turns out to require significantly more.\nOur reduction is designed so that the assignments to the variables of the CNF are encoded to the choices of some specific weights of the DNN, henceforth referred to as the network edge main inputs. These are the weights that determine the outputs of a gadget that we call the generating gadget. A generating gadget has a fixed input (representing the CFX), uses only intervals of width \\(2 \\cdot \\delta\\), and produces a value in [0, 1] where the Boolean false is represented by a value close to 0 and the Boolean true is represented by a value close to 1.\nThe output of the generating gadget (one per each variable of the formula ) is sent both to the network simulating the CNF formula (as in [16]) and to a further gadget (the discretizer-gadget) that controls whether the output of the generating gadget is a discrete value in {0, 1}.\nThe output of the network simulating the formula is then combined with the output of the discretizer-gadgets in such a way that the final output is < 0.5 whenever, either there is one of the outputs of the generating-gadgets (determined by the choice of the network edge main input) which is not in {0, 1} or the output of the subnetwork simulating the formula implies that the network edge main input encode and assignment not satisfying some clause of \\(\\phi\\).\nFrom Theorem 1, it follows that deciding whether a CFX x' is not robust to a set of PMS \\(\\triangle\\) is NP-complete. This results lead to the following corollary.\nTheorem 2. Given a model \\(M_\\theta\\) and a set of plausible model shifts \\(\\triangle\\), computing the number of model shifts in \\(\\triangle\\) for which a given CFX x' is \\(\\triangle\\)-robust is NP-hard.\nIndeed, deciding non-\\(\\triangle\\)-robustness and \\(\\triangle\\)-robustness are equivalent under Turing reductions. Therefore, the counting problem for \\(\\triangle\\)-robustness is at least as hard as the decision problem for non-\\(\\triangle\\)-robustness. This hardness result motivates an approximate approach to estimate the robustness of a counterfactual under a set of PMS \\(\\triangle\\)."}, {"title": "Probabilistic Guarantees for Existing Notions of Model Shifts", "content": "As we have established in the previous section, exact methods for computing robustness under model shifts are bound to lack scalability. This motivates the design of probabilistic approaches to solve the problem. Previous work by Hamman et al. [10] presented an approach to obtain counterfactual explanations that are probabilistically robust under NOMS. A natural question that arises then is whether guarantees obtained for NOMS also transfer to the PMS setting. As we show for the first time below, this is not the case in general.\nLemma 3. Naturally-occurring model shifts may not be Plausible, and vice-versa.\nProof. Consider the DNN \\(M_\\theta\\) depicted with two input nodes, one hidden layer with two ReLU nodes and one single output. The parameters \\(\\theta = [w_1,..., w_6]\\) are the weights on the edges listed top-bottom and left-right.\nPropagating an input vector \\(x = [x_1,x_2]^T\\) through \\(M_\\theta\\), we obtain \\(M_\\theta(x) = y = w_5 \\max\\{0, w_1 \\cdot x_1 + x_2 \\cdot w_3\\} + w_6 \\cdot \\max\\{0, w_2 \\cdot x_1 + x_2 \\cdot w_4\\}\\). Now assume an input vector \\(x = [0.9, 0.9]\\) and weights \\(w_1 = 1, w_2 = 0, w_3 = 0, w_4 = 0.6, w_5 = 1, w_6 = -1\\). The corresponding output generated by the DNN is \\(M_\\theta(x) = 0.46\\). A counterfactual for x could be given as a new input vector \\(x' = [1,0.8]^T\\), for which we obtain \\(M_\\theta(x') = 0.52 > 0.5\\).\nNow, following Definition 6, we consider a set of plausible model changes obtained for \\(\\delta = 0.3\\). This can be captured by defining on each weight \\(w_i\\) the corresponding interval in depicted that represents the set of all the possible models obtained from \\(M_\\theta\\), replacing each \\(w_i\\) with a weight in the interval \\([w_i - \\delta, w_i + \\delta]\\). We then have that, the expected result of a model sampled uniformly from such a set satisfies:\n\\begin{align*}\n\\mathbb{E}[M_{\\theta'}(x')] &= \\mathbb{E}[w_5] \\mathbb{E}[ReLU(x_1w_1 + x_2w_3)] + \\\\\n& \\mathbb{E}[w_6] \\mathbb{E}[ReLU(x_1w_2 + x_2 \\cdot w_4)] \\\\\n&= \\mathbb{E}[[0.7, 1.3]] \\mathbb{E} [\\max\\{0, x_1 [0.7, 1.3] + \\\\\n&x_2[-0.3, 0.3]\\}] + \\mathbb{E}[[-1.7, -0.3]] \\cdot \\\\\n&\\mathbb{E}[\\max\\{0,x_1[-0.3, 0.3] + x_2 [0.3, 0.9]\\}] \\\\\n&> 0.52 \\neq M_\\theta(x')\n\\end{align*}\nDefinition 5 states that a model shift is naturally occurring if \\(\\mathbb{E}[M_{\\theta'}(x)] = M_\\theta(x)\\). This implies that \\(\\triangle\\) contains models that cannot be characterized as naturally occurring model changes. Vice versa, the existence of Naturally occurring model shifts not being plausible is implicit in the definition thus giving our result."}, {"title": "Robustness under PMS with Probabilistic Guarantees", "content": "Jiang et al. [12] proposed to use INNs to enable a compact representation of a superset of the models that can be obtained by a perturbation of the starting model under a set \\(\\triangle\\). By exploiting an exact reachable set computation method, e.g., based on MILP [31], the authors could determine whether or not a CFX is robust under the chosen \\(\\triangle\\) via a single forward propagation of the CFX. However, in view of the NP-hardness of the problem discussed in the \u00a7 4 and the typical non-linear nature of the classifiers, it presents some computational limitations.\nIn general, interval neural networks map inputs to intervals representing an over-approximation of all possible outcomes that can be produced by any shifted model \\(M_{\\theta'}\\) obtained under \\(\\triangle\\). Given this property, if the output reachable set is completely disjoint from the decision threshold 0.5, then one can assert - in a sound and complete fashion - whether or not a given CFX is robust. On the other hand, if we run into a situation such as the one depicted in Fig. 2 (b), one cannot assert robustness with certainty. In this scenario, Jiang et al. [12] propose to classify the CFX as not robust, which preserves the soundness of their result. Nonetheless, this might lead to discarding a CFX even when the actual density of (equivalently, the probability that after retraining, we incur in) plausible model shifts for which the CFX is not robust is extremely low. As we will show in \u00a7 7, this worst-case notion of robustness affects the CFXs generated by [12], which may end up being unnecessarily expensive (in terms of proximity) and having low plausibility. Additionally, computing the exact output reachable set of an interval abstraction may be costly (e.g., MILP is known to be NP-hard). This is expected: Theorems 1 and 2 show that there is no polynomial time algorithm able to return an exact estimate of the fraction of plausible shifts for which the CFX is robust (hence a fortiori deciding whether it is \\(\\triangle\\)-robust), unless P=NP. In the following, we propose a novel certification approach that aims to alleviate this problem."}, {"title": "A Provable Probabilistic Approach", "content": "One possible idea to avoid exact reachable set computation to determine the robustness of a CFX under PMS is to use naive interval propagation. Given an input CFX, we propagate this input through the network, keeping track of all the possible activation values that can be obtained under \\(\\triangle\\) until the output layer is reached. However, the non-linear and non-convex nature of DNNs may result in a significant overestimation of the actual reachable set, thus resulting in a spurious decision of non-robustness. In such cases, a CFX may end up being labeled as non-robust even though the CFX is actually robust. Additionally, even with exact methods, a CFX may be discarded even though the fraction of plausible model shifts in \\(\\triangle\\) for which the CFX is not robust is negligible.\nTo avoid these problems, we propose an approximate certification approach based on Monte-Carlo sampling that draws sample realizations directly from \\(\\triangle\\) to obtain an underestimation of the space of possible classifications under PMS. The idea of using a sample-based approach stems from the fact that the \\(\\triangle\\) set, representing all the plausible model shifts, abstracts an infinite number of models to test. As testing this infinite number of models may be impossible in practice, efficient sampling-based solutions hold great promise. In detail, given a CFX x' we can compute an underestimation of the output reachable set under \\(\\triangle\\) by sampling n random realizations \\(M_{\\theta_1},..., M_{\\theta_n}\\) from \\(\\triangle\\), and compute the output reachable set by taking, respectively, the \\(\\min_i M_{\\theta_i}(x')\\) and the \\(\\max_i M_{\\theta_i}(x')\\) for \\(i \\in \\{1, ..., n\\}\\).\nThis approach is very effective and allows us to obtain an estimate of the output reachable set without using an exact solver. Nonetheless, the number n of realization to sample in order to achieve a good reachable set estimation remains unclear, as well as what kind of guarantees one could obtain from this approach. To answer these questions, we leverage previous results on the statistical prediction of tolerance limits [37, 25, 21]. Indeed, we observe that for each realization \\(M_\\theta\\) sampled from \\(\\triangle\\), the resulting output of the DNN \\(M_\\theta(x')\\) can be interpreted as an instantiation of a random variable X whose tolerance interval we are trying to estimate. Following this observation, we can derive a probabilistic bound on the correctness of the solution returned from n samples, using the following lemma based on [37]:\nLemma 4. Fix an integer n > 0 and an approximation parameter R\u2208 (0,1). Given a sample of n models \\(M_{\\theta_1},..., M_{\\theta_n}\\) from the (continuous) set of possible realizations \\(\\triangle\\), the probability that for at least a fraction R of the models in a further possibly infinite sequence of samples \\(M_{\\theta_1}^2... M_{\\theta_m}^2\\) from \\(\\triangle\\) we have\n\\[\\min M_{\\theta_i}^2(x) \\geq \\min M_{\\theta_i}(x) \\tag{2}\\]\n\\[\\text{(respectively } \\max. M_{\\theta_i}^2(x) \\leq \\max M_{\\theta_i}(x)\\text{)}\\]\nis given by \\(\\alpha = n \\cdot \\int_R^1 x^{n-1} dx = 1 - R^n\\).\nInformally, Lemma 4 allows us to derive the minimum number of realizations n needed to guarantee that at least R models within \\(\\triangle\\) satisfy the robustness property with probability \\(\\alpha\\). Therefore, using these n realizations, we can obtain an underestimation of the reachable set that is correct with confidence \\(\\alpha\\) for at least a fraction R of indefinitely large further realizations of models from \\(\\triangle\\). In practice, if we set, e.g. \\(\\alpha = 0.999\\) and \\(R = 0.995\\), we can derive n as \\(n = \\frac{\\log(1-\\alpha)}{\\log(R)} = 1378\\). After having selected 1378 random realizations from \\(\\triangle\\), if the lower bound of the underestimated reachable set computed as \\(\\min_i M_{\\theta_i}(x')\\) is greater than 0.5, then with probability \\(\\alpha = 0.999\\), R is a lower bound on the fraction of plausible model shifts in \\(\\triangle\\) for which x' is robust. In other words, Lemma 4 allows us to assert with a confidence \\(\\alpha\\) that x' is not \\(\\triangle\\)-robust for at most a fraction \\((1 - R) = 0.05\\) of models from \\(\\triangle\\)."}, {"title": "The APAS Algorithm", "content": "Using the result of Lemma 4, we now present our approximation method APAS to generate probabilistic robustness guarantees. The procedure, shown in Algorithm 1, receives as input a model \\(M_\\theta\\), a CFX x' for which robustness guarantees are sought, and the two confidence parameters \\(\\alpha, R\\). The algorithm then searches for the largest \\(\\delta_{max}\\) such that the CFX x' is robust for at least a fraction R of the plausible model shifts up to \\(\\delta_{max}\\) with probability \\(\\alpha\\).\nThe algorithm starts by computing the size n of a sample of realizations that is sufficient to guarantee the condition in Lemma 4 (line 3). APAS then initializes a small \\(\\delta_{init}\\) and checks if x' is at least robust to a small model shift. To this end, it employs realizations \\((M_\\theta, x', \\delta, n)\\) which samples n realizations, perturbating each model parameter by at most a factor \\(\\delta\\) and checks if for each of these realization \\(M_{\\theta_i}(x') \\geq 0.5\\), thus computing a robustness rate. If not all these realizations result in a robust outcome, thus achieving a final rate not equal to 1, the algorithm discards the CFX x' as non-robust (lines 6-8). Otherwise, it combines an exponential search (lines 9-12) and a binary search (lines 13-24) to find \\(\\delta_{max}\\). At each step of this search, the procedure checks whether for each of the n realizations from \\(\\triangle = \\{S | d_p(M_\\theta, S(M_\\theta)) \\leq \\delta_{max}\\}\\) holds \\(M_{\\theta_i}(x') \\geq 0.5\\).\nProposition 5. Given a model \\(M_\\theta\\) and a CFX x', let \\(\\delta^*\\) be the (exact) maximum magnitude of model shifts such that x' is robust with respect to the set of PMS \\(\\delta^*\\). Then, with probability \\(\\alpha\\), APAS returns a \\(\\delta_{max} \\geq \\delta^*\\) such that the CFX x' is robust for at least a fraction R of the set of PMS \\(\\delta_{max}\\) Moreover, the computation of \\(\\delta_{max}\\) is polynomial.\nProof sketch. The \\(\\delta_{max}\\) returned by the algorithm is obtained by iteratively increasing \\(\\delta\\), sampling n models from the corresponding \\(\\triangle_\\delta\\) and verifying that \\(M_{\\theta_i}(x) \\geq 0.5\\) for each model \\(M_{\\theta_i}\\) sampled. By definition, \\(\\delta^*\\) is the actual value we are trying to estimate. Therefore, for each \\(\\delta < \\delta^*\\), we will always obtain realizations for which \\(M_{\\theta_i}(x') \\geq 0.5\\). Therefore, APAS will always return \\(\\delta_{max}\\) values that are at least equal to \\(\\delta^*\\). Once the exponential search ends, by exploiting Lemma 4, we can state that with probability \\(\\alpha\\), the CFX x' is robust for at least R of any infinite further realizations from \\(\\triangle_{\\delta_{max}}\\). The time complexity of the algorithm corresponds to nm forward propagations, with n being the sample size and \\(m = log \\frac{\\delta_{max}}{\\delta_{init}}\\) being the number of iterations of the exponential search, which is polynomial in the input size of the problem."}, {"title": "Experimental Analysis", "content": "Section 6 laid the theoretical foundations of a novel sampling-based method that allows to obtain provable probabilistic guarantees on the robustness of CFXs. In this section", "experiments": "n\\begin{itemize"}, "n    \\item In \u00a7 7.1 we show how to instantiate APAS in practice using a synthetic example. Specifically, we first demonstrate the interplay of parameters n, \\(\\alpha\\) and R used to obtain a probabilistic guarantee. Then, using the maximum \\(\\delta_{max}\\) discovered by APAS, we formally enumerate the number of models inside a set of PMS \\(\\triangle_{\\delta_{max}}\\) for which a given CFX x' is not robust. We show that this percentage is at most a fraction \\((1 - R)\\), empirically confirming our theoretical results.\n    \\item In \u00a7 7.2 we compare our certification approach with the one proposed in [12]. In particular, we focus```json\n{\n  \"title\": \"Rigorous Probabilistic Guarantees for Robust Counterfactual Explanations\",\n  \"authors\": [\n    \"Luca Marzari\",\n    \"Francesco Leofante\",\n    \"Ferdinando Cicalese\",\n    \"Alessandro Farinelli\"\n  ],\n  \"abstract\":", "We study the problem of assessing the robustness of counterfactual explanations for deep learning models. We focus on plausible model shifts altering model parameters and propose a novel framework to reason about the robustness property in this setting. To motivate our solution, we begin by showing for the first time that computing the robustness of counterfactuals with respect to plausible model shifts is NP-complete. As this (practically) rules out the existence of scalable algorithms for exactly computing robustness, we propose a novel probabilistic approach which is able to provide tight estimates of robustness with strong guarantees while preserving scalability. Remarkably, and differently from existing solutions targeting plausible model shifts, our approach does not impose requirements on the network to be analyzed, thus enabling robustness analysis on a wider range of architectures. Experiments on four binary classification datasets indicate that our method improves the state of the art in generating robust explanations, outperforming existing methods on a range of metrics.", "sections\": [\n    {\n      \"title\": \"Introduction", "content\": \"Understanding and interpreting the decisions of black-box deep learning models has become a dominant goal of Explainable AI (XAI). Several strategies have been proposed to this end. In this paper, we focus on counterfactual explanations (CFX) (see [30, 15] for recent surveys on the topic), which aim to demystify the decision-making of a Deep Neural Network (DNN) by showing how an input needs to be changed to yield a different, typically more desirable, decision. Consider the widely used example of a loan application, where a mortgage applicant represented by an input x with features unemployed status, 25 years of age, and low credit rating applies for a loan and is rejected by the bank's AI. A CFX for this decision could be a slightly modified input, where increasing credit rating to medium would result in the loan being granted.\nAs CFXs have the potential to influence decisions with strong societal implications, their reliability has become the subject of intensive study (see [13] for a survey). In particular, recent work has highlighted issues related to the robustness of CFXs against Plausible Model Shifts (PMS) [32, 12], showing that the validity of CFXs is likely to be compromised when bounded perturbations are applied to the parameters of a DNN, e.g., as a result of fine-tuning [32, 1, 22, 12, 10]. Consider the loan example: if retraining occurs while the applicant is working toward improving their credit rating, without robustness, their modified case may still result in a rejected application, leaving the bank liable due to their conflicting statements.\nIn this paper, we focus on this troubling phenomenon and advance the state of the art in CFX robustness research in several directions. More specifically, in \u00a7 4, we study the computational complexity of exactly determining whether a CFX is robust to PMS, formally showing for the first time that answering this question is NP-complete. As our result rules out the existence of practical algorithms to exactly compute the CFX robustness, we shift our attention toward methods for obtaining probabilistic guarantees on the robustness of CFX under model shifts. We, therefore, consider the work by Hamman et al. [10], where they propose a probabilistic approach to compute the robustness of CFX under Naturally-Occurring Model Shifts (NOMS). Even though both PMS and NOMS notions are commonly used in the literature, very little is known about their potential interplay, and the question of whether robustness to NOMS subsumes robustness to PMS is still unresolved. We settle this question in \u00a7 5, where we show that these two notions capture profoundly different scenarios, proving that robustness guarantees given for NOMS do not directly extend to PMS. Having settled this, in \u00a7 6 we present \u0391\u03a1AS, a novel sampling-based certification algorithm which allows to determine a provable probabilistic bound on the maximum shift a CFX can tolerate under PMS. Unlike existing solutions for robustness under PMS, our approach comes with significantly reduced computational requirements and does not make any assumption on the underlying DNN, thus making it applicable to a wider range of architectures. Finally, to assess the effectiveness of our proposed solution, in \u00a7 7, we study the general performance of our certification algorithm and provide a comprehensive comparison of the proposed approach against several state-of-the-art methodologies for CFX robustness certification. Crucially, we show that our approach can also be used to generate robust CFXs, outperforming existing methods on a number of metrics from the CFX literature."], "content": "Various methods for generating CFXs for DNNs have been proposed. The seminal work of [36] framed the task of generating CFXs as a gradient-based optimization problem and proposed a loss that promotes CFX validity (i.e., the CFX successfully changes the classification outcome of the network) and proximity (i.e., the CFX is as close as possible to the original input for some distance metric). In addition to these metrics, other important properties have been highlighted as crucial for the practical applicability of CFXs. Prominent examples include plausiblity (i.e., the CFX must lie on the data manifold) [26, 23] and actionability (i.e., the changes suggested by the CFX must be achievable by the user in practice) [33]. Differently from these works, here we focus on the robustness property of CFXs.\nSeveral forms of CFX robustness have been studied in the literature [13]. Robustness to input changes is the focus of, e.g. [28, 3, 38, 18], where solutions are devised to ensure that explanation algorithms return similar CFXs for similar inputs. In another line of work, [9, 35, 17, 24] considered the problem of generating adversarially robust CFXs that preserve validity under imperfect (or noisy) execution. Robustness to model multiplicity is instead considered in, e.g. [23, 19, 14], where CFXs that preserve validity across sets of models are sought. However, the study of these forms of robustness is outside the scope of this paper as our focus is on model shifts. Robustness to model shift has been studied in, e.g. [32, 1, 22, 12, 10]. Of these, the approaches of [32] and [12] are the most closely related to our work. The former presents an approach to generate robust CFXs under PMS using techniques from continuous optimization, which is able to guarantee robustness in the average-case scenario. The latter instead solves the same problem using abstraction techniques and discrete optimization tools, obtaining robustness guarantees that hold under worst-case conditions. Given their relevance, both approaches will be considered for an extensive experimental comparison in \u00a7 7."}, {"title": "Background", "content": "(Neural) Classification model. Let \\(X \\subset \\mathbb{R}^d\\) denote the input space of a classifier \\(M_\\theta: X \\rightarrow [0,1]\\) mapping an input \\(x \\in X\\) to an output probability between 0 and 1. We consider classifiers implemented by feed-forward DNNs parameterized by a (parameter) vector \\(\\theta \\in \\Theta \\subseteq \\mathbb{R}^k\\). Given two parameter vectors \\(\\theta, \\theta' \\in \\Theta\\), we refer to the corresponding classifiers \\(M_\\theta\\) and \\(M_{\\theta'}\\) as instantiations of the same parametric classifier \\(M_\\theta\\). We assume concrete valuations of \\(\\theta\\) are learned from a set of labeled inputs as customary in supervised learning settings [7]. Once \\(\\theta\\) has been learned, the classifier can be used for inference. Without any loss of generality, we focus on binary classification tasks, i.e., the classification decision produced by \\(M_\\theta\\) for an unlabeled input x is 1 if \\(M_\\theta(x) \\geq 0.5\\), and 0 otherwise.\nCounterfactual explanations. Existing methods in the literature define CFXs as follows.\nDefinition 1. Consider an input \\(x \\in X\\) and a classifier \\(M_\\theta\\) s.t. \\(M_\\theta(x) < 0.5\\). Given a distance metric \\(d: X \\times X \\rightarrow \\mathbb{R}^+\\), a (valid) counterfactual explanation is any \\(x'\\) such that:\n\\[\\begin{array}{c}\n\\arg \\min_{x' \\in X} d(x, x') \\\\\n\\text{subject to } M_\\theta(x') \\geq 0.5\n\\end{array}\\]\nIntuitively, given an input x for which the classifier produces a negative outcome, a counterfactual explanation is a new input x' which is similar to x, e.g., in terms of some specified distance between features values, and for which the classifier predicts a different outcome. Common choices for d include the \\(l_1\\) and \\(l_\\infty\\) norms [36], which will also be used in this work.\nRobustness to model shifts Among several notions of robustness, recent work has placed emphasis on generating CFXs that remain valid under (slight) shifts in the classifier they were generated for. While existing approaches rely on a diverse range of techniques to solve this problem, they all share a common understanding of what constitutes a model shift, which we present next.\nDefinition 2 (Jiang et al. [12]). Let \\(M_\\theta\\) and \\(M_{\\theta'}\\) be two instantiations of a parametric classifier \\(M_\\theta\\). For \\(0 < p < \\infty\\), the p-distance between \\(M_\\theta\\) and \\(M_{\\theta'}\\) is defined as \\(d_p(M_\\theta, M_{\\theta'}) = ||\\theta - \\theta' ||_p\\).\nDefinition 3 (Jiang et al. [12]). A model shift (w.r.t. a fixed p-distance) is a function S mapping a classifier \\(M_\\theta\\) into another classifier \\(M_1 = S(M_\\theta)\\) such that:\n\\begin{itemize}\n    \\item \\(M_\\theta\\) and \\(M_{\\theta'}\\) are instantiations of the same \\(M_\\theta\\);\n    \\item \\(d_p(M_\\theta, M_{\\theta'}) > 0\\).\n\\end{itemize}\nInformally, a model shift captures changes in the parameters of a DNN, but does not affect its architecture. Based on this definition, we can formalize the robustness property for a CFX as follows.\nDefinition 4. Consider an input \\(x \\in X\\) and a classifier \\(M_\\theta\\) s.t. \\(M_\\theta(x) < 0.5\\). Let x' be a counterfactual explanation computed for x s.t. \\(M_\\theta(x') \\geq 0.5\\). Given a set of model shifts \\(\\triangle\\), we say that the counterfactual x' is \\(\\triangle\\)-robust if \\(S(M_\\theta)(x') \\geq 0.5\\) for all \\(S \\in \\triangle\\).\nThe definition of a model shift can be specialized to better characterize how \\(\\theta\\) is allowed to change under S. In the following, we report two most commonly studied notions of model shifts: Naturally-Occurring Model Shifts and Plausible Model Shifts.\nDefinition 5 (Hamman et al. [10] (NOMS)). Consider a classifier \\(M_\\theta\\). A set of model shifts is said to be naturally occurring if for a (randomly) chosen model shift S from \\(\\triangle\\) and \\(M_{\\theta'} = S(M_\\theta)\\) being the new classifier obtained after applying S to \\(M_\\theta\\) the following hold:\n\\begin{itemize}\n    \\item \\(\\mathbb{E}[M_{\\theta'}(x)] = M_\\theta(x)\\); where the expectation is over the randomness of \\(M_{\\theta'}\\) given a fixed value of x;\n    \\item \\(\\text{Var}[M_{\\theta'}(x)] = v_x\\), where \\(v_x\\) represents the maximum variance of the prediction of \\(M_{\\theta'}(x)\\), and whenever x lies on the data manifold X, \\(v_x\\) is upper bounded by a small constant v;\n    \\item If \\(M_\\theta\\) is Lipschitz continuous for some \\(\\Upsilon_1\\), then \\(M_{\\theta'}(x)\\) is also Lipschitz continuous for some \\(\\Upsilon_2\\).\n\\end{itemize}\nBroadly speaking, a naturally-occurring model shift allows the application of arbitrary changes to \\(\\theta\\) as long as the overall behavior of the classifier is not affected. This is in contrast with the notion of plausible model shift [32, 12], which requires changes to be bounded.\nDefinition 6 (Jiang et al. [12] (PMS)). Consider a classifier \\(M_\\theta\\) and a new classifier \\(M_1 = S(M_\\theta)\\) obtained after applying a model shift S to \\(M_\\theta\\). Given some \\(\\delta \\in \\mathbb{R}_{>0}\\) and \\(0 < p < \\infty\\), S is said to be plausible (w.r.t. the choice of parameters \\(\\delta\\) and p) if \\(d_p(M_\\theta, S(M_\\theta)) < \\delta\\).\nGiven an upper bound \\(\\delta\\), a set of PMS \\(\\triangle\\) can then be obtained by considering all shifts S that satisfy Definition 6, i.e. \\(\\triangle = \\{S | d_p(M_\\theta, S(M_\\theta)) \\leq \\delta\\}\\). In the rest of this paper, we will sometimes use \\(\\triangle_\\delta\\) to make the upper bound \\(\\delta\\) explicit. Given a set \\(\\triangle\\), a realization of \\(\\triangle\\) can be defined as follows.\nDefinition 7. Given a classifier \\(M_\\theta\\), and a set of plausible model shifts, we say that a realization of \\(\\triangle\\) is a classifier \\(M_{\\theta'}\\) such that \\(\\theta' \\in [\\theta - \\delta, \\theta + \\delta]\\)"}, {"title": "Checking Robustness is Hard", "content": "In this section, we study the computational complexity of deciding whether a given counterfactual explanation is robust in the presence of model shifts. Our aim here is to better understand the computational challenges arising from this problem and to use these results to guide the development of novel, more efficient certification procedures. Without loss of generality, we consider PMS to encode the problem. Deciding whether a given CFX x' is robust to a set of PMS \\(\\triangle\\) requires to check whether \\(\\triangle\\) contains at least one realization which yields a classification outcome that is different from the intended CFX outcome. For instance, in the case of the loan example, this would correspond to a model rejecting the loan (\\(M_\\theta(x') < 0.5\\)) as opposed to accepting it as intended. This problem can be formulated as follows.\nDISTINCT-REALIZATIONS PROBLEM (DRP)\nInput: a classifier \\(M_\\theta\\), a set of PMS, an input x, and a threshold T.\nOutput: yes \\(\\exists M_{\\theta_1}, M_{\\theta_2} \\in \\triangle: M_{\\theta_1}(x) < T < M_{\\theta_2}(x)\\).\nTheorem 1. Deciding DRP is NP-complete.\nProof sketch. The inclusion of DRP in NP requires two forward propagations of \\(\\theta\\) through two concretizations (i.e., the certificates) checking if T is between the two computed outputs. This is clearly polynomial in the size of the classifier. Regarding the hardness, we can show that 3-SAT reduces to DRP. Given a formula \\(\\phi\\) we can produce a DNN classifier \\(M_\\theta\\), an input x and a \\(\\delta\\) (maximum shift on the edge weights), defining a \\(\\triangle\\) such that \\(\\phi\\) is satisfiable if and only if there exists another \\(M_{\\theta'}\\) which is also a realization of \\(\\triangle\\) such that \\(0.5 \\leq M_\\theta(x)\\) and \\(M_\\theta(x) < 0.5\\).\nWe build on the reduction of [16], which, given a formula \\(\\phi\\) produces a neural network such that satisfying assignments for \\(\\phi\\) are encoded into inputs to the network producing a desired output.\nA first observation is that if we start with the neural network \\(M_\\theta\\) produced by the reduction of [16], and replace any weight equal to 1 with the interval \\([1 - 2\\delta, 1]\\) and any weight -1 with the interval \\([-1 - 2\\delta, -1]\\), we obtain \\(\\triangle\\) such that for any \\(M_{\\theta'}\\) being a realization of \\(\\triangle\\) and any input x, \\(M_{\\theta'}(x) \\in [(1 - p_1(\\delta))M_\\theta(x), (1 + p_2(\\delta))M_\\theta(x)]\\), where \\(p_1, p_2\\) are some fixed polynomials defined by the number of layers in \\(M_\\theta\\). Therefore, we can extend the hardness of [16] to the case of plausible model shift, where instead of checking for the output to be t, we could use an additional layer to check whether the output is in the interval \\([(1 - p_1(\\delta))t, (1 + p_2(\\delta))t]\\).\nHowever, with respect to the hardness proof of [16], there is a substantial difference in our problem since, besides starting from a DNN with each weight in \\([\\theta_i - \\delta, \\theta_i + \\delta]\\), we want to map satisfying assignments to realizations of the \\(\\triangle\\), rather than inputs of the network. In fact, encoding assignments to choices of the weights turns out to require significantly more.\nOur reduction is designed so that the assignments to the variables of the CNF are encoded to the choices of some specific weights of the DNN, henceforth referred to as the network edge main inputs. These are the weights that determine the outputs of a gadget that we call the generating gadget. A generating gadget has a fixed input (representing the CFX), uses only intervals of width \\(2 \\cdot \\delta\\), and produces a value in [0, 1] where the Boolean false is represented by a value close to 0 and the Boolean true is represented by a value close to 1.\nThe output of the generating gadget (one per each variable of the formula ) is sent both to the network simulating the CNF formula (as in [16]) and to a further gadget (the discretizer-gadget) that controls whether the output of the generating gadget is a discrete value in {0, 1}.\nThe output of the network simulating the formula is then combined with the output of the discretizer-gadgets in such a way that the final output is < 0.5 whenever, either there is one of the outputs of the generating-gadgets (determined by the choice of the network edge main input) which is not in {0, 1} or the output of the subnetwork simulating the formula implies that the network edge main input encode and assignment not satisfying some clause of \\(\\phi\\).\nFrom Theorem 1, it follows that deciding whether a CFX x' is not robust to a set of PMS \\(\\triangle\\) is NP-complete. This results lead to the following corollary.\nTheorem 2. Given a model \\(M_\\theta\\) and a set of plausible model shifts \\(\\triangle\\), computing the number of model shifts in \\(\\triangle\\) for which a given CFX x' is \\(\\triangle\\)-robust is NP-hard.\nIndeed, deciding non-\\(\\triangle\\)-robustness and \\(\\triangle\\)-robustness are equivalent under Turing reductions. Therefore, the counting problem for \\(\\triangle\\)-robustness is at least as hard as the decision problem for non-\\(\\triangle\\)-robustness. This hardness result motivates an approximate approach to estimate the robustness of a counterfactual under a set of PMS \\(\\triangle\\)."}, {"title": "Probabilistic Guarantees for Existing Notions of Model Shifts", "content": "As we have established in the previous section, exact methods for computing robustness under model shifts are bound to lack scalability. This motivates the design of probabilistic approaches to solve the problem. Previous work by Hamman et al. [10] presented an approach to obtain counterfactual explanations that are probabilistically robust under NOMS. A natural question that arises then is whether guarantees obtained for NOMS also transfer to the PMS setting. As we show for the first time below, this is not the case in general.\nLemma 3. Naturally-occurring model shifts may not be Plausible, and vice-versa.\nProof. Consider the DNN \\(M_\\theta\\) depicted with two input nodes, one hidden layer with two ReLU nodes and one single output. The parameters \\(\\theta = [w_1,..., w_6]\\) are the weights on the edges listed top-bottom and left-right.\nPropagating an input vector \\(x = [x_1,x_2]^T\\) through \\(M_\\theta\\), we obtain \\(M_\\theta(x) = y = w_5 \\max\\{0, w_1 \\cdot x_1 + x_2 \\cdot w_3\\} + w_6 \\cdot \\max\\{0, w_2 \\cdot x_1 + x_2 \\cdot w_4\\}\\). Now assume an input vector \\(x = [0.9, 0.9]\\) and weights \\(w_1 = 1, w_2 = 0, w_3 = 0, w_4 = 0.6, w_5 = 1, w_6 = -1\\). The corresponding output generated by the DNN is \\(M_\\theta(x) = 0.46\\). A counterfactual for x could be given as a new input vector \\(x' = [1,0.8]^T\\), for which we obtain \\(M_\\theta(x') = 0.52 > 0.5\\).\nNow, following Definition 6, we consider a set of plausible model changes obtained for \\(\\delta = 0.3\\). This can be captured by defining on each weight \\(w_i\\) the corresponding interval in depicted that represents the set of all the possible models obtained from \\(M_\\theta\\), replacing each \\(w_i\\) with a weight in the interval \\([w_i - \\delta, w_i + \\delta]\\). We then have that, the expected result of a model sampled uniformly from such a set satisfies:\n\\begin{align*}\n\\mathbb{E}[M_{\\theta'}(x')] &= \\mathbb{E}[w_5] \\mathbb{E}[ReLU(x_1w_1 + x_2w_3)] + \\\\\n& \\mathbb{E}[w_6] \\mathbb{E}[ReLU(x_1w_2 + x_2 \\cdot w_4)] \\\\\n&= \\mathbb{E}[[0.7, 1.3]] \\mathbb{E} [\\max\\{0, x_1 [0.7, 1.3] + \\\\\n&x_2[-0.3, 0.3]\\}] + \\mathbb{E}[[-1.7, -0.3]] \\cdot \\\\\n&\\mathbb{E}[\\max\\{0,x_1[-0.3, 0.3] + x_2 [0.3, 0.9]\\}] \\\\\n&> 0.52 \\neq M_\\theta(x')\n\\end{align*}\nDefinition 5 states that a model shift is naturally occurring if \\(\\mathbb{E}[M_{\\theta'}(x)] = M_\\theta(x)\\). This implies that \\(\\triangle\\) contains models that cannot be characterized as naturally occurring model changes. Vice versa, the existence of Naturally occurring model shifts not being plausible is implicit in the definition thus giving our result."}, {"title": "Robustness under PMS with Probabilistic Guarantees", "content": "Jiang et al. [12] proposed to use INNs to enable a compact representation of a superset of the models that can be obtained by a perturbation of the starting model under a set \\(\\triangle\\). By exploiting an exact reachable set computation method, e.g., based on MILP [31], the authors could determine whether or not a CFX is robust under the chosen \\(\\triangle\\) via a single forward propagation of the CFX. However, in view of the NP-hardness of the problem discussed in the \u00a7 4 and the typical non-linear nature of the classifiers, it presents some computational limitations.\nIn general, interval neural networks map inputs to intervals representing an over-approximation of all possible outcomes that can be produced by any shifted model \\(M_{\\theta'}\\) obtained under \\(\\triangle\\). Given this property, if the output reachable set is completely disjoint from the decision threshold 0.5, then one can assert - in a sound and complete fashion - whether or not a given CFX is robust. On the other hand, if we run into a situation such as the one depicted in Fig. 2 (b), one cannot assert robustness with certainty. In this scenario, Jiang et al. [12] propose to classify the CFX as not robust, which preserves the soundness of their result. Nonetheless, this might lead to discarding a CFX even when the actual density of (equivalently, the probability that after retraining, we incur in) plausible model shifts for which the CFX is not robust is extremely low. As we will show in \u00a7 7, this worst-case notion of robustness affects the CFXs generated by [12], which may end up being unnecessarily expensive (in terms of proximity) and having low plausibility. Additionally, computing the exact output reachable set of an interval abstraction may be costly (e.g., MILP is known to be NP-hard). This is expected: Theorems 1 and 2 show that there is no polynomial time algorithm able to return an exact estimate of the fraction of plausible shifts for which the CFX is robust (hence a fortiori deciding whether it is \\(\\triangle\\)-robust), unless P=NP. In the following, we propose a novel certification approach that aims to alleviate this problem."}, {"title": "A Provable Probabilistic Approach", "content": "One possible idea to avoid exact reachable set computation to determine the robustness of a CFX under PMS is to use naive interval propagation. Given an input CFX, we propagate this input through the network, keeping track of all the possible activation values that can be obtained under \\(\\triangle\\) until the output layer is reached. However, the non-linear and non-convex nature of DNNs may result in a significant overestimation of the actual reachable set, thus resulting in a spurious decision of non-robustness. In such cases, a CFX may end up being labeled as non-robust even though the CFX is actually robust. Additionally, even with exact methods, a CFX may be discarded even though the fraction of plausible model shifts in \\(\\triangle\\) for which the CFX is not robust is negligible.\nTo avoid these problems, we propose an approximate certification approach based on Monte-Carlo sampling that draws sample realizations directly from \\(\\triangle\\) to obtain an underestimation of the space of possible classifications under PMS. The idea of using a sample-based approach stems from the fact that the \\(\\triangle\\) set, representing all the plausible model shifts, abstracts an infinite number of models to test. As testing this infinite number of models may be impossible in practice, efficient sampling-based solutions hold great promise. In detail, given a CFX x' we can compute an underestimation of the output reachable set under \\(\\triangle\\) by sampling n random realizations \\(M_{\\theta_1},..., M_{\\theta_n}\\) from \\(\\triangle\\), and compute the output reachable set by taking, respectively, the \\(\\min_i M_{\\theta_i}(x')\\) and the \\(\\max_i M_{\\theta_i}(x')\\) for \\(i \\in \\{1, ..., n\\}\\).\nThis approach is very effective and allows us to obtain an estimate of the output reachable set without using an exact solver. Nonetheless, the number n of realization to sample in order to achieve a good reachable set estimation remains unclear, as well as what kind of guarantees one could obtain from this approach. To answer these questions, we leverage previous results on the statistical prediction of tolerance limits [37, 25, 21]. Indeed, we observe that for each realization \\(M_\\theta\\) sampled from \\(\\triangle\\), the resulting output of the DNN \\(M_\\theta(x')\\) can be interpreted as an instantiation of a random variable X whose tolerance interval we are trying to estimate. Following this observation, we can derive a probabilistic bound on the correctness of the solution returned from n samples, using the following lemma based on [37]:\nLemma 4. Fix an integer n > 0 and an approximation parameter R\u2208 (0,1). Given a sample of n models \\(M_{\\theta_1},..., M_{\\theta_n}\\) from the (continuous) set of possible realizations \\(\\triangle\\), the probability that for at least a fraction R of the models in a further possibly infinite sequence of samples \\(M_{\\theta_1}^2... M_{\\theta_m}^2\\) from \\(\\triangle\\) we have\n\\[\\min M_{\\theta_i}^2(x) \\geq \\min M_{\\theta_i}(x) \\tag{2}\\]\n\\[\\text{(respectively } \\max. M_{\\theta_i}^2(x) \\leq \\max M_{\\theta_i}(x)\\text{)}\\]\nis given by \\(\\alpha = n \\cdot \\int_R^1 x^{n-1} dx = 1 - R^n\\).\nInformally, Lemma 4 allows us to derive the minimum number of realizations n needed to guarantee that at least R models within \\(\\triangle\\) satisfy the robustness property with probability \\(\\alpha\\). Therefore, using these n realizations, we can obtain an underestimation of the reachable set that is correct with confidence \\(\\alpha\\) for at least a fraction R of indefinitely large further realizations of models from \\(\\triangle\\). In practice, if we set, e.g. \\(\\alpha = 0.999\\) and \\(R = 0.995\\), we can derive n as \\(n = \\frac{\\log(1-\\alpha)}{\\log(R)} = 1378\\). After having selected 1378 random realizations from \\(\\triangle\\), if the lower bound of the underestimated reachable set computed as \\(\\min_i M_{\\theta_i}(x')\\) is greater than 0.5, then with probability \\(\\alpha = 0.999\\), R is a lower bound on the fraction of plausible model shifts in \\(\\triangle\\) for which x' is robust. In other words, Lemma 4 allows us to assert with a confidence \\(\\alpha\\) that x' is not \\(\\triangle\\)-robust for at most a fraction \\((1 - R) = 0.05\\) of models from \\(\\triangle\\)."}, {"title": "The APAS Algorithm", "content": "Using the result of Lemma 4, we now present our approximation method APAS to generate probabilistic robustness guarantees. The procedure, shown in Algorithm 1, receives as input a model \\(M_\\theta\\), a CFX x' for which robustness guarantees are sought, and the two confidence parameters \\(\\alpha, R\\). The algorithm then searches for the largest \\(\\delta_{max}\\) such that the CFX x' is robust for at least a fraction R of the plausible model shifts up to \\(\\delta_{max}\\) with probability \\(\\alpha\\).\nThe algorithm starts by computing the size n of a sample of realizations that is sufficient to guarantee the condition in Lemma 4 (line 3). APAS then initializes a small \\(\\delta_{init}\\) and checks if x' is at least robust to a small model shift. To this end, it employs realizations \\((M_\\theta, x', \\delta, n)\\) which samples n realizations, perturbating each model parameter by at most a factor \\(\\delta\\) and checks if for each of these realization \\(M_{\\theta_i}(x') \\geq 0.5\\), thus computing a robustness rate. If not all these realizations result in a robust outcome, thus achieving a final rate not equal to 1, the algorithm discards the CFX x' as non-robust (lines 6-8). Otherwise, it combines an exponential search (lines 9-12) and a binary search (lines 13-24) to find \\(\\delta_{max}\\). At each step of this search, the procedure checks whether for each of the n realizations from \\(\\triangle = \\{S | d_p(M_\\theta, S(M_\\theta)) \\leq \\delta_{max}\\}\\) holds \\(M_{\\theta_i}(x') \\geq 0.5\\).\nProposition 5. Given a model \\(M_\\theta\\) and a CFX x', let \\(\\delta^*\\) be the (exact) maximum magnitude of model shifts such that x' is robust with respect to the set of PMS \\(\\delta^*\\). Then, with probability \\(\\alpha\\), APAS returns a \\(\\delta_{max} \\geq \\delta^*\\) such that the CFX x' is robust for at least a fraction R of the set of PMS \\(\\delta_{max}\\) Moreover, the computation of \\(\\delta_{max}\\) is polynomial.\nProof sketch. The \\(\\delta_{max}\\) returned by the algorithm is obtained by iteratively increasing \\(\\delta\\), sampling n models from the corresponding \\(\\triangle_\\delta\\) and verifying that \\(M_{\\theta_i}(x) \\geq 0.5\\) for each model \\(M_{\\theta_i}\\) sampled. By definition, \\(\\delta^*\\) is the actual value we are trying to estimate. Therefore, for each \\(\\delta < \\delta^*\\), we will always obtain realizations for which \\(M_{\\theta_i}(x') \\geq 0.5\\). Therefore, APAS will always return \\(\\delta_{max}\\) values that are at least equal to \\(\\delta^*\\). Once the exponential search ends, by exploiting Lemma 4, we can state that with probability \\(\\alpha\\), the CFX x' is robust for at least R of any infinite further realizations from \\(\\triangle_{\\delta_{max}}\\). The time complexity of the algorithm corresponds to nm forward propagations, with n being the sample size and \\(m = log \\frac{\\delta_{max}}{\\delta_{init}}\\) being the number of iterations of the exponential search, which is polynomial in the input size of the problem."}, {"title": "Experimental Analysis", "content": "Section 6 laid the theoretical foundations of a novel sampling-based method that allows to obtain provable probabilistic guarantees on the robustness of CFXs. In this section", "experiments": "n\\begin{itemize}\n    \\item In \u00a7 7.1 we show how to instantiate APAS in practice using a synthetic example. Specifically, we first demonstrate the interplay of parameters n, \\(\\alpha\\) and R used to obtain a probabilistic guarantee. Then, using the maximum \\(\\delta_{max}\\) discovered by APAS, we formally enumerate the number of models inside a set of PMS \\(\\triangle_{\\delta_{max}}\\) for which a given CFX x' is not robust. We show that this percentage is at most a fraction \\((1 - R)\\), empirically confirming our theoretical results.\n    \\item In \u00a7 7.2 we compare our certification approach with the one proposed in [12]. In particular, we focus"}]